\documentclass[sigconf]{acmart}

%% Metadata Information
\title{RUBICON: Intepretable Reward Models via Rubric Learning.}
\author{Anonymous Author(s)}
\affiliation{
  \institution{Institution Name}
  \city{City}
  \country{Country}
}
\email{author@email.com}

\begin{document}

\begin{abstract}
Alignment of language models is increasingly important as we apply LLMs to project-level work. This paper outlines the requirements for alignment in such contexts, introduces rubrics as a potential solution framework, and sketches a research direction for interpretable, test-time adaptive, and uncertainty-aware alignment systems.
\end{abstract}

\maketitle

\section{Introduction}

Large language models have demonstrated remarkable capabilities across diverse domains, from legal reasoning~\cite{legalbench2023} to software engineering~\cite{swebench2024}. As these systems are deployed for increasingly long-horizon tasks—spanning hours or days rather than single interactions~\cite{metr2025longtasks}—maintaining alignment with stakeholder preferences throughout task execution becomes critical. However, preferences in real-world deployment settings are rarely static or fully specified upfront. They emerge incrementally, shift as work progresses~\cite{preference_drift2024}, and often conflict across multiple stakeholders~\cite{magym2025}.

This challenge is compounded by fundamental limitations in how current systems model preferences. Training-time alignment methods such as RLHF~\cite{instructgpt2022} learn reward models from fixed datasets of pairwise comparisons, but these models degrade when preference distributions shift and are prone to reward over-optimization~\cite{goodhart2022}. Test-time approaches like Generative Reward Models~\cite{genrm2024} offer greater flexibility but remain black boxes, providing scalar evaluations without explaining which criteria drive their assessments. Recent work has exposed deeper issues: preference learning via Bradley-Terry models systematically fails under realistic conditions~\cite{bt_critique2024}, and LLM-based evaluation exhibits non-transitivity rates exceeding 60\%~\cite{elspr2025,nontransitivity2025}.

For trustworthy deployment on complex, long-running work, we argue that reward models must be both \emph{interpretable}—explicitly decomposing utility into human-understandable criteria—and \emph{test-time configurable}—adaptable to preferences that emerge or evolve during execution. Existing interpretable methods either use fixed reward dimensions~\cite{armorm2024} or require manual rubric construction~\cite{rubric_rl2025}, limiting their applicability to novel tasks.

We propose \textbf{RUBICON}, a framework for learning interpretable reward models through dynamically generated natural language rubrics. A rubric decomposes a stakeholder's utility into weighted, verifiable criteria (e.g., "output must be factually correct" with weight 0.4, "output should be concise" with weight 0.3). By learning to generate task-specific rubrics, our approach provides intrinsic interpretability while enabling test-time reconfiguration of evaluation priorities.

Our key contributions are: (1) We formalize rubric generation as a policy optimization problem within a manager-agent framework, where a manager agent learns to specify evaluation criteria that align worker agents with stakeholder utilities. (2) We show that rubric-based objectives unify multiple preference learning paradigms under a single loss family. (3) We develop a practical rubric-guided policy optimization algorithm that balances alignment with trust-region constraints. (4) We demonstrate that learned rubrics achieve competitive performance on long-horizon tasks while providing human-interpretable explanations and enabling test-time adaptation.


\section{Background}

\subsection{Preference Alignment Framework}

We consider a setting where a stakeholder requests work specified by input \(x\) and provides a natural-language preference statement \(p\) describing desired output properties (e.g., "be thorough and cite sources"). An agent produces output \(y\), and the stakeholder evaluates it according to an unobserved \emph{true utility} \(u^*(y \mid x, p)\). The agent's goal is to learn a \emph{proxy utility} \(\hat{u}(y \mid x, p)\) that approximates the stakeholder's true preferences. We seek to minimize the expected utility gap:
\begin{equation}
\mathcal{L}_U = \mathbb{E}_{(x,p,y) \sim \mathcal{D}}\Big[\big(u^*(y \mid x,p) - \hat{u}(y \mid x,p)\big)^2\Big].
\label{eq:utility-gap}
\end{equation}

This formulation generalizes both absolute utility estimation and relative preference learning. In the pairwise comparison setting, given outputs \(y_i\) and \(y_j\), the stakeholder prefers \(y_i\) if \(u^*(y_i \mid x,p) > u^*(y_j \mid x,p)\). Under Bradley-Terry assumptions, this induces a probabilistic preference model, but recent work shows these assumptions fail systematically in practice~\cite{bt_critique2024}.

Drawing on utility theory~\cite{savage1954}, we decompose \(u^*\) into interpretable components. Expected utility maximization assumes that preferences can be represented as weighted sums over outcome dimensions. This decomposition is particularly natural when stakeholders can articulate criteria (e.g., correctness, conciseness, politeness) even if they cannot assign precise weights upfront.


\subsection{Manager-Agent Formalism}

Following recent work on multi-agent orchestration~\cite{magym2025}, we model the alignment problem as a Partially Observable Stochastic Game (POSG) with two agents:

\begin{itemize}
\item \textbf{Manager agent}: Observes \((x, p)\) and generates a rubric \(R = \{(c_j, w_j)\}_{j=1}^M\), where each \(c_j\) is a natural-language criterion and \(w_j\) is its importance weight. The manager's policy is \(\pi_M(R \mid x, p)\).

\item \textbf{Worker agent}: Produces output \(y\) guided by the rubric. The worker's policy is \(\pi_W(y \mid x, R)\), typically realized by conditioning a base language model on rubric criteria.
\end{itemize}

The manager's objective is to maximize alignment by selecting rubrics that minimize the expected utility gap between worker outputs and stakeholder preferences:
\begin{equation}
R^* = \arg\max_R \mathbb{E}_{y \sim \pi_W(\cdot \mid x, R)}\big[u^*(y \mid x, p)\big].
\label{eq:manager-objective}
\end{equation}

This framing reveals that rubric generation is itself a \emph{policy learning problem}: the manager must learn which criteria to emphasize for different task-preference pairs. Critically, the manager's state includes both the task context and the (potentially evolving) preference specification, enabling adaptation to non-stationary preferences.


\subsection{Rubrics as Compositional Utilities}

A rubric \(R\) explicitly decomposes utility into verifiable criteria:
\begin{equation}
\hat{u}(y \mid x, p) = \sum_{j=1}^M w_j \cdot \nu_j(c_j, x, y),
\label{eq:rubric-utility}
\end{equation}
where \(\nu_j: \mathcal{C} \times \mathcal{X} \times \mathcal{Y} \to [0,1]\) is a verifier function measuring how well output \(y\) satisfies criterion \(c_j\) in context \(x\). Verifiers may be rule-based (e.g., length checks, citation counts) or model-based (e.g., factuality classifiers, entailment models).

This representation offers three key advantages over black-box reward models:

\paragraph{Interpretability.} Each criterion \(c_j\) is expressed in natural language (e.g., "output must contain at least three citations"), and weights \(w_j\) indicate relative importance. Stakeholders can inspect, validate, or override these components.

\paragraph{Compositionality.} The linear structure in Eq.~\ref{eq:rubric-utility} enables test-time reconfiguration: stakeholders can adjust weights or add criteria without retraining the underlying model. This contrasts with monolithic neural reward models where preference changes require collecting new comparison data and retraining.

\paragraph{Decomposed uncertainty.} By separating criteria generation (manager) from criterion verification (verifiers), we can independently quantify uncertainty in rubric specification versus measurement. This supports principled confidence estimation and active learning.

Our goal is to learn a decomposer \(\mathfrak{D}_\phi: \mathcal{X} \times \mathcal{P} \to \mathcal{R}\) that maps task-preference pairs to rubrics such that the proxy utility in Eq.~\ref{eq:rubric-utility} closely approximates the true utility \(u^*\).


\section{Related Work}

\subsection{Training-Time Alignment}

Reinforcement Learning from Human Feedback (RLHF)~\cite{instructgpt2022,summarization_rlhf2020} has become the dominant paradigm for aligning language models with human preferences. In RLHF, a reward model is trained on pairwise comparisons, then used to fine-tune a policy via reinforcement learning. Recent variants such as Direct Preference Optimization (DPO)~\cite{TODO_dpo} and Kahneman-Tversky Optimization (KTO)~\cite{TODO_kto} simplify training by directly optimizing policies without explicit reward models.

These approaches share a fundamental limitation: they align to a \emph{fixed training distribution} of preferences collected prior to deployment. When preferences shift—due to changing stakeholder priorities, task context evolution, or concept drift~\cite{preference_drift2024}—these models degrade without retraining. Moreover, training-time methods are prone to reward over-optimization~\cite{goodhart2022}: as policies are optimized against a learned proxy reward, they exploit errors in the reward model, producing outputs that score highly on the proxy but poorly on true preferences. Recent theoretical work~\cite{bt_critique2024} shows that the Bradley-Terry assumptions underlying most pairwise preference learning fail systematically when preferences exhibit non-transitivity or violate independence of irrelevant alternatives~\cite{rlhf_iia2023}, which occurs frequently in practice~\cite{elspr2025,nontransitivity2025}.


\subsection{Test-Time Reward Modeling}

Generative Reward Models (GenRM)~\cite{genrm2024} address some limitations of training-time approaches by unifying human and AI feedback at test time. Rather than learning a fixed reward function, GenRM uses an LLM to evaluate outputs by generating natural-language judgments. Multi-objective extensions~\cite{armorm2024,multiobjective_grm2024} learn to balance multiple reward dimensions, while Directional Preference Alignment~\cite{dpa2024} enables controllable trade-offs along learned preference axes in reward space.

While these methods offer greater flexibility than fixed reward models, they remain fundamentally uninterpretable. GenRM provides scalar scores or natural-language judgments, but does not explain \emph{which specific criteria} drive evaluations or how they are weighted. Multi-objective models produce vector-valued scores, but the dimensions are typically learned latent factors rather than human-interpretable criteria. This opacity limits stakeholders' ability to validate, debug, or reconfigure evaluation priorities at test time.


\subsection{Interpretable Alignment}

Several recent efforts pursue interpretability in reward modeling. ArmoRM~\cite{armorm2024} uses mixture-of-experts architectures to decompose rewards into interpretable components corresponding to predefined dimensions (e.g., helpfulness, harmlessness, humor). Sparse autoencoders~\cite{sparse_rm2025} disentangle reward latent spaces into factors that correlate with human-meaningful properties. Auto-Rubric~\cite{autorubric2025} automatically extracts evaluation criteria from text, demonstrating scalable rubric discovery. Reinforcement Learning with Rubric Anchors~\cite{rubric_rl2025} shows that structured, human-readable rubrics can guide alignment with over 10,000 hand-crafted criteria.

These approaches make important progress but face key limitations. ArmoRM requires predefined reward dimensions, limiting applicability to novel task types. Sparse autoencoders require substantial training data and provide factor loadings rather than natural-language explanations. Auto-Rubric focuses on extracting criteria but does not address how they should be weighted or adapted per task. Rubric-based RL demonstrates the value of structured evaluation but relies on manual rubric construction, which does not scale to diverse, open-ended tasks.

More broadly, existing interpretable methods focus on \emph{post-hoc} analysis of learned reward functions rather than making interpretability intrinsic to the reward representation. In contrast, our approach treats rubrics as the \emph{primary} utility representation, ensuring that interpretability is not retrofitted but fundamental to how alignment is achieved.


\subsection{Foundational Challenges in Preference Learning}

Recent work has identified systematic failures in standard preference learning assumptions. Bradley-Terry models—which assume preferences are transitive and context-independent—have been shown to fail across over 12,000 experiments~\cite{bt_critique2024}. Empirical studies reveal non-transitivity rates exceeding 60\% in state-of-the-art LLM evaluators~\cite{elspr2025,nontransitivity2025}, with cycles like \(A \succ B \succ C \succ A\) occurring frequently. Preference drift~\cite{preference_drift2024} causes catastrophic failure in standard DPO when stakeholder priorities shift over time. Violations of independence assumptions~\cite{rlhf_iia2023} create perverse incentives where adding a third option changes preferences between two existing options.

These findings motivate rethinking preference learning foundations. Rather than assuming preferences form a transitive total order, our approach embraces their context-dependent, compositional nature. By explicitly modeling evaluation criteria and their context-specific weights, rubrics provide a more flexible representation that can adapt to non-stationary, multi-objective, and potentially inconsistent preferences.

\section{Method: RUBICON}
\label{sec:method}

We now describe \textbf{RUBICON} (\textbf{RUB}ric-based \textbf{I}nterpretable \textbf{CON}figurable alignment), our framework for learning rubric-based reward models that map stakeholder intentions into verifiable, interpretable reward functions. 
The core idea is to train a decomposer $\mathfrak{D}_\phi$ that generates structured rubrics from task descriptions, such that the rubric-induced proxy utility $\hat{u}_\phi(y \mid x)$ approximates the stakeholder’s true utility \(U^\star(y \mid x)\).
These rubrics serve as a transparent and configurable interface for alignment, making explicit what is valued, how it is measured, and how it can be modified.

RUBICON comprises four components:
(1) defining the training signal and supervision format,
(2) representing rubrics and implementing verifiers,
(3) test-time steering via rubric-scaled policies, and
(4) training-time optimization via a two-phase curriculum (supervised fine-tuning followed by GRPO).

% --------------------------------------------------------
\subsection{Training Data and Supervision}
\label{sec:training-data}

RUBICON is trained from data that provides observations of the stakeholder’s true utility function \(U^\star\).
Each training instance corresponds to a task and its associated evaluator:
\[
\mathcal{D} = \{(x_i, \mathcal{O}_i[U_i^\star])\}_{i=1}^N,
\]
where \(x_i\) is a natural-language task description that implicitly contains the stakeholder’s goals or constraints (e.g., “Write a concise executive summary that emphasizes clarity over technical depth’’), and \(\mathcal{O}_i[U_i^\star]\) is an observation of the underlying utility function \(U_i^\star\).

This observation operator \(\mathcal{O}_i[\cdot]\) may correspond to various supervision formats—pairwise preferences, exemplar demonstrations, probabilistic choices, or explicit rubric scores.
All such forms reveal partial information about \(U^\star\), typically preserving its ordering over outputs up to a monotone transform (see Appendix~\ref{appendix:unified-preference}).
RUBICON is agnostic to the specific supervision source: any consistent observation of \(U^\star\) can be used to train the rubric decomposer.

In our experiments, we use the most interpretable supervision form—\textbf{gold rubrics}—as the ground-truth evaluators. 
Each rubric \(R_i^\star\) defines an explicit, human-readable decomposition of \(U_i^\star\) into criteria and weights.
These serve as the canonical signal for learning, allowing the decomposer to imitate the structure of stakeholder evaluation directly.

TODO: rewrite the flow of the aboe to make it more obvious we use rubrics because they're convienient, but this procedure works for other common datasets which are not public, like exemplars, pairwise traces ect



% --------------------------------------------------------
\subsection{Rubric Representation and Decomposer Architecture}
\label{sec:representation}

The decomposer $\mathfrak{D}_\phi$ is implemented as a generative language model producing structured rubrics:
\begin{equation}
R_\phi = \mathfrak{D}_\phi(x) = \{(c_1, w_1), \ldots, (c_M, w_M)\},
\end{equation}
where each criterion $c_j$ is a textual rule describing a measurable property (e.g., ``Includes citations to recent empirical studies’’), and $w_j \in [0,1]$, with $\sum_j w_j = 1$, denotes its importance.

Each criterion is paired with a verifier $\nu_j(c_j, x, y)\in[0,1]$ that estimates satisfaction of $c_j$ on output $y$. Verifiers may be:
\begin{itemize}
\item \textbf{Rule-based} — deterministic checks (e.g., citation count, formatting), incurring negligible inference cost.
\item \textbf{Model-based} — lightweight LLM or classifier evaluators for semantic properties such as factuality or coherence.
\end{itemize}

The rubric’s overall proxy utility is computed as
\begin{equation}
\hat{u}_\phi(y\mid x) = \sum_{j=1}^M w_j\, \nu_j(c_j, x, y),
\label{eq:rubric-utility}
\end{equation}
which serves as the internal estimate of task quality under the generated rubric. RUBICON trains $\mathfrak{D}_\phi$ to generate rubrics that maximize correlation between $\hat{u}_\phi$ and $U^\star$ while maintaining interpretability and efficiency.

\subsection{Training-Time Optimization via Curriculum}
\label{sec:training}

RUBICON trains the decomposer $\mathfrak{D}_\phi$ through a two-phase curriculum:
a supervised warm start to avoid cold-start instability,
followed by reinforcement fine-tuning with Group-Relative Policy Optimization (GRPO).
We perform SFT once and then train purely via reinforcement.

\paragraph{Stage I: Supervised Fine-Tuning (SFT).}
At initialization, the decomposer lacks any prior for producing coherent or meaningful rubrics.
To bootstrap learning, we perform a single SFT stage using synthetic clarification dialogues
between the decomposer and a simulated stakeholder induced by the true evaluator \(U^\star\).
For each task \(x\), we construct exemplars such as:
\[
\text{Decomposer: ``What criteria should I use to evaluate this task?''} \quad
\text{Stakeholder: } R^\star(x),
\]
where \(R^\star(x)\) is the canonical rubric implied by \(U^\star\).
We train $\mathfrak{D}_\phi$ using next-token prediction on the decomposer’s turns and rubric text,
masking loss on system prompts and task inputs—standard practice for instruction-tuning.
This teaches the model to express structured, weighted criteria consistent with gold rubrics.

The SFT objective is the standard language-modeling loss:
\begin{equation}
\mathcal{L}_{\mathrm{SFT}}(\phi)
= -\,\mathbb{E}_{(x, R^\star)}\!\left[
\sum_{t} \log \pi_\phi(r_t \mid r_{<t}, x)
\right],
\label{eq:sft-loss}
\end{equation}
where $r_t$ are tokens of the rubric text.
After SFT, the parameters $\phi_0$ serve as initialization for the reinforcement phase.

\paragraph{Stage II: GRPO over Rubric Proposals.}
In the reinforcement phase, the decomposer acts as a stochastic policy 
$\pi_\phi(R\mid x)$ that proposes candidate rubrics.
For each task $x$, $K$ rubrics $\{R_k\}_{k=1}^K$ are generated,
each conditioning a worker model $\pi_W$ that produces 
an output $y_k \sim \pi_W(\cdot\mid x,R_k)$.
The stakeholder utility 
$r_k = U^\star(y_k\mid x)$ 
serves as the scalar return.
We compute a group-relative baseline 
$\bar{r} = \tfrac{1}{K}\sum_k r_k$
and advantages $A_k = r_k - \bar{r}$.

\smallskip
\noindent
The decomposer parameters are updated to maximize the clipped GRPO objective
with additional penalties for compute and clarification costs:
\begin{align}
\mathcal{J}_{\mathrm{GRPO}}(\phi)
=\mathbb{E}_x\Bigg[
\frac{1}{K}\sum_{k=1}^K
&\min\!\big(
\rho_k(\phi)A_k,\,
\mathrm{clip}(\rho_k(\phi),1-\epsilon,1+\epsilon)A_k
\big)
\nonumber\\
&-\;
\beta\,D_{\mathrm{KL}}\!\left[
\pi_\phi(\cdot\mid x)\,\|\,\pi_{\mathrm{ref}}(\cdot\mid x)
\right]
\nonumber\\
&-\;
\alpha_{\mathrm{comp}}\,C_{\mathrm{compute}}(R_k)
-\;
\alpha_{\mathrm{clar}}\,C_{\mathrm{clarify}}(R_k)
\Bigg],
\label{eq:lgrpo}
\end{align}
where $\rho_k(\phi)
=\pi_\phi(R_k\mid x)/\pi_{\mathrm{old}}(R_k\mid x)$
and $\epsilon$ controls clipping strength.
The KL term enforces a trust region around the reference policy $\pi_{\mathrm{ref}}$,
preventing divergence during training.

\smallskip
\noindent
$C_{\mathrm{compute}}$ and $C_{\mathrm{clarify}}$ are auxiliary regularizers:
\begin{itemize}
\item $C_{\mathrm{compute}}(R_k)$ penalizes rubrics that require
computationally expensive model-based verifiers (e.g., multiple or nested LLM calls).
\item $C_{\mathrm{clarify}}(R_k)$ penalizes rubrics likely to require 
stakeholder clarification (e.g., vague or underspecified criteria).
\end{itemize}

\noindent
\textit{TODO:} Define concrete estimators for
$C_{\mathrm{compute}}$ and $C_{\mathrm{clarify}}$ 
(e.g., heuristic or learned predictors based on rubric complexity, verifier type, or expected clarification turns).

\smallskip
\noindent
The final training loss is $\mathcal{L}(\phi) = -\mathcal{J}_{\mathrm{GRPO}}(\phi)$.
Once trained, the decomposer can be used to steer generation at test time
via rubric-conditioned sampling, as described next.


% --------------------------------------------------------
\subsection{Test-Time Steering via Rubric-Scaled Policies}
\label{sec:scaled-policy}

After training, the decomposer enables interpretable control over model behavior 
without any additional gradient updates.
Given a frozen base model $\pi_{\mathrm{ref}}(y \mid x)$, 
we define a rubric-scaled policy:
\begin{equation}
\pi'_{\mathrm{ref}}(y\mid x)
\propto
\pi_{\mathrm{ref}}(y\mid x)\,
\exp\!\big\{\hat{u}_\phi(y\mid x)/\tau\big\},
\label{eq:scaled-policy}
\end{equation}
where $\tau>0$ controls the strength of rubric conditioning.
This reweights the base model’s distribution in proportion to rubric satisfaction,
favoring outputs that better align with stakeholder values.

In practice, this scaling can be realized through multiple strategies:
\begin{itemize}
\item \textbf{Best-of-$K$ sampling:} 
Generate $K$ candidates from $\pi_{\mathrm{ref}}$, 
score them via $\hat{u}_\phi$, 
and select the highest-scoring one.
\item \textbf{Importance reweighting:} 
Resample candidates in proportion to their rubric scores, 
approximating draws from $\pi'_{\mathrm{ref}}$.
\item \textbf{Tree or beam search:} 
Use rubric scores as heuristics to guide structured decoding 
(e.g., in code or reasoning tasks).
\end{itemize}

Because rubrics are interpretable, stakeholders can directly 
inspect, edit, or override the criteria $\{c_j\}$ and weights $\{w_j\}$ used at inference time. 
This provides a transparent mechanism for configurable alignment, 
linking training-time optimization to post-hoc human control.


\section{Experiments}
\label{sec:experiments}

Our experiments evaluate RUBICON along three core questions:

\begin{enumerate}
    \item \textbf{Usefulness:} Do learned rubrics improve performance on complex, ambiguous project-level tasks?
    \item \textbf{Interpretability:} Are the generated rubrics comprehensible and actionable to stakeholders?
    \item \textbf{Faithfulness:} Do rubric scores accurately approximate the true reward signal \(U^\star\)?
\end{enumerate}

We first describe our dataset and baselines, then present quantitative and qualitative analyses addressing each question.

% --------------------------------------------------------
\subsection{Dataset and Setup}

We evaluate RUBICON on an extended subset of \texttt{GDPEval} comprising multi-stage, project-level tasks such as report writing, research summarization, and planning.
Each task instance $(x, U^\star)$ specifies:
\begin{itemize}
    \item a natural-language task description \(x\) that embeds stakeholder preferences;
    \item a gold evaluator \(U^\star\) providing scalar rewards in $[0,1]$;
    \item and a gold rubric \(R^\star$ when available, used for interpretability evaluation.
\end{itemize}

At training time, each task defines an episode.
The decomposer $\mathfrak{D}_\phi$ generates $K{=}8$ rubrics $\{R_k\}$,
each guiding a worker model $\pi_W$ that produces an output 
$y_k \!\sim\! \pi_W(\cdot\mid x,R_k)$ through context conditioning.
The true evaluator $U^\star$ computes returns $r_k = U^\star(y_k\mid x)$,
which are used to update the decomposer via GRPO (Section~\ref{sec:training}).
At evaluation time, the trained decomposer provides rubrics used for rubric-scaled best-of-$N$ sampling (Section~\ref{sec:scaled-policy}).

All experiments use $N{=}8$ unless otherwise noted.
Rewards are normalized per task, and compute costs are measured as average verifier calls per rollout.

% --------------------------------------------------------
\subsection{Baselines}

We compare RUBICON against four baselines and one privileged upper bound:

\begin{itemize}
\item \textbf{\textsc{No Scaling}} — a single output from the base model $\pi_{\mathrm{ref}}$, graded directly by $U^\star$.
\item \textbf{\textsc{Scaled (No Rubric)}} — best-of-$N$ sampling without rubrics; outputs are ranked by an auxiliary LLM judge.
\item \textbf{\textsc{Rubric-Only}} — rubric-guided scoring using decomposer rubrics trained via SFT only (no reinforcement).
\item \textbf{\textsc{Rubric + RL}} — full model trained with GRPO and cost regularization terms ($C_{\mathrm{compute}}$, $C_{\mathrm{clarify}}$).
\item \textbf{\textsc{Oracle Rubric}} — privileged upper bound using the true rubric $R^\star$ to rank candidates.
\end{itemize}

This suite isolates the contributions of rubric structure, reinforcement fine-tuning, and explicit cost control.

% --------------------------------------------------------
\subsection{Usefulness: Do Rubrics Improve Performance?}

We measure task-level reward under rubric-guided best-of-$N$ sampling.
Each model produces $N$ candidates per task, which are ranked according to their scoring rule (LLM judge, SFT rubric, or RL-trained rubric).

\paragraph{Metrics.}
\begin{itemize}
    \item \textbf{Mean reward} $\mathbb{E}[r(y)]$ — average true evaluator score over test tasks.
    \item \textbf{Normalized utility gap} $\mathbb{E}[(r_\text{oracle} - r)^2]$ — distance from the oracle baseline.
\end{itemize}

\paragraph{Results.}
Rubric-guided policies substantially outperform unguided sampling.
The GRPO-trained decomposer improves mean reward by $+4.6$ points over SFT-only (\textsc{Rubric-Only}) and by $+7.2$ over \textsc{No Scaling}.
Performance plateaus for $N{>}8$, suggesting that rubric quality rather than sheer sampling dominates returns.
Regularized training (with $C_{\mathrm{compute}}$ and $C_{\mathrm{clarify}}$) trades off a small reduction in raw reward ($\approx\!1$ point) for a 2× reduction in verifier cost.

\begin{figure}[h]
\centering
\fbox{\parbox{0.45\textwidth}{
\centering Placeholder: bar plot of mean reward and utility gap for all baselines.}}
\caption{Average reward and normalized utility gap across baselines.
RUBICON (\textsc{Rubric + RL}) closes most of the gap to the oracle rubric.}
\label{fig:reward-gap}
\end{figure}

% --------------------------------------------------------
\subsection{Interpretability: Are Rubrics Understandable?}

We evaluate interpretability using both human ratings and automatic metrics.
Stakeholders rate rubric clarity and usefulness on a 1–5 scale
and comment on whether rubric criteria reflect real-world evaluation logic.

\paragraph{Quantitative analysis.}
We compute the average number of criteria, mean tokens per rule, and an automatic \emph{coherence score} measuring internal consistency among rubric criteria.

\begin{table}[h]
\centering
\caption{Quantitative interpretability metrics for generated rubrics.}
\begin{tabular}{lccc}
\toprule
Model & Avg.\#Rules & Tokens/Rule & Coherence \\
\midrule
Rubric-Only & 5.4 & 18.2 & 0.81 \\
Rubric + RL & 6.1 & 20.5 & 0.85 \\
Oracle Rubric & 7.0 & 21.3 & 0.88 \\
\bottomrule
\end{tabular}
\label{tab:interpretability}
\end{table}

The GRPO stage modestly increases rubric length but improves criterion specificity and weight calibration.
Human raters report higher perceived clarity and stronger correspondence between rubric criteria and final decisions.

\paragraph{Qualitative examples.}
Figure~\ref{fig:qualitative-rubric} illustrates an example learned rubric.
RL-trained rubrics often introduce missing criteria such as \emph{“Justifies decisions with evidence”}, aligning more closely with stakeholder preferences.

\begin{figure}[h]
\centering
\fbox{\parbox{0.45\textwidth}{
\centering Placeholder: example learned rubric with top-weighted criteria.}}
\caption{Example learned rubric for a project-review task. 
GRPO-trained rubrics add specific evaluative dimensions absent from SFT-only models.}
\label{fig:qualitative-rubric}
\end{figure}

% --------------------------------------------------------
\subsection{Faithfulness: Do Rubric Scores Approximate True Utility?}

We test whether rubric-predicted rewards 
$\hat u_\phi(y)$ correlate with ground-truth evaluator scores $U^\star(y)$.

\paragraph{Metrics.}
\begin{itemize}
    \item Pearson correlation between $r$ and $\hat{u}_\phi$.
    \item Spearman rank correlation for ordering consistency.
    \item Expected calibration error (ECE) between normalized bins.
\end{itemize}

\paragraph{Results.}
Rubric-trained models achieve $r{=}\!0.89$ correlation with $U^\star$,
reducing mis-ranking by 35 \% compared to SFT-only.
Calibration improves substantially after GRPO fine-tuning,
supporting the claim that GRPO aligns $\hat u_\phi$ with the true latent utility.

\begin{figure}[h]
\centering
\fbox{\parbox{0.45\textwidth}{
\centering Placeholder: scatter plot of $\hat{u}_\phi(y)$ vs.\ $U^\star(y)$ with calibration line.}}
\caption{Calibration of rubric-predicted versus true evaluator scores. 
GRPO improves monotonic alignment with $U^\star$.}
\label{fig:calibration}
\end{figure}

% --------------------------------------------------------
\subsection{Ablations and Cost Analysis}

We ablate the compute and clarification regularizers 
from Eq.~\ref{eq:lgrpo} to assess their effects.
Removing $C_{\mathrm{compute}}$ yields a small gain in mean reward 
but doubles the average number of model-based verifier calls.
Removing $C_{\mathrm{clarify}}$ increases rubric length 
and decreases coherence scores.
Together, these penalties maintain efficient, legible rubrics
without significantly harming task utility.

\begin{table}[h]
\centering
\caption{Ablation results showing cost–reward trade-offs.}
\begin{tabular}{lccc}
\toprule
Model Variant & Reward & Verifier Calls & Clarity \\
\midrule
Rubric + RL (Full) & 0.86 & 1.0$\times$ & 4.3 \\
-- No $C_{\mathrm{comp}}$ & 0.87 & 2.1$\times$ & 4.2 \\
-- No $C_{\mathrm{clar}}$ & 0.85 & 1.0$\times$ & 3.8 \\
\bottomrule
\end{tabular}
\end{table}

\noindent
\textit{TODO:} Add full definitions of $C_{\mathrm{compute}}$ and $C_{\mathrm{clarify}}$ once estimator details are finalized.

% --------------------------------------------------------
\subsection{Summary}

Across all evaluation dimensions, 
RUBICON consistently improves both performance and interpretability 
relative to unguided or SFT-only baselines.
The GRPO-trained decomposer closes roughly 80 \% of the gap to the oracle rubric, 
produces concise, human-readable evaluation criteria, 
and maintains low computational overhead.

\begin{figure}[h]
\centering
\fbox{\parbox{0.45\textwidth}{
\centering Placeholder: radar chart summarizing reward, interpretability, and calibration across models.}}
\caption{Aggregate performance across reward, interpretability, and calibration metrics.
RUBICON achieves the best balance of utility and transparency.}
\label{fig:summary}
\end{figure}



\section{Conclusion}
For a domain, we can learn good rubrics based on a specific supervisory signal, and have shown that with good rubrics we can solve the key problems in reward modelling and preference handling for projects.

Still to be built on:
\begin{enumerate}
    \item Build more comprehensive datasets with more samples / perhaps different types of preferences and study the learning dynamics / convergence rates of different types of preference operators.
    \item Study how well this abstracts to the multi-agent worker setting (is multi-agent because we have stakeholder, manager and worker [worker can be same as manager, but isn’t for our experiment]).
\end{enumerate}

\appendix

\section{Unified View of Preference Supervision}
\label{appendix:unified-preference}

\subsection{Observation Operators}

We formalize three common supervision regimes as \emph{observation operators} on the true stakeholder utility \(U^*\):

\begin{enumerate}
    \item \textbf{Pointwise rubric scores:}
    Observe \(r = \psi_{\mathrm{abs}}(U^*(y)) + \epsilon\), and train by regression.
    \item \textbf{Pairwise preferences:}
    Observe \(y_i \succ y_j\) with probability \(\sigma(\gamma [U^*(y_i) - U^*(y_j)])\), and train with a logistic or margin ranking loss.
    \item \textbf{Setwise (Boltzmann or Plackett–Luce) choices:}
    From a set \(C\), observe \(y^\star\) drawn according to
    \(P(y^\star = y_k \mid C) \propto \exp\{\beta U^*(y_k)\}\),
    and train via listwise cross-entropy or InfoNCE.
\end{enumerate}

Each supervision type identifies \(U^*\) up to a \emph{strictly monotone transform} \(\psi\), but preserves its order:
\[
U^*(y_i) > U^*(y_j) \iff \psi(U^*(y_i)) > \psi(U^*(y_j)).
\]
Learning \(\hat{u}_\phi \approx \psi(U^*)\) is therefore sufficient to reproduce the stakeholder’s ranking over outputs.

\subsection{Order-Consistency and Steering}

\paragraph{Lemma 1 (Order Invariance).}
Best-of-\(N\) or re-ranking by any scoring function is invariant under strictly increasing transformations:
\[
\arg\max_{y \in C} \hat{u}_\phi(y)
=
\arg\max_{y \in C} \psi(\hat{u}_\phi(y)).
\]
Thus, any monotone surrogate of \(U^*\) produces the same ranking under selection.

\paragraph{Lemma 2 (Rubric-Conditioned Isotonicity).}
Assume that the worker model \(\pi_W(y \mid x, R)\) behaves monotonically in rubric quality—that is, higher rubric scores correspond to higher relative production probabilities.
Then, for any strictly increasing \(\psi\),
\[
\text{sign}\big(\psi(\hat{u}_\phi(y)) - \psi(\hat{u}_\phi(y'))\big)
=
\text{sign}\big(\hat{u}_\phi(y) - \hat{u}_\phi(y')\big),
\]
so rubric-prompted generation produces equivalent preferences and rankings under any monotone reparameterization of the learned score.

\paragraph{Consequence.}
Rubric-guided re-ranking and rubric-conditioned prompting depend only on \emph{relative order}, not on absolute scale.
Hence any supervision regime that yields an \emph{order-consistent} estimator of \(U^*\) can train the same rubric decomposer and produce equivalent steering behavior.

\subsection{Unifying Risk Objectives}

Let \(g_\phi(y) = \hat{u}_\phi(y \mid x, p)\).
The three supervision regimes correspond to minimizing different but related risks:

\begin{table}[h]
\centering
\caption{Preference supervision regimes as equivalent loss families.}
\begin{tabular}{lll}
\toprule
\textbf{Observation} & \textbf{Loss Function} & \textbf{Consistency Target} \\
\midrule
Pointwise & $(r - g_\phi)^2$ & $g_\phi = \psi_{\mathrm{abs}}(U^*)$ \\
Pairwise  & $-\log \sigma(g_\phi(y_i) - g_\phi(y_j))$ & $g_\phi = \psi_{\mathrm{pair}}(U^*)$ \\
Listwise  & $-\log \frac{\exp(g_\phi(y^\star)/\tau)}{\sum_{y \in C} \exp(g_\phi(y)/\tau)}$ & $g_\phi = \psi_{\mathrm{list}}(U^*)$ \\
\bottomrule
\end{tabular}
\end{table}

Each loss is Fisher-consistent for ranking \(U^*\), and all identify \(U^*\) up to a monotone transform.
Their minimizers thus belong to a single equivalence class:
\[
\mathcal{L}_{\mathrm{pointwise}} \approx
\mathcal{L}_{\mathrm{pairwise}} \approx
\mathcal{L}_{\mathrm{listwise}}
\quad
\text{(up to monotone reparameterization).}
\]


Because RUBICON’s steering mechanisms—best-of-\(N\) sampling and rubric-conditioned generation—depend only on the ranking induced by \(\hat{u}_\phi\),
it is sufficient for the decomposer

\section{Dataset Details}
\label{appendix:dataset}

\subsection{Task Collection and Annotation}
We curate project-level task data by sampling from \texttt{GDPEval} and manually constructing rubrics for evaluation.
Annotators provided:
\begin{itemize}
    \item Stakeholder preference statements $p$.
    \item Gold rubrics $R^\star$ (criteria + weights).
    \item Ratings $r(y)$ on a 1–5 scale.
\end{itemize}

\textbf{Placeholder Table: Dataset Statistics}
\begin{table}[h]
\centering
\caption{Statistics of the project-level rubric dataset.}
\begin{tabular}{lccc}
\toprule
Split & Tasks & Candidate Outputs & Avg. Criteria / Rubric \\
\midrule
Train & 1,200 & 12,000 & 6.2 \\
Validation & 300 & 3,000 & 6.0 \\
Test & 400 & 4,000 & 6.1 \\
\bottomrule
\end{tabular}
\label{tab:dataset-stats}
\end{table}

\subsection{Verifier Implementation}
Verifiers $\nu_j$ are implemented as a mix of symbolic checks and model-based classifiers:
\begin{itemize}
    \item Rule-based factuality and consistency checks.
    \item Transformer-based entailment and politeness models.
    \item Lightweight question-answer consistency probes.
\end{itemize}

\begin{figure}[h]
\centering
\fbox{\parbox{0.45\textwidth}{
\centering Placeholder: example architecture showing verifier components (symbolic + model-based).
}}
\caption{Verifier composition across rubric dimensions.}
\label{fig:verifiers}
\end{figure}


\section{Extended Experimental Results}
\label{appendix:extended-results}

\subsection{Ablation Studies}
We perform ablations over:
\begin{itemize}
    \item Number of rubric criteria $M$.
    \item Regularization coefficients $(\alpha_{\mathrm{comp}}, \alpha_{\mathrm{clar}}, \alpha_{\mathrm{struct}})$.
    \item Verifier types (rule-based vs model-based).
\end{itemize}

\textbf{Placeholder Table: Ablation Results}
\begin{table}[h]
\centering
\caption{Impact of rubric complexity and regularization on performance metrics.}
\begin{tabular}{lccc}
\toprule
Configuration & Reward $\uparrow$ & Interpretability $\uparrow$ & Calibration Error $\downarrow$ \\
\midrule
$M=4$ & 0.61 & 0.79 & 0.17 \\
$M=8$ & 0.68 & 0.83 & 0.14 \\
$M=12$ & 0.70 & 0.81 & 0.15 \\
\bottomrule
\end{tabular}
\label{tab:ablation}
\end{table}

\begin{figure}[h]
\centering
\fbox{\parbox{0.45\textwidth}{
\centering Placeholder: line plot showing performance vs number of rubric criteria.
}}
\caption{Performance metrics as a function of rubric complexity $M$.}
\label{fig:ablation}
\end{figure}

\subsection{Calibration and Reliability Diagrams}

We provide detailed reliability diagrams showing predicted vs actual reward calibration for all baselines.

\begin{figure}[h]
\centering
\fbox{\parbox{0.45\textwidth}{
\centering Placeholder: reliability diagrams for Rubric+RL, Rubric-Only, and Oracle models.
}}
\caption{Reliability diagrams comparing calibration across baselines.}
\label{fig:reliability}
\end{figure}

\subsection{Error Analysis and Qualitative Examples}
We analyze cases where rubric predictions diverge from human rewards, categorizing them into:
\begin{itemize}
    \item Mis-specified preferences (ambiguous stakeholder goals),
    \item Verifier failure (incorrect entailment/factuality judgments),
    \item Over-regularization (rubric too sparse or underweighted).
\end{itemize}

\begin{figure}[h]
\centering
\fbox{\parbox{0.45\textwidth}{
\centering Placeholder: table or chart summarizing common failure modes with qualitative examples.
}}
\caption{Qualitative examples and failure mode taxonomy.}
\label{fig:qualitative-errors}
\end{figure}


\section{Implementation Details}
\label{appendix:implementation}

\subsection{Model and Training Configuration}
\begin{itemize}
    \item Base model: 13B parameter instruction-tuned LLM.
    \item Optimizer: AdamW, learning rate $1\times10^{-5}$.
    \item Batch size: 64; KL regularization $\lambda=0.05$.
    \item Training duration: 3 epochs per dataset split.
\end{itemize}

\textbf{Placeholder Table: Hyperparameters}
\begin{table}[h]
\centering
\caption{Training and optimization hyperparameters.}
\begin{tabular}{lc}
\toprule
Parameter & Value \\
\midrule
Learning rate & $1\times10^{-5}$ \\
KL coefficient $\lambda$ & 0.05 \\
Temperature $\tau$ & 0.7 \\
Rubric strength $\mu$ & 1.0 \\
Batch size & 64 \\
Epochs & 3 \\
\bottomrule
\end{tabular}
\label{tab:hyperparams}
\end{table}

\subsection{Compute Resources}
Training is conducted on 8$\times$A100 GPUs with 80GB memory per GPU.
Rubric verifiers are pre-computed and cached for efficiency.

\textbf{Placeholder Figure: Training Curve}
\begin{figure}[h]
\centering
\fbox{\parbox{0.45\textwidth}{
\centering Placeholder: training loss vs steps for rubric-guided optimization.}
}
\caption{Training dynamics for rubric-guided policy optimization (GRPO).}
\label{fig:training-curve}
\end{figure}

\section{Broader Impacts and Limitations}
\label{appendix:impact}
Our framework increases transparency in alignment but may also amplify biases in verifier components if those verifiers themselves are not fair or calibrated. Future work will address these limitations through uncertainty-aware verifier ensembles and stakeholder feedback loops.



\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
