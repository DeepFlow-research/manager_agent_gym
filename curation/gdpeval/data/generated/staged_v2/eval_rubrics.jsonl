{"task_id": "efca245f-c24f-4f75-a9d5-59201330ab7a", "rubric": {"category_name": "Manufacturing | First-Line Supervisors | Running Board Recovery Plan (Manitoba, 2018)", "rationale": "Mixed deliverable: a structured Excel planning model plus a written scenario summary. Stage 1 uses LLM-only gating to enforce an exact, verifiable file structure. Stage 2 combines light, resilient code checks (bounds, schedule rules, single-product constraint) with heavier LLM verification for sequencing priorities and deadline coverage, reflecting that nuanced reasoning requires LLM evaluation. Stage 3 assesses presentation quality, managerial usefulness, and clarity for an operations managers meeting.", "max_total_score": 28.0, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate", "description": "MANDATORY gate. Verify the candidate produced: (a) a well-structured Excel with three daily production scenarios in a consistent format and an assumptions sheet; and (b) a written scenario summary document with required sections.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Excel Plan Structure Present (3 Scenarios + Assumptions)", "description": "Check the spreadsheet exists and follows the required, verifiable structure enabling downstream checks.", "weight": 5.0, "judge_prompt": "You are reviewing the candidate's outputs. Focus ONLY on structure (not correctness). Determine if there is an Excel spreadsheet with the following structure. Be flexible with similar sheet/section names but strict about presence of all required elements.\n\nRequired Excel file (XLSX):\n- Must contain FOUR sheets total (or more, but at least these four):\n  1) Scenario 1 \u2014 Current Capacity and Cells (name can be similar, e.g., \"Scenario 1: Current Capacity\")\n  2) Scenario 2 \u2014 Current Capacity without Truck Grill Guard (name can be similar, must clearly mean no Grill Guard in this cell)\n  3) Scenario 3 \u2014 Expanded Capacity with 10-Hour Shift (no Grill Guard) (name can be similar, must clearly indicate 10-hour shift and no Grill Guard)\n  4) Assumptions & Rules (or similarly named sheet containing assumptions)\n\nEach Scenario sheet must have a clearly labeled DAILY plan table with a consistent column structure across all three scenarios. The table MUST include at least the following columns (exact names may vary, but intent must be clear):\n- Date (daily granularity)\n- Product (values: Crew Cab Running Boards, Extended Cab Running Boards, or Truck Grill Guard)\n- PO Month (e.g., Nov, Dec, Jan, Feb, Mar, Apr, May 2018)\n- Planned Qty (units planned for that date)\n- Cumulative Planned Crew (cumulative crew-cab running boards produced to date)\n- Cumulative Planned Ext (cumulative extended-cab running boards produced to date)\n- Open PO Remaining \u2014 Crew (running total remaining vs open POs)\n- Open PO Remaining \u2014 Ext (running total remaining vs open POs)\n- Optional but helpful: a Deadline/Checkpoint column (e.g., flags for Apr 13 in-transit, May 1 ship)\n\nCoverage/scope requirements for each Scenario sheet:\n- The daily plan must cover working days from at least Jan 22, 2018 through at least May 1, 2018 (weekends/holidays may be omitted or listed with zero production). \n- The layout should make it possible to verify that only ONE product is run per day (e.g., one row per date or a clear indicator that only one product has a nonzero plan per day).\n\nAssumptions & Rules sheet must explicitly list (as text):\n- Start/restart and time window references (restart Jan 22, 2018)\n- Capacity assumptions: 120 sets/day baseline; increase to 135 sets/day from Feb 5, 2018; 10-hour shift scenario allows up to 170 sets/day for a 4-week period after 30-day notice\n- Grill Guard: if produced in this cell, 100 units/week target; if moved, specify by Feb 1\n- One-product-at-a-time constraint; no overtime; weekdays only; observes provincial/federal stat holidays\n- Customer priorities: Crew Cab Dec\u2013Feb before Extended Cab Nov\u2013Feb; then Crew Cab Mar/Apr before Extended Cab Mar/Apr; plus due dates (April PO in transit by Apr 13; May PO ships by May 1)\n\nScoring (structure only):\n- 5.0: Excel present, all three scenario sheets present with required daily-plan columns, coverage Jan 22\u2013May 1, and an Assumptions & Rules sheet with all listed assumption bullets.\n- 4.0: Minor omissions (e.g., one of the cumulative/remaining columns missing) but three scenario sheets and assumptions are clearly present.\n- 2.5: Two scenario sheets present or major column/coverage gaps, but intent is clear and layouts largely enable verification.\n- 0.0: Not an Excel spreadsheet OR missing multiple scenario sheets OR no daily plan structure OR no assumptions/rules sheet.\n\nDo not assess accuracy of calculations. Only verify presence, format, and structural completeness.", "expectation": "A four-sheet Excel with three consistently structured scenario tables and an assumptions sheet, daily planning from Jan 22 to May 1, and required columns enabling verification."}, {"type": "llm_judge", "name": "Written Scenario Summaries Present (3 Scenarios)", "description": "Check a separate written document exists (PDF/DOCX/MD) with required scenario write-ups and explicit yes/no outcomes.", "weight": 3.0, "judge_prompt": "Check for a separate document (PDF/DOCX/Markdown) containing concise scenario summaries. Be flexible with section titles but strict about the presence of all required elements.\n\nRequired structure:\n- The document is PDF, DOCX, or Markdown (not Excel-only).\n- Contains three clearly delineated sections for Scenario 1, Scenario 2, and Scenario 3 (names can vary but must clearly map to the three spreadsheet scenarios).\n- For EACH scenario, include:\n  - Actions (what changes are made: e.g., grill guard moved by Feb 1, 10-hour shift for 4 weeks after notice, etc.)\n  - Implications by product: Crew Cab Running Boards; Extended Cab Running Boards; Truck Grill Guard\n  - Deadline compliance statements explicitly answering yes/no:\n    \u2022 April PO in transit by Apr 13?\n    \u2022 May PO ships by May 1 (on time)?\n    \u2022 Grill guard shipments remain on schedule?\n\nScoring:\n- 3.0: Valid document with all three scenarios, all required subsections, and explicit yes/no for each deadline item.\n- 2.0: Valid document with all three scenarios but one required subsection missing in one scenario or missing explicit yes/no for one deadline item.\n- 1.0: Only 1\u20132 scenarios covered or multiple missing subsections; still a valid formatted document.\n- 0.0: No separate written document found, or it is not PDF/DOCX/MD.", "expectation": "A professional scenario summary document with 3 sections, actions, implications per product, and explicit yes/no deadline outcomes."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness and Constraints)", "description": "Now that the structure exists, verify factual and logical constraints using light code checks and deeper LLM reasoning. Code rules focus on deterministic bounds; LLM rules verify sequencing and deadline logic across artifacts.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Capacity Bounds by Date and Scenario", "description": "Daily planned quantities must not exceed allowable capacity windows for each scenario.", "weight": 0.5, "code": "import pandas as pd\\nimport numpy as np\\nimport re\\nfrom datetime import datetime, date\\n\\nDEF_S1 = 's1'\\nDEF_S2 = 's2'\\nDEF_S3 = 's3'\\n\\nSTART_UPGRADE = pd.Timestamp('2018-02-05')\\nTEN_HR_START = pd.Timestamp('2018-02-05')  # 10-hr window aligned to base 135/day\\nTEN_HR_END = pd.Timestamp('2018-03-04')    # 4 weeks from Feb 5 inclusive\\n\\nSCENARIO_PATTERNS = {\\n    DEF_S1: ['scenario 1', 'current capacity'],\\n    DEF_S2: ['scenario 2', 'without', 'no grill', 'no grill guard'],\\n    DEF_S3: ['scenario 3', '10-hour', '10 hour', 'expanded capacity']\\n}\\n\\nDATE_KEYS = [['date']]\\nPRODUCT_KEYS = [['product'], ['sku'], ['item'], ['model'], ['variant']]\\nQTY_KEYS = [['planned', 'qty'], ['planned', 'quantity'], ['planned', 'units'], ['production'], ['plan qty']]\\n\\nRB_CREW_PAT = re.compile(r'crew', re.I)\\nRB_EXT_PAT = re.compile(r'ext|extended', re.I)\\nGRILL_PAT = re.compile(r'grill', re.I)\\n\\ndef find_spreadsheet(context):\\n    # Prefer primary output if spreadsheet, else search all outputs\\n    out = context.get_primary_output()\\n    if out and getattr(out, 'is_spreadsheet', False):\\n        return out\\n    for r in context.get_all_outputs():\\n        if getattr(r, 'is_spreadsheet', False):\\n            return r\\n    return None\\n\\ndef get_sheet_map(xls):\\n    try:\\n        return {sn: sn for sn in xls.sheet_names}\\n    except Exception:\\n        return {}\\n\\ndef normalize(s):\\n    return re.sub(r'\\s+', ' ', str(s)).strip().lower()\\n\\ndef col_find(df, keys_list):\\n    cols = [normalize(c) for c in df.columns]\\n    for i, c in enumerate(cols):\\n        for keys in keys_list:\\n            if all(k in c for k in keys):\\n                return df.columns[i]\\n    return None\\n\\ndef detect_scenario(name):\\n    n = normalize(name)\\n    for sid, pats in SCENARIO_PATTERNS.items():\\n        if all(any(p in n for p in [pat]) for pat in [pats[0]]):\\n            # quick first token match\\n            pass\\n        # match if any of the pattern tokens appear in name\\n        hits = sum(1 for p in pats if p in n)\\n        if hits >= 1:\\n            if sid == DEF_S2:\\n                # ensure it's the 'without/no grill' one if indicated\\n                if any(term in n for term in ['without', 'no grill', 'no grill guard']):\\n                    return sid\\n            else:\\n                return sid\\n    # fallback guesses\\n    if 'scenario 1' in n: return DEF_S1\\n    if 'scenario 2' in n: return DEF_S2\\n    if 'scenario 3' in n: return DEF_S3\\n    return None\\n\\ndef allowed_capacity(sid, d):\\n    # Default capacities per instructions\\n    if sid in (DEF_S1, DEF_S2):\\n        if d < START_UPGRADE:\\n            return 120\\n        else:\\n            return 135\\n    elif sid == DEF_S3:\\n        # 10-hour shift allows 170/day only within Feb 5 to Mar 4 inclusive\\n        if d < START_UPGRADE:\\n            return 120\\n        if TEN_HR_START <= d <= TEN_HR_END:\\n            return 170\\n        else:\\n            return 135\\n    return 999999\\n\\ndef evaluate(workflow, context):\\n    try:\\n        ss = find_spreadsheet(context)\\n        if not ss:\\n            return 0.0, 'No spreadsheet found'\\n        path = context.files.get_path(ss.id)\\n        xls = pd.ExcelFile(path)\\n        sheet_names = xls.sheet_names\\n        total_rows = 0\\n        ok_rows = 0\\n        feedback_bits = []\\n        for sn in sheet_names:\\n            sid = detect_scenario(sn)\\n            if sid not in (DEF_S1, DEF_S2, DEF_S3):\\n                continue\\n            df = pd.read_excel(path, sheet_name=sn)\\n            if df.empty:\\n                continue\\n            dcol = col_find(df, DATE_KEYS)\\n            qcol = col_find(df, QTY_KEYS)\\n            if dcol is None or qcol is None:\\n                feedback_bits.append(f'{sn}: missing Date/Planned Qty columns')\\n                continue\\n            # coerce date and qty\\n            dser = pd.to_datetime(df[dcol], errors='coerce')\\n            qser = pd.to_numeric(df[qcol], errors='coerce').fillna(0)\\n            mask_valid = dser.notna() & qser.notna()\\n            dfv = df[mask_valid].copy()\\n            dser = dser[mask_valid]\\n            qser = qser[mask_valid]\\n            caps = dser.apply(lambda d: allowed_capacity(sid, d))\\n            comp = (qser <= caps + 1e-6)  # tolerance\\n            total_rows += len(comp)\\n            ok_rows += int(comp.sum())\\n            if len(comp) > 0:\\n                viol = int((~comp).sum())\\n                if viol>0:\\n                    feedback_bits.append(f'{sn}: {viol} rows exceed capacity window')\\n        if total_rows == 0:\\n            return 0.0, 'No readable scenario rows to check'\\n        frac = ok_rows / total_rows\\n        score = frac * 0.5\\n        return score, f'Capacity compliance: {ok_rows}/{total_rows} rows OK. ' + ('; '.join(feedback_bits) if feedback_bits else '')\\n    except Exception as e:\\n        return 0.0, f'Error: {e}'"}, {"type": "code", "name": "Single Product Per Day Constraint", "description": "For each scenario sheet, on any given date only one product should have a nonzero planned quantity.", "weight": 0.5, "code": "import pandas as pd\\nimport numpy as np\\nimport re\\n\\nDATE_KEYS = [['date']]\\nPRODUCT_KEYS = [['product'], ['sku'], ['item'], ['model'], ['variant']]\\nQTY_KEYS = [['planned', 'qty'], ['planned', 'quantity'], ['planned', 'units'], ['production'], ['plan qty']]\\n\\ndef find_spreadsheet(context):\\n    out = context.get_primary_output()\\n    if out and getattr(out, 'is_spreadsheet', False):\\n        return out\\n    for r in context.get_all_outputs():\\n        if getattr(r, 'is_spreadsheet', False):\\n            return r\\n    return None\\n\\ndef normalize(s):\\n    import re\\n    return re.sub(r'\\s+', ' ', str(s)).strip().lower()\\n\\ndef col_find(df, keys_list):\\n    cols = [normalize(c) for c in df.columns]\\n    for i, c in enumerate(cols):\\n        for keys in keys_list:\\n            if all(k in c for k in keys):\\n                return df.columns[i]\\n    return None\\n\\ndef evaluate(workflow, context):\\n    try:\\n        ss = find_spreadsheet(context)\\n        if not ss:\\n            return 0.0, 'No spreadsheet found'\\n        path = context.files.get_path(ss.id)\\n        xls = pd.ExcelFile(path)\\n        total_days = 0\\n        ok_days = 0\\n        notes = []\\n        for sn in xls.sheet_names:\\n            if 'scenario' not in normalize(sn):\\n                continue\\n            df = pd.read_excel(path, sheet_name=sn)\\n            if df.empty:\\n                continue\\n            dcol = col_find(df, DATE_KEYS)\\n            pcol = col_find(df, PRODUCT_KEYS)\\n            qcol = col_find(df, QTY_KEYS)\\n            if dcol is None or pcol is None or qcol is None:\\n                notes.append(f'{sn}: missing Date/Product/Planned Qty columns')\\n                continue\\n            dser = pd.to_datetime(df[dcol], errors='coerce')\\n            qser = pd.to_numeric(df[qcol], errors='coerce').fillna(0)\\n            pser = df[pcol].astype(str).fillna('')\\n            mask = dser.notna()\\n            dfv = pd.DataFrame({'date': dser[mask], 'qty': qser[mask], 'prod': pser[mask]})\\n            # Aggregate by date and count how many products have qty>0\\n            for d, sub in dfv.groupby('date'):\\n                total_days += 1\\n                positive = (sub['qty'] > 0).sum() if not sub.empty else 0\\n                if positive <= 1:\\n                    ok_days += 1\\n            # Continue across sheets\\n        if total_days == 0:\\n            return 0.0, 'No scenario days to evaluate'\\n        frac = ok_days / total_days\\n        return 0.5 * frac, f'Single-product days OK: {ok_days}/{total_days}'\\n    except Exception as e:\\n        return 0.0, f'Error: {e}'"}, {"type": "code", "name": "Weekend Non-Production (No Saturdays/Sundays)", "description": "Planned quantity on Saturdays or Sundays should be zero or rows omitted.", "weight": 0.5, "code": "import pandas as pd\\nimport numpy as np\\n\\nDATE_KEYS = [['date']]\\nQTY_KEYS = [['planned', 'qty'], ['planned', 'quantity'], ['planned', 'units'], ['production'], ['plan qty']]\\n\\ndef find_spreadsheet(context):\\n    out = context.get_primary_output()\\n    if out and getattr(out, 'is_spreadsheet', False):\\n        return out\\n    for r in context.get_all_outputs():\\n        if getattr(r, 'is_spreadsheet', False):\\n            return r\\n    return None\\n\\ndef normalize(s):\\n    import re\\n    return re.sub(r'\\s+', ' ', str(s)).strip().lower()\\n\\ndef col_find(df, keys_list):\\n    cols = [normalize(c) for c in df.columns]\\n    for i, c in enumerate(cols):\\n        for keys in keys_list:\\n            if all(k in c for k in keys):\\n                return df.columns[i]\\n    return None\\n\\ndef evaluate(workflow, context):\\n    try:\\n        ss = find_spreadsheet(context)\\n        if not ss:\\n            return 0.0, 'No spreadsheet found'\\n        path = context.files.get_path(ss.id)\\n        xls = pd.ExcelFile(path)\\n        weekend_rows = 0\\n        ok_rows = 0\\n        for sn in xls.sheet_names:\\n            if 'scenario' not in normalize(sn):\\n                continue\\n            df = pd.read_excel(path, sheet_name=sn)\\n            if df.empty:\\n                continue\\n            dcol = col_find(df, DATE_KEYS)\\n            qcol = col_find(df, QTY_KEYS)\\n            if dcol is None or qcol is None:\\n                continue\\n            dser = pd.to_datetime(df[dcol], errors='coerce')\\n            qser = pd.to_numeric(df[qcol], errors='coerce').fillna(0)\\n            mask = dser.notna()\\n            dser = dser[mask]\\n            qser = qser[mask]\\n            wknd = dser.dt.weekday >= 5\\n            weekend_rows += int(wknd.sum())\\n            if int(wknd.sum())>0:\\n                ok_rows += int((qser[wknd] <= 0).sum())\\n        if weekend_rows == 0:\\n            return 0.5, 'No weekend rows present; assuming compliant.'\\n        frac = ok_rows / weekend_rows if weekend_rows>0 else 1.0\\n        return 0.5 * frac, f'Weekend non-production compliance: {ok_rows}/{weekend_rows} weekend rows have zero plan'\\n    except Exception as e:\\n        return 0.0, f'Error: {e}'"}, {"type": "code", "name": "Grill Guard Scheduling Rule by Scenario", "description": "Scenario 1 should include Grill Guard production near 100 units/week; Scenarios 2 and 3 should show no Grill Guard in this cell.", "weight": 0.5, "code": "import pandas as pd\\nimport numpy as np\\nimport re\\n\\nDATE_KEYS = [['date']]\\nPRODUCT_KEYS = [['product'], ['sku'], ['item'], ['model'], ['variant']]\\nQTY_KEYS = [['planned', 'qty'], ['planned', 'quantity'], ['planned', 'units'], ['production'], ['plan qty']]\\n\\nGRILL_PAT = re.compile(r'grill', re.I)\\n\\ndef find_spreadsheet(context):\\n    out = context.get_primary_output()\\n    if out and getattr(out, 'is_spreadsheet', False):\\n        return out\\n    for r in context.get_all_outputs():\\n        if getattr(r, 'is_spreadsheet', False):\\n            return r\\n    return None\\n\\ndef normalize(s):\\n    import re\\n    return re.sub(r'\\s+', ' ', str(s)).strip().lower()\\n\\ndef col_find(df, keys_list):\\n    cols = [normalize(c) for c in df.columns]\\n    for i, c in enumerate(cols):\\n        for keys in keys_list:\\n            if all(k in c for k in keys):\\n                return df.columns[i]\\n    return None\\n\\ndef scenario_kind(name):\\n    n = normalize(name)\\n    if 'scenario 1' in n or ('current' in n and 'capacity' in n and 'grill' not in n and 'without' not in n and 'no' not in n):\\n        return 1\\n    if 'scenario 2' in n or 'without' in n or 'no grill' in n or 'no grill guard' in n:\\n        return 2\\n    if 'scenario 3' in n or '10-hour' in n or '10 hour' in n or 'expanded' in n:\\n        return 3\\n    return 0\\n\\ndef evaluate(workflow, context):\\n    try:\\n        ss = find_spreadsheet(context)\\n        if not ss:\\n            return 0.0, 'No spreadsheet found'\\n        path = context.files.get_path(ss.id)\\n        xls = pd.ExcelFile(path)\\n        score = 0.5\\n        parts = []\\n        # We'll average compliance across the three scenarios (if present)\\n        comps = []\\n        for sn in xls.sheet_names:\\n            kind = scenario_kind(sn)\\n            if kind == 0:\\n                continue\\n            df = pd.read_excel(path, sheet_name=sn)\\n            if df.empty:\\n                continue\\n            dcol = col_find(df, DATE_KEYS)\\n            pcol = col_find(df, PRODUCT_KEYS)\\n            qcol = col_find(df, QTY_KEYS)\\n            if dcol is None or pcol is None or qcol is None:\\n                continue\\n            dser = pd.to_datetime(df[dcol], errors='coerce')\\n            pser = df[pcol].astype(str)\\n            qser = pd.to_numeric(df[qcol], errors='coerce').fillna(0)\\n            mask = dser.notna()\\n            dfv = pd.DataFrame({'date': dser[mask], 'prod': pser[mask], 'qty': qser[mask]})\\n            if kind == 1:\\n                # Expect ~100/week; compute weekly sums where prod contains 'grill'\\n                dfv['is_grill'] = dfv['prod'].str.contains(GRILL_PAT)\\n                grill = dfv[dfv['is_grill']].copy()\\n                if grill.empty:\\n                    comps.append(0.0)\\n                    parts.append(f'{sn}: No grill guard production found (expected ~100/wk)')\\n                else:\\n                    grp = grill.groupby([grill['date'].dt.isocalendar().year, grill['date'].dt.isocalendar().week])['qty'].sum()\\n                    if len(grp)==0:\\n                        comps.append(0.0)\\n                    else:\\n                        diffs = grp.apply(lambda x: abs(x - 100))\\n                        # Allow +/-10 tolerance\\n                        ok_weeks = int((diffs <= 10).sum())\\n                        comps.append(ok_weeks / max(1, len(grp)))\\n                        parts.append(f'{sn}: Grill guard weeks within tolerance {ok_weeks}/{len(grp)}')\\n            elif kind in (2,3):\\n                # Expect zero grill production in this cell\\n                dfv['is_grill'] = dfv['prod'].str.contains(GRILL_PAT)\\n                total_grill = int((dfv['qty'][dfv['is_grill']]).sum())\\n                comps.append(1.0 if total_grill == 0 else max(0.0, 1 - min(1.0, total_grill/500.0)))\\n                if total_grill>0:\\n                    parts.append(f'{sn}: Found grill guard qty {total_grill} (expected 0)')\\n        if not comps:\\n            return 0.0, 'No scenario sheets detected for grill guard check'\\n        frac = float(np.mean(comps))\\n        return 0.5 * frac, '; '.join(parts) if parts else 'Grill guard scheduling compliant'\\n    except Exception as e:\\n        return 0.0, f'Error: {e}'"}, {"type": "llm_judge", "name": "Priority Sequencing Verified (Crew vs Extended)", "description": "Crew Cab Dec\u2013Feb before Extended Cab Nov\u2013Feb; then Crew Mar/Apr before Extended Mar/Apr, as reflected by the daily plans.", "weight": 4.0, "judge_prompt": "Using the Excel scenarios, verify that the production priority rules are actually reflected in the day-by-day schedule (do not recalculate quantities; check sequencing):\n- Rule 1: Crew Cab Running Boards for December\u2013February POs are produced before any Extended Cab Running Boards for November\u2013February POs.\n- Rule 2: Once Rule 1 is satisfied, Crew Cab Running Boards for March/April are produced before Extended Cab Running Boards for March/April.\n\nGuidance:\n- Look for daily plan segments that show Crew Cab days completing the specified PO months before any Extended Cab days for overlapping months.\n- A small amount of interleaving for changeovers is acceptable only if Extended Cab for the restricted months does not occur before Crew Cab for those months.\n- Consider each of the three scenario sheets; all should respect the stated priorities.\n\nScoring:\n- 4.0: All three scenarios clearly respect both priority rules without contradictions.\n- 3.0: Small inconsistencies in one scenario but overall sequencing intent is followed.\n- 1.5: Multiple inconsistencies or ambiguous labeling; intent partially followed.\n- 0.0: Sequencing priorities not evident or clearly violated in most scenarios.", "expectation": "Visible sequencing across daily schedules showing Crew-first priorities for the specified months, then Extended."}, {"type": "llm_judge", "name": "Deadline Compliance Evidence (Apr 13 and May 1)", "description": "Check whether each scenario shows clear evidence and statements that April PO is in transit by Apr 13 and May PO ships by May 1.", "weight": 3.0, "judge_prompt": "For each scenario, verify deadline coverage using BOTH the Excel (e.g., a Deadline/Checkpoint column, milestone markers, or annotations) and the written summary:\n- April PO is indicated to be in transit by April 13, 2018.\n- May PO is indicated to ship by May 1, 2018 (on time).\n\nYou do NOT need exact PO quantities; check for explicit markers, flags, notes, or summary statements that tie back to specific dates in the plan. Cross-reference that the dates mentioned in the summary align with the Excel schedule (e.g., the plan shows production completion prior to these dates and a ship/in-transit note).\n\nScoring:\n- 3.0: All scenarios contain clear, date-specific evidence in the Excel and matching yes/no statements in the summary.\n- 2.0: Minor gaps (e.g., missing explicit flag on one scenario) but summary and schedule appear aligned.\n- 1.0: Evidence is weak/implicit or inconsistent across the two artifacts.\n- 0.0: No clear evidence of deadline compliance.", "expectation": "Explicit deadline flags/notes in Excel and yes/no statements in the summary that align by date."}, {"type": "llm_judge", "name": "Cross-Consistency: Summary vs Excel Assumptions", "description": "Actions and assumptions in the written summary should match the Assumptions & Rules sheet and observed plan behavior.", "weight": 3.0, "judge_prompt": "Check consistency between the written summary and the Excel \"Assumptions & Rules\" sheet and the observed daily plans:\n- Scenario 2 and 3 should explicitly indicate Grill Guard production is moved out of the running board cell (thus plan shows no Grill Guard days for those scenarios).\n- Scenario 3 should indicate a 10-hour shift window (after 30-day notice) for four weeks starting early February; the daily plan should show higher daily outputs in that window only.\n- Scenario 1 should reflect current capacity (120/day until Feb 5, then 135/day) and include Grill Guard at about 100 units/week.\n- One-product-per-day constraint should be stated and reflected in the plan.\n\nScoring:\n- 3.0: All listed items are consistently described in the summary and reflected in the Excel plan behavior.\n- 2.0: Minor inconsistencies in phrasing or timing but overall alignment.\n- 1.0: Multiple mismatches across items.\n- 0.0: Largely inconsistent or unverifiable claims.", "expectation": "Summary accurately describes the same rules/timings that the Excel plan implements."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Managerial Usefulness", "description": "Holistic assessment of professionalism, clarity, and decision usefulness for the operations managers meeting.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Excel Clarity and Usability", "description": "Professional formatting, consistent labels, readable dates and units, and easy navigation to verify assumptions and milestones.", "weight": 2.0, "judge_prompt": "Evaluate the Excel model's clarity and usability:\n- Clear, consistent column headers; readable date formats; units labeled (sets/day, units/week)\n- Consistent structure across all scenario sheets; freeze panes/filters helpful; legible fonts\n- Assumptions & Rules sheet is easy to find and read\n- Milestones/deadline flags are visible and unambiguous\n\nScoring: 2.0 excellent; 1.0 adequate/minor issues; 0.0 poor/unusable.", "expectation": "A clean, consistent workbook that makes verification straightforward."}, {"type": "llm_judge", "name": "Comparative Insight and Recommendation", "description": "Does the written summary compare scenarios, articulate trade-offs, and make a justified recommendation?", "weight": 2.0, "judge_prompt": "In the written summary, assess whether the author:\n- Compares the three scenarios on feasibility, risks, and customer impact\n- Explains trade-offs (e.g., grill guard relocation impact, labor scheduling, capacity windows)\n- Makes a clear recommendation (or conditional recommendation) supported by the plan\n\nScoring: 2.0 strong comparative analysis + recommendation; 1.0 partial; 0.0 absent/weak.", "expectation": "Concise comparison with a reasoned recommendation grounded in the plan."}, {"type": "llm_judge", "name": "Risk, Constraints, and Contingencies", "description": "Identification of risks (materials, labor, schedule), constraints (no overtime, one product at a time), and contingencies/mitigations.", "weight": 2.0, "judge_prompt": "Assess whether the summary identifies:\n- Key risks: supply re-start hiccups, labor constraints, grill guard transition risk, holiday impacts\n- Constraints acknowledged: 1 product/day, no overtime, stat holidays\n- Contingencies: buffer days, expediting within rules, communication with customer\n\nScoring: 2.0 comprehensive; 1.0 partial; 0.0 minimal.", "expectation": "Explicit risks and practical mitigations aligned to constraints."}, {"type": "llm_judge", "name": "Meeting-Ready Communication", "description": "Fitness for the operations managers weekly meeting: concise, structured, action-oriented.", "weight": 2.0, "judge_prompt": "Evaluate whether the combined deliverables are meeting-ready:\n- Executive-style brevity in the summary with clear next steps\n- Visual cues (e.g., highlighting of key dates) in Excel aiding quick review\n- Clear mapping between scenarios in summary and Excel\n\nScoring: 2.0 meeting-ready; 1.0 somewhat; 0.0 not suitable.", "expectation": "Concise, action-oriented communication with clear linkage between artifacts."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "b57efde3-26d6-4742-bbff-2b63c43b4baa", "rubric": {"category_name": "Aqua Nor 2025 OEM Prospecting Workbook (Aquaculture AUV/ROV/UC)", "rationale": "This rubric enforces a self-documenting, verifiable Excel workbook for trade show prospecting. Stage 1 is a strict LLM-only shape gate that mandates a specific workbook structure, enabling automated checks. Stage 2 mixes light, robust code checks (bounds, data hygiene) with higher-weight LLM verification for relevance and reasoning, reflecting that nuanced correctness requires judgment. Stage 3 assesses professional quality and event usability for real-world sales outcomes.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Structured Workbook Gate", "description": "LLM-only gate to ensure the candidate produced a verifiable Excel workbook with the exact structure required for downstream validation.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.5, "rules": [{"type": "llm_judge", "name": "Structured Prospecting Workbook Requirement", "description": "Check that the output is a properly structured Excel prospecting workbook with all required sheets, sections, and columns present to enable verification.", "weight": 6.0, "judge_prompt": "You are verifying the SHAPE and STRUCTURE (not the correctness) of the candidate's output for a trade show prospecting task.\n\nRequirements:\nFORMAT\n- Must be a Microsoft Excel workbook (.xlsx). Not CSV, not Google Sheets link, not PDF/DOCX.\n- Should be clearly formatted for tabular review (e.g., headers in first row, one row per company).\n\nREQUIRED SHEET 1: \"Prospect List\" (or a very close name like \"Prospects\", \"Lead List\", \"Aqua Nor Prospects\")\n- Must be a single flat table with one row per company/exhibitor.\n- Required columns (be flexible on exact names, but the fields must be present and distinct):\n  1) Company Name\n  2) Booth ID/Number\n  3) Country\n  4) Website\n  5) Product Category (must indicate AUV, ROV, or Underwater Camera/UC; multiples allowed)\n  6) Product Line/Model (or equivalent)\n  7) Product Notes / Capabilities (why it is an AUV/ROV/UC, features, specs)\n  8) Aquaculture Relevance (how it\u2019s used in aquaculture; species/farm/application)\n  9) LakeHealth DO Fit (Yes/No/Maybe)\n  10) Integration Rationale (technical fit, where DO sensor adds value)\n  11) Priority (A/B/C or High/Med/Low)\n  12) Contact Name\n  13) Title/Role\n  14) Email\n  15) Phone\n  16) LinkedIn URL\n  17) Source URL (Aqua Nor exhibitor page or company page used)\n  18) Last Verified (date)\n  19) Status (e.g., To Contact/Contacted/Meeting Set/Not a Fit)\n  20) Owner / Next Action (or Meeting Time if scheduled)\n\nREQUIRED SHEET 2: \"Sources & Method\" (or a close variant like \"Methodology & Sources\")\n- Must be a readable documentation sheet with:\n  A) Data Sources: list the Aqua Nor Exhibitor list URL and any other sources\n  B) Selection Criteria: must explicitly mention AUV and/or ROV and/or Underwater Camera\n  C) Collection Methodology & Date (how findings were compiled)\n  D) Glossary/Allowed Values for key fields: Product Category, LakeHealth DO Fit, Priority, Status\n  E) Summary Metrics: counts by Product Category and counts by Priority\n\nOPTIONAL (nice-to-have) SHEET 3: \"Shortlist\" or \"Outreach Plan\"\n- A filtered view or top leads with concise notes (e.g., target personas, first-touch messaging).\n\nScoring (STRUCTURE ONLY):\n- 6.0: .xlsx format, both required sheets present; Prospect List has all required columns (allow close naming), Sources & Method includes all 5 elements; optional sheet present or not (does not affect full credit).\n- 5.0: .xlsx format, both required sheets present; Prospect List missing up to 2 minor fields OR Sources & Method missing 1 of the 5 elements.\n- 3.5: .xlsx format, both required sheets present but Prospect List missing 3\u20135 fields OR Sources & Method missing 2 of the 5 elements.\n- 2.0: .xlsx format, only one required sheet present OR Prospect List table not clearly structured.\n- 0.0: Not an Excel .xlsx, OR missing both required sheets.\n\nOnly assess presence/structure, not data correctness or quality.", "expectation": "A clearly structured .xlsx with a comprehensive Prospect List table and a Sources & Method sheet enabling verification."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification of Correctness and Relevance", "description": "Automated and LLM checks to verify dataset sufficiency, data hygiene, categorical validity, and substantive relevance to AUV/ROV/UC and DO sensor integration.", "is_required": false, "max_points": 9.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Minimum Prospect Volume", "description": "Verify the Prospect List contains a meaningful number of unique companies (rows).", "weight": 0.4, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef _load_prospect_sheet(context, output):\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        # find a sheet with name containing 'prospect' or 'lead'\n        target = None\n        for s in xls.sheet_names:\n            s_low = str(s).lower()\n            if 'prospect' in s_low or 'lead' in s_low:\n                target = s\n                break\n        if target is None:\n            # fallback to first sheet\n            target = xls.sheet_names[0]\n        df = pd.read_excel(path, sheet_name=target)\n        return df\n    except Exception:\n        return None\n\ndef _find_col(df, keys):\n    cols = list(df.columns)\n    for i, c in enumerate(cols):\n        cl = str(c).strip().lower()\n        for k in keys:\n            if k in cl:\n                return c\n    return None\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n    df = _load_prospect_sheet(context, output)\n    if df is None or df.empty:\n        return 0.0, \"Could not read Prospect sheet or it is empty.\"\n\n    # Identify a company column; fallback to counting non-empty rows\n    company_col = _find_col(df, ['company'])\n    if company_col is not None:\n        series = df[company_col].astype(str).str.strip()\n        n = (series.replace({'nan':'', 'None':'', 'NaN':''}).astype(str).str.len() > 0).sum()\n    else:\n        # use any row with any non-null across row\n        n = int((~df.isna()).any(axis=1).sum())\n\n    # Graded thresholds\n    if n >= 25:\n        score = 0.4\n        fb = f\"{n} prospects found (>=25).\"\n    elif n >= 15:\n        score = 0.3\n        fb = f\"{n} prospects found (>=15).\"\n    elif n >= 10:\n        score = 0.2\n        fb = f\"{n} prospects found (>=10).\"\n    elif n >= 5:\n        score = 0.1\n        fb = f\"{n} prospects found (>=5).\"\n    else:\n        score = 0.0\n        fb = f\"Only {n} prospects found (<5).\"\n    return score, fb"}, {"type": "code", "name": "Category Validity and Cleanliness", "description": "Check Product Category values are constrained to AUV/ROV/UC (singular/plural, synonyms).", "weight": 0.4, "code": "import re\nimport pandas as pd\nimport numpy as np\n\nVALID_TOKENS = {\n    'auv': ['auv','auvs','autonomous underwater vehicle','autonomous underwater vehicles'],\n    'rov': ['rov','rovs','remotely operated vehicle','remotely operated vehicles'],\n    'uc': ['uc','ucs','underwater camera','underwater cameras','camera','cameras']\n}\n\nALL_VALID = set(sum(VALID_TOKENS.values(), []))\nSEPS = r\"[,;/|]\"\n\ndef _load_df(context, output):\n    try:\n        path = context.files.get_path(output.id)\n        # attempt to find a sheet with 'prospect' or 'lead'\n        xls = pd.ExcelFile(path)\n        target = None\n        for s in xls.sheet_names:\n            sl = str(s).lower()\n            if 'prospect' in sl or 'lead' in sl:\n                target = s\n                break\n        if target is None:\n            target = xls.sheet_names[0]\n        return pd.read_excel(path, sheet_name=target)\n    except Exception:\n        return None\n\ndef _find_col(df, keys):\n    for c in df.columns:\n        cl = str(c).lower()\n        if any(k in cl for k in keys):\n            return c\n    return None\n\ndef _normalize_token(tok):\n    return re.sub(r\"\\s+\", \" \", tok.strip().lower())\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    df = _load_df(context, output)\n    if df is None or df.empty:\n        return 0.0\n\n    cat_col = _find_col(df, ['category','product type','auv','rov','camera'])\n    if cat_col is None:\n        return 0.0\n\n    vals = df[cat_col].astype(str).fillna('').tolist()\n    if len(vals) == 0:\n        return 0.0\n\n    valid_count = 0\n    nonempty = 0\n    for v in vals:\n        v = v.strip()\n        if not v or v.lower() in ('nan', 'none'):\n            continue\n        nonempty += 1\n        parts = re.split(SEPS, v)\n        good = True\n        for p in parts:\n            t = _normalize_token(p)\n            # allow short forms present as substrings\n            if t in ALL_VALID:\n                continue\n            # flexible contain check\n            if any(t == k or k in t for k in ALL_VALID):\n                continue\n            good = False\n            break\n        if good:\n            valid_count += 1\n\n    if nonempty == 0:\n        return 0.0\n\n    ratio = valid_count / nonempty\n    # Directly scale to weight=0.4\n    return 0.4 * ratio, f\"Category validity ratio={ratio:.2f} over {nonempty} non-empty rows.\""}, {"type": "code", "name": "Contact and URL Sanity", "description": "Check presence of workable contact channels (email/phone/LinkedIn) and valid URLs for Website/Source.", "weight": 0.4, "code": "import re\nimport pandas as pd\n\nEMAIL_RE = re.compile(r\"^[^@\\s]+@[^@\\s]+\\.[^@\\s]+$\")\nHTTP_RE = re.compile(r\"^https?://\", re.I)\n\ndef _load_df(context, output):\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        target = None\n        for s in xls.sheet_names:\n            sl = str(s).lower()\n            if 'prospect' in sl or 'lead' in sl:\n                target = s\n                break\n        if target is None:\n            target = xls.sheet_names[0]\n        return pd.read_excel(path, sheet_name=target)\n    except Exception:\n        return None\n\ndef _find_col(df, keys):\n    for c in df.columns:\n        cl = str(c).lower()\n        if any(k in cl for k in keys):\n            return c\n    return None\n\ndef _has_valid_phone(val):\n    s = re.sub(r\"\\D\", \"\", str(val))\n    return len(s) >= 7\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    df = _load_df(context, output)\n    if df is None or df.empty:\n        return 0.0\n\n    email_col = _find_col(df, ['email'])\n    phone_col = _find_col(df, ['phone','tel'])\n    li_col = _find_col(df, ['linkedin'])\n    web_col = _find_col(df, ['website','site','url'])\n    src_col = _find_col(df, ['source'])\n\n    n = len(df)\n    if n == 0:\n        return 0.0\n\n    def sget(row, col):\n        if col is None:\n            return ''\n        v = row.get(col, '')\n        return '' if pd.isna(v) else str(v)\n\n    good_rows = 0\n    good_urls = 0\n    for _, row in df.iterrows():\n        email = sget(row, email_col)\n        phone = sget(row, phone_col)\n        li = sget(row, li_col)\n        web = sget(row, web_col)\n        src = sget(row, src_col)\n\n        has_contact = bool(EMAIL_RE.match(email)) or _has_valid_phone(phone) or ('linkedin.com' in li.lower())\n        has_urls = (HTTP_RE.match(web or '') is not None) and (HTTP_RE.match(src or '') is not None)\n\n        if has_contact:\n            good_rows += 1\n        if has_urls:\n            good_urls += 1\n\n    contact_ratio = good_rows / n\n    url_ratio = good_urls / n\n    # average of the two ratios scaled to weight\n    score = 0.4 * ((contact_ratio + url_ratio) / 2)\n    fb = f\"Contact ratio={contact_ratio:.2f}, URL ratio={url_ratio:.2f}.\"\n    return score, fb"}, {"type": "code", "name": "Duplicate Company Check", "description": "Check for duplicate Company Names (case and punctuation-insensitive).", "weight": 0.3, "code": "import re\nimport pandas as pd\n\nTRANS = str.maketrans('', '', \"`'\\\".,;:!?()[]{}-/\\\\\")\n\ndef _load_df(context, output):\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        target = None\n        for s in xls.sheet_names:\n            sl = str(s).lower()\n            if 'prospect' in sl or 'lead' in sl:\n                target = s\n                break\n        if target is None:\n            target = xls.sheet_names[0]\n        return pd.read_excel(path, sheet_name=target)\n    except Exception:\n        return None\n\n\ndef _find_col(df, keys):\n    for c in df.columns:\n        cl = str(c).lower()\n        if any(k in cl for k in keys):\n            return c\n    return None\n\n\ndef _norm_name(s):\n    s = str(s).strip().lower()\n    s = s.translate(TRANS)\n    s = re.sub(r\"\\s+\", \" \", s)\n    return s\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    df = _load_df(context, output)\n    if df is None or df.empty:\n        return 0.0\n\n    company_col = _find_col(df, ['company'])\n    if company_col is None:\n        return 0.0\n\n    names = df[company_col].dropna().astype(str).map(_norm_name)\n    if names.empty:\n        return 0.0\n\n    total = len(names)\n    dup_count = int(names.duplicated().sum())\n    dup_ratio = dup_count / total if total else 1.0\n\n    if dup_ratio <= 0.05:\n        score = 0.3\n    elif dup_ratio <= 0.10:\n        score = 0.2\n    elif dup_ratio <= 0.20:\n        score = 0.1\n    else:\n        score = 0.0\n    return score, f\"Duplicate ratio={dup_ratio:.2f} (duplicates={dup_count} of {total}).\""}, {"type": "llm_judge", "name": "Lead Relevance to AUV/ROV/Underwater Camera", "description": "Check that entries genuinely pertain to AUVs, ROVs, and/or underwater cameras and are aquaculture-relevant.", "weight": 3.0, "judge_prompt": "Open the Excel workbook. Inspect the Prospect List rows and Product Notes/Aquaculture Relevance fields.\nEvaluate:\n- Does a clear majority of entries describe products that are actually AUVs, ROVs, or underwater cameras? (Look for vehicle/camera terminology, model names, payloads, depth ratings, tether, autonomy, imaging)\n- Is aquaculture relevance articulated (e.g., net pen inspection, biomass assessment, environmental monitoring, cage inspection, farm operations)?\n\nScoring:\n- 3.0: Strong relevance. >=80% rows clearly tied to AUV/ROV/UC and aquaculture context described for most.\n- 2.0: Moderate relevance. 60\u201379% rows fit; some vague or mixed relevance.\n- 1.0: Weak relevance. 30\u201359% fit; many generic or off-topic.\n- 0.0: Mostly irrelevant or unclear.", "expectation": "Most rows clearly map to AUV/ROV/UC with aquaculture context."}, {"type": "llm_judge", "name": "DO Sensor Integration Rationale Quality", "description": "Assess whether the Integration Rationale and LakeHealth DO Fit fields specifically explain how a DO sensor would add value.", "weight": 2.5, "judge_prompt": "Review the Integration Rationale and LakeHealth DO Fit columns.\nLook for specifics such as: dissolved oxygen telemetry during missions, cage or pond DO profiling, payload integration points, connectors/power/data, environmental monitoring stacks, benefits to fish welfare, alarms/thresholds, logging, or API integration.\n\nScoring:\n- 2.5: Specific and technically plausible rationales on most rows; DO sensor value is clear and contextual.\n- 1.5: Some specificity; several rows generic (e.g., \"monitor DO\") but still plausible overall.\n- 0.5: Mostly generic statements; limited technical reasoning.\n- 0.0: No meaningful rationale or misapplied use-cases.", "expectation": "Clear, technically plausible DO integration rationale aligned to AUV/ROV/UC use."}, {"type": "llm_judge", "name": "Methodology Completeness (Sources & Method)", "description": "Evaluate whether documentation explains how the list was created and provides summary metrics and allowed values.", "weight": 1.5, "judge_prompt": "Open the Sources & Method sheet. Confirm presence and sufficiency of:\nA) Data Sources (include the Aqua Nor Exhibitor List URL)\nB) Selection Criteria (explicit mention of AUV/ROV/Underwater Camera)\nC) Collection Methodology & Date (at least 3\u20135 sentences)\nD) Glossary/Allowed Values for Product Category, LakeHealth DO Fit, Priority, Status\nE) Summary Metrics (counts by Product Category and by Priority)\n\nScoring:\n- 1.5: All five items present and sufficiently detailed.\n- 1.0: Four of five present, or light detail.\n- 0.5: Two\u2013three items present.\n- 0.0: One or none present.", "expectation": "Transparent, reproducible process documentation with basic summaries."}, {"type": "llm_judge", "name": "Coverage Balance Across Categories", "description": "Assess whether the list represents AUVs, ROVs, and underwater cameras with reasonable balance for event prospecting.", "weight": 0.5, "judge_prompt": "Scan the Product Category column and tally impressionistically: are there entries for AUVs, ROVs, and underwater cameras? Balance is not strict, but there should be meaningful representation (e.g., at least several entries in each of two categories and some in the third).\n\nScoring:\n- 0.5: All three categories represented with reasonable counts.\n- 0.3: Two categories well represented; third minimal.\n- 0.1: Only one category meaningfully represented.\n- 0.0: No clear category coverage.", "expectation": "All three categories appear with non-trivial representation."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Event Usability", "description": "Holistic LLM assessment of presentation quality, prioritization strategy, and readiness to use at Aqua Nor.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Prioritization and Event Usability", "description": "Evaluate whether the sheet is immediately usable at the trade show: sorting, filters, freeze panes, clear priorities, and statuses.", "weight": 1.5, "judge_prompt": "Assess the Prospect List for practical usability at Aqua Nor:\n- Are Priority and Status populated and used consistently?\n- Are there filters, frozen headers, or clear sorting/grouping to work the list at the booth?\n- Are Booth IDs and Next Actions visible to guide conversations/meetings?\n\nScoring:\n- 1.5: Highly usable: priorities/statuses filled, easy to filter/sort, booth and next steps clear.\n- 1.0: Generally usable with minor gaps.\n- 0.5: Some usability elements present but inconsistent.\n- 0.0: Hard to use during the event.", "expectation": "A ready-to-work list with clear priorities, statuses, booths, and next actions."}, {"type": "llm_judge", "name": "Outreach Readiness", "description": "Check if contact details and outreach planning elements would enable immediate follow-up.", "weight": 1.5, "judge_prompt": "Evaluate whether the workbook would let a rep reach out immediately:\n- Contacts (name/title/email/phone/LinkedIn) are present for many leads.\n- Optional Shortlist/Outreach Plan sheet or fields include messaging hooks or personalization notes.\n\nScoring:\n- 1.5: Strong outreach readiness; most priority leads have solid contacts and notes.\n- 1.0: Moderate readiness; contacts for many leads, some notes.\n- 0.5: Limited contacts/notes.\n- 0.0: Little to no outreach information.", "expectation": "Most priority leads have usable contacts and initial outreach cues."}, {"type": "llm_judge", "name": "Presentation and Data Hygiene", "description": "Professional formatting, consistent naming, and absence of placeholders or obvious errors.", "weight": 1.0, "judge_prompt": "Review formatting and hygiene:\n- Professional look (consistent headers, no obvious typos, consistent country names, dates formatted)\n- Minimal placeholders like \"TBD\", \"Lorem\", or obvious copy-paste artifacts\n- No glaring inconsistencies across key fields\n\nScoring:\n- 1.0: Clean and professional.\n- 0.6: Minor issues.\n- 0.3: Noticeable issues.\n- 0.0: Messy/unprofessional.", "expectation": "Clean, consistent, professional workbook."}, {"type": "llm_judge", "name": "Strategic Value for HiTech H20", "description": "Evaluate whether the list reflects an understanding of where a DO sensor delivers the most value for AUV/ROV/UC in aquaculture.", "weight": 1.0, "judge_prompt": "Consider the selection and notes for strategic fit:\n- Do the chosen companies and rationales reflect high-value DO use cases (e.g., cage/pen DO mapping, welfare monitoring, environmental compliance)?\n- Are top-priority leads those most likely to benefit from LakeHealth DO integration?\n\nScoring:\n- 1.0: Strong strategic alignment throughout.\n- 0.6: Generally aligned with a few mismatches.\n- 0.3: Mixed alignment.\n- 0.0: Poor alignment.", "expectation": "Priorities and rationales align with LakeHealth DO\u2019s strongest value props."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "0e4fe8cd-16d0-4f41-8247-6385b4762582", "rubric": {"category_name": "UHNW Istanbul Trip Itinerary (Excel) - Concierge Excellence", "rationale": "This rubric enforces a self-documenting, verifiable Excel itinerary for a 4-day UHNW trip to Istanbul. Stage 1 is an LLM-only gate that mandates a precise workbook structure (4 day-tabs, standardized columns, and visible hyperlinks). Stage 2 combines light code checks (structure/link presence and entity linkage) with heavier LLM verification for sequence correctness, link relevance, and time-zone math. Stage 3 assesses executive polish and white-glove concierge quality. Code rules use flexible matching and robust parsing per the file-based API; LLM rules handle complex, contextual checks.", "max_total_score": 23.0, "stages": [{"name": "Stage 1: Shape Enforcement Gate \u2013 4-Day Excel Itinerary", "description": "Gate check that the output is a properly structured Excel workbook with one tab per day (4 tabs total) and standardized, verification-friendly columns. LLM-only per instructions.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Workbook Structure and Required Tabs", "description": "Verify the candidate produced a multi-tab Excel workbook with 4 day tabs, each containing a properly structured itinerary table enabling verification.", "weight": 4.0, "judge_prompt": "You are evaluating whether the submitted output has the required STRUCTURE for verification. Only assess format and structural completeness, not content quality or calculation correctness.\n\nCheck the following:\n1) File format:\n   - Must be an Excel workbook (.xlsx or .xls). Not CSV, not PDF/DOCX/plain text.\n\n2) Tabs (Sheets):\n   - There must be 4 tabs, one per day of the journey. Accept names like:\n     \u2022 Day 1 / Day One / June 1 / 2024-06-01\n     \u2022 Day 2 / June 2 / 2024-06-02\n     \u2022 Day 3 / June 3 / 2024-06-03\n     \u2022 Day 4 / June 4 / 2024-06-04\n   - Each tab should clearly correspond to the correct day number/date.\n\n3) Table structure on EACH day tab (flexible on exact header wording but must cover these roles):\n   - Time (start or end time)\n   - Action/Activity/Event\n   - Location/Venue\n   - Provider/Contact/Company\n   - Link/URL/Website (clickable hyperlinks preferred)\n   - Notes/Details/Instructions\n   - Columns should be in a tabular layout with a header row; formatting should make scanning easy.\n\n4) Link visibility:\n   - Each day tab should include multiple clickable links (e.g., to hotel, restaurants, tour guide, service providers). Links should look like real URLs/hyperlinks.\n\n5) Row coverage:\n   - Each day tab should contain a sequence of planned actions spanning the day\u2019s start through end (at least ~5-6 rows per day). Events should be time-ordered or reasonably sequential.\n\nScoring (0\u20134):\n- 4: Excel workbook with exactly 4 day tabs that clearly map to Days 1\u20134; each tab has a table with the six roles above (headers may use synonymous terms), multiple visible hyperlinks per day, and at least ~5-6 sequential rows per day.\n- 3: Excel workbook with 4 day tabs present but with minor structural gaps (e.g., one role missing on one tab, or fewer links on a single tab) while still clearly enabling verification.\n- 2: Excel workbook present but missing a required day tab OR missing multiple required column roles on multiple tabs OR minimal rows that make verification difficult.\n- 0: Not an Excel workbook OR fewer than 3 day tabs OR no recognizable itinerary tables.\n\nOnly evaluate the presence and structure. Do not judge correctness or quality here.", "expectation": "A clean 4-tab Excel itinerary, each tab a day with standardized columns (Time, Action, Location, Provider, Link, Notes) and multiple hyperlinks."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Verification & Consistency Checks", "description": "Now verify factual coverage, links and entity presence, sequencing logic, and time-zone math using a mix of code (light, deterministic checks) and LLM (heavier contextual checks).", "is_required": true, "max_points": 11.0, "min_score_to_pass": 6.0, "rules": [{"type": "code", "name": "Column Structure + Link Presence by Day", "description": "Programmatically check that the primary output is an Excel workbook and that each day-sheet contains core columns (fuzzy-matched) plus visible URLs. Partial credit for partial coverage.", "weight": 1.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float (score) or tuple[float, str]\n    \"\"\"\n    weight = 1.0\n    out = context.get_primary_output()\n    if not out or not out.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(out.id)\n        xls = pd.ExcelFile(path)\n        sheet_names = xls.sheet_names\n        if len(sheet_names) < 4:\n            # Not enough day tabs to verify broadly\n            # Still attempt on available sheets\n            pass\n        # Helpers\n        def norm_cols(cols):\n            return [str(c).strip().lower() for c in cols]\n        # Column role synonym sets\n        role_syns = {\n            'time': {'time','start time','local time','when','timestamp'},\n            'action': {'action','activity','event','item','agenda'},\n            'location': {'location','venue','place','address','site'},\n            'provider': {'provider','vendor','contact','company','service','host'},\n            'link': {'link','url','website','web','hyperlink'},\n            'notes': {'notes','details','remarks','comments','instructions','info'}\n        }\n        # Score components aggregated across sheets\n        role_score_sum = 0.0\n        link_score_sum = 0.0\n        considered = 0\n        for s in sheet_names[:4]:  # consider first 4 tabs as the 4 days\n            try:\n                df = pd.read_excel(path, sheet_name=s)\n            except Exception:\n                continue\n            if df is None or df.shape[1] == 0:\n                continue\n            considered += 1\n            cols = norm_cols(df.columns)\n            # Map roles to presence\n            roles_present = 0\n            for role, syns in role_syns.items():\n                if any(any(sc in c for sc in syns) for c in cols):\n                    roles_present += 1\n            # Normalize by required roles (6)\n            role_score = roles_present / 6.0\n            role_score_sum += role_score\n            # Link presence: count cells with http(s)\n            link_count = 0\n            try:\n                sample_df = df.astype(str).applymap(lambda x: x.strip())\n                link_count = int(sample_df.applymap(lambda x: bool(re.search(r\"https?://\", x, re.I))).sum().sum())\n            except Exception:\n                link_count = 0\n            # Heuristic: at least 2 links per sheet for minimal credit, >=5 for full per-sheet link credit\n            if link_count >= 5:\n                link_score_sum += 1.0\n            elif link_count >= 2:\n                link_score_sum += 0.6\n            elif link_count >= 1:\n                link_score_sum += 0.3\n            else:\n                link_score_sum += 0.0\n        if considered == 0:\n            return 0.0\n        # Average across considered sheets\n        avg_role = role_score_sum / considered\n        avg_link = link_score_sum / considered  # already per-sheet in [0,1]\n        # Combine: 60% column roles + 40% link presence\n        combined = 0.6 * avg_role + 0.4 * avg_link\n        combined = max(0.0, min(1.0, combined))\n        return combined * weight\n    except Exception:\n        return 0.0\n"}, {"type": "code", "name": "Key Entities Present and Linked", "description": "Checks that key named entities (venues/providers) are present with associated URLs somewhere in the workbook. Accepts high-quality alternatives where noted (e.g., if Oguz or Maserto not verifiable, a comparable tour guide or tuxedo provider row with link suffices). Also checks mention of reservation name 'Smith'.", "weight": 1.0, "code": "import re\nimport pandas as pd\n\nEXPECTED = [\n    {\"label\": \"Four Seasons Bosphorus\", \"aliases\": [\"four seasons bosphorus\"], \"fallback\": None},\n    {\"label\": \"Yali\", \"aliases\": [\"yali\"], \"fallback\": None},\n    {\"label\": \"Oguz (Tour Guide)\", \"aliases\": [\"oguz\",\"o\u011fuz\",\"private tour guide\"], \"fallback\": \"tour guide\"},\n    {\"label\": \"Hidden Garden\", \"aliases\": [\"hidden garden\"], \"fallback\": None},\n    {\"label\": \"Garden 1897\", \"aliases\": [\"garden 1897\"], \"fallback\": None},\n    {\"label\": \"Maserto (Tuxedo)\", \"aliases\": [\"maserto\"], \"fallback\": \"tuxedo\"},\n    {\"label\": \"Four Seasons GM\", \"aliases\": [\"general manager\",\"gm\"], \"fallback\": None},\n    {\"label\": \"Samira Lowell\", \"aliases\": [\"samira lowell\"], \"fallback\": None},\n    {\"label\": \"Bespoke Tuxedo\", \"aliases\": [\"bespoke tuxedo\"], \"fallback\": None},\n    {\"label\": \"Adile Sultan Palace\", \"aliases\": [\"adile sultan palace\"], \"fallback\": None},\n    {\"label\": \"JVY Airport\", \"aliases\": [\"jvy\"], \"fallback\": None},\n    {\"label\": \"ISL (Istanbul Airport)\", \"aliases\": [\"isl\",\"istanbul airport\"], \"fallback\": None},\n]\n\nURL_RE = re.compile(r\"https?://\", re.I)\n\ndef evaluate(workflow, context):\n    weight = 1.0\n    out = context.get_primary_output()\n    if not out or not out.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(out.id)\n        xls = pd.ExcelFile(path)\n        sheets = xls.sheet_names\n        # Gather all rows as records with concatenated text and per-row url presence\n        rows = []\n        for s in sheets:\n            try:\n                df = pd.read_excel(path, sheet_name=s)\n            except Exception:\n                continue\n            if df is None or df.shape[0] == 0:\n                continue\n            sdf = df.astype(str).fillna(\"\")\n            for _, r in sdf.iterrows():\n                cells = [str(v) for v in r.values]\n                joined = \" | \".join(cells).lower()\n                has_url = any(URL_RE.search(v) for v in cells)\n                rows.append((joined, has_url))\n        if not rows:\n            return 0.0\n        # Helper to check presence with link on same row\n        def present_with_link(aliases, fallback=None):\n            # direct match\n            for txt, has_url in rows:\n                if any(a in txt for a in aliases):\n                    return True if has_url else False\n            # fallback concept (e.g., generic 'tour guide' or 'tuxedo')\n            if fallback:\n                for txt, has_url in rows:\n                    if fallback in txt and has_url:\n                        return True\n            return False\n        satisfied = 0\n        for item in EXPECTED:\n            if present_with_link(item[\"aliases\"], item[\"fallback\"]):\n                satisfied += 1\n        total = len(EXPECTED)\n        # Bonus: mention of reservation under 'Smith'\n        smith_found = any('smith' in txt for txt, _ in rows)\n        ratio = satisfied / total if total else 0.0\n        # Add small bonus for 'smith' mention\n        score = min(1.0, ratio + (0.05 if smith_found else 0.0))\n        return score * weight\n    except Exception:\n        return 0.0\n"}, {"type": "llm_judge", "name": "Event Coverage & Sequencing per Brief", "description": "LLM verifies that all required events appear on the appropriate days, with separate actions where specified (e.g., pickup vs dinner), correct venues, and reasonable sequencing.", "weight": 3.0, "judge_prompt": "Evaluate whether the Excel workbook covers ALL required events in the correct day tabs with sensible sequencing. Check for the following specifics:\n\nDay 1 (June 1):\n- 8:00 AM pickup at main house front door.\n- 9:00 AM wheels up from JVY Airport.\n- Flight detail: 10h duration + 8h forward (notate technical duration ~18h in details).\n\nDay 2 (arrival ~3:00 AM local):\n- 3:00 AM wheels down at ISL (Istanbul Airport).\n- SUV pickup, hotel drop at Four Seasons Bosphorus by ~4:30 AM.\n- 9:00 AM breakfast at Yali (reservation under Smith).\n- 11:00 AM private tour with Oguz (include guide link or comparable alt noted).\n- 2:00 PM lunch at Hidden Garden, continue tour until 5:00 PM.\n- 5:00 PM return to hotel; meeting with Four Seasons Bosphorus General Manager (include GM link).\n- 7:00 PM Maserto tuxedo drop to room (include link or comparable alt).\n- 8:30 PM SUV pickup to dinner at Garden 1897; 9:00 PM reservation under Smith. Separate pickup and dinner actions. SUV stages during dinner. ~10:30 PM depart back to hotel.\n\nDay 3 (June 3 \u2013 Wedding day):\n- 9:00 AM breakfast at Yali.\n- 11:00 AM hair & makeup in guest room by Samira Lowell team (include link).\n- 1:00 PM Bespoke Tuxedo final fitting in master bedroom.\n- 2:00 PM in-room dining lunch (delivered to hotel room kitchen).\n- 4:00 PM SUV at lobby to Adile Sultan Palace (link), drop main entrance; SUV stages until ~10:30 PM return.\n\nDay 4 (June 4 \u2013 Return):\n- 8:00 AM pickup from hotel to ISL; plane-side drop.\n- 9:00 AM wheels up to JVY.\n- 11:00 AM landing at JVY (note 10h flight and gaining 8h in details).\n- 11:00 AM plane-side SUV pickup; 11:30 AM drop at main house.\n\nScoring (0\u20133):\n- 3: All events above appear on correct day tabs, with separate pickup/dinner actions on Day 2, staging notes where required, and sequences/time flow are sensible.\n- 2: Minor omissions (1\u20132 small items) or slight sequencing issues, but overall coverage is strong.\n- 1: Multiple omissions (3\u20135 items) or confusing sequencing that hinders execution.\n- 0: Major gaps or events missing across multiple days.\nProvide brief justification.", "expectation": "All required actions present by day with sensible sequencing; explicit separate pickup vs dinner entries; staging instructions included."}, {"type": "llm_judge", "name": "Link Relevance and Verifiability", "description": "LLM inspects that links exist for each named venue/provider and appear to be official or high-quality sources. If an entity cannot be verified, a comparable alternative is provided and clearly labeled.", "weight": 3.0, "judge_prompt": "Evaluate link coverage and plausibility. For each of the following, confirm a link is present and appears relevant/high quality (official site, Google Maps, reputable directory, or LinkedIn for people): Four Seasons Bosphorus (hotel), Yali (restaurant), Oguz (tour guide) or a clearly labeled high-quality alternative, Hidden Garden (restaurant), Garden 1897 (restaurant), Four Seasons Bosphorus General Manager (LinkedIn or official bio), Maserto (tuxedo) or comparable alt, Samira Lowell (hair/makeup), Bespoke Tuxedo (fitting), Adile Sultan Palace (venue), JVY Airport, and ISL (Istanbul Airport). Also confirm that links are formatted as clickable hyperlinks in the sheet.\n\nIf any named individual/company truly cannot be verified, confirm the sheet clearly provides a comparable alternative with rationale/notation.\n\nScoring (0\u20133):\n- 3: All named items covered with plausible links; alternatives clearly labeled where applicable; links appear clickable.\n- 2: 1\u20132 link gaps or questionable sources; most others are solid.\n- 1: 3\u20135 gaps, or many links appear low-quality or non-clickable.\n- 0: Few or no relevant links, or links are largely missing.\nProvide brief justification.", "expectation": "All providers/venues have plausible hyperlinks; clearly labeled alternatives where necessary."}, {"type": "llm_judge", "name": "Time Math and Time-Zone Consistency", "description": "LLM validates that flight durations and time-zone shifts are reflected correctly across days and annotations, and that day boundaries make sense.", "weight": 3.0, "judge_prompt": "Check time math and time-zone logic:\n- Day 1: 9:00 AM JVY departure + 10h flight + +8h time shift should yield ~3:00 AM next-day arrival at ISL; confirm Day 2 shows ~3:00 AM arrival.\n- Day 2: Allow ~4:30 AM hotel drop as shown; subsequent times are local to Istanbul and make sense.\n- Day 4: 9:00 AM ISL departure to JVY with ~10h flight and -8h shift gives ~11:00 AM JVY landing local time; confirm workbook notes this correctly.\n- Within each day, times should progress sensibly; buffers around key events (pickups, dinner, wedding) should be reasonable.\n\nScoring (0\u20133):\n- 3: Time math is correct and consistently annotated; sequences are realistic.\n- 2: Minor discrepancies or unclear annotations, but overall consistent.\n- 1: Several inconsistencies that could confuse execution.\n- 0: Major time math errors or day-boundary mistakes.\nProvide brief justification.", "expectation": "Accurate 10h flight + 8h/-8h shift handling; coherent daily time sequences."}], "on_failure_action": "skip_remaining", "on_failure_score": 0.0}, {"name": "Stage 3: Quality and Executive Polish", "description": "Holistic LLM assessment of presentation quality, white-glove concierge touches, and actionability for an ultra-high-net-worth principal.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Presentation & Readability", "description": "Evaluate formatting for executive readability: clear headers, consistent columns, time format, frozen header row, legible fonts, spacing, and clean layout that is easy to scan on laptop/mobile.", "weight": 2.0, "judge_prompt": "Assess the workbook\u2019s professional presentation:\n- Clear, consistent headers on all tabs; logical column order; frozen header rows if visible.\n- Time formatting consistent (e.g., 8:00 AM, 09:00); alignment and spacing make it easy to scan.\n- Hyperlinks formatted cleanly (not raw pasted text if possible).\n- Minimal clutter; no broken or orphaned rows.\nScoring (0\u20132): 2 = polished executive look; 1 = adequate but some inconsistencies; 0 = messy or hard to read.", "expectation": "Clean, consistent, executive-ready formatting across all tabs."}, {"type": "llm_judge", "name": "White-Glove Concierge Touches", "description": "Assesses proactive concierge excellence: buffers, staging instructions, fallback options, VIP/security considerations, cultural etiquette, and personalized notes for UHNW expectations.", "weight": 2.0, "judge_prompt": "Evaluate whether the itinerary shows white-glove detail:\n- Explicit staging/waiting notes for SUVs; buffers before/after key events.\n- Backup options (alt restaurants, tour contacts, second vehicle, or contingency if delays occur).\n- VIP/security considerations (plane-side details, discretion, contact points) appropriately noted.\n- Cultural etiquette and dress notes where relevant; tipping guidance when appropriate.\n- Personalized touches for the principal (e.g., room preferences, quiet cars, privacy reminders).\nScoring (0\u20132): 2 = strong proactive concierge detail; 1 = some touches; 0 = minimal/none.", "expectation": "Proactive, anticipatory details throughout, suitable for UHNW service level."}, {"type": "llm_judge", "name": "Actionability & Clarity for Execution", "description": "Judges whether another staff member could run the itinerary flawlessly: clear who/what/where/when, contacts, confirmation names/numbers, and explicit instructions.", "weight": 2.0, "judge_prompt": "Assess execution clarity:\n- Each row/action clearly indicates who, what, where (exact lobby/main entrance/room), and when (local time noted).\n- Provider contacts (names, phone, email) and confirmation identifiers (e.g., reservation under Smith) are present where appropriate.\n- Directions and pickup/drop details are explicit (e.g., plane-side, hotel lobby, master bedroom, guest room).\nScoring (0\u20132): 2 = fully actionable; 1 = generally clear but some gaps; 0 = lacks needed detail.", "expectation": "Clear, specific instructions and contacts so the team can execute without ambiguity."}, {"type": "llm_judge", "name": "Strategic Value & Extras", "description": "Evaluates whether the itinerary adds strategic value: high-value introductions, optional add-ons, useful notes (weather, traffic), and a thoughtful balance of pace/rest.", "weight": 2.0, "judge_prompt": "Consider strategic enhancements:\n- Inclusion of high-value individuals to connect with in Istanbul (e.g., industry contacts or local leaders) with context and links.\n- Optional experiences or add-ons (boat rides on the Bosphorus, museum options) time-permitting.\n- Practical notes (traffic patterns, security lines, weather considerations) and sensible pacing.\nScoring (0\u20132): 2 = strong strategic additions; 1 = some; 0 = little to none.", "expectation": "Thoughtful extras and connections that enhance the trip\u2019s value beyond logistics."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "91060ff0-3eb5-4ddf-9edb-f6758b95499e", "rubric": {"category_name": "Health Education Poster \u2013 Warts (Retail Pharmacist)", "rationale": "This rubric enforces a self-documenting deliverable: a one-page 36\u00d724 inch educational PDF poster with a clear, section-based layout, verifiable structural elements (including an OTC comparison table and references), followed by correctness checks on core clinical content (HPV etiology, OTC therapies, referral red flags), and a final holistic quality appraisal for visual design, audience fit, and practical pharmacist utility. Stage 1 is a strict, LLM-only gate on document shape. Stage 2 blends light code checks (keyword/coverage) with higher-weight LLM checks for clinical accuracy and alignment with references. Stage 3 evaluates presentation quality and usability for a mixed lay/professional audience.", "max_total_score": 28.0, "stages": [{"name": "Stage 1 \u2013 Poster Format and Structural Requirements (GATE)", "description": "LLM-only gate verifying the deliverable is a properly structured, one-page poster PDF (36\u00d724 inches) with all required sections, at least one OTC comparison table, at least three visuals, and a references section. Structure only; do not judge content quality.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Poster Format and Section Structure Gate", "description": "Verify poster file type, size/orientation, and presence of all required sections, visuals, and table per instructions. Check structure only.", "weight": 8.0, "judge_prompt": "You are checking ONLY STRUCTURE and FORMAT of the submitted deliverable. Do not judge content quality or accuracy.\n\nRequirement: A one-page PDF poster sized 36 \u00d7 24 inches (landscape), designed for a mixed audience (public + healthcare professionals). Professional, section-based layout with headers, balanced text and visuals.\n\nCheck the following strictly structural elements:\n1) File/Canvas\n   - Must be a PDF (not Word, not image-only submission). One page only.\n   - Page size should be 36 \u00d7 24 inches (landscape). Allow minor print-bleed or layout margins, but it must clearly be a landscape poster roughly 36\u00d724 inches.\n\n2) Title and Intro\n   - Prominent poster title related to warts.\n   - Subtitle or brief introductory blurb setting context.\n\n3) Required Section Headers (flexible naming as long as intent is clear):\n   - What warts are / How they develop\n   - Causes (HPV) and contributing factors\n   - Signs and symptoms\n   - Goals of treatment\n   - When to refer (red flags)\n   - Pharmacological treatments (focus on OTC in community pharmacy)\n   - Non-pharmacological / Prevention strategies\n   - When to follow up\n\n4) OTC Product Comparison Table (required)\n   - A clearly labeled table focused on OTC options, with multiple columns. Flexible column names, but must include at least: Active Ingredient, Strength/Concentration, Form, How it works/Mechanism, How to use/Duration, Key Warnings/Contraindications, and Age/Who it\u2019s for or Counseling points.\n\n5) Visuals (required)\n   - At least three supportive visuals (e.g., icons, table(s), comparison chart, simple flowchart/decision box for referral/follow-up, product images or schematic). They must be visibly distinct graphical elements, not just text.\n\n6) References (required)\n   - A References or Sources section with at least 3 citations (e.g., textbooks, peer-reviewed articles, reputable OTC product sites). URLs are acceptable if readable.\n\n7) Contact/Attribution (optional)\n   - Pharmacist name/affiliation/contact is optional (not required for full credit).\n\nSCORING for this gate (STRUCTURE ONLY):\n- 8: PDF one-page poster ~36\u00d724 in landscape AND all required sections present AND OTC comparison table present AND at least 3 visuals AND References section (\u22653 citations).\n- 7: Meets format + nearly complete, but missing one minor element (e.g., only 2 visuals OR table columns are slightly incomplete but still clearly an OTC comparison table).\n- 5: Valid PDF poster but missing 1 required section OR references fewer than 3 OR OTC table clearly present but missing multiple key columns.\n- 3: Valid PDF but poster lacks multiple required sections OR visuals mostly missing (\u22641) OR no OTC table at all.\n- 0: Not a PDF, not a poster format, wrong size/orientation (clearly not ~36\u00d724), or fundamentally unstructured.\n\nBe flexible with exact section names; judge by intent. Again, evaluate format and structure only\u2014do not assess medical accuracy.", "expectation": "A one-page, landscape 36\u00d724 inch PDF poster with all required sections, a clearly labeled OTC comparison table, \u22653 visuals, and \u22653 references."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Clinical Correctness and Internal Consistency", "description": "Now that structure is verified, evaluate correctness and consistency of clinical content, with light code checks and higher-weight LLM judgment. Focus on HPV etiology, OTC therapies (salicylic acid, cryo), safety/contraindications, referral red flags, and alignment with references.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "HPV and Definition Presence Check", "description": "Verify text mentions HPV (human papillomavirus) and defines/frames warts appropriately. Keyword coverage check using extracted PDF text.", "weight": 0.6, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_document:\\n            return 0.0, 'No document output.'\\n        try:\\n            text = context.files.read_pdf_text(output.id) or ''\\n        except Exception as e:\\n            return 0.0, f'Failed to read PDF text: {e}'\\n        txt = text.lower()\\n        # Basic concepts to verify presence\\n        hits = 0\\n        total_checks = 3\\n        if ('hpv' in txt) or ('human papillomavirus' in txt) or ('human papilloma virus' in txt):\\n            hits += 1\\n        if ('wart' in txt) or ('verruca' in txt):\\n            hits += 1\\n        if ('virus' in txt) or ('viral' in txt):\\n            hits += 1\\n        score = (hits/total_checks) * 0.6\\n        return score, f'HPV/definition hits: {hits}/{total_checks}'\\n    except Exception as e:\\n        return 0.0, f'Rule error: {e}'"}, {"type": "code", "name": "OTC Treatments and Dosing Specifics Coverage", "description": "Check presence of key OTC therapy concepts (salicylic acid, cryotherapy/DMEP) and practical usage cues (daily use, weeks of therapy, percent strengths).", "weight": 0.6, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_document:\\n            return 0.0, 'No document output.'\\n        try:\\n            text = context.files.read_pdf_text(output.id) or ''\\n        except Exception as e:\\n            return 0.0, f'Failed to read PDF text: {e}'\\n        t = text.lower()\\n        checks = [\\n            any(k in t for k in ['salicylic acid','salicylate']),\\n            any(k in t for k in ['cryotherapy','dimethyl ether','propane','dmep','freeze'] ),\\n            any(k in t for k in ['17%','17 %','40%','40 %']),\\n            any(k in t for k in ['daily','once daily','every day','nightly']),\\n            any(k in t for k in ['week','weeks'])\\n        ]\\n        hits = sum(1 for c in checks if c)\\n        score = (hits/len(checks)) * 0.6\\n        return score, f'OTC/dosing hits: {hits}/{len(checks)}'\\n    except Exception as e:\\n        return 0.0, f'Rule error: {e}'"}, {"type": "code", "name": "Referral Red Flags Keyword Coverage", "description": "Check presence of common referral criteria keywords (e.g., face/genital area, diabetes/poor circulation, immunocompromised, bleeding/pain, uncertain diagnosis, very young age, pregnancy).", "weight": 0.6, "code": "import re\\n\\nRED_FLAG_TERMS = [\\n    'face', 'facial', 'eye', 'eyelid', 'lips', 'genital', 'mucosal',\\n    'diabetes', 'diabetic', 'poor circulation', 'peripheral vascular',\\n    'immunocompromised', 'immunosuppressed', 'chemotherapy',\\n    'painful', 'bleeding', 'infected', 'spreading',\\n    'pregnant', 'pregnancy', 'breastfeeding',\\n    'uncertain diagnosis', 'unsure diagnosis', 'mole', 'birthmark',\\n    'young children', 'children under', 'under 4', 'under four', 'infant'\\n]\\n\\ndef evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_document:\\n            return 0.0, 'No document output.'\\n        try:\\n            text = context.files.read_pdf_text(output.id) or ''\\n        except Exception as e:\\n            return 0.0, f'Failed to read PDF text: {e}'\\n        t = text.lower()\\n        found = set([term for term in RED_FLAG_TERMS if term in t])\\n        # Reward broader coverage; aim for at least 4+ unique concepts\\n        unique_hits = len(found)\\n        if unique_hits >= 8:\\n            score = 0.6\\n        elif unique_hits >= 6:\\n            score = 0.45\\n        elif unique_hits >= 4:\\n            score = 0.3\\n        elif unique_hits >= 2:\\n            score = 0.15\\n        else:\\n            score = 0.0\\n        return score, f'Red flag terms found: {unique_hits} ({sorted(list(found))[:10]})'\\n    except Exception as e:\\n        return 0.0, f'Rule error: {e}'"}, {"type": "llm_judge", "name": "Clinical Accuracy and Consistency", "description": "Judge whether clinical statements are accurate and internally consistent: HPV etiology; appropriate OTC recommendations (salicylic acid concentrations; cryotherapy mechanisms/limitations); proper cautions for special sites/patient groups; realistic expectations (time to improvement, recurrence, non-guaranteed cure).", "weight": 3.4, "judge_prompt": "Evaluate clinical correctness and internal consistency of the poster. Focus on: (a) correct viral etiology (HPV), (b) accurate characterization of common wart types/signs, (c) OTC therapy accuracy: typical salicylic acid ranges (e.g., 17\u201340%), keratolytic mechanism, expected duration (weeks), application frequency; (d) cryotherapy described accurately for OTC products (DMEP/propellant devices; not equivalent to liquid nitrogen in-office), (e) safety and contraindications (avoid face/genitals/mucosal surfaces; caution or referral for diabetes/poor circulation/immunocompromised/pregnancy/young children; avoid treating moles/birthmarks), and (f) realistic outcomes (may require multiple treatments; recurrence possible). Penalize clear inaccuracies or unsafe advice. Score 0\u20133.4 proportionally to correctness and consistency. Provide brief justification.", "expectation": "Content is clinically sound, consistent, and free of unsafe or misleading advice."}, {"type": "llm_judge", "name": "Evidence Support and Citations Alignment", "description": "Check that references are appropriate (textbooks, peer-reviewed articles, reputable OTC sites) and that key claims are plausibly supported by cited sources.", "weight": 3.4, "judge_prompt": "Assess the REFERENCES section for quality and alignment. Are there \u22653 legitimate sources (textbooks, peer-reviewed articles, reputable OTC product sites, or authoritative clinical guidelines)? Do key factual claims about etiology, OTC treatments, cautions, and follow-up reasonably map to these sources (via in-text cues or proximity)? Deduct if references are clearly missing, non-credible, or unrelated to claims. Do not require formal citation style; readability is sufficient. Score 0\u20133.4 with a one-sentence rationale.", "expectation": "At least three credible sources that plausibly underpin the poster\u2019s key clinical statements."}, {"type": "llm_judge", "name": "OTC Comparison Table Plausibility", "description": "Evaluate whether the OTC comparison table contains appropriate columns and plausible, non-misleading entries for common OTC wart treatments.", "weight": 3.4, "judge_prompt": "Examine the OTC comparison table. Does it include multiple relevant columns (e.g., active ingredient, strength, form, mechanism, usage/duration, warnings, counseling/age suitability) and entries that plausibly match real OTC products or generics (e.g., salicylic acid 17\u201340% liquids/patches; OTC freeze devices using DMEP)? Are mechanisms and directions reasonable and not harmful? Score 0\u20133.4 based on completeness and plausibility; cite any glaring errors briefly.", "expectation": "A clear, accurate comparison table with sensible columns and safe, plausible entries."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Presentation Quality and Practical Utility", "description": "Holistic quality assessment of visual design, readability, mixed-audience suitability, and practical pharmacist value. Not about structure or raw factual correctness, but about professionalism and usability.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Visual Design and Readability", "description": "Judge visual hierarchy, typography, spacing, and scanability at poster distance. Check that text density is appropriate and sections are easy to navigate.", "weight": 2.0, "judge_prompt": "Evaluate visual design quality: clear title hierarchy, consistent headers, adequate whitespace, readable fonts for a poster viewed from ~3\u20134 feet, color/contrast accessibility, and logical section layout. Penalize cluttered walls of text, inconsistent styles, or hard-to-read color choices. Score 0\u20132.", "expectation": "Professional, readable, and well-organized poster with clear visual hierarchy."}, {"type": "llm_judge", "name": "Audience Appropriateness and Tone", "description": "Assess whether the poster fits both lay audience and healthcare professionals with approachable yet professional tone.", "weight": 2.0, "judge_prompt": "Does the poster balance plain-language explanations for the public with adequate specificity for healthcare professionals? Look for minimal unexplained jargon, short definitions where needed, and professional yet approachable tone. Score 0\u20132 with a brief note.", "expectation": "Accessible language with sufficient clinical precision for a mixed audience."}, {"type": "llm_judge", "name": "Clarity and Self-Guided Flow", "description": "Determine if the poster can stand alone without the presenter, with clear navigation and self-guided learning aids (e.g., icons, flowchart for referral/follow-up).", "weight": 2.0, "judge_prompt": "Assess whether a reader can follow the poster unaided: clear sectioning, numbering or visual anchors, concise bullets, and supportive visuals (e.g., decision flow for referral/follow-up). Penalize if the reader would struggle to know what to do or where to look next. Score 0\u20132.", "expectation": "Logical, self-guided flow with clear calls to action and supportive visuals."}, {"type": "llm_judge", "name": "Pharmacist Role and Practical Utility", "description": "Evaluate how well the poster highlights the pharmacist\u2019s frontline role and gives actionable, real-world guidance.", "weight": 2.0, "judge_prompt": "Does the poster emphasize the pharmacist\u2019s role (counseling points, product selection guidance, red flags, when to refer/follow up) and deliver actionable steps (how to use OTC products, expected timelines, safety notes)? Score 0\u20132 with brief justification.", "expectation": "Practical, pharmacist-centered guidance that a reader can apply immediately."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "9e39df84-ac57-4c9b-a2e3-12b8abf2c797", "rubric": {"category_name": "Manufacturing \u2014 Operator/Machine Output Dashboard (Excel)", "rationale": "Task Type: Analytical (Pattern A). Output Format: Excel workbook with structured data, pivots, charts. The rubric enforces a self-documenting shape first (LLM-only Stage 1 gate), then verifies correctness with a mix of light-weight code checks and LLM cross-referencing (Stage 2), and finally assesses presentation and usability quality (Stage 3). Code rules are intentionally lower weight than LLM rules (~5x less on average) and focus on deterministic validations enabled by the mandated structure.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate", "description": "LLM-only verification that the submission is an Excel workbook titled/representing \u201cDashboard Output\u201d with exactly the required sheets and structural elements that make later verification trivial.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.1, "rules": [{"type": "llm_judge", "name": "Sheet: Operator Output Data \u2014 Structural Requirements", "description": "Verify the workbook is an Excel file and contains a worksheet named exactly \u201cOperator Output Data\u201d with the mandated columns and 48-week structure.", "weight": 1.5, "judge_prompt": "You are validating the SHAPE ONLY, not calculation correctness.\n\nCheck the candidate output (Excel workbook) for the following structure:\n\nA) File format\n- Must be an Excel workbook (.xlsx or .xlsm accepted).\n- Workbook title/name should reflect \u201cDashboard Output\u201d (exact file name preferred but allow minor variations).\n\nB) Worksheet: \u201cOperator Output Data\u201d\n- A clearly structured tabular dataset with column headers including ALL of the following (be flexible with minor naming variations like spaces/case):\n  \u2022 Week # (values 1\u201348 present in the table, one row per Operator per Week)\n  \u2022 Operator (Operators 1 through Operator 9)\n  \u2022 Machine Line (e.g., Machine 1, Machine 2, Machine 3)\n  \u2022 Shift (Day or Night)\n  \u2022 Daily Output columns for Monday, Tuesday, Wednesday, Thursday, Friday\n  \u2022 Average Output (marked or placed as an automatically-calculated weekly average column)\n  \u2022 Total Output (marked or placed as an automatically-calculated weekly total column)\n- The table should include rows for Weeks 1\u201348 and 9 operators per week (structure present). Week 1 entries should be populated.\n- Each operator appears consistently assigned to one machine and one shift across all weeks (structure/pattern visible in the dataset).\n\nDo NOT verify math or conditional formatting quality here (that will be checked later). Only confirm presence/structure enabling verification.\n\nScoring:\n- 1.5: Workbook is Excel + sheet \u201cOperator Output Data\u201d present + all required columns present + 48-week x 9-operator structure visually present + Week 1 populated + consistent assignment columns visible.\n- 1.0: Excel + sheet present + most required columns present (minor variations ok) + weeks structure present but with small gaps OR unclear operator consistency.\n- 0.5: Excel + sheet present but multiple required columns/structure elements missing (e.g., missing several daily columns or no Average/Total columns).\n- 0.0: Not an Excel workbook OR required sheet missing entirely.", "expectation": "A well-structured Excel table with the specified columns and 48-week design, Week 1 populated, enabling deterministic validation later."}, {"type": "llm_judge", "name": "Sheet: Dashboard \u2014 Structural Requirements", "description": "Verify the \u201cDashboard\u201d worksheet exists and contains the required pivot tables, week selection controls, charts, and KPI summary table (shape only).", "weight": 1.5, "judge_prompt": "You are validating SHAPE ONLY for the \u201cDashboard\u201d sheet (no math accuracy yet).\n\nCheck the candidate Excel for:\n\nA) Worksheet: \u201cDashboard\u201d present\n\nB) Pivot and Controls Structure\n- PivotTables or equivalent summaries to show, for a selected week or range of weeks:\n  (a) operator performance/output,\n  (b) total machine output,\n  (c) average day/night shift output,\n  (d) a YTD operator leaderboard (total output per operator).\n- A data validation list (dropdown) or equivalent control enabling selection of a specific week or a range of weeks that the pivots respond to.\n\nC) Charts (arranged in four quadrants) based on Week 1 data:\n- Bar chart: individual operator total output for the week.\n- Pie chart: each machine\u2019s total output for the week.\n- Pie chart: average output by shift (day vs. night) for the week.\n- Bar chart: YTD total output per operator.\n\nD) KPI Summary Table for Week 1 containing at least:\n- Total units produced\n- Top performing operator and machine (with output totals)\n- Average output per operator (units)\n- Day shift contribution % of total\n- Night shift contribution % of total\n\nScoring:\n- 1.5: Dashboard sheet present with all pivots, week selection control, all four charts in quadrant layout, and KPI summary table with all listed fields.\n- 1.0: Dashboard present with most elements (e.g., pivots + charts) but missing either the week selection control or 1 KPI field.\n- 0.5: Dashboard present but missing multiple required elements (e.g., only charts but no pivots/controls, or no KPI table).\n- 0.0: No Dashboard sheet or structure is largely absent.", "expectation": "A single dashboard sheet with pivots tied to a week selection, four quadrant charts, and a KPI table scaffold for Week 1."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Functional Verification", "description": "Mixed code and LLM checks to validate data completeness/consistency, calculation correctness, interactivity (pivots and filters), chart fidelity, conditional formatting presence, and KPI accuracy for Week 1.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Data Completeness, Consistency, and Calculations (Week Structure + Totals/Averages)", "description": "Verify presence of required columns (flexible matching), weeks coverage (1\u201348), per-operator consistency of machine/shift, daily outputs numeric/non-negative, and that Total/Average match sums/means of Mon\u2013Fri for populated rows.", "weight": 1.2, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float or (float, str)\n    \"\"\"\n    weight = 1.2\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheet_name = None\n        for s in xls.sheet_names:\n            sl = s.strip().lower()\n            if sl == 'operator output data' or sl == 'operator_output_data':\n                sheet_name = s\n                break\n        if sheet_name is None:\n            # fuzzy fallback\n            candidates = []\n            for s in xls.sheet_names:\n                sl = s.lower()\n                if 'operator' in sl and 'output' in sl:\n                    candidates.append(s)\n            sheet_name = candidates[0] if candidates else xls.sheet_names[0]\n        df = pd.read_excel(path, sheet_name=sheet_name)\n        if df.empty:\n            return 0.0, \"Operator Output Data sheet appears empty\"\n        # Normalize columns\n        def norm(c):\n            return re.sub(r'[^a-z0-9]+', '', str(c).strip().lower())\n        colmap = {c: norm(c) for c in df.columns}\n        inv = {}\n        for k, v in colmap.items():\n            inv.setdefault(v, k)\n        # Identify key columns flexibly\n        week_col = None\n        operator_col = None\n        machine_col = None\n        shift_col = None\n        total_col = None\n        avg_col = None\n        day_cols = []\n        for c in df.columns:\n            nc = colmap[c]\n            if week_col is None and 'week' in nc:\n                week_col = c\n            if operator_col is None and 'operator' in nc:\n                operator_col = c\n            if machine_col is None and (('machineline' in nc) or ('machine' in nc) or ('line' in nc)):\n                machine_col = c\n            if shift_col is None and 'shift' in nc:\n                shift_col = c\n            if total_col is None and (('totaloutput' in nc) or ('total' in nc and 'output' in nc)):\n                total_col = c\n            if avg_col is None and (('averageoutput' in nc) or ('avgoutput' in nc) or ('average' in nc and 'output' in nc)):\n                avg_col = c\n            # day columns\n            if any(nc.startswith(p) for p in ['mon','monday','tue','tuesday','wed','wednesday','thu','thur','thurs','thursday','fri','friday']):\n                day_cols.append(c)\n        # Minimal required columns\n        required_ok = all([week_col, operator_col, machine_col, shift_col]) and len(day_cols) >= 5\n        if not required_ok:\n            return 0.2 * weight, \"Key columns missing or insufficient daily columns\"\n        # Clean and coerce week\n        wk = df[week_col]\n        # Extract numeric week if needed\n        def to_int_safe(x):\n            try:\n                if pd.isna(x):\n                    return np.nan\n                if isinstance(x, (int, np.integer)):\n                    return int(x)\n                if isinstance(x, float):\n                    return int(x)\n                m = re.search(r\"\\d+\", str(x))\n                return int(m.group(0)) if m else np.nan\n            except Exception:\n                return np.nan\n        df['_week_num_'] = wk.apply(to_int_safe)\n        # Coerce daily outputs\n        for c in day_cols:\n            df[c] = pd.to_numeric(df[c], errors='coerce')\n        # Scores aggregation\n        score = 0.0\n        feedback = []\n        # 1) Weeks coverage & structure: target weeks 1..48 present\n        weeks_present = set(int(x) for x in df['_week_num_'].dropna().unique().tolist())\n        coverage = len(weeks_present.intersection(set(range(1,49)))) / 48.0\n        s_coverage = 0.5 * weight * min(1.0, coverage)\n        score += s_coverage\n        feedback.append(f\"Weeks coverage: {coverage:.2%}\")\n        # 2) Operator consistency of machine/shift across weeks\n        s_consistency = 0.0\n        if operator_col is not None:\n            ok_count = 0\n            total_ops = 0\n            for op, g in df.dropna(subset=[operator_col]).groupby(operator_col):\n                total_ops += 1\n                mc_uniq = g[machine_col].dropna().astype(str).str.strip().str.lower().nunique() if machine_col else 1\n                sh_uniq = g[shift_col].dropna().astype(str).str.strip().str.lower().nunique() if shift_col else 1\n                if mc_uniq <= 1 and sh_uniq <= 1:\n                    ok_count += 1\n            if total_ops > 0:\n                s_consistency = 0.25 * weight * (ok_count / total_ops)\n        score += s_consistency\n        feedback.append(\"Operator machine/shift consistency OK ratio: %.2f\" % ((ok_count / total_ops) if ('ok_count' in locals() and total_ops>0) else 0))\n        # 3) Calculations: recompute totals/averages where daily data present\n        s_calc = 0.0\n        try:\n            # consider rows with at least one daily value present\n            day_mat = df[day_cols]\n            has_any = day_mat.notna().any(axis=1)\n            calc_total = day_mat.sum(axis=1)\n            calc_avg = day_mat.mean(axis=1)\n            good_total = 1.0\n            good_avg = 1.0\n            if total_col is not None:\n                diff_t = (df[total_col] - calc_total).abs()\n                tol_t = 0.01\n                good_total = (diff_t[has_any] <= tol_t).mean() if has_any.any() else 1.0\n            if avg_col is not None:\n                diff_a = (df[avg_col] - calc_avg).abs()\n                tol_a = 0.01\n                good_avg = (diff_a[has_any] <= tol_a).mean() if has_any.any() else 1.0\n            s_calc = 0.30 * weight * ((good_total + good_avg) / 2.0)\n        except Exception:\n            s_calc = 0.05 * weight\n        score += s_calc\n        feedback.append(f\"Calc checks: total match ratio ~ {good_total:.2f}, avg match ratio ~ {good_avg:.2f}\")\n        # 4) Daily numeric & non-negative sanity\n        s_values = 0.0\n        try:\n            vals = day_mat.values.flatten()\n            vals = vals[~pd.isna(vals)]\n            if vals.size == 0:\n                s_values = 0.05 * weight\n            else:\n                nonneg_ratio = (vals >= 0).mean()\n                s_values = 0.15 * weight * nonneg_ratio\n                feedback.append(f\"Non-negative ratio of daily values: {nonneg_ratio:.2f}\")\n        except Exception:\n            s_values = 0.05 * weight\n        score += s_values\n        # Cap to weight\n        score = float(max(0.0, min(score, weight)))\n        return score, \"; \".join(feedback)\n    except Exception as e:\n        return 0.0, f\"Error reading spreadsheet: {e}\""}, {"type": "code", "name": "Domain Structure: Machine Lines, Shifts, and Week 1 Coverage", "description": "Verify machine lines cover exactly 1\u20133, shifts are Day/Night, and Week 1 has 9 operator rows with positive totals.", "weight": 0.8, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    weight = 0.8\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        # Find operator data sheet\n        sheet_name = None\n        for s in xls.sheet_names:\n            if s.strip().lower() == 'operator output data':\n                sheet_name = s\n                break\n        if sheet_name is None:\n            # fuzzy\n            candidates = [s for s in xls.sheet_names if ('operator' in s.lower() and 'output' in s.lower())]\n            sheet_name = candidates[0] if candidates else xls.sheet_names[0]\n        df = pd.read_excel(path, sheet_name=sheet_name)\n        if df.empty:\n            return 0.0, \"Operator Output Data sheet empty\"\n        # Normalize\n        def norm(c):\n            return re.sub(r'[^a-z0-9]+', '', str(c).strip().lower())\n        colmap = {c: norm(c) for c in df.columns}\n        machine_col = next((c for c in df.columns if ('machine' in colmap[c] or 'line' in colmap[c])), None)\n        shift_col = next((c for c in df.columns if 'shift' in colmap[c]), None)\n        week_col = next((c for c in df.columns if 'week' in colmap[c]), None)\n        # Identify daily columns and total\n        day_cols = [c for c in df.columns if any(colmap[c].startswith(p) for p in ['mon','monday','tue','tuesday','wed','wednesday','thu','thur','thurs','thursday','fri','friday'])]\n        total_col = next((c for c in df.columns if ('totaloutput' in colmap[c] or ('total' in colmap[c] and 'output' in colmap[c]))), None)\n        if machine_col is None or shift_col is None or week_col is None or total_col is None or len(day_cols)<5:\n            return 0.2 * weight, \"Missing key columns for domain checks\"\n        # Extract machine numbers\n        mach_vals = df[machine_col].dropna().astype(str)\n        mach_nums = set()\n        for v in mach_vals:\n            m = re.search(r\"(\\d+)\", v)\n            if m:\n                try:\n                    n = int(m.group(1))\n                    mach_nums.add(n)\n                except:\n                    pass\n        # Shifts\n        shifts = set(df[shift_col].dropna().astype(str).str.strip().str.lower().unique().tolist())\n        # Week 1 subset\n        def to_int_safe(x):\n            try:\n                if pd.isna(x): return np.nan\n                if isinstance(x,(int,np.integer)): return int(x)\n                if isinstance(x,float): return int(x)\n                m = re.search(r\"\\d+\", str(x))\n                return int(m.group(0)) if m else np.nan\n            except: return np.nan\n        df['_wk_'] = df[week_col].apply(to_int_safe)\n        wk1 = df[df['_wk_']==1].copy()\n        # compute total for Week 1\n        for c in day_cols:\n            wk1[c] = pd.to_numeric(wk1[c], errors='coerce')\n        calc_total = wk1[day_cols].sum(axis=1)\n        # Scoring components\n        s_machines = 0.35 * weight if mach_nums == {1,2,3} else 0.15 * weight if len(mach_nums)>=2 else 0.0\n        s_shifts = 0.30 * weight if shifts.issuperset({'day','night'}) else 0.15 * weight if len(shifts)>=1 else 0.0\n        # Week 1 coverage: 9 rows and positive totals\n        rows_ok = len(wk1) >= 9\n        positive_ratio = float((calc_total.fillna(0) > 0).mean()) if len(wk1)>0 else 0.0\n        s_wk1 = 0.35 * weight * min(1.0, (1.0 if rows_ok else 0.5) * positive_ratio if rows_ok else positive_ratio)\n        score = s_machines + s_shifts + s_wk1\n        score = float(max(0.0, min(score, weight)))\n        fb = f\"Machines {sorted(list(mach_nums))}; Shifts {sorted(list(shifts))}; Week1 rows {len(wk1)}, positive_ratio {positive_ratio:.2f}\"\n        return score, fb\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "llm_judge", "name": "PivotTables and Week Selection Functionality", "description": "Verify the Dashboard pivots exist for all four required views and respond to a week or week-range selection via data validation control.", "weight": 3.0, "judge_prompt": "Evaluate functional presence and linkage (not numeric accuracy):\n- Confirm there are PivotTables (or equivalent summaries) for:\n  (a) operator performance/output for selected period,\n  (b) total machine output for selected period,\n  (c) average output by shift (day vs. night) for selected period,\n  (d) a YTD operator leaderboard (totals per operator).\n- Confirm a data validation dropdown (or slicer-like control) exists that selects a specific week or range of weeks and pivots are visibly linked to it (field filter responds to selection).\nScoring:\n- 3.0: All four pivots present and clearly react to week selection.\n- 2.0: Three pivots present and linked OR all pivots present but link is ambiguous for one view.\n- 1.0: One\u2013two pivots present and appear linked.\n- 0.0: Pivots and/or selection control missing or not functioning.", "expectation": "Four pivots covering operator, machine, shift, and YTD views, connected to a visible week selection control."}, {"type": "llm_judge", "name": "Charts Fidelity and Layout (Quadrant Visuals)", "description": "Verify four charts exist with correct types and depict Week 1 metrics; arranged in quadrant layout with appropriate titles/labels.", "weight": 2.5, "judge_prompt": "Inspect the Dashboard charts:\n- Four charts present and arranged roughly as four quadrants (2x2 grid layout).\n- Chart types and subjects:\n  \u2022 Bar chart: individual operator total output for the week.\n  \u2022 Pie chart: each machine\u2019s total output for the week.\n  \u2022 Pie chart: average output by shift for the week.\n  \u2022 Bar chart: YTD total output per operator.\n- Each chart should have clear title/labels/legend, and appear to reflect Week 1 where specified.\nScoring:\n- 2.5: All four charts present, correct types, clear labels/titles, depict intended metrics and Week 1 where relevant.\n- 1.5: Three charts correct or minor type/label/layout issues.\n- 0.5: One\u2013two charts present or significant mismatches.\n- 0.0: Charts largely missing or wrong types/metrics.", "expectation": "Four well-labeled charts in quadrant layout, matching specified metrics and time scope."}, {"type": "llm_judge", "name": "KPI Summary Accuracy for Week 1", "description": "Cross-check the KPI summary table values on the Dashboard against the Operator Output Data for Week 1: totals, top operator/machine, averages, and shift contribution percentages.", "weight": 2.5, "judge_prompt": "Validate that the KPI Summary Table on the Dashboard reflects Week 1 data from the Operator Output Data sheet:\n- KPIs to verify:\n  (a) total units produced (sum of all operators for Week 1),\n  (b) top performing operator and its output total,\n  (c) top performing machine and its output total,\n  (d) average output per operator (Week 1),\n  (e) day shift contribution % and night shift contribution % of the Week 1 total.\n- Use visible numbers in the Operator Output Data sheet to compute/estimate and compare with the Dashboard table values.\nScoring:\n- 2.5: All KPIs present and consistent with underlying Week 1 data.\n- 1.5: Most KPIs correct (1 minor mismatch) or one KPI missing but others correct.\n- 0.5: Multiple mismatches or missing several KPIs.\n- 0.0: KPI table missing or values largely inconsistent.", "expectation": "KPI values on the Dashboard should match what is derivable from Operator Output Data for Week 1."}, {"type": "llm_judge", "name": "Conditional Formatting Presence on Total/Average", "description": "Check that the Operator Output Data sheet applies conditional formatting to Total Output and Average Output columns to highlight top/bottom performers.", "weight": 2.0, "judge_prompt": "Inspect the Operator Output Data sheet and confirm conditional formatting exists for: \n- Total Output column and Average Output column.\n- Formatting should visually differentiate top vs. bottom performers (e.g., color scales, data bars, icon sets).\nScoring:\n- 2.0: Conditional formatting clearly applied to both columns across the table (visible with Week 1 data at least).\n- 1.0: Formatting applied to one column or applied inconsistently/partially.\n- 0.0: No visible conditional formatting on these columns.", "expectation": "Both Total and Average columns use conditional formatting to highlight performance extremes."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Presentation and Usability Quality", "description": "LLM assessment of professional polish, usability in weekly production meetings, and maintainability/scalability of the workbook.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Layout and Readability", "description": "Assess clarity of layout, titles, labels, number formats, freeze panes, and legibility suitable for weekly meetings.", "weight": 1.5, "judge_prompt": "Evaluate visual professionalism:\n- Clear titles and section labels; appropriate number formats for units.\n- Readable fonts, sufficient contrast; consistent color usage.\n- Freeze panes or headers to keep key fields visible during scrolling.\n- Minimal clutter; no overlapping objects; logical flow.\nScoring: 1.5 excellent, 1.0 good with minor issues, 0.5 fair/noticeable issues, 0.0 poor.", "expectation": "A clean, readable workbook ready to present in meetings with consistent formatting and labels."}, {"type": "llm_judge", "name": "Interactivity and Maintainability", "description": "Assess use of Excel Tables, named ranges, refreshability, and guidance for users (e.g., notes/instructions).", "weight": 1.5, "judge_prompt": "Assess:\n- Data table on Operator Output Data is an Excel Table (structured references) enabling easy expansion to new weeks.\n- Pivots/charts refresh against the table; minimal hard-coded ranges.\n- Presence of brief user instructions/notes for interacting with week selection and refreshing.\nScoring: 1.5 strong across items, 1.0 partial, 0.5 minimal, 0.0 absent.", "expectation": "Uses Excel Table and maintainable design with brief usage guidance."}, {"type": "llm_judge", "name": "Audience Appropriateness and Insightfulness", "description": "Evaluate whether the dashboard surfaces the right insights for a production supervisor audience.", "weight": 1.0, "judge_prompt": "Assess if the dashboard highlights what matters:\n- Operator comparisons are clear; machine and shift views provide actionable insight.\n- Leaderboard and KPIs emphasize performance drivers.\n- Minimal cognitive load; terminology matches shop-floor conventions.\nScoring: 1.0 strong fit, 0.7 good, 0.4 fair, 0.0 poor.", "expectation": "Focused metrics and visuals that help supervisors act quickly."}, {"type": "llm_judge", "name": "Scalability and Robustness", "description": "Evaluate robustness to adding weeks/operators and avoiding fragile design choices (e.g., merged cells).", "weight": 1.0, "judge_prompt": "Check for scalable practices:\n- Avoids merged cells that break sorting/pivots.\n- Formulas/formatting fill down correctly; adding Weeks 49\u201352 or new operators should work with minimal rework.\n- Charts/pivots tied to table rather than static ranges.\nScoring: 1.0 robust, 0.7 mostly robust, 0.4 some fragility, 0.0 fragile.", "expectation": "Design remains intact if data grows and avoids brittle constructs."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "6a900a40-8d2b-4064-a5b1-13a60bc173d8", "rubric": {"category_name": "Wholesale Technical Sales \u2013 Updated NGO Quotation with Multi-Modal Transport Options", "rationale": "Mixed-output task: a structured Excel quotation (pricing, totals, and transport options) that must be self-documenting and verification-ready. Stage 1 enforces exact workbook shape so that Stage 2 can verify calculations and scenario logic. Stage 3 evaluates professional quality and NGO-client suitability.", "max_total_score": 25.0, "stages": [{"name": "Stage 1 \u2013 Shape Enforcement Gate (LLM-only)", "description": "MANDATORY structure for a verifiable Excel quotation including items, EXW total, three transport options placed under EXW, grand totals per option, and mandatory red-font freight validity remark.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Excel Quotation with Transport Options", "description": "Output must be a single Excel file with specific sheets/sections, transport options below EXW, and a red-font general remark about freight validity.", "weight": 4.0, "judge_prompt": "You are evaluating whether the candidate output is a properly structured Excel quotation for an NGO buyer. Use flexible matching for naming, but require the following structure to be visible and readable in the workbook:\n\nFORMAT REQUIREMENTS\n- File type: Excel (.xlsx). Not PDF/DOCX/CSV.\n- Intended filename: \"Q9749821-revised_including_transport.xlsx\" (be lenient on minor case/spacing; filename will also be verified later by code).\n- Professional quotation layout suitable for an NGO client.\n\nREQUIRED STRUCTURE (SHEET AND SECTION LAYOUT)\n- A main quotation sheet (name can be \u201cQuotation\u201d, \u201cOffer\u201d, \u201cCommercial Offer\u201d, or similar) that includes:\n  1) Header block: Client name (Health NGO), supplier (Danish Wholesale & Co.), quotation number (Q9749821), date, currency, and Incoterm EXW.\n  2) Items table for the sterilization kits with columns similar to: [Item/Description | Quantity | Unit Price (EXW) | Line Total | Delivery Time]. Quantity must show 400 kits. Delivery Time should be present as a column or clearly listed per item and should come \"as per internal reference\" (exact values not validated here).\n  3) Totals block with a visible \"Total EXW\" value.\n  4) Transport Options block placed immediately below the \"Total EXW\" block. It must contain exactly three options:\n       - Airfreight (Euro Air Cargo)\n       - Seafreight (Red Ocean/Red Water Shipping \u2013 accept slight naming variations)\n       - Road Freight (Euro Road Logistics Co.)\n     Each option must be in a tabular structure with clearly visible columns similar to: [Mode | Transit Time | Freight Cost | Remarks/Notes | Grand Total]. Grand Total must be defined as EXW + Freight (do not check math here; only structure and presence).\n  5) General Remarks section that includes a red-font note stating that freight rates are subject to change, have limited validity (between 14 and 30 days), and are subject to reconfirmation at time of final order. The text must explicitly reference the 14\u201330 day validity window and reconfirmation.\n\nSCORING\n- 4.0: Excel file present AND all required structural elements present: items table with 400 kits, delivery time field, Total EXW, a clearly tabulated Transport Options block (3 options) positioned under Total EXW with the stated columns, and a General Remarks section containing the red-font freight validity/reconfirmation note.\n- 3.0\u20133.5: Excel file present; minor deviations in section names/column labels or slight placement differences, but all five requirements are effectively present and readable (e.g., remarks column named \u201cNotes\u201d).\n- 2.0: Excel file present but missing 1\u20132 core components (e.g., missing one transport option, or no visible Delivery Time column, or no visible General Remarks section).\n- 0.0\u20131.0: Not an Excel file, or missing multiple core components (no Total EXW, no transport options table, etc.).\n\nOnly judge structure and presence/placement\u2014not calculation correctness or content accuracy.", "expectation": "A single, professional Excel quotation with an Items table (400 kits), Total EXW, a Transport Options table (Air/Sea/Road) placed under the EXW total, per-option transit time and remarks, and a red-font general freight validity note (14\u201330 days, subject to reconfirmation)."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification: Calculations and Compliance (Code + LLM)", "description": "Now that the structure exists, verify key calculations and business rules: filename, transport scenarios, grand totals logic, risk flagging, and required notes.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Filename Compliance", "description": "Check the output file is named exactly as required: Q9749821-revised_including_transport.xlsx (case-insensitive).", "weight": 0.6, "code": "import re\nfrom pathlib import Path\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float score in [0, 0.6]\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        required = 'q9749821-revised_including_transport.xlsx'\n        fname = path.name.lower()\n        if fname == required:\n            return 0.6\n        else:\n            return 0.0\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Transport Options Presence (Air/Sea/Road)", "description": "Confirm the sheet(s) contain exactly three distinct transport modes: Air, Sea/Ocean, and Road/Truck.", "weight": 0.6, "code": "import pandas as pd\nimport re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Returns proportional credit based on how many distinct modes are detected (Air, Sea/Ocean, Road/Truck).\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        found = set()\n        mode_patterns = {\n            'air': re.compile(r'\\bair(?!port)\\b|air\\s*freight', re.I),\n            'sea': re.compile(r'\\bsea\\b|ocean|seafreight|sea\\s*freight', re.I),\n            'road': re.compile(r'\\broad\\b|truck|trucking|road\\s*freight', re.I)\n        }\n        for sheet in xls.sheet_names:\n            df = pd.read_excel(path, sheet_name=sheet, header=None)\n            for _, row in df.iterrows():\n                row_text = ' '.join([str(v) for v in row.values if pd.notna(v)])\n                for key, pat in mode_patterns.items():\n                    if pat.search(row_text):\n                        found.add(key)\n        score = 0.6 * (len(found) / 3.0)\n        return score\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Grand Total = EXW + Freight (per transport)", "description": "For each transport option row detected, check that the row\u2019s largest numeric equals (within tolerance) Total EXW + a smaller numeric interpreted as the freight cost.", "weight": 0.6, "code": "import pandas as pd\nimport numpy as np\nimport re\n\nNUM_RE = re.compile(r'[-+]?\\d[\\d,]*\\.?\\d*')\n\ndef parse_number(x):\n    if x is None or (isinstance(x, float) and np.isnan(x)):\n        return None\n    if isinstance(x, (int, float)):\n        return float(x)\n    s = str(x)\n    m = NUM_RE.search(s.replace('\\u00A0',' '))\n    if not m:\n        return None\n    val = m.group(0).replace(',', '')\n    try:\n        return float(val)\n    except Exception:\n        return None\n\ndef find_total_exw(frames):\n    candidates = []\n    for df in frames:\n        # Work on a copy with string lower for matching\n        str_df = df.applymap(lambda v: str(v).lower() if pd.notna(v) else '')\n        for r in range(str_df.shape[0]):\n            for c in range(str_df.shape[1]):\n                cell = str_df.iat[r, c]\n                if 'total exw' in cell or 'exw total' in cell:\n                    # neighbor right\n                    nums = []\n                    if c + 1 < df.shape[1]:\n                        n = parse_number(df.iat[r, c+1])\n                        if n is not None:\n                            nums.append(n)\n                    # neighbor below\n                    if r + 1 < df.shape[0]:\n                        n = parse_number(df.iat[r+1, c])\n                        if n is not None:\n                            nums.append(n)\n                    # same cell numeric\n                    n = parse_number(df.iat[r, c])\n                    if n is not None:\n                        nums.append(n)\n                    if nums:\n                        candidates.append(max(nums))\n    if candidates:\n        return max(candidates)\n    return None\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Returns proportional credit based on matches for up to 3 transport modes (Air/Sea/Road).\n    A match is counted when a row mentioning a mode also contains two or more numbers where\n    max_number \u2248 total_exw + freight_number within tolerance.\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        frames = [pd.read_excel(path, sheet_name=s, header=None) for s in xls.sheet_names]\n        total_exw = find_total_exw(frames)\n        if total_exw is None or total_exw <= 0:\n            return 0.0\n        mode_map = {'air': ['air', 'air freight'], 'sea': ['sea', 'ocean', 'sea freight', 'seafreight'], 'road': ['road', 'truck', 'trucking', 'road freight']}\n        matched = set()\n        for df in frames:\n            str_df = df.applymap(lambda v: str(v).lower() if pd.notna(v) else '')\n            for idx in range(str_df.shape[0]):\n                row_vals = list(str_df.iloc[idx, :].values)\n                numeric_vals = [parse_number(v) for v in df.iloc[idx, :].values]\n                numeric_vals = [v for v in numeric_vals if v is not None and v >= 0]\n                row_text = ' '.join(row_vals)\n                for mode, keys in mode_map.items():\n                    if any(k in row_text for k in keys):\n                        if len(numeric_vals) >= 2:\n                            # assume largest is grand total, a smaller is freight\n                            numeric_vals_sorted = sorted(numeric_vals)\n                            freight_candidates = [v for v in numeric_vals_sorted if v <= total_exw * 0.9 and v > 0]\n                            if not freight_candidates:\n                                continue\n                            freight = max(freight_candidates)\n                            grand = max(numeric_vals_sorted)\n                            # tolerance: 2% of grand or 50 units, whichever larger\n                            tol = max(0.02 * grand, 50.0)\n                            if abs(grand - (total_exw + freight)) <= tol:\n                                matched.add(mode)\n        score = 0.6 * (len(matched) / 3.0)\n        return score\n    except Exception:\n        return 0.0"}, {"type": "llm_judge", "name": "Risk Flag for Road Freight", "description": "Road option must explicitly flag border-zone delays/disruption risk.", "weight": 3.6, "judge_prompt": "Review the Transport Options section. Determine if the Road Freight option explicitly flags potential delays or disruptions due to active border zones. The risk warning should be clearly visible in the Road Freight row/remarks.\n\nScoring:\n- 3.6: Explicit, clear risk flag for border-zone delays/disruptions in the Road option (in the row or immediately adjacent note).\n- 2.0: Implicit or vague mention of potential delays, but not clearly tied to border crossings.\n- 0.0: No risk mention for Road Freight.", "expectation": "The Road Freight option clearly warns about border-zone risk and possible delays/disruptions."}, {"type": "llm_judge", "name": "General Remarks \u2013 Red Font and Validity/Reconfirmation Content", "description": "General remarks must include the required red-font freight validity and reconfirmation note.", "weight": 3.6, "judge_prompt": "Locate the General Remarks section. Confirm the presence of a red-font statement that includes BOTH (a) freight rates have limited validity specifically between 14 and 30 days, and (b) freight must be reconfirmed at the time of final order.\n\nScoring:\n- 3.6: Statement present in red font, explicitly mentions 14\u201330 day validity and reconfirmation at final order.\n- 2.0: Statement present but missing one required element (e.g., validity window or reconfirmation) OR not clearly in red font.\n- 0.0: No appropriate general remark.", "expectation": "A red-font note stating: freight rates are subject to change, valid for ~14\u201330 days, and subject to reconfirmation at final order."}, {"type": "llm_judge", "name": "Per-Option Transit Times and Suitability Rationale", "description": "Each transport option should include a transit time and a brief why/why-not suitability note aligned with a ~2-month target delivery.", "weight": 3.6, "judge_prompt": "Inspect the Transport Options table under the Total EXW block. For each of the three options (Air, Sea/Ocean, Road), verify that:\n- A transit time is provided (in days/weeks or a range) and is plausible for the mode (Air shortest, Sea longest, Road intermediate/variable).\n- A brief rationale/remark explains why the option may be more or less suitable (e.g., budget constraints, timeline flexibility, border risks).\n- Together, the options reasonably address the ~2-month target delivery window (they need not all fit the window, but their suitability notes should reference time/cost trade-offs).\n\nScoring:\n- 3.6: All three options show plausible transit times and short suitability notes; overall reflects the 2-month target context.\n- 2.0: Two options adequately documented; the third is missing time or rationale.\n- 0.0\u20131.0: One or zero options adequately documented.", "expectation": "Three options with plausible transit times and brief suitability notes tied to cost/timeline trade-offs and the 2-month target."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Quality Assessment (LLM)", "description": "Holistic evaluation of professionalism, client orientation, and decision support value for an NGO buyer.", "is_required": false, "max_points": 9.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Readability", "description": "Assess formatting quality, clear labeling, and ease of reading for an NGO procurement audience.", "weight": 2.25, "judge_prompt": "Evaluate the overall presentation of the Excel quotation:\n- Is the layout clean and readable (headers, alignment, currency formatting, thousand separators)?\n- Are section headers clear (Items, Total EXW, Transport Options, General Remarks)?\n- Are units and currencies consistently indicated?\nScoring:\n- 2.25: Professional, consistent formatting and clear labeling.\n- 1.2: Minor formatting inconsistencies but overall readable.\n- 0.0: Cluttered or confusing presentation.", "expectation": "Clean, professional quotation layout with consistent labels and currency formatting."}, {"type": "llm_judge", "name": "Volume Discount and Pricing Rationale Communication", "description": "Communicates that unit price reflects confirmed 400-unit volume (discounted versus initial) with a concise note or rationale.", "weight": 2.25, "judge_prompt": "Check whether the quotation communicates that the unit price reflects the confirmed quantity of 400 kits and implies or states a volume-based discount compared to the initial quotation. A brief note or footnote is sufficient.\n\nScoring:\n- 2.25: Clear mention of volume-based pricing/discount tied to the 400-kit confirmation.\n- 1.2: Implicit or weakly stated volume logic.\n- 0.0: No indication of discounting due to volume.", "expectation": "A concise statement that pricing reflects the confirmed 400 units (volume discount)."}, {"type": "llm_judge", "name": "Client Appropriateness and Decision Support", "description": "Transport comparison helps a budget-constrained NGO choose appropriately for a ~2-month target timeline.", "weight": 2.25, "judge_prompt": "Assess whether the quotation supports decision-making for a budget-constrained NGO aiming for delivery in ~2 months:\n- Do the transport options clearly communicate cost vs. timeline trade-offs?\n- Is there guidance or clear differentiation that would help the client choose (e.g., fastest vs. cheapest, risk considerations)?\nScoring:\n- 2.25: Strong comparative clarity and actionable guidance.\n- 1.2: Basic comparison with limited decision help.\n- 0.0: Minimal or confusing comparison.", "expectation": "Clear comparison enabling the client to weigh cost/timeline/risk trade-offs."}, {"type": "llm_judge", "name": "Traceability to Sources and Lead Times", "description": "References to internal price/lead-time source and named freight quotes are visible, enabling traceability (dates/validity where applicable).", "weight": 2.25, "judge_prompt": "Verify the quotation includes brief references to: (a) the internal price & lead time source for the kit, and (b) the three freight quotes (Euro Air Cargo, Red Water/Red Ocean Shipping, Euro Road Logistics Co.), ideally with quote numbers/dates or validity notes.\n\nScoring:\n- 2.25: Clear references to internal source and all three freight quotes, with dates/validity where applicable.\n- 1.2: Partial references (e.g., internal source mentioned but freight quotes not fully cited).\n- 0.0: No meaningful traceability to sources.", "expectation": "Visible references enabling a reviewer to trace unit prices/lead times and freight quotes."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "01d7e53e-0513-4109-a242-8ccaf442cd21", "rubric": {"category_name": "Government | Recreation Workers \u2014 Draft Interagency Agreement (City\u2013University RecFit Program)", "rationale": "This rubric enforces a self-documenting, verifiable Word/PDF agreement draft. Stage 1 mandates a precise document structure and key elements so verification is trivial. Stage 2 mixes light-weight code checks (deterministic inclusions like dates and signatories) with LLM cross-reference checks for legal and operational correctness. Stage 3 evaluates overall drafting quality, legal sufficiency for a North Carolina municipal\u2013university program, and readiness for attorney review.", "max_total_score": 28.0, "stages": [{"name": "Stage 1 \u2014 Structure & Format Gate (LLM only)", "description": "Gate that enforces deliverable shape: a review-ready agreement as a DOCX/PDF with required sections, exhibits, and signature blocks. Only structure/presence is scored, not content correctness.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 5.0, "rules": [{"type": "llm_judge", "name": "Structured Agreement Format Requirement", "description": "Verify the candidate produced a DOCX or PDF agreement with the required structure, sections, exhibits, and signature blocks.", "weight": 8.0, "judge_prompt": "You are evaluating whether the output is a properly structured contract document for the RecFit program between the City of Recreare and Recreare University (RU). Only check presence/format and structural completeness, not content quality or correctness.\n\nFormat requirements:\n- Must be a DOCX or PDF contract (not spreadsheet or plain text dump)\n- Minimum length: 3 pages\n- Professional formatting: clear headings, numbered articles/sections, and page footers/numbering\n\nRequired top-level structure (be flexible on exact headings but ensure the ideas are present and clearly delineated):\n1) Title and Parties with Effective Date\n   - Agreement title indicating RecFit or RU Healthy Lifestyles Fitness Program\n   - Parties: City of Recreare (Parks and Recreation) and Recreare University (Pediatrician's Office / RU Health System)\n   - Date/Term details may be referenced here or in Term article\n2) Recitals/Background\n3) Definitions (key terms such as Program, Facilities, Business Hours, Calendar, Contacts)\n4) Term and Renewal\n5) Facilities/Space Use (include Fitness Center as principal space and locked storage closet; may reference other spaces)\n6) Program Schedule & Access (weeknights and weekend usage windows)\n7) Roles and Responsibilities with clear subsections for each party (City; RU)\n8) Staffing and Volunteers\n9) Costs/Funding/Grants\n10) Reporting and Data (annual participant counts; appropriate privacy language may be referenced)\n11) Equipment and Storage (and Equipment Liability)\n12) Scheduling and Master Calendar (calendar provided 3x/year)\n13) Compliance and Standards (federal/state/city requirements)\n14) Insurance and Indemnification (identify both parties as self-insured; mutual indemnification heading present)\n15) Contacts for Day-to-Day Program Decisions (names/titles/phone/email blocks)\n16) Termination and Default/Remedies\n17) Miscellaneous (must explicitly state inclusion/incorporation of the City of Recreare \u201cStandard Contract Language\u201d or similar phrasing; this is the language that would come from Recreare_Official_Contract_Language.docx)\n18) Exhibits section listing at least:\n    - Exhibit A: Program Schedule and Space Use (or equivalent)\n    - Exhibit B: Equipment Liability/Inventory (or equivalent)\n19) Signature blocks for both organizations with the following signatories (names and titles visible):\n    - City: Beth Cobb, City Clerk; Robert Howell, CPRE, Director of Parks and Recreation\n    - University: Steve Southgate, MD, Chief of General Pediatrics; Mark Coleman, PH.D, Executive Vice Dean of Administration\n\nScoring (0 to 8):\n- 8: Valid DOCX/PDF, \u22653 pages, professional formatting, and all 19 structural elements present (allow synonyms for headings)\n- 6\u20137: Valid DOCX/PDF, \u22653 pages, professional formatting; missing 1\u20132 required elements or exhibits, but overall structure is sound\n- 4\u20135: Valid DOCX/PDF with partial structure (missing 3\u20135 key elements or missing signature blocks) but still recognizable as a contract\n- 1\u20133: Valid DOCX/PDF but largely incomplete structure (missing >5 key elements)\n- 0: Not DOCX/PDF, or fewer than 3 pages, or structure absent\n\nOnly evaluate presence/format, not correctness or legal sufficiency.", "expectation": "A clean, multi-page DOCX/PDF contract with clearly labeled sections/articles, two exhibits (Program Schedule/Spaces; Equipment Liability), a Miscellaneous section that incorporates the City's standard contract language, and signature blocks with the four named officials."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness Verification (Mixed Code + LLM)", "description": "Substantive verification of key terms, allocations of responsibilities, dates, renewals, indemnity/self-insurance, and cross-references to exhibits. Code rules perform deterministic text checks; LLM judges assess nuanced correctness and alignment to brief.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Term and Renewal Specificity", "description": "Checks the presence of the exact term (Jan 1, 2026 through Dec 31, 2027) and option for two additional one-year renewals.", "weight": 0.5, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_document:\\n            return 0.0, 'No document output'\\n        # Extract text\\n        text = ''\\n        try:\\n            if output.file_extension.lower().endswith('pdf'):\\n                text = context.files.read_pdf_text(output.id) or ''\\n            else:\\n                text = context.files.read_docx_text(output.id) or ''\\n        except Exception:\\n            # Fallback: try both\\n            try:\\n                text = context.files.read_docx_text(output.id) or ''\\n            except Exception:\\n                try:\\n                    text = context.files.read_pdf_text(output.id) or ''\\n                except Exception:\\n                    text = ''\\n        t = text.lower()\\n\\n        # Date checks (flexible: full dates or numeric)\\n        has_start = ('january 1, 2026' in t) or re.search(r'\\b01\\s*[/.-]\\s*01\\s*[/.-]\\s*2026\\b', t) or re.search(r'\\bjan(uary)?\\s*1\\s*,?\\s*2026\\b', t)\\n        has_end = ('december 31, 2027' in t) or re.search(r'\\b12\\s*[/.-]\\s*31\\s*[/.-]\\s*2027\\b', t) or re.search(r'\\bdec(ember)?\\s*31\\s*,?\\s*2027\\b', t)\\n\\n        # Renewal checks\\n        renew_patterns = [\\n            r'two\\s*\\(\\s*2\\s*\\)\\s*additional\\s*one[- ]year\\s*renewals',\\n            r'2\\s*additional\\s*one[- ]year\\s*renewals',\\n            r'two\\s*additional\\s*one[- ]year\\s*renewals',\\n            r'option\\s*for\\s*(two|2)\\s*one[- ]year\\s*renewal(s)?'\\n        ]\\n        has_renewal = any(re.search(p, t) for p in renew_patterns)\\n\\n        score = 0.0\\n        if has_start:\\n            score += 0.2\\n        if has_end:\\n            score += 0.2\\n        if has_renewal:\\n            score += 0.1\\n        # Cap at weight\\n        score = min(score, 0.5)\\n        feedback = []\\n        if not has_start: feedback.append('Missing explicit start date (Jan 1, 2026).')\\n        if not has_end: feedback.append('Missing explicit end date (Dec 31, 2027).')\\n        if not has_renewal: feedback.append('Missing clear option for two additional one-year renewals.')\\n        return score, ' '.join(feedback) if feedback else 'Term and renewals specified.'\\n    except Exception as e:\\n        return 0.0, f'Error: {e}'"}, {"type": "code", "name": "Signatories Presence Check", "description": "Confirms inclusion of all four signatories and their titles/credentials.", "weight": 0.5, "code": "import re\\n\\nNAMES = [\\n    ('beth cobb', ['city clerk']),\\n    ('robert howell', ['cpre','director of parks and recreation']),\\n    ('steve southgate', ['md','chief of general pediatrics']),\\n    ('mark coleman', ['ph.d','phd','executive vice dean of administration'])\\n]\\n\\ndef evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_document:\\n            return 0.0, 'No document output'\\n        # Extract text\\n        text = ''\\n        try:\\n            if output.file_extension.lower().endswith('pdf'):\\n                text = context.files.read_pdf_text(output.id) or ''\\n            else:\\n                text = context.files.read_docx_text(output.id) or ''\\n        except Exception:\\n            try:\\n                text = context.files.read_docx_text(output.id) or ''\\n            except Exception:\\n                try:\\n                    text = context.files.read_pdf_text(output.id) or ''\\n                except Exception:\\n                    text = ''\\n        t = text.lower()\\n        found = 0\\n        feedback = []\\n        for person, reqs in NAMES:\\n            has_name = person in t\\n            has_any_title = any(r in t for r in reqs)\\n            if has_name:\\n                found += 0.1\\n            else:\\n                feedback.append(f'Missing name: {person.title()}')\\n            if has_any_title:\\n                found += 0.025\\n            else:\\n                feedback.append(f'Missing title/credential for: {person.title()}')\\n        # Max 0.5\\n        score = min(found, 0.5)\\n        return score, ' '.join(feedback) if feedback else 'All signatories and titles present.'\\n    except Exception as e:\\n        return 0.0, f'Error: {e}'"}, {"type": "code", "name": "Program Access Windows", "description": "Verifies presence of the required access schedule: two hours, twice per week on weeknights, and two hours on a weekend day.", "weight": 0.5, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_document:\\n            return 0.0, 'No document output'\\n        # Extract text\\n        text = ''\\n        try:\\n            if output.file_extension.lower().endswith('pdf'):\\n                text = context.files.read_pdf_text(output.id) or ''\\n            else:\\n                text = context.files.read_docx_text(output.id) or ''\\n        except Exception:\\n            try:\\n                text = context.files.read_docx_text(output.id) or ''\\n            except Exception:\\n                try:\\n                    text = context.files.read_pdf_text(output.id) or ''\\n                except Exception:\\n                    text = ''\\n        t = text.lower()\\n        # Match flexible phrasing\\n        two_hours = any(s in t for s in ['two hours','2 hours','two (2) hours'])\\n        twice_per_week = any(s in t for s in ['twice per week','2x per week','two times per week','two (2) times per week'])\\n        weeknights = 'weeknight' in t or 'weeknights' in t or any(d in t for d in ['monday','tuesday','wednesday','thursday','friday'])\\n        weekend_day = 'weekend day' in t or 'weekend' in t or 'saturday' in t or 'sunday' in t\\n\\n        parts = [two_hours, twice_per_week, weeknights, weekend_day]\\n        score = sum(0.125 for p in parts if p)  # up to 0.5\\n        fb = []\\n        if not two_hours: fb.append('Missing 2-hour duration language')\\n        if not twice_per_week: fb.append('Missing twice-per-week language')\\n        if not weeknights: fb.append('Missing weeknight access language')\\n        if not weekend_day: fb.append('Missing weekend day access language')\\n        return score, ' ; '.join(fb) if fb else 'Access windows specified.'\\n    except Exception as e:\\n        return 0.0, f'Error: {e}'"}, {"type": "code", "name": "Mutual Indemnification and Self-Insurance", "description": "Checks for mutual indemnification language and that both parties are identified as self-insured.", "weight": 0.5, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_document:\\n            return 0.0, 'No document output'\\n        text = ''\\n        try:\\n            if output.file_extension.lower().endswith('pdf'):\\n                text = context.files.read_pdf_text(output.id) or ''\\n            else:\\n                text = context.files.read_docx_text(output.id) or ''\\n        except Exception:\\n            try:\\n                text = context.files.read_docx_text(output.id) or ''\\n            except Exception:\\n                try:\\n                    text = context.files.read_pdf_text(output.id) or ''\\n                except Exception:\\n                    text = ''\\n        t = text.lower()\\n        # Indemnification\\n        has_indemn = ('indemnify' in t or 'indemnification' in t)\\n        # Mutuality hint\\n        mutual_terms = ['mutual', 'each party shall indemnify', 'both parties shall indemnify']\\n        has_mutual = any(m in t for m in mutual_terms)\\n        # Self-insured\\n        self_insured_terms = ['self-insured','self insured','self insurance','self-insurance']\\n        has_self_insured = any(s in t for s in self_insured_terms)\\n\\n        score = 0.0\\n        if has_indemn: score += 0.2\\n        if has_mutual: score += 0.2\\n        if has_self_insured: score += 0.1\\n        score = min(score, 0.5)\\n        fb = []\\n        if not has_indemn: fb.append('Indemnification clause not found')\\n        if not has_mutual: fb.append('Mutuality of indemnification not evident')\\n        if not has_self_insured: fb.append('Self-insured status not identified for both parties')\\n        return score, ' ; '.join(fb) if fb else 'Mutual indemnity and self-insurance identified.'\\n    except Exception as e:\\n        return 0.0, f'Error: {e}'"}, {"type": "llm_judge", "name": "Allocation of Responsibilities (City vs. RU)", "description": "Assesses whether the agreement correctly assigns responsibilities as specified in the brief and that they are operationally coherent.", "weight": 4.0, "judge_prompt": "Evaluate whether the agreement correctly and explicitly assigns responsibilities as required. Look for:\n- RU responsibilities: manages grant funding; staffs the program; covers all program expenses; allows City staff to volunteer as part of the program; provides an annual report of total participants.\n- City responsibilities: provides Fitness Center as principal space; provides a locked storage closet in the Fitness Center; provides master calendar three times per year; provides additional spaces if referenced; coordinates scheduling.\n- Program access windows: two hours twice per week on weeknights, plus two hours on a weekend day (ensure these are within the RU program\u2019s permitted access and clearly scheduled/reservable).\nScoring (0\u20134):\n- 4: All listed responsibilities present, correctly allocated to each party, and operationally clear.\n- 3: One item partially missing or ambiguous, but overall allocation is correct.\n- 2: Multiple items missing/ambiguous, but the general allocation is mostly right.\n- 1: Major misallocations or vagueness.\n- 0: Responsibilities not addressed or grossly incorrect.\nProvide brief justification referencing locations/sections in the document.", "expectation": "Clear, unambiguous assignment of duties aligned with the brief, with City and RU subsections and actionable language."}, {"type": "llm_judge", "name": "Compliance, Privacy, and Risk Provisions", "description": "Assesses legal/compliance inclusions appropriate for municipal\u2013university pediatric wellness programming in NC, including privacy where applicable.", "weight": 3.5, "judge_prompt": "Check whether the agreement addresses applicable compliance and risk topics suitable for a municipal\u2013university pediatric wellness program in North Carolina. Look for:\n- Compliance with federal/state/local law and policies (e.g., nondiscrimination/Title VI, ADA, child safety/background checks for staff/volunteers, public records considerations for the City, and any health privacy statements acknowledging that medical care occurs under RU policies and that the City is not a covered entity unless explicitly so).\n- Insurance/Risk: both parties identified as self-insured; mutual indemnification suitable for public entities and to the extent permitted by law; risk allocation around equipment use and facilities.\n- Reporting and data handling: annual participant counts and any data sharing framed to avoid improper disclosure of protected health information.\nScoring (0\u20133.5):\n- 3.5: All areas substantively addressed and appropriate.\n- 2.5\u20133.0: Minor omissions but broadly sufficient.\n- 1.0\u20132.0: Several gaps; partially sufficient.\n- 0\u20130.5: Largely missing or inappropriate.\nProvide a short rationale pointing to relevant sections.", "expectation": "Reasonable municipal risk allocation, self-insurance clarity, and compliance provisions including child safety and accessibility."}, {"type": "llm_judge", "name": "Exhibits and Cross-References Integrity", "description": "Verifies exhibits exist, are correctly referenced in the body, and logically match their descriptions (Program Schedule/Spaces; Equipment Liability).", "weight": 2.5, "judge_prompt": "Evaluate whether Exhibits are present and correctly integrated:\n- Exhibit A (or equivalent): Program Schedule and Space Use \u2014 contains time blocks, spaces (Fitness Center as principal), and scheduling mechanics.\n- Exhibit B (or equivalent): Equipment Liability/Inventory \u2014 addresses equipment, storage, responsibility for loss/damage, and procedures.\n- The body references the exhibits where appropriate (e.g., Program Schedule references Exhibit A; Equipment/Storage references Exhibit B), and cross-references use consistent labels.\nScoring (0\u20132.5):\n- 2.5: Exhibits present and correctly referenced by name/letter; content aligns with references.\n- 1.5\u20132.0: Minor inconsistencies or thin content but substantially correct.\n- 0.5\u20131.0: Exhibits present but poorly integrated or mislabeled.\n- 0: Missing exhibits or no cross-references.\nBriefly justify your score.", "expectation": "Exhibits A and B exist, are labeled, and are referenced at the right places in the body text."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Readiness Assessment (LLM)", "description": "Holistic evaluation of drafting quality, legal sufficiency for a NC municipal\u2013university agreement, operational clarity, and readiness for attorney review.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Drafting Quality", "description": "Assesses clarity, organization, headings/numbering, defined terms, and absence of contradictions.", "weight": 2.0, "judge_prompt": "Assess the professional quality of the drafting:\n- Clear, concise language; consistent defined terms.\n- Logical organization with numbered articles/sections and cross-references.\n- No obvious contradictions; internal consistency (e.g., same dates, same exhibit labels).\nScore 0\u20132.0 with brief rationale.", "expectation": "A clean, consistent, professionally formatted agreement with coherent structure and definitions."}, {"type": "llm_judge", "name": "Legal Sufficiency for Municipal\u2013University Context (NC)", "description": "Evaluates whether the agreement reads as legally practical for a North Carolina city and a public university.", "weight": 2.0, "judge_prompt": "Evaluate legal sufficiency for a North Carolina municipal\u2013university program:\n- Appropriate municipal clauses (e.g., governing law/venue, public records, non-appropriation if relevant, equal opportunity/Title VI/ADA references, independent contractor, no waiver of sovereign/governmental immunity by the City or University beyond permitted law).\n- Risk/insurance provisions appropriately framed for self-insured public entities.\nProvide a 0\u20132.0 score with a short explanation.", "expectation": "Appropriate public-entity contract posture reflecting NC norms and limitations."}, {"type": "llm_judge", "name": "Operational Clarity and Implementability", "description": "Assesses whether staff could implement the program from the agreement: scheduling, conflicts, notices, reporting cadence, and escalation.", "weight": 2.0, "judge_prompt": "Does the agreement provide enough operational clarity to run RecFit?\n- Scheduling mechanics (master calendar 3x/year), notice periods for changes/cancellations, conflict resolution for space, holiday closures.\n- Reporting: annual participant report timing and format.\n- Points of contact: day-to-day contact info is actionable (name/title/email/phone) and located in a dedicated section.\nScore 0\u20132.0 with a brief justification.", "expectation": "Actionable procedures for scheduling, notice, conflicts, and reporting with clear contacts."}, {"type": "llm_judge", "name": "Attorney-Review Readiness", "description": "Evaluates polish for submission to City Attorney before sending to RU counsel.", "weight": 2.0, "judge_prompt": "Is the document ready for attorney review?\n- All required sections present and internally consistent; placeholders minimized and clearly marked.\n- Exhibits attached and referenced; signature blocks correct; dates present on signature lines or a mechanism for effective date.\n- Inclusion by reference of City standard contract language in Miscellaneous.\nScore 0\u20132.0 and explain briefly.", "expectation": "A near-final draft requiring minor legal polishing, not substantive rework."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "a0ef404e-82a6-4507-bff1-633d7c8e0004", "rubric": {"category_name": "Car Rental Agreement Guide \u2014 Counter and Rental Clerks", "rationale": "This rubric enforces a self-documenting structure for a step-by-step instructional guide used by newly onboarded car rental clerks. Stage 1 mandates a rigid, verifiable document shape so later checks are trivial. Stage 2 mixes lightweight code checks (text extraction, keyword coverage, length) with higher-weight LLM judgments for procedural correctness, explanations, tips, and troubleshooting. Stage 3 evaluates overall professional quality, clarity, navigability, and compliance-awareness.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Format and Structural Gate", "description": "LLM-only gate verifying the document is a properly structured instructional guide in DOCX or PDF with all required sections and per-step substructure. Failure zeros the entire category.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.5, "rules": [{"type": "llm_judge", "name": "Document Format and Structure Requirements (Gate)", "description": "Check that the candidate output is a DOCX or PDF guide with all required sections and per-step substructure present.", "weight": 6.0, "judge_prompt": "You are evaluating whether the submitted output meets strict STRUCTURAL requirements for a self-service instructional guide for new car rental clerks. Only check PRESENCE and STRUCTURE, not content quality or correctness.\n\nRequired format:\n- Must be a DOCX or PDF document (Word preferred but PDF acceptable).\n- Professionally formatted with clear headings. Minimum 2 pages.\n\nRequired sections and structure (flexible on exact names; prioritize intent):\n1) Title and Document Metadata\n   - Title includes terms like \u201cNew Rental Agreement\u201d or \u201cRental Agreement Process\u201d.\n   - Include author or role, date, and version/revision info (anywhere in document: header, footer, or a section labeled Version/Revision History).\n\n2) Purpose and Scope\n   - A short section describing the document\u2019s purpose and who should use it.\n\n3) Prerequisites and Required Documents\n   - Lists eligibility and materials: valid driver\u2019s license, acceptable payment method/deposit, age requirements, reservation number, etc.\n\n4) Process Overview / Quick Flow\n   - A brief overview of the process before the detailed steps (could be a numbered list or short paragraph overview).\n\n5) Step-by-Step Procedure (primary body)\n   - At least 10 numbered steps labeled in a clear way (e.g., \u201cStep 1: \u2026\u201d). Steps MUST cover at minimum these topics somewhere among them:\n     a) Welcoming/Greeting the customer\n     b) Finding/confirming the reservation\n     c) Verifying identification and required documents\n     d) Recording contact details\n     e) Obtaining a valid payment method (pre-auth/deposit)\n     f) Assigning a vehicle and recording unit details\n     g) Reviewing rental terms (fuel, mileage, insurance/coverage, fees)\n     h) Collecting signatures/consents and printing/saving the agreement\n     i) Pre-handoff inspection/odometer/fuel checks and key handoff\n     j) Final confirmation and next steps (e.g., return instructions)\n   - For EACH step block, include clearly labeled sub-elements:\n     \u2022 Why/Purpose (why the step is necessary)\n     \u2022 How/Procedure (bullet or numbered actions)\n     \u2022 Tips (practical suggestions)\n     \u2022 Common Mistakes and/or Troubleshooting\n\n6) Checklists\n   - At least one \u201cPre-Handoff Checklist\u201d and one \u201cSystem Data Fields Checklist\u201d (or similar names). Bullet/checkbox-style lists acceptable.\n\n7) Troubleshooting Scenarios\n   - A dedicated section listing common issues (e.g., declined card, no reservation, underage driver, expired license, vehicle class unavailable) with brief guidance.\n\n8) Legal/Compliance Notes\n   - Short section calling out compliance/safety topics (e.g., ID/age verification, PCI/handling payment data, privacy/data protection, jurisdictional constraints).\n\n9) Appendix or Quick Reference\n   - A condensed summary or quick-reference page/section (e.g., 1-page runbook or field list) to use during peak hours.\n\nScoring (return a fractional score 0.0\u20131.0 that reflects structural completeness):\n- 1.0: Correct format (DOCX/PDF), length OK, and all 9 structural elements present, including per-step sub-elements (Why/Procedure/Tips/Common Mistakes/Troubleshooting) for the majority of steps.\n- 0.8: One minor supporting element missing (e.g., Version/Revision History, Quick Reference) but core sections and per-step sub-elements are present.\n- 0.6: Up to two supporting elements missing, but core structure (Steps with sub-elements, Prereqs, Checklists, Troubleshooting) intact.\n- 0.3: Major structural gaps (e.g., per-step sub-elements largely missing, fewer than ~7 of the required topics covered in steps, or no Checklists/Troubleshooting).\n- 0.0: Wrong format (not DOCX/PDF), <2 pages, or multiple core sections missing (e.g., no Step-by-Step Procedure, no Prereqs, no Checklists, no Troubleshooting).\n\nBe flexible on exact section names; judge intent and visible structure only. Do not evaluate correctness or writing quality.", "expectation": "A DOCX/PDF instructional guide with title/metadata, purpose/scope, prerequisites, quick overview, detailed numbered steps each with Why/Procedure/Tips/Common Mistakes/Troubleshooting, checklists, troubleshooting scenarios, legal/compliance notes, and an appendix/quick-reference, plus versioning."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Coverage Verification", "description": "Verifies the guide\u2019s procedural soundness, completeness of the required elements (explanations, tips, troubleshooting), and coverage of core steps. Combines lightweight code checks with higher-weight LLM judgments.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Minimum Length and Structural Keywords (Code)", "description": "Check word count and presence of key structural keywords (purpose, tips, troubleshooting, checklist, appendix, version, legal/compliance, step).", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        text = \"\"\n        try:\n            if output.name.lower().endswith('.pdf'):\n                text = context.files.read_pdf_text(output.id)\n            elif output.name.lower().endswith('.docx'):\n                text = context.files.read_docx_text(output.id)\n            else:\n                # fallback\n                text = context.files.read_text(output.id)\n        except Exception:\n            return 0.0\n        if not text:\n            return 0.0\n        t = text.lower()\n        # word count heuristic\n        words = re.findall(r\"\\w+\", t)\n        wc = len(words)\n        length_score = min(1.0, wc / 900.0)  # full credit around ~900 words\n        keywords = [\n            'purpose', 'scope', 'prerequisite', 'required documents', 'step',\n            'tips', 'common mistake', 'troubleshooting', 'checklist',\n            'appendix', 'quick reference', 'version', 'revision', 'legal', 'compliance'\n        ]\n        hits = 0\n        for k in keywords:\n            if k in t:\n                hits += 1\n        structure_score = hits / len(keywords)\n        score = 0.5 * length_score + 0.5 * structure_score\n        # return as fraction 0..1 (framework will scale by weight)\n        return max(0.0, min(1.0, score))\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Core Steps Coverage (Code)", "description": "Verify coverage of the core required steps using fuzzy keyword sets.", "weight": 0.6, "code": "def evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        try:\n            if output.name.lower().endswith('.pdf'):\n                text = context.files.read_pdf_text(output.id)\n            elif output.name.lower().endswith('.docx'):\n                text = context.files.read_docx_text(output.id)\n            else:\n                text = context.files.read_text(output.id)\n        except Exception:\n            return 0.0\n        if not text:\n            return 0.0\n        t = text.lower().replace(\"\u2019\", \"'\")\n        # Define synonym groups for 7 core steps\n        groups = {\n            'welcome': ['welcome', 'greet', 'greeting'],\n            'reservation': ['reservation', 'booking', 'confirmation', 'record locator', 'itinerary', 'pnr'],\n            'id_docs': ['id', 'identification', \"driver's license\", 'drivers license', 'licence', 'passport'],\n            'contact': ['contact', 'phone', 'email', 'address'],\n            'payment': ['payment', 'credit card', 'debit', 'pre-auth', 'preauth', 'authorization', 'deposit'],\n            'assign_vehicle': ['assign', 'vehicle', 'car', 'unit', 'vin', 'license plate', 'plate', 'inspection'],\n            'review_terms': ['terms', 'conditions', 'fuel policy', 'mileage', 'insurance', 'coverage', 'liability', 'waiver']\n        }\n        covered = 0\n        for k, synonyms in groups.items():\n            if any(s in t for s in synonyms):\n                covered += 1\n        score = covered / len(groups)\n        return max(0.0, min(1.0, score))\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Checklists and Critical Fields Coverage (Code)", "description": "Check presence of checklists and key data fields typically required on a rental agreement.", "weight": 0.4, "code": "def evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        try:\n            if output.name.lower().endswith('.pdf'):\n                text = context.files.read_pdf_text(output.id)\n            elif output.name.lower().endswith('.docx'):\n                text = context.files.read_docx_text(output.id)\n            else:\n                text = context.files.read_text(output.id)\n        except Exception:\n            return 0.0\n        if not text:\n            return 0.0\n        t = text.lower()\n        fields = [\n            'checklist', 'driver', 'license number', 'expiry', 'expiration', 'issuing state',\n            'date of birth', 'dob', 'reservation number', 'pickup date', 'return date',\n            'rate code', 'vehicle class', 'odometer', 'fuel level', 'insurance', 'coverage',\n            'additional driver', 'authorization', 'pre-auth', 'deposit', 'notes'\n        ]\n        hits = sum(1 for f in fields if f in t)\n        score = hits / len(fields)\n        return max(0.0, min(1.0, score))\n    except Exception:\n        return 0.0"}, {"type": "llm_judge", "name": "Procedural Validity and Safe Order (LLM)", "description": "Assess whether the sequence reflects real-world rental counter workflow: verify ID/eligibility, confirm reservation, capture contact/payment (pre-auth) before vehicle handoff, explain terms/coverage, inspect vehicle, signatures, then handoff.", "weight": 1.8, "judge_prompt": "Evaluate the PROCEDURAL ORDER and validity of the guide. Check that the sequence is realistic and safe for a car rental counter:\n- Identity/eligibility verification occurs before payment and handoff.\n- Reservation is confirmed/created before contract issuance.\n- Payment method is validated with pre-authorization or deposit before releasing vehicle.\n- Terms/coverage, fees, fuel/mileage, and disclosures are reviewed before signature.\n- Vehicle assignment and inspection (odometer/fuel/condition) occur prior to handoff.\n- Final confirmation and return instructions are provided.\nScore 0.0\u20131.0: 1.0 = logical, safe sequence with all elements; 0.5 = mostly right with minor ordering gaps; 0.0\u20130.3 = significant procedural flaws or missing critical steps.", "expectation": "A safe, logical sequence that mirrors common airport rental workflows."}, {"type": "llm_judge", "name": "Explanations of Why (Per-Step Rationale) (LLM)", "description": "Verify that most steps include clear reasons why the step is necessary (risk, compliance, customer experience).", "weight": 1.6, "judge_prompt": "Assess whether each step includes a clear explanation of WHY it is necessary (risk reduction, compliance, accuracy, customer experience). Inspect several step blocks and sample across the document.\nScore 0.0\u20131.0: 1.0 = Nearly every step states an accurate rationale; 0.6 = rationales present for most steps but some are vague; 0.0\u20130.4 = few or inaccurate rationales.", "expectation": "Clear per-step rationales connected to compliance, accuracy, and customer experience."}, {"type": "llm_judge", "name": "Troubleshooting Thoroughness and Soundness (LLM)", "description": "Evaluate the breadth and practicality of troubleshooting guidance for common issues (declined card, no reservation, expired license, underage driver, unavailable class, etc.).", "weight": 1.5, "judge_prompt": "Evaluate troubleshooting coverage for common counter issues: declined/blocked card, no/duplicate reservation, underage/ineligible driver, expired/mismatched ID, unavailable vehicle class, flight delays, cross-border restrictions, additional driver missing, etc. Guidance should be concrete, safe, and escalate appropriately.\nScore 0.0\u20131.0: 1.0 = broad, practical coverage with clear actions/alternatives; 0.6 = covers several issues but missing important ones; 0.0\u20130.4 = sparse or unsafe guidance.", "expectation": "Coverage of multiple realistic scenarios with actionable, safe steps and escalation paths."}, {"type": "llm_judge", "name": "Practical Efficiency Tips (LLM)", "description": "Assess whether tips are specific and help new clerks work efficiently during peak hours (queues, flight schedules, upsell timing, batching tasks, using checklists).", "weight": 1.5, "judge_prompt": "Review the Tips provided across steps. Are they specific, time-saving, and appropriate for a busy airport location (e.g., greet while scanning ID, confirm flight arrival before assigning unit, batch printing, defer long explanations until after pre-auth, quick upsell phrasing, use of checklists)?\nScore 0.0\u20131.0: 1.0 = many concrete, context-aware tips; 0.6 = some useful but generic tips; 0.0\u20130.4 = vague or minimal tips.", "expectation": "Actionable, context-aware tips that improve speed and accuracy during peak hours."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Usability", "description": "Holistic LLM assessment of presentation, clarity, navigability, and audience fit for newly onboarded clerks working under time pressure.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity and Readability for New Clerks", "description": "Plain language, concise step wording, unambiguous instructions, helpful examples.", "weight": 1.5, "judge_prompt": "Judge clarity and readability: Are steps concise and unambiguous? Is language plain, jargon minimized or explained, and examples used where helpful? Would a new clerk understand and follow it without supervision?\nScore 0.0\u20131.0: 1.0 = crystal clear and unambiguous; 0.6 = mostly clear with minor ambiguities; 0.0\u20130.4 = confusing or overly verbose.", "expectation": "Clear, plain-language instructions a new clerk can follow independently."}, {"type": "llm_judge", "name": "Formatting and Navigability", "description": "Professional formatting: headings, numbering, visual hierarchy, quick-reference elements, and ease of scanning under time pressure.", "weight": 1.5, "judge_prompt": "Assess professional formatting and navigability: descriptive headings, numbered steps, consistent hierarchy, useful lists/tables/callouts, optional TOC, and a quick-reference/summary. Is it easy to scan mid-shift?\nScore 0.0\u20131.0: 1.0 = highly navigable and professional; 0.6 = acceptable with minor issues; 0.0\u20130.4 = hard to navigate.", "expectation": "Well-structured document with clear hierarchy, numbered steps, and quick-reference aids."}, {"type": "llm_judge", "name": "Actionability Under Peak Conditions", "description": "Does the guide enable fast, correct execution during busy shifts: checklists, time-savers, escalation cues?", "weight": 1.5, "judge_prompt": "Evaluate whether the guide supports fast, correct execution under peak load: checklists at point-of-use, time-saving patterns, decision points, and clear escalation cues. Are next steps obvious after each major action?\nScore 0.0\u20131.0: 1.0 = highly actionable under pressure; 0.6 = moderately actionable; 0.0\u20130.4 = difficult to apply in real time.", "expectation": "Practical, at-a-glance aids that keep the process moving with minimal supervision."}, {"type": "llm_judge", "name": "Compliance and Professional Tone", "description": "Tone is professional and customer-service oriented; includes compliance reminders (ID/age verification, PCI, privacy) without fearmongering.", "weight": 1.5, "judge_prompt": "Assess tone and compliance awareness: professional, courteous, and customer-focused; includes succinct compliance reminders (ID/age verification, PCI/handling payment data, privacy/data protection) woven into steps. Avoids unsafe or non-compliant advice.\nScore 0.0\u20131.0: 1.0 = professional and compliance-aware; 0.6 = mostly appropriate; 0.0\u20130.4 = tone or compliance gaps.", "expectation": "Professional, courteous tone with clear, accurate compliance cues integrated into the workflow."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "7b08cd4d-df60-41ae-9102-8aaa49306ba2", "rubric": {"category_name": "Excel P&L Report \u2014 2024 Fall Music Tour (Accountants & Auditors)", "rationale": "This rubric enforces a self-documenting, verifiable Excel deliverable for a professional P&L suitable for executive review. Stage 1 is an LLM-only gate that mandates an explicit workbook shape enabling trivial verification. Stage 2 mixes deterministic code checks (arithmetic, bounds, cross-sheet consistency) with LLM cross-references (methodology, exchange rates, USD conversion, scope). Stage 3 assesses presentation quality, executive readiness, and decision-usefulness.", "max_total_score": 25.0, "stages": [{"name": "Stage 1 \u2014 Structured Output Gate (Excel Shape Enforcement)", "description": "Gate: Verify the workbook is an Excel file with the exact structure needed to enable verification. Only structure and presence are checked, not correctness.", "is_required": true, "max_points": 7.0, "min_score_to_pass": 5.0, "rules": [{"type": "llm_judge", "name": "Structured Excel P&L Format Requirement", "description": "Must be a professionally structured Excel workbook with required sheets, sections, and tables to enable verification.", "weight": 7.0, "judge_prompt": "You are checking ONLY the structure and presence of required elements (not correctness). The candidate must submit a single Excel workbook (.xlsx). Be flexible on exact naming (e.g., 'P&L Summary' vs 'Profit & Loss Summary' vs 'Income Statement Summary'), but the required elements must be clearly present and visible.\n\nCheck the following:\n\nFormat requirements\n- File type: Excel (.xlsx). Not PDF/DOCX/CSV.\n- Clear professional formatting; headers and tables are legible.\n- The header on the summary sheet contains both: '2024 Fall Music Tour' and 'As of 12/31/2024'.\n\nSheet A: 'P&L Summary' (or similar: 'Profit & Loss Summary', 'Income Statement Summary')\n- Must present a columnar breakdown: Tour Manager, Production Company, Total Combined.\n- A leftmost label column (e.g., Line Item) listing rows organized into:\n  \u2022 Revenue block with a clearly labeled 'Net Revenue' line.\n  \u2022 Expenses block including at least these categories as separate lines: Band and Crew; Other Tour Costs; Hotel & Restaurants; Other Travel Costs; and a 'Total Expenses' line.\n  \u2022 A 'Net Income' line computed below the above blocks.\n- All monetary columns clearly formatted as currency (USD).\n\nSheet B: 'Revenue Detail' (or similar)\n- A line-by-line list of tour stops by city and country (one row per stop).\n- A table with columns that clearly include: City, Country, Gross Revenue (USD), Withholding Rate (%), Withholding Amount (USD), Net Revenue (USD). Allow additional helpful columns (e.g., Date, Show #, Local Currency, FX Rate), but the USD columns must be present.\n- The intent must be clear that revenue figures are reported in USD.\n\nSheet C: 'Expenses Detail' (or similar)\n- A table of expenses by the broad categories (Band and Crew; Other Tour Costs; Hotel & Restaurants; Other Travel Costs) with a breakdown by source (Tour Manager vs Production Company) and totals.\n\nSheet D: 'Assumptions & Methodology' (or similar)\n- Withholding schedule listing the four countries with exact rates: UK 20%, France 15%, Spain 24%, Germany 15.825%.\n- Exchange Rate Log table with columns: Currency, USD Exchange Rate, Date, Source.\n- Methodology text (at least 3 sentences) describing: how FX conversion to USD was performed; how withholding was applied (gross less withholding = net); any data sources/assumptions; and reiterating the as-of date.\n\nScoring\n- 7.0: Excel file with all four sheets present and clearly structured as above; required columns and lines visible; header includes 'As of 12/31/2024'.\n- 5.5: Excel file; minor deviations only (e.g., sheet naming variants), all required sections exist but one table misses a non-critical column label while intent is still unmistakable.\n- 4.0: Excel file; missing one required sheet or a required block on P&L Summary (but majority present).\n- 2.0: Excel file; partial scaffold only (e.g., summary without detail or missing source breakdown columns).\n- 0.0: Not an Excel file OR missing multiple core elements so verification would be impossible.\n\nOnly check presence/structure, not the correctness of numbers or formulas.", "expectation": "A single .xlsx workbook containing: (1) P&L Summary with required columns/lines and 'As of 12/31/2024' in header; (2) Revenue Detail with city/country and USD gross/withholding/net columns; (3) Expenses Detail with required categories and source breakdown; (4) Assumptions & Methodology with the specified withholding schedule, an exchange rate log, and a 3+ sentence methodology narrative."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness and Consistency)", "description": "Now verify correctness, consistency, and cross-references enabled by the enforced structure. Mix deterministic code checks with LLM judgment.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Revenue Withholding Arithmetic and Country Rates", "description": "Verify that Revenue Detail applies country withholding rates correctly and arithmetic is consistent: Net = Gross \u2212 Withholding; rates match UK 20%, France 15%, Spain 24%, Germany 15.825% (tolerance allowed).", "weight": 1.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    weight = 1.0\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        # Find Revenue Detail-like sheet\n        xfile = pd.ExcelFile(context.files.get_path(output.id))\n        sheets = xfile.sheet_names\n        rev_sheet = None\n        for name in sheets:\n            lname = name.lower()\n            if 'revenue' in lname:\n                rev_sheet = name\n                if 'detail' in lname or 'summary' not in lname:\n                    break\n        if not rev_sheet:\n            return 0.0\n        df = pd.read_excel(context.files.get_path(output.id), sheet_name=rev_sheet)\n        if df.shape[1] == 0 or df.shape[0] == 0:\n            return 0.0\n        cols = [str(c).strip() for c in df.columns]\n        lcols = [c.lower() for c in cols]\n        # Helper to find a column containing all tokens\n        def find_col(tokens_list):\n            for i, c in enumerate(lcols):\n                if all(t in c for t in tokens_list):\n                    return cols[i]\n            return None\n        country_col = find_col(['country'])\n        gross_col = find_col(['gross','usd']) or find_col(['gross'])\n        rate_col = find_col(['withholding','rate']) or find_col(['tax','rate']) or find_col(['rate'])\n        w_amt_col = find_col(['withholding','amount']) or find_col(['tax','usd']) or find_col(['withholding','usd'])\n        net_col = find_col(['net','usd']) or find_col(['net'])\n        required = [country_col, gross_col, rate_col, w_amt_col, net_col]\n        if any(c is None for c in required):\n            # Not enough structure to verify deterministically\n            return 0.0\n        # Numeric coercion helpers\n        def to_num(s):\n            if pd.isna(s):\n                return np.nan\n            if isinstance(s, (int, float, np.integer, np.floating)):\n                return float(s)\n            s = str(s)\n            s = s.replace(',', '')\n            s = re.sub(r'[^0-9.\\-]', '', s)\n            try:\n                return float(s) if s not in ['', '.', '-'] else np.nan\n            except:\n                return np.nan\n        def to_rate(v):\n            if pd.isna(v):\n                return np.nan\n            if isinstance(v, str) and '%' in v:\n                try:\n                    return float(v.replace('%','').strip())/100.0\n                except:\n                    pass\n            val = to_num(v)\n            if pd.isna(val):\n                return np.nan\n            # If in [1,100], treat as percent\n            if 1.0 < val <= 100.0:\n                return val/100.0\n            return val\n        df['_gross'] = df[gross_col].apply(to_num)\n        df['_rate'] = df[rate_col].apply(to_rate)\n        df['_withheld'] = df[w_amt_col].apply(to_num)\n        df['_net'] = df[net_col].apply(to_num)\n        # Arithmetic check: net \u2248 gross - withheld\n        tol_amt = 1.0  # $1 tolerance\n        valid_arith = (df[['_gross','_withheld','_net']].notna().all(axis=1)) & (np.abs((df['_gross'] - df['_withheld']) - df['_net']) <= tol_amt)\n        arith_score = valid_arith.mean() if len(df) > 0 else 0.0\n        # Country rate check for specified countries only\n        expected = {\n            'uk': 0.20,\n            'united kingdom': 0.20,\n            'france': 0.15,\n            'spain': 0.24,\n            'germany': 0.15825\n        }\n        def expected_rate_for(country):\n            if pd.isna(country):\n                return None\n            c = str(country).strip().lower()\n            return expected.get(c, None)\n        mask_spec = df[country_col].apply(lambda x: expected_rate_for(x) is not None)\n        if mask_spec.any():\n            tol_rate = 0.0005\n            exp_rates = df.loc[mask_spec, country_col].apply(expected_rate_for)\n            got_rates = df.loc[mask_spec, '_rate']\n            valid_rate = (exp_rates - got_rates).abs() <= tol_rate\n            rate_score = valid_rate.mean()\n            combined = 0.5*arith_score + 0.5*rate_score\n        else:\n            # If no specified-country rows, rely on arithmetic only\n            combined = arith_score\n        combined = float(max(0.0, min(1.0, combined)))\n        return combined * weight, f\"Arithmetic ok on {valid_arith.sum()}/{len(df)} rows; rate check rows: {int(mask_spec.sum())}.\"\n    except Exception as e:\n        return 0.0, f\"Error verifying revenue withholding: {e}\""}, {"type": "code", "name": "P&L Totals Consistency Across Sheets", "description": "Verify P&L Summary totals: Total Combined \u2248 Tour Manager + Production Company; Total Expenses equals sum of expense categories; Net Income \u2248 Net Revenue \u2212 Total Expenses; and Net Revenue matches sum of Revenue Detail net USD (within tolerance).", "weight": 1.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    weight = 1.0\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        xfile = pd.ExcelFile(path)\n        sheets = xfile.sheet_names\n        # Find P&L Summary sheet\n        pnl_sheet = None\n        for name in sheets:\n            ln = name.lower()\n            if ('p&l' in ln) or ('profit' in ln and 'loss' in ln) or ('income' in ln and 'summary' in ln) or ('summary' in ln and ('p&l' in ln or 'income' in ln)):\n                pnl_sheet = name\n                break\n            if 'summary' in ln:\n                pnl_sheet = name\n        if pnl_sheet is None:\n            return 0.0\n        pnl = pd.read_excel(path, sheet_name=pnl_sheet)\n        if pnl.shape[1] < 3:\n            return 0.0\n        cols = [str(c) for c in pnl.columns]\n        lcols = [c.lower() for c in cols]\n        # Identify columns\n        def col_like(*tokens):\n            for i,c in enumerate(lcols):\n                if all(t in c for t in tokens):\n                    return cols[i]\n            return None\n        label_col = cols[0]\n        tm_col = col_like('tour','manager')\n        pc_col = col_like('production') or col_like('prod','company')\n        tot_col = col_like('total','combined') or col_like('combined') or col_like('total')\n        if not all([tm_col, pc_col, tot_col]):\n            return 0.0\n        # Normalize numbers\n        def to_num(s):\n            if pd.isna(s): return np.nan\n            if isinstance(s,(int,float,np.integer,np.floating)): return float(s)\n            s = str(s).replace(',','')\n            s = re.sub(r'[^0-9.\\-]','',s)\n            try: return float(s)\n            except: return np.nan\n        for c in [tm_col, pc_col, tot_col]:\n            pnl[c] = pnl[c].apply(to_num)\n        labels = pnl[label_col].astype(str).str.strip().str.lower()\n        # Locate key rows\n        def row_idx(*tokens):\n            m = labels.apply(lambda s: all(t in s for t in tokens))\n            idx = np.where(m)[0]\n            return int(idx[0]) if len(idx)>0 else None\n        idx_net_rev = row_idx('net','revenue')\n        idx_tot_exp = row_idx('total','expense')\n        idx_net_inc = row_idx('net','income')\n        # Expense category rows\n        cat_tokens = [\n            ('band','crew'),\n            ('other','tour','cost'),\n            ('hotel','restaurant'),\n            ('other','travel','cost')\n        ]\n        cat_idxs = []\n        for toks in cat_tokens:\n            i = row_idx(*toks)\n            if i is not None:\n                cat_idxs.append(i)\n        checks = []\n        # Combined \u2248 TM + PC for key totals\n        for i in [idx_net_rev, idx_tot_exp, idx_net_inc]:\n            if i is not None:\n                tm = pnl.loc[i, tm_col]\n                pc = pnl.loc[i, pc_col]\n                tot = pnl.loc[i, tot_col]\n                if not np.isnan([tm,pc,tot]).any():\n                    checks.append(abs((tm+pc)-tot) <= 2.0)\n        # Total Expenses equals sum of category rows (for each column)\n        if idx_tot_exp is not None and len(cat_idxs) >= 2:\n            sum_tm = pnl.loc[cat_idxs, tm_col].apply(to_num).sum()\n            sum_pc = pnl.loc[cat_idxs, pc_col].apply(to_num).sum()\n            sum_tot = pnl.loc[cat_idxs, tot_col].apply(to_num).sum()\n            tm_ok = abs(sum_tm - pnl.loc[idx_tot_exp, tm_col]) <= 2.0\n            pc_ok = abs(sum_pc - pnl.loc[idx_tot_exp, pc_col]) <= 2.0\n            tot_ok = abs(sum_tot - pnl.loc[idx_tot_exp, tot_col]) <= 2.0\n            checks.extend([tm_ok, pc_ok, tot_ok])\n        # Net Revenue matches Revenue Detail net sum (Total Combined)\n        # Find revenue detail sheet and net USD column\n        rev_sheet = None\n        for name in sheets:\n            if 'revenue' in name.lower():\n                rev_sheet = name\n                break\n        if rev_sheet is not None and idx_net_rev is not None:\n            rev = pd.read_excel(path, sheet_name=rev_sheet)\n            if rev.shape[0] > 0:\n                rcols = [str(c) for c in rev.columns]\n                lrcols = [c.lower() for c in rcols]\n                def rcol_like(*tokens):\n                    for i,c in enumerate(lrcols):\n                        if all(t in c for t in tokens):\n                            return rcols[i]\n                    return None\n                net_col = rcol_like('net','usd') or rcol_like('net')\n                if net_col is not None:\n                    def to_num2(s):\n                        if pd.isna(s): return 0.0\n                        if isinstance(s,(int,float,np.integer,np.floating)): return float(s)\n                        s = str(s).replace(',','')\n                        s = re.sub(r'[^0-9.\\-]','',s)\n                        try: return float(s)\n                        except: return 0.0\n                    net_sum = rev[net_col].apply(to_num2).sum()\n                    net_summary = to_num(pnl.loc[idx_net_rev, tot_col])\n                    if not np.isnan(net_summary):\n                        checks.append(abs(net_sum - net_summary) <= max(5.0, 0.01*abs(net_summary)))\n        if not checks:\n            return 0.0\n        score = sum(1 for c in checks if c) / len(checks)\n        score = float(max(0.0, min(1.0, score)))\n        return score * weight, f\"Passed {sum(checks)}/{len(checks)} consistency checks.\"\n    except Exception as e:\n        return 0.0, f\"Error verifying P&L consistency: {e}\""}, {"type": "llm_judge", "name": "USD Conversion and Exchange Rate Sufficiency", "description": "Confirm that revenue figures are reported in USD; an Exchange Rate Log exists with Currency, USD Exchange Rate, Date, Source; and the methodology explains the FX conversion approach consistent with the as-of date.", "weight": 5.0, "judge_prompt": "Evaluate whether the workbook convincingly documents and applies USD conversion:\n\nCheck:\n- Presence of an Exchange Rate Log (likely on 'Assumptions & Methodology' or similarly named) with columns: Currency, USD Exchange Rate, Date, Source.\n- Methodology text (\u22653 sentences) clearly states how local revenue was converted to USD and references the as-of timing (the workbook header must show 'As of 12/31/2024').\n- In Revenue Detail, the monetary columns explicitly show USD (e.g., column headers or annotations). Values appear consistent with USD formatting.\n\nScoring:\n- 5.0: Clear Exchange Rate Log present with all four columns; methodology explicitly describes USD conversion and timing; Revenue Detail columns labeled and evidently in USD.\n- 3.5: Mostly present but one element is weak (e.g., methodology brief or missing timing reference) while conversion is still evident.\n- 2.0: Exchange Rate Log or methodology is incomplete/ambiguous; USD labeling is unclear.\n- 0.0: No Exchange Rate Log and no usable conversion methodology; currency basis unclear.", "expectation": "A clear, auditable USD conversion trail with an Exchange Rate Log and a sufficiently detailed methodology tied to the as-of date."}, {"type": "llm_judge", "name": "Scope, Withholding Documentation, and Net-From-Gross Behavior", "description": "Confirm October 2024 scope, withholding schedule documentation, and that net revenue is actually gross less withholding across sampled rows.", "weight": 5.0, "judge_prompt": "Evaluate scope and withholding application:\n\nCheck:\n- Revenue Detail covers the October 2024 tour period (rows indicate October 2024 dates or otherwise clearly limited to October 2024; itineraries may be illustrative but should indicate October 2024).\n- Assumptions/Methodology includes a withholding schedule documenting exactly: UK 20%, France 15%, Spain 24%, Germany 15.825%.\n- Sample at least three rows in Revenue Detail: confirm visually that Net Revenue (USD) is less than Gross Revenue (USD) and that a withholding amount is present, indicating subtraction of withholding, not addition.\n\nScoring:\n- 5.0: All elements present; dates clearly in October 2024; withholding schedule shows the exact four rates; sampled rows show net < gross with withholding amounts.\n- 3.5: Minor issues (e.g., one date omission or slight labeling ambiguity) but overall the scope and net-from-gross behavior is evident.\n- 2.0: Ambiguous period coverage or incomplete withholding schedule; net-from-gross behavior not consistently visible.\n- 0.0: No reliable indication of October 2024 scope and no usable withholding schedule; net vs gross behavior unclear.", "expectation": "A clearly documented October 2024 Revenue Detail, the specified withholding schedule, and visible net-from-gross subtraction in the detail."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Executive Readiness", "description": "Holistic quality assessment for professional presentation, traceability, and decision-usefulness for executives.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Executive Readability and Professional Formatting", "description": "Clarity and professionalism: labeled columns, aligned currency formatting in USD, clear headers, and logical layout.", "weight": 1.5, "judge_prompt": "Assess presentation quality:\n- Are headers clear and professional (includes '2024 Fall Music Tour' and 'As of 12/31/2024' on the summary)?\n- Are currency columns consistently formatted in USD with separators and sign conventions?\n- Is the workbook easy to read, with bold headers, adequate spacing, and logical sectioning?\n\nScoring: 1.5 excellent; 1.0 good with minor issues; 0.5 passable but cluttered/inconsistent; 0.0 poor.", "expectation": "Clearly labeled, cleanly formatted Excel with consistent USD formatting and professional visual hierarchy."}, {"type": "llm_judge", "name": "Decision-Usefulness for Executives", "description": "Does the summary meaningfully support performance evaluation and planning (e.g., clear totals, margins, and source breakdown that executives can consume quickly)?", "weight": 1.5, "judge_prompt": "Evaluate decision-usefulness:\n- Does the P&L Summary highlight the key figures (Net Revenue, Total Expenses, Net Income) clearly by source and combined total?\n- Is there sufficient aggregation and clarity to enable quick executive insight (e.g., totals at top/bottom, separation of revenue vs expenses, obvious net result)?\n- Are any helpful indicators present (e.g., subtotals, notes) without clutter?\n\nScoring: 1.5 strong executive clarity; 1.0 generally useful; 0.5 marginally useful; 0.0 not useful.", "expectation": "An executive-friendly summary that makes Net Revenue, Total Expenses, and Net Income obvious and comparable across sources."}, {"type": "llm_judge", "name": "Traceability and Auditability", "description": "Evaluate how easily one can trace summary figures to detail, and whether labels and references support auditability.", "weight": 1.5, "judge_prompt": "Assess traceability:\n- Can Net Revenue and Total Expenses on the summary be traced to the Revenue Detail and Expenses Detail (e.g., consistent labels, totals that correspond, sheet names that make sense)?\n- Are rows and columns clearly labeled to allow reconciliation?\n- Are any references or notes provided that guide a reviewer from summary to detail?\n\nScoring: 1.5 excellent traceability; 1.0 reasonable; 0.5 weak; 0.0 opaque.", "expectation": "A summary that clearly ties to detailed sheets with consistent labels and totals."}, {"type": "llm_judge", "name": "Assumptions Narrative Quality", "description": "Quality of the methodology/assumptions narrative for a professional finance audience.", "weight": 1.5, "judge_prompt": "Evaluate the assumptions/methodology narrative:\n- Is it at least three sentences and written for a professional finance audience?\n- Does it explicitly cover FX conversion approach, withholding application, data sources, and timing (as-of date) clearly?\n- Is the tone concise and appropriate for executives and auditors?\n\nScoring: 1.5 comprehensive and professional; 1.0 adequate; 0.5 minimal/unclear; 0.0 missing or unprofessional.", "expectation": "A concise, professional narrative that communicates methods, sources, and timing suitable for executives and auditors."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "62f04c2f-e0f7-4710-876c-54ee9c2e8256", "rubric": {"category_name": "Gravon Shoes Exchange Program Overview and Authorization Form", "rationale": "Mixed-output task: a one-page management-ready overview document (DOCX/PDF) plus an operational Excel form. Stage 1 mandates exact structural shape for both artifacts (LLM-only gate). Stage 2 verifies correctness and cross-linking with a mix of lightweight code checks (deterministic presence/pattern checks) and LLM judges (nuanced policy/process consistency). Stage 3 assesses professional quality, usability, and business value. Code rules carry ~5x less weight than LLM rules within Stage 2, reflecting precise but narrower checks.", "max_total_score": 15.6, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate", "description": "LLM-only gate to enforce exact output shape: a one-page overview document (PDF/DOCX) with specific sections and an Excel Exchange Authorization form with specific fields/sections. No correctness checks here\u2014only structure/presence and format.", "is_required": true, "max_points": 2.0, "min_score_to_pass": 1.8, "rules": [{"type": "llm_judge", "name": "Overview One-Pager Structure (DOCX/PDF)", "description": "Confirm a single-page professional overview document exists with required section headers and structural elements enabling verification.", "weight": 1.0, "judge_prompt": "You are evaluating whether the candidate produced a valid overview document for the Gravon Shoes Exchange Program with the exact structural requirements below. Only check presence/format, not correctness of content.\n\nFormat requirements:\n- The overview must be a PDF or DOCX file (not Excel, not plain text).\n- It should be exactly one page or clearly designed to fit on one page (minor spillover due to rendering margins is acceptable).\n- Professional layout with clear section headers.\n\nRequired visible sections/headers (flexible naming allowed if meaning is clear):\n1) Program Overview (or Executive Summary)\n2) Eligibility & Limits (e.g., credit-worthy customers, one exchange per season)\n3) Process Steps (how to initiate with an Exchange Authorization form, email to rep, approval with EA number, include form in box, write EA number on outside)\n4) Fees & Charges (freight paid by customer, $5 per pair restocking fee)\n5) Warehouse & Contact (must include full warehouse address and phone)\n6) Effective Date (date displayed)\n\nScoring:\n- 1.0: Valid PDF/DOCX, one page, and all 6 required sections are clearly present with headers or strong visual delineation.\n- 0.7: Valid format and one page, but missing exactly one of the required sections OR headers are present but one section is ambiguously labeled.\n- 0.4: Valid format and one page, but only 3\u20134 sections present.\n- 0.0: Not PDF/DOCX, not one page, or fewer than 3 required sections present.\n\nOnly evaluate presence/format, not content accuracy.", "expectation": "A one-page PDF/DOCX with clearly labeled sections covering overview, eligibility, process steps, fees, contact/warehouse, and effective date."}, {"type": "llm_judge", "name": "Exchange Authorization Form Structure (Excel)", "description": "Confirm an Excel spreadsheet exists for the Exchange Authorization form with top header fields, main table columns, bottom note, and signature spaces.", "weight": 1.0, "judge_prompt": "You are evaluating whether the candidate produced a valid Exchange Authorization form as an Excel file with the exact structural requirements below. Only check presence/format, not correctness of values.\n\nFormat requirements:\n- The form must be an Excel spreadsheet (.xlsx preferred).\n\nRequired top header fields (may appear as labeled cells or a header block; flexible naming allowed if meaning is clear):\n- Customer Name and Address\n- Phone Number\n- Customer Number\n- Exchange Authorization (EA) Number\n- Date of Return Request\n\nRequired main table columns (a single item table is fine as long as columns are present):\n- Style/Color\n- Style Name\n- Pairs Shipped\n- Pairs to be Returned\n\nRequired bottom elements:\n- Note stating customers must prepay freight and pay the restocking fee\n- Signature/approval section with spaces for: Sales Representative, GM (General Manager), and Sales Manager \u2014 each with Name, Signature, and Date fields/lines\n\nScoring:\n- 1.0: Excel file present with all top fields, all main table columns, and all bottom elements (note + all three roles with name/signature/date spaces).\n- 0.7: Excel file present with top fields and columns, but bottom section missing one element (either the note or one role\u2019s signature block incomplete).\n- 0.4: Excel file present but missing two or more required headers/columns OR missing most of the bottom section.\n- 0.0: Not an Excel spreadsheet or the structure is largely absent.\n\nOnly evaluate presence/format, not content accuracy or formulas.", "expectation": "An Excel form with the specified header fields, item table columns, bottom compliance note, and signature blocks for Sales Rep, GM, and Sales Manager."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Consistency Verification", "description": "Now that structure is present, verify policy correctness, operational consistency between overview and form, and presence of required data points. Mix of code-based checks for deterministic elements and LLM judges for nuanced consistency.", "is_required": false, "max_points": 7.6, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Overview Doc \u2014 Critical Policy Coverage", "description": "Checks the overview document text for required policy statements: exchange vs return, credits only for replacement, immediate replacement order at EA grant, credit-worthy eligibility, and one exchange per season.", "weight": 0.4, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    \\\"\\\"\\n    Args:\\n        workflow: Workflow object\\n        context: ValidationContext with .files accessor\\n    Returns:\\n        float or (float, str) up to weight 0.4\\n    \\\"\\\"\\n    weight = 0.4\\n\\n    # Find the overview document among all outputs\\n    doc_text = ''\\n    try:\\n        for res in context.get_all_outputs():\\n            if getattr(res, 'is_document', False) or getattr(res, 'is_text_format', False):\\n                # Try DOCX then PDF then text\\n                try:\\n                    t = context.files.read_docx_text(res.id)\\n                    if t: doc_text = t\\n                except Exception:\\n                    pass\\n                if not doc_text:\\n                    try:\\n                        t = context.files.read_pdf_text(res.id)\\n                        if t: doc_text = t\\n                    except Exception:\\n                        pass\\n                if not doc_text:\\n                    try:\\n                        t = context.files.read_text(res.id)\\n                        if t: doc_text = t\\n                    except Exception:\\n                        pass\\n                if doc_text:\\n                    break\\n    except Exception:\\n        pass\\n\\n    if not doc_text:\\n        return 0.0, 'No overview document text could be read.'\\n\\n    text = doc_text.lower()\\n\\n    checks = []\\n    # 1) Explicitly an exchange (not return)\\n    is_exchange = ('exchange program' in text or 'exchange authorization' in text or 'ea number' in text)\\n    not_return = ('not a return' in text or 'not a returns' in text or 'this is not a return' in text or 'no returns' in text)\\n    checks.append(is_exchange and not_return)\\n\\n    # 2) Credits only for replacement merchandise\\n    credits_replacement = bool(re.search(r'credit[s]?\\s+(can only|only)\\s+.*replace', text)) or ('credits can only be used for replacement' in text) or ('credit can only be used for replacement' in text)\\n    checks.append(credits_replacement)\\n\\n    # 3) Replacement order must be placed when EA is granted\\n    ea_replacement_order = (('replacement' in text and 'order' in text and ('exchange authorization' in text or 'ea number' in text) and ('granted' in text or 'issued' in text)))\\n    checks.append(ea_replacement_order)\\n\\n    # 4) Only credit-worthy customers\\n    creditworthy = bool(re.search(r'credit[-\\s]?worthy|creditworthy', text))\\n    checks.append(creditworthy)\\n\\n    # 5) One time per season\\n    one_time_per_season = bool(re.search(r'one[-\\s]?time\\s+per\\s+season', text))\\n    checks.append(one_time_per_season)\\n\\n    passed = sum(1 for c in checks if c)\\n    score = weight * (passed / len(checks))\\n    feedback = f\"Policy coverage checks passed {passed}/{len(checks)}.\"\\n    return score, feedback\\n"}, {"type": "code", "name": "Overview Doc \u2014 Process Steps Presence", "description": "Checks overview for presence of key operational steps: submit Exchange Authorization form to rep via email; rep returns form with EA number; include printed form in box; write EA number on outside of box.", "weight": 0.4, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    weight = 0.4\\n    # Extract overview text\\n    doc_text = ''\\n    try:\\n        for res in context.get_all_outputs():\\n            if getattr(res, 'is_document', False) or getattr(res, 'is_text_format', False):\\n                try:\\n                    t = context.files.read_docx_text(res.id)\\n                    if t: doc_text = t\\n                except Exception:\\n                    pass\\n                if not doc_text:\\n                    try:\\n                        t = context.files.read_pdf_text(res.id)\\n                        if t: doc_text = t\\n                    except Exception:\\n                        pass\\n                if not doc_text:\\n                    try:\\n                        t = context.files.read_text(res.id)\\n                        if t: doc_text = t\\n                    except Exception:\\n                        pass\\n                if doc_text:\\n                    break\\n    except Exception:\\n        pass\\n\\n    if not doc_text:\\n        return 0.0, 'No overview document text could be read.'\\n\\n    text = doc_text.lower()\\n\\n    checks = []\\n    # Submit EA form via email to assigned rep\\n    submit_form = ('exchange authorization form' in text) and (('email' in text and 'rep' in text) or 'emailed' in text)\\n    checks.append(submit_form)\\n\\n    # Rep returns form with EA number\\n    rep_returns_ea = bool(re.search(r'rep|representative', text)) and (('return the form' in text or 'returns the form' in text) or 'send back' in text) and (('exchange authorization number' in text) or ('ea number' in text))\\n    checks.append(rep_returns_ea)\\n\\n    # Include printed form in return shipment\\n    include_form = ('printed' in text or 'print' in text) and ('return shipment' in text or 'shipment' in text or 'box' in text) and ('form' in text)\\n    checks.append(include_form)\\n\\n    # Write EA number on outside of the box\\n    write_ea_on_box = (('outside' in text or 'outside of the box' in text) and (('exchange authorization number' in text) or ('ea number' in text)) and ('box' in text))\\n    checks.append(write_ea_on_box)\\n\\n    passed = sum(1 for c in checks if c)\\n    score = weight * (passed / len(checks))\\n    return score, f\"Process steps checks passed {passed}/{len(checks)}.\"\\n"}, {"type": "code", "name": "Overview Doc \u2014 Fees, Address/Phone, Effective Date", "description": "Verifies restocking fee and freight responsibility, warehouse address and phone, and effective date presence.", "weight": 0.4, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    weight = 0.4\\n    # Extract overview text\\n    doc_text = ''\\n    try:\\n        for res in context.get_all_outputs():\\n            if getattr(res, 'is_document', False) or getattr(res, 'is_text_format', False):\\n                try:\\n                    t = context.files.read_docx_text(res.id)\\n                    if t: doc_text = t\\n                except Exception:\\n                    pass\\n                if not doc_text:\\n                    try:\\n                        t = context.files.read_pdf_text(res.id)\\n                        if t: doc_text = t\\n                    except Exception:\\n                        pass\\n                if not doc_text:\\n                    try:\\n                        t = context.files.read_text(res.id)\\n                        if t: doc_text = t\\n                    except Exception:\\n                        pass\\n                if doc_text:\\n                    break\\n    except Exception:\\n        pass\\n\\n    if not doc_text:\\n        return 0.0, 'No overview document text could be read.'\\n\\n    text = doc_text.lower()\\n\\n    checks = []\\n    # Restocking fee $5 per pair and freight paid by customer\\n    fee = (bool(re.search(r'\\$?\\s*5(\\.00)?\\s*(per|/)\\s*pair', text)) and ('restocking fee' in text))\\n    freight_customer = ('freight' in text and (('prepay' in text) or ('paid by the customer' in text) or ('paid by customer' in text) or ('customer pays' in text)))\\n    checks.append(fee and freight_customer)\\n\\n    # Address and phone\\n    address_ok = ('555 waters avenue' in text and 'austin' in text and '78726' in text)\\n    phone_ok = bool(re.search(r'455[-\\s]?864[-\\s]?3867', text))\\n    checks.append(address_ok and phone_ok)\\n\\n    # Effective date (July 1, 2025 or 07/01/2025 variants)\\n    date_ok = ('july 1, 2025' in text) or bool(re.search(r'0?7\\s*/\\s*0?1\\s*/\\s*2025', text)) or bool(re.search(r'jul(y)?\\.?\\s*1\\s*,?\\s*2025', text))\\n    checks.append(date_ok)\\n\\n    passed = sum(1 for c in checks if c)\\n    score = weight * (passed / len(checks))\\n    return score, f\"Fees/contact/date checks passed {passed}/{len(checks)}.\"\\n"}, {"type": "code", "name": "Excel Form \u2014 Required Labels and Columns Present", "description": "Reads Excel form and checks for presence of key header labels, item columns, compliance note, and signature roles.", "weight": 0.4, "code": "import re\\nimport pandas as pd\\n\\ndef evaluate(workflow, context):\\n    weight = 0.4\\n    # Find spreadsheet\\n    xls_path = None\\n    try:\\n        for res in context.get_all_outputs():\\n            if getattr(res, 'is_spreadsheet', False):\\n                xls_path = context.files.get_path(res.id)\\n                break\\n    except Exception:\\n        pass\\n\\n    if not xls_path:\\n        return 0.0, 'No Excel form found.'\\n\\n    try:\\n        xf = pd.ExcelFile(xls_path)\\n        texts = []\\n        for sheet in xf.sheet_names:\\n            try:\\n                df = pd.read_excel(xls_path, sheet_name=sheet, header=None, dtype=str)\\n            except Exception:\\n                continue\\n            # Flatten to strings\\n            for val in df.values.flatten().tolist():\\n                if pd.notna(val):\\n                    texts.append(str(val).strip().lower())\\n        blob = '\\n'.join(texts)\\n    except Exception as e:\\n        return 0.0, f'Failed reading Excel: {e}'\\n\\n    def has_any(patterns):\\n        return any((p in blob) for p in patterns)\\n\\n    checks = []\\n    # Header fields\\n    header_ok = (has_any(['customer name']) and has_any(['address']) and has_any(['phone', 'phone number']) and has_any(['customer number', 'cust #', 'customer #']) and has_any(['exchange authorization number', 'ea number']) and has_any(['date of return request', 'return request date', 'date']))\\n    checks.append(header_ok)\n\\n    # Item columns\n    columns_ok = (has_any(['style/color', 'style / color', 'style- color', 'style color']) and has_any(['style name']) and has_any(['pairs shipped']) and has_any(['pairs to be returned', 'pairs returned']))\n    checks.append(columns_ok)\n\n    # Bottom note\n    note_ok = (('prepay' in blob or 'pre-paid' in blob or 'pre paid' in blob) and 'freight' in blob and 'restocking fee' in blob)\n    checks.append(note_ok)\n\n    # Signature roles\n    roles = [('sales representative','rep'), ('general manager','gm'), ('sales manager','sales manager')]\n    roles_ok = True\n    for long,label in roles:\n        role_present = (long in blob) or (label in blob)\n        sig_present = ('signature' in blob) and ('date' in blob) and ('name' in blob)\n        roles_ok = roles_ok and role_present and sig_present\n    checks.append(roles_ok)\n\n    passed = sum(1 for c in checks if c)\n    score = weight * (passed / len(checks))\n    return score, f\"Excel structure checks passed {passed}/{len(checks)}.\""}, {"type": "llm_judge", "name": "Policy Completeness (LLM)", "description": "Judge whether the overview accurately covers all policy requirements: exchange-only (not returns), credits only for replacements, credit-worthiness, one exchange per season, replacement order at EA grant, fees and responsibilities.", "weight": 2.0, "judge_prompt": "Evaluate the overview document (PDF/DOCX) for completeness of policy content. Check for the following REQUIRED elements:\n1) Clearly states this is an exchange program, not a return program\n2) Credits can only be used for replacement merchandise (no refunds)\n3) Only credit-worthy customers may participate\n4) Customers can exchange qualified merchandise one time per season\n5) Orders for replacement merchandise must be placed when the Exchange Authorization (EA) number is granted\n6) Fees: customer prepays freight AND a $5 per pair restocking fee charged to their account\n\nScoring:\n- 2.0: All 6 present and stated unambiguously\n- 1.4: One minor element is ambiguous or missing\n- 0.8: Two elements are ambiguous/missing\n- 0.0: Three or more elements missing/incorrect\nFocus on correctness and clarity of the required policies.", "expectation": "All six policy elements are explicitly and unambiguously included in the overview."}, {"type": "llm_judge", "name": "Process Consistency: Overview \u2194 Form", "description": "Judge whether the steps described in the overview align with the fields and instructions on the form, enabling the prescribed workflow.", "weight": 2.0, "judge_prompt": "Cross-check the overview document against the Excel form for process alignment. Look for:\n- The overview requires retailers to submit a Gravon Shoes Exchange Authorization form via email to the assigned sales rep; the form includes necessary fields for that submission (customer info, EA number, date)\n- The overview states the rep returns the form with an EA number; the form provides a space for the EA number\n- The overview instructs to include a printed copy of the form in the return shipment and to write the EA number on the outside of the box; the form includes clear identification (title, EA number field) that would support these steps\n- The overview mentions fees and responsibilities; the form notes that customers must prepay freight and pay the restocking fee\n\nScoring:\n- 2.0: Strong, consistent alignment across all bullets\n- 1.4: Mostly aligned with one minor gap\n- 0.8: Several gaps or ambiguities\n- 0.0: Major misalignment or missing core fields/instructions", "expectation": "Overview instructions and form fields/instructions align so the workflow is executable without contradictions."}, {"type": "llm_judge", "name": "Operational Specifics and Logistics", "description": "Judge whether operational details are precise and unambiguous: emailing to assigned rep, EA number handling, printed copy in box, EA number on box, warehouse address/phone, effective date present and clear.", "weight": 2.0, "judge_prompt": "Evaluate the clarity of operational/logistical instructions in the overview:\n- Email submission to the assigned sales rep is explicit (destination address/role clear)\n- The rep returns approval with an Exchange Authorization (EA) number\n- A printed copy of the EA form goes inside the box and the EA number is written on the outside of the box\n- Warehouse address (555 Waters Avenue, Austin, TX 78726) and phone (455-864-3867) are present and clearly labeled as the destination/contact\n- Effective date (July 1, 2025) is present and clearly labeled as effective date\n\nScoring:\n- 2.0: All items precise and unambiguous\n- 1.4: One item slightly unclear or missing\n- 0.8: Two items unclear/missing\n- 0.0: Three or more items unclear/missing", "expectation": "All specified logistics are explicitly covered with no ambiguity."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Usability", "description": "Holistic assessment of presentation quality, strategic value, audience appropriateness, and form usability. LLM-only.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Management-Ready One-Pager Quality", "description": "Professional tone, clarity, and scannability of the one-page overview (headings, bullets, brevity).", "weight": 1.5, "judge_prompt": "Assess the overview document\u2019s professional quality for a management audience:\n- Clear, concise one-page layout with meaningful headings and bullet points\n- Professional tone and formatting (consistent fonts, spacing, alignment)\n- Easy to scan and understand quickly (key points surfaced)\n\nScoring:\n- 1.5: Exemplary professional one-pager\n- 1.0: Generally professional with minor issues\n- 0.5: Adequate but several formatting/clarity issues\n- 0.0: Poorly formatted or hard to follow", "expectation": "A crisp, well-formatted one-page overview suitable for management review."}, {"type": "llm_judge", "name": "Business Impact and Rationale", "description": "Explains strategic benefits: faster processing, increased sales via updated inventory and full size runs, and removing dated merchandise.", "weight": 1.5, "judge_prompt": "Evaluate how well the overview articulates business value:\n- Communicates efficiency benefits (quick processing, organized workflow)\n- Highlights sales impact (updated styles, full size runs in stock)\n- Notes market hygiene (removing old/dated merchandise)\n\nScoring:\n- 1.5: Strong, explicit articulation of all benefits\n- 1.0: Covers most benefits with minor gaps\n- 0.5: Mentions benefits vaguely\n- 0.0: Little to no business rationale", "expectation": "Clear articulation of operational and commercial benefits tied to the program."}, {"type": "llm_judge", "name": "Audience Appropriateness and Actionability", "description": "Clarity for both management approval and field reps/retailers; instructions actionable with minimal ambiguity.", "weight": 1.5, "judge_prompt": "Judge whether content is appropriate and actionable for both management and reps/retailers:\n- Policy statements are unambiguous and enforceable\n- Steps are actionable with minimal assumptions\n- Language is accessible and avoids jargon\n\nScoring:\n- 1.5: Highly actionable and audience-appropriate\n- 1.0: Generally good with minor ambiguities\n- 0.5: Some confusion or missing actionable detail\n- 0.0: Not suited to the audience", "expectation": "Clear, actionable content suited to management and frontline sales reps."}, {"type": "llm_judge", "name": "Form Usability and Print-Readiness", "description": "Excel form\u2019s readability, logical flow, space for entries, and print-readiness.", "weight": 1.5, "judge_prompt": "Evaluate the Excel form for usability:\n- Logical layout with clear labels and sufficient space for entries\n- Supports multiple line items cleanly (rows under the item columns)\n- Print-friendly (reasonable margins, headings repeat if needed)\n\nScoring:\n- 1.5: Excellent usability and print-readiness\n- 1.0: Usable with minor layout issues\n- 0.5: Usable but notable layout/readability concerns\n- 0.0: Hard to use or confusing layout", "expectation": "A clean, easy-to-complete form suitable for on-screen use and printing."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "6241e678-4ba3-4831-b3c7-78412697febc", "rubric": {"category_name": "Production Schedule (B2B Live-Action Video) \u2014 Calendar PDF", "rationale": "This rubric enforces a self-documenting, verifiable calendar deliverable (PDF) for a 60-second B2B video production. Stage 1 mandates a precise, visual calendar structure that makes later checks trivial. Stage 2 mixes light code checks (anchors/coverage) with LLM verification of dependencies, rounds/reviews, workday adherence, and duration reasonableness. Stage 3 evaluates presentation quality and operational usefulness for department heads. Code rules are limited and flexible; LLM rules perform the nuanced evaluation.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement (Calendar PDF Gate)", "description": "Gate: The output must be a visual calendar exported to PDF with the required structural elements so that verification is possible.", "is_required": true, "max_points": 2.0, "min_score_to_pass": 1.5, "rules": [{"type": "llm_judge", "name": "Calendar PDF Structure & Timeframe", "description": "Verify the deliverable is a PDF export from a visual calendar tool that displays the full project timeline from Mon Jul 7, 2025 through Fri Aug 29, 2025 across month views.", "weight": 1.0, "judge_prompt": "You are validating only the STRUCTURE (not correctness) of the candidate output. Check: 1) The file is a PDF of a visual calendar or timeline (not a text document or spreadsheet), 2) It includes a clear monthly view or equivalent visual calendar layout for BOTH July 2025 and August 2025, 3) It visibly includes an event on Monday, July 7, 2025 for the Kickoff Call and a milestone on Friday, August 29, 2025 for Final Delivery, 4) It shows business days (weekdays) clearly and can display multiple items per day without hidden \"+X more\" truncation.\nScoring:\n- 1.0: PDF calendar view covering both months; kickoff (Jul 7) and final delivery (Aug 29) clearly shown; days laid out visibly with events shown (no hidden \"+X more\").\n- 0.7: PDF calendar with both months, kickoff and final milestones present, but minor structural issues (e.g., compressed days but still fully visible).\n- 0.4: PDF calendar present but only one month or one of the two anchor dates is missing.\n- 0.0: Not a PDF calendar (e.g., plain text/Word doc) or missing both months.", "expectation": "A two-month PDF calendar (or equivalent visual view) showing July and August 2025 with the kickoff and final delivery marked."}, {"type": "llm_judge", "name": "Color Coding & Legend Presence", "description": "Verify that color-coding is present and explained via a legend for phases and client tasks.", "weight": 1.0, "judge_prompt": "STRUCTURE CHECK ONLY. Confirm the calendar visually uses color-coding AND provides a legend or visible labeling that makes it clear which color corresponds to: 1) Pre-production, 2) Post-production (editing), 3) Graphics/design, and 4) Client tasks (reviews/approvals). Be flexible on exact names but the categories must be distinguishable and consistently colored.\nScoring:\n- 1.0: Clear, consistent color-coding with a legend or explicit labeling that maps colors to the three phases plus client tasks.\n- 0.7: Color-coding is present and consistent, but legend is missing; categories are still inferable.\n- 0.3: Colors appear but are inconsistent/unclear; categories cannot be reliably mapped.\n- 0.0: No color-coding apparent.", "expectation": "A visible legend or clear labeling mapping colors to phases and a distinct color for client tasks."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness & Compliance)", "description": "Now that the calendar shape is validated, verify correctness of scheduling logic, rounds/reviews, workday adherence, and that key items are present.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Dependencies & Sequencing Validity", "description": "Verify that schedule sequencing honors logical dependencies and industry norms.", "weight": 1.25, "judge_prompt": "Evaluate whether the schedule sequencing is logically valid for a one-day live-action shoot:\n- Scriptwriting and pre-production tasks occur before the shoot; casting/location/crew prep can overlap with scripting but still precede the shoot.\n- Shoot Day occurs once and is followed by Footage Ingest + Project Set Up, then Editing (3 rounds), then Client Final Approval of picture, THEN Audio Mixing and Color Grading.\n- A 1-day client review of audio/color is scheduled (can be same day as mixing/color), then Final Delivery on Aug 29, 2025.\n- Lock milestones (Lock Budget, Lock Cast, Lock Location, Lock Crew) occur before the shoot.\nScore higher if all these dependencies are met and the flow is coherent within the given dates. Provide brief feedback if deductions are made.", "expectation": "A coherent flow: pre-pro \u2192 shoot \u2192 ingest \u2192 edit rounds \u2192 client final approval \u2192 audio/color \u2192 final delivery."}, {"type": "llm_judge", "name": "Rounds and Client Reviews Compliance", "description": "Verify rounds for Script (2), Graphics (2), Edit (3), and required client review windows.", "weight": 1.25, "judge_prompt": "Check for compliance:\n- Script: exactly two rounds of revisions, with Client Script Review (2 days) and Client Script Approval.\n- Storyboard: delivery followed by 2-day Client Storyboard Review, then Client Storyboard Approval.\n- Graphics: two rounds of revisions, with 2-day Client Graphics Review and approval.\n- Editing: three distinct rounds (R1/R2/R3 or equivalent); after delivery of each round, at least a 2-day client review is scheduled (the prompt explicitly mentions two days upon delivery of edit round 1 and generally requires client edit reviews of 2 days).\nScore based on presence and correct placement of these review windows and approvals relative to deliveries. Provide brief feedback on any missing or misordered elements.", "expectation": "Two rounds for script and graphics, three for edit; 2-day client reviews after deliveries; approvals present."}, {"type": "llm_judge", "name": "Workdays-Only Adherence & Holidays", "description": "Confirm no scheduled work on weekends or US federal holidays in the covered period.", "weight": 1.25, "judge_prompt": "Verify that tasks are scheduled only on business days (Mon\u2013Fri). Ensure there are no events on weekends. Consider US federal holidays in the window: July\u2013August 2025 (Independence Day is Fri July 4, 2025; schedule begins Jul 7 so it should not include work on that holiday). Deduct if any weekend work is scheduled or holidays are used. Be lenient if the calendar displays weekends but has no tasks on them.", "expectation": "No work scheduled on weekends; no federal holiday work in the date range."}, {"type": "llm_judge", "name": "Duration Reasonableness vs. Constraints", "description": "Verify that durations allocate realistic working-day counts and fit within the timeline.", "weight": 1.25, "judge_prompt": "Evaluate whether the scheduled durations match guidance and fit between Jul 7 and Aug 29:\n- Budgeting \u22484 days; Scriptwriting total \u22486\u20137 working days across two rounds; Graphics total \u22486\u20137 working days across two rounds; Editing total \u224810\u201312 working days across three rounds; Storyboard \u22483 days; Casting Call \u22484 days; Location Scouting \u22484 days; Crew Hire \u22482 days; ingest/setup \u22481 day; audio \u22481 day; color \u22481 day. Client reviews typically 2 days each.\n- Overlaps are used appropriately to meet the deadline.\nScore higher if durations are within ranges and the entire plan is feasible and complete.", "expectation": "Durations align with given ranges; overlaps used appropriately to meet the Aug 29 deadline."}, {"type": "code", "name": "PDF Contains Key Anchors (Months, Dates, Milestones)", "description": "Deterministically confirm July/August 2025, kickoff, and final delivery anchors appear in the PDF text.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float or (float, str)\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output found.\"\n\n    text = \"\"\n    try:\n        if str(output.path).lower().endswith('.pdf'):\n            text = context.files.read_pdf_text(output.id)\n        elif str(output.path).lower().endswith('.docx'):\n            text = context.files.read_docx_text(output.id)\n        else:\n            # Attempt PDF read by default\n            text = context.files.read_pdf_text(output.id)\n    except Exception as e:\n        return 0.0, f\"Could not read document text: {e}\"\n\n    t = (text or \"\").lower()\n\n    # Month and year anchors\n    has_july = \"july\" in t\n    has_august = \"august\" in t or \"aug\" in t\n    has_2025 = \"2025\" in t\n\n    # Date anchors (be flexible on punctuation)\n    kickoff_date = (\"july 7\" in t) or (re.search(r\"jul\\s*7\\b\", t) is not None)\n    final_date = (\"august 29\" in t) or (\"aug 29\" in t) or (re.search(r\"aug\\s*29\\b\", t) is not None)\n\n    # Milestone labels\n    has_kickoff = (\"kickoff\" in t) or (\"kick-off\" in t)\n    has_final_delivery = \"final delivery\" in t\n\n    checks = [has_july, has_august, has_2025, kickoff_date, final_date, has_kickoff, has_final_delivery]\n    score = sum(1 for c in checks if c)\n    total = len(checks)\n    weight = 0.5\n    return weight * (score / total), f\"Anchors present: {score}/{total}.\""}, {"type": "code", "name": "Required Tasks Coverage (Keyword Match)", "description": "Check for presence of key task names in the PDF text. Scores proportionally based on coverage.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output found.\"\n\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0, \"Unable to read document text.\"\n\n    t = (text or \"\").lower()\n\n    required = [\n        \"internal creative workshopping\",\n        \"internal creative review\",\n        \"client pitch meeting\",\n        \"client pitch review\",\n        \"client pitch approval\",\n        \"budgeting\",\n        \"lock budget\",\n        \"scriptwriting\",\n        \"client script review\",\n        \"client script approval\",\n        \"storyboard\",\n        \"client storyboard review\",\n        \"client storyboard approval\",\n        \"graphics\",\n        \"client graphics review\",\n        \"client graphics approval\",\n        \"casting call\",\n        \"client casting review\",\n        \"client casting approval\",\n        \"location scouting\",\n        \"client location review\",\n        \"client location approval\",\n        \"crew hire\",\n        \"lock cast\",\n        \"lock location\",\n        \"lock crew\",\n        \"script to cast\",\n        \"reserve gear rental\",\n        \"prep call sheet\",\n        \"call sheet to crew\",\n        \"final preproduction tweaks\",\n        \"shoot day\",\n        \"footage ingest\",\n        \"project set up\",\n        \"editing\",\n        \"client edit reviews\",\n        \"client final approval\",\n        \"audio mixing\",\n        \"color grading\",\n        \"final delivery\"\n    ]\n\n    # Flexible matching: accept singular/plural and minor variations\n    def present(keyword: str) -> bool:\n        k = keyword\n        # Try variations for key phrases\n        variants = set([k])\n        if \"client\" in k:\n            variants.add(k.replace(\"client \", \"\"))\n        if k == \"project set up\":\n            variants.add(\"project setup\")\n            variants.add(\"ingest + project set up\")\n        if k == \"editing\":\n            variants.update([\"edit\", \"edit round\", \"edit r1\", \"edit r2\", \"edit r3\"])\n        if k == \"graphics\":\n            variants.update([\"graphic\", \"graphics round\", \"graphics r1\", \"graphics r2\"])        \n        if k == \"scriptwriting\":\n            variants.update([\"script\", \"script writing\"])        \n        # basic lowercase containment check\n        return any(v in t for v in variants)\n\n    hits = sum(1 for k in required if present(k))\n    total = len(required)\n    weight = 0.5\n    score = weight * (hits / total)\n    return score, f\"Matched {hits}/{total} required task keywords.\\nNote: Flexible variants accepted.\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality Assessment (Presentation & Utility)", "description": "Holistic quality and professional usefulness of the calendar for production management and resourcing.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Visual Clarity & Readability", "description": "Assess whether each day\u2019s tasks are clearly visible, legible, and not truncated.", "weight": 0.7, "judge_prompt": "Assess overall visual clarity: Are tasks fully visible on their days without hidden \"+X more\" truncation? Are date labels clear? Are event bars or blocks readable with unambiguous task names and rounds? Score higher if the layout is clean and legible when printed or shared as a PDF.", "expectation": "All tasks per day are visible and legible; no hidden truncations."}, {"type": "llm_judge", "name": "Operational Usefulness for Department Heads", "description": "Evaluate if the calendar is directly useful for scheduling and utilization planning.", "weight": 0.7, "judge_prompt": "Judge whether the calendar is actionable for department heads: clear ownership (team vs client), obvious milestones, adequate buffers for reviews, and logical overlaps that help staffing and revenue forecasting. Score higher if it is immediately usable for staffing and utilization planning.", "expectation": "Shows owners (team vs client), milestones, buffers, and overlaps helpful for staffing and forecasting."}, {"type": "llm_judge", "name": "Professional Polish & Consistency", "description": "Evaluate professional formatting, consistent color usage, labeling, and overall polish.", "weight": 0.6, "judge_prompt": "Assess professional presentation: consistent color use per phase and client tasks, clear legend, consistent labeling of rounds and approvals, clean typography and spacing, and correctly labeled timeframe (dates/months). Score higher for a polished, share-ready artifact.", "expectation": "Polished, consistent, share-ready calendar with clear legend and labels."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "5a2d70da-0a42-4a6b-a3ca-763e03f070a5", "rubric": {"category_name": "CNC Production Launch Tooling and Process Planning (Manufacturing - Mechanical Engineering)", "rationale": "This rubric enforces a self-documenting, two-workbook deliverable for a CNC production launch: a tooling BOM (Master Tool List) and a process plan (Manufacturing Steps). Stage 1 uses LLM-only gates to require precise spreadsheet structures that enable deterministic verification. Stage 2 mixes code checks (math, cross-references, budget/email, basic integrity) with LLM reasonableness checks (manufacturing sequence, tooling/workholding suitability, tax and sourcing transparency). Stage 3 evaluates professional quality, traceability, and risk planning.", "max_total_score": 10.0, "stages": [{"name": "Stage 1: Shape Enforcement Gate (LLM-only)", "description": "Verify the EXACT deliverable structure: two separate Excel workbooks with mandated sheets/sections/columns that enable deterministic verification. Conditional email document if over budget.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.0, "rules": [{"type": "llm_judge", "name": "Excel 1: Master Tool List - Structure Gate", "description": "Check that there is a dedicated Excel workbook for the Master Tool List with a clear table of items and a totals/tax section.", "weight": 1.5, "judge_prompt": "You are validating the STRUCTURE ONLY of the candidate outputs for a manufacturing tooling BOM (Master Tool List). Ignore calculation correctness; only check presence/format. The deliverables should include TWO separate Excel workbooks. Evaluate the workbook that is clearly the Master Tool List by its content.\n\nRequirements for the Master Tool List workbook (be flexible with sheet names like \"Master Tool List\", \"Tooling BOM\", etc.):\n- It must be an Excel file (XLSX preferred).\n- Contains at least one sheet with a single, main tabular list of items to purchase. The table should have the following columns (allow reasonable synonyms):\n  \u2022 Type (work holding, tool holder, cutting tool)\n  \u2022 Short Description (or Item/Description)\n  \u2022 Manufacturer\n  \u2022 Manufacturer Part Number (MFR P/N)\n  \u2022 Quantity (Qty)\n  \u2022 Cost Each (unit price)\n  \u2022 Cost Total (extended cost)\n  \u2022 Purchase Link (URL)\n- Each row should include a purchase link that looks like a valid URL.\n- A summary/totals section must be present on the sheet (or an adjacent section) with ALL of:\n  \u2022 Sub-Total (pre-sales tax)\n  \u2022 Sales Tax Rate labeled for Suffolk County, NY (accept variations like \"Sales Tax (Suffolk County, NY)\"), OR an explicit sales tax amount line that implies a rate\n  \u2022 Grand Total (post-sales tax)\n- The list should clearly include work holding, tool holders, and cutting tools needed for the Cover Plate first, with any extra quantities for future use optional but present if the author chose to include them.\n\nScoring:\n- 1.5: Excel workbook present AND all required columns AND purchase links AND subtotal/tax/grand total section present.\n- 1.0: Excel workbook present with table and links, but missing one element of the totals set (e.g., explicit tax rate line) OR missing exactly one required column.\n- 0.5: Excel workbook present but multiple required columns missing OR no purchase links OR no totals/tax section.\n- 0.0: Not an Excel file or no identifiable Master Tool List structure.\n\nOnly score structure and presence. Do not verify math or budget.", "expectation": "A clean Excel BOM table with mandatory columns, working-looking links, and a subtotal/tax/grand total section."}, {"type": "llm_judge", "name": "Excel 2: Cover Plate Manufacturing Steps - Structure Gate", "description": "Check that there is a second Excel workbook for manufacturing steps with a header block and an operations table using part-number references to the tool list.", "weight": 1.5, "judge_prompt": "You are validating the STRUCTURE ONLY of the candidate outputs for a Cover Plate manufacturing process plan. Identify the workbook that is clearly the Manufacturing Steps deliverable.\n\nRequirements for the Manufacturing Steps workbook (be flexible with sheet names like \"Cover Plate Steps\", \"Manufacturing Plan\", etc.):\n- It must be an Excel file (XLSX preferred), separate from the Master Tool List.\n- Header block (a small key\u2013value section at top or on a dedicated sheet) containing ALL of:\n  \u2022 Part Name\n  \u2022 Material Type\n  \u2022 Stock Size (in) \u2014 dimensions in inches (e.g., L \u00d7 W \u00d7 T)\n  \u2022 Number of Operations (count of setups/orientations)\n  \u2022 Part Manufacturing Volume (e.g., monthly quantity)\n  \u2022 Machine (name/model suggested by Integration Proposal)\n- An operations/steps table below the header with at least these columns (allow reasonable synonyms):\n  \u2022 Step Order Number\n  \u2022 Operation Number (per setup/orientation)\n  \u2022 Cutting Tool (MFR P/N)\n  \u2022 Tool Holder(s) (MFR P/Ns)\n- The \"Cutting Tool\" and \"Tool Holder(s)\" columns should reference items by manufacturer part numbers that correspond to the Master Tool List (not just generic names). Multiple holders may be listed per step.\n- At least 4 steps are listed.\n\nScoring:\n- 1.5: Excel workbook present with header containing all required fields and an operations table with all required columns referencing tools/holders by MFR P/N.\n- 1.0: Excel present with header and table, but missing exactly one header field or one required column OR uses names instead of P/Ns in one of the two reference columns.\n- 0.5: Excel present but multiple header fields/columns missing OR references are mostly generic names rather than P/Ns.\n- 0.0: Not an Excel file or no identifiable manufacturing-steps structure.\n\nOnly check for presence/format, not the correctness of the process or references.", "expectation": "A process workbook with a clear header and a steps table that references tool list items by manufacturer part numbers."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Verification (Correctness and Consistency)", "description": "Now that the outputs have the right shape, verify math and consistency with code rules and assess technical reasonableness with LLM judges.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Tool List Math and Tax Plausibility", "description": "Verify Cost Total \u2248 Quantity \u00d7 Cost Each per row, subtotal equals sum of Cost Totals, tax rate/grand total are plausible for Suffolk County, NY, or cleanly implied.", "weight": 0.3, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        # Helper: normalize column names\n        def norm(s):\n            return re.sub(r\"[^a-z0-9]+\", \" \", str(s).strip().lower())\n        def to_num(x):\n            if pd.isna(x):\n                return np.nan\n            s = str(x)\n            # Extract first numeric pattern (allow commas, $)\n            s = s.replace(',', '')\n            m = re.search(r\"[-+]?\\d*\\.?\\d+\", s)\n            if not m:\n                return np.nan\n            try:\n                return float(m.group(0))\n            except:\n                return np.nan\n        # Find spreadsheets\n        excel_res = [r for r in context.get_all_outputs() if getattr(r, 'is_spreadsheet', False)]\n        if not excel_res:\n            return 0.0, \"No spreadsheets found\"\n        # Classify: find Master Tool List by columns\n        mtl = None\n        mtl_df = None\n        mtl_raw = None\n        for r in excel_res:\n            try:\n                path = context.files.get_path(r.id)\n                xls = pd.ExcelFile(path)\n                # inspect first sheet that loads\n                for sheet in xls.sheet_names:\n                    df = pd.read_excel(path, sheet_name=sheet)\n                    cols = [norm(c) for c in df.columns]\n                    has_mpn = any('manufacturer part number' in c or 'mfr p n' in c or 'mpn' == c.strip() for c in cols)\n                    has_cost_each = any('cost each' in c or 'unit price' in c or 'unit cost' in c for c in cols)\n                    has_cost_total = any('cost total' in c or 'extended cost' in c or ('total' == c.strip()) for c in cols)\n                    has_type = any(c.startswith('type') for c in cols)\n                    has_link = any('purchase link' in c or 'url' in c or 'link' == c.strip() for c in cols)\n                    if sum([has_mpn, has_cost_each, has_cost_total, has_type, has_link]) >= 4:\n                        mtl = r\n                        mtl_df = df\n                        # raw with header=None for label scanning\n                        mtl_raw = pd.read_excel(path, sheet_name=sheet, header=None)\n                        raise StopIteration\n            except StopIteration:\n                break\n            except Exception:\n                continue\n        if mtl is None or mtl_df is None:\n            return 0.0, \"Could not locate Master Tool List workbook\"\n        # Map columns\n        ncols = {norm(c): c for c in mtl_df.columns}\n        # Identify key columns by fuzzy match\n        def find_col(keys):\n            for k in ncols:\n                for target in keys:\n                    if target in k:\n                        return ncols[k]\n            return None\n        col_qty = find_col(['quantity','qty'])\n        col_cost_each = find_col(['cost each','unit price','unit cost'])\n        col_cost_total = find_col(['cost total','extended cost']) or (find_col(['total']) if col_cost_each else None)\n        if not (col_qty and col_cost_each and col_cost_total):\n            return 0.0, \"Missing required cost/quantity columns for math checks\"\n        # Row-level extended price check\n        row_scores = []\n        for idx, row in mtl_df.iterrows():\n            q = to_num(row.get(col_qty))\n            ce = to_num(row.get(col_cost_each))\n            ct = to_num(row.get(col_cost_total))\n            if np.isnan(q) or np.isnan(ce) or np.isnan(ct):\n                continue\n            if q < 0 or ce < 0 or ct < 0:\n                continue\n            expected = q * ce\n            if expected == 0 and ct == 0:\n                ok = True\n            else:\n                ok = (abs(ct - expected) <= max(1.0, 0.02 * expected))\n            row_scores.append(1.0 if ok else 0.0)\n        row_ok = np.mean(row_scores) if row_scores else 0.0\n        # Compute subtotal from table\n        ct_vals = [to_num(v) for v in mtl_df[col_cost_total].values]\n        ct_vals = [v for v in ct_vals if not np.isnan(v) and v >= 0]\n        subtotal_calc = float(sum(ct_vals)) if ct_vals else 0.0\n        # Try to locate displayed subtotal, tax rate, grand total in raw sheet\n        def scan_label_value(raw_df, label_patterns):\n            best = None\n            for i in range(raw_df.shape[0]):\n                for j in range(raw_df.shape[1]):\n                    cell = raw_df.iat[i,j]\n                    s = str(cell).strip().lower()\n                    for pat in label_patterns:\n                        if pat in s:\n                            # check same row neighbors to the right\n                            for jj in range(j+1, min(j+4, raw_df.shape[1])):\n                                v = to_num(raw_df.iat[i,jj])\n                                if not np.isnan(v):\n                                    return v\n                            # or below cell\n                            for ii in range(i+1, min(i+4, raw_df.shape[0])):\n                                v = to_num(raw_df.iat[ii,j])\n                                if not np.isnan(v):\n                                    return v\n            return best\n        subtotal_disp = scan_label_value(mtl_raw, ['sub total','subtotal'])\n        tax_rate_disp = None\n        # look for an explicit percent-like value near tax labels\n        # try to get tax amount and/or rate\n        tax_amount_disp = scan_label_value(mtl_raw, ['sales tax','tax'])\n        # If a rate appears as a percent in text, infer from raw\n        # Also try to infer rate from grand vs subtotal\n        grand_total_disp = scan_label_value(mtl_raw, ['grand total','total post tax','post tax total'])\n        # Choose subtotal to use for plausibility\n        subtotal = subtotal_disp if (subtotal_disp is not None and subtotal_disp > 0) else subtotal_calc\n        plaus_ok = 0.0\n        if grand_total_disp and subtotal > 0:\n            implied_rate = grand_total_disp / subtotal - 1.0\n            # Suffolk County NY combined rate ~8.625% (0.08625). Accept [5%, 9.5%]\n            if 0.05 <= implied_rate <= 0.095:\n                plaus_ok = 1.0\n        elif tax_amount_disp and subtotal > 0:\n            implied_rate = tax_amount_disp / subtotal\n            if 0.05 <= implied_rate <= 0.095:\n                plaus_ok = 1.0\n        else:\n            # No explicit grand/tax; if table math was mostly correct, give partial credit\n            plaus_ok = 0.5 if row_ok >= 0.8 else 0.0\n        # Combine: row math 60%, tax/grand plausibility 40%\n        score = 0.3 * (0.6 * row_ok + 0.4 * plaus_ok)\n        feedback = f\"Row math ok ratio={row_ok:.2f}; tax/grand plausibility={plaus_ok:.2f}; subtotal_calc={subtotal_calc:.2f}.\"\n        return score, feedback\n    except Exception as e:\n        return 0.0, f\"Exception in math/tax check: {e}\""}, {"type": "code", "name": "Budget Compliance or Budget-Increase Email Present", "description": "Check if pre-tax or post-tax total is within $7,500. If over budget, verify a separate email document exists requesting an increase and explaining rationale.", "weight": 0.2, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        def to_num(x):\n            if x is None or (isinstance(x, float) and np.isnan(x)):\n                return np.nan\n            s = str(x).replace(',', '')\n            m = re.search(r\"[-+]?\\d*\\.?\\d+\", s)\n            return float(m.group(0)) if m else np.nan\n        def norm(s):\n            return re.sub(r\"[^a-z0-9]+\", \" \", str(s).strip().lower())\n        # Find Master Tool List workbook\n        excel_res = [r for r in context.get_all_outputs() if getattr(r, 'is_spreadsheet', False)]\n        mtl = None\n        mtl_raw = None\n        for r in excel_res:\n            try:\n                path = context.files.get_path(r.id)\n                xls = pd.ExcelFile(path)\n                for sheet in xls.sheet_names:\n                    df = pd.read_excel(path, sheet_name=sheet)\n                    cols = [norm(c) for c in df.columns]\n                    if any('manufacturer part number' in c or 'mfr p n' in c for c in cols) and any('cost each' in c or 'unit price' in c for c in cols):\n                        mtl = r\n                        mtl_raw = pd.read_excel(path, sheet_name=sheet, header=None)\n                        raise StopIteration\n            except StopIteration:\n                break\n            except Exception:\n                continue\n        if mtl is None or mtl_raw is None:\n            return 0.0, \"Master Tool List workbook not found\"\n        # Scan for subtotal and grand total\n        def scan_label_value(raw_df, label_patterns):\n            for i in range(raw_df.shape[0]):\n                for j in range(raw_df.shape[1]):\n                    s = str(raw_df.iat[i,j]).strip().lower()\n                    for pat in label_patterns:\n                        if pat in s:\n                            # rightwards\n                            for jj in range(j+1, min(j+4, raw_df.shape[1])):\n                                v = to_num(raw_df.iat[i,jj])\n                                if not np.isnan(v):\n                                    return v\n                            # downwards\n                            for ii in range(i+1, min(i+4, raw_df.shape[0])):\n                                v = to_num(raw_df.iat[ii,j])\n                                if not np.isnan(v):\n                                    return v\n            return None\n        subtotal = scan_label_value(mtl_raw, ['sub total','subtotal'])\n        grand_total = scan_label_value(mtl_raw, ['grand total','total post tax','post tax total'])\n        # If not found, try to compute subtotal from table (fallback)\n        if subtotal is None:\n            # try to find a table area by locating a 'cost total' column row-wise (rough heuristic)\n            try:\n                df_guess = pd.read_excel(context.files.get_path(mtl.id))\n                if 'Cost Total' in df_guess.columns:\n                    subtotal = float(pd.to_numeric(df_guess['Cost Total'], errors='coerce').fillna(0).sum())\n            except Exception:\n                pass\n        # Decide budget compliance: accept either pre- or post-tax within budget\n        budget = 7500.0\n        within = False\n        if grand_total is not None and grand_total <= budget + 1e-6:\n            within = True\n        if subtotal is not None and subtotal <= budget + 1e-6:\n            within = True\n        if within:\n            return 0.2, \"Within budget threshold (pre-tax or post-tax).\"\n        # Over budget: look for an email document requesting increase\n        docs = [r for r in context.get_all_outputs() if getattr(r, 'is_document', False) or getattr(r, 'is_text_format', False)]\n        found_email = False\n        for d in docs:\n            try:\n                text = ''\n                if hasattr(d, 'is_document') and d.is_document:\n                    if str(d.file_ext).lower().endswith('.pdf'):\n                        text = context.files.read_pdf_text(d.id)\n                    else:\n                        text = context.files.read_docx_text(d.id)\n                else:\n                    text = context.files.read_text(d.id)\n                t = text.lower()\n                # Look for email-like structure and request\n                if ('budget' in t and ('increase' in t or 'additional' in t)) and (('$' in t) or ('usd' in t) or re.search(r\"\\b\\d{4,}\\b\", t)):\n                    # Reason explanation keywords\n                    if any(k in t for k in ['reason', 'justif', 'lead time', 'quantity', 'tool breakage', 'tax']):\n                        found_email = True\n                        break\n            except Exception:\n                continue\n        if found_email:\n            return 0.2, \"Over budget but budget-increase request email present with rationale.\"\n        return 0.0, \"Over budget and no budget-increase email found.\"\n    except Exception as e:\n        return 0.0, f\"Exception in budget/email check: {e}\""}, {"type": "code", "name": "Steps \u2194 Tool List Cross-Reference (MFR P/N coverage)", "description": "All Cutting Tool and Tool Holder MFR P/Ns referenced in the steps must exist in the Master Tool List.", "weight": 0.3, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        def norm(s):\n            return re.sub(r\"[^a-z0-9]+\", \"\", str(s).strip().lower())\n        # Find spreadsheets\n        excels = [r for r in context.get_all_outputs() if getattr(r, 'is_spreadsheet', False)]\n        mtl = None; steps = None\n        mtl_df = None; steps_df = None\n        for r in excels:\n            try:\n                path = context.files.get_path(r.id)\n                xls = pd.ExcelFile(path)\n                for sh in xls.sheet_names:\n                    df = pd.read_excel(path, sheet_name=sh)\n                    cols = [c.strip().lower() for c in df.columns]\n                    if any('manufacturer part number' in c or 'mfr p/n' in c or 'mpn' == c for c in cols):\n                        if any('cost each' in c or 'unit price' in c for c in cols):\n                            mtl = r; mtl_df = df\n                            break\n                if mtl is None:\n                    # maybe different sheet layout; continue\n                    pass\n            except Exception:\n                continue\n        for r in excels:\n            if mtl and r.id == mtl.id:\n                continue\n            try:\n                path = context.files.get_path(r.id)\n                xls = pd.ExcelFile(path)\n                for sh in xls.sheet_names:\n                    df = pd.read_excel(path, sheet_name=sh)\n                    cols = [c.strip().lower() for c in df.columns]\n                    if any('step order' in c or 'step' == c for c in cols) and any('operation' in c for c in cols) and any('cutting tool' in c for c in cols):\n                        steps = r; steps_df = df\n                        break\n            except Exception:\n                continue\n        if mtl_df is None or steps_df is None:\n            return 0.0, \"Could not locate both Master Tool List and Steps tables\"\n        # Build set of MPNs from tool list\n        mpn_col = None\n        for c in mtl_df.columns:\n            if 'manufacturer' in c.lower() and ('part' in c.lower() and 'number' in c.lower()):\n                mpn_col = c; break\n            if 'mfr' in c.lower() and 'p' in c.lower():\n                mpn_col = c; break\n            if c.strip().lower() in ['mpn','pn','p/n']:\n                mpn_col = c; break\n        if mpn_col is None:\n            return 0.0, \"MPN column not found in Master Tool List\"\n        mpns = set()\n        for v in mtl_df[mpn_col].astype(str).tolist():\n            if v and v.strip():\n                mpns.add(norm(v))\n        # Extract referenced MPNs from steps\n        def find_col(df, keys):\n            for c in df.columns:\n                cl = c.lower()\n                for k in keys:\n                    if k in cl:\n                        return c\n            return None\n        cut_col = find_col(steps_df, ['cutting tool'])\n        hold_col = find_col(steps_df, ['tool holder'])\n        if cut_col is None or hold_col is None:\n            return 0.0, \"Required steps columns not found\"\n        refs = []\n        seps = r\"[;,/\\n\\|]+\"\n        for c in [cut_col, hold_col]:\n            for v in steps_df[c].astype(str).tolist():\n                if not v or v.lower() in ['nan','none']:\n                    continue\n                parts = re.split(seps, v)\n                for p in parts:\n                    p2 = norm(p)\n                    if p2:\n                        refs.append(p2)\n        if not refs:\n            return 0.0, \"No MPN references found in steps\"\n        hits = sum(1 for rmpn in refs if rmpn in mpns)\n        coverage = hits / max(1, len(refs))\n        # Score proportionally but cap at weight\n        score = 0.3 * coverage\n        return score, f\"MPN coverage {coverage:.2%} ({hits}/{len(refs)} refs found)\"\n    except Exception as e:\n        return 0.0, f\"Exception in cross-reference check: {e}\""}, {"type": "code", "name": "Operations Count and Step Order Consistency", "description": "Check that the number of unique Operation Numbers matches the declared Number of Operations (if present) and that Step Order increases monotonically.", "weight": 0.2, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        def to_int(x):\n            try:\n                return int(float(str(x).strip()))\n            except:\n                return None\n        def norm(s):\n            return re.sub(r\"[^a-z0-9]+\", \" \", str(s).strip().lower())\n        # Find Steps workbook\n        steps_res = None\n        steps_df = None\n        excels = [r for r in context.get_all_outputs() if getattr(r, 'is_spreadsheet', False)]\n        for r in excels:\n            try:\n                path = context.files.get_path(r.id)\n                xls = pd.ExcelFile(path)\n                for sh in xls.sheet_names:\n                    df = pd.read_excel(path, sheet_name=sh)\n                    cols = [norm(c) for c in df.columns]\n                    if any('step order' in c or 'step' == c for c in cols) and any('operation' in c for c in cols):\n                        steps_res = r; steps_df = df; break\n            except Exception:\n                continue\n        if steps_df is None:\n            return 0.0, \"Steps workbook/table not found\"\n        # Identify columns\n        ncols = {norm(c): c for c in steps_df.columns}\n        step_col = None\n        op_col = None\n        for k, c in ncols.items():\n            if step_col is None and ('step order' in k or k == 'step'):\n                step_col = c\n            if op_col is None and 'operation' in k:\n                op_col = c\n        if step_col is None or op_col is None:\n            return 0.0, \"Required columns not found\"\n        # Monotonic step order check\n        steps_vals = [to_int(v) for v in steps_df[step_col].tolist() if str(v).strip() != '']\n        steps_vals = [v for v in steps_vals if v is not None]\n        mono_ok = 1.0\n        for i in range(1, len(steps_vals)):\n            if steps_vals[i] < steps_vals[i-1]:\n                mono_ok = 0.0; break\n        # Unique operation count\n        ops_vals = [to_int(v) for v in steps_df[op_col].tolist() if str(v).strip() != '']\n        ops_vals = [v for v in ops_vals if v is not None]\n        uniq_ops = len(set(ops_vals))\n        # Try to read declared Number of Operations from header region by raw read\n        declared_ops = None\n        try:\n            path = context.files.get_path(steps_res.id)\n            # read first sheet raw\n            raw = pd.read_excel(path, header=None)\n            for i in range(min(30, raw.shape[0])):\n                for j in range(min(8, raw.shape[1])):\n                    s = str(raw.iat[i,j]).strip().lower()\n                    if s and ('number of operations' in s or 'operations' == s):\n                        # neighbor right or below\n                        for jj in range(j+1, min(j+4, raw.shape[1])):\n                            v = raw.iat[i,jj]\n                            vi = None\n                            try:\n                                vi = int(float(str(v)))\n                            except:\n                                vi = None\n                            if vi is not None:\n                                declared_ops = vi; break\n                        if declared_ops is None:\n                            for ii in range(i+1, min(i+4, raw.shape[0])):\n                                v = raw.iat[ii,j]\n                                try:\n                                    declared_ops = int(float(str(v)))\n                                except:\n                                    declared_ops = None\n                                if declared_ops is not None:\n                                    break\n                        break\n        except Exception:\n            pass\n        match_ok = 1.0\n        if declared_ops is not None and uniq_ops > 0:\n            match_ok = 1.0 if abs(declared_ops - uniq_ops) <= 0 else 0.0\n        # Score: 50% monotonic, 50% ops count match (or partial if no header)\n        if declared_ops is None:\n            score = 0.2 * (0.5 * mono_ok + 0.5 * 0.8)  # partial credit if monotonic\n            fb = f\"Monotonic={mono_ok:.0f}; Declared ops not found; unique ops={uniq_ops}\"\n        else:\n            score = 0.2 * (0.5 * mono_ok + 0.5 * match_ok)\n            fb = f\"Monotonic={mono_ok:.0f}; Declared={declared_ops}; Unique={uniq_ops}; Match={match_ok:.0f}\"\n        return score, fb\n    except Exception as e:\n        return 0.0, f\"Exception in operations/step order check: {e}\""}, {"type": "llm_judge", "name": "Manufacturing Plan Coherence and Completeness", "description": "Assess if the operations sequence is reasonable for a plate component and covers necessary machining actions and setups.", "weight": 1.4, "judge_prompt": "Evaluate the Manufacturing Steps workbook for technical reasonableness of the process plan (not just structure). Consider typical operations for a plate-like component:\n- Reasonable setup plan (e.g., OP10 top rough/facing, OP20 flip, milling contours/slots, hole-making, chamfer/deburr) and any required stock prep.\n- Sequence should minimize re-clamps and consider datum strategy and orientation changes.\n- Includes necessary finalization steps (deburr/chamfer, edge break; optional inspection checkpoints acceptable).\n- The declared Number of Operations should make sense given the listed steps.\n\nScoring guidance:\n- 1.4: Sequence is coherent, covers all obvious actions for a cover plate, and setups/orientations are sensible.\n- 0.9: Mostly coherent with minor gaps (e.g., missing explicit deburr) or slightly suboptimal ordering.\n- 0.5: Major omissions (e.g., no hole-making despite obvious need) or confusing setup logic.\n- 0.0: Steps are incoherent or unrelated to a plate machining process.", "expectation": "A sound, stepwise plan with logical setups, covering facing, contouring, holes, and finishing for a cover plate."}, {"type": "llm_judge", "name": "Tooling and Holder Suitability for Material", "description": "Check that listed cutting tools/holders are appropriate for the stated material and operations.", "weight": 1.3, "judge_prompt": "Review both the Master Tool List and the Manufacturing Steps. Judge whether the cutting tools and holders are appropriate for the stated material (e.g., aluminum, stainless steel, titanium) and the described operations.\nConsider:\n- Presence of reasonable end mills, drills, spot/chamfer tools; sizes roughly compatible with features implied by a typical cover plate.\n- Tool materials/coatings (e.g., carbide, TiAlN/TiB2 for hard materials or Al alloys) make sense.\n- Holder choices (ER collets, sidelock/solid endmill holders, shell mill arbors, drill chucks) are consistent with tools used, and quantities account for multiple operations and potential breakage.\n- No obvious mismatches (e.g., HSS-only plan for tough stainless without justification).\n\nScoring guidance:\n- 1.3: Strong fit across tools/holders/material with adequate redundancy/spares.\n- 0.8: Generally correct with minor mismatches or thin redundancy.\n- 0.4: Several mismatches or missing key tools/holders.\n- 0.0: Tools/holders clearly unsuitable for the material/operations.", "expectation": "Appropriate carbide/coatings and holder mix with realistic quantities for the stated material."}, {"type": "llm_judge", "name": "Workholding Strategy Viability", "description": "Assess whether the workholding approach is feasible for a flat plate part and aligns with purchased items.", "weight": 0.8, "judge_prompt": "Assess the feasibility of the workholding approach for a thin/flat plate (cover plate). Consider:\n- Use of vise with parallels/soft jaws, fixture plate with modular clamps, or similar plate-appropriate methods.\n- Steps indicate how the part is located and clamped for each orientation and how flip operations are handled.\n- The required workholding devices referenced in steps exist in the Master Tool List.\n- Any reliance on the provided standard clamp set is reasonable, with additional purchases listed when needed.\n\nScoring guidance:\n- 0.8: Clear, viable workholding for each setup, references exist in Master Tool List.\n- 0.5: Generally viable but with gaps or missing references.\n- 0.2: Weak or risky approach, unclear flip/locating strategy.\n- 0.0: Infeasible or absent workholding plan.", "expectation": "A realistic vise/fixture/clamp strategy with traceable items to the tool list."}, {"type": "llm_judge", "name": "Tax and Sourcing Transparency", "description": "Check that the Master Tool List documents sales tax context and uses credible purchase links/manufacturers.", "weight": 0.5, "judge_prompt": "Review the Master Tool List for sourcing transparency:\n- Sales tax context for Suffolk County, NY is documented (rate or tax amount clearly indicated as local tax).\n- Purchase links look credible and specific (product pages, not generic homepages), and manufacturers/sellers are reputable.\n- Optional but positive: brief notes on lead time/availability.\n\nScoring guidance:\n- 0.5: Clear tax labeling and credible links; sourcing feels trustworthy.\n- 0.3: Mostly clear but some generic links or vague tax labeling.\n- 0.1: Sparse sourcing info or unclear tax labeling.\n- 0.0: No tax mention and links appear non-credible.", "expectation": "Transparent tax labeling and solid, specific sourcing links."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3: Quality Assessment", "description": "Holistic review of presentation quality, traceability, and risk planning across both workbooks (and email if present).", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Clarity", "description": "Formatting, clear headers, units, consistent terminology, and overall readability.", "weight": 0.7, "judge_prompt": "Assess both Excel workbooks for professional presentation:\n- Clear headers and units (e.g., inches, qty, USD), consistent terminology and P/N formatting.\n- Clean table formatting, readable column widths, freeze panes or filters helpful but optional.\n- Avoids clutter; information is easy to follow.\n\nScoring: 0.7 excellent polish; 0.4 acceptable with minor issues; 0.2 messy; 0.0 poor.", "expectation": "Clean, readable spreadsheets with consistent labels and units."}, {"type": "llm_judge", "name": "Traceability and Reuse", "description": "Items in tool list are easily traceable from steps; structure supports reuse for future parts.", "weight": 0.7, "judge_prompt": "Evaluate traceability and reuse:\n- The steps reference MFR P/Ns that are unambiguous in the tool list.\n- Optional but positive: a simple ID/key that ties steps to tool list entries.\n- The tool list groups items logically (workholding, holders, cutters), facilitating future reuse.\n\nScoring: 0.7 strong traceability and organization; 0.4 partial; 0.2 weak; 0.0 none.", "expectation": "Unambiguous references and logical grouping for future reuse."}, {"type": "llm_judge", "name": "Risk and Contingency Planning", "description": "Allowance for tool breakage, spares, and production reliability; optional inspection/verification steps.", "weight": 0.6, "judge_prompt": "Assess whether the plan anticipates production realities:\n- Quantities include spares for common breakage/consumables.\n- Steps include finishing/edge-break and optionally inspection points.\n- If budget is tight, a sensible trade-off rationale is present (or addressed in the email).\n\nScoring: 0.6 strong risk planning; 0.3 moderate; 0.1 minimal; 0.0 none.", "expectation": "Pragmatic spares/contingencies and finishing/inspection considerations."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "e222075d-5d62-4757-ae3c-e34b0846583b", "rubric": {"category_name": "Broadcast Spot Edit: \u201cSupport Green Energy\u201d (30s)", "rationale": "This rubric enforces a self-documenting workflow for a 30-second H.264 broadcast spot. Stage 1 (LLM-only) mandates a precise bundle: the MP4 plus verifiable documentation (shot list, VO/script alignment, stock asset log with direct links, and delivery specs). Stage 2 mixes code and LLM checks to validate correctness against requirements: time-structure and placements (graphics cards), asset coverage, delivery specs, music selection/pacing, and narrative alignment. Stage 3 applies holistic LLM quality judgments on editorial craft, visual curation/representation, audio mix and intelligibility, and persuasive impact/CTA clarity.", "max_total_score": 30.0, "stages": [{"name": "Stage 1 \u2013 Deliverables Shape Gate (LLM-only)", "description": "Gate that enforces the exact output bundle and structure needed for verification. If the structured artifacts are missing or malformed, downstream verification is impossible.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Required Bundle Present (MP4 + Tech Docs)", "description": "Verify the candidate delivers: (1) a 1920x1080 H.264 .mp4 as the primary output; (2) a Delivery/Tech Specs document (PDF/DOCX/MD) stating exact duration (30.0s), frame rate, resolution 1920x1080, codec H.264, audio 48kHz stereo, and a note that a scratch VO and classical-style music were used.", "weight": 3.0, "judge_prompt": "You are evaluating the STRUCTURE ONLY. Review the outputs. Confirm the following are present:\n\n1) Primary video deliverable: an H.264 .mp4 intended for broadcast review. It should be indicated (by filename and/or delivery report) as 1920x1080 and exactly 30 seconds long.\n2) A Delivery/Tech Specs document (PDF, DOCX, or Markdown) that explicitly lists: resolution (1920x1080), codec (H.264/AVC), container (MP4), frame rate (e.g., 23.976/24/29.97/30), audio (48kHz stereo), exact duration (30.0s or 00:30), presence of scratch VO, and that the music is classical-style.\n\nScoring:\n- 1.0: MP4 present AND Delivery/Tech Specs doc present with all items listed clearly.\n- 0.7: MP4 present AND Specs doc present but missing up to two required fields.\n- 0.4: MP4 present but Specs doc missing or too vague; or Specs doc present but MP4 missing.\n- 0.0: MP4 missing AND no Specs doc.\n\nOnly check presence/structure, not correctness of content.", "expectation": "A cohesive bundle with the MP4 plus a clearly itemized delivery/spec sheet."}, {"type": "llm_judge", "name": "Structured Logs: Shot List + VO Alignment", "description": "Verify presence and structure of a timecoded Shot List spreadsheet and a VO/Script Alignment artifact tying script lines to timecodes, including explicit entries for the two required graphics card lines.", "weight": 3.0, "judge_prompt": "Check for these structured artifacts:\n\nA) Shot List (spreadsheet preferred: CSV/XLSX) with a timeline table including columns similar to: [Index/Order, Start/TC In, End/TC Out, Duration, Type (e.g., B-roll/Graphic), Description/Notes]. It should cover the full 30s and include at least two rows marked as graphic cards.\nB) VO/Script Alignment (spreadsheet or document) mapping script lines to timecodes. It must explicitly include the two phrases:\n   - \u201cRenewable, reliable, green energy projects will create jobs\u201d\n   - \u201cUrge your legislator to support green energy in California\u201d\n\nScoring:\n- 1.0: Shot List AND VO Alignment both present, clearly structured, with timecodes and both required lines included. Shot List has graphic card rows.\n- 0.7: Both artifacts present but minor structural gaps (e.g., missing a column or unclear labeling) OR only one of the two lines explicitly shown.\n- 0.4: Only one artifact present (Shot List or VO Alignment) OR both present but too unstructured to verify.\n- 0.0: Neither artifact present.\n\nDo not assess correctness; only presence and structural completeness.", "expectation": "A timecoded shot list covering the full timeline and a VO/script timing reference capturing the two specified lines."}, {"type": "llm_judge", "name": "Stock Asset Log with Direct Links", "description": "Verify presence and structure of a stock asset log with direct URLs to footage and music previews. Must not include purchased/licensed media; preview/watermarked entries are acceptable.", "weight": 2.0, "judge_prompt": "Confirm there is a Stock Asset Log (CSV/XLSX preferred) listing all stock footage and music assets used. Structure should resemble columns: [Type (Video/Music), Description, Source Platform, Direct URL, License/Status (Preview/Watermarked), Clip ID/Code, In, Out, Usage Notes]. Check that at least one MUSIC row is present with a classical-style track noted.\n\nScoring:\n- 1.0: Asset log present with clear columns and direct URLs for all/most items; includes at least one music row; licensing noted as previews/watermarked.\n- 0.6: Asset log present but missing some columns or a few direct links; music entry unclear.\n- 0.2: Asset list present but unstructured (e.g., pasted list without columns) or mostly missing links.\n- 0.0: No asset log present.\n\nOnly evaluate presence/structure, not content quality.", "expectation": "A spreadsheet-like listing with direct links for footage and music, marked as preview/watermarked."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification (Correctness)", "description": "With the shape enforced, verify correctness and requirement compliance via deterministic code checks and LLM cross-validation.", "is_required": false, "max_points": 14.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Shot List Timing and Graphics Cards", "description": "Verify the shot list sums to ~30s, maintains medium-high pace (avg shot length <= 3s), and contains at least two Graphic/Card entries.", "weight": 0.8, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    # Normalized score in [0,1]\n    def parse_time(t):\n        if pd.isna(t):\n            return None\n        if isinstance(t, (int, float)):\n            return float(t)\n        s = str(t).strip()\n        # Common forms: mm:ss, m:ss, hh:mm:ss, hh:mm:ss:ff, mm:ss.ff\n        if re.match(r\"^\\d{1,2}:\\d{2}(\\.\\d+)?$\", s):\n            mm, ss = s.split(\":\")\n            return float(mm)*60 + float(ss)\n        parts = s.split(\":\")\n        try:\n            if len(parts) == 3:\n                h, m, sec = parts\n                return int(h)*3600 + int(m)*60 + float(sec)\n            if len(parts) == 4:\n                # assume frames at ~30fps\n                h, m, sec, fr = parts\n                return int(h)*3600 + int(m)*60 + int(sec) + int(fr)/30.0\n            # Fallback: extract float seconds\n            return float(s)\n        except:\n            return None\n    \n    # Find a likely shot list (spreadsheet or CSV with time columns)\n    outputs = context.get_all_outputs()\n    candidate = None\n    for r in outputs:\n        if r.is_spreadsheet:\n            candidate = r\n            break\n        if str(getattr(r, 'name', '')).lower().endswith('.csv'):\n            candidate = r\n            break\n    if not candidate:\n        return 0.0\n    \n    # Read table\n    df = None\n    try:\n        if str(getattr(candidate, 'name', '')).lower().endswith('.csv') or getattr(candidate, 'mime', '') == 'text/csv':\n            df = context.files.read_csv(candidate.id)\n        else:\n            # Try first sheet\n            path = context.files.get_path(candidate.id)\n            xls = pd.ExcelFile(path)\n            sheet = xls.sheet_names[0]\n            df = pd.read_excel(path, sheet_name=sheet)\n    except:\n        return 0.0\n    if df is None or df.empty:\n        return 0.0\n    \n    # Normalize columns\n    cols = [str(c).strip().lower() for c in df.columns]\n    df.columns = cols\n\n    # Identify time columns\n    start_cols = [c for c in cols if any(k in c for k in ['start', 'in', 'tc in', 'in_tc', 'tc_in'])]\n    end_cols = [c for c in cols if any(k in c for k in ['end', 'out', 'tc out', 'out_tc', 'tc_out'])]\n    dur_cols = [c for c in cols if any(k in c for k in ['dur', 'duration', 'length'])]\n\n    durations = []\n    if dur_cols:\n        for v in df[dur_cols[0]].values:\n            durations.append(parse_time(v))\n    else:\n        # Try compute from start/end\n        if start_cols and end_cols:\n            s_col, e_col = start_cols[0], end_cols[0]\n            for s, e in zip(df[s_col].values, df[e_col].values):\n                ts = parse_time(s)\n                te = parse_time(e)\n                durations.append(te - ts if ts is not None and te is not None else None)\n    durations = [d for d in durations if isinstance(d, (int, float)) and d is not None and d >= 0]\n    if not durations:\n        return 0.0\n\n    total = np.nansum(durations)\n    avg = float(np.nanmean(durations)) if len(durations) else None\n\n    # Check for graphic/card rows\n    type_cols = [c for c in cols if any(k in c for k in ['type', 'category', 'label', 'shot'])]\n    text_cols = [c for c in cols if any(k in c for k in ['desc', 'description', 'notes', 'text'])]\n    def row_has_graphic(row):\n        for c in type_cols + text_cols:\n            val = str(row.get(c, '')).lower()\n            if any(k in val for k in ['graphic', 'title card', 'card']):\n                return True\n        return False\n    graphic_count = 0\n    for _, row in df.iterrows():\n        try:\n            if row_has_graphic(row):\n                graphic_count += 1\n        except:\n            pass\n\n    # Scoring components\n    score = 0.0\n    # Total duration within broadcast tolerance\n    if 29.8 <= total <= 30.2:\n        score += 0.34\n    elif 29.5 <= total <= 30.5:\n        score += 0.2\n    # Average shot length for medium-high energy\n    if avg is not None:\n        if avg <= 2.5:\n            score += 0.33\n        elif avg <= 3.0:\n            score += 0.22\n        elif avg <= 3.5:\n            score += 0.12\n    # Graphics cards count\n    if graphic_count >= 2:\n        score += 0.33\n    elif graphic_count == 1:\n        score += 0.18\n\n    return max(0.0, min(1.0, score))"}, {"type": "code", "name": "VO Alignment Includes Required Lines", "description": "Verify the VO/script alignment artifacts explicitly include the two required sentences.", "weight": 0.6, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    targets = [\n        'renewable, reliable, green energy projects will create jobs',\n        'urge your legislator to support green energy in california'\n    ]\n    def norm(s):\n        return re.sub(r\"\\s+\", \" \", str(s).strip().lower())\n\n    found = set()\n    # Search all outputs for text content\n    for r in context.get_all_outputs():\n        try:\n            text = ''\n            if r.is_document:\n                # Try PDF then DOCX\n                try:\n                    text = context.files.read_pdf_text(r.id)\n                except:\n                    try:\n                        text = context.files.read_docx_text(r.id)\n                    except:\n                        text = ''\n            elif r.is_text_format:\n                try:\n                    text = context.files.read_text(r.id)\n                except:\n                    text = ''\n            elif r.is_spreadsheet:\n                # Read first sheet to text\n                try:\n                    path = context.files.get_path(r.id)\n                    xls = pd.ExcelFile(path)\n                    df = pd.read_excel(path, sheet_name=xls.sheet_names[0])\n                    text = ' '.join(df.astype(str).fillna('').values.ravel().tolist())\n                except:\n                    text = ''\n            t = norm(text)\n            for i, tgt in enumerate(targets):\n                if tgt in t:\n                    found.add(i)\n        except:\n            continue\n    hits = len(found)\n    if hits == 2:\n        return 1.0\n    if hits == 1:\n        return 0.5\n    return 0.0"}, {"type": "code", "name": "Delivery Specs Self-Report Check", "description": "Confirm delivery specs document states 1920x1080, H.264/MP4, ~30s duration, and 48kHz stereo audio.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    text = ''\n    for r in context.get_all_outputs():\n        if not (r.is_document or r.is_text_format):\n            continue\n        try:\n            t = ''\n            if r.is_document:\n                try:\n                    t = context.files.read_pdf_text(r.id)\n                except:\n                    try:\n                        t = context.files.read_docx_text(r.id)\n                    except:\n                        t = ''\n            else:\n                t = context.files.read_text(r.id)\n            text += '\\n' + t\n        except:\n            pass\n    s = text.lower()\n    score = 0.0\n    if '1920x1080' in s or ('1920' in s and '1080' in s and 'resolution' in s):\n        score += 0.25\n    if 'h.264' in s or 'h264' in s or 'avc' in s:\n        score += 0.25\n    if 'mp4' in s:\n        score += 0.15\n    # duration ~30s\n    if re.search(r'(\\b30(\\.0+)?\\s*s(ec)?\\b|\\b00:30\\b)', s):\n        score += 0.2\n    # audio 48kHz stereo\n    if ('48khz' in s or '48000' in s) and ('stereo' in s or '2.0' in s or '2ch' in s):\n        score += 0.15\n    return max(0.0, min(1.0, score))"}, {"type": "code", "name": "Asset Log \u2013 Direct Links and Coverage", "description": "Validate stock asset log has direct URLs from reputable stock sites, includes music, and covers required visual categories (CA beauty, renewables, workers).", "weight": 0.6, "code": "import re\nimport pandas as pd\n\nALLOWED = ['adobestock', 'shutterstock', 'istock', 'pond5', 'storyblocks', 'artlist', 'artgrid', 'pixabay', 'pexels', 'videvo', 'coverr']\n\nLANDMARK_KWS = ['golden gate', 'bridge', 'beach', 'coast', 'los angeles', 'la skyline', 'skyline', 'agriculture', 'farm', 'orchard', 'field', 'yosemite', 'sierra', 'california']\nRENEW_KWS = ['solar', 'panel', 'pv', 'photovoltaic', 'wind', 'turbine', 'renewable', 'geothermal', 'hydro']\nWORKER_KWS = ['worker', 'technician', 'engineer', 'installer', 'office', 'restaurant', 'construction', 'lineman']\n\ndef evaluate(workflow, context):\n    # Find a CSV/XLSX with a URL column\n    asset_df = None\n    for r in context.get_all_outputs():\n        try:\n            if str(getattr(r, 'name', '')).lower().endswith('.csv'):\n                df = context.files.read_csv(r.id)\n            elif r.is_spreadsheet:\n                path = context.files.get_path(r.id)\n                xls = pd.ExcelFile(path)\n                df = pd.read_excel(path, sheet_name=xls.sheet_names[0])\n            else:\n                continue\n            cols = [str(c).strip().lower() for c in df.columns]\n            df.columns = cols\n            if any('url' == c or 'link' in c for c in cols):\n                asset_df = df\n                break\n        except:\n            continue\n    if asset_df is None or asset_df.empty:\n        return 0.0\n\n    # Basic fields\n    cols = asset_df.columns.tolist()\n    txt_rows = []\n    url_col = None\n    for c in cols:\n        if 'url' in c or 'link' in c:\n            url_col = c\n            break\n    for _, row in asset_df.iterrows():\n        row_str = ' '.join([str(v) for v in row.values]).lower()\n        txt_rows.append(row_str)\n    \n    # URL validity\n    total = len(asset_df)\n    good_urls = 0\n    music_rows = 0\n    for _, row in asset_df.iterrows():\n        u = str(row.get(url_col, '')).strip().lower() if url_col else ''\n        if u.startswith('http') and re.search(r'https?://([^/]+)', u):\n            dom = re.search(r'https?://([^/]+)', u).group(1)\n            if any(d in dom for d in ALLOWED):\n                good_urls += 1\n        # detect music rows\n        row_txt = ' '.join([str(v).lower() for v in row.values])\n        if any(k in row_txt for k in ['music', 'track', 'score']):\n            music_rows += 1\n    url_score = good_urls / total if total else 0\n\n    # Category coverage\n    def has_any(kws):\n        for t in txt_rows:\n            for k in kws:\n                if k in t:\n                    return True\n        return False\n    cov = 0\n    cov += 1 if has_any(LANDMARK_KWS) else 0\n    cov += 1 if has_any(RENEW_KWS) else 0\n    cov += 1 if has_any(WORKER_KWS) else 0\n\n    score = 0.0\n    # URLs component (50%)\n    score += 0.5 * (url_score)\n    # Music presence (10%)\n    score += 0.1 if music_rows >= 1 else 0.0\n    # Coverage of 3 categories (40%)\n    score += 0.4 * (cov/3.0)\n    return max(0.0, min(1.0, score))"}, {"type": "llm_judge", "name": "Narrative and Coverage Match to Script", "description": "Cross-check that the planned visuals cover California beauty, renewable energy sources, and diverse Californians at work (including green energy jobs), and that the narrative flow aligns with environmental benefits \u2192 jobs \u2192 CTA.", "weight": 4.0, "judge_prompt": "Using the shot list, VO/script alignment, and asset log, evaluate if the planned content:\n- Includes California beauty (e.g., Golden Gate Bridge, agriculture, beaches, LA skyline) and renewable energy visuals (solar fields, wind turbines).\n- Shows diverse Californians at work with at least some roles clearly connected to green energy jobs.\n- Follows a logical persuasive arc consistent with the script: environmental benefits \u2192 job creation \u2192 final CTA.\n\nScoring:\n- 1.0: All categories clearly present with a coherent arc culminating in the CTA.\n- 0.7: Most categories present; arc mostly coherent.\n- 0.4: Some categories missing or arc unclear.\n- 0.0: Major categories missing and/or no discernible persuasive arc.", "expectation": "A coverage-complete storyboard consistent with the script\u2019s intent and categories."}, {"type": "llm_judge", "name": "Graphics Cards Compliance", "description": "Verify that two simple black cards with white type (clean font like Arial) are used for the exact lines specified, and appear at the appropriate moments per the VO alignment/shot list.", "weight": 4.0, "judge_prompt": "Check the planned graphics:\n- Two cards with a black background and white text in a clean sans-serif (e.g., Arial or similar).\n- Text content matches the two required lines verbatim (allow trivial punctuation/case variance):\n  1) \u201cRenewable, reliable, green energy projects will create jobs\u201d\n  2) \u201cUrge your legislator to support green energy in California\u201d\n- Placement aligns with the VO/script timing as indicated in the Shot List/VO Alignment.\n\nScoring:\n- 1.0: Both cards specified, visually compliant, and correctly placed.\n- 0.7: Both present but with minor deviations (e.g., similar font, slight wording variance) or timing slightly off.\n- 0.4: Only one card present or significant style/text deviation.\n- 0.0: No compliant graphics cards planned.", "expectation": "Two compliant title cards, correctly placed at the specified lines."}, {"type": "llm_judge", "name": "Music Selection and Pacing Correctness", "description": "Evaluate whether the music is classical in feel, elegant yet energetic, edited to 30s with strong start and ending, and that pacing supports medium-high energy with elegance.", "weight": 3.5, "judge_prompt": "Using the asset log, any music cue/edit notes, shot list timing, and delivery specs:\n- Confirm the music selection is classical in style (e.g., orchestral/strings/piano) and described as elegant yet energetic.\n- Check that the music edit is clearly planned to 30 seconds with a strong start and strong ending.\n- Assess pacing: shot durations and structure suggest medium-high energy without sacrificing elegance.\n\nScoring:\n- 1.0: Classical, elegant/energetic track with clear 30s edit and strong intro/outro; pacing supports the tone.\n- 0.7: Generally classical/appropriate and mostly paced well; intro/outro or 30s edit noted but not fully clear.\n- 0.4: Style or pacing partially mismatched; edit plan unclear.\n- 0.0: Not classical/appropriate, no 30s plan, or glaring pacing issues.", "expectation": "A classical-feel track choice with a well-planned 30s edit that drives confident, elegant pacing."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Holistic Quality Assessment", "description": "Professional craft, persuasion effectiveness, and overall fitness for a progressive campaign broadcast spot.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Editorial Craft and Cohesion", "description": "Assess storytelling clarity, flow, and editorial finesse across 30 seconds (open, build, climax/CTA), including tasteful transitions and coherent visual logic.", "weight": 2.0, "judge_prompt": "Judge the edit plan and materials for overall editorial craft: Does the spot build cleanly from opening to CTA within 30s? Are transitions and cuts tasteful and coherent? Does the visual logic feel professional and broadcast-ready?\n\nScoring: 1.0 outstanding cohesion and flow; 0.7 solid; 0.4 uneven; 0.0 poor.", "expectation": "A cohesive 30s arc with professional pacing and transitions."}, {"type": "llm_judge", "name": "Visual Curation and Representation", "description": "Evaluate the quality and appropriateness of selected visuals (composition, emotion, diversity of Californians, recognition of locations), while avoiding stereotypes and ensuring relevance.", "weight": 2.0, "judge_prompt": "Based on the asset log and shot list, evaluate whether visuals are high-quality, relevant, and representative: Do they showcase California beauty recognizably? Are Californians depicted with diversity and respect, including green-energy workers? Avoidance of clich\u00e9s/stereotypes?\n\nScoring: 1.0 excellent; 0.7 good; 0.4 mixed; 0.0 weak.", "expectation": "Curated, representative visuals that elevate the message."}, {"type": "llm_judge", "name": "Audio Mix and VO Intelligibility (Scratch)", "description": "Assess whether the plan reasonably ensures the scratch VO will be intelligible over music, with tasteful levels and clarity for broadcast review.", "weight": 2.0, "judge_prompt": "From delivery specs and notes, assess if VO is likely to be intelligible over music (e.g., music ducking, EQ/levels indicated, clear timing). Even as a scratch, does the plan reflect broadcast-appropriate clarity?\n\nScoring: 1.0 strong; 0.7 acceptable; 0.4 questionable; 0.0 poor.", "expectation": "A clear, intelligible VO plan with music appropriately managed."}, {"type": "llm_judge", "name": "Persuasive Impact and CTA Clarity", "description": "Evaluate the persuasive strength for a progressive audience: optimism, pride, importance conveyed; clear CTA to urge legislators.", "weight": 2.0, "judge_prompt": "Considering the narrative and graphics, does the spot convey optimism, pride in California, and the importance of green energy? Is the CTA to urge legislators clear, timely, and compelling?\n\nScoring: 1.0 compelling; 0.7 good; 0.4 modest; 0.0 weak.", "expectation": "A persuasive, optimistic tone culminating in a clear CTA."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "3f625cb2-f40e-4ead-8a97-6924356d5989", "rubric": {"category_name": "Legal Memorandum: COPPA and California Privacy (Minor, YouTube)", "rationale": "This rubric uses a self-documenting, staged approach. Stage 1 is an LLM-only structural gate requiring a tightly specified PDF memo format that enables verification. Stage 2 mixes lightweight code checks (presence of key statutes, parties, and context) with heavier LLM judgment for legal correctness and case/enforcement plausibility. Stage 3 assesses overall professional quality, clarity, and client usability. Code rules are narrowly scoped and weighted lower than LLM rules, per guidance.", "max_total_score": 12.0, "stages": [{"name": "Stage 1: Format and Structural Gate (LLM only)", "description": "Gate that enforces exact document shape and format so verification is possible.", "is_required": true, "max_points": 2.0, "min_score_to_pass": 1.5, "rules": [{"type": "llm_judge", "name": "PDF Memo Structure and Required Sections", "description": "Verify the output is a PDF memo of 1\u20133 pages with required headers and sections, enabling later verification.", "weight": 2.0, "judge_prompt": "You are evaluating whether the candidate output satisfies the STRICT structure requirements for this task. Only check presence/format, not legal quality.\n\nRequired FORMAT:\n- Must be a PDF file (not Word, not plain text, not spreadsheet)\n- Length: 1\u20133 pages total (must not exceed 3 pages)\n- Professional memo style with clear section headers\n\nRequired STRUCTURE (flexible header names allowed, but content must be clearly present):\n1) Memo Header block near the top with at least TWO of the following fields visible: \"To:\", \"From:\", \"Date:\", \"Re:\" (or similar, e.g., Recipient/Author/Subject)\n2) Executive Summary or Overview (client-friendly summary of conclusions)\n3) Facts or Background (summarize situation: child, YouTube, California)\n4) Issues or Questions Presented\n5) Legal Analysis section with two distinct subsections:\n   - Federal (COPPA) analysis (explicitly labeled or clearly focused on COPPA/Children\u2019s Online Privacy Protection Act)\n   - California law analysis (e.g., CCPA/CPRA, CalOPPA, AADC, UCL) \u2014 naming flexible but must clearly be California-specific\n6) Relevant Case Law or Enforcement Actions (at least two items, as a list or mini-summaries; may include FTC/AG actions against YouTube/Google)\n7) Legal Options and Recommendations (client-focused next steps)\n8) References/Authorities or Citations section (flexible naming), listing statutes/cases/regulations; brief list acceptable\n\nScoring (STRUCTURE ONLY):\n- 2.0: PDF; 1\u20133 pages; memo header with \u22652 fields; all 8 structural elements present.\n- 1.5: PDF; 1\u20133 pages; memo header with \u22652 fields; core sections present but missing exactly ONE supporting element (usually References/Authorities) OR case/enforcement list has only one item.\n- 1.0: PDF; 1\u20133 pages; memo header present; missing TWO required elements OR Legal Analysis is not split into federal vs. California.\n- 0.0: Not PDF OR exceeds 3 pages OR missing multiple core sections; structure unclear.\n\nOnly evaluate presence/format, not correctness or writing quality.", "expectation": "A 1\u20133 page PDF memo with a memo header, executive summary, background, issues, split legal analysis (COPPA and California), a case/enforcement list (\u22652 items), recommendations, and references."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Correctness Verification (Mixed)", "description": "Checks correctness and completeness of legal content, with light code-based presence checks and heavier LLM judgment for legal accuracy.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Mentions Core Statutes and Agencies", "description": "Verifies that the memo text references core laws and agencies relevant to the scenario (COPPA, FTC, and at least one California-specific law).", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float (0..1 normalized) or tuple[float, str]\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0, \"Unable to read document text\"\n\n    t = text.lower()\n\n    # Checks\n    has_coppa = (\"coppa\" in t) or (\"children's online privacy protection act\" in t) or (\"childrens online privacy protection act\" in t)\n    has_ftc = (\"ftc\" in t) or (\"federal trade commission\" in t)\n\n    ca_tokens = [\n        \"ccpa\", \"cpra\", \"caloppa\", \"cal-oppa\", \n        \"california consumer privacy act\", \"california privacy rights act\",\n        \"cal. civ. code\", \"civil code\", \"bus. & prof. code\", \"business and professions code\",\n        \"age-appropriate design code\", \"aadc\", \"attorney general of california\"\n    ]\n    has_calaw = any(tok in t for tok in ca_tokens)\n\n    total = 3\n    met = sum([has_coppa, has_ftc, has_calaw])\n    score = met / total\n    feedback = f\"COPPA:{has_coppa} FTC:{has_ftc} CA-law-ref:{has_calaw}\"\n    return score, feedback"}, {"type": "code", "name": "Identifies Parties and Context", "description": "Checks whether the memo mentions YouTube, California residency, and minor/parent context.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Returns a normalized score in [0,1] based on presence of parties and context cues.\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0, \"Unable to read document text\"\n\n    t = text.lower()\n\n    has_youtube = \"youtube\" in t\n    has_california = \"california\" in t or \"ca\" in t\n    has_child = any(k in t for k in [\"minor\", \"child\", \"10-year\", \"10 year\", \"ten-year\", \"ten year\"])\n    has_parent = any(k in t for k in [\"parent\", \"father\", \"guardian\"])\n\n    total = 4\n    met = sum([has_youtube, has_california, has_child, has_parent])\n    score = met / total\n    feedback = f\"YouTube:{has_youtube} California:{has_california} Child:{has_child} Parent:{has_parent}\"\n    return score, feedback"}, {"type": "llm_judge", "name": "Substantive Accuracy: COPPA Requirements and Enforcement", "description": "Assesses whether the memo accurately explains COPPA\u2019s scope, definitions, consent requirements, and enforcement/remedies in this context.", "weight": 2.0, "judge_prompt": "Evaluate the memo\u2019s accuracy and completeness regarding COPPA as applied to YouTube and a 10-year-old in California. Check for:\n- Correct scope: applies to operators collecting personal information from children under 13; concepts of child-directed services and actual knowledge; examples of covered PI (name, address, online identifiers, etc.)\n- Parental consent requirements and notice; data handling obligations (e.g., deletion upon request)\n- Enforcement: FTC and state AGs; penalties; lack of a private right of action for parents under COPPA\n- Any mention of the 2019 FTC/NY AG YouTube/Google COPPA settlement or similar enforcement, if included, should be characterized correctly\nScoring:\n- 2.0: Accurate on all points with no material errors; clearly applies framework to YouTube/child context\n- 1.5: Mostly accurate; minor omissions or nuance issues; no serious errors\n- 1.0: Some inaccuracies/omissions; partially correct but risks misleading\n- 0.0: Major errors (e.g., claims parents have a private right under COPPA) or fundamentally wrong scope/application", "expectation": "A correct, concise explanation of COPPA\u2019s applicability, duties, and enforcement, noting the absence of a private right of action."}, {"type": "llm_judge", "name": "California Law Landscape: Applicability and Limits", "description": "Assesses whether the memo correctly describes relevant California privacy laws and their limits for this situation.", "weight": 1.8, "judge_prompt": "Evaluate the accuracy and appropriateness of the California law discussion. Consider:\n- CCPA/CPRA scope (for-profit business thresholds), consumer rights, minors\u2019 opt-in for sale of personal information (ages 13\u201316), and the narrow private right of action (primarily for certain data breaches)\n- CalOPPA (privacy policy obligations for online services) as relevant\n- California Age-Appropriate Design Code Act (AADC) mention, if included, acknowledging current enforcement status/ongoing litigation/injunctions\n- Potential use of Unfair Competition Law (Bus. & Prof. Code \u00a717200) and COPPA preemption risks for state-law claims mirroring COPPA\nScoring:\n- 1.8: Accurate on key points; no material misstatements; reasonably tailored to facts\n- 1.2: Generally accurate with small gaps or minor inaccuracies; not misleading overall\n- 0.6: Noticeable inaccuracies/omissions; limited usefulness\n- 0.0: Materially incorrect or misleading treatment of California law", "expectation": "A balanced, accurate overview of California privacy avenues and their limits, tailored to a parent/child situation."}, {"type": "llm_judge", "name": "Case Law/Enforcement Citations Plausibility", "description": "Checks whether cited cases/enforcement actions are plausible and relevant, and summaries are consistent and not obviously fabricated.", "weight": 1.8, "judge_prompt": "Review the memo\u2019s case law/enforcement section. Look for at least two specific and relevant authorities (e.g., FTC action against YouTube/Google in 2019, federal cases addressing COPPA preemption, or relevant California privacy cases). Evaluate:\n- Plausibility of citations (case names, agencies, years, reporters/dockets where provided)\n- Basic accuracy of descriptions (no obvious fabrications or contradictions)\n- Relevance to the scenario (child data on YouTube; COPPA; California privacy)\nScoring:\n- 1.8: At least two plausible, relevant authorities with consistent, accurate summaries\n- 1.2: Authorities mostly plausible; minor citation issues; generally relevant\n- 0.6: Weak plausibility or relevance; sparse or questionable references\n- 0.0: Citations appear fabricated or irrelevant; no usable authorities", "expectation": "Provide at least two credible, relevant authorities with short, accurate summaries."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3: Quality and Client Usefulness", "description": "Holistic assessment of clarity, professionalism, and actionable guidance for the client.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Client-Friendly Clarity and Organization", "description": "Assesses readability, plain-language explanations, and coherent organization for a non-lawyer client.", "weight": 1.2, "judge_prompt": "Rate the memo on clarity and organization for a client audience (non-lawyer):\n- Clear, plain-language explanations of legal concepts and implications\n- Logical flow across sections; concise sentences; defined terms\n- Minimal jargon; when used, it\u2019s explained\nScoring: 1.2 excellent; 0.8 good with minor issues; 0.4 uneven or hard to follow; 0.0 confusing or highly technical.", "expectation": "Plain, clear explanations with clean organization suitable for a client."}, {"type": "llm_judge", "name": "Actionable Guidance and Risk Assessment", "description": "Evaluates whether the memo gives concrete next steps and discusses risks/limitations.", "weight": 1.2, "judge_prompt": "Assess how actionable the recommendations are:\n- Specific steps (e.g., exercising CCPA/CPRA rights, contacting FTC/CA AG, evidence preservation, notices/requests to YouTube)\n- Realistic assessment of remedies and constraints (e.g., no COPPA private right; CCPA private right is narrow)\n- Tailored advice to the client\u2019s facts\nScoring: 1.2 strong and actionable; 0.8 generally useful; 0.4 limited or generic; 0.0 not actionable.", "expectation": "Concrete steps with realistic expectations and tailored risk analysis."}, {"type": "llm_judge", "name": "Professionalism and Citations", "description": "Checks tone, professionalism, and basic citation hygiene.", "weight": 0.8, "judge_prompt": "Evaluate professional presentation:\n- Professional tone; consistent formatting; minimal typos\n- Citations/authorities presented in a recognizable form (case names, statutes like 15 U.S.C. \u00a7 6501 et seq., Cal. Civ. Code, etc.)\n- Inclusion of assumptions/limitations where appropriate\nScoring: 0.8 excellent; 0.5 good; 0.3 fair; 0.0 poor.", "expectation": "Professional tone with reasonably formatted citations and stated assumptions/limits."}, {"type": "llm_judge", "name": "Conciseness and Formatting Discipline", "description": "Assesses adherence to length constraint and clean sectioning.", "weight": 0.8, "judge_prompt": "Evaluate adherence to practical constraints:\n- Length at or under 3 pages\n- Clean headings for each required section; consistent formatting; minimal fluff\nScoring: 0.8 fully compliant and tight; 0.5 minor excess or slight formatting issues; 0.2 noticeable verbosity or messy layout; 0.0 substantially noncompliant.", "expectation": "\u22643 pages, clean headings, minimal fluff."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "4de6a529-4f61-41a1-b2dc-64951ba03457", "rubric": {"category_name": "Stanton Capital Q1 2025 Active Allocation Table", "rationale": "Task type: Mixed (Pattern C). Output format: PDF document with structured tables. Stage 1 uses an LLM gate to strictly enforce the self-documenting shape (two clearly labeled sections, standardized table columns, and coverage of main and sub-asset classes). Stage 2 mixes light code checks (detecting key headers/markers in extracted text) with LLM verification for nuanced structural correctness (one-of-three selection, conviction compliance, one-sentence rationales). Stage 3 provides a holistic quality assessment focused on strategic coherence, macro alignment (slight growth improvement, Fed cutting), professionalism, and investor actionability. Code rules carry much lower weight than LLM rules, per guidelines.", "max_total_score": 22.0, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate", "description": "LLM-only gate to ensure the output is a properly structured PDF with the exact required sections and table columns. If this shape is not present, verification is not possible.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured PDF Requirements (GATE)", "description": "Check that the candidate output is a PDF with the mandated sections and table structures to enable verification.", "weight": 4.0, "judge_prompt": "You are evaluating whether the submitted output meets the strict STRUCTURAL requirements for this task. Only assess presence and structure, not content quality or correctness.\n\nRequirements:\n- File format: PDF (not Word, not Excel, not plain text)\n- The document should clearly be labeled for quarter ending March 31, 2025 and attributable to Stanton Capital (flexible on exact phrasing, e.g., \"Q1 2025\" or \"Quarter ended Mar 31, 2025\").\n- Two distinct sections must be present:\n  1) A top section labeled \"Main Asset Classes\" (or very close variant like \"Cross-Asset Opportunity Sets\"), containing a table with rows for broad asset classes (e.g., Equities, Fixed Income, Cash, Commodities, Real Estate/Real Assets, Alternatives). Expect at least 4 asset classes listed.\n  2) A bottom section labeled \"Preference by Asset Class\" with three sub-groups: Equities, Fixed Income, and Currency. Each sub-group must be further broken into sub-asset classes with a consistent table format.\n- In BOTH top and bottom tables, each line item should have columns (headers or visibly equivalent labels):\n  - UW (underweight)\n  - N (neutral) \u2014 allow \"Neutral\" as a label\n  - OW (overweight)\n  - Change (relative to prior quarter; display may be arrow up/down or blank)\n  - Conviction (values limited later to low or moderate \u2014 here only check that a Conviction column exists)\n  - One-sentence justification (a Rationale/Notes/Comment/Justification column; naming can vary but must clearly contain a sentence-level explanation per row)\n\nScoring:\n- 4.0: PDF format AND both sections present with correct labeling; both tables exist with all required columns; bottom section includes all three sub-groups (Equities, Fixed Income, Currency) with sub-asset rows.\n- 3.0: PDF format AND both sections present, but minor naming variance or one minor column label inconsistency; OR one sub-group missing a few sub-asset rows.\n- 2.0: PDF format but only one of the two major sections is clearly present OR required columns are incomplete/missing in multiple places.\n- 1.0: PDF format but tables/columns are largely missing or not recognizable for verification.\n- 0.0: Not a PDF or structure is absent/illegible.\n\nOnly evaluate structure and presence, not whether the views are correct.", "expectation": "A well-structured PDF with two labeled sections, standardized columns, and clear sub-asset coverage to enable verification."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Structure Correctness and Consistency)", "description": "Mixed code + LLM checks to verify the document follows the mandated schema in a verifiable way: presence of key columns, indicators, one-of-three selection logic, conviction compliance, and one-sentence rationales.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Core Columns Detected in Text", "description": "Check that key column headers/labels (UW, N/Neutral, OW, Change, Conviction) and allowed conviction values (low or moderate) appear in the document text.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float or (float, str)\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n\n    text = \"\"\n    try:\n        if output.file_extension.lower() == '.pdf' or output.is_pdf:\n            text = context.files.read_pdf_text(output.id)\n        else:\n            text = context.files.read_docx_text(output.id)\n    except Exception as e:\n        return 0.0, f\"Failed to read document text: {e}\"\n\n    t = text or \"\"\n    score_items = []\n    checks = {\n        'UW': bool(re.search(r\"\\bUW\\b\", t, re.IGNORECASE)),\n        'N_or_Neutral': bool(re.search(r\"\\bN\\b|\\bNeutral\\b\", t, re.IGNORECASE)),\n        'OW': bool(re.search(r\"\\bOW\\b\", t, re.IGNORECASE)),\n        'Change': bool(re.search(r\"\\bChange\\b\", t, re.IGNORECASE)),\n        'Conviction': bool(re.search(r\"\\bConviction\\b\", t, re.IGNORECASE)),\n        'ConvictionValuesLowModerate': bool(re.search(r\"\\b(low|moderate)\\b\", t, re.IGNORECASE)),\n    }\n    passed = sum(1 for v in checks.values() if v)\n    total = len(checks)\n    frac = passed / total if total else 0.0\n    score = frac * 0.8\n    missing = [k for k, v in checks.items() if not v]\n    feedback = f\"Detected {passed}/{total} core elements. Missing: {', '.join(missing)}\" if missing else \"All core elements detected.\"\n    return score, feedback"}, {"type": "code", "name": "Change Indicators Present", "description": "Verify presence of change markers (arrows or words) associated with the Change column.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n\n    try:\n        if output.file_extension.lower() == '.pdf' or getattr(output, 'is_pdf', False):\n            text = context.files.read_pdf_text(output.id)\n        else:\n            text = context.files.read_docx_text(output.id)\n    except Exception as e:\n        return 0.0, f\"Failed to read document text: {e}\"\n\n    t = text or \"\"\n    # Look for arrows or explicit up/down near the word Change\n    pattern_near_change = re.compile(r\"Change.{0,40}(\u2191|\u2193|\u2197|\u2198|\u25b2|\u25bc|up|down)\", re.IGNORECASE | re.DOTALL)\n    pattern_anywhere = re.compile(r\"(\u2191|\u2193|\u2197|\u2198|\u25b2|\u25bc)\\s*\", re.UNICODE)\n\n    found = bool(pattern_near_change.search(t)) or bool(pattern_anywhere.search(t))\n    return (0.6 if found else 0.0, \"Change markers detected\" if found else \"No change markers found (arrows or up/down)\")"}, {"type": "code", "name": "Required Section Labels Present", "description": "Detect that the document includes the two required section labels and at least two of the three sub-section labels (Equities, Fixed Income, Currency).", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n\n    try:\n        if output.file_extension.lower() == '.pdf' or getattr(output, 'is_pdf', False):\n            text = context.files.read_pdf_text(output.id)\n        else:\n            text = context.files.read_docx_text(output.id)\n    except Exception as e:\n        return 0.0, f\"Failed to read document text: {e}\"\n\n    t = (text or '').lower()\n    # Flexible matching for section names\n    main_ok = ('main asset classes' in t) or ('cross-asset' in t and 'opportunity' in t)\n    pref_ok = ('preference by asset class' in t) or ('asset class preferences' in t)\n    labels = [\n        'equities' in t,\n        'fixed income' in t,\n        'currency' in t or 'currencies' in t\n    ]\n    sub_count = sum(1 for x in labels if x)\n    frac = 0.0\n    if main_ok:\n        frac += 0.33\n    if pref_ok:\n        frac += 0.33\n    if sub_count >= 2:\n        frac += 0.34\n    score = frac * 0.6\n    return score, f\"Main: {bool(main_ok)}, Preference: {bool(pref_ok)}, Sub-sections present: {sub_count}/3\""}, {"type": "llm_judge", "name": "One-of-Three Selection Integrity (UW/N/OW)", "description": "Verify that for each line item, exactly one of UW, N/Neutral, or OW appears selected/marked (e.g., X, checkmark, filled cell).", "weight": 2.7, "judge_prompt": "Assess whether the tables implement a clear one-of-three selection rule per line item: exactly one of UW, N (or Neutral), or OW is marked. Evidence may include an X/check, bold/filled cell, or exclusive indicator. Consider both the Main Asset Classes table and the Preference by Asset Class sub-asset tables.\n\nScoring:\n- 2.7: In nearly all rows (>90%), exactly one selection is clearly indicated; no frequent double-marking or missing marks.\n- 1.8: Mostly compliant (\u224870\u201390% of rows), a few inconsistencies.\n- 0.9: Partial implementation (\u224840\u201370%); multiple rows unclear or multi-selected.\n- 0.0: Rarely or never follows one-of-three exclusivity; selection state generally unclear.\n\nIgnore content quality; only evaluate selection exclusivity and consistency.", "expectation": "Clear, exclusive selection among UW/N/OW for each row across all tables."}, {"type": "llm_judge", "name": "Conviction Label Compliance", "description": "Confirm that each row includes a conviction label and that only the allowed values (low or moderate) are used consistently.", "weight": 2.7, "judge_prompt": "Evaluate whether the Conviction column is populated on each row and uses ONLY the allowed values: low or moderate (case-insensitive). Check both Main Asset Classes and sub-asset tables.\n\nScoring:\n- 2.7: Conviction present for virtually all rows and strictly limited to low or moderate.\n- 1.8: Minor deviations (a few missing values or a rare disallowed value like \"high\").\n- 0.9: Frequent omissions or several disallowed values.\n- 0.0: Conviction labels largely missing or commonly outside the allowed set.\n\nOnly assess label presence and allowed values, not whether the choice is appropriate.", "expectation": "Complete coverage with only low or moderate used as conviction values."}, {"type": "llm_judge", "name": "One-Sentence Justifications per Line Item", "description": "Check that each row contains a concise one-sentence rationale/justification.", "weight": 2.6, "judge_prompt": "Review the Rationale/Justification/Notes column for each row. Determine whether each line item includes a concise, single-sentence justification (not multi-sentence, not missing).\n\nScoring:\n- 2.6: Nearly all rows (>90%) have a single, grammatical sentence; very few are missing or run-on/multi-sentence.\n- 1.7: Mostly compliant (\u224870\u201390%), some multi-sentence or missing entries.\n- 0.9: Partial compliance (\u224840\u201370%), many missing or multi-sentence entries.\n- 0.0: Largely missing or non-sentence fragments across rows.", "expectation": "Consistent, one-sentence justifications for almost every line item."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Strategic Value", "description": "Holistic LLM assessment of investment insight, macro alignment, clarity, and professional readiness for an investor audience.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Macro Alignment and Cross-Sectional Coherence", "description": "Assess whether stated views reasonably reflect the macro context: slight global growth improvement, Fed rate-cutting cycle, overall healthy economy; minimal changes from last quarter.", "weight": 2.0, "judge_prompt": "Judge macro-consistency: Do the stated preferences across Main Asset Classes and sub-asset classes reasonably reflect a backdrop of (1) slight improvement in global growth, (2) Fed in a rate-cutting cycle, and (3) broadly healthy economic conditions? Also consider whether changes vs. last quarter appear limited and selectively applied (i.e., minimal macro changes overall).\n\nScoring:\n- 2.0: Strongly consistent with macro setup; changes are selective and plausible.\n- 1.3: Mostly consistent with minor mismatches or an arguably high number of changes.\n- 0.7: Mixed or frequently inconsistent with macro backdrop.\n- 0.0: Mostly misaligned with stated macro environment.", "expectation": "Views reflect modest growth tailwinds and easing policy, with minimal broad-brush changes."}, {"type": "llm_judge", "name": "Top-Down and Bottom-Up Consistency", "description": "Evaluate whether the Main Asset Classes preferences logically map to the sub-asset class preferences.", "weight": 2.0, "judge_prompt": "Check if the high-level Main Asset Classes signals map logically to sub-asset choices in Equities, Fixed Income, and Currency. For example, an OW in Equities should generally be supported by overweight tilts in certain equity sub-asset classes; a defensive or duration stance in Fixed Income should align with rates and credit preferences. Look for contradictions.\n\nScoring:\n- 2.0: Strong logical alignment between top-down and bottom-up preferences.\n- 1.3: Mostly aligned with a few contradictions.\n- 0.7: Several inconsistencies reduce interpretability.\n- 0.0: Poor alignment; top-down and bottom-up signals conflict repeatedly.", "expectation": "Clear, consistent mapping between high-level and sub-asset views."}, {"type": "llm_judge", "name": "Professional Presentation and Investor Readiness", "description": "Assess visual clarity, labeling, date/attribution, and general polish appropriate for CIO-level publication.", "weight": 2.0, "judge_prompt": "Assess presentation quality: clear section headers, readable tables, consistent typography, visible column headers (UW, N, OW, Change, Conviction, Rationale), appropriate labeling for Q1 2025 / quarter ending March 31, 2025, and attribution to Stanton Capital. Consider page layout, spacing, and absence of clutter.\n\nScoring:\n- 2.0: Highly professional and publication-ready.\n- 1.3: Generally professional with minor formatting or labeling issues.\n- 0.7: Adequate but with multiple readability or labeling concerns.\n- 0.0: Poorly formatted; difficult to use as an investor reference.", "expectation": "Clean, branded, and publication-ready layout."}, {"type": "llm_judge", "name": "Actionability and Clarity of Takeaways", "description": "Evaluate whether an investor could quickly understand and act on the views and reported changes.", "weight": 2.0, "judge_prompt": "Judge how actionable and clear the tables are for investors: Are the relative preferences easy to identify? Are the one-sentence rationales specific enough to guide allocation decisions? Are the changes vs. prior quarter apparent without excessive narrative?\n\nScoring:\n- 2.0: Clear, concise, and directly actionable.\n- 1.3: Mostly actionable; some vagueness.\n- 0.7: Limited actionability; rationales generic or unclear.\n- 0.0: Not actionable; confusing or opaque takeaways.", "expectation": "Concise, unambiguous signals with useful one-sentence rationales."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "a73fbc98-90d4-4134-a54f-2b1d0c838791", "rubric": {"category_name": "Spring Bazaar Vendor Table Assignment", "rationale": "This rubric enforces a self-documenting, verifiable Excel-based plan for assigning vendors to tables across the arena and meeting room with power and adjacency constraints. Stage 1 (LLM-only) mandates an exact workbook structure that makes verification trivial. Stage 2 mixes light code checks (deterministic bounds/consistency) with heavier LLM verification for cross-references and constraint satisfaction. Stage 3 assesses professional quality, operational readiness, and stakeholder value.", "max_total_score": 19.0, "stages": [{"name": "Stage 1 \u2014 Structured Workbook Gate", "description": "Gate: The primary output must be a single Excel workbook with an exact, verification-friendly structure enabling downstream checks. LLM-only per philosophy.", "is_required": true, "max_points": 1.0, "min_score_to_pass": 0.7, "rules": [{"type": "llm_judge", "name": "Workbook Structure and Sections Present", "description": "Verify the candidate produced a single Excel workbook with required sheets and clearly labeled tables/columns that enable verification.", "weight": 1.0, "judge_prompt": "You are validating ONLY the presence/shape of a structured Excel workbook for a spring bazaar vendor table assignment plan. Do not judge calculation correctness or quality.\n\nCheck that the PRIMARY output is an Excel file (.xlsx) with the following sheets and structures (be flexible on exact names, but the function must be clear):\n\nCORE SHEETS (all required):\n1) \"Vendor Assignments\" (or similar, e.g., Assignments, Vendor Table Plan)\n   - Must have a single tabular section with clearly labeled columns including at least:\n     \u2022 Business Name\n     \u2022 Product Category or Product Description\n     \u2022 Tables Purchased (numeric)\n     \u2022 Location Preference (Arena/Meeting Room)\n     \u2022 Electricity Needed (Y/N)\n     \u2022 Additional Requests (free text)\n     \u2022 Assigned Tables (comma- or semicolon-separated list of specific table IDs/numbers)\n     \u2022 Assigned Area (Arena/Meeting Room) \u2014 if not present here, it must be derivable from Layout Mapping\n     \u2022 Adjacency Notes (optional but preferred)\n\n2) \"Layout Mapping\" (or similar, e.g., Floor Layout, Table Map)\n   - Must include a table with columns:\n     \u2022 Table Number/ID\n     \u2022 Area (Arena or Meeting Room)\n     \u2022 Has Power (Y/N)\n     \u2022 Assigned Vendor/Business Name\n     \u2022 Adjacency information (at least one of: Adjacent Tables list OR Left/Right/Front/Back neighbor columns)\n\n3) \"Constraints & Decisions\" (or similar, e.g., Methodology & Conflicts)\n   - A documented section that lists:\n     \u2022 Rules/constraints applied (e.g., separate same-product neighbors, honor location/power)\n     \u2022 Special requests (e.g., requested neighbor) and their outcomes\n     \u2022 Any violations/exceptions and justifications\n\nSUPPORTIVE SHEETS (recommended):\n4) \"Power Allocation\" (or similar)\n   - Table showing: Powered tables by area, count of powered outlets used vs available, list of vendors requiring electricity and their assigned powered tables.\n\n5) \"Summary\" (or similar, e.g., Dashboard)\n   - Totals & KPIs: vendor count, tables purchased vs assigned, breakdown by area, # power-required vendors served, conflicts resolved, and any unassigned tables.\n\nScoring:\n- 1.0: Excel format AND all 3 core sheets present with appropriate structures. Supportive sheets present OR clearly integrated into the core sheets as distinct, labeled sections.\n- 0.8: Excel format AND all core sheets present, but one supportive sheet missing (or embedded but unclear).\n- 0.5: Excel format but missing exactly one core sheet (even if supportive sheets exist).\n- 0.0: Not an Excel file OR missing multiple core sheets OR no recognizable structures.\n\nOnly evaluate presence/structure. Do NOT assess correctness, completeness of data, or presentation quality.", "expectation": "A single .xlsx file with core sheets (Vendor Assignments, Layout Mapping, Constraints & Decisions) and their required columns/sections; supportive sheets for Power Allocation and Summary are strongly preferred."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification and Correctness", "description": "Deterministic checks plus LLM verification for cross-sheet consistency, constraints satisfaction, and reasonableness.", "is_required": true, "max_points": 10.0, "min_score_to_pass": 5.0, "rules": [{"type": "code", "name": "Assigned Tables Count Matches Purchase Qty", "description": "For each vendor, the count of parsed assigned table IDs equals the number of tables purchased.", "weight": 0.6, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"Primary output missing or not a spreadsheet.\"\n    # Helper: find sheet name by fuzzy match\n    def find_sheet(resource_id, keywords):\n        try:\n            path = context.files.get_path(resource_id)\n            xls = pd.ExcelFile(path)\n            for name in xls.sheet_names:\n                low = name.lower()\n                if any(k in low for k in keywords):\n                    return name\n        except Exception:\n            return None\n        return None\n\n    # Read Vendor Assignments sheet\n    sheet_va = find_sheet(output.id, [\"assign\", \"vendor\", \"plan\"])  # flexible\n    if not sheet_va:\n        return 0.0, \"Vendor Assignments sheet not found.\"\n    try:\n        df = context.files.read_excel(output.id, sheet_name=sheet_va)\n    except Exception:\n        return 0.0, \"Failed to read Vendor Assignments sheet.\"\n\n    # Helper: find column by fuzzy keys\n    def find_col(df, keys):\n        cols = list(df.columns)\n        low = [str(c).strip().lower() for c in cols]\n        for i, c in enumerate(low):\n            for k in keys:\n                if k in c:\n                    return cols[i]\n        return None\n\n    col_purchased = find_col(df, [\"tables purchased\", \"tables\", \"qty\", \"quantity\"])\n    col_assigned = find_col(df, [\"assigned table\", \"table assignment\", \"assigned tables\"])\n    if col_purchased is None or col_assigned is None:\n        return 0.0, \"Required columns (Tables Purchased, Assigned Tables) not found.\"\n\n    def parse_ids(s):\n        if pd.isna(s):\n            return []\n        txt = str(s)\n        # split on common delimiters and whitespace\n        parts = re.split(r\"[;,\\n\\r\\t\\s]+\", txt)\n        parts = [p.strip() for p in parts if p and p.strip()]\n        # keep alphanum-ish tokens as IDs\n        return [p for p in parts]\n\n    valid = 0\n    total = 0\n    for _, row in df.iterrows():\n        try:\n            purchased = row[col_purchased]\n            if pd.isna(purchased):\n                continue\n            try:\n                purchased_num = int(float(purchased))\n            except Exception:\n                continue\n            assigned_list = parse_ids(row[col_assigned])\n            total += 1\n            if len(assigned_list) == purchased_num:\n                valid += 1\n        except Exception:\n            # ignore row on error\n            continue\n\n    if total == 0:\n        return 0.0, \"No rows with both purchase quantity and assigned tables to evaluate.\"\n\n    score = valid / total\n    return score, f\"Matched count for {valid}/{total} vendors.\""}, {"type": "code", "name": "Duplicate Table Assignment Check", "description": "No table number should be assigned to more than one vendor across the layout.", "weight": 0.5, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"Primary output missing or not a spreadsheet.\"\n\n    def find_sheet(resource_id, keywords):\n        try:\n            xls = pd.ExcelFile(context.files.get_path(resource_id))\n            for name in xls.sheet_names:\n                low = name.lower()\n                if any(k in low for k in keywords):\n                    return name\n        except Exception:\n            return None\n        return None\n\n    sheet_layout = find_sheet(output.id, [\"layout\", \"map\", \"floor\", \"table\"])\n    if not sheet_layout:\n        return 0.0, \"Layout Mapping sheet not found.\"\n    try:\n        df = context.files.read_excel(output.id, sheet_name=sheet_layout)\n    except Exception:\n        return 0.0, \"Failed to read Layout Mapping sheet.\"\n\n    def find_col(df, keys):\n        cols = list(df.columns)\n        low = [str(c).strip().lower() for c in cols]\n        for i, c in enumerate(low):\n            for k in keys:\n                if k in c:\n                    return cols[i]\n        return None\n\n    col_table = find_col(df, [\"table\", \"table id\", \"table number\"]) \n    col_vendor = find_col(df, [\"assigned vendor\", \"business name\", \"vendor\"])\n    if col_table is None or col_vendor is None:\n        return 0.0, \"Required columns (Table Number, Assigned Vendor) not found.\"\n\n    # consider only rows with a vendor name\n    assigned = df[[col_table, col_vendor]].dropna(subset=[col_vendor]).copy()\n    assigned[col_table] = assigned[col_table].astype(str).str.strip().str.lower()\n    assigned[col_vendor] = assigned[col_vendor].astype(str).str.strip().str.lower()\n\n    counts = assigned.groupby(col_table)[col_vendor].nunique()\n    total_assigned_tables = (assigned[col_table].nunique())\n    if total_assigned_tables == 0:\n        return 1.0, \"No tables assigned; treating as no duplicates.\"\n    duplicates = (counts > 1).sum()\n    # score 1.0 if no duplicates; otherwise linear penalty\n    score = max(0.0, 1.0 - (duplicates / total_assigned_tables))\n    return score, f\"Duplicate tables: {int(duplicates)} of {int(total_assigned_tables)}.\""}, {"type": "code", "name": "Electricity Requirement Satisfaction", "description": "Vendors marked as needing electricity are assigned only to powered tables.", "weight": 0.5, "code": "import re\nimport pandas as pd\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"Primary output missing or not a spreadsheet.\"\n\n    def find_sheet(resource_id, keywords):\n        try:\n            xls = pd.ExcelFile(context.files.get_path(resource_id))\n            for name in xls.sheet_names:\n                low = name.lower()\n                if any(k in low for k in keywords):\n                    return name\n        except Exception:\n            return None\n        return None\n\n    def find_col(df, keys):\n        cols = list(df.columns)\n        low = [str(c).strip().lower() for c in cols]\n        for i, c in enumerate(low):\n            for k in keys:\n                if k in c:\n                    return cols[i]\n        return None\n\n    def to_bool(val):\n        if pd.isna(val):\n            return None\n        s = str(val).strip().lower()\n        if s in [\"y\", \"yes\", \"true\", \"1\"]:\n            return True\n        if s in [\"n\", \"no\", \"false\", \"0\"]:\n            return False\n        return None\n\n    def parse_ids(s):\n        if pd.isna(s):\n            return []\n        parts = re.split(r\"[;,\\n\\r\\t\\s]+\", str(s))\n        return [p.strip() for p in parts if p and p.strip()]\n\n    # sheets\n    sheet_va = find_sheet(output.id, [\"assign\", \"vendor\", \"plan\"])  # vendor assignments\n    sheet_layout = find_sheet(output.id, [\"layout\", \"map\", \"floor\", \"table\"])  # layout mapping\n    if not sheet_va or not sheet_layout:\n        return 0.0, \"Required sheets not found.\"\n\n    try:\n        va = context.files.read_excel(output.id, sheet_name=sheet_va)\n        lm = context.files.read_excel(output.id, sheet_name=sheet_layout)\n    except Exception:\n        return 0.0, \"Failed to read necessary sheets.\"\n\n    # columns\n    col_need_power = find_col(va, [\"electric\", \"power\", \"electricity\", \"needs power\"])\n    col_assigned = find_col(va, [\"assigned table\", \"table assignment\", \"assigned tables\"])\n    if col_need_power is None or col_assigned is None:\n        return 0.0, \"Columns not found in Vendor Assignments (Electricity, Assigned Tables).\"\n\n    col_table = find_col(lm, [\"table\", \"table id\", \"table number\"]) \n    col_has_power = find_col(lm, [\"has power\", \"power\", \"outlet\", \"electricity\"])\n    if col_table is None or col_has_power is None:\n        return 0.0, \"Columns not found in Layout Mapping (Table, Has Power).\"\n\n    lm_map = lm.copy()\n    lm_map[col_table] = lm_map[col_table].astype(str).str.strip().str.lower()\n    lm_map[col_has_power] = lm_map[col_has_power].apply(lambda x: to_bool(x))\n    power_by_table = dict(zip(lm_map[col_table], lm_map[col_has_power]))\n\n    need_rows = va[va[col_need_power].apply(lambda x: to_bool(x) is True)]\n    if need_rows.shape[0] == 0:\n        return 1.0, \"No vendors require electricity.\"\n\n    ok = 0\n    total = 0\n    for _, row in need_rows.iterrows():\n        tables = [t.lower() for t in parse_ids(row[col_assigned])]\n        for t in tables:\n            total += 1\n            if power_by_table.get(t, False):\n                ok += 1\n    if total == 0:\n        return 0.0, \"No assigned tables for power-required vendors.\"\n    score = ok / total\n    return score, f\"Powered placements: {ok}/{total}.\""}, {"type": "code", "name": "Location Preference Adherence", "description": "Vendors\u2019 location preferences (Arena vs Meeting Room) align with assigned area (either directly on Vendor sheet or via Layout Mapping).", "weight": 0.4, "code": "import re\nimport pandas as pd\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"Primary output missing or not a spreadsheet.\"\n\n    def find_sheet(resource_id, keywords):\n        try:\n            xls = pd.ExcelFile(context.files.get_path(resource_id))\n            for name in xls.sheet_names:\n                low = name.lower()\n                if any(k in low for k in keywords):\n                    return name\n        except Exception:\n            return None\n        return None\n\n    def find_col(df, keys):\n        cols = list(df.columns)\n        low = [str(c).strip().lower() for c in cols]\n        for i, c in enumerate(low):\n            for k in keys:\n                if k in c:\n                    return cols[i]\n        return None\n\n    def norm_area(s):\n        if s is None or (isinstance(s, float) and pd.isna(s)):\n            return None\n        x = str(s).strip().lower()\n        if any(k in x for k in [\"arena\", \"main floor\", \"floor\"]):\n            return \"arena\"\n        if any(k in x for k in [\"meeting\", \"room\", \"annex\"]):\n            return \"meeting room\"\n        return None\n\n    def parse_ids(s):\n        if pd.isna(s):\n            return []\n        parts = re.split(r\"[;,\\n\\r\\t\\s]+\", str(s))\n        return [p.strip() for p in parts if p and p.strip()]\n\n    # sheets\n    sheet_va = find_sheet(output.id, [\"assign\", \"vendor\", \"plan\"])  # vendor assignments\n    sheet_layout = find_sheet(output.id, [\"layout\", \"map\", \"floor\", \"table\"])  # layout mapping\n    if not sheet_va or not sheet_layout:\n        return 0.0, \"Required sheets not found.\"\n\n    try:\n        va = context.files.read_excel(output.id, sheet_name=sheet_va)\n        lm = context.files.read_excel(output.id, sheet_name=sheet_layout)\n    except Exception:\n        return 0.0, \"Failed to read necessary sheets.\"\n\n    col_pref = find_col(va, [\"location preference\", \"preference\", \"preferred location\"])\n    col_assigned_area = find_col(va, [\"assigned area\", \"assigned location\"])\n    col_assigned_tables = find_col(va, [\"assigned table\", \"table assignment\", \"assigned tables\"])\n    if col_pref is None or col_assigned_tables is None:\n        return 0.0, \"Required columns (Location Preference, Assigned Tables) not found.\"\n\n    col_table = find_col(lm, [\"table\", \"table id\", \"table number\"]) \n    col_area = find_col(lm, [\"area\", \"zone\", \"room\"])\n    if col_table is None or col_area is None:\n        return 0.0, \"Required columns (Table, Area) not found in Layout Mapping.\"\n\n    lm_map = lm.copy()\n    lm_map[col_table] = lm_map[col_table].astype(str).str.strip().str.lower()\n    lm_map[col_area] = lm_map[col_area].apply(norm_area)\n    area_by_table = dict(zip(lm_map[col_table], lm_map[col_area]))\n\n    total = 0\n    ok = 0\n    for _, row in va.iterrows():\n        pref = norm_area(row[col_pref])\n        if pref is None:\n            continue\n        assigned_area = None\n        if col_assigned_area is not None:\n            assigned_area = norm_area(row[col_assigned_area])\n        if assigned_area is None:\n            # infer from table mapping\n            tables = [t.lower() for t in parse_ids(row[col_assigned_tables])]\n            # If multiple tables, require all to map to same area; else majority\n            areas = [area_by_table.get(t) for t in tables if t in area_by_table]\n            if areas:\n                # majority vote\n                assigned_area = max(set(areas), key=areas.count)\n        if assigned_area is None:\n            continue\n        total += 1\n        if assigned_area == pref:\n            ok += 1\n\n    if total == 0:\n        return 0.0, \"No vendors with clear preference and assignment to evaluate.\"\n    score = ok / total\n    return score, f\"Location aligned for {ok}/{total} vendors.\""}, {"type": "llm_judge", "name": "Cross-Sheet Consistency (IDs, Areas, Vendors)", "description": "Assigned tables listed for each vendor match the Layout Mapping entries, with consistent vendor names and areas.", "weight": 3.0, "judge_prompt": "Evaluate cross-sheet consistency within the Excel workbook:\n- For a sample of at least 10 vendors (or all if fewer), confirm that each Assigned Tables entry in the Vendor Assignments sheet appears in the Layout Mapping with the SAME vendor/business name (allow for minor naming variations) and matching Area (Arena vs Meeting Room).\n- Spot-check that the Assigned Area in Vendor Assignments (if present) matches the Layout Mapping Area for those tables.\n- Note any missing table IDs, mismatched vendor names, or area inconsistencies.\nScoring:\n- 3.0: All sampled entries consistent; no substantive mismatches.\n- 2.0: Minor inconsistencies (e.g., 1-2 small naming variances) with clear intent and no structural errors.\n- 1.0: Several inconsistencies (3-5) but majority is correct.\n- 0.0: Widespread inconsistencies or cannot locate mapping for many assigned tables.", "expectation": "Vendor Assignments and Layout Mapping reference the same table IDs, the same vendors, and the same areas consistently."}, {"type": "llm_judge", "name": "Preferences and Special Requests Honored or Justified", "description": "Location preferences, electricity needs, and adjacency/specific-neighbor requests are honored where feasible, or deviations are explicitly justified in Constraints & Decisions.", "weight": 2.5, "judge_prompt": "Review the workbook focusing on the Vendor Assignments and Constraints & Decisions sheets:\n- Are location preferences (Arena vs Meeting Room) generally honored? If not, are reasons documented?\n- Are electricity requirements matched to powered tables, and is the Power Allocation sheet consistent?\n- For any vendor requesting to be located next to or away from a specific vendor, is the outcome logged with justification?\n- Are conflicts or trade-offs explicitly noted with a rationale (e.g., limited outlets)?\nScoring:\n- 2.5: Preferences and requests are consistently honored or clearly justified; documentation is complete and specific.\n- 1.5: Mostly honored with some gaps; justifications present but occasionally thin.\n- 0.5: Many unmet preferences without clear documentation.\n- 0.0: Little evidence of honoring preferences or tracking requests.", "expectation": "Clear alignment to preferences and transparent justifications for any deviations."}, {"type": "llm_judge", "name": "Separation of Similar Products (Adjacency)", "description": "Vendors with the same product category are not placed adjacent; where adjacency occurs, it is rare and justified.", "weight": 2.5, "judge_prompt": "Using the product categories/descriptions in Vendor Assignments and the adjacency information in Layout Mapping (adjacent tables or neighbor columns), assess whether vendors selling similar products (e.g., candles) are generally NOT adjacent.\n- Spot-check several adjacency pairs across both areas.\n- If same-category adjacency occurs, check for a clear justification in Constraints & Decisions (e.g., space limits) and whether occurrences are minimized.\nScoring:\n- 2.5: No same-category adjacencies found, or any rare cases are explicitly justified.\n- 1.5: A few lapses but overall separation is evident.\n- 0.5: Frequent same-category adjacencies with weak or no justification.\n- 0.0: No evidence of intentional separation, or adjacency info missing.", "expectation": "Deliberate distribution of similar product types to improve shopper experience, with justifications for any exceptions."}], "on_failure_action": "skip_remaining", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism", "description": "Holistic assessment of presentation, operational readiness, and stakeholder value (LLM-only).", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Operational Readiness and Clarity", "description": "The workbook reads like an implementation-ready plan for setup and day-of operations.", "weight": 2.5, "judge_prompt": "Assess whether the workbook provides clear, actionable guidance for event setup and operations:\n- Are there clear summaries of counts (vendors, tables by area, powered tables used/available)?\n- Are check-in/setup notes, timing, or instructions present (either in Summary, Constraints & Decisions, or a dedicated section)?\n- Is it easy for staff to locate each vendor and understand adjacency/power placement?\nScoring: 2.5 excellent readiness, 1.5 adequate with minor gaps, 0.5 incomplete, 0.0 unclear/unusable.", "expectation": "A staff-ready plan: clear summaries, easy table localization, and basic operational notes."}, {"type": "llm_judge", "name": "Professional Presentation and Accessibility", "description": "Formatting, labeling, and layout are professional and accessible for city staff and vendors.", "weight": 2.0, "judge_prompt": "Evaluate presentation quality:\n- Clear headers, consistent naming, frozen panes, readable fonts, and appropriate use of filters/tables.\n- Basic accessibility considerations (e.g., sufficient contrast, simple legends for power/areas, minimal jargon).\n- Avoids clutter; columns/sections are logically ordered and labeled.\nScoring: 2.0 professional and accessible; 1.0 acceptable with issues; 0.0 poor/unclear.", "expectation": "Well-formatted workbook that is easy to read and navigate."}, {"type": "llm_judge", "name": "Stakeholder-Focused Value (Vendors and Shoppers)", "description": "The plan supports a positive experience for vendors and shoppers via variety and sensible flow.", "weight": 2.0, "judge_prompt": "Assess whether the plan demonstrates thoughtfulness for vendors and shoppers:\n- Variety: similar products distributed to avoid clustering; power users positioned sensibly.\n- Shopper flow: logical paths, balanced coverage across arena and meeting room.\n- Vendor fairness: preferences considered without favoritism.\nScoring: 2.0 strong value for both groups; 1.0 moderate; 0.0 weak or unconsidered.", "expectation": "A balanced plan with variety and fair consideration of stakeholder needs."}, {"type": "llm_judge", "name": "Risk Awareness and Contingencies", "description": "Evidence of contingencies for common risks (no-shows, late power issues, last-minute changes).", "weight": 1.5, "judge_prompt": "Look for notes/plans addressing typical risks:\n- Unassigned/backup tables listed; approach for last-minute vendor swaps.\n- Handling of power failures or insufficient outlets.\n- Process for resolving adjacency conflicts on event day.\nScoring: 1.5 strong contingency planning, 0.8 minimal but present, 0.0 none evident.", "expectation": "Basic contingency planning embedded in Summary or Constraints & Decisions."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "2d06bc0a-89c6-4e89-9417-5ffe725c1bc6", "rubric": {"category_name": "Real Estate Broker LOI (Word) \u2014 CRECO Denver", "rationale": "Pattern B (Document). The task requires a professionally formatted Letter of Intent (LOI) in Word (.docx) with specific business terms, parties, property details, timelines, deposits, and an expiration. Stage 1 uses an LLM gate to enforce exact structure and format that make verification trivial. Stage 2 mixes precise code checks (numeric/consistency) with higher-weight LLM judges for nuanced, cross-referenced validation. Stage 3 applies a holistic quality assessment for professional tone, negotiation posture, and clarity. Code rules in Stage 2 are lightweight and robust, while LLM rules carry ~5x greater weight, per guidance.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Structure and Format Gate (LLM Only)", "description": "Verify the candidate produced a properly structured LOI as a .docx document with the essential sections to enable verification. This is a gate. Failure zeros the category.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 2.8, "rules": [{"type": "llm_judge", "name": "LOI Format and Section Presence (DOCX)", "description": "Check for Word format, page count, professional LOI layout, and presence of all required sections.", "weight": 4.0, "judge_prompt": "You are evaluating whether the submission is a properly structured Letter of Intent (LOI) for a commercial real estate acquisition, saved as a Word document (.docx). Do NOT judge quality yet; only verify presence and structure. Be flexible with exact header names, but ensure the required content is clearly present and organized.\n\nFormat Requirements:\n- Must be a Word document (.docx), not PDF, not plain text.\n- Professional LOI layout with clear headings and sections.\n- Maximum length: 5 pages.\n\nRequired LOI Sections/Elements (flexible titles ok, but content must be clear):\n1) Date and Addressee Block:\n   - Dated as of July 13, 2025\n   - Addressed to selling broker: Bob Crobens, HPTR (downtown Denver office), including the broker\u2019s office address: 457 89th Street, Denver, CO 80202\n   - Optional: Subject/RE line indicating LOI to purchase the subject property\n2) Parties and Property:\n   - Buyer: Annocium Investors\n   - Seller: Denver Services Bank\n   - Property identification: 48,000-sf multi-tenant office building on 4 acres at 536-41 Fraanklyn Ave, Denver, Colorado (be flexible to minor spelling variations of 'Fraanklyn')\n3) Business Terms Summary:\n   - Purchase price reflecting a 6.5% cap rate (price rounded to nearest $100,000)\n   - Escrow with First American Title\n   - Deposits and timing\n   - Feasibility/due diligence period\n   - Closing timing and one-month extension option with additional $20,000 deposit\n   - Closing costs allocation customary for Denver\n   - Buyer to draft PSA; buyer\u2019s right to assign prior to closing\n   - Seller deliverables (e.g., P&L, leases, surveys)\n   - 1031 exchange cooperation (at no cost/burden to seller)\n4) Expiration and Binding Nature:\n   - Explicit expiration date or acceptance window of 7\u201310 days from delivery\n   - Non-binding nature of the LOI (good faith terms only), with any limited binding provisions (e.g., exclusivity/confidentiality) clearly identified\n5) Signature Blocks:\n   - Signature for Buyer (Annocium Investors)\n   - Space for Seller acknowledgement (or similar)\n\nScoring:\n- 4.0: .docx + \u22645 pages + all required elements above are clearly present and labeled.\n- 3.0: .docx + \u22645 pages + missing 1 secondary element (e.g., explicit broker address line or separate seller acknowledgement), core sections intact.\n- 2.0: .docx + \u22645 pages + missing 2\u20133 elements but still resembles a formal LOI with headings.\n- 0.0: Not .docx OR clearly lacks core structure (e.g., no parties or no business terms) OR exceeds 5 pages.\n\nOnly evaluate structure/format and presence. Do not evaluate correctness of numbers or strategy.", "expectation": "A well-structured .docx LOI with all key sections present, not exceeding 5 pages."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Compliance Verification", "description": "Check that specific numeric values, business terms, parties, and cross-references are correct and internally consistent, per instructions. Mix of targeted code rules and higher-weight LLM judges.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Offer Price Consistent with 6.5% Cap (Rounded)", "description": "Verify the LOI states a 6.5% cap rate and an offer price close to NOI/0.065 with rounding to nearest $100,000, using NOI derived from $9,000,000 at 6% cap.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0, \"Unable to read document text\"\n    if not text:\n        return 0.0, \"Empty document\"\n\n    t = text.lower()\n    # Check cap rate mention ~6.5%\n    cap_ok = bool(re.search(r\"(cap\\s*rate[^\\n\\r%]*6\\.5\\s*%|6\\.5\\s*%[^\\n\\r]*cap\\s*rate|cap\\s*rate[^\\n\\r]*6\\.5)\", t))\n\n    # Compute expected offer price from NOI based on 6% cap on $9,000,000\n    listing_price = 9000000.0\n    noi = listing_price * 0.06  # 540,000\n    target_price = noi / 0.065  # ~8,307,692.31\n    # Round to nearest $100,000\n    rounded_target = round(target_price / 100000.0) * 100000.0  # 8,300,000\n\n    # Extract dollar amounts and find any large figure within $100k of rounded target\n    amounts = []\n    for m in re.finditer(r\"\\$\\s*([0-9]{1,3}(?:,[0-9]{3})+(?:\\.[0-9]{2})?|[0-9]{7,})\", text):\n        s = m.group(1)\n        s_clean = s.replace(\",\", \"\")\n        try:\n            val = float(s_clean)\n            amounts.append(val)\n        except Exception:\n            pass\n    # Consider large amounts likely to be purchase price\n    large = [a for a in amounts if a >= 1000000]\n    price_ok = any(abs(a - rounded_target) <= 100000 for a in large)\n\n    score = 0.0\n    details = []\n    if cap_ok:\n        score += 0.4\n    else:\n        details.append(\"Cap rate 6.5% not clearly stated near 'cap rate'.\")\n    if price_ok:\n        score += 0.4\n    else:\n        details.append(f\"No large dollar amount within $100k of expected ${int(rounded_target):,} found.\")\n\n    return score, \"; \".join(details) if details else \"OK\""}, {"type": "code", "name": "Deposits, Timelines, Escrow, and Extension Consistency", "description": "Verify presence and proximity of key numeric business terms: initial $100,000 deposit within 5 days of PSA execution; additional $150,000 after feasibility approval; feasibility period 90 days after PSA execution; closing 90 days after feasibility approval; one-month/30-day extension for $20,000; escrow with First American Title.", "weight": 0.7, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0, \"Unable to read document text\"\n    if not text:\n        return 0.0, \"Empty document\"\n\n    t = text.lower()\n\n    def contains_near(patterns, window_text, max_gap=120):\n        # For each pattern group, require all tokens are within max_gap in the text.\n        for p in patterns:\n            if isinstance(p, str):\n                if p not in window_text:\n                    return False\n            else:\n                # regex\n                if not re.search(p, window_text):\n                    return False\n        return True\n\n    score = 0.0\n    max_score = 0.7\n    checks = []\n\n    # 1) Initial $100,000 within 5 days of PSA execution\n    init_dep = bool(re.search(r\"(initial|earnest)[^\\n\\r]{0,80}?deposit[^\\n\\r]{0,120}?\\$?\\s*100,?000\", t)) and \\\n               bool(re.search(r\"within\\s+5\\s+day\", t)) and \\\n               bool(re.search(r\"psa|purchase\\s+and\\s+sale\\s+agreement\", t)) and \\\n               bool(re.search(r\"execution\", t))\n    checks.append(init_dep)\n\n    # 2) Additional $150,000 after feasibility approval\n    add_dep = bool(re.search(r\"(additional|further|second)[^\\n\\r]{0,80}?deposit[^\\n\\r]{0,120}?\\$?\\s*150,?000\", t)) and \\\n              bool(re.search(r\"feasibilit[y|ies].{0,80}approval|approval.{0,80}feasibilit[y|ies]\", t))\n    checks.append(add_dep)\n\n    # 3) Feasibility period 90 days after PSA execution\n    feas_90 = bool(re.search(r\"feasibilit[y|ies].{0,60}(90|ninety).{0,20}day\", t)) and \\\n              bool(re.search(r\"psa|purchase\\s+and\\s+sale\\s+agreement\", t)) and \\\n              bool(re.search(r\"execution\", t))\n    checks.append(feas_90)\n\n    # 4) Closing 90 days after feasibility approval\n    closing_90 = bool(re.search(r\"closing.{0,80}(90|ninety).{0,20}day\", t)) and \\\n                 bool(re.search(r\"approval.{0,60}feasibilit[y|ies]|feasibilit[y|ies].{0,60}approval\", t))\n    checks.append(closing_90)\n\n    # 5) One-month/30-day extension for $20,000\n    ext_20k = (bool(re.search(r\"extend|extension\", t)) and \\\n               (bool(re.search(r\"one\\s+month|30\\s*day\", t))) and \\\n               bool(re.search(r\"\\$\\s*20,?000\", t)))\n    checks.append(ext_20k)\n\n    # 6) Escrow with First American Title\n    escrow = bool(re.search(r\"first\\s+american(\\s+title)?\", t)) and bool(re.search(r\"escrow\", t))\n    checks.append(escrow)\n\n    satisfied = sum(1 for c in checks if c)\n    per_check = max_score / len(checks)\n    return satisfied * per_check, f\"Satisfied {satisfied}/6 deposit/timeline/escrow checks\""}, {"type": "code", "name": "Letter Date and Expiration Window (7\u201310 days)", "description": "Verify presence of the letter date (July 13, 2025, or equivalent format) and an explicit LOI expiration/acceptance window of 7\u201310 days from delivery.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0, \"Unable to read document text\"\n    if not text:\n        return 0.0, \"Empty document\"\n\n    t = text.lower()\n\n    # Date variants: \"July 13, 2025\", \"July 13th, 2025\", \"07/13/2025\", \"2025-07-13\"\n    date_ok = bool(re.search(r\"july\\s+13(?:st|nd|rd|th)?\\s*,?\\s*2025|07/13/2025|2025-07-13\", t))\n\n    # Expiration 7-10 days: look for expire/valid/acceptance window\n    exp_ctx = re.search(r\"(expire|expiration|valid\\s+until|open\\s+until|acceptance)[^\\n\\r]{0,120}\", t)\n    exp_ok = False\n    if exp_ctx:\n        seg = exp_ctx.group(0)\n        exp_ok = bool(re.search(r\"\\b(7|seven|8|eight|9|nine|10|ten)\\b[^\\n\\r]{0,20}\\bday\", seg))\n\n    score = 0.0\n    if date_ok:\n        score += 0.25\n    if exp_ok:\n        score += 0.25\n    feedback = []\n    if not date_ok:\n        feedback.append(\"Missing or incorrect date (July 13, 2025)\")\n    if not exp_ok:\n        feedback.append(\"Missing explicit 7\u201310 day expiration/acceptance window\")\n    return score, \"; \".join(feedback) if feedback else \"OK\""}, {"type": "llm_judge", "name": "Required Business Terms Present and Aligned", "description": "Check that all specified business terms from the brief are present and align with instructions (not quality, just correctness and inclusion).", "weight": 3.0, "judge_prompt": "Verify the LOI includes and aligns with ALL of the following business terms (flexible wording is fine, but substance must match):\n- Purchase price reflecting a 6.5% cap rate and rounded to the nearest $100,000 (do not recompute exact number; just confirm the LOI states 6.5% cap and a rounded price figure).\n- Deposits: $100,000 initial within 5 days of PSA execution; additional $150,000 after feasibility approval.\n- Feasibility period: 90 days after PSA execution.\n- Closing: 90 days after feasibility approval, with one-month option (\u224830 days) to extend for an additional $20,000 deposit.\n- Escrow with First American Title.\n- Buyer drafts the PSA and reserves right to assign prior to closing.\n- Seller deliverables: customary information (P&L, leases, surveys, etc.).\n- 1031 exchange cooperation at no cost/burden to seller.\n- Closing costs to be split as customary in Denver.\n\nScoring:\n- 3.0: All listed terms present and aligned.\n- 2.0: Missing 1 minor item (e.g., closing costs phrasing) but core terms present.\n- 1.0: Missing 2\u20133 items.\n- 0.0: Missing multiple core items or materially misstates them.", "expectation": "All enumerated terms are explicitly included and correctly framed."}, {"type": "llm_judge", "name": "Parties, Property, and Addressing Details", "description": "Check identity of parties, property description, and addressee details, including date.", "weight": 1.5, "judge_prompt": "Confirm the LOI correctly states:\n- Buyer: Annocium Investors; Seller: Denver Services Bank.\n- Property: 48,000-sf multi-tenant office building on 4 acres at 536-41 Fraanklyn Ave, Denver, CO (tolerate minor misspelling of 'Fraanklyn').\n- Addressed to: Bob Crobens, HPTR\u2019s downtown Denver office, with address 457 89th Street, Denver, CO 80202.\n- Letter date: July 13, 2025.\n\nScoring:\n- 1.5: All four are correct.\n- 1.0: Three correct.\n- 0.5: Two correct.\n- 0.0: One or none correct.", "expectation": "All identities and address/date details are correct."}, {"type": "llm_judge", "name": "Non-Binding Nature and Limited Binding Provisions", "description": "Verify LOI is explicitly non-binding (except any limited binding provisions if stated) and framed as good-faith terms for a definitive PSA.", "weight": 2.5, "judge_prompt": "Check that the LOI explicitly states it is non-binding, serving as a good-faith outline of business terms for a subsequent PSA. If any limited binding terms are included (e.g., confidentiality, exclusivity), they should be clearly identified as binding while the remainder remains non-binding.\n\nScoring:\n- 2.5: Clear non-binding statement and any limited binding carve-outs are explicit and consistent.\n- 1.5: Non-binding intent is present but phrasing is weak or ambiguous.\n- 0.5: Non-binding intent is implied but not clearly stated.\n- 0.0: Incorrectly presented as binding overall or unclear.", "expectation": "Clear non-binding framing, with any limited binding items clearly delineated."}, {"type": "llm_judge", "name": "Property Description and Broker Involvement", "description": "Validate the property description completeness and that the selling broker is referenced appropriately.", "weight": 3.0, "judge_prompt": "Confirm the LOI includes a clear and complete property description (size: 48,000 sf; site: 4 acres; address: 536-41 Fraanklyn Ave, Denver, Colorado) and properly references the selling broker (Bob Crobens of HPTR) as the addressee and/or within the body where appropriate.\n\nScoring:\n- 3.0: Full property description and broker reference are clear and correct.\n- 2.0: Minor omissions (e.g., omits acres or square footage) but address and broker are correct.\n- 1.0: Significant omissions or unclear broker reference.\n- 0.0: Lacks clear property description and no broker reference.", "expectation": "Property details and broker involvement are clearly and correctly specified."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Strategic Effectiveness", "description": "Holistic LLM assessment of formatting, clarity, tone, and negotiation posture appropriate for a commercial real estate LOI.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Formatting and LOI Structure", "description": "Assess professional presentation, clear headings, readable layout, and within 5 pages.", "weight": 2.0, "judge_prompt": "Evaluate whether the LOI presents as a professional, standard real estate LOI: clear headings, logical section ordering (parties, property, price, deposits, feasibility, closing, deliverables, other terms, expiration, signatures), clean typography, and page count within 5 pages.\n\nScoring:\n- 2.0: Highly professional, well-structured, clearly within 5 pages.\n- 1.2: Generally professional, minor formatting issues.\n- 0.6: Adequate but somewhat disorganized or cluttered.\n- 0.0: Poorly formatted or confusing.", "expectation": "Professional, clean LOI structure within 5 pages."}, {"type": "llm_judge", "name": "Clarity and Concision", "description": "Assess whether the LOI is succinct, avoids unnecessary legalese, and is easy to interpret for business decision-making.", "weight": 1.5, "judge_prompt": "Assess clarity and concision: Are terms succinct and unambiguous? Does the LOI avoid excessive legalese and stick to business terms suitable for an LOI (reserving granularity for the PSA)?\n\nScoring:\n- 1.5: Very clear and concise; easy to read and interpret.\n- 1.0: Mostly clear; a few verbose or ambiguous areas.\n- 0.5: Readable but contains unnecessary complexity.\n- 0.0: Confusing or overly legalistic.", "expectation": "Succinct, clear business terms; complexity deferred to PSA."}, {"type": "llm_judge", "name": "Negotiation Posture and Strategic Framing", "description": "Evaluate whether the LOI frames terms persuasively for a 6.5% cap rate offer and uses the expiration to induce timely negotiations while remaining respectful and constructive.", "weight": 1.5, "judge_prompt": "Evaluate strategic tone and posture: Does the LOI present a persuasive rationale consistent with a 6.5% cap rate offer? Are timelines and extension terms reasonable and balanced? Is the expiration used to induce negotiations without being adversarial? Is the tone courteous and professional throughout?\n\nScoring:\n- 1.5: Strong, balanced negotiation framing; persuasive and professional.\n- 1.0: Generally sound posture; minor missed opportunities.\n- 0.5: Weak or inconsistent framing.\n- 0.0: Inappropriate or counterproductive tone.", "expectation": "Balanced, professional, and persuasive negotiation framing."}, {"type": "llm_judge", "name": "Risk Management and Compliance Signals", "description": "Evaluate that the LOI responsibly delineates non-binding status, avoids overcommitment, and uses accurate/consistent information.", "weight": 1.0, "judge_prompt": "Assess risk management cues: Is the non-binding status unmistakable? Are any binding carve-outs (if included) narrowly tailored? Are names, addresses, and figures consistent throughout? Does the LOI avoid making warranties or commitments inappropriate for an LOI?\n\nScoring:\n- 1.0: Clear risk controls and consistency throughout.\n- 0.6: Minor inconsistencies.\n- 0.3: Several issues that could create ambiguity.\n- 0.0: Material risk or inconsistency issues.", "expectation": "Clear non-binding delineation, consistent facts, and no inappropriate commitments."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "7151c60a-d4cb-4fc4-8169-3d4cb446e6b9", "rubric": {"category_name": "Healthcare Docs: Dialysis Transfer Fax Cover + Pre-Screen Checklist", "rationale": "This rubric enforces a self-documenting, two-document deliverable: a one-page fax cover sheet and a \u22642-page Patient Admission Pre-Screening Checklist. Stage 1 (LLM-only) strictly mandates document shape and presence of required structural elements so verification becomes trivial. Stage 2 mixes light code checks (to deterministically verify key strings/fields) with LLM checks for compliance cues and cross-references. Stage 3 provides a holistic quality assessment focused on professional presentation, clarity for administrative staff, and appropriateness to seasonal dialysis transfers in Florida.", "max_total_score": 24.0, "stages": [{"name": "Stage 1: Format and Structural Gate", "description": "LLM-only validation that both required documents exist as DOCX/PDF with mandated sections, fields, and layout. Gate must pass to proceed.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.5, "rules": [{"type": "llm_judge", "name": "Fax Cover Sheet Structural Requirements", "description": "Verify presence and structure of the one-page fax cover sheet document.", "weight": 2.0, "judge_prompt": "You are evaluating ALL candidate outputs. Determine whether there is a single-page Fax Cover Sheet (in DOCX or PDF) with the exact structural requirements below. Be flexible with minor wording but strict about the presence of the elements. Do NOT judge writing quality\u2014just structure and fields exist.\n\nRequired for the Fax Cover Sheet:\n- File type: DOCX or PDF\n- Exactly one page\n- Clearly labeled as a Fax Cover Sheet at the top\n- Company logo visible near the top (logo image or clear placeholder labeled as logo)\n- Sender section with fillable fields for: Sender Name, Fax Number, Telephone Number\n- Recipient section with fillable fields for: Recipient/To, Fax Number, Telephone Number\n- Fields for: Date, Subject, Number of Pages (including cover)\n- Action/checkbox options present: Urgent, For Review, Please Comment, Please Reply\n- A Confidentiality Statement present on the page (full text or clearly labeled section)\n\nScoring:\n- 2.0: All required elements present, one page, clear sections and fields visible\n- 1.0\u20131.5: Mostly present; missing up to two minor items (e.g., one field label or one checkbox option) but still clearly a usable fax cover sheet\n- 0.5: Several elements missing but recognizable attempt at a fax cover sheet in correct format and 1 page\n- 0.0: Not present, wrong file type, more than one page, or missing multiple core sections (sender/recipient/date/subject/pages/confidentiality)\n\nOnly check presence/structure; do not assess content accuracy.", "expectation": "A professional, one-page fax cover sheet DOCX/PDF with logo, labeled sections for sender/recipient/date/subject/pages, the four checkbox options, and a confidentiality statement."}, {"type": "llm_judge", "name": "Patient Admission Pre-Screening Checklist Structural Requirements", "description": "Verify presence and structure of the \u22642-page Patient Admission Pre-Screening Checklist document.", "weight": 3.0, "judge_prompt": "Evaluate ALL outputs. Determine whether there is a Patient Admission Pre-Screening Checklist document (DOCX or PDF) with the exact structural requirements below. Be strict about presence, flexible with minor wording.\n\nRequired for the Checklist:\n- File type: DOCX or PDF\n- Length: no more than 2 pages\n- Title: Clearly identifies as \"Facility Admission: Pre-Screening Checklist\"\n- Company logo visible on the document\n- Page footer contains page numbers on all pages\n- At the top of each page (above the table): spaces/fields to enter Patient Name and Date of Birth\n- Main content presented as a table that includes columns to allow documentation of: Requested Item/Requirement; Date Sent; Date Received; Initials\n- Clear instruction indicating: \"Date Received and Initials are completed by Internal Dialysis Facility Staff Only\" (or equivalent wording)\n- Includes the contact and process statement verbatim or near-verbatim: \n  \u2022 \"Please fax or send the information requested to Fax #: (000) 111-1234 or Email: Sunny@Sunnydialysisclinic.com. Please include your preferred method of contact with the requested documents. Our clinical team will review within 48 hours of receiving ALL required documents and notification of the facility\u2019s decision will be sent to the preferred method of contact provided.\"\n\nScoring:\n- 3.0: All required elements present and clearly structured; \u22642 pages\n- 2.0\u20132.5: One to two minor elements missing (e.g., page numbers format slightly different) but the document is still clearly compliant and \u22642 pages\n- 1.0: Multiple structural elements missing, but it's clearly a pre-screening checklist with the required table columns and \u22642 pages\n- 0.0: Not present, wrong format, or >2 pages\n\nOnly check presence/structure; do not judge content quality or medical completeness.", "expectation": "A \u22642-page DOCX/PDF checklist with logo, title, page numbers, patient name/DOB fields on each page, a table including Date Sent, Date Received, Initials, clear internal-only notation, and the required contact/process statement."}, {"type": "llm_judge", "name": "Two Separate Documents Present", "description": "Confirm both required documents exist as separate files: one fax cover sheet and one checklist.", "weight": 1.0, "judge_prompt": "Review all outputs together. Confirm there are TWO distinct documents (DOCX or PDF):\n1) A one-page Fax Cover Sheet with logo, fields, the four checkbox options, and a confidentiality statement.\n2) A \u22642-page Facility Admission: Pre-Screening Checklist with logo, title, page numbers, patient name/DOB fields, required table columns, internal-only notation, and the contact/process statement.\n\nScoring:\n- 1.0: Both documents present as separate files in acceptable formats\n- 0.5: Both documents exist but are combined into a single multi-page file, or one exists correctly while the other is embedded/less distinct\n- 0.0: Only one document present or neither present", "expectation": "Two separate DOCX/PDF files: one fax cover sheet and one pre-screening checklist."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Verification and Compliance Checks", "description": "Mixed verification to ensure key requirements are explicitly included and usable. Code rules perform deterministic checks; LLM rules confirm compliance cues and comprehensive content coverage.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Two Document Files Detected (Type Check)", "description": "Verify at least two document outputs (DOCX or PDF) are present.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs() or []\n    doc_outputs = [r for r in outputs if getattr(r, 'is_document', False)]\n    score = 0.0\n    if len(doc_outputs) >= 2:\n        score = 0.5\n    elif len(doc_outputs) == 1:\n        score = 0.25\n    else:\n        score = 0.0\n    return score"}, {"type": "code", "name": "Contact Instructions and Timeline Present", "description": "Search all documents for the mandated contact lines and review-timeline language.", "weight": 0.7, "code": "import re\n\ndef _read_text(context, rid):\n    text = ''\n    try:\n        text = context.files.read_docx_text(rid)\n    except Exception:\n        pass\n    if not text:\n        try:\n            text = context.files.read_pdf_text(rid)\n        except Exception:\n            pass\n    return text or ''\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs() or []\n    docs = [r for r in outputs if getattr(r, 'is_document', False)]\n    full_text = '\\n'.join(_read_text(context, r.id) for r in docs).lower()\n    need_patterns = [\n        r\"fax\\s*#:?\\s*\\(000\\)\\s*111-1234\",\n        r\"email\\s*:\\s*sunny@sunnydialysisclinic\\.com\",\n        r\"preferred method of contact\",\n        r\"within\\s*48\\s*hours\"\n    ]\n    hits = 0\n    for pat in need_patterns:\n        if re.search(pat, full_text):\n            hits += 1\n    return 0.7 * (hits / len(need_patterns))"}, {"type": "code", "name": "Urgency Options Present in Fax Cover", "description": "Detect the four urgency/review options in the fax cover sheet text.", "weight": 0.5, "code": "import re\n\ndef _read_text(context, rid):\n    text = ''\n    try:\n        text = context.files.read_docx_text(rid)\n    except Exception:\n        pass\n    if not text:\n        try:\n            text = context.files.read_pdf_text(rid)\n        except Exception:\n            pass\n    return text or ''\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs() or []\n    docs = [r for r in outputs if getattr(r, 'is_document', False)]\n    # Heuristic: pick the doc that mentions 'fax' or 'cover'\n    fax_texts = []\n    for r in docs:\n        t = _read_text(context, r.id).lower()\n        if 'fax' in t or 'cover sheet' in t:\n            fax_texts.append(t)\n    text = ' '.join(fax_texts) if fax_texts else ' '.join(_read_text(context, r.id).lower() for r in docs)\n    options = [\"urgent\", \"for review\", \"please comment\", \"please reply\"]\n    found = sum(1 for o in options if o in text)\n    return 0.5 * (found / len(options))"}, {"type": "code", "name": "Internal Staff Only Notation in Checklist", "description": "Confirm the checklist marks Date Received and Initials as for Internal Dialysis Facility Staff Only.", "weight": 0.3, "code": "import re\n\ndef _read_text(context, rid):\n    text = ''\n    try:\n        text = context.files.read_docx_text(rid)\n    except Exception:\n        pass\n    if not text:\n        try:\n            text = context.files.read_pdf_text(rid)\n        except Exception:\n            pass\n    return text or ''\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs() or []\n    docs = [r for r in outputs if getattr(r, 'is_document', False)]\n    text = '\\n'.join(_read_text(context, r.id) for r in docs).lower()\n    pats = [\n        r\"internal dialysis facility staff only\",\n        r\"date received\",\n        r\"initials\"\n    ]\n    hits = sum(1 for p in pats if re.search(p, text))\n    return 0.3 * (hits / len(pats))"}, {"type": "llm_judge", "name": "Fillable Fields and Checkboxes Usability", "description": "Confirm fields have visible blanks/areas to fill and checkboxes are present for the four options on the fax cover.", "weight": 3.0, "judge_prompt": "Review both documents. Assess whether:\n- Fax Cover Sheet: The Sender/Recipient sections, Date, Subject, and Number of Pages fields are clearly fillable (e.g., lines, blanks, form fields). The four options (Urgent, For Review, Please Comment, Please Reply) are represented as checkboxes or clear selectable markers.\n- Checklist: The table clearly allows entries for Date Sent, Date Received, and Initials, with enough space in each row to write or type.\n\nScoring:\n- 3.0: All listed fields are obviously fillable and the checkboxes are clearly present/functional-looking\n- 2.0: Minor spacing/visibility issues but fields and checkboxes are present and usable\n- 1.0: Some fields missing or not obviously fillable; checkboxes unclear\n- 0.0: Fields or checkboxes largely absent or unusable", "expectation": "Both documents present clear, fillable fields and checkboxes that staff can use without confusion."}, {"type": "llm_judge", "name": "Confidentiality Statement Quality (HIPAA-style)", "description": "Evaluate whether the fax confidentiality statement is appropriate and complete.", "weight": 2.0, "judge_prompt": "Inspect the Fax Cover Sheet\u2019s confidentiality statement. Determine whether it resembles a standard HIPAA-style fax disclaimer including the following ideas: the message is intended only for the named recipient; unauthorized review, use, disclosure, or distribution is prohibited; if received in error, notify the sender and destroy the material.\n\nScoring:\n- 2.0: Contains all key concepts clearly\n- 1.0: Partially complete (missing one concept or phrased ambiguously)\n- 0.0: Absent or clearly inadequate", "expectation": "A standard, adequate HIPAA-style fax confidentiality statement is present and complete."}, {"type": "llm_judge", "name": "Checklist Content Coverage (Dialysis Transfer Essentials)", "description": "Judge whether the checklist reasonably covers standard dialysis transfer requirements, given the unspecified external list.", "weight": 3.0, "judge_prompt": "Since the exact external required patient information list is not available, evaluate whether the checklist\u2019s table items reasonably cover typical dialysis transfer requirements. Look for items such as: patient demographics/face sheet; insurance; nephrologist and dialysis orders (Rx, dry weight, treatment time, dialyzer, bath, heparin protocol); access type/status; allergy and medication lists; recent dialysis run sheets; labs (including Hep B surface antigen/antibody with dates, Hep C, TB screening); vaccination status; problem list/diagnoses; recent hospitalizations; MAR; advanced directives/consents; emergency contacts; sending facility contact info. Allow comprehensive placeholders (e.g., \u201cOther/Additional Required Documents\u201d) to substitute for niche items.\n\nScoring:\n- 3.0: Broad and coherent coverage of the majority of these categories\n- 2.0: Reasonable coverage with a few notable gaps\n- 1.0: Limited coverage; misses many standard items\n- 0.0: Minimal or irrelevant checklist items", "expectation": "The checklist reflects a comprehensive, practical set of items needed to safely accept a transferred dialysis patient."}, {"type": "llm_judge", "name": "Footer Page Numbers on Checklist", "description": "Verify that the checklist shows page numbers in the footer on all pages.", "weight": 1.0, "judge_prompt": "Check the Patient Admission Pre-Screening Checklist. Are page numbers visible in the footer on all pages?\n\nScoring:\n- 1.0: Yes, on all pages\n- 0.5: Present but inconsistent or not in the footer\n- 0.0: Not present", "expectation": "Page numbers appear in the footer on each page of the checklist."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3: Professional Quality and Fitness for Purpose", "description": "Holistic LLM assessment of presentation quality, clarity for administrative staff, and appropriateness for seasonal dialysis transfers in Florida.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Formatting and Readability", "description": "Assess typography, layout, spacing, and visual clarity.", "weight": 3.0, "judge_prompt": "Evaluate both documents for professional formatting: consistent fonts and headings, aligned fields/tables, appropriate white space, logo sized and positioned cleanly, single page for fax cover, \u22642 pages for checklist, and overall readability.\n\nScoring:\n- 3.0: Highly professional, clean, and easy to read\n- 2.0: Generally professional with minor issues\n- 1.0: Noticeable formatting issues but usable\n- 0.0: Poorly formatted or hard to read", "expectation": "Clean, consistent, and professional layouts suitable for clinical administration."}, {"type": "llm_judge", "name": "Clarity and Staff Usability", "description": "Judge how easily administrative staff can follow and use the documents.", "weight": 2.0, "judge_prompt": "Consider whether administrative staff can easily understand and use the documents. Are instructions prominent? Are fields/columns clearly labeled with adequate space? Are patient name/DOB present on each checklist page? Are steps to send materials and expected review timeframe clear?\n\nScoring:\n- 2.0: Very clear and straightforward to use\n- 1.0: Mostly clear with minor ambiguities\n- 0.0: Confusing or missing key guidance", "expectation": "Documents are intuitive and clearly labeled, allowing efficient intake processing."}, {"type": "llm_judge", "name": "Fit for Florida Seasonal Transfer Context", "description": "Evaluate whether the checklist anticipates out-of-state temporary transfers.", "weight": 2.0, "judge_prompt": "Assess whether the checklist is well-suited for seasonal transfers (patients visiting Florida Nov\u2013Apr). Look for fields or sections accommodating sending facility details, temporary address/contact while in Florida, travel dates/anticipated schedule, and preferred method of contact. These can be explicit fields or clearly directed notes.\n\nScoring:\n- 2.0: Strongly tailored to the seasonal transfer context\n- 1.0: Generally applicable with some transfer-related cues\n- 0.0: Generic with no evident consideration of transfer context", "expectation": "The checklist anticipates practical needs of temporary dialysis transfers to Florida."}, {"type": "llm_judge", "name": "Consistency and Error-Free Presentation", "description": "Check for internal consistency and absence of obvious errors.", "weight": 1.0, "judge_prompt": "Verify consistent contact information across documents, consistent terminology (e.g., fax/facsimile, sender/recipient labels), and absence of obvious typos or truncated/conflicting statements (especially the confidentiality statement and contact lines).\n\nScoring:\n- 1.0: No notable inconsistencies or errors\n- 0.5: Minor issues\n- 0.0: Multiple or significant inconsistencies/errors", "expectation": "Documents are polished, consistent, and free of obvious mistakes."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "401a07f1-d57e-4bb0-889b-22de8c900f0e", "rubric": {"category_name": "Information \u2014 Editors: 500-word Science Editorial", "rationale": "This rubric enforces a self-documenting, file-based workflow for a professional editorial. Stage 1 is a strict LLM-only structural gate requiring a DOCX/PDF with headline, standfirst, ~500-word body, in-text links to reputable outlets, and a visible call to action. Stage 2 mixes light code checks (word count, link domains) with higher-weight LLM verification of opinion clarity, factual support/traceability, Guardian style alignment, and call-to-action specificity. Stage 3 applies LLM-only holistic quality assessment for narrative craft, audience fit, headline/standfirst effectiveness, and timeliness/originality. Code rules carry much less weight than LLM judgments, while Stage 1 gating ensures verification is possible.", "max_total_score": 12.0, "stages": [{"name": "Stage 1 \u2014 Format and Structural Gate: Editorial Document", "description": "LLM-only gate to confirm the candidate produced a properly structured editorial document enabling verification.", "is_required": true, "max_points": 2.0, "min_score_to_pass": 1.5, "rules": [{"type": "llm_judge", "name": "Editorial Document Structure Requirements", "description": "Check that the output is a DOCX or PDF editorial with the mandated structure and elements present to enable verification.", "weight": 2.0, "judge_prompt": "You are evaluating whether the candidate produced a properly structured editorial document. Examine the primary output only.\n\nFormat requirements:\n- File type must be a DOCX or PDF (not plain text, not spreadsheet).\n- Approximately 500 words of body copy (acceptable range for shape check: 400\u2013650 words). Do not count header/footer/boilerplate.\n\nStructural requirements:\n1) A clear headline at the top.\n2) A standfirst (short subheading/summary) immediately beneath the headline.\n3) Body copy organized into paragraphs with a clear narrative flow (beginning, development, end).\n4) At least two in-text hyperlinks to reputable outlets, drawn from: Nature (nature.com), Science (science.org or sciencemag.org), Scientific American (scientificamerican.com), and the Guardian (theguardian.com). Links must appear inline (not only as footnotes or a detached list) and be referenced in the copy.\n5) A visible call to action near the end (addressed to researchers, policymakers, or the informed public).\n\nScoring (0.0\u20132.0):\n- 2.0: DOCX/PDF with all five structural requirements present and body length ~400\u2013650 words.\n- 1.5: DOCX/PDF with only one minor element missing or ambiguous (e.g., standfirst present but not clearly separated; or only one qualifying link), body length within 350\u2013700 words.\n- 1.0: DOCX/PDF but missing two structural elements OR body length far off (under 350 or over 700) while other elements are present.\n- 0.0: Not DOCX/PDF OR missing multiple core elements (e.g., no headline/standfirst and no qualifying links).\n\nOnly check structure and presence/placement, not the quality or correctness of content.", "expectation": "A professionally structured editorial document (DOCX/PDF) with headline, standfirst, ~500 words of body text, two or more in-text links to specified reputable outlets, and a closing call to action."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification: Compliance and Correctness", "description": "Mixed verification of compliance details and correctness. Code rules do deterministic checks; LLM rules verify opinion clarity, sourcing/traceability, Guardian style, and the call to action.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Word Count Within Target Band", "description": "Verify body length is near the requested 500 words.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected\"\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        pass\n    if not text:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            pass\n    if not text:\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            return 0.0, \"Unable to read document text\"\n    # Rough word count\n    words = re.findall(r\"\\b\\w[\\w'-]*\\b\", text)\n    wc = len(words)\n    # Scoring: full if 450\u2013600; partial if 400\u2013650; small credit if 350\u2013700\n    if 450 <= wc <= 600:\n        score = 1.0\n    elif 400 <= wc <= 650:\n        score = 0.6\n    elif 350 <= wc <= 700:\n        score = 0.3\n    else:\n        score = 0.0\n    return score, f\"Word count detected: {wc}\""}, {"type": "code", "name": "Reputable Source Links Present", "description": "Detect inline URLs and award credit if at least two come from allowed domains (Nature, Science, Scientific American, the Guardian).", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected\"\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        pass\n    if not text:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            pass\n    if not text:\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            return 0.0, \"Unable to read document text\"\n    urls = re.findall(r'https?://[^\\s)>\\]]+', text)\n    domains = []\n    for u in urls:\n        m = re.match(r'https?://([^/]+)', u)\n        if m:\n            domains.append(m.group(1).lower())\n    allowed = [\n        'nature.com',\n        'www.nature.com',\n        'science.org',\n        'www.science.org',\n        'sciencemag.org',\n        'www.sciencemag.org',\n        'scientificamerican.com',\n        'www.scientificamerican.com',\n        'theguardian.com',\n        'www.theguardian.com',\n        'guardian.com',\n        'www.guardian.com'\n    ]\n    allowed_hits = [d for d in domains if any(d==a or d.endswith('.'+a) for a in allowed)]\n    # Fallback: brand mentions if URLs not extractable\n    brands = re.findall(r\"\\b(Nature|Science|Scientific\\s+American|The\\s+Guardian)\\b\", text, flags=re.I)\n    if len(allowed_hits) >= 2:\n        score = 1.0\n    elif len(allowed_hits) == 1 and len(urls) >= 2:\n        score = 0.7\n    elif len(brands) >= 2:\n        score = 0.4\n    else:\n        score = 0.0\n    fb = f\"URLs found: {len(urls)}; Allowed-domain links: {len(allowed_hits)}; Brand mentions: {len(brands)}\"\n    return score, fb"}, {"type": "llm_judge", "name": "Opinion Clarity and Consistency", "description": "Editorial stance is explicit early and sustained logically throughout.", "weight": 1.5, "judge_prompt": "Assess the document for a clear editorial opinion. Criteria:\n- The central viewpoint is explicitly stated in the headline, standfirst, or opening two paragraphs.\n- The stance is maintained and developed coherently across the piece (no contradictory positions).\n- The conclusion returns to and reinforces the stated position.\nScoring (0\u20131.5):\n- 1.5: Clear, explicit stance early; consistent, well-developed reasoning; strong close.\n- 1.0: Stance present but somewhat hedged or inconsistently emphasized; acceptable close.\n- 0.5: Implicit or weak stance; noticeable hedging/conflict.\n- 0.0: No discernible opinion.\nProvide brief justification referencing where the opinion is stated and how it\u2019s maintained.", "expectation": "A clearly articulated and consistently maintained editorial stance."}, {"type": "llm_judge", "name": "Factual Support and Traceability to Sources", "description": "Key factual assertions are supported by and traceable to reputable linked sources.", "weight": 1.5, "judge_prompt": "Evaluate whether the editorial\u2019s key factual claims are backed by and traceable to the linked sources from Nature, Science, Scientific American, or the Guardian.\n- Identify the main factual assertions.\n- Check that each is anchored to at least one in-text link or clearly attributable reference.\n- Note any claims that appear unsupported or contradictory to common knowledge in these outlets.\nScoring (0\u20131.5):\n- 1.5: All major facts are traceable to the provided reputable links; no obvious unsupported claims.\n- 1.0: Most facts supported; one notable unsupported or weakly attributed claim.\n- 0.5: Several claims lack clear attribution.\n- 0.0: Little to no traceability to reputable sources.\nDo not perform external browsing; rely on visible links and attributions in the document.", "expectation": "All key facts have clear, reputable sourcing via in-text links."}, {"type": "llm_judge", "name": "Guardian Style Alignment (High-Level)", "description": "High-level alignment with Guardian style guide conventions (tone, spelling, punctuation, capitalisation).", "weight": 1.0, "judge_prompt": "Judge high-level alignment with the Guardian style guide (no need to be pedantic):\n- Overall tone and readability consistent with Guardian editorials.\n- UK spelling where appropriate; sensible capitalisation; restrained use of acronyms; minimal jargon.\n- Punctuation and numbers use broadly in line with the guide (no Oxford comma unless clarity demands, en dashes, etc.).\nScoring (0\u20131.0):\n- 1.0: Strong alignment with few or no noticeable departures.\n- 0.7: Generally aligned with some minor inconsistencies.\n- 0.4: Mixed alignment with recurring issues.\n- 0.0: Clearly not aligned in tone or style.\nCite 2\u20133 concrete examples to justify the score.", "expectation": "Copy broadly aligns with Guardian style expectations."}, {"type": "llm_judge", "name": "Call to Action Specificity and Relevance", "description": "Evaluate whether the call to action is explicit, actionable, and appropriate to researchers/policymakers/informed public.", "weight": 1.0, "judge_prompt": "Assess the call to action near the end:\n- Is it explicit and targeted (e.g., to researchers, policymakers, funders, institutions, or the informed public)?\n- Is it feasible and specific (e.g., propose steps, timelines, or mechanisms)?\n- Is it justified by the preceding argumentation?\nScoring (0\u20131.0):\n- 1.0: Clear, targeted, feasible CTA grounded in the piece\u2019s argument.\n- 0.7: Clear CTA but generic or weakly targeted.\n- 0.4: Vague or rhetorical CTA.\n- 0.0: No CTA present.\nBriefly quote or paraphrase the CTA to justify your score.", "expectation": "A targeted, actionable CTA aligned with the editorial\u2019s argument."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Editorial Excellence", "description": "Holistic LLM evaluation of narrative craft, audience fit, headline/standfirst effectiveness, and timeliness/originality.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Narrative Arc and Readability", "description": "Clarity of setup, development, and resolution; paragraphing; flow and coherence.", "weight": 1.5, "judge_prompt": "Evaluate narrative structure and readability:\n- Clear beginning (context/problem), middle (analysis/argument), and end (resolution/close).\n- Smooth transitions; coherent paragraph structure; avoids redundancy.\n- Maintains reader engagement for an international science-policy audience.\nScoring (0\u20131.5):\n- 1.5: Strong arc, excellent flow, highly readable.\n- 1.0: Solid arc with minor issues.\n- 0.5: Choppy or uneven flow; partial arc.\n- 0.0: Disorganised or difficult to follow.", "expectation": "A well-structured, engaging narrative with clear arc and flow."}, {"type": "llm_judge", "name": "Audience Fit and Accessibility", "description": "Suitability for international researchers and policymakers; clarity, precision, minimal jargon.", "weight": 1.0, "judge_prompt": "Assess audience fit:\n- Addresses an international readership; avoids region-specific assumptions.\n- Minimises jargon; explains necessary terms concisely.\n- Provides sufficient context for policymakers and researchers without oversimplifying.\nScoring (0\u20131.0):\n- 1.0: Highly accessible and globally relevant.\n- 0.7: Generally accessible with minor gaps.\n- 0.4: Noticeable jargon/assumption issues.\n- 0.0: Poor fit for the stated audience.", "expectation": "Globally relevant, accessible editorial for researchers and policymakers."}, {"type": "llm_judge", "name": "Headline and Standfirst Effectiveness", "description": "Strength, accuracy, and alignment of headline and standfirst with the piece.", "weight": 0.8, "judge_prompt": "Evaluate the headline and standfirst:\n- Accurate reflection of the argument and scope.\n- Strong, concise wording that attracts attention without overclaiming.\n- Complementary roles: standfirst adds context or angle not fully in headline.\nScoring (0\u20130.8):\n- 0.8: Precise, compelling, well-aligned pairing.\n- 0.5: Generally good with minor issues.\n- 0.2: Weak or somewhat misleading.\n- 0.0: Misleading or unsuitable.", "expectation": "Compelling, accurate headline and a supportive, informative standfirst."}, {"type": "llm_judge", "name": "Timeliness and Originality of Angle", "description": "Topical relevance and freshness of perspective within current science news.", "weight": 0.7, "judge_prompt": "Assess the editorial\u2019s topicality and originality:\n- Clearly tied to recent or ongoing science news.\n- Offers a fresh or valuable perspective beyond summary.\n- Avoids clich\u00e9s and generic platitudes.\nScoring (0\u20130.7):\n- 0.7: Highly timely with a distinct, original angle.\n- 0.5: Timely with some originality.\n- 0.2: Timely but generic.\n- 0.0: Not evidently timely.", "expectation": "A timely editorial with a distinctive, thoughtful angle."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "3600de06-3f71-4e48-9480-e4828c579924", "rubric": {"category_name": "Finance/Insurance \u2014 Advisor Enablement Deck (CDs vs Variable Annuities)", "rationale": "This rubric enforces a self-documenting, verifiable 10-slide advisor deck that argues against rolling CDs into variable annuities. Stage 1 (LLM-only) strictly mandates the slide format and structure so verification is trivial. Stage 2 mixes light code checks (citations/keyword coverage) with heavier LLM verification for regulatory alignment and factual accuracy. Stage 3 assesses professional quality and usefulness for field advisors.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Presentation Format and Structure Gate", "description": "LLM-only gate that requires a 10-slide presentation with specific slide structure enabling verification of content in later stages. Failure zeros the category.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.1, "rules": [{"type": "llm_judge", "name": "Deck Format and Slide Count", "description": "Checks that the candidate output is a slide deck (PDF or PPTX) with exactly 10 slides and professional slide formatting (clear titles on each slide).", "weight": 1.2, "judge_prompt": "You are checking ONLY format/structure, not content quality. Examine the submitted file.\n\nRequirements:\n- Must be a slide deck in PDF or PPTX format (not Word, not Excel, not plain text).\n- Must have exactly 10 slides/pages.\n- Each slide should have a visible, concise slide title.\n\nScoring (0 to 1.2):\n- 1.2: PDF/PPTX with exactly 10 slides, all slides have clear titles.\n- 0.8: PDF/PPTX with exactly 10 slides but 1\u20132 slides missing clear titles.\n- 0.4: PDF/PPTX with 8\u201312 slides and mostly titled, but not exactly 10.\n- 0.0: Not a slide deck format OR fewer than 8 slides OR more than 12 slides OR largely untitled.\nOnly evaluate format and count; do not judge content accuracy.", "expectation": "A 10-slide PDF/PPTX with clear slide titles."}, {"type": "llm_judge", "name": "Required Section Presence", "description": "Checks that required sections are present across the 10 slides in a reasonable structure to enable later verification.", "weight": 1.8, "judge_prompt": "You are checking ONLY presence/structure, not content accuracy. Review the deck and confirm the presence of the following sections mapped across the 10 slides. Be flexible with naming but the intent must be clear.\n\nExpected 10-slide structure (titles may vary):\n1) Title & Objective (states fiduciary stance against rolling CDs into variable annuities)\n2) Agenda/Overview\n3) Product Features Comparison: CDs vs Variable Annuities (prefer a comparison table)\n4) Risk/Return & Growth Impact\n5) Penalties & Liquidity (early withdrawal vs surrender charges, lock-ups)\n6) Suitability & Risk Tolerance (NAIC Best Interest concepts)\n7) FINRA Concerns/Investor Cautions (e.g., high-yield CDs page and VA sales cautions)\n8) NAIC Best Interest/Model requirements (suitability vs best interest obligations)\n9) Advisor Talking Points & Prudent Alternatives (e.g., laddered CDs, fixed annuities, bond funds)\n10) Summary & References (should list both sources/URLs or clear citations)\n\nScoring (0 to 1.8):\n- 1.8: All 10 sections present and clearly mapped across 10 slides as above (flexible titles allowed). References include FINRA and NAIC sources.\n- 1.3: Core 8\u20139 sections present (including Features, Risk/Return, Penalties, Suitability/NAIC, FINRA and NAIC sections) but references missing or only one source cited.\n- 0.8: At least 6 core sections present but notable gaps (e.g., missing either FINRA or NAIC dedicated slide).\n- 0.0: Fewer than 6 relevant sections or content not about CDs vs variable annuities suitability.\nDo not grade correctness; only check presence and structure.", "expectation": "All 10 required sections are present and clearly identifiable across the deck."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Regulatory Verification", "description": "Verifies correctness/coverage using light code checks and heavier LLM judgment for regulatory alignment, factual accuracy, and defensible fiduciary stance.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Citations to NAIC and FINRA present", "description": "Checks that the deck text includes NAIC and FINRA references/URLs. Partial credit if only organization names appear.", "weight": 0.6, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Returns a normalized score [0,1] based on presence of NAIC and FINRA citations.\n    Prefers explicit URLs (naic.org, finra.org); partial credit for names.\n    \"\"\"\n    import re\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n\n    text = \"\"\n    try:\n        path = context.files.get_path(output.id)\n        p = str(path).lower()\n        if p.endswith('.pdf'):\n            text = context.files.read_pdf_text(output.id)\n        elif p.endswith('.docx'):\n            text = context.files.read_docx_text(output.id)\n        elif output.is_text_format:\n            text = context.files.read_text(output.id)\n        else:\n            # Unable to parse (e.g., PPTX). Return 0 but don't crash.\n            return 0.0\n    except Exception:\n        return 0.0\n\n    t = (text or \"\").lower()\n    finra_score = 0.0\n    naic_score = 0.0\n\n    if 'finra.org' in t:\n        finra_score = 0.5\n    elif re.search(r'\\bfinra\\b', t):\n        finra_score = 0.35\n\n    if 'naic.org' in t:\n        naic_score = 0.5\n    elif ('national association of insurance commissioners' in t) or re.search(r'\\bnaic\\b', t):\n        naic_score = 0.35\n\n    total = finra_score + naic_score  # max 1.0\n    if total > 1.0:\n        total = 1.0\n    return total"}, {"type": "code", "name": "Topic coverage via keywords", "description": "Heuristically verifies that required topics are addressed via keyword groups: features comparison, risk/return & growth, penalties/liquidity, suitability/best interest, FINRA concerns, NAIC regulations.", "weight": 0.4, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Returns normalized coverage score [0,1] across 6 topic groups using keyword proxies.\n    If file not text-readable (e.g., PPTX), returns 0 (LLM rules will handle content).\n    \"\"\"\n    import re\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    text = \"\"\n    try:\n        path = context.files.get_path(output.id)\n        p = str(path).lower()\n        if p.endswith('.pdf'):\n            text = context.files.read_pdf_text(output.id)\n        elif p.endswith('.docx'):\n            text = context.files.read_docx_text(output.id)\n        elif output.is_text_format:\n            text = context.files.read_text(output.id)\n        else:\n            return 0.0\n    except Exception:\n        return 0.0\n\n    t = (text or \"\").lower()\n\n    topics = [\n        # Features comparison CDs vs VAs\n        [r'feature', r'fdic', r'certificate of deposit', r'cds', r'variable annuit', r'fees', r'tax deferral'],\n        # Risk/Return & Growth\n        [r'risk', r'return', r'growth', r'volatility', r'market (risk|exposure)', r'equity subaccount'],\n        # Penalties & Liquidity\n        [r'penalt', r'surrender', r'lock(-| )?up', r'withdrawal', r'liquidity', r'mva( |\\b)|market value adjustment'],\n        # Suitability & Best Interest\n        [r'suitability', r'best interest', r'customer profile', r'care obligation', r'disclosure obligation'],\n        # FINRA concerns\n        [r'finra', r'high-yield cds', r'investor alert', r'conflicts of interest'],\n        # NAIC regulations\n        [r'naic', r'model', r'annuity suitability', r'best interest model']\n    ]\n\n    covered = 0\n    for group in topics:\n        found = any(re.search(k, t) for k in group)\n        covered += 1 if found else 0\n\n    return covered / len(topics)"}, {"type": "llm_judge", "name": "Accuracy of CDs vs VAs product comparisons", "description": "Checks factual accuracy and internal consistency of the features comparison (FDIC insurance for CDs; VAs have market exposure, fees, tax deferral, riders).", "weight": 1.1, "judge_prompt": "Evaluate whether the deck accurately and consistently compares CDs and variable annuities. Check for:\n- CDs: FDIC insurance, fixed term/rate, early withdrawal penalties, limited upside.\n- VAs: market-exposed subaccounts, tax deferral, fees (M&E, admin, subaccount expenses), riders/guarantees, surrender charges.\n- No misleading or overbroad claims; distinctions between principal protection and guarantees are correct.\nScoring (0\u20131.1):\n- 1.1: All comparisons accurate and consistent; no material inaccuracies.\n- 0.7: Mostly accurate; minor omissions/nuances (e.g., incomplete fee/rider description).\n- 0.3: Multiple inaccuracies/omissions but broadly on track.\n- 0.0: Misleading or incorrect comparisons.", "expectation": "Clear, correct, and balanced comparison of CDs vs VAs features and mechanics."}, {"type": "llm_judge", "name": "Regulatory alignment with NAIC Best Interest and FINRA guidance", "description": "Checks that the deck\u2019s suitability/best-interest discussion aligns with NAIC model and FINRA cautions, referencing provided sources.", "weight": 1.3, "judge_prompt": "Using these sources: (1) https://content.naic.org/sites/default/files/government-affairs-brief-annuity-suitability-best-interest-model.pdf and (2) https://www.finra.org/investors/insights/high-yield-cds, assess whether the deck correctly reflects:\n- NAIC Best Interest Model: care, disclosure, conflict of interest mitigation, documentation obligations; suitability and best-interest concepts; consideration of surrender charges/expenses, liquidity needs, risk tolerance.\n- FINRA cautions/issues relevant to CDs and, by analogy, to complex/market-linked products and sales practices.\n- Appropriate use of citations/quotes without misrepresentation.\nScoring (0\u20131.3):\n- 1.3: Accurately aligns with both sources; obligations and cautions are correctly summarized and applied to the recommendation context.\n- 0.9: Generally aligned with minor gaps or vague treatment.\n- 0.5: Partial/ambiguous; key obligations or cautions missing.\n- 0.0: Misaligned or misrepresents guidance.", "expectation": "Accurate, source-aligned articulation of NAIC best-interest duties and FINRA cautions."}, {"type": "llm_judge", "name": "Risk-return and penalties analysis correctness", "description": "Evaluates whether risk/return tradeoffs, growth implications, and penalties/liquidity are correctly described and tied to client outcomes.", "weight": 0.8, "judge_prompt": "Review the slides on risk/return, growth impact, and penalties/liquidity. Check for:\n- Correct framing of market risk for VAs vs. rate/term certainty for CDs.\n- Realistic discussion of long-term growth tradeoffs with fees and volatility.\n- Accurate description of surrender charges, lock-ups, early withdrawal penalties, and liquidity constraints.\n- Clear linkage to client objectives/time horizon and emergency reserves.\nScoring (0\u20130.8): 0.8 excellent; 0.5 mostly correct with minor gaps; 0.2 partially correct; 0.0 incorrect/misleading.", "expectation": "Sound, client-outcome-oriented risk/return and penalty analysis."}, {"type": "llm_judge", "name": "Fiduciary stance and prudent alternatives", "description": "Assesses whether the deck presents a defensible fiduciary rationale for advising against CD-to-VA rollovers and suggests prudent alternatives when appropriate.", "weight": 0.8, "judge_prompt": "Evaluate the fiduciary argumentation. Look for:\n- Clear recommendation against rolling CDs into VAs for most clients, grounded in best interest and suitability.\n- Recognition of limited circumstances where VAs might fit (without encouraging rollovers broadly).\n- Practical alternatives (e.g., CD ladders, high-quality short-duration bonds, fixed annuities) tied to goals/risk tolerance.\n- Avoids blanket, unqualified prohibitions; centers on process and documentation.\nScoring (0\u20130.8): 0.8 strong, nuanced fiduciary stance with alternatives; 0.5 decent but generic; 0.2 weak/unsupported; 0.0 absent/misleading.", "expectation": "Nuanced, defensible fiduciary guidance with actionable alternatives."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Advisor Usefulness", "description": "Holistic LLM assessment of design quality, clarity, audience fit, and actionable value for field advisors.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Slide design and structural clarity", "description": "Assesses visual professionalism, logical flow, and slide readability (headlines, whitespace, consistent formatting).", "weight": 0.5, "judge_prompt": "Assess design quality and structure: clear headlines, logical flow across slides, appropriate whitespace, consistent fonts/colors, and uncluttered layouts suitable for internal advisor audiences. Score 0\u20130.5: 0.5 excellent, 0.3 adequate, 0.1 poor, 0.0 unacceptable.", "expectation": "Professional, consistent slide design with clear information hierarchy."}, {"type": "llm_judge", "name": "Audience appropriateness and tone", "description": "Evaluates whether the content matches field advisors\u2019 sophistication and internal training context.", "weight": 0.5, "judge_prompt": "Is the content pitched at the right level for field advisors? Avoids retail-client simplifications; uses advisor-relevant terminology; includes compliance-aware phrasing. Score 0\u20130.5: 0.5 excellent fit, 0.3 acceptable, 0.1 marginal, 0.0 off-target.", "expectation": "Advisor-level depth and compliance-aware tone."}, {"type": "llm_judge", "name": "Actionability of talking points", "description": "Checks for concise, reusable talking points, decision checklists, or steps advisors can use in client conversations.", "weight": 0.5, "judge_prompt": "Evaluate whether the deck provides actionable talking points, checklists, or steps advisors can reuse (e.g., suitability checklist, key questions to assess risk tolerance, alternative recommendations by client profile). Score 0\u20130.5: 0.5 highly actionable, 0.3 somewhat, 0.1 minimal, 0.0 none.", "expectation": "Clear, reusable talking points and steps for advisors."}, {"type": "llm_judge", "name": "Sourcing and citation quality", "description": "Assesses clarity and consistency of citations/attributions to NAIC and FINRA throughout the deck (not just on a final slide).", "weight": 0.5, "judge_prompt": "Evaluate whether sources are properly cited where used (e.g., slide footers or reference notations), particularly NAIC Best Interest Model and FINRA materials. Prefer explicit URLs and accurate attributions. Score 0\u20130.5: 0.5 consistent, 0.3 present but sparse, 0.1 weak, 0.0 absent/misleading.", "expectation": "Clear, consistent in-deck citations to NAIC and FINRA where relevant."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "854f3814-681c-4950-91ac-55b0db0e3781", "rubric": {"category_name": "OverpassQL Dataset and Usage Guide for I-40 (ABQ \u2194 OKC)", "rationale": "This rubric enforces a self-documenting deliverable: a Markdown guide that contains a runnable OverpassQL query and precise, reproducible instructions to generate a filtered OSM dataset for I-40 between Albuquerque and Oklahoma City. The task is Mixed (Pattern C): a document with embedded code and practical execution steps. Stage 1 uses an LLM-only gate to require a very specific Markdown structure and an OverpassQL code block. Stage 2 combines lightweight code checks (syntax/keywords/recursion/mentions) with LLM verification of targeting, executability, and sufficiency for speed/lane analysis. Stage 3 evaluates professional quality, clarity, and practical utility.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate (LLM only)", "description": "Gate: Ensure the output is a Markdown document with a precise structure enabling verification and execution.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Document Shape and Structure Requirements (Markdown + OverpassQL + How-To)", "description": "Verify the candidate produced a single Markdown guide with the exact structure required to enable verification and execution.", "weight": 4.0, "judge_prompt": "You are evaluating whether the provided output is a properly structured Markdown document for generating a filtered OpenStreetMap dataset of Interstate 40 (I-40) between Albuquerque, New Mexico and Oklahoma City, Oklahoma using OverpassQL.\n\nCheck ONLY structure/format and presence of required elements (do NOT assess technical correctness yet). Be flexible with exact header names, but the required content must be clearly present.\n\nRequired format and sections:\n- File format: Markdown (.md) document. If not clearly Markdown (headings, fenced code blocks, etc.), score = 0.\n- Title and Objective section: Title mentions OverpassQL/Overpass and I-40 and both cities (Albuquerque and Oklahoma City or OKC). Objective/Overview explains that the dataset is for speed and lane availability analysis for autonomous freight routing.\n- OverpassQL Query section: Contains a fenced code block labeled with language \"overpassql\" or \"ql\". The query must attempt to target I-40 and include relations/ways/nodes. Acceptable evidence: keywords such as relation/way/node, highway=motorway or route=road + network=US:I, ref=40 or I-40/I 40, recursion operators like '>' or '>>', and some geographic constraint (bbox, areas, or textual mention of ABQ/OKC corridor). You are ONLY checking the presence of these elements, not if they work.\n- How to Run / Usage section: Includes instructions for BOTH Overpass Turbo (web UI) and a command-line approach (e.g., curl) to run the query and save results.\n- Export and Post-processing section: Explains exporting to GeoJSON/JSON and basic post-processing for analysis. Must mention the key tags for analysis: maxspeed and lanes (word mentions are sufficient).\n- Data/Tag Dictionary section: A short list/table of key tags (must include maxspeed and lanes; others optional like oneway, surface, hgv/access).\n- Notes/Troubleshooting/Limits section: Mentions API endpoint(s) and at least one constraint like rate limits, timeouts, or large response handling.\n\nScoring:\n- 4.0: All required sections/elements present and clearly identifiable in Markdown, with a fenced OverpassQL code block.\n- 3.0: Missing exactly one required section/element OR the title doesn\u2019t include all keywords but everything else is present.\n- 2.0: Missing two required sections/elements but still clearly a Markdown guide with an OverpassQL code block.\n- 1.0: Minimal Markdown with a code block, but most required sections are missing.\n- 0.0: Not Markdown OR no OverpassQL fenced code block.\n\nOnly evaluate structure/presence. Do not judge correctness of the query or instructions here.", "expectation": "A well-structured Markdown guide containing a fenced OverpassQL query and step-by-step usage instructions (Overpass Turbo and curl), export guidance, tag dictionary (including maxspeed and lanes), and troubleshooting/limits."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Executability Verification", "description": "Verify that the query plausibly targets I-40 (ABQ \u2194 OKC), includes relations/ways/nodes with recursion, and that instructions are executable and suitable for speed/lane analysis.", "is_required": true, "max_points": 10.0, "min_score_to_pass": 6.0, "rules": [{"type": "code", "name": "Markdown + Fenced OverpassQL Block Detected", "description": "Detects text format and presence of a fenced code block labeled overpassql or ql.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_text_format:\n            return 0.0, \"Primary output missing or not a text/Markdown file.\"\n        text = context.files.read_text(output.id)\n        if not text:\n            return 0.0, \"No text content found.\"\n        # Look for fenced code block with overpassql or ql\n        pattern = re.compile(r\"```\\s*(overpassql|ql)\\b[\\s\\S]+?```\", re.IGNORECASE)\n        found = bool(pattern.search(text))\n        return (0.5 if found else 0.0, \"Fenced OverpassQL block {}found.\".format(\"\" if found else \"not \"))\n    except Exception as e:\n        return 0.0, f\"Error during detection: {e}\""}, {"type": "code", "name": "Query Targets I-40 Heuristic Tokens", "description": "Heuristic: OverpassQL content mentions I-40 via ref/network/highway tokens and relation/way selection.", "weight": 0.5, "code": "import re\n\ndef extract_first_code_block(text):\n    m = re.search(r\"```\\s*(overpassql|ql)\\b([\\s\\S]+?)```\", text, re.IGNORECASE)\n    return m.group(2) if m else \"\"\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_text_format:\n            return 0.0, \"Primary output missing or not text.\"\n        text = context.files.read_text(output.id) or \"\"\n        code = extract_first_code_block(text)\n        hay = (code or text).lower()\n        checks = []\n        # relation/way/node presence\n        checks.append(bool(re.search(r\"\\b(rel(ation)?|way|node)\\b\", hay)))\n        # ref 40 or I-40/I 40\n        checks.append(bool(re.search(r\"\\bref\\b.*(i\\s*-?\\s*40|\\b40\\b)\", hay)))\n        # network US:I or highway motorway\n        checks.append(bool(re.search(r\"network\\s*[:=]\\s*\\\"?us:i|\\bhighway\\b.*\\bmotorway\\b\", hay)))\n        score = 0.5 * (sum(checks) / 3)\n        feedback = f\"Heuristic matches: {sum(checks)}/3\"\n        return score, feedback\n    except Exception as e:\n        return 0.0, f\"Error in heuristic: {e}\""}, {"type": "code", "name": "Recursion and Metadata/Out Checks", "description": "Checks for Overpass recursion to pull ways/nodes and presence of out meta/body statements.", "weight": 0.5, "code": "import re\n\ndef extract_first_code_block(text):\n    m = re.search(r\"```\\s*(overpassql|ql)\\b([\\s\\S]+?)```\", text, re.IGNORECASE)\n    return m.group(2) if m else \"\"\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_text_format:\n            return 0.0, \"Primary output missing or not text.\"\n        text = context.files.read_text(output.id) or \"\"\n        code = extract_first_code_block(text).lower() or text.lower()\n        recursion = bool(re.search(r\"\\>\\>|\\bway\\(r\\)|\\bnode\\(w\\)|\\brel\\(.*\\)\\s*;\\s*\\>\\b\", code))\n        out_stmt = bool(re.search(r\"\\bout\\b\\s*(meta|body|tags|skel)\\b\", code))\n        score = 0.0\n        if recursion:\n            score += 0.25\n        if out_stmt:\n            score += 0.25\n        return score, f\"Recursion: {recursion}, out-stmt: {out_stmt}\"\n    except Exception as e:\n        return 0.0, f\"Error checking recursion/out: {e}\""}, {"type": "code", "name": "Geographic Context Mentions (ABQ and OKC)", "description": "Checks the document mentions both cities, indicating the corridor focus.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_text_format:\n            return 0.0, \"Primary output missing or not text.\"\n        text = (context.files.read_text(output.id) or \"\").lower()\n        has_abq = (\"albuquerque\" in text) or (\"abq\" in text)\n        has_okc = (\"oklahoma city\" in text) or (\"okc\" in text)\n        if has_abq and has_okc:\n            return 0.5, \"Both ABQ and OKC mentioned.\"\n        if has_abq or has_okc:\n            return 0.25, \"Only one city mentioned.\"\n        return 0.0, \"Neither city mentioned.\"\n    except Exception as e:\n        return 0.0, f\"Error checking city mentions: {e}\""}, {"type": "llm_judge", "name": "Query Targeting and Coverage (I-40 ABQ \u2194 OKC)", "description": "Does the query plausibly target only the I-40 corridor segment between Albuquerque and Oklahoma City and include relations, ways, and nodes?", "weight": 2.0, "judge_prompt": "Evaluate the OverpassQL query for targeting and coverage:\n- It should target Interstate 40 (I-40), not generic highways.\n- It should reasonably restrict the geography to the segment between Albuquerque, NM and Oklahoma City, OK (flexible: bbox/areas/filters/around/route relations are all acceptable, as long as intent is clear).\n- It should include relations and ways and the nodes comprising those ways (using recursion or explicit selections).\n- It should request enough detail to be analyzable (e.g., out body/meta/tags).\n\nScoring:\n- 2.0: Clearly targets I-40 segment (ABQ \u2194 OKC) and includes relations, ways, and nodes with appropriate out statements.\n- 1.0: Targets I-40 but geographic restriction is vague or too broad; still includes ways and nodes.\n- 0.5: Attempts to target I-40 but likely retrieves an overbroad or incomplete set (e.g., only ways, no nodes/relations).\n- 0.0: Not specific to I-40 or no evidence of relations/ways/nodes inclusion.", "expectation": "A plausible, segment-limited I-40 query using route relations and/or highway=motoway/ref filters with recursion to pull ways and nodes, and proper out statements."}, {"type": "llm_judge", "name": "Executability via Overpass Turbo and CLI", "description": "Are the instructions executable both in Overpass Turbo and via curl/CLI to produce a JSON/GeoJSON dataset?", "weight": 2.0, "judge_prompt": "Assess the practicality of the instructions:\n- Overpass Turbo: Clear steps to paste the query, set map/timeout if needed, run, and export (preferably GeoJSON/JSON).\n- CLI (curl or similar): Shows a concrete command against a valid Overpass API endpoint (e.g., overpass-api.de/api/interpreter) including posting the query, and saving results to a file.\n- Mentions response size/timeouts or splitting strategies if large.\n\nScoring:\n- 2.0: Complete, copy-paste friendly steps for both Overpass Turbo and CLI; file export steps are clear.\n- 1.0: Usable but missing a key detail (e.g., saving file or endpoint info).\n- 0.5: Only one method (Turbo or CLI) is described clearly.\n- 0.0: Not practically executable from instructions.", "expectation": "Clear, reproducible steps for Overpass Turbo and curl to obtain a dataset file (JSON/GeoJSON)."}, {"type": "llm_judge", "name": "Suitability for Speed and Lane Analysis", "description": "Will the resulting dataset and guidance support speed and lane availability analysis for autonomous freight routing?", "weight": 2.0, "judge_prompt": "Evaluate whether the deliverable enables speed and lane availability analysis:\n- The guide explicitly calls out relevant tags (must include maxspeed and lanes; optionally oneway, surface, width, access/hgv, shoulder, turn:lanes, units like mph).\n- Query/instructions preserve tags and geometry sufficient for analysis (ways and nodes for linear referencing).\n- Post-processing guidance to extract/normalize these tags (e.g., notes on mph vs km/h, missing tag handling, joining ways, exporting to GeoJSON/CSV).\n\nScoring:\n- 2.0: Clearly identifies key tags and provides actionable guidance to extract/normalize them for analysis.\n- 1.0: Mentions tags but lacks clear steps to use them.\n- 0.5: Vague relevance to speed/lanes.\n- 0.0: Does not address speed/lanes at all.", "expectation": "Explicit mention of maxspeed and lanes and how to export/use them for analysis."}, {"type": "llm_judge", "name": "Internal Consistency of Query and Instructions", "description": "Cross-check that described steps match the provided query and outputs.", "weight": 2.0, "judge_prompt": "Check for consistency between the OverpassQL query and the written instructions:\n- If the instructions say the output is GeoJSON, they explain how to export GeoJSON (or convert from JSON) consistently.\n- Endpoint names and parameters are consistent across examples.\n- The scope described in the narrative matches what the query appears to fetch (I-40 ABQ \u2194 OKC, including relations/ways/nodes).\n- No contradictory steps (e.g., filtering for different highway or wrong ref in text vs query).\n\nScoring:\n- 2.0: Fully consistent across query, instructions, and claimed outputs.\n- 1.0: Minor inconsistencies that a competent user could resolve.\n- 0.5: Noticeable inconsistencies likely to cause confusion.\n- 0.0: Contradictory or mismatched content.", "expectation": "Narrative, commands, and query align on scope, endpoint, and export format."}], "on_failure_action": "skip_remaining", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality Assessment", "description": "Holistic evaluation of clarity, organization, and practical value for a professional software developer audience.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity and Organization", "description": "Structure, readability, and navigability of the Markdown guide.", "weight": 1.5, "judge_prompt": "Assess clarity and organization:\n- Logical section flow with clear headings.\n- Concise, readable explanations with minimal ambiguity.\n- Fenced code blocks and commands are visually distinct and labeled.\n\nScoring:\n- 1.5: Highly clear and well-organized.\n- 1.0: Generally clear with minor rough edges.\n- 0.5: Understandable but scattered/confusing.\n- 0.0: Poorly organized and hard to follow.", "expectation": "A clean, well-structured Markdown document with clear headings and code blocks."}, {"type": "llm_judge", "name": "Professional Completeness and Reproducibility", "description": "Does the guide provide enough context and detail to reproduce results reliably?", "weight": 1.5, "judge_prompt": "Evaluate professional completeness:\n- Mentions Overpass API endpoints and alternatives; includes timeouts and tips for large queries.\n- Notes data currency/source (OSM) and that results reflect current OSM state.\n- Provides versioning or seed notes (e.g., date of run) and suggests saving the raw JSON/GeoJSON.\n\nScoring:\n- 1.5: Strong completeness with reproducibility tips.\n- 1.0: Adequate but missing one helpful best practice.\n- 0.5: Bare minimum reproducibility info.\n- 0.0: Lacks necessary details for reliable reproduction.", "expectation": "Clear endpoint info, timeouts, and reproducibility guidance."}, {"type": "llm_judge", "name": "Technical Depth and Best Practices", "description": "Inclusion of practical tips relevant to freight/autonomous routing use cases.", "weight": 1.5, "judge_prompt": "Assess technical depth and best practices:\n- Notes on capturing motorway_link ramps if relevant, lane-specific tags (turn:lanes), heavy vehicle restrictions (hgv/access), oneway, width.\n- Suggestions for post-processing: dissolving/merging ways by ref, handling splits, normalizing units (mph vs km/h), dealing with missing tags.\n- Considerations for performance: bounding boxes, splitting corridor, using bbox or area filters, reducing payload via out tags.\n\nScoring:\n- 1.5: Strong, actionable best practices for this domain.\n- 1.0: Some useful tips.\n- 0.5: Minimal technical depth.\n- 0.0: No meaningful best practices.", "expectation": "Actionable tips on tags, geometry handling, and performance for freight routing analysis."}, {"type": "llm_judge", "name": "Operational Caveats and Safety Considerations", "description": "Responsible guidance regarding data limitations and safe use.", "weight": 1.5, "judge_prompt": "Evaluate operational caveats and safety considerations:\n- Warns about OSM data completeness/accuracy limitations and the need to validate before operational use.\n- Mentions legal/safety implications of autonomous routing and the need for compliance.\n- Suggests verification against authoritative sources where relevant.\n\nScoring:\n- 1.5: Thoughtful and appropriate caveats.\n- 1.0: Some caveats mentioned.\n- 0.5: Minimal caveats.\n- 0.0: No caveats at all.", "expectation": "Appropriate disclaimers and suggestions to validate data before deployment."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "a079d38f-c529-436a-beca-3e291f9e62a3", "rubric": {"category_name": "Video Production Budgeting & Scheduling (Producers and Directors)", "rationale": "This rubric enforces a self-documenting Excel workbook for a pre-production budget and schedule. Stage 1 is a strict LLM gate that mandates an auditable workbook structure. Stage 2 mixes lightweight code checks (bounds, exclusions) with LLM correctness checks (constraint compliance, arithmetic consistency, schedule coherence). Stage 3 evaluates overall professional quality, clarity, and usefulness for stakeholders. Code rules are intentionally lower-weight than LLM rules to reflect nuanced, cross-sheet reasoning by LLM judges.", "max_total_score": 24.0, "stages": [{"name": "Stage 1 \u2013 STRUCTURE GATE (Excel Shape Enforcement)", "description": "LLM-only gate. Verifies the workbook is an Excel file with the exact sheets/sections needed to make verification trivial. No quality or correctness checks here\u2014only structure and presence.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.2, "rules": [{"type": "llm_judge", "name": "Required Workbook Structure Present", "description": "Check if output is an Excel workbook containing all required sheets and sections for a pre-production budget and schedule (no post-production). Be flexible with sheet names but strict on content/sections.", "weight": 6.0, "judge_prompt": "You are validating the SHAPE (structure) of the candidate output. Do not assess content quality or math correctness\u2014only the presence of the required sheets, sections, and tables in an Excel workbook.\n\nFormat Requirements:\n- Must be an Excel workbook (.xlsx). Not a PDF, DOCX, CSV, or plain text.\n- Professionally labeled sheets (names can vary slightly). Content must be visible as tables (not just blank or free text).\n\nRequired Sheets and Sections (allow minor naming variations like plurals or synonyms):\n1) Sheet: \"Shoot Plan\"\n   - Section A: \"Video List\" table with columns (or clear equivalents):\n     [Video ID/Title | Type/Format | Est. Duration | Shoot Day | Location | Notes]\n   - Section B: \"Daily Schedule\" table with columns (or equivalents):\n     [Day | Setup Hours | Shoot Hours | Total On-site Hours]\n\n2) Sheet: \"Cost Breakdown\" (or \"Budget\" / \"Production Estimate\")\n   - Line-item budget table with columns (or equivalents):\n     [Category | Role/Service | Quantity | Unit | Unit Rate | Line Total | Basis/Assumption]\n   - Subtotals by Category AND a Grand Total\n   - Must NOT include any post-production category/lines (this is structure presence only; detailed exclusion is verified later)\n\n3) Sheet: \"Time Estimate\"\n   - Table with columns (or equivalents): [Role | Hours | Days | Basis/Notes]\n   - Totals row for Hours and Days\n\n4) Sheet: \"Rate Card\"\n   - Table with columns (or equivalents): [Role/Service | Unit | Standard Rate | Source/Notes]\n\n5) Sheet: \"Summary\"\n   - KPIs block with fields (or equivalents):\n     - Total Production Cost (no post)\n     - Total Shoot Days\n     - Total Setup Hours\n     - Total On-site Hours\n     - Average Hours per Day\n   - A clearly labeled list/box for \"Constraints & Assumptions\" explicitly including ALL of:\n     \u2022 2 cameras\n     \u2022 No PA\n     \u2022 Producer on site\n     \u2022 Audio technician on site\n     \u2022 6\u20138 hour shoot day\n     \u2022 1\u20132 hour setup per day\n     \u2022 No post-production included\n\nScoring (return a single numeric score between 0.0 and 6.0):\n- 6.0: Excel workbook present AND all 5 sheets exist with the specified sections/fields; constraints list explicitly includes all listed items.\n- 4.5: Excel workbook present; exactly one required sheet is missing OR one required section/table is missing, but others are complete.\n- 3.0: Excel workbook present; two required sheets/sections missing.\n- 1.5: Excel workbook present but largely incomplete (three or more required sheets/sections missing).\n- 0.0: Not an Excel file OR only free-form notes without the required tables/structure.", "expectation": "A fully structured Excel with Shoot Plan, Cost Breakdown, Time Estimate, Rate Card, and Summary (with assumptions), each containing the specified tables/fields."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness Verification (Calculations, Constraints, Consistency)", "description": "Verifies the workbook\u2019s internal consistency, numerical bounds, and compliance with explicit constraints. Mix of small deterministic code checks and higher-weight LLM checks.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Schedule Bounds and Arithmetic Checks", "description": "Daily Schedule setup hours must be 1\u20132; shoot hours 6\u20138; total on-site hours \u2248 setup + shoot (within 0.25h).", "weight": 1.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0\n        path = context.files.get_path(output.id)\n        try:\n            xls = pd.ExcelFile(path)\n            sheet_names = [s for s in xls.sheet_names]\n        except Exception:\n            # Fallback: try direct read of first sheet\n            try:\n                df0 = pd.read_excel(path)\n                sheet_names = [0]\n            except Exception:\n                return 0.0\n        # Find a sheet likely to contain Daily Schedule\n        def pick_sheet(names, keywords):\n            best = None\n            score = -1\n            for s in names:\n                sl = str(s).lower()\n                sc = sum(1 for k in keywords if k in sl)\n                if sc > score:\n                    best, score = s, sc\n            return best\n        shoot_sheet = pick_sheet(sheet_names, [\"shoot plan\",\"schedule\",\"shoot\",\"plan\",\"production\"])\n        if shoot_sheet is None:\n            return 0.0\n        try:\n            df = pd.read_excel(path, sheet_name=shoot_sheet)\n        except Exception:\n            return 0.0\n        # Normalize columns\n        cols = [str(c).strip() for c in df.columns]\n        lower_map = {str(c).lower(): c for c in cols}\n        # Fuzzy find target columns\n        def find_col(candidates):\n            for c in cols:\n                cl = str(c).lower()\n                if any(k in cl for k in candidates):\n                    return c\n            return None\n        col_setup = find_col([\"setup\"])  # setup hours\n        # shoot hours should not collide with setup\n        col_shoot = None\n        for c in cols:\n            cl = str(c).lower()\n            if (\"shoot\" in cl or \"on-set\" in cl or \"on set\" in cl) and \"setup\" not in cl:\n                if \"hour\" in cl or \"hrs\" in cl or \"h\" in cl:\n                    col_shoot = c\n                    break\n        col_total = None\n        for c in cols:\n            cl = str(c).lower()\n            if \"total\" in cl and (\"hour\" in cl or \"hrs\" in cl or \"on-site\" in cl or \"on site\" in cl):\n                col_total = c\n                break\n        if not (col_setup and col_shoot and col_total):\n            # Try an alternative: any numeric columns that look like hours\n            num_cols = [c for c in cols if pd.api.types.is_numeric_dtype(df[c])]\n            if len(num_cols) >= 3:\n                col_setup, col_shoot, col_total = num_cols[:3]\n            else:\n                return 0.0\n        sub = df[[col_setup, col_shoot, col_total]].copy()\n        # Coerce to numeric\n        for c in [col_setup, col_shoot, col_total]:\n            sub[c] = pd.to_numeric(sub[c], errors='coerce')\n        sub = sub.dropna(how='any')\n        if sub.empty:\n            return 0.0\n        # Bounds checks\n        setup_ok = ((sub[col_setup] >= 1.0) & (sub[col_setup] <= 2.0)).mean()\n        shoot_ok = ((sub[col_shoot] >= 6.0) & (sub[col_shoot] <= 8.0)).mean()\n        total_ok = (np.abs(sub[col_total] - (sub[col_shoot] + sub[col_setup])) <= 0.25).mean()\n        # Average of three checks\n        score = float(np.mean([setup_ok, shoot_ok, total_ok]))\n        return max(0.0, min(1.0, score))\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Role Coverage and Exclusions (No Post, No PA)", "description": "Cost Breakdown must include Producer and Audio Technician; exclude PA/Production Assistant and any Post-Production items (e.g., edit, color).", "weight": 1.0, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0\n        path = context.files.get_path(output.id)\n        try:\n            xls = pd.ExcelFile(path)\n            sheet_names = xls.sheet_names\n        except Exception:\n            try:\n                df0 = pd.read_excel(path)\n                sheet_names = [0]\n            except Exception:\n                return 0.0\n        def pick_sheet(names, keywords):\n            best = None\n            score = -1\n            for s in names:\n                sl = str(s).lower()\n                sc = sum(1 for k in keywords if k in sl)\n                if sc > score:\n                    best, score = s, sc\n            return best\n        cost_sheet = pick_sheet(sheet_names, [\"cost\",\"budget\",\"breakdown\",\"estimate\"])\n        if cost_sheet is None:\n            return 0.0\n        try:\n            df = pd.read_excel(path, sheet_name=cost_sheet)\n        except Exception:\n            return 0.0\n        # Identify a role/service column if possible\n        cols = [str(c) for c in df.columns]\n        role_col = None\n        for c in cols:\n            cl = c.lower()\n            if any(k in cl for k in [\"role\",\"service\",\"item\",\"line\",\"resource\",\"position\",\"description\"]):\n                role_col = c\n                break\n        text_cells = []\n        if role_col is not None:\n            text_cells = df[role_col].astype(str).str.lower().tolist()\n        else:\n            # Flatten all cells as text\n            for c in df.columns:\n                text_cells += df[c].astype(str).str.lower().tolist()\n        haystack = \"\\n\".join(text_cells)\n        # Checks\n        has_producer = re.search(r\"producer\", haystack) is not None\n        has_audio = re.search(r\"audio|sound\", haystack) is not None\n        has_pa = re.search(r\"\\bproduction\\s+assistant\\b|\\bpa\\b\", haystack) is not None\n        has_post = re.search(r\"post[-\\s]?production|edit(ing)?|color(ing)?|grade|grading|vfx|graphics\", haystack) is not None\n        score = 1.0\n        feedback_bits = []\n        if not has_producer:\n            score -= 0.3\n            feedback_bits.append(\"Producer missing\")\n        if not has_audio:\n            score -= 0.3\n            feedback_bits.append(\"Audio tech missing\")\n        if has_pa:\n            score -= 0.4\n            feedback_bits.append(\"PA present (should be excluded)\")\n        if has_post:\n            score -= 0.4\n            feedback_bits.append(\"Post-production items present (should be excluded)\")\n        score = max(0.0, min(1.0, score))\n        if feedback_bits:\n            return score, \"; \".join(feedback_bits)\n        return score\n    except Exception:\n        return 0.0"}, {"type": "llm_judge", "name": "Constraint Compliance (2 cams, no PA, producer + audio, day/ setup limits, no post)", "description": "Check that the workbook explicitly adheres to the given constraints across sheets and does not contradict them.", "weight": 3.0, "judge_prompt": "Assess the workbook for compliance with ALL explicit constraints stated in the task:\n- Exactly a simple shoot planned for two cameras (no third/fourth camera lines, unless clearly noted as 2 cameras total).\n- No PA/Production Assistant included anywhere.\n- Producer on site is included.\n- Audio technician on site is included.\n- Each shoot day is 6\u20138 hours with 1\u20132 hours setup per day.\n- No post-production included anywhere (lines, categories, or totals).\n- Summary sheet\u2019s Constraints & Assumptions list explicitly captures the above items.\n\nCheck for contradictions across sheets: e.g., Rate Card lists PA but Cost Breakdown includes it; or Shoot Plan shows 9-hour days without setup; or a Post category sneaks into totals.\n\nScoring (return a single numeric score between 0.0 and 3.0):\n- 3.0: All constraints present and consistent across sheets; no contradictions; explicit statements in Summary.\n- 2.0: Minor ambiguity (e.g., unclear phrasing) but overall compliant; no material contradictions.\n- 1.0: Several omissions or weak/unclear constraint signaling; still mostly aligned.\n- 0.0: Clear violations (e.g., PA billed, third camera, post-production included, or day/setup outside limits).", "expectation": "All constraints clearly represented and internally consistent with no PA/post items and correct 2-camera assumption."}, {"type": "llm_judge", "name": "Arithmetic and Totals Consistency (Qty\u00d7Rate=Line Total, Subtotals=Grand Total)", "description": "Visually validate that line totals match quantities and rates, subtotals roll up, and the Summary ties to Cost Breakdown (pre-post only).", "weight": 3.0, "judge_prompt": "Check the budget math for visible consistency (do not recompute in code; inspect visually):\n- For line items: Line Total should equal Quantity \u00d7 Unit Rate.\n- Category subtotals should equal the sum of their lines.\n- Grand Total should equal the sum of category subtotals.\n- Summary\u2019s Total Production Cost (no post) should match the Cost Breakdown Grand Total.\n- Time Estimate totals (hours/days) should be consistent with the Shoot Plan\u2019s Daily Schedule totals.\n\nScoring (return a single numeric score between 0.0 and 3.0):\n- 3.0: All visible arithmetic and rollups are consistent; Summary ties perfectly.\n- 2.0: Minor rounding or a small, well-explained discrepancy.\n- 1.0: Multiple inconsistencies but the majority appears correct.\n- 0.0: Major inconsistencies or totals do not tie to Summary.", "expectation": "All visible arithmetic checks pass and cross-sheet totals tie out."}, {"type": "llm_judge", "name": "Schedule Coherence vs. Deliverables", "description": "Ensure the Shoot Plan reasonably maps videos to shoot days and that total time estimates are plausible for the listed deliverables.", "weight": 2.0, "judge_prompt": "Review the Shoot Plan\u2019s Video List and Daily Schedule, and the Time Estimate sheet:\n- Videos are assigned to shoot days in a plausible manner (e.g., multiple short educational videos per day is reasonable; extremely long lists crammed into one day may not be).\n- Daily setup and shoot time align with the number and type of videos planned per day.\n- Total shoot days, total setup hours, and total on-site hours are coherent with the number of deliverables.\n\nScoring (return a single numeric score between 0.0 and 2.0):\n- 2.0: Mapping is clear and plausible; totals and allocations make sense.\n- 1.0: Some ambiguity or questionable pacing; overall still feasible.\n- 0.0: Implausible schedule vs. deliverables or missing mapping.", "expectation": "A coherent, plausible schedule aligning videos to days with realistic time estimates."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Quality Assessment (Professionalism and Usefulness)", "description": "Holistic LLM assessment of presentation, clarity, and decision usefulness for a producer/director and client stakeholders.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Readability", "description": "Assess formatting, labeling, readability, and professional polish of the workbook.", "weight": 2.0, "judge_prompt": "Evaluate the workbook\u2019s presentation quality:\n- Clear, consistent headers; tables are well-formatted; currency and hour formats are applied.\n- Logical layout; easy to navigate between sheets; helpful freeze panes or table styles.\n- Minimal clutter; adequate whitespace; no orphaned data.\n\nScoring (0.0\u20132.0):\n- 2.0: Highly professional, clean, and readable.\n- 1.0: Adequate but could be clearer.\n- 0.0: Poorly formatted and hard to read.", "expectation": "Professional formatting with clear headers, consistent units/currency, and easy navigation."}, {"type": "llm_judge", "name": "Assumptions, Notes, and Transparency", "description": "Evaluate clarity of assumptions, sources, and exclusions for stakeholder understanding.", "weight": 2.0, "judge_prompt": "Assess how well the workbook documents assumptions and sources:\n- Summary includes a clear Constraints & Assumptions list.\n- Rate Card includes sources/notes; Cost Breakdown has Basis/Assumption notes.\n- Explicit statement that post-production is excluded and any other exclusions are transparent.\n\nScoring (0.0\u20132.0):\n- 2.0: Assumptions and sources are explicit, complete, and easy to find.\n- 1.0: Partially documented or scattered.\n- 0.0: Largely undocumented or unclear.", "expectation": "Clear, centralized documentation of assumptions, sources, and exclusions."}, {"type": "llm_judge", "name": "Decision Usefulness and Reusability", "description": "Assess whether the workbook is directly useful for planning/approvals and adaptable for future shoots.", "weight": 2.0, "judge_prompt": "Evaluate usefulness for stakeholders:\n- Provides enough detail (roles, units, rates, quantities) to approve budget and schedule.\n- Structure supports easy what-if updates (e.g., changing rates, days, or counts).\n- Summary highlights key metrics for quick decisions.\n\nScoring (0.0\u20132.0):\n- 2.0: Immediately actionable and reusable.\n- 1.0: Generally useful but limited flexibility or missing a key element.\n- 0.0: Not decision-ready.", "expectation": "Actionable, with clear KPIs and a structure that\u2019s easy to update."}, {"type": "llm_judge", "name": "Risk Awareness and Practical Considerations", "description": "Evaluate whether practical production risks/considerations are acknowledged appropriately for a simple two-camera shoot.", "weight": 2.0, "judge_prompt": "Consider whether the workbook briefly addresses practical considerations (without overreaching beyond scope):\n- Notes on setup constraints (1\u20132 hours), crew dependencies, equipment availability, and any venue-related assumptions.\n- Avoids unnecessary detail on teardown since venue teams handle it.\n- Any contingency notes (buffers) or clear indication why they\u2019re not needed for this simple shoot.\n\nScoring (0.0\u20132.0):\n- 2.0: Appropriate, concise risk/consideration notes.\n- 1.0: Minimal but present.\n- 0.0: Absent or irrelevant.", "expectation": "Concise, relevant production considerations aligned with the simple shoot scope."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "6974adea-8326-43fa-8187-2724b15d9546", "rubric": {"category_name": "Enterprise Tech Feature: RTLS Livestock Tracking (Journalism)", "rationale": "This rubric uses a 3-stage, self-documenting approach tailored to a professional feature article for an enterprise technology audience. Stage 1 is an LLM-only gate enforcing exact structure in a DOCX, including headline, standfirst, subheadings, direct quotes, and a Source Log that proves quotes and sourcing. Stage 2 mixes light code checks (bounds and structural validations enabled by Stage 1) with heavier LLM verification for correctness, attribution integrity, impartial tone, and UK/Guardian style. Stage 3 evaluates overall quality: clarity for a mainstream audience with expert depth, narrative use of interviews, structure and flow, and SEO effectiveness for an international readership.", "max_total_score": 50.0, "stages": [{"name": "Stage 1 \u2013 Shape Enforcement Gate (LLM only)", "description": "Gate: The output must be a DOCX feature article with specific, verifiable structure enabling later checks. No scoring for content quality or factual accuracy here\u2014only presence/shape.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Document Format and Section Structure Gate", "description": "Verify the candidate produced a single Word document with required sections and visible structure for later verification.", "weight": 4.0, "judge_prompt": "You are validating structure only. Inspect the primary output file. Confirm the following are present and clearly visible in a DOCX (Word) document, not PDF or plain text:\n\n1) File format: A valid .docx Word document.\n2) Headline: A single, SEO-style headline at the very top (not a sentence case paragraph). Should read as a headline, not body copy.\n3) Standfirst/Deck: A brief summary paragraph directly under the headline, distinct in formatting or placement.\n4) Body with subheadings: The main body should be broken into at least three subheadings that help non-expert readers navigate the piece. Subheadings should be visually distinct (e.g., heading style or on their own lines).\n5) Direct quotes: The body should contain direct, quoted speech (quotation marks) attributed to named interviewees (e.g., Jim Dalton, Gaspar Olafsen, Anne Smith, Lars Andersen).\n6) Word-count suitability: The body appears to be a long-form feature (approximately 1,000\u20131,500 words). Do not count exact words here\u2014just verify it is clearly in that range by length.\n7) Ending sections: A final section titled \u201cSource Log\u201d (or similar, e.g., \u201cSources and Attribution Log\u201d) exists at the end of the document.\n\nScoring (structure only):\n- Full (weight): All 7 items clearly present.\n- 75%: One minor element missing or ambiguous (e.g., only two subheadings, or deck placement ambiguous).\n- 50%: Two or more core elements missing or ambiguous.\n- 0: Not a DOCX or lacks core structure (no headline or no subheadings or no quotes or no Source Log).\nDo not assess factual correctness, style quality, impartiality, or exact word counts in this rule\u2014only presence and structure.", "expectation": "A .docx feature article with headline, standfirst, 3+ subheadings, visible direct quotes with attributions, and a final Source Log section."}, {"type": "llm_judge", "name": "Source Log Presence and Mappability Gate", "description": "Verify the document ends with a Source Log that is sufficiently structured to map each direct quote to a source/interviewee.", "weight": 4.0, "judge_prompt": "Check the final section of the DOCX for a clearly labeled Source Log (or similarly titled section). This gate checks the presence and basic mappability, not verification against external files.\n\nRequired structural elements in the Source Log:\n- A list (bullets or table) where each direct quote in the article can be mapped to: (a) the quoted text or a concise paraphrase tag, (b) the interviewee\u2019s full name (preferably one of: Jim Dalton, Gaspar Olafsen, Anne Smith, Lars Andersen), and (c) the source type/identifier (e.g., \u201cInterview notes\u201d, \u201cPress release\u201d, or a filename/article title) plus date if noted.\n- At least three mapped quotes total.\n- Names used in the article text appear in the Source Log (consistency of names).\n\nBe flexible on formatting (bullets or table), but the mapping must be explicit enough that a reviewer could trace quotes back to a specific interviewee/source.\n\nScoring:\n- Full (weight): Structured Source Log present with 3+ mappings; interviewee names in article match names in the log; each entry includes interviewee name and source type/identifier.\n- 75%: Source Log present and mostly structured; minor omissions (e.g., one entry missing a date or source type) but still mappable.\n- 50%: Source Log present but too vague to map quotes reliably (e.g., generic list without clear quote-to-source ties).\n- 0: No Source Log or completely unmappable.\n\nDo not judge factual accuracy or the truthfulness of quotes\u2014only whether the mapping structure exists for later verification.", "expectation": "A clearly labeled Source Log section mapping at least three quotes to named interviewees and identifiable sources."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification (Correctness, Compliance, Integrity)", "description": "With shape enforced, verify bounds, compliance, and correctness. Code rules do deterministic checks; LLM rules verify attribution integrity, impartiality, UK style compliance, and factual coherence.", "is_required": true, "max_points": 22.0, "min_score_to_pass": 11.0, "rules": [{"type": "code", "name": "Word Count Bounds (1,000\u20131,500)", "description": "Checks the article\u2019s word count is within required bounds.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float score in [0, 1]\n    \"\"\"\n    import re\n    W = 1.0\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = None\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n    if not text:\n        return 0.0\n    words = re.findall(r\"\\b\\w+\\b\", text)\n    n = len(words)\n    # Full credit if within [1000, 1500]\n    if 1000 <= n <= 1500:\n        return W\n    # Linear tolerance: partial credit within 20% outside bounds\n    # 20% of 1000 = 200 below; 20% of 1500 = 300 above\n    if 800 <= n < 1000:\n        # scale from 0 at 800 to 1 at 1000\n        return W * ((n - 800) / 200)\n    if 1500 < n <= 1800:\n        # scale from 1 at 1500 down to 0 at 1800\n        return W * max(0.0, 1 - ((n - 1500) / 300))\n    return 0.0"}, {"type": "code", "name": "SEO Headline Check", "description": "Validates headline length and presence of key terms relevant to the topic.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    import re\n    W = 1.0\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    # Read text\n    text = None\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n    if not text:\n        return 0.0\n    # Extract first non-empty line as headline candidate\n    lines = [ln.strip() for ln in text.splitlines()]\n    headline = next((ln for ln in lines if ln), '')\n    if not headline:\n        return 0.0\n    hl = headline.strip()\n    length = len(hl)\n    # Length scoring\n    length_score = 0.0\n    if 50 <= length <= 90:\n        length_score = 0.4\n    elif 35 <= length <= 110:\n        length_score = 0.2\n    else:\n        length_score = 0.0\n    # Keyword groups\n    hl_low = hl.lower()\n    g1 = any(k in hl_low for k in [\"rtls\", \"real-time location\", \"real time location\"])  # core tech\n    g2 = any(k in hl_low for k in [\"livestock\", \"cattle\", \"dairy\", \"herd\"])            # domain\n    g3 = any(k in hl_low for k in [\"tag\", \"tags\", \"collar\", \"collars\"])                 # device\n    keyword_score = (0.2 if g1 else 0.0) + (0.2 if g2 else 0.0) + (0.2 if g3 else 0.0)\n    return min(W, length_score + keyword_score)"}, {"type": "code", "name": "Direct Quotes and Interviewee Attribution", "description": "Checks presence of multiple direct quotes and attributions to at least two distinct named interviewees.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    import re\n    W = 1.0\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = None\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n    if not text:\n        return 0.0\n    # Count quotes (curly and straight)\n    curly = re.findall('\u201c[^\u201d]{6,}\u201d', text)\n    straight = re.findall('\"[^\\\"]{6,}\"', text)\n    total_quotes = len(curly) + len(straight)\n    # Distinct interviewees present\n    interviewees = [\"Jim Dalton\", \"Gaspar Olafsen\", \"Anne Smith\", \"Lars Andersen\"]\n    present = set([n for n in interviewees if n.lower() in text.lower()])\n    # Scoring\n    quote_score = 0.0\n    if total_quotes >= 3:\n        quote_score = 0.6\n    elif total_quotes >= 1:\n        quote_score = 0.2\n    name_score = 0.4 if len(present) >= 2 else (0.1 if len(present) == 1 else 0.0)\n    return min(W, quote_score + name_score)"}, {"type": "code", "name": "UK English Spelling Preference (Guardian style tendency)", "description": "Heuristic check that UK spellings dominate over US variants in the article body.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    import re\n    W = 1.0\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = None\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n    if not text:\n        return 0.0\n    t = ' ' + text.lower() + ' '\n    pairs = [\n        (\"colour\", \"color\"), (\"centre\", \"center\"), (\"organisation\", \"organization\"),\n        (\"organise\", \"organize\"), (\"organised\", \"organized\"), (\"organising\", \"organizing\"),\n        (\"analyse\", \"analyze\"), (\"analysed\", \"analyzed\"), (\"analysing\", \"analyzing\"),\n        (\"licence\", \"license\"), (\"defence\", \"defense\"), (\"metre\", \"meter\"),\n        (\"litre\", \"liter\"), (\"fibre\", \"fiber\"), (\"labour\", \"labor\"), (\"programme\", \"program\")\n    ]\n    uk = 0\n    us = 0\n    for uk_sp, us_sp in pairs:\n        uk += len(re.findall(r\"\\b\" + re.escape(uk_sp) + r\"\\b\", t))\n        us += len(re.findall(r\"\\b\" + re.escape(us_sp) + r\"\\b\", t))\n    # If no occurrences found, give neutral partial credit (not penalising technical jargon)\n    if uk + us == 0:\n        return 0.6\n    ratio_us = us / max(1, (uk + us))\n    # Score decreases as US proportion rises\n    score = max(0.0, 1.0 - 1.5 * ratio_us)\n    return min(W, score)"}, {"type": "llm_judge", "name": "RTLS Technical Coherence and Accuracy", "description": "Checks that RTLS is accurately explained and applied to livestock health/operations without material technical errors.", "weight": 5.0, "judge_prompt": "Evaluate the article for technical correctness and coherence regarding RTLS and its use in livestock monitoring:\n- Defines RTLS correctly (e.g., tracking location in or around confined areas; uses wireless like Wi-Fi, Bluetooth, RFID; near real-time visibility).\n- Applies RTLS plausibly to livestock health and operations (tags/collars, movement/rumination/heat detection, alerts, barn/pasture context).\n- Avoids common confusions (e.g., purely GPS-based tracking without RTLS context; indoor precision claims that conflict with radio technologies; impossible battery life claims without justification).\n- Connects to dairy/livestock economic pressures in a reasonable way (e.g., herd health, yield, labour efficiency) without unfounded causal claims.\nScoring guidance:\n- Full (weight): Clear, precise, and accurate throughout with no material errors.\n- 75%: Minor omissions or small simplifications; overall accurate.\n- 50%: Some ambiguity or a couple of notable inaccuracies.\n- 0\u201325%: Material technical errors or misleading claims.", "expectation": "Technically accurate RTLS description and realistic agricultural application details."}, {"type": "llm_judge", "name": "Attribution Integrity, Impartiality, and Balance", "description": "Verifies that quotes are properly attributed; reporting is impartial, avoids PR tone, and includes benefits/limitations.", "weight": 5.0, "judge_prompt": "Assess attribution and impartiality:\n- Direct quotes are clearly attributed to named interviewees (ideally: Jim Dalton, Gaspar Olafsen, Anne Smith, Lars Andersen). Paraphrases are not presented as quotes.\n- The piece is impartial and balanced (not an opinion column). It presents both benefits and limitations/risks (e.g., costs, connectivity, accuracy, animal welfare, data privacy).\n- Company mentions (Fair Farm Technologies, Useful Technologies, CattleWatch) are reported neutrally with appropriate context rather than promotional language.\n- Reliance on interviewee words to tell the story is evident, but the reporter maintains a neutral, informative voice.\nScoring:\n- Full (weight): Clean attribution, neutral tone, clear balance of pros/cons.\n- 75%: Mostly neutral with minor lapses or thin on limitations.\n- 50%: Attribution unclear in places or tone leans promotional.\n- 0\u201325%: Misattribution, clear bias, or one-sided PR framing.", "expectation": "Neutral, well-attributed reporting with balanced coverage of impacts and trade-offs."}, {"type": "llm_judge", "name": "UK English and Guardian Style Compliance", "description": "Assesses consistency with UK English and The Guardian\u2019s style conventions at a practical level.", "weight": 4.0, "judge_prompt": "Check for UK English and Guardian style alignment:\n- UK spellings (e.g., organise, programme (for TV), metre, labour, licence (noun)), date formats, and number/style conventions consistent with The Guardian\u2019s style guide.\n- Consistent, professional punctuation and grammar. Avoid reproducing errors present in interview notes.\n- Job titles, company names, first mention formatting, and general style are coherent with Guardian norms; avoid Americanisms and inconsistent capitalisation.\nScoring:\n- Full (weight): Consistent UK English and solid Guardian-style presentation.\n- 75%: Minor inconsistencies that do not impede professionalism.\n- 50%: Noticeable mixture of US/UK or style issues.\n- 0\u201325%: Frequent US spellings or style errors inconsistent with Guardian norms.", "expectation": "Predominantly UK English with Guardian-style conventions."}, {"type": "llm_judge", "name": "Source Log Sanity and Internal Consistency", "description": "Evaluates whether the Source Log\u2019s mappings are internally consistent with the article\u2019s quotes and attributions.", "weight": 4.0, "judge_prompt": "Cross-check the article body and the Source Log for internal consistency:\n- For each named quote in the article, there is a plausible corresponding entry in the Source Log listing interviewee and identifiable source type/identifier.\n- Names and roles in the article match those in the Source Log (no contradictions or mismatched identities).\n- There are at least three mapped entries; coverage appears comprehensive rather than selective.\nScoring:\n- Full (weight): Strong internal consistency and comprehensive mapping.\n- 75%: Minor gaps or a small number of ambiguous mappings.\n- 50%: Several quotes lack clear Source Log mapping.\n- 0\u201325%: Little to no alignment between quotes and the Source Log.", "expectation": "Article quotes align cleanly with a comprehensive, internally consistent Source Log."}], "on_failure_action": "skip_remaining", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Quality and Impact Assessment", "description": "Holistic LLM assessment of craft quality, accessibility, depth for experts, narrative use of interviews, and SEO effectiveness for an international audience.", "is_required": false, "max_points": 20.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity, Structure, and Readability", "description": "Evaluates whether the article is clear, well-structured, and accessible to mainstream readers.", "weight": 5.0, "judge_prompt": "Judge the article\u2019s clarity and structure:\n- Standfirst sets context; subheadings guide readers logically; paragraphs are well-sized; jargon is explained.\n- The piece remains accessible to a general audience while introducing enterprise-tech concepts without oversimplification.\n- Smooth transitions and signposting; minimal redundancy.\nScoring: Full (weight) for clear, well-organised, highly readable structure; deduct for dense jargon, poor signposting, or confusing flow.", "expectation": "Clear, logical structure with accessible explanations for non-experts."}, {"type": "llm_judge", "name": "Depth and Insight for Expert Readers", "description": "Assesses whether the article includes sufficient depth to interest expert readers without alienating general readers.", "weight": 5.0, "judge_prompt": "Evaluate depth and insight:\n- Provides meaningful specifics (how RTLS works, signal types, sensors in tags/collars, accuracy considerations, battery life/maintenance, integration with farm management systems, alerts/analytics).\n- Addresses economic/operational context (dairy pricing pressures, productivity, welfare), and practical constraints (connectivity, costs, privacy, data ownership).\n- Offers original synthesis beyond surface-level description.\nScoring: Full (weight) for rich, precise details and thoughtful analysis; reduce for shallow coverage or missing key dimensions.", "expectation": "Substantive technical and operational insight that still reads smoothly."}, {"type": "llm_judge", "name": "Narrative Use of Interviews", "description": "Evaluates how effectively interviewee quotes drive the story and add human perspective without becoming anecdote-only.", "weight": 5.0, "judge_prompt": "Assess narrative use of interviews:\n- Quotes are vivid, purposeful, and diversified across interviewees; they illuminate the technology\u2019s impact rather than restating marketing claims.\n- The reporter weaves quotes into a coherent narrative with context and analysis.\n- Avoids over-quoting or filler quotes; attributions are crisp and unobtrusive.\nScoring: Full (weight) for deft integration of quotes that carry the story; deduct for shallow or misused quotes.", "expectation": "Quotes from multiple sources advance the narrative and insight."}, {"type": "llm_judge", "name": "SEO and International Audience Suitability", "description": "Assesses headline/standfirst SEO, keyword usage, and global relevance for an international audience.", "weight": 5.0, "judge_prompt": "Evaluate SEO and international suitability:\n- Headline/standfirst contain relevant keywords (RTLS, livestock/cattle/dairy, tags/collars) and reflect article content without clickbait.\n- Avoids UK-only jargon; provides context intelligible to international readers while acknowledging UK base.\n- Meta signals in the copy (first 150\u2013200 words) orient readers quickly; subheadings also reinforce core terms.\nScoring: Full (weight) for strong keyword use, clarity, and global accessibility; deduct for weak SEO cues or parochial framing.", "expectation": "SEO-aware headline/deck and content accessible to an international readership."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "a941b6d8-4289-4500-b45a-f8e4fc94a724", "rubric": {"category_name": "Film/TV VFX Shot: Teleportation Composite (Video Editor/Compositor)", "rationale": "This rubric enforces a self-documenting VFX workflow. Stage 1 mandates a structured PDF/DOCX \u201cVFX Shot Package\u201d that captures the entire compositing pipeline and enumerates deliverables for verifiable checks. Stage 2 mixes lightweight code checks (file presence, names, basic sanity) with LLM verification of methodological correctness and internal consistency. Stage 3 assesses professional quality, storytelling impact, polish, and clarity.", "max_total_score": 25.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (VFX Shot Package)", "description": "LLM-only gate that verifies the candidate\u2019s primary output is a structured PDF/DOCX VFX Shot Package with the exact sections and artifacts needed for verification.", "is_required": true, "max_points": 5.0, "min_score_to_pass": 3.5, "rules": [{"type": "llm_judge", "name": "Structured VFX Shot Package (PDF/DOCX)", "description": "Check the primary output is a PDF or DOCX VFX Shot Package with required sections, embedded stills, and a Deliverables Summary table naming the output files.", "weight": 5.0, "judge_prompt": "You are evaluating the SHAPE ONLY of the candidate\u2019s primary output. Do not judge the quality or correctness here\u2014only structure and presence. The task is a VFX teleportation composite.\n\nConfirm the primary output is a PDF or DOCX document that is at least 2 pages, clearly formatted with headings, and includes ALL the following sections (be flexible on exact heading names):\n\nRequired structure:\n1) Title and Shot Metadata\n   - Title like \u201cVFX Teleportation Shot Package\u201d (or similar)\n   - Shot metadata that references both source files by name: Base Clip: \u201cTWT_001_02.mp4\u201d and Overlay Clip: \u201cTWT_A001_03.mp4\u201d\n   - Frame size, framerate, codec intended for final render\n\n2) Source Footage and Method Overview\n   - Brief description of both clips and the target effect\n\n3) Stabilization and Window Masking (Overlay Clip)\n   - States that the overlay clip is stabilized\n   - States that the actor window is isolated via a mask, outside area alpha\u2019d out\n\n4) Stitch Edit Plan (Actor Disappearance)\n   - Select approx six seconds of actor performance in overlay clip and ~three seconds after the actor ducks out (starting around ~20s in the overlay), stitched to simulate disappearance\n   - Provide approximate timecodes/durations\n\n5) Tracking/Match Move and Composite on Base Clip\n   - States tracking of motion/scale/perspective in the base clip and application to the overlay clip\n\n6) Color Match/Grade\n   - States color grading/matching of overlay to base\n\n7) VFX Elements: Flash + Smoke\n   - Mentions a flash at the disappearance moment\n   - Mentions smoke VFX overlays and cites royalty-free sources (name and/or URL)\n\n8) Render Specs & Export\n   - Confirms final video render that matches base clip dimensions, framerate, and codec\n\n9) Deliverables Summary Table\n   - A clear table or bulleted list explicitly naming output file(s) with exact filenames and extensions, including: at least one Final Shot video (.mp4 or .mov), and at least two still frame images (.png or .jpg) for before/after. Optional additional assets are acceptable.\n\n10) QA/Checks Notes\n   - A short checklist or notes of visual checks (e.g., edges, tracking stick, color match)\n\nAlso required:\n- Embedded still images (at least 2), for example a before and after of the teleportation window/actor region.\n- Minimum length: 2 pages.\n\nScoring (shape only):\n- 5.0: Valid PDF/DOCX, \u22652 pages, clear headings, contains all 10 sections, includes Deliverables Summary with explicit filenames + \u22652 embedded stills\n- 4.0: Valid PDF/DOCX, \u22652 pages, missing any one non-core detail (e.g., missing royalty-free source details or partial render specs) but core sections are present\n- 2.5: Valid PDF/DOCX with structure but missing multiple core sections (e.g., no Stitch Edit Plan or no Tracking/Composite section) OR missing Deliverables Summary\n- 0.0: Not PDF/DOCX, or <2 pages, or lacks most required structure\n\nOnly evaluate structural presence and format, not correctness or quality.", "expectation": "A 2+ page PDF/DOCX \u201cVFX Shot Package\u201d with all listed sections, embedded stills, and a Deliverables Summary table listing filenames for final video and at least two stills."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness and Consistency)", "description": "Now verify correctness using code and LLM. Code rules check deliverables and basic sanity. LLM rules check methodological correctness, internal consistency, and plausibility.", "is_required": true, "max_points": 12.0, "min_score_to_pass": 6.0, "rules": [{"type": "code", "name": "Deliverables listed in doc actually exist", "description": "Parse the primary PDF/DOCX text for Deliverables filenames and verify those files exist among outputs, ensuring at least one video and two stills are both listed and present.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No primary output.\"\n\n    # Read text from document\n    text = \"\"\n    try:\n        if getattr(output, 'is_document', False):\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    text = \"\"\n        elif getattr(output, 'is_text_format', False):\n            text = context.files.read_text(output.id)\n    except Exception:\n        text = \"\"\n\n    if not text:\n        return 0.0, \"Could not read document text.\"\n\n    # Extract candidate filenames from the document\n    pattern = re.compile(r\"\\b[\\w\\-\\.\\s]+\\.(mp4|mov|png|jpg|jpeg)\\b\", re.IGNORECASE)\n    mentioned = [m.group(0).strip() for m in pattern.finditer(text)]\n    mentioned_lower = [m.lower() for m in mentioned]\n\n    # Build set of actual output filenames\n    all_outputs = context.get_all_outputs() or []\n    actual_names = set()\n    for r in all_outputs:\n        try:\n            p = context.files.get_path(r.id)\n            actual_names.add(p.name.lower())\n        except Exception:\n            pass\n\n    # Count deliverables that both are mentioned and actually exist\n    video_refs = [n for n in mentioned_lower if n.endswith((\".mp4\", \".mov\"))]\n    image_refs = [n for n in mentioned_lower if n.endswith((\".png\", \".jpg\", \".jpeg\"))]\n\n    video_present = any(n in actual_names for n in video_refs)\n    images_present_count = sum(1 for n in image_refs if n in actual_names)\n\n    score = 0.0\n    if video_refs and video_present:\n        score += 0.5\n    if len(image_refs) >= 2 and images_present_count >= 2:\n        score += 0.5\n\n    feedback = f\"Mentioned videos: {len(video_refs)}, present: {video_present}; Mentioned images: {len(image_refs)}, present count: {images_present_count}.\"\n    return score, feedback"}, {"type": "code", "name": "Image deliverables sanity (file sizes and relative similarity)", "description": "Check at least two image deliverables (.png/.jpg) exist among outputs and are non-trivially sized; ensure the first two are roughly similar size (suggesting comparable resolution).", "weight": 0.6, "code": "def evaluate(workflow, context):\n    outputs = context.get_all_outputs() or []\n    images = []\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            suf = p.suffix.lower()\n            if suf in [\".png\", \".jpg\", \".jpeg\"]:\n                size = p.stat().st_size\n                images.append((p.name, size))\n        except Exception:\n            continue\n    if len(images) < 2:\n        return 0.0, \"Fewer than two image outputs found.\"\n\n    # Take the first two\n    (n1, s1), (n2, s2) = images[0], images[1]\n    non_trivial = (s1 >= 20_000) + (s2 >= 20_000)  # >= ~20KB each\n    ratio_ok = 0\n    if s1 > 0 and s2 > 0:\n        ratio = s1 / s2 if s1 >= s2 else s2 / s1\n        # within ~1.8x suggests comparable resolution\n        if ratio <= 1.8:\n            ratio_ok = 1\n    score = (non_trivial / 2.0) * 0.5 + (ratio_ok * 0.5)\n    feedback = f\"Image1 {n1} size={s1}B, Image2 {n2} size={s2}B, non_trivial={non_trivial==2}, ratio_ok={bool(ratio_ok)}\"\n    return score, feedback"}, {"type": "code", "name": "Timing details mentioned (6s, 3s, ~20s)", "description": "Verify the document text references the intended durations and timing (about 6 seconds + 3 seconds; starting around 20 seconds) using flexible pattern matching.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    text = \"\"\n    try:\n        if getattr(output, 'is_document', False):\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    text = \"\"\n        elif getattr(output, 'is_text_format', False):\n            text = context.files.read_text(output.id)\n    except Exception:\n        text = \"\"\n    if not text:\n        return 0.0\n    tl = text.lower()\n\n    patterns = [\n        r\"\\b(6|six)\\s*(sec|secs|second|seconds|s)\\b\",\n        r\"\\b(3|three)\\s*(sec|secs|second|seconds|s)\\b\",\n        r\"\\b(20|twenty)\\s*(sec|secs|second|seconds|s)?\\b\"\n    ]\n    hits = sum(1 for pat in patterns if re.search(pat, tl))\n    return hits / 3.0"}, {"type": "code", "name": "Final video naming sanity", "description": "Check that at least one video output exists and that at least one video filename suggests it is the final comp (e.g., contains words like 'final', 'comp', or 'vfx') and references the teleportation shot.", "weight": 0.6, "code": "def evaluate(workflow, context):\n    outputs = context.get_all_outputs() or []\n    video_names = []\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            if p.suffix.lower() in [\".mp4\", \".mov\"]:\n                video_names.append(p.name.lower())\n        except Exception:\n            continue\n    if not video_names:\n        return 0.0, \"No video outputs found.\"\n\n    keywords_final = [\"final\", \"comp\", \"vfx\"]\n    keywords_shot = [\"teleport\", \"teleportation\", \"window\", \"twt\"]\n\n    good = 0\n    for name in video_names:\n        if any(k in name for k in keywords_final) and any(k in name for k in keywords_shot):\n            good = 1\n            break\n    score = 1.0 if good else 0.5  # partial credit if video exists but naming not informative\n    feedback = f\"Videos found: {video_names}; naming_ok={bool(good)}\"\n    return score, feedback"}, {"type": "llm_judge", "name": "Method completeness and internal consistency", "description": "Verify the document describes the full, correct method and components per brief: stabilization/masking of overlay, stitch edit (\u22486s + \u22483s starting around 20s), tracking/match move to base, composite and color grade, plus flash and smoke with royalty-free sourcing.", "weight": 3.0, "judge_prompt": "Judge the METHOD for completeness and internal consistency against the brief (do not assess aesthetics here):\n- Stabilization: Overlay clip stabilized and window isolated with a mask (outside area made alpha/transparent)\n- Stitch Edit: About six seconds of performance + about three seconds after the actor ducks out starting \u224820s in the overlay, stitched to simulate disappearance (timecodes or durations stated)\n- Tracking/Match Move: Camera motion/scale/perspective tracked in the base clip and transformation applied to overlay\n- Composite: Overlay window blends convincingly into base (transform applied to match)\n- Color Grade: Overlay matched to base\n- Flash + Smoke: Flash at the disappearance moment; smoke overlays; royalty-free libraries referenced (name/URL)\n- Render Specs: Final render matches base clip dimensions, framerate, and codec\n- Deliverables: Final video + at least two stills named explicitly in a Deliverables Summary\nScore higher when all items are clearly addressed with consistent timings and steps; lower if key steps are missing, contradictory, or vague.", "expectation": "A clear, step-by-step method covering all bullets with specific timings and sourcing details."}, {"type": "llm_judge", "name": "Tracking and compositing plausibility", "description": "Assess whether the described tracking, perspective/scale match, and compositing steps are technically plausible and self-consistent.", "weight": 2.5, "judge_prompt": "Evaluate the plausibility and self-consistency of tracking and compositing: Does the document explain how motion/scale/perspective of the base clip are tracked and applied to the overlay window? Are blend, edges, and integration steps described (e.g., feathered mask, edge treatment, perspective transform, or corner pin)? Are before/after stills consistent with the described alignment and disappearance logic? Score higher if the approach would plausibly hold up in production.", "expectation": "Coherent description of tracking (e.g., 2D/planar/corner pin), transform application, masking/feathering, and examples that align with the stills."}, {"type": "llm_judge", "name": "Color and effects coherence", "description": "Assess whether color matching and effects (flash and smoke) are used coherently, with attention to exposure/temperature continuity and effect integration.", "weight": 2.5, "judge_prompt": "Evaluate color and effects coherence: Does the document describe color matching (temperature/exposure/contrast) so the overlay matches the base? Is the flash timed exactly at disappearance and integrated believably (e.g., glow/decay, screen/add blending)? Are smoke elements cited, oriented, timed, and blended sensibly (e.g., depth, opacity, color tint to match scene)?", "expectation": "Color match considerations, effect timing and blending notes that would plausibly integrate the elements."}, {"type": "llm_judge", "name": "QC and risk handling", "description": "Check whether the document anticipates artifacts and includes a QC checklist for seams, edges, tracking drift, color mismatch, etc.", "weight": 2.0, "judge_prompt": "Evaluate QC and risk handling: Does the package include a checklist or notes for potential issues (mask edges, tracking drift, parallax, motion blur continuity, grain/noise match, color shifts, exposure spikes at flash)? Are mitigation steps included (e.g., feathering, motion blur settings, grain match, denoise)?", "expectation": "A practical QC list with mitigations that reflect professional practice."}], "on_failure_action": "skip_remaining", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism", "description": "Holistic LLM assessment of craft, clarity, and storytelling value of the work and documentation.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional communication and organization", "description": "Assess clarity, structure, and professionalism of the document (tone, readability, logical flow, diagrams/stills, and labeling).", "weight": 2.0, "judge_prompt": "Assess the professional quality of the document: Is it well-structured with clear headings and logical flow? Are the stills/figures labeled and referenced? Is the tone concise and professional? Are tables (e.g., Deliverables Summary) neat and readable?", "expectation": "Clear, concise, and well-organized package with labeled visuals and readable tables."}, {"type": "llm_judge", "name": "Storytelling impact and creative choices", "description": "Evaluate whether the teleportation effect supports the story and visual impact (timing, pacing, emphasis).", "weight": 2.0, "judge_prompt": "Evaluate the storytelling and creative impact: Do the timing and pacing of the disappearance (including flash/smoke) enhance the moment? Are choices justified to support the narrative (e.g., timing of flash relative to actor exit, smoke density/duration)?", "expectation": "A thoughtful rationale for creative decisions that enhances narrative impact."}, {"type": "llm_judge", "name": "Technical polish", "description": "Judge the overall technical polish described: edge handling, tracking robustness, grain/noise, motion blur, and color continuity.", "weight": 2.0, "judge_prompt": "Judge technical polish: Do the described steps suggest clean edges (feathered masks, spill control), stable tracking (planar/corner pin robustness), proper grain/noise matching, motion blur continuity, and consistent color/exposure?", "expectation": "Descriptions indicating industry-standard finishing practices and attention to detail."}, {"type": "llm_judge", "name": "Licensing and sourcing diligence", "description": "Ensure third-party smoke assets are genuinely royalty-free and properly cited; check for responsible usage notes.", "weight": 2.0, "judge_prompt": "Evaluate licensing diligence: Are smoke/flash (if stock) sources identified as royalty-free with attribution or license info? Are any usage constraints noted? Are links or library names provided?", "expectation": "Clear, responsible sourcing with royalty-free confirmation and links/library names."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "8c8fc328-69fc-4559-a13f-82087baef0a1", "rubric": {"category_name": "Basic Documentary Script (Pre-Papercut) for Microscopic Life", "rationale": "Pattern B (Document). The deliverable is a DOCX basic script with generalized scenes and timecoded beats, suitable for pre-papercut editing. Stage 1 uses an LLM-only gate to enforce precise structure and format so verification becomes trivial. Stage 2 mixes lightweight code checks (file text parsing for title, timecodes, bounds) with heavier LLM judgment on accuracy, audience fit, and consistency. Stage 3 provides a holistic quality review on narrative strength, readability, and production usefulness. Code rules are kept robust and flexible and contribute ~5x less than LLM rules, per guidance.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Structure and Format Gate (LLM-only)", "description": "Verify the output is a .docx basic script with the mandated sections, title, timecoded beats, and constraints that enable verification.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.0, "rules": [{"type": "llm_judge", "name": "Document Format and Structural Completeness", "description": "Check that the candidate output is a DOCX basic script with required sections and elements, enabling later verification.", "weight": 3.0, "judge_prompt": "You are evaluating whether the delivered file is a valid Word document (DOCX) basic script in the exact structure needed to verify it later. Be flexible with section names, but strict about presence and structure.\n\nCheck the following Shape Requirements:\n1) File format: Must be a Word document (DOCX). Not PDF, not plain text.\n2) Title: Must include the exact title \u201cUnseen Realms: The Microscopic Marvels.\u201d Preferably on the first page, clearly distinguished as the title.\n3) Metadata/Header block near the top that states, in any wording:\n   - Target runtime (2\u20138 minutes)\n   - Intended audiences: children 6\u201312 and adults 25\u201334\n   - Distribution/platforms: broadcast and internet/online\n   - Brand tone keywords: calm, enriching, trustworthy, intellectually stimulating (minor variations acceptable)\n4) Required Sections (names may vary, but content must be present):\n   A) Overview/Synopsis: 1\u20132 short paragraphs describing the documentary concept at a high level.\n   B) Timecoded Beat or Structure Outline: A list or simple table of generalized scenes, each with:\n      - Timecode stamps (e.g., 00:00, 00:30, 01:10; mm:ss format)\n      - A beat/scene title or description (generalized visuals; not a shot list)\n      - A brief VO/narration summary for that beat\n   C) Narration/Voiceover Script: The VO lines, grouped by timecode or beat, separated from other text so they\u2019re easy to identify.\n   D) Audience & Tone Notes: Short notes on how the script addresses both age groups and matches the brand tone.\n5) Constraints:\n   - The script reads as a pre-papercut basic script (no shot-by-shot detail; generalized scenes are ok)\n   - Includes multiple timecoded beats (at least 4) covering the whole piece\n   - Overall length under 5 pages (approximate check is acceptable)\n\nScoring:\n- 3.0: DOCX + all required sections present with clear headers/labels; exact title matches; timecoded beats with VO summaries; metadata present; appears under 5 pages.\n- 2.4: DOCX with all core sections (Overview, Timecoded Beat Outline, VO Script) present; minor omission (e.g., metadata partially missing OR audience/tone notes missing OR length slightly uncertain) but structure is otherwise solid.\n- 1.5: DOCX present but missing one core section OR no timecodes OR title not exact; still clearly a basic script with partial structure.\n- 0.5: A document exists but wrong format (e.g., PDF) OR multiple required sections missing.\n- 0.0: Not a document OR not recognizable as a basic script.\n\nOnly judge presence/format/structure. Do NOT judge correctness of content or quality here.", "expectation": "A DOCX basic script with the exact title, clear overview, a timecoded beat outline with generalized scenes and VO summaries, a separate VO section, audience/tone notes, metadata for runtime/audience/platforms/brand tone, and <5 pages."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness and Consistency)", "description": "Now that structure is enforced, verify correctness: title match, timecode bounds, platform mentions via code; plus LLM checks for audience fit, scientific reasonableness, consistency, and brand tone.", "is_required": true, "max_points": 5.0, "min_score_to_pass": 2.5, "rules": [{"type": "code", "name": "Exact Title Present", "description": "Verify the exact title string appears in the document text.", "weight": 0.3, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    \\\"\\\"\\n    Args:\\n        workflow: Workflow object\\n        context: ValidationContext\\n\\n    Returns:\\n        float or (float, str)\\n    \\\"\\\"\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0, \\\"No document output to evaluate.\\\"\\n\\n    text = ''\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_text(output.id)\\n            except Exception:\\n                text = ''\\n\\n    if not text:\\n        return 0.0, \\\"Unable to read document text.\\\"\\n\\n    t = text.lower()\\n    title_exact = \\\"unseen realms: the microscopic marvels\\\"\\n    if title_exact in t:\\n        return 0.3, \\\"Exact title found.\\\"\\n    # partial credit if core words present\\n    has_unseen = 'unseen realms' in t\\n    has_micro = ('microscopic' in t) and ('marvel' in t or 'marvels' in t)\\n    if has_unseen and has_micro:\\n        return 0.2, \\\"Partial title match (key words present).\\\"\\n    return 0.0, \\\"Title not found.\\\""}, {"type": "code", "name": "Timecode Coverage and Duration Bounds", "description": "Check presence of multiple mm:ss timecodes and ensure max time falls within 2\u20138 minutes. Grants partial for near-bounds.", "weight": 0.5, "code": "import re\\n\\ndef _to_seconds(tc):\\n    try:\\n        parts = tc.split(':')\\n        if len(parts) != 2:\\n            return None\\n        m, s = int(parts[0]), int(parts[1])\\n        if m < 0 or s < 0 or s > 59:\\n            return None\\n        return m * 60 + s\\n    except Exception:\\n        return None\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0, \\\"No document output to evaluate.\\\"\\n\\n    text = ''\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_text(output.id)\\n            except Exception:\\n                text = ''\\n\\n    if not text:\\n        return 0.0, \\\"Unable to read document text.\\\"\\n\\n    # find mm:ss timecodes\\n    codes = re.findall(r\"\\\\b(?:[0-5]?\\\\d):[0-5]\\\\d\\\\b\", text)\\n    seconds = [s for s in (_to_seconds(c) for c in codes) if s is not None]\\n    count = len(seconds)\\n    max_sec = max(seconds) if seconds else None\\n\\n    score = 0.0\\n    msg_parts = []\\n\\n    # Coverage component (up to 0.2)\\n    if count >= 6:\\n        score += 0.2; msg_parts.append(f\\\"coverage ok ({count} timecodes)\\\")\\n    elif count >= 4:\\n        score += 0.15; msg_parts.append(f\\\"coverage fair ({count})\\\")\\n    elif count >= 2:\\n        score += 0.08; msg_parts.append(f\\\"minimal coverage ({count})\\\")\\n    else:\\n        msg_parts.append(f\\\"insufficient coverage ({count})\\\")\\n\\n    # Duration bounds component (up to 0.3)\\n    if max_sec is not None:\\n        if 120 <= max_sec <= 480:\\n            score += 0.3; msg_parts.append(f\\\"duration in range (max {max_sec}s)\\\")\\n        elif 90 <= max_sec <= 540:\\n            score += 0.15; msg_parts.append(f\\\"duration near range (max {max_sec}s)\\\")\\n        else:\\n            msg_parts.append(f\\\"duration out of range (max {max_sec}s)\\\")\\n    else:\\n        msg_parts.append(\\\"no parsable timecodes\\\")\\n\\n    return min(score, 0.5), '; '.join(msg_parts) if msg_parts else ''"}, {"type": "code", "name": "Platforms and Section Keywords Present", "description": "Verify mentions of platforms (broadcast and internet/online) and presence of key section cues (overview/synopsis, voiceover/narration, scenes/sequence/beat).", "weight": 0.2, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0, \\\"No document output to evaluate.\\\"\\n\\n    text = ''\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_text(output.id)\\n            except Exception:\\n                text = ''\\n\\n    if not text:\\n        return 0.0, \\\"Unable to read document text.\\\"\\n\\n    t = text.lower()\\n\\n    # Platforms component (0.08)\\n    has_broadcast = 'broadcast' in t\\n    has_internet = ('internet' in t) or ('online' in t) or ('web' in t)\\n    plat_score = 0.08 if (has_broadcast and has_internet) else (0.04 if (has_broadcast or has_internet) else 0.0)\\n\\n    # Sections component (0.12 total, 0.04 each)\\n    has_overview = ('overview' in t) or ('synopsis' in t)\\n    has_vo = ('voiceover' in t) or ('voice-over' in t) or ('vo' in t) or ('narration' in t)\\n    has_scene = ('scene' in t) or ('sequence' in t) or ('beat' in t)\\n    sec_score = (0.04 if has_overview else 0.0) + (0.04 if has_vo else 0.0) + (0.04 if has_scene else 0.0)\\n\\n    score = plat_score + sec_score\\n    msg = f\\\"platforms:{'ok' if plat_score>=0.08 else 'partial' if plat_score>0 else 'missing'}; sections OVR:{has_overview} VO:{has_vo} SCN:{has_scene}\\\"\\n    return min(score, 0.2), msg"}, {"type": "llm_judge", "name": "Audience Fit (Dual Address: Kids 6\u201312 and Adults 25\u201334)", "description": "Assess whether wording, concepts, and explanations appropriately address both age groups simultaneously (curious, accessible, and not condescending).", "weight": 1.2, "judge_prompt": "Evaluate the script\u2019s audience fit for two demographics: children ages 6\u201312 and adults ages 25\u201334. Consider vocabulary, clarity, curiosity, and whether the tone engages both without being too childish or too technical. Look for helpful analogies and clear explanations of microscopic concepts without oversimplifying. Penalize jargon without explanation or content that would be frightening or confusing for children.\\n\\nScoring (0\u20131.2):\\n- 1.2: Strong dual-address: clear, engaging for kids and adults; explanations are accessible; no inappropriate content.\\n- 0.8: Generally works for both audiences with minor issues (occasional jargon or moments skewing too young/old).\\n- 0.4: Uneven targeting; significant stretches skew to only one audience or contain confusing terms.\\n- 0.0: Inappropriate tone or content for children; or wholly mismatched to both audiences.", "expectation": "A calm, curious script that uses vivid but age-appropriate language and explains terms in simple, accurate ways for both kids and young adults."}, {"type": "llm_judge", "name": "Scientific Reasonableness and Accuracy", "description": "Check that depictions of microscopic life cycles and intricacies are broadly accurate and responsibly framed (no sensationalism or major factual errors).", "weight": 1.0, "judge_prompt": "Assess whether the scientific content about microscopic life (e.g., microbes, cells, reproduction, ecosystems, symbiosis, food webs) appears broadly accurate and responsibly framed. Look for clear, non-misleading explanations and avoid sensationalized or fear-based claims. Minor simplifications for accessibility are acceptable, but no glaring scientific mistakes.\\n\\nScoring (0\u20131.0):\\n- 1.0: Scientifically sound with correct concepts and responsible framing.\\n- 0.7: Mostly accurate with small simplifications or minor issues.\\n- 0.3: Several inaccuracies, overstatements, or confusing explanations.\\n- 0.0: Major factual errors or misleading claims.", "expectation": "High-level scientific accuracy with accessible explanations and careful language."}, {"type": "llm_judge", "name": "Beat-to-VO Mapping Consistency", "description": "Confirm the timecoded beat outline aligns with the VO section: each beat has corresponding VO or summary, and coverage is complete.", "weight": 0.9, "judge_prompt": "Compare the timecoded beat/structure outline to the VO/Narration section. Check that each beat has corresponding VO (or a clear VO summary), that timecodes align roughly, and that there are no large gaps. Accept reasonable flexibility in timing.\\n\\nScoring (0\u20130.9):\\n- 0.9: Clear one-to-one or coherent mapping; timing alignment is evident; no gaps.\\n- 0.6: Mostly aligned with minor mismatches or small omissions.\\n- 0.3: Incomplete mapping; multiple beats lack corresponding VO or timings are unclear.\\n- 0.0: VO and beats appear unrelated or misaligned.", "expectation": "A coherent mapping where the editor can follow beats and corresponding narration easily."}, {"type": "llm_judge", "name": "Brand Personality Alignment", "description": "Evaluate whether tone reflects calm, enriching, trustworthy, and intellectually stimulating personality without sensationalism.", "weight": 0.9, "judge_prompt": "Judge whether the script\u2019s tone matches the brand personality: calm, enriching, trustworthy, intellectually stimulating. Penalize hype, fear, or sensationalism. Look for measured, confident language, gentle pacing, and a sense of wonder.\\n\\nScoring (0\u20130.9):\\n- 0.9: Strong alignment across all four traits.\\n- 0.6: Good alignment with one trait weaker.\\n- 0.3: Noticeable mismatches in tone.\\n- 0.0: Opposite tone (sensationalistic, alarmist, or gimmicky).", "expectation": "Measured, insightful narration that encourages curiosity and trust."}], "on_failure_action": "skip_remaining", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Holistic Quality Assessment", "description": "Assess overall narrative strength, readability, and production usefulness as a pre-papercut script.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Narrative Arc and Engagement", "description": "Evaluate hook, progression, transitions, and closing. Does the script sustain interest while staying calm and enriching?", "weight": 0.7, "judge_prompt": "Assess the overall story arc: an inviting opening, progression through distinct beats, smooth transitions, and a satisfying close. The tone should remain calm yet engaging throughout.\\n\\nScoring (0\u20130.7):\\n- 0.7: Strong arc with smooth flow and engaging transitions.\\n- 0.45: Clear arc with a few rough transitions or pacing dips.\\n- 0.2: Fragmented or meandering with weak transitions.\\n- 0.0: Disjointed with no apparent structure.", "expectation": "A polished, coherent arc with clear transitions and satisfying close."}, {"type": "llm_judge", "name": "Readability and Formatting Quality", "description": "Judge clarity, grammar, and formatting (headings, clear VO separation, consistent timecode formatting).", "weight": 0.7, "judge_prompt": "Evaluate clarity of writing, grammar, and formatting conventions. Look for clear headers, consistent timecode formatting (e.g., mm:ss), and obvious separation of VO from other content.\\n\\nScoring (0\u20130.7):\\n- 0.7: Clean, professional formatting and writing.\\n- 0.45: Minor issues but generally clear.\\n- 0.2: Multiple clarity or formatting problems.\\n- 0.0: Hard to read or poorly formatted.", "expectation": "Professionally readable with consistent timecodes and clear sectioning."}, {"type": "llm_judge", "name": "Production Usefulness (Pre-Papercut)", "description": "Assess how actionable the script is for an editor: generalized scenes, clear beats, and VO that can guide assembly without shot specificity.", "weight": 0.6, "judge_prompt": "From an editor\u2019s perspective, is this pre-papercut script actionable? Check that beats are clear, scenes are generalized (not a shot list), and VO provides enough guidance to assemble a first pass.\\n\\nScoring (0\u20130.6):\\n- 0.6: Highly actionable; easy to assemble.\\n- 0.4: Generally usable with minor ambiguities.\\n- 0.2: Usable but requires substantial interpretation.\\n- 0.0: Not useful as a pre-papercut guide.", "expectation": "A clear beat map with VO guidance that supports a first assembly."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "85d95ce5-b20c-41e2-834e-e788ce9622b6", "rubric": {"category_name": "Government \u2022 Child, Family, and School Social Workers \u2014 Social Developmental History (SDH) Report for JOHN SMITH", "rationale": "Pattern B (Document task). The rubric enforces a self-documenting, verifiable deliverable: a PDF titled \u201cJ.S.\u201d that follows the SDH template, includes mandated sections, and uses SCHOOL as the placeholder. Stage 1 (LLM-only) is a strict structural gate. Stage 2 mixes lightweight code checks (file name, key headers, recommendation count) with LLM cross-checks (use of notes, impressions narrative with conclusion, recommendations relevance, identity/date consistency). Stage 3 holistically assesses professional quality, ethics, clarity, and actionability for a K\u20135 HCPS context.", "max_total_score": 50.0, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate (LLM-only)", "description": "Gate ensuring the deliverable is a properly structured PDF following the SDH template with required sections and placeholders. Failure zeros the category.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Valid PDF, Title, and Length (8\u201315 pages)", "description": "Check the final deliverable is a PDF titled \u201cJ.S.\u201d and is 8\u201315 pages. Be flexible with minor filename variations (e.g., J.S.pdf, J_S.pdf).", "weight": 3.0, "judge_prompt": "You are validating the final deliverable for a Social Developmental History (SDH) report.\n\nCheck ONLY format and shape (not content quality):\n- Output must be a PDF file (not Word, not plain text).\n- File title/name should be \u201cJ.S.\u201d (accept minor filename variants like J.S.pdf, J_S.pdf, JS.pdf if clearly intended).\n- Document length should be between 8 and 15 pages, inclusive.\n\nScoring:\n- 3.0: PDF format + filename clearly intended as \u201cJ.S.\u201d + 8\u201315 pages.\n- 2.0: PDF format + filename acceptable but pages slightly outside range by 1 page (7 or 16), OR perfect length but filename is a close variant.\n- 1.0: PDF format only (filename off and/or length far outside range).\n- 0.0: Not a PDF.\n\nOnly evaluate the presence of these shape features; do not judge content quality.", "expectation": "A PDF named as a clear variant of \u201cJ.S.\u201d with 8\u201315 pages."}, {"type": "llm_judge", "name": "Template Sections Present", "description": "Verify the SDH template structure is present with visible section headers.", "weight": 3.0, "judge_prompt": "Check the PDF has the following SDH structure with clear headers (allow minor header name variations/synonyms):\nRequired sections:\n1) Title/cover indicating Social Developmental History\n2) Student identifiers (contains student name JOHN SMITH)\n3) School Social Work Impressions (narrative section)\n4) School Social Work Recommendations (numbered list expected later)\n5) Background/Developmental or Family History sections (typical SDH content)\n6) Education/School functioning section(s)\n7) Services/Supports or Resources section(s) OR an Appendix/Supporting Information\n\nScoring:\n- 3.0: Clear SDH title/cover + all 6 of the remaining required areas present with headers.\n- 2.0: SDH title/cover + 4\u20135 of the remaining areas present.\n- 1.0: SDH title/cover + 2\u20133 areas present.\n- 0.0: SDH title/cover missing or fewer than 2 areas present.\n\nDo not check content quality, only the presence of sections.", "expectation": "A recognizable SDH document with the major sections present and labeled."}, {"type": "llm_judge", "name": "Placeholder and Required Field Gate", "description": "Verify SCHOOL placeholder usage and blank fields requirements on the first page.", "weight": 2.0, "judge_prompt": "Confirm the following shape requirements:\n- The school\u2019s name is represented as the placeholder word \u201cSCHOOL\u201d throughout the document (look for consistent use; minor case variation acceptable).\n- On the first page, the fields for social worker name and address, and the student\u2019s address, are left blank (i.e., not filled with actual names/addresses). It is acceptable if labels are present but values are blank.\n\nScoring:\n- 2.0: SCHOOL consistently used as school placeholder + required first-page fields are blank.\n- 1.0: SCHOOL used but inconsistently OR only some first-page fields are blank.\n- 0.0: SCHOOL not used as placeholder AND first-page fields appear filled.\n\nOnly check presence/absence; not narrative quality.", "expectation": "Consistent SCHOOL placeholder and blank identity/address fields on the first page."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Compliance Verification", "description": "Check factual/structural correctness enabled by the Stage 1 shape: identifiers, dates, section headers, recommendations count, and substantive compliance with instructions.", "is_required": true, "max_points": 22.0, "min_score_to_pass": 12.0, "rules": [{"type": "code", "name": "File Type and Name Check (J.S.)", "description": "Deterministically verify the output is a document/PDF and filename approximates J.S.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Returns up to weight points. Checks:\n      - Output exists and is a document\n      - File name stem approximates 'J.S' (flexible variants)\n    \"\"\"\n    import re\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    if not output.is_document:\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        name = path.name.lower()\n        stem = path.stem.lower()\n        # Accept close variants of J.S\n        acceptable = {\"j.s\", \"js\", \"j_s\", \"j-s\"}\n        score = 0.0\n        # Partial credit for being a PDF-like extension in name; robust to double dot\n        if name.endswith('.pdf'):\n            score += 0.4\n        # Filename stem check (flexible)\n        if stem in acceptable or 'j.s' in stem or (len(stem) <= 6 and re.fullmatch(r\"j[._-]?s\", stem)):\n            score += 0.6\n        return min(score, 1.0)\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Key Headers Present (SDH, Impressions, Recommendations)", "description": "Parse text to ensure presence of core headers indicating correct placement of content.", "weight": 1.5, "code": "def evaluate(workflow, context):\n    \"\"\"Return up to weight points based on presence of key headers in the PDF text.\"\"\"\n    import re\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n    if not text:\n        return 0.0\n    t = text.lower()\n    checks = 0\n    total = 3\n    if 'social developmental history' in t or 'developmental history' in t:\n        checks += 1\n    if 'school social work impressions' in t or 'social work impressions' in t:\n        checks += 1\n    if 'school social work recommendations' in t or 'recommendations' in t:\n        checks += 1\n    return 1.5 * (checks / total)"}, {"type": "code", "name": "Numbered Recommendations Count (10\u201312)", "description": "Count numbered items in the Recommendations section; award full points if 10\u201312.", "weight": 1.5, "code": "def evaluate(workflow, context):\n    \"\"\"Return up to weight points based on count of numbered recommendations (10\u201312 ideal).\"\"\"\n    import re\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n    if not text:\n        return 0.0\n    t = text\n    low = t.lower()\n    # Try to isolate recommendations section if possible\n    rec_split = re.split(r\"school\\s+social\\s+work\\s+recommendations\\b\", low, maxsplit=1)\n    segment = t if len(rec_split) < 2 else t[len(t) - len(rec_split[1]):]\n    # Count lines starting with 1-2 digit numbers followed by '.' or ')'\n    items = re.findall(r\"^\\s*(\\d{1,2})[\\.)]\\s+\", segment, flags=re.M)\n    # Unique ordered numbers to avoid counting inline references\n    try:\n        nums = [int(n) for n in items]\n    except Exception:\n        nums = []\n    count = 0\n    seen = set()\n    for n in nums:\n        if 1 <= n <= 30 and n not in seen:\n            count += 1\n            seen.add(n)\n    # Scoring: full if 10-12, partial if near (8-13)\n    if 10 <= count <= 12:\n        return 1.5\n    elif 8 <= count <= 13:\n        return 1.0\n    elif 5 <= count <= 15:\n        return 0.5\n    else:\n        return 0.0"}, {"type": "llm_judge", "name": "Notes Integration and Professional Rewriting", "description": "Confirm that content appears to be derived from the shorthand notes and rewritten in polished, complete sentences without copy/paste; cross-check with any available input files named like \u201cNotes for JOHN SMITH.\u201d", "weight": 5.0, "judge_prompt": "Evaluate whether the SDH report integrates content from the shorthand notes and rewrites it in a professional tone.\n\nInstructions:\n- If input resources include a file named similar to \u201cNotes for JOHN SMITH,\u201d compare it to the final PDF and judge whether the report appropriately transforms shorthand into complete, polished sentences (no direct copy/paste of raw shorthand).\n- If the notes are not available, assess internal coherence and plausibility of details as if they could feasibly come from such notes (i.e., not generic filler).\n\nScoring:\n- 5.0: Clear, thorough integration; content is fully rewritten, specific, and professional; no evidence of copy/paste shorthand.\n- 3.0: Mostly integrated and rewritten, with minor phrasing carryover or mild generic content.\n- 1.0: Sparse or generic integration; some obvious shorthand remnants.\n- 0.0: Appears copied verbatim from shorthand or lacks any meaningful integration.", "expectation": "A cohesive, fully rewritten narrative reflecting the notes without verbatim copying."}, {"type": "llm_judge", "name": "Impressions Section Completeness and Conclusion on Supports", "description": "Verify that the Impressions section narratively describes settings, behaviors, and any known diagnoses affecting school functioning, and ends with a clear professional opinion about need for supports.", "weight": 5.0, "judge_prompt": "Focus on the section titled \u201cSchool Social Work Impressions\u201d (or close variant).\nCheck that it:\n- Describes the student\u2019s situation and behaviors across relevant school settings (e.g., classroom, recess, transitions).\n- Includes any diagnosed conditions known to affect functioning (if present in notes or elsewhere in the document).\n- Synthesizes observations and information into a coherent narrative (not just bullet points).\n- Concludes with a clear professional opinion regarding whether the student needs additional supports and, if so, what type (e.g., counseling, behavioral interventions, MTSS, IEP considerations).\n\nScoring:\n- 5.0: All elements present with a strong, coherent narrative and explicit concluding opinion with support types.\n- 3.0: Most elements present; conclusion present but limited specificity.\n- 1.0: Minimal narrative; missing key elements or lacks a clear opinion.\n- 0.0: Impressions section missing.", "expectation": "A cohesive narrative ending with a clear, support-focused professional opinion."}, {"type": "llm_judge", "name": "Recommendations Relevance and Structure", "description": "Ensure the recommendations list contains 10\u201312 numbered, actionable items tailored to a K\u20135 HCPS context, drawn from the bank or appropriately customized.", "weight": 4.0, "judge_prompt": "Review the section titled \u201cSchool Social Work Recommendations.\u201d\n- Verify there are 10\u201312 numbered recommendations (LLM count as a cross-check; the code rule separately counts too).\n- Judge whether items are specific, actionable, and appropriate for K\u20135 in Hillsborough County Public Schools (e.g., MTSS supports, small-group counseling, behavior plan, SEL curriculum, parent collaboration, referral pathways, data progress monitoring).\n- Accept items whether drawn from a \u201cRecommendation Bank\u201d (if present) or crafted de novo, provided they are relevant and specific to this student.\n\nScoring:\n- 4.0: 10\u201312 numbered, tailored, concrete, and feasible recommendations.\n- 3.0: 10\u201312 numbered recommendations with minor generalities.\n- 2.0: 8\u20139 or 13 items, or items are too generic for the student/context.\n- 1.0: Fewer than 8 items or mostly vague/inapplicable.\n- 0.0: Recommendations section missing.", "expectation": "A numbered list of 10\u201312 concrete, school-implementable recommendations."}, {"type": "llm_judge", "name": "Identity, Date, and Placeholder Consistency", "description": "Confirm correct use of student name, evaluation date, and SCHOOL placeholder without leaking a real school name; ensure first-page name/address fields remain blank.", "weight": 4.0, "judge_prompt": "Check the final PDF for the following correctness items:\n- Student name: \u201cJOHN SMITH\u201d is used consistently wherever the student is referenced.\n- Date of evaluation appears as 9/27/23 (accept small formatting variants like 09/27/2023).\n- The school is always referenced as the placeholder \u201cSCHOOL\u201d (not an actual school name).\n- The first-page fields for social worker name/address and student address remain blank (not filled in).\n\nScoring:\n- 4.0: All four items correct and consistent.\n- 3.0: One minor inconsistency (e.g., date format variation) but overall correct.\n- 2.0: Two inconsistencies.\n- 1.0: Three inconsistencies.\n- 0.0: Four or more errors, or clearly wrong identity/date/placeholder usage.", "expectation": "Correct, consistent identifiers and date; placeholders and blank fields respected."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Holistic Quality Assessment", "description": "Professional quality, appropriateness for K\u20135 HCPS stakeholders, ethical compliance, and actionability of supports.", "is_required": false, "max_points": 20.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Tone, Clarity, and Readability", "description": "Assess writing quality, grammar, and audience-appropriate tone for multidisciplinary IEP stakeholders.", "weight": 6.0, "judge_prompt": "Evaluate overall writing quality for a professional school audience (CST/IEP team):\n- Tone is professional, objective, and strengths-based.\n- Grammar, spelling, and mechanics are correct; sentences are clear and concise.\n- Avoids jargon or explains it; accessible to educators and caregivers.\n- Logical flow with coherent paragraphs.\n\nScoring:\n- 6.0: Excellent professional quality and readability.\n- 4.0: Generally strong with minor issues.\n- 2.0: Mixed clarity or noticeable errors.\n- 0.0: Unprofessional tone or poor readability.", "expectation": "Polished, professional school-report writing."}, {"type": "llm_judge", "name": "Organization and Formatting", "description": "Judge whether the document is well-structured with clear headings, consistent formatting, and easy navigation.", "weight": 4.0, "judge_prompt": "Assess the document\u2019s overall organization and formatting:\n- Clear headings/subheadings aligned to SDH sections.\n- Consistent style (fonts, spacing, numbering, margins).\n- Effective use of lists and paragraphs; minimal formatting artifacts from templates.\n- Easy to navigate for IEP decision-making.\n\nScoring:\n- 4.0: Excellent organization and consistency.\n- 3.0: Minor inconsistencies but overall well-structured.\n- 2.0: Noticeable formatting issues or confusing layout.\n- 0.0: Disorganized or distracting formatting.", "expectation": "Clean, consistent, and navigable formatting aligned with SDH expectations."}, {"type": "llm_judge", "name": "Ethical, Family-Centered, and Culturally Sensitive Practice", "description": "Evaluate whether the report is respectful, non-stigmatizing, and trauma-informed, appropriate for a school context.", "weight": 5.0, "judge_prompt": "Evaluate for ethical and culturally sensitive practice:\n- Person-first, non-stigmatizing language; respectful and trauma-informed.\n- Maintains confidentiality; avoids unnecessary sensitive details; consistent with FERPA considerations in school reports.\n- Frames concerns with strengths and collaboration with caregivers and teachers.\n- Avoids medical/clinical determinations beyond scope unless documented diagnoses are cited appropriately.\n\nScoring:\n- 5.0: Strong ethical, family-centered, culturally responsive approach.\n- 3.0: Generally appropriate with minor lapses.\n- 1.0: Multiple concerns or stigmatizing phrasing.\n- 0.0: Significant ethical problems or confidentiality violations.", "expectation": "Respectful, family-centered, and school-appropriate language and content."}, {"type": "llm_judge", "name": "Actionability and Alignment of Supports", "description": "Judge whether the supports and recommendations are actionable, measurable, and aligned to K\u20135 HCPS processes (e.g., MTSS, counseling, behavior supports).", "weight": 5.0, "judge_prompt": "Assess the practical value of proposed supports:\n- Recommendations are actionable, measurable/monitorable, and time-bound where appropriate.\n- Aligned to K\u20135 public school processes (e.g., MTSS tiers, behavior intervention plans, SEL curricula, small-group/individual counseling, data progress monitoring, caregiver collaboration, referrals).\n- Tailored to the student\u2019s described needs and feasible within HCPS resources.\n\nScoring:\n- 5.0: Highly actionable, aligned, and tailored.\n- 3.0: Generally actionable with some generic phrasing.\n- 1.0: Vague or misaligned with school processes.\n- 0.0: Not actionable or inappropriate for the setting.", "expectation": "Concrete, feasible, and aligned supports that aid IEP decision-making."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "27e8912c-8bd5-44ba-ad87-64066ea05264", "rubric": {"category_name": "Government \u2014 Administrative Services Managers \u2014 Ergonomics Materials", "rationale": "Pattern B (Document). Two deliverables are required: a PDF checklist (<=5 pages) and a DOCX action tracker. Stage 1 uses LLM-only judges to strictly enforce structure and file formats so later checks are trivial. Stage 2 mixes light code rules (deterministic presence checks) with LLM rules (content consistency, image/source integrity, actionability). Stage 3 assesses professional quality and suitability for HR/facilities in government context.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (LLM-only)", "description": "Gate that verifies exact deliverable formats and structural completeness so the work is verifiable. If this fails, the entire category is scored as 0.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.0, "rules": [{"type": "llm_judge", "name": "Deliverables Presence and Format", "description": "Submission must include one PDF checklist (<=5 pages) and one DOCX action tracker document.", "weight": 3.0, "judge_prompt": "You are verifying the presence and basic format of the deliverables. Inspect all provided files.\n\nPass criteria (structure only, do not judge quality):\n- Exactly these primary deliverables are present:\n  1) A PDF checklist (Workstation Ergonomics Checklist) that is no more than 5 pages. It should appear to be a form/checklist (checkboxes or similar) and be obviously intended for staff to complete.\n  2) A DOCX file that is an organizational action tracker (table-based tracker) for issues arising from the checklist.\n- Additional supporting files are acceptable but NOT required. The necessary content must be within the PDF and DOCX themselves.\n\nScoring:\n- 3.0: Both a PDF (<=5 pages) and a DOCX are present and correctly formatted for their intended purpose (checklist form; table-based tracker).\n- 2.0: Both files present but minor format concern (e.g., PDF looks like a narrative rather than a checklist, or DOCX appears not to be table-based).\n- 1.0: Only one of the two required files is present or obvious file-type mismatch (e.g., checklist is a DOCX, tracker is a PDF).\n- 0.0: Required deliverables missing or the PDF exceeds 5 pages.", "expectation": "One PDF checklist (<=5 pages) and one DOCX action tracker are present."}, {"type": "llm_judge", "name": "Checklist Structural Requirements (PDF)", "description": "Verify the PDF checklist includes required sections and fields.", "weight": 2.0, "judge_prompt": "Check the PDF checklist (not the DOCX) for structure only:\nRequired in the PDF:\n- Title indicating it is a workstation ergonomics checklist.\n- A stated goal (e.g., purpose statement or objective).\n- Fields: Name, Position, Email, Date (visible fields/labels on the form).\n- Scope-limited sections focused ONLY on: Office Chair; Keyboard & Mouse; Work Surface. Each section should present checklist-style items (e.g., checkboxes or clearly listable items) rather than prose.\n- Ergonomic setup images for the three areas (chair; keyboard & mouse; work surface). Images may be placed within an appendix but must be included within the PDF and clearly labeled/captioned.\n\nScoring (structure presence only):\n- 2.0: All required elements present (title, goal, fields, the three sections with checklist items, and images for each section within the PDF).\n- 1.0: Missing one structural element (e.g., goal OR one of the three sections OR images for one section).\n- 0.5: Missing two structural elements.\n- 0.0: Missing three or more elements or not a proper checklist form.", "expectation": "A compact, scope-limited checklist with goal, fields, three sections, and images (in-PDF)."}, {"type": "llm_judge", "name": "Action Tracker Structural Requirements (DOCX)", "description": "Verify the DOCX action tracker includes the table and required fields/sections.", "weight": 1.0, "judge_prompt": "Check the DOCX action tracker for these structural elements:\n- Employee/workstation details fields at the top or in a header area (Employee Name, Department, Email, Date, and Who Resolved the Issue \u2014 the latter may be in a resolution section).\n- A main action-tracking table with columns that cover at least: Issue/Item, Recommended Action, Responsible/Owner, Due Date, Status/Comments (exact wording can vary but intent must be clear).\n- A clearly labeled Process section that lists these four steps:\n  1) Determine if alternate equipment is available on site.\n  2) If unavailable, review options with the Vendor of Record.\n  3) Order item (requires People Leader approval and cost centre).\n  4) Confirm resolution with employee.\n\nScoring (structure presence only):\n- 1.0: Employee/workstation detail fields + action table + process section with all four steps.\n- 0.7: Minor omission (e.g., one detail field missing OR a table column missing OR one process step missing).\n- 0.4: Two omissions among the above.\n- 0.0: Major omissions (action table missing or process section absent) or wrong file type.", "expectation": "A DOCX tracker with employee details, action table, and the full 4-step process."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness Verification (Mixed)", "description": "Now that structure is verified, check correctness, traceability, and alignment with ergonomic best practices. Uses a mix of light code rules and LLM rules. Code rules focus on deterministic presence/consistency; LLM rules assess nuanced correctness.", "is_required": false, "max_points": 9.5, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Credible Source Citation Present", "description": "Confirm at least one credible, public-domain ergonomic source is cited (e.g., NIH, OSHA, CDC, Canada.ca, HSE, WHO, CCOHS).", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 0.5\n    outputs = context.get_all_outputs() or []\n    if not outputs:\n        return 0.0, \"No outputs found.\"\n\n    texts = []\n    for r in outputs:\n        try:\n            # Try PDF\n            p = context.files.get_path(r.id)\n            suf = p.suffix.lower()\n            if suf == '.pdf':\n                txt = context.files.read_pdf_text(r.id)\n                if txt:\n                    texts.append(txt)\n            elif suf == '.docx':\n                txt = context.files.read_docx_text(r.id)\n                if txt:\n                    texts.append(txt)\n            elif r.is_text_format:\n                txt = context.files.read_text(r.id)\n                if txt:\n                    texts.append(txt)\n        except Exception:\n            continue\n\n    full = \"\\n\".join(texts).lower()\n    if not full:\n        return 0.0, \"Could not read text from outputs.\"\n\n    # Common credible domains\n    patterns = [\n        r\"nih\\.gov\", r\"ors\\.od\\.nih\\.gov\", r\"osha\\.gov\", r\"cdc\\.gov\", r\"canada\\.ca\",\n        r\"hse\\.gov\\.uk\", r\"who\\.int\", r\"ccohs\\.ca\", r\"eu-osha\\.europa\\.eu\", r\"gsa\\.gov\",\n        r\"nist\\.gov\", r\"worksafe\\.govt\\.nz\", r\"safework\\.gov\\.au\", r\"nasa\\.gov\"\n    ]\n    found = any(re.search(pat, full) for pat in patterns)\n    return (weight if found else 0.0, \"Credible source citation {}found.\".format(\"\" if found else \"not \"))"}, {"type": "code", "name": "Checklist Includes Required Fields (PDF)", "description": "Detect presence of labeled fields in the PDF: Name, Position, Email, Date, and a stated Goal.", "weight": 0.5, "code": "def evaluate(workflow, context):\n    weight = 0.5\n    # Find a PDF output\n    pdf = None\n    for r in context.get_all_outputs() or []:\n        try:\n            if context.files.get_path(r.id).suffix.lower() == '.pdf':\n                pdf = r\n                break\n        except Exception:\n            continue\n    if not pdf:\n        return 0.0, \"No PDF found.\"\n\n    try:\n        text = (context.files.read_pdf_text(pdf.id) or \"\").lower()\n    except Exception:\n        return 0.0, \"Unable to read PDF text.\"\n\n    labels = [\"name\", \"position\", \"email\", \"date\", \"goal\"]\n    hits = sum(1 for l in labels if l in text)\n    score = weight * (hits / len(labels))\n    return score, f\"Found {hits}/5 required labels in PDF (Name, Position, Email, Date, Goal).\""}, {"type": "code", "name": "Process Steps Present (DOCX)", "description": "Verify the DOCX action tracker contains the four specific process steps with flexible matching.", "weight": 0.5, "code": "import re\n\ndef _has_all(text, terms):\n    return all(t in text for t in terms)\n\ndef evaluate(workflow, context):\n    weight = 0.5\n    # Find DOCX\n    docx = None\n    for r in context.get_all_outputs() or []:\n        try:\n            if context.files.get_path(r.id).suffix.lower() == '.docx':\n                docx = r\n                break\n        except Exception:\n            continue\n    if not docx:\n        return 0.0, \"No DOCX found.\"\n\n    try:\n        text = (context.files.read_docx_text(docx.id) or \"\").lower()\n    except Exception:\n        return 0.0, \"Unable to read DOCX text.\"\n\n    # Step 1: alternate equipment available on site\n    s1 = (\"alternate equipment\" in text or \"alternative equipment\" in text) and (\"available\" in text or \"availability\" in text) and (\"on site\" in text or \"onsite\" in text)\n    # Step 2: vendor of record\n    s2 = (\"vendor of record\" in text)\n    # Step 3: order item + people leader approval + cost centre/center\n    s3 = (\"order\" in text) and (\"people leader\" in text or \"manager approval\" in text or \"supervisor approval\" in text) and (\"cost centre\" in text or \"cost center\" in text)\n    # Step 4: confirm resolution with employee\n    s4 = (\"confirm\" in text) and (\"resolution\" in text) and (\"employee\" in text)\n\n    hits = sum([s1, s2, s3, s4])\n    score = weight * (hits / 4)\n    return score, f\"Found {hits}/4 process steps in DOCX.\""}, {"type": "llm_judge", "name": "Image Source Integrity and Fit (PDF)", "description": "Check that the PDF includes images for chair, keyboard & mouse, and work surface that depict recommended setups and include credible attributions.", "weight": 2.0, "judge_prompt": "Examine the PDF checklist and verify image correctness and attribution:\n- Images for each of the three areas (chair; keyboard & mouse; work surface) are present (either inline or in an appendix) and clearly labeled.\n- Images depict recommended/best-practice setups (e.g., neutral wrist, elbows ~90\u00b0, chair lumbar support positioning) appropriate to the section where they appear.\n- Each image includes a caption or adjacent text that cites a credible, public-domain source (e.g., NIH, OSHA, CDC, Canada.ca, HSE, WHO, CCOHS) or indicates public-domain/appropriate licensing. No watermarks or obvious copyright issues.\n\nScoring:\n- 2.0: All three images present, clearly relevant, and credibly attributed (or public domain) with clean presentation.\n- 1.0: One gap (missing image OR attribution not shown for one image OR image not clearly relevant for one section).\n- 0.5: Two gaps as above.\n- 0.0: No images or pervasive attribution/licensing problems.", "expectation": "Three clearly relevant images with captions/attribution to credible sources."}, {"type": "llm_judge", "name": "Checklist Actionability and Specificity", "description": "Assess whether the PDF checklist items are concrete and measurable, enabling quick self-assessment and discussion.", "weight": 2.0, "judge_prompt": "Evaluate the PDF checklist content for actionability:\n- Items are phrased as observable/measurable checks with checkboxes or pass/fail (e.g., seat height allows feet flat on floor or footrest; elbows ~90\u2013100\u00b0; wrists neutral; keyboard and mouse at same height; forearms parallel to floor; work surface height accommodates elbow height).\n- Each of the three sections (chair; keyboard & mouse; work surface) includes multiple concrete items and space for notes.\n- Avoids out-of-scope content (e.g., monitor/lighting) except brief references in an appendix if clearly demarcated.\n\nScoring:\n- 2.0: Items are specific/measurable across all three sections with checkboxes and space for notes.\n- 1.0: Items are partly specific but some are vague or missing in one section.\n- 0.5: Mostly vague or non-measurable; lacks checkboxes.\n- 0.0: Not a usable checklist for ergonomic assessment.", "expectation": "Specific, measurable checklist items for the three focus areas with checkboxes and notes."}, {"type": "llm_judge", "name": "Alignment to Credible Source Guidance", "description": "Ensure the checklist\u2019s recommendations align with the cited credible source(s) with no contradictions.", "weight": 2.0, "judge_prompt": "Compare the PDF checklist recommendations to the cited credible source (e.g., NIH self-assessment). Evaluate for alignment and absence of obvious contradictions (e.g., recommending bent wrists, unsupported lower back, or mismatched keyboard/mouse heights). Minor wording differences are fine; the essence should align with mainstream ergonomic guidance.\n\nScoring:\n- 2.0: Clear alignment to a credible source; no contradictions; appropriate citations are visible.\n- 1.0: Generally aligned with minor inconsistencies or unclear citation placement.\n- 0.0: Significant contradictions with the cited source or no credible source referenced.", "expectation": "Recommendations track with NIH/OSHA/CDC/HSE/CCOHS or similar guidance without contradictions."}, {"type": "llm_judge", "name": "Action Tracker Traceability and Completeness (DOCX)", "description": "Check that the DOCX table can trace issues from the checklist through to resolution with owners and dates.", "weight": 2.0, "judge_prompt": "Evaluate the DOCX action tracker for traceability and completeness:\n- The table supports linking entries back to the checklist (e.g., includes an Issue/Item reference or section/ID).\n- Includes columns for Recommended Action, Responsible/Owner, Due Date, and Status/Comments.\n- Includes resolution fields (e.g., Resolved By/Who Resolved and Resolution Date or similar) and space to capture final confirmation with the employee.\n\nScoring:\n- 2.0: Clear traceability to checklist issues and all key columns present including resolution tracking.\n- 1.0: Mostly complete but missing explicit traceability field or a resolution detail.\n- 0.0: Lacks core tracking columns or provides no way to trace issues to checklist findings.", "expectation": "A practical tracker with linkage to checklist items, owner/due, status, and resolution fields."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism (LLM)", "description": "Holistic assessment of presentation, accessibility, government-appropriate tone, and practical usefulness for HR and facilities teams.", "is_required": false, "max_points": 4.5, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Readability", "description": "Assess layout, clarity, and ease-of-use for HR/facilities discussions.", "weight": 1.2, "judge_prompt": "Assess both documents for professional presentation and readability:\n- Clear headings, logical hierarchy, consistent typography, and adequate white space.\n- The PDF checklist is easy to complete (checkboxes, short items, space for notes). The DOCX table is easy to populate and print.\n- Visual balance and formatting support quick review in meetings.\n\nScore:\n- 1.2: Highly professional and easy to use.\n- 0.8: Generally good with minor issues.\n- 0.4: Readable but cluttered or inconsistent.\n- 0.0: Poorly formatted or confusing.", "expectation": "Clean, consistent formatting optimized for quick completion and review."}, {"type": "llm_judge", "name": "Accessibility and Inclusivity", "description": "Evaluate accessibility features and inclusive language/visuals appropriate for a government workplace.", "weight": 1.1, "judge_prompt": "Review accessibility and inclusivity:\n- Legible fonts, sufficient contrast, clear image captions. Avoids jargon or explains terms plainly.\n- Images add clarity; if icons/symbols are used, labels clarify meaning. Consideration for screen-reader friendliness (e.g., descriptive captions around images).\n- Information requests (e.g., personal details) are minimal and appropriate.\n\nScore:\n- 1.1: Strong accessibility and inclusive language practices.\n- 0.7: Adequate with minor gaps.\n- 0.3: Several accessibility concerns.\n- 0.0: Not accessible or exclusionary tone.", "expectation": "Plain language, good contrast, clear captions, minimal necessary personal info."}, {"type": "llm_judge", "name": "Scope and Length Appropriateness", "description": "Ensure the PDF stays within 5 pages and content remains focused on chair, keyboard & mouse, and work surface.", "weight": 1.1, "judge_prompt": "Check that the PDF is within 5 pages and remains tightly focused on the specified scope (chair; keyboard & mouse; work surface). Brief appendix references are acceptable. Penalize inclusion of large out-of-scope sections (e.g., monitor setup, lighting) that distract from the objective.\n\nScore:\n- 1.1: Within length and tightly scoped.\n- 0.7: Minor scope creep or near length limit.\n- 0.3: Noticeable scope creep or filler content.\n- 0.0: Exceeds 5 pages or largely off-scope.", "expectation": "Concise, scope-aligned checklist within page limit."}, {"type": "llm_judge", "name": "Government-Appropriate Tone and Practical Utility", "description": "Evaluate tone, citation clarity, and practical usefulness for HR/facilities workflows.", "weight": 1.1, "judge_prompt": "Assess tone and practical utility:\n- Neutral, professional, and policy-aligned tone suitable for government use.\n- Clear citations and, if present, brief disclaimer (e.g., not medical advice) is appropriate but optional.\n- The action tracker supports prioritization and follow-through; the process section is easy to act on.\n\nScore:\n- 1.1: Strongly appropriate tone and very useful.\n- 0.7: Generally appropriate/useful with minor gaps.\n- 0.3: Questionable tone or limited utility.\n- 0.0: Inappropriate tone or impractical for HR/facilities use.", "expectation": "Neutral tone, clear references, actionable tracker and process guidance."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "17111c03-aac7-45c2-857d-c06d8223d6ad", "rubric": {"category_name": "Gov Admin: Blight Cleanup Memo + Schedule Conversion", "rationale": "This rubric enforces a self-documenting, two-deliverable output: a professional PDF/DOCX memo and an Excel schedule. Stage 1 (LLM-only) is a strict shape gate mandating both files and specific structures that enable verification. Stage 2 mixes light code checks (file types, columns, dates, placeholders) with heavier LLM judgment for correctness of guidance, disruption protocols, and service expectations. Stage 3 evaluates overall professional quality and usability for administrative staff and volunteers.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2013 Shape Enforcement Gate (Deliverables and Structure)", "description": "LLM-only verification that BOTH deliverables exist and have the mandated structures, enabling trivial verification later. If this gate is not passed, the entire category is zeroed.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.5, "rules": [{"type": "llm_judge", "name": "Deliverables Present: Memo (PDF/DOCX) + Excel Schedule", "description": "Verify that there are two separate outputs: 1) a memo as a PDF or DOCX document, and 2) an Excel schedule (.xlsx). Names can vary. Both must appear among the task outputs.", "weight": 2.5, "judge_prompt": "You are checking the candidate outputs for required deliverables.\n\nConfirm BOTH are present:\n1) A professional memo document in PDF or DOCX format.\n2) An Excel workbook (.xlsx) that contains the schedule.\n\nScoring:\n- 2.5: Both deliverables present (memo is PDF/DOCX AND schedule is Excel .xlsx)\n- 1.0: Only one of the two deliverables present\n- 0.0: Neither present or formats are incorrect (e.g., memo is plain text or schedule is not Excel)\n\nBe strict about file formats but flexible on filenames. Only verify presence and file types, not content quality.", "expectation": "Two separate files: a PDF/DOCX memo and an .xlsx schedule."}, {"type": "llm_judge", "name": "Memo Structure Present", "description": "Check the memo has required section headers and professional structure to enable later verification.", "weight": 2.0, "judge_prompt": "Evaluate the memo document structure (PDF or DOCX). Confirm it contains:\n- A header with To, From, Date, and Subject (exact words may vary but must be clearly present and populated)\n- An opening purpose/overview paragraph referencing the tentative cleanup schedule and informing admin staff\n- Clearly labeled sections or paragraphs covering: Background/Context, Schedule Overview (set/rotating schedule intent), Volunteer/Call Handling Guidance for staff, Disruption Protocols (emergencies/severe weather with temporary shifts and plan to return/reschedule), and Customer Service/Estimates for callers\n\nScoring:\n- 2.0: All listed elements present and clearly identifiable\n- 1.0: Missing 1 required section OR headers present but sections are not clearly delineated\n- 0.0: Missing multiple required sections or looks like an unstructured note\n\nCheck presence/structure only; do not assess the quality of writing.", "expectation": "Memo includes formal header and distinct sections enabling verification in Stage 2."}, {"type": "llm_judge", "name": "Excel Schedule Structure Present", "description": "Check the Excel file contains a primary schedule sheet with expected tabular structure that admin staff can reference.", "weight": 1.5, "judge_prompt": "Open the Excel workbook. Confirm there is a primary sheet for the schedule (flexible name such as 'Cleanup Schedule', 'Schedule', 'Calendar', etc.) with a clear table. The table should include columns (names can vary slightly) covering:\n- A date or week start column (e.g., Date, Week Start, Week Of)\n- Area/Zone/Region/Location\n- Crew/Team\n- Activities/Tasks/Focus\n- Status and/or Notes/Contingency\nAlso confirm it has at least several scheduled entries (e.g., 4 or more rows) visible.\n\nScoring:\n- 1.5: Sheet present with a clear table containing all listed column types and 4+ data rows\n- 1.0: Table present but one listed column type is missing OR only 2\u20133 rows\n- 0.0: No proper schedule sheet/table present\n\nCheck structure only; do not validate calculations or dates.", "expectation": "Excel includes a primary schedule sheet with the listed column types and multiple rows."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness Verification (Mixed Code + LLM)", "description": "Verify correctness and operational adequacy now that structure is in place. Code rules do precise checks; LLM rules judge policy alignment and guidance clarity.", "is_required": false, "max_points": 9.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Excel Structural and Date Validity", "description": "Programmatically verify the schedule workbook has expected columns (fuzzy), sufficient rows, and valid chronological dates.", "weight": 1.2, "code": "import re\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\ndef evaluate(workflow, context):\n    weight = 1.2\n    # Find an Excel output\n    try:\n        outputs = getattr(context, 'get_all_outputs', None)\n        resources = outputs() if outputs else [context.get_primary_output()] if context.get_primary_output() else []\n        xls_list = [r for r in resources if r and getattr(r, 'is_spreadsheet', False)]\n        if not xls_list:\n            return 0.0, 'No spreadsheet output found.'\n        xres = xls_list[0]\n        path = context.files.get_path(xres.id)\n        try:\n            xfile = pd.ExcelFile(path)\n            sheet_names = [s for s in xfile.sheet_names]\n        except Exception as e:\n            return 0.0, f'Failed to open Excel: {e}'\n        # Pick schedule-like sheet\n        cand = None\n        for s in sheet_names:\n            sl = s.lower()\n            if any(k in sl for k in ['cleanup', 'schedule', 'calendar', 'crew']):\n                cand = s\n                break\n        if cand is None:\n            cand = sheet_names[0]\n        try:\n            df = pd.read_excel(path, sheet_name=cand)\n        except Exception as e:\n            return 0.0, f'Failed to read schedule sheet: {e}'\n        if df.empty or df.shape[0] < 1:\n            return 0.2, 'Schedule sheet has no data.'\n        cols = [str(c).strip() for c in df.columns]\n        cols_l = [c.lower() for c in cols]\n        def has_any(keys):\n            return any(any(k in c for k in keys) for c in cols_l)\n        # Required column groups (fuzzy)\n        date_ok = has_any(['date', 'week', 'start'])\n        area_ok = has_any(['area', 'zone', 'region', 'district', 'location'])\n        crew_ok = has_any(['crew', 'team'])\n        task_ok = has_any(['activity', 'task', 'focus'])\n        status_ok = has_any(['status', 'note', 'contingency'])\n        # Try to parse a date column\n        parsed_dates_ok = 0\n        if date_ok:\n            # find first matching date-like column\n            dcol = None\n            for c in cols:\n                cl = c.lower()\n                if any(k in cl for k in ['date', 'week', 'start']):\n                    dcol = c\n                    break\n            if dcol is not None:\n                series = df[dcol]\n                # Coerce to datetime\n                try:\n                    dt = pd.to_datetime(series, errors='coerce', infer_datetime_format=True)\n                    # Valid if at least 3 non-null and monotonic non-decreasing (ignoring nulls)\n                    valid_count = dt.notna().sum()\n                    if valid_count >= 3:\n                        ordered = dt.dropna().sort_values().index.equals(dt.dropna().index) or dt.dropna().is_monotonic_increasing\n                        parsed_dates_ok = 1 if ordered else 0.5\n                except Exception:\n                    parsed_dates_ok = 0\n        # Row count check (at least 4 rows with any non-null values)\n        non_empty_rows = int((df.notna().sum(axis=1) > 0).sum())\n        rows_ok = 1 if non_empty_rows >= 4 else (0.5 if non_empty_rows >= 2 else 0)\n        # Aggregate scoring: each component contributes equally\n        components = [date_ok, area_ok, crew_ok, task_ok, status_ok]\n        comp_score = sum(1 if c else 0 for c in components) / 5.0\n        # Add date parsing and rows\n        comp_score = (comp_score + parsed_dates_ok + rows_ok) / 3.0\n        comp_score = max(0.0, min(1.0, comp_score))\n        return weight * comp_score, f'Schedule sheet: {cand}; columns found={cols}; rows={non_empty_rows}.'\n    except Exception as e:\n        return 0.0, f'Exception in code rule: {e}'"}, {"type": "code", "name": "Memo Date/Role/Placeholder Check", "description": "Verify memo uses today\u2019s date, includes the sender role, references the attached Excel schedule, and removes common placeholders.", "weight": 0.8, "code": "import re\nfrom datetime import datetime\n\ndef evaluate(workflow, context):\n    weight = 0.8\n    try:\n        outputs = getattr(context, 'get_all_outputs', None)\n        resources = outputs() if outputs else [context.get_primary_output()] if context.get_primary_output() else []\n        docs = [r for r in resources if r and getattr(r, 'is_document', False)]\n        if not docs:\n            return 0.0, 'No memo document found.'\n        dres = docs[0]\n        text = ''\n        # Try PDF then DOCX\n        try:\n            text = context.files.read_pdf_text(dres.id)\n        except Exception:\n            try:\n                text = context.files.read_docx_text(dres.id)\n            except Exception:\n                text = ''\n        if not text:\n            return 0.2, 'Could not extract memo text.'\n        low = text.lower()\n        # Check today's date in common formats\n        today = datetime.today()\n        month_name = today.strftime('%B')\n        date_long = today.strftime(f'%B %-d, %Y') if '%-d' in '%-d' else today.strftime('%B %d, %Y')\n        date_alt = today.strftime('%m/%d/%Y')\n        date_alt2 = today.strftime('%Y-%m-%d')\n        has_date = (month_name.lower() in low and str(today.year) in low) or (date_alt in text) or (date_alt2 in text) or (date_long in text)\n        # Role present\n        has_role = 'administrative services manager' in low\n        # Reference to schedule attachment\n        mentions_schedule = ('schedule' in low and ('attached' in low or 'enclosed' in low or 'included' in low)) or ('excel' in low and 'schedule' in low)\n        # Placeholder checks (flag if any present)\n        placeholder_patterns = [r'your name', r'\\[insert', r'\\[replace', r'<your', r'lorem ipsum', r'xx/xx/xxxx']\n        has_placeholders = any(re.search(p, low) for p in placeholder_patterns)\n        score = 0\n        score += 1 if has_date else 0\n        score += 1 if has_role else 0\n        score += 1 if mentions_schedule else 0\n        score += 1 if not has_placeholders else 0\n        return weight * (score/4.0), f\"date={has_date}, role={has_role}, schedule_ref={mentions_schedule}, placeholders={not has_placeholders}\"\n    except Exception as e:\n        return 0.0, f'Exception in code rule: {e}'"}, {"type": "llm_judge", "name": "Disruption and Rescheduling Protocol Correctness", "description": "Assess whether the memo provides clear, correct procedures for emergencies/severe weather: temporary shifts, communication to staff/volunteers, and explicit plan to return or reschedule missed areas.", "weight": 2.5, "judge_prompt": "Read the memo. Does it clearly describe what to do during emergencies or severe weather? Specifically check for:\n- Acknowledgement crews may temporarily shift to another area (e.g., to respond to urgent issues)\n- Guidance on who informs volunteers and how (admin staff instructions)\n- A specific plan to return to the original location OR reschedule missed areas (e.g., rolling to next available slot)\n\nScoring:\n- 2.5: Addresses all three points clearly and operationally\n- 1.5: Covers two of the points adequately\n- 0.5: Mentions disruptions but lacks clear rescheduling/return plan\n- 0.0: No meaningful disruption/rescheduling guidance\n\nFocus on operational clarity and correctness.", "expectation": "A concrete, actionable disruption and rescheduling protocol."}, {"type": "llm_judge", "name": "Customer Service and Call Handling Guidance", "description": "Assess whether the memo equips admin staff to handle calls from residents/volunteers with estimates and clear talking points.", "weight": 2.5, "judge_prompt": "Evaluate the memo for customer service guidance. Check for:\n- Clear instruction that admin staff can provide an estimate/ETA for abatement using the schedule\n- Explicit guidance or steps/talking points for handling calls from residents and volunteers (e.g., confirm area and week, advise volunteer-friendly activities, note reporting/logging process)\n- Connection to the goal of improved satisfaction after historical issues (understaffing, unfinished jobs)\n\nScoring:\n- 2.5: All elements present with clear steps/talking points\n- 1.5: Two elements present or steps are vague\n- 0.5: Mentions estimates but lacks steps and context\n- 0.0: No meaningful call handling guidance\n\nAssess operational readiness, not writing style.", "expectation": "Staff can confidently provide ETAs and follow clear steps with callers."}, {"type": "llm_judge", "name": "Schedule Usability for Volunteers and Staff", "description": "Assess whether the memo and schedule together make it obvious which areas are addressed in which weeks and how volunteers can align.", "weight": 2.0, "judge_prompt": "Review both the memo and the Excel schedule. Do they, together, make it easy for staff to inform volunteers where/when crews will be working?\nLook for:\n- Clear mapping of areas to weeks/dates in the schedule\n- Memo instructions that reference how to read the schedule (e.g., week start date, area/zone, crew)\n- Note on volunteer-friendly tasks or constraints if present\n\nScoring:\n- 2.0: Clear mapping and memo references make alignment straightforward\n- 1.0: Mostly clear but minor ambiguities\n- 0.0: Unclear mapping or memo fails to explain how to use the schedule\n\nDo not judge aesthetics; judge practical usability.", "expectation": "A staffer can quickly answer: when and where are crews, and can volunteers join?"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Professional Quality and Usability", "description": "Holistic quality check of communication, clarity, and user-friendliness for administrative staff and volunteers.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Tone and Clarity", "description": "Evaluate whether the memo reads professionally and clearly for a government admin audience.", "weight": 1.5, "judge_prompt": "Assess the memo\u2019s overall professionalism and clarity for administrative government staff. Consider tone (neutral, service-oriented), clarity of purpose, and absence of jargon.\n\nScoring:\n- 1.5: Highly professional and clear\n- 0.8: Generally professional with minor issues\n- 0.0: Unprofessional or confusing tone\n\nDo not penalize minor formatting issues here (handled elsewhere).", "expectation": "Professional, clear, service-oriented memo."}, {"type": "llm_judge", "name": "Organization and Readability", "description": "Assess sectioning, headings, and scannability of the memo.", "weight": 1.0, "judge_prompt": "Evaluate the memo\u2019s organization and scannability: clear headers, logical flow (Background \u2192 Schedule \u2192 Guidance \u2192 Disruptions \u2192 Customer Service \u2192 Contacts), and concise paragraphs/bullets.\n\nScoring:\n- 1.0: Well-organized with clear headers and logical flow\n- 0.5: Mostly organized with minor issues\n- 0.0: Poorly organized or hard to scan\n\nFocus on readability for busy staff.", "expectation": "Clearly structured, easy to scan."}, {"type": "llm_judge", "name": "Accessibility and Contact Information", "description": "Check for plain language, actionable next steps, and contact details for questions/escalations.", "weight": 1.0, "judge_prompt": "Does the memo use plain, accessible language and include actionable next steps and contact information (e.g., phone/email or role) for questions or schedule updates?\n\nScoring:\n- 1.0: Plain language + clear next steps + contact info\n- 0.5: Two of the three present\n- 0.0: One or none present\n\nJudge general accessibility, not typography.", "expectation": "Plain language, clear actions, and contact details."}, {"type": "llm_judge", "name": "Excel Presentation and Shareability", "description": "Judge whether the schedule sheet is readable and practical for sharing with volunteers.", "weight": 1.5, "judge_prompt": "Examine the Excel schedule\u2019s presentation. Is the table readable (clear column labels, consistent date formatting), and practical for sharing (no obvious clutter; simple, scannable layout)?\n\nScoring:\n- 1.5: Clear labels, consistent dates, and easy to scan\n- 0.8: Generally readable with minor inconsistencies\n- 0.0: Confusing labels or inconsistent formatting impairs use\n\nJudge presentation; not content correctness.", "expectation": "Clean, readable, and shareable schedule view."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "aad21e4c-1d43-45fc-899a-97754a1b1b63", "rubric": {"category_name": "Share Subscription Agreement (Early-Stage Private Placement)", "rationale": "This rubric enforces a self-documenting, verifiable Word/PDF agreement. Stage 1 (LLM-only) strictly mandates the document\u2019s structure so later checks are trivial. Stage 2 mixes lightweight code checks (format/numeric/placeholder sanity) with heavier LLM verification of legal terms and cross-references, giving LLM rules ~5x the weight of code rules. Stage 3 provides a holistic legal-quality assessment focused on drafting quality, client alignment, and professional usability.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2013 Shape Enforcement Gate (Document Structure and Schedules)", "description": "Gate: Ensure the output is a professionally structured Word/PDF share subscription agreement with required sections and the capitalization schedule that enable downstream verification.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.0, "rules": [{"type": "llm_judge", "name": "Document Format and Structural Completeness", "description": "Verify the candidate is a Word/DOCX or PDF agreement with all required sections, placeholders, and a separate Schedule A with before/after cap table.", "weight": 6.0, "judge_prompt": "You are checking ONLY the structure/format of the candidate output. Do not judge correctness or quality yet.\n\nConfirm the document is a Word/DOCX or PDF agreement (not plain text or Excel) and is professionally formatted (headings, numbered sections). It must include bracketed placeholders for unknowns (e.g., [Date], [Investor Address], [Company Address]).\n\nRequired structural elements (be flexible with header names but the content must be clearly present):\n1) Title: \u201cShare Subscription Agreement\u201d (or materially equivalent) on the first page.\n2) Parties block identifying NoxaPulse Technologies, Inc. (a Delaware corporation) and investor Alan Gane (individual).\n3) Recitals/Background.\n4) Definitions section (or clearly defined terms embedded but identifiable) including \u201cCapital Stock\u201d or \u201cFully Diluted Capitalization.\u201d\n5) Subscription and Purchase Terms (share count and price, payment mechanics, closing/deliverables).\n6) Company Representations and Warranties (early-stage customary reps).\n7) Investor Representations and Warranties (accredited investor, investment intent, etc.).\n8) Covenants/Undertakings.\n9) Information and Inspection Rights (minority investor info rights; not a board/observer seat grant).\n10) Pre-emptive Rights (pro rata participation in future issuances).\n11) Minimum Ownership / Anti-Dilution mechanism to maintain at least 10% fully diluted, with a top-up provision and carve-outs for exempt issuances.\n12) Minority Consent/Protective Provisions (Company cannot take specified extraordinary actions without Alan\u2019s prior consent).\n13) Transfer Restrictions and Securities Legends/Compliance (Securities Act exemption).\n14) Miscellaneous/Boilerplate (Notices, Confidentiality, Governing Law [Delaware], Expenses, Assignment, Entire Agreement, Amendments/Waivers, Severability, Counterparts/e-signature).\n15) Signature blocks for Company and Investor with placeholders.\n16) Schedule A \u2013 Capitalization Before and After the Financing (a clearly labeled schedule or appendix) that contains a tabular before/after cap table.\n\nScoring:\n- 6.0: Valid DOCX/PDF and all 16 structural elements present (reasonable variations in naming allowed). Bracketed placeholders appear in the body and signature/notice sections.\n- 5.0: Valid DOCX/PDF, missing only 1 of the 16 elements OR schedule present but not tabularly clear.\n- 4.0: Valid DOCX/PDF with 2\u20133 elements missing.\n- 2.0: Valid DOCX/PDF but 4\u20136 elements missing.\n- 0.0: Not DOCX/PDF OR more than 6 required elements missing.\n\nOnly evaluate presence/structure, not the correctness of the legal content.", "expectation": "A DOCX or PDF agreement with clearly labeled sections and a separate Schedule A showing before/after capitalization in a table, plus bracketed placeholders."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification (Correctness of Key Terms)", "description": "Now that structure is fixed, verify correctness of core deal terms and protective provisions. Mix small, robust code checks with heavier LLM legal checks.", "is_required": false, "max_points": 9.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Document Type and Text Extraction", "description": "Confirm primary output is a document and text can be extracted (DOCX/PDF). Reward longer, substantive text.", "weight": 0.3, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    if not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        if str(output.ext).lower() in [\".docx\", \"docx\"]:\n            text = context.files.read_docx_text(output.id)\n        elif str(output.ext).lower() in [\".pdf\", \"pdf\"]:\n            text = context.files.read_pdf_text(output.id)\n        else:\n            # Fallback attempt\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_pdf_text(output.id)\n                except Exception:\n                    text = \"\"\n    except Exception:\n        text = \"\"\n    if not text:\n        return 0.0\n    ln = len(text)\n    if ln > 5000:\n        return 1.0\n    if ln > 1500:\n        return 0.8\n    if ln > 800:\n        return 0.6\n    if ln > 300:\n        return 0.4\n    if ln > 100:\n        return 0.2\n    return 0.1"}, {"type": "code", "name": "Deal Parties and Economics Mentioned", "description": "Check presence of company and investor names and the stated deal economics (1,000,000 shares, $500,000). Case/format tolerant.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    # Extract text\n    text = \"\"\n    try:\n        if str(output.ext).lower() in [\".docx\", \"docx\"]:\n            text = context.files.read_docx_text(output.id)\n        else:\n            text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = \"\"\n    t = text.lower()\n    score = 0\n    # Names\n    if \"noxapulse\" in t:\n        score += 1\n    if \"alan gane\" in t:\n        score += 1\n    # Share count variants\n    if re.search(r\"\\b1[, ]?0{6}\\b\", t) or \"1000000\" in t:\n        score += 1\n    # Price variants\n    if (\"$500,000\" in text) or (\"500,000\" in text) or (\"usd 500,000\" in t) or re.search(r\"\\b500000\\b\", t):\n        score += 1\n    # Delaware mention\n    if \"delaware\" in t:\n        score += 1\n    return min(1.0, score/5)"}, {"type": "code", "name": "Bracketed Placeholders Present", "description": "Ensure the agreement uses bracketed placeholders for unknowns (e.g., [Date], [Address]).", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        if str(output.ext).lower() in [\".docx\", \"docx\"]:\n            text = context.files.read_docx_text(output.id)\n        else:\n            text = context.files.read_pdf_text(output.id)\n    except Exception:\n        text = \"\"\n    # Look for bracketed placeholders \u2013 avoid single bracket noise\n    placeholders = re.findall(r\"\\[[^\\[\\]\\n]{2,}\\]\", text)\n    n = len(placeholders)\n    if n >= 6:\n        return 1.0\n    if n >= 3:\n        return 0.7\n    if n >= 1:\n        return 0.4\n    return 0.0"}, {"type": "code", "name": "Cap Table Numbers Present (Before/After)", "description": "Check that the text references pre/post share counts consistent with the transaction (5,000,000 pre; issuance of 1,000,000; 6,000,000 post).", "weight": 0.3, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        if str(output.ext).lower() in [\".docx\", \"docx\"]:\n            text = context.files.read_docx_text(output.id)\n        else:\n            text = context.files.read_pdf_text(output.id)\n    except Exception:\n        text = \"\"\n    t = text.replace(\",\", \"\")\n    score = 0\n    if \"5000000\" in t:\n        score += 1\n    if \"1000000\" in t:\n        score += 1\n    if \"6000000\" in t:\n        score += 1\n    if score == 3:\n        return 1.0\n    if score == 2:\n        return 0.7\n    if score == 1:\n        return 0.4\n    return 0.0"}, {"type": "llm_judge", "name": "Deal Terms and Role Constraints Fidelity", "description": "Verify the agreement states purchase of 1,000,000 common shares for $500,000; Alan is investing individually and is an accredited investor; includes information/inspection rights; and does NOT grant a board seat or observer seat.", "weight": 2.2, "judge_prompt": "Review the agreement text:\n- Does it clearly state that Alan Gane is purchasing 1,000,000 shares of common stock for $500,000?\n- Does it state he is investing in his individual capacity and includes an accredited investor rep?\n- Does it include minority information and inspection rights?\n- Critically: It should NOT grant a board seat or board observer rights to the investor. If an observer right is granted, deduct heavily unless it is explicitly a prohibition (e.g., states that no observer rights are granted).\nScoring (2.2 max):\n- 2.2: All items explicitly covered; information/inspection rights present; no board/observer seat granted.\n- 1.5: One element missing or ambiguous (e.g., accredited investor implied but not explicit OR inspection rights missing) with no board/observer seat granted.\n- 0.8: Two elements missing or ambiguous, still no board/observer.\n- 0.0: Board/observer seat is granted OR deal economics (1,000,000 for $500,000) not clearly stated.", "expectation": "A clear deal section with the exact share/price terms; investor accredited representation; info and inspection rights; and an explicit or implicit absence of any board/observer grant."}, {"type": "llm_judge", "name": "Ownership Protections: Minimum Ownership Anti-Dilution + Pre-emptive Rights", "description": "Check for an explicit minimum 10% fully diluted ownership covenant with top-up mechanics and customary carve-outs, plus pre-emptive rights for future issuances.", "weight": 2.2, "judge_prompt": "Evaluate whether the agreement includes:\n1) A minimum ownership covenant ensuring Alan maintains at least 10% of the Company\u2019s fully diluted capitalization, implemented through a customary top-up mechanism.\n2) A clear definition of Fully Diluted Capitalization (reasonable for startups).\n3) Carve-outs for exempt issuances (e.g., ESOP up to a cap, SAFEs/notes conversions, strategic issuances, customary exceptions).\n4) Separate pre-emptive rights allowing pro rata participation in new issuances with notice, election window, and pricing parity.\nScoring (2.2 max):\n- 2.2: All four elements present and coherent.\n- 1.6: Three elements present.\n- 1.0: Two elements present.\n- 0.4: One element present.\n- 0.0: None present or contradictory (e.g., no carve-outs but an inconsistent FDC definition).", "expectation": "Minimum 10% anti-dilution via top-up with carve-outs, plus a stand-alone pre-emptive right with practical notice/election mechanics."}, {"type": "llm_judge", "name": "Minority Consent Rights Coverage", "description": "Verify protective provisions preventing the Company from taking extraordinary actions without Alan\u2019s prior consent.", "weight": 1.8, "judge_prompt": "Check that the agreement includes minority investor consent rights (protective provisions) requiring Alan\u2019s written consent before the Company can:\n- Change of control, merger, sale of substantially all assets.\n- Liquidation, dissolution, or winding up.\n- Amend charter/bylaws or other governing docs in a manner adverse to the investor/common.\n- Incur material indebtedness above a stated threshold or grant liens.\n- Declare dividends, distributions, or repurchase/ redeem shares.\n- Make material changes to management or the Company\u2019s principal business.\nScore based on coverage and clarity:\n- 1.8: All listed items (or their equivalents) are clearly covered with a consent requirement.\n- 1.2: One category missing or overly vague.\n- 0.7: Two categories missing or vague.\n- 0.3: Three categories missing or vague.\n- 0.0: No meaningful protective provisions.", "expectation": "A protective provisions section listing the enumerated extraordinary actions that require the investor\u2019s prior written consent."}, {"type": "llm_judge", "name": "Cap Table Schedule Consistency (Before/After)", "description": "Evaluate whether Schedule A presents a before/after capitalization reflecting the facts and internal consistency with the deal terms.", "weight": 1.3, "judge_prompt": "Inspect Schedule A (Capitalization Before and After). Confirm:\n- It exists and is clearly labeled.\n- Before: 5,000,000 common shares outstanding, all owned by Eleanor Byrne (individual). Authorized capital: 10,000,000 common, par $0.00001.\n- The transaction issues 1,000,000 new common shares to Alan for $500,000.\n- After: total outstanding should reflect the issuance (e.g., 6,000,000 if no other changes), showing post-ownership for Eleanor and Alan. If an option pool or other fully diluted items are included, the schedule remains internally consistent and the math is coherent.\nScoring (1.3 max):\n- 1.3: Schedule A present with a clear table; before/after numbers align with the deal; internal math is coherent.\n- 0.9: Schedule present and mostly consistent but with minor omissions (e.g., authorized/par not shown) or slight ambiguity.\n- 0.5: Schedule present but lacks before/after differentiation or unclear ownership breakdown.\n- 0.0: No Schedule A or numbers contradict the stated deal (e.g., wrong pre/post counts).", "expectation": "A tabular Schedule A that is numerically consistent with 5,000,000 pre-outstanding and the 1,000,000 share issuance to the investor, with coherent post figures."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Quality Assessment (Professional Drafting and Client Alignment)", "description": "Holistic legal drafting quality and professional usability for the client\u2019s needs.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Drafting Quality and Enforceability", "description": "Assess clarity, organization, defined terms, cross-references, and completeness of reps/covenants/boilerplate.", "weight": 1.5, "judge_prompt": "Evaluate legal drafting quality:\n- Clear, consistent defined terms and cross-references; logical section numbering.\n- Customary early-stage reps and warranties for the Company and Investor.\n- Coherent covenants and remedies; survival and severability addressed.\n- No obvious contradictions.\nScoring:\n- 1.5: High-quality, publication-ready legal drafting.\n- 1.0: Generally solid with minor issues.\n- 0.5: Several clarity or consistency issues.\n- 0.0: Disorganized or materially flawed drafting.", "expectation": "A well-structured, consistent agreement with professional legal drafting conventions."}, {"type": "llm_judge", "name": "Professional Structure and Usability", "description": "Evaluate formatting, headings, numbering, signature blocks, notices, and schedules for practitioner usability.", "weight": 1.2, "judge_prompt": "Assess professional usability:\n- Clear headings, numbered subsections, and table formatting for the schedule.\n- Signature blocks correctly formatted with bracketed name/title lines.\n- Notices section usable with bracket placeholders for addresses.\n- Exhibits/schedules referenced correctly within the body.\nScoring:\n- 1.2: Highly usable, professional format.\n- 0.8: Usable with minor formatting gaps.\n- 0.4: Several formatting/cross-reference issues.\n- 0.0: Poor usability.", "expectation": "A practitioner-friendly document with coherent headings, cross-references, and properly formatted signatures/notices."}, {"type": "llm_judge", "name": "Client Alignment and Practicality", "description": "Confirm the agreement reflects the client\u2019s preferences: minority posture, info rights without governance seat, and practical mechanics.", "weight": 1.2, "judge_prompt": "Evaluate client alignment:\n- Investor as minority without board/observer role; explicit information/inspection rights.\n- Practical mechanics (reasonable notice/election periods; thresholds for consent/debt are sensible for a startup).\n- Delaware governing law; confidentiality and expenses addressed.\nScoring:\n- 1.2: Fully aligned and practical.\n- 0.8: Mostly aligned with minor practical gaps.\n- 0.4: Several misalignments (e.g., governance role granted, impractical notice periods).\n- 0.0: Poor alignment with client preferences.", "expectation": "A pragmatic minority-investor agreement matching the client\u2019s expressed constraints and jurisdiction."}, {"type": "llm_judge", "name": "Risk and Compliance Coverage", "description": "Evaluate securities law compliance statements, legends, transfer restrictions, ROFR consistency, and overall risk mitigation.", "weight": 1.1, "judge_prompt": "Check inclusion of:\n- Securities Act exemption (e.g., Regulation D/Section 4(a)(2)), investment intent reps, transfer restrictions, and legends.\n- Consistency with existing ROFR/transfer restrictions in bylaws.\n- Standard boilerplate (integration/entire agreement, amendments/waivers, survival, assignment limits, counterparts/e-sign).\nScoring:\n- 1.1: Comprehensive and coherent risk/compliance coverage.\n- 0.7: Minor omissions.\n- 0.3: Multiple gaps.\n- 0.0: Significant compliance omissions.", "expectation": "Compliance-centered clauses that reduce legal risk and align with startup private placement norms."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "e14e32ba-d310-4d45-9b8a-6d73d0ece1ae", "rubric": {"category_name": "NYC Jewish Deli Research One Sheet", "rationale": "This rubric enforces a self-documenting, producer-ready research one-sheet. Stage 1 strictly mandates a DOCX/PDF with a clear per-establishment structure so verification is trivial. Stage 2 mixes lightweight code checks (counts, URL patterns, hours formats) with LLM verification of category fit, per-entry completeness, and media-link relevance. Stage 3 assesses professional presentation and production usefulness. Code rules are weighted ~4x-5x less than LLM rules to reflect that nuanced checks require vision and contextual judgment.", "max_total_score": 30.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate", "description": "Output must be a professionally structured one-sheet document with clearly separated entries for 4\u20136 NYC Jewish delicatessens and required sub-elements per entry.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Document Format and Structural Completeness", "description": "Verify the candidate produced a PDF/DOCX titled correctly and containing 4\u20136 deli entries, each with the required sub-elements in a clear, scannable layout.", "weight": 8.0, "judge_prompt": "You are the Stage 1 GATE. Only check FORMAT and STRUCTURE, not content quality. Evaluate the primary output file.\n\nRequirements (be flexible with exact wording of headers):\n- File format: Must be PDF or DOCX (a professional document, not plain text/Excel).\n- Title at/near top: Must contain the exact phrase \u201cNYC Jewish Deli Research One Sheet\u201d. Minor punctuation/case variations are acceptable.\n- Entry count: 4 to 6 distinct establishments, each clearly separated (e.g., headings, boxes, or numbered sections).\n- Per-establishment required sub-elements:\n  1) Photo of the establishment (interior or exterior). Images must be visible in the document (not just links).\n  2) Location (address and NYC borough or neighborhood).\n  3) Business Hours (days and times in a readable format).\n  4) Website URL (official site preferred; visible as text or link).\n  5) Notable Dishes (at least two items listed).\n  6) Important Notes about the business (e.g., history, kosher status, cash-only, iconic elements).\n  7) Media/Video Links to prior features/interviews/segments (YouTube, Facebook, Instagram, Vimeo, etc.). At least one link per establishment is expected if available.\n- Clear, scan-friendly layout resembling a one-sheet (section headers, bullets or mini-tables, consistent formatting).\n\nScoring (0 to 8 points):\n- 8.0: Correct file type AND title present AND 4\u20136 entries AND each entry includes all 7 required sub-elements AND layout is clearly a one-sheet.\n- 6.0\u20137.5: Correct file type AND title present AND 4\u20136 entries; up to two total missing sub-elements across the whole document OR one entry missing the photo only; layout still clear.\n- 3.0\u20135.5: Correct file type but structural gaps (e.g., fewer than 4 or more than 6 entries; multiple entries missing required sub-elements; missing title; unclear section separation) yet still resembles the intended document.\n- 0.0\u20132.5: Not PDF/DOCX OR no recognizable per-establishment structure OR fewer than 3 entries.\n\nReturn only a numeric score from 0 to 8 based on structure presence and format, not content correctness or quality.", "expectation": "A PDF/DOCX titled \"NYC Jewish Deli Research One Sheet\" with 4\u20136 clearly separated entries, each containing: photo, location, hours, website, notable dishes (\u22652), important notes, and media links. Layout should be scannable as a one-sheet."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification Checks (Correctness)", "description": "Validate correctness and completeness using mixed code and LLM checks enabled by the enforced structure.", "is_required": false, "max_points": 15.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Entity Count via Key Fields (4\u20136 required)", "description": "Heuristically validate there are 4\u20136 establishments by counting repeated key field labels (Website/Location/Hours) in the document text.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float or (float, str)\n    \"\"\"\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    if not output.is_document:\n        return 0.0\n\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        pass\n    if not text:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text:\n        return 0.0\n\n    t = text.lower()\n    counts = [t.count('website'), t.count('location'), t.count('business hours') + t.count('hours')]\n    n = max(counts) if counts else 0\n\n    # Scoring: 4-6 -> 1.0; 3 or 7 -> 0.6; 2 or 8 -> 0.3; else 0.0\n    if 4 <= n <= 6:\n        score = 1.0\n    elif n in (3, 7):\n        score = 0.6\n    elif n in (2, 8):\n        score = 0.3\n    else:\n        score = 0.0\n\n    feedback = f\"Detected approx. {n} establishments via key-field repetition (counts={counts}).\"\n    return score, feedback"}, {"type": "code", "name": "Media Links Presence (Video platforms)", "description": "Check for presence of video/social media feature links (YouTube, Vimeo, Facebook, Instagram, TikTok, etc.).", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    if not output.is_document:\n        return 0.0\n\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        pass\n    if not text:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text:\n        return 0.0\n\n    urls = re.findall(r'https?://[^\\s)]+', text)\n    hosts = ('youtube.com', 'youtu.be', 'vimeo.com', 'facebook.com', 'fb.watch', 'instagram.com', 'tiktok.com', 'dailymotion.com')\n    video_links = [u for u in urls if any(h in u.lower() for h in hosts)]\n    n = len(video_links)\n\n    # Scoring: >=4 -> 1.0; 2-3 -> 0.6; 1 -> 0.3; 0 -> 0.0\n    if n >= 4:\n        score = 1.0\n    elif n >= 2:\n        score = 0.6\n    elif n == 1:\n        score = 0.3\n    else:\n        score = 0.0\n\n    feedback = f\"Found {n} video-platform links: {video_links[:5]}\"  # preview up to 5\n    return score, feedback"}, {"type": "code", "name": "Business Hours Formatting Plausibility", "description": "Detect plausible hours strings (day names with times) to confirm presence of business hours details.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        pass\n    if not text:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text:\n        return 0.0\n\n    t = text\n    # Match patterns like \"Mon-Fri 10am-6pm\", \"Monday 11:00 AM \u2013 8:00 PM\", etc.\n    day_pattern = r\"(?:Mon|Tue|Tues|Wed|Thu|Thur|Fri|Sat|Sun|Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday)\"\n    time_pattern = r\"\\b\\d{1,2}(:\\d{2})?\\s?(?:AM|PM|am|pm)?\\b\"\n    range_sep = r\"\\s?(?:-|\u2013|to)\\s?\"\n    hours_regex = re.compile(fr\"{day_pattern}[^\\n\\r]{{0,40}}{time_pattern}{range_sep}{time_pattern}\")\n    matches = hours_regex.findall(t)\n    n = len(matches)\n\n    # Scoring: >=4 matches -> 1.0; 2-3 -> 0.6; 1 -> 0.3; else 0.0\n    if n >= 4:\n        score = 1.0\n    elif n >= 2:\n        score = 0.6\n    elif n == 1:\n        score = 0.3\n    else:\n        score = 0.0\n\n    feedback = f\"Detected {n} business-hours patterns.\"\n    return score, feedback"}, {"type": "llm_judge", "name": "Category Fit and NYC Location Verification", "description": "Confirm each listed establishment is a NYC Jewish delicatessen (or clearly Jewish deli/appetizing style) and located within NYC.", "weight": 4.0, "judge_prompt": "Evaluate the document and verify, for each establishment listed, that:\n- It is a Jewish delicatessen (Jewish deli/delicatessen/appetizing shop is acceptable; generic diners or unrelated cuisines are not).\n- It is located in New York City (any borough). If the doc lists addresses outside NYC, mark them invalid.\n\nScoring (0 to 4 points):\n- 4.0: All listed (4\u20136) entries are valid NYC Jewish delis/appetizing establishments.\n- Deduct ~0.8 points per invalid or clearly uncertain entry (cannot tell if it is a Jewish deli or not in NYC), down to 0.\nReturn a numeric score between 0 and 4 only.", "expectation": "Every entry is a bona fide NYC Jewish deli (or appetizing deli) with NYC location."}, {"type": "llm_judge", "name": "Per-Entry Required Elements Present", "description": "Check each entry includes photo, location, business hours, website, notable dishes (\u22652), important notes, and at least one media link.", "weight": 4.0, "judge_prompt": "Assess completeness across all entries. For each establishment, check the presence of:\n1) Photo image, 2) Location (address + borough/neighborhood), 3) Business Hours, 4) Website URL, 5) Notable Dishes (at least two items), 6) Important Notes, 7) Media/Video links to prior features/interviews.\n\nScore by average coverage across entries:\n- 4.0: \u226590% of required fields present across all entries (e.g., at most 2 total missing fields across the full set).\n- 3.0: ~75\u201389% fields present.\n- 2.0: ~60\u201374% fields present.\n- 1.0: ~40\u201359% fields present.\n- 0.0: <40% fields present.\nReturn only a numeric score 0\u20134.", "expectation": "Each entry substantially complete with all required sub-elements."}, {"type": "llm_judge", "name": "Media Link Relevance to Prior Features", "description": "Ensure media links correspond to prior features/interviews/segments about the establishment (not just the homepage or unrelated videos).", "weight": 4.0, "judge_prompt": "Review all linked media items (YouTube, Facebook, Instagram, Vimeo, etc.). Judge whether they are relevant prior features/interviews/segments about the specific establishment (not generic or unrelated content). Consider captions/labels adjacent to links when judging relevance.\n\nScoring (0 to 4 points):\n- 4.0: \u226580% of entries have at least one clearly relevant prior feature/interview media link.\n- 3.0: 60\u201379% entries have relevant media links.\n- 2.0: 40\u201359% entries have relevant media links.\n- 1.0: 1\u201339% entries have relevant media links.\n- 0.0: None are clearly relevant.\nReturn only a numeric score from 0 to 4.", "expectation": "Most entries include at least one clearly relevant prior media feature link."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality Assessment", "description": "Holistic evaluation of presentation quality, usefulness for a producer, clarity, and visual polish.", "is_required": false, "max_points": 7.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "One-Sheet Presentation and Readability", "description": "Professional, scannable layout with consistent headings/labels, logical ordering, and clean typography suitable for a producer one-sheet.", "weight": 1.75, "judge_prompt": "Assess overall presentation quality: clarity of the title and section headers, consistency of formatting, use of bullets/tables, spacing, and overall readability suitable for a producer\u2019s one-sheet. Is it easy to scan and extract key info quickly?\n\nScore 0\u20131.75: higher for clean, consistent, producer-ready formatting; lower for cluttered, inconsistent, or hard-to-scan layouts.", "expectation": "A polished, scannable one-sheet with consistent formatting and clear hierarchy."}, {"type": "llm_judge", "name": "Production-Readiness of Notes", "description": "Are the Important Notes materially helpful for planning a shoot (e.g., ambiance, crowd levels, iconic visuals, cash-only, owner background)?", "weight": 1.75, "judge_prompt": "Evaluate whether the Important Notes sections are specifically helpful to a video producer: details on ambiance, iconic visuals, peak hours/crowd levels, sound environment, cash-only policies, kosher status, owner/chef points, filming considerations. Reward concise, practical insights over generic statements.\n\nScore 0\u20131.75: higher for practical, production-relevant insights; lower for generic or sparse notes.", "expectation": "Notes provide practical on-the-ground insights helpful for filming decisions."}, {"type": "llm_judge", "name": "Image Quality and Relevance", "description": "Photos are clearly of the establishments (interior/exterior), legible, and helpful to visualize potential shots.", "weight": 1.75, "judge_prompt": "Judge the images: Are there photos for each entry? Are they relevant (establishment interior/exterior or signature items) rather than generic stock? Are they clear enough to gauge visual appeal for filming?\n\nScore 0\u20131.75: higher for clear, relevant photos across entries; lower for missing, irrelevant, or poor-quality images.", "expectation": "Clear, relevant photos that help visualize the location on camera."}, {"type": "llm_judge", "name": "Clarity and Internal Consistency", "description": "Addresses, hours, and links are presented clearly and consistently; information does not contradict itself.", "weight": 1.75, "judge_prompt": "Evaluate clarity and internal consistency: Are addresses and boroughs clearly stated and consistent? Are hours formatted uniformly? Do link labels/descriptions match the linked domains? Note any contradictions or confusing presentation.\n\nScore 0\u20131.75: higher for clear, consistent, contradiction-free presentation; lower if inconsistent or confusing.", "expectation": "Consistent, unambiguous details with no internal contradictions."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "bbe0a93b-ebf0-40b0-98dc-8d9243099034", "rubric": {"category_name": "Government - Child, Family, and School Social Workers: Bilingual Needs Assessment Forms and Kent County Resource Guide", "rationale": "This rubric enforces a self-documenting, file-based workflow. Stage 1 (LLM-only) strictly gates structure: two separate bilingual needs assessment PDFs (English and Spanish) each containing a screening table (questions | Yes | No) and a tracking/follow-up table, plus a separate Resource Guide PDF organized by service categories with contact info. Stage 2 mixes light code checks (format/text cues, category coverage, contact density) with higher-weight LLM verification of correctness and coherence. Stage 3 provides holistic quality assessment focused on accessibility, cultural humility, staff usability, and professional presentation.", "max_total_score": 25.0, "stages": [{"name": "Stage 1 \u2013 Structure Gate (LLM-only)", "description": "Verify the exact required deliverables and structural elements exist to enable verification: two separate needs assessment PDFs (English and Spanish) with required tables/columns, and a separate Resource Guide PDF organized by categories with contact info.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Bilingual Needs Assessment Forms: Structure Gate", "description": "Check that there are TWO separate PDF files: one English needs assessment and one Spanish needs assessment. Each file must include BOTH: (1) a screening table with three columns [Questions related to areas of needs | Yes | No]; and (2) a tracking/follow-up table with three columns [Individual or Family Need | Resource Given | Follow-Up Needed]. The screening questions must cover the listed domains: income, food, housing/shelter, clothing, education, financial literacy, transportation, employment, legal assistance, pet services, healthcare.", "weight": 2.0, "judge_prompt": "You are verifying the presence and structure of two separate needs assessment PDF files: one in English and one in Spanish. Use flexible matching for section headers and column labels, but the structure must be clearly visible as tables.\n\nRequirements to confirm:\n1) Two separate PDF documents exist, clearly one in English and one in Spanish (Spanish should show clear Spanish text, e.g., S\u00ed/No, Necesidades, Seguimiento, etc.).\n2) In EACH of the two PDFs, verify BOTH of the following tables are present:\n   a) Screening table with three columns whose intent matches: [Questions related to areas of needs | Yes | No]. The screening questions should visibly cover at least these domains: income, food, housing/shelter, clothing, education, financial literacy, transportation, employment, legal assistance, pet services, healthcare.\n   b) Tracking/follow-up table with three columns labeled or clearly equivalent to: [Individual or Family Need | Resource Given | Follow-Up Needed].\n3) Tables should be presented cleanly with clear headers and sufficient rows for staff use.\n\nScoring:\n- 2.0: Both separate PDFs exist (EN and ES) and BOTH tables appear in BOTH PDFs with appropriate columns and coverage of the listed domains.\n- 1.0: Two PDFs exist but one required table is missing in ONE of the PDFs OR domain coverage is clearly incomplete (missing multiple required domains) in ONE of the PDFs.\n- 0.5: Only ONE needs assessment PDF exists OR both exist but table structures are unclear/partial in both.\n- 0.0: No valid needs assessment PDFs or wrong file format.\nOnly check structure/presence, not the quality of questions.", "expectation": "Two distinct PDFs (EN, ES) each containing a screening table (Questions | Yes | No) covering required domains and a tracking/follow-up table (Individual or Family Need | Resource Given | Follow-Up Needed)."}, {"type": "llm_judge", "name": "Resource Guide: Structure Gate", "description": "Check that there is a separate PDF serving as a Kent County Resource Guide. It must be organized by category (e.g., Financial Assistance, Transportation, Food Pantry, Employment, Clothing, Healthcare, Counseling, Legal Services, Pregnancy Support, etc.) and list resource names with contact information (e.g., phone, address, website).", "weight": 2.0, "judge_prompt": "Verify there is a separate PDF document that is clearly a Resource Guide for Kent County, Michigan. Use flexibility for exact section names, but it must visibly present categories and resource entries with contact information.\n\nRequirements to confirm:\n1) The file is a PDF separate from the two needs assessment PDFs.\n2) Organized by categories or headings such as: Financial Assistance, Transportation, Food Pantry, Employment, Clothing, Healthcare, Counseling, Legal Services, Pregnancy Support (additional relevant categories acceptable). Multiple categories should be present.\n3) For each category, list multiple resource entries by name along with contact information (at least a phone number and/or website and/or address). Ideally entries indicate Kent County/Grand Rapids relevance.\n\nScoring:\n- 2.0: Separate PDF exists, organized by multiple relevant categories, with multiple entries per category including clear contact info.\n- 1.0: Separate PDF exists, categories present but sparse OR many entries lack contact info.\n- 0.0: No separate resource guide PDF or wrong format.\nOnly check structure/presence, not the quality/accuracy of listings.", "expectation": "A standalone PDF resource guide with multiple categories and entries, each with contact information."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification (Mixed: light code + LLM)", "description": "Check correctness and completeness of content now that structure is enforced. Code rules perform deterministic text checks; LLM judges verify nuanced correctness and coherence.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Detect Bilingual Assessment Forms and Language Markers", "description": "Programmatically detect two separate assessment PDFs and verify language markers (English vs. Spanish), presence of Yes/No (or S\u00ed/No), and tracking table markers.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    import re\n    # Helper to read text from a PDF resource id safely\n    def read_text_safe(res):\n        try:\n            if res.is_document:\n                return (context.files.read_pdf_text(res.id) or '').lower()\n        except Exception:\n            return ''\n        return ''\n\n    outputs = context.get_all_outputs() or []\n    pdfs = [r for r in outputs if getattr(r, 'is_document', False)]\n    if not pdfs:\n        return 0.0\n\n    # Heuristics for identifying assessment forms vs resource guide\n    domain_terms = [\n        'income','food','housing','shelter','clothing','education','financial literacy',\n        'transportation','employment','legal','pet','healthcare','health care'\n    ]\n    en_markers = ['yes', 'no', 'needs assessment', 'individual or family need', 'resource given', 'follow-up needed']\n    es_markers = ['s\u00ed', 'si', 'no', 'necesidad', 'seguimiento', 'recurso']\n\n    texts = [(r, read_text_safe(r)) for r in pdfs]\n\n    # Classify likely assessment forms: contain many domains and yes/no markers or tracking headers\n    def is_assessment(text):\n        domain_hits = sum(1 for t in domain_terms if t in text)\n        yn_hits = int('yes' in text or 's\u00ed' in text or 'si' in text) + int('no' in text)\n        track_hits = int('individual or family need' in text) + int('resource given' in text) + int('follow-up needed' in text) + int('seguimiento' in text)\n        return domain_hits >= 5 and (yn_hits >= 2 or track_hits >= 1)\n\n    assessments = [(r, t) for r, t in texts if is_assessment(t)]\n\n    # Language detection heuristics\n    en_count = 0\n    es_count = 0\n    for r, t in assessments:\n        if any(m in t for m in en_markers):\n            en_count += 1\n        if any(m in t for m in es_markers):\n            # Require some other Spanish words beyond 'no' to reduce false positives\n            if 's\u00ed' in t or 'si' in t or 'seguimiento' in t or 'recurso' in t or 'necesidad' in t:\n                es_count += 1\n\n    score = 0.0\n    # Need at least two assessments and at least one in each language\n    if len(assessments) >= 2:\n        score += 0.5\n    if en_count >= 1 and es_count >= 1:\n        score += 0.3\n    # Check explicit presence of yes/no and tracking headers somewhere among assessments\n    has_yesno = any(('yes' in t and 'no' in t) or ('s\u00ed' in t and 'no' in t) or ('si' in t and 'no' in t) for _, t in assessments)\n    has_tracking = any(('individual or family need' in t and 'resource given' in t and 'follow-up needed' in t) or ('seguimiento' in t) for _, t in assessments)\n    if has_yesno:\n        score += 0.1\n    if has_tracking:\n        score += 0.1\n\n    return min(score, 1.0)"}, {"type": "code", "name": "Required Domains Coverage in Assessments", "description": "Verify that the assessments collectively include the required domains by name (flexible matching).", "weight": 1.0, "code": "def evaluate(workflow, context):\n    # Read all document texts\n    outputs = context.get_all_outputs() or []\n    pdfs = [r for r in outputs if getattr(r, 'is_document', False)]\n    if not pdfs:\n        return 0.0\n    def read_text(res):\n        try:\n            return (context.files.read_pdf_text(res.id) or '').lower()\n        except Exception:\n            return ''\n    texts = [read_text(r) for r in pdfs]\n    all_text = ' \\n '.join(texts)\n\n    required = {\n        'income': ['income','ingreso'],\n        'food': ['food','alimento','comida'],\n        'housing/shelter': ['housing','shelter','vivienda','albergue','refugio'],\n        'clothing': ['clothing','ropa'],\n        'education': ['education','educaci\u00f3n','educacion'],\n        'financial literacy': ['financial literacy','alfabetizaci\u00f3n financiera','alfabetizacion financiera'],\n        'transportation': ['transportation','transporte'],\n        'employment': ['employment','empleo','trabajo'],\n        'legal assistance': ['legal','asistencia legal','ayuda legal'],\n        'pet services': ['pet','mascota'],\n        'healthcare': ['healthcare','health care','salud','atenci\u00f3n m\u00e9dica','atencion medica']\n    }\n    hits = 0\n    for k, variants in required.items():\n        if any(v in all_text for v in variants):\n            hits += 1\n    return hits / max(len(required), 1)"}, {"type": "code", "name": "Resource Guide: Contact Density and Category Presence", "description": "Check that the Resource Guide contains multiple categories and sufficient contact info indicators (phones, websites) and local references to Kent County/Grand Rapids/Michigan.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    import re\n    outputs = context.get_all_outputs() or []\n    pdfs = [r for r in outputs if getattr(r, 'is_document', False)]\n    if not pdfs:\n        return 0.0\n    def read_text(res):\n        try:\n            return (context.files.read_pdf_text(res.id) or '').lower()\n        except Exception:\n            return ''\n    texts = {r.id: read_text(r) for r in pdfs}\n\n    # Heuristic: pick the doc with most category keyword hits as the resource guide\n    categories = ['financial assistance','transportation','food pantry','employment','clothing','healthcare','counseling','legal services','pregnancy support','rent','utilities','housing','shelter']\n    guide_id = None\n    best_hits = -1\n    for rid, text in texts.items():\n        hits = sum(1 for c in categories if c in text)\n        if hits > best_hits:\n            best_hits = hits\n            guide_id = rid\n    if guide_id is None or best_hits <= 0:\n        return 0.0\n\n    text = texts[guide_id]\n\n    phone_re = re.compile(r\"(?:\\+1[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\")\n    url_re = re.compile(r\"(https?://|www\\.)[\\w.-/]+\", re.IGNORECASE)\n    email_re = re.compile(r\"[\\w\\.-]+@[\\w\\.-]+\\.[a-z]{2,}\")\n\n    phones = len(phone_re.findall(text))\n    urls = len(url_re.findall(text))\n    emails = len(email_re.findall(text))\n\n    locality = any(x in text for x in ['kent county','grand rapids','mi','michigan'])\n\n    # Score components\n    score = 0.0\n    # Category coverage\n    if best_hits >= 6:\n        score += 0.4\n    elif best_hits >= 4:\n        score += 0.25\n    elif best_hits >= 2:\n        score += 0.15\n\n    # Contact density\n    contacts = phones + urls + emails\n    if contacts >= 12:\n        score += 0.4\n    elif contacts >= 6:\n        score += 0.25\n    elif contacts >= 3:\n        score += 0.15\n\n    # Local relevance\n    if locality:\n        score += 0.2\n\n    return min(score, 1.0)"}, {"type": "llm_judge", "name": "Form Clarity and Specificity of Screening Questions", "description": "Assess whether the screening questions are clear, domain-specific, and align with the Yes/No logging intent in both English and Spanish forms.", "weight": 3.0, "judge_prompt": "Evaluate BOTH the English and Spanish needs assessment PDFs. Focus on the clarity and specificity of the screening questions and whether they align well with a Yes/No response workflow.\n\nConsider:\n- Each required domain (income, food, housing/shelter, clothing, education, financial literacy, transportation, employment, legal assistance, pet services, healthcare) has at least one clear, plain-language screening question.\n- Yes/No columns are consistently placed and visually aligned with questions so staff can quickly mark responses.\n- Wording avoids ambiguity (e.g., uses concrete phrases like \u201cDo you need help obtaining\u2026?\u201d rather than vague terms).\n- Spanish version conveys the same intent as the English version (not a partial or incorrect translation).\n\nScoring:\n- 3.0: All domains covered with clear, specific, and parallel EN/ES questions; Yes/No layout is coherent and scannable.\n- 2.0: Minor issues (e.g., 1\u20132 domains weaker or slight layout inconsistencies) but overall clear and usable.\n- 1.0: Several domains unclear/missing or inconsistent Yes/No alignment; translation quality issues.\n- 0.0: Questions largely unclear, missing many domains, or not aligned with Yes/No use.", "expectation": "Clear, domain-specific, parallel EN/ES screening questions with coherent Yes/No layout."}, {"type": "llm_judge", "name": "Tracking/Follow-Up Table Correctness and Usability", "description": "Validate the presence and functionality of the tracking/follow-up table columns and their clarity for logging needs, resources given, and follow-up requirements.", "weight": 3.0, "judge_prompt": "Examine the tracking/follow-up table in BOTH the English and Spanish assessment PDFs.\n\nRequirements:\n- Table has three columns clearly labeled (or equivalent): \u201cIndividual or Family Need\u201d, \u201cResource Given\u201d, and \u201cFollow-Up Needed\u201d.\n- Adequate space/rows for multiple entries.\n- Labels are unambiguous and mirror in Spanish appropriately (e.g., equivalents for the three columns).\n- Table design facilitates quick logging and later review (readable headers, consistent column widths, easy to fill).\n\nScoring:\n- 3.0: All requirements met in both EN and ES; table is clean, readable, and straightforward to use.\n- 2.0: Minor labeling or spacing issues in one document but still usable.\n- 1.0: Missing/unclear labels or insufficient rows/space; hard to use.\n- 0.0: Table absent or unusable.", "expectation": "A clean, well-labeled follow-up table with the three required columns in both EN and ES forms."}, {"type": "llm_judge", "name": "Resource Guide Completeness and Local Relevance", "description": "Assess whether the Resource Guide lists sufficient, relevant Kent County resources across categories with usable contact details.", "weight": 3.0, "judge_prompt": "Evaluate the Resource Guide PDF for completeness and practical utility for Kent County, Michigan.\n\nConsider:\n- Breadth: Multiple relevant categories (e.g., Financial Assistance, Transportation, Food Pantry, Employment, Clothing, Healthcare, Counseling, Legal Services, Pregnancy Support, etc.).\n- Depth: Several entries per category (not just one or two total) where possible.\n- Contact details: Each entry should have at least one of phone, website, or address. Preferably includes phone numbers.\n- Local relevance: Names/locations appropriate for Kent County/Grand Rapids; avoid unrelated or out-of-area resources.\n\nScoring:\n- 3.0: Broad category coverage with multiple local resources per category and consistent contact details.\n- 2.0: Good coverage but a few categories sparse or occasional missing contact details.\n- 1.0: Limited categories or many entries missing usable contact info; questionable locality.\n- 0.0: Minimal content, not locally relevant, or mostly missing contact info.", "expectation": "A robust, locally-relevant guide with multiple categories and clear contact details for each resource."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Quality and Professionalism (LLM)", "description": "Holistic assessment of accessibility, cultural humility, usability for staff, and professional presentation.", "is_required": false, "max_points": 9.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Accessibility and Plain Language", "description": "Judge whether both forms use plain, readable language suitable for clients with low literacy/limited English proficiency and whether the bilingual presentation supports accessibility.", "weight": 2.5, "judge_prompt": "Assess the English and Spanish forms for accessibility:\n- Plain language (short sentences, common words, concrete phrasing).\n- Readability for lower literacy levels.\n- Visual accessibility (clear headings, sufficient spacing/contrast, simple layout, legible fonts, clear checkboxes/mark areas if present).\n- Bilingual parity: Spanish mirrors English content and layout so staff can navigate both consistently.\n\nScoring:\n- 2.5: Highly accessible and plain language in both EN and ES with strong visual clarity and parity.\n- 1.5: Generally accessible with minor issues (jargon, small text, cramped spacing, minor parity gaps).\n- 0.5: Noticeable accessibility issues or inconsistent parity.\n- 0.0: Hard to read/understand; poor accessibility.", "expectation": "Plain-language, visually clear, bilingual-parity forms accessible to diverse clients."}, {"type": "llm_judge", "name": "Cultural Humility and Non-Stigmatizing Tone", "description": "Evaluate tone and wording for respectfulness, non-stigmatizing language, and trauma-informed sensitivity toward clients facing poverty, language barriers, and immigration issues.", "weight": 2.0, "judge_prompt": "Review both forms for tone:\n- Uses respectful, person-first, non-stigmatizing language.\n- Avoids blame or assumptions about clients (e.g., immigration status, housing insecurity, employment).\n- Trauma-informed phrasing (offers choices, avoids intrusive wording where not necessary, normalizes help-seeking).\n\nScoring:\n- 2.0: Consistently respectful and trauma-informed; exemplary cultural humility.\n- 1.0: Generally respectful but with occasional phrasing that could be softened.\n- 0.0: Multiple instances of stigmatizing or insensitive language.", "expectation": "Consistently respectful, person-first, trauma-informed language."}, {"type": "llm_judge", "name": "Usability for Staff Workflow", "description": "Assess whether the materials support efficient staff use: quick assessment flow, space for notes, clear instructions, and easy mapping from needs to resources and follow-up.", "weight": 2.0, "judge_prompt": "Consider how well the forms and guide support the staff workflow:\n- Clear, brief instructions for staff use (if included) and logical ordering of domains.\n- Adequate space for notes or checkmarks; easy to scan and complete during intake.\n- Direct linkage from Yes/No screening to the tracking table and then to the Resource Guide categories.\n\nScoring:\n- 2.0: Very efficient workflow, easy to use under time constraints.\n- 1.0: Usable but with some friction (ordering/space/instructions).\n- 0.0: Cumbersome to use; unclear flow from screening to follow-up.", "expectation": "Streamlined intake workflow with clear flow from screening to follow-up and resource referral."}, {"type": "llm_judge", "name": "Professional Presentation of Resource Guide", "description": "Judge the guide\u2019s organization, formatting consistency, and professional polish (headings, spacing, deduplication, and scannability).", "weight": 2.5, "judge_prompt": "Evaluate the Resource Guide\u2019s professional quality:\n- Clear category headings and consistent formatting for entries (name, brief descriptor optional, and contact info).\n- Logical ordering and scannable layout; avoids clutter.\n- Minimal duplication; reasonable completeness for common needs in Kent County.\n\nScoring:\n- 2.5: Highly professional, consistent, and easy to scan.\n- 1.5: Generally professional with minor inconsistencies.\n- 0.5: Noticeable formatting issues or clutter.\n- 0.0: Poorly organized/presented.", "expectation": "A clean, consistent, professionally formatted guide with scannable entries."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "c3525d4d-2012-45df-853e-2d2a0e902991", "rubric": {"category_name": "Wholesale Trade \u2014 Order Clerks: Holiday Floor Stand Budget Update", "rationale": "This rubric enforces a self-documenting, mixed-output workflow: an Excel model that makes verification trivial plus a Word email that summarizes decisions. Stage 1 (LLM-only) strictly enforces the output shape and required sections/tables so downstream checks are deterministic. Stage 2 mixes lightweight code rules (deterministic, bounds/consistency) with heavier LLM rules (cross-referencing, nuanced reasoning) to validate correctness. Stage 3 uses LLM-only holistic quality criteria (professionalism, clarity, stakeholder readiness). Code rules contribute substantially less weight than LLM rules in Stage 2, per guidance.", "max_total_score": 28.0, "stages": [{"name": "Stage 1 \u2014 Format & Structure Gate (LLM-only)", "description": "Gate that requires BOTH: (1) a structured Excel model with specific sheets/sections/tables and (2) a professional email draft in DOCX/PDF with required summary elements. Only presence/shape is checked here, not correctness.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Excel Shape: Budget and Comparison Workbook", "description": "Verify the delivered Excel has the exact, verifiable structure needed for downstream checks.", "weight": 5.0, "judge_prompt": "You are verifying STRUCTURE ONLY. Examine all candidate outputs from the last task. Confirm there is an Excel workbook (.xlsx) that contains at least the following two sheets (be flexible with names, e.g., 'Budget & Cost Comparison' vs 'Cost Comparison', 'Final Store List' vs 'Stores \u2013 Final') and required sections/tables:\n\nRequired Sheet A: Cost Comparison / Budget Summary\nMust contain clearly labeled sections (section titles or headers visible), each with tabular content:\n1) Assumptions (table): columns similar to [Parameter | Value | Source/Notes]. Rows must include:\n   - Overage % (used for breakage/backup)\n   - Shelf strip cost BEFORE and AFTER (or a Shelf strip cost delta of $0.25)\n   - Shelf strips per unit (integer count per floor stand) OR an equivalent variable that allows computing the impact of a $0.25 per-strip increase\n2) Unit Cost Comparison (table): clearly shows Original Unit Cost vs Revised Unit Cost and Delta\n3) Program Summary (table): includes at minimum these items as labeled rows/columns:\n   - Original Store Count\n   - Final Store Count\n   - Overage % (repeated OK)\n   - Units Needed (Original)\n   - Units Needed (Final)\n   - Original Total Program Cost\n   - Revised Total Program Cost\n4) Store List Changes (section): either separate small tables or a summary identifying Added vs Removed stores (counts and/or enumerations). This can be a section in this sheet or a clear cross-reference to the Final Store List sheet.\n\nRequired Sheet B: Final Store List\n- A complete tabular list of final stores (e.g., [Store ID | Store Name | City/State | ...])\n- A visible way to identify new/added stores (e.g., a 'Change Type' or 'Status' column with values like 'Added', or a boolean/flag, or visibly highlighted entries). The identification must be in-sheet (LLM can see formatting) and unambiguous.\n\nScoring (STRUCTURE ONLY):\n- Full (1.0): Excel present and both sheets present with all required sections/tables/fields above.\n- 0.8: Minor omissions (e.g., Store List Changes summary is in the Final Store List sheet only, or Assumptions missing Source/Notes column) but overall structure is verifiable.\n- 0.5: One major required section missing (e.g., no Assumptions table or no Program Summary).\n- 0.2: Workbook present but missing multiple required sections or only one sheet present.\n- 0.0: No Excel workbook or wrong format.\nOnly check presence/format, not correctness of numbers or formulas.", "expectation": "Excel workbook with two sheets: a Cost Comparison/Budget Summary containing Assumptions, Unit Cost Comparison, Program Summary, and Store List Changes; plus a Final Store List sheet with a clear flag/column for added stores."}, {"type": "llm_judge", "name": "Email Draft Shape (DOCX/PDF)", "description": "Verify a professional email draft is delivered with required elements.", "weight": 3.0, "judge_prompt": "Check for a Word (DOCX) or PDF email draft. Structure/format only (not correctness of numbers):\nRequired elements:\n- Subject line referencing Holiday Floor Stand program/budget\n- Greeting and concise context of the update (Production cost change and Final store matrix update)\n- Bullet or clearly separated lines summarizing:\n  * Updated number of floor stands (final units)\n  * Change in budget ($ delta and/or %)\n  * New total program budget (revised total)\n- Brief mention that overage % was applied consistently\n- Clear next steps and/or request for approval before vendor production start\n- Professional sign-off with name/title placeholder\n\nScoring:\n- 1.0: Valid DOCX/PDF with all elements above\n- 0.7: Valid DOCX/PDF with 1-2 minor elements missing\n- 0.4: Valid DOCX/PDF but missing 3+ elements\n- 0.0: No DOCX/PDF email or wrong format\nOnly check presence/format, not content accuracy.", "expectation": "A professional email (DOCX/PDF) containing subject, context, numeric summary bullets, overage mention, next steps, and sign-off."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness & Consistency Verification", "description": "Now that the structure is enforced, verify key calculations and cross-references: unit cost increase mechanics, units with overage, store change identification, and alignment between workbook and email.", "is_required": true, "max_points": 12.0, "min_score_to_pass": 6.0, "rules": [{"type": "code", "name": "Parsable Excel with Required Sheets (light check)", "description": "Programmatic check that an .xlsx exists and contains a cost/budget sheet and a final store list sheet (fuzzy names).", "weight": 0.5, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    # Find an Excel output\n    xl_res = None\n    for r in context.get_all_outputs():\n        if getattr(r, 'is_spreadsheet', False):\n            xl_res = r\n            break\n    if not xl_res:\n        return 0.0, \"No spreadsheet found\"\n    try:\n        path = context.files.get_path(xl_res.id)\n        xls = pd.ExcelFile(path)\n        sheets = [s.strip() for s in xls.sheet_names]\n        low = [s.lower() for s in sheets]\n        has_cost = any(any(k in s for k in ['cost','budget','comparison','summary']) for s in low)\n        has_final = any(('final' in s and 'store' in s) or any(k in s for k in ['store list','stores']) for s in low)\n        score = (1.0 if has_cost else 0.0) * 0.5 + (1.0 if has_final else 0.0) * 0.5\n        return score, f\"Sheets: {sheets}\"\n    except Exception as e:\n        return 0.0, f\"Excel parse error: {e}\""}, {"type": "code", "name": "Unit Cost Delta reflects $0.25 per shelf strip", "description": "Check that Revised Unit Cost >= Original Unit Cost and the delta is plausibly a multiple of $0.25 (shelf strip increase).", "weight": 0.8, "code": "import re\nimport math\nimport pandas as pd\n\nKEYS_ORIG = ['original unit cost','unit cost - original','orig unit cost','unit cost (original)']\nKEYS_REV = ['revised unit cost','unit cost - revised','new unit cost','unit cost (revised)']\n\n\ndef _find_cost_sheet(xls):\n    for s in xls.sheet_names:\n        sl = s.lower()\n        if any(k in sl for k in ['cost','budget','comparison','summary']):\n            return s\n    return xls.sheet_names[0]\n\n\ndef _nearest_number(row):\n    # Return first numeric candidate in the row\n    for v in row:\n        try:\n            if v is None:\n                continue\n            if isinstance(v, (int,float)) and pd.notna(v):\n                return float(v)\n            if isinstance(v, str):\n                # strip currency and percent\n                vs = v.replace(',','')\n                vs = re.sub(r'[$%]','',vs)\n                if re.fullmatch(r\"[-+]?\\d*\\.?\\d+\", vs):\n                    return float(vs)\n        except Exception:\n            continue\n    return None\n\n\ndef _scan_for_label(df, labels):\n    labels = [l.lower() for l in labels]\n    for _, row in df.iterrows():\n        row_low = [str(x).lower() for x in row]\n        if any(any(lbl in cell for lbl in labels) for cell in row_low):\n            val = _nearest_number(row)\n            if val is not None:\n                return val\n    return None\n\n\ndef evaluate(workflow, context):\n    # Locate spreadsheet\n    xl = None\n    for r in context.get_all_outputs():\n        if getattr(r, 'is_spreadsheet', False):\n            xl = r\n            break\n    if not xl:\n        return 0.0, \"No spreadsheet\"\n    try:\n        path = context.files.get_path(xl.id)\n        xls = pd.ExcelFile(path)\n        sheet = _find_cost_sheet(xls)\n        df = pd.read_excel(path, sheet_name=sheet, header=None)\n        orig = _scan_for_label(df, KEYS_ORIG)\n        rev = _scan_for_label(df, KEYS_REV)\n        if orig is None or rev is None:\n            return 0.3 if (orig is not None or rev is not None) else 0.0, f\"Found orig={orig}, rev={rev}\"\n        delta = rev - orig\n        score = 0.0\n        # Basic monotonicity\n        if rev >= orig:\n            score += 0.5\n        # Multiple of $0.25 within tolerance\n        if delta >= 0:\n            if delta == 0:\n                mult_ok = False\n            else:\n                m = delta / 0.25\n                mult_ok = abs(m - round(m)) <= 0.1 and delta <= 25.0\n            if mult_ok:\n                score += 0.5\n        return score, f\"orig={orig}, rev={rev}, delta={delta}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Units incorporate overage for Final stores (consistency)", "description": "Check that Units Needed (Final) approximately equals Final Store Count \u00d7 (1 + Overage), within rounding tolerance.", "weight": 0.7, "code": "import re\nimport math\nimport pandas as pd\n\nLABELS_OVERAGE = ['overage','backup','breakage','spoilage']\nLABELS_UNITS_FINAL = ['units needed (final)','total units (final)','final units','revised units']\nLABELS_UNITS_ORIG = ['units needed (original)','total units (original)','original units']\nLABELS_STORE_FINAL = ['final store count','store count (final)','final stores']\nLABELS_STORE_ORIG = ['original store count','store count (original)','original stores']\n\n\ndef _find_cost_sheet(xls):\n    for s in xls.sheet_names:\n        sl = s.lower()\n        if any(k in sl for k in ['cost','budget','comparison','summary']):\n            return s\n    return xls.sheet_names[0]\n\n\ndef _scan_for_label(df, labels):\n    labels = [l.lower() for l in labels]\n    for _, row in df.iterrows():\n        row_low = [str(x).lower() for x in row]\n        if any(any(lbl in cell for lbl in labels) for cell in row_low):\n            # pick the rightmost numeric\n            nums = []\n            for v in row:\n                if pd.isna(v):\n                    continue\n                if isinstance(v, (int,float)):\n                    nums.append(float(v))\n                elif isinstance(v, str):\n                    vs = v.replace(',','')\n                    vs = re.sub(r'[$%]','',vs)\n                    if re.fullmatch(r\"[-+]?\\d*\\.?\\d+\", vs):\n                        nums.append(float(vs))\n            if nums:\n                return nums[-1]\n    return None\n\n\ndef _find_final_store_sheet(xls):\n    for s in xls.sheet_names:\n        sl = s.lower()\n        if (('final' in sl and 'store' in sl) or any(k in sl for k in ['store list','stores'])):\n            return s\n    return None\n\n\ndef evaluate(workflow, context):\n    xl = None\n    for r in context.get_all_outputs():\n        if getattr(r, 'is_spreadsheet', False):\n            xl = r\n            break\n    if not xl:\n        return 0.0, \"No spreadsheet\"\n    try:\n        path = context.files.get_path(xl.id)\n        xls = pd.ExcelFile(path)\n        cost_sheet = _find_cost_sheet(xls)\n        df = pd.read_excel(path, sheet_name=cost_sheet, header=None)\n        overage = _scan_for_label(df, LABELS_OVERAGE)\n        units_final = _scan_for_label(df, LABELS_UNITS_FINAL)\n        # Prefer store count from cost sheet; fallback to final store sheet row count\n        store_final = _scan_for_label(df, LABELS_STORE_FINAL)\n        if store_final is None:\n            final_sheet = _find_final_store_sheet(xls)\n            if final_sheet:\n                sdf = pd.read_excel(path, sheet_name=final_sheet)\n                # Heuristic: count non-null Store ID-like column, else count rows\n                cols = [c for c in sdf.columns]\n                store_cols = [c for c in cols if str(c).lower() in ['store id','store','store number','store #','id'] or 'store' in str(c).lower()]\n                if store_cols:\n                    store_final = int(pd.to_numeric(sdf[store_cols[0]], errors='coerce').notna().sum())\n                else:\n                    store_final = int(len(sdf.index))\n        if overage is None or units_final is None or store_final is None:\n            # partial credit if at least 2 of 3 are present\n            present = sum(x is not None for x in [overage, units_final, store_final])\n            return 0.2 * present / 3.0, f\"overage={overage}, units_final={units_final}, store_final={store_final}\"\n        # overage could be given as percent (e.g., 5) or decimal (0.05); normalize\n        ov = overage\n        if ov > 1.0:\n            ov = ov / 100.0\n        expected_units = store_final * (1.0 + ov)\n        # allow rounding differences\n        if expected_units == 0:\n            return 0.0, \"Zero expected units\"\n        rel_err = abs(units_final - expected_units) / max(1.0, expected_units)\n        score = 1.0 if (rel_err <= 0.02 or abs(units_final - expected_units) <= 1.0) else (0.5 if rel_err <= 0.05 else 0.0)\n        return score, f\"overage={overage}, store_final={store_final}, units_final={units_final}, expected\u2248{expected_units:.2f}, rel_err={rel_err:.4f}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Final Store List flags new stores", "description": "Check the Final Store List includes a column or values flagging added/new stores (e.g., Change Type/Status with 'Added').", "weight": 0.5, "code": "import re\nimport pandas as pd\n\ndef _find_final_store_sheet(xls):\n    for s in xls.sheet_names:\n        sl = s.lower()\n        if (('final' in sl and 'store' in sl) or any(k in sl for k in ['store list','stores'])):\n            return s\n    return None\n\n\ndef evaluate(workflow, context):\n    xl = None\n    for r in context.get_all_outputs():\n        if getattr(r, 'is_spreadsheet', False):\n            xl = r\n            break\n    if not xl:\n        return 0.0, \"No spreadsheet\"\n    try:\n        path = context.files.get_path(xl.id)\n        xls = pd.ExcelFile(path)\n        sheet = _find_final_store_sheet(xls)\n        if not sheet:\n            return 0.0, \"No final store sheet\"\n        df = pd.read_excel(path, sheet_name=sheet)\n        cols = [str(c).strip() for c in df.columns]\n        low = [c.lower() for c in cols]\n        # look for a change/status/new/added column\n        target_idx = None\n        for i,c in enumerate(low):\n            if any(k in c for k in ['change','status','added','new','delta','flag']):\n                target_idx = i\n                break\n        if target_idx is None:\n            return 0.3, \"No explicit change/status column found\"\n        col = df.columns[target_idx]\n        ser = df[col].astype(str).str.lower()\n        has_added = ser.str.contains('add').any() or ser.isin(['true','1','yes','y']).any()\n        return (1.0 if has_added else 0.5), f\"Change column '{col}', has_added={has_added}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "llm_judge", "name": "Store Add/Remove Cross-Reference (incl. Stores 4099 & 3737)", "description": "LLM cross-check that added/removed stores are clearly enumerated and that example stores 4099 and 3737 are explicitly addressed as requested.", "weight": 3.5, "judge_prompt": "Using the Excel workbook only (do not invent facts), verify that:\n- The comparison identifies which stores were added and which were removed between the original and final lists. This can be a dedicated section/table or a clear list with counts and enumerations.\n- Stores 4099 and 3737 (mentioned in the prompt) are explicitly checked: confirm whether each appears on the final list or is marked removed.\nScoring:\n- 1.0: Clear added/removed delineation and explicit confirmation for both 4099 and 3737\n- 0.7: Clear added/removed delineation but only one of 4099/3737 explicitly addressed\n- 0.4: Partial/implicit delineation (e.g., only counts) with no explicit mention of 4099/3737\n- 0.0: No usable add/remove delineation", "expectation": "A table/list showing added and removed stores, plus explicit confirmation of 4099 and 3737 status."}, {"type": "llm_judge", "name": "Overage reuse and cost recomputation logic", "description": "LLM verifies that the same overage % was reused and that cost recomputation steps correctly incorporate: (a) final store count, (b) overage, and (c) $0.25 per shelf strip increase into unit and total costs.", "weight": 3.0, "judge_prompt": "Read the Excel cost/budget sheet. Confirm the following are documented and consistent:\n- The overage % used for revised scenario matches the original overage % (i.e., same value reused)\n- Revised Units = Final Store Count \u00d7 (1 + overage %) (allow standard rounding)\n- Revised Unit Cost reflects the $0.25 per shelf strip increase (showing either a component delta or a direct unit delta consistent with shelf strips per unit)\n- Revised Total Program Cost = Revised Unit Cost \u00d7 Revised Units\nScoring:\n- 1.0: All four checks are clearly evidenced in the sheet(s)\n- 0.7: Three checks clear\n- 0.4: Two checks clear\n- 0.2: One check clear\n- 0.0: None clear", "expectation": "Clear, step-by-step recomputation using identical overage %, updated final store count, and $0.25/strip increase."}, {"type": "llm_judge", "name": "Email-to-Workbook Numeric Alignment", "description": "LLM cross-check that the email\u2019s key numbers match the workbook\u2019s Program Summary.", "weight": 2.0, "judge_prompt": "Compare the email draft against the Excel Program Summary numbers. Check alignment of:\n- Updated number of floor stands (final units)\n- Change in budget ($ delta and/or %)\n- New total program budget (revised total)\nGrant tolerance for minor rounding only.\nScoring:\n- 1.0: All three match (or are within rounding)\n- 0.7: Two match\n- 0.4: One matches\n- 0.0: None match or cannot be found", "expectation": "Email numbers mirror the Excel Program Summary."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Holistic Quality & Communication", "description": "Assess professionalism, clarity, and stakeholder readiness of the deliverables.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional presentation (Excel + Email)", "description": "Evaluate formatting, readability, and professional tone.", "weight": 2.5, "judge_prompt": "Assess overall presentation quality:\n- Excel: clean headers, consistent currency/percent formats, freeze panes/filters where appropriate, clear sectioning\n- Email: concise, professional tone; clear subject; actionable ask/next steps; correct grammar/spelling\nScoring: 1.0 excellent; 0.7 good; 0.4 fair; 0.2 poor; 0.0 unacceptable", "expectation": "Polished, professional deliverables ready for stakeholder circulation."}, {"type": "llm_judge", "name": "Clarity and traceability of assumptions", "description": "Judge whether a reader can audit the work easily.", "weight": 2.0, "judge_prompt": "Consider whether the workbook makes it easy to trace numbers:\n- Assumptions table complete with sources/notes\n- Clear labels linking assumptions to calculations\n- Components of unit cost visible (e.g., shelf strip component) and delta rationale clear\nScoring: 1.0 high clarity; 0.7 moderate; 0.4 limited; 0.0 unclear", "expectation": "Assumptions and calculations clearly linked and annotated."}, {"type": "llm_judge", "name": "Stakeholder appropriateness and actionability", "description": "Does the deliverable enable timely decision-making before vendor production starts?", "weight": 1.5, "judge_prompt": "Evaluate whether the materials are suited for Sales/Production leaders:\n- Explicit summary of risks/impact of higher store count and cost increase\n- Clear approval/decision ask and production deadline timing\n- Suggests next steps/mitigations (e.g., confirm budget, adjust PO quantities)\nScoring: 1.0 strong; 0.7 adequate; 0.4 weak; 0.0 missing", "expectation": "Audience-appropriate with clear decisions and next steps."}, {"type": "llm_judge", "name": "Accuracy of change highlighting in Final Store List", "description": "Holistic check that added stores are easy to identify and the sheet is usable.", "weight": 2.0, "judge_prompt": "On the Final Store List sheet, assess:\n- Are added/new stores unambiguously highlighted (column, flag, or formatting)?\n- Is the store list tidy (consistent IDs, no obvious duplicates, sortable columns)?\nScoring: 1.0 strong; 0.7 adequate; 0.4 weak; 0.0 poor", "expectation": "A clean, usable final store list with clear added-store highlighting."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "7de33b48-5163-4f50-b5f3-8deea8185e57", "rubric": {"category_name": "Accessibility Utility (ScreenReaderStatusMessage)", "rationale": "This rubric enforces a self-documenting, file-based submission: a single ZIP containing a React/TypeScript utility, tests, CSS, package.json, and README. Stage 1 (LLM-only) strictly gates on correct archive shape so later verification is trivial. Stage 2 mixes small, deterministic code checks (structure, presence of key attributes, test scaffolding) with higher-weight LLM judgments for nuanced WCAG ARIA22 and behavior (queuing, visible prop semantics). Stage 3 holistically assesses documentation, API usability, robustness, and packaging quality.", "max_total_score": 20.0, "stages": [{"name": "Stage 1: Archive Shape and Contents Gate", "description": "LLM-only validation that the primary output is a ZIP package containing the exact required files and basic structural elements to enable verification.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.0, "rules": [{"type": "llm_judge", "name": "Zip Package and Required Files Present", "description": "Check that the output is a ZIP archive containing all required files with correct types and names (exact filenames, case-insensitive, may be in root or subfolder).", "weight": 4.0, "judge_prompt": "You are validating the SHAPE of the output only. Inspect the candidate's primary output. Determine if it is a ZIP archive that contains the following files (in any directory inside the zip, names may be case-insensitive but must clearly match intent):\n\nRequired files:\n- ScreenReaderStatusMessage.tsx (TypeScript React utility)\n- ScreenReaderStatusMessage.test.tsx (TypeScript test file using React Testing Library and Sinon)\n- ScreenReaderStatusMessage.css (CSS with a visually-hidden class)\n- package.json (NPM package file)\n- README.md (usage and test instructions)\n\nScoring:\n- 1.0: ZIP present AND all five required files present (filenames match intent)\n- 0.7: ZIP present AND 4/5 required files present\n- 0.4: ZIP present AND 3/5 required files present\n- 0.2: ZIP present AND 1-2 required files present\n- 0.0: Not a ZIP OR contains none of the required files\n\nImportant: Only check for presence and basic file types/names. Do not evaluate code correctness or content quality.", "expectation": "A single ZIP containing all five required files, discoverable at any depth."}, {"type": "llm_judge", "name": "Basic File Structure and Headers Present", "description": "Check that core files show basic, readable structure sufficient to proceed with verification (not correctness).", "weight": 2.0, "judge_prompt": "Open the files inside the ZIP (do not judge correctness, only structure/presence of key surface cues):\n- ScreenReaderStatusMessage.tsx: Should contain a React component definition with the identifier name \"ScreenReaderStatusMessage\" and an export (export default or named export). Presence of TSX syntax is sufficient.\n- ScreenReaderStatusMessage.test.tsx: Should import React Testing Library (e.g., '@testing-library/react') and Sinon (e.g., 'sinon'). Should contain at least 4 test blocks (it(...) or test(...)), suggesting tests for: role=\"status\" container exists prior to message, message appears within container when triggered, equivalent info (e.g., image alt) resides in container, and visible prop behavior.\n- ScreenReaderStatusMessage.css: Contains at least one class selector that looks intended for visually hiding content (e.g., '.visually-hidden', '.sr-only').\n- package.json: Must be valid JSON with a \"name\" and a \"scripts\" section.\n- README.md: Contains visible section headers and mentions how to install and run tests.\n\nScoring:\n- 1.0: All listed structural cues present across the files\n- 0.6: Missing one of the structural cues\n- 0.3: Missing two structural cues\n- 0.0: Missing three or more cues or unreadable files\n\nOnly validate presence of these cues, not their correctness or depth.", "expectation": "Each file is syntactically plausible for its role and includes minimal structural cues."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Correctness and Compliance Verification", "description": "Mixed verification of ARIA22 alignment, semantics, and basic project viability. Code checks do deterministic validations; LLM reviews nuanced behavior (queueing, visible prop semantics, ARIA22 conformance).", "is_required": false, "max_points": 10.0, "min_score_to_pass": 5.0, "rules": [{"type": "code", "name": "TSX contains status container semantics", "description": "Verify ScreenReaderStatusMessage.tsx includes a role=\"status\" container (or equivalent aria-live usage) likely rendered before messages occur.", "weight": 0.5, "code": "import re, json, io, zipfile\n\ndef _open_zip(context):\n    output = context.get_primary_output()\n    if not output:\n        return None\n    try:\n        p = context.files.get_path(output.id)\n    except Exception:\n        return None\n    try:\n        zf = zipfile.ZipFile(p, 'r')\n        return zf\n    except Exception:\n        return None\n\n\ndef _read_from_zip(zf, wanted_names):\n    # wanted_names: list of filenames to match by endswith (case-insensitive)\n    wanted_lower = [w.lower() for w in wanted_names]\n    for name in zf.namelist():\n        lower = name.lower()\n        for w in wanted_lower:\n            if lower.endswith(w):\n                try:\n                    with zf.open(name) as f:\n                        return f.read().decode('utf-8', errors='ignore')\n                except Exception:\n                    return None\n    return None\n\n\ndef evaluate(workflow, context):\n    zf = _open_zip(context)\n    if not zf:\n        return 0.0, 'No ZIP or unreadable archive.'\n    tsx = _read_from_zip(zf, ['ScreenReaderStatusMessage.tsx'])\n    if not tsx:\n        return 0.0, 'Missing ScreenReaderStatusMessage.tsx.'\n    text = tsx.lower()\n    score = 0.0\n    # Check for role=\"status\" or role={'status'}\n    if re.search(r\"role\\s*=\\s*[{\\\"]\\s*status\\s*[}\\\"]\", text):\n        score += 0.6\n    # Allow aria-live polite/assertive as alternative\n    if 'aria-live' in text:\n        score += 0.2\n    # Check container likely always rendered (presence of role outside conditional render logic impossible to ensure statically)\n    # Heuristic: component name and return JSX with role attribute\n    if ('screenreaderstatusmessage' in text) and ('return' in text) and ('role' in text):\n        score += 0.2\n    return min(score, 1.0), f'semantics_score={score:.2f}'"}, {"type": "code", "name": "CSS visually hidden pattern", "description": "Verify CSS includes a visually-hidden pattern that hides visually but remains available to assistive tech.", "weight": 0.5, "code": "import re, zipfile\n\ndef _open_zip(context):\n    output = context.get_primary_output()\n    if not output:\n        return None\n    try:\n        p = context.files.get_path(output.id)\n    except Exception:\n        return None\n    try:\n        return zipfile.ZipFile(p, 'r')\n    except Exception:\n        return None\n\n\ndef _read_from_zip(zf, wanted_names):\n    wanted_lower = [w.lower() for w in wanted_names]\n    for name in zf.namelist():\n        lower = name.lower()\n        for w in wanted_lower:\n            if lower.endswith(w):\n                try:\n                    with zf.open(name) as f:\n                        return f.read().decode('utf-8', errors='ignore')\n                except Exception:\n                    return None\n    return None\n\n\ndef evaluate(workflow, context):\n    zf = _open_zip(context)\n    if not zf:\n        return 0.0\n    css = _read_from_zip(zf, ['ScreenReaderStatusMessage.css'])\n    if not css:\n        return 0.0\n    t = css.lower()\n    # Look for typical visually-hidden rules\n    cues = 0\n    for cue in ['position: absolute', 'clip:', 'clip-path', 'height: 1px', 'width: 1px', 'overflow: hidden', 'white-space: nowrap', 'margin: -1px']:\n        if cue in t:\n            cues += 1\n    # Normalize to [0,1] with 4 cues = full credit, cap at 1.0\n    score = min(cues / 4.0, 1.0)\n    return score, f'css_cues={cues}'"}, {"type": "code", "name": "Tests reference ARIA22 checks and visible prop", "description": "Verify tests import React Testing Library and Sinon, and include cues for the 4 required checks.", "weight": 0.6, "code": "import re, zipfile\n\ndef _open_zip(context):\n    output = context.get_primary_output()\n    if not output:\n        return None\n    try:\n        p = context.files.get_path(output.id)\n    except Exception:\n        return None\n    try:\n        return zipfile.ZipFile(p, 'r')\n    except Exception:\n        return None\n\n\ndef _read_from_zip(zf, wanted_names):\n    wanted_lower = [w.lower() for w in wanted_names]\n    for name in zf.namelist():\n        lower = name.lower()\n        for w in wanted_lower:\n            if lower.endswith(w):\n                try:\n                    with zf.open(name) as f:\n                        return f.read().decode('utf-8', errors='ignore')\n                except Exception:\n                    return None\n    return None\n\n\ndef evaluate(workflow, context):\n    zf = _open_zip(context)\n    if not zf:\n        return 0.0\n    test = _read_from_zip(zf, ['ScreenReaderStatusMessage.test.tsx'])\n    if not test:\n        return 0.0\n    t = test.lower()\n    checks = 0\n    # Imports\n    if ('@testing-library/react' in t) or ('testing-library/react' in t):\n        checks += 1\n    if 'sinon' in t:\n        checks += 1\n    # Four key cues\n    if 'getbyrole' in t and 'status' in t:\n        checks += 1\n    if 'alt' in t or 'getbyalttext' in t:\n        checks += 1\n    if 'aria-hidden' in t:\n        checks += 1\n    if 'visible' in t:  # visible prop usage in tests\n        checks += 1\n    # At least 4 test blocks\n    test_blocks = len(re.findall(r'\\b(it|test)\\s*\\(', t))\n    if test_blocks >= 4:\n        checks += 1\n    # Normalize: 7 cues total -> 1.0\n    score = min(checks / 7.0, 1.0)\n    return score, f'checks={checks}, test_blocks={test_blocks}'"}, {"type": "code", "name": "package.json scripts and deps for testing", "description": "Verify package.json includes a test script and references to testing libraries (RTL and Sinon) in deps or devDeps.", "weight": 0.4, "code": "import json, zipfile\n\ndef _open_zip(context):\n    output = context.get_primary_output()\n    if not output:\n        return None\n    try:\n        p = context.files.get_path(output.id)\n    except Exception:\n        return None\n    try:\n        return zipfile.ZipFile(p, 'r')\n    except Exception:\n        return None\n\n\ndef _read_from_zip(zf, wanted_names):\n    wanted_lower = [w.lower() for w in wanted_names]\n    for name in zf.namelist():\n        lower = name.lower()\n        for w in wanted_lower:\n            if lower.endswith(w):\n                try:\n                    with zf.open(name) as f:\n                        return f.read().decode('utf-8', errors='ignore')\n                except Exception:\n                    return None\n    return None\n\n\ndef evaluate(workflow, context):\n    zf = _open_zip(context)\n    if not zf:\n        return 0.0\n    pj = _read_from_zip(zf, ['package.json'])\n    if not pj:\n        return 0.0\n    try:\n        data = json.loads(pj)\n    except Exception:\n        return 0.0\n    score = 0.0\n    scripts = data.get('scripts', {}) or {}\n    if any(k.lower()=='test' for k in scripts.keys()):\n        score += 0.5\n    deps = {}\n    for k in ['dependencies','devDependencies','peerDependencies']:\n        d = data.get(k, {}) or {}\n        deps.update({(kk or '').lower(): True for kk in d.keys()})\n    if any(x in deps for x in ['@testing-library/react','@testing-library/jest-dom','sinon','jest','vitest']):\n        score += 0.5\n    return min(score, 1.0), f'scripts={list(scripts.keys())}'"}, {"type": "llm_judge", "name": "ARIA22 Technique Compliance (3 checks)", "description": "Evaluate whether the code and tests substantively satisfy ARIA22:\n1) Status container role present before message; 2) When triggered, message is inside container; 3) Equivalent info (e.g., image alt text) resides in the container. Only verify against the code/tests provided.", "weight": 3.0, "judge_prompt": "Read ScreenReaderStatusMessage.tsx and ScreenReaderStatusMessage.test.tsx from the ZIP. Based on code and tests, judge if the solution substantively satisfies WCAG Technique ARIA22:\n1) There is a container intended to hold status messages that has role=\"status\" (or equivalent polite live region) BEFORE any status message occurs.\n2) When a status message is triggered, the message content is placed inside that container.\n3) Elements/attributes that provide equivalent information to the visual experience (e.g., an image with useful alt text) also reside inside the container when used to convey status.\n\nScore guidance:\n- 1.0: All three are clearly and correctly implemented and covered in tests.\n- 0.7: Two of three clearly implemented and tested, third is implied but weakly covered.\n- 0.4: Only one check is solid or coverage is mostly incomplete.\n- 0.0: Not demonstrated.\nProvide brief justification referencing specific code/test cues.", "expectation": "Clear role=\"status\" container present early; messages injected into it; equivalents (e.g., alt text) included within same container; tests assert these."}, {"type": "llm_judge", "name": "Queueing and Non-Interference Behavior", "description": "The utility should allow multiple messages from different parts of the page to queue and not interfere with each other. Assess whether the design and code achieve this intent.", "weight": 2.5, "judge_prompt": "Assess ScreenReaderStatusMessage.tsx for evidence of message queueing and non-interference among multiple concurrent updates:\n- Does the utility maintain a queue or sequencing mechanism (e.g., internal queue, timestamped updates, alternating live regions) so that rapid or concurrent updates from different panels are announced in order without being overwritten?\n- Does the design minimize duplication and ensure each update is communicated once?\n- Is the approach compatible with common screen readers (e.g., uses role=\"status\"/aria-live with small delays or other known patterns)?\n\nScore guidance:\n- 1.0: Robust queueing/sequencing strategy clearly implemented with non-interference across instances or shared manager; comments/tests support this.\n- 0.7: Partial but plausible approach (e.g., alternating regions or simple queue) with some gaps.\n- 0.4: Minimal handling (last-write-wins) that risks interference.\n- 0.0: No discernible handling for sequencing or concurrency.\nProvide concise reasoning with references to code sections.", "expectation": "A deliberate, documented mechanism that sequences announcements without cross-panel interference."}, {"type": "llm_judge", "name": "Visible Prop Accessibility Semantics", "description": "Evaluate the special-case 'visible' prop behavior: render a sibling visible to sighted users immediately but hidden from the accessibility tree to avoid duplicate announcements and delay.", "weight": 2.5, "judge_prompt": "Evaluate how the 'visible' prop is implemented in the TSX and covered in tests:\n- When visible={true}, does the component render a sibling element that displays the message immediately (no live-region delay), while that visible element is hidden from the accessibility tree (e.g., aria-hidden=\"true\") to prevent duplicate announcements?\n- Does the hidden live-region (role=\"status\" or aria-live) still receive the message appropriately for assistive technologies?\n- Are tests verifying that wrapping existing text with visible={true} preserves visual layout (no visible side-effects) while avoiding announcement duplication?\n\nScore guidance:\n- 1.0: All aspects clearly implemented and tested.\n- 0.7: Mostly implemented; minor gaps in tests or semantics.\n- 0.4: Some attempt but risks duplication or hides from SR inappropriately.\n- 0.0: Not implemented.\nCite relevant code/test snippets in your reasoning.", "expectation": "visible sibling rendered with aria-hidden to users of assistive tech, while SR live region receives the message for announcement."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3: Quality and Professionalism Assessment", "description": "Holistic LLM assessment of API usability, documentation, robustness for enterprise scale, and packaging quality.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 2.0, "rules": [{"type": "llm_judge", "name": "API Usability and Documentation Quality", "description": "Assess the component\u2019s API clarity (props/types), examples, and README guidance to install, run tests, and integrate into React+TS apps.", "weight": 1.5, "judge_prompt": "Review ScreenReaderStatusMessage.tsx for clear TypeScript types, prop documentation (especially message type, visible prop, optional settings), and usage examples in README.md. Evaluate whether instructions to install dependencies and run tests are complete and accurate. Prefer concise examples and clear prop tables or sections.", "expectation": "Well-typed props, succinct examples, and accurate, step-by-step instructions for install and testing."}, {"type": "llm_judge", "name": "Code Quality and Maintainability", "description": "Assess code organization, comments, naming, and test clarity suitable for enterprise use.", "weight": 1.0, "judge_prompt": "Evaluate code readability (consistent naming, modularity), comments explaining non-obvious ARIA/live-region patterns, and test code clarity (arrange-act-assert, targeted assertions). Consider TypeScript strictness and minimal tech debt.", "expectation": "Readable, maintainable TSX with comments for ARIA decisions and clean tests."}, {"type": "llm_judge", "name": "WCAG Robustness Beyond Minimum", "description": "Consider whether the design anticipates cross-AT behavior and edge cases (polite vs assertive, aria-atomic, debouncing).", "weight": 1.0, "judge_prompt": "Assess whether the solution goes beyond minimal ARIA22 by considering aria-atomic, polite/assertive tradeoffs, throttling/debouncing rapid updates, and cross-browser/screen-reader quirks. Look for notes in code/README and sensible defaults.", "expectation": "Thoughtful defaults and notes indicating awareness of cross-AT nuances and rapid update handling."}, {"type": "llm_judge", "name": "Packaging and Project Hygiene", "description": "Evaluate package.json fields, scripts, and repository hygiene elements.", "weight": 0.5, "judge_prompt": "Check package.json for coherent name/version, license, scripts (build/test/lint if applicable), and that README aligns with these scripts. Minor extras (e.g., tsconfig, eslint) are a plus if present in the archive.", "expectation": "Professional, coherent packaging; scripts match README; optional config files are consistent."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "90f37ff3-e4ed-4a0b-94bb-bed0f7def1ef", "rubric": {"category_name": "Lease Rate Analysis Report - Commercial Retail (Miami Gardens, FL)", "rationale": "This rubric enforces a self-documenting, presentation-ready 4-page PDF report with a structured Market Rent Survey and Lease Rate Recommendation. Stage 1 (LLM-only) mandates the exact document structure to make verification trivial. Stage 2 mixes lightweight code checks (pattern/bounds) with higher-weight LLM checks for comparables validity, proximity, and recency. Stage 3 provides a holistic professional quality assessment for landlord-ready decision support.", "max_total_score": 11.2, "stages": [{"name": "Stage 1: Structure & Format Gate (LLM-only)", "description": "Gate: Verify the output is a 4-page (\u00b11 page) PDF with required sections and tables to enable verification.", "is_required": true, "max_points": 1.0, "min_score_to_pass": 0.7, "rules": [{"type": "llm_judge", "name": "Document Format and Structural Completeness", "description": "Check the report is a PDF with 4 pages (acceptable range: 3\u20135) and includes all required sections with the specified structure.", "weight": 1.0, "judge_prompt": "You are evaluating if the candidate output satisfies STRICT structure requirements for a Lease Rate Analysis Report. Only check format and presence/structure, not correctness of content.\n\nRequired OUTPUT FORMAT:\n- File must be a PDF (not Word, not Excel). \n- Page count target: 4 pages. Acceptable: 3\u20135 pages. \n- Professional layout with visible section headers.\n\nRequired SECTIONS and STRUCTURE (be flexible on exact header wording but headers must be visible):\n\n1) Subject Property Summary (on Page 1):\n   - Contains the subject identifier (address or name) in Miami Gardens, FL area\n   - Mentions the suite size: 2,225 SF (may appear as 2,225 sf / 2225 sf / 2.225 ks.f. formats)\n\n2) Market Rent Survey (table required):\n   - 3\u20136 comparable retail spaces within ~3 miles (within last 3 years)\n   - One table listing comps with columns/fields that cover at least: Property/Center Name; Address/City; Distance from Subject (miles); Suite Size (SF); Asking Rent ($/SF/yr or PSF); Data Source/Platform (e.g., LoopNet, Crexi) and/or Date Observed\n   - The table must be clearly rendered as a table with rows for each comparable\n\n3) Lease Rate Recommendation:\n   - Clearly presents a recommended rent RANGE per square foot (e.g., $X\u2013$Y PSF per year) and lease type if provided (e.g., NNN/Gross)\n   - Brief bullet justification referencing the surveyed comps\n\n4) Methodology & Data Sources (can be a brief section):\n   - Notes data sources (e.g., LoopNet/Crexi/public listings) and basic selection criteria (radius/timeframe)\n\nOptional (credit but not required): A small locator map or visual (map/table/chart) that situates the subject or comps.\n\nScoring:\n- 1.0: PDF + exactly 4 pages + all 4 required sections present with correct structure (Market Survey as a table with 3\u20136 comps) \n- 0.9: PDF + 3\u20135 pages but not exactly 4 + all required sections present\n- 0.8: PDF + 3\u20135 pages + one minor structural gap (e.g., Market Survey present but missing 1 supporting field like Distance or Date; or Methodology present but minimal)\n- 0.5: PDF + 3\u20135 pages but missing one required section OR Market Survey table not clearly tabular\n- 0.0: Not a PDF OR fewer than 3 pages OR missing multiple required sections OR Market Survey missing\n\nOnly assess structure and formatting presence. Do not judge accuracy or quality here.", "expectation": "A 4-page PDF with: Subject Summary (incl. 2,225 SF), a Market Rent Survey table of 3\u20136 comps with key fields, a Lease Rate Recommendation with range PSF and brief justification, and a Methodology & Sources section."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Verification - Comparables, Constraints, and Grounding", "description": "Now that structure is correct, verify key elements are present and reasonable. Mix of code rules (deterministic pattern checks) and LLM rules (comparables adherence to proximity/recency and grounding of recommendation).", "is_required": false, "max_points": 6.2, "min_score_to_pass": 3.1, "rules": [{"type": "code", "name": "Explicit Recommended Rent Range Present (PSF)", "description": "Detect a recommended rent range per square foot (e.g., $X\u2013$Y PSF/yr). Award partial credit if only a single recommended rate is found.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    # Read text from PDF/DOCX\n    text = ''\n    try:\n        if str(output.id).lower().endswith('.pdf'):\n            text = context.files.read_pdf_text(output.id)\n        else:\n            text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                return 0.0\n    if not text:\n        return 0.0\n    tlow = text.lower()\n\n    # Look for PSF context\n    psf_terms = ['psf', '/sf', 'per sf', 'per square foot', '$/sf', '$/sf/yr', 'per sf/yr']\n\n    # Range like $X - $Y or $X to $Y\n    range_pattern = re.compile(r\"\\$?\\s*([0-9]{1,3}(?:,[0-9]{3})*(?:\\.[0-9]{1,2})?)\\s*(?:-|\u2013|\u2014|to)\\s*\\$?\\s*([0-9]{1,3}(?:,[0,9]{3})*(?:\\.[0-9]{1,2})?)\", re.IGNORECASE)\n\n    # Correct a small typo in regex for thousands groups\n    range_pattern = re.compile(r\"\\$?\\s*([0-9]{1,3}(?:,[0-9]{3})*(?:\\.[0-9]{1,2})?)\\s*(?:-|\u2013|\u2014|to)\\s*\\$?\\s*([0-9]{1,3}(?:,[0-9]{3})*(?:\\.[0-9]{1,2})?)\", re.IGNORECASE)\n\n    found_range = False\n    plausible = False\n\n    for m in range_pattern.finditer(text):\n        start, end = m.span()\n        window = tlow[max(0, start-80):min(len(tlow), end+80)]\n        if any(term in window for term in psf_terms):\n            try:\n                low = float(m.group(1).replace(',', ''))\n                high = float(m.group(2).replace(',', ''))\n                if low < high:\n                    found_range = True\n                    # Plausible Miami-Dade retail asking PSF/yr bounds (rough): 8 to 200\n                    if 8 <= low <= 200 and 8 <= high <= 200:\n                        plausible = True\n                    break\n            except Exception:\n                continue\n\n    if found_range and plausible:\n        return 0.5\n    if found_range:\n        return 0.35\n\n    # Check for single recommended rate\n    single_pattern = re.compile(r\"recommended[^\\n\\r$]*\\$\\s*([0-9]{1,3}(?:,[0-9]{3})?(?:\\.[0-9]{1,2})?).{0,40}(psf|/sf|per sf|per square foot|\\$/sf|\\$/sf/yr)\", re.IGNORECASE)\n    if single_pattern.search(text):\n        return 0.25\n\n    # Broader single PSF figure near 'recommend'\n    single2 = re.compile(r\"\\$\\s*([0-9]{1,3}(?:,[0-9]{3})?(?:\\.[0-9]{1,2})?).{0,60}(psf|/sf|per sf|per square foot).{0,60}(recommend|list|ask)\", re.IGNORECASE)\n    if single2.search(text):\n        return 0.2\n\n    return 0.0"}, {"type": "code", "name": "Subject Suite Size Mentioned (2,225 SF)", "description": "Verify the report mentions the subject suite size approximately 2,225 SF.", "weight": 0.3, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        if str(output.id).lower().endswith('.pdf'):\n            text = context.files.read_pdf_text(output.id)\n        else:\n            text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                return 0.0\n    if not text:\n        return 0.0\n    tlow = text.lower()\n\n    # Accept 2225, 2,225 and allow optional spacing; require SF context nearby\n    patterns = [\n        r\"2\\s*,?\\s*225\\s*(sf|sq\\s*ft|square\\s*feet)\",\n        r\"\\b2225\\b\\s*(sf|sq\\s*ft|square\\s*feet)\"\n    ]\n    for p in patterns:\n        if re.search(p, tlow):\n            return 0.3\n\n    # Slight tolerance: 2200-2250 with SF\n    approx = re.compile(r\"\\b(22[0-4][0-9]|2250)\\b\\s*(sf|sq\\s*ft|square\\s*feet)\")\n    if approx.search(tlow):\n        return 0.2\n\n    return 0.0"}, {"type": "code", "name": "Sources Cited (e.g., LoopNet/Crexi/Other)", "description": "Check if at least one recognizable data source/platform is cited.", "weight": 0.4, "code": "def evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = ''\n    try:\n        if str(output.id).lower().endswith('.pdf'):\n            text = context.files.read_pdf_text(output.id)\n        else:\n            text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                return 0.0\n    tlow = (text or '').lower()\n    if not tlow:\n        return 0.0\n\n    keywords = [\n        'loopnet', 'crexi', 'costar', 'reonomy', 'miami-dade property appraiser',\n        'miami dade property appraiser', 'miami-dade gis', 'mls', 'cbre', 'jll', 'colliers',\n        'kres', 'marcus & millichap', 'svn', 'commercialedge', 'propertyshark'\n    ]\n\n    if any(k in tlow for k in keywords):\n        return 0.4\n    return 0.0"}, {"type": "llm_judge", "name": "Comparable Table Completeness", "description": "Verify the Market Rent Survey table lists 3\u20136 comps with address and asking rent filled in, plus at least one of: distance (miles) or city proximity indicator, and shows suite size when available.", "weight": 2.0, "judge_prompt": "Review the Market Rent Survey table(s).\nCheck for:\n- Count of comparables between 3 and 6 inclusive.\n- Each row includes an address (or property name + address/city) and an asking rent (numeric $/SF/yr, PSF, or similar).\n- At least one proximity indicator is present for most comps: distance in miles from subject or obviously same-neighborhood city naming.\n- Suite size (SF) listed for most comps when available.\n\nScoring:\n- 2.0: 3\u20136 comps; all comps have address + numeric asking rent; most have distance or clear proximity; sizes included for most.\n- 1.2: 3\u20136 comps; minor gaps (e.g., 1\u20132 comps missing size or proximity), but addresses + asking rents are present for all.\n- 0.6: 3\u20136 comps but several missing asking rent or address fields; table present but incomplete.\n- 0.0: Fewer than 3 comps or no asking rents visible.", "expectation": "A clear table of 3\u20136 comps with addresses and numeric asking rents; proximity and sizes mostly present."}, {"type": "llm_judge", "name": "Proximity and Radius Adherence (~3 miles)", "description": "Check that comps are plausibly within approximately 3 miles of the subject property in Miami Gardens, FL.", "weight": 1.5, "judge_prompt": "Evaluate whether the comparables are within approximately 3 miles of the subject in Miami Gardens, FL.\n- Prefer explicit distance (miles). If distances not shown, infer via neighborhood/city cues (addresses in Miami Gardens or adjacent neighborhoods).\n- Penalize comps clearly outside the area (> ~5\u20137 miles) unless explicitly justified.\n\nScoring:\n- 1.5: All comps appear within ~3 miles (or clearly close by) or minor justified exceptions.\n- 1.0: Most comps plausibly within ~3 miles; 1 exception without strong justification.\n- 0.5: Multiple comps likely beyond ~3 miles or unclear proximity.\n- 0.0: Comps clearly not local to Miami Gardens area.", "expectation": "Comps are local to Miami Gardens (within ~3 miles) or well-justified nearby alternatives."}, {"type": "llm_judge", "name": "Recency (Last 3 Years) and Date Visibility", "description": "Verify that comp data dates are visible and within the last 3 years (as-of dates, listing timestamps, or notes).", "weight": 1.5, "judge_prompt": "Check whether the Market Rent Survey includes dates (as-of/listed/observed) and that these dates are within the last 3 years.\n- If explicit dates are missing but text indicates recency (e.g., \u2018Q1 2024\u2019, \u2018updated 2023\u2019), treat as valid recency.\n\nScoring:\n- 1.5: Dates visible for most comps and within last 3 years.\n- 1.0: Dates visible for at least half the comps and appear within last 3 years.\n- 0.5: Dates sparsely shown; recency mostly implied but weak.\n- 0.0: No dates or clearly outdated (>3 years).", "expectation": "As-of dates for the comps, with recent observations (<=3 years)."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3: Quality - Professionalism, Insight, and Actionability", "description": "Holistic assessment of presentation quality, clarity of recommendations, strategic value for landlord decision-making, and localization to Miami Gardens.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Presentation Quality and Readiness", "description": "Evaluate polish: clean layout, readable tables, appropriate typography/branding, clear headers, and visual elements (optional map/chart).", "weight": 1.0, "judge_prompt": "Assess presentation polish:\n- Clean, professional formatting; readable fonts; consistent spacing.\n- Tables are properly aligned and legible; numbers formatted (e.g., $/SF/yr).\n- Visual aids (optional) improve clarity without clutter.\n- Overall, is it presentation-ready for a landlord?\n\nScore 0.0\u20131.0 accordingly.", "expectation": "Professional, client-ready formatting with legible tables and consistent styling."}, {"type": "llm_judge", "name": "Actionable and Clear Lease Rate Recommendation", "description": "Quality of the recommendation: clarity of range, lease type (NNN/Gross), and actionable guidance for pricing and negotiation.", "weight": 1.0, "judge_prompt": "Evaluate the recommendation quality:\n- Is the rent range clearly stated and tied to lease type (NNN/Gross) if applicable?\n- Are rationale bullets concise and specific (e.g., anchor co-tenancy, visibility, traffic counts, suite size fit)?\n- Are actionable next steps included (e.g., list at mid-range, floor/ceiling, concessions)?\n\nScore 0.0\u20131.0 based on clarity and actionability.", "expectation": "A crisp range with rationale and next-step guidance suitable for landlord decision-making."}, {"type": "llm_judge", "name": "Analytical Rigor and Grounding in Data", "description": "Judge whether the analysis shows sound reasoning tied to comparables (e.g., adjustments for size, condition, frontage, co-tenancy, lease type).", "weight": 1.0, "judge_prompt": "Assess analytical rigor:\n- Does the narrative connect the recommended range to comp metrics (rent distribution, adjustments for suite size, condition, location, visibility)?\n- Are lease structures (NNN vs Gross) normalized or discussed?\n- Does the analysis avoid claims without support?\n\nScore 0.0\u20131.0.", "expectation": "Reasoned linkage between comps and recommendation, with sensible adjustments/normalization."}, {"type": "llm_judge", "name": "Local Market Relevance and Audience Fit", "description": "Evaluate whether the document reflects Miami Gardens/NE Miami-Dade retail nuances and is tailored to a landlord audience.", "weight": 1.0, "judge_prompt": "Assess localization and audience fit:\n- References to Miami Gardens or adjacent submarkets; plausibility of tenant mix and anchors.\n- Uses terms a landlord expects (asking vs effective rent, concessions, TI, free rent, NNN/CAM).\n- Tone is professional and concise, appropriate for a landlord.\n\nScore 0.0\u20131.0.", "expectation": "Localized, landlord-focused communication with appropriate terminology and tone."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "7bbfcfe9-132d-4194-82bb-d6f29d001b01", "rubric": {"category_name": "Government \u2014 Compliance Officers | SCRA \u00a7\u00a73937 and 3919 Audit Test Questions (Excel)", "rationale": "This rubric enforces a self-documenting, verifiable Excel deliverable containing binary (Yes/No/Not Applicable) audit questions tied to 50 U.S.C. \u00a7\u00a73937 and 3919. Stage 1 is an LLM-only gate that mandates the exact spreadsheet structure (sheet/table, columns, row IDs, citations, and response constraints). Stage 2 mixes lightweight code checks (IDs and response-option integrity) with LLM verification of statute alignment and binary-answerability. Stage 3 assesses professional quality, coverage, and practical usability for regulatory audits. Code rules are weighted ~5x less than LLM rules on average in Stage 2, per guidance.", "max_total_score": 28.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate", "description": "MANDATORY structure check of the Excel deliverable. LLM-only. Verifies the file is an Excel spreadsheet with a single clear question table including the required columns, IDs, citations, and allowed responses (Yes/No/Not Applicable). No calculation/content quality checks here\u2014structure only.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.0, "rules": [{"type": "llm_judge", "name": "Excel Structure and Required Fields Present", "description": "Check that the output is an Excel (.xlsx) file containing exactly one clear table of SCRA audit questions, with required columns and IDs, and citations present per row. Responses must be constrained to Yes/No/Not Applicable via a dedicated column.", "weight": 6.0, "judge_prompt": "You are evaluating whether the candidate\u2019s output satisfies the REQUIRED STRUCTURE for an Excel spreadsheet of SCRA audit questions. Only check structure/presence, not content quality or correctness.\n\nRequirements (be flexible with naming variants but enforce presence):\n- File format: Excel spreadsheet (.xlsx). Not PDF, DOCX, or plain text.\n- Must contain a recognizable tabular sheet with all questions in one place (one sheet preferred; if multiple sheets exist, at least one sheet must contain the complete table below).\n- The table must have columns equivalent to ALL of the following (allow reasonable header variants):\n  1) ID (e.g., \"ID\", \"Question ID\", etc.)\n  2) Statute (should indicate \u00a73937 or \u00a73919)\n  3) Citation (e.g., \"50 U.S.C. \u00a7 3937\" or similar)\n  4) Question (the Yes/No/Not Applicable test prompt)\n  5) Permitted Responses (must state that only Yes/No/Not Applicable are allowed; accept variants like Yes/No/N.A., etc.)\n- Row requirements:\n  \u2022 Exactly 10 total questions in the table.\n  \u2022 IDs must be present and unique as follows:\n    - \u00a73937: SCRA-12a, SCRA-12b, SCRA-12c, SCRA-12d (4 rows)\n    - \u00a73919: SCRA-13, SCRA-14, SCRA-15, SCRA-16, SCRA-17, SCRA-18 (6 rows)\n  \u2022 Each row must include a visible citation in the Citation column.\n  \u2022 The Permitted Responses column should clearly constrain responses to Yes/No/Not Applicable (or an obvious equivalent like Yes/No/N.A.).\n\nScoring (structure only):\n- 6: Excel file with a clear single table containing all 10 required rows, all required columns present, correct and unique IDs, and each row shows a citation and Permitted Responses constrained to Yes/No/Not Applicable.\n- 5: Excel with the table and all required columns present; minor deviations in response phrasing (e.g., Yes/No/N.A.) but unambiguously constrained; all 10 rows with correct IDs.\n- 4: Excel with table and most columns present; up to one column slightly mislabeled or missing but inferable; IDs present with at most one error (e.g., small typo) and still clearly 10 rows intended; citations mostly present (1 missing acceptable).\n- 2: Excel present but missing multiple required columns or fewer than 10 rows; IDs significantly incomplete or not unique; citations largely missing.\n- 0: Not an Excel spreadsheet OR no recognizable table meeting the requirements.\n\nOnly evaluate structure and presence of required elements. Do not judge correctness or content quality.", "expectation": "An .xlsx file containing one table with columns for ID, Statute, Citation, Question, Permitted Responses; exactly 10 rows with IDs SCRA-12a-d and SCRA-13 through SCRA-18; each row shows a citation; permitted responses constrained to Yes/No/Not Applicable."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness Verification", "description": "Verification of the deliverable\u2019s factual and logical correctness given the mandated shape. Includes code-based integrity checks and LLM checks for statute alignment and binary-answer suitability.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "IDs Complete and Unique (Expected Set Present)", "description": "Checks that all required IDs exist exactly once across the workbook: SCRA-12a, SCRA-12b, SCRA-12c, SCRA-12d, SCRA-13, SCRA-14, SCRA-15, SCRA-16, SCRA-17, SCRA-18.", "weight": 1.0, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    weight = 1.0\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        # Concatenate all sheets to be robust\n        frames = []\n        for sheet in xls.sheet_names:\n            try:\n                df = pd.read_excel(path, sheet_name=sheet)\n                if df is not None and len(df.columns) > 0:\n                    frames.append(df)\n            except Exception:\n                continue\n        if not frames:\n            return 0.0\n        all_df = pd.concat(frames, ignore_index=True)\n        # Find an ID-like column\n        id_col = None\n        for c in all_df.columns:\n            cl = str(c).strip().lower()\n            if 'id' in cl or 'identifier' in cl or cl.startswith('scr') or 'question id' in cl:\n                id_col = c\n                break\n        if id_col is None:\n            return 0.0\n        vals = all_df[id_col].astype(str).str.strip()\n        # Normalize IDs to uppercase without surrounding spaces\n        vals_norm = vals.str.upper()\n        expected = [\n            'SCRA-12A','SCRA-12B','SCRA-12C','SCRA-12D',\n            'SCRA-13','SCRA-14','SCRA-15','SCRA-16','SCRA-17','SCRA-18'\n        ]\n        score_count = 0\n        for eid in expected:\n            count = (vals_norm == eid).sum()\n            if count == 1:\n                score_count += 1\n        return (score_count / len(expected)) * weight\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Permitted Responses Integrity", "description": "Verifies that the Permitted Responses column constrains answers to Yes/No/Not Applicable (allowing variants like N/A). Requires that each row\u2019s cell lists only from the allowed set and includes Yes and No and some form of Not Applicable.", "weight": 1.0, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    weight = 1.0\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        frames = []\n        for sheet in xls.sheet_names:\n            try:\n                df = pd.read_excel(path, sheet_name=sheet)\n                if df is not None and len(df.columns) > 0:\n                    frames.append(df)\n            except Exception:\n                continue\n        if not frames:\n            return 0.0\n        all_df = pd.concat(frames, ignore_index=True)\n        # Find a column that likely holds the response options\n        resp_col = None\n        for c in all_df.columns:\n            cl = str(c).strip().lower()\n            if ('response' in cl and ('permitted' in cl or 'allowed' in cl or 'option' in cl)) or cl in ('permitted responses','allowed responses','response options','responses'):\n                resp_col = c\n                break\n        if resp_col is None:\n            return 0.0\n        allowed = {\n            'yes': {'yes'},\n            'no': {'no'},\n            'na': {'n/a','na','not applicable','not-applicable'}\n        }\n        def normalize_tokens(s):\n            if pd.isna(s):\n                return set()\n            txt = str(s).lower()\n            # split on anything non-alphabetic\n            toks = re.split(r'[^a-z/]+', txt)\n            toks = [t.strip() for t in toks if t.strip()]\n            norm = set()\n            for t in toks:\n                if t in ('yes','no','n/a','na','not','applicable','not applicable','not-applicable'):\n                    # join 'not'+'applicable' into one if split\n                    pass\n            # recreate common forms\n            joined = txt.replace(';', ' ').replace(',', ' ').replace('|',' ').replace('/', ' / ')\n            joined = re.sub(r'\\s+', ' ', joined).strip()\n            items = re.split(r'\\s*/\\s*|\\s*,\\s*|\\s*;\\s*|\\s+or\\s+|\\s+\\|\\s+', joined)\n            items = [i.strip() for i in items if i.strip()]\n            mapped = set()\n            for i in items:\n                ii = i.lower()\n                if ii in ('yes','y'):\n                    mapped.add('yes')\n                elif ii in ('no','n'):\n                    mapped.add('no')\n                elif ii in ('n/a','na','n.a.','not applicable','not-applicable'):\n                    mapped.add('na')\n            return mapped\n        rows_ok = 0\n        total = len(all_df)\n        if total == 0:\n            return 0.0\n        for _, r in all_df.iterrows():\n            opts = normalize_tokens(r[resp_col])\n            if not opts:\n                continue\n            # must include yes and no and some NA token, and contain only these\n            if 'yes' in opts and 'no' in opts and 'na' in opts and opts.issubset({'yes','no','na'}):\n                rows_ok += 1\n        return (rows_ok / total) * weight\n    except Exception:\n        return 0.0"}, {"type": "llm_judge", "name": "Statute Alignment and Correct Citations", "description": "Verify that each question\u2019s statute and citation align correctly with the ID ranges: SCRA-12a\u201312d correspond to \u00a73937 (interest rate cap on pre-service debts); SCRA-13\u201318 correspond to \u00a73919 (exercise of rights under chapter not to affect certain future financial transactions). Check that the citation text in each row plausibly references the correct statutory section and that question focus matches the statute\u2019s scope.", "weight": 5.0, "judge_prompt": "Review the Excel spreadsheet. For each row, confirm:\n1) The ID-to-statute mapping is correct: IDs SCRA-12a, -12b, -12c, -12d are tied to 50 U.S.C. \u00a7 3937; IDs SCRA-13 through SCRA-18 are tied to 50 U.S.C. \u00a7 3919.\n2) The Citation cell text plausibly references the correct section (e.g., \u201c50 U.S.C. \u00a7 3937\u201d or a close variant for \u00a73937; \u201c50 U.S.C. \u00a7 3919\u201d for \u00a73919). Minor formatting differences are acceptable.\n3) The question\u2019s focus matches the statute\u2019s scope:\n   - \u00a73937 questions should address interest rate cap on debts incurred before military service (e.g., 6% cap including fees, eligibility period, refund/forgiveness mechanics, timing/notice based on orders, etc.).\n   - \u00a73919 questions should address that exercising rights under the SCRA should not negatively affect future financial transactions (e.g., no adverse credit actions, denial of credit, higher interest or fees on future obligations solely due to exercising rights, etc.).\n\nScoring:\n- 5: All rows have correct statute alignment, plausible citations, and question scope matches the statute.\n- 4: One minor mismatch or weak citation formatting; otherwise aligned.\n- 3: Up to two issues (misalignment or questionable citation/scope) but majority correct.\n- 2: Several mismatches; less than half correctly aligned.\n- 0\u20131: Pervasive misalignment or citations mostly incorrect/absent.", "expectation": "Every row cleanly aligns to the correct statute with a plausible citation; \u00a73937 items deal with 6% cap on pre-service debts; \u00a73919 items deal with not harming future transactions due to exercising rights."}, {"type": "llm_judge", "name": "Binary-Answerability and Auditability", "description": "Confirms each question is framed to be answered strictly Yes/No/Not Applicable, is unambiguous, and is appropriate for audit testing (no multi-part or speculative prompts).", "weight": 5.0, "judge_prompt": "Evaluate whether each question in the spreadsheet:\n- Is strictly answerable as Yes/No/Not Applicable (no open-ended prompts, no multi-part compound requirements that would need multiple answers).\n- Is unambiguous and specific to a compliance check an auditor could verify from records.\n- Avoids subjective or speculative phrasing.\n\nScoring:\n- 5: All questions meet the criteria (binary-answerable, unambiguous, audit-appropriate).\n- 4: One minor instance of ambiguity or slight drift, but still mostly binary-answerable.\n- 3: Two issues affecting binary-answerability or clarity.\n- 2: Several questions are ambiguous or not strictly binary.\n- 0\u20131: Majority of questions are not suitable for Yes/No/NA audit use.", "expectation": "Ten concise, single-issue compliance checks, each clearly answerable as Yes/No/Not Applicable."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism", "description": "Holistic assessment of presentation, coverage of key statutory themes, and practical usability for audit workflows.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Formatting and Consistency", "description": "Assess whether the spreadsheet is professionally presented: clear headers, consistent ID formatting, grammar, punctuation, and consistent citation formatting across rows.", "weight": 2.5, "judge_prompt": "Assess professional presentation:\n- Column headers are clear and consistently styled.\n- IDs follow a consistent format (e.g., SCRA-12a, SCRA-12b, etc.).\n- Citations are consistently formatted across all rows.\n- Grammar and punctuation are appropriate for a regulatory audit template.\n\nScoring: 2.5 = fully professional; 1.5\u20132.0 = minor inconsistencies; 0.5\u20131.0 = noticeable issues; 0 = unprofessional.", "expectation": "A clean, professional audit template with consistent headers, IDs, and citations."}, {"type": "llm_judge", "name": "Coverage of \u00a73937 Key Elements", "description": "Evaluate whether the four \u00a73937 questions collectively cover core elements: pre-service debt status, 6% cap including fees, effective period, and adjustment/refund handling upon notice/orders.", "weight": 2.5, "judge_prompt": "For the four \u00a73937 questions (SCRA-12a\u201312d), judge whether they collectively cover key controls, such as:\n- Debt incurred before military service (eligibility scope).\n- Interest cap at 6% including fees/charges.\n- Effective period of the cap triggered by active duty orders and notice.\n- Proper adjustment mechanics: rate reduction, forgiveness/refund of interest above 6%.\n\nScoring: 2.5 = strong, comprehensive coverage; 1.5\u20132.0 = mostly covered with minor gap; 0.5\u20131.0 = partial coverage; 0 = poor coverage.", "expectation": "Four \u00a73937 questions that together test pre-service status, 6% cap scope, timing/effectiveness, and refund/forgiveness controls."}, {"type": "llm_judge", "name": "Coverage of \u00a73919 Key Elements", "description": "Evaluate whether the six \u00a73919 questions cover core protections: no adverse treatment in future credit decisions solely due to exercising SCRA rights, no penalty rates/fees on future obligations, no negative credit reporting solely due to SCRA exercise, and documentation of decision rationale.", "weight": 2.5, "judge_prompt": "For the six \u00a73919 questions (SCRA-13\u2013SCRA-18), assess coverage of protections including:\n- No denial of credit or less favorable terms for future transactions solely because the borrower exercised SCRA rights.\n- No increased interest, fees, or adverse conditions on future obligations for that reason.\n- No negative credit reporting or similar retaliation due to SCRA exercise.\n- Documentation of decision rationale showing actions were not based on SCRA exercise.\n\nScoring: 2.5 = comprehensive; 1.5\u20132.0 = mostly covered; 0.5\u20131.0 = partial; 0 = poor.", "expectation": "Six \u00a73919 questions that capture the breadth of protections against adverse future-transaction treatment due to exercising SCRA rights."}, {"type": "llm_judge", "name": "Practical Usability for Audit Workflow", "description": "Assesses whether the spreadsheet is ready-to-use in an audit: clear enough for testers, includes necessary columns (e.g., IDs, statute, citation, question, permitted responses), and optionally useful notes/guidance without clutter.", "weight": 2.5, "judge_prompt": "Judge practical usability:\n- Columns present support efficient testing (ID, Statute, Citation, Question, Permitted Responses). Optional columns (e.g., Notes/Guidance) add value without clutter.\n- Questions are sequenced logically (e.g., group \u00a73937, then \u00a73919) and easily filterable (by ID or Statute).\n- Overall, the sheet is turnkey for auditors to apply to sampled accounts.\n\nScoring: 2.5 = highly usable; 1.5\u20132.0 = usable with minor tweaks; 0.5\u20131.0 = needs work; 0 = impractical.", "expectation": "A turnkey audit worksheet that testers can immediately use to evaluate accounts."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "f2986c1f-2bbf-4b83-bc93-624a9d617f45", "rubric": {"category_name": "ER Medication Identification Spreadsheet (Pharmacist)", "rationale": "This rubric enforces a self-documenting, verifiable Excel output that supports rapid ER decision-making. Stage 1 (LLM-only) mandates a strict, inspection-friendly spreadsheet shape so downstream verification is trivial. Stage 2 mixes light code checks (structure, bounds, URL/domain, formats) with higher-weight LLM checks (clinical plausibility and cross-field consistency) per the required 5:1 weighting bias toward LLMs. Stage 3 evaluates overall professional quality, clinical usefulness, and readiness for continuity-of-care workflows.", "max_total_score": 14.0, "stages": [{"name": "Stage 1 \u2013 Shape Enforcement Gate (LLM Only)", "description": "Verify the output is an Excel workbook with the exact, verifiable structure required for downstream checks. Only structure/presence is judged here (not correctness).", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.25, "rules": [{"type": "llm_judge", "name": "Excel Structure and Required Columns Present", "description": "The candidate must deliver an Excel (.xlsx) file containing a medication identification table with all required columns and at least one data row. This is a gate: if the structure is not correct, the category scores 0.", "weight": 3.0, "judge_prompt": "You are evaluating ONLY the structure/format of the candidate output for a pharmacist ER task. The agent received an image of a patient's meds and was instructed to identify each medication via Drugs.com and compile an Excel spreadsheet.\n\nCheck the following strictly for structure (not content accuracy):\n\nFormat requirements:\n- File must be an Excel workbook (.xlsx). Not CSV, not DOCX/PDF.\n- At least one sheet must contain the medication table.\n\nRequired table columns (flexible on minor naming variations, but intent must be clear):\n- Markings\n- Color\n- Shape\n- Dose form\n- Name of medication\n- Strength of medication\n- Type of medication (must accommodate: Controlled substance, Legend drug, Over the counter, Unknown)\n- A link to patient counseling information from MedlinePlus.gov (e.g., a URL in the column)\n\nAdditional allowances (do not penalize):\n- Extra columns, e.g., Drugs.com Source URL, Notes, Image reference (e.g., \"what are these.jpg\").\n- Column name variants like \"Imprint\" for Markings, \"Dosage form\" for Dose form, \"Medication name\" for Name of medication, \"Medication type\" for Type of medication, \"MedlinePlus link\" for counseling link.\n\nRow/data presence:\n- The table must have at least 1 data row.\n- Missing data is acceptable only if represented as \"NA\" (case-insensitive). Blank cells for required fields should be treated as missing structure.\n\nScoring (structure only; be flexible on header wording but strict on presence):\n- 1.0: Valid .xlsx + a clear medication table with all required columns present (allowing reasonable synonyms) + at least 1 data row. Missing values are consistently encoded as \"NA\" rather than left blank.\n- 0.8: Valid .xlsx + all required information present but with minor header ambiguities or occasional blanks instead of \"NA\".\n- 0.5: Valid .xlsx but missing 1\u20132 required columns OR the table exists with headers but no data rows.\n- 0.0: Not an Excel file, or missing multiple required columns, or no recognizable medication table.\n\nImportant: Do not judge calculation or content correctness here\u2014only the presence and shape that enables verification in later stages.", "expectation": "An .xlsx workbook with a table including the 8 required fields (or clear synonyms), at least one data row, and NA used where info is unavailable."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness and Consistency Verification (Mixed)", "description": "With the shape enforced, verify correctness and consistency using a mix of deterministic code checks and higher-weight LLM judgments for clinical plausibility and cross-field consistency.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Structural Coverage, Rows, and NA Policy (light check)", "description": "Programmatic check for presence of required columns (flexible synonyms), at least one row, and consistent use of NA for missing values.", "weight": 0.2, "code": "def evaluate(workflow, context):\n    import re\n    import pandas as pd\n    \n    def norm(s):\n        return re.sub(r\"[^a-z0-9]\", \"\", str(s).lower())\n    \n    required = {\n        'markings': ['markings','marking','imprint','imprints','pill imprint','pill markings'],\n        'color': ['color','colour','pill color'],\n        'shape': ['shape','pill shape'],\n        'dose_form': ['dose form','dosage form','doseform','form'],\n        'name': ['name of medication','medication name','drug name','name','medication'],\n        'strength': ['strength','dose strength','strength (mg)'],\n        'type': ['type of medication','medication type','drug type','type','schedule','regulatory status'],\n        'counseling_link': ['counseling link','patient counseling link','medlineplus link','counseling url','medlineplus url','medlineplus.gov link']\n    }\n\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n    \n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        best_df = None\n        best_map = None\n        \n        for sheet in xls.sheet_names:\n            try:\n                df = pd.read_excel(path, sheet_name=sheet)\n            except Exception:\n                continue\n            normcols = [norm(c) for c in df.columns]\n            colmap = {}\n            for key, syns in required.items():\n                found = None\n                for i, nc in enumerate(normcols):\n                    for syn in syns:\n                        if norm(syn) in nc:\n                            found = df.columns[i]\n                            break\n                    if found:\n                        break\n                if found:\n                    colmap[key] = found\n            if best_df is None or len(colmap) > len(best_map or {}):\n                best_df = df\n                best_map = colmap\n        \n        if best_df is None:\n            return 0.0, \"No readable sheets.\"\n        \n        coverage = len(best_map) / len(required)\n        rows_ok = 1.0 if len(best_df) >= 1 else 0.0\n        \n        # NA usage compliance across found required columns\n        total_cells = 0\n        bad_blanks = 0\n        for key, col in best_map.items():\n            ser = best_df[col]\n            for v in ser:\n                total_cells += 1\n                if pd.isna(v):\n                    bad_blanks += 1\n                else:\n                    s = str(v).strip()\n                    if s == \"\":\n                        bad_blanks += 1\n                    else:\n                        # Accept NA markers\n                        if s.lower() in [\"na\",\"n/a\",\"unknown\"]:\n                            pass\n        na_score = 1.0 if total_cells == 0 else max(0.0, 1.0 - bad_blanks / total_cells)\n        final = 0.6 * min(1.0, coverage) + 0.2 * rows_ok + 0.2 * na_score\n        return float(max(0.0, min(1.0, final))), f\"coverage={coverage:.2f}, rows={len(best_df)}, na_score={na_score:.2f}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Medication Type Value Validity", "description": "Type of medication must map to one of the allowed categories (Controlled, Legend, OTC, Unknown) with flexible synonyms (e.g., C-II => Controlled). Scores by fraction of valid rows.", "weight": 0.4, "code": "def evaluate(workflow, context):\n    import re\n    import pandas as pd\n\n    def norm(s):\n        return re.sub(r\"[^a-z0-9]\",\"\", str(s).lower())\n\n    def find_sheet_and_col(df, required_syns):\n        normcols = [norm(c) for c in df.columns]\n        for key, syns in required_syns.items():\n            for i, nc in enumerate(normcols):\n                for syn in syns:\n                    if norm(syn) in nc:\n                        return df.columns[i]\n        return None\n\n    type_syns = {\n        'type': ['type of medication','medication type','drug type','type','schedule','regulatory status']\n    }\n\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        best_df = None\n        type_col = None\n        for sheet in xls.sheet_names:\n            try:\n                df = pd.read_excel(path, sheet_name=sheet)\n            except Exception:\n                continue\n            col = find_sheet_and_col(df, type_syns)\n            if col is not None:\n                best_df = df\n                type_col = col\n                break\n        if best_df is None or type_col is None or len(best_df) == 0:\n            return 0.0, \"No usable type column.\"\n\n        def classify(val):\n            if pd.isna(val):\n                return 'na'\n            s = str(val).strip().lower()\n            if s in ['na','n/a','unknown','unk']:\n                return 'unknown'\n            s_clean = s.replace(' ', '')\n            # Controlled indicators\n            controlled_tokens = ['controlled','c-','c ','cii','ciii','civ','cv','c2','c3','c4','c5','scheduleii','scheduleiii','scheduleiv','schedulev']\n            if any(tok in s_clean for tok in controlled_tokens):\n                return 'controlled'\n            if any(t in s for t in ['otc','over the counter','nonprescription','non-prescription']):\n                return 'otc'\n            if any(t in s for t in ['legend','rx only','rx-only','rx','prescription']):\n                return 'legend'\n            return 'invalid'\n\n        total = 0\n        valid = 0\n        for v in best_df[type_col]:\n            # Only count non-empty rows\n            if pd.isna(v) or str(v).strip() == '':\n                total += 1\n                # Blank is invalid (should be NA)\n                continue\n            cat = classify(v)\n            total += 1\n            if cat in ['controlled','legend','otc','unknown']:\n                valid += 1\n        if total == 0:\n            return 0.0, \"No rows to check.\"\n        return valid/total, f\"valid={valid}/{total}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "MedlinePlus Counseling Link Validity", "description": "Counseling link domain and URL shape check: must be http(s) and within medlineplus.gov. Scores by fraction of valid links among non-NA rows.", "weight": 0.5, "code": "def evaluate(workflow, context):\n    import re\n    import pandas as pd\n    from urllib.parse import urlparse\n\n    def norm(s):\n        return re.sub(r\"[^a-z0-9]\",\"\", str(s).lower())\n\n    link_syns = {\n        'counseling_link': ['counseling link','patient counseling link','medlineplus link','counseling url','medlineplus url','medlineplus.gov link']\n    }\n\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        best_df = None\n        link_col = None\n        for sheet in xls.sheet_names:\n            try:\n                df = pd.read_excel(path, sheet_name=sheet)\n            except Exception:\n                continue\n            normcols = [norm(c) for c in df.columns]\n            for i, nc in enumerate(normcols):\n                for syn in link_syns['counseling_link']:\n                    if norm(syn) in nc:\n                        best_df = df\n                        link_col = df.columns[i]\n                        break\n                if link_col is not None:\n                    break\n            if link_col is not None:\n                break\n        if best_df is None or link_col is None or len(best_df) == 0:\n            return 0.0, \"No counseling link column found.\"\n\n        total = 0\n        valid = 0\n        for v in best_df[link_col]:\n            if pd.isna(v) or str(v).strip() == '':\n                # Blank is invalid; should be NA if unknown\n                total += 1\n                continue\n            s = str(v).strip()\n            if s.lower() in ['na','n/a','unknown']:\n                # Treat NA as neutral/valid\n                total += 1\n                valid += 1\n                continue\n            try:\n                p = urlparse(s)\n                is_http = p.scheme in ['http','https']\n                host = (p.netloc or '').lower()\n                ok_domain = 'medlineplus.gov' in host\n                total += 1\n                if is_http and ok_domain:\n                    valid += 1\n            except Exception:\n                total += 1\n        if total == 0:\n            return 0.0, \"No rows to check.\"\n        return valid/total, f\"valid_links={valid}/{total}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Strength Format Plausibility", "description": "Strength values should contain a numeric quantity and a recognizable unit (e.g., mg, mcg, g, mL, mg/mL, %, IU, units). Scores by fraction of non-NA entries matching patterns.", "weight": 0.4, "code": "def evaluate(workflow, context):\n    import re\n    import pandas as pd\n\n    def norm(s):\n        return re.sub(r\"[^a-z0-9]\",\"\", str(s).lower())\n\n    strength_syns = {\n        'strength': ['strength','dose strength','strength (mg)']\n    }\n\n    units = ['mg','mcg','g','kg','ml','mg/ml','mcg/ml','%','iu','units','meq','mmol','mcg/hr','mg/hr','mcg/actuation','mg/actuation','mg/5ml','mcg/5ml']\n\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        best_df = None\n        strength_col = None\n        for sheet in xls.sheet_names:\n            try:\n                df = pd.read_excel(path, sheet_name=sheet)\n            except Exception:\n                continue\n            normcols = [norm(c) for c in df.columns]\n            for i, nc in enumerate(normcols):\n                for syn in strength_syns['strength']:\n                    if norm(syn) in nc:\n                        best_df = df\n                        strength_col = df.columns[i]\n                        break\n                if strength_col is not None:\n                    break\n            if strength_col is not None:\n                break\n        if best_df is None or strength_col is None or len(best_df) == 0:\n            return 0.0, \"No strength column found.\"\n\n        checked = 0\n        ok = 0\n        for v in best_df[strength_col]:\n            if pd.isna(v) or str(v).strip() == '':\n                # Blank is invalid; should be NA\n                checked += 1\n                continue\n            s = str(v).strip().lower()\n            if s in ['na','n/a','unknown']:\n                # Treat NA as neutral/valid\n                checked += 1\n                ok += 1\n                continue\n            # Must have a digit and a known unit somewhere\n            has_digit = bool(re.search(r\"\\d\", s))\n            has_unit = any(u in s for u in units)\n            checked += 1\n            if has_digit and has_unit:\n                ok += 1\n        if checked == 0:\n            return 0.0, \"No rows to check.\"\n        return ok/checked, f\"strength_ok={ok}/{checked}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "llm_judge", "name": "Cross-field Consistency (Shape \u2194 Dose form, Markings plausibility)", "description": "LLM checks whether Shape and Dose form are compatible (e.g., tablet vs capsule) and whether Markings look like realistic pill imprints rather than names/notes. Samples several rows for consistency.", "weight": 2.5, "judge_prompt": "Evaluate the spreadsheet for cross-field consistency:\n- Shape vs Dose form: Do entries use compatible concepts (e.g., Dose form = Tablet with Shape like \"Round\", \"Oval\"; Capsule forms should not have tablet-type shapes)?\n- Markings plausibility: Imprints should look like alphanumeric stampings (e.g., \"M 30\", \"IP 190\"), not brand names or free text notes.\nProvide a score from 0.0 to 1.0 with reasoning:\n- 1.0: All sampled rows consistent; markings are plausible imprints.\n- 0.7: Mostly consistent; a few minor mismatches or unclear marking formats.\n- 0.4: Multiple inconsistencies between Dose form and Shape and/or many markings not plausible imprints.\n- 0.0: Widespread inconsistencies; markings are generally not imprints or fields are misused.", "expectation": "Most rows show realistic imprint strings and compatible Shape/Dose form combinations."}, {"type": "llm_judge", "name": "Medication Name, Strength, and Type Alignment", "description": "LLM checks if medication names and strengths look medically plausible and if the stated Type (Controlled/Legend/OTC/Unknown) is reasonable for the listed medication examples.", "weight": 2.5, "judge_prompt": "Review several rows to assess alignment:\n- Medication names and strengths should look plausible (e.g., ibuprofen 200 mg, amoxicillin 500 mg).\n- The Type should be reasonable given the medication (e.g., ibuprofen typically OTC; oxycodone controlled; many antibiotics are legend/Rx-only).\nScore from 0.0 to 1.0:\n- 1.0: Names and strengths look plausible across sampled rows; Type labels are appropriate.\n- 0.7: Mostly plausible with minor issues in type labeling or strength formatting.\n- 0.4: Several mismatches (e.g., common OTC mislabeled as Legend, controlled substances not marked Controlled).\n- 0.0: Major, frequent mismatches across rows.", "expectation": "High concordance between common drug regulatory status and the Type column; strengths/names look standard."}, {"type": "llm_judge", "name": "Source Traceability to Drugs.com (light)", "description": "If a source column/notes are present, judge whether they credibly cite Drugs.com. If no source column, judge whether the data fields (imprints, shapes, colors) look like they could have been obtained from Drugs.com tables.", "weight": 1.5, "judge_prompt": "Check the workbook for evidence of Drugs.com sourcing:\n- If present, look for a source URL column or notes referencing Drugs.com per row.\n- If absent, assess whether the structured fields (Markings, Color, Shape, Dose form, Strength) look like they come from a pill identification workflow typical of Drugs.com.\nScore 0.0\u20131.0:\n- 1.0: Clear per-row Drugs.com links or equivalent sourcing; or fields are strongly consistent with Drugs.com pill ID outputs.\n- 0.6: Some global note or occasional rows cite Drugs.com; overall looks plausible.\n- 0.3: Weak/ambiguous sourcing; limited hints only.\n- 0.0: No evidence of Drugs.com use and fields look ad hoc/unverifiable.\nDo not require a source column if not present in the instruction; reward when present but do not punish absence harshly if the fields are otherwise consistent.", "expectation": "Either explicit Drugs.com links/notes or highly consistent pill-ID fields indicating Drugs.com-like sourcing."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Professional Quality and Clinical Utility (LLM)", "description": "Holistic assessment of presentation, clinical usefulness, and readiness for ER workflows and continuity of care.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clinical Usefulness and Completeness", "description": "Does the spreadsheet provide enough clearly organized information to support ER provider decision-making and pharmacist intervention assessment?", "weight": 1.0, "judge_prompt": "Judge whether the spreadsheet is clinically useful in the ER context:\n- Are all requested fields present and filled with values or NA consistently?\n- Does the content allow quick recognition of each medication (imprint, color, shape, dose form, name, strength, type)?\n- Would a pharmacist or provider be able to triage potential safety issues from this sheet?\nScore 0.0\u20131.0 based on overall completeness and ER utility.", "expectation": "A concise, complete table that enables rapid identification and safety review."}, {"type": "llm_judge", "name": "Presentation and Clarity", "description": "Formatting and readability for fast ER use (headers, filters/freeze panes, consistent capitalization, consistent NA usage).", "weight": 0.7, "judge_prompt": "Assess presentation quality:\n- Clear headers, readable layout, consistent units/labels, and uniform NA usage.\n- Optional but helpful: filters enabled, freeze header row, legible column widths.\nScore 0.0\u20131.0 based on readability and professionalism.", "expectation": "Professional-looking sheet that is easy to scan quickly."}, {"type": "llm_judge", "name": "Update Readiness and Continuity of Care", "description": "Does the file include small touches that make it easy to update during the stay and useful upon discharge (e.g., date/time, patient identifier, image reference)?", "weight": 0.6, "judge_prompt": "Evaluate whether the workbook is ready for ongoing updates and discharge use:\n- Presence of a date/time, patient identifier, and/or reference to the intake image (e.g., \"what are these.jpg\").\n- Structure that makes adding rows straightforward.\nScore 0.0\u20131.0 based on how ready the artifact is for updates and handoff.", "expectation": "Minor but meaningful metadata that supports updates and discharge documentation."}, {"type": "llm_judge", "name": "Safety and Patient-Centeredness", "description": "Are the MedlinePlus links appropriate for patient counseling and safety information, and is the sheet mindful of patient understanding?", "weight": 0.7, "judge_prompt": "Assess the counseling links and patient-centered aspects:\n- Do links appear to point to patient-friendly MedlinePlus pages (by URL shape and labeling)?\n- Are there cues that help staff/patients (e.g., clear link text, brief notes)?\nScore 0.0\u20131.0 based on how well the sheet supports quick counseling and safety review.", "expectation": "Clear counseling links that likely lead to appropriate MedlinePlus content and support staff/patient needs."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "a10ec48c-168e-476c-8fe3-23b2a5f616ac", "rubric": {"category_name": "Concierge Local Restaurant Recommendations (Sarasota Downtown) - Self-Documenting Evaluation", "rationale": "This rubric enforces a self-documenting, verifiable Word document tailored for concierge use. Stage 1 (LLM-only) strictly mandates the exact structural shape: a properly titled DOCX/PDF with headline, intro, cuisine-grouped tables, required column headers, and a Sources section. Stage 2 mixes light code checks (sources/address/name/type) with LLM verification for correctness (source adherence and closure exclusion, directions plausibility from 1991 Main St, and category classification consistency). Stage 3 performs holistic quality assessment of formatting, clarity, and concierge utility. Code rules are kept lightweight and robust, while LLM judges handle nuanced verification.", "max_total_score": 24.0, "stages": [{"name": "Stage 1 \u2013 Shape Enforcement Gate (LLM-only)", "description": "Validate that the output is a properly structured Word/PDF document with the exact required sections and table structures to enable verification. Do not assess correctness, only presence/format/shape.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.0, "rules": [{"type": "llm_judge", "name": "Document Format and Mandatory Sections Present", "description": "Check if the output is a DOCX (preferred) or PDF with the exact document-level structure that enables verification.", "weight": 3.0, "judge_prompt": "You are checking ONLY the presence/format/shape of the output, not the factual correctness.\nConsider the candidate output you see (rendered document view).\n\nRequired format and sections:\n- File format: Microsoft Word (DOCX). PDF is acceptable but should score slightly lower than DOCX.\n- Document title/headline: should closely match \u201cConcierge Local Restaurant Recommendations (Sarasota Downtown)\u201d and appear prominently near the start.\n- An introductory passage/paragraph below the headline.\n- Tables: at least TWO separate tables. Each table must be titled \u201cSarasota Downtown Restaurant Recommendations\u201d and include a subtitle indicating the cuisine (e.g., American/Continental, Asian, Italian, Seafood, etc.).\n- Each table must have the following column headers (exact or very close variants):\n  1) \u201cRestaurant Name\u201d\n  2) \u201cBusiness Hours\u201d\n  3) \u201cDescription\u201d\n  4) \u201cDirections\u201d\n  5) \u201cCategory\u201d\n- A \u201cSources\u201d section (or clearly labeled citations) listing both: the Downtown Sarasota restaurants page and Google Maps as sources.\n\nScoring (0 to 3):\n- 3.0: Valid DOCX OR clean PDF; clear title (close match), intro present; \u22652 cuisine-specific tables titled as required; all five required column headers present; Sources section with both sources present.\n- 2.0: Mostly correct but with one minor deviation (e.g., PDF not DOCX; slight title wording variance; or Sources present but only one of the two sources listed).\n- 1.0: Multiple requirements missing (e.g., only one table, or missing several required columns) but still clearly attempting the required structure.\n- 0.0: Not a document (e.g., plain text) OR lacks tables entirely OR structure is not recognizable.\n\nBe flexible with near-synonyms of headers and section names, but insist on the overall structure. Do NOT judge content accuracy\u2014only presence and formatting.", "expectation": "A DOCX with the specified title, intro paragraph, 2+ cuisine-specific recommendation tables with the five columns, and a Sources section citing Downtown Sarasota restaurants page and Google Maps."}, {"type": "llm_judge", "name": "Table Column and Row Shape Integrity", "description": "Verify that the tables visibly implement the required columns and basic row structure sufficient for later verification.", "weight": 3.0, "judge_prompt": "You are checking ONLY structure and presence, not factual correctness.\n\nWithin the document:\n- Inspect at least two of the cuisine tables titled \u201cSarasota Downtown Restaurant Recommendations\u201d with a visible subtitle for the cuisine type.\n- Confirm each inspected table visibly has columns named (or very close to): \u201cRestaurant Name\u201d, \u201cBusiness Hours\u201d, \u201cDescription\u201d, \u201cDirections\u201d, \u201cCategory\u201d.\n- For the \u201cRestaurant Name\u201d column: verify the cell contents appear as hyperlink text titled with the restaurant name (e.g., styled as an underlined/blue link or clearly marked link text).\n- For the \u201cDirections\u201d column: verify the text explicitly references the origin \u201c1991 Main Street, Sarasota, FL 34236\u201d (or an unmistakably close variant like \u201c1991 Main St\u201d / \u201cSarasota, Florida 34236\u201d).\n- Ensure each inspected table contains at least 3 restaurant rows (not counting header rows).\n\nScoring (0 to 3):\n- 3.0: Two or more cuisine tables inspected; all five columns present; restaurant names appear as link text; directions reference the 1991 Main Street origin; each table has \u22653 rows.\n- 2.0: Minor issues (e.g., only one table meeting all requirements or small header naming deviations) but clear overall compliance.\n- 1.0: Significant omissions (e.g., missing a required column or rows <3) but tables are still present.\n- 0.0: Tables absent or columns largely not matching; cannot verify links/directions structure.\n\nDo NOT assess whether the links or directions are correct\u2014only that the shape is present.", "expectation": "Two or more cuisine tables with the five columns, link-like restaurant names, directions referencing 1991 Main Street, and at least 3 data rows per table."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness and Consistency Verification (Mixed)", "description": "With the correct structure in place, verify adherence to sources, plausibility of directions from 1991 Main St, valid use of categories, and basic document/file sanity.", "is_required": true, "max_points": 10.0, "min_score_to_pass": 5.0, "rules": [{"type": "code", "name": "Document Type and Naming Sanity Check", "description": "Verify primary output is a document (prefer DOCX) and name/title references the required phrase.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float score in [0, 1]\n    \"\"\"\n    import re\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    score = 0.0\n    # Check it's a document\n    if not output.is_document:\n        return 0.0\n    # Prefer DOCX\n    filename = getattr(output, 'name', '') or ''\n    name_l = filename.lower()\n    # +0.4 if DOCX, +0.2 if PDF\n    if name_l.endswith('.docx'):\n        score += 0.4\n    elif name_l.endswith('.pdf'):\n        score += 0.2\n    # Title/name contains key phrases\n    title_hits = 0\n    needles = [\"concierge\", \"local restaurant recommendations\", \"sarasota\", \"downtown\"]\n    for n in needles:\n        if n in name_l:\n            title_hits += 1\n    # Up to +0.6 for name/title phrase presence\n    score += min(title_hits / len(needles), 1.0) * 0.6\n    return max(0.0, min(1.0, score))"}, {"type": "code", "name": "Sources and Origin Address Presence", "description": "Check that the document text mentions the Downtown Sarasota restaurants page, Google Maps, and the origin address 1991 Main Street.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Verify mentions of:\n    - downtownsarasota.com/restaurants.php (or nearby path)\n    - Google Maps (maps.google or 'Google Maps')\n    - 1991 Main Street (flexible variants)\n    \"\"\"\n    import re\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    if not output.is_document:\n        return 0.0\n    text = ''\n    try:\n        name_l = (getattr(output, 'name', '') or '').lower()\n        if name_l.endswith('.docx'):\n            text = context.files.read_docx_text(output.id)\n        elif name_l.endswith('.pdf'):\n            text = context.files.read_pdf_text(output.id)\n        else:\n            # Try generic text extraction\n            text = context.files.read_text(output.id)\n    except Exception:\n        text = ''\n    t = (text or '').lower()\n    hits = 0\n    total = 3\n    # Downtown Sarasota restaurants page\n    if re.search(r\"downtownsarasota\\.com/restaurant\", t) or \"downtownsarasota.com/restaurants.php\" in t:\n        hits += 1\n    # Google Maps\n    if (\"google maps\" in t) or (\"maps.google\" in t) or (\"goo.gl/maps\" in t):\n        hits += 1\n    # 1991 Main Street variants\n    if re.search(r\"1991\\s+main\\s+st(reet)?\", t) and (\"sarasota\" in t):\n        hits += 1\n    return (hits / total) * 1.0"}, {"type": "llm_judge", "name": "Source Adherence and Closure Exclusion", "description": "Cross-check that listed restaurants plausibly come from the Downtown Sarasota restaurants page and do not include permanently closed venues, using Google Maps as a secondary check.", "weight": 3.0, "judge_prompt": "Evaluate adherence to the specified data sources and exclusion of closed venues.\nInstructions:\n- Spot-check a few restaurant entries across different cuisine tables.\n- Verify that the restaurant names plausibly match entries from the Downtown Sarasota restaurants page (http://www.downtownsarasota.com/restaurants.php), and that none appear obviously permanently closed based on visible cues (e.g., labeled closed, or the document flags closures correctly by excluding them).\n- Confirm that Google Maps is cited/used for operational info (hours, directions) and details look plausibly derived from Google Maps (e.g., realistic hours formatting, distances/times mentioned, etc.).\n\nScoring (0 to 3):\n- 3.0: Entries plausibly align with the Downtown Sarasota list; no clearly closed listings included; details appear consistent with Google Maps usage.\n- 2.0: Minor issues (e.g., uncertain inclusion or a possible closure ambiguity) but generally consistent with sources.\n- 1.0: Multiple discrepancies or probable inclusion of closed venues.\n- 0.0: Clearly not sourced from the specified list or includes closed restaurants without explanation.", "expectation": "Sampled entries align with the Downtown Sarasota restaurants page, exclude closed venues, and reflect information likely sourced from Google Maps."}, {"type": "llm_judge", "name": "Directions Plausibility from 1991 Main St", "description": "Check that directions in the 'Directions' column are clearly from 1991 Main Street, Sarasota, FL 34236 and appear reasonable.", "weight": 2.5, "judge_prompt": "Evaluate the plausibility and origin specificity of the directions provided.\n- Confirm the directions are written from \u201c1991 Main Street, Sarasota, Florida 34236\u201d (or a close variant) to each restaurant.\n- Spot-check a few entries: Do the directions include reasonable distance/time estimates or concise wayfinding suitable for residents? (Do not require exact road-by-road steps, but they should look plausible and helpful.)\n\nScoring (0 to 2.5):\n- 2.5: Directions clearly originate from the specified address and are succinct and plausible for multiple entries.\n- 1.5: Generally correct origin but directions are vague or inconsistent.\n- 0.5: Origin unclear or only some entries reflect the correct origin.\n- 0.0: Directions missing or unrelated to the specified origin.", "expectation": "Directions consistently originate from 1991 Main Street and appear concise and plausible."}, {"type": "llm_judge", "name": "Category Classification Consistency", "description": "Assess whether the assigned categories conform to the provided definitions (Quick Service, Fast Casual, Casual Dining, Family Style, Upscale Casual, Fine Dining, Michelin-Starred, Pop-Up/Concept).", "weight": 2.5, "judge_prompt": "Review category assignments and compare to these definitions:\n- Fine Dining: gourmet cuisine, formal service, elegant settings.\n- Upscale Casual: high-quality food and service in a relaxed, stylish environment.\n- Casual Dining: comfortable and family-friendly with moderate prices.\n- Fast Casual: quick service with fresh, quality ingredients in a modern setting.\n- Quick Service: very fast counter service, minimal table service.\n- Family Style: communal dishes, family-friendly, often shared platters.\n- Michelin-Starred: explicitly Michelin-starred venue.\n- Pop-Up/Concept: temporary or concept-driven setups.\n\nScoring (0 to 2.5):\n- 2.5: Category labels consistently align with the above definitions across sampled entries.\n- 1.5: Mostly correct with a few borderline misclassifications.\n- 0.5: Frequent inconsistencies.\n- 0.0: Categories appear largely incorrect or misused.\n\nJudge only plausibility based on the table content; do not require external verification beyond what you can infer.", "expectation": "Categories used are appropriate and consistent with provided definitions."}], "on_failure_action": "skip_remaining", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Professional Quality and Concierge Utility", "description": "Holistic assessment of presentation quality, clarity, and usefulness for luxury residential concierge use.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Formatting and Readability", "description": "Assess layout, typography, consistent tables, headings, and overall polish befitting a luxury property concierge document.", "weight": 2.0, "judge_prompt": "Evaluate professional presentation:\n- Clean, consistent formatting for headings and tables.\n- Adequate spacing, alignment, and legible typography.\n- Tables do not break awkwardly across pages; headers repeat if needed.\n\nScoring (0 to 2):\n- 2.0: Highly professional and polished.\n- 1.0: Generally acceptable with minor formatting issues.\n- 0.0: Sloppy or hard to read.", "expectation": "Polished, consistent, and readable formatting."}, {"type": "llm_judge", "name": "Concierge Utility and Coverage", "description": "Judge the document\u2019s practical usefulness for residents: coverage of diverse cuisines, sufficient number of options, and actionability.", "weight": 2.0, "judge_prompt": "Evaluate utility for concierge use:\n- Includes multiple cuisine tables with a good variety of reputable options.\n- Enough entries per cuisine to be useful (e.g., 3\u20136+ per table is reasonable).\n- Actionable details (website links, business hours, concise directions) present.\n\nScoring (0 to 2):\n- 2.0: Strong variety and actionable details across cuisines.\n- 1.0: Adequate but could include more variety or entries.\n- 0.0: Sparse or not actionable.", "expectation": "Diverse, well-populated tables with actionable info for residents."}, {"type": "llm_judge", "name": "Clarity and Brevity of Descriptions", "description": "Descriptions should concisely convey the restaurant\u2019s essence, cuisine, and standout features without fluff.", "weight": 2.0, "judge_prompt": "Assess description quality:\n- Are descriptions concise (1\u20133 sentences), clear, and informative?\n- Do they convey cuisine type and what makes the venue appealing for residents?\n\nScoring (0 to 2):\n- 2.0: Concise and informative throughout.\n- 1.0: Mixed quality; some verbose or vague entries.\n- 0.0: Commonly unclear or unhelpful.", "expectation": "Short, clear, informative descriptions highlighting cuisine and appeal."}, {"type": "llm_judge", "name": "Hyperlink and Table Usability", "description": "Check whether restaurant names look like clickable links and whether tables are easy to navigate.", "weight": 2.0, "judge_prompt": "Evaluate usability:\n- Do restaurant names appear as clickable links to official websites?\n- Are tables consistently structured and easy to scan?\n- Any obvious broken link formatting (e.g., link text without clear destination cues) or inconsistent column usage?\n\nScoring (0 to 2):\n- 2.0: Links and tables are consistently usable and scannable.\n- 1.0: Minor issues but overall usable.\n- 0.0: Poor usability due to link/table inconsistencies.", "expectation": "Clickable-looking restaurant name links; consistent, scannable tables."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "c94452e4-39cd-4846-b73a-ab75933d1ad7", "rubric": {"category_name": "Film and Video Editing \u2014 15s Broadcast Spot: \u201cCare Not Cutbacks\u201d", "rationale": "This rubric enforces a self-documenting delivery for a broadcast-ready 15-second MP4 and a companion edit package document. Stage 1 (LLM-only) mandates the exact deliverable shape (video + a tightly structured PDF/DOCX), enabling straightforward verification. Stage 2 mixes deterministic code checks (timecode math, URL presence, uniqueness constraints) with LLM verification of placement and alignment. Stage 3 evaluates holistic editorial quality and broadcast readiness.", "max_total_score": 26.0, "stages": [{"name": "Stage 1 \u2014 Deliverable Shape Enforcement (GATE)", "description": "Gate that requires both the finished MP4 and a structured companion Edit Package document with a specific, verifiable layout enabling downstream checks.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.5, "rules": [{"type": "llm_judge", "name": "Structured Deliverables Present (MP4 + Companion Edit Package)", "description": "Verify the candidate outputs include BOTH: (1) A 1920x1080 H.264 MP4 of exactly 15 seconds, and (2) a companion PDF or DOCX \u201cEdit Package\u201d document with the specified sections and structures. Do NOT judge content quality or correctness\u2014only presence and structure.", "weight": 6.0, "judge_prompt": "You are verifying deliverable SHAPE only. Review all outputs. Confirm the presence of BOTH items below, and that the companion document includes the required sections and tables. Do not judge the creative quality or the correctness of claims\u2014only the structure and format. Be flexible with section headings that are clearly equivalent.\n\nRequired Deliverables:\nA) Final Video Export\n- One H.264 .mp4 video file\n- 1920x1080 resolution\n- Exactly 15 seconds (00:15)\n\nB) Companion Edit Package (PDF or DOCX)\nMust include ALL sections below. A clean, clearly labeled structure is required (tables may be rendered as tables or neatly formatted lists). Be flexible with synonymous headings that clearly match.\n\n1) Project Cover & Info\n- Project title (e.g., \u201cCare Not Cutbacks\u201d) and client\n- Editor name, date, version\n\n2) Shot List & Timecode Breakdown (Table)\n- Columns (or clearly labeled fields): [Shot # | Start TC | End TC | Duration (sec) | Visual Description | Stock Source URL | License Type]\n- One row per shot used in the cut\n\n3) Supers Transcript & Placement (Table)\n- Columns: [Shot # | On-screen Text | In-TC | Out-TC | Duration (sec)]\n- Each super appears on a unique shot; include the initial shot without a super clearly indicated (e.g., \u201cNone\u201d)\n\n4) Music Cue Sheet (Table)\n- Columns: [Track Title | Composer/Artist | Library/Source | URL | License Type | In-TC | Out-TC | Edit Notes (e.g., fade-in/out, cutdown)]\n\n5) Color/Finishing & Motion Notes (Bulleted or brief paragraphs)\n- Global treatments (desaturation/darken amount), any slow/fast motion (\u2264 50% slow), reposition/blow-up notes\n\n6) Export Settings & QC Checklist\n- Export: H.264 .mp4, 1920x1080, exactly 15.00 seconds (state the frame rate used), stereo audio\n- QC checklist with explicit confirmations: no VO present; initial shot without super; one super per shot; supers shown long enough to read; dramatic tone; ending feels conclusive\n\nScoring (6 max):\n- 6.0: MP4 present + Companion PDF/DOCX present; all 6 sections present with appropriate tables/fields and clearly labeled. \n- 4.5: MP4 present + Companion doc present; only one section lightly under-specified (e.g., missing one minor column) but overall structure is clear. \n- 3.0: Both present but multiple sections missing or tables not discernible. \n- 0.0: Missing MP4 or missing companion document, or wrong formats.\n\nOnly evaluate the presence and structural completeness\u2014not correctness of numbers, timing, or creative quality.", "expectation": "A valid MP4 plus a well-structured companion PDF/DOCX that clearly exposes shots, supers, timing, sources, and export/QC info."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Structure-to-Correctness)", "description": "Deterministic code checks based on the companion document plus LLM cross-verification of timing, supers placement, and music edit. Code rules are intentionally lighter-weight than LLM rules.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Presence + Declared Export Settings", "description": "Checks that an .mp4 exists and companion PDF/DOCX text declares 1920x1080 and 15s duration.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0, \"No outputs.\"\n\n    # Find MP4\n    mp4_present = False\n    mp4_name = None\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            if p and str(p).lower().endswith('.mp4'):\n                mp4_present = True\n                mp4_name = p.name\n                break\n        except Exception:\n            continue\n\n    # Find companion doc (PDF or DOCX) and read text\n    doc_res = None\n    for r in outputs:\n        try:\n            if getattr(r, 'is_document', False):\n                doc_res = r\n                break\n        except Exception:\n            continue\n\n    text = \"\"\n    detail_hits = 0\n    if doc_res:\n        try:\n            # Attempt PDF first, then DOCX\n            if str(context.files.get_path(doc_res.id)).lower().endswith('.pdf'):\n                text = context.files.read_pdf_text(doc_res.id) or \"\"\n            else:\n                text = context.files.read_docx_text(doc_res.id) or \"\"\n        except Exception:\n            text = \"\"\n\n    # Scoring: 0.3 for MP4 presence, 0.2 for doc presence, 0.05 for 1920x1080 mention, 0.05 for 15s mention\n    score = 0.0\n    feedback_parts = []\n\n    if mp4_present:\n        score += 0.3\n        feedback_parts.append(f\"Found MP4: {mp4_name}\")\n    else:\n        feedback_parts.append(\"Missing MP4.\")\n\n    if doc_res is not None:\n        score += 0.2\n        feedback_parts.append(\"Found companion document.\")\n        t = text.lower()\n        if '1920x1080' in t or '1920 x 1080' in t:\n            score += 0.05\n            feedback_parts.append(\"Export settings mention 1920x1080.\")\n        else:\n            feedback_parts.append(\"No clear 1920x1080 mention in document.\")\n        # Duration mention: look for 15 sec expressions or 00:15 time\n        if re.search(r\"\\b(15\\s*(?:sec|secs|seconds|s))\\b\", t) or re.search(r\"\\b00?:?15\\b\", t):\n            score += 0.05\n            feedback_parts.append(\"Duration 15s is declared.\")\n        else:\n            feedback_parts.append(\"No explicit 15s duration declaration found.\")\n    else:\n        feedback_parts.append(\"Missing companion document (PDF/DOCX).\")\n\n    return min(score, 0.6), \"; \".join(feedback_parts)\n"}, {"type": "code", "name": "Shot Durations Sum to 15s", "description": "Parses durations (in seconds) from the Shot List section of the companion document and checks total duration ~15.0s.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    # Get companion document text\n    outputs = context.get_all_outputs()\n    doc_res = None\n    for r in outputs:\n        try:\n            if getattr(r, 'is_document', False):\n                doc_res = r\n                break\n        except Exception:\n            continue\n    if not doc_res:\n        return 0.0, \"No companion document found.\"\n\n    try:\n        path = str(context.files.get_path(doc_res.id)).lower()\n        if path.endswith('.pdf'):\n            text = context.files.read_pdf_text(doc_res.id) or \"\"\n        else:\n            text = context.files.read_docx_text(doc_res.id) or \"\"\n    except Exception:\n        return 0.0, \"Could not read companion document text.\"\n\n    t = text\n\n    # Narrow to Shot List section if possible\n    lower = t.lower()\n    start_idx = lower.find('shot list')\n    if start_idx == -1:\n        start_idx = lower.find('shots')\n    if start_idx == -1:\n        # fallback to entire doc\n        segment = t\n    else:\n        # end at next major section keyword\n        end_keys = ['super', 'music', 'cue', 'color', 'finishing', 'export', 'qc']\n        end_idx = len(t)\n        for k in end_keys:\n            pos = lower.find(k, start_idx + 5)\n            if pos != -1:\n                end_idx = min(end_idx, pos)\n        segment = t[start_idx:end_idx]\n\n    # Extract durations like \"1.8s\", \"2 s\", \"2.0 sec\", \"3 seconds\"\n    dur_matches = re.findall(r\"(?<![\\d.])(\\d+(?:\\.\\d+)?)\\s*(?:sec|secs|seconds|s)\\b\", segment, flags=re.IGNORECASE)\n    durations = []\n    for m in dur_matches:\n        try:\n            durations.append(float(m))\n        except Exception:\n            pass\n\n    if not durations:\n        return 0.0, \"No shot durations (seconds) detected in Shot List section.\"\n\n    total = sum(durations)\n    # Full credit if within \u00b10.10s, partial if within \u00b10.50s\n    diff = abs(total - 15.0)\n    if diff <= 0.10:\n        return 0.6, f\"Durations sum to {total:.2f}s (within \u00b10.10s).\"\n    elif diff <= 0.50:\n        return 0.3, f\"Durations sum to {total:.2f}s (within \u00b10.50s).\"\n    else:\n        return 0.0, f\"Durations sum to {total:.2f}s (outside tolerance).\"\n"}, {"type": "code", "name": "Unique Shot Per Super + Initial Shot Without Super", "description": "Checks the Supers Transcript section for one-to-one mapping of supers to unique shots, and that the initial shot is declared as having no super.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    # Get companion document text\n    outputs = context.get_all_outputs()\n    doc_res = None\n    for r in outputs:\n        try:\n            if getattr(r, 'is_document', False):\n                doc_res = r\n                break\n        except Exception:\n            continue\n    if not doc_res:\n        return 0.0, \"No companion document found.\"\n\n    try:\n        path = str(context.files.get_path(doc_res.id)).lower()\n        if path.endswith('.pdf'):\n            text = context.files.read_pdf_text(doc_res.id) or \"\"\n        else:\n            text = context.files.read_docx_text(doc_res.id) or \"\"\n    except Exception:\n        return 0.0, \"Could not read companion document text.\"\n\n    lower = text.lower()\n    # Identify Supers Transcript section\n    start = lower.find('super')\n    if start == -1:\n        return 0.0, \"No 'Supers' section detected.\"\n    # Heuristic end at next section\n    end_keys = ['music', 'cue', 'color', 'finishing', 'export', 'qc', 'shot list']\n    end = len(text)\n    for k in end_keys:\n        pos = lower.find(k, start + 5)\n        if pos != -1:\n            end = min(end, pos)\n    seg = text[start:end]\n\n    # Extract shot numbers and any indication of super text\n    shot_nums = re.findall(r\"shot\\s*#?\\s*(\\d+)\", seg, flags=re.IGNORECASE)\n    # Look for lines indicating no super or none\n    initial_no_super = bool(re.search(r\"shot\\s*#?\\s*1[^\\n]*\\b(no\\s*super|none|\\bnone\\b)\\b\", seg, flags=re.IGNORECASE))\n\n    uniq = len(set(shot_nums))\n    total = len(shot_nums)\n\n    score = 0.0\n    fb = []\n\n    # Uniqueness: if all shot references are unique, award 0.3; if mostly unique, partial 0.15\n    if total == 0:\n        fb.append(\"No shot references found under Supers Transcript.\")\n    else:\n        if uniq == total:\n            score += 0.3\n            fb.append(\"Supers map to unique shots (no duplicates).\")\n        elif uniq >= max(1, int(0.8 * total)):\n            score += 0.15\n            fb.append(\"Most supers map to unique shots (minor duplicates).\")\n        else:\n            fb.append(\"Many duplicate shot references for supers.\")\n\n    # Initial shot without super: 0.2 if clearly indicated\n    if initial_no_super:\n        score += 0.2\n        fb.append(\"Initial shot indicated as having no super.\")\n    else:\n        fb.append(\"Initial shot without super not clearly indicated.\")\n\n    return min(score, 0.5), \"; \".join(fb)\n"}, {"type": "code", "name": "Stock Footage and Music URLs Present", "description": "Extracts URLs from the companion document and verifies that at least two appear to be stock video sources and at least one appears to be a music source.", "weight": 0.3, "code": "import re\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    doc_res = None\n    for r in outputs:\n        try:\n            if getattr(r, 'is_document', False):\n                doc_res = r\n                break\n        except Exception:\n            continue\n    if not doc_res:\n        return 0.0, \"No companion document found.\"\n\n    try:\n        path = str(context.files.get_path(doc_res.id)).lower()\n        if path.endswith('.pdf'):\n            text = context.files.read_pdf_text(doc_res.id) or \"\"\n        else:\n            text = context.files.read_docx_text(doc_res.id) or \"\"\n    except Exception:\n        return 0.0, \"Could not read companion document text.\"\n\n    urls = re.findall(r\"https?://[^\\s)>,]+\", text)\n    if not urls:\n        return 0.0, \"No URLs detected.\"\n\n    stock_domains = [\n        'shutterstock.com', 'istockphoto.com', 'gettyimages.com', 'pond5.com', 'pexels.com', 'storyblocks.com', 'artgrid.io', 'videohive.net', 'envato.com'\n    ]\n    music_domains = [\n        'pond5.com', 'artlist.io', 'audiojungle.net', 'premiumbeat.com', 'musicbed.com', 'storyblocks.com', 'epidemicsound.com'\n    ]\n\n    def domain(u):\n        m = re.search(r\"https?://([^/]+)/\", u + '/')"}, {"type": "llm_judge", "name": "Supers Placement and Readability vs. Document", "description": "Cross-check the video against the Supers Transcript and Shot List. Verify: initial shot has no super; each subsequent super appears on a unique shot; supers are on screen long enough to read; and placements align with the timecodes claimed in the document.", "weight": 4.0, "judge_prompt": "Review the final MP4 video alongside the companion Edit Package document. Focus strictly on verifying placement and alignment.\n\nCheck:\n1) Initial Shot Without Super: The opening shot should have no super. \n2) One Super Per Shot: Each super should appear on its own unique shot (i.e., supers change when the shot changes). \n3) Readability Duration: Supers must be on-screen long enough to read comfortably (typically \u2265 1.0\u20131.5 seconds depending on length). \n4) Timecode Alignment: The in/out timing shown in the Supers Transcript should match what appears in the video within a small tolerance. \n\nScoring (4 max):\n- 4.0: All four checks pass.\n- 3.0: Minor timing deviations only; structure is correct.\n- 2.0: One major issue (e.g., a super spans two shots, or initial shot has a super), others OK.\n- 1.0: Multiple issues but supers generally present.\n- 0.0: Supers largely do not match the document or are unreadable.\n\nKeep this strictly to verification of placement, timing coherence, and readability\u2014not creative quality.", "expectation": "Video and document agree: first shot clean, supers change with shots, readable timing, coherent timecodes."}, {"type": "llm_judge", "name": "Music Edit Verification", "description": "Verify the background music is dramatic, edited to fit exactly 15 seconds with a sense of beginning and strong ending, and that no VO is present.", "weight": 3.0, "judge_prompt": "Review the MP4 and companion Music Cue Sheet. Verify:\n- The track feels dramatic and tonally appropriate for the stated issue.\n- The music edit is shaped to 15 seconds with a clear beginning and a strong ending (e.g., purposeful cadence, button, or fade).\n- There is no voiceover present.\n- The document\u2019s cue timing and edit notes plausibly match what you hear/see.\n\nScoring (3 max):\n- 3.0: All conditions met; edit feels intentional across 15s; no VO.\n- 2.0: Generally correct but the beginning/ending shape is weak OR minor mismatch to cue notes.\n- 1.0: Timing okay but tonal mismatch or unclear shaping.\n- 0.0: No clear 15s structure or VO present.\n\nFocus on verification against the documented intent and timing.", "expectation": "Dramatic 15s music bed with intentional arc and no VO, consistent with cue sheet."}, {"type": "llm_judge", "name": "Licensing/Sourcing and Content Coherence", "description": "Verify that the document\u2019s listed sources (stock footage and music) plausibly correspond to the visuals/audio in the MP4; and the supers\u2019 text in the video matches the transcript and the campaign theme.", "weight": 3.0, "judge_prompt": "Review the Edit Package document and the MP4. Verify:\n- The listed stock footage sources (URLs/domains) are plausible for the shots used.\n- The listed music source is plausible for the track used.\n- The supers\u2019 text on-screen matches the transcript and supports the campaign theme (Care Not Cutbacks; VitalNet; autism care context). \n- No contradictory or irrelevant supers.\n\nScoring (3 max):\n- 3.0: Sources plausibly align with visuals/audio; supers match transcript and theme.\n- 2.0: Minor discrepancies but overall coherent.\n- 1.0: Multiple mismatches or unclear sourcing.\n- 0.0: Sources absent/mismatch and supers do not align with the theme.\n\nDo not judge creative excellence here\u2014only coherence and consistency with the documented sources and the campaign message.", "expectation": "Plausible links align with what\u2019s used; supers are thematically and textually consistent with the transcript."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism", "description": "Holistic assessment of editorial pacing, graphic legibility, finishing, and broadcast readiness.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Emotional Tone and Pacing", "description": "Does the cut feel appropriately dramatic, paced to emphasize gravity, with shots and timing serving the message within 15 seconds?", "weight": 2.0, "judge_prompt": "Evaluate overall tone and pacing of the 15s spot. Is the pacing measured (not rushed), supporting emotional gravity? Do shot choices and durations create urgency and seriousness without sensationalism? Does the flow feel cohesive within 15 seconds?\n\nScoring (2 max):\n- 2.0: Strong, cohesive dramatic tone; pacing supports message.\n- 1.0: Mixed effectiveness but generally serviceable.\n- 0.0: Tone/pacing undermine the message.", "expectation": "Measured, emotionally resonant pacing and tone supporting the campaign."}, {"type": "llm_judge", "name": "Graphics Legibility and Composition", "description": "Assess the readability of supers (contrast, font size/weight, safe margins, placement) and whether each super is comfortably readable.", "weight": 2.0, "judge_prompt": "Assess on-screen supers: contrast against footage, font size/weight, safe margins, and placement. Are they cleanly readable for the duration they appear? Any crowding into title-safe? Any color/background treatments (e.g., subtle box/shadow) aiding readability?\n\nScoring (2 max):\n- 2.0: Clear, professional, comfortably readable supers throughout.\n- 1.0: Minor issues but mostly readable.\n- 0.0: Readability problems undermine comprehension.", "expectation": "Supers are clean, high-contrast, within safe areas, and readable for long enough."}, {"type": "llm_judge", "name": "Color Grading and Finishing Quality", "description": "Evaluate desaturation/darken treatment, any slow motion or reframe, and overall polish (transitions, cuts, noise/grain, artifacts).", "weight": 2.0, "judge_prompt": "Evaluate finishing: does the desaturation/darken treatment convey a somber tone without crushing detail? Are any slow-motion or reframing moves tasteful and stable? Are cuts/transitions clean? Any compression artifacts, flicker, or distracting issues?\n\nScoring (2 max):\n- 2.0: Strong finishing; tasteful grading; stable motion.\n- 1.0: Minor finishing issues but acceptable.\n- 0.0: Finishing/grade issues distract or feel inappropriate.", "expectation": "Polished, tasteful finishing that supports the somber tone."}, {"type": "llm_judge", "name": "Broadcast Readiness and Professionalism", "description": "Assess start/end cleanliness, absence of VO, watermark tolerance, and overall professional polish suitable for client review.", "weight": 2.0, "judge_prompt": "Assess broadcast readiness: clean start (no unintended black/pops), clean end with purpose; no voiceover; any watermarks are acceptable per brief but should not dominate; overall polish suitable for client review.\n\nScoring (2 max):\n- 2.0: Ready for client review; clean, professional.\n- 1.0: Minor issues but acceptable for review.\n- 0.0: Not ready (technical or presentation issues).", "expectation": "Clean start/end, no VO, professional presentation suitable for client review."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "9e8607e7-a38a-491f-ace1-e5ea7dc477cb", "rubric": {"category_name": "LatAm Fintech Strategy Deck (PDF) \u2014 Structured, Verifiable Client Presentation", "rationale": "This rubric enforces a self-documenting, verifiable PDF slide deck that an MD could present to a global consumer internet client. Stage 1 is a strict LLM-only format/structure gate that requires a slide-based PDF with specific sections, dividers, and visual artifacts so later checks are trivial. Stage 2 mixes light code-based text parsing (keywords, years, sources) with heavier LLM judgment on plausibility, coverage, and consistency. Stage 3 assesses professional quality, clarity, and strategic value for a client conversation. Code rules are intentionally narrower in weight than LLM rules (~5x less on average) to reflect nuanced judgment needs while still providing deterministic checks.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Format & Structure Gate (LLM only)", "description": "Gate that verifies the output is a professionally structured slide-deck PDF with the exact sections and artifacts required for downstream verification.", "is_required": true, "max_points": 2.0, "min_score_to_pass": 1.2, "rules": [{"type": "llm_judge", "name": "Presentation Shape and Section Completeness", "description": "Verify the candidate output is a slide-style PDF with required sections, counts, and visual artifacts to enable verification. Only check structure/presence, not content quality or correctness.", "weight": 2.0, "judge_prompt": "You are evaluating a client-facing presentation. Check ONLY structure and format (not content quality). The output must be a slide-style PDF exported from PowerPoint/Keynote (landscape slides). Be flexible with section naming, but ensure the required structure exists.\n\nFormat requirements:\n- File type: PDF (not Word, not Excel, not plain text)\n- Slide-based layout (landscape pages with slide formatting)\n- Slide count: roughly 24\u201336 slides (target ~30). Minor deviation acceptable if structure is complete.\n- Professional slide formatting with clear slide titles/headings.\n\nRequired slides/sections (accept close synonyms):\n1) Title/Cover slide with presentation title and date (Fall 2023 timeframe acceptable).\n2) Agenda/Contents slide listing three main sections.\n3) Section divider: \"Latin America Macro Overview\" (or similar wording).\n   - 6\u20138 slides covering: GDP/size & growth, inflation/monetary policy, FX, demographics/urbanization/digital/internet/mobile, financial/banking inclusion (unbanked/underbanked), at least one country snapshot slide for major markets (Brazil, Mexico; ideally also Colombia/Argentina/Chile/Peru), and macro risks.\n4) Section divider: \"State of LatAm Technology and Venture Markets\" (synonyms OK).\n   - 6\u20138 slides covering: venture funding trends 2018\u20132023 YTD, stage mix (Seed/Series A/B+), sector mix with fintech share, unicorns/exits IPO/M&A, international/cross-border investors, and a post-2021 cycle/2022\u201323 reset.\n5) Section divider: \"Latin America Fintech Landscape\" (synonyms OK).\n   - 8\u201310 slides covering: sub-sectors (payments/acquiring, wallets, BNPL, lending/credit, neobanks, infrastructure/open banking, insurtech, remittances), key players by country, regulatory context (Brazil and Mexico at minimum; PIX/SPEI or open banking/open finance), competitive maps/value chain and partnership opportunities.\n6) Closing section with: Operating considerations vs. Investing considerations; Next steps.\n7) Appendix and Sources/References slide(s).\n\nVisual artifacts requirements:\n- At least 3 charts/graphs across the deck.\n- At least 1 table.\n- At least 1 LatAm regional map or country map/heatmap.\n- Sources/footnotes present on relevant slides or in a dedicated Sources slide.\n\nScoring guidance (structure only):\n- 2.0: PDF slide deck with 24\u201336 slides; all three main sections present with their expected subtopics; title, agenda, closing (ops vs investing + next steps), appendix, and sources present; at least 3 charts, 1 table, and 1 map present.\n- 1.6: PDF slide deck; all main sections present but one required subtopic or one visual artifact missing (e.g., no map or table) OR slide count slightly outside range but otherwise fully structured.\n- 1.2: PDF slide deck; main sections present but multiple required subtopics missing OR missing closing OR missing appendix/sources.\n- 0.8: PDF slide deck but missing an entire main section OR largely unstructured (no dividers/agenda) OR very low slide count (<20).\n- 0.0: Not a PDF; not slide-formatted; egregiously incomplete structure.\n\nOnly evaluate presence/format/structure. Do not judge correctness or quality beyond these checks.", "expectation": "A slide-based PDF (~30 slides) with clearly labeled sections, dividers, agenda, closing, appendix/sources, and required visual elements (charts, table, map)."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification: Coverage, Recency, and Consistency", "description": "Checks that the deck verifiably covers the required topics, geographies, timeframe, sourcing, and contains plausible, self-consistent content. Mix of code rules for text signals and LLM rules for nuanced validation.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Topic Coverage Signals (Macro/Tech/Fintech)", "description": "Parse PDF text for keyword coverage across macro, venture, and fintech topics. Rewards breadth of coverage signals.", "weight": 0.25, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception as e:\n        return 0.0, f\"Failed to read PDF text: {e}\"\n    if not text:\n        return 0.0, \"Empty PDF text.\"\n    t = text.lower()\n    macro_keywords = [\n        'gdp','inflation','cpi','fx','exchange rate','currency','monetary policy',\n        'demographic','population','urbanization','middle class','internet penetration',\n        'mobile','smartphone','e-commerce','banking penetration','financial inclusion',\n        'unbanked','underbanked'\n    ]\n    venture_keywords = [\n        'venture','funding','deal','round','seed','series a','series b','late-stage',\n        'valuation','unicorn','exit','ipo','m&a','investor','capital','dry powder','ytd'\n    ]\n    fintech_keywords = [\n        'fintech','payments','acquiring','wallet','lending','credit','bnpl','buy now pay later',\n        'neobank','digital bank','insurtech','remittance','cross-border','infrastructure',\n        'open banking','open finance','pix','spei','instant payment','qr','issuer','acquirer',\n        'kyc','fraud','compliance','risk'\n    ]\n\n    def coverage_score(keywords, threshold):\n        hits = sum(1 for k in keywords if k in t)\n        return min(1.0, hits / float(threshold))\n\n    macro = coverage_score(macro_keywords, 6)\n    venture = coverage_score(venture_keywords, 6)\n    fintech = coverage_score(fintech_keywords, 8)\n    score = (macro + venture + fintech) / 3.0\n    fb = f\"Macro:{macro:.2f} Venture:{venture:.2f} Fintech:{fintech:.2f}\"\n    return max(0.0, min(1.0, score)), fb"}, {"type": "code", "name": "Country Coverage (Core LatAm Markets)", "description": "Check that major LatAm countries are mentioned in the deck text.", "weight": 0.25, "code": "def evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception as e:\n        return 0.0, f\"Failed to read PDF text: {e}\"\n    if not text:\n        return 0.0, \"Empty PDF text.\"\n    t = text.lower()\n    countries = ['brazil','mexico','colombia','argentina','chile','peru']\n    present = [c for c in countries if c in t]\n    score = len(present) / len(countries)\n    fb = f\"Countries mentioned: {', '.join(present) if present else 'none'}\"\n    return max(0.0, min(1.0, score)), fb"}, {"type": "code", "name": "Recency and Time Horizon", "description": "Reward presence of recent timeframe and multi-year context (2018\u20132023).", "weight": 0.25, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception as e:\n        return 0.0, f\"Failed to read PDF text: {e}\"\n    if not text:\n        return 0.0, \"Empty PDF text.\"\n    t = text.lower()\n    years = [str(y) for y in range(2018, 2024)]\n    found = set()\n    for y in years:\n        if y in t:\n            found.add(y)\n    has_2023 = '2023' in found\n    distinct = len(found)\n    # Scoring: 0.5 for mentioning 2023; +0.5 for >=3 distinct years in 2018\u20132023\n    score = 0.0\n    if has_2023:\n        score += 0.5\n    if distinct >= 3:\n        score += 0.5\n    fb = f\"Years found: {sorted(found)}\"\n    return max(0.0, min(1.0, score)), fb"}, {"type": "code", "name": "Sourcing and Quantification Signals", "description": "Check for the presence of sources/references and quantitative markers ($, %). Bonus if credible sources appear.", "weight": 0.25, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception as e:\n        return 0.0, f\"Failed to read PDF text: {e}\"\n    if not text:\n        return 0.0, \"Empty PDF text.\"\n    t = text.lower()\n    has_source_word = ('source:' in t) or ('sources:' in t) or ('source ' in t) or ('sources ' in t)\n    credible_list = ['world bank','imf','bis','oecd','wef','cb insights','pitchbook','crunchbase','lavca','statista','bain','mckinsey','bcg','idb','banco central','banxico']\n    has_credible = any(s in t for s in credible_list)\n    has_currency = bool(re.search(r'\\$\\s?\\d', t))\n    has_percent = bool(re.search(r'\\d+\\s?%',' '.join(t.split())))\n    components = [has_source_word, has_credible, (has_currency or has_percent)]\n    score = sum(1 for c in components if c) / 3.0\n    noted = []\n    if has_source_word: noted.append('source/s present')\n    if has_credible: noted.append('credible source detected')\n    if has_currency or has_percent: noted.append('quant markers ($ or %) present')\n    fb = f\"Signals: {', '.join(noted) if noted else 'none'}\"\n    return max(0.0, min(1.0, score)), fb"}, {"type": "llm_judge", "name": "Topical Plausibility and Internal Consistency", "description": "Judge if macro, venture, and fintech assertions are broadly plausible for 2018\u20132023 and do not contradict each other (e.g., funding boom 2021, reset 2022\u201323; Brazil PIX; Mexico SPEI). Spot any glaring inconsistencies.", "weight": 1.0, "judge_prompt": "Check the PDF for topical plausibility and internal consistency for 2018\u20132023:\n- Macro: Inflation surge 2022, elevated rates, FX volatility plausible; demographics/digital penetration trends sensible.\n- Venture: Funding boom in 2021 and contraction in 2022\u20132023 plausible; stage mix and sector mix reasonable; exits/unicorn counts not obviously contradictory.\n- Fintech: Brazil PIX and open finance references plausible; Mexico SPEI; common sub-sectors (payments, neobanks, lending, insurtech, remittances) are represented reasonably.\n- Internal consistency: Similar metrics (e.g., market sizes, rankings) should not conflict across slides.\nScoring:\n- 1.0: Broadly plausible with no material contradictions.\n- 0.7: Minor inconsistencies but acceptable overall.\n- 0.4: Multiple questionable claims or inconsistencies.\n- 0.0: Clearly implausible narrative or self-contradictory.\nOnly judge plausibility/consistency, not style or visuals.", "expectation": "A coherent, plausible deck aligned with well-known 2018\u20132023 LatAm trends and no contradictions."}, {"type": "llm_judge", "name": "Operational vs Investing Considerations Coverage", "description": "Verify the deck explicitly covers both operating and investing angles with actionable considerations.", "weight": 1.0, "judge_prompt": "Confirm the deck includes both:\n- Operating considerations (e.g., regulatory licensing, FX/cash management, payments rails, talent/hiring, legal/tax setup, data/AML/KYC, local partnerships).\n- Investing considerations (e.g., entry strategy, ownership structures, JV/partnership options, co-investors, valuation drivers, pipeline, risks, governance).\nAlso check that next steps or a roadmap slide exists.\nScoring:\n- 1.0: Both angles covered with at least 3 concrete points each + next steps/roadmap present.\n- 0.7: Both covered but one side is light (1\u20132 points) or next steps missing.\n- 0.4: Only one side covered.\n- 0.0: Neither covered or unclear.", "expectation": "Clear, actionable bullets for operations and investment, plus a next-steps slide."}, {"type": "llm_judge", "name": "Regulatory Coverage Depth (Brazil & Mexico emphasis)", "description": "Assess whether key regulatory elements are addressed, particularly for Brazil and Mexico.", "weight": 1.0, "judge_prompt": "Check that the deck covers regulatory context with emphasis on Brazil and Mexico:\n- Brazil: PIX/instant payments, open banking/open finance, relevant licensing and regulator references (e.g., Banco Central do Brasil).\n- Mexico: SPEI/real-time rails, fintech law, licensing categories, Banxico/CNBV references.\n- Optional: Brief notes on Colombia/Chile/Peru where applicable.\nScoring:\n- 1.0: Brazil and Mexico specifics clearly addressed; at least 2 items for Brazil and 2 for Mexico; mentions other markets briefly.\n- 0.7: Both countries addressed but shallow (1 item each) or missing others entirely.\n- 0.4: Only one of Brazil/Mexico addressed.\n- 0.0: No meaningful regulatory coverage.", "expectation": "Meaningful country-level specifics for Brazil and Mexico; brief regional mentions elsewhere."}, {"type": "llm_judge", "name": "Visual Evidence Supports Claims", "description": "Evaluate whether charts/tables/maps meaningfully support the narrative and are interpretable (titles, labels/legends, units).", "weight": 1.0, "judge_prompt": "Verify the deck uses visuals to support key claims:\n- At least 3 charts and 1 table and 1 map should exist (structure gate likely ensured this).\n- Visuals should have titles and generally interpretable labels/legends/axes/units.\n- Visuals should be tied to points made on the slide or adjacent text.\nScoring:\n- 1.0: Visual mix present with clear titles/labels; visuals support the arguments.\n- 0.7: Visuals present but some are unlabeled or weakly tied to claims.\n- 0.4: Visuals are present but mostly decorative or unlabeled.\n- 0.0: Visuals absent or unusable.", "expectation": "Labeled, readable visuals that substantiate the deck\u2019s key points."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Client Readiness", "description": "Holistic assessment of professionalism, clarity, and strategic usefulness for a senior client discussion.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Executive Readability and Storyline", "description": "Assesses clarity of messaging, slide titling, and logical flow tailored to a senior audience.", "weight": 0.75, "judge_prompt": "Evaluate the deck\u2019s executive readability:\n- Clear, informative slide titles and succinct bullets.\n- Logical flow from macro to tech/venture to fintech, ending with implications/next steps.\n- High-level orientation suitable for a 30\u201360 minute discussion.\nScoring: 0.75 excellent clarity and flow; 0.5 decent with minor issues; 0.25 confusing in parts; 0.0 poor.", "expectation": "Concise, well-titled slides with a coherent narrative arc."}, {"type": "llm_judge", "name": "Design and Visual Communication", "description": "Professional polish, whitespace, consistency, and readability for print/presentation.", "weight": 0.75, "judge_prompt": "Assess design quality:\n- Consistent formatting, typography, and color usage.\n- Adequate whitespace and legible charts/tables.\n- Professional look suitable for a client MD meeting.\nScoring: 0.75 strong professional design; 0.5 minor inconsistencies; 0.25 cluttered/messy; 0.0 unprofessional.", "expectation": "Clean, consistent, and readable design throughout."}, {"type": "llm_judge", "name": "Strategic Value and Actionability for the Client", "description": "Does the deck help a global consumer internet client assess LatAm fintech operations and investments effectively?", "weight": 0.75, "judge_prompt": "Judge strategic usefulness:\n- Clear articulation of where to explore operationally (market entry, partners, rails, compliance) and investment theses (sub-sectors, stages, co-investors).\n- Prioritized opportunities and key risks.\n- Enables informed follow-up workstreams.\nScoring: 0.75 highly actionable; 0.5 somewhat actionable; 0.25 generic; 0.0 not actionable.", "expectation": "Actionable insights tailored to operational and investing decisions."}, {"type": "llm_judge", "name": "Balance and Risk Awareness", "description": "Evaluates whether the deck is balanced (opportunities vs risks) and avoids salesy bias.", "weight": 0.75, "judge_prompt": "Assess balance and risk framing:\n- Presents both upside and risks (macro, regulatory, FX, competitive, execution).\n- Neutral, analytical tone appropriate for an advisor.\nScoring: 0.75 balanced and nuanced; 0.5 some imbalance; 0.25 biased; 0.0 one-sided or promotional.", "expectation": "Even-handed presentation of opportunities and risks."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "ec591973-04d5-48c0-981c-1ab2fcec2dc1", "rubric": {"category_name": "Executive Strategy Slide Rubric \u2014 Wholesale Trade (First-Line Supervisors of Non-Retail Sales Workers)", "rationale": "This rubric enforces a self-documenting, verifiable one-slide strategy deliverable. Stage 1 (LLM-only) mandates exact structural elements in a PowerPoint/PDF slide to enable verification. Stage 2 mixes light code checks (file-type validity) with LLM verification of content correctness grounded in the mandated structure (channel differentiation, challenge-to-action mapping, and investment prioritization logic). Stage 3 provides a holistic quality assessment of executive clarity, visual design, strategic coherence, and actionability.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (STRUCTURE ONLY)", "description": "Gate that verifies the deliverable is a single-slide executive strategy deck with required sections and artifacts enabling verification. LLM-only checks; failing this gate zeros the category.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Single-Slide PowerPoint/PDF With Required Sections", "description": "Confirm the candidate output is a single slide (PPTX or PDF exported from PPT) with the mandated sections and artifacts. Check presence/format only, not quality or correctness.", "weight": 4.0, "judge_prompt": "You are evaluating ONLY the presence and structure of a one-slide executive strategy deliverable. Use the rendered preview of the file (PPTX or PDF). Do not judge content quality or correctness here.\n\nFormat requirements:\n- File format: PowerPoint (PPTX). PDF is acceptable only if clearly an exported single slide from PowerPoint.\n- Exactly one slide/page (a one-pager). If multiple slides/pages, fail.\n- Professional slide formatting (title, section headers, readable layout). \n\nRequired visible sections on the single slide (flexible header names allowed, but intent must be clear):\n1) Title that references differentiated distribution investment (e.g., \u201cDifferentiated Distribution Investment to Protect Client Retention\u201d).\n2) Strategic Objective or Purpose (why differentiated investment matters for retention/brand health).\n3) Channel Roles & Differentiated Levers table/matrix (a visible grid or structured table) covering the three channels:\n   - Open-sell (department store/open-sell environments)\n   - Traditional specialty stores\n   - Owned brand boutiques/flagships\n   For each channel, indicate at least some of these levers: assortment, activations, CRM, loyalty, collateral, services, value/GWP/curated sets.\n4) Investment Prioritization Framework (e.g., tiering, 2x2 value vs. feasibility/ROI, or allocation principles). It must be a labeled section or visual.\n5) Risks/Constraints & Mitigations section that references current business challenges (store closures, staffing/activation efficiency, expertise gap in open-sell, low ROI/over-assortment/stockouts, avoid over-investment).\n6) KPIs/Success Metrics section (examples: repeat rate, client retention, NPS/CSAT, OOS rate, door productivity, LTV, ROI).\n\nOptional but beneficial (do not penalize if missing):\n- A small \u201cPresenter Notes/Pitch Script\u201d area on the slide with 2\u20136 bullet points for a 5-minute pitch.\n- At least one visual element (icons, matrix, simple chart) beyond plain text.\n\nScoring (return a single number 0.0\u20131.0 reflecting compliance):\n- 1.0: PPTX or exported single-slide PDF; all 6 required sections visibly present and clearly labeled/structured.\n- 0.8: PPTX/PDF one-pager; exactly one of the required sections missing or ambiguously labeled; others present.\n- 0.5: PPTX/PDF one-pager; two required sections missing OR channel table/matrix missing/unclear.\n- 0.2: Wrong file type OR not evidently a slide OR more than one slide/page, even if some sections exist.\n- 0.0: No identifiable slide structure or major format mismatch (e.g., plain text doc).", "expectation": "A single professional slide with the six required sections and a channel-by-levers matrix enabling later verification."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness and Internal Consistency)", "description": "Verify that the structured slide contains specific, defensible content mappings and logic consistent with the brief. Mix light code checks with LLM judgment. Focus on correctness, not display polish.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Valid Slide File Type", "description": "Basic deterministic check: ensure the primary output is a document and is PPTX or PDF.", "weight": 1.5, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n\n    Returns:\n        float (score) or tuple[float, str] (score, feedback)\n    \"\"\"\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output found.\"\n\n    # Must be a document (PPTX or PDF acceptable)\n    if not getattr(output, 'is_document', False):\n        return 0.0, \"Primary output is not a document-type file.\"\n\n    try:\n        path = context.files.get_path(output.id)\n        suffix = path.suffix.lower()\n        if suffix in [\".pptx\", \".pdf\"]:\n            return 1.5, f\"Valid document type: {suffix}\"\n        else:\n            return 0.0, f\"Invalid document type: {suffix}. Expected .pptx or .pdf\"\n    except Exception as e:\n        return 0.0, f\"Exception checking file type: {e}\""}, {"type": "llm_judge", "name": "Channel Differentiation Specificity", "description": "Check that each channel (open-sell, specialty, owned boutiques) has specific, differentiated tactics across assortment and marketing levers (activations, CRM, loyalty, collateral/services, value constructs like curated sets/GWP).", "weight": 3.0, "judge_prompt": "Evaluate the content of the slide for concrete channel differentiation. Consider only the visible content on the one slide.\n\nRequirements to check:\n- For each channel (open-sell, traditional specialty, owned boutiques/flagships), are there explicit, distinct tactics?\n- Coverage across levers: assortment, activations, CRM, loyalty, collateral/services, and value constructs (e.g., curated sets, exclusive services, gift-with-purchase).\n- Does the differentiation reflect client lifecycle retention (e.g., acquisition vs. retention/upsell) and acknowledge channel-shopping shifts?\n\nScoring (0.0\u20131.0):\n- 1.0: All three channels present with clearly distinct tactics spanning multiple levers; lifecycle/shift considerations evident.\n- 0.7: All channels present but one is thin/overlapping; most levers covered; lifecycle mention light.\n- 0.4: One channel weak or missing; tactics largely generic; levers sparsely covered.\n- 0.0: No real channel differentiation beyond generic statements.", "expectation": "Three clearly differentiated channel strategies with lever-level specificity tied to lifecycle retention."}, {"type": "llm_judge", "name": "Challenge-to-Action Mapping", "description": "Verify explicit mapping from listed business challenges to mitigations/actions on the slide.", "weight": 3.0, "judge_prompt": "Check whether the slide clearly addresses each of the stated challenges with a corresponding mitigation or action principle. Challenges to look for and map:\n1) Corporate store closures (esp. specialty)\n2) Resource efficiency in staffing/activations\n3) Lack of brand expertise in open-sell (value communication for high-price products)\n4) Low ROI in over-assorted, low-volume doors; frequent stockouts\n5) Need to optimize allocation and avoid over-investment where returns are unsustainable\n\nEvidence of correctness includes: explicit bullets under a Risks/Constraints & Mitigations section, embedded policies in the prioritization framework (e.g., tiering, thresholds), or notes tied to channels.\n\nScoring (0.0\u20131.0):\n- 1.0: Direct, traceable mitigations for all five challenges, clearly stated.\n- 0.7: Four of five mapped with reasonable specificity.\n- 0.4: Two\u2013three challenges addressed; others vague or missing.\n- 0.0: Little to no mapping to the specified challenges.", "expectation": "Each specified challenge is explicitly paired with a practical mitigation or decision rule on the slide."}, {"type": "llm_judge", "name": "Investment Prioritization Rigor", "description": "Assess whether the slide includes a defensible, long-term investment allocation framework that prioritizes retention/brand health and clarifies trade-offs (what to fund vs. avoid).", "weight": 2.5, "judge_prompt": "Evaluate the investment prioritization framework on the slide for rigor and clarity.\n\nLook for:\n- Criteria such as retention impact, door productivity, ROI/feasibility, brand health signals, client LTV.\n- A clear method (tiering A/B/C, 2x2 matrix, thresholds, or allocation principles) showing where to invest more/less across channels/doors.\n- Explicit trade-offs and guardrails (e.g., reduce assortment in low-volume/stockout-prone doors; concentrate staff/training in high-potential specialty; deploy education assets in open-sell).\n- Long-term orientation: prioritizing customer experience and sustainable value vs. short-term volume.\n\nScoring (0.0\u20131.0):\n- 1.0: Clear criteria plus mechanism (matrix/tiering) and stated trade-offs aligned to long-term brand health.\n- 0.7: Criteria present and some allocation logic; trade-offs partially explicit.\n- 0.4: Vague principles without concrete mechanism or criteria.\n- 0.0: No meaningful prioritization mechanism.", "expectation": "A specific, defensible allocation method with criteria and trade-offs that optimize long-term retention and brand health."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality Assessment (Holistic)", "description": "Judge professional quality, executive suitability, strategic coherence, and actionability. LLM-only qualitative evaluation.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Executive Clarity and Brevity", "description": "Assess whether a senior leader can grasp the strategy in 60\u201390 seconds and deliver a 5-minute pitch from it.", "weight": 1.5, "judge_prompt": "Evaluate the slide for concise, executive-ready communication.\nConsider: sharp headlines, minimal jargon, logical flow, and whether a 5-minute elevator pitch is naturally supported.\n\nScoring (0.0\u20131.0):\n- 1.0: Crystal-clear headlines and flow; easy to verbalize a tight 5-minute pitch.\n- 0.7: Generally clear with minor clutter or wording issues.\n- 0.4: Dense or jargon-heavy; talk track not obvious.\n- 0.0: Confusing or incoherent.", "expectation": "A crisp one-pager that clearly supports a short executive pitch."}, {"type": "llm_judge", "name": "Visual Design and Readability", "description": "Evaluate visual hierarchy, spacing, typography, and legibility of tables/visuals on a single slide.", "weight": 1.5, "judge_prompt": "Assess the slide\u2019s professional design: hierarchy of information, whitespace, font sizing/contrast, color discipline, and legibility of any tables/visuals.\n\nScoring (0.0\u20131.0):\n- 1.0: Polished, readable at a glance; tables/visuals are clean and legible.\n- 0.7: Minor design issues but overall readable.\n- 0.4: Crowded or inconsistent; difficult to scan.\n- 0.0: Poor readability and messy layout.", "expectation": "A clean, professional one-page slide with clear hierarchy and easily readable elements."}, {"type": "llm_judge", "name": "Strategic Coherence and Long-Term Orientation", "description": "Judge whether the narrative coherently prioritizes customer experience and long-term value while addressing near-term constraints.", "weight": 1.5, "judge_prompt": "Evaluate the strategic coherence and long-term orientation.\nConsider: clear link from differentiated channel strategy to client retention/LTV and brand health; balanced acknowledgement of constraints; sustainable approach vs. short-term tactics.\n\nScoring (0.0\u20131.0):\n- 1.0: Strong, coherent story linking investments to long-term retention and brand health.\n- 0.7: Mostly coherent; long-term framing present but not dominant.\n- 0.4: Disconnected points with limited long-term framing.\n- 0.0: No coherent strategy.", "expectation": "A cohesive long-term strategy centered on the customer experience and sustainable brand value."}, {"type": "llm_judge", "name": "Actionability and Measurement", "description": "Assess clarity of next steps/guardrails and the relevance of KPIs to retention and distribution performance.", "weight": 1.5, "judge_prompt": "Evaluate whether the slide yields clear next steps or decision guardrails and includes relevant KPIs.\nLook for: unambiguous actions, thresholds/tiers, and metrics such as repeat rate, retention, NPS/CSAT, OOS rate, door productivity, ROI, LTV.\n\nScoring (0.0\u20131.0):\n- 1.0: Clear, actionable guidance with directly relevant KPIs.\n- 0.7: Generally actionable; KPIs somewhat generic but acceptable.\n- 0.4: Vague actions or weak metrics.\n- 0.0: Not actionable and lacks meaningful KPIs.", "expectation": "Concrete actions and measurement that enable leadership decisions and accountability."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "ffed32d8-d192-4e3f-8cd4-eda5a730aec3", "rubric": {"category_name": "Retail Pharmacy: Auto-Refill 90-day vs 100-day Financial Impact Analysis", "rationale": "Pattern C (Mixed): The deliverable is a one\u2013two page PDF report (document) that must include a comparative table (embedded quantitative analysis). Stage 1 uses an LLM gate to enforce a strict, auditable structure that makes verification trivial. Stage 2 combines lightweight code checks (text extraction, token presence) with higher-weight LLM judgment for methodology and decision consistency. Stage 3 assesses professional quality and usefulness for an operational manual. Code rules are kept ~5x lighter than LLM rules in Stage 2.", "max_total_score": 13.0, "stages": [{"name": "Stage 1 \u2014 Format and Structural Gate (LLM only)", "description": "Enforce exact document format and structural elements required to enable verification. If this gate fails, the entire category scores 0.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.1, "rules": [{"type": "llm_judge", "name": "PDF Report Shape and Required Sections Present", "description": "Verify the output is a 1\u20132 page PDF (or DOCX) report with the exact sections, formulas, and a comparative table enabling verification.", "weight": 3.0, "judge_prompt": "You are evaluating whether the candidate output satisfies the REQUIRED STRUCTURE for a mixed analysis/report deliverable. Only check presence/structure, not correctness of numbers.\n\nAcceptable formats: PDF (preferred) or DOCX. Length: 1\u20132 pages. Professional formatting with clear headings and a readable table.\n\nRequired elements (be flexible with exact header wording, but ALL must be present):\n1) Title and Context\n   - Mentions an auto-refill policy comparison of 90-day vs 100-day fills for an independent pharmacy making ~$800,000 annual revenue\n   - States goal: maintain adherence >80%, but decision here is based solely on annual revenue impact\n\n2) Methodology & Assumptions section\n   - Must explicitly state ALL of the following:\n     \u2022 10 maintenance medications (the listed drugs and strengths)\n     \u2022 300 patients per medication enrolled in auto-refill\n     \u2022 Dosing: 1 tablet once daily\n     \u2022 Fills/year difference: 90-day = 4 fills (covers ~360 days); 100-day = 3 fills (covers ~300 days)\n   - Shows formulas in words or simple math for annual calculations for each model:\n     \u2022 Annual Expense = (Drug Cost per Fill + Vial/Supply Cost per Fill) \u00d7 Fills/Year \u00d7 300\n     \u2022 Annual Reimbursement = Reimbursement per Fill \u00d7 Fills/Year \u00d7 300\n     \u2022 Annual Revenue = Annual Reimbursement \u2212 Annual Expense\n   - References using Wholesale Price.pdf (for drug and vial costs) and Reimbursement.pdf (for reimbursement per fill)\n\n3) Comparative Table (single consolidated table)\n   - One row per drug/strength (10 rows total) plus a Totals row\n   - Columns must enable verification of both models. Accept either grouped columns per model or clearly labeled separate columns, but ensure the following fields are explicitly visible:\n     \u2022 Drug Name\n     \u2022 Strength (mg)\n     \u2022 Patients (should show 300 or reference constant of 300)\n     \u2022 90-day model: Fills/Year (4), Drug Cost/Fill, Vial/Supply Cost/Fill, Annual Expense, Reimbursement/Fill, Annual Reimbursement, Annual Revenue\n     \u2022 100-day model: Fills/Year (3), Drug Cost/Fill, Vial/Supply Cost/Fill, Annual Expense, Reimbursement/Fill, Annual Reimbursement, Annual Revenue\n     \u2022 Revenue Difference (100-day minus 90-day) per drug\n   - Totals row summing Annual Revenue for 90-day and 100-day and the overall Revenue Difference\n\n4) Summary & Recommendation section\n   - States total annual revenue = ~$800,000 baseline and 2% threshold = $16,000\n   - Clearly states whether the overall revenue difference is < or \u2265 $16,000 and gives a final recommendation (switch to 100-day if <2%; maintain 90-day if \u22652%)\n\nScoring guidance:\n- 3.0: Valid format (PDF/DOCX), 1\u20132 pages, and all four required sections present with the specified sub-elements, including the full comparative table and totals row.\n- 2.5: Valid format and length, minor structural omissions (e.g., missing one sub-field in table OR formulas partially specified) but core sections and table exist.\n- 1.5: Valid format and length, but missing a required section or the table is not sufficient to verify both models per drug.\n- 0.0: Not PDF/DOCX, or missing multiple core sections, or no comparative table, or length grossly outside 1\u20132 pages.\n\nOnly evaluate structure/presence, not numerical correctness.", "expectation": "A concise, professional 1\u20132 page PDF/DOCX with Methodology & Assumptions, a comprehensive comparative table (10 drugs + totals), and a clear summary and recommendation referencing the $16,000 (2%) threshold."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Consistency Verification", "description": "Verify logical and methodological correctness based on the enforced structure. Mix of light code checks and higher-weight LLM reasoning checks. Numbers are judged for internal consistency rather than exact external references.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 3.0, "rules": [{"type": "code", "name": "Presence of All 10 Drugs and Patient Assumption", "description": "Check that the document text mentions all 10 specified drugs/strengths and the 300-patient assumption.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float or (float, str)\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0, \"Unable to read document text.\"\n\n    low = text.lower()\n\n    # Define patterns for each drug/strength\n    patterns = [\n        r\"atorvastatin\\s*10\\s*mg\",\n        r\"atorvastatin\\s*20\\s*mg\",\n        r\"amlodipine\\s*5\\s*mg\",\n        r\"amlodipine\\s*10\\s*mg\",\n        r\"rosuvastatin\\s*5\\s*mg\",\n        r\"rosuvastatin\\s*10\\s*mg\",\n        r\"losartan\\s*25\\s*mg\",\n        r\"losartan\\s*50\\s*mg\",\n        r\"metformin\\s*500\\s*mg\",\n        r\"tamsulosin\\s*0\\.?4\\s*mg\",\n    ]\n\n    found = 0\n    for pat in patterns:\n        if re.search(pat, low):\n            found += 1\n\n    has_300 = bool(re.search(r\"300\\s*(patient|patients)\", low)) or \"patients: 300\" in low or \"300 patients\" in low\n\n    total_checks = len(patterns) + 1\n    score_frac = (found + (1 if has_300 else 0)) / total_checks\n\n    feedback = f\"Drugs found: {found}/10; 300-patient assumption: {'yes' if has_300 else 'no'}.\"\n    return score_frac, feedback"}, {"type": "code", "name": "Threshold and Decision References Present", "description": "Check that the document references $800,000 revenue baseline, the 2% ($16,000) threshold, and includes a recommendation/decision statement.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0, \"Unable to read document text.\"\n\n    low = text.lower()\n\n    has_800k = bool(re.search(r\"\\$?\\s*800[, ]?000\", low)) or \"800,000\" in low\n    has_threshold = bool(re.search(r\"2\\s*%|two\\s*percent|\\$?\\s*16[, ]?000\", low))\n    has_recommendation = (\"recommend\" in low) or (\"recommendation\" in low) or (\"we will\" in low and (\"switch\" in low or \"maintain\" in low))\n\n    checks = [has_800k, has_threshold, has_recommendation]\n    score_frac = sum(1 for c in checks if c) / len(checks)\n    feedback = f\"Refs \u2014 $800k:{has_800k}, threshold(2%/$16k):{has_threshold}, recommendation:{has_recommendation}.\"\n    return score_frac, feedback"}, {"type": "llm_judge", "name": "Fills/Year Logic and Methodology Correctness", "description": "Confirm the report uses correct fills-per-year logic and applies formulas correctly in narrative: 90-day=4 fills (~360 days), 100-day=3 fills (~300 days); 1 tablet daily; expense and reimbursement formulas per patient cohort are correctly described; vial/supply cost per fill included; cites the provided PDFs as sources.", "weight": 2.0, "judge_prompt": "Evaluate methodological correctness and logical consistency (not the exact numeric values):\n\nCheck for ALL of the following and score proportionally:\n- States and uses correct fills-per-year: 90-day = 4 fills (~360 days), 100-day = 3 fills (~300 days)\n- Assumes 1 tablet once daily and uses that consistently\n- Explicitly includes vial/supply cost per fill in expense calculations (not just drug cost)\n- Expresses formulas or equivalent logic:\n  \u2022 Annual Expense = (Drug Cost/Fill + Vial/Supply Cost/Fill) \u00d7 Fills/Year \u00d7 300\n  \u2022 Annual Reimbursement = Reimbursement/Fill \u00d7 Fills/Year \u00d7 300\n  \u2022 Annual Revenue = Annual Reimbursement \u2212 Annual Expense\n- References using Wholesale Price.pdf (for drug + vial costs) and Reimbursement.pdf (for reimbursement per fill) as sources (even if those files aren\u2019t attached here)\n\nScoring:\n- Full credit: All bullets clearly present and internally consistent\n- Partial: Some bullets present but missing or contradicting others\n- Zero: Major logic errors in fills/year or formulas, or omission of expense components\nReturn a score in [0, weight].", "expectation": "Methodology text correctly states fills/year logic, includes vial/supply costs, applies formulas, and cites both PDFs as the data sources."}, {"type": "llm_judge", "name": "Comparative Table Completeness and Computation Flow", "description": "Assess whether the table shows both models per drug with the necessary intermediate and annual fields, plus a totals row, enabling verification that the 3 vs 4 fills are applied to reimbursement and costs.", "weight": 2.0, "judge_prompt": "Review the comparative table. Confirm it contains, for EACH of the 10 drugs, fields allowing computation for both models (90-day and 100-day), and a Totals row:\n- Per drug includes: Patients (or clear statement that 300 is used), Fills/Year (90=4, 100=3), Drug Cost/Fill, Vial/Supply Cost/Fill, Annual Expense, Reimbursement/Fill, Annual Reimbursement, Annual Revenue for each model\n- Includes Revenue Difference (100-day minus 90-day) per drug\n- Totals row summing Annual Revenue for 90-day and 100-day and the overall Revenue Difference\n- Reimbursement/Fill for a given drug is applied consistently with Fills/Year to produce Annual Reimbursement (i.e., multiplied by 4 vs 3)\n\nScoring:\n- Full: Table clearly contains all the above and internal computation flow is coherent\n- Partial: Minor omissions (e.g., one intermediate field missing) but flow still inferable\n- Zero: Missing many required fields or table does not distinguish both models properly\nReturn a score in [0, weight].", "expectation": "A coherent table with both models per drug, intermediate fields, a totals row, and visible Revenue Difference per drug and overall."}, {"type": "llm_judge", "name": "Decision Consistency with Threshold", "description": "Check that the final recommendation matches the computed overall revenue difference compared against the $16,000 (2%) threshold and is explicitly justified.", "weight": 1.0, "judge_prompt": "Evaluate the Summary/Recommendation section for decision consistency:\n- Does it explicitly compare the overall revenue difference (100-day vs 90-day total) with $16,000 (2% of $800,000)?\n- Is the stated final decision aligned with this comparison? (Switch if < $16,000; maintain 90-day if \u2265 $16,000.)\n- Is the justification clearly tied to revenue-only criteria (acknowledging adherence but not using it as the deciding factor here)?\n\nScore guidance:\n- Full: Clear comparison to $16,000 and a consistent decision\n- Partial: Decision present but threshold comparison vague or implicit\n- Zero: Decision contradicts presented totals/threshold or missing\nReturn a score in [0, weight].", "expectation": "A clear, revenue-threshold-based recommendation consistent with the reported totals."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Presentation and Operational Quality", "description": "Holistic assessment of professional quality, clarity, and operational usefulness for inclusion in an SOP/operational manual.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professionalism and Clarity", "description": "Assess tone, organization, concision, and readability appropriate for a pharmacist-owner audience within 1\u20132 pages.", "weight": 1.0, "judge_prompt": "Assess the document\u2019s professional quality and clarity for an independent retail pharmacist audience:\n- Professional tone, clear organization, and concise delivery within 1\u20132 pages\n- Headings and typography support quick scanning; table is visually legible\n- Minimal jargon or well-explained; currency and units are unambiguous\nScore 0 to weight.", "expectation": "A clean, concise, professional report that\u2019s easy for a pharmacist-owner to read and act on."}, {"type": "llm_judge", "name": "Actionability for Policy Decision", "description": "Evaluate whether the report provides an actionable, unambiguous recommendation and notes implementation scope/next steps appropriate for an operational manual.", "weight": 1.0, "judge_prompt": "Judge actionability:\n- Clear, unambiguous recommendation tied to the revenue threshold\n- Brief note on scope (eligible patients) and operational implications (e.g., update auto-refill policy, billing cycles) without drifting into non-revenue criteria as the decisive factor\n- If recommending a switch, indicate monitoring plan or caveat (e.g., verify reimbursement schedules, periodic review)\nScore 0 to weight.", "expectation": "A specific recommendation plus brief, practical next steps suitable for SOP inclusion."}, {"type": "llm_judge", "name": "Table Readability and Labeling Quality", "description": "Check if the table uses clear labels, currency formatting, and any footnotes/sources to aid auditing.", "weight": 1.0, "judge_prompt": "Assess table presentation:\n- Columns are clearly labeled; groupings for 90-day vs 100-day are obvious\n- Currency values include symbols or are clearly denominated; units for strengths (mg) are shown\n- Footnotes or source notes cite Wholesale Price.pdf and Reimbursement.pdf where appropriate\nScore 0 to weight.", "expectation": "A well-labeled, easily auditable table with clear currency and source notes."}, {"type": "llm_judge", "name": "Self-Documentation and Auditability", "description": "Evaluate how well the report enables later verification by others (clear formulas, definitions, totals, and references).", "weight": 1.0, "judge_prompt": "Evaluate self-documentation:\n- Formulas/definitions are stated so another reviewer can reproduce totals\n- Totals and subtotals are clearly labeled and traceable back to per-drug entries\n- References to source PDFs and assumptions (300 patients, fills/year) appear where needed\nScore 0 to weight.", "expectation": "Sufficient explanations and cross-references to let a reviewer audit calculations without external clarification."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "a328feea-47db-4856-b4be-2bdc63dd88fb", "rubric": {"category_name": "Government \u2013 Administrative Services Managers \u2013 Unscheduled Absence/Lateness Phone-Call Procedure", "rationale": "This rubric enforces a self-documenting, verifiable one-page policy document. Stage 1 (LLM-only) gates on exact structure and format to make verification trivial. Stage 2 combines lightweight code checks (keyword/structure detection) with higher-weight LLM correctness checks mapped to the forum\u2019s five issues (a\u2013e). Stage 3 assesses professional quality, clarity, and government-appropriate presentation. Code rules intentionally contribute ~1/5 the points of LLM rules in Stage 2, aligning with the philosophy that LLMs handle nuanced content while code performs deterministic checks.", "max_total_score": 18.0, "stages": [{"name": "Stage 1 \u2013 Structural Format Gate (LLM only)", "description": "Gate: Enforce exact deliverable shape for verifiable document. Checks file type, 1-page professional policy layout, and required sections with a clearly numbered procedure centered on phone-call reporting.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Document Format and Required Sections Present", "description": "Verify the candidate output is a one-page policy document (DOCX or PDF) with required sections and a clearly numbered call-based procedure.", "weight": 4.0, "judge_prompt": "You are evaluating a policy/procedure deliverable for government administrative services. Review the candidate output and score ONLY on shape and structural completeness (not content quality). Requirements:\n\nFormat requirements:\n- Must be a DOCX or PDF policy document (not Excel, not plain text).\n- Target length: one page. Minor spillover to a second page is acceptable with small penalty.\n- Professional formatting with clear headers and a numbered procedure list.\n\nRequired elements (be flexible with near-synonyms):\n1) Title near top referencing the intent (e.g., \u201cReporting of Unscheduled Absence or Lateness Policy/Procedure\u201d).\n2) Section: \u201cPurpose\u201d.\n3) Section: \u201cScope\u201d.\n4) Section: \u201cDefinitions\u201d (should include at least two of the following terms: Unscheduled Absence; Lateness/Tardy; MFA (Medical or Family Assistance) case; Appropriate Person (Supervisor/Team Lead); Start Time/Shift Start).\n5) Section: \u201cProcedures\u201d with a clearly numbered list focused on PHONE CALL reporting. Inside Procedures, expect presence of at least three of these subsections/topics as subheaders or clearly labeled bullets:\n   - Notification Method: Phone Call Only (no texting/email/coworker relays; voicemail only as backup).\n   - Timing (before shift or ASAP within a defined window) to support coverage.\n   - Escalation/Two-way contact (if no answer, call back/escalate to Team Lead/Supervisor; allow questions/support).\n   - MFA/HR communication (how MFA info gets to HR).\n   - Documentation/Timekeeping (record of the call/logging).\n\nScoring (STRUCTURE ONLY):\n- 4.0: DOCX/PDF; one page or very close; clear title; Purpose, Scope, Definitions present; Procedures present AND includes at least 3 of the listed procedure subtopics with a numbered list that centers on phone calls.\n- 3.5: All format requirements, but Procedures only include 2 of the subtopics OR document slightly exceeds one page.\n- 3.0: Valid format and sections present but Procedures only include 1 subtopic OR numbered list unclear.\n- 2.0: Valid format but missing 2+ required sections OR Procedures lack a numbered call-based sequence.\n- 0.0: Not a DOCX/PDF, or clearly not a policy document structure.\n\nOnly assess presence/format/structure\u2014not correctness of policy content.", "expectation": "A one-page DOCX/PDF policy with Purpose, Scope, Definitions, and a numbered Procedures section centered on phone calls, containing multiple clearly labeled subtopics."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification: Completeness and Correctness", "description": "Now verify the policy actually resolves the forum\u2019s issues (a\u2013e) with clear, workable rules. Mix of code checks for structural signals and LLM checks for substantive correctness.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Basic Document and Header Keywords", "description": "Deterministic check for document type and presence of core section keywords in extracted text (Purpose, Scope, Definitions, Procedures).", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float in [0,1] or (float, str)\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No primary document output.\"\n\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text:\n        return 0.0, \"Unable to extract text from document.\"\n\n    t = text.lower()\n    checks = [\n        ('purpose' in t),\n        ('scope' in t),\n        ('definition' in t or 'definitions' in t),\n        ('procedure' in t or 'procedures' in t)\n    ]\n    score = sum(1 for c in checks if c) / 4.0\n    return score, f\"Header hits: {sum(1 for c in checks if c)}/4\""}, {"type": "code", "name": "Procedure Emphasizes Phone Call and Numbered Steps", "description": "Checks for a numbered steps list and explicit mention of calling/phone; partial credit if it also discourages texting/email.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text:\n        return 0.0\n\n    t = text.lower()\n    # Numbered steps\n    steps = re.findall(r\"(?m)^\\s*(?:\\d+|[a-z])\\s*[\\.|\\)]\\s+\", t)\n    num_steps = len(steps)\n    c1 = 1.0 if num_steps >= 3 else (num_steps / 3.0)\n\n    # Phone call emphasis\n    call_present = any(w in t for w in [\"call\", \"phone\", \"telephone\"])\n    c2 = 1.0 if call_present else 0.0\n\n    # Discouraging text/email (look for negation patterns)\n    negative_patterns = [\n        \"do not text\", \"no texting\", \"not via text\", \"not by text\", \"text is not accepted\",\n        \"do not email\", \"no email\", \"not via email\", \"not by email\", \"email is not accepted\",\n        \"do not send a text\", \"avoid texting\", \"avoid email\"\n    ]\n    c3 = 1.0 if any(p in t for p in negative_patterns) else 0.0\n\n    score = 0.5 * c1 + 0.3 * c2 + 0.2 * c3\n    return max(0.0, min(1.0, score))"}, {"type": "code", "name": "Coverage and HR/MFA Mentions", "description": "Deterministic check for coverage planning and HR/MFA references.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text:\n        return 0.0\n\n    t = text.lower()\n    coverage = any(w in t for w in [\"coverage\", \"cover staffing\", \"arrange coverage\", \"coverage plan\", \"ensure coverage\", \"backfill\"])\n    hr = (\" human resources\" in t) or (\" hr\" in t) or (\"hr \" in t) or (\"hr.\" in t)\n    mfa = (\"mfa\" in t) or (\"medical\" in t) or (\"family assistance\" in t)\n\n    hits = sum([1 if coverage else 0, 1 if hr else 0, 1 if mfa else 0])\n    return hits / 3.0"}, {"type": "llm_judge", "name": "Addresses Forum Issues (a\u2013e) Substantively", "description": "Check whether the Procedures directly solve the five issues raised at the forum: (a) no-notice lateness, (b) inconsistent channels, (c) late reporting causing coverage gaps, (d) lack of two-way contact, (e) MFA info not reaching HR.", "weight": 2.8, "judge_prompt": "Evaluate whether the policy\u2019s Procedures substantively address EACH of the five issues raised:\n(a) No-notice lateness: Requires prior-to-shift or ASAP call upon realizing lateness; defines expectations for late starts.\n(b) Inconsistent channels: Standardizes to a PHONE CALL to the appropriate person; disallows texting/email/coworker relay except narrowly as backup (e.g., voicemail with callback).\n(c) Coverage gaps: Requires advance notice or specific time window to enable coverage planning; explicitly references arranging coverage.\n(d) Two-way contact: Requires conversation or callback, permits questions/support, includes escalation if no answer.\n(e) MFA to HR: Provides steps to ensure HR receives information related to MFA cases while safeguarding privacy.\n\nScoring:\n- 2.8: Clearly addresses all five issues with specific, actionable steps in Procedures.\n- 2.2: Addresses 4/5 issues well.\n- 1.6: Addresses 3/5 issues or addresses more with vague language.\n- 0.8: Addresses only 1\u20132 issues.\n- 0.0: Fails to address the issues substantively.\n\nFocus on whether the steps would actually resolve the issues if followed, not just mentioning terms.", "expectation": "Procedures that concretely resolve a\u2013e with clear steps: phone-call only, time windows, escalation, coverage planning, and HR/MFA handling."}, {"type": "llm_judge", "name": "Appropriate Person and Escalation Path", "description": "Verify the document specifies who to call (Supervisor/Team Lead or equivalent) and defines a practical escalation ladder if unreachable.", "weight": 1.4, "judge_prompt": "Check whether the policy:\n- Specifies the primary recipient of the call (Supervisor and/or Team Lead for the regional branch) and provides guidance for after-hours or if roles differ by shift.\n- Defines a clear escalation path if the primary contact is unreachable (e.g., call Team Lead \u2192 Supervisor \u2192 Branch Admin/On-call desk) including expected number/spacing of attempts and voicemail callback expectations.\n\nScoring:\n- 1.4: Precise primary contact definition and a practical, step-wise escalation ladder.\n- 0.9: Primary contact defined and some escalation guidance, but incomplete or vague.\n- 0.5: Mentions who to call but lacks escalation details.\n- 0.0: Does not specify who to call or any escalation.", "expectation": "A clear call recipient and an explicit, stepwise escalation tree with voicemail/callback expectations."}, {"type": "llm_judge", "name": "Timing Window and Documentation Specifics", "description": "Assess whether the policy sets explicit notification timelines and requires documentation for timekeeping accuracy.", "weight": 1.3, "judge_prompt": "Assess the presence of:\n- Explicit notification timing (e.g., call at least X minutes before shift start or within Y minutes of realizing you\u2019ll be late/absent).\n- Requirements to document the call (call log, supervisor notes) and update the timekeeping/HR system, or a follow-up email for record after the call.\n\nScoring:\n- 1.3: Clear, concrete timing windows and explicit documentation/timekeeping steps.\n- 0.8: Timing specified but documentation steps vague or partial.\n- 0.4: Vague timing and documentation.\n- 0.0: No timing or documentation requirements.", "expectation": "Specific timing windows tied to shift start and concrete instructions for documenting and updating time records."}, {"type": "llm_judge", "name": "Privacy and MFA Handling", "description": "Evaluate whether MFA-related communications are routed to HR with privacy-conscious guidance.", "weight": 1.3, "judge_prompt": "Check whether the policy:\n- Instructs employees to use minimal necessary information when discussing health/family matters during the call.\n- Provides a mechanism for HR to be informed about MFA-related absences (e.g., supervisor notifies HR or directs employee to contact HR), without over-sharing details.\n\nScoring:\n- 1.3: Clear privacy-first guidance and an explicit pathway to HR for MFA cases.\n- 0.8: References privacy and HR but lacks one of the two elements above.\n- 0.4: Mentions MFA or HR only in passing.\n- 0.0: No privacy or MFA guidance.", "expectation": "Privacy-respecting instructions and a concrete HR notification step for MFA cases."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Quality and Professionalism", "description": "Holistic quality assessment for professional policy presentation, clarity, and usability in a government context.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Tone and One-Page Concision", "description": "Evaluate tone, concision, and whether the policy fits on a single page while remaining complete.", "weight": 2.0, "judge_prompt": "Assess whether the document:\n- Uses a professional, neutral, government-appropriate tone.\n- Fits on one page (or nearly so without sacrificing clarity) while covering the required elements.\n\nScoring:\n- 2.0: Professional tone and crisp one-page completeness.\n- 1.4: Professional tone; minor verbosity or slight spillover.\n- 0.8: Readable but noticeably verbose or underdeveloped.\n- 0.0: Unprofessional tone or substantially over/under length.", "expectation": "A concise, one-page professional policy."}, {"type": "llm_judge", "name": "Policy Formatting and Readability", "description": "Check the layout: clear headings, numbered steps, consistent style, and easy skimmability for frontline staff.", "weight": 1.5, "judge_prompt": "Evaluate:\n- Clear hierarchy of headings (Purpose, Scope, Definitions, Procedures).\n- Numbered steps for Procedures with consistent typography (numbers/indentation).\n- Skimmability: whitespace, bullets, consistent capitalization.\n\nScoring:\n- 1.5: Highly readable with consistent, skimmable formatting.\n- 1.0: Generally clear with minor inconsistencies.\n- 0.5: Readable but cluttered or inconsistent.\n- 0.0: Poorly formatted and hard to follow.", "expectation": "Well-structured headings and numbered procedures that are easy to scan."}, {"type": "llm_judge", "name": "Implementation Readiness and Usability", "description": "Assess whether a supervisor could implement this immediately (contact info placeholders, hours/exceptions, simple scripts).", "weight": 1.5, "judge_prompt": "Consider whether the policy is plug-and-play:\n- Identifies or provides placeholders for contact info (Supervisor/Team Lead/on-call) or where that list is kept.\n- Notes hours/edge cases (after-hours, emergencies, power/phone outage) at least briefly.\n- Offers practical guidance (e.g., a short call script or what minimum info the employee should provide).\n\nScoring:\n- 1.5: Ready to implement with minimal adaptation.\n- 1.0: Usable but missing one helpful element.\n- 0.5: Requires significant additions to be usable.\n- 0.0: Not practically usable without major rework.", "expectation": "Actionable guidance with contact placeholders and minimal viable scripts/edge-case notes."}, {"type": "llm_judge", "name": "Plain Language and Inclusivity", "description": "Evaluate clarity for diverse staff audiences using plain language and inclusive phrasing.", "weight": 1.0, "judge_prompt": "Rate the document\u2019s clarity and inclusivity:\n- Plain language (avoids jargon; active voice; short sentences).\n- Inclusive phrasing and respectful tone.\n\nScoring:\n- 1.0: Very clear, plain, and inclusive.\n- 0.6: Generally clear with minor jargon.\n- 0.3: Mixed clarity; several dense or complex passages.\n- 0.0: Confusing or non-inclusive tone.", "expectation": "Accessible plain-language policy suitable for a broad workforce."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "dfb4e0cd-a0b7-454e-b943-0dd586c2764c", "rubric": {"category_name": "Government | Compliance Officers \u2014 Post-Award Spending Rate Risk Analysis (2 CFR Part 200)", "rationale": "Pattern A (Analytical). This rubric enforces a self-documenting, verifiable Excel deliverable. Stage 1 uses LLM-only gating to mandate a precise worksheet/table shape. Stage 2 mixes small, robust code checks (percent math, classification logic, bounds) with higher-weight LLM verification for semantic correctness and documentation of assumptions. Stage 3 assesses professional quality, actionability for compliance review, and clarity for stakeholders.", "max_total_score": 15.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (LLM only)", "description": "Output must be an Excel workbook with the exact analysis table and columns needed to verify spending rate risk flags. This is a structural gate; no calculation checking here.", "is_required": true, "max_points": 2.0, "min_score_to_pass": 1.4, "rules": [{"type": "llm_judge", "name": "Workbook Structure and Required Table Present", "description": "Check that the candidate output is an Excel workbook containing a primary analysis sheet with the required columns and structure for the spending rate analysis.", "weight": 2.0, "judge_prompt": "You are verifying ONLY the structure/format of the submitted output (not the correctness of calculations). Inspect the candidate file.\n\nFormat Requirements:\n- File must be a valid Excel workbook (.xlsx or .xls). Not PDF/DOCX/CSV/MD.\n- Must contain a primary analysis sheet. Accept flexible sheet names such as: \"Spending Rate Analysis\", \"Spending Rate Review\", \"Risk Flags\", \"Spending Classification\", or similar.\n\nPrimary Analysis Table \u2014 Required Columns (case-insensitive; allow minor variations like punctuation, %, or abbreviations):\n1) Recipient Award Number (e.g., \"Recipient Award Number\", \"Award Number\", \"Recipient Award #\")\n2) Start Date (e.g., \"Start Date\", \"Project Start Date\")\n3) End Date (e.g., \"End Date\", \"Project End Date\")\n4) % Time Elapsed (e.g., \"% Time Elapsed\", \"Percent Time Elapsed\", \"Time Elapsed %\")\n5) Total Awarded Amt (e.g., \"Total Awarded Amt\", \"Total Award Amount\", \"Award Total\")\n6) FFR Expenditure Amt (e.g., \"FFR Expenditure Amt\", \"FFR Expenditure Amount\", \"Expenditures to Date\")\n7) % of Funds Spent (e.g., \"% of Funds Spent\", \"Percent of Funds Spent\", \"% Funds Spent\")\n8) Spending Rate Analysis (e.g., \"Spending Rate Analysis\", \"Spending Rate\", \"Spending Classification\", \"Flag\")\n\nContent Expectations for Structure Only:\n- The primary table should list awards as rows.\n- The \"Spending Rate Analysis\" column should use only the labels \"Fast Spending\" and/or \"Slow Spending\" (case-insensitive). It is acceptable if only one of those appears, or both.\n\nOptional but Encouraged (do NOT penalize if missing):\n- A sheet for parameters/notes (e.g., \"Parameters\", \"Notes\", or \"Methodology\") mentioning the as-of date 03/31/2025 and the thresholds used (>50% funds & \u226425% time; <25% funds & \u226575% time).\n- A summary sheet with counts by category.\n\nScoring (structure only):\n- 2.0: Excel workbook present with a clearly labeled primary analysis sheet containing all 8 required columns (allowing minor naming variations), and the Spending Rate Analysis column contains only the expected labels (Fast/Slow) as values.\n- 1.4: Excel workbook present and primary analysis sheet found; 1\u20132 required columns have naming/placement issues but are still identifiable OR Spending Rate Analysis column exists but includes a small number of other values (e.g., a few blanks or miscellaneous text).\n- 0.8: Excel workbook present but primary analysis sheet is unclear OR 3\u20134 required columns are missing/ambiguous.\n- 0.0: Not an Excel file OR no identifiable analysis sheet/table OR 5+ required columns missing.\n\nOnly assess presence/format structure. Do NOT verify math, logic, or content accuracy.", "expectation": "A single, well-structured primary analysis sheet in Excel with the eight required columns, containing rows of awards and a Spending Rate Analysis column using Fast/Slow labels."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness Verification (Mixed)", "description": "Now that the workbook shape is known, verify the math and logic for time elapsed, percent spent, and correct risk classification as of 03/31/2025. Include both deterministic code checks and higher-weight LLM validation.", "is_required": false, "max_points": 7.0, "min_score_to_pass": 3.5, "rules": [{"type": "code", "name": "Percent Time Elapsed Calculation Accuracy", "description": "Check that % Time Elapsed approximates (as_of - start) / (end - start), clamped to [0,1], with small tolerance.", "weight": 0.4, "code": "import pandas as pd, numpy as np, re\n\ndef evaluate(workflow, context):\n    try:\n        as_of = pd.to_datetime('2025-03-31')\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, 'No spreadsheet output.'\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        # Helper: fuzzy column find\n        syns = {\n            'start_date': ['start date','project start date','period start'],\n            'end_date': ['end date','project end date','period end'],\n            'pct_time': ['% time elapsed','percent time elapsed','time elapsed %','pct time elapsed','%time elapsed'],\n            'label': ['spending rate analysis','spending rate','spending classification','flag']\n        }\n        def find_col(cols, names):\n            lc = [str(c).strip().lower() for c in cols]\n            for name in names:\n                for i,c in enumerate(lc):\n                    if name in c:\n                        return cols[i]\n            return None\n        best_df = None\n        best_hits = -1\n        for sn in xls.sheet_names:\n            df = pd.read_excel(path, sheet_name=sn)\n            cols = list(df.columns)\n            hits = 0\n            for key in syns:\n                if find_col(cols, syns[key]) is not None:\n                    hits += 1\n            if hits > best_hits:\n                best_hits = hits\n                best_df = df\n        if best_df is None or best_hits < 3:\n            return 0.0, 'Could not locate primary analysis sheet with required columns.'\n        cols = list(best_df.columns)\n        c_start = find_col(cols, syns['start_date'])\n        c_end = find_col(cols, syns['end_date'])\n        c_pct_time = find_col(cols, syns['pct_time'])\n        if c_start is None or c_end is None or c_pct_time is None:\n            return 0.0, 'Missing start/end/% time elapsed columns.'\n        df = best_df.copy()\n        # Parse dates\n        sd = pd.to_datetime(df[c_start], errors='coerce')\n        ed = pd.to_datetime(df[c_end], errors='coerce')\n        # Coerce given % time to float in [0,1]\n        def to_ratio(x):\n            if pd.isna(x):\n                return np.nan\n            if isinstance(x, str):\n                xs = x.strip().replace('%','')\n                try:\n                    v = float(xs)\n                    return v/100.0\n                except:\n                    try:\n                        return float(x)\n                    except:\n                        return np.nan\n            try:\n                xv = float(x)\n                # If likely percent (e.g., >1 up to 100+), convert\n                if xv > 1.0 and xv <= 200.0:\n                    return xv/100.0\n                return xv\n            except:\n                return np.nan\n        given = df[c_pct_time].apply(to_ratio)\n        # Compute expected\n        dur = (ed - sd).dt.total_seconds()\n        num = (as_of - sd).dt.total_seconds()\n        with np.errstate(invalid='ignore', divide='ignore'):\n            exp = num / dur\n        exp = exp.clip(lower=0, upper=1)\n        mask = (~given.isna()) & (~exp.isna()) & (dur>0)\n        if mask.sum() == 0:\n            return 0.0, 'No comparable rows for % time elapsed.'\n        mad = (given[mask] - exp[mask]).abs().median()\n        # Score: full if median abs diff <= 0.005 (0.5pp), linear down to 0 at 0.05 (5pp)\n        tol_full = 0.005\n        tol_zero = 0.05\n        if mad <= tol_full:\n            score = 0.4\n        elif mad >= tol_zero:\n            score = 0.0\n        else:\n            score = 0.4 * (1 - (mad - tol_full)/(tol_zero - tol_full))\n        return float(score), f\"Median abs diff: {mad:.4f}\"\n    except Exception as e:\n        return 0.0, f'Error: {e}'"}, {"type": "code", "name": "Percent of Funds Spent Accuracy", "description": "Check that % of Funds Spent \u2248 FFR Expenditure Amt / Total Awarded Amt within small tolerance.", "weight": 0.4, "code": "import pandas as pd, numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, 'No spreadsheet output.'\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        syns = {\n            'total': ['total awarded amt','total award amount','total awarded amount','award total','total award amt'],\n            'ffr': ['ffr expenditure amt','ffr expenditure amount','amount expended','expenditures to date','ffr expended'],\n            'pct_spent': ['% of funds spent','percent of funds spent','% funds spent','pct funds spent']\n        }\n        def find_col(cols, names):\n            lc = [str(c).strip().lower() for c in cols]\n            for name in names:\n                for i,c in enumerate(lc):\n                    if name in c:\n                        return cols[i]\n            return None\n        best_df, best_hits = None, -1\n        for sn in xls.sheet_names:\n            df = pd.read_excel(path, sheet_name=sn)\n            cols = list(df.columns)\n            hits = sum(find_col(cols, syns[k]) is not None for k in syns)\n            if hits > best_hits:\n                best_hits = hits\n                best_df = df\n        if best_df is None or best_hits < 3:\n            return 0.0, 'Could not find all required columns.'\n        cols = list(best_df.columns)\n        c_total = find_col(cols, syns['total'])\n        c_ffr = find_col(cols, syns['ffr'])\n        c_pct = find_col(cols, syns['pct_spent'])\n        if any(c is None for c in [c_total, c_ffr, c_pct]):\n            return 0.0, 'Missing columns for funds spent check.'\n        df = best_df.copy()\n        def to_num(x):\n            if pd.isna(x): return np.nan\n            if isinstance(x,str):\n                xs = x.replace(',','').replace('$','').strip()\n                try: return float(xs)\n                except: return np.nan\n            try: return float(x)\n            except: return np.nan\n        def to_ratio(x):\n            if pd.isna(x): return np.nan\n            if isinstance(x,str):\n                xs = x.strip().replace('%','')\n                try:\n                    v=float(xs); return v/100.0\n                except:\n                    try: return float(x)\n                    except: return np.nan\n            try:\n                v=float(x)\n                return v/100.0 if v>1 and v<=200 else v\n            except:\n                return np.nan\n        total = df[c_total].apply(to_num)\n        ffr = df[c_ffr].apply(to_num)\n        given = df[c_pct].apply(to_ratio)\n        with np.errstate(invalid='ignore', divide='ignore'):\n            exp = (ffr/total)\n        mask = (~given.isna()) & (~exp.isna()) & (total>0)\n        if mask.sum()==0:\n            return 0.0, 'No comparable rows for % funds spent.'\n        mad = (given[mask]-exp[mask]).abs().median()\n        tol_full=0.005; tol_zero=0.05\n        if mad<=tol_full:\n            score=0.4\n        elif mad>=tol_zero:\n            score=0.0\n        else:\n            score=0.4*(1-(mad-tol_full)/(tol_zero-tol_full))\n        return float(score), f\"Median abs diff: {mad:.4f}\"\n    except Exception as e:\n        return 0.0, f'Error: {e}'"}, {"type": "code", "name": "Classification Logic Correctness (Fast/Slow)", "description": "Verify that the Spending Rate Analysis label matches the computed conditions using as-of 03/31/2025.", "weight": 0.4, "code": "import pandas as pd, numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        as_of = pd.to_datetime('2025-03-31')\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, 'No spreadsheet output.'\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        syns = {\n            'start': ['start date','project start date','period start'],\n            'end': ['end date','project end date','period end'],\n            'total': ['total awarded amt','total award amount','total awarded amount','award total','total award amt'],\n            'ffr': ['ffr expenditure amt','ffr expenditure amount','amount expended','expenditures to date','ffr expended'],\n            'label': ['spending rate analysis','spending rate','spending classification','flag'],\n            'pct_time': ['% time elapsed','percent time elapsed','time elapsed %','pct time elapsed'],\n            'pct_spent': ['% of funds spent','percent of funds spent','% funds spent','pct funds spent']\n        }\n        def find_col(cols, names):\n            lc=[str(c).strip().lower() for c in cols]\n            for name in names:\n                for i,c in enumerate(lc):\n                    if name in c:\n                        return cols[i]\n            return None\n        best_df, best_hits = None, -1\n        for sn in xls.sheet_names:\n            df = pd.read_excel(path, sheet_name=sn)\n            cols = list(df.columns)\n            hits = sum(find_col(cols, syns[k]) is not None for k in syns)\n            if hits>best_hits:\n                best_hits=hits; best_df=df\n        if best_df is None:\n            return 0.0, 'No suitable sheet found.'\n        cols = list(best_df.columns)\n        c_start=find_col(cols,syns['start']); c_end=find_col(cols,syns['end'])\n        c_total=find_col(cols,syns['total']); c_ffr=find_col(cols,syns['ffr'])\n        c_label=find_col(cols,syns['label'])\n        if any(c is None for c in [c_start,c_end,c_total,c_ffr,c_label]):\n            return 0.0, 'Missing required columns for classification check.'\n        df = best_df.copy()\n        sd=pd.to_datetime(df[c_start], errors='coerce')\n        ed=pd.to_datetime(df[c_end], errors='coerce')\n        def to_num(x):\n            import math\n            if pd.isna(x): return np.nan\n            if isinstance(x,str):\n                xs=x.replace(',','').replace('$','').strip()\n                try: return float(xs)\n                except: return np.nan\n            try: return float(x)\n            except: return np.nan\n        total=df[c_total].apply(to_num)\n        ffr=df[c_ffr].apply(to_num)\n        dur=(ed-sd).dt.total_seconds()\n        num=(as_of-sd).dt.total_seconds()\n        with np.errstate(invalid='ignore', divide='ignore'):\n            pct_time=(num/dur).clip(lower=0, upper=1)\n            pct_spent=(ffr/total)\n        labels = df[c_label].astype(str).str.strip().str.lower()\n        is_fast = (pct_spent>0.5) & (pct_time<=0.25)\n        is_slow = (pct_spent<0.25) & (pct_time>=0.75)\n        rec_fast = labels.str.contains('fast')\n        rec_slow = labels.str.contains('slow')\n        mask = (~pct_time.isna()) & (~pct_spent.isna()) & (~labels.isna()) & (total>0) & (dur>0)\n        if mask.sum()==0:\n            return 0.0, 'No comparable rows for classification.'\n        correct = ((is_fast & rec_fast) | (is_slow & rec_slow)) & mask\n        # Penalize rows labeled fast/slow that do not meet either condition\n        labeled = (rec_fast | rec_slow) & mask\n        if labeled.sum()==0:\n            return 0.0, 'No rows labeled Fast/Slow after filtering.'\n        acc = correct.sum()/labeled.sum()\n        return float(0.4*acc), f'Classification accuracy: {acc:.2%} over {int(labeled.sum())} rows.'\n    except Exception as e:\n        return 0.0, f'Error: {e}'"}, {"type": "code", "name": "Data Validity and Bounds Checks", "description": "Check basic validity: dates ordered, non-negative amounts, percents in [0,1] after normalization, and FFR not exceeding Total by large margin.", "weight": 0.4, "code": "import pandas as pd, numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, 'No spreadsheet output.'\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        syns = {\n            'start': ['start date','project start date','period start'],\n            'end': ['end date','project end date','period end'],\n            'total': ['total awarded amt','total award amount','total awarded amount','award total','total award amt'],\n            'ffr': ['ffr expenditure amt','ffr expenditure amount','amount expended','expenditures to date','ffr expended'],\n            'pct_time': ['% time elapsed','percent time elapsed','time elapsed %','pct time elapsed'],\n            'pct_spent': ['% of funds spent','percent of funds spent','% funds spent','pct funds spent']\n        }\n        def find_col(cols, names):\n            lc=[str(c).strip().lower() for c in cols]\n            for name in names:\n                for i,c in enumerate(lc):\n                    if name in c:\n                        return cols[i]\n            return None\n        best_df, best_hits = None, -1\n        for sn in xls.sheet_names:\n            df = pd.read_excel(path, sheet_name=sn)\n            cols = list(df.columns)\n            hits = sum(find_col(cols, syns[k]) is not None for k in syns)\n            if hits>best_hits:\n                best_hits=hits; best_df=df\n        if best_df is None:\n            return 0.0, 'No suitable sheet found.'\n        cols=list(best_df.columns)\n        c_start=find_col(cols,syns['start']); c_end=find_col(cols,syns['end'])\n        c_total=find_col(cols,syns['total']); c_ffr=find_col(cols,syns['ffr'])\n        c_pt=find_col(cols,syns['pct_time']); c_ps=find_col(cols,syns['pct_spent'])\n        if any(c is None for c in [c_start,c_end,c_total,c_ffr,c_pt,c_ps]):\n            return 0.0, 'Missing required columns for validity checks.'\n        df=best_df.copy()\n        sd=pd.to_datetime(df[c_start], errors='coerce')\n        ed=pd.to_datetime(df[c_end], errors='coerce')\n        def to_num(x):\n            if pd.isna(x): return np.nan\n            if isinstance(x,str):\n                xs=x.replace(',','').replace('$','').strip()\n                try: return float(xs)\n                except: return np.nan\n            try: return float(x)\n            except: return np.nan\n        def to_ratio(x):\n            if pd.isna(x): return np.nan\n            if isinstance(x,str):\n                xs=x.strip().replace('%','')\n                try:\n                    v=float(xs); return v/100.0\n                except:\n                    try: return float(x)\n                    except: return np.nan\n            try:\n                v=float(x)\n                return v/100.0 if v>1 and v<=200 else v\n            except:\n                return np.nan\n        total=df[c_total].apply(to_num)\n        ffr=df[c_ffr].apply(to_num)\n        pt=df[c_pt].apply(to_ratio)\n        ps=df[c_ps].apply(to_ratio)\n        # Checks\n        ok_dates = (sd.notna() & ed.notna() & (sd<=ed))\n        ok_total = (total.notna() & (total>0))\n        ok_ffr = (ffr.notna() & (ffr>=0))\n        ok_ffr_le_total = (ffr <= total*1.001) | (~ok_total) | (~ok_ffr)\n        ok_pt = pt.notna() & (pt>=0) & (pt<=1)\n        ok_ps = ps.notna() & (ps>=0) & (ps<=1.1)  # slight tolerance\n        checks = pd.DataFrame({\n            'dates': ok_dates,\n            'total': ok_total,\n            'ffr': ok_ffr,\n            'ffr_le_total': ok_ffr_le_total,\n            'pt': ok_pt,\n            'ps': ok_ps\n        })\n        valid_rows = checks.all(axis=1).sum()\n        total_rows = len(checks)\n        if total_rows==0:\n            return 0.0, 'No rows to validate.'\n        frac = valid_rows/total_rows\n        return float(0.4*frac), f'Valid rows: {valid_rows}/{total_rows} ({frac:.1%})'\n    except Exception as e:\n        return 0.0, f'Error: {e}'"}, {"type": "llm_judge", "name": "Logical Consistency of Fast/Slow Flags", "description": "Visually verify that rows flagged as Fast or Slow align with their displayed percent values relative to the stated criteria.", "weight": 2.2, "judge_prompt": "Evaluate the main analysis sheet in the Excel workbook.\n\nTask: Verify that the Spending Rate Analysis labels logically match the displayed values for % Time Elapsed and % of Funds Spent, using the criteria as of 03/31/2025:\n- Fast Spending: Over 50% funds spent AND 25% or less of project time elapsed\n- Slow Spending: Under 25% funds spent AND 75% or more of project time elapsed\n\nWhat to check (sample several rows of each label if present):\n- Rows labeled \"Fast Spending\" show % of Funds Spent > 50% and % Time Elapsed \u2264 25%.\n- Rows labeled \"Slow Spending\" show % of Funds Spent < 25% and % Time Elapsed \u2265 75%.\n- If only one label appears in the sheet, evaluate that label only.\n\nScoring:\n- 2.2: All inspected labeled rows accurately reflect the criteria; no contradictory examples.\n- 1.5: Minor issues (1\u20132 rows borderline or slightly inconsistent) but overall logic holds.\n- 0.8: Mixed correctness; several labeled rows contradict the criteria.\n- 0.0: Labels frequently contradict the criteria or cannot be assessed from the visible values.\n\nFocus on whether the label logically matches the displayed percentages. Do not re-compute from dates in this LLM rule.", "expectation": "Labels are consistent with the displayed percent values for time and funds spent."}, {"type": "llm_judge", "name": "Documentation of As-Of Date and Thresholds", "description": "Check that the workbook documents the as-of date (03/31/2025) and the thresholds/criteria used for Fast/Slow determinations.", "weight": 2.0, "judge_prompt": "Check the workbook (all visible sheets) for documentation that clearly states:\n- The as-of date: 03/31/2025.\n- The exact thresholds used for classification: Fast Spending (>50% funds AND \u226425% time), Slow Spending (<25% funds AND \u226575% time).\nThis can appear in a header row, a notes/parameters sheet, or visible annotations.\n\nScoring:\n- 2.0: Both as-of date and thresholds are clearly documented and unambiguous.\n- 1.0: Only one of the two (as-of date or thresholds) is clearly documented; the other is implied but not explicit.\n- 0.0: Neither is documented.\n\nDo not assess presentation quality here; only the presence/clarity of these items.", "expectation": "Clear, explicit documentation of the as-of date and classification thresholds."}, {"type": "llm_judge", "name": "Methodology Transparency for Reproducibility", "description": "Check for a brief explanation of how time elapsed and percent spent were computed, including handling of edge cases (e.g., zero-duration periods).", "weight": 1.4, "judge_prompt": "Scan the workbook for a short methodology note (may be in a Notes/Parameters/Methodology sheet or in annotated cells) that explains:\n- How % Time Elapsed is calculated from Start/End dates as of 03/31/2025, including clamping to 0\u2013100% if applicable.\n- How % of Funds Spent is calculated (FFR Expenditure Amt / Total Awarded Amt), including handling of zero/blank totals.\n- Any edge cases handled (e.g., Start Date equals End Date, missing dates/amounts) and any rounding approach.\n\nScoring:\n- 1.4: Clear methodology covering both calculations and at least one edge case.\n- 0.7: Partial methodology (mentions only one calculation or no edge cases).\n- 0.0: No visible methodology explanation.", "expectation": "A concise, visible methodology note sufficient for another analyst to reproduce the calculations."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism", "description": "Holistic quality assessment of presentation, clarity, and actionability for compliance review.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity and Professional Presentation", "description": "Formatting, readability, and professional polish appropriate for a compliance audience.", "weight": 1.5, "judge_prompt": "Assess the workbook\u2019s presentation quality:\n- Clear sheet/tab names and a prominent title for the primary analysis.\n- Column headers are readable, descriptive, and consistently formatted.\n- Proper data types/formatting (dates as dates, percents as %, currency with separators).\n- Usability features (frozen header row, filters, consistent alignment) if visible.\n\nScoring:\n- 1.5: Highly professional formatting; easy to read and use.\n- 1.0: Generally clear with small formatting issues.\n- 0.5: Several readability issues but still usable.\n- 0.0: Poorly formatted and hard to read.", "expectation": "A professional, readable analysis sheet with appropriate formatting and labels."}, {"type": "llm_judge", "name": "Actionability for Compliance Outreach", "description": "Utility of the output for prioritizing and contacting recipients regarding spending patterns under 2 CFR Part 200.", "weight": 1.5, "judge_prompt": "Evaluate how actionable the workbook is for compliance follow-up:\n- Are Fast/Slow categories obvious and easy to filter or sort?\n- Are key identifiers (e.g., Recipient Award Number) prominent to facilitate outreach?\n- Is there any summary (counts by category or simple totals) to help triage?\n\nScoring:\n- 1.5: Highly actionable; categorization and identifiers make outreach straightforward; useful summary present.\n- 1.0: Actionable with minor gaps; summary optional.\n- 0.5: Limited actionability; requires extra work to use.\n- 0.0: Not actionable for compliance purposes.", "expectation": "Clear categorization and identifiers that enable immediate outreach prioritization."}, {"type": "llm_judge", "name": "Documentation and Data Dictionary", "description": "Presence of brief notes defining columns, assumptions, and any caveats, aiding transparency and onboarding of reviewers.", "weight": 1.0, "judge_prompt": "Check for a simple data dictionary or notes that define each column, identify data sources (e.g., award report), and note any caveats or limitations.\n\nScoring:\n- 1.0: Clear, concise data dictionary/notes present.\n- 0.5: Partial notes with gaps.\n- 0.0: No documentation aiding interpretation.", "expectation": "A brief data dictionary or notes sheet that defines columns and caveats."}, {"type": "llm_judge", "name": "Context and Compliance Relevance (2 CFR Part 200)", "description": "Whether the deliverable frames the analysis in the context of 2 CFR Part 200 and provides brief next-step guidance.", "weight": 2.0, "judge_prompt": "Assess whether the workbook includes compliance context and guidance:\n- Mentions that this is a proactive review to support 2 CFR Part 200 post-award compliance.\n- Briefly indicates suggested next steps (e.g., contact recipients with unusual spending rates for clarification).\n\nScoring:\n- 2.0: Clear compliance framing and practical next-step guidance.\n- 1.0: Mentions compliance context or next steps, but not both.\n- 0.0: No compliance context or actionable guidance present.", "expectation": "A concise note connecting the analysis to 2 CFR Part 200 with suggested follow-up actions."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "0419f1c3-d669-45d0-81cd-f4d5923b06a5", "rubric": {"category_name": "Performance Improvement Plan (PIP) \u2014 Property, Real Estate, and Community Association Managers", "rationale": "This rubric enforces a self-documenting, verifiable PIP deliverable tailored to NYS multi-family property management. Stage 1 is an LLM-only gate that mandates a DOCX/PDF with exact sections and artifacts needed for verification. Stage 2 mixes lightweight code checks (procedural compliance, objective count, training selection) with higher-weight LLM judges (data grounding, KPI definitions, alignment of objectives and training). Stage 3 provides holistic quality assessment for tone, clarity, organization, and HR/Legal readiness. Code rules use the provided file-access API and are robust to minor variations. LLM rules check nuanced alignment and professional standards.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Structure and Format Gate", "description": "LLM-only gate to ensure the output is a properly structured PIP document enabling verification.", "is_required": true, "max_points": 2.0, "min_score_to_pass": 1.4, "rules": [{"type": "llm_judge", "name": "PIP Document Structure Gate", "description": "Verify the candidate produced a DOCX or PDF of ~2\u20133 pages with all required PIP sections and structural elements.", "weight": 2.0, "judge_prompt": "You are evaluating whether the submitted file is a properly structured Performance Improvement Plan (PIP) document. Only check structure and presence of required elements, not content quality or correctness.\n\nFormat requirements:\n- Must be a PDF or DOCX (not Excel, not plain text).\n- Approximately 2\u20133 pages in length (be flexible; accept 2\u20134 pages if content is complete).\n- Professional formatting with clear section headers.\n\nRequired sections and structural elements (flexible on exact header names):\n1) Factual Summary of Performance Gaps (must reference both the \u201cWork Order Log\u201d and the \u201cResident Complaint Log\u201d). This section must explicitly define metrics related to:\n   - Timeliness (e.g., acknowledgement time vs. 4 business hours, completion time vs. 72 hours)\n   - Work Quality (e.g., redo rate vs. 5% target)\n   - Task Volume (e.g., number of work orders completed/received)\n   Include a brief explanation of how the metrics show underperformance.\n2) Objectives: 3\u20135 specific, measurable objectives that address the above gaps (accept synonyms like Goals/Targets/SMART Objectives).\n3) Support and Resources: Describe what management will provide, including assigned training modules from the approved list. The list to match against: Advanced Plumbing Diagnostics (Online Module); HVAC Fundamentals (Online Module); NFPA 70E Electrical Safety (Online Module); Customer Service & Professionalism (Video Library). The section should outline how support will be provided.\n4) Consequences: A clear statement of consequences for not meeting objectives within the PIP period.\n5) Signature Section: Signature lines/placeholders for Manager, Employee (John Miller), and Witness, each with printed name and date fields.\n\nPIP procedural elements (must be present somewhere in the document; flexible location):\n- PIP duration of 90 days.\n- Assigned training modules to be completed within the first 30 days.\n- Weekly 30-minute check-in meetings.\n\nScoring:\n- 2.0: Correct file type and length; all five sections present with clear headers; includes references to both logs; includes all three PIP procedural elements; signature lines for Manager, Employee (John Miller), and Witness with date.\n- 1.5: Correct file type and length; all five sections present; references to both logs; missing 1 procedural element OR missing one signature detail (e.g., dates).\n- 1.0: Correct file type; at least 4 of 5 sections; at least one log referenced; at least one procedural element present.\n- 0.5: Correct file type but only 2\u20133 sections present OR missing both logs and most procedural elements.\n- 0.0: Wrong format (not PDF/DOCX) or fewer than 2 pages or missing most required sections.\n\nOnly evaluate structure/presence, not whether the analysis or numbers are correct.", "expectation": "A 2\u20133 page PDF/DOCX PIP with the five required sections, explicit KPI definitions for timeliness/quality/volume, the three procedural elements, referenced logs, and signature lines for Manager, John Miller, and Witness."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification of Compliance and Alignment", "description": "Mixed verification of procedural compliance, metric definitions, and alignment between gaps, objectives, and training.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Procedural Compliance Mentions (90/30/weekly)", "description": "Checks for mentions of PIP duration (90 days), training completion within 30 days, and weekly 30-minute check-ins.", "weight": 0.35, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float or (float, str)\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n\n    # Read text from DOCX/PDF/markdown\n    text = \"\"\n    try:\n        if output.ext.lower().endswith('.docx'):\n            text = context.files.read_docx_text(output.id)\n        elif output.ext.lower().endswith('.pdf'):\n            text = context.files.read_pdf_text(output.id)\n        else:\n            text = context.files.read_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            return 0.0, \"Unable to read text from document\"\n\n    t = text.lower()\n\n    # 90-day PIP\n    has_90 = bool(re.search(r\"(90\\s*-?\\s*day\\b|ninety\\s*day|\\b90\\b\\s*days)\\s*(pip|review|period)?\", t))\n\n    # 30 days for training within first 30 days\n    has_30_training = bool(re.search(r\"(first\\s*30\\s*days|within\\s*30\\s*days).{0,40}(training|module|course)\", t))\n\n    # Weekly 30-minute check-ins\n    weekly = bool(re.search(r\"weekly\", t))\n    has_30min = bool(re.search(r\"30\\s*-?\\s*minute|half\\s*hour\", t))\n    has_checkin = bool(re.search(r\"check\\s*-?\\s*in|checkin|meeting\", t))\n    has_weekly_checkins = weekly and has_30min and has_checkin\n\n    score = 0.0\n    if has_90:\n        score += 0.12\n    if has_30_training:\n        score += 0.12\n    if has_weekly_checkins:\n        score += 0.11\n\n    return min(score, 0.35)\n"}, {"type": "code", "name": "Objectives Count (3\u20135 SMART items)", "description": "Estimates number of distinct objectives listed in the Objectives section and rewards 3\u20135.", "weight": 0.35, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    # Read text\n    text = \"\"\n    try:\n        if output.ext.lower().endswith('.docx'):\n            text = context.files.read_docx_text(output.id)\n        elif output.ext.lower().endswith('.pdf'):\n            text = context.files.read_pdf_text(output.id)\n        else:\n            text = context.files.read_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            return 0.0\n\n    t = text\n    low = t.lower()\n\n    # Try to isolate the Objectives section\n    start_match = re.search(r\"\\b(objectives?|goals?|smart\\s*objectives?)\\b\", low)\n    if start_match:\n        start = start_match.start()\n        # Find likely next section header\n        next_match = re.search(r\"\\n\\s*(support|resources|manager support|consequences|signature|signatures)\\b\", low[start:])\n        segment = t[start: start + next_match.start()] if next_match else t[start: start + 4000]\n    else:\n        segment = t\n\n    # Count list items in the segment\n    lines = [ln.strip() for ln in segment.splitlines()]\n    bullet_like = [ln for ln in lines if re.match(r\"^(\\d+\\s*[\\).\\-]|[-\u2013\u2022]\\s+)\", ln) and len(ln.split())>3]\n\n    # Fallback: count sentences that start with action verbs\n    action_sentences = re.findall(r\"(?m)^(increase|reduce|improve|achieve|complete|acknowledge|respond|maintain)\\b.*\", segment, flags=re.IGNORECASE)\n\n    est_count = max(len(bullet_like), len(action_sentences))\n\n    # Scoring: reward 3\u20135 items best\n    if est_count >= 3 and est_count <= 5:\n        return 0.35\n    elif est_count == 2 or est_count == 6:\n        return 0.2\n    elif est_count == 1 or est_count >= 7:\n        return 0.1\n    else:\n        return 0.0\n"}, {"type": "code", "name": "Approved Training Modules Referenced with Justification", "description": "Checks for mention of at least one approved training module and evidence of justification/linkage to gaps.", "weight": 0.3, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    try:\n        if output.ext.lower().endswith('.docx'):\n            text = context.files.read_docx_text(output.id)\n        elif output.ext.lower().endswith('.pdf'):\n            text = context.files.read_pdf_text(output.id)\n        else:\n            text = context.files.read_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            return 0.0\n\n    low = text.lower()\n\n    modules = [\n        \"advanced plumbing diagnostics\",\n        \"hvac fundamentals\",\n        \"nfpa 70e electrical safety\",\n        \"customer service & professionalism\",\n        \"customer service and professionalism\"\n    ]\n\n    module_hits = sum(1 for m in modules if m in low)\n\n    # Look for justification language near training mentions\n    justification = bool(re.search(r\"(addresses|to address|to improve|mitigate|reduce|improve|close\\s+gap|remediate|aligns\\s+with).{0,80}(issue|gap|complaint|timeliness|quality|redo|acknowledg|completion|hvac|plumb|electri)\", low))\n\n    if module_hits == 0:\n        return 0.0\n    if module_hits >= 1 and not justification:\n        return 0.18\n    # Has at least one module and justification/linkage\n    return 0.3\n"}, {"type": "llm_judge", "name": "KPI Definitions and Baselines Validity", "description": "Checks that timeliness, quality, and volume KPIs are clearly defined with baselines and compared to company standards.", "weight": 1.5, "judge_prompt": "Evaluate whether the PIP document clearly defines and uses KPIs for:\n- Timeliness (acknowledgement time vs. 4 business hours; completion time vs. 72 hours)\n- Work quality (redo rate vs. 5% target)\n- Task volume (e.g., work orders received/completed)\n\nLook for:\n- Clear metric definitions (what is measured and how)\n- Baseline values from the recent quarter and comparison against standards\n- Brief, factual explanation of how metrics demonstrate underperformance\n\nScoring:\n- 1.5: All three KPI domains defined with baselines and explicit comparison to standards; underperformance explained.\n- 1.0: Two domains fully defined with baselines and comparisons; third is partial.\n- 0.5: Metrics mentioned but unclear definitions or no baselines/standards.\n- 0.0: No meaningful KPI definitions.\n", "expectation": "Explicit KPI definitions for acknowledgement time, completion time, redo rate, and task volume, with recent-quarter baselines and comparison to standards."}, {"type": "llm_judge", "name": "Complaint Themes Synthesis and Linkage", "description": "Assesses whether Resident Complaint Log themes are synthesized and linked to the KPI shortfalls.", "weight": 1.0, "judge_prompt": "Review the section that references the Resident Complaint Log. Does the document synthesize complaint themes (e.g., slow response, repeated fixes, communication issues) and link them to the KPI gaps (timeliness, quality, volume)?\n\nScoring:\n- 1.0: Clear thematic synthesis of complaints tied to specific KPI metrics and trends.\n- 0.6: Themes identified but linkage to KPIs is weak or implicit.\n- 0.3: Mentions complaints but no synthesis.\n- 0.0: No meaningful use of complaint information.\n", "expectation": "At least two complaint themes clearly connected to the KPI shortfalls."}, {"type": "llm_judge", "name": "Objectives Alignment and SMARTness", "description": "Evaluates whether objectives directly address identified gaps and are SMART.", "weight": 1.5, "judge_prompt": "Evaluate the Objectives section. Are there 3\u20135 objectives that directly address the identified performance gaps and are SMART (Specific, Measurable, Achievable, Relevant, Time-bound)?\n\nScoring:\n- 1.5: 3\u20135 objectives that each map to a gap and include measurable targets and timeframes.\n- 1.0: 3\u20135 objectives; most are measurable/time-bound and aligned; minor gaps.\n- 0.6: Objectives present but weak measurability or unclear linkage to gaps.\n- 0.0: Objectives missing or not aligned.\n", "expectation": "3\u20135 SMART objectives tied to timeliness, quality, and volume improvements with explicit targets and dates."}, {"type": "llm_judge", "name": "Training Alignment and Implementation Plan", "description": "Checks that selected training maps to gaps and includes a 30-day completion plan and application to work.", "weight": 1.0, "judge_prompt": "Assess whether the assigned training modules are chosen from the approved list and are justified as remedies for the observed gaps (e.g., technical training for redo rate; customer service for complaints). Verify a plan exists to complete training within 30 days and to apply learnings during the 90-day PIP with checkpoints.\n\nScoring:\n- 1.0: Training modules from the list, strong justification aligned to gaps, and a clear completion/application plan within 30 days and integrated into the 90-day period.\n- 0.6: Modules from the list with partial justification and a basic timing plan.\n- 0.3: Mentions training but unclear alignment or timing.\n- 0.0: No appropriate training or plan.\n", "expectation": "At least one approved module selected with clear rationale and a 30-day completion and application plan."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Holistic Quality Assessment", "description": "Professionalism, clarity, organization, and HR/Legal readiness of the PIP document.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Tone and Fairness", "description": "Evaluates whether the PIP maintains a respectful, objective, and fair tone, avoiding blame while being clear about expectations.", "weight": 0.5, "judge_prompt": "Assess tone and fairness. Is the document professional, respectful, and objective? Does it avoid accusatory language while clearly setting expectations and accountability?\n\nScoring:\n- 0.5: Consistently professional and fair; objective language throughout.\n- 0.3: Generally professional with minor tone issues.\n- 0.1: Noticeable lapses in tone or fairness.\n- 0.0: Unprofessional, biased, or hostile tone.\n", "expectation": "Neutral, factual tone with clear expectations and accountability."}, {"type": "llm_judge", "name": "Clarity and Actionability", "description": "Checks whether a manager and employee could realistically execute the plan as written.", "weight": 0.5, "judge_prompt": "Is the PIP clear and actionable? Consider whether timelines, owners, and success criteria are understandable so that both manager and employee could follow the plan.\n\nScoring:\n- 0.5: Crystal-clear instructions, timelines, owners, and criteria.\n- 0.3: Mostly clear but some ambiguity remains.\n- 0.1: Hard to follow or missing key details.\n- 0.0: Not actionable.\n", "expectation": "Concrete steps, dates/timeframes, and measurable success criteria."}, {"type": "llm_judge", "name": "Organization and Formatting", "description": "Assesses document structure, headings, readability, and correct use of sections and lists.", "weight": 0.5, "judge_prompt": "Evaluate organization and formatting: clear headings, logical flow of sections, readable lists/tables, and consistent styles. Minor variations in wording are acceptable.\n\nScoring:\n- 0.5: Well-structured with excellent readability.\n- 0.3: Adequate structure with minor issues.\n- 0.1: Disorganized or hard to read.\n- 0.0: Poorly organized or missing basic formatting.\n", "expectation": "Clear hierarchy, consistent formatting, readable lists/tables."}, {"type": "llm_judge", "name": "HR/Legal Readiness and Documentation Sufficiency", "description": "Assesses whether the PIP would be defensible and useful for HR documentation if performance does not improve.", "weight": 0.5, "judge_prompt": "Would this PIP help HR make an informed decision if performance does not improve? Consider specificity of expectations, consequences, meeting cadence, and documentation sufficiency.\n\nScoring:\n- 0.5: Strong, defensible documentation ready for HR use.\n- 0.3: Generally sufficient, minor gaps.\n- 0.1: Weak documentation, significant gaps.\n- 0.0: Not suitable for HR purposes.\n", "expectation": "Specific expectations, consequences, meeting cadence, and sufficient documentation to support HR decisions."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
