{"task_id": "788d2bc6-82df-4dc7-8467-a0f31405dc14", "rubric": {"category_name": "Sales Deck: Amazon + TikTok Full-Stack Growth Partner (Wholesale Trade > Sales Managers)", "rationale": "This rubric enforces a self-documenting, verifiable slide-deck structure first, then verifies correctness of content against the mandated shape, and finally evaluates professional quality. Stage 1 uses only LLM judges to confirm the PDF/DOCX deck has the exact structural elements (slide count, required sections, slide-level components) needed for later verification. Stage 2 mixes lightweight code checks (format, keyword coverage, bullets/summary heuristics) with LLM checks for coverage, balance, visuals, and claims. Stage 3 uses LLM-only holistic quality criteria for narrative, polish, and executive suitability.", "max_total_score": 30.0, "stages": [{"name": "Stage 1 \u2013 Shape Enforcement Gate (LLM ONLY)", "description": "Mandatory structural validation of the deliverable. Output must be a professional PDF/DOCX slide deck, ~15\u201318 slides, with required sections and per-slide elements. No content quality judgments here\u2014only structure and presence.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 5.0, "rules": [{"type": "llm_judge", "name": "Format + Slide Count Check", "description": "Verify the output is a PDF or DOCX presentation deck with approximately 15\u201318 slides/pages and professional slide formatting (clearly delineated slides).", "weight": 3.0, "judge_prompt": "You are evaluating a deliverable for STRUCTURE ONLY. Do not judge content quality. Review the candidate output and check:\n\n1) File format: It must be a PDF or DOCX (not PPTX, not plain text, not Excel).\n2) Slide count: Approximately 15\u201318 slides/pages. Be flexible: acceptable range is 14\u201320 if the structure and intent are clear.\n3) Slide formatting: Content appears as slides (titles, slide bodies), not as a long-form report. Slides look distinct and consistently formatted.\n\nScoring (0\u20133):\n- 3.0: PDF/DOCX, clearly a slide deck, 14\u201320 slides.\n- 2.0: PDF/DOCX and a slide deck, but slide count is slightly out of range (e.g., 12\u201313 or 21\u201322) OR minor formatting inconsistencies.\n- 1.0: PDF/DOCX but appears more like a long-form report than a deck, or slide count far outside range, but still resembles slides.\n- 0.0: Not PDF/DOCX OR not a slide deck at all.\n\nOnly evaluate presence and format, not content quality.", "expectation": "A PDF/DOCX deck with ~15\u201318 slides, clearly formatted as slides."}, {"type": "llm_judge", "name": "Required Sections Presence", "description": "Check that the deck contains the core sections needed for later verification across Amazon and TikTok services.", "weight": 2.0, "judge_prompt": "You are evaluating STRUCTURE ONLY. Check if the deck includes the following sections (flexible with naming, but intent must be clear):\n\nMust-have sections:\n- Cover slide (agency name and positioning as full-stack growth partner)\n- About/Who We Are (agency snapshot)\n- Amazon Services Overview\n- TikTok Services Overview\n- Amazon Account Management\n- PPC/Advertising Strategy (Amazon)\n- Creative Optimization (Amazon) \u2013 e.g., A+ Content, Brand Story, image revamps\n- Analytics & Reporting\n- Review Generation and/or Social Proof Program\n- TikTok Shop Setup / Commerce Enablement\n- Influencer Outreach / Creator Partnerships\n- Process/Engagement Model or Onboarding\n- Case Studies/Results (at least one slide; preferably two)\n- Next Steps and/or Contact\n\nScoring (0\u20132):\n- 2.0: All items present OR missing at most one minor section while intent is obviously covered elsewhere.\n- 1.0: Missing 2\u20133 of the required sections, but majority are present.\n- 0.0: Missing more than 3 required sections or sections are not identifiable as slides.\n\nDo not assess content accuracy\u2014only whether these sections exist as slides.", "expectation": "All core sections present with clear slide-level headers."}, {"type": "llm_judge", "name": "Slide-Level Elements (Title, Summary, Bullets, Visuals)", "description": "Each service slide should include a clear title, 1\u20132 sentence summary, concise bullet list of capabilities, and appropriate visual elements on most slides.", "weight": 3.0, "judge_prompt": "STRUCTURE ONLY, not quality. Inspect a representative sample or the whole deck. For the majority of service slides (not just cover/contact), check:\n- Title present (clear, concise)\n- 1\u20132 sentence summary under the title (not a full paragraph)\n- A concise bulleted list (3\u20136 bullets typical)\n- Visual elements on most slides (icons, dashboards, product/creative samples)\n\nScoring (0\u20133):\n- 3.0: Clear evidence that most service slides have all four elements; visuals appear on most slides.\n- 2.0: One element inconsistently applied (e.g., summaries present on only ~50\u201370% of service slides or visuals appear on fewer than half).\n- 1.0: Several elements frequently missing; only a minority of service slides follow the requested format.\n- 0.0: Slides generally lack the requested structure (titles only, or mostly text blocks without bullets/visuals).\n\nOnly verify presence/structure, not the quality or correctness of content.", "expectation": "Most service slides have all four required elements with consistent structure."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness Verification (Mixed: Code + LLM)", "description": "Now that structure exists, verify content correctness and coverage across Amazon and TikTok. Code rules perform deterministic checks; LLM judges handle nuanced verification and cross-checks. If SERVICESV5.docx is provided, judges should consider alignment; if not available, judge internal consistency and domain-plausibility.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Document Type and Non-trivial Size", "description": "Confirm the primary output is a document (PDF/DOCX) and of non-trivial size indicative of a visual deck.", "weight": 0.8, "code": "def evaluate(workflow, context):\n    from pathlib import Path\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No primary output.\"\n    if not output.is_document:\n        return 0.0, \"Primary output is not a document.\"\n    path = context.files.get_path(output.id)\n    suffix = path.suffix.lower()\n    valid = suffix in [\".pdf\", \".docx\"]\n    try:\n        size_kb = path.stat().st_size / 1024.0\n    except Exception:\n        size_kb = 0.0\n    # Scoring tiers\n    if valid and size_kb >= 80:\n        score = 1.0\n        fb = f\"Valid {suffix} with size {size_kb:.1f} KB.\"\n    elif valid and size_kb >= 40:\n        score = 0.8\n        fb = f\"Valid {suffix} but somewhat small ({size_kb:.1f} KB).\"\n    elif valid:\n        score = 0.5\n        fb = f\"Valid {suffix} but very small ({size_kb:.1f} KB).\"\n    else:\n        score = 0.0\n        fb = f\"Invalid extension {suffix}.\"\n    return score, fb"}, {"type": "code", "name": "Keyword Coverage for Required Service Areas", "description": "Heuristic coverage check: ensure text references major Amazon/TikTok service areas (account mgmt, PPC, creative/A+, analytics, reviews, TikTok Shop, influencer).", "weight": 0.8, "code": "def evaluate(workflow, context):\n    import re\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No primary output.\"\n    text = \"\"\n    try:\n        path = context.files.get_path(output.id)\n        suffix = path.suffix.lower()\n        if suffix == \".pdf\":\n            text = context.files.read_pdf_text(output.id)\n        elif suffix == \".docx\":\n            text = context.files.read_docx_text(output.id)\n        else:\n            return 0.0, \"Unsupported document type for text extraction.\"\n    except Exception as e:\n        return 0.0, f\"Text extraction failed: {e}\"\n    if not text:\n        return 0.0, \"No extractable text.\"\n    t = text.lower()\n    buckets = [\n        [\"account management\", \"catalog\", \"listing\", \"ops\"],\n        [\"ppc\", \"advertising\", \"sponsored products\", \"paid media\"],\n        [\"a+ content\", \"brand story\", \"creative\", \"image\"],\n        [\"analytics\", \"reporting\", \"dashboard\", \"insight\"],\n        [\"review\", \"ratings\", \"ugc\", \"feedback\", \"vine\"],\n        [\"tiktok shop\", \"shop setup\", \"shopping\", \"tiktok\"],\n        [\"influencer\", \"creator\", \"outreach\", \"affiliate\", \"seeding\"],\n        [\"amazon\"]\n    ]\n    total = len(buckets)\n    hits = 0\n    matched = []\n    for bucket in buckets:\n        found = any(term in t for term in bucket)\n        if found:\n            hits += 1\n            matched.append(bucket[0])\n    score = hits / total\n    fb = f\"Matched {hits}/{total} service areas: {', '.join(matched)}\"\n    return score, fb"}, {"type": "code", "name": "Bullets and Summary Heuristic", "description": "Check that many lines look like bullets and that several lines resemble 1\u20132 sentence summaries.", "weight": 0.4, "code": "def evaluate(workflow, context):\n    import re\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No primary output.\"\n    try:\n        path = context.files.get_path(output.id)\n        suffix = path.suffix.lower()\n        if suffix == \".pdf\":\n            text = context.files.read_pdf_text(output.id)\n        elif suffix == \".docx\":\n            text = context.files.read_docx_text(output.id)\n        else:\n            return 0.0, \"Unsupported document type.\"\n    except Exception as e:\n        return 0.0, f\"Text extraction failed: {e}\"\n    if not text:\n        return 0.0, \"No extractable text.\"\n    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n    bullet_starts = tuple([\"\u2022\", \"- \", \"\u2013 \", \"\u2014 \", \"\u25aa\", \"\u00b7 \"])\n    bullet_count = sum(1 for ln in lines if ln.startswith(bullet_starts))\n    # Summary-like: 8\u201335 words ending with a period\n    def word_count(s):\n        return len(re.findall(r\"\\b\\w+\\b\", s))\n    summary_like = [ln for ln in lines if ln.endswith(('.','!')) and 8 <= word_count(ln) <= 35]\n    bullet_score = min(1.0, bullet_count / 15.0)\n    summary_score = 1.0 if len(summary_like) >= 10 else (len(summary_like) / 10.0)\n    score = (bullet_score + summary_score) / 2.0\n    fb = f\"Bullets: {bullet_count}, summary-like lines: {len(summary_like)}\"\n    return score, fb"}, {"type": "llm_judge", "name": "Service Coverage and Mapping Correctness", "description": "Verify that each required service category slide accurately summarizes the service and lists relevant, plausible capabilities for that category. If SERVICESV5.docx is present, check alignment; if absent, judge domain-plausibility and internal consistency.", "weight": 3.0, "judge_prompt": "Review the deck and check correctness of service coverage for these categories (flexible naming allowed): Amazon Account Management, PPC/Advertising (Amazon), Creative Optimization (A+ Content/Brand Story/images), Analytics & Reporting, Review Generation/Social Proof, TikTok Shop Setup, Influencer Outreach/Creator Partnerships. For each category that appears, assess whether its 1\u20132 sentence summary and bullets are actually about that service and are plausible for a specialist agency. If SERVICESV5.docx is provided in the workspace, consider its guidance; otherwise, judge based on industry norms.\n\nScoring (0\u20133):\n- 3.0: All listed categories are present and correctly mapped with appropriate, plausible bullets and summaries.\n- 2.0: One category has minor inaccuracies or is thin; others are correct.\n- 1.0: Multiple categories are off-topic or vague.\n- 0.0: Most categories are missing or improperly described.\n\nFocus on correctness and mapping, not prose polish.", "expectation": "Accurate, plausible service descriptions and capabilities for all required categories."}, {"type": "llm_judge", "name": "Amazon vs. TikTok Balance and Depth", "description": "Ensure both platforms have dedicated overviews and substantive deep-dive slides\u2014not token mentions.", "weight": 2.5, "judge_prompt": "Assess whether the deck provides balanced, substantive coverage of BOTH Amazon and TikTok:\n- Each has an overview slide and at least one deep-dive slide (ideally multiple for Amazon given breadth, and at least TikTok Shop + Influencer for TikTok).\n- The depth for each is sufficient for a CEO/founder audience exploring outsourced growth support.\n\nScoring (0\u20132.5):\n- 2.5: Both platforms are well-covered with clear overviews and multiple deep-dive slides.\n- 1.5: One platform is clearly deeper; the other is present but thin.\n- 0.5: One platform is only briefly mentioned without depth.\n- 0.0: One platform absent.\n\nJudge presence and depth, not design quality.", "expectation": "Both platforms covered with overviews and at least one deep dive each."}, {"type": "llm_judge", "name": "Visual Appropriateness and Open-Source Suitability", "description": "Check that visuals are relevant, premium-feeling, and appear to be appropriately sourced (no obvious watermarks or copyright issues).", "weight": 2.5, "judge_prompt": "Inspect the visuals across the deck. Evaluate:\n- Relevance: dashboards, product/creative samples, icons that support the message\n- Premium feel: clean, modern, not clip-art heavy or pixelated\n- Sourcing: no visible watermarks or stock warnings; visuals appear allowable as open-source or properly non-infringing\n\nScoring (0\u20132.5):\n- 2.5: Visuals are relevant, premium, and show no sourcing issues.\n- 1.5: Generally good but 1\u20132 visuals are low quality or ambiguous sourcing.\n- 0.5: Multiple visuals feel off or low quality; possible sourcing concerns.\n- 0.0: Visuals largely irrelevant or exhibit clear copyright/watermark issues.\n\nThis is a correctness check about appropriate use of visuals, not aesthetic nitpicking.", "expectation": "Most slides include relevant, premium visuals without sourcing issues."}, {"type": "llm_judge", "name": "Claims, Metrics, and Evidence", "description": "Evaluate whether case studies/metrics are framed responsibly (attribution, context, no overpromising).", "weight": 2.2, "judge_prompt": "Review any case studies, metrics, or performance claims. Check:\n- Context (client, timeframe, channel) is indicated or reasonably implied\n- No sweeping guarantees; claims are framed as examples or ranges\n- Where applicable, sources or methodology are hinted (e.g., platform dashboards)\n\nScoring (0\u20132.2):\n- 2.2: Claims are responsibly presented with context and no overpromises.\n- 1.2: Mostly responsible, but some metrics are thin on context.\n- 0.5: Several claims feel ungrounded.\n- 0.0: Largely unsubstantiated or misleading claims.\n\nFocus on correctness and responsibility of claims, not writing style.", "expectation": "Case studies/claims are contextualized and responsible."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Holistic Quality Assessment (LLM ONLY)", "description": "Professional polish and strategic resonance for CEO/founder audience. Assess narrative flow, tone, consistency, and persuasive strength as a premium yet approachable growth partner deck.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Narrative Flow and Positioning", "description": "Assesses whether the deck tells a cohesive story positioning the agency as a full-stack growth partner for Amazon and TikTok.", "weight": 3.0, "judge_prompt": "Evaluate the overall narrative arc:\n- Clear positioning as a full-stack growth partner across Amazon and TikTok\n- Logical flow: problem/opportunity \u2192 services \u2192 proof/results \u2192 engagement \u2192 next steps\n- Smooth transitions and slide ordering that supports executive comprehension\n\nScoring (0\u20133):\n- 3.0: Strong, cohesive story with clear positioning and logical flow.\n- 2.0: Generally coherent with minor gaps.\n- 1.0: Choppy flow or muddled positioning.\n- 0.0: No clear narrative or positioning.\n\nJudge overall narrative, not micro-level copy edits.", "expectation": "Cohesive, executive-ready story emphasizing full-stack partnership."}, {"type": "llm_judge", "name": "Professional Polish and Consistency", "description": "Tone, formatting consistency, and readability across slides.", "weight": 3.0, "judge_prompt": "Assess professional polish:\n- Consistent typography, spacing, and styling across slides\n- Premium yet approachable tone; concise, scannable copy\n- Appropriate slide density (no walls of text; clear bullets)\n\nScoring (0\u20133):\n- 3.0: Highly polished and consistent; premium yet approachable.\n- 2.0: Generally polished with minor inconsistencies.\n- 1.0: Uneven polish; noticeable inconsistencies.\n- 0.0: Unpolished and inconsistent.\n\nDo not double-count structural presence\u2014focus on quality execution.", "expectation": "Premium yet approachable tone; consistent, readable slides."}, {"type": "llm_judge", "name": "Audience Fit and Clarity for Executives", "description": "Suitability for CEOs/founders/brand leads, focusing on clarity and decisiveness.", "weight": 2.0, "judge_prompt": "Judge whether the deck effectively serves busy executives:\n- Clear value propositions and outcomes\n- Actionable clarity: what you do, how you engage, expected timelines\n- Avoids jargon without explanation; uses concise, decisive language\n\nScoring (0\u20132):\n- 2.0: Highly clear and executive-friendly.\n- 1.0: Mostly clear with some ambiguity or jargon.\n- 0.0: Unclear, indecisive, or jargon-heavy.\n\nDo not assess correctness of claims, only audience fit and clarity.", "expectation": "Clear, executive-appropriate messaging with decisive value propositions."}, {"type": "llm_judge", "name": "CTA and Next Steps Effectiveness", "description": "Strength of call-to-action and guidance on engagement steps.", "weight": 2.0, "judge_prompt": "Evaluate the Next Steps / Contact content:\n- Clear CTA (e.g., book a discovery call, audit, pilot)\n- Easy-to-find contact details and expected response times or process\n- Low-friction path to engage\n\nScoring (0\u20132):\n- 2.0: Strong CTA with clear engagement steps and contact info.\n- 1.0: Present but somewhat vague or incomplete.\n- 0.0: Missing or ineffective CTA.\n\nFocus on practical usefulness for a prospect.", "expectation": "Clear, compelling CTA and contact/next steps for prospects."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "fccaa4a1-1c39-49ac-b701-55361a19966b", "rubric": {"category_name": "Real Estate & Rental \u2014 Concierge: VIP Tour Plan (Statue of Liberty & Ellis Island)", "rationale": "Pattern B (Document). The task requires a professionally formatted, client-facing PDF with a strict structure that enables verification. Stage 1 uses an LLM-only gate to enforce document shape (PDF, sections, headers, layout with icons, image, itinerary presence). Stage 2 mixes code (deterministic text checks) and LLM (nuanced itinerary feasibility and source fidelity), with code rules weighted ~5x less than LLM rules. Stage 3 provides a holistic quality assessment across design, client-centric tone, clarity/actionability, and photo attribution.", "max_total_score": 25.0, "stages": [{"name": "Stage 1 \u2014 Format & Structure Gate (LLM-only)", "description": "Enforce exact document shape and structural completeness so later verification is trivial. If this gate fails, the entire category scores 0.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.5, "rules": [{"type": "llm_judge", "name": "PDF Structure, Sections, and Visual Elements Present", "description": "Check that the output is a properly structured PDF tour document with required sections, headers, and visual elements.", "weight": 6.0, "judge_prompt": "You are validating a client-facing tour PDF for a luxury concierge service. Check ONLY format and structural presence (not correctness of claims). Use the rendered PDF to verify layout and visual elements.\n\nRequired format and layout:\n- File must be a PDF (not Word/Excel/plain text).\n- Professionally formatted; at least 2 pages total.\n- Title visible: \u201cEarly Access Statue of Liberty & Ellis Island Tour\u201d (prominent on the first page).\n- A small photo of the Statue of Liberty near the title or hero area (no obvious watermark; any small, relevant photo is acceptable for structure).\n- Use of icons or small pictograms to visually label key details (examples: location, duration, overview, inclusions, requirements). Do not penalize for icon style; just verify icons or similar visual markers are present.\n\nRequired sections and headers (check headers are visible):\n1) Header exactly: \u201coverview of activities\u201d\n2) Header exactly: \u201cinclusions\u201d\n3) Header exactly: \u201crequirements\u201d\n\nAdditional required labeled content areas (headers may vary slightly unless otherwise specified):\n- Location (must state New York City, United States)\n- Duration (must indicate 4 hours)\n- Highlights list (should include items like: first group of the day, licensed New York tour guide, small group of max 25, visiting Ellis Island, seeing NYC skyline)\n- Description of the tour operator and sites visited (must reference www.TakeWalks.com somewhere in this section)\n- Itinerary (step-by-step tour explanation; intended to span roughly two pages, but allow reasonable variation as long as it is substantial and clearly step-by-step)\n- Meeting Location (Harbor House, 22 Battery Place, in Battery Park)\n- End Point (Ellis Island or option to take the ferry back to Battery Park with a Guide)\n- Inclusions and Requirements block(s), clearly separated and visually organized using icons or similar markers\n\nScoring guidance (structural presence only):\n- 6.0: PDF format; at least 2 pages; title present; small Statue of Liberty photo; icons/visual markers present; all three exact headers present (\u201coverview of activities\u201d, \u201cinclusions\u201d, \u201crequirements\u201d); all additional required content areas present.\n- 5.0\u20135.5: One minor visual element missing (e.g., icons limited or photo slightly misplaced) but all key sections and headers present.\n- 4.0\u20134.5: Missing one required section (e.g., Highlights or Description) OR only two of the three exact headers present, but otherwise PDF, 2+ pages, title, itinerary, and locations are present.\n- 2.0\u20133.5: Missing two required sections/headers OR the itinerary is not clearly step-by-step; still a PDF with 2+ pages and includes title.\n- 0.0\u20131.5: Not a PDF, fewer than 2 pages, missing title, or missing multiple critical sections.\n\nDo not judge correctness or quality here\u2014only structure and presence.", "expectation": "A cleanly laid out, at least 2-page PDF with the exact headers \u201coverview of activities\u201d, \u201cinclusions\u201d, and \u201crequirements\u201d; visible title; small Statue of Liberty photo; icon usage; and all required content areas present."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification of Content and Claims (Mixed Code + LLM)", "description": "Verify that the content is internally consistent, matches key requirements, and reflects accurate, non-contradictory claims aligned with the operator reference.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Core Facts and Headers Present in Text", "description": "Deterministic scan for essential facts, headers, and required references in the extracted PDF/DOCX text.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float in [0, weight] based on presence of core facts\n    \"\"\"\n    import re\n    \n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    \n    text = None\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        text = None\n    if not text:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = None\n    if not text:\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            text = None\n    if not text:\n        return 0.0\n    \n    lt = text.lower()\n    score_bits = []\n    \n    # Title: allow & or 'and'\n    title_ok = bool(re.search(r\"early access statue of liberty\\s*(?:&|and)\\s*ellis island tour\", lt))\n    score_bits.append(title_ok)\n    \n    # Location\n    loc_ok = (\"new york city\" in lt) and (\"united states\" in lt or \"usa\" in lt or \"u.s.\" in lt)\n    score_bits.append(loc_ok)\n    \n    # Duration 4 hours\n    duration_ok = bool(re.search(r\"\\b4\\s*hour\", lt)) or (\"four hour\" in lt) or (\"4-hour\" in lt)\n    score_bits.append(duration_ok)\n    \n    # Required exact headers\n    headers_ok = (\"overview of activities\" in lt) and (\"inclusions\" in lt) and (\"requirements\" in lt)\n    score_bits.append(headers_ok)\n    \n    # Highlights and Itinerary sections present\n    hi_ok = (\"highlights\" in lt)\n    itinerary_ok = (\"itinerary\" in lt)\n    score_bits.append(hi_ok)\n    score_bits.append(itinerary_ok)\n    \n    # Meeting Location details\n    meet_ok = (\"harbor house\" in lt) and (\"22 battery place\" in lt) and (\"battery park\" in lt)\n    score_bits.append(meet_ok)\n    \n    # End Point options\n    end_ok = (\"ellis island\" in lt) and (\"battery park\" in lt)\n    score_bits.append(end_ok)\n    \n    # Operator reference\n    op_ok = (\"takewalks\" in lt) or (\"www.takewalks.com\" in lt) or (\"https://www.takewalks.com\" in lt)\n    score_bits.append(op_ok)\n    \n    # Compute fractional score\n    if not score_bits:\n        return 0.0\n    frac = sum(1 for b in score_bits if b) / len(score_bits)\n    return frac * 1.0"}, {"type": "code", "name": "Inclusions, Requirements, and Key Highlights Specifics", "description": "Deterministic check for language, exclusions, difficulty, restrictions, age band, and signature highlights (small group 25, licensed guide, early access).", "weight": 1.0, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow\n        context: ValidationContext\n    Returns:\n        float in [0, 1] based on presence of specific policy/option terms\n    \"\"\"\n    import re\n    \n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    \n    text = None\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        text = None\n    if not text:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = None\n    if not text:\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            text = None\n    if not text:\n        return 0.0\n    \n    lt = text.lower()\n    checks = []\n    \n    # Language\n    checks.append(\"english\" in lt)\n    \n    # Not included options\n    crown_or_pedestal = (\"crown\" in lt) or (\"pedestal\" in lt)\n    gratuities = (\"gratuities\" in lt) or (\"tip\" in lt) or (\"tipping\" in lt)\n    hotel_pickup = (\"hotel pick-up\" in lt) or (\"hotel pickup\" in lt) or (\"hotel pick up\" in lt)\n    dropoff = (\"drop-off\" in lt) or (\"drop off\" in lt)\n    checks.extend([crown_or_pedestal, gratuities, hotel_pickup or dropoff])\n    \n    # Difficulty\n    checks.append(\"moderate\" in lt)\n    \n    # Restrictions (clear containers/bottles)\n    restr_ok = (\"clear container\" in lt) or (\"clear bottle\" in lt) or (\"clear plastic\" in lt) or (\"clear bag\" in lt)\n    checks.append(restr_ok)\n    \n    # Age requirements (2-14)\n    age_ok = bool(re.search(r\"\\b(2\\s*[-\u2013\u2014]\\s*14|2\\s*to\\s*14|ages?\\s*2\\s*(?:to|\\-)\u001914)\\b\", lt)) or (\"2-14\" in lt) or (\"2 to 14\" in lt)\n    checks.append(age_ok)\n    \n    # Signature highlights\n    small_group_25 = (\"25\" in lt) and (\"small group\" in lt or \"maximum\" in lt or \"max\" in lt)\n    licensed_guide = (\"licensed\" in lt and \"guide\" in lt) or (\"licensed new york tour guide\" in lt)\n    early_access = (\"early access\" in lt) or (\"first group of the day\" in lt) or (\"first ferry\" in lt) or (\"priority\" in lt)\n    checks.extend([small_group_25, licensed_guide, early_access])\n    \n    frac = sum(1 for c in checks if c) / len(checks) if checks else 0.0\n    return frac * 1.0"}, {"type": "llm_judge", "name": "Itinerary Feasibility and Coverage (<= 4 hours, step-by-step)", "description": "Judge whether the itinerary is step-by-step, time-aware, covers both Liberty and Ellis Islands with ferry logistics, respects the 4-hour limit, and mentions the option to end at Ellis Island or return to Battery Park.", "weight": 4.0, "judge_prompt": "Evaluate the ITINERARY section for feasibility and completeness (not just presence). Look for: step-by-step sequencing with approximate times or durations, ferry/security timing, coverage of Statue of Liberty (grounds/museum; no crown/pedestal implied as included) and Ellis Island, and the option to finish at Ellis Island or return to Battery Park with the guide. Confirm that total activity fits within about 4 hours and suits 2 guests. Reasonable buffers for security and ferry boarding should be included. Penalize if the plan clearly exceeds 4 hours or omits a critical transfer.\n\nScoring:\n- 4.0: Clear step-by-step flow with times/durations; includes both islands; security/ferry buffers; option to end at Ellis Island or return; plausibly within 4 hours.\n- 3.0: Solid sequence and coverage; minor timing ambiguity but plausibly under 4 hours.\n- 2.0: Step-by-step present but important timing gaps or unclear feasibility.\n- 1.0: Vague/linear list with poor timing; unclear whether both islands are covered or 4-hour cap is respected.\n- 0.0: No usable itinerary or clearly exceeds 4 hours/omits essential segments.", "expectation": "A realistic, time-aware, step-by-step plan covering Liberty and Ellis Islands within ~4 hours with return option noted."}, {"type": "llm_judge", "name": "Operator Source Fidelity and Claims Accuracy", "description": "Confirm description references www.TakeWalks.com and that claims align with common offerings (early access/first ferry, small group max 25, licensed guides) without falsely including crown/pedestal. Check that exclusions are clearly noted.", "weight": 4.0, "judge_prompt": "Review the section describing the tour operator and sites. Confirm that www.TakeWalks.com is referenced and that the narrative aligns with typical features: early access/first ferry or priority entry, small group (max 25), licensed New York guides. Verify it does NOT promise Statue of Liberty Crown or Pedestal access. Check that exclusions (e.g., crown/pedestal, gratuities, hotel pickup/drop-off) are mentioned somewhere in the document. Flag contradictions (e.g., highlights say small group 25 but text says 30).\n\nScoring:\n- 4.0: Clear reference to www.TakeWalks.com; claims consistent (early access/priority, max 25, licensed guides); exclusions correctly noted; no contradictions.\n- 3.0: Mostly consistent; minor omissions (e.g., one exclusion not explicit) but no major contradictions.\n- 2.0: Several gaps or vague operator description; possible minor inconsistency.\n- 1.0: Weak sourcing and/or conflicting claims.\n- 0.0: No operator reference or seriously misleading/false claims.", "expectation": "Accurate operator attribution and consistent claims without overpromising inclusions like crown/pedestal."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Holistic Quality Assessment (LLM)", "description": "Evaluate professional polish, client-centric tone, clarity, and visual storytelling.", "is_required": false, "max_points": 9.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Design, Visual Hierarchy, and Iconography", "description": "Assess cleanliness of layout, consistent styling, effective use of icons, and readability.", "weight": 2.25, "judge_prompt": "Assess the PDF\u2019s professional design: clean typography, spacing, consistent styles, and clear visual hierarchy. Icons or pictograms should meaningfully label key items (location, duration, inclusions, requirements). Headings should be distinct and scannable. Deduct for clutter, inconsistent styles, or poor hierarchy.", "expectation": "A polished, luxury-brand-appropriate layout with consistent icon usage and clear section hierarchy."}, {"type": "llm_judge", "name": "Client-Centric Personalization and Concierge Tone", "description": "Evaluate how well the plan suits a 45-year-old father and 16-year-old son with a VIP/early-access focus, and the concierge tone.", "weight": 2.25, "judge_prompt": "Judge whether the document feels tailored for a 45-year-old father and 16-year-old son: engaging highlights for both ages, VIP/early-access messaging, and a warm yet professional concierge tone. Look for practical touches (meeting time guidance, comfort tips, brief safety/security notes) suited to first-time visitors. Deduct for generic or impersonal language.", "expectation": "Warm, tailored, discreetly luxurious concierge tone with details that fit the family\u2019s profile and first-time status."}, {"type": "llm_judge", "name": "Clarity, Actionability, and Practical Guidance", "description": "Evaluate whether the guest can confidently follow the plan: clear meeting point, directions, timing, and contingencies.", "weight": 2.25, "judge_prompt": "Assess clarity and actionability: meeting point description should be unmistakable (Harbor House, 22 Battery Place in Battery Park), include timing guidance and what to bring (ID, clear bottles/containers), and note security screening. The plan should be easy to follow without confusion and include brief contingencies (weather/ferry notes). Deduct if the guest would still have major questions.", "expectation": "Clear, step-by-step guidance that a first-time visitor can follow without confusion."}, {"type": "llm_judge", "name": "Photo Selection and Attribution", "description": "Evaluate the quality, relevance, placement, and attribution of the Statue of Liberty photo.", "weight": 2.25, "judge_prompt": "Check that the PDF features a small, relevant photo of the Statue of Liberty near the title or hero area. Prefer royalty-free imagery without watermarks. Look for a simple credit or source mention (URL or site name). Deduct for irrelevant imagery, watermarks, or missing/unclear attribution.", "expectation": "A tasteful, relevant Statue of Liberty photo with unobtrusive placement and a simple source/credit."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "403b9234-6299-4b5f-a106-70c1bc11ec4c", "rubric": {"category_name": "Government \u2013 Recreation Workers: Chamber Partnership Presentation", "rationale": "This rubric enforces a self-documenting, presentation-first structure, then verifies coverage and correctness of required content, and finally assesses persuasive quality for a skeptical county Recreation Advisory Board. Stage 1 uses an LLM-only gate to mandate an 8\u201310 slide deck with clearly labeled slides for rationale, what Chambers do, why the Chamber is the right first partner, and direct/indirect benefits, plus discussion. Stage 2 mixes light code checks (file type, keyword presence) with LLM verification of content coverage and realism. Stage 3 evaluates persuasiveness, design clarity, actionability, and public-sector alignment.", "max_total_score": 12.0, "stages": [{"name": "Stage 1 \u2013 Shape Enforcement Gate (Presentation Structure)", "description": "LLM-only gate verifying the output is a presentation (PPTX or PDF export) with 8\u201310 slides and required slide types/titles. Only checks presence and structure, not content quality.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.0, "rules": [{"type": "llm_judge", "name": "Presentation Format and Structural Completeness", "description": "Check that the candidate produced a concise 8\u201310 slide presentation in PPTX or PDF format with all required slide types present and clearly labeled.", "weight": 3.0, "judge_prompt": "You are evaluating whether the submitted output is a properly structured presentation for a county Recreation Advisory Board. Only check PRESENCE and FORMAT, not content quality or factual accuracy.\n\nFormat requirements (be flexible with synonymous titles/wording):\n- File should be a presentation (PPTX) or a clean PDF export of slides. If DOCX is provided, it must be clearly formatted as slides (one slide per page) with slide titles.\n- Deck length must be between 8 and 10 slides, inclusive.\n- Each slide should have a clear, visible title and concise bullet points (no walls of text).\n\nRequired slide types/sections (titles may vary, count them if merged/split as noted):\n1) Title slide: indicates the Chamber partnership topic, county/department name, presenter, and date (or at least 2 of these elements plus the topic).\n2) Why pursue community partnerships (overview/rationale): 1 slide with 3\u20135 bullets.\n3) What Chambers of Commerce do: 1 slide with 3\u20136 bullets describing typical functions (advocacy, networking, business support, economic development, convening, etc.).\n4) Why our County Chamber is the right first partner: 1\u20132 slides with reasons/fit.\n5) Benefits: Direct and Indirect benefits must both be clearly present. This may be one slide with two subsections or two separate slides (Direct Benefits, Indirect Benefits). Together should total 1\u20132 slides.\n6) Discussion/Q&A prompt slide: explicitly invites open discussion, questions, or next steps conversation.\n(Optional but acceptable within the 8\u201310): Risks & mitigations and/or Next Steps can be included as separate slides or integrated where appropriate, as long as the Discussion/Q&A slide is present.\n\nScoring (structure only):\n- 3.0: Valid presentation format and length (8\u201310 slides) AND all required slide types present (Title; Partnerships Rationale; What Chambers Do; Why Chamber Partner; Benefits showing both Direct and Indirect; Discussion/Q&A).\n- 2.0: Valid format and length but missing exactly one required slide type OR benefits shown but not clearly split into direct vs. indirect.\n- 1.0: Valid format but length outside 8\u201310 OR missing two required slide types.\n- 0.0: Not a presentation (e.g., plain text), unusable format, or missing three or more required slide types.\n\nOnly evaluate presence/format/structure and slide counting. Do not judge accuracy, persuasiveness, or visual design beyond verifying that slides have titles and bullet-style content.", "expectation": "A 8\u201310 slide PPTX or PDF deck containing the required slide types with clear titles and bullet points."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification of Coverage and Correctness", "description": "Verify that the deck actually covers the specified content accurately and consistently. Light code checks plus LLM checks for content coverage and realism.", "is_required": true, "max_points": 5.0, "min_score_to_pass": 2.5, "rules": [{"type": "code", "name": "Valid Presentation File Type", "description": "Checks that the primary output is a document suitable for a presentation (PDF, PPTX, or DOCX formatted as slides).", "weight": 0.3, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float score in [0, 0.3] or (score, feedback)\n    \"\"\"\n    import os\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output resource found.\"\n    # Prefer PDFs/PPTX/DOCX; reject spreadsheets/plain text/images for this task\n    try:\n        path = context.files.get_path(output.id)\n        ext = os.path.splitext(str(path))[-1].lower()\n    except Exception:\n        ext = \"\"\n    allowed_exts = {\".pdf\", \".pptx\", \".docx\"}\n    is_ok = (ext in allowed_exts) or (getattr(output, 'is_document', False))\n    return (0.3 if is_ok else 0.0, f\"Detected extension: {ext} | is_document={getattr(output, 'is_document', None)}\")"}, {"type": "code", "name": "Mentions Chamber and Benefits Keywords", "description": "Light text check for presence of key terms indicating topical coverage. Gracefully degrades if text extraction is not available (e.g., PPTX).", "weight": 0.2, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Returns up to 0.2 based on keyword presence.\n    Tries PDF->DOCX->text. If unreadable (e.g., PPTX), awards 0.1 as neutral baseline to avoid over-penalizing when LLM will assess content.\n    \"\"\"\n    import re\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output resource.\"\n    text = \"\"\n    tried = []\n    try:\n        text = context.files.read_pdf_text(output.id)\n        tried.append('pdf')\n    except Exception:\n        pass\n    if not text:\n        try:\n            text = context.files.read_docx_text(output.id)\n            tried.append('docx')\n        except Exception:\n            pass\n    if not text and getattr(output, 'is_text_format', False):\n        try:\n            text = context.files.read_text(output.id)\n            tried.append('text')\n        except Exception:\n            pass\n    if not text:\n        # Likely PPTX or image-based PDF; return neutral partial credit.\n        return 0.1, f\"Could not extract text; attempted: {tried}. Granting neutral partial credit.\"\n    t = text.lower()\n    score = 0.0\n    if 'chamber of commerce' in t or 'chamber' in t:\n        score += 0.08\n    if 'benefit' in t or 'value' in t:\n        score += 0.06\n    # Direct/Indirect mentions\n    direct = bool(re.search(r'\\bdirect\\b', t))\n    indirect = bool(re.search(r'\\bindirect\\b', t))\n    if direct:\n        score += 0.03\n    if indirect:\n        score += 0.03\n    # Cap at weight\n    if score > 0.2:\n        score = 0.2\n    return score, f\"Keyword hits -> chamber: {'chamber of commerce' in t or 'chamber' in t}, benefit: {'benefit' in t or 'value' in t}, direct: {direct}, indirect: {indirect}\""}, {"type": "llm_judge", "name": "Coverage of Required Content", "description": "Confirms the deck actually covers each required topic with at least one dedicated slide (or clearly labeled section), including clear separation of direct vs. indirect benefits.", "weight": 2.2, "judge_prompt": "Evaluate whether the presentation substantively covers the required topics, beyond mere presence of headings:\n- A clear slide explaining why the department should pursue community partnerships (rationale tailored to parks & recreation: resource leverage, program reach, cost-effectiveness, community engagement).\n- A slide describing what Chambers of Commerce do (typical, realistic functions: advocacy, business networking, information sharing, convening, small business support, economic development partnerships). Avoids claiming control over county operations.\n- One or more slides giving concrete reasons why THIS County Chamber is a strong first partner (fit, shared goals, readiness, capacity), not just generic praise.\n- Benefits are explicitly distinguished into Direct (e.g., sponsorships, volunteers, marketing reach for events/programs, in-kind resources) and Indirect (e.g., reputation, business community goodwill, broader participation, economic vitality). These can be one slide with two clear subsections or two slides.\nScoring:\n- 2.2: All four areas covered clearly and specifically.\n- 1.5: Three areas strong; one area thin/generic.\n- 0.8: Only two areas substantive; others superficial/missing.\n- 0.0: Major gaps; content not aligned to required topics.", "expectation": "Each required topic is addressed with specific, parks & recreation\u2013relevant content."}, {"type": "llm_judge", "name": "Realism and Policy Consistency", "description": "Checks that the depiction of Chamber roles and the partnership concept are realistic and consistent with county government practices/policies.", "weight": 1.7, "judge_prompt": "Assess whether the content is realistic and consistent with public-sector norms:\n- Depiction of Chamber functions is accurate (business advocacy, networking, convening, information/resource sharing) and does not overstate legal/operational authority over county programs.\n- Partnership ideas respect county rules: no promises of exclusivity that would violate procurement/ethics, recognition handled fairly, data-sharing and sponsorship handled with appropriate safeguards.\n- Claims about benefits are plausible and do not promise guaranteed revenue or outcomes without caveats.\nScoring:\n- 1.7: Fully realistic and policy-consistent; no red flags.\n- 1.1: Minor issues but generally sound.\n- 0.6: Several overstatements or policy risks.\n- 0.0: Unrealistic claims or policy-inconsistent proposals.", "expectation": "Accurate depiction of Chamber roles; partnership concept compatible with county ethics/procurement norms."}, {"type": "llm_judge", "name": "Explicit Discussion/Q&A Facilitation", "description": "Verifies that the deck intentionally invites open discussion with a clear prompt and/or questions for the Advisory Board.", "weight": 0.8, "judge_prompt": "Confirm there is a dedicated Discussion/Q&A slide (or end slide) that explicitly invites the Board to engage (e.g., key questions, prompts, decision points). It should encourage open dialogue, not assume approval.\nScoring:\n- 0.8: Clear discussion/Q&A prompt with 2\u20135 guiding questions or prompts.\n- 0.4: Discussion/Q&A present but minimal/ambiguous.\n- 0.0: No clear invitation for discussion.", "expectation": "A visible final or near-final slide that encourages Board discussion with concrete prompts."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Quality and Persuasive Effectiveness", "description": "Holistic LLM assessment of persuasive quality, clarity, and strategic value for a skeptical Advisory Board in a county government context.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Persuasive Narrative for a Skeptical Board", "description": "Evaluates whether the deck makes a coherent, evidence-informed case tailored to the Board's concerns and motivations.", "weight": 1.2, "judge_prompt": "Rate the persuasiveness of the narrative for a skeptical Recreation Advisory Board:\n- Clear problem framing (limited partnerships now, missed opportunities) and how a Chamber partnership is a low-risk, high-learning first step.\n- Anticipates likely objections (mission drift, control, equity, workload) and offers practical mitigations.\n- Provides a credible path to evaluate/exit the partnership if goals aren\u2019t met.\nScoring: 1.2 excellent; 0.8 good; 0.4 fair; 0.0 weak.", "expectation": "Concise, Board-relevant argument that anticipates objections and shows measured, reversible steps."}, {"type": "llm_judge", "name": "Slide Design Clarity and Concision", "description": "Assesses readability and focus: strong titles, scannable bullets, minimal clutter, appropriate visuals if any.", "weight": 1.0, "judge_prompt": "Evaluate slide design for clarity:\n- Each slide has a strong, descriptive title.\n- 3\u20135 concise bullets per content slide; avoids dense paragraphs and jargon.\n- Visuals (if used) aid understanding without clutter.\n- Consistent, professional formatting.\nScoring: 1.0 excellent; 0.7 good; 0.4 adequate; 0.0 poor.", "expectation": "Clean, scannable slides suitable for in-meeting discussion."}, {"type": "llm_judge", "name": "Actionability and Implementation Readiness", "description": "Assesses whether next steps, milestones, and responsibilities are clear enough to move forward.", "weight": 0.9, "judge_prompt": "Check for practical actionability:\n- Clear immediate next steps (e.g., draft MOU outline, pilot scope, timeline, points of contact).\n- How success will be measured (KPIs such as event attendance, sponsorship value, volunteer hours).\n- Roles/responsibilities and timeline highlights.\nScoring: 0.9 strong; 0.6 moderate; 0.3 minimal; 0.0 absent.", "expectation": "Specific, realistic next steps and basic KPIs for a pilot partnership."}, {"type": "llm_judge", "name": "Public-Sector Alignment and Balance of Interests", "description": "Evaluates tone and compliance awareness: fairness, equity, and community benefit emphasized; no inappropriate commitments.", "weight": 0.9, "judge_prompt": "Assess alignment with county government values and guardrails:\n- Emphasizes fairness, transparency, inclusivity, and community benefit.\n- Avoids exclusivity or endorsements that could breach procurement/ethics.\n- Balanced portrayal of business interests and public mission.\nScoring: 0.9 strong alignment; 0.6 generally aligned; 0.3 mixed; 0.0 misaligned.", "expectation": "Content reflects government ethics and community-first values while engaging the business community."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "7d7fc9a7-21a7-4b83-906f-416dea5ad04f", "rubric": {"category_name": "Prepaid Amortization Schedules (Aurisic)", "rationale": "Pattern A (Analytical). Output is a structured Excel workbook enabling deterministic checks. Stage 1 uses an LLM gate to enforce exact workbook shape so Stage 2 code can verify reconciliations and logic with simple, robust checks. Stage 2 mixes lightweight code (bounds, recon tie-outs) with heavier LLM judgment (amortization logic, policy handling, cross-tab consistency). Stage 3 judges professional quality and auditability.", "max_total_score": 30.0, "stages": [{"name": "Stage 1 \u2014 Structured Workbook Gate (LLM only)", "description": "Gate: Verify the candidate produced a single Excel workbook with the exact tab structure and required sections to enable deterministic verification.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Structured Excel Workbook Requirements", "description": "Check that the output is an Excel workbook (.xlsx/.xlsm) with exactly the required tabs and section structures to make verification trivial.", "weight": 8.0, "judge_prompt": "You are validating ONLY the presence and structure of an Excel workbook (not correctness of numbers). Use flexible matching for labels, but ensure the required content exists visibly. If the artifact is not an Excel workbook, score 0.\n\nRequired workbook structure:\n\nA) File format\n- Must be an Excel workbook (.xlsx or .xlsm). Not CSV, not PDF, not DOCX.\n\nB) Tabs (sheet names may have minor variations but intent must be clear):\n1) Tab: \"Prepaid Summary\"\n   - Header area includes BOTH:\n     \u2022 Company name: \"Aurisic\"\n     \u2022 Reporting period: \"Through April 30, 2025\" (or equivalent wording for 4/30/2025)\n   - Summary section(s) clearly showing for BOTH accounts: Prepaid Expenses (1250) and Prepaid Insurance (1251):\n     \u2022 Year-to-date prepaid additions (or total prepaid additions YTD)\n     \u2022 Total amortization YTD\n     \u2022 Ending balance as of 4/30/2025\n   - GL Reconciliation section or table with monthly end-of-month balances for Jan\u2013Apr 2025 for BOTH accounts (1250 and 1251). Columns should include Month and Balance (or equivalent). The table must clearly show rows or columns for: Jan 2025, Feb 2025, Mar 2025, Apr 2025.\n\n2) Tab: \"Prepaid Expenses (1250)\"\n   - A detailed amortization schedule organized by vendor (sorted by vendor or grouped by vendor) for 2025 prepaid services invoices, listing for each invoice at least:\n     \u2022 Vendor\n     \u2022 Invoice date (or start month)\n     \u2022 Description (or memo)\n     \u2022 Original amount\n     \u2022 Amortization period (months) and start/end months\n     \u2022 Monthly expense\n   - A monthly timeline/columns or clear per-month breakout that shows remaining balance by month across 2025 at least through April (preferably Jan\u2013Dec 2025), with remaining balance visible by month.\n   - A bottom summary showing monthly activity totals and ending balances at least for Jan\u2013Apr 2025.\n\n3) Tab: \"Prepaid Insurance (1251)\"\n   - A detailed amortization schedule organized by vendor (e.g., Good Insurance, BCBS) with the same breakdowns as in the Prepaid Expenses tab:\n     \u2022 Vendor\n     \u2022 Policy/invoice period dates\n     \u2022 Original amount\n     \u2022 Amortization period and monthly expense\n     \u2022 Remaining balance by month across 2025 at least through April\n   - Must explicitly reflect:\n     \u2022 Good Insurance policy period: 1/1/2025 \u2013 12/31/2025\n     \u2022 BCBS coverage: 2/1/2025 \u2013 1/31/2026; first payment due 1/15/2025; billed monthly with payments made monthly\n   - A bottom summary showing monthly activity totals and ending balances at least for Jan\u2013Apr 2025.\n\nScoring guidance (structure only):\n- 8.0: Valid Excel file AND all three tabs present with all specified sections and elements, including GL Reconciliation and headers (company + reporting period).\n- 6.0\u20137.0: Excel present; minor omissions (e.g., small label variations) but all required sections broadly present; OR one secondary element missing (e.g., bottom summary on one tab) while core structure exists.\n- 3.0\u20135.0: Excel present but missing one major section (e.g., no GL Reconciliation, or missing remaining balance by month, or missing one required tab\u2019s required structure).\n- 1.0\u20132.0: Excel present but largely unstructured; multiple major sections missing.\n- 0.0: Not an Excel workbook or wrong artifact type.\n\nDo NOT assess numerical correctness, formulas, or reconciliation here\u2014only presence/structure.", "expectation": "A single Excel file with three clearly labeled tabs, required headers, tables, and summaries enabling verification."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness Verification (Mixed)", "description": "Verify numerical and logical correctness leveraging the enforced structure. Emphasis on LLM judgment; code rules do deterministic checks and recon ties.", "is_required": true, "max_points": 12.0, "min_score_to_pass": 7.0, "rules": [{"type": "code", "name": "Spreadsheet and Sheet Presence (Flexible Match)", "description": "Confirm the primary output is an Excel workbook and all required sheets are present with flexible name matching.", "weight": 1.0, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output detected.\"\n    # Get sheet names\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheet_names = xls.sheet_names\n    except Exception as e:\n        return 0.0, f\"Failed reading workbook: {e}\"\n\n    norm = lambda s: re.sub(r\"[^a-z0-9]\", \"\", str(s).lower())\n    norm_sheets = {norm(s): s for s in sheet_names}\n\n    # Flexible targets\n    targets = {\n        'summary': ['prepaidsummary', 'summary', 'prepaidssummary'],\n        'exp': ['prepaidexpenses', 'prepaidexpenses1250', '1250', 'prepaidservices'],\n        'ins': ['prepaidinsurance', 'prepaidinsurance1251', '1251']\n    }\n\n    found = {}\n    for key, variants in targets.items():\n        hit = None\n        for v in variants:\n            if v in norm_sheets:\n                hit = norm_sheets[v]\n                break\n        found[key] = hit\n\n    score = 0\n    fb = []\n    for key, label in [('summary','Prepaid Summary'), ('exp','Prepaid Expenses (1250)'), ('ins','Prepaid Insurance (1251)')]:\n        if found[key]:\n            score += 1/3\n            fb.append(f\"Found sheet for {label}: '{found[key]}'\")\n        else:\n            fb.append(f\"Missing sheet for {label}\")\n\n    return score * 1.0, \"; \".join(fb)\n"}, {"type": "code", "name": "GL Balance Tie-Out (April) from Summary", "description": "From the Prepaid Summary tab, locate the GL Reconciliation table and verify April 2025 balances for 1250 and 1251 match provided GL values within a reasonable tolerance.", "weight": 1.0, "code": "import re\nimport numpy as np\nimport pandas as pd\n\nMONTH_ALIASES = {\n    'jan': ['jan','january','1/2025','01/2025','1/31/2025','01/31/2025','jan-25','jan 2025'],\n    'feb': ['feb','february','2/2025','02/2025','2/28/2025','02/29/2025','feb-25','feb 2025'],\n    'mar': ['mar','march','3/2025','03/2025','3/31/2025','03/31/2025','mar-25','mar 2025'],\n    'apr': ['apr','april','4/2025','04/2025','4/30/2025','04/30/2025','apr-25','apr 2025']\n}\n\nAPR_TARGETS = {\n    '1250': 559377.61,\n    '1251': 369976.70\n}\n\nACCOUNT_ALIASES = {\n    '1250': ['1250','prepaid expenses','prepaid exp'],\n    '1251': ['1251','prepaid insurance']\n}\n\n\ndef find_sheet_name(sheet_names):\n    def norm(s):\n        return re.sub(r\"[^a-z0-9]\", \"\", str(s).lower())\n    norm_map = {norm(s): s for s in sheet_names}\n    for key in ['prepaidsummary','summary','prepaidssummary']:\n        if key in norm_map:\n            return norm_map[key]\n    # Fallback: the first sheet\n    return sheet_names[0] if sheet_names else None\n\n\ndef cell_str(x):\n    try:\n        return str(x).strip().lower()\n    except Exception:\n        return \"\"\n\n\ndef evaluate(workflow, context):\n    out = context.get_primary_output()\n    if not out or not out.is_spreadsheet:\n        return 0.0, \"No spreadsheet or wrong type.\"\n    try:\n        path = context.files.get_path(out.id)\n        xls = pd.ExcelFile(path)\n        sheet = find_sheet_name(xls.sheet_names)\n        if not sheet:\n            return 0.0, \"Could not locate summary sheet.\"\n        df = pd.read_excel(path, sheet_name=sheet, header=None)\n    except Exception as e:\n        return 0.0, f\"Failed to read summary sheet: {e}\"\n\n    # Build lowercase string view\n    sdf = df.fillna(\"\").astype(str).applymap(cell_str)\n\n    # Find a header row that contains at least two month aliases\n    header_row_idx = None\n    header_cols = {}\n    for r in range(sdf.shape[0]):\n        row_vals = sdf.iloc[r,:].tolist()\n        hit_months = {}\n        for c, val in enumerate(row_vals):\n            for m, aliases in MONTH_ALIASES.items():\n                if any(a in val for a in aliases):\n                    hit_months.setdefault(m, []).append(c)\n        if len(hit_months) >= 2:\n            header_row_idx = r\n            header_cols = {m: cols[0] for m, cols in hit_months.items()}\n            break\n\n    # If we didn't detect headers, try to infer an April column anywhere\n    apr_col = None\n    if header_row_idx is not None:\n        apr_col = header_cols.get('apr')\n    if apr_col is None:\n        # brute force search any cell containing an April marker; use that column index\n        for r in range(sdf.shape[0]):\n            for c in range(sdf.shape[1]):\n                if any(a in sdf.iat[r,c] for a in MONTH_ALIASES['apr']):\n                    apr_col = c\n                    break\n            if apr_col is not None:\n                break\n\n    if apr_col is None:\n        return 0.0, \"Could not identify April column in GL reconciliation area.\"\n\n    def find_account_rows(alias_list):\n        hits = []\n        for r in range(sdf.shape[0]):\n            row_text = \" \".join(sdf.iloc[r,:].tolist())\n            if any(a in row_text for a in alias_list):\n                hits.append(r)\n        return hits\n\n    score = 0.0\n    feedback = []\n    for acct, aliases in ACCOUNT_ALIASES.items():\n        rows = find_account_rows(aliases)\n        if not rows:\n            feedback.append(f\"No row found for account {acct}.\")\n            continue\n        # take the first plausible row\n        val = None\n        for r in rows:\n            try:\n                raw = df.iat[r, apr_col]\n                num = pd.to_numeric(raw, errors='coerce')\n                if pd.notna(num):\n                    val = float(num)\n                    break\n            except Exception:\n                continue\n        if val is None:\n            feedback.append(f\"No numeric April value found for account {acct}.\")\n            continue\n        target = APR_TARGETS[acct]\n        # Allow 0.5% relative tolerance or $250 absolute, whichever larger\n        tol = max(0.005 * abs(target), 250.0)\n        if abs(val - target) <= tol:\n            score += 0.5\n            feedback.append(f\"Account {acct} April ties within tolerance (found {val:.2f}, target {target:.2f}).\")\n        else:\n            feedback.append(f\"Account {acct} April off by {val - target:+.2f} (found {val:.2f}, target {target:.2f}).\")\n\n    return score * 1.0, \"; \".join(feedback)\n"}, {"type": "llm_judge", "name": "Amortization Logic Correctness (1250)", "description": "Verify the Prepaid Expenses schedule correctly applies amortization periods, monthly expense = original/term, rollforwards are consistent, and 6-month default is applied when unspecified (starting in invoice month).", "weight": 2.5, "judge_prompt": "Evaluate the Prepaid Expenses (1250) tab for logical correctness (not just structure):\n- For a sample of invoices, check that monthly expense equals original amount divided by the amortization term in months.\n- Where no amortization period is specified in source invoices, confirm a default of six months is applied starting in the month of the invoice.\n- Check that remaining balance by month reduces exactly by the monthly expense and reaches zero at the end of the amortization period.\n- Confirm vendor sorting/grouping is applied and monthly summary totals align with detail lines through April 2025.\n\nScoring guidance:\n- 2.5: All checks hold across sampled lines; rollforwards are consistent and monthly summaries tie to detail through April.\n- 1.5\u20132.0: Minor arithmetic or timing issues on a small number of lines; overall logic is sound and summaries mostly tie.\n- 0.5\u20131.0: Multiple inconsistencies in amortization or rollforwards, but general approach is present.\n- 0.0: Amortization mechanics largely incorrect or missing.", "expectation": "Mechanically accurate amortization with 6-month default and clean rollforward math through April 2025."}, {"type": "llm_judge", "name": "Insurance Policy Treatment (1251)", "description": "Verify the Prepaid Insurance schedule correctly reflects policy periods and monthly recognition for Good Insurance and BCBS.", "weight": 2.5, "judge_prompt": "Evaluate the Prepaid Insurance (1251) tab:\n- Good Insurance policy period 1/1/2025\u201312/31/2025 is amortized evenly across Jan\u2013Dec 2025; monthly expenses and remaining balances follow exactly.\n- BCBS coverage runs 2/1/2025\u20131/31/2026. First payment due 1/15/2025 to avoid lapse; billed and paid monthly. Verify the schedule reflects monthly recognition of coverage starting in February 2025, with any January payment treated as a prepayment for February coverage (i.e., expense begins with Feb coverage month). Ensure amortization spans Feb 2025\u2013Jan 2026 and is reflected through April 2025 appropriately.\n- Bottom summaries tie to detailed lines through April 2025.\n\nScoring guidance:\n- 2.5: Both policies correctly reflected; monthly amounts and timing are accurate; summaries tie.\n- 1.5\u20132.0: Minor timing or rounding issues that do not materially affect April balances.\n- 0.5\u20131.0: Notable discrepancies but general policy timing is recognizable.\n- 0.0: Misinterprets policy periods or recognition timing.", "expectation": "Accurate monthly recognition for Good Insurance and BCBS, correct timing with Feb start for BCBS coverage, and correct rollforward through April."}, {"type": "llm_judge", "name": "Cross-Tab Consistency and Summary Pulls", "description": "Confirm Prepaid Summary totals are pulled from detail tabs and monthly summaries reconcile through April for both accounts.", "weight": 2.5, "judge_prompt": "Check cross-sheet consistency:\n- Prepaid Summary totals (YTD additions, YTD amortization, ending balance as of 4/30/2025) agree with the aggregated figures from the respective detail tabs (1250 and 1251) through April.\n- The GL Reconciliation table\u2019s monthly balances (Jan\u2013Apr) reasonably match the rollforward implied by the detail tabs.\n- If there are rounding pennies, totals should still agree within trivial rounding.\n\nScoring guidance:\n- 2.5: Summary fully ties to detail; monthly balances are consistent through April.\n- 1.5\u20132.0: Minor rounding/tracing variances but clear linkage.\n- 0.5\u20131.0: Several mismatches; linkage is present but unreliable.\n- 0.0: No clear linkage; summary appears independent of detail.", "expectation": "Summary clearly and accurately aggregates from detail; tie-outs through April are evident."}, {"type": "llm_judge", "name": "Assumptions, Periods, and COA Application", "description": "Check that assumptions are documented, periods are explicit, and correct COA numbers are applied consistently across tabs and headers.", "weight": 2.5, "judge_prompt": "Evaluate whether:\n- Assumptions are documented (e.g., 6-month default when unspecified; monthly recognition convention; any proration rules) and are applied consistently.\n- Each invoice/policy clearly shows start/end months and amortization term.\n- Correct Chart of Accounts numbers appear where appropriate: 1250 for Prepaid Expenses and 1251 for Prepaid Insurance, on tabs and/or headers.\n\nScoring guidance:\n- 2.5: Assumptions clearly documented and consistently applied; periods explicit; COA labeling correct throughout.\n- 1.5\u20132.0: Minor omissions in documentation; labeling still clear.\n- 0.5\u20131.0: Documentation sparse and some inconsistencies.\n- 0.0: No assumptions noted; COA labeling incorrect or missing.", "expectation": "Clear assumptions and COA usage; explicit periods across invoices/policies."}], "on_failure_action": "skip_remaining", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Auditability", "description": "Holistic assessment of presentation, auditability, and usability for finance stakeholders.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation & Formatting", "description": "Assess formatting conventions for a finance-ready workbook.", "weight": 3.0, "judge_prompt": "Assess presentation quality:\n- Consistent currency and date formats; thousand separators; two decimals where appropriate.\n- Clear headings, freeze panes, filters, and vendor sorting/grouping.\n- Clean layout with readable column widths and no clipped text.\n- Appropriate use of tables or named ranges.\n\nScoring guidance: 3.0 excellent professional formatting; 2.0 good with minor issues; 1.0 passable but inconsistent; 0.0 poor/unprofessional.", "expectation": "Polished, consistent formatting suitable for executive finance review."}, {"type": "llm_judge", "name": "Auditability & Traceability", "description": "Evaluate how easily an auditor can trace numbers from summary to detail and to GL.", "weight": 3.0, "judge_prompt": "Assess auditability:\n- Transparent formulas or calculation notes; no hidden/merged cells that obscure logic.\n- Footnotes or methodology notes explaining recognition conventions and any proration.\n- Ability to trace a summary amount back to specific invoices/policies and monthly lines.\n- Minimal manual hard-keys; references link across tabs where appropriate.\n\nScoring guidance: 3.0 highly auditable; 2.0 generally auditable with minor gaps; 1.0 traceability requires effort; 0.0 opaque.", "expectation": "Clear calculation logic and easy trace-back from summary to source lines."}, {"type": "llm_judge", "name": "Usability & Clarity for Stakeholders", "description": "Assess navigability and clarity for Controllers/FP&A.", "weight": 2.0, "judge_prompt": "Evaluate usability:\n- Clear tab names and a brief TOC/notes on the Summary tab for navigation.\n- Key metrics highlighted (YTD adds, YTD amort, April ending balances).\n- Vendor-level totals and monthly totals are evident.\n\nScoring: 2.0 excellent; 1.0 decent; 0.0 poor.", "expectation": "Clear navigation and emphasis on decision-relevant metrics."}, {"type": "llm_judge", "name": "Reconciliation Narrative & Controls", "description": "Evaluate whether the workbook communicates reconciliation to GL and includes basic control cues.", "weight": 2.0, "judge_prompt": "Review for control-minded communication:\n- Brief reconciliation note explaining tie-out to GL balances Jan\u2013Apr, including any variances.\n- Sign-off or prepared-by/reviewed-by block with dates.\n- Version/date in header or footer for period specificity.\n\nScoring: 2.0 includes all; 1.0 partial; 0.0 none.", "expectation": "Clear reconciliation narrative and basic control sign-offs."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "f3351922-dbdd-45da-85c5-e7110696bbe5", "rubric": {"category_name": "Finance & Insurance \u2014 TSP Client Email Response (Document Pattern B)", "rationale": "This is a document task (professional email). The rubric enforces a strict, verifiable document shape first (Stage 1, LLM-only gate), then verifies correctness and coverage (Stage 2, mixed with lightweight code checks and heavier LLM checks), and finally evaluates overall communication quality and client-appropriateness (Stage 3, LLM). Stage 2 weights favor LLM judges (~5x over code) to assess nuanced factual accuracy and completeness, while code rules provide deterministic presence checks enabled by the mandated structure.", "max_total_score": 25.0, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate (LLM only)", "description": "Gate that enforces the exact deliverable shape for a professional email document suitable for verification.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Document Format and Structure Requirements (Email)", "description": "Validate that the output is a professionally formatted email document (PDF/DOCX) with the required sections and elements.", "weight": 4.0, "judge_prompt": "You are verifying only the FORMAT and STRUCTURE of the candidate output, not the content quality or factual accuracy.\n\nCheck these requirements by inspecting the provided file (rendered PDF/DOCX):\n\nAcceptable formats:\n- Must be a PDF or DOCX (not plain text, not Excel).\n\nEmail shape requirements:\n- Clear email format with:\n  1) A subject line that exactly matches: \"Comprehensive overview of TSP investment funds and benefits for transitioning service members\". Preferably prefixed by \"Subject:\" or clearly labeled as the email subject at the top.\n  2) Greeting (e.g., Dear [Name], Hello, etc.).\n  3) Two clearly labeled main sections with headers or bolded titles:\n     - Section A: Overview of TSP Investment Funds\n       \u2022 Must explicitly enumerate the G Fund, F Fund, C Fund, S Fund, I Fund, and L Funds.\n       \u2022 For each fund, provide at least 2 bullet points or short sub-paragraphs indicating objective/what it invests in and a basic note on risk/volatility.\n     - Section B: Benefits for Military Members Transitioning to Federal Civilian Service\n       \u2022 Must be presented as bullets or short sub-sections.\n  4) Closing that includes a courteous sign-off and a signature block (name and role/title at minimum; contact info preferred).\n\nProfessional presentation:\n- At least one full page or a clearly multi-section document (not just a few lines).\n- Legible headings/subheadings and list formatting (bullets or numbered lists used where appropriate).\n\nScoring (STRUCTURE ONLY):\n- 1.0: Valid PDF/DOCX email format + exact subject present + both main sections clearly labeled + all six funds listed with at least 2 bullets each + benefits section present in bullet/subsection form + closing/signature present.\n- 0.75: Valid format + exact subject present + both main sections present but one fund missing bullets or a minor sub-structure issue (e.g., one fund lacks 2 bullets) + closing/signature present.\n- 0.5: Valid format + exact subject present + at least one of the two main sections present and recognizable, but substantial structural elements missing (e.g., multiple funds missing, or benefits section not properly structured) + closing/signature may be partial.\n- 0.25: Valid format but email structure largely missing (e.g., missing subject line, missing most sections, or no clear list formatting).\n- 0.0: Not a PDF/DOCX, or completely wrong format/structure.\n\nReturn a score in [0,1] based on the above, focusing only on structure presence and format.", "expectation": "A professionally formatted PDF/DOCX email with exact subject, greeting, two labeled sections (fund overviews and transition benefits), bullet points for each fund, and a proper closing/signature."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Coverage Verification", "description": "Verify factual accuracy and coverage of TSP fund overviews and transition-related benefits. Mix deterministic code checks with LLM judgment; code weights are much lower than LLM.", "is_required": true, "max_points": 12.0, "min_score_to_pass": 6.0, "rules": [{"type": "code", "name": "Subject Line Presence and Match", "description": "Check that the exact required subject line is present, ideally as a labeled subject line.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n\n    Returns:\n        float (0-1) or (float, str)\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n\n    text = \"\"\n    # Try PDF then DOCX\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text:\n        return 0.0, \"Unable to extract text from document.\"\n\n    target = \"comprehensive overview of tsp investment funds and benefits for transitioning service members\".strip().lower()\n    t = text.lower()\n\n    # Prefer exact subject line match at/near top\n    lines = [ln.strip() for ln in t.splitlines() if ln.strip()]\n    subject_lines = [ln for ln in lines[:30] if ln.startswith(\"subject\") or ln.startswith(\"re:\") or ln.startswith(\"fw:\") or \"subject:\" in ln]\n\n    # Criteria\n    found_phrase_anywhere = target in t\n    found_subject_line_exact = False\n    for ln in subject_lines:\n        # Strip any 'subject:' prefix and whitespace\n        ln_norm = re.sub(r\"^subject\\s*:\\s*\", \"\", ln).strip()\n        if target == ln_norm.lower():\n            found_subject_line_exact = True\n            break\n\n    if found_subject_line_exact:\n        score = 1.0\n        fb = \"Exact subject line found at top.\"\n    elif found_phrase_anywhere:\n        score = 0.7\n        fb = \"Subject phrase found, but not as a clean labeled subject at top.\"\n    else:\n        score = 0.0\n        fb = \"Required subject phrase not found.\"\n\n    return score, fb"}, {"type": "code", "name": "All TSP Funds Mentioned", "description": "Verify presence of all six TSP fund names (G, F, C, S, I, and L Funds).", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text:\n        return 0.0, \"Unable to extract text from document.\"\n\n    t = text.lower()\n    # Accept common variants for L Funds terms\n    checks = {\n        'g fund': any(s in t for s in [' g fund', '\\\ng fund', 'g fund']),\n        'f fund': any(s in t for s in [' f fund', '\\\nf fund', 'f fund']),\n        'c fund': any(s in t for s in [' c fund', '\\\nc fund', 'c fund']),\n        's fund': any(s in t for s in [' s fund', '\\\ns fund', 's fund']),\n        'i fund': any(s in t for s in [' i fund', '\\\ni fund', 'i fund']),\n        'l funds': any(s in t for s in [' l fund', ' l funds', 'l fund', 'l funds', 'lifecycle fund', 'lifecycle funds'])\n    }\n\n    matched = sum(1 for k, v in checks.items() if v)\n    score = matched / 6.0\n    fb = f\"Funds mentioned: {matched}/6. Details: \" + \", \".join([f\"{k}:{'Y' if v else 'N'}\" for k, v in checks.items()])\n    return score, fb"}, {"type": "code", "name": "Key Transition Benefits Mentioned (Match + Consolidation)", "description": "Check for two critical elements: (1) agency matching up to 5% for civilian service, and (2) guidance on combining/transferring military (uniformed services) and civilian TSP accounts.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text:\n        return 0.0, \"Unable to extract text from document.\"\n\n    t = text.lower()\n\n    # 5% matching (look for 5 and % near match/matching)\n    five_match = False\n    for m in re.finditer(r\"5\\s*%\", t):\n        start = max(0, m.start()-80)\n        end = min(len(t), m.end()+80)\n        window = t[start:end]\n        if re.search(r\"\\bmatch(ing)?\\b|\\bagency\\b|\\bemployer\\b\", window):\n            five_match = True\n            break\n\n    # Account combination/transfer between military/uniformed and civilian\n    combo = False\n    # Look for verbs + TSP/account + (uniformed|military) and (civilian|federal)\n    if re.search(r\"(combine|consolidate|transfer|roll\\s*over).{0,80}\\b(tsp|account)s?\", t) and \\\n       (re.search(r\"uniformed|military\", t) and re.search(r\"civilian|federal\", t)):\n        combo = True\n\n    score = (1.0 if five_match else 0.0) * 0.5 + (1.0 if combo else 0.0) * 0.5\n    fb = f\"5% match mention: {'Yes' if five_match else 'No'}; Combine/transfer mention: {'Yes' if combo else 'No'}.\"\n    return score, fb"}, {"type": "llm_judge", "name": "Fund Descriptions \u2014 Accuracy and Completeness", "description": "Assess whether the descriptions for G, F, C, S, I, and L Funds are factually accurate and sufficiently cover objective/index/exposure and relative risk/volatility at a high level.", "weight": 3.5, "judge_prompt": "Evaluate the accuracy and completeness of fund descriptions for: G, F, C, S, I, and L Funds. Focus on whether each fund has a correct objective/what it invests in (e.g., index tracked or exposure) and a general risk/volatility note. Do not require exact phrasing, but be strict on factual alignment with official TSP guidance.\n\nGuidance examples (not exhaustive):\n- G Fund: U.S. Treasury securities, principal preservation, very low volatility.\n- F Fund: Broad U.S. investment-grade bond market (e.g., Bloomberg U.S. Aggregate), bond risks (rate/credit).\n- C Fund: U.S. large-cap stocks (e.g., S&P 500), equity market risk.\n- S Fund: U.S. mid/small caps excluding S&P 500 (e.g., completion index), higher volatility.\n- I Fund: International developed markets (e.g., MSCI EAFE), currency/foreign market risk.\n- L Funds: Target-date portfolios that mix underlying funds, automatically rebalanced and glide down risk over time.\n\nScoring:\n- 1.0: All six funds described accurately with objective/index exposure AND risk notes (minor omissions acceptable if overall accurate).\n- 0.7: Mostly accurate; 1-2 minor inaccuracies/omissions.\n- 0.4: Several gaps; 3+ omissions or a notable inaccuracy.\n- 0.0: Material inaccuracies or missing multiple funds\u2019 descriptions.\n\nReturn a score in [0,1].", "expectation": "High-fidelity alignment with official TSP fund definitions and risk characterizations."}, {"type": "llm_judge", "name": "Transition Benefits \u2014 Correctness and Breadth", "description": "Evaluate correctness and breadth of benefits and guidance for a military member transitioning to federal civilian service.", "weight": 3.5, "judge_prompt": "Assess whether the email accurately explains key TSP-related considerations for military-to-civilian transition, such as:\n- Ability to combine/consolidate uniformed services and civilian TSP accounts (and general how/where to initiate).\n- Employer/FERS agency automatic 1% and matching contributions up to 5% (if applicable) for civilian service.\n- Traditional vs. Roth contributions and tax considerations.\n- Catch-up contributions eligibility (age 50+), annual limits awareness.\n- Rollovers to/from other plans (high-level mention), and notes on loans/withdrawals and tax implications.\n- Reminders to update investment elections, contribution allocations, and beneficiaries.\n\nScoring:\n- 1.0: Accurate, covers most items above, no material errors.\n- 0.7: Mostly correct but missing 1-2 important items or light on practical guidance.\n- 0.4: Several gaps or vague/unclear descriptions.\n- 0.0: Material inaccuracies about benefits or eligibility.\n\nReturn a score in [0,1].", "expectation": "Accurate, action-guiding overview of the main TSP benefits and steps relevant to transition."}, {"type": "llm_judge", "name": "Official Sources and References", "description": "Check whether the email points the client to official TSP resources (e.g., tsp.gov pages) for further reading or confirmation.", "weight": 1.5, "judge_prompt": "Determine if the email provides at least one clear reference or link to official TSP resources (e.g., tsp.gov) relevant to fund descriptions or transition steps (combining accounts, employer match, contribution limits). Prefer direct links or explicit citations.\n\nScoring:\n- 1.0: Provides 2+ relevant official references/links.\n- 0.7: Provides 1 relevant official reference/link.\n- 0.3: Mentions official resources vaguely without a concrete link.\n- 0.0: No official resource references.\n\nReturn a score in [0,1].", "expectation": "Direct, relevant references to tsp.gov pages."}, {"type": "llm_judge", "name": "Cautions and Non-Advice Compliance", "description": "Ensure the email avoids prescriptive investment advice and includes appropriate cautionary language (e.g., risk, personal circumstances, suggest consulting official guidance or a fiduciary as appropriate).", "weight": 1.5, "judge_prompt": "Review the email for a professional, non-prescriptive tone that avoids personalized investment advice. Look for general risk disclosures and suggestions to consult official TSP guidance or qualified advisors for personal circumstances.\n\nScoring:\n- 1.0: Clear non-advisory tone with appropriate cautions/disclaimers.\n- 0.7: Generally compliant but could use stronger wording on limitations.\n- 0.3: Some advisory language with minimal caution.\n- 0.0: Overt investment advice without disclaimers.\n\nReturn a score in [0,1].", "expectation": "Neutral, compliant language with appropriate caveats and referrals to official resources."}], "on_failure_action": "skip_remaining", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Communication Quality and Client Fit", "description": "Holistic quality assessment of the email\u2019s clarity, tone, organization, and usefulness for this transitioning military client.", "is_required": false, "max_points": 9.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Tone, Professionalism, and Empathy", "description": "Evaluate whether the email demonstrates a courteous, supportive tone tailored to a long-tenured military member transitioning to civilian service.", "weight": 2.5, "judge_prompt": "Assess tone and professionalism. Consider empathy for the client\u2019s long military service and upcoming transition. Look for respectful, appreciative language and supportive framing.\n\nScoring:\n- 1.0: Warm, respectful, professional, and empathetic.\n- 0.7: Professional but somewhat impersonal.\n- 0.4: Neutral with limited rapport.\n- 0.0: Inappropriate or dismissive tone.\n\nReturn a score in [0,1].", "expectation": "Warm, respectful tone that acknowledges service and supports the transition."}, {"type": "llm_judge", "name": "Clarity and Organization", "description": "Evaluate clarity of explanations, logical structure, and scannability (headings, bullets, concise language).", "weight": 2.5, "judge_prompt": "Assess how clearly the email explains the funds and benefits, whether the structure makes it easy to scan (headings, bullets), and whether jargon is minimized or explained. Consider whether key takeaways are easy to find.\n\nScoring:\n- 1.0: Clear, well-structured, concise with excellent scannability.\n- 0.7: Mostly clear with minor organization issues.\n- 0.4: Somewhat cluttered or jargon-heavy.\n- 0.0: Disorganized and hard to follow.\n\nReturn a score in [0,1].", "expectation": "Clear, well-structured content with effective headings/bullets."}, {"type": "llm_judge", "name": "Personalization and Relevance to Scenario", "description": "Check for tailoring to a long-tenured military client entering a civilian federal role and nearing retirement.", "weight": 2.0, "judge_prompt": "Determine whether the email meaningfully addresses the client\u2019s context: long-tenured service, upcoming retirement, and shift to a civilian federal role. Look for tailored points (e.g., combining accounts, employer match implications, catch-up contributions if 50+, aligning L Funds with time horizon).\n\nScoring:\n- 1.0: Clearly tailored with context-aware guidance/examples.\n- 0.7: Some light tailoring.\n- 0.4: Mostly generic TSP info.\n- 0.0: No evident tailoring.\n\nReturn a score in [0,1].", "expectation": "Context-aware tailoring to the client\u2019s transition and retirement horizon."}, {"type": "llm_judge", "name": "Actionability and Next Steps", "description": "Assess whether the email provides concrete, practical next steps and a helpful close (e.g., how to combine accounts, where to update allocations, how to reach support).", "weight": 2.0, "judge_prompt": "Evaluate inclusion of clear next steps (e.g., links or directions on combining accounts, updating contributions/allocations, reviewing beneficiaries, where to find forms on tsp.gov, contacting support). Check that the closing invites questions and offers availability for follow-up.\n\nScoring:\n- 1.0: Concrete, stepwise actions and supportive close.\n- 0.7: Some actionable items provided.\n- 0.4: Vague or generic suggestions.\n- 0.0: No actionable guidance.\n\nReturn a score in [0,1].", "expectation": "Specific, practical actions plus an open invitation for follow-up."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "0ec25916-1b5c-4bfe-93d3-4e103d860f3a", "rubric": {"category_name": "SBAR ED Patient Transfer Handover Template", "rationale": "This rubric enforces a self-documenting, verifiable artifact for an ED SBAR handover guide. Stage 1 (LLM-only) strictly mandates the deliverable\u2019s shape: a 1-page PDF with a 2-column by 4-row SBAR table, required title, top identifiers, and critical prompts that were previously missed (allergies, arrival date/time, investigations). Stage 2 mixes light code checks (file type, key phrases) with LLM verification of content completeness and source-informed alignment. Stage 3 evaluates overall professional quality, usability at the point of care, and clarity for rapid ED workflows. Code rules are low-weight and robust (flexible regex/substring checks); LLM rules carry most of the scoring for nuanced clinical review.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate (LLM-only)", "description": "Gate: Verify the candidate produces a 1-page PDF in the exact required structure to enable verification. No calculation or quality judgment here\u2014only presence and structure.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.5, "rules": [{"type": "llm_judge", "name": "Structured 1-page PDF SBAR Template Presence", "description": "Check the document format and exact structural requirements for the ED SBAR handover guide.", "weight": 6.0, "judge_prompt": "You are evaluating whether the submission satisfies strict STRUCTURE requirements for an SBAR handover template. Do NOT judge writing quality or correctness\u2014only presence, format, and layout.\n\nRequired format and structure:\n- File format: PDF only.\n- Length: Exactly 1 page suitable for printing in ED.\n- Title at top: \u201cSBAR Template Emergency Department\u201d (minor punctuation/case variations acceptable, but both SBAR Template and Emergency Department must be clearly present).\n- A single table for the template with two columns and four rows (2x4). A small header row is allowed, but there must be exactly 4 main rows dedicated to the four SBAR blocks.\n- Left column: the four SBAR building blocks as row labels: Situation, Background, Assessment, Recommendation(s). Flexible with pluralization (Recommendation/Recommendations) and capitalization.\n- For each of the four SBAR rows, the left column MUST include at least two brief guiding points (bullets, short phrases, or sub-points) indicating what to cover in that block.\n- Right column: practical prompts for what to capture, with blank, lined spaces underneath prompts for handwriting by the nurse. The blank space must be visually apparent (e.g., underlines, dotted lines, or space clearly intended for writing).\n- Top identifiers area (above the table or clearly at top of page): labeled spaces prompting the nurse to state their Name and Department at the beginning of the call; also include a space to capture the Receiving clinician\u2019s name (who took the handover). Lines or clearly writable space must be present.\n- Critical prompts included somewhere in the right column (where appropriate): Allergies, Arrival date/time, and Nursing investigations (or equivalent phrasing like investigations/tests performed/ED workup).\n\nScoring (0 to 6):\n- 6.0: PDF, exactly 1 page; clear 2-column x 4-row SBAR table; correct SBAR row labels; at least two guiding points per SBAR row; right column shows prompts with visible writable lines/spaces; top identifiers (Nurse Name, Department, Receiving clinician) present; and the three critical prompts (Allergies, Arrival date/time, Investigations) are clearly included.\n- 5.0: Minor deviations (e.g., slight title variation; small formatting differences) but all essential structural elements are present (PDF 1 page; 2x4 SBAR table; two guiding points per row; right-column prompts plus writable space; top identifiers present; all three critical prompts present or one phrased slightly differently but still clearly covered).\n- 4.0: Mostly correct but missing 1\u20132 key elements (e.g., one SBAR row lacks guiding points; or one critical prompt missing; or top identifiers missing one required item). Still must be PDF and clearly a 2-column SBAR table on 1 page.\n- 2.0: Major structural issues (e.g., not clearly 2 columns x 4 SBAR rows; multiple pages; unclear writable space) but still an SBAR-like table.\n- 0.0: Not a PDF; not a table; or not an SBAR handover template.\n\nReturn a score from 0 to 6 only, based solely on the above structural criteria.", "expectation": "A 1-page PDF with the exact SBAR table structure, required title, top identifiers, and critical prompts present."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Content Presence and Consistency)", "description": "Now that structure is enforced, verify the presence and plausibility of key content and prompts using a mix of targeted code checks and LLM evaluation.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "PDF and Title Presence (light check)", "description": "Checks that output is a PDF and contains the expected title text elements.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output:\n            return 0.0, \"No output\"\n        # Verify PDF by file extension\n        path = context.files.get_path(output.id)\n        is_pdf = str(path).lower().endswith('.pdf')\n        if not is_pdf:\n            return 0.0, \"Output is not a PDF\"\n        # Read text from PDF\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = \"\"\n        tl = (text or \"\").lower()\n        # Look for title components\n        has_sbar = 'sbar' in tl\n        has_template = 'template' in tl\n        has_phrase = 'sbar template' in tl or (has_sbar and has_template)\n        has_ed = 'emergency department' in tl or 'ed ' in tl or tl.strip().startswith('ed')\n        score = 0.0\n        # Allocate partial credit; max 1.0 relative scale\n        if is_pdf:\n            score += 0.34\n        if has_phrase:\n            score += 0.33\n        if has_ed:\n            score += 0.33\n        score = min(score, 1.0)\n        return score, f\"PDF={is_pdf}, title_phrase={has_phrase}, ED_term={has_ed}\"\n    except Exception as e:\n        return 0.0, f\"Exception: {e}\""}, {"type": "code", "name": "Critical Prompts Present (Allergies, Arrival Time/Date, Investigations)", "description": "Detects presence of critical prompts that were historically omitted.", "weight": 0.7, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output:\n            return 0.0, \"No output\"\n        text = \"\"\n        # Try PDF then DOCX then text\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_text(output.id)\n                except Exception:\n                    text = \"\"\n        tl = (text or \"\").lower()\n        found = 0\n        # Allergies\n        if re.search(r'allerg', tl):\n            found += 1\n        # Arrival date/time (various phrasings)\n        if re.search(r'(arrival\\s*(date|time))|((time|date)\\s*of\\s*arrival)|arrived\\s*(at|on)|\\beta\\b', tl):\n            found += 1\n        # Investigations / tests / workup\n        if re.search(r'investigat|workup|tests?\\b|labs?\\b|imaging\\b', tl):\n            found += 1\n        score = found / 3.0\n        return score, f\"Critical prompts found: {found}/3\"\n    except Exception as e:\n        return 0.0, f\"Exception: {e}\""}, {"type": "code", "name": "SBAR Headings Presence", "description": "Checks that all four SBAR headings appear.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output:\n            return 0.0, \"No output\"\n        text = \"\"\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_text(output.id)\n                except Exception:\n                    text = \"\"\n        tl = (text or \"\").lower()\n        items = {\n            'situation': bool(re.search(r'\\bsituation\\b', tl)),\n            'background': bool(re.search(r'\\bbackground\\b', tl)),\n            'assessment': bool(re.search(r'\\bassessment\\b', tl)),\n            'recommendation': bool(re.search(r'\\brecommendation(s)?\\b', tl)),\n        }\n        count = sum(1 for v in items.values() if v)\n        score = count / 4.0\n        return score, f\"SBAR headings present: {count}/4\"\n    except Exception as e:\n        return 0.0, f\"Exception: {e}\""}, {"type": "code", "name": "Writable Line Placeholders Detected", "description": "Checks for visual placeholders (e.g., underlines/dots) suggesting space to write under prompts.", "weight": 0.2, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output:\n            return 0.0, \"No output\"\n        text = \"\"\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_text(output.id)\n                except Exception:\n                    text = \"\"\n        tl = (text or \"\")\n        has_lines = bool(re.search(r'(_{4,}|\\.{4,}|-{4,})', tl))\n        return (1.0 if has_lines else 0.0), f\"Line placeholders: {has_lines}\"\n    except Exception as e:\n        return 0.0, f\"Exception: {e}\""}, {"type": "llm_judge", "name": "SBAR Section Prompt Completeness and ED Relevance", "description": "Judge whether each SBAR row includes at least two guiding points and whether the right-column prompts are ED-relevant and comprehensive.", "weight": 3.0, "judge_prompt": "Evaluate the content (not just structure) of the SBAR table:\n- For each of Situation, Background, Assessment, Recommendation(s):\n  - Left column: at least two brief guiding points that clearly say what to cover in that block.\n  - Right column: ED-relevant prompts that would help a nurse prepare a safe, complete handover. Look for items like: identifiers, presenting complaint, triage category, allergies, isolation status, vital signs and scores (e.g., NEWS2/MEWS), pain score, relevant history/medications, assessment findings, treatments done, IV access/fluids, oxygen/airway, investigations/labs/imaging, pending results, arrival mode and time, current status, escalation needs, disposition/bed type requested, outstanding tasks, safety concerns.\n- The three historically omitted details (Allergies, Arrival date/time, Investigations) should be included in appropriate sections.\n\nScoring (0 to 3):\n- 3.0: All four SBAR rows have 2+ clear guiding points; right-column prompts are practical and ED-appropriate; all three critical details included.\n- 2.0: Minor gaps (e.g., one section with just one guiding point or one ED prompt area thin) but overall fit for purpose and all three critical details included.\n- 1.0: Multiple gaps (e.g., several sections lack guiding points or prompts are generic), or one critical detail missing.\n- 0.0: Sparse/unclear prompts; several sections missing guiding points; multiple critical details missing.", "expectation": "Each SBAR row is actionable with multiple guiding points and ED-appropriate prompts covering critical safety items."}, {"type": "llm_judge", "name": "Writable Spaces and Top Identifiers", "description": "Judge whether writable spaces and top identifiers are clearly provided and usable.", "weight": 2.0, "judge_prompt": "Check the document for clear writable spaces:\n- Top identifiers: labeled places to capture Nurse Name and Department at the start of the call AND the Receiving clinician\u2019s name. There should be an obvious place (line/box/space) to write each.\n- Under each right-column prompt: visible, sufficient lined/dotted/blank space for handwriting.\n- Layout supports quick pre-call preparation and in-call reference.\n\nScoring (0 to 2):\n- 2.0: All top identifiers present with writable lines/boxes; each prompt area has obvious, sufficient handwriting space; layout is practical.\n- 1.0: Mostly present but one area light (e.g., limited space under one section or one identifier missing a line/box).\n- 0.0: Writable spaces largely absent or unusable.", "expectation": "Clearly labeled, sufficient writing space throughout plus top identifiers for Nurse and Receiving clinician."}, {"type": "llm_judge", "name": "Source-Informed Alignment with SBAR Best Practice", "description": "Check whether the template reflects recognized SBAR guidance; optionally note if references are cited or acknowledged.", "weight": 1.0, "judge_prompt": "Evaluate whether the content aligns with SBAR best practices as reflected in the provided resources (e.g., NHS Education for Scotland SBAR template, peer-reviewed SBAR handover literature). You are NOT checking for formal citation quality, but whether the structure and prompts appear source-informed.\n\nSignals of alignment: logical SBAR flow; concise guiding points; prompts that mirror SBAR standards (situation/ID, background/history/meds/allergies, assessment/vitals/findings, recommendation/plan/escalation). If references are listed in a footer or small print, acknowledge positively, but do not require it.\n\nScoring (0 to 1):\n- 1.0: Clearly consistent with SBAR guidance; optional references or footnotes are a plus.\n- 0.5: Generally consistent but with some mismatches or omissions.\n- 0.0: Poor alignment with SBAR standards.", "expectation": "The template content reflects recognized SBAR structure and prompts as per credible sources."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Usability Assessment", "description": "Holistic quality of the artifact for professional ED use: readability, usability, safety emphasis, and print-ready formatting.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Layout and Printability", "description": "Assess overall layout for print-ready, professional use at ED phones.", "weight": 2.0, "judge_prompt": "Assess visual professionalism for a 1-page printable tool: clean title, clear table borders, adequate margins, consistent fonts, good contrast, no clutter, and logical spacing. It should look like a professional reference sheet that can be taped near phones.\n\nScoring (0 to 2):\n- 2.0: Highly professional, balanced spacing, clear table/grid, easy to print and read at a glance.\n- 1.0: Acceptable but with minor layout issues (crowding, inconsistent spacing, weak contrast).\n- 0.0: Poor layout; difficult to read or print.", "expectation": "A clean, professional, print-ready 1-page reference sheet."}, {"type": "llm_judge", "name": "Clarity and Brevity of Guiding Points", "description": "Guidance text should be concise, directive, and quickly scannable in ED context.", "weight": 1.5, "judge_prompt": "Evaluate the clarity/conciseness of guiding points: brief phrases, action-oriented, minimal jargon, high signal-to-noise, and easy scanning during a phone call.\n\nScoring (0 to 1.5):\n- 1.5: Very clear, succinct, and directive; high readability under time pressure.\n- 1.0: Generally clear with mild verbosity or occasional jargon.\n- 0.0\u20130.5: Wordy/ambiguous; hard to scan quickly.", "expectation": "Short, directive guidance suitable for rapid ED handovers."}, {"type": "llm_judge", "name": "ED-Specific Utility and Safety Emphasis", "description": "Judge inclusion and emphasis of safety-critical and ED-relevant details.", "weight": 1.5, "judge_prompt": "Evaluate whether the template highlights safety-critical ED items and practical workflow elements: allergies, isolation, code status/ceilings of care, vital signs and early warning scores, pain/analgesia, airway/oxygen/IV access, investigations and pending results, escalation needs, disposition/bed requirements, and outstanding tasks.\n\nScoring (0 to 1.5):\n- 1.5: Strong emphasis and placement for safety-critical items and ED workflow.\n- 1.0: Mostly covered with minor gaps.\n- 0.0\u20130.5: Weak or missing ED-specific safety cues.", "expectation": "Prompts surface key safety and workflow elements prominently."}, {"type": "llm_judge", "name": "Accessibility and Usability", "description": "Evaluate legibility and ease-of-use for diverse users.", "weight": 1.0, "judge_prompt": "Consider font size/legibility, plain language, minimal unexplained abbreviations, clear headings, and intuitive flow. The template should be accessible and immediately usable by ED nurses with varying experience.\n\nScoring (0 to 1):\n- 1.0: Highly accessible and usable.\n- 0.5: Generally fine with minor accessibility issues.\n- 0.0: Poor accessibility/usability.", "expectation": "Legible, intuitive, and accessible to a broad nursing audience."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "40a99a31-42d6-4f23-b3ec-8f591afe25b6", "rubric": {"category_name": "Manufacturing | Industrial Engineers \u2014 Robotic CNC Cell Safety & Integration", "rationale": "This rubric enforces a self-documenting, multi-artifact delivery: an Excel for structured hardware selection and IO/protocol mapping, a PNG diagram for physical layout and safety coverage, and a PDF report for rationale and integration strategy. Stage 1 (LLM-only) strictly gates structure and formats. Stage 2 mixes light code checks (counts, protocol coverage, cost sanity) with LLM verification for cross-file consistency, safety standards, and integration plausibility. Stage 3 assesses professional quality, clarity, and strategic value. Code rules are intentionally lower-weight than LLM rules to reflect nuanced domain judgments made by the LLM when the mandated shape enables verification.", "max_total_score": 27.0, "stages": [{"name": "Stage 1 \u2014 Shape & Deliverable Gate", "description": "Gate: Verify presence and structure of all required deliverables with mandated sections and tables. LLM-only per guidance.", "is_required": true, "max_points": 2.0, "min_score_to_pass": 1.5, "rules": [{"type": "llm_judge", "name": "Excel Hardware Selection Structure Gate", "description": "Check the Excel deliverable exists and contains a clearly labeled hardware selection table enabling verification.", "weight": 1.0, "judge_prompt": "You are validating SHAPE ONLY. Examine the candidate outputs and confirm there is at least one Excel spreadsheet (.xlsx or .csv opened as a sheet image) containing a hardware selection table with the following structure:\n\nRequired Sheet: Preferably named \"Hardware Selection\" (flexible names accepted like \"BOM\", \"Hardware List\").\nRequired Columns (flexible naming, but meaning must be clear and visible as column headers):\n- Category/Type (e.g., LIDAR, Camera, AMR, Pressure Mat)\n- Make/Model\n- Interface/Protocol (should mention Ethernet/IP, Modbus TCP, IO-Link as applicable)\n- Compatibility Notes\n- Estimated Cost (unit cost)\n- Quantity (qty)\nOptional but helpful columns (do not penalize if missing): Safety Standard, Total Cost\n\nRequired Rows by Category (at least one row for each): LIDAR, Camera, AMR, Pressure Mat\n\nScoring:\n- 1.0: Excel present AND table includes all 6 required columns (or clear equivalents) AND all 4 required categories appear as rows.\n- 0.7: Excel present, 4\u20135 required columns present, and all 4 required categories present.\n- 0.4: Excel present but only 2\u20133 required columns OR missing one of the required categories.\n- 0.0: No Excel OR not a recognizable hardware selection table.\n\nOnly assess presence/structure, not correctness of content.", "expectation": "A clean, filterable hardware table with columns for category, model, protocols, notes, unit cost, quantity; rows covering LIDAR, camera, AMR, and pressure mats."}, {"type": "llm_judge", "name": "Diagram + PDF Structure Gate", "description": "Check the PNG diagram and PDF report exist with required labeled content and sections.", "weight": 1.0, "judge_prompt": "You are validating SHAPE ONLY for the diagram and report.\n\nDiagram (PNG):\n- Must be a PNG image illustrating the robot rail with six CNC machines.\n- Must label six static LIDAR zones (five between CNCs and one at the West/front), plus one robot-mounted LIDAR; the South/East rail sides are screened/barriered.\n- Indicate pressure mats in front of each mill and camera positions (at least six cameras total). A simple legend or direct labels are acceptable.\n\nReport (PDF):\n- Must be a PDF (not DOCX), professionally formatted, minimum 2 pages.\n- Must include the following section headers (flexible naming allowed): Overview, Hardware Selection Summary, Integration Strategy, Installation & Layout, Conclusion.\n\nScoring:\n- 1.0: PNG diagram present with clearly labeled rail, six CNCs, six static LIDAR zones + robot LIDAR, cameras, and mats; PDF present and includes all 5 sections.\n- 0.7: Diagram present and mostly labeled but one element missing or unclear (e.g., zones unlabeled) AND PDF has at least 4 of the 5 sections.\n- 0.4: Diagram present but sparse/unclear (multiple unlabeled elements) OR PDF with only 2\u20133 sections.\n- 0.0: Missing diagram or PDF, or PDF not in PDF format.\n\nDo not assess technical correctness\u2014only presence and labeled structure.", "expectation": "A simple, labeled PNG layout, and a 2+ page PDF with the five named sections."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness & Consistency)", "description": "Now that structure is guaranteed, verify counts, protocol interoperability, costs, safety references, cross-file consistency, and integration logic.", "is_required": false, "max_points": 15.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Device Counts by Category", "description": "From the Excel, verify minimum quantities: LIDAR \u2265 7 (six static + one robot-mounted), Cameras \u2265 6, Pressure Mats \u2265 6, AMR \u2265 1. Uses Quantity if present, else row counts by category.", "weight": 0.8, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs() or []\n        excel_res = None\n        for r in outputs:\n            if getattr(r, 'is_spreadsheet', False):\n                excel_res = r\n                break\n        if not excel_res:\n            return 0.0, 'No spreadsheet found.'\n\n        # Try to read the Hardware Selection sheet first, otherwise first sheet\n        df = None\n        try:\n            df = context.files.read_excel(excel_res.id, sheet_name='Hardware Selection')\n        except Exception:\n            try:\n                # Fallback: first visible sheet\n                path = context.files.get_path(excel_res.id)\n                xls = pd.ExcelFile(path)\n                sheet = xls.sheet_names[0]\n                df = pd.read_excel(path, sheet_name=sheet)\n            except Exception as e:\n                return 0.0, f'Unable to read Excel: {e}'\n        if df is None or df.empty:\n            return 0.0, 'Spreadsheet is empty.'\n\n        # Helper to pick columns by fuzzy match\n        def pick_col(possible):\n            cols = list(df.columns)\n            for c in cols:\n                lc = str(c).strip().lower()\n                for p in possible:\n                    if p in lc:\n                        return c\n            return None\n\n        cat_col = pick_col(['category', 'type', 'hardware'])\n        qty_col = pick_col(['qty', 'quantity', 'count'])\n\n        if cat_col is None:\n            return 0.0, 'No category/type column found.'\n\n        # Normalize category values\n        cats = df[cat_col].astype(str).str.lower()\n        def count_required(label_keywords):\n            mask = pd.Series(False, index=df.index)\n            for kw in label_keywords:\n                mask = mask | cats.str.contains(kw, na=False)\n            if qty_col and qty_col in df.columns:\n                q = pd.to_numeric(df[qty_col], errors='coerce').fillna(1)\n                return float(q[mask].sum())\n            else:\n                return float(mask.sum())\n\n        # Category matching\n        lidar_qty = count_required(['lidar', 'scanner', 'safety laser'])\n        cam_qty = count_required(['camera', 'vision'])\n        mat_qty = count_required(['mat'])\n        amr_qty = count_required(['amr', 'autonomous mobile robot', 'mobile robot'])\n\n        score = 0.0\n        feedback = []\n\n        # Each check worth 0.2 (total 0.8)\n        if lidar_qty >= 7: score += 0.2\n        else: feedback.append(f'LIDAR qty insufficient: {lidar_qty} < 7')\n        if cam_qty >= 6: score += 0.2\n        else: feedback.append(f'Camera qty insufficient: {cam_qty} < 6')\n        if mat_qty >= 6: score += 0.2\n        else: feedback.append(f'Pressure mat qty insufficient: {mat_qty} < 6')\n        if amr_qty >= 1: score += 0.2\n        else: feedback.append('AMR qty insufficient: 0 < 1')\n\n        return score, '; '.join(feedback) if feedback else 'All minimum device counts satisfied.'\n    except Exception as e:\n        return 0.0, f'Rule error: {e}'"}, {"type": "code", "name": "Protocol Coverage in Excel", "description": "Ensure most devices specify at least one interoperable protocol (Ethernet/IP, Modbus TCP, IO-Link) in the Interface/Protocol column.", "weight": 0.8, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs() or []\n        excel_res = None\n        for r in outputs:\n            if getattr(r, 'is_spreadsheet', False):\n                excel_res = r\n                break\n        if not excel_res:\n            return 0.0, 'No spreadsheet found.'\n\n        # Read sheet\n        try:\n            df = context.files.read_excel(excel_res.id, sheet_name='Hardware Selection')\n        except Exception:\n            try:\n                path = context.files.get_path(excel_res.id)\n                xls = pd.ExcelFile(path)\n                df = pd.read_excel(path, sheet_name=xls.sheet_names[0])\n            except Exception as e:\n                return 0.0, f'Unable to read Excel: {e}'\n        if df is None or df.empty:\n            return 0.0, 'Spreadsheet is empty.'\n\n        # Pick protocol column\n        def pick_col(possible):\n            for c in df.columns:\n                lc = str(c).strip().lower()\n                for p in possible:\n                    if p in lc:\n                        return c\n            return None\n        proto_col = pick_col(['protocol', 'interface'])\n        if proto_col is None:\n            return 0.0, 'No protocol/interface column found.'\n\n        protos = df[proto_col].astype(str).str.lower()\n        keywords = ['ethernet/ip', 'modbus tcp', 'io-link', 'iolink']\n        def has_supported(s):\n            return any(k in s for k in keywords)\n        if len(protos) == 0:\n            return 0.0, 'No protocol values.'\n\n        coverage = protos.apply(has_supported).mean()  # fraction\n        # Score proportionally, target \u2265 0.8 for full credit\n        score = min(coverage / 0.8, 1.0) * 0.8\n        return score, f'Protocol coverage: {coverage:.2%}'\n    except Exception as e:\n        return 0.0, f'Rule error: {e}'"}, {"type": "code", "name": "Cost Presence and Sanity", "description": "Check unit costs are present and non-negative for most items; compute total; compare to any provided total cost column; basic plausibility checks.", "weight": 0.9, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs() or []\n        excel_res = None\n        for r in outputs:\n            if getattr(r, 'is_spreadsheet', False):\n                excel_res = r\n                break\n        if not excel_res:\n            return 0.0, 'No spreadsheet found.'\n\n        # Read sheet\n        try:\n            df = context.files.read_excel(excel_res.id, sheet_name='Hardware Selection')\n        except Exception:\n            try:\n                path = context.files.get_path(excel_res.id)\n                xls = pd.ExcelFile(path)\n                df = pd.read_excel(path, sheet_name=xls.sheet_names[0])\n            except Exception as e:\n                return 0.0, f'Unable to read Excel: {e}'\n        if df is None or df.empty:\n            return 0.0, 'Spreadsheet is empty.'\n\n        def pick_col(possible):\n            for c in df.columns:\n                lc = str(c).strip().lower()\n                for p in possible:\n                    if p in lc:\n                        return c\n            return None\n\n        cost_col = pick_col(['estimated cost', 'unit cost', 'price', 'cost'])\n        qty_col = pick_col(['qty', 'quantity', 'count'])\n        total_col = pick_col(['total cost', 'extended cost', 'line total'])\n        if cost_col is None:\n            return 0.0, 'No unit cost column found.'\n\n        # Clean numerics\n        def to_num(series):\n            return pd.to_numeric(series.astype(str).str.replace('[^0-9.\\-]', '', regex=True), errors='coerce')\n\n        unit_cost = to_num(df[cost_col])\n        qty = to_num(df[qty_col]) if qty_col else pd.Series(1, index=df.index, dtype=float)\n        qty = qty.fillna(1)\n\n        # Scoring components (0.3 + 0.3 + 0.3 = 0.9)\n        score = 0.0\n        fb = []\n\n        # Presence and non-negative for >=80% rows\n        valid = unit_cost.dropna()\n        coverage = (unit_cost.notna()).mean() if len(unit_cost) else 0.0\n        nonneg = (unit_cost.dropna() >= 0).mean() if len(valid) else 0.0\n        if coverage >= 0.8 and nonneg == 1.0:\n            score += 0.3\n        else:\n            fb.append(f'Cost coverage {coverage:.0%}, nonnegativity {(nonneg*100):.0f}%')\n\n        # Compute total and basic plausibility\n        computed_total = float((unit_cost.fillna(0) * qty).sum())\n        if computed_total > 0 and computed_total < 1_000_000:\n            score += 0.3\n        else:\n            fb.append(f'Computed total not plausible: {computed_total:.2f}')\n\n        # Compare to provided total column if present\n        if total_col:\n            line_total = to_num(df[total_col])\n            provided_total = float(line_total.fillna(0).sum())\n            if provided_total == 0:\n                # Allow if no populated totals\n                score += 0.15\n            else:\n                # within 10% or $5k\n                if abs(provided_total - computed_total) <= max(0.1 * computed_total, 5000):\n                    score += 0.3\n                else:\n                    fb.append(f'Total mismatch: provided {provided_total:.2f} vs computed {computed_total:.2f}')\n        else:\n            # No total column; partial credit\n            score += 0.15\n\n        return score, '; '.join(fb) if fb else 'Costs present and plausible; totals consistent.'\n    except Exception as e:\n        return 0.0, f'Rule error: {e}'"}, {"type": "llm_judge", "name": "Cross-File Consistency: Models, Counts, and Layout", "description": "Verify the PDF references the same device categories and representative models as the Excel; diagram reflects six CNCs, six static LIDAR zones + robot LIDAR, mats, and cameras; counts align across files.", "weight": 3.1, "judge_prompt": "Evaluate cross-file consistency:\n- Excel vs PDF: Do the device categories (LIDAR, Cameras, AMR, Pressure Mats) appear in the PDF Hardware Selection Summary with models consistent with Excel? Exact strings aren\u2019t required, but the make/model families should be recognizable.\n- Counts: Do the quantities implied/described in the PDF (e.g., six static LIDARs + one robot-mounted; six cameras; six mats; one AMR) align with the Excel quantities? Minor wording differences are okay.\n- Diagram: Does the PNG locate six CNCs and depict six static zones and one robot-mounted LIDAR, with cameras and mats positioned? Do labels/legend align with the described layout in the PDF?\n\nScoring guidance:\n- 3.1: Strong alignment across all three artifacts (categories, model families, and counts) and the diagram matches the described layout.\n- 2.2: Mostly aligned with minor omissions (e.g., camera positions not all labeled) but clearly consistent overall.\n- 1.2: Partial alignment; multiple inconsistencies but core intent evident.\n- 0.0: Major inconsistencies or missing alignment between files.", "expectation": "All three artifacts reference and depict the same devices and counts with coherent labeling/layout."}, {"type": "llm_judge", "name": "Safety Standards Referencing for Selected Hardware", "description": "Check whether the PDF explicitly references relevant industrial safety standards for each safety-relevant device (e.g., LIDAR and mats) and suitability for guarding static zones.", "weight": 3.1, "judge_prompt": "Assess whether the report (PDF) cites relevant safety standards and demonstrates appropriateness:\n- LIDAR: Mentions standards such as IEC 61496 (electro-sensitive protective equipment), ISO 13849-1 (PL ratings), IEC 61508/IEC 62061 (SIL), and suitability of selected LIDAR type (e.g., Type 3 ESPE) for zone protection.\n- Pressure mats: References standards like ISO 13856 and integration considerations (reset/EDM, muting/interlocking with robot cell safety).\n- General robot cell: Mentions ISO 10218 and/or RIA/ANSI standards where appropriate.\n- The rationale connects standards to the chosen devices (not just name-dropping).\n\nScoring:\n- 3.1: Clear, correct standards cited for LIDAR and mats with rationale tied to use-case.\n- 2.2: Standards cited but shallow or missing linkage for one device class.\n- 1.2: Minimal or generic compliance mentions without specificity.\n- 0.0: No meaningful safety standard references.", "expectation": "Accurate, relevant safety standards connected to device suitability and integration."}, {"type": "llm_judge", "name": "Integration Strategy Plausibility and Software Separation", "description": "Judge whether the proposed integration uses industrial protocols properly, keeps the robot\u2019s proprietary control software independent, and minimizes rewiring via IO mapping.", "weight": 3.1, "judge_prompt": "Evaluate the Integration Strategy section of the PDF:\n- Does it propose a realistic architecture to connect devices via Ethernet/IP, Modbus TCP, and/or IO-Link, considering the robot IO layer programmable in Python/C+? (Gateways/IO masters acceptable.)\n- Are PLC/IO master or gateway choices described to avoid deep changes to the robot controller, maintaining software independence and clean boundaries?\n- Is minimal physical rewiring addressed (e.g., leveraging unused IO in control cabinet, structured IO mapping, use of IO-Link hubs, and compatible safety relays/safety controller)?\n- Are trigger signals for event-based camera capture or live feed integration described (e.g., edge-triggered IO or message-based over Ethernet/IP)?\n\nScoring:\n- 3.1: Highly plausible architecture with clear boundaries, realistic stack, and minimal rewiring approach.\n- 2.2: Plausible but missing one key detail (e.g., triggers or gateway selection).\n- 1.2: Vague or overly generic; unclear separation.\n- 0.0: Unworkable or no integration plan.", "expectation": "A clean, modular architecture using standard protocols, IO masters/gateways, and explicit triggers/mapping."}, {"type": "llm_judge", "name": "IO Mapping and Compatibility Logic", "description": "Evaluate whether IO mapping is described clearly and aligns with compatibility notes from the Excel.", "weight": 3.2, "judge_prompt": "Check the PDF for an IO mapping narrative aligned with the Excel Compatibility Notes:\n- Are device signals enumerated (e.g., LIDAR zone safe/unsafe bits, mat presence bit, camera trigger/ready, AMR mission signals) with direction (input/output) and protocol path?\n- Is there a clear mapping strategy (addresses/tags) that could be implemented on PLC/IO master or robot IO layer without tight coupling?\n- Do the described mappings match compatibility notes in Excel (e.g., protocol availability, power requirements, and safety I/O routing to a safety controller)?\n\nScoring:\n- 3.2: Clear mapping across all device classes, aligned with Excel notes.\n- 2.2: Mostly clear; minor gaps or one device class under-specified.\n- 1.2: Fragmentary mapping; limited alignment with Excel.\n- 0.0: No meaningful mapping provided.", "expectation": "Traceable IO mapping that a controls engineer could implement with minimal iteration."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality & Professionalism", "description": "Holistic assessment of presentation quality, clarity, modularity emphasis, and practicality for stakeholders.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Clarity", "description": "Assess the overall professionalism of all artifacts: readability, labeling, organization, and visual clarity.", "weight": 2.5, "judge_prompt": "Evaluate professionalism across the PDF, Excel, and PNG:\n- PDF: Clear writing, logical flow, clean formatting, readable figures.\n- Excel: Clean headers, consistent units, filterable/sortable; no obvious typos.\n- PNG: Simple, legible labels and legend; uncluttered diagram.\nScore 0\u20132.5 based on overall professional quality and clarity.", "expectation": "Stakeholder-ready artifacts with clean formatting and legible visuals."}, {"type": "llm_judge", "name": "Modularity and Future-Proofing", "description": "Judge how well the solution emphasizes modularity, software independence, and expandability.", "weight": 2.5, "judge_prompt": "Assess whether the proposal:\n- Demonstrates modular hardware selection (swappable components, standard interfaces).\n- Preserves software independence via protocol boundaries and gateways.\n- Notes future expansion (additional CNCs/cameras/zones) and how the design scales.\nScore 0\u20132.5 based on strength and specificity.", "expectation": "Clear modular design with explicit protocol boundaries and path to scale."}, {"type": "llm_judge", "name": "Actionability for Industrial Stakeholders", "description": "Evaluate whether the deliverables provide actionable guidance for installation and commissioning.", "weight": 2.5, "judge_prompt": "Consider whether a controls/industrial engineer could act on the deliverables:\n- Are installation & layout steps concrete (mounting, cabling, safety controller integration, commissioning tests)?\n- Are procurement details (models, quantities, costs) clear enough for a BOM?\n- Are acceptance criteria or test steps hinted (e.g., zone validation, E-stop integration, camera trigger checks)?\nScore 0\u20132.5 based on practicality and actionability.", "expectation": "Clear steps and details that enable procurement and commissioning planning."}, {"type": "llm_judge", "name": "Risk, Validation, and Maintenance Considerations", "description": "Assess inclusion of risk mitigations, validation steps, and maintenance planning.", "weight": 2.5, "judge_prompt": "Check for:\n- Risk and hazard considerations (e.g., nuisance trips, occlusions, reflections, mat edge failures).\n- Validation plan (safety functional test, IO checks, zone teach/verify, camera trigger verification).\n- Maintenance and lifecycle notes (spares, cleaning, recalibration, firmware updates).\nScore 0\u20132.5 based on depth and practicality.", "expectation": "Thoughtful risk/validation plan with maintenance considerations suited to an industrial cell."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "b5d2e6f1-62a2-433a-bcdd-95b260cdd860", "rubric": {"category_name": "Wholesale Trade \u2013 Order Clerks: Sales Performance Pivot Workbook Evaluation", "rationale": "Analytical task (Pattern A). The output should be an Excel workbook with a strict, verifiable shape enabling automated and LLM checks. Stage 1 uses an LLM gate to enforce exact workbook structure (sheets, headers, pivots, grand totals). Stage 2 mixes light code checks (numeric plausibility and ST% consistency) with heavier LLM consistency checks across sheets and periods. Stage 3 evaluates professional quality, usability for decision-making, and documentation/readiness.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2013 Shape Enforcement Gate", "description": "Verify the workbook is in the exact required structure so deeper verification is possible. LLM-only per policy.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.25, "rules": [{"type": "llm_judge", "name": "Workbook Structure and Required Tabs/Headers", "description": "Check the file is an Excel workbook with the mandated sheets and table structures for pivot-based analysis.", "weight": 3.0, "judge_prompt": "You are evaluating whether the candidate output is a properly structured Excel workbook ready for verification. Only check PRESENCE/STRUCTURE, not calculation correctness.\n\nRequirements (be flexible with minor naming variations, but structure must clearly match):\n\n1) File format\n- Must be an Excel spreadsheet (.xlsx). Not PDF/DOCX/CSV.\n\n2) Required sheets (tabs)\n- A sheet named exactly \u201cData\u201d (or clearly equivalent like \u201cDATA\u201d). It should contain the underlying weekly sales rows or a clean consolidated data table used as the pivot source. It must look like a data table with fields suitable for Brand, Store, and WTD/MTD/YTD measures. Do not require exact column names here; just confirm it is a data table suitable for pivots.\n- A sheet titled \u201cSales by Brand\u201d (or a close variant like \u201cBrand Sales\u201d, \u201cSales \u2013 Brand\u201d). This tab must present totals BY BRAND only (no store breakdown), using a pivot or equivalent summary table.\n- A sheet titled \u201cSales by Store\u201d (or a close variant like \u201cStore Sales\u201d, \u201cSales \u2013 Store\u201d). This tab must present totals BY STORE for EACH BRAND (Store x Brand structure) using a pivot or equivalent summary table.\n\n3) Required columns on Sales by Brand\n- Must include the following headers (allow reasonable variants like spacing, pluralization, or $ symbol):\n  Brand\n  WTD Sales Quantity\n  WTD Sales $\n  WTD Stock On Hand\n  WTD ST%\n  MTD Sales Quantity\n  MTD Sales $\n  MTD Stock On Hand\n  MTD ST%\n  YTD Sales Quantity\n  YTD Sales $\n  YTD Stock On Hand\n  YTD ST%\n- Grand Total must be visible on this tab.\n\n4) Required columns on Sales by Store\n- Must include the following headers (allow reasonable variants as above):\n  Store\n  Brand Name (or Brand)\n  WTD Sales Quantity\n  WTD Total Sales $\n  WTD Stock On Hand\n  WTD ST%\n  MTD Sales Quantity\n  MTD Total Sales $\n  MTD Stock On Hand\n  MTD ST%\n  YTD Sales Quantity\n  YTD Total Sales $\n  YTD Stock On Hand\n  YTD ST%\n- Grand Total must be visible on this tab.\n\n5) Structural expectations\n- The two summary tabs must clearly be pivot-like summaries (or equivalent grouped totals) and not raw row-level data dumps.\n- ST% columns are present for WTD, MTD, YTD (labeling reflects they are sell-through percentages), with the stated formula ST% = Sales / Stock On Hand (do not verify math here\u2014just presence and labeling).\n\nScoring:\n- 3.0: Excel file with all three sheets present, both summary tabs have all required columns (allowing minor header variations), and both show Grand Totals. Structure clearly matches intent (brand-only on first tab; store-by-brand on second), and ST% columns for WTD/MTD/YTD are present on both.\n- 2.5: Excel file with all sheets, both summaries substantially complete but with small variations (e.g., one header slightly renamed or in different order), Grand Totals present on both.\n- 2.0: Excel file but missing one required summary tab OR multiple key columns absent on one tab OR just one Grand Total missing.\n- 0.0: Not an Excel file OR missing two or more required tabs OR the summaries are not present as grouped totals.\n\nOnly evaluate format/structure presence. Do NOT check numerical correctness.", "expectation": "A three-sheet Excel workbook: Data, Sales by Brand, Sales by Store. Each summary tab shows the specified columns (with minor naming flexibility) and Grand Totals present."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness and Consistency Verification", "description": "Now that structure is enforced, verify computations and consistency using code (light, robust checks) and LLM (deeper semantic checks).", "is_required": true, "max_points": 5.0, "min_score_to_pass": 2.5, "rules": [{"type": "code", "name": "ST% Calculation Plausibility and Non-Negative Metrics", "description": "Verify ST% approximately equals Sales / Stock On Hand per period (WTD/MTD/YTD) on both summary tabs, using either Sales Quantity or Sales $ as numerator (whichever aligns best). Also check Sales and Stock columns are numeric and generally non-negative.", "weight": 0.6, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheet_names = [str(s) for s in xls.sheet_names]\n    except Exception as e:\n        return 0.0, f\"Unable to open workbook: {e}\"\n\n    def find_sheet(target):\n        target = target.lower()\n        for s in sheet_names:\n            if target in s.lower():\n                return s\n        return None\n\n    brand_sheet = find_sheet('sales by brand') or find_sheet('brand')\n    store_sheet = find_sheet('sales by store') or find_sheet('store')\n\n    if not brand_sheet or not store_sheet:\n        # Structure gate was earlier; here return 0 if we can't find sheets for checks\n        return 0.0, \"Required summary sheets not found for verification.\"\n\n    try:\n        df_brand = context.files.read_excel(output.id, sheet_name=brand_sheet)\n        df_store = context.files.read_excel(output.id, sheet_name=store_sheet)\n    except Exception as e:\n        return 0.0, f\"Failed to read sheets: {e}\"\n\n    def standardize_columns(df):\n        cols = [str(c) for c in df.columns]\n        return cols, [c.lower() for c in cols]\n\n    def find_period_cols(cols_lower, period):\n        # period in {'wtd','mtd','ytd'}\n        def has(*tokens, exclude=None):\n            out = []\n            for i, c in enumerate(cols_lower):\n                ok = all(t in c for t in tokens)\n                if exclude:\n                    for ex in exclude:\n                        if ex in c:\n                            ok = False\n                if ok:\n                    out.append(i)\n            return out\n        # ST%\n        st_idx = None\n        for i, c in enumerate(cols_lower):\n            if period in c and 'st' in c and '%' in c:\n                st_idx = i\n                break\n            if period in c and 'st%' in c:\n                st_idx = i\n                break\n            if period in c and ('st' in c):\n                st_idx = i\n                break\n        # Stock on Hand\n        stock_idx = None\n        for i, c in enumerate(cols_lower):\n            if period in c and 'stock' in c and 'hand' in c:\n                stock_idx = i\n                break\n        # Sales qty\n        qty_idx = None\n        for i, c in enumerate(cols_lower):\n            if period in c and 'sales' in c and ('qty' in c or 'quantity' in c):\n                qty_idx = i\n                break\n        # Sales $\n        amt_idx = None\n        for i, c in enumerate(cols_lower):\n            if period in c and 'sales' in c and ('$' in c or 'amount' in c or 'amt' in c):\n                # avoid capturing quantity columns\n                if 'qty' not in c and 'quantity' not in c:\n                    amt_idx = i\n                    break\n        # fallback: any 'sales' if not qty/amt\n        if amt_idx is None and qty_idx is None:\n            for i, c in enumerate(cols_lower):\n                if period in c and 'sales' in c and 'stock' not in c:\n                    amt_idx = i\n                    break\n        return st_idx, stock_idx, qty_idx, amt_idx\n\n    def check_sheet(df):\n        if df.empty:\n            return 0.0, 0.0, 0, \"Empty sheet\"\n        cols, cols_lower = standardize_columns(df)\n        periods = ['wtd','mtd','ytd']\n        matches = []\n        negatives = 0\n        neg_count = 0\n        num_checked_rows = 0\n        feedback_bits = []\n        for p in periods:\n            st_i, stock_i, qty_i, amt_i = find_period_cols(cols_lower, p)\n            if st_i is None or stock_i is None or (qty_i is None and amt_i is None):\n                feedback_bits.append(f\"{p.upper()}: missing one of ST%/Stock/Sales columns\")\n                continue\n            # Extract series\n            st = pd.to_numeric(df.iloc[:, st_i], errors='coerce')\n            stock = pd.to_numeric(df.iloc[:, stock_i], errors='coerce')\n            qty = pd.to_numeric(df.iloc[:, qty_i], errors='coerce') if qty_i is not None else None\n            amt = pd.to_numeric(df.iloc[:, amt_i], errors='coerce') if amt_i is not None else None\n\n            # Count negatives in sales/stock\n            series_to_check = [s for s in [stock, qty, amt] if s is not None]\n            for s in series_to_check:\n                neg_count += int((s.dropna() < 0).sum())\n\n            mask = stock.notna() & (stock != 0) & st.notna()\n            if mask.any():\n                num_checked_rows += int(mask.sum())\n                tol = 0.05  # 5 percentage points in decimal terms\n                best_match = 0.0\n                if qty is not None:\n                    r = (qty[mask] / stock[mask]).astype(float)\n                    diff = (r - st[mask]).abs()\n                    ok = (diff <= tol).mean()\n                    best_match = max(best_match, float(ok))\n                if amt is not None:\n                    r2 = (amt[mask] / stock[mask]).astype(float)\n                    diff2 = (r2 - st[mask]).abs()\n                    ok2 = (diff2 <= tol).mean()\n                    best_match = max(best_match, float(ok2))\n                matches.append(best_match)\n            else:\n                feedback_bits.append(f\"{p.upper()}: insufficient rows with nonzero stock and ST% values\")\n        match_score = float(np.mean(matches)) if matches else 0.0\n        neg_frac = 0.0\n        if num_checked_rows > 0:\n            # approximate negative rate across checked columns; avoid over-penalizing\n            neg_frac = min(1.0, neg_count / max(1, num_checked_rows*2))\n        feedback = \", \".join(feedback_bits) if feedback_bits else \"\"\n        return match_score, neg_frac, num_checked_rows, feedback\n\n    b_match, b_neg, b_rows, b_fb = check_sheet(df_brand)\n    s_match, s_neg, s_rows, s_fb = check_sheet(df_store)\n\n    avg_match = (b_match + s_match) / 2 if (b_rows + s_rows) > 0 else 0.0\n    avg_neg = (b_neg + s_neg) / 2\n\n    # Penalty for negatives (light): up to 0.3 reduction\n    penalty = min(0.3, avg_neg * 0.5)\n    score = max(0.0, avg_match - penalty)\n\n    feedback = f\"Brand match={b_match:.2f}, Store match={s_match:.2f}, penalty={penalty:.2f}. \"\n    if b_fb:\n        feedback += f\"Brand notes: {b_fb}. \"\n    if s_fb:\n        feedback += f\"Store notes: {s_fb}.\"\n    return float(score), feedback"}, {"type": "code", "name": "Grand Totals Presence on Both Summary Tabs", "description": "Confirm that both Sales by Brand and Sales by Store tabs visibly include a Grand Total row/section.", "weight": 0.4, "code": "import pandas as pd\nimport numpy as np\nimport re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheet_names = [str(s) for s in xls.sheet_names]\n    except Exception as e:\n        return 0.0, f\"Unable to open workbook: {e}\"\n\n    def find_sheet(target):\n        target = target.lower()\n        for s in sheet_names:\n            if target in s.lower():\n                return s\n        return None\n\n    brand_sheet = find_sheet('sales by brand') or find_sheet('brand')\n    store_sheet = find_sheet('sales by store') or find_sheet('store')\n    if not brand_sheet or not store_sheet:\n        return 0.0, \"Required summary sheets not found.\"\n\n    def has_total(df):\n        try:\n            # search any cell for 'total'\n            data = df.astype(str).applymap(lambda x: x.lower())\n            return bool((data.apply(lambda col: col.str.contains('total', na=False))).any().any())\n        except Exception:\n            return False\n\n    try:\n        dfb = context.files.read_excel(output.id, sheet_name=brand_sheet)\n        dfs = context.files.read_excel(output.id, sheet_name=store_sheet)\n    except Exception as e:\n        return 0.0, f\"Read error: {e}\"\n\n    b_has = has_total(dfb)\n    s_has = has_total(dfs)\n    score = (1.0 if b_has else 0.0 + 1.0 if s_has else 0.0) / 2.0\n    fb = f\"Brand total: {'yes' if b_has else 'no'}; Store total: {'yes' if s_has else 'no'}.\"\n    return float(score), fb"}, {"type": "llm_judge", "name": "Pivot Integrity and Cross-Sheet Alignment", "description": "Check that both summary tabs are genuinely grouped totals with correct entity grain, and fields align with the Data tab.", "weight": 2.5, "judge_prompt": "Evaluate the following in the Excel workbook:\n\n1) Pivot-like summaries\n- Sales by Brand should summarize totals BY BRAND only (no store breakdown). It should be a grouped total table or pivot, not raw data rows.\n- Sales by Store should summarize totals BY STORE and include a Brand column (store-by-brand breakdown). It should also be a grouped total table or pivot.\n\n2) Field alignment with Data\n- The Data tab should appear to contain the underlying fields used (e.g., Brand, Store, and WTD/MTD/YTD measures). The summary tabs should be plausibly derivable from Data (fields present in Data match what is summarized).\n\n3) Internal consistency between tabs\n- Pick 1\u20132 brands and stores visible. Check that totals on Sales by Brand are consistent with aggregations visible on Sales by Store (e.g., brand totals approximate the sum across stores for that brand). This can be a rough visual check; do not require exact numeric equality, just clear consistency.\n\nScoring:\n- 2.5: Both tabs appear to be true grouped summaries at correct grain; fields clearly align with Data; cross-tab totals look consistent for sampled items.\n- 1.5: Minor issues (e.g., slight grain confusion or unclear derivation) but structure largely consistent and plausible.\n- 0.5: Significant doubt about pivot/grouping integrity or field alignment with Data.\n- 0.0: Tabs are not grouped totals OR cannot be derived from Data.", "expectation": "Two pivot-like summaries at the specified grains that are visibly derivable from the Data tab; sampled totals align across tabs."}, {"type": "llm_judge", "name": "Period Logic and Formatting Sanity", "description": "Check period monotonicity and basic formatting consistency.", "weight": 1.5, "judge_prompt": "Assess the following:\n\n1) Period monotonicity (tolerance for minor exceptions):\n- For given brands/stores, WTD should generally be <= MTD <= YTD for Sales Quantity and Sales $. Occasional exceptions due to returns or timing are acceptable, but overall the pattern should hold.\n\n2) ST% and currency formatting:\n- ST% columns should display as percentages. Sales $ columns should be formatted as currency. Quantities should be whole numbers or clearly numeric.\n\n3) Grand totals clarity:\n- Confirm Grand Total rows/sections are clearly visible and labeled on both summary tabs.\n\nScoring:\n- 1.5: Period logic is generally correct across the sheets; formatting for % and currency is appropriate; Grand Totals clearly shown.\n- 1.0: One area is weak (e.g., some formatting issues or a few monotonicity anomalies) but overall acceptable.\n- 0.5: Multiple issues present; still somewhat usable.\n- 0.0: Monotonicity widely violated and/or formatting unreadable and/or Grand Totals absent.", "expectation": "Reasonable WTD/MTD/YTD progression, correct % and currency formatting, and visible grand totals."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Presentation and Decision Usefulness", "description": "Holistic quality review focusing on professional presentation and usefulness for the buying team and DMM.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Formatting and Readability", "description": "Evaluate titles, headers, freeze panes, consistent number formats, and overall readability.", "weight": 0.8, "judge_prompt": "Evaluate whether the workbook is professionally presented:\n- Clear tab names and titles on each summary sheet (e.g., a header like \u201cSales by Brand\u201d).\n- Header rows styled (bold/shading), freeze panes for easier scrolling, readable column widths.\n- Consistent number formats: currency for $ columns, integers for quantities, percent for ST%.\n- Sorting that helps reading (e.g., by Sales $ descending) where appropriate.\n\nScoring:\n- 0.8: Clean, professional formatting on both summary tabs.\n- 0.5: Generally readable with minor issues.\n- 0.2: Noticeably rough formatting.\n- 0.0: Sloppy/unreadable.", "expectation": "Well-formatted, readable summary tabs with consistent number formats and helpful layout."}, {"type": "llm_judge", "name": "Usability for Decision-Making", "description": "Assess whether the workbook helps the buying team/DMM quickly analyze performance and act.", "weight": 0.6, "judge_prompt": "Consider decision-usefulness:\n- Filters/slicers or easy-to-use pivot filters for timeframe/brand/store if present.\n- Sorting/highlighting to surface top/bottom performers.\n- Totals/subtotals visible and useful for quick insights.\n- Optional: charts or conditional formatting that adds clarity without clutter.\n\nScoring:\n- 0.6: Strongly supports quick decision-making with helpful interactivity/organization.\n- 0.4: Adequate for decisions; minor improvements needed.\n- 0.2: Usable but not optimized.\n- 0.0: Hard to use for decisions.", "expectation": "Organized summaries that make it easy to spot trends and take action."}, {"type": "llm_judge", "name": "Documentation and Update Readiness", "description": "Check for brief notes or cues explaining data source and refresh, aiding maintenance and reuse.", "weight": 0.6, "judge_prompt": "Look for simple documentation elements that help teams maintain and reuse the workbook:\n- A brief note (in a cell or a small README/Notes sheet) describing the Data tab as the pivot source.\n- Optional instructions on how to refresh pivots when data updates.\n- Clear field naming aligning Data with the summary tabs.\n\nScoring:\n- 0.6: Contains clear notes/instructions and alignment cues.\n- 0.4: Minimal but helpful cues.\n- 0.2: Barely any documentation.\n- 0.0: No documentation or cues visible.", "expectation": "Lightweight, visible documentation for future updates and handoff."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "9a8c8e28-ce76-408b-83c3-488422892e58", "rubric": {"category_name": "Editorial Accessibility Pack (Guide, Checklist, Quiz) \u2014 Staged Evaluation", "rationale": "This rubric enforces a self-documenting structure for three PDFs (guide, checklist, quiz), then verifies correctness against UK law and WCAG with mixed LLM and light code rules, and finally assesses overall quality and usability for a mixed-literacy newsroom. Stage 1 is a strict LLM-only gate ensuring evaluable structure. Stage 2 emphasizes LLM verification with small code checks (~5x lower weight) leveraging the mandated shape. Stage 3 provides holistic quality assessment.", "max_total_score": 35.0, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate (LLM-only)", "description": "Gate: Verify the candidate produced three distinct PDFs with the required structures that enable verification. Do NOT judge content accuracy here\u2014only presence and structure.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Three-PDF Pack Structure Present", "description": "Confirm there are three separate PDFs: (1) an Accessibility Framework Guide tailored to editors/journalists, (2) a Quick Best-Practice Checklist for daily use, (3) a Multiple-Choice Quiz with answer key and scoring guide. Check structural elements only.", "weight": 8.0, "judge_prompt": "You are evaluating whether the submission includes three distinct PDF documents with the required structures. You can open and read the PDFs. Only assess structure/presence\u2014not accuracy or writing quality.\n\nConfirm the following:\n\nA) Accessibility Framework Guide (PDF)\n- Title suggests a comprehensive guide (e.g., includes terms like Accessibility, Guide/Framework, Editors/Journalists/Editorial).\n- Audience: explicitly for editors/journalists (not developers).\n- Required sections present (headers may vary, be flexible):\n  1) Executive Summary or Overview (on first 1\u20132 pages)\n  2) Legal Context and Compliance: references to UK law (Equality Act 2010) and Public Sector Bodies (Websites and Mobile Applications) Accessibility Regulations 2018, plus alignment with WCAG 2.1 and/or 2.2\n  3) Best-Practice Framework mapped to editorial tasks (headings/structure, links, images/alt text, graphics/data viz, video/audio captions & transcripts, colour/contrast, tables, errors/clarity)\n  4) Roles & Responsibilities for editors/journalists\n  5) CMS scope note: changes beyond basic authoring (beyond creating text, processing/embedding images/graphics/video, basic semantic formatting, links, alt text) are handled by the dev team\n  6) Contact & Training info: instructions to contact section editor; training dates to be announced in the Slack editorial advice channel\n  7) Bibliography / Further reading with links\n- Length: at least 5 pages (be flexible if dense but must be clearly more than a 1\u20132 page memo)\n\nB) Quick Best-Practice Checklist (PDF)\n- Title indicates checklist and daily use by editorial staff\n- Clearly formatted checklist items (checkboxes like [ ], \u2610 or similar) grouped by topics such as: text & language, headings/structure, links, images/alt text, graphics/data viz, video/audio captions/transcripts, colour/contrast, tables, pre-publish checks\n- Designed for day-to-day reference; 1\u20133 pages typical\n\nC) Multiple-Choice Quiz (PDF)\n- Contains at least 12 questions total\n- Each question has exactly four options with one correct answer\n- Includes an Answer Key with explanations and a Scoring Guide (e.g., interpretation bands or pass/fail)\n- Questions cover content from the guide and checklist, including legal/WCAG, alt text, captions/transcripts, roles vs dev-team responsibilities, contact/training info\n\nScoring:\n- 8.0: All three PDFs present and each meets all required structural elements\n- 7.0: All three PDFs present; only minor structural omissions (e.g., one missing minor sub-section or small deviation)\n- 5.0\u20136.0: Three PDFs present but one document is missing a major required section (e.g., guide lacks legal references or quiz lacks answer key) OR checklist not clearly a checklist\n- 0.0\u20134.0: Missing one or more of the three PDFs; or wrong format (not PDFs); or documents are too skeletal to evaluate\n\nImportant: Judge only the presence/structure that enables verification later. Do not check correctness of claims here.", "expectation": "Three well-structured PDFs matching the specified shapes so subsequent verification is feasible."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Compliance Verification", "description": "Verify factual and procedural correctness against UK/EU/US accessibility expectations, WCAG 2.1/2.2 alignment, and newsroom applicability. Mix of LLM depth checks and light, robust code checks.", "is_required": false, "max_points": 17.0, "min_score_to_pass": 10.0, "rules": [{"type": "code", "name": "Statutory and Standards Mentions Present", "description": "Check that the pack explicitly mentions Equality Act 2010, the Public Sector Bodies (Websites and Mobile Applications) Accessibility Regulations 2018, and WCAG 2.1/2.2 across the PDFs.", "weight": 0.75, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs()\n        if not outputs:\n            return 0.0\n        texts = []\n        for r in outputs:\n            try:\n                if getattr(r, 'is_document', False):\n                    # Prefer PDFs; skip if cannot read\n                    txt = context.files.read_pdf_text(r.id)\n                    if txt:\n                        texts.append(txt)\n            except Exception:\n                continue\n        if not texts:\n            return 0.0\n        all_text = '\\n'.join(texts).lower()\n        checks = [\n            'equality act 2010' in all_text,\n            ('public sector bodies' in all_text and '2018' in all_text and 'websites and mobile applications' in all_text),\n            ('wcag 2.1' in all_text or 'wcag 2.2' in all_text)\n        ]\n        score = sum(1 for c in checks if c) / 3.0\n        return max(0.0, min(1.0, score))\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Checklist Heuristics (Checkbox Items)", "description": "Heuristic check that the checklist has checkbox-style items suitable for daily use.", "weight": 0.75, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs()\n        if not outputs:\n            return 0.0\n        best_text = ''\n        for r in outputs:\n            try:\n                if getattr(r, 'is_document', False):\n                    txt = context.files.read_pdf_text(r.id) or ''\n                    # Prefer a file that looks like a checklist\n                    if 'checklist' in txt.lower() and len(txt) > len(best_text):\n                        best_text = txt\n            except Exception:\n                continue\n        # fallback: longest document\n        if not best_text:\n            longest = ''\n            for r in outputs:\n                try:\n                    if getattr(r, 'is_document', False):\n                        txt = context.files.read_pdf_text(r.id) or ''\n                        if len(txt) > len(longest):\n                            longest = txt\n                except Exception:\n                    continue\n            best_text = longest\n        text = best_text\n        if not text:\n            return 0.0\n        lines = [l.strip() for l in text.splitlines()]\n        checkbox_patterns = [r'^\\s*\\[\\s?\\]', r'^\\s*[\\u2610\\u25A1\\u25FB\\u25FD\\u274F]']  # [ ], \u2610, \u25a1, \u25fb, \u25fd\n        count = 0\n        for l in lines:\n            for pat in checkbox_patterns:\n                if re.search(pat, l):\n                    count += 1\n                    break\n        # Score: 10+ checkbox lines => full credit, else proportional\n        score = min(1.0, count / 10.0)\n        return max(0.0, score)\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Quiz Structure and Answer Key", "description": "Verify the quiz has >=10 questions with four options and includes an Answer Key and scoring guidance.", "weight": 0.75, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs()\n        if not outputs:\n            return 0.0\n        quiz_text = ''\n        # Try to find the quiz by keyword\n        for r in outputs:\n            try:\n                if getattr(r, 'is_document', False):\n                    txt = context.files.read_pdf_text(r.id) or ''\n                    if 'quiz' in txt.lower() or 'answer key' in txt.lower():\n                        if len(txt) > len(quiz_text):\n                            quiz_text = txt\n            except Exception:\n                continue\n        if not quiz_text:\n            # fallback: choose the doc with the most occurrences of options A-D\n            best_txt = ''\n            best_score = -1\n            for r in outputs:\n                try:\n                    if getattr(r, 'is_document', False):\n                        txt = context.files.read_pdf_text(r.id) or ''\n                        opt_hits = len(re.findall(r'\\b[a-dA-D][\\).]', txt))\n                        if opt_hits > best_score:\n                            best_score = opt_hits\n                            best_txt = txt\n                except Exception:\n                    continue\n            quiz_text = best_txt\n        if not quiz_text:\n            return 0.0\n        text = quiz_text\n        # Count questions by patterns like Q1, 1., Question 1\n        q_matches = re.findall(r'(?im)^(?:q\\s*\\d+|question\\s*\\d+|\\d+\\.)\\s', text)\n        q_count = len(q_matches)\n        # Check options A-D pattern existence near questions (heuristic via global count)\n        opt_patterns = [r'\\bA[\\).]\\s', r'\\bB[\\).]\\s', r'\\bC[\\).]\\s', r'\\bD[\\).]\\s']\n        opt_presence = all(re.search(p, text) for p in opt_patterns)\n        has_answer_key = ('answer key' in text.lower())\n        has_scoring = ('scoring' in text.lower() or 'score' in text.lower())\n        # Scoring heuristic\n        base = 0.0\n        if q_count >= 12 and opt_presence and has_answer_key and has_scoring:\n            base = 1.0\n        elif q_count >= 10 and has_answer_key:\n            base = 0.7\n        elif q_count >= 8:\n            base = 0.4\n        else:\n            base = 0.0\n        return max(0.0, min(1.0, base))\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Contact and Training Instructions Present", "description": "Check presence of instructions to contact the section editor and that training dates will be announced in the Slack editorial advice channel.", "weight": 0.75, "code": "def evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs()\n        if not outputs:\n            return 0.0\n        all_text = ''\n        for r in outputs:\n            try:\n                if getattr(r, 'is_document', False):\n                    txt = context.files.read_pdf_text(r.id) or ''\n                    all_text += '\\n' + txt\n            except Exception:\n                continue\n        lt = all_text.lower()\n        has_contact = ('contact' in lt and 'section editor' in lt)\n        has_slack = ('slack' in lt and 'editorial advice' in lt)\n        has_tba = ('to be announced' in lt) or ('tba' in lt) or ('announced' in lt and 'dates' in lt)\n        count = sum([has_contact, has_slack, has_tba])\n        if count == 3:\n            return 1.0\n        elif count == 2:\n            return 0.66\n        elif count == 1:\n            return 0.33\n        else:\n            return 0.0\n    except Exception:\n        return 0.0"}, {"type": "llm_judge", "name": "Legal and WCAG Mapping Accuracy", "description": "Check that the guide accurately contextualizes UK legal obligations and aligns with WCAG 2.1/2.2, mapping editorial tasks to relevant success criteria with correct, non-misleading explanations.", "weight": 4.0, "judge_prompt": "Open the guide PDF and verify legal and standards accuracy. Assess:\n- Does it correctly reference the Equality Act 2010 and the Public Sector Bodies (Websites and Mobile Applications) Accessibility Regulations 2018, and explain their relevance to an online news outlet serving UK and international audiences? (Be fair: the outlet may not be a public sector body, but aligning with WCAG is still best practice and reduces risk.)\n- Does it accurately describe WCAG 2.1/2.2, including core principles (POUR) and how editorial practices map to specific success criteria (e.g., text alternatives, headings, link purpose, captions/transcripts, colour contrast, keyboard, timing)?\n- Are examples accurate and not technically misleading for non-developer staff?\n\nScoring:\n- 4.0: Accurate and comprehensive, with clear, correct mapping of editorial tasks to WCAG and appropriate legal context\n- 3.0: Generally accurate; minor omissions/ambiguities\n- 2.0: Some inaccuracies or weak mapping, but mostly usable\n- 1.0: Significant inaccuracies or misleading claims\n- 0.0: Largely incorrect or missing this content", "expectation": "Clear, correct legal context and WCAG mapping tailored to editorial work."}, {"type": "llm_judge", "name": "Coverage of Modalities and Responsibilities", "description": "Verify the pack covers text, images/graphics, video/audio, tables/data viz, contrast/colour, structure/links, and distinguishes editorial responsibilities from dev-team CMS changes.", "weight": 4.0, "judge_prompt": "Review the three PDFs collectively. Confirm they cover:\n- Text language/clarity, headings/structure, link text, spelling (UK), and error avoidance\n- Images/graphics: alt text, decorative images, complex figures, data visualizations (descriptions or data tables)\n- Video/audio: captions (closed), transcripts, audio descriptions where applicable\n- Colour and contrast, and reliance on more than colour\n- Tables: headers, summaries, simple vs complex tables\n- Keyboard considerations (as awareness for editors) and non-flashing media\n- Clear delineation between what editors/journalists must do vs. what the dev team will handle (CMS changes beyond basic authoring tasks per instructions)\n\nScoring:\n- 4.0: All areas covered with practical guidance and clear responsibility boundaries\n- 3.0: Most areas covered; minor gaps\n- 2.0: Several gaps but still provides actionable coverage\n- 1.0: Major omissions\n- 0.0: Little to no relevant coverage", "expectation": "Comprehensive, practical coverage with explicit responsibilities."}, {"type": "llm_judge", "name": "Accessibility of the Documents Themselves", "description": "Assess whether the PDFs model good accessibility: clear headings, logical structure, readable typography, lists/tables formatted clearly, and non-ambiguous link text.", "weight": 3.0, "judge_prompt": "Evaluate the accessibility and readability of the PDFs themselves (within what you can observe):\n- Clear, hierarchical headings and consistent styles\n- Short paragraphs, plain language suitable for mixed technical literacy\n- Descriptive link text (avoid bare URLs), meaningful figure/table captions, and clear lists\n- Logical order, labeled sections, and sensible page layout\n- UK English spelling and newsroom tone\n\nScoring:\n- 3.0: Strongly accessible and readable across all three docs\n- 2.0: Generally good with minor issues\n- 1.0: Mixed quality; notable issues\n- 0.0: Poor accessibility/clarity", "expectation": "Documents exemplify accessibility and clarity for the newsroom."}, {"type": "llm_judge", "name": "Quiz Validity and Alignment", "description": "Judge whether the quiz reliably assesses understanding of the guide and checklist with unambiguous single-correct answers and informative rationales.", "weight": 3.0, "judge_prompt": "Review the quiz PDF in detail:\n- Questions reflect key practices from the guide/checklist (legal context, WCAG alignment, alt text, captions/transcripts, responsibilities, contact/training info)\n- Four options per question with only one clearly correct answer; distractors are plausible but incorrect\n- Answer key includes explanations that teach/reinforce correct practice\n- Scoring guide interprets scores (e.g., pass/fail, remediation guidance)\n\nScoring:\n- 3.0: High-quality, aligned, fair assessment with strong explanations\n- 2.0: Generally good; minor flaws\n- 1.0: Weak alignment or ambiguous items\n- 0.0: Poor or unusable quiz", "expectation": "A fair, instructionally sound quiz with explanations and scoring."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Holistic Quality and Usability", "description": "Professional quality, adoption-readiness, and integration into editorial workflows.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Audience Fit and Change Management", "description": "Tone, motivation, and support for a team with mixed technical literacy and resistance to change.", "weight": 3.0, "judge_prompt": "Assess whether the materials are suitable for experienced journalists and editors with mixed technical literacy and some resistance to change:\n- Clear rationale for accessibility (legal, ethical, audience reach, brand/reputation)\n- Empathetic tone that supports adoption without jargon overload\n- Actionable steps to get started and improve over time\n\nScore 0\u20133 based on how well these are achieved.", "expectation": "Supportive, motivating materials tailored to newsroom realities."}, {"type": "llm_judge", "name": "Workflow Integration and Practicality", "description": "Day-to-day usability and integration into onboarding and editorial processes.", "weight": 3.0, "judge_prompt": "Evaluate how well the materials integrate into daily editorial work and onboarding:\n- Checklist aligns with typical editorial tasks and pre-publish checks\n- Guide includes practical examples, do/don\u2019t lists, and remediation guidance\n- Clear pre-publication and post-publication review steps; references to where to ask for help\n\nScore 0\u20133 based on practicality and integration strength.", "expectation": "Seamless fit into daily workflows and onboarding."}, {"type": "llm_judge", "name": "Professional Presentation and Referencing", "description": "Style consistency, UK English, and quality of bibliography/links.", "weight": 2.0, "judge_prompt": "Review professional polish:\n- Consistent styles, headings, and terminology; UK English spelling\n- Clean layout, minimal typos\n- Bibliography/further reading is credible, current, and includes working links (as visible in the PDF)\n\nScore 0\u20132 based on presentation and referencing quality.", "expectation": "Polished, consistent, well-referenced documents."}, {"type": "llm_judge", "name": "Maintainability and Onboarding Readiness", "description": "Clarity about ongoing updates, training, and ownership.", "weight": 2.0, "judge_prompt": "Check that the pack will sustain over time:\n- Indicates training will be announced in the Slack editorial advice channel; suggests where updates will be communicated\n- Directs staff to contact their section editor for questions\n- Signals that materials are part of standard onboarding/review\n\nScore 0\u20132 based on maintainability and readiness.", "expectation": "Clear ownership and continuity cues for lasting adoption."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "a46d5cd2-55fe-48fa-a4c6-6aaf6b9991b5", "rubric": {"category_name": "Fraudulent Insurance Claim Investigation Report (Retail Trade \u2022 Private Detectives and Investigators)", "rationale": "This rubric enforces a self-documenting, verifiable PDF investigation report that consolidates two field investigators\u2019 findings, integrates photographic evidence, and concludes with actionable guidance for the client (Safely Insurance Agency). Stage 1 is an LLM-only gate that mandates an exact, reviewable structure, enabling Stage 2\u2019s targeted verification via mixed code and LLM checks. Stage 3 provides a holistic quality assessment for professional standards, clarity, and client value.", "max_total_score": 30.0, "stages": [{"name": "Stage 1: Structure and Format Gate (LLM Only)", "description": "Gate ensuring the output is a properly structured, client-ready PDF report with specific sections and embedded photographs, enabling reliable verification in later stages.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Structured Report Format Requirement (PDF on Letterhead)", "description": "Verify the candidate produced a professionally structured PDF report on company letterhead, with required sections, metadata, and embedded photos.", "weight": 8.0, "judge_prompt": "You are evaluating whether the SUBMITTED PRIMARY OUTPUT is a properly structured PDF investigation report that consolidates two field investigators\u2019 findings. Only assess PRESENCE/STRUCTURE, not the quality or correctness of content. Be flexible with exact wording of headers.\n\nFormat requirements:\n- File type: PDF (not Word, not plain text, not Excel)\n- Company letterhead present on the first page (e.g., firm name/logo and contact info in header area). Logo optional; a clearly identifiable letterhead with firm name and contact details is acceptable.\n- Professional report title on the first page (e.g., \u201cInsurance Claim Investigation Report\u201d or \u201cSurveillance & Claims Investigation Report\u201d).\n- At least 2 pages total.\n\nRequired content on first page:\n- Case metadata block including fields (flexible labels): Client (must include \u201cSafely Insurance Agency\u201d), Claim/File No. (or similar), Claimant (or Subject), Incident Date (or similar), Report Date, Assigned Investigators, Surveillance Dates/Location.\n\nRequired section headers (accept close variants):\n1) Executive Summary\n2) Case Background (or Context)\n3) Objectives and Scope\n4) Methodology and Surveillance Plan (must reference adjusted strategy: second investigator at earlier time)\n5) Findings \u2013 Investigator A (with dated/timed observations)\n6) Findings \u2013 Investigator B (with dated/timed observations)\n7) Photographic Evidence (photos integrated inline OR a distinct gallery section, with captions)\n8) Analysis/Assessment (cross-referencing observations with the claimant\u2019s injury claims)\n9) Conclusion and Recommendations\nOptional but preferred: Appendix (e.g., observation logs, photo index, chain of custody)\n\nPhotographs requirement:\n- At least 2 photographs included and visible within the PDF, each with a caption that includes at minimum a label (e.g., Photo 1) and preferably a date/time and/or location.\n\nScoring (0\u20138):\n- 8.0: PDF; letterhead; title; \u22652 pages; all 9 required sections present; case metadata present; \u22652 captioned photos.\n- 6.5\u20137.5: PDF; letterhead; title; \u22652 pages; case metadata present; missing exactly 1 required section OR has only 1 embedded photo/caption.\n- 3.0\u20136.0: PDF present with partial structure (\u22652 required sections) but missing multiple required sections and/or missing photos/captions or metadata block.\n- 0.0\u20132.5: Not a PDF, or <2 pages, or lacks clear report structure (no identifiable sections/letterhead/metadata).\n\nOnly assess structure/format/presence. Do not judge calculation accuracy or report quality at this stage.", "expectation": "A multi-page PDF on letterhead, with a clear title, case metadata, all required sections for both investigators, and at least two captioned photographs."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Verification of Correctness (Mixed)", "description": "Now that the document is in the correct shape, verify correctness and internal consistency using code and LLM checks. Focus on cross-references, plausibility, and coverage of both investigators\u2019 work.", "is_required": true, "max_points": 12.0, "min_score_to_pass": 8.0, "rules": [{"type": "code", "name": "PDF Format and Readability Check", "description": "Confirm the primary output is a PDF and contains a substantive amount of extractable text (proxy for real report content).", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output:\n            return 0.0, \"No output resource.\"\n        # Require PDF specifically\n        path = context.files.get_path(output.id)\n        is_pdf = str(path).lower().endswith('.pdf')\n        if not is_pdf:\n            return 0.0, \"Output is not a PDF.\"\n        # Attempt to read text\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception as e:\n            return 0.0, f\"Could not read PDF text: {e}\"\n        if not text:\n            return 0.2, \"PDF has no extractable text (might be image-only).\"\n        # Score by content length (very rough proxy):\n        length = len(text)\n        if length > 3000:\n            return 0.8, f\"PDF readable with substantial text (~{length} chars).\"\n        elif length > 1500:\n            return 0.6, f\"PDF readable with moderate text (~{length} chars).\"\n        elif length > 500:\n            return 0.4, f\"PDF readable with limited text (~{length} chars).\"\n        else:\n            return 0.2, f\"PDF readable but very short (~{length} chars).\"\n    except Exception as e:\n        return 0.0, f\"Exception in rule: {e}\""}, {"type": "code", "name": "Client Name Present (Safely Insurance Agency)", "description": "Check that the client name is explicitly mentioned in the report text.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output:\n            return 0.0\n        # Prefer PDF text if possible; fallback to DOCX/MD if not\n        text = \"\"\n        try:\n            path = context.files.get_path(output.id)\n            if str(path).lower().endswith('.pdf'):\n                text = context.files.read_pdf_text(output.id)\n            elif str(path).lower().endswith('.docx'):\n                text = context.files.read_docx_text(output.id)\n            else:\n                text = context.files.read_text(output.id)\n        except Exception:\n            text = \"\"\n        if not text:\n            return 0.0\n        t = text.lower()\n        if \"safely insurance agency\" in t or re.search(r\"\\bsafely\\s+insurance\\b\", t):\n            return 0.5\n        return 0.0\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Separate Sections for Investigator A and B", "description": "Verify both investigators are distinctly covered by scanning for common section headers/labels.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output:\n            return 0.0\n        text = \"\"\n        try:\n            path = context.files.get_path(output.id)\n            if str(path).lower().endswith('.pdf'):\n                text = context.files.read_pdf_text(output.id)\n            elif str(path).lower().endswith('.docx'):\n                text = context.files.read_docx_text(output.id)\n            else:\n                text = context.files.read_text(output.id)\n        except Exception:\n            text = \"\"\n        t = (text or \"\").lower()\n        patterns_a = [r\"investigator\\s*a\", r\"field\\s+investigator\\s*a\", r\"findings\\s*[-\u2013:]?\\s*investigator\\s*a\"]\n        patterns_b = [r\"investigator\\s*b\", r\"field\\s+investigator\\s*b\", r\"findings\\s*[-\u2013:]?\\s*investigator\\s*b\"]\n        has_a = any(re.search(p, t) for p in patterns_a)\n        has_b = any(re.search(p, t) for p in patterns_b)\n        score = 0.0\n        if has_a:\n            score += 0.3\n        if has_b:\n            score += 0.3\n        return score\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Date/Time Stamps Present in Observations", "description": "Check for presence of plausible date and/or time stamps indicating surveillance logging.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output:\n            return 0.0\n        text = \"\"\n        try:\n            path = context.files.get_path(output.id)\n            if str(path).lower().endswith('.pdf'):\n                text = context.files.read_pdf_text(output.id)\n            elif str(path).lower().endswith('.docx'):\n                text = context.files.read_docx_text(output.id)\n            else:\n                text = context.files.read_text(output.id)\n        except Exception:\n            text = \"\"\n        t = (text or \"\")\n        # Basic date patterns (US-style and Month-name style)\n        date_patterns = [\n            r\"\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b\",\n            r\"\\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Sept|Oct|Nov|Dec)[a-z]*\\s+\\d{1,2},\\s*\\d{4}\\b\",\n        ]\n        time_patterns = [\n            r\"\\b(?:[01]?\\d|2[0-3]):[0-5]\\d\\s?(?:AM|PM|am|pm)?\\b\",\n            r\"\\b\\d{1,2}\\s?(?:AM|PM|am|pm)\\b\",\n        ]\n        date_hits = sum(len(re.findall(p, t)) for p in date_patterns)\n        time_hits = sum(len(re.findall(p, t)) for p in time_patterns)\n        total = date_hits + time_hits\n        if total >= 6:\n            return 0.6\n        elif total >= 3:\n            return 0.4\n        elif total >= 1:\n            return 0.2\n        else:\n            return 0.0\n    except Exception:\n        return 0.0"}, {"type": "llm_judge", "name": "Findings Integration and Cross-Reference Accuracy", "description": "Check that the report accurately consolidates both investigators\u2019 observations, reflects the strategy adjustment, and maintains consistent chronology with references to dates/times and the correct location.", "weight": 4.0, "judge_prompt": "Judge whether the report integrates both investigators\u2019 work accurately and consistently.\n\nLook for:\n- A clear summary of Investigator A\u2019s observations (limited activity) and Investigator B\u2019s follow-up at an earlier time.\n- An explicit mention of strategy adjustment (sending Investigator B earlier due to limited activity).\n- Chronological clarity with dates/times and location continuity.\n- No contradictions between narrative text and embedded photo captions.\n\nScoring (0\u20134):\n- 4.0: Both investigators\u2019 findings are accurately integrated; strategy shift is explicit; clear chronological, location-consistent narrative with no contradictions.\n- 2.5\u20133.5: Mostly accurate integration, minor gaps (e.g., limited timestamps or a small inconsistency) but overall coherent.\n- 1.0\u20132.0: Partial integration; missing or unclear strategy shift; noticeable inconsistencies.\n- 0.0: Fails to integrate both investigators or contains major contradictions.", "expectation": "Accurate, consistent consolidation of Investigator A and B with explicit strategy shift and coherent timeline."}, {"type": "llm_judge", "name": "Photographic Corroboration and Referencing", "description": "Evaluate whether photographs are meaningfully integrated, captioned, and referenced to support observations.", "weight": 3.0, "judge_prompt": "Assess how well the report uses photographs to corroborate observations.\n\nLook for:\n- At least two photographs embedded and visible.\n- Captions that include at least labels; preferably date/time/location.\n- Cross-references in the text to specific photos (e.g., \u201cSee Photo 2\u201d).\n- Visual evidence that aligns with described observations (e.g., vehicle, subject activity, residence).\n\nScoring (0\u20133):\n- 3.0: \u22652 photos with informative captions; consistently referenced in text; images clearly support claims.\n- 2.0: Photos present and somewhat captioned/referenced; minor mismatches or sparse references.\n- 1.0: Minimal/uncaptioned photos or weak linkage to text.\n- 0.0: No usable photographic corroboration.", "expectation": "Embedded photos with captions that are referenced and substantively corroborate observations."}, {"type": "llm_judge", "name": "Conclusion Validity and Evidence Alignment", "description": "Determine whether the conclusion appropriately reflects the collected evidence and addresses consistency with the claimant\u2019s injury claims.", "weight": 3.0, "judge_prompt": "Evaluate the reasonableness of the report\u2019s conclusion.\n\nLook for:\n- A clear statement about whether observed activities are consistent or inconsistent with claimed lower back/neck injuries and inability to work.\n- Conclusions grounded in documented observations and photos (no overreach).\n- Practical recommendations for the client (e.g., continue surveillance, independent medical exam, claim review).\n\nScoring (0\u20133):\n- 3.0: Conclusion is precise, evidence-backed, and offers appropriate recommendations.\n- 2.0: Generally sound but lacks precision or specific recommendations.\n- 1.0: Vague or partly unsupported.\n- 0.0: Unsupported or missing conclusion.", "expectation": "Evidence-aligned conclusion with practical recommendations for Safely Insurance Agency."}, {"type": "llm_judge", "name": "Compliance, Integrity, and Documentation Hygiene", "description": "Check for ethical/legal awareness, chain-of-custody notes, redaction of sensitive info, and non-inflammatory, professional tone.", "weight": 2.0, "judge_prompt": "Assess compliance and documentation integrity.\n\nLook for:\n- Ethical/legal considerations (public vantage points, no trespass, privacy awareness).\n- Chain-of-custody notes or photo/source handling (even brief).\n- Redaction of sensitive PII where appropriate and professional, neutral tone.\n\nScoring (0\u20132):\n- 2.0: Clear compliance cues and documentation hygiene.\n- 1.0: Some indications but incomplete.\n- 0.0: No evidence of compliance considerations.", "expectation": "Professional, compliant documentation with appropriate handling of evidence and privacy."}], "on_failure_action": "skip_remaining", "on_failure_score": 0.0}, {"name": "Stage 3: Holistic Quality Assessment", "description": "Evaluate overall professionalism, clarity, organization, and client value beyond correctness.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Letterhead Use", "description": "Overall polish: letterhead execution, layout, grammar, tone suitable for client distribution.", "weight": 3.0, "judge_prompt": "Evaluate professional presentation.\n\nConsider:\n- Clear company letterhead and consistent headers/footers/page numbers.\n- Clean layout, legible typography, minimal errors, and objective tone.\n- Appropriateness for client-facing distribution.\n\nScoring (0\u20133):\n- 3.0: Highly professional and polished.\n- 2.0: Generally professional with minor issues.\n- 1.0: Noticeable formatting/grammar distractions.\n- 0.0: Poorly presented.", "expectation": "Polished, client-ready PDF with professional tone and formatting."}, {"type": "llm_judge", "name": "Organization and Clarity", "description": "Assess logical flow, scannability, and clarity of sectioning and summaries.", "weight": 3.0, "judge_prompt": "Judge the organization and clarity of the report.\n\nConsider:\n- Logical sequencing from background to findings to analysis to conclusion.\n- Clear headings/subheadings, bulleting where appropriate, and concise summaries.\n- Easy to follow for a claims professional.\n\nScoring (0\u20133):\n- 3.0: Exceptionally clear and well-organized.\n- 2.0: Mostly clear; minor structural or clarity issues.\n- 1.0: Somewhat disorganized or verbose.\n- 0.0: Hard to follow.", "expectation": "Well-structured, concise report with clear headings and logical flow."}, {"type": "llm_judge", "name": "Client Orientation and Actionability", "description": "Determine if the report anticipates client needs and provides actionable next steps.", "weight": 2.0, "judge_prompt": "Evaluate client orientation and actionability.\n\nConsider:\n- Explicit recommendations (e.g., further surveillance, IME, claim review steps).\n- Relevance to insurer decision-making.\n- Clear callouts of key risks/red flags.\n\nScoring (0\u20132):\n- 2.0: Strong client orientation with concrete actions.\n- 1.0: Some guidance but limited specificity.\n- 0.0: Little to no actionable guidance.", "expectation": "Actionable recommendations tailored to Safely Insurance Agency\u2019s needs."}, {"type": "llm_judge", "name": "Limitations and Objectivity", "description": "Assess disclosure of investigation limitations and preservation of objectivity.", "weight": 2.0, "judge_prompt": "Assess whether the report acknowledges limitations and maintains objectivity.\n\nConsider:\n- Stated limitations (e.g., observation windows, weather, line of sight, camera constraints).\n- Avoidance of speculation; evidence-based language.\n\nScoring (0\u20132):\n- 2.0: Clear limitations and objective stance.\n- 1.0: Partial acknowledgement or minor speculative language.\n- 0.0: No limitations and/or speculative tone.", "expectation": "Transparent about limits; objective, evidence-based narrative."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "4d1a8410-e9c5-4be5-ab43-cc55563c594c", "rubric": {"category_name": "NAMC MTP Interview Day Scheduling Packet and Applicant Itineraries", "rationale": "Task Type: Pattern B (Document) with structured tables and embedded images; verification benefits from LLM structure checks plus light code checks for anchors and inclusions. Output Format: Three documents (DOCX or PDF): (1) Master Schedule document with a table, (2) one-page itinerary for Allen (Group A), (3) one-page itinerary for Isabelle (Group B). Stage 1 uses only LLM judges to enforce an explicit, verifiable document shape that enables Stage 2 correctness checks. Stage 2 mixes small-weight code rules (anchors, inclusions) with higher-weight LLM judgments (constraint satisfaction, sequencing, cross-consistency). Stage 3 assesses professional quality, usability, and robustness.", "max_total_score": 29.0, "stages": [{"name": "Stage 1 \u2014 Deliverable Structure Gate (LLM-only)", "description": "Gate that enforces exact deliverable shapes and visible structures enabling verification. Must have one Master Schedule document (DOCX/PDF) with a comprehensive table and required labeled sections, plus two one-page itineraries (Allen, Isabelle) each with required elements and images.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Master Schedule Document Structure", "description": "Verify there is a DOCX or PDF Master Schedule with a clear master table and labeled sections enabling verification.", "weight": 4.0, "judge_prompt": "You are checking only structure/format (not correctness of timings). Examine the candidate outputs. Confirm there is a Master Schedule document in DOCX or PDF format with the following visible structure:\n\nFormat requirements:\n- A professional document (DOCX or PDF), at least 1 full page, clearly titled as the interview day schedule or equivalent.\n- Contains a single comprehensive master table that planners can read without hidden content.\n\nMaster table column structure (flexible header naming, but columns must be clearly present):\n- Room Number (e.g., \"Room #\" or \"Interview Room\")\n- Physician/Resident Name\n- Time blocks with activity labels (Interview / Break / Lunch / Tour) for that room\n- Applicant Name(s) aligned to time blocks\n\nRequired labeled/non-table sections visible somewhere in the document:\n- Breakfast at 7:00 AM in the Conference Room\n- Welcome talks at 7:35 AM in the Forge Auditorium by Dr. Jones and Dr. Garrett\n- A 10-minute research talk by Dr. Meade after the welcome talks\n- A 5-minute break before interviews/tours begin\n- Indication that Group A interviews first and Group B does tours first\n- Lunch labeled as an event\n- A list or explicit mention of all tour stops (Main Hospital, Pediatric Center, Cancer Center, Rural Area Clinic, Simulation Learning Center)\n\nScoring:\n- 4.0: File is DOCX/PDF and includes the master table with all required columns plus all required labeled sections present.\n- 3.0: File is DOCX/PDF and master table exists with most columns; at most one labeled section missing.\n- 2.0: File is DOCX/PDF and has a schedule table, but multiple required sections are missing OR columns are ambiguous.\n- 1.0: File is DOCX/PDF but lacks a coherent master table; only scattered schedule info.\n- 0.0: No DOCX/PDF schedule document found or structure not present.\n\nOnly check presence/format, not timing correctness.", "expectation": "A DOCX/PDF with a clear table capturing rooms, physicians, time blocks with activities, and applicant names, plus labeled global sections anchoring the day."}, {"type": "llm_judge", "name": "Applicant Itinerary \u2014 Allen (Group A) Structure", "description": "Verify a single-page itinerary document exists for Allen (Group A) with required elements and images.", "weight": 2.0, "judge_prompt": "Check for a single-page DOCX or PDF itinerary for Allen from Group A with the following structure:\n\nRequired elements (flexible naming, but all must be visibly present on the page):\n- Applicant name: Allen\n- Group: Group A\n- A photo image (use of avatar-764x1024 indicated or a clearly placed headshot/photo)\n- Interview schedule: times and physician/resident names\n- The Floor Layout for Interviews image (include/inserted, not just referenced)\n- Site logos present to indicate the tour destinations (logos can be placed near a tour list/section)\n\nScoring:\n- 2.0: DOCX/PDF; clearly single-page; all required elements/images present and readable.\n- 1.5: DOCX/PDF; single-page; one minor element missing or weakly presented (e.g., logos not clearly tied to tour).\n- 1.0: DOCX/PDF; single-page; multiple required elements missing but itinerary structure still apparent.\n- 0.0: No single-page itinerary for Allen (Group A) in DOCX/PDF format, or page count > 1, or images absent entirely.", "expectation": "A one-page, visually clear itinerary for Allen with image(s), schedule, group label, floor layout, and site logos."}, {"type": "llm_judge", "name": "Applicant Itinerary \u2014 Isabelle (Group B) Structure", "description": "Verify a single-page itinerary document exists for Isabelle (Group B) with required elements and images.", "weight": 2.0, "judge_prompt": "Check for a single-page DOCX or PDF itinerary for Isabelle from Group B with the following structure:\n\nRequired elements (flexible naming, but all must be visibly present on the page):\n- Applicant name: Isabelle\n- Group: Group B\n- A photo image (use of avatar-764x1024 indicated or a clearly placed headshot/photo)\n- Interview schedule: times and physician/resident names\n- The Floor Layout for Interviews image (include/inserted, not just referenced)\n- Site logos present to indicate the tour destinations (logos can be placed near a tour list/section)\n\nScoring:\n- 2.0: DOCX/PDF; clearly single-page; all required elements/images present and readable.\n- 1.5: DOCX/PDF; single-page; one minor element missing or weakly presented.\n- 1.0: DOCX/PDF; single-page; multiple required elements missing but itinerary structure still apparent.\n- 0.0: No single-page itinerary for Isabelle (Group B) in DOCX/PDF format, or page count > 1, or images absent entirely.", "expectation": "A one-page, visually clear itinerary for Isabelle with image(s), schedule, group label, floor layout, and site logos."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Cross-Verification", "description": "Verify that the schedule meets operational constraints and that itineraries align with the schedule. Uses small-weight code rules for anchors and inclusions, and higher-weight LLM judges for nuanced sequencing, buffers, and cross-document consistency.", "is_required": false, "max_points": 13.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Deliverable Count and Types Sanity Check", "description": "Ensure at least three document outputs (DOCX/PDF) exist to represent: Master Schedule + two itineraries.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs() or []\n        count_docs = 0\n        for r in outputs:\n            if getattr(r, 'is_document', False):\n                count_docs += 1\n        score = 0.5 if count_docs >= 3 else (0.25 if count_docs == 2 else 0.0)\n        feedback = f\"Found {count_docs} document(s).\" \n        return score, feedback\n    except Exception as e:\n        return 0.0, f\"Error checking documents: {e}\""}, {"type": "code", "name": "Key Time Anchors Present", "description": "Check text for critical time anchors and lunch duration mention.", "weight": 0.5, "code": "import re\n\ndef _read_text(resource, files):\n    if resource.is_document:\n        try:\n            if str(resource.name or '').lower().endswith('.pdf'):\n                return files.read_pdf_text(resource.id) or ''\n            else:\n                return files.read_docx_text(resource.id) or ''\n        except Exception:\n            return ''\n    return ''\n\ndef evaluate(workflow, context):\n    try:\n        texts = []\n        for r in context.get_all_outputs() or []:\n            texts.append(_read_text(r, context.files))\n        blob = '\\n'.join(texts).lower()\n        anchors = {\n            '7:00': bool(re.search(r'\\b7\\s*[:\\.]?\\s*0{2}\\b', blob)),\n            '7:35': bool(re.search(r'\\b7\\s*[:\\.]?\\s*3?5\\b', blob)),\n            '8:50': bool(re.search(r'\\b8\\s*[:\\.]?\\s*5?0\\b', blob)),\n            '4:40': bool(re.search(r'\\b4\\s*[:\\.]?\\s*4?0\\b', blob)) or '16:40' in blob,\n            'lunch40': ('40' in blob and 'lunch' in blob) or bool(re.search(r'40\\s*min', blob))\n        }\n        present = sum(1 for v in anchors.values() if v)\n        ratio = present / len(anchors)\n        score = ratio * 0.5\n        feedback = f\"Anchors present {present}/{len(anchors)}: {anchors}\"\n        return score, feedback\n    except Exception as e:\n        return 0.0, f\"Error checking anchors: {e}\""}, {"type": "code", "name": "Tour Stops Included (All Five Sites)", "description": "Verify the documents mention all five tour sites.", "weight": 0.5, "code": "def evaluate(workflow, context):\n    try:\n        texts = []\n        for r in context.get_all_outputs() or []:\n            if getattr(r, 'is_document', False):\n                try:\n                    if str(r.name or '').lower().endswith('.pdf'):\n                        texts.append(context.files.read_pdf_text(r.id) or '')\n                    else:\n                        texts.append(context.files.read_docx_text(r.id) or '')\n                except Exception:\n                    continue\n        blob = ('\\n'.join(texts)).lower()\n        req = [\n            'main hospital',\n            'pediatric center',\n            'cancer center',\n            'rural area clinic',\n            'simulation learning center'\n        ]\n        found = {k: (k in blob) for k in req}\n        ratio = sum(1 for v in found.values() if v) / len(req)\n        score = ratio * 0.5\n        feedback = f\"Tour sites found: {found}\"\n        return score, feedback\n    except Exception as e:\n        return 0.0, f\"Error checking tour stops: {e}\""}, {"type": "code", "name": "Interview Cadence Mentioned (20-min + 5-min buffer)", "description": "Check that documents explicitly reference 20-minute interviews and 5-minute transitions (buffers).", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        texts = []\n        for r in context.get_all_outputs() or []:\n            if getattr(r, 'is_document', False):\n                try:\n                    if str(r.name or '').lower().endswith('.pdf'):\n                        texts.append(context.files.read_pdf_text(r.id) or '')\n                    else:\n                        texts.append(context.files.read_docx_text(r.id) or '')\n                except Exception:\n                    continue\n        blob = ('\\n'.join(texts)).lower()\n        has_20 = bool(re.search(r'\\b20\\s*-?\\s*min(ute)?s?\\b', blob)) or 'twenty-minute' in blob or '20-minute' in blob\n        has_5 = bool(re.search(r'\\b5\\s*-?\\s*min(ute)?s?\\b', blob)) or 'five-minute' in blob or '5-minute' in blob or 'transition' in blob\n        ratio = (1 if has_20 else 0 + 1 if has_5 else 0) / 2\n        score = ratio * 0.5\n        feedback = f\"20-min mentioned: {has_20}; 5-min buffer mentioned: {has_5}\"\n        return score, feedback\n    except Exception as e:\n        return 0.0, f\"Error checking cadence mentions: {e}\""}, {"type": "llm_judge", "name": "Schedule Feasibility and Constraint Satisfaction", "description": "Assess whether the master schedule satisfies all key operational constraints and time logic.", "weight": 4.0, "judge_prompt": "Using the Master Schedule document identified in Stage 1, judge whether the schedule satisfies these constraints. Award partial credit where most constraints are clearly satisfied. You may rely on visible tables and labeled annotations.\n\nConstraints to check:\n1) Group sequencing: Group A interviews first, Group B tours first.\n2) Start-of-day flow: 7:00 breakfast (Conference Room), 7:35 welcome talks by Dr. Jones and Dr. Garrett (Forge Auditorium), then a 10-minute research talk by Dr. Meade, then a 5-minute break before interviews/tours start.\n3) Interviews: Each interview slot is 20 minutes per room with a 5-minute transition buffer between interview slots. No buffer applied into/out of lunch or the designated breaks.\n4) Breaks: There is a 15-minute break after the first 5 interviews in the AM and in the PM.\n5) Room-specific breaks: Dr. Jones has a 10-minute break at 8:50 AM for a consultation; Dr. Garrett has the last break of the day, enabling him to leave 20 minutes early.\n6) Lunch: 40 minutes in the middle of the day (between 8:20 AM and 4:40 PM), clearly labeled.\n7) Tour timing: The touring group returns 10 minutes before lunch (buffer), and, when on a tour later, also returns 10 minutes early as required.\n\nScoring guidance:\n- 4.0: All listed constraints are satisfied with clear, conflict-free timing.\n- 3.0: One minor element unclear or slightly off, but overall feasible.\n- 2.0: Multiple elements unclear or slightly inconsistent, but structure mostly follows requirements.\n- 1.0: Major constraint violations or obvious conflicts, though some intent is visible.\n- 0.0: Does not satisfy the basic sequencing/constraints.", "expectation": "A feasible, constraint-compliant schedule with clear buffers, breaks, and lunch placed correctly."}, {"type": "llm_judge", "name": "Room Sequencing and Buffer Logic", "description": "Evaluate whether per-room timelines are coherent, non-overlapping, and apply buffers correctly except around lunch/breaks.", "weight": 3.0, "judge_prompt": "Inspect the master table per room. Verify that interview blocks per room:\n- Are in 20-minute increments.\n- Have 5-minute buffer slots between interview blocks (but not applied into/out of lunch or designated breaks).\n- Do not overlap within the same room.\n- Reflect the required built-in break per room in addition to the AM/PM 15-minute breaks.\n\nScoring guidance:\n- 3.0: All rooms show consistent, non-overlapping 20-minute interview slots with proper 5-minute transitions and designated breaks as specified.\n- 2.0: Minor inconsistencies in one room or slight ambiguity, but overall the buffer logic is clear.\n- 1.0: Several rooms have unclear buffers or overlaps, but partial sequencing is visible.\n- 0.0: Buffer logic missing or room timelines are incoherent.", "expectation": "Clear, per-room sequences that operational staff can follow without collisions or confusion."}, {"type": "llm_judge", "name": "Itinerary Alignment with Master Schedule", "description": "Confirm Allen\u2019s and Isabelle\u2019s itineraries align with the master schedule (times, physicians, group flow) and include required images.", "weight": 2.5, "judge_prompt": "Cross-check the one-page itineraries against the master schedule:\n- For Allen (Group A): interview times and physician/resident names match the Master Schedule for Group A\u2019s interview-first flow; group label correct. Avatar photo present; Floor Layout for Interviews image present; site logos included for tour destinations.\n- For Isabelle (Group B): interview times and physician/resident names match the Master Schedule for Group B\u2019s tour-first flow; group label correct. Avatar photo present; Floor Layout image present; site logos included.\n\nScoring guidance:\n- 2.5: Both itineraries fully align on times/physicians and include all required images/logos.\n- 2.0: One small mismatch or omission across both itineraries; otherwise aligned.\n- 1.0: Multiple mismatches or missing images, but basic alignment is visible.\n- 0.0: Itineraries do not match the schedule or lack key elements.", "expectation": "Itineraries mirror the master schedule exactly and include the images/logos requested."}, {"type": "llm_judge", "name": "Tour Return Buffer Compliance", "description": "Check that touring groups return 10 minutes before lunch (and other tour endpoints) as specified.", "weight": 1.5, "judge_prompt": "Check the schedule\u2019s tour blocks for both groups. Confirm:\n- The group on tour before lunch is scheduled to return 10 minutes before lunch begins.\n- When the other group is on tour later, they also return 10 minutes before the next dependent event.\n- The master schedule clearly indicates these buffers (e.g., labels or time offset).\n\nScoring guidance:\n- 1.5: Both tour return buffers are clearly present and correctly timed.\n- 1.0: One buffer is clear; the other is ambiguous but likely compliant.\n- 0.5: Buffers are only implied or poorly labeled.\n- 0.0: No clear evidence of the required 10-minute return buffers.", "expectation": "Tour blocks visibly end 10 minutes before lunch/next events for both groups."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Presentation Quality and Usability", "description": "Holistic assessment of professionalism, clarity, and practical usability for staff and applicants.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Formatting and Readability", "description": "Evaluate overall visual quality, layout, and readability of the master schedule and itineraries.", "weight": 3.0, "judge_prompt": "Assess the professional presentation:\n- Master Schedule uses clear headings, consistent fonts, readable table layout (column labels, alignment, spacing), and page organization.\n- Itineraries are single-page, visually balanced (image placement, white space), and easy to scan.\n- File export quality (sharp images, no artifacts), and consistent branding (logos, colors if used).\n\nScoring:\n- 3.0: Highly professional, clean, and easy to read.\n- 2.0: Generally professional with minor layout or readability issues.\n- 1.0: Readable but inconsistent formatting or clutter.\n- 0.0: Poorly formatted or difficult to read.", "expectation": "Clean tables, clear headings, good spacing, and consistent styling."}, {"type": "llm_judge", "name": "Usability for Operations", "description": "Judge whether staff can operationalize the schedule and applicants can follow itineraries without confusion.", "weight": 2.0, "judge_prompt": "Consider whether the documents can be used directly:\n- Are time blocks, rooms, and transitions clearly labeled so staff can route applicants and manage rooms?\n- Are legends/notes provided where needed (e.g., buffer rules, abbreviations)?\n- Are locations (Conference Room, Forge Auditorium, tour sites) clearly referenced on both schedule and itineraries?\n\nScoring:\n- 2.0: Immediately usable; minimal to no additional explanation required.\n- 1.0: Usable with minor clarifications.\n- 0.0: Confusing or missing key operational cues.", "expectation": "Clear labels, legends if needed, and unambiguous routing information."}, {"type": "llm_judge", "name": "Completeness of Details", "description": "Evaluate completeness of named roles, rooms, venues, and imagery per requirements.", "weight": 2.0, "judge_prompt": "Check the inclusion and clarity of these details:\n- Physicians/residents are named and associated with room numbers.\n- Venues for breakfast (Conference Room) and welcome talks (Forge Auditorium) are clearly labeled.\n- Lunch is labeled and placed logically.\n- Itineraries include the Floor Layout for Interviews image and site logos, and the avatar photos are clear.\n\nScoring:\n- 2.0: All details complete and clear.\n- 1.0: Minor omissions or unclear labels.\n- 0.0: Major required details missing or ambiguous.", "expectation": "All required venues, names, and images are present and clearly labeled."}, {"type": "llm_judge", "name": "Risk Mitigation and Practicality", "description": "Assess whether buffers/notes anticipate delays and minimize conflicts (e.g., Dr. Garrett\u2019s early departure).", "weight": 1.0, "judge_prompt": "Evaluate whether the schedule demonstrates practical risk mitigation:\n- Buffers appear sufficient at key handoffs.\n- Dr. Garrett\u2019s last break of the day clearly allows him to leave 20 minutes early without disrupting other rooms.\n- Any notes or legends help handle overruns or late transitions.\n\nScoring:\n- 1.0: Clear, practical mitigations and explanatory notes are present.\n- 0.5: Some mitigations visible but could be clearer.\n- 0.0: Little evidence of proactive risk handling.", "expectation": "Clear buffers and annotations that reduce operational risk."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "3c19c6d1-672c-467a-8437-6fe21afb8eae", "rubric": {"category_name": "Monthly Project Report (PPTX) \u2014 BridgeMind AI POC (Oct 2025)", "rationale": "Pattern C (Mixed): A document-style deliverable (PowerPoint) that consolidates structured data/analysis (progress, spend, risks) and narrative. Stage 1 uses an LLM gate to enforce the exact deck structure (slides 1\u20139) so verification becomes trivial. Stage 2 mixes small, robust code checks (file type, slide count, keyword/date presence) with higher-weight LLM checks for consistency and correctness of mappings to required sections. Stage 3 applies holistic LLM quality criteria for professional presentation, clarity for a grant assessor, and usefulness of insights. Code rules have materially lower weight than LLM rules as required.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (LLM only)", "description": "Hard gate ensuring the output is a PPTX deck with the exact slide structure and visible sections required for verification.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.5, "rules": [{"type": "llm_judge", "name": "PowerPoint Structure Gate: 9-slide monthly report with mandated sections", "description": "Verify the candidate output is a PowerPoint deck with at least 9 slides and the exact structure/content placeholders required. This only checks presence/structure, not content correctness.", "weight": 6.0, "judge_prompt": "You are verifying STRUCTURE ONLY for a required monthly project report deck. Review the candidate output (PowerPoint/PPTX expected). Do NOT judge quality or numerical correctness here; only check presence and format of required elements.\n\nFormat requirements:\n- File must be a PowerPoint deck (PPTX). PDF/DOCX are not acceptable for full credit.\n- At least 9 slides. Extra slides after slide 9 are acceptable if the first 9 satisfy requirements.\n\nRequired slides and structural content (be flexible on exact wording of titles, but the intent must be clear):\n1) Slide 1 \u2014 Title slide dated as of 30 October 2025 (accept \u201c30th October 2025\u201d or \u201c30 Oct 2025\u201d). Must indicate: project title \u201cBridgeMind AI\u201d, supplier \u201cBridge Mind\u201d, and that it\u2019s a monthly project report for October 2025. Presence of client/use-case context like \u201cCommon Ground Bikes\u201d and/or \u201cOxfordshire\u201d anywhere in deck is acceptable, but the date must be on the title slide.\n2) Slide 2 \u2014 High-level overview/summary of how the project is going (brief summary of slides 4\u20137). It should read as an executive overview.\n3) Slide 3 \u2014 \u201cWhat\u2019s in this report\u201d slide that lists: basic descriptors (Date of Report: 30th October 2025; Supplier Name: Bridge Mind; Proposal Title: \u2018BridgeMind AI\u2019 \u2013 An easy to use software application to improve your bicycle maintenance business.; Proposal Number: IUK6060_BIKE), followed by a clearly numbered list of sections: 1) Progress Summary, 2) Project Spend to date, 3) Risk Review, 4) Current Focus, 5) Auditor Q&A, 6) ANNEX A \u2013 Project Summary.\n4) Slide 4 \u2014 Progress Summary: a summary of progress excluding any financial values (no \u00a3/GBP currency values on this slide). It should visibly look like a progress/status summary, often derived from a table.\n5) Slide 5 \u2014 Project Spend to Date: a financial summary including associated spend/financials (expect to see \u00a3 or GBP figures and totals). Visibly distinct from slide 4.\n6) Slide 6 \u2014 Risk Review: a summary of project risks derived from a risk register (table-like presentation with risk, likelihood/impact, mitigations, owners, or RAG is acceptable).\n7) Slide 7 \u2014 Current Focus: summarizes current project considerations, derived from a project log; bullet points or brief list acceptable.\n8) Slide 8 \u2014 Auditor Q&A: a slide that invites questions or provides space/prompts for Q&A with the assessor.\n9) Slide 9 \u2014 Annex A \u2013 Project Summary: a concise summary of the overall project. Accept \u201cAnnex\u201d/\u201cAppendix\u201d variations if clearly labeled as a project summary annex.\n\nScoring (STRUCTURE ONLY):\n- 6.0: PPTX format AND all 9 required slides are present with clearly correct intent; date placement on title slide is correct; slide 4 excludes financials and slide 5 includes financial figures; slide 3 includes both the basic descriptors and the numbered list (1\u20136) as specified.\n- 5.0: PPTX format AND all 9 sections covered with minor deviations (e.g., slight title wording differences, or date shown prominently on title but marginal placement issues). Core separation of slides 4 and 5 is still clear (4 non-financial; 5 financial).\n- 4.0: PPTX format AND at least 8 of 9 sections present; or slide 3 includes either the basic descriptors or the numbered list but not both; or the date is present but not strictly formatted. Structure still enables verification.\n- 2.5: PPTX format AND at least 6 sections present with some misplacements (e.g., spend mixed into progress slide) but a reviewer can still identify most required sections.\n- 0.0: Not PPTX OR fewer than 6 required sections present OR slides are not clearly mapped to the required structure.\n\nOnly score based on presence/format, not content correctness or prose quality.", "expectation": "A 9-slide PPTX deck matching the specified sections, with title date 30 October 2025, clear separation of progress vs. spend, and a structured table-like risk review."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness and Consistency)", "description": "Now that the shape is enforced, verify correctness and consistency using light code checks and LLM judgment. Code rules carry substantially lower weight than LLM rules.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "PPTX File and Slide Count Check", "description": "Deterministically verify the output is a PPTX and has at least 9 slides by inspecting the PPTX zip contents.", "weight": 0.6, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float in [0,1] or (float, str)\n    \"\"\"\n    import os\n    try:\n        import zipfile\n    except Exception:\n        zipfile = None\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output resource\"\n    # Basic type/extension check\n    try:\n        path = context.files.get_path(output.id)\n    except Exception as e:\n        return 0.0, f\"Could not access file path: {e}\"\n    ext = os.path.splitext(str(path))[-1].lower()\n    if ext != \".pptx\":\n        return 0.0, f\"Expected .pptx, got {ext}\"\n    # Count slides via PPTX zip structure\n    slide_count = 0\n    if zipfile is not None:\n        try:\n            with zipfile.ZipFile(path, 'r') as zf:\n                names = zf.namelist()\n                slide_names = [n for n in names if n.startswith('ppt/slides/slide') and n.endswith('.xml')]\n                slide_count = len(slide_names)\n        except Exception as e:\n            # Fall back to minimal success based on extension only\n            return 0.4, f\"PPTX detected but could not read slides: {e}\"\n    else:\n        # zipfile not available; minimal score for correct extension\n        return 0.4, \"PPTX extension detected; zipfile module unavailable\"\n\n    if slide_count >= 9:\n        return 1.0, f\"Slide count OK: {slide_count}\"\n    elif slide_count >= 7:\n        return 0.7, f\"Slide count {slide_count} (<9)\"\n    elif slide_count >= 5:\n        return 0.4, f\"Slide count {slide_count} (<7)\"\n    else:\n        return 0.0, f\"Insufficient slides: {slide_count}\""}, {"type": "code", "name": "Mandatory Keywords and Date Presence", "description": "Search deck XML text for essential tokens: project/company names, client/location, proposal number, and October 2025 date variants.", "weight": 0.4, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Returns normalized score [0,1] based on presence of key tokens in PPTX slide XML.\n    Tokens: 'BridgeMind AI', 'Bridge Mind', 'Common Ground Bikes', 'Oxfordshire', 'IUK6060_BIKE', and a date variant like '30 October 2025'/'30th October 2025'/'30 Oct 2025'.\n    \"\"\"\n    import os, re\n    try:\n        import zipfile\n    except Exception:\n        zipfile = None\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output resource\"\n    try:\n        path = context.files.get_path(output.id)\n    except Exception as e:\n        return 0.0, f\"Could not access file path: {e}\"\n    ext = os.path.splitext(str(path))[-1].lower()\n    if ext != \".pptx\":\n        return 0.0, f\"Expected .pptx, got {ext}\"\n\n    text_all = \"\"\n    if zipfile is not None:\n        try:\n            with zipfile.ZipFile(path, 'r') as zf:\n                slide_names = [n for n in zf.namelist() if n.startswith('ppt/slides/slide') and n.endswith('.xml')]\n                for n in slide_names:\n                    try:\n                        content = zf.read(n)\n                        try:\n                            text_all += content.decode('utf-8', errors='ignore') + \"\\n\"\n                        except Exception:\n                            text_all += str(content) + \"\\n\"\n                    except Exception:\n                        continue\n        except Exception as e:\n            # Cannot read slides; minimal score for extension\n            return 0.2, f\"PPTX detected; could not read slide XML: {e}\"\n    else:\n        return 0.2, \"zipfile unavailable; cannot parse slides\"\n\n    low = text_all.lower()\n    tokens = [\n        (r\"bridgemind ai\", False),\n        (r\"bridge\\s*mind\", False),\n        (r\"common\\s*ground\\s*bikes\", False),\n        (r\"oxfordshire\", False),\n        (r\"iuk6060_bike\", False),\n        (r\"30\\s*october\\s*2025|30th\\s*october\\s*2025|30\\s*oct\\s*2025\", True)\n    ]\n    found = 0\n    total = len(tokens)\n    for pat, is_date in tokens:\n        if re.search(pat, low):\n            found += 1\n    score = found / total if total else 0.0\n    feedback = f\"Found {found}/{total} key tokens\"\n    return score, feedback"}, {"type": "llm_judge", "name": "Cross-slide Alignment and Traceability", "description": "Check that Slide 3\u2019s enumerated contents map correctly to Slides 4\u20139 and that Slide 2 provides an accurate high-level summary of those sections.", "weight": 2.8, "judge_prompt": "Evaluate the internal consistency of the deck:\n- Does Slide 3 list the required basic descriptors (Date of Report = 30th October 2025; Supplier Name = Bridge Mind; Proposal Title; Proposal Number = IUK6060_BIKE) followed by a numbered list that matches Slides 4\u20139: 1) Progress Summary, 2) Project Spend to date, 3) Risk Review, 4) Current Focus, 5) Auditor Q&A, 6) ANNEX A \u2013 Project Summary?\n- Do the subsequent slides\u2019 titles/content match those labels? Minor wording variations are acceptable if the intent is clear.\n- Does Slide 2 provide a concise high-level overview that reflects the content later presented (i.e., progress themes, spend status, risks, and current focus), without introducing contradictions?\n\nScoring:\n- 1.0: Clear, accurate mapping from Slide 3 to Slides 4\u20139 and Slide 2 faithfully reflects later content.\n- 0.7: Mostly aligned with small inconsistencies or omissions.\n- 0.4: Several mismatches but overall still somewhat traceable.\n- 0.0: Poor or no alignment; overview/contents do not match subsequent slides.", "expectation": "Slide 3 serves as a faithful table of contents and Slide 2 accurately summarizes the following sections."}, {"type": "llm_judge", "name": "Financial Presentation and Separation", "description": "Verify that Slide 4 excludes financial figures while Slide 5 includes a clear spend-to-date summary with currency notation and totals.", "weight": 2.6, "judge_prompt": "Assess how the deck handles the financial information versus non-financial progress:\n- Slide 4 (Progress Summary) should avoid financial figures (no \u00a3/GBP amounts). It should focus on activities/milestones/deliverables.\n- Slide 5 (Project Spend to date) should include visible financial figures (e.g., \u00a3 values, totals, or budget vs. actual to date), presented clearly.\n- Slide 5 should read as a summary suitable for a grant assessor (e.g., totals, categories, or a concise table/chart). Minor formatting variations are acceptable.\n\nScoring:\n- 1.0: Clear separation; Slide 4 has no financial figures; Slide 5 presents spend with currency and totals.\n- 0.7: Generally separated; small spillover of financial terms on Slide 4 or weak clarity on Slide 5.\n- 0.4: Noticeable mixing or unclear spend representation.\n- 0.0: No clear spend slide or financials missing/indistinct.", "expectation": "Non-financial progress is on Slide 4, and Slide 5 cleanly summarizes spend-to-date with \u00a3 amounts and totals."}, {"type": "llm_judge", "name": "Risk Review and Current Focus Fidelity", "description": "Check that risks are presented in a structured way (e.g., table with risk, likelihood, impact, mitigation, owner/RAG) and that current focus provides actionable bullets derived from a project log context.", "weight": 1.6, "judge_prompt": "Evaluate Slides 6\u20137:\n- Slide 6 (Risk Review): Should present a structured summary of risks (table-like or list with fields such as Risk, Likelihood, Impact, Mitigation, Owner, and optionally RAG). Layout can vary but the elements should be visible.\n- Slide 7 (Current Focus): Should list current priorities or work items (3\u20136 bullets typical), sounding like they come from a project log or sprint focus.\n\nScoring:\n- 1.0: Both slides meet intent clearly (structured risk summary; actionable, current focus bullets).\n- 0.7: Minor gaps (e.g., missing one field in risks or thin current focus).\n- 0.4: Partial structure with limited actionability or sparse risk detail.\n- 0.0: One or both slides missing or not serving intended purpose.", "expectation": "A comprehensible risk table/list and a specific, actionable current focus list."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality Assessment (Professionalism and Impact)", "description": "Holistic evaluation of presentation quality, clarity for a grant assessor audience, and usefulness of insights.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Assessor Readiness and Executive Clarity", "description": "Is the deck immediately useful to a grant assessor? Clear headline messages, key highlights, and understandable at-a-glance context.", "weight": 1.5, "judge_prompt": "Consider the deck from the viewpoint of a funding assessor:\n- Are key messages and status immediately clear (e.g., progress highlights, headline spend status, top risks)?\n- Is the Annex Summary sufficient to quickly grasp the project purpose and scope (BridgeMind AI for bicycle maintenance inventory management in Oxfordshire/Common Ground Bikes)?\n- Are next steps or asks implied/clear (e.g., Q&A slide invites dialogue)?\n\nScoring: 1.0 excellent; 0.7 good; 0.4 fair; 0.0 poor.", "expectation": "A crisp, assessor-ready deck with clear takeaways and quick comprehension."}, {"type": "llm_judge", "name": "Professional Design and Readability", "description": "Visual polish, consistent formatting, legible charts/tables, correct spelling/grammar (UK English acceptable).", "weight": 1.5, "judge_prompt": "Evaluate presentation craftsmanship:\n- Are slide titles consistent and informative?\n- Are tables/charts legible with proper labels and units (\u00a3 for currency)?\n- Is formatting consistent (fonts, colors, spacing), and spelling/grammar professional (UK variants acceptable)?\n\nScoring: 1.0 excellent; 0.7 good; 0.4 fair; 0.0 poor.", "expectation": "Clean, consistent slides with readable visuals and professional language."}, {"type": "llm_judge", "name": "UK Context and Compliance Tone", "description": "Appropriate UK context (currency, spelling) and a tone aligned with Innovate UK grant reporting expectations.", "weight": 1.5, "judge_prompt": "Assess alignment to the UK grant context:\n- Currency shown as \u00a3/GBP where appropriate; dates formatted in a UK-consistent style (e.g., 30 October 2025 acceptable).\n- Tone is factual, transparent, and suitable for Innovate UK/assessor review.\n- Mentions of Oxfordshire/Common Ground Bikes/Bridge Mind are contextually correct.\n\nScoring: 1.0 excellent; 0.7 good; 0.4 fair; 0.0 poor.", "expectation": "A UK-appropriate, compliance-oriented tone and formatting."}, {"type": "llm_judge", "name": "Data Storytelling and Actionability", "description": "Coherence of narrative across slides and clarity of actions/next steps.", "weight": 1.5, "judge_prompt": "Evaluate the narrative and actionability:\n- Does the story flow from overview to details (progress, spend, risk, focus) and end with Q&A and a clear Annex summary?\n- Are key actions, decisions, or next steps identifiable?\n\nScoring: 1.0 excellent; 0.7 good; 0.4 fair; 0.0 poor.", "expectation": "A coherent storyline that leads to clear actions/next steps appropriate for month 2 of a 6-month POC."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "8314d1b1-5b0f-42a4-b5d5-91c0867b0913", "rubric": {"category_name": "Legal Memorandum - Delaware Controller Transaction Review", "rationale": "This rubric enforces a self-documenting, file-based deliverable: a DOCX/PDF legal memo with a precise section structure. Stage 1 (LLM-only) gates on shape so later checks are trivial. Stage 2 mixes light code rules (deterministic bounds and presence checks) with heavier LLM rules (substantive legal correctness, with LLM weight ~5x code). Stage 3 assesses professional quality, clarity for a sophisticated non-lawyer, and actionability.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Document Shape Enforcement (GATE)", "description": "Confirm the output is a DOCX/PDF legal memo addressed to Elias with the mandated sections and structural elements enabling verification.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Document Format and Structural Completeness", "description": "Verify the memo is a DOCX/PDF, addressed to Elias Veynor, and includes the required sections and subsections enabling verification.", "weight": 4.0, "judge_prompt": "You are grading a legal memorandum's structure only (not content quality). Inspect the primary output file.\n\nCheck these STRUCTURE requirements (be flexible with naming variants; judge by visible headers):\nFormat:\n- File must be a DOCX or PDF (not plain text, not Excel).\n- Professionally formatted memorandum with headings/subheadings. Page count is not critical; only ensure it is a full memo, not an outline.\n\nAddressing:\n- Addressed to Elias Veynor by name somewhere prominent (e.g., header, To/Recipient line, opening address).\n- Include a title/subject line or header indicating the topic (e.g., proposed acquisition of Luminor Dynamics by Clarivon Group).\n\nRequired top-level sections (exact names or close synonyms acceptable):\n1) \"Introduction\"\n2) \"Executive Summary\" (or \"Summary\")\n3) \"Analysis\"\n4) \"Conclusion\"\n\nRequired Analysis subsections (accept close synonyms, flexible ordering):\n- \"Common Law Framework\" (Delaware judicial review of controller/conflict transactions)\n- \"DGCL Amendments (March 2025)\" (or similar, e.g., \"2025 Amendments to DGCL\", specifically referencing \u00a7 144 changes)\n- \"Application to Facts\"\n- \"Recommendations\" or \"Risk Mitigation Recommendations\"\n\nCitations presence (structure only):\n- Visible legal citations indicating cases (e.g., a case name with \"v.\") and statutes (e.g., DGCL \u00a7 144 or similar). Do not judge citation correctness here, only that they exist.\n\nScoring (0 to 4):\n- 4.0: Valid DOCX/PDF AND addressed to Elias Veynor AND all four top-level sections present AND all four analysis subsections present AND citations visibly included.\n- 3.0: Valid DOCX/PDF AND addressed to Elias AND all four top-level sections present, but exactly one analysis subsection missing OR citations not clearly visible.\n- 2.0: Valid DOCX/PDF with 3 of 4 top-level sections present OR 2+ analysis subsections missing, but memo is otherwise recognizable.\n- 1.0: Valid DOCX/PDF but only 1\u20132 top-level sections are present OR it lacks clear memo structure.\n- 0.0: Not a DOCX/PDF OR not a memo (e.g., outline/notes) OR totally missing required sections.\n\nOnly evaluate structural presence and formatting. Do not evaluate legal accuracy or depth here.", "expectation": "A well-structured DOCX/PDF memo addressed to Elias with Introduction, Executive Summary, Analysis, Conclusion; and within Analysis: Common Law Framework, DGCL 2025 Amendments (\u00a7144), Application to Facts, and Recommendations; plus visible citations."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness Verification (Substance)", "description": "Verify legal correctness and factual application given the mandated structure. Code rules do deterministic checks; LLM judges assess nuanced legal reasoning.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Word Count Within Limit (\u2264 3,500)", "description": "Ensure the memo does not exceed 3,500 words.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float or (float, str)\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n    text = None\n    err = None\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception as e:\n        err = e\n    if not text:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception as e2:\n            err = e2\n    if not text:\n        return 0.0, f\"Unable to read document text: {err}\"\n    words = re.findall(r\"\\b\\w+\\b\", text)\n    count = len(words)\n    if count <= 3500:\n        return 0.6, f\"Word count OK: {count}.\"\n    else:\n        return 0.0, f\"Word count exceeds 3500 (found {count}).\""}, {"type": "code", "name": "Key Parties and Context Present", "description": "Check mentions of key parties/facts enabling correct application: Elias Veynor, Clarivon Group, Luminor Dynamics, controller/dual-class context.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n    text = None\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0, \"Unable to read document text.\"\n    t = text.lower()\n    checks = {\n        'elias_veynor': ('elias' in t and 'veynor' in t),\n        'clarivon_group': ('clarivon' in t and 'group' in t),\n        'luminor_dynamics': ('luminor' in t and 'dynamics' in t),\n        'controller_or_dual_class': (('controller' in t) or ('control' in t) or ('dual-class' in t) or ('dual class' in t) or ('class b' in t) or ('super-voting' in t) or ('super voting' in t))\n    }\n    score = sum(1 for v in checks.values() if v)\n    weight = 0.6\n    return (weight * score / 4.0, f\"Presence checks: {checks}\")"}, {"type": "code", "name": "Citation Presence and Variety", "description": "Detect typical Delaware case/statute citation patterns and specific reference to DGCL \u00a7 144 and March 2025 timing.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n    text = None\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0, \"Unable to read document text.\"\n    t = text\n    t_low = t.lower()\n    # Heuristics for legal citations\n    has_case_v = bool(re.search(r\"\\b[vV]\\.\\b\", t))\n    has_del_reporter = bool(re.search(r\"\\bA\\.(?:2|3)d\\b|Del\\. Ch\\.|Del\\. Supr\\.\", t))\n    has_stat_144 = (('\u00a7' in t and '144' in t) or ('dgcl' in t_low and '144' in t_low) or ('8 del. c' in t_low and '144' in t_low))\n    has_march_2025 = ('march 2025' in t_low or '2025' in t_low)\n    checks = {\n        'case_v': has_case_v,\n        'delaware_reporter_cite': has_del_reporter,\n        'dgcl_144': has_stat_144,\n        'mentions_2025': has_march_2025\n    }\n    score = sum(1 for v in checks.values() if v)\n    weight = 0.8\n    return (weight * score / 4.0, f\"Citation signals: {checks}\")"}, {"type": "llm_judge", "name": "Common Law Standard of Review \u2014 Correctness", "description": "Assess whether the memo correctly explains Delaware common law for controller/conflict transactions (entire fairness vs business judgment; MFW pathway; Corwin limits) and burdens.", "weight": 3.0, "judge_prompt": "Evaluate the legal accuracy of the memo's explanation of Delaware common law standards of review for conflicted controller transactions:\n\nLook for:\n- Recognition that a controller on both sides (or receiving non-ratable benefit) generally triggers entire fairness review absent effective procedural protections.\n- Proper articulation of the Weinberger entire fairness test (fair dealing and fair price) and burden-shifting principles (e.g., Kahn v. Lynch).\n- Accurate description of Kahn v. M&F Worldwide (MFW) as a pathway to business judgment review if BOTH an independent, empowered special committee AND an uncoerced, informed majority-of-the-minority stockholder vote are established ab initio and satisfied.\n- Appropriate treatment of Corwin cleansing (stockholder ratification) and why it generally does not cleanse conflicted controller transactions.\n- Attention to the dual-class/super-voting facts indicating controller status and how that affects standard of review.\n\nScoring (0 to 3):\n- 3.0: Correct, complete, and internally consistent explanation with appropriate case references (e.g., Weinberger, Kahn v. Lynch, MFW, Corwin) and no material legal misstatements.\n- 2.0: Mostly correct with minor omissions or minor imprecision; no material errors.\n- 1.0: Partially correct but missing key elements or containing a notable misstatement.\n- 0.0: Materially incorrect or fundamentally misunderstands the framework.", "expectation": "Clear, accurate mapping of controller/conflict transactions to entire fairness vs BJR under MFW; correct explanation of Weinberger, Lynch, MFW, and Corwin boundaries."}, {"type": "llm_judge", "name": "DGCL March 2025 Amendments \u2014 Correctness and Impact", "description": "Assess whether the memo correctly describes the March 2025 DGCL amendments (including \u00a7 144 changes) and their interaction with the common law standards.", "weight": 2.5, "judge_prompt": "Evaluate the accuracy and coherence of the memo's discussion of the March 2025 amendments to Delaware's General Corporation Law, including changes to DGCL \u00a7 144, and how those changes interact with Delaware common law standards of review.\n\nLook for:\n- A clear summary of what changed in March 2025 (especially \u00a7 144) and the purpose/scope of those changes.\n- An explanation of how statutory safe harbors or cleansing provisions relate to or differ from common law standards (e.g., they may affect liability or burden but do not automatically dictate entire fairness vs business judgment unless conditions are met).\n- Identification of any new requirements or clarifications affecting conflicted transactions approval processes.\n- Use of primary sources (statute text, official releases) or reputable secondary sources; internal consistency; avoidance of speculation.\n\nScoring (0 to 2.5):\n- 2.5: Accurate, specific, and non-misleading explanation with clear linkage to \u00a7 144 and how it interfaces with common law; cites plausible primary sources.\n- 1.5: Generally accurate but somewhat thin on specifics or linkage; no major errors.\n- 0.5: Vague or partially incorrect; limited linkage to standards of review.\n- 0.0: Materially inaccurate or misleading regarding the amendments.", "expectation": "A cogent, accurate description of the 2025 DGCL/\u00a7144 changes and their practical effect on cleansing/approval frameworks."}, {"type": "llm_judge", "name": "Application to Facts and Risk-Reduction Steps", "description": "Assess whether the memo correctly applies the standards to Elias/Clarivon/Luminor facts and provides concrete, process-focused recommendations to mitigate risk.", "weight": 2.5, "judge_prompt": "Evaluate how well the memo applies the legal principles to the presented facts (Clarivon Group acquiring Luminor Dynamics, Elias as controller via dual-class/super-voting) and presents risk mitigation steps.\n\nLook for:\n- Clear identification of controller conflict and expected baseline standard of review.\n- Analysis of how adopting the MFW framework (independent, empowered special committee and informed, uncoerced majority-of-the-minority vote, set ab initio) could shift review to business judgment and reduce litigation risk.\n- Concrete recommendations: committee composition/mandate, selection of advisors, bargaining process, disclosure practices, majority-of-the-minority conditions, recusal, information barriers, fairness opinion, possible market check/go-shop, documentation of process, timing, and sequencing.\n- Discussion of residual risks and limitations even if procedures are followed.\n\nScoring (0 to 2.5):\n- 2.5: Strong, tailored application with specific, actionable steps and sound reasoning.\n- 1.5: Solid but somewhat generic; steps are present but not fully tailored.\n- 0.5: Minimal application; vague recommendations.\n- 0.0: No meaningful application or recommendations.", "expectation": "Fact-specific application with a practical, stepwise risk mitigation plan aligned to MFW and disclosure best practices."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Writing Quality and Professional Value", "description": "Holistic quality assessment for a sophisticated non-lawyer client: clarity, tone, usefulness, and citation hygiene.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity and Organization for a Sophisticated Non-Lawyer", "description": "Assesses readability, logical flow, and accessibility while preserving legal rigor.", "weight": 1.5, "judge_prompt": "Evaluate how clearly the memo communicates complex Delaware corporate law to a sophisticated but non-lawyer client.\n\nConsider: clear roadmap, well-labeled headings, concise explanations, minimal jargon or defined terms, use of summaries/mini-conclusions, and readability of long sections.\n\nScore 0\u20131.5: 1.5 = exceptionally clear and well-structured; 1.0 = generally clear with minor issues; 0.5 = somewhat dense or disorganized; 0.0 = confusing or poorly structured.", "expectation": "Well-structured, easy-to-follow memo with clear headings and succinct explanations."}, {"type": "llm_judge", "name": "Professional Tone and Objectivity", "description": "Assesses neutrality, professionalism, and balanced presentation of risks and options.", "weight": 1.0, "judge_prompt": "Assess whether the memo maintains a professional, neutral, and objective tone suitable for a client memorandum, presenting benefits and risks without advocacy bias.\n\nScore 0\u20131.0: 1.0 = consistently neutral and professional; 0.5 = minor tone slips; 0.0 = overly promotional or biased.", "expectation": "Neutral, objective tone with balanced risk discussion."}, {"type": "llm_judge", "name": "Actionability and Practical Guidance", "description": "Assesses whether the memo offers specific, prioritized, and sequenced steps that Elias can follow.", "weight": 2.0, "judge_prompt": "Evaluate the practical utility of the recommendations.\n\nConsider: specificity (who does what, when), prioritization/sequence (e.g., ab initio conditions), feasibility, and alignment with legal standards (independent committee, MoM vote, disclosure, advisor selection, record-building, timing).\n\nScore 0\u20132.0: 2.0 = highly actionable, prioritized, and tailored; 1.0 = useful but generic; 0.0 = vague or inactionable.", "expectation": "Concrete, prioritized plan aligned with the legal analysis."}, {"type": "llm_judge", "name": "Citation and Sourcing Hygiene", "description": "Assesses whether citations are consistently formatted and sufficient to locate sources (cases, statutes, secondary sources/URLs).", "weight": 1.5, "judge_prompt": "Review the memo\u2019s citations for consistency and sufficiency: recognizable case names with reporters/courts, DGCL provisions with section numbers, and links/URLs where feasible. Do not re-check legal correctness; focus on citation clarity and completeness.\n\nScore 0\u20131.5: 1.5 = consistent and complete; 1.0 = generally fine with minor inconsistencies; 0.5 = several issues; 0.0 = poor or missing citations.", "expectation": "Consistent, findable citations enabling verification."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "46fc494e-a24f-45ce-b099-851d5c181fd4", "rubric": {"category_name": "Thermal Screening Analysis \u2013 C/SiC Forward-Edge Protection (Mechanical Engineering)", "rationale": "This rubric enforces a self-documenting, mixed-output workflow centered on a single, verifiable Excel workbook that contains all structured data, calculations, plots, and a concise report sheet. Stage 1 (LLM-only) mandates an exact workbook shape so Stage 2 rules know where to look. Stage 2 mixes lightweight code checks (bounds/structure) with heavier LLM verification (physics consistency, cross-references, conclusions and mitigations). Stage 3 judges overall professional quality, clarity, and strategic value. Code rules are intentionally lower-weight than LLM rules (\u22485x less on average) to reflect the complexity of engineering judgment.", "max_total_score": 15.0, "stages": [{"name": "Stage 1 \u2013 Shape Enforcement Gate (Workbook Structure)", "description": "Gate: Verify the candidate delivers a single Excel workbook as the primary output with required sheets, tables, and embedded plots so downstream verification is trivial. No correctness checks here\u2014only structure and presence.", "is_required": true, "max_points": 1.0, "min_score_to_pass": 0.7, "rules": [{"type": "llm_judge", "name": "Structured Workbook Requirement", "description": "Confirm the primary output is an Excel workbook that includes all required sheets, tables, and embedded charts for this thermal screening analysis.", "weight": 1.0, "judge_prompt": "You are evaluating ONLY the STRUCTURE of the primary output. The expected deliverable is a single Excel workbook (.xlsx) that is self-contained and includes narrative, data tables, and plots.\n\nCheck the following REQUIRED structure (be flexible with sheet names, but the intent must be clear):\n\n1) Sheet: \"Report\" (or similar: Report, Summary, Findings)\n   - Contains a concise report with clear headers/sections covering: Executive Summary, Model Setup (inputs and boundary conditions), Results, Recommendation/Mitigations.\n   - Plots and summary tables are referenced and/or embedded.\n\n2) Sheet: \"Inputs\" (or similar: Model Inputs, Parameters)\n   - A table with columns like: Parameter | Value | Units | Notes/Source.\n   - Must include the given parameters: thermal conductivity, density, specific heat, external gas temperature, external convective coefficient, internal ambient temperature, internal convective coefficient, node spacing, and number of nodes (22).\n\n3) Sheet: \"Node Temperature Profiles\" (or similar)\n   - A table listing Node Index (1..22) and temperature columns for the requested times: 0.5, 5, 10, and 20 minutes (e.g., columns like T_0.5min, T_5min, etc.).\n   - At least one profile plot/chart present on this sheet (or an obviously associated sheet) showing temperature vs. node index for the requested times.\n\n4) Sheet: \"Time Traces\" (or similar)\n   - A table with columns: Time (in minutes) and traces for representative nodes: Node 1, Node 13, Node 22 (e.g., T_Node1, T_Node13, T_Node22).\n   - At least one time-series plot/chart present on this sheet.\n\n5) Sheet: \"Back-Face Summary\" (or similar)\n   - A summary table giving: identified back-face node, maximum back-face temperature during the event, the 150 \u00b0C limit, and the computed margin to limit. Also include an explicit PASS/FAIL verdict.\n\n6) Sheet: \"Contour 20min\" (or similar)\n   - An isotherm/contour-style representation at 20 minutes. This can be an embedded heatmap/contour chart or a clearly labeled image in the sheet.\n\nScoring:\n- 1.0: Excel workbook present and all 6 sheets present with appropriate tables/plots in place.\n- 0.8: Workbook present and all required sheets present but one sheet is missing its plot, or the contour is represented in a clearly acceptable alternative form.\n- 0.5: Workbook present but exactly one required sheet is missing or clearly incomplete.\n- 0.0: Not an Excel workbook OR multiple required sheets are missing.\n\nOnly assess presence and structure. Do NOT judge numerical correctness or content quality.", "expectation": "A single Excel workbook containing the specified sheets with clearly labeled tables/charts so verification is straightforward."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness and Consistency Verification", "description": "Using the enforced structure, verify physical parameters, data presence, cross-sheet consistency, correct limit/margin logic, and use of boundary conditions. Mix light code checks with heavier LLM judgment.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 4.0, "rules": [{"type": "code", "name": "Inputs Sanity Check (Parameters Match Spec)", "description": "Verify that the Inputs sheet contains the required parameters and their values are close to the specified values with reasonable tolerance.", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"Primary output is not a spreadsheet.\"\n    # Helper to find a sheet by fuzzy name\n    def find_sheet(xls, keywords):\n        for s in xls.sheet_names:\n            low = s.lower()\n            if any(k in low for k in keywords):\n                return s\n        return None\n    def to_float(x):\n        if x is None or (isinstance(x, float) and np.isnan(x)):\n            return None\n        if isinstance(x, (int, float, np.number)):\n            return float(x)\n        m = re.search(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\", str(x))\n        return float(m.group(0)) if m else None\n    try:\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n        s_inputs = find_sheet(xls, [\"input\", \"parameter\", \"model\"])\n        if not s_inputs:\n            return 0.0, \"Inputs sheet not found.\"\n        df = pd.read_excel(file_path, sheet_name=s_inputs)\n        if df.empty:\n            return 0.0, \"Inputs sheet is empty.\"\n        # Identify parameter and value columns flexibly\n        cols_lower = [str(c).lower() for c in df.columns]\n        # Choose likely parameter column\n        try:\n            pcol = next(c for c in df.columns if any(k in str(c).lower() for k in [\"param\", \"name\"]))\n        except StopIteration:\n            pcol = df.columns[0]\n        try:\n            vcol = next(c for c in df.columns if \"value\" in str(c).lower())\n        except StopIteration:\n            vcol = df.columns[1] if len(df.columns) > 1 else df.columns[0]\n        # Build mapping\n        kv = {}\n        for _, row in df.iterrows():\n            key = str(row.get(pcol, \"\")).strip().lower()\n            val = to_float(row.get(vcol, None))\n            if key:\n                kv[key] = val\n        def find(keys):\n            for k, v in kv.items():\n                for t in keys:\n                    if t in k:\n                        return v\n            return None\n        checks = []\n        # Expected values\n        ex = {\n            'k': (['conductivity', 'k'], 5.0, 0.15),               # \u00b115%\n            'rho': (['density', 'rho'], 2200.0, 0.15),             # \u00b115%\n            'cp': (['specific heat', 'cp'], 800.0, 0.20),          # \u00b120%\n            'text': (['external', 'hot', 'freestream', 't_ext'], 700.0, 0.10),  # \u00b110%\n            'hext': (['external', 'hot', 'h_ext', 'outside'], 1200.0, 0.20),    # \u00b120%\n            'tint': (['internal', 'ambient', 'bay', 't_int'], 25.0, 0.20),      # \u00b120%\n            'hint': (['internal', 'h_int', 'inside'], 15.0, 0.30),              # \u00b130%\n            'dx': (['spacing', 'node spacing', 'dx'], 0.05, 0.20),              # \u00b120%\n            'n': (['nodes', 'n=', 'count'], 22.0, 0.01)                          # exact-ish\n        }\n        passed = 0\n        total = 0\n        details = []\n        for key, (keys, exp, tol) in ex.items():\n            total += 1\n            v = find(keys)\n            if v is None:\n                details.append(f\"Missing {key}\")\n                continue\n            # Special case: number of nodes should be an integer near 22\n            if key == 'n':\n                ok = abs(v - 22.0) <= 0.5\n            else:\n                ok = (v is not None) and (abs(v - exp) <= abs(exp) * tol)\n            if ok:\n                passed += 1\n            else:\n                details.append(f\"{key}={v} deviates from {exp}\")\n        score = passed / max(total, 1)\n        fb = f\"Inputs checks passed {passed}/{total}. \" + (\"; \".join(details) if details else \"\")\n        return score, fb\n    except Exception as e:\n        return 0.0, f\"Error reading inputs: {e}\""}, {"type": "code", "name": "Node Temperature Profiles \u2013 Structure and Plausibility", "description": "Verify the profiles sheet has 22 node rows, required time columns (0.5, 5, 10, 20 min), and reasonable temperature bounds.", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"Primary output is not a spreadsheet.\"\n    def find_sheet(xls, keywords):\n        for s in xls.sheet_names:\n            low = s.lower()\n            if any(k in low for k in keywords):\n                return s\n        return None\n    def extract_time(cname):\n        # return time in minutes if header contains a number\n        m = re.findall(r\"\\d*\\.?\\d+\", str(cname))\n        if not m:\n            return None\n        try:\n            return float(m[0])\n        except:\n            return None\n    try:\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n        s_prof = find_sheet(xls, [\"profile\", \"node temp\", \"vs node\", \"node temperature\"])\n        if not s_prof:\n            return 0.0, \"Profiles sheet not found.\"\n        df = pd.read_excel(file_path, sheet_name=s_prof)\n        if df.empty:\n            return 0.0, \"Profiles sheet is empty.\"\n        # Identify node column\n        node_col = None\n        for c in df.columns:\n            if 'node' in str(c).lower():\n                node_col = c\n                break\n        if node_col is None:\n            node_col = df.columns[0]\n        # Required times\n        required_times = [0.5, 5.0, 10.0, 20.0]\n        time_cols = {}\n        for c in df.columns:\n            if c == node_col: \n                continue\n            t = extract_time(c)\n            if t is None:\n                continue\n            for rt in required_times:\n                if abs(t - rt) < 0.11:  # allow small header rounding\n                    time_cols[rt] = c\n        # Checks\n        checks = 3\n        passed = 0\n        details = []\n        # Row count / node coverage\n        nodes = pd.to_numeric(df[node_col], errors='coerce').dropna().astype(int).tolist()\n        unique_nodes = sorted(set(nodes))\n        if len(unique_nodes) == 22:\n            passed += 1\n        elif len(unique_nodes) >= 20:\n            passed += 0.5\n            details.append(f\"Only {len(unique_nodes)} unique nodes found\")\n        else:\n            details.append(f\"Insufficient node rows: {len(unique_nodes)}\")\n        # Time columns present\n        present = sum(1 for rt in required_times if rt in time_cols)\n        passed += present / 4.0\n        if present < 4:\n            details.append(f\"Missing {4 - present} required time columns\")\n        # Temperature bounds\n        ok_bounds = True\n        for rt, col in time_cols.items():\n            vals = pd.to_numeric(df[col], errors='coerce').dropna()\n            if vals.empty:\n                ok_bounds = False\n                break\n            if (vals.min() < -50) or (vals.max() > 1500):\n                ok_bounds = False\n                break\n        if ok_bounds:\n            passed += 1\n        else:\n            details.append(\"Temperature values out of plausible bounds\")\n        score = max(0.0, min(1.0, passed / checks))\n        fb = \"; \".join(details) if details else \"Profiles structure OK\"\n        return score, fb\n    except Exception as e:\n        return 0.0, f\"Error checking profiles: {e}\""}, {"type": "code", "name": "Time Traces \u2013 Coverage and Continuity", "description": "Verify time-series sheet includes Time (min), traces for Nodes 1, 13, 22, includes the required times, monotonic time, and last time >= 20 min.", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"Primary output is not a spreadsheet.\"\n    def find_sheet(xls, keywords):\n        for s in xls.sheet_names:\n            low = s.lower()\n            if any(k in low for k in keywords):\n                return s\n        return None\n    try:\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n        s_tr = find_sheet(xls, [\"trace\", \"time series\", \"time\", \"transient\"])\n        if not s_tr:\n            return 0.0, \"Time traces sheet not found.\"\n        df = pd.read_excel(file_path, sheet_name=s_tr)\n        if df.empty:\n            return 0.0, \"Time traces sheet is empty.\"\n        # Time column\n        time_col = None\n        for c in df.columns:\n            if 'time' in str(c).lower():\n                time_col = c\n                break\n        if time_col is None:\n            time_col = df.columns[0]\n        t = pd.to_numeric(df[time_col], errors='coerce').dropna().values\n        if t.size == 0:\n            return 0.0, \"No numeric time data.\"\n        # Required node columns\n        def find_col_for_node(n):\n            for c in df.columns:\n                s = str(c).lower()\n                if ('node' in s or 't_' in s) and str(n) in s:\n                    return c\n            return None\n        cols = {n: find_col_for_node(n) for n in [1, 13, 22]}\n        present_nodes = sum(1 for v in cols.values() if v is not None)\n        # Checks: nodes present, required times present, monotonic time and last >=20\n        checks = 3\n        passed = 0\n        details = []\n        # Node columns\n        passed += present_nodes / 3.0\n        if present_nodes < 3:\n            details.append(f\"Missing {3 - present_nodes} node trace columns\")\n        # Required time stamps\n        req = [0.5, 5.0, 10.0, 20.0]\n        present_req = 0\n        for r in req:\n            if np.any(np.isfinite(t) & (np.abs(t - r) < 1e-2)):\n                present_req += 1\n        passed += present_req / 4.0\n        if present_req < 4:\n            details.append(f\"Missing {4 - present_req} required time stamps\")\n        # Monotonic and coverage\n        is_mono = np.all(np.diff(t) >= -1e-9)\n        has_20 = np.nanmax(t) >= 19.99\n        if is_mono and has_20:\n            passed += 1\n        else:\n            if not is_mono:\n                details.append(\"Time not monotonic\")\n            if not has_20:\n                details.append(\"Max time < 20 min\")\n        score = max(0.0, min(1.0, passed / checks))\n        fb = \"; \".join(details) if details else \"Time traces OK\"\n        return score, fb\n    except Exception as e:\n        return 0.0, f\"Error checking time traces: {e}\""}, {"type": "llm_judge", "name": "Boundary Conditions and Method Consistency", "description": "Verify the workbook and report explicitly use the specified material properties and convective boundary conditions, and a 22-node transient conduction model consistent with the Inputs sheet.", "weight": 2.0, "judge_prompt": "Check that the workbook (especially Report, Inputs, and Methodology/notes inside the Report) clearly indicates:\n- Material properties approximately: k\u22485 W/m\u00b7K, density\u22482200 kg/m\u00b3, cp\u2248800 J/kg\u00b7K.\n- Convective boundary conditions: external gas temperature \u2248700 \u00b0C with h\u22481200 W/m\u00b2\u00b7K; internal ambient \u224825 \u00b0C with h\u224815 W/m\u00b2\u00b7K.\n- Discretization and model size: a 22-node transient conduction model, with node spacing \u22480.05 m.\n- That these parameters are actually the ones used for the calculations presented (not just copied text).\n\nScoring:\n- 1.0: All items explicitly stated and consistent across sheets/plots.\n- 0.7: Minor omissions but overall consistent and clearly used.\n- 0.4: Several items unclear or inconsistent; usage uncertain.\n- 0.0: Requirements not addressed or inconsistent.", "expectation": "Clear, consistent use of the given parameters and boundary conditions in the actual analysis."}, {"type": "llm_judge", "name": "Back-Face Limit Check and Mitigation Logic", "description": "Validate that the back-face node is identified, the maximum back-face temperature and margin to 150 \u00b0C are correctly summarized, PASS/FAIL is stated, and if margin <10 \u00b0C, mitigations are recommended.", "weight": 2.5, "judge_prompt": "Review the Back-Face Summary sheet and the Report sheet.\nConfirm:\n1) The back-face node is clearly identified (e.g., Node 22 or as defined).\n2) The maximum back-face temperature over the event is labeled and a margin vs. 150 \u00b0C limit is computed (margin = 150 \u2212 T_max_backface).\n3) A clear PASS/FAIL is provided based on the margin (PASS if margin \u2265 0; FAIL if negative).\n4) If the margin is under 10 \u00b0C (even if still PASS), the report recommends mitigations (e.g., thicker panel, improved coatings, increased internal convection) with brief rationale.\n\nScoring:\n- 1.0: All four items clearly and correctly addressed.\n- 0.7: One item weak/missing.\n- 0.4: Two items weak/missing.\n- 0.0: Three or more items missing/incorrect.", "expectation": "A decisive verdict plus appropriate mitigation advice when margin <10 \u00b0C."}, {"type": "llm_judge", "name": "Plots Completeness and Correspondence", "description": "Ensure required plots are present and consistent with the data tables: node profiles at requested times, time-trace plots for nodes 1/13/22, and a 20-min contour/isotherm.", "weight": 1.5, "judge_prompt": "Check that the workbook includes the following plots and that they correspond to the data tables:\n- Node temperature profile plots for 0.5, 5, 10, and 20 minutes (temperature vs. node index). Lines/markers should match the profile table values at those times.\n- Time-trace plots for nodes 1, 13, and 22 (temperature vs. time). Curves should match values in the Time Traces table.\n- A contour/isotherm visualization at 20 minutes (may be a heatmap/contour on a rectangular layout). It should be consistent with the 20-minute profile values.\n\nAlso check for basic physical reasonableness (e.g., hottest near the hot face, cooler near the back face; temperatures generally increase over time at the back face).\n\nScoring:\n- 1.0: All plots present and consistent with tables; physically reasonable.\n- 0.7: Minor inconsistencies or one plot missing but overall consistent.\n- 0.4: Multiple issues; hard to reconcile with tables.\n- 0.0: Plots missing or not consistent.", "expectation": "All required plots are present and align with the tabulated data."}], "on_failure_action": "skip_remaining", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Professional Quality and Communication", "description": "Holistic assessment of the engineering communication, clarity, and strategic value for decision-making.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Report Clarity and Structure", "description": "Evaluate the Report sheet\u2019s structure, readability, and concision tailored to engineering stakeholders.", "weight": 1.5, "judge_prompt": "Assess the Report sheet for:\n- Clear sections (Executive Summary, Model Setup, Results, Recommendation/Mitigations).\n- Concise, readable prose tailored to an engineering manager.\n- The Executive Summary states the back-face verdict and margin in the first paragraph.\n\nScoring:\n- 1.0: Clear, concise, well-structured; summary up front.\n- 0.7: Generally clear with minor structural issues.\n- 0.4: Hard to follow; key points buried.\n- 0.0: Disorganized or missing key sections.", "expectation": "A crisp, well-structured summary with the key verdict up front."}, {"type": "llm_judge", "name": "Visualization Quality", "description": "Judge labeling, units, legends, and readability of figures in the workbook.", "weight": 1.5, "judge_prompt": "Evaluate figures for:\n- Proper axis labels and units (\u00b0C, minutes, node index).\n- Legends/titles clarifying lines (e.g., time stamps for profiles; node IDs for traces).\n- Readability (fonts, scales) and consistent styling.\n\nScoring:\n- 1.0: Professional-quality visuals; fully labeled and readable.\n- 0.7: Minor labeling/readability issues.\n- 0.4: Several issues; undermines interpretation.\n- 0.0: Poor or unlabeled plots.", "expectation": "Professional, well-labeled plots that can be read without the text."}, {"type": "llm_judge", "name": "Actionability and Strategic Insight", "description": "Evaluate whether recommendations and mitigations are actionable and tied to the findings (especially when margin <10 \u00b0C).", "weight": 1.5, "judge_prompt": "Assess whether recommendations:\n- Clearly follow from the results (e.g., thicker panel, improved coatings, increased internal convection, operational constraints).\n- Indicate estimated impact direction (e.g., thicker panel likely reduces back-face temperature).\n- Note limitations/uncertainties appropriate for a rapid screening analysis.\n\nScoring:\n- 1.0: Specific, actionable, with rationale and caveats.\n- 0.7: Useful but somewhat generic.\n- 0.4: Vague or weak linkage to results.\n- 0.0: No meaningful recommendations.", "expectation": "Specific, justified actions aligned with the data and constraints."}, {"type": "llm_judge", "name": "Reproducibility and Traceability", "description": "Check that the workbook enables reproduction: clear inputs table, methodology notes, and traceable links between inputs, tables, and plots.", "weight": 1.5, "judge_prompt": "Verify that a reader could reproduce the analysis:\n- Inputs are centralized and clearly referenced by the results.\n- Methodology or calculation notes explain the transient approach (e.g., time step, scheme) at a high level.\n- Tables/plots cite which inputs/time points they use.\n\nScoring:\n- 1.0: Reproducible with clear traceability.\n- 0.7: Mostly reproducible; minor gaps.\n- 0.4: Significant gaps.\n- 0.0: Not reproducible from the workbook.", "expectation": "A reader can retrace how results were produced from the inputs and methodology notes."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "be830ca0-b352-4658-a5bd-57139d6780ba", "rubric": {"category_name": "Lean Six Sigma Analyze Tollgate (Industrial Engineering - Manufacturing Logistics)", "rationale": "This is a Mixed task (Pattern C): a professional presentation (PowerPoint/PDF) that embeds quantitative analyses derived from a defined dataset and must include an A3 summary and DMAIC timeline. The rubric enforces a self-documenting shape: a presentation with mandated slides AND a companion Analysis Workbook (Excel) with standardized sheets and fields that make verification trivial. Stage 1 is an LLM-only gate checking structural compliance. Stage 2 mixes lightweight code checks (date bounds, presence of target and key stats) with LLM judgment for statistical interpretation and cross-consistency, weighting LLM ~5x more than code. Stage 3 assesses professional quality and Lean Six Sigma rigor.", "max_total_score": 35.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate: Deck + Analysis Workbook", "description": "Mandatory format/structure gate. The candidate MUST provide: (a) a presentation file (PPTX or PDF) containing specific slides and sections, and (b) a companion Excel \u201cAnalysis Workbook\u201d with prescribed sheets and tables. Only structural presence is checked here (no correctness).", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Presentation Structure Requirement (PPTX or PDF)", "description": "Verify the presence and structure of the Analyze tollgate presentation file with required slides/sections.", "weight": 5.0, "judge_prompt": "You are verifying STRUCTURE ONLY (not correctness). Inspect all outputs. Determine if there is a presentation file (PPTX or PDF) that includes ALL required elements below. Be flexible with exact titles, but the content/sections must be clearly present on slides.\n\nRequired Deck Elements:\n1) Slide 1: Project Charter that explicitly contains sections:\n   - Project Overview\n   - Problem/Purpose (clearly stated)\n   - Project Goal with measurable targets (must reference 3400 UPR by April 1, 2025)\n   - Project Rationale (operational + financial impact of underperforming <3400 UPR)\n   - Project Scope\n   - Project Schedule indicating when each DMAIC phase begins/ends\n2) Data provenance reference to the source dataset: \u201cProcessing Data.xlsx\u201d (LLS Dashboard) and relevant date bounds (Jan 4, 2025 through Mar 1, 2025)\n3) Five analysis charts included as slides or embedded images:\n   - One-Way ANOVA interval plot (processing rate by day-of-week)\n   - I-MR Control Chart (full Analyze range Jan 4\u2013Mar 1, 2025)\n   - Linear Regression Analysis (with Time of Day as continuous predictor; other categorical factors would require dummies if used)\n   - 1-Sample Hypothesis Test vs target of 3400 UPR\n   - Process Capability Analysis (histogram + Cp/Cpk)\n4) A3 Summary slide that contains sections: Background, Project Purpose, Current Conditions, Goals, Analysis Results, Follow-up. The \u201cAnalysis Results\u201d section must reference/collect the five charts. The \u201cCurrent Conditions\u201d section must include an I-MR chart limited to Baseline/Define/Measure dates (Jan 4\u2013Feb 21, 2025).\n5) Final slide: Project Timeline with DMAIC phases and dates (Define 2025-02-08, Measure 2025-02-15, Analyze 2025-02-22 to 2025-03-01, Improve TBD, Control TBD). Clearly indicate tollgates completed vs in-progress.\n\nScoring:\n- 5.0 = Valid PPTX or PDF AND all 5 elements above are clearly present\n- 4.0 = Missing exactly one required element\n- 3.0 = Missing two required elements\n- 2.0 = Presentation exists but only 2\u20133 elements present\n- 1.0 = Presentation exists but only 1 element present\n- 0.0 = No presentation file OR wrong format (not PPTX/PDF)\nOnly check presence/structure, not the analytic correctness.", "expectation": "A professional PPTX/PDF deck with a charter on Slide 1, five required analyses, an A3 summary, and a final timeline slide; explicitly referencing the dataset and date range."}, {"type": "llm_judge", "name": "Analysis Workbook Shape Requirement (Excel)", "description": "Verify the presence and structure of a companion Excel workbook that enables code verification in Stage 2.", "weight": 3.0, "judge_prompt": "Inspect all outputs. Confirm there is an Excel Analysis Workbook with the following sheets and structural elements (column names can be reasonably similar; be flexible but the information must be clearly captured in table form):\n\nRequired Sheets and Key Elements:\nA) Charter \u2014 fields (can be a two-column key/value table):\n   - Overview\n   - Problem/Purpose\n   - Goal (must mention 3400 UPR and target date Apr 1, 2025)\n   - Rationale (operational + financial)\n   - Scope\n   - Schedule/DMAIC dates (Define 2025-02-08, Measure 2025-02-15, Analyze 2025-02-22 to 2025-03-01, Improve TBD, Control TBD)\nB) Data Overview \u2014 table with columns like [Start Date, End Date, N, Source]. Dates must reflect the Analyze-phase data window (Jan 4, 2025 to Mar 1, 2025)\nC) ANOVA \u2014 table with columns similar to [Factor (DayOfWeek), Group N, Mean, Std, F, p-value]; note of an interval plot (image optional)\nD) IMR_Full \u2014 table with columns like [Center Line, UCL, LCL, N points, Start Date, End Date] and optionally list of out-of-control dates\nE) IMR_Baseline \u2014 same structure as IMR_Full but End Date = 2025-02-21 (Baseline/Define/Measure window from Jan 4\u2013Feb 21, 2025)\nF) Regression \u2014 table with [Term, Coef, SE Coef, T, p-value]; must include TimeOfDay predictor; and a summary block [S, R-sq, R-sq(adj)]\nG) OneSampleT \u2014 table with [Target, Sample Mean, N, StDev, T, p-value, CI Lower, CI Upper] (Target must be 3400 UPR)\nH) Capability \u2014 table with [USL, LSL (optional), Mean, StDev, Cp, Cpk]; USL must be 3400\nI) Timeline \u2014 table with [Phase, Start, End, Tollgate Status]\n\nScoring:\n- 3.0 = Excel present with ALL required sheets and key fields/tables\n- 2.0 = Excel present but missing exactly one required sheet or clearly incomplete tables\n- 1.0 = Excel present but only 2\u20133 relevant sheets populated\n- 0.0 = No Excel workbook present\nOnly check presence/structure, not correctness.", "expectation": "An Excel workbook containing standardized sheets for Charter, Data Overview, ANOVA, IMR (Full and Baseline), Regression, OneSampleT, Capability, and Timeline, enabling automated checks."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification: Data, Dates, Targets, and Statistical Interpretations", "description": "Now that the structure is in place, verify correctness and cross-consistency using code (deterministic checks) and LLM (interpretation, internal consistency, DMAIC alignment). Code rules are lightweight; LLM handles nuanced checks.", "is_required": true, "max_points": 17.0, "min_score_to_pass": 8.5, "rules": [{"type": "code", "name": "Date Range Compliance (Analyze Window)", "description": "Check that the Analysis Workbook\u2019s Data Overview sheet uses the correct overall date window (Start=2025-01-04, End=2025-03-01).", "weight": 0.6, "code": "import pandas as pd\nfrom datetime import datetime\n\ndef evaluate(workflow, context):\n    # Return between 0.0 and 1.0\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0\n    # Find an Excel workbook among outputs\n    excel = None\n    for r in outputs:\n        try:\n            if getattr(r, 'is_spreadsheet', False):\n                excel = r\n                # Prefer a likely analysis workbook by filename\n                p = context.files.get_path(r.id)\n                name = p.name.lower()\n                if any(k in name for k in ['analysis', 'analyze', 'tollgate', 'workbook']):\n                    excel = r\n                    break\n        except Exception:\n            continue\n    if not excel:\n        return 0.0\n    # Read Data Overview sheet (flexible name handling done by Stage 1 LLM). Assume 'Data Overview' exists.\n    try:\n        df = context.files.read_excel(excel.id, sheet_name='Data Overview')\n    except Exception:\n        # Try a fallback\n        try:\n            df = context.files.read_excel(excel.id, sheet_name='Overview')\n        except Exception:\n            return 0.0\n    if df is None or df.empty:\n        return 0.0\n    # Normalize columns\n    cols = {str(c).strip().lower(): c for c in df.columns}\n    start_keys = ['start date','start','min date','from']\n    end_keys = ['end date','end','max date','to']\n    def get_col(key_list):\n        for k in key_list:\n            if k in cols:\n                return cols[k]\n        return None\n    start_col = get_col(start_keys)\n    end_col = get_col(end_keys)\n    if start_col is None or end_col is None:\n        return 0.0\n    try:\n        start_val = pd.to_datetime(df[start_col].dropna().astype(str).iloc[0], errors='coerce')\n        end_val = pd.to_datetime(df[end_col].dropna().astype(str).iloc[0], errors='coerce')\n    except Exception:\n        return 0.0\n    target_start = pd.to_datetime('2025-01-04')\n    target_end = pd.to_datetime('2025-03-01')\n    score = 0.0\n    if pd.notnull(start_val) and start_val.date() == target_start.date():\n        score += 0.5\n    if pd.notnull(end_val) and end_val.date() == target_end.date():\n        score += 0.5\n    return score"}, {"type": "code", "name": "Baseline I-MR Date Window (Jan 4\u2013Feb 21, 2025)", "description": "Check IMR_Baseline sheet dates match Baseline/Define/Measure window.", "weight": 0.6, "code": "import pandas as pd\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0\n    excel = None\n    for r in outputs:\n        try:\n            if getattr(r, 'is_spreadsheet', False):\n                p = context.files.get_path(r.id)\n                name = p.name.lower()\n                excel = r if excel is None else excel\n                if any(k in name for k in ['analysis', 'analyze', 'tollgate', 'workbook']):\n                    excel = r\n                    break\n        except Exception:\n            continue\n    if not excel:\n        return 0.0\n    try:\n        df = context.files.read_excel(excel.id, sheet_name='IMR_Baseline')\n    except Exception:\n        return 0.0\n    if df is None or df.empty:\n        return 0.0\n    cols = {str(c).strip().lower(): c for c in df.columns}\n    start_col = None\n    end_col = None\n    for k in ['start date','start','from','min date']:\n        if k in cols:\n            start_col = cols[k]\n            break\n    for k in ['end date','end','to','max date']:\n        if k in cols:\n            end_col = cols[k]\n            break\n    if start_col is None or end_col is None:\n        return 0.0\n    try:\n        s = pd.to_datetime(df[start_col].dropna().astype(str).iloc[0], errors='coerce')\n        e = pd.to_datetime(df[end_col].dropna().astype(str).iloc[0], errors='coerce')\n    except Exception:\n        return 0.0\n    score = 0.0\n    if pd.notnull(s) and s.date().isoformat() == '2025-01-04':\n        score += 0.5\n    if pd.notnull(e) and e.date().isoformat() == '2025-02-21':\n        score += 0.5\n    return score"}, {"type": "code", "name": "1-Sample Test Target/P-Value/CI Presence", "description": "Verify OneSampleT uses target 3400 and includes valid p-value and confidence interval fields.", "weight": 0.6, "code": "import pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0\n    excel = None\n    for r in outputs:\n        try:\n            if getattr(r, 'is_spreadsheet', False):\n                p = context.files.get_path(r.id)\n                name = p.name.lower()\n                excel = r if excel is None else excel\n                if any(k in name for k in ['analysis', 'analyze', 'tollgate', 'workbook']):\n                    excel = r\n                    break\n        except Exception:\n            continue\n    if not excel:\n        return 0.0\n    try:\n        df = context.files.read_excel(excel.id, sheet_name='OneSampleT')\n    except Exception:\n        return 0.0\n    if df is None or df.empty:\n        return 0.0\n    cols = {str(c).strip().lower(): c for c in df.columns}\n    def col_like(keys):\n        for k in keys:\n            if k in cols:\n                return cols[k]\n        return None\n    target_col = col_like(['target'])\n    p_col = col_like(['p-value','p value','p'])\n    lo_col = col_like(['ci lower','lower ci','lower bound'])\n    hi_col = col_like(['ci upper','upper ci','upper bound'])\n    score = 0.0\n    try:\n        if target_col is not None:\n            vals = pd.to_numeric(df[target_col], errors='coerce').dropna()\n            if not vals.empty and np.isclose(vals.iloc[0], 3400, atol=0.5):\n                score += 0.5\n        if p_col is not None:\n            pvals = pd.to_numeric(df[p_col], errors='coerce').dropna()\n            if not pvals.empty and 0.0 <= float(pvals.iloc[0]) <= 1.0:\n                score += 0.25\n        if lo_col is not None and hi_col is not None:\n            lo = pd.to_numeric(df[lo_col], errors='coerce').dropna()\n            hi = pd.to_numeric(df[hi_col], errors='coerce').dropna()\n            if not lo.empty and not hi.empty and float(hi.iloc[0]) >= float(lo.iloc[0]):\n                score += 0.25\n    except Exception:\n        pass\n    return score"}, {"type": "code", "name": "Capability Sheet: USL=3400 and Cp/Cpk Present", "description": "Verify Capability analysis includes USL=3400 and valid Cp/Cpk metrics.", "weight": 0.6, "code": "import pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0\n    excel = None\n    for r in outputs:\n        try:\n            if getattr(r, 'is_spreadsheet', False):\n                p = context.files.get_path(r.id)\n                name = p.name.lower()\n                excel = r if excel is None else excel\n                if any(k in name for k in ['analysis', 'analyze', 'tollgate', 'workbook']):\n                    excel = r\n                    break\n        except Exception:\n            continue\n    if not excel:\n        return 0.0\n    try:\n        df = context.files.read_excel(excel.id, sheet_name='Capability')\n    except Exception:\n        return 0.0\n    if df is None or df.empty:\n        return 0.0\n    cols = {str(c).strip().lower(): c for c in df.columns}\n    def pick(*names):\n        for n in names:\n            if n in cols:\n                return cols[n]\n        return None\n    usl_col = pick('usl','upper spec','upper specification','upper limit')\n    cp_col = pick('cp')\n    cpk_col = pick('cpk')\n    score = 0.0\n    try:\n        if usl_col is not None:\n            usl_vals = pd.to_numeric(df[usl_col], errors='coerce').dropna()\n            if not usl_vals.empty and np.isclose(usl_vals.iloc[0], 3400, atol=0.5):\n                score += 0.34\n        if cp_col is not None:\n            cp_vals = pd.to_numeric(df[cp_col], errors='coerce').dropna()\n            if not cp_vals.empty and float(cp_vals.iloc[0]) > 0:\n                score += 0.33\n        if cpk_col is not None:\n            cpk_vals = pd.to_numeric(df[cpk_col], errors='coerce').dropna()\n            if not cpk_vals.empty and float(cpk_vals.iloc[0]) > 0:\n                score += 0.33\n    except Exception:\n        pass\n    return score"}, {"type": "code", "name": "Regression Uses Time of Day + Valid R-sq", "description": "Verify Regression sheet includes a TimeOfDay predictor with numeric coefficient and a valid R-sq summary (0\u2013100).", "weight": 0.6, "code": "import pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0\n    excel = None\n    for r in outputs:\n        try:\n            if getattr(r, 'is_spreadsheet', False):\n                p = context.files.get_path(r.id)\n                name = p.name.lower()\n                excel = r if excel is None else excel\n                if any(k in name for k in ['analysis', 'analyze', 'tollgate', 'workbook']):\n                    excel = r\n                    break\n        except Exception:\n            continue\n    if not excel:\n        return 0.0\n    try:\n        df = context.files.read_excel(excel.id, sheet_name='Regression')\n    except Exception:\n        return 0.0\n    if df is None or df.empty:\n        return 0.0\n    cols = {str(c).strip().lower(): c for c in df.columns}\n    # Identify term and coefficient columns\n    term_col = None\n    for k in ['term','predictor','variable']:\n        if k in cols:\n            term_col = cols[k]\n            break\n    coef_col = None\n    for k in ['coef','coefficient','estimate','beta']:\n        if k in cols:\n            coef_col = cols[k]\n            break\n    # R-sq columns may be in same sheet\n    rsq_col = None\n    for k in ['r-sq','r-sq(adj)','r^2','r2','r squared']:\n        if k in cols:\n            rsq_col = cols[k]\n            break\n    score = 0.0\n    try:\n        if term_col is not None and coef_col is not None:\n            terms = df[term_col].astype(str).str.lower().fillna('')\n            time_rows = df[terms.str.contains('time of day|timeofday|time_of_day|time-of-day|time\\b', regex=True)]\n            if not time_rows.empty:\n                coef_vals = pd.to_numeric(time_rows[coef_col], errors='coerce').dropna()\n                if not coef_vals.empty and np.isfinite(coef_vals.iloc[0]):\n                    score += 0.6 * 0.6 / 0.6  # 0.6 * 1.0 portion -> we will split below\n        # R-sq may be in a single-row summary; if found, validate range 0-100\n        rsq_score = 0.0\n        if rsq_col is not None:\n            rsq_vals = pd.to_numeric(df[rsq_col], errors='coerce').dropna()\n            if not rsq_vals.empty:\n                val = float(rsq_vals.iloc[0])\n                if 0.0 <= val <= 100.0:\n                    rsq_score = 0.4\n        # If no rsq_col, try to find an R-sq value anywhere in the frame\n        if rsq_score == 0.0:\n            for c in df.columns:\n                if 'r' in str(c).lower():\n                    vals = pd.to_numeric(df[c], errors='coerce').dropna()\n                    if not vals.empty:\n                        v = float(vals.iloc[0])\n                        if 0.0 <= v <= 100.0:\n                            rsq_score = 0.4\n                            break\n        # Combine: 0.6 for TimeOfDay term, 0.4 for R-sq validity\n        return min(score, 0.6) + rsq_score\n    except Exception:\n        return 0.0"}, {"type": "llm_judge", "name": "Statistical Interpretations Are Present and Appropriate", "description": "Check that each analysis slide includes an interpretation that aligns with the method\u2019s purpose and references key statistics (ANOVA significance, I-MR control state, regression direction/strength and staffing implications, 1-sample vs 3400 with p-value/CI, capability assessment with Cp/Cpk).", "weight": 5.0, "judge_prompt": "Evaluate the deck\u2019s analysis slides for INTERPRETATION QUALITY and PRESENCE (not numerical accuracy):\n- One-Way ANOVA interval plot: Does the narrative explicitly state whether processing rate differs by day-of-week (significant vs not), and suggest plausible operational impacts?\n- I-MR Control Chart (full Jan 4\u2013Mar 1): Does it discuss control limits, the process average during Analyze, and any unusual points/trends?\n- Linear Regression: Does it state correlation direction/strength with Time of Day and provide staffing model insights?\n- 1-Sample Test vs 3400 UPR: Does it conclude whether current average differs from 3400, referencing p-value and CI?\n- Process Capability: Does it include Cp/Cpk and conclude whether the process is capable of achieving 3400 UPR?\n\nScoring:\n- 5.0 = All five analyses have clear, method-appropriate interpretations covering the bullet points above\n- 4.0 = One analysis interpretation is weak or missing a key element\n- 3.0 = Two analyses weak/missing\n- 2.0 = Three analyses weak/missing\n- 1.0 = Only 1\u20132 analyses have reasonable interpretations\n- 0.0 = No substantive interpretations present", "expectation": "Each analytic artifact has a concise, method-appropriate interpretation that references the right statistics and ties to operations/staffing implications where relevant."}, {"type": "llm_judge", "name": "Dataset Provenance and Date Alignment", "description": "Verify all references to data and dates are consistent with the specification and A3 Current Conditions subset.", "weight": 3.0, "judge_prompt": "Check the deck and workbook for alignment with required dataset and dates:\n- Data source is cited as \u201cProcessing Data.xlsx\u201d; no data past March 1, 2025 are referenced\n- Analyze-phase analyses use data from Jan 4, 2025 through Mar 1, 2025\n- A3 Current Conditions includes a Baseline I-MR using only Jan 4\u2013Feb 21, 2025\n- Dates shown in charts/captions match the workbook\u2019s Data Overview and IMR_Baseline sheets\n\nScoring:\n- 3.0 = All bullets satisfied, no inconsistencies\n- 2.0 = Minor inconsistency in phrasing/date labeling but overall compliant\n- 1.0 = One major inconsistency (wrong end date, missing baseline subset, or references beyond allowed dates)\n- 0.0 = Multiple inconsistencies or absent provenance/date references", "expectation": "Strict adherence to the Jan 4\u2013Mar 1 Analyze window, proper Baseline subset to Feb 21 on the A3, and clear citation of the source file."}, {"type": "llm_judge", "name": "Timeline Accuracy and DMAIC Alignment", "description": "Verify that the timeline and charter schedule reflect the specified DMAIC dates and tollgate statuses.", "weight": 3.0, "judge_prompt": "Review the deck\u2019s final timeline slide and the Charter/Timeline sheets:\n- Define starts 2025-02-08, Measure starts 2025-02-15, Analyze 2025-02-22 (ending 2025-03-01)\n- Improve and Control are TBD\n- Tollgates completed vs in-progress are clearly indicated (Analyze prepared for submission)\n- Charter schedule aligns with the final timeline (no contradictions)\n\nScoring:\n- 3.0 = All elements correct and consistent across deck and workbook\n- 2.0 = Minor mismatch or omission in one item\n- 1.0 = One major mismatch\n- 0.0 = Multiple mismatches or missing timeline", "expectation": "DMAIC dates exactly as specified; tollgate status visually clear; no contradictions between charter and timeline."}, {"type": "llm_judge", "name": "Cross-Consistency of Targets and Findings", "description": "Check that targets and key findings are consistent across deck and workbook (e.g., 3400 UPR target, baseline average references, p-values/decisions).", "weight": 3.0, "judge_prompt": "Assess cross-consistency between the presentation and the Analysis Workbook:\n- Target of 3400 UPR is consistently referenced in Charter Goal, One-Sample Test, and Capability (USL)\n- Any stated averages, p-values, Cp/Cpk, and regression direction are not contradictory across slides vs workbook summaries (allow small rounding differences)\n- Day-of-week factor is clearly identified for ANOVA and is consistently labeled\n\nScoring:\n- 3.0 = Fully consistent; no contradictions observed\n- 2.0 = Minor discrepancy (e.g., label or rounding) but conclusions remain consistent\n- 1.0 = One notable contradiction (e.g., capability says capable while Cp/Cpk suggest otherwise)\n- 0.0 = Multiple contradictions that undermine trust", "expectation": "Uniform use of the 3400 UPR target and mutually consistent conclusions across all artifacts."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism Assessment", "description": "Holistic quality assessment suitable for executive stakeholders; evaluates clarity, Lean Six Sigma rigor, actionability, and accessibility.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Executive Presentation Quality", "description": "Assess professional formatting, clarity, slide flow, and labeling of charts and sections.", "weight": 3.0, "judge_prompt": "Evaluate the overall presentation quality:\n- Logical flow from Charter to Analyses to A3 to Timeline\n- Clear, concise slide titles and subtitles; charts labeled with axes/units (UPR) and date ranges\n- Visual cleanliness (legible fonts, consistent styles, appropriate color use)\n- Minimal clutter; insights highlighted for executives\n\nScoring:\n- 3.0 = Polished, executive-ready; excellent clarity and flow\n- 2.0 = Generally clear with minor formatting or flow issues\n- 1.0 = Noticeable issues with readability or organization\n- 0.0 = Poorly formatted or confusing", "expectation": "A clear, professional deck with crisp titles, labeled charts, and a coherent narrative."}, {"type": "llm_judge", "name": "Lean Six Sigma Rigor and Insight", "description": "Judge the rigor of LSS framing and depth of insights.", "weight": 3.0, "judge_prompt": "Assess Lean Six Sigma rigor and insightfulness:\n- Accurate use of DMAIC context and terminology; Analyze-phase focus on variation/root causes\n- Clear linkage between operational findings and financial/ROI impact (why 3400 UPR matters)\n- Assumptions and data limitations acknowledged briefly\n- Appropriate selection of tools (ANOVA, I-MR, Regression, 1-Sample, Capability) and coherent story\n\nScoring:\n- 3.0 = Rigorous, insightful, and well-grounded in LSS\n- 2.0 = Solid with minor gaps\n- 1.0 = Superficial or terminology misused\n- 0.0 = Not aligned with LSS practice", "expectation": "A coherent Analyze-phase narrative grounded in LSS, tying variation and capability to business impact."}, {"type": "llm_judge", "name": "Actionability and Next Steps", "description": "Evaluate whether results translate into clear actions and a cadence for follow-up.", "weight": 2.0, "judge_prompt": "Assess actionability:\n- Clear hypotheses and focus areas emerging from the analyses\n- Implications for staffing/model adjustments suggested by regression\n- Follow-up cadence described for Analyze/Improve phases; readiness for Improve\n\nScoring:\n- 2.0 = Concrete, actionable next steps and follow-up plan\n- 1.0 = Some actions implied but not concrete\n- 0.0 = No actionable guidance", "expectation": "Specific next steps tied to findings and a practical follow-up cadence."}, {"type": "llm_judge", "name": "Accessibility and Self-Documentation", "description": "Check for glossary/notes, reproducibility cues, and stakeholder accessibility.", "weight": 2.0, "judge_prompt": "Evaluate accessibility and self-documentation:\n- Brief glossary or notes clarifying terms (UPR, Cp/Cpk, I-MR)\n- Captioning/footnotes indicating software (e.g., Minitab) and data window used\n- Reproducibility cues (e.g., workbook sheets referenced, filters applied)\n\nScoring:\n- 2.0 = Well-documented and accessible\n- 1.0 = Partially documented\n- 0.0 = Not documented", "expectation": "Key terms and methods are briefly explained; sources and settings are cited for reproducibility."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "772e7524-174e-4c88-957e-6e510b61ea69", "rubric": {"category_name": "Clinical SOAP Note (Nurse Practitioner) - CAP/RLL Pneumonia Case", "rationale": "Pattern B (Document). The task is a professional clinical SOAP note. Stage 1 enforces a strict, verifiable document shape (SOAP with specific subsections) so later checks are trivial. Stage 2 mixes lightweight code rules (string/regex checks and safety flags) with higher-weight LLM judges for clinical coherence, safety, and cross-referencing. Stage 3 evaluates overall professional quality and actionability.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 GATE \u2013 SOAP Shape Enforcement (Documents only)", "description": "LLM-only gate to ensure the output is a properly structured clinical SOAP note document that is self-verifiable.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "SOAP Document Format and Structural Completeness", "description": "Verify the candidate produced a professional SOAP note in a verifiable document format with all required sections and key clinical subcomponents present.", "weight": 4.0, "judge_prompt": "You are validating the SHAPE (structure) of a clinical SOAP note. Do NOT judge medical correctness here\u2014only presence and structure. Examine the candidate output file(s) provided. Requirements:\n\nFormat Requirements:\n- Must be a single primary document in PDF or DOCX format (not plain text, not Excel).\n- Professionally formatted with clear headings and section delineations.\n- Minimum length: at least 1 full page of content.\n\nRequired SOAP Sections (exact or reasonable variants; allow headers like \"S:\" / \"O:\" / \"A:\" / \"P:\" and flexible capitalization):\n1) Subjective\n   - Must contain: Chief Complaint (or Reason for Visit), HPI (history of present illness), and at least a brief ROS section (e.g., constitutional, ENT, respiratory).\n   - Past History bundle somewhere in Subjective (or dedicated area) including: Medications, Allergies, Past Medical/Surgical history, Family history, Social history. Section titles may vary (e.g., PMH/PSH/SH/FH/Meds/Allergies).\n2) Objective\n   - Vital Signs explicitly listed (should include at minimum T/Temp, P/HR, R/RR, BP, and SpO2/O2 Sat). Height/Weight/BMI may be present as available.\n   - Physical Exam with system-based subsections or bullet points (e.g., General, HEENT, Neck, Lungs, Cardio, Abdomen, MSK, Neuro, Skin, Lymph). Reasonable variations accepted.\n   - Diagnostics/Studies subsection that references the chest imaging (CXR) result.\n3) Assessment\n   - A primary diagnosis statement section (e.g., community-acquired pneumonia) and space for differential (titles like \"Assessment\", \"Impression\", or \"Dx\" accepted).\n4) Plan\n   - A plan section with orders/interventions, patient education/return precautions, and follow-up instructions. Reasonable naming variations accepted.\n\nScoring (structure-only):\n- 4.0: PDF/DOCX + all four SOAP sections present with their required sub-elements (Subjective bundle + vitals + PE + diagnostics + assessment + actionable plan).\n- 3.0: PDF/DOCX + SOAP present but missing ONE key sub-element (e.g., diagnostics section not explicit, or ROS missing) while the four main SOAP sections are still clearly present.\n- 1.5: PDF/DOCX + SOAP present but missing TWO or more key sub-elements OR one of the core SOAP sections is unclear.\n- 0.0: Not PDF/DOCX OR SOAP structure not apparent.\n\nBe flexible on exact section titles, but ensure that the sections exist and are clearly delineated. Only evaluate presence/structure, not the accuracy of clinical content.", "expectation": "A well-structured PDF/DOCX SOAP note with Subjective, Objective (with vitals, PE, and diagnostics), Assessment, and Plan sections present."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness and Safety Verification", "description": "Now that structure is enforced, check clinical consistency, cross-references, and basic safety. Includes light code rules and higher-weight LLM judgments.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Vitals Presence Check", "description": "Detect whether key vitals (Temp, Pulse/HR, Resp Rate, BP, SpO2/O2) are present in the document.", "weight": 0.7, "code": "import re\\n\\n\\ndef evaluate(workflow, context):\\n    \\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n\\n    text = \"\"\\n    try:\\n        text = context.files.read_pdf_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_docx_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_text(output.id)\\n            except Exception:\\n                return 0.0\\n    if not text:\\n        return 0.0\\n\\n    t = text.lower()\\n\\n    # Patterns for vitals (flexible)\\n    patterns = {\\n        'temp': r'(temp|t\\\\b|temperature)[^\\n:\\\\r]*?(\\d{2,3}(?:\\\\.\\\\d)?)[^\\n%]*?(?:f|\\\\u00b0f|fahrenheit)?',\\n        'pulse': r'(pulse|p\\\\b|hr|heart rate)[^\\n:\\\\r]*?(\\d{2,3})',\\n        'resp': r'(resp|rr|respiratory rate|r\\\\b)[^\\n:\\\\r]*?(\\d{1,2})',\\n        'bp': r'(bp|blood pressure)[^\\n:\\\\r]*?(\\d{2,3}\\s*/\\s*\\d{2,3})',\\n        'spo2': r'(spo2|o2|oxygen saturation|sat)[^\\n:\\\\r]*?(\\d{2,3})\\s*%'\n    }\\n\\n    found = 0\\n    for key, pat in patterns.items():\\n        if re.search(pat, t):\\n            found += 1\\n\\n    score_ratio = found / len(patterns)\\n    return score_ratio * 0.7\\n"}, {"type": "code", "name": "Antibiotic\u2013Allergy Safety Check", "description": "If an antibiotic is recommended in the Plan, ensure no clear conflict with stated PCN/sulfa allergies.", "weight": 0.7, "code": "import re\\n\\nSAFE_ABX = [\\n    'azithromycin', 'z-pak', 'zpak', 'doxycycline', 'levofloxacin', 'moxifloxacin', 'clarithromycin', 'erythromycin',\\n    'cefuroxime', 'cefpodoxime', 'ceftriaxone', 'cefdinir'\\n]\\nCONTRA_PCN = [\\n    'penicillin', 'pcn', 'amoxicillin', 'augmentin', 'amox-clav', 'amox clav', 'ampicillin', 'unasyn', 'nafcillin',\\n    'oxacillin', 'piperacillin', 'zosyn', 'ticarcillin', 'dicloxacillin'\\n]\\nCONTRA_SULFA = [\\n    'bactrim', 'tmp-smx', 'tmp smx', 'trimethoprim-sulfamethoxazole', 'sulfamethoxazole', 'sulfa'\\n]\\n\\n\\ndef evaluate(workflow, context):\\n    \\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n\\n    # Read text from document\\n    text = ''\\n    try:\\n        text = context.files.read_pdf_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_docx_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_text(output.id)\\n            except Exception:\\n                return 0.0\\n    if not text:\\n        return 0.0\\n\\n    t = text.lower()\\n\\n    # Try to isolate Plan section if possible\\n    plan_text = t\\n    plan_match = re.split(r'\\n\\s*(assessment|a:)\\s*[:\\n]', t)\\n    # Split by Plan heading variants\\n    plan_splits = re.split(r'\\n\\s*(plan|p:)\\s*[:\\n]', t)\\n    if len(plan_splits) >= 3:\\n        # Content after first occurrence of Plan heading\n        plan_text = plan_splits[2]\n    \n    allergies_section = t\n    # Look for allergies\n    has_pcn_allergy = bool(re.search(r'allerg[^\\n]*?(pcn|penicillin)', t)) or 'pcn' in t or 'penicillin' in t\n    has_sulfa_allergy = bool(re.search(r'allerg[^\\n]*?(sulfa|sulf)', t)) or 'sulfa' in t or 'sulfameth' in t\n\n    # Check antibiotics mentioned in plan\n    abx_present = any(abx in plan_text for abx in SAFE_ABX + CONTRA_PCN + CONTRA_SULFA)\n    abx_contra = False\n    if has_pcn_allergy:\n        abx_contra = any(x in plan_text for x in CONTRA_PCN)\n    if has_sulfa_allergy:\n        abx_contra = abx_contra or any(x in plan_text for x in CONTRA_SULFA)\n\n    if abx_present and abx_contra:\n        return 0.0  # unsafe\n    elif abx_present and not abx_contra:\n        return 0.7  # safe antibiotic recommended\n    else:\n        # No antibiotic mentioned; partial credit since pneumonia likely needs one\n        return 0.21  # 30% of weight\n"}, {"type": "code", "name": "Imaging and Localization Cross-Reference", "description": "Verify the note references pneumonia diagnosis, right lower lobe localization, and chest imaging.", "weight": 0.6, "code": "import re\\n\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\n        return 0.0\n\n    text = ''\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n    if not text:\n        return 0.0\n\n    t = text.lower()\n\n    checks = [\n        bool(re.search(r'\\b(pneumonia|cap)\\b', t)),\n        bool(re.search(r'(right lower lobe|\\brll\\b)', t)),\n        bool(re.search(r'(cxr|chest\\s*x-?ray|radiograph|xray)', t))\n    ]\n    score_ratio = sum(checks) / 3.0\n    return score_ratio * 0.6\n"}, {"type": "llm_judge", "name": "Clinical Coherence and Differential", "description": "Assess if the Assessment logically supports CAP (RLL) given HPI, exam, vitals, and CXR; and includes a reasonable differential.", "weight": 2.0, "judge_prompt": "Evaluate clinical coherence, not formatting. Considering the provided case context (45-year-old with fever, productive cough, RLL crackles, and CXR showing RLL consolidation), score whether the Assessment:\n- Clearly identifies community-acquired pneumonia (with RLL localization).\n- Includes a plausible differential (e.g., influenza, COVID-19, acute bronchitis, asthma exacerbation, PE, pleurisy), at least 2 alternatives.\n- Reasonably ties subjective and objective findings to the working diagnosis.\nScoring:\n- 2.0: Clear CAP/RLL diagnosis + at least 2 plausible differentials + reasoning ties findings to dx.\n- 1.0: CAP diagnosis present but weak localization or only 1 differential or minimal linkage.\n- 0.0: Diagnosis unclear/incorrect or no differentials.", "expectation": "Assessment states CAP (RLL), provides at least 2 differentials, and briefly justifies."}, {"type": "llm_judge", "name": "Plan Appropriateness and Safety", "description": "Evaluate whether the Plan is clinically appropriate, actionable, and safe for this patient\u2019s allergies and meds.", "weight": 2.0, "judge_prompt": "Assess the Plan for outpatient CAP in a 45-year-old ER nurse with PCN and sulfa allergies, on escitalopram 10 mg daily, SpO2 93% RA. Consider:\n- Appropriate first-line antibiotic choice(s) avoiding penicillins and sulfonamides; includes dose, route, frequency, and duration (e.g., azithromycin or doxycycline, or a respiratory fluoroquinolone with risks noted). If azithromycin is chosen, mention of QT risk with escitalopram is a plus but not mandatory.\n- Symptomatic management: antipyretics, hydration, rest, cough management.\n- Return/ER precautions (worsening dyspnea, persistent fever, confusion, chest pain, hypoxia) and a clear follow-up interval (24\u201348 hours if not improving or specific timeline).\n- Work considerations/infection control testing as an ER nurse.\nScoring:\n- 2.0: Antibiotic with dosing/duration and no allergy conflict + supportive care + return precautions + follow-up timeline; acknowledges safety considerations where relevant.\n- 1.0: Most elements present but missing one (e.g., no duration or no return precautions) or minor safety omissions.\n- 0.0: Unsafe antibiotic choice conflicting with allergies or plan clearly insufficient.", "expectation": "Actionable, safe outpatient CAP plan with dosing, supportive care, precautions, and follow-up."}, {"type": "llm_judge", "name": "Diagnostics and Testing", "description": "Assess whether diagnostics are referenced appropriately and additional reasonable tests are considered.", "weight": 2.0, "judge_prompt": "Evaluate whether the note: (a) references the chest imaging result (RLL consolidation), and (b) reasonably considers additional tests given the setting and presentation (e.g., COVID-19/flu testing, possibly CBC if indicated). Do not require exhaustive testing. Score based on appropriateness and mention of key imaging and sensible adjuncts.\nScoring:\n- 2.0: CXR result clearly documented + reasonable ancillary tests considered or ordered.\n- 1.0: CXR referenced but no ancillary tests considered OR ancillary tests with unclear CXR reference.\n- 0.0: No mention of CXR or any testing.", "expectation": "CXR referenced; reasonable mention of COVID/flu testing and/or basic labs if warranted."}, {"type": "llm_judge", "name": "Patient-Specific Documentation Accuracy", "description": "Check whether key patient-specific details from the case are captured in the note.", "weight": 2.0, "judge_prompt": "Verify that the note accurately reflects key patient-specific details from the case: age/sex, ER nurse occupation, allergies (PCN and sulfa), current medication (escitalopram 10 mg daily), relevant past surgical history (hysterectomy), non-smoker/rare alcohol, up-to-date immunizations (including flu), family history highlights, and the acute symptom constellation. Score completeness of these data elements (flexible placement across Subjective/History sections is fine).\nScoring:\n- 2.0: Accurately documents most of these elements (\u22657) without contradictions.\n- 1.0: Partially documents (4\u20136 elements) or minor inconsistency.\n- 0.0: Few elements (\u22643) or clear contradictions.", "expectation": "The note mirrors the provided history and avoids contradictions."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Overall Quality and Professionalism", "description": "Holistic assessment of clarity, professionalism, coding/actionability, and patient communication.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "SOAP Organization and Clarity", "description": "Professional tone, clear headings, logical flow, readable formatting.", "weight": 1.5, "judge_prompt": "Evaluate the note\u2019s readability and organization: Are sections easy to find? Is the language professional and concise? Are bullet lists/tables used appropriately for vitals and meds? Is the flow coherent with minimal redundancy?\nScoring:\n- 1.5: Highly readable, well-organized, professional.\n- 0.8: Adequate but minor issues.\n- 0.0: Disorganized/confusing tone.", "expectation": "Clear, concise, professional SOAP layout."}, {"type": "llm_judge", "name": "Clinical Reasoning Communication", "description": "Clarity of rationale linking findings to diagnosis and plan; risk stratification if applicable.", "weight": 1.5, "judge_prompt": "Assess how well the note communicates reasoning from HPI/Exam/Diagnostics to the diagnosis and plan. Look for brief justification, risk assessment (e.g., outpatient suitability), and alignment of plan with severity.\nScoring:\n- 1.5: Clear, succinct rationale and risk assessment.\n- 0.8: Partial linkage or generic reasoning.\n- 0.0: Little to no rationale.", "expectation": "Succinct, evidence-aligned reasoning with outpatient justification."}, {"type": "llm_judge", "name": "Actionability and Specificity of Plan", "description": "Orders with specifics (drug, dose, route, frequency, duration), follow-up timing, and clear patient instructions.", "weight": 1.5, "judge_prompt": "Score the specificity and actionability of the Plan: medication details (drug, dose, route, frequency, duration), supportive care instructions, return precautions, and a concrete follow-up plan.\nScoring:\n- 1.5: Fully actionable with specifics and clear instructions.\n- 0.8: Mostly actionable but missing one specificity.\n- 0.0: Vague/insufficient plan.", "expectation": "Discrete, implementable orders and clear instructions."}, {"type": "llm_judge", "name": "Compliance and Coding Readiness", "description": "Presence of appropriate codes and documentation elements supporting billing and compliance.", "weight": 1.5, "judge_prompt": "Evaluate whether the note is close to billing-ready: appropriate ICD-10 for CAP (e.g., J18.9 if unspecified), optional CPT/E/M coding considerations supported by history/exam/MDM, and documentation of shared decision-making/consent where reasonable.\nScoring:\n- 1.5: Includes appropriate diagnosis code(s) and has elements supporting coding; mentions shared decision-making or consent where relevant.\n- 0.8: Partially coding-ready (diagnosis clear but codes not listed or missing minor elements).\n- 0.0: Not coding-ready.", "expectation": "ICD-10 included and documentation supports coding."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "dd724c67-8118-4b99-ab50-4761af705c3b", "rubric": {"category_name": "ACO Care Coordination Workbook (Hospitals/Rehab Contacts + CMS TFU Guide)", "rationale": "Pattern C (Mixed): The deliverable is a single Excel workbook that combines a data table (facility contacts) with a structured, reference-style guide (CMS TFU). Stage 1 uses an LLM-only gate to strictly enforce the required workbook structure so later verification is trivial. Stage 2 blends lightweight code checks (structural/data sanity) with heavier LLM verification (coverage, plausibility, and internal consistency). Stage 3 applies holistic LLM quality assessment focused on professional usability for case managers.", "max_total_score": 21.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (LLM Only)", "description": "Gate: Verify the output is a single Excel workbook (.xlsx) with the exact sheets and structural elements required to enable verification.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Excel Structure Gate for ACO Discharge Planning Workbook", "description": "Check that the candidate output is an Excel workbook with the required two-sheet structure and clearly labeled sections/tables so verification is possible.", "weight": 4.0, "judge_prompt": "You are evaluating whether the candidate output has the REQUIRED STRUCTURE for this task. Only assess structure/format and presence of elements, NOT content correctness.\n\nREQUIRED FORMAT:\n- Must be a single Excel workbook (.xlsx), not CSV/Google Sheets/PDF/DOCX.\n- Must contain AT LEAST two sheets with these roles and structures:\n\nSheet A: Facility Contacts (sheet name can vary but should clearly indicate contacts/facilities)\n- Contains a tabular list of hospitals and rehabilitation facilities located on Long Island (Nassau and Suffolk counties, NY).\n- Table must include, at minimum, the following clearly labeled columns (allow reasonable naming variants):\n  - Facility Name (e.g., \"Facility\", \"Hospital/Facility Name\")\n  - Address (either a single \"Address\" column OR split across Street/City/State/ZIP)\n  - Telephone (e.g., \"Phone\", \"Telephone\")\n- The table must contain multiple rows (at least 10 entries) and appear to be real facilities (not placeholders).\n- Additional helpful columns (optional) are allowed (e.g., Type, Fax, Notes).\n\nSheet B: TFU Guide (sheet name can vary but should clearly indicate TFU / Follow-Up / CMS)\n- Serves as a case manager reference guide for the CMS Timely Follow-Up (TFU) quality measure per ACO REACH PY 2025.\n- Must include clearly delineated sections (accept reasonable synonyms):\n  1) Overview of TFU measure\n  2) Rationale for TFU\n  3) A tabular mapping of Condition -> Recommended Follow-Up Timeframe (e.g., columns like [Condition | Recommended Follow-Up | Notes/Source])\n  4) Source/Citation section that references the \"ACO REACH Model PY 2025 Quality Measurement Methodology Report\" and indicates cms.gov as the source.\n- The TFU table must list multiple conditions (e.g., CAD, diabetes, COPD, etc.).\n\nSCORING (Structure only):\n- 4.0: Excel .xlsx with BOTH sheets present; Facility Contacts has clearly labeled required columns and >=10 rows; TFU Guide has overview, rationale, a clear condition->timeframe table, and source/citation referencing ACO REACH PY 2025 on cms.gov.\n- 3.0: Excel .xlsx with both sheets; one minor structural element missing (e.g., TFU source present but missing cms.gov mention, or Facility Contacts missing one minor label but still unambiguous).\n- 2.0: Excel .xlsx but one REQUIRED component absent or unclear (e.g., TFU table missing or Facility Contacts lacks a required column or has too few rows).\n- 1.0: Excel .xlsx but multiple required elements missing/unclear across sheets.\n- 0.0: Not an Excel .xlsx OR lacks one of the two required sheets entirely.\n\nBe flexible with section and sheet naming, but strict about the presence of the required elements.", "expectation": "A clean .xlsx with two clearly structured sheets: a facility contacts table and a TFU guide (overview, rationale, condition->timeframe table, and a cms.gov ACO REACH PY2025 citation)."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Verification", "description": "Now that the shape is enforced, verify structural correctness, coverage, plausibility, and internal consistency of content using mixed code + LLM rules.", "is_required": true, "max_points": 11.0, "min_score_to_pass": 5.5, "rules": [{"type": "code", "name": "Facility Contacts \u2014 Core Fields and Data Sanity", "description": "Validate presence of key columns, sufficient rows, phone coverage, and basic duplication sanity on the Facility Contacts sheet.", "weight": 0.8, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n\n    try:\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n        sheet_candidates = [s for s in xls.sheet_names]\n        # Prefer sheets indicating facility/contact list\n        priority = ['contact', 'facility', 'facilities', 'hospital', 'rehab', 'long island', 'provider', 'directory', 'list']\n        def score_name(name):\n            ln = name.lower()\n            return sum(1 for p in priority if p in ln)\n        sheet_name = max(sheet_candidates, key=score_name)\n\n        df = pd.read_excel(file_path, sheet_name=sheet_name)\n        if df is None or df.shape[0] == 0:\n            return 0.0, f\"Sheet '{sheet_name}' appears empty.\"\n\n        cols = [str(c).strip().lower() for c in df.columns]\n        # Column presence checks (flexible)\n        has_name = any(('name' in c) or ('facility' in c) for c in cols)\n        has_phone_col = any(('phone' in c) or ('tel' in c) for c in cols)\n        has_address_single = any('address' in c for c in cols)\n        has_street = any('street' in c or 'addr1' in c for c in cols)\n        has_city = any('city' in c for c in cols)\n        has_state = any('state' in c for c in cols)\n        has_zip = any('zip' in c or 'postal' in c for c in cols)\n        has_address = has_address_single or ((has_street and has_city) and (has_state or has_zip))\n\n        # Row count\n        row_score = 1.0 if len(df) >= 10 else (len(df) / 10.0)\n        row_score = max(0.0, min(1.0, row_score))\n\n        # Phone coverage using regex for 10 digits\n        phone_score = 0.0\n        if has_phone_col:\n            phone_col_idx = [i for i, c in enumerate(cols) if ('phone' in c) or ('tel' in c)]\n            # Try first phone-like column\n            pc = df.iloc[:, phone_col_idx[0]].astype(str).fillna('')\n            def has_10_digits(s):\n                digits = re.sub(r'\\D', '', s)\n                return len(digits) >= 10\n            coverage = pc.apply(has_10_digits).mean() if len(pc) > 0 else 0.0\n            phone_score = float(coverage)\n\n        # Duplicate sanity if we can combine name+address\n        dup_score = 1.0\n        try:\n            name_idx = None\n            for i, c in enumerate(cols):\n                if ('name' in c) or ('facility' in c):\n                    name_idx = i; break\n            addr_idx = None\n            for i, c in enumerate(cols):\n                if 'address' in c:\n                    addr_idx = i; break\n            if name_idx is not None and addr_idx is not None:\n                key = df.iloc[:, name_idx].astype(str).str.strip() + '|' + df.iloc[:, addr_idx].astype(str).str.strip()\n                dup_ratio = key.duplicated(keep=False).mean()\n                # Full credit if few duplicates; penalize if many\n                dup_score = 1.0 - min(1.0, dup_ratio)\n        except Exception:\n            dup_score = 0.7  # neutral if we can't compute\n\n        checks = [1.0 if has_name else 0.0, 1.0 if has_address else 0.0, 1.0 if has_phone_col else 0.0, row_score, phone_score, dup_score]\n        score = float(np.mean(checks))\n        feedback = f\"Cols(name={has_name}, address={has_address}, phone={has_phone_col}); rows={len(df)}; phone_cov~{phone_score:.2f}; dup_score~{dup_score:.2f}\"\n        return score, feedback\n    except Exception as e:\n        return 0.0, f\"Error reading spreadsheet: {e}\""}, {"type": "code", "name": "TFU Guide \u2014 Conditions and Timeframe Presence", "description": "Check the TFU sheet contains multiple conditions and explicit follow-up timeframe language, plus overview/rationale text.", "weight": 0.8, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n\n    try:\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n        # Identify TFU-like sheet\n        candidates = xls.sheet_names\n        priority = ['tfu', 'timely', 'follow', 'cms', 'guide', 'reference']\n        def score_name(n):\n            ln = n.lower()\n            return sum(1 for p in priority if p in ln)\n        sheet_name = max(candidates, key=score_name)\n\n        # Read as raw grid to capture all text\n        grid = pd.read_excel(file_path, sheet_name=sheet_name, header=None)\n        if grid is None or grid.size == 0:\n            return 0.0, f\"TFU sheet '{sheet_name}' empty.\"\n        text = ' '.join(grid.fillna('').astype(str).values.flatten()).lower()\n\n        known_conditions = [\n            'cad','coronary artery disease','mi','myocardial infarction','heart failure','hf',\n            'copd','pneumonia','stroke','tia','diabetes','ckd','hypertension','sepsis','uti','depression'\n        ]\n        cond_hits = set()\n        for c in known_conditions:\n            if re.search(r'\\b' + re.escape(c) + r'\\b', text):\n                cond_hits.add(c)\n        cond_score = min(1.0, len(cond_hits) / 5.0)  # full credit at 5 distinct hits\n\n        timeframe_terms = ['follow-up','follow up','timeframe','within','days','hours','weeks']\n        tf_present = any(t in text for t in timeframe_terms)\n        overview_present = ('overview' in text) or ('summary' in text) or ('what is tfu' in text)\n        rationale_present = ('rationale' in text) or ('why' in text) or ('purpose' in text)\n\n        parts = [cond_score, 1.0 if tf_present else 0.0, 1.0 if (overview_present and rationale_present) else 0.0]\n        score = float(np.mean(parts))\n        feedback = f\"conditions>={len(cond_hits)}; timeframe={tf_present}; overview={overview_present}; rationale={rationale_present}\"\n        return score, feedback\n    except Exception as e:\n        return 0.0, f\"Error reading TFU sheet: {e}\""}, {"type": "code", "name": "TFU Guide \u2014 CMS ACO REACH PY2025 Source Citation", "description": "Verify that the TFU sheet references the ACO REACH Model PY 2025 Quality Measurement Methodology Report and cms.gov.", "weight": 0.4, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n\n    try:\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n        candidates = xls.sheet_names\n        priority = ['tfu', 'timely', 'follow', 'cms', 'guide', 'reference']\n        def score_name(n):\n            ln = n.lower()\n            return sum(1 for p in priority if p in ln)\n        sheet_name = max(candidates, key=score_name)\n\n        grid = pd.read_excel(file_path, sheet_name=sheet_name, header=None)\n        text = ' '.join(grid.fillna('').astype(str).values.flatten()).lower()\n\n        has_aco_reach = ('aco reach' in text) or ('aco-reach' in text)\n        has_py2025 = ('py 2025' in text) or ('py2025' in text) or ('2025' in text)\n        has_cms = ('cms.gov' in text) or ('centers for medicare' in text)\n\n        # Need explicit ACO REACH + PY2025 + CMS reference for full credit\n        score = 1.0 if (has_aco_reach and has_py2025 and has_cms) else 0.0\n        feedback = f\"aco_reach={has_aco_reach}; py2025={has_py2025}; cms={has_cms}\"\n        return score, feedback\n    except Exception as e:\n        return 0.0, f\"Error checking citation: {e}\""}, {"type": "llm_judge", "name": "Facility Coverage \u2014 Long Island Hospitals and Rehab Facilities", "description": "Assess whether the Facility Contacts sheet plausibly lists Long Island (Nassau/Suffolk) hospitals and rehabilitation facilities with usable phone numbers and minimal duplicates.", "weight": 3.5, "judge_prompt": "Review the Facility Contacts sheet. Check for plausible coverage of Long Island (Nassau and Suffolk counties) facilities and basic usability:\n- Do entries appear to be hospitals and rehabilitation facilities located on Long Island (not NYC boroughs outside LI)? Look for cities like Mineola, Manhasset, Hempstead, Garden City, Rockville Centre, Oceanside, West Islip, Riverhead, Stony Brook, Port Jefferson, Smithtown, Huntington, etc.\n- Are both hospitals and rehab facilities represented? (If a Type column exists, it should reflect both. If not, infer from names.)\n- Do telephone numbers look valid (typical 10-digit US numbers, common LI area codes like 516/631 acceptable, but accept other formats)?\n- Are there obvious duplicates or placeholder entries?\n\nScoring:\n- 3.5: Clearly Long Island facilities with a reasonable mix of hospitals and rehab; phones look valid; no obvious duplicates.\n- 2.5: Mostly appropriate LI facilities; minor gaps (e.g., light on rehab or a few non-LI entries/duplicates).\n- 1.5: Mixed quality; several non-LI entries or many missing/invalid phones; duplicates present.\n- 0.0: Largely not Long Island or mostly placeholders/invalid contacts.\n", "expectation": "A credible, working directory of Long Island hospitals and rehab facilities with valid phone numbers and minimal duplication."}, {"type": "llm_judge", "name": "TFU Content Fidelity \u2014 Condition-to-Timeframe Mapping", "description": "Evaluate whether the TFU guide accurately and coherently maps conditions to recommended follow-up timeframes and includes the required overview and rationale.", "weight": 3.0, "judge_prompt": "On the TFU Guide sheet, assess the clarity and plausibility of the content:\n- Is there a concise Overview of TFU and a Rationale that explains intent (e.g., improving care transitions, reducing readmissions, timely post-discharge care)?\n- Does the table map multiple conditions (e.g., CAD, diabetes, COPD, heart failure, pneumonia, stroke/TIA) to specific follow-up timeframes that are clinically plausible and consistent across the sheet?\n- Do notes/sources align with referring to the ACO REACH PY 2025 TFU specification (do not verify externally; just check internal consistency)?\n\nScoring:\n- 3.0: Clear overview+rationale; multiple conditions with specific, plausible timeframes; internally consistent.\n- 2.0: Overview+rationale present; some conditions/timeframes are vague or incomplete but mostly usable.\n- 1.0: Minimal or unclear mapping; overview/rationale weak.\n- 0.0: Little/no usable TFU guidance.\n", "expectation": "A coherent, internally consistent TFU guide that case managers can rely on to schedule timely follow-ups by condition."}, {"type": "llm_judge", "name": "Internal Consistency and Usability \u2014 Workbook", "description": "Check that the workbook is internally consistent and immediately usable for case management workflows.", "weight": 2.5, "judge_prompt": "Evaluate the overall internal consistency and usability of the workbook:\n- Do column headers and sheet names clearly communicate purpose? Is the TFU table easy to read (clear columns for Condition and Timeframe)?\n- Are there any contradictions (e.g., different timeframes for the same condition without explanation)?\n- Are there brief usage notes or cues that help a case manager apply the TFU guidance when scheduling?\n\nScoring:\n- 2.5: Clear, consistent, and readily usable for scheduling; no obvious contradictions.\n- 1.5: Generally usable with minor inconsistencies or missing cues.\n- 0.5: Usability issues that would slow case managers significantly.\n- 0.0: Disorganized or confusing.\n", "expectation": "A consistent, practical tool that supports quick, correct scheduling decisions."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Actionability", "description": "Holistic LLM assessment of presentation quality, actionability for case managers, and clarity.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation (Workbook Design)", "description": "Assess the professional look and feel of the workbook for day-to-day operational use.", "weight": 2.0, "judge_prompt": "Assess formatting and presentation quality:\n- Appropriate sheet names; table headers bold/clear; freeze panes and filters (if visible) helpful.\n- Phone/address formatting consistent; reasonable alignment and readability.\n- Minimal clutter; effective use of whitespace.\n\nScoring:\n- 2.0: Polished, professional, easy to navigate.\n- 1.0: Adequate but could be cleaner or more consistent.\n- 0.0: Sloppy or hard to read.\n", "expectation": "A clean, professional Excel workbook that looks ready for operational use."}, {"type": "llm_judge", "name": "Actionability for Case Managers", "description": "Evaluate whether the workbook anticipates real workflow needs for discharge planning and scheduling.", "weight": 2.0, "judge_prompt": "Consider how actionable this is for case managers:\n- Does the contacts sheet include helpful extras (e.g., facility type, fax, care coordination line, notes, hours) or at least a clear structure to add them?\n- Does the TFU guide include brief instructions or caveats (e.g., exceptions, when to escalate)?\n- Is information scannable so a case manager can act quickly?\n\nScoring:\n- 2.0: Highly actionable with practical details and/or clear instructions.\n- 1.0: Usable but missing helpful operational touches.\n- 0.0: Bare-bones and not immediately helpful.\n", "expectation": "A tool that speeds up scheduling and coordination, not just a static reference."}, {"type": "llm_judge", "name": "Clarity and Accessibility", "description": "Judge the clarity of writing and accessibility for a multidisciplinary audience.", "weight": 1.5, "judge_prompt": "Review the language used in the TFU guide and any notes in the contacts sheet:\n- Clear, concise writing without unnecessary jargon.\n- Logical sectioning with headings.\n- Key points and timeframes are easy to find quickly.\n\nScoring:\n- 1.5: Very clear and accessible.\n- 0.8: Mostly clear with minor issues.\n- 0.0: Confusing or poorly organized text.\n", "expectation": "Straightforward, concise guidance that non-clinical staff can follow."}, {"type": "llm_judge", "name": "Versioning and Source Transparency", "description": "Check for update date/version and explicit source links/citations for maintainability.", "weight": 0.5, "judge_prompt": "Look for signs of maintainability:\n- A visible \"Last Updated\" date or version.\n- Clear source/citation for TFU (cms.gov) and any links.\n\nScoring:\n- 0.5: Date/version present and sources clearly cited.\n- 0.2: One present but not both.\n- 0.0: Neither present.\n", "expectation": "A workbook with visible recency and trustworthy sourcing to aid future updates."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "8079e27d-b6f3-4f75-a9b5-db27903c798d", "rubric": {"category_name": "Finance & Insurance \u2014 Equity Capital Markets (ECM) \u2014 S&P 500 P/E Deep Dive", "rationale": "This rubric enforces a self-documenting, analysis-ready Excel deliverable for ECM stakeholders. Stage 1 (LLM-only) mandates a strict, verification-friendly workbook shape. Stage 2 mixes lightweight code checks (sanity, bounds, consistency) with heavier LLM verification (methodology transparency, aggregation coherence, and explicit comparison to historical P/E range). Stage 3 assesses professional quality and client value. Code rules are kept simple and flexible; LLM rules carry most of the weight as they evaluate nuanced structure and reasoning across sheets.", "max_total_score": 29.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (LLM-only)", "description": "Validate that the output is an Excel workbook with exact, ECM-usable structure enabling downstream verification. If this shape contract is not met, evaluation stops and the category scores 0.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Excel Workbook With Required Sheets and Columns", "description": "Check for an Excel workbook that contains the mandated sheets and columns enabling verification and analysis.", "weight": 4.0, "judge_prompt": "You are evaluating whether the candidate produced an Excel workbook in the exact verifiable structure needed for downstream checks. Look only for presence/format, not correctness of numbers.\n\nFormat Requirements:\n- Output must be a single Excel spreadsheet (.xlsx preferred).\n- Data should be arranged in clearly labeled tables with header rows (filterable/sortable is ideal but not required in Stage 1).\n\nRequired Sheets (be flexible with similar naming):\n1) \"Sub-Sector Summary\" (acceptable variants: \"Subsector Summary\", \"Sub-Industry Summary\", \"Sub-Sector Overview\")\n   Required columns (flexible naming, but meaning must be clear):\n   - Sector\n   - Sub-Sector (GICS sub-industry or similar granular grouping)\n   - No. of Companies (company count per sub-sector)\n   - Market Capitalization (aggregate for the sub-sector)\n   - % of Index (weight of sub-sector in S&P 500)\n   - LTM P/E (or Trailing/TTM P/E) \u2014 indicate that it\u2019s backward-looking\n   - NTM P/E (or Forward P/E) \u2014 indicate that it\u2019s forward-looking\n   - Dividend Yield\n   - Annual EPS (CY+1) \u2014 next calendar year EPS\n   - Quarterly EPS (CQ+1) \u2014 next calendar quarter EPS\n   Optional but encouraged: Above/Below historical S&P average flags for LTM/NTM.\n\n2) \"Company Detail\" (acceptable variants: \"Constituent Detail\", \"Company-Level\", \"S&P 500 Companies\")\n   Required columns (flexible naming, but meaning must be clear):\n   - Ticker (or Symbol)\n   - Company Name\n   - Sector\n   - Sub-Sector (GICS sub-industry / industry group)\n   - Market Capitalization\n   - % of Index (company weight in index)\n   - LTM P/E (or Trailing/TTM P/E)\n   - NTM P/E (or Forward P/E)\n   - Dividend Yield\n   - Annual EPS (CY+1)\n   - Quarterly EPS (CQ+1)\n   Optional: Source/URL, As-of Date, Data Currency/Units.\n\n3) \"Methodology & Sources\" (acceptable variants: \"Data Dictionary & Methodology\", \"Methodology / Sources\", \"Notes & Sources\")\n   Must include:\n   - Data sources list with URL names or publishers\n   - As-of date (should be around April 2025)\n   - Definitions of LTM vs NTM P/E and EPS (CY+1 and CQ+1)\n   - How % of Index was computed (e.g., by Mkt Cap weights)\n   - Any assumptions/limitations (e.g., handling negative EPS)\n   - At least 5 sentences of explanatory text overall\n\nOptional Sheets (do not penalize if missing):\n- \"Data Pull Log\" or \"Source Log\"\n- \"Flags & Filters\" or \"Pivot/Charts\"\n\nScoring Guidance (STRUCTURE ONLY):\n- 4.0: Excel file present AND all 3 required sheets exist with the listed column sets clearly present on the first two sheets and the methodology sheet contains the items listed.\n- 3.0: Excel file present, required sheets present but missing up to 2 minor required columns total across the two data sheets OR methodology text is shorter than requested but still contains sources and definitions.\n- 2.0: Excel file present but missing one required sheet OR over 2 missing required columns across the two data sheets.\n- 1.0: Excel file present but largely unstructured (only one of the two data tables exists or missing most required columns; methodology absent).\n- 0.0: Not an Excel file OR completely wrong structure.\n\nOnly evaluate presence and structure, not numeric correctness.", "expectation": "A cleanly structured Excel workbook with the specified sheets and columns to enable automated and human verification."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness & Consistency)", "description": "Given structure is present, verify plausibility and internal consistency using a mix of code and LLM checks. Code rules do deterministic sanity checks; LLM rules assess nuanced cross-sheet consistency and methodology clarity.", "is_required": true, "max_points": 15.0, "min_score_to_pass": 7.5, "rules": [{"type": "code", "name": "Company Coverage and Index Weight Sanity", "description": "Check that the Company Detail sheet covers most constituents and index weights roughly sum to 100%.", "weight": 0.75, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n    except Exception:\n        return 0.0\n\n    def norm(s):\n        return re.sub(r\"[^a-z0-9]+\", \" \", str(s).strip().lower()).strip()\n\n    # Synonyms for Company Detail columns\n    syn = {\n        'ticker': ['ticker','symbol'],\n        'company': ['company','name','company name','issuer','security name'],\n        'sector': ['sector'],\n        'subsector': ['sub sector','sub-sector','subsector','sub industry','sub-industry','subindustry','industry group','gics sub industry','gics sub-industry'],\n        'mcap': ['market cap','market capitalization','mkt cap','marketcapitalization','market capitalisation'],\n        'weight': ['% of index','pct of index','index weight','weight','spx weight','sp500 weight','index %','weight %','index wt']\n    }\n\n    def find_col(cols, keys):\n        cols_n = [norm(c) for c in cols]\n        for i, c in enumerate(cols_n):\n            for k in keys:\n                if k in c:\n                    return cols[i]\n        return None\n\n    best_df = None\n    best_score = -1\n    best_cols = None\n    # Identify the \"Company Detail\"-like sheet by presence of key columns\n    for sh in xls.sheet_names:\n        try:\n            df = pd.read_excel(path, sheet_name=sh, nrows=500)\n        except Exception:\n            continue\n        if df.empty:\n            continue\n        cols = list(df.columns)\n        score = 0\n        needed = ['ticker','company','sector','subsector','mcap','weight']\n        found_map = {}\n        for key in needed:\n            col = find_col(cols, syn[key])\n            if col is not None:\n                score += 1\n                found_map[key] = col\n        if score > best_score:\n            best_score = score\n            best_df = df\n            best_cols = found_map\n    if best_df is None or best_score < 3:\n        return 0.0\n\n    df = best_df.copy()\n    # Coerce numeric for weight\n    weight_col = best_cols.get('weight')\n    ticker_col = best_cols.get('ticker')\n\n    coverage_score = 0.0\n    try:\n        if ticker_col is not None:\n            n = pd.Series(df[ticker_col]).astype(str).nunique(dropna=True)\n            if n >= 450:\n                coverage_score = 1.0\n            elif n >= 350:\n                coverage_score = 0.5\n            else:\n                coverage_score = 0.0\n    except Exception:\n        coverage_score = 0.0\n\n    weight_score = None\n    try:\n        if weight_col is not None:\n            w = pd.to_numeric(df[weight_col].astype(str).str.replace('%','', regex=False), errors='coerce')\n            wsum = np.nansum(w.values)\n            # If appears as 0-100 scale, normalize\n            if wsum > 10:\n                wsum = wsum/100.0\n            if 0.95 <= wsum <= 1.05:\n                weight_score = 1.0\n            elif 0.90 <= wsum <= 1.10:\n                weight_score = 0.5\n            else:\n                weight_score = 0.0\n    except Exception:\n        weight_score = None\n\n    parts = [coverage_score]\n    if weight_score is not None:\n        parts.append(weight_score)\n    if len(parts) == 0:\n        return 0.0\n    return float(np.mean(parts))"}, {"type": "code", "name": "P/E Coverage and Plausibility", "description": "Ensure LTM/NTM P/E columns exist with reasonable coverage and without extreme outliers dominating.", "weight": 0.75, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n    except Exception:\n        return 0.0\n\n    def norm(s):\n        return re.sub(r\"[^a-z0-9]+\", \" \", str(s).strip().lower()).strip()\n\n    pe_syn = {\n        'ltm': ['ltm pe','pe ltm','trailing pe','pe (ltm)','ttm pe','pe ttm'],\n        'ntm': ['ntm pe','pe ntm','forward pe','pe (ntm)']\n    }\n\n    def find_col(cols, keys):\n        cols_n = [norm(c) for c in cols]\n        for i, c in enumerate(cols_n):\n            for k in keys:\n                if k in c:\n                    return cols[i]\n        return None\n\n    best_df = None\n    best_hits = 0\n    for sh in xls.sheet_names:\n        try:\n            df = pd.read_excel(path, sheet_name=sh, nrows=2000)\n        except Exception:\n            continue\n        if df.empty:\n            continue\n        cols = list(df.columns)\n        ltm_col = find_col(cols, pe_syn['ltm'])\n        ntm_col = find_col(cols, pe_syn['ntm'])\n        hits = int(ltm_col is not None) + int(ntm_col is not None)\n        if hits > best_hits:\n            best_hits = hits\n            best_df = df\n            best_ltm = ltm_col\n            best_ntm = ntm_col\n    if best_df is None or best_hits == 0:\n        return 0.0\n\n    df = best_df\n    scores = []\n    def col_stats(col):\n        s = pd.to_numeric(df[col], errors='coerce')\n        # Consider positive values for plausibility percentile\n        pos = s[s > 0]\n        nonnull_ratio = float(s.notna().mean()) if len(s) else 0.0\n        p95 = float(pos.quantile(0.95)) if len(pos) >= 20 else np.nan\n        return nonnull_ratio, p95\n\n    try:\n        if best_ltm is not None:\n            r, p = col_stats(best_ltm)\n            # Coverage target >= 60%\n            cov = 1.0 if r >= 0.6 else (0.5 if r >= 0.4 else 0.0)\n            # Plausibility target: 95th percentile <= 500x (lenient)\n            pl = 1.0 if (not np.isnan(p) and p <= 500) else (0.5 if (not np.isnan(p) and p <= 800) else 0.0)\n            scores.append((cov + pl)/2.0)\n        if best_ntm is not None:\n            r, p = col_stats(best_ntm)\n            cov = 1.0 if r >= 0.6 else (0.5 if r >= 0.4 else 0.0)\n            pl = 1.0 if (not np.isnan(p) and p <= 500) else (0.5 if (not np.isnan(p) and p <= 800) else 0.0)\n            scores.append((cov + pl)/2.0)\n    except Exception:\n        return 0.0\n\n    if not scores:\n        return 0.0\n    return float(np.mean(scores))"}, {"type": "llm_judge", "name": "Aggregation Coherence Across Sheets", "description": "Visually confirm that sub-sector aggregation aligns with company-level data and that totals are coherent.", "weight": 4.5, "judge_prompt": "Review the workbook and assess aggregation coherence between the Sub-Sector Summary and Company Detail.\nLook for:\n- The Sub-Sector Summary has a clear table with Sub-Sector, No. of Companies, Market Cap, and % of Index.\n- The No. of Companies across sub-sectors sums to approximately 500 (\u00b110%).\n- The % of Index column on the Sub-Sector Summary sums to approximately 100% (\u00b15%).\n- Sub-sector naming appears consistent between sheets (e.g., similar naming patterns, not obviously mismatched).\n- There is indication of how aggregation was computed (e.g., count and sum/median with clear labels) and how weights relate to company-level weights.\n\nScoring Guidance:\n- 4.5: Clear, consistent aggregation; totals near 500 companies and ~100% index weight; naming consistent; methodology labeling present.\n- 3.0: Mostly consistent; minor discrepancies in totals or labels; names mostly consistent.\n- 1.5: Significant inconsistencies or unclear aggregation, but some structure present.\n- 0.0: No coherent aggregation or missing Sub-Sector Summary.", "expectation": "The summary table coherently aggregates company-level information with consistent naming and near-accurate totals."}, {"type": "llm_judge", "name": "Historical Index Average Comparison Support", "description": "Confirm that the workbook enables fast identification of above/below historical S&P P/E range (15\u201320x) at both company and sub-sector levels.", "weight": 4.5, "judge_prompt": "Check whether the workbook explicitly supports comparing LTM and NTM P/E to the historical S&P 500 range (approx. 15\u201320x):\n- Look for clearly labeled columns or flags at the company level (e.g., Above 20x / Below 15x / Between 15\u201320x) for LTM and/or NTM.\n- Look for similar flags or summary indicators at the sub-sector level (e.g., median NTM P/E above 20x flagged, or counts by bucket).\n- Verify that these fields appear numeric/sortable or filterable to enable quick analysis.\n\nScoring Guidance:\n- 4.5: Both company and sub-sector have explicit, sortable flags/columns for comparing to 15\u201320x (LTM and NTM), enabling rapid filtering.\n- 3.0: Present at either company or sub-sector level (not both), or only for one of LTM vs. NTM.\n- 1.5: Implicit support (e.g., text notes referencing 15\u201320x) but no concrete columns/flags.\n- 0.0: No support for comparing to historical range.", "expectation": "Filterable, explicit flags or columns that classify above/below 15\u201320x for both LTM and NTM at company and sub-sector levels."}, {"type": "llm_judge", "name": "Methodology, Sources, and Definitions Sufficiency", "description": "Verify the Methodology & Sources sheet provides transparent sourcing, definitions, and assumptions appropriate for April 2025.", "weight": 4.5, "judge_prompt": "Evaluate the Methodology & Sources (or similarly named) sheet:\n- Contains a list of data sources with publisher names or URLs.\n- Includes an as-of date near April 2025 for the data pull.\n- Defines LTM vs. NTM P/E, Dividend Yield, Annual EPS (CY+1), and Quarterly EPS (CQ+1).\n- Explains how % of Index was computed (e.g., market-cap weight), currency/units, and any handling for negative EPS or NA values.\n- At least 5 sentences of explanatory text, written clearly.\n\nScoring Guidance:\n- 4.5: All items present and clear.\n- 3.0: Minor omissions (e.g., missing one definition or incomplete as-of notation) but still usable.\n- 1.5: Multiple omissions; unclear or sparse methodology.\n- 0.0: Missing methodology or sources.", "expectation": "A transparent methodology sheet with sources, date, definitions, and assumptions sufficient for professional use."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality Assessment (Professional/Client Readiness)", "description": "Holistic assessment of presentation, usability, and client relevance for ECM decision-makers.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Usability and Presentation", "description": "Assess whether the workbook is easy to navigate and professionally presented.", "weight": 2.5, "judge_prompt": "Evaluate professional usability and presentation:\n- Clear sheet names; frozen header rows; consistent numeric formats for P/E, yields, market cap.\n- Sort/filter enabled or obviously usable (column headers in table format).\n- Readable layout without clutter; consistent thousands/percent formatting.\n- Minimal merged cells that would impede sorting/filtering.\n\nScore 0.0\u20132.5 based on how well it meets these criteria.", "expectation": "A clean, sortable workbook with consistent formatting and good navigation."}, {"type": "llm_judge", "name": "Analytical Insight and Client Value", "description": "Judge whether the file surfaces actionable insights for ECM conversations.", "weight": 2.5, "judge_prompt": "Does the workbook provide analytical value beyond raw data?\n- Useful flags (e.g., above/below 15\u201320x), top/bottom lists, or conditional formatting highlighting extremes.\n- Sub-sector rollups that quickly reveal potential over-enthusiasm or over-selling.\n- Optional summaries/charts/pivots that aid discovery.\n\nScore 0.0\u20132.5 based on usefulness for identifying opportunities/risks.", "expectation": "Highlights that help seniors/clients quickly see over/under-valued pockets."}, {"type": "llm_judge", "name": "Coverage and Completeness", "description": "Evaluate breadth and completeness of required fields across the index.", "weight": 2.5, "judge_prompt": "Assess coverage and completeness:\n- Nearly all S&P 500 companies included; sub-sectors fully represented.\n- Most companies have values for LTM/NTM P/E, Dividend Yield, Market Cap, and at least one of EPS (CY+1 or CQ+1).\n- Missing values are minimal or clearly annotated.\n\nScore 0.0\u20132.5 based on overall completeness and clarity of any gaps.", "expectation": "Comprehensive coverage with minimal unexplained gaps."}, {"type": "llm_judge", "name": "ECM Relevance and Professionalism", "description": "Ensure the deliverable aligns with ECM use-cases and is client-ready.", "weight": 2.5, "judge_prompt": "Evaluate ECM relevance and professionalism:\n- Aligns with ECM workflows (sortable lists to spot candidates for issuance/marketing angles).\n- Clear disclaimers/assumptions; citation of sources; as-of dating is prominent.\n- Tone and formatting suitable for leadership/client circulation.\n\nScore 0.0\u20132.5 based on relevance and polish.", "expectation": "Client-ready artifact aligned with ECM decision-making and communication."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "ff85ee58-bc9f-4aa2-806d-87edeabb1b81", "rubric": {"category_name": "Audio Mix Delivery: Tavarua (Audio and Video Technicians)", "rationale": "Self-documenting delivery is required because we cannot directly \"hear\" quality in this environment. The rubric forces a verifiable delivery document plus machine-checkable artifacts (final WAV, loudness CSV, sync log CSV). Stage 1 mandates an exact delivery document structure that makes verification trivial. Stage 2 mixes lightweight code checks (format, bounds, timing consistency) with LLM cross-checks against the documentation and logs. Stage 3 assesses professionalism and creative/technical rationale communicated in the report.", "max_total_score": 24.0, "stages": [{"name": "Stage 1 \u2014 Delivery Shape Enforcement (Gate)", "description": "MANDATE an auditable delivery package shape so downstream verification is trivial. The primary output must be a professional PDF/DOCX delivery document with clearly labeled sections and embedded/file-listed artifacts.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.0, "rules": [{"type": "llm_judge", "name": "Delivery Document Structure Gate", "description": "Check the primary output is a professional Delivery Document (PDF/DOCX) for Tavarua with required sections and tables.", "weight": 3.0, "judge_prompt": "You are validating ONLY the presence and structure of a professional delivery document for an audio mix (no need to validate correctness yet).\n\nRequirements (be flexible with exact section names, but all elements must be PRESENT):\n- File format: PDF or DOCX (not plain text, not spreadsheet)\n- Visible title with the project name (e.g., \u201cTavarua Final Mix Delivery\u201d or similar), author, date, version\n- Section: Delivery Checklist or File Manifest listing included deliverables, with filenames and brief purpose. Must mention:\n  * Final stereo WAV mix at 24-bit, 48 kHz\n  * Loudness report CSV\n  * Sync/Alignment log CSV\n  * Mix Report (this very document) or equivalent\n- Section: Technical Specs (sample rate 48 kHz, bit depth 24-bit, channels, target loudness: -16 LUFS \u00b11, true peak \u2264 -0.1 dBFS)\n- Section: Sax Resync & Editing Log containing a TABLE (not just prose) with columns such as: [Event/Region | Timestamp (s) | Action/Adjustment | Target Grid Note (1/8) | Deviation (ms)] with at least a few rows\n- Section: Processing Chain & Effects with chain order and specific processors (EQ, compression, reverb, delay, stereo/spatial) and at least a few key parameters per processor\n- Section: Loudness Metering Summary with claimed Integrated LUFS and True Peak and the meter/tool used\n- Section: Export/Bounce Settings (24-bit, 48 kHz, dither strategy if applicable, inter-sample peak control/true-peak limiting)\n- Section: QA Checklist (artifacts checks, mono compatibility or masking check, no clipping)\n\nScoring:\n- 3.0: All required sections present with clear headers; table present for the resync/editing log; professional layout\n- 2.0: Present but missing exactly one supporting section (e.g., QA checklist) or resync table is too minimal\n- 1.0: Present but missing two required sections or the resync/editing content is prose only (no table)\n- 0.0: Not a PDF/DOCX or missing multiple core sections (manifest, technical specs, or resync log)\n\nOnly check structure and presence, not correctness of content.", "expectation": "A polished PDF/DOCX report titled for Tavarua with the manifest, technical specs, resync/editing log table, processing chain details, loudness summary, export settings, and QA checklist."}, {"type": "llm_judge", "name": "File Naming and Manifest Specificity", "description": "Ensure the delivery document explicitly lists the expected files with clear, consistent names.", "weight": 3.0, "judge_prompt": "Check the Delivery Document (PDF/DOCX) for a clear file manifest that enumerates the deliverables with explicit filenames and descriptions.\n\nExpected deliverables (naming can be close variants; be flexible but look for clear intent):\n- Final mix WAV at 24-bit/48 kHz, e.g., \u201cTAVARUA_FINAL_MIX.wav\u201d\n- Loudness report CSV, e.g., \u201cLOUDNESS_REPORT.csv\u201d\n- Sync log CSV, e.g., \u201cSYNC_LOG.csv\u201d or \u201cALIGNMENT_LOG.csv\u201d\n- The Delivery/Mix Report document (this PDF/DOCX)\n\nScoring:\n- 3.0: Manifest lists all four deliverables with unambiguous names and purpose\n- 2.0: Lists three of four clearly\n- 1.0: Lists only two clearly or vague/ambiguous naming\n- 0.0: No clear manifest or filenames not stated\n\nOnly validate the presence of a clear manifest in the document; do not verify files yet.", "expectation": "A manifest section that clearly enumerates final WAV, loudness CSV, sync CSV, and the report itself with recognizable names."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Mixed Code + LLM)", "description": "Now that the structure exists, verify technical compliance and internal consistency via machine-checkable artifacts (CSV logs, WAV headers) and targeted LLM cross-checks.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Final Audio Format Compliance", "description": "Verify a final WAV exists and is 24-bit, 48 kHz. Partial credit if only one criterion passes.", "weight": 0.5, "code": "import os, wave, contextlib, re\n\ndef evaluate(workflow, context):\n    # Find a candidate final WAV (prefer names containing both 'tavarua' and 'final')\n    wav_res = None\n    fallbacks = []\n    for res in context.get_all_outputs():\n        try:\n            path = context.files.get_path(res.id)\n            name = path.name.lower()\n            if name.endswith('.wav'):\n                if 'final' in name and 'tavarua' in name:\n                    wav_res = res\n                    break\n                fallbacks.append(res)\n        except Exception:\n            continue\n    if wav_res is None and fallbacks:\n        wav_res = fallbacks[0]\n    if wav_res is None:\n        return 0.0, 'No WAV output found.'\n\n    # Read WAV header\n    try:\n        path = context.files.get_path(wav_res.id)\n        with contextlib.closing(wave.open(str(path), 'rb')) as wf:\n            fr = wf.getframerate()\n            sw = wf.getsampwidth()  # bytes per sample\n            # channels = wf.getnchannels()\n        sr_ok = (fr == 48000)\n        bd_ok = (sw == 3)  # 3 bytes = 24-bit PCM\n        score = 1.0 if (sr_ok and bd_ok) else (0.5 if (sr_ok or bd_ok) else 0.0)\n        fb = f'SampleRate={fr} Hz (target 48000); SampleWidth={sw*8} bit (target 24).'\n        return score, fb\n    except Exception as e:\n        return 0.0, f'Failed to read WAV header: {e}'"}, {"type": "code", "name": "Loudness Report Bounds", "description": "Check LOUDNESS_REPORT.csv exists and that Integrated LUFS is within [-17, -15], and True Peak \u2264 -0.1 dBFS. Partial credit if only one metric passes.", "weight": 0.5, "code": "import re\nimport numpy as np\nimport pandas as pd\n\ndef _to_float(val):\n    try:\n        if pd.isna(val):\n            return None\n        s = str(val)\n        # strip units and keep number, dot, minus\n        m = re.findall(r'-?\\d+\\.?\\d*', s)\n        if not m:\n            return None\n        return float(m[0])\n    except Exception:\n        return None\n\ndef _find_csv(context, keywords):\n    for res in context.get_all_outputs():\n        try:\n            p = context.files.get_path(res.id)\n            name = p.name.lower()\n            if name.endswith('.csv') and any(k in name for k in keywords):\n                return res\n        except Exception:\n            continue\n    return None\n\ndef _extract_metrics(df):\n    df_cols = [c.lower() for c in df.columns]\n    metrics = {'integrated_lufs': None, 'true_peak_dbfs': None}\n\n    # Case 1: key-value style with Metric/Value columns\n    if ('metric' in df_cols and ('value' in df_cols or 'val' in df_cols)):\n        mcol = df.columns[df_cols.index('metric')]\n        vcol = df.columns[df_cols.index('value') if 'value' in df_cols else df_cols.index('val')]\n        for _, row in df.iterrows():\n            key = str(row[mcol]).strip().lower()\n            val = _to_float(row[vcol])\n            if val is None:\n                continue\n            if 'lufs' in key and ('integrated' in key or key.startswith('i')):\n                metrics['integrated_lufs'] = val\n            if ('true' in key and 'peak' in key) or key in {'tp','true_peak'}:\n                metrics['true_peak_dbfs'] = val\n    else:\n        # Case 2: wide format\n        for c in df.columns:\n            lc = c.lower()\n            if ('lufs' in lc and 'integrated' in lc) or lc in {'integrated_lufs','lufs_i','i','integrated'}:\n                v = _to_float(df[c].iloc[0]) if len(df) else None\n                if v is not None:\n                    metrics['integrated_lufs'] = v\n            if ('true' in lc and 'peak' in lc) or lc in {'true_peak','tp','true_peak_dbfs'}:\n                v = _to_float(df[c].iloc[0]) if len(df) else None\n                if v is not None:\n                    metrics['true_peak_dbfs'] = v\n    return metrics\n\ndef evaluate(workflow, context):\n    res = _find_csv(context, ['loudness'])\n    if res is None:\n        # fallback: any csv\n        res = _find_csv(context, ['loud', 'meter', 'report'])\n    if res is None:\n        return 0.0, 'No loudness CSV found.'\n    try:\n        df = context.files.read_csv(res.id)\n    except Exception as e:\n        return 0.0, f'Cannot read loudness CSV: {e}'\n\n    metrics = _extract_metrics(df)\n    i_lufs = metrics['integrated_lufs']\n    tp = metrics['true_peak_dbfs']\n\n    score = 0.0\n    notes = []\n    if i_lufs is not None:\n        if -17.0 <= i_lufs <= -15.0:\n            score += 0.5\n            notes.append(f'Integrated LUFS in range: {i_lufs:.2f} LUFS')\n        else:\n            notes.append(f'Integrated LUFS out of range: {i_lufs}')\n    else:\n        notes.append('Integrated LUFS not found')\n\n    if tp is not None:\n        if tp <= -0.1:\n            score += 0.5\n            notes.append(f'True Peak OK: {tp:.2f} dBFS')\n        else:\n            notes.append(f'True Peak too high: {tp} dBFS')\n    else:\n        notes.append('True Peak not found')\n\n    return score, '; '.join(notes)"}, {"type": "code", "name": "Sync Log Timing Consistency", "description": "Validate SYNC_LOG.csv shows events landing on 1/8-note grid at 50 BPM within \u00b11/16-note tolerance. Uses timestamp spacing with a tolerant check.", "weight": 0.5, "code": "import math\nimport pandas as pd\nimport re\n\ndef _find_sync_csv(context):\n    for res in context.get_all_outputs():\n        try:\n            p = context.files.get_path(res.id)\n            name = p.name.lower()\n            if name.endswith('.csv') and any(k in name for k in ['sync', 'align', 'timing']):\n                return res\n        except Exception:\n            continue\n    return None\n\ndef _parse_time_series(df):\n    # Try common columns for seconds\n    candidates = [c for c in df.columns if str(c).lower() in ['time_seconds','seconds','time_s','timestamp_s','time','t']]\n    for c in candidates:\n        try:\n            series = df[c].apply(lambda x: float(str(x).strip()))\n            return series.dropna().tolist()\n        except Exception:\n            continue\n    # Try to parse mm:ss or mm:ss.ms formats in a generic 'timestamp' column\n    for c in df.columns:\n        lc = str(c).lower()\n        if 'time' in lc or 'stamp' in lc:\n            vals = []\n            ok = True\n            for v in df[c].fillna(''):\n                s = str(v).strip()\n                m = re.match(r'^(\\d+):(\\d{1,2})(?:\\.(\\d{1,3}))?$', s)\n                if m:\n                    mins = int(m.group(1)); secs = int(m.group(2)); ms = int(m.group(3) or 0)\n                    vals.append(mins*60 + secs + ms/1000.0)\n                else:\n                    ok = False; break\n            if ok and vals:\n                return vals\n    return []\n\ndef evaluate(workflow, context):\n    res = _find_sync_csv(context)\n    if res is None:\n        return 0.0, 'No sync/alignment CSV found.'\n    try:\n        df = context.files.read_csv(res.id)\n    except Exception as e:\n        return 0.0, f'Cannot read sync CSV: {e}'\n\n    times = sorted([t for t in _parse_time_series(df) if t is not None])\n    if len(times) < 3:\n        return 0.0, 'Not enough sync events (<3).'\n\n    eighth = 60.0/50.0/2.0  # 0.6 s\n    tol = (60.0/50.0)/4.0   # \u00b11/16-note = 0.3 s\n\n    dts = [times[i+1]-times[i] for i in range(len(times)-1)]\n    ok = 0\n    for dt in dts:\n        mult = round(dt / eighth)\n        err = abs(dt - mult*eighth)\n        if err <= tol:\n            ok += 1\n    frac = ok/len(dts) if dts else 0.0\n    return frac, f'{ok}/{len(dts)} intervals within \u00b11/16 of 1/8-note grid (0.6s).'"}, {"type": "code", "name": "Package Completeness (Files Present)", "description": "Confirm the presence of the key artifacts among outputs: final WAV, loudness CSV, sync CSV, and a PDF/DOCX/MD report. Partial credit per item.", "weight": 0.5, "code": "def evaluate(workflow, context):\n    found_wav = False\n    found_loud = False\n    found_sync = False\n    found_report = False\n\n    outs = context.get_all_outputs()\n    for res in outs:\n        try:\n            p = context.files.get_path(res.id)\n            name = p.name.lower()\n            if name.endswith('.wav') and 'final' in name:\n                found_wav = True\n            if name.endswith('.csv') and any(k in name for k in ['loud', 'meter']):\n                found_loud = True\n            if name.endswith('.csv') and any(k in name for k in ['sync','align','timing']):\n                found_sync = True\n            if any(name.endswith(ext) for ext in ['.pdf','.docx','.md','.markdown']) and any(k in name for k in ['mix','report','delivery']):\n                found_report = True\n        except Exception:\n            continue\n\n    count = sum([found_wav, found_loud, found_sync, found_report])\n    score = count/4.0\n    fb = f'WAV={found_wav}, LoudnessCSV={found_loud}, SyncCSV={found_sync}, Report={found_report}'\n    return score, fb"}, {"type": "llm_judge", "name": "Loudness Cross-Check (Report vs CSV)", "description": "Cross-verify the Delivery Document\u2019s stated Integrated LUFS and True Peak against LOUDNESS_REPORT.csv values.", "weight": 2.0, "judge_prompt": "Open the Delivery/Mix Report (PDF/DOCX) and the LOUDNESS_REPORT.csv. Verify that:\n- The report states Integrated LUFS (target -16 \u00b11). The CSV shows a matching integrated LUFS within \u00b10.2 LU of the report\u2019s claim.\n- The report states a True Peak target \u2264 -0.1 dBFS. The CSV true peak matches within \u00b10.1 dB.\n- The meter/tool used is named in the report.\n\nScoring:\n- 2.0: Both metrics present in report and CSV; values match within tolerances; meter named\n- 1.0: One metric present/matching or both present but mismatched beyond tolerance\n- 0.0: Metrics not present or CSV not provided\n\nJudge by reading both files; do not assume values.", "expectation": "Report and CSV agree on Integrated LUFS (~-16 LUFS) and True Peak (\u2264 -0.1 dBFS), with the report citing the meter used."}, {"type": "llm_judge", "name": "Sync Methodology and Evidence", "description": "Ensure the resync/editing methodology is explained and supported by SYNC_LOG.csv with multiple events and acceptable deviations.", "weight": 2.0, "judge_prompt": "Open the Delivery/Mix Report and the SYNC_LOG.csv (or ALIGNMENT_LOG.csv). Validate:\n- Report describes how the sax was resynced to the reference (e.g., transient matching, visual alignment, time-stretch/elastic audio, nudge to grid).\n- Log table has at least 3 distinct events/regions with timestamps and indicates alignment to a 1/8-note grid at 50 BPM, including a deviation per event.\n- Deviations are within \u00b11/16 note (\u2248 \u00b10.3 s) based on the log.\n\nScoring:\n- 2.0: Clear methodology + \u22653 events in the log + deviations within tolerance\n- 1.0: Methodology present but log minimal (e.g., only 2 events) or some deviations outside tolerance\n- 0.0: No methodology or no usable sync log\n\nOnly use the documentation/logs; do not infer from audio.", "expectation": "A concrete description of resync steps and a multi-row sync log with deviations within \u00b10.3 s of the 0.6 s 1/8-note grid."}, {"type": "llm_judge", "name": "Effects Processing Appropriateness", "description": "Assess whether reverb/delay/EQ/stereo choices described would achieve a tasteful blend without muddying the mix.", "weight": 2.0, "judge_prompt": "From the Delivery/Mix Report, evaluate whether the described effects chain and settings for the sax (EQ, compression, reverb, delay, stereo/spatial treatment) are appropriate for an experimental rock interlude at 50 BPM, aiming to fill the stereo image without losing clarity.\n\nLook for:\n- EQ choices that control low-mid buildup; high-pass on reverb sends\n- Reverb predelay, decay times, and stereo width coherent with tempo/ambience\n- Delay times synced (e.g., 1/8, dotted 1/8) with feedback and filtering to avoid clutter\n- Dynamic control (e.g., gentle compression, sidechain ducking against the mix if claimed)\n- Gain staging and send levels stated sensibly\n\nScoring:\n- 2.0: Settings are specific and technically sound for the stated goal\n- 1.0: Generally reasonable but vague or missing key safeguards (e.g., HPF on verb)\n- 0.0: Inappropriate or non-credible settings", "expectation": "Credible signal chain and parameterization that widens/ambiences the sax tastefully while preserving clarity."}, {"type": "llm_judge", "name": "Safety and Export Settings Verification", "description": "Check export readiness and safety: dither/bit-depth handling, inter-sample peaks, and naming/specs alignment.", "weight": 2.0, "judge_prompt": "From the Delivery/Mix Report, verify:\n- Export/bounce settings explicitly state 24-bit, 48 kHz stereo for the final WAV\n- Consideration of dither (if bit-depth reduction is mentioned) and true-peak limiting or inter-sample peak management\n- File naming matches the manifest (e.g., includes \u201cTAVARUA\u201d and \u201cFINAL\u201d)\n\nScoring:\n- 2.0: All listed items present and consistent\n- 1.0: Partially present or vague\n- 0.0: Missing/contradictory information", "expectation": "Clear, consistent export settings at 24/48 with safety headroom/true-peak control and sensible file naming."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality Assessment (LLM)", "description": "Holistic evaluation of professional rigor and communicative quality based on the Delivery Document and artifacts.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity and Professionalism of Documentation", "description": "Assess organization, readability, and professionalism of the Delivery Document.", "weight": 2.0, "judge_prompt": "Evaluate the Delivery/Mix Report for:\n- Clear structure with informative headers, tables, and consistent formatting\n- Concise, readable explanations free of jargon overload\n- Visual aids (optional) such as screenshots/waveforms/spectrograms to support claims\n\nScoring:\n- 2.0: Highly clear, professional, and easy to follow\n- 1.0: Adequate but with organizational or clarity issues\n- 0.0: Confusing or unprofessional presentation", "expectation": "A polished, easy-to-follow report with clear sections and tables."}, {"type": "llm_judge", "name": "Reproducibility and Technical Detail", "description": "Can another engineer reproduce the result from the documentation?", "weight": 2.0, "judge_prompt": "Judge whether another engineer could reasonably reproduce the result using the report:\n- BPM and grid explicitly noted; alignment method reproducible\n- Processing chain lists specific plugins/tools and key parameters\n- Loudness metering tool and settings indicated\n\nScoring:\n- 2.0: Sufficiently detailed to reproduce\n- 1.0: Partially reproducible; missing some key parameters\n- 0.0: Too vague to reproduce", "expectation": "Explicit BPM/grid, alignment steps, plugins, and key parameters enabling reproduction."}, {"type": "llm_judge", "name": "Cohesion with Creative Intent", "description": "Assess whether the described choices serve the creative goal (immersive, cohesive interlude with sax).", "weight": 2.0, "judge_prompt": "From the report, does the stated mixing approach for the sax contribute to a cohesive, immersive interlude that blends with the instrumental?\nConsider:\n- Justification of spatial and tonal choices relative to the mix context\n- Handling of transitions and ambience to unify with surrounding material\n\nScoring:\n- 2.0: Strong, well-justified creative rationale aligned with goals\n- 1.0: Some rationale but limited linkage to creative goals\n- 0.0: No clear link between choices and creative intent", "expectation": "A clear rationale connecting technical decisions to the immersive, cohesive goal."}, {"type": "llm_judge", "name": "Technical Credibility and Risk Management", "description": "Evaluate whether the approach demonstrates solid engineering judgment and risk mitigation.", "weight": 2.0, "judge_prompt": "Assess the overall technical credibility:\n- Sensible gain staging, avoidance of clipping/muddiness, mono compatibility considerations\n- Attention to inter-sample peaks, headroom, and export best practices\n- Awareness of potential artifacts from time-stretch/editing and how they were mitigated\n\nScoring:\n- 2.0: Strong technical judgment with explicit safeguards\n- 1.0: Generally sound but missing some considerations\n- 0.0: Technically naive or risky approach", "expectation": "Evidence of thoughtful safeguards and technically sound practices suitable for release."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "61f546a8-c374-467f-95cc-d0d9b5656eb6", "rubric": {"category_name": "Real Estate: Counter and Rental Clerks \u2014 Turnover Scheduling Report", "rationale": "This rubric enforces a self-documenting, verifiable PDF report with two tables that make schedule checks trivial. Stage 1 strictly mandates the exact structure and format as a gate. Stage 2 mixes precise code checks (dates, weekdays/holidays, appliance extra day, overlap) with LLM cross-reference and reasoning about consistency and rule application. Stage 3 assesses professional quality and managerial usefulness.", "max_total_score": 15.0, "stages": [{"name": "Stage 1 \u2014 Structured Report Format Gate", "description": "Enforce the exact PDF structure so verification is possible. LLM-only gate checks file format, section headers, and table schemas.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.0, "rules": [{"type": "llm_judge", "name": "Structured PDF with Two Required Sections and Tables", "description": "Verify the candidate produced a PDF/DOCX with the exact structure that enables automated verification.", "weight": 3.0, "judge_prompt": "You are verifying ONLY the presence and structure of a report, not its correctness.\n\nCheck the candidate output (PDF or DOCX only) for the following required structure. Be flexible with section titles (e.g., 'Vendor Services' vs 'Vendor Schedule') but the content and columns must exist. Dates must appear in ISO format (YYYY-MM-DD).\n\nRequired File Format:\n- Must be a PDF or DOCX (not Excel/CSV/Markdown). At least 1 full page.\n\nSection 1 (by vendor): Header similar to 'Vendor Services Schedule' or 'Section 1 \u2014 Vendor Services'. Must include ONE clear table with columns (flexible wording allowed but same meaning):\n- Vendor\n- Apartment #\n- Service Type (e.g., painting, carpet, appliance install)\n- New Appliance Needed? (Yes/No; if Yes, list type)\n- Scheduled Date (YYYY-MM-DD)\n- Make-Ready Adjustment (Yes/No)\n- Adjusted Make-Ready Date (if applicable)\n\nAlso include a short note (2\u20133 sentences) summarizing how the scheduling rules were applied (e.g., no two vendors same day, staff takes 2 days, deliveries on business days).\n\nSection 2 (by apartment): Header similar to 'Unit Work Plan' or 'Section 2 \u2014 Apartment Work Plan'. Must include ONE clear table with columns (flexible wording allowed but same meaning):\n- Apartment #\n- Work Type (include any appliance deliveries/installs explicitly)\n- Party (Vendor Name or the literal phrase 'our staff')\n- Date (YYYY-MM-DD)\n\nStructural expectations:\n- Each row in Section 2 represents a single work event on a single date for a single apartment.\n- The literal phrase 'our staff' appears in the Party column for on-site staff work days.\n\nScoring:\n- 3.0: Valid PDF/DOCX AND both sections present AND both tables present with all listed columns AND dates shown in ISO format (YYYY-MM-DD) AND a short note on how rules were applied.\n- 2.0: Valid PDF/DOCX AND both sections present with both tables, but 1\u20132 minor column issues OR missing the short note OR inconsistent date formatting in a few places.\n- 1.0: Valid PDF/DOCX but only one section/table present or multiple key columns missing.\n- 0.0: Not PDF/DOCX OR missing both tables/sections.\n\nOnly check structure and presence. Do NOT judge scheduling correctness or quality.", "expectation": "A PDF/DOCX with two clearly labeled sections and the exact tables/columns that enable automated verification."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Rule Compliance", "description": "Verify that the schedule follows constraints: dates after move-out, weekdays/holidays rules, no vendor overlaps, staff duration, appliance extra day, and cross-section consistency.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 5.0, "rules": [{"type": "code", "name": "Date Validity and Business-Day Compliance", "description": "Programmatically validate that scheduled dates are on/after 2025-07-01, staff work occurs on weekdays (Mon\u2013Fri), and deliveries occur on business days (no weekends or US federal holidays).", "weight": 0.8, "code": "import re\nimport numpy as np\n\ndef evaluate(workflow, context):\n    weight = 0.8\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output.\"\n\n    # Try to read text from PDF first, then DOCX\n    text = None\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0, \"Unable to extract text from document.\"\n    if not text or len(text.strip()) == 0:\n        return 0.0, \"Empty document text.\"\n\n    # Find all dates (YYYY-MM-DD or YYYY/M/D etc.) and standardize to YYYY-MM-DD\n    date_pattern = re.compile(r\"\\b(20\\d{2})[\\-/](\\d{1,2})[\\-/](\\d{1,2})\\b\")\n    matches = list(date_pattern.finditer(text))\n    if not matches:\n        return 0.0, \"No dates found in document.\"\n\n    def std_date(m):\n        y, mo, d = int(m.group(1)), int(m.group(2)), int(m.group(3))\n        return f\"{y:04d}-{mo:02d}-{d:02d}\"\n\n    # Classify each date based on nearby context words\n    lower_text = text.lower()\n    staff_dates = []\n    delivery_dates = []\n    all_dates = []\n\n    for m in matches:\n        d = std_date(m)\n        start = m.start()\n        window_l = max(0, start - 120)\n        window_r = min(len(lower_text), start + 120)\n        win = lower_text[window_l:window_r]\n        is_staff = ('our staff' in win)\n        is_delivery = (('deliver' in win) or ('appliance' in win) or ('install' in win))\n        # Record\n        all_dates.append(d)\n        if is_staff:\n            staff_dates.append(d)\n        if is_delivery:\n            delivery_dates.append(d)\n\n    # Helper: business day/weekday checks using numpy (avoid weekends; holidays optional)\n    def to_np_day(ds):\n        try:\n            return np.array(ds, dtype='datetime64[D]')\n        except Exception:\n            return None\n\n    # US Federal holidays in 2025 (observed dates)\n    us_holidays = [\n        '2025-01-01','2025-01-20','2025-02-17','2025-05-26','2025-06-19','2025-07-04',\n        '2025-09-01','2025-10-13','2025-11-11','2025-11-27','2025-12-25'\n    ]\n\n    # Check 1: All dates on/after 2025-07-01 (move-outs were 2025-06-30)\n    moveout_cutoff = np.array('2025-07-01', dtype='datetime64[D]')\n    on_or_after = []\n    for ds in all_dates:\n        nd = to_np_day(ds)\n        if nd is not None:\n            on_or_after.append(bool(nd >= moveout_cutoff))\n    s_moveout = (sum(on_or_after) / len(on_or_after)) if on_or_after else 0.0\n\n    # Check 2: Staff only on weekdays (Mon-Fri). Holidays not enforced for staff per guideline.\n    staff_weekday = []\n    for ds in staff_dates:\n        nd = to_np_day(ds)\n        if nd is not None:\n            # np.is_busday with no holidays enforces Mon-Fri only\n            staff_weekday.append(bool(np.is_busday(nd)))\n    s_staff = (sum(staff_weekday) / len(staff_weekday)) if staff_weekday else 0.0\n\n    # Check 3: Deliveries only on business days (exclude weekends + US federal holidays)\n    delivery_busday = []\n    for ds in delivery_dates:\n        nd = to_np_day(ds)\n        if nd is not None:\n            delivery_busday.append(bool(np.is_busday(nd, holidays=us_holidays)))\n    s_delivery = (sum(delivery_busday) / len(delivery_busday)) if delivery_busday else 0.0\n\n    # Weighted blend for this rule\n    parts = []\n    parts.append((s_moveout, 0.3))\n    parts.append((s_staff, 0.3))\n    parts.append((s_delivery, 0.4))\n    ratio = 0.0\n    if parts:\n        ratio = sum(v*w for v,w in parts) / sum(w for _,w in parts)\n    return ratio * weight, f\"Moveout-on/after: {s_moveout:.2f}, Staff weekdays: {s_staff:.2f}, Deliveries busdays: {s_delivery:.2f}\""}, {"type": "code", "name": "Appliance Extra Day, Staff 2-Day Minimum, and Basic Overlap Detection", "description": "Validate that each apartment with an appliance installation/delivery (excluding hot water tank) has at least 3 unique scheduled dates and that every apartment has at least 2 'our staff' days. Also check obvious same-day multi-vendor overlaps per unit. Verify presence of Make-Ready adjustment fields.", "weight": 0.7, "code": "import re\nimport numpy as np\n\ndef evaluate(workflow, context):\n    weight = 0.7\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output.\"\n\n    # Extract text\n    text = None\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0, \"Unable to extract text from document.\"\n    if not text or len(text.strip()) == 0:\n        return 0.0, \"Empty document text.\"\n\n    lower = text.lower()\n\n    # Regex patterns\n    date_re = re.compile(r\"\\b(20\\d{2})[\\-/](\\d{1,2})[\\-/](\\d{1,2})\\b\")\n    apt_re = re.compile(r\"(?:apt(?:\\.|#)?|apartment|unit)\\s*([A-Za-z0-9\\-]+)\", re.IGNORECASE)\n\n    def std_date(m):\n        y, mo, d = int(m.group(1)), int(m.group(2)), int(m.group(3))\n        return f\"{y:04d}-{mo:02d}-{d:02d}\"\n\n    # Build per-apartment schedule\n    matches = list(date_re.finditer(text))\n    if not matches:\n        return 0.0, \"No dates found to validate schedules.\"\n\n    apts = {}\n\n    for m in matches:\n        ds = std_date(m)\n        start = m.start()\n        window_l = max(0, start - 150)\n        window_r = min(len(lower), start + 150)\n        win = lower[window_l:window_r]\n\n        # Find nearest apartment identifier around the date\n        apt_id = None\n        # Search left window first (more likely in same row preface)\n        left_chunk = lower[max(0, start-150):start]\n        right_chunk = lower[start:min(len(lower), start+150)]\n        left_match = list(apt_re.finditer(left_chunk))\n        if left_match:\n            apt_id = left_match[-1].group(1)\n        else:\n            right_match = apt_re.search(right_chunk)\n            if right_match:\n                apt_id = right_match.group(1)\n\n        if not apt_id:\n            # If no apt found, skip this date (can't attribute)\n            continue\n\n        entry = apts.setdefault(apt_id, {\n            'dates': set(),\n            'staff_dates': set(),\n            'has_appliance': False,\n            'has_hot_water': False,\n            'events_by_date': {}\n        })\n\n        is_staff = ('our staff' in win)\n        # Treat any mention of delivery/appliance/install as appliance-related\n        has_appliance_kw = (('appliance' in win) or ('deliver' in win) or ('install' in win))\n        is_hot_water = (('hot water tank' in win) or ('water heater' in win))\n\n        entry['dates'].add(ds)\n        if is_staff:\n            entry['staff_dates'].add(ds)\n        if has_appliance_kw:\n            entry['has_appliance'] = True\n        if is_hot_water:\n            entry['has_hot_water'] = True\n\n        events = entry['events_by_date'].setdefault(ds, [])\n        if is_staff:\n            events.append('staff')\n        else:\n            # Not staff => assume vendor-related work\n            events.append('vendor')\n\n    if not apts:\n        return 0.0, \"Could not attribute any dates to apartments.\"\n\n    # Evaluate conditions\n    apt_ids = list(apts.keys())\n    staff_ok = []\n    extra_day_ok = []\n    overlap_ok = []\n\n    for aid in apt_ids:\n        info = apts[aid]\n        # Staff must have at least 2 distinct days\n        staff_ok.append(len(info['staff_dates']) >= 2)\n\n        # Appliance extra day rule (exclude pure hot water-only cases)\n        needs_extra = info['has_appliance'] and not info['has_hot_water']\n        if needs_extra:\n            extra_day_ok.append(len(info['dates']) >= 3)\n        else:\n            # If no appliance (or only hot water), count as pass\n            extra_day_ok.append(True)\n\n        # No two vendors on the same day per unit (basic detection)\n        unit_ok = True\n        for d, evs in info['events_by_date'].items():\n            vendor_count = sum(1 for e in evs if e == 'vendor')\n            if vendor_count > 1:\n                unit_ok = False\n                break\n        overlap_ok.append(unit_ok)\n\n    # Make-Ready adjustment presence: if any 'Yes' near 'Make-Ready' there should be a date nearby on the same line\n    make_ready_ok = True\n    lines = [ln.strip().lower() for ln in text.splitlines() if ln.strip()]\n    for ln in lines:\n        if ('make' in ln and 'ready' in ln) and ('yes' in ln):\n            if not date_re.search(ln):\n                make_ready_ok = False\n                break\n\n    # Compute ratios\n    def frac_true(arr):\n        return (sum(1 for x in arr if x) / len(arr)) if arr else 0.0\n\n    s_staff = frac_true(staff_ok)\n    s_extra = frac_true(extra_day_ok)\n    s_overlap = frac_true(overlap_ok)\n    s_make_ready = 1.0 if make_ready_ok else 0.0\n\n    # Weighted aggregation inside this rule\n    ratio = (0.3*s_staff + 0.3*s_extra + 0.3*s_overlap + 0.1*s_make_ready)\n    feedback = f\"Staff>=2: {s_staff:.2f}, Appliance extra day: {s_extra:.2f}, No vendor overlaps: {s_overlap:.2f}, Make-Ready fields: {s_make_ready:.2f}\"\n    return ratio * weight, feedback"}, {"type": "llm_judge", "name": "Cross-Section Consistency and Mapping", "description": "Check that apartments/vendors listed in Section 1 line up with Section 2 rows, and that vendor assignments and appliance flags are consistent.", "weight": 2.5, "judge_prompt": "Evaluate the report for internal consistency between sections. Use the visual tables.\n\nChecks:\n- Every apartment listed under any vendor in Section 1 appears in Section 2 with at least two rows that specify Party='our staff' on two distinct dates.\n- If Section 1 marks 'New Appliance Needed? = Yes' and names a type, Section 2 should include a corresponding delivery/installation event for the same apartment, labeled as a vendor (not 'our staff'). Hot water tank installs do not require the extra day but should still appear if marked.\n- Vendor names in Section 1 match the Party names used in Section 2 for vendor work on those units (reasonable string match is fine).\n\nScoring (2.5 max):\n- 2.5: All apartments/vendor mappings consistent; all staff minimum days present; appliance flags correctly reflected with matching work entries.\n- 1.7: Minor mismatches (1\u20132 units) but generally consistent.\n- 0.8: Several inconsistencies or missing mappings.\n- 0.0: Sections do not align or mappings largely missing.", "expectation": "Clean cross-references between Section 1 and Section 2 with correct staff days and appliance entries."}, {"type": "llm_judge", "name": "No Same-Day Multi-Vendor per Unit (LLM Reasoning)", "description": "Visually verify that no unit has more than one vendor scheduled on the same day. Staff may work on a different day.", "weight": 2.0, "judge_prompt": "Using the tables (especially Section 2), verify the rule: No two vendors can work in the same unit on the same day.\n\nConsider for each apartment number:\n- Look for cases where the same apartment has multiple rows on the same date with different vendor names (Party not equal to 'our staff'). If found, it's a violation.\n- Staff days ('our staff') do not count as vendor work; ignore staff-only overlaps.\n\nScoring (2.0 max):\n- 2.0: No violations identified across all units.\n- 1.2: One minor/edge-case violation.\n- 0.5: Multiple instances but less than half of units affected.\n- 0.0: Widespread violations or impossible to verify due to structure issues.", "expectation": "Zero same-day vendor overlaps per unit."}, {"type": "llm_judge", "name": "Timeline Logic: Move-out, Staff Duration, and Appliance Extra Day", "description": "Verify rule application in the actual schedule: starts after move-out, staff 2-day minimum, and appliance extra day (except hot water tank).", "weight": 2.0, "judge_prompt": "Check the planned schedules follow the rules:\n- All scheduled dates occur on/after 2025-07-01 (move-outs were 2025-06-30).\n- Each apartment has at least two distinct 'our staff' workdays (Mon\u2013Fri only). You only need to check presence/count and weekdays; ignore holidays for staff.\n- If an apartment has an appliance delivery/installation (other than a hot water tank), the overall timeline includes at least one additional day beyond the two staff days (i.e., total of 3 or more distinct dates for that unit). Hot water tank installs do not require the extra day.\n\nScoring (2.0 max):\n- 2.0: All rules clearly satisfied for all units.\n- 1.3: One minor deviation or unclear case.\n- 0.6: Several deviations.\n- 0.0: Rules largely not followed.", "expectation": "All units adhere to start-after-move-out, staff 2-day minimum, and appliance extra day when applicable."}], "on_failure_action": "skip_remaining", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Managerial Utility", "description": "Assess overall clarity, professionalism, and usefulness for the manager to plan turn timelines.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Formatting and Readability", "description": "Check visual polish, layout, headings, and legibility of tables.", "weight": 1.2, "judge_prompt": "Assess professional presentation:\n- Clear section headers and table captions, consistent fonts and spacing.\n- Tables fit the page, columns aligned, no clipping/truncation.\n- Dates consistently in YYYY-MM-DD format.\n\nScoring (1.2 max): 1.2 excellent; 0.8 good with minor issues; 0.4 acceptable with multiple issues; 0.0 poor.", "expectation": "A clean, professional PDF with readable, well-aligned tables and consistent date formatting."}, {"type": "llm_judge", "name": "Actionability for Manager", "description": "How directly useful is the report for planning and executing turns?", "weight": 1.3, "judge_prompt": "Evaluate how actionable the report is for a property manager:\n- Can the manager immediately see who does what, when, and any make-ready date changes?\n- Are dependencies and sequencing clear (staff vs. vendor days vs. deliveries)?\n- Are any potential bottlenecks or conflicts called out?\n\nScoring (1.3 max): 1.3 highly actionable; 0.9 generally useful; 0.5 somewhat useful; 0.0 not useful.", "expectation": "The manager can use the report directly to schedule and communicate work."}, {"type": "llm_judge", "name": "Assumptions, Constraints, and Risks", "description": "Quality of stated assumptions and handling of missing vendor availability references.", "weight": 0.8, "judge_prompt": "Judge how well the report documents assumptions and constraints:\n- Concise note on how rules were interpreted (no same-day vendors, staff days, deliveries on business days).\n- If vendor schedules/availability weren\u2019t provided, are reasonable assumptions stated (e.g., next available business day scheduling)?\n- Any risks, lead times, or dependencies (e.g., appliance order lead times) acknowledged?\n\nScoring (0.8 max): 0.8 strong; 0.5 adequate; 0.2 minimal; 0.0 absent.", "expectation": "Transparent assumptions and identified risks that inform planning."}, {"type": "llm_judge", "name": "Consistency and Data Hygiene", "description": "Check consistent labeling, apartment identifiers, vendor names, and minimal errors.", "weight": 0.7, "judge_prompt": "Evaluate data consistency:\n- Apartment numbers/labels are consistent across sections.\n- Vendor names are consistent and unambiguous.\n- Minimal typos, duplicated rows, or mismatched IDs.\n\nScoring (0.7 max): 0.7 consistent; 0.5 minor issues; 0.3 noticeable inconsistencies; 0.0 poor.", "expectation": "Consistent identifiers and clean tables without obvious data errors."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "93b336f3-61f3-4287-86d2-87445e1e0f90", "rubric": {"category_name": "EV Battery Pack Assembly Localisation Proposal (CPO-Ready)", "rationale": "Task Type: Mixed (Pattern C). Output is a 2\u20133 page DOCX/PDF business proposal with embedded cost calculations in INR. Stage 1 uses an LLM gate to enforce strict document structure that enables verification. Stage 2 mixes light code checks (numeric/bounds/consistency) with heavier LLM judgment on correctness and alignment to PMP and the brief. Stage 3 evaluates professional quality and executive readiness. Code rules carry smaller weights than LLM rules, per guidance.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (Document Structure)", "description": "MANDATORY structure and format check for a CPO-brief proposal with embedded cost table/calculations in INR.", "is_required": true, "max_points": 5.0, "min_score_to_pass": 3.5, "rules": [{"type": "llm_judge", "name": "Document Format and Required Sections Present", "description": "Validates that the candidate output is a professionally structured 2\u20133 page proposal in PDF/DOCX with all required sections and calculational elements present (structure only, not content accuracy).", "weight": 5.0, "judge_prompt": "You are checking ONLY the structure and presence of elements, not the correctness of calculations or the quality of writing.\n\nRequirements:\nFormat\n- File must be a PDF or DOCX (not Excel, not plain text).\n- Length: 2\u20133 pages of content (be flexible about small deviations if clearly within range). Professional formatting with headings.\n\nRequired Sections (flexible on exact headings, but must be clearly present):\n1) Executive Summary or Overview (on page 1) summarizing purpose: assembly-only localisation partnership with EV Batteries Inc. and EvTronics.\n2) Partnership Structure: explicitly states 49:51 ownership split (EvTronics:EV Batteries Inc.), EV Batteries Inc. retains technical oversight, EvTronics leads assembly/local operations from Delhi.\n3) Sourcing Model: describes EV Batteries Inc. supplying child parts (cells, housing, thermal systems, BMS, connectors) to EvTronics; EvTronics assembles locally and supplies to the plant.\n4) Cost and Savings Analysis (INR): has a clearly labeled section/table showing inputs and calculations, including:\n   - Baseline per-pack price 830,000 INR (10,000 USD at 83 INR/USD)\n   - Local assembly cost 20,000 INR and local overhead 590 INR\n   - An INR-based per-unit localized cost and per-unit savings figure\n   - Reference to USD=83 INR conversion assumption\n   - Annual volume assumption 110,000 units and multi-year horizon (5 years)\n   The section can be a table or bullet list but must be clearly identifiable and calculated in INR.\n5) Localisation Roadmap aligned to India\u2019s PMP phases (Phase 1\u20134) with a phased timeline; must explicitly show that only assembly localisation is proposed now, with future phases noted at a high level.\n6) Benefits and Risks: includes at minimum benefits (regulatory compliance under FAME II/PMP, reduced forex exposure/long-term cost), and key risks (dependency on imported cells, coordination complexity, initial capex).\n7) Recommendations / Next Steps: a short list of concrete next steps for the CPO.\n\nScoring Guide (structure only):\n- 5.0: Valid PDF/DOCX, ~2\u20133 pages, and all 7 sections clearly present, with the Cost & Savings section explicitly in INR with inputs listed.\n- 4.0: Valid format and 6/7 sections present; minor omissions (e.g., volume mentioned elsewhere but not in the cost section).\n- 3.0: Valid format and 5/7 sections present.\n- 2.0: Valid format and only 3\u20134 sections present.\n- 1.0: Valid format but only 1\u20132 sections present.\n- 0.0: Not a PDF/DOCX or clearly not a 2\u20133 page proposal.\n\nOnly evaluate presence and structure, not correctness or writing quality.", "expectation": "A 2\u20133 page PDF/DOCX with all required sections and an INR cost section that enables verification in later stages."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Verification", "description": "Verifies cost math, assumptions, PMP alignment, and proposal specifics using a mix of code checks (numeric consistency) and LLM judgment (methodology, alignment, completeness).", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "INR Cost Math Consistency (Assembly-only)", "description": "Checks the presence and coherence of key INR amounts and assumptions: baseline 830,000 INR, local assembly 20,000 INR, local overhead 590 INR, USD=83 INR, localized per-pack cost (~726,090 INR), and per-unit savings (~103,910 INR). Awards partial credit for each found.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0, \"Unable to read document text\"\n\n    if not text:\n        return 0.0, \"Empty document text\"\n\n    raw = text\n    t = text.lower()\n    # Expected values\n    rate = 83\n    baseline_inr = 830000\n    assembly_local_inr = 20000\n    overhead_local_inr = 590\n    components_inr = (10000 - 1300 - 200) * rate  # 705,500\n    localized_total = components_inr + assembly_local_inr + overhead_local_inr  # 726,090\n    savings_per_unit = baseline_inr - localized_total  # 103,910\n\n    def present_any(num):\n        # Check appearance with/without commas\n        s1 = f\"{int(round(num)):,}\"\n        s2 = f\"{int(round(num))}\"\n        return (s1 in raw) or (s2 in raw)\n\n    score_parts = []\n    # USD=83 INR mention\n    has_rate = (\"usd=83\" in t) or (\"83 inr\" in t) or (\"83 inr/usd\" in t) or (\"83 inr per usd\" in t) or (\"usd 83\" in t) or (\"83\\u20b9\" in t) or (\"exchange rate\" in t and \"83\" in t)\n    score_parts.append(1.0 if has_rate else 0.0)\n\n    # Baseline per-pack\n    score_parts.append(1.0 if present_any(baseline_inr) else 0.0)\n\n    # Local assembly and overhead\n    score_parts.append(1.0 if present_any(assembly_local_inr) else 0.0)\n    score_parts.append(1.0 if present_any(overhead_local_inr) else 0.0)\n\n    # Localized total cost and savings per unit (allow +/- 2%)\n    def approx_present(val, tol=0.02):\n        # search all integers in text and compare\n        nums = re.findall(r\"[0-9][0-9,]*\", raw)\n        vals = []\n        for n in nums:\n            try:\n                vals.append(int(n.replace(\",\", \"\")))\n            except Exception:\n                pass\n        for v in vals:\n            if abs(v - val) <= tol * val:\n                return True\n        return False\n\n    score_parts.append(1.0 if approx_present(int(round(localized_total))) else 0.0)\n    score_parts.append(1.0 if approx_present(int(round(savings_per_unit))) else 0.0)\n\n    # Require INR labeling presence to count as a coherent INR section\n    has_inr_label = (\"inr\" in t) or (\"\u20b9\" in raw)\n\n    # Aggregate: average of found items, but zero if no INR label\n    base_score = sum(score_parts) / len(score_parts)\n    final_score = base_score * (1.0 if has_inr_label else 0.0)\n\n    return max(0.0, min(1.0, final_score)), \"Cost tokens found: {} of {}\".format(int(sum(score_parts)), len(score_parts))"}, {"type": "code", "name": "Volume and Multi-year Savings Coherence", "description": "Checks presence of 110,000 units/year, 5-year horizon, and detects annual (~11.43B INR) and 5-year (~57.15B INR) savings magnitudes within reasonable tolerance if totals are provided.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0, \"Unable to read document text\"\n\n    if not text:\n        return 0.0, \"Empty document text\"\n\n    raw = text\n    t = text.lower()\n\n    # Expected values\n    per_unit_savings = 103910\n    annual_volume = 110000\n    annual_savings = per_unit_savings * annual_volume           # 11,430,100,000\n    five_year_savings = annual_savings * 5                      # 57,150,500,000\n\n    # Presence checks\n    has_volume = bool(re.search(r\"\\b110\\s?[, ]?0{3}\\b\", t)) or (\"110k\" in t) or (\"110 k\" in t) or (\"110,000\" in raw)\n    has_five_year = (\"5-year\" in t) or (\"5 year\" in t) or (\"five years\" in t)\n\n    # Numeric proximity checks\n    nums = re.findall(r\"[0-9][0-9,]*\", raw)\n    vals = []\n    for n in nums:\n        try:\n            vals.append(int(n.replace(\",\", \"\")))\n        except Exception:\n            pass\n\n    def present_approx(target, tol_ratio=0.1):\n        for v in vals:\n            if abs(v - target) <= tol_ratio * target:\n                return True\n        return False\n\n    has_annual_total = present_approx(annual_savings, 0.1)\n    has_5yr_total = present_approx(five_year_savings, 0.1)\n\n    parts = [has_volume, has_five_year, has_annual_total, has_5yr_total]\n    score = sum(1.0 for x in parts if x) / len(parts)\n\n    return max(0.0, min(1.0, score)), f\"Volume token: {has_volume}, 5yr token: {has_five_year}, annual total approx: {has_annual_total}, 5yr total approx: {has_5yr_total}\""}, {"type": "code", "name": "Partnership Split and Roles Presence", "description": "Checks for the partnership details: both company names, 49:51 split, Delhi location, and role cues (technical oversight; assembly/local operations).", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0, \"Unable to read document text\"\n\n    if not text:\n        return 0.0, \"Empty document text\"\n\n    t = text.lower()\n    raw = text\n\n    has_evbat = \"ev batteries inc\" in t or \"evbatteries inc\" in t\n    has_evtron = \"evtronics\" in t\n\n    # Ownership split patterns\n    ratio_patterns = [r\"49\\s*:\\s*51\", r\"49\\s*\\/\\s*51\", r\"49\\s*-\\s*51\", r\"49%.*51%\", r\"51%.*49%\"]\n    has_ratio = any(re.search(p, t) for p in ratio_patterns)\n\n    has_delhi = \"delhi\" in t\n\n    # Role cues\n    has_technical_oversight = (\"technical\" in t and \"oversight\" in t)\n    has_assembly_local_ops = (\"assembly\" in t and \"local\" in t and \"operation\" in t)\n\n    parts = [has_evbat and has_evtron, has_ratio, has_delhi, (has_technical_oversight and has_assembly_local_ops)]\n    score = sum(1.0 for x in parts if x) / len(parts)\n\n    return max(0.0, min(1.0, score)), f\"Names: {has_evbat and has_evtron}, Ratio: {has_ratio}, Delhi: {has_delhi}, Roles: {has_technical_oversight and has_assembly_local_ops}\""}, {"type": "llm_judge", "name": "Cost Analysis Integrity and Assumptions Clarity", "description": "Verifies that the proposal shows a clear INR-based calculation trail from the 10,000 USD baseline to per-unit localized cost and savings, explicitly applying USD=83 INR, and scaling to annual (110k) and 5-year totals.", "weight": 1.8, "judge_prompt": "Evaluate the COST ANALYSIS section for correctness and clarity (not formatting):\nExpectations:\n- States the conversion assumption USD=83 INR and uses INR consistently.\n- Starts from the baseline 10,000 USD per pack (830,000 INR) and explicitly identifies which elements are localized now (assembly and overhead only), keeping other components constant.\n- Shows localized assembly (20,000 INR) and overhead (590 INR), derives a per-unit localized cost (~726,090 INR) and per-unit savings (~103,910 INR) versus baseline. Minor rounding is acceptable.\n- Scales the per-unit savings to annual volume 110k units and to a 5-year horizon with realistic totals.\n- Notes sensitivities/considerations (e.g., future localisation of cells/BMS, forex exposure) even if only assembly is localized now.\nScoring:\n- 1.8: All bullets clearly satisfied; numbers are coherent and traceable step-by-step.\n- 1.2: Most elements present; minor omissions or small inconsistencies (e.g., totals shown only for one horizon).\n- 0.6: Partial, lacks key steps (no conversion assumption or missing per-unit savings), but some correct INR math is shown.\n- 0.0: No coherent INR calculation trail or very unclear/incorrect approach.", "expectation": "A transparent INR calculation chain with per-unit savings and scaled totals, grounded in the stated assumptions."}, {"type": "llm_judge", "name": "PMP Alignment and Localisation Roadmap Verification", "description": "Checks that the roadmap aligns to PMP phases and clearly positions assembly-only localisation now with phased deepening over time.", "weight": 1.6, "judge_prompt": "Evaluate the localisation roadmap:\n- Properly references PMP phases and sequencing relevant to 4-wheeler EVs (Phase 1\u20134), and maps proposed actions to a timeline.\n- Clearly states that only assembly is localized now; indicates potential later phases (motors/VCUs/chargers; deeper power electronics including BMS; and eventually cells/semiconductors) aligning with PMP years.\n- Connects roadmap to expected volumes and operational ramp, noting dependencies/risks.\nScoring:\n- 1.6: Strong alignment with PMP and a credible phased plan tied to volumes and risks.\n- 1.0: Generally aligned but light on timing or dependencies.\n- 0.5: Mentions PMP superficially without a usable phased roadmap.\n- 0.0: No roadmap or misaligned with PMP.", "expectation": "A phased, PMP-aligned roadmap with clear near-term assembly focus and plausible future localisation steps."}, {"type": "llm_judge", "name": "Partnership Structure and Sourcing Model Specificity", "description": "Assesses whether the partnership, roles, location, and sourcing flow are specific and actionable.", "weight": 1.6, "judge_prompt": "Check the partnership and sourcing model details:\n- Ownership split 49:51 (EvTronics:EV Batteries Inc.) explicitly stated.\n- Roles: EV Batteries Inc. retains technical oversight; EvTronics leads assembly and local operations from Delhi.\n- Sourcing flow: EV Batteries Inc. supplies child parts (cells, housing, thermal systems, BMS, connectors) to EvTronics; EvTronics assembles and supplies packs to the plant.\n- Governance/quality assurance touchpoints (e.g., PPAP/APQP-equivalents, audits) mentioned or implied.\nScoring:\n- 1.6: All specifics present and clearly articulated.\n- 1.0: Most specifics present; minor gaps in governance/QA detail.\n- 0.5: Vague description missing key elements (split or roles or flow).\n- 0.0: Not addressed.", "expectation": "A concrete, role-clear partnership and sourcing model suitable for executive decision-making."}, {"type": "llm_judge", "name": "Benefits, Risks, and Recommendations Completeness", "description": "Evaluates whether benefits/risks are complete and whether recommendations are decisive and relevant for the CPO.", "weight": 1.5, "judge_prompt": "Assess benefits, risks, and recommendations:\n- Benefits include: regulatory compliance (FAME II/PMP), long-term cost reduction, reduced forex exposure; any added operational benefits.\n- Risks include: dependency on imported cells, coordination complexity, initial capex; with practical mitigations (e.g., SLAs, phased capex, dual-sourcing for cells later).\n- Recommendations/Next Steps are clear, prioritized, and time-bound (e.g., pilot lot, JV MoU, CAPEX outline, regulatory filings, timeline to SOP).\nScoring:\n- 1.5: Comprehensive benefits/risks with concrete, prioritized next steps.\n- 1.0: Mostly complete but light on mitigations or specificity.\n- 0.5: Superficial enumeration; weak or generic next steps.\n- 0.0: Largely missing or irrelevant.", "expectation": "Clear articulation of value, risks, mitigations, and actionable next steps for the CPO."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Executive Readiness", "description": "Holistic assessment of writing quality, professionalism, strategic insight, and executive usability for the CPO.", "is_required": false, "max_points": 7.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Executive Readability and Conciseness", "description": "Assesses whether the proposal is concise (2\u20133 pages), with an executive summary that highlights the decision and headline numbers.", "weight": 2.0, "judge_prompt": "Evaluate for executive readability:\n- Clear, direct executive summary with headline savings and key decision points upfront.\n- Concise 2\u20133 pages, minimal fluff, skimmable layout.\n- Uses meaningful headings and callouts/bullets where helpful.\nScoring:\n- 2.0: Excellent executive orientation and concision.\n- 1.3: Good but could be tighter or clearer upfront.\n- 0.7: Some structure, but meandering or dense.\n- 0.0: Hard to skim or lacks an effective summary.", "expectation": "A crisp, skimmable brief with headline conclusions upfront."}, {"type": "llm_judge", "name": "Clarity, Structure, and Use of Exhibits", "description": "Evaluates organization, clarity, and whether tables/figures (e.g., cost table, timeline) aid understanding.", "weight": 1.5, "judge_prompt": "Assess clarity and structure:\n- Logical flow from context to proposal to analysis to recommendations.\n- At least one well-structured table (cost breakdown and/or timeline) that aids verification.\n- Minimal ambiguity; terms and assumptions defined.\nScoring:\n- 1.5: Very clear, well-structured with helpful exhibits.\n- 1.0: Generally clear; exhibits present but could be tighter.\n- 0.5: Passable clarity; exhibits missing or weak.\n- 0.0: Poorly structured/unclear.", "expectation": "A well-organized document using exhibits to make analysis transparent."}, {"type": "llm_judge", "name": "Strategic Insight and Feasibility", "description": "Assesses whether the proposal anticipates operational realities and strategic implications.", "weight": 1.5, "judge_prompt": "Evaluate strategic depth:\n- Acknowledges supply risk, forex management, quality/PPAP readiness, ramp timing, and capex considerations.\n- Positions the JV for compliance and cost advantage over time; notes triggers for deeper localisation.\n- Recognizes stakeholder management (OEM plant, suppliers, regulators) and governance cadence.\nScoring:\n- 1.5: Strong, pragmatic strategic insight.\n- 1.0: Some strategic points but not fully integrated.\n- 0.5: Surface-level strategy.\n- 0.0: Not strategic/feasible.", "expectation": "A realistic, strategically grounded plan that the CPO can trust."}, {"type": "llm_judge", "name": "Actionability and Decision Support", "description": "Evaluates whether the document enables a near-term decision with clear next steps and defined owners/timelines.", "weight": 2.0, "judge_prompt": "Assess actionability:\n- Clear recommenda\u00adtions and a 30/60/90-day or phased plan with accountable owners.\n- Explicit decision requests or thresholds (e.g., proceed to JV MoU, approve pilot budget).\n- Identifies key dependencies/approvals and a path to mitigate them.\nScoring:\n- 2.0: Highly actionable with concrete plan and asks.\n- 1.3: Actionable but missing some specificity.\n- 0.7: Vague plan; limited decision support.\n- 0.0: Not actionable.", "expectation": "A decisive, owner-attributed plan that supports an immediate CPO decision."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "045aba2e-4093-42aa-ab7f-159cc538278c", "rubric": {"category_name": "Retail Pharmacy Compliance Checklists (CA)", "rationale": "This rubric enforces a self-documenting, file-based deliverable: three separate, printer-friendly PDF checklists (Daily; Weekly+Monthly; Quarterly+Annual) aligned to California Board of Pharmacy requirements. Stage 1 is a strict LLM gate that checks that the output shape exists and is usable for verification. Stage 2 mixes lightweight code checks (structure signals across PDFs) with LLM verification of legal grounding, frequency appropriateness, coverage, and consistency. Stage 3 assesses professional quality, clarity, and practical usability for a community pharmacy team. Code rules are intentionally lower-weight than LLM rules (~5x less) and rely only on file access via the provided API.", "max_total_score": 29.4, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (LLM only)", "description": "Verify the candidate produced exactly the required, printer-friendly PDFs with the mandated structure so that verification is possible.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Output Set and File Format", "description": "Confirms there are three separate, one-page PDFs corresponding to Daily; Weekly+Monthly; Quarterly+Annual checklists.", "weight": 4.0, "judge_prompt": "You are validating the SHAPE of the candidate\u2019s deliverables only. Do not judge quality or correctness. Review all provided outputs.\n\nPass criteria for this rule:\n- Exactly three separate PDF files are present (not DOCX/Excel/Markdown).\n- Each PDF is limited to one page only (printer-friendly single page per checklist).\n- The three PDFs correspond to these scopes (naming may vary but must be unambiguous):\n  1) Daily tasks\n  2) Weekly + Monthly tasks (combined on a single page)\n  3) Quarterly + Annual tasks (combined on a single page)\n\nScoring:\n- 4.0: All three PDFs exist, each is one page, scopes are correctly represented (naming can be flexible but clear). \n- 2.5: Three PDFs exist and are one page each, but one scope labeling is ambiguous or mildly incorrect.\n- 1.0: Three documents exist but one is not a PDF or one file clearly is not one page.\n- 0.0: Fewer than three PDFs, or multi-page PDFs, or scopes not represented.\n\nOnly check for presence/format/structure, not content accuracy.", "expectation": "Three distinct, one-page PDFs corresponding to Daily, Weekly+Monthly, and Quarterly+Annual checklists."}, {"type": "llm_judge", "name": "Checklist Page Structure", "description": "Checks each page has clearly structured checklist elements enabling downstream verification.", "weight": 4.0, "judge_prompt": "Assess only STRUCTURE and FORMAT of each PDF (not correctness). For each of the three one-page PDFs, verify the presence of:\n- A clear title indicating frequency scope (e.g., Daily; Weekly & Monthly; Quarterly & Annual)\n- Header fields for operational use: Pharmacy Name, Date, Prepared by (or Preparer), Reviewed by (or PIC), and optional Location/Address\n- Organized sections with headings (e.g., Licensing & Posting; Prescription Processing; Controlled Substances; Recordkeeping; Storage & Environment; Compounding (if applicable); Quality Assurance; Notifications/Reporting)\n- Visible checkbox-style items (checkbox glyphs or bracketed boxes) with space to initial/sign/date completion\n\nScoring:\n- 4.0: All three PDFs show coherent titles, header fields, checkboxes, and organized sections.\n- 3.0: Minor omissions (e.g., one header field missing on one page) but overall structured and usable.\n- 1.5: Multiple missing elements across pages but pages still recognizable as checklists.\n- 0.0: Pages lack basic checklist hallmarks (no checkboxes, no sections, no operational header fields).\n\nDo not evaluate legal accuracy or item content here\u2014only the structural readiness for verification.", "expectation": "Each one-page PDF contains a clear title, operational header fields, structured sections, and checkbox items with sign-off space."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Compliance Verification", "description": "With the correct shape in place, verify legal grounding, frequency appropriateness, coverage, and consistency using a mix of code and LLM rules.", "is_required": false, "max_points": 14.4, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "File Count and Type Sanity Check (3 PDFs)", "description": "Confirm there are exactly three PDF outputs among the produced documents.", "weight": 0.6, "code": "import re\nfrom pathlib import Path\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0, \"No outputs found.\"\n    pdfs = []\n    for r in outputs:\n        try:\n            if getattr(r, 'is_document', False):\n                p = context.files.get_path(r.id)\n                if p and str(p).lower().endswith('.pdf'):\n                    pdfs.append(r)\n        except Exception:\n            continue\n    score = 0.0\n    fb = f\"Detected {len(pdfs)} PDF(s).\"\n    if len(pdfs) == 3:\n        score = 0.6\n    elif len(pdfs) >= 3:\n        score = 0.3\n    else:\n        score = 0.0\n    return score, fb"}, {"type": "code", "name": "Operational Header Fields Present", "description": "Checks that each PDF contains practical header fields like Pharmacy Name, Date, Prepared/Reviewed by (flexible matching).", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs() or []\n    pdfs = []\n    for r in outputs:\n        try:\n            if getattr(r, 'is_document', False):\n                p = context.files.get_path(r.id)\n                if p and str(p).lower().endswith('.pdf'):\n                    pdfs.append(r)\n        except Exception:\n            continue\n    if not pdfs:\n        return 0.0, \"No PDFs to check.\"\n    # Flexible header tokens\n    header_tokens = [\n        r\"pharmacy name\", r\"date\", r\"prepared\\s*by|preparer|prepared\", r\"reviewed\\s*by|reviewed|pic\", r\"address|location\"\n    ]\n    per_doc_scores = []\n    feedback = []\n    for r in pdfs:\n        try:\n            text = (context.files.read_pdf_text(r.id) or '').lower()\n        except Exception:\n            text = ''\n        hits = 0\n        for pat in header_tokens:\n            if re.search(pat, text):\n                hits += 1\n        # Require at least 3/5 tokens per doc to count as present\n        per_doc_scores.append(min(1.0, hits/3.0))\n        feedback.append(f\"{getattr(r, 'name', 'PDF')}: matched {hits} header tokens\")\n    avg = sum(per_doc_scores)/len(per_doc_scores)\n    return 0.6*avg, \"; \".join(feedback)"}, {"type": "code", "name": "Checklist Markers Detected", "description": "Checks presence of checkbox-like markers across PDFs (e.g., \u2610, \u2611, [ ], [x], - [ ]).", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs() or []\n    pdfs = []\n    for r in outputs:\n        try:\n            if getattr(r, 'is_document', False):\n                p = context.files.get_path(r.id)\n                if p and str(p).lower().endswith('.pdf'):\n                    pdfs.append(r)\n        except Exception:\n            continue\n    if not pdfs:\n        return 0.0, \"No PDFs to check.\"\n    markers = [\"\u2610\", \"\u2611\", \"\u25a1\", \"\u25a0\", \"[ ]\", \"[x]\", \"- [ ]\", \"- [x]\", \"yes / no\", \"yes/no\"]\n    count_present = 0\n    details = []\n    for r in pdfs:\n        try:\n            text = (context.files.read_pdf_text(r.id) or '').lower()\n        except Exception:\n            text = ''\n        found = any(m.lower() in text for m in markers)\n        count_present += 1 if found else 0\n        details.append(f\"{getattr(r, 'name', 'PDF')}: markers {'found' if found else 'not found'}\")\n    ratio = count_present/len(pdfs)\n    return 0.6*ratio, \"; \".join(details)"}, {"type": "code", "name": "Frequency Mapping Present Across Set", "description": "Ensures the three PDFs collectively map to Daily, Weekly/Monthly, and Quarterly/Annual scopes based on text matches.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs() or []\n    pdfs = []\n    for r in outputs:\n        try:\n            if getattr(r, 'is_document', False):\n                p = context.files.get_path(r.id)\n                if p and str(p).lower().endswith('.pdf'):\n                    pdfs.append(r)\n        except Exception:\n            continue\n    if not pdfs:\n        return 0.0, \"No PDFs to check.\"\n    cats = {\"daily\": False, \"weekly_monthly\": False, \"quarterly_annual\": False}\n    det_fb = []\n    for r in pdfs:\n        try:\n            text = (context.files.read_pdf_text(r.id) or '').lower()\n        except Exception:\n            text = ''\n        has_daily = 'daily' in text\n        has_week = 'weekly' in text or 'week' in text\n        has_month = 'monthly' in text or 'month' in text\n        has_quarter = 'quarterly' in text or 'quarter' in text\n        has_annual = 'annual' in text or 'yearly' in text or 'biennial' in text\n        if has_daily:\n            cats['daily'] = True\n        if has_week or has_month:\n            cats['weekly_monthly'] = True\n        if has_quarter or has_annual:\n            cats['quarterly_annual'] = True\n        det_fb.append(f\"{getattr(r,'name','PDF')}: daily={has_daily}, weekly/monthly={has_week or has_month}, quarterly/annual={has_quarter or has_annual}\")\n    ratio = sum(1 for k,v in cats.items() if v)/3.0\n    return 0.6*ratio, \"; \".join(det_fb)"}, {"type": "llm_judge", "name": "Legal Grounding and Citations (CA BOP)", "description": "Assesses whether checklist items are grounded in California Board of Pharmacy requirements and/or cite authoritative sources (2025 Lawbook; Self-Assessment; BPC/CCR sections).", "weight": 3.0, "judge_prompt": "Evaluate whether the checklists demonstrate legal grounding suitable for California pharmacies:\n- Look for explicit references/citations to relevant sources: California Business & Professions Code (BPC), Title 16 California Code of Regulations (CCR), the California Board of Pharmacy 2025 Lawbook, and/or the Community Pharmacy Self-Assessment form. Citations may be inline, footnoted, abbreviated (e.g., \u201cBPC 4125\u201d, \u201cCCR 1711\u201d, \u201cSelf-Assessment 17M-13\u201d).\n- Rate the extent and clarity of these references and whether each page indicates traceability to source requirements.\n\nScoring:\n- 3.0: Consistent, clear use of appropriate CA references across pages; items map sensibly to cited sources.\n- 2.0: Some citations or source references present; mostly appropriate but not comprehensive.\n- 1.0: Few or very generic references without clear traceability.\n- 0.0: No visible legal grounding or references.\n\nJudge content only for legal grounding/traceability. Do not penalize for minor citation formatting variations.", "expectation": "Meaningful inclusion of CA BOP-related citations or references enhancing traceability."}, {"type": "llm_judge", "name": "Frequency Appropriateness (Daily vs Weekly/Monthly vs Quarterly/Annual)", "description": "Evaluates whether tasks are placed in reasonable frequencies consistent with CA compliance practice.", "weight": 3.0, "judge_prompt": "Assess whether tasks are assigned to appropriate frequencies for a California community pharmacy.\nConsider common compliance rhythms (examples, not exhaustive):\n- Daily: refrigerator/freezer/ambient temperature logging; controlled substances receipt verification; Rx labeling and counseling compliance; privacy notices posted; CURES submissions timing alignment; security/alarms functional checks.\n- Weekly/Monthly: outdate checks; cleaning logs; perpetual inventory reconciliations (if maintained); physical key count audits; QA log reviews; recall checks and quarantine area audits; DEA-222/CSOS reconciliation; staff license/ratio spot-checks.\n- Quarterly/Annual: BOP Self-Assessment timing (odd years by July 1 and upon PIC/address/permit changes); DEA biennial inventory; license renewals; policy reviews; disaster recovery drills; PIC attestations.\n\nScoring:\n- 3.0: Task placement overwhelmingly makes sense with minimal mismatches.\n- 2.0: Mostly sensible with a few misplaced items.\n- 1.0: Several mismatches creating confusion.\n- 0.0: Frequencies are broadly inappropriate.\n\nBe flexible\u2014pharmacies differ\u2014but ensure no glaring frequency misplacements for CA compliance.", "expectation": "Tasks are grouped at plausible frequencies with no major misplacements."}, {"type": "llm_judge", "name": "Coverage of Key Compliance Domains (Breadth)", "description": "Checks that the checklists cover the major CA compliance domains relevant to community pharmacies.", "weight": 3.0, "judge_prompt": "Evaluate the breadth of coverage across essential California community pharmacy compliance domains. Look for clear checklist items in these areas (adapt names if sections differ):\n- Licensing & Posting (permits, PIC posting, required notices)\n- Prescription Processing & Labeling (including patient counseling)\n- Controlled Substances Management (ordering, receipt, storage, losses/returns, inventories)\n- Recordkeeping & Retention (including CURES-related records)\n- Storage, Security, and Environmental Controls (temps, alarms, locks)\n- Compounding (if applicable): policies, beyond-use dating, logs\n- Quality Assurance / Medication Error Program (BPC 4125/CCR 1711 concepts)\n- Notifications & Reporting (Board notifications, address/PIC changes, recalls, robberies/theft reporting)\n- Staff Licensure, Training, and Ratios\n\nScoring:\n- 3.0: Strong coverage across most domains, with clear, actionable items.\n- 2.0: Moderate coverage with some gaps.\n- 1.0: Limited coverage; multiple major gaps.\n- 0.0: Very sparse or off-topic coverage.\n\nDo not require all domains if inapplicable (e.g., no compounding). Reward reasonable breadth and relevance.", "expectation": "Broad, relevant coverage of CA community pharmacy compliance domains."}, {"type": "llm_judge", "name": "Internal Consistency and Traceability", "description": "Evaluates cross-page consistency, no contradictions, and traceability of items to sources or policies.", "weight": 3.0, "judge_prompt": "Review all three checklists together for consistency and traceability:\n- Are terminology and roles (e.g., PIC, Pharmacist, Technician) used consistently across pages?\n- Are similar items phrased uniformly and not duplicated with conflicting instructions?\n- Do items that reference a policy or law include a clear citation or internal policy reference (e.g., SOP code), enabling traceability?\n\nScoring:\n- 3.0: Consistent terminology and structure; traceability is clear; no contradictions.\n- 2.0: Minor inconsistencies but overall coherent.\n- 1.0: Multiple inconsistencies or unclear traceability.\n- 0.0: Contradictions or disorganized/unclear traceability across the set.", "expectation": "Consistent terminology and structure with traceable items across pages."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Usability", "description": "Holistic assessment of presentation, clarity, and practical usability as a live compliance tool in the pharmacy.", "is_required": false, "max_points": 7.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Printer-Friendly, Readable Layout", "description": "Assesses if each page is genuinely printer-friendly and easy to use on paper.", "weight": 1.75, "judge_prompt": "Evaluate physical usability for a busy pharmacy setting:\n- One-page layout with legible font sizes, sensible spacing, and margins\n- Clear section headings and visual hierarchy\n- Checkbox alignment suitable for quick scanning\n\nScoring:\n- 1.75: Highly usable, clean, and scan-friendly.\n- 1.0: Usable with minor layout issues.\n- 0.5: Cluttered or small fonts hinder usability.\n- 0.0: Not printer-friendly or difficult to read.", "expectation": "Each page is easy to print and scan quickly during operations."}, {"type": "llm_judge", "name": "Clarity and Concision of Items", "description": "Assesses whether checklist items are concise, unambiguous, and action-oriented.", "weight": 1.75, "judge_prompt": "Assess the wording of the checklist items:\n- Concise, plain-language prompts\n- Unambiguous actions (what to check or do)\n- Avoids vague generalities; uses specific verbs and objects\n\nScoring:\n- 1.75: Clear, concise, action-oriented throughout.\n- 1.0: Mostly clear with a few vague items.\n- 0.5: Several ambiguous items.\n- 0.0: Largely unclear or verbose.", "expectation": "Short, specific, action-oriented checklist items."}, {"type": "llm_judge", "name": "Actionability and Role Assignment", "description": "Checks for sign-off areas, role ownership, and practical fields to support accountability.", "weight": 1.75, "judge_prompt": "Evaluate whether the checklists support accountable execution:\n- Fields for initials/signature and date for each section or item set\n- Role ownership cues (e.g., PIC vs Pharmacist vs Technician) where appropriate\n- Space for notes or corrective actions when deficiencies are found\n\nScoring:\n- 1.75: Strong accountability structure with clear sign-offs and roles.\n- 1.0: Some accountability elements present.\n- 0.5: Minimal sign-off cues.\n- 0.0: No clear accountability features.", "expectation": "Includes sign-offs and role guidance enabling accountability."}, {"type": "llm_judge", "name": "Maintainability and Version Control", "description": "Evaluates whether the documents are easy to maintain over time.", "weight": 1.75, "judge_prompt": "Assess whether the checklists support ongoing maintenance:\n- Document title/version/date and next review date\n- Reference to source set (e.g., \u201cBased on CA BOP 2025 Lawbook and 17M-13 Self-Assessment\u201d)\n- Stable section structure that can be updated incrementally\n\nScoring:\n- 1.75: Clearly versioned with references and evident maintainability.\n- 1.0: Some versioning or references present.\n- 0.5: Minimal maintainability cues.\n- 0.0: No versioning or references.\n", "expectation": "Version/date labels and source references promote reliable updates over time."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "2c249e0f-4a8c-4f8e-b4f4-6508ba29b34f", "rubric": {"category_name": "Robotics Fleet Data Submission API (OpenAPI + Data Flow)", "rationale": "This rubric enforces a self-documenting deliverable: an OpenAPI 3.x YAML spec plus a data_flow.txt narrative. Stage 1 is a strict LLM-only gate that checks file presence and structural shape. Stage 2 mixes lightweight code checks (regex over YAML/text) with higher-weight LLM verification of correctness and cross-file consistency, emphasizing resumable uploads, insight vs payload prioritization, mission lifecycle/resume semantics, and cloud pipeline stages. Stage 3 provides a holistic LLM quality assessment of API design clarity, security/robustness, documentation completeness, and operational realism for large robotics data at scale.", "max_total_score": 26.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate", "description": "Gate that verifies deliverables and required structure exist to enable verification. LLM-only checks per self-documenting philosophy.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.0, "rules": [{"type": "llm_judge", "name": "Deliverables Presence and Format Gate", "description": "Verify that outputs include: (1) an OpenAPI 3.0+ specification in YAML format (.yaml or .yml), and (2) a text file named data_flow.txt describing the data flow and robot usage.", "weight": 2.0, "judge_prompt": "You are validating the presence and basic format of required deliverables across all provided files.\n\nRequirements:\n- There MUST be an OpenAPI 3.0+ specification in YAML format (.yaml or .yml). It should visibly start with an 'openapi: 3' line and include top-level sections like 'info' and 'paths'.\n- There MUST be a text file named exactly 'data_flow.txt' that explains the expected data flow and how robots interact with the API.\n- The OpenAPI file should not be provided as JSON or Markdown blocks; it should be an actual YAML document/file.\n\nScoring:\n- 2.0: Both files present; OpenAPI clearly YAML (openapi: 3.x present) and data_flow.txt is present as a standalone text file.\n- 1.0: One deliverable present correctly; the other is present but incorrect format/name (e.g., OpenAPI not YAML or text file not named data_flow.txt) OR clearly incomplete.\n- 0.0: Missing one or both required deliverables, or OpenAPI not in YAML.", "expectation": "One .yaml/.yml OpenAPI 3.x file plus a data_flow.txt text file."}, {"type": "llm_judge", "name": "OpenAPI Minimum Structural Shape", "description": "Check that the OpenAPI YAML includes the minimum sections and high-level endpoints to enable verification (structure only, not correctness).", "weight": 2.0, "judge_prompt": "Evaluate the OpenAPI YAML structure. Be flexible with endpoint names but strict about coverage and presence.\n\nRequired structural elements:\n- Top-level: 'openapi: 3.x', 'info', 'servers', 'paths', and 'components.schemas'.\n- Resource coverage in paths (names can vary, examples provided):\n  1) Robot and mission lifecycle:\n     - register/list robots (e.g., /robots)\n     - create/list missions and get mission by ID (e.g., /missions, /missions/{missionId})\n  2) Sensor uploads with resumable/multipart design for large files:\n     - presign or initiate upload (e.g., /uploads or /missions/{id}/uploads:initiate)\n     - upload parts/chunks and complete/abort (e.g., /uploads/{uploadId}/parts, /uploads/{uploadId}:complete)\n  3) Prioritization for insight vs payload data (e.g., parameter or schema enum with INSIGHT/PAYLOAD; or separate endpoints/queues)\n  4) Processing pipeline or job orchestration endpoints or callbacks (e.g., /processing, /pipelines, or callbacks/webhooks)\n\nScoring:\n- 2.0: All required top-level sections present + all 4 coverage categories represented in paths/components (naming variations OK).\n- 1.0: Top-level sections present but 1 coverage category missing.\n- 0.0: Missing key top-level sections or multiple coverage categories.", "expectation": "A navigable OpenAPI 3.x YAML with info/servers/paths/components and endpoints that cover robot/mission lifecycle, resumable uploads, prioritization, and processing pipeline."}, {"type": "llm_judge", "name": "data_flow.txt Structural Shape", "description": "Check that data_flow.txt contains clear, labeled sections that correspond to the system requirements.", "weight": 2.0, "judge_prompt": "Assess the structure (not the depth) of data_flow.txt.\n\nLook for clearly delineated sections (headers or numbered items) covering:\n1) Overview and objectives\n2) On-robot data staging and prioritization (insight vs payload)\n3) Resumable upload protocol and failure recovery (chunking/multipart, retries, checksum/ETag)\n4) Cloud ingestion and multistage processing pipeline (triggers/events)\n5) DynamoDB tables/keys for mission metadata and sensor upload status\n6) S3 bucket structure, prefixes, and lifecycle/retention\n7) Offline/SSD shipping ingestion path for payload data\n8) Mission lifecycle and resume semantics (partial missions, multiple per day)\n9) Security and auth assumptions\n\nScoring:\n- 2.0: 8\u20139 sections present with clear headers/labels and brief descriptions.\n- 1.0: 5\u20137 sections present or sections merged but still evidently covered.\n- 0.0: Fewer than 5 sections or content too vague to identify sections.", "expectation": "A well-structured outline in data_flow.txt that mirrors key system areas to enable later verification."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness & Consistency)", "description": "Now verify correctness and cross-file consistency. Includes lightweight code checks and higher-weight LLM reasoning checks focused on resumable uploads, prioritization, mission lifecycle/resume, and processing pipeline.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "OpenAPI Core Markers and Key Paths", "description": "Regex-check the YAML for OpenAPI 3.x markers and presence of key path categories (missions, uploads, processing).", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    # Find an OpenAPI YAML file among all outputs\n    outputs = context.get_all_outputs() or []\n    if not outputs:\n        return 0.0, \"No outputs found.\"\n    yaml_res = None\n    for r in outputs:\n        try:\n            path = context.files.get_path(r.id)\n            if path.suffix.lower() in {'.yaml', '.yml'}:\n                yaml_res = r\n                break\n        except Exception:\n            continue\n    if not yaml_res:\n        return 0.0, \"No .yaml/.yml OpenAPI file detected.\"\n    try:\n        text = context.files.read_text(yaml_res.id)\n    except Exception as e:\n        return 0.0, f\"Failed to read YAML: {e}\"\n    lt = text.lower()\n    checks = []\n    # a) openapi: 3.x\n    checks.append(bool(re.search(r\"^\\s*openapi:\\s*3(\\.[0-9]+)?\", text, flags=re.M)))\n    # b) paths present\n    checks.append(\"\\npaths:\" in text or re.search(r\"^\\s*paths:\\s*$\", text, flags=re.M))\n    # c) has mission path\n    checks.append(\"/missions\" in lt)\n    # d) has upload/multipart/presign-related path\n    checks.append(any(s in lt for s in [\"/upload\", \"/uploads\", \"multipart\", \"/parts\", \"presign\"]))\n    # e) has processing/pipeline/ingest path\n    checks.append(any(s in lt for s in [\"/process\", \"/processing\", \"/pipeline\", \"/pipelines\", \"/ingest\"]))\n    score = sum(1 for c in checks if c) / len(checks)\n    return score, f\"OpenAPI markers present: {sum(1 for c in checks if c)}/{len(checks)}\""}, {"type": "code", "name": "Components, Security, and SensorType Hints", "description": "Check components/schemas exist, security schemes mentioned, and presence of SensorType with INSIGHT/PAYLOAD or equivalent.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs() or []\n    if not outputs:\n        return 0.0, \"No outputs found.\"\n    yaml_res = None\n    for r in outputs:\n        try:\n            path = context.files.get_path(r.id)\n            if path.suffix.lower() in {'.yaml', '.yml'}:\n                yaml_res = r\n                break\n        except Exception:\n            continue\n    if not yaml_res:\n        return 0.0, \"No .yaml/.yml OpenAPI file detected.\"\n    try:\n        text = context.files.read_text(yaml_res.id)\n    except Exception as e:\n        return 0.0, f\"Failed to read YAML: {e}\"\n    lt = text.lower()\n    checks = []\n    # a) components present\n    checks.append(\"\\ncomponents:\" in text or re.search(r\"^\\s*components:\\s*$\", text, flags=re.M))\n    # b) schemas present\n    checks.append(\"\\nschemas:\" in lt or re.search(r\"^\\s*schemas:\\s*$\", lt, flags=re.M))\n    # c) SensorType and enums\n    has_sensor_type = (\"sensortype\" in lt) and (\"insight\" in lt) and (\"payload\" in lt)\n    checks.append(has_sensor_type)\n    # d) security schemes (bearer/apikey/sigv4)\n    has_sec_block = (\"securityschemes\" in lt) and (\"bearer\" in lt or \"apikey\" in lt or \"api key\" in lt or \"sigv4\" in lt)\n    checks.append(has_sec_block)\n    # e) examples mentioned at least once\n    has_example = (\"example:\" in lt) or (\"examples:\" in lt)\n    checks.append(has_example)\n    score = sum(1 for c in checks if c) / len(checks)\n    return score, f\"Components/security/enums/examples present: {sum(1 for c in checks if c)}/{len(checks)}\""}, {"type": "llm_judge", "name": "Resumable Uploads and Failure Recovery Correctness", "description": "Confirm the API and data_flow together define a robust resumable upload flow suitable for 200GB files with intermittent connectivity.", "weight": 2.6, "judge_prompt": "Evaluate whether the combination of OpenAPI and data_flow.txt defines a practical resumable upload design.\n\nLook for:\n- Multipart or chunked uploads with an initiate/upload-part/complete/abort sequence, or S3 multipart via presigned part URLs.\n- Upload identifiers (uploadId), partNumber indexing, ETag/checksum handling, and idempotency for retried parts.\n- Resume semantics after connectivity loss (re-list uploaded parts, continue at next missing part) and safe abort.\n- Explicit large file considerations (200GB scale) and 1 Gbps links.\n- Server-side validation hooks (e.g., size, checksum) and backpressure.\n\nScoring:\n- 2.6: All elements present with clear, interoperable behavior described across API and data_flow.\n- 1.3: Mostly present but missing one key element (e.g., checksum or list-parts).\n- 0.0: Lacks a concrete resumable strategy.", "expectation": "A multipart/presigned design with uploadId, part numbers, list/complete/abort, retries, ETags/checksums, and resume instructions."}, {"type": "llm_judge", "name": "Insight vs Payload Prioritization Enforcement", "description": "Verify that insight data is prioritized for fast availability while payload data is de-prioritized (e.g., batch or offline).", "weight": 2.6, "judge_prompt": "Check how the API and data_flow enforce prioritization.\n\nIndicators:\n- A schema enum or field (e.g., sensorType: INSIGHT|PAYLOAD) with rules that insight uploads are expedited/priority queue.\n- Separate endpoints/queues or priority parameters; SLOs/availability commitments for insight data.\n- Data_flow explains faster processing path and exposure to customers; payload can be delayed (monthly, SSD shipping path).\n- Lifecycle/policies in S3/DynamoDB reflecting priority, and pipeline branching accordingly.\n\nScoring:\n- 2.6: Clear, actionable prioritization encoded in API and described in data_flow, including SLO/SLA hints and queueing/processing differences.\n- 1.3: Prioritization mentioned but weakly enforced (no fields/queues or unclear processing differences).\n- 0.0: No meaningful prioritization mechanism.", "expectation": "An explicit, enforceable priority mechanism that routes INSIGHT faster than PAYLOAD, with clear processing availability."}, {"type": "llm_judge", "name": "Mission Lifecycle and Resume Semantics", "description": "Ensure the design supports multiple missions per day, partial mission completion due to battery, and resuming missions after recharge.", "weight": 2.6, "judge_prompt": "Evaluate mission lifecycle coverage.\n\nLook for:\n- Endpoints for creating missions, updating progress/status, and completing missions.\n- Ability to indicate incomplete objectives and resume later (e.g., PATCH mission progress, append new segments, idempotent updates).\n- Handling multiple missions per robot per day (filters, pagination, timestamps) and unique mission IDs.\n- Consistency with mission_metadata.json and variability of sensors across robot types.\n\nScoring:\n- 2.6: Comprehensive lifecycle with explicit resume semantics and idempotent operations.\n- 1.3: Basic lifecycle present but resume/partial completion unclear.\n- 0.0: Lifecycle insufficient for partial missions/resume.", "expectation": "Complete mission CRUD with progress updates, resume after recharge, and alignment to metadata variability."}, {"type": "llm_judge", "name": "Cloud Processing Pipeline Triggers and Status", "description": "Validate that a multistage processing pipeline is represented with triggers, state transitions, and status visibility.", "weight": 2.6, "judge_prompt": "Assess the orchestration and visibility of the cloud pipeline.\n\nLook for:\n- Events/triggers on upload completion (e.g., S3 event, explicit API call) that initiate processing.\n- Modeled stages (e.g., INGESTED -> VALIDATED -> TRANSFORMED -> INDEXED -> AVAILABLE) and status endpoints per mission/sensor.\n- Webhooks/callbacks/notifications to external systems; ability to poll status; error states and retries.\n- DynamoDB usage for mission metadata and upload/processing status; S3 prefixes/buckets for each stage.\n\nScoring:\n- 2.6: Clear, multi-stage pipeline with triggers, retries, and status exposure.\n- 1.3: Partial pipeline or unclear status/triggering.\n- 0.0: No meaningful pipeline orchestration.", "expectation": "A multi-stage, event-driven pipeline with observable status and retry/error handling."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism", "description": "Holistic assessment of API design and documentation quality for production robotics data systems.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "API Design Clarity and Consistency", "description": "Evaluate REST resource modeling, naming consistency, versioning, pagination/filtering, and response consistency.", "weight": 2.0, "judge_prompt": "Rate the API design quality.\n\nConsider:\n- Resource modeling and naming consistency (e.g., /v1/robots, /v1/missions/{id}, /v1/missions/{id}/uploads).\n- Standard patterns (pagination, filtering, sorting) and consistent request/response shapes.\n- Error model (problem+json or structured error schema), status codes, and idempotency keys for unsafe operations.\n- Schema quality: required fields, enums, examples.\n\nScore 0\u20132 based on professional polish and internal consistency.", "expectation": "A clean, versioned, consistent REST design with good error handling and pagination."}, {"type": "llm_judge", "name": "Security, Integrity, and Robustness", "description": "Assess authN/Z, multi-tenant isolation, integrity checks, and rate limiting/backpressure.", "weight": 2.0, "judge_prompt": "Evaluate security and robustness provisions.\n\nLook for:\n- Authentication (e.g., Bearer JWT, API keys, or AWS SigV4) and role-based access.\n- Least-privilege presigned URLs scoped to object/part with expirations; server-side checksums, ETags, content-type/size limits.\n- Idempotency keys, replay protection, throttling/rate limits, retries with backoff.\n- Encryption at rest/in transit, and audit/traceability.\n\nScore 0\u20132 based on completeness and practicality.", "expectation": "Thoughtful auth, presigned URL scoping, checksums, idempotency, throttling, and encryption."}, {"type": "llm_judge", "name": "Documentation Completeness and Developer Experience", "description": "Assess the helpfulness of descriptions, examples, and guidance for clients implementing the API.", "weight": 2.0, "judge_prompt": "Evaluate developer experience.\n\nConsider:\n- Operation summaries/descriptions, parameter and schema descriptions, clear response examples.\n- Error examples with troubleshooting tips.\n- Webhook/callback documentation and example payloads.\n- Clear guidance for chunk sizing, retries, and client-side resume logic.\n\nScore 0\u20132 based on clarity and completeness.", "expectation": "Well-documented API with examples and actionable guidance for implementers."}, {"type": "llm_judge", "name": "Operational Realism and Data Flow Clarity", "description": "Judge whether data_flow.txt provides practical, end-to-end operational guidance aligned to large-scale robotics constraints.", "weight": 2.0, "judge_prompt": "Assess the realism and clarity of data_flow.txt.\n\nConsider:\n- Separate fast path for INSIGHT data and deferred/offline path for PAYLOAD (including SSD shipping intake steps).\n- Concrete step-by-step flows from robot staging to cloud processing and customer availability.\n- Handling intermittent connectivity, partial missions, variable sensor configs, and multiple robot types.\n- Observability (metrics/logs), cost controls (S3 lifecycle), and capacity planning.\n\nScore 0\u20132 based on practicality and completeness.", "expectation": "Clear, actionable operations narrative covering both fast and slow paths, failure modes, and scale considerations."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "4c18ebae-dfaa-4b76-b10c-61fcdf26734c", "rubric": {"category_name": "Government - Compliance Officers - SAR Preparation (FinCEN)", "rationale": "Mixed-output task: a SAR narrative document (DOCX/PDF) and a supporting transactions workbook (Excel). The rubric enforces a strict, self-documenting shape first (LLM-only gate), then verifies correctness and cross-file consistency using a mix of LLM and code rules, and finally assesses professional quality and suitability for senior management review.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 - Shape and Format Gate (LLM ONLY)", "description": "MANDATORY gate verifying that outputs exist in the exact, verifiable structure: a \u22644-page SAR document (DOCX/PDF) with required sections and a structured Excel workbook with specific sheets and columns.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.5, "rules": [{"type": "llm_judge", "name": "SAR Document Structure Requirement", "description": "The SAR must be a DOCX or PDF, no more than 4 pages, with the required sections that enable later verification.", "weight": 3.0, "judge_prompt": "You are reviewing all submitted files. Identify the SAR narrative document (must be DOCX or PDF). Check ONLY format and section presence; do NOT judge content quality or calculation correctness.\n\nRequired format:\n- File type: PDF or DOCX\n- Length: No more than 4 pages\n- Professional formatting with clear section headers\n\nRequired sections (flexible naming allowed, but headers must be clearly visible):\n1) Executive Summary or Overview (must be on page 1)\n2) Parties/Suspects Involved (include entities/individuals)\n3) Account Information (accounts/relationships at The Golden Apple Bank)\n4) Narrative of Suspicious Activity (Who, What, When, Where, Why/How per FinCEN)\n5) Red Flags / Typology Mapping (human trafficking-related indicators)\n6) Actions Taken / Law Enforcement Interaction and Confidentiality Statement\n7) Attachments / Supporting Records (explicit reference to the Excel transactions workbook)\n8) Conclusion or Recommendations\n\nScoring:\n- 3.0: Valid DOCX/PDF, \u22644 pages, and all 8 sections present (reasonable header synonyms allowed)\n- 2.5: Valid DOCX/PDF, \u22644 pages, and 7/8 sections present\n- 2.0: Valid DOCX/PDF, \u22644 pages, and 6/8 sections present\n- 1.0: Valid DOCX/PDF, \u22644 pages, and 4\u20135 sections present\n- 0.5: Valid DOCX/PDF but only 1\u20133 sections or >4 pages\n- 0.0: Not DOCX/PDF or cannot identify the SAR document", "expectation": "A \u22644-page DOCX/PDF SAR narrative with all required sections clearly labeled."}, {"type": "llm_judge", "name": "Excel Workbook Structure Requirement", "description": "The transactions workbook must be an Excel file with required sheets and columns that enable automated checks.", "weight": 3.0, "judge_prompt": "You are reviewing all submitted files. Identify the transactions workbook (Excel). Check ONLY structure and presence of required sheets/columns; do NOT judge data correctness.\n\nRequired file: Excel (.xlsx)\nRequired sheets (flexible naming allowed but must be clear):\n- Transactions (primary detail table)\n- Summary (two-column KPI summary)\nOptional sheet:\n- Red Flag Mapping (links transactions to FinCEN human trafficking red flags)\n\nRequired Transactions columns (exact or clear synonyms):\n- Date\n- Account Number\n- Account Name\n- Counterparty or Source\n- Location (City, State)\n- Channel (e.g., Branch/ATM/Online)\n- Type (e.g., Cash Deposit, Wire, ACH)\n- Amount\n- Cash Indicator (Y/N)\n- CTR Threshold Flag (Y/N)\n- Structuring Pattern ID (blank or an ID if suspected)\n- Notes\n\nRequired Summary structure:\n- A two-column table with headers like Metric and Value\n- Must include rows for: Total Transactions, Total Amount, Cash Deposits (Count), Cash Deposits (Amount), States Involved (Count)\n\nScoring:\n- 3.0: Excel present + Transactions and Summary sheets present with nearly all required columns/rows; Red Flag Mapping sheet present (optional)\n- 2.5: Excel present + Transactions and Summary present with most required columns/rows; Red Flag Mapping missing\n- 2.0: Excel present + Transactions present with most columns; Summary present but missing some required KPI rows\n- 1.0: Excel present but Transactions/Summary are incomplete (large gaps)\n- 0.0: No Excel file detected or sheets/columns not identifiable", "expectation": "An Excel workbook with a detailed Transactions table and a KPI Summary table enabling automated verification checks."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 - Verification and Cross-File Consistency", "description": "Now that the shape is correct, verify correctness, internal consistency, and adherence to FinCEN guidance using code and LLM checks.", "is_required": false, "max_points": 9.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "FinCEN Narrative Completeness (5 Essential Elements)", "description": "Check whether the SAR narrative addresses Who, What, When, Where, Why/How per FinCEN guidance and ties to human trafficking indicators.", "weight": 3.5, "judge_prompt": "Evaluate the SAR narrative (DOCX/PDF) for FinCEN completeness:\n- Who: identifies subjects (Bluehaven Collective LLC, Owen Tavery, Silverleaf Partners LLC, Victor Curcun) and their roles/relationships\n- What: suspicious activity description (cash deposits from unknown sources, structuring, interstate activity)\n- When: relevant timeframes (e.g., 2012/2015/2018 onboarding dates, account openings/closures, Jul 2024 overlap, Sep 2023\u2013Aug 2024 period)\n- Where: locations (Las Vegas NV, Miami FL, address 6903 Oakridge Way Suite 1, I-95 corridor reference if used)\n- Why/How: reasoning linking activity to potential human trafficking (citing FinCEN advisories and specific red flags)\n\nScoring guidance:\n- 3.5: All five elements present, specific, and well integrated with the indicators\n- 2.5: Four elements present with reasonable detail\n- 1.5: Three elements present, somewhat generic\n- 0.5: One\u2013two elements present\n- 0.0: Does not address the FinCEN five elements in a meaningful way", "expectation": "A complete and specific narrative covering all five elements per FinCEN guidance, clearly linked to human trafficking indicators."}, {"type": "llm_judge", "name": "Cross-Reference Consistency: Narrative vs. Excel", "description": "Check that the SAR narrative descriptions match the Excel workbook facts (entities, dates, locations, patterns, totals).", "weight": 2.5, "judge_prompt": "Cross-check the SAR document against the Excel workbook:\n- Do the entities/individuals in the narrative appear in the Transactions/Accounts info?\n- Are key dates (e.g., Sep 2023\u2013Aug 2024 for Bluehaven, July 2024 for Silverleaf at same address) consistent with transaction periods cited in the workbook?\n- Are locations mentioned (Las Vegas NV, Miami FL, 6903 Oakridge Way Suite 1) reflected in transaction locations or notes?\n- Do described patterns (cash-heavy deposits, structuring near $10,000, interstate/I-95 link) appear to be supported by the workbook data?\n- If totals or counts are cited in the narrative, are they approximately consistent with Summary values?\n\nScoring guidance:\n- 2.5: Strong alignment on entities, timeframes, locations, and patterns; no contradictions\n- 1.7: Mostly aligned with minor inconsistencies\n- 1.0: Mixed alignment; several inconsistencies\n- 0.3: Weak alignment; many contradictions\n- 0.0: No apparent alignment", "expectation": "Narrative statements are consistent with workbook facts and summaries."}, {"type": "llm_judge", "name": "Human Trafficking Red Flags Mapping", "description": "Assess whether the SAR correctly identifies and maps red flags to FinCEN advisories (FIN-2014-A008, 2020 supplemental) with appropriate cautionary language.", "weight": 1.5, "judge_prompt": "Evaluate how the SAR identifies human trafficking red flags and ties them to FinCEN advisories:\n- Explicit references (or clear paraphrases) to relevant advisory indicators (e.g., cash-intensive spa operations, frequent cash deposits just under CTR thresholds, interstate movements along major corridors like I-95, co-location with prior suspect entities)\n- Appropriate phrasing (avoids conclusory statements; uses language like \"suspected\"/\"indicative of\")\n- Clear linkage between observed transactions and specific red flags\n\nScoring guidance:\n- 1.5: Clear, correct mapping to multiple specific advisory red flags with careful language\n- 1.0: Generally correct mapping to at least one advisory with some specificity\n- 0.5: Vague mention of red flags without clear linkage\n- 0.0: No meaningful mapping to FinCEN advisories", "expectation": "Specific, appropriate mapping of observed behaviors to FinCEN human trafficking red flags."}, {"type": "code", "name": "Spreadsheet Structural Integrity (Flexible)", "description": "Programmatic check for presence of a transactions sheet, required columns, and a summary sheet with Metric/Value columns.", "weight": 0.75, "code": "import re\nimport json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        # Find a spreadsheet among all outputs\n        spreadsheet = None\n        for res in context.get_all_outputs():\n            if getattr(res, 'is_spreadsheet', False):\n                spreadsheet = res\n                break\n        if not spreadsheet:\n            return 0.0, 'No spreadsheet found among outputs.'\n\n        # Helper: load sheet names\n        xls_path = context.files.get_path(spreadsheet.id)\n        try:\n            xls = pd.ExcelFile(xls_path)\n            sheet_names = [str(s) for s in xls.sheet_names]\n        except Exception as e:\n            return 0.0, f'Failed to read Excel file: {e}'\n\n        # Fuzzy sheet matching\n        def find_sheet(candidates):\n            lowered = [s.lower().strip() for s in sheet_names]\n            for i, s in enumerate(lowered):\n                for c in candidates:\n                    if c in s:\n                        return sheet_names[i]\n            return None\n\n        transactions_sheet = find_sheet(['transaction', 'txn', 'records', 'data'])\n        summary_sheet = find_sheet(['summary', 'kpi', 'overview'])\n\n        score_components = []\n        details = []\n\n        # Check Transactions sheet and columns\n        required_txn_cols = [\n            'date', 'account number', 'account name', 'counterparty', 'source',\n            'location', 'city', 'state', 'channel', 'type', 'amount',\n            'cash indicator', 'ctr threshold flag', 'structuring pattern id', 'notes'\n        ]\n        # We'll treat (counterparty/source) and (location or city+state) flexibly\n\n        if transactions_sheet is None:\n            score_components.append(0.0)\n            details.append('Transactions sheet not found.')\n        else:\n            try:\n                df_txn = context.files.read_excel(spreadsheet.id, sheet_name=transactions_sheet)\n                # Normalize column names\n                cols = [re.sub(r'[^a-z]', '', str(c).lower()) for c in df_txn.columns]\n                colset = set(cols)\n\n                # Define flexible targets\n                def has_any(targets):\n                    return any(t in colset for t in targets)\n\n                checks = []\n                # Must have date, account number, account name, type, amount\n                checks.append('date' in colset)\n                checks.append('accountnumber' in colset)\n                checks.append('accountname' in colset)\n                checks.append('type' in colset)\n                checks.append('amount' in colset)\n                # Cash indicator and CTR flag\n                checks.append('cashindicator' in colset)\n                checks.append('ctrthresholdflag' in colset)\n                # Location: either single combined or city+state\n                loc_ok = ('location' in colset) or ('city' in colset and 'state' in colset)\n                checks.append(loc_ok)\n                # Counterparty or Source\n                cp_ok = has_any({'counterparty', 'source'})\n                checks.append(cp_ok)\n                # Structuring Pattern ID and Notes\n                checks.append('structuringpatternid' in colset)\n                checks.append('notes' in colset)\n\n                txn_score = sum(checks) / len(checks)\n                score_components.append(txn_score)\n                details.append(f'Transactions sheet found ({transactions_sheet}); column coverage {txn_score:.2f}.')\n            except Exception as e:\n                score_components.append(0.0)\n                details.append(f'Failed to read Transactions sheet: {e}')\n\n        # Check Summary sheet with Metric/Value structure\n        if summary_sheet is None:\n            score_components.append(0.0)\n            details.append('Summary sheet not found.')\n        else:\n            try:\n                df_sum = context.files.read_excel(spreadsheet.id, sheet_name=summary_sheet)\n                cols = [re.sub(r'[^a-z]', '', str(c).lower()) for c in df_sum.columns]\n                colset = set(cols)\n                has_metric = 'metric' in colset\n                has_value = 'value' in colset\n                sum_score = (1.0 if (has_metric and has_value) else 0.5 if (has_metric or has_value) else 0.0)\n                score_components.append(sum_score)\n                details.append(f'Summary sheet found ({summary_sheet}); metric/value presence score {sum_score:.2f}.')\n            except Exception as e:\n                score_components.append(0.0)\n                details.append(f'Failed to read Summary sheet: {e}')\n\n        # Aggregate: average of components\n        if not score_components:\n            return 0.0, 'No structural checks performed.'\n        final_score = float(np.mean(score_components))\n        feedback = '; '.join(details)\n        return max(0.0, min(1.0, final_score)), feedback\n    except Exception as e:\n        return 0.0, f'Unexpected error in structural integrity check: {e}'"}, {"type": "code", "name": "CTR Threshold Flag Consistency", "description": "For cash transactions with Amount >= 10,000, CTR Threshold Flag should be Y. Scores by proportion correctly flagged.", "weight": 0.75, "code": "import re\nimport json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        # Find a spreadsheet among all outputs\n        spreadsheet = None\n        for res in context.get_all_outputs():\n            if getattr(res, 'is_spreadsheet', False):\n                spreadsheet = res\n                break\n        if not spreadsheet:\n            return 0.0, 'No spreadsheet found.'\n\n        # Load transactions sheet name\n        xls_path = context.files.get_path(spreadsheet.id)\n        try:\n            xls = pd.ExcelFile(xls_path)\n            sheet_names = [str(s) for s in xls.sheet_names]\n        except Exception as e:\n            return 0.0, f'Failed to read Excel file: {e}'\n\n        def find_sheet(candidates):\n            for s in sheet_names:\n                sl = s.lower().strip()\n                if any(c in sl for c in candidates):\n                    return s\n            return None\n\n        transactions_sheet = find_sheet(['transaction', 'txn', 'records', 'data'])\n        if not transactions_sheet:\n            return 0.0, 'Transactions sheet not found.'\n\n        df = context.files.read_excel(spreadsheet.id, sheet_name=transactions_sheet)\n        # Normalize columns\n        norm_cols = {re.sub(r'[^a-z]', '', str(c).lower()): c for c in df.columns}\n        def get_col(key_options):\n            for k in key_options:\n                if k in norm_cols:\n                    return norm_cols[k]\n            return None\n\n        col_amount = get_col(['amount'])\n        col_cash = get_col(['cashindicator'])\n        col_ctr = get_col(['ctrthresholdflag'])\n        if not (col_amount and col_cash and col_ctr):\n            return 0.0, 'Required columns (Amount, Cash Indicator, CTR Threshold Flag) not found.'\n\n        # Coerce\n        amt = pd.to_numeric(df[col_amount], errors='coerce')\n        cash_raw = df[col_cash].astype(str).str.lower().str.strip()\n        ctr_raw = df[col_ctr].astype(str).str.lower().str.strip()\n\n        def to_bool(s):\n            return s in {'y','yes','true','1'}\n\n        is_cash = cash_raw.apply(to_bool)\n        is_ctr_flag = ctr_raw.apply(to_bool)\n\n        mask = is_cash & (amt >= 10000)\n        denom = int(mask.sum())\n        if denom == 0:\n            # No applicable transactions; do not penalize\n            return 1.0, 'No cash transactions >= $10,000; CTR flag consistency not applicable.'\n\n        correct = int((is_ctr_flag[mask]).sum())\n        score = correct / max(1, denom)\n        return float(max(0.0, min(1.0, score))), f'CTR correctness: {correct}/{denom} applicable transactions flagged.'\n    except Exception as e:\n        return 0.0, f'Error during CTR consistency check: {e}'"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 - Professional Quality and Actionability", "description": "Holistic assessment of writing quality, presentation, and usefulness for senior management and regulatory compliance.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Tone and Compliance Style", "description": "Assess whether the SAR uses professional, objective, and compliant language appropriate for FinCEN and senior management.", "weight": 1.5, "judge_prompt": "Evaluate the SAR document for:\n- Objective, professional tone; clear avoidance of tipping-off language\n- Proper use of qualifying language (e.g., \"suspected,\" \"indicative of\")\n- Concise, compliant SAR style suitable for regulatory submission and internal review\nScore:\n- 1.5 excellent; 1.0 good; 0.5 fair; 0.0 poor", "expectation": "An objective, compliant tone with precise, non-speculative language."}, {"type": "llm_judge", "name": "Clarity, Organization, and Brevity", "description": "Check structure, flow, clarity, and adherence to the \u22644-page limit; assess whether headings, paragraphs, and tables are well organized.", "weight": 1.5, "judge_prompt": "Assess the SAR document for clarity and organization:\n- Logical flow across required sections; effective headings and concise paragraphs\n- Clear signposting of the five FinCEN elements within the narrative\n- Adherence to the \u22644-page limit\nScore:\n- 1.5 excellent; 1.0 good; 0.5 fair; 0.0 poor", "expectation": "Well-structured, readable report within the 4-page limit."}, {"type": "llm_judge", "name": "Actionability for Senior Management", "description": "Evaluate whether the SAR provides clear next steps, risk assessment, and filing recommendations appropriate for internal approval.", "weight": 1.0, "judge_prompt": "Review for actionability:\n- Clear recommendation to file SAR and timeline\n- Suggested risk rating, monitoring steps, and potential account actions (as appropriate)\n- References to attached workbook for decision support\nScore:\n- 1.0 actionable; 0.6 mostly actionable; 0.3 limited; 0.0 not actionable", "expectation": "Explicit recommendations and next steps for senior management."}, {"type": "llm_judge", "name": "Excel Data Presentation Quality", "description": "Assess whether the workbook is readable and decision-useful (clean formatting, consistent date/amount formats, helpful summary KPIs).", "weight": 1.0, "judge_prompt": "Evaluate the Excel workbook for:\n- Consistent date formats and currency formatting\n- Clear headers, filters, and readable layout\n- Summary sheet conveys key KPIs (e.g., counts, totals) clearly\nScore:\n- 1.0 excellent; 0.6 good; 0.3 fair; 0.0 poor", "expectation": "A clean, readable workbook with clear KPIs and consistent formats."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "552b7dd0-96f4-437c-a749-0691e0e4b381", "rubric": {"category_name": "Manufacturing \u2014 Shipping/Receiving/Inventory: Incident Analysis Presentation", "rationale": "This rubric enforces a self-documenting workflow: Stage 1 (LLM-only gate) mandates a precise, reviewable presentation structure so verification is possible. Stage 2 mixes lightweight code checks (text extraction, numeric plausibility) with heavier LLM consistency checks (counts vs. percentages, scope/methodology). Stage 3 assesses professional communication and strategic value. Code rules are deliberately low-weight compared to LLM rules, per guidance.", "max_total_score": 13.0, "stages": [{"name": "Stage 1 \u2014 Presentation Structure Gate", "description": "LLM-only check that the candidate produced a properly structured presentation export with required sections enabling verification.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.0, "rules": [{"type": "llm_judge", "name": "Structured Presentation Format Requirement", "description": "Verify the output is a presentation export (PDF or PPTX preferred; DOCX acceptable only if clearly slide-like) with specific slides and artifacts present so later verification is trivial.", "weight": 3.0, "judge_prompt": "You are checking ONLY the format/structure, not the correctness of numbers. Evaluate the candidate\u2019s FINAL OUTPUT file. Acceptable formats: PDF (preferred) or PPTX. If DOCX is used, it must clearly be structured as slides with slide-like pages.\n\nRequirements (be flexible with slide titles but strict on presence):\n- File format: PDF or PPTX (preferred). DOCX acceptable only if slide-like. Not Excel/CSV/MD.\n- Length: At least 8 slides/pages.\n- Slide 1: Title slide with project/report name, organization or warehouse, analyst/author, and date/year.\n- Slide: Data Source & Scope \u2014 References the attached/underlying incident Excel data for the year; defines incident types (RMA vs Work Order) and time window (this year) explicitly.\n- Slide: Incidents per Supplier \u2014 shows number of incidents per supplier with a visual (bar chart preferred) AND presents counts (labels or table visible).\n- Slide: Percentage of Incidents per Supplier \u2014 shows percentages compared to total with a visual (pie/donut/100% bar) AND percentage labels are visible.\n- Slide: Total Cost of Resolving Incidents \u2014 a KPI or chart summarizing total cost (can include breakdown by supplier).\n- Slide: Average Time to Resolve \u2014 shows overall average duration with units (e.g., days/hours) AND separate statistics for RMAs and Work Orders (either on the same slide or adjacent slide). Labels must clearly indicate which is which.\n- Final slide: Summary/Recommendations \u2014 synthesizes recurring themes from incident descriptions into key takeaways and offers concrete recommendations to management.\n- Visuals: At least two charts total across the deck.\n\nScoring (structure only):\n- 3.0: Valid presentation format + all required slides/elements present (minor naming variations okay).\n- 2.5: Valid format, missing exactly one required slide/element OR only one chart present.\n- 2.0: Valid format, missing two required slides/elements.\n- 1.0: Valid format but missing three or more required elements OR fewer than 8 slides/pages.\n- 0.0: Wrong format (not PDF/PPTX/DOCX) OR a single flat text page with no slide structure.\n\nDo not judge calculation correctness or content quality here\u2014only verify presence and structure that makes later verification possible.", "expectation": "A PDF or PPTX presentation with the specified slides and visuals present."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification and Consistency Checks", "description": "Check correctness and internal consistency, using small code checks for plausibility and LLM checks for cross-figure consistency and methodology clarity.", "is_required": false, "max_points": 7.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Keyword Coverage for Required Analyses", "description": "Extract text (PDF/DOCX) and verify presence of key topics: incidents per supplier, percentages, total cost, and time to resolve (including RMA and Work Order). Scores proportionally.", "weight": 0.3, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float (score) or tuple[float, str]\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = ''\n    # Try reading PDF first, then DOCX, then generic text\n    try:\n        if hasattr(context.files, 'read_pdf_text'):\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                text = ''\n        if not text and hasattr(context.files, 'read_docx_text'):\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                text = ''\n        if not text and hasattr(context.files, 'read_text'):\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = ''\n    except Exception:\n        text = ''\n\n    if not text:\n        return 0.0\n\n    t = text.lower()\n    checks = []\n\n    # Incidents per supplier (look for co-occurrence)\n    checks.append(('incidents_per_supplier', ('incident' in t and 'supplier' in t)))\n\n    # Percentages per supplier (either '%' near supplier or phrase)\n    percent_present = ('%' in t and 'supplier' in t) or ('percentage' in t and 'supplier' in t)\n    checks.append(('percentages_per_supplier', percent_present))\n\n    # Total cost of resolving incidents\n    cost_keywords = any(k in t for k in ['total cost', 'cost of resolving', 'resolution cost'])\n    dollars = re.findall(r\"\\$\\s*[0-9][0-9,]*(?:\\.[0-9]{2})?\", text)\n    checks.append(('total_cost_present', cost_keywords or len(dollars) > 0))\n\n    # Average time required to resolve (overall + separate for RMA and Work Order)\n    time_kw = any(k in t for k in ['average time', 'avg time', 'mean time', 'average resolution', 'avg resolution'])\n    unit_kw = any(u in t for u in ['day', 'days', 'hour', 'hours', 'hr', 'hrs'])\n    rma_present = 'rma' in t\n    wo_present = ('work order' in t) or ('work-order' in t) or ('workorder' in t)\n    checks.append(('time_and_types_present', time_kw and unit_kw and rma_present and wo_present))\n\n    passed = sum(1 for _, ok in checks if ok)\n    score = (passed / 4.0) * 0.3\n    feedback = \", \".join([f\"{name}:{'ok' if ok else 'missing'}\" for name, ok in checks])\n    return score, feedback\n"}, {"type": "code", "name": "Percentages Sum Plausibility", "description": "From extracted text, gather percent values. If at least 3 percentages are present, their sum should plausibly be near 100%. Full credit if sum in [95,105]; partial if in [80,120].", "weight": 0.3, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = ''\n    try:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = ''\n        if not text:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                text = ''\n        if not text:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = ''\n    except Exception:\n        text = ''\n\n    if not text:\n        return 0.0\n\n    percents = re.findall(r\"(-?\\d+(?:\\.\\d+)?)\\s*%\", text)\n    values = [float(p) for p in percents]\n    if len(values) < 3:\n        return 0.0\n\n    total = sum(values)\n    # Full credit if very close to 100, half credit if reasonably close\n    if 95 <= total <= 105:\n        return 0.3, f\"percent_sum={total:.1f}% (full)\"\n    elif 80 <= total <= 120:\n        return 0.15, f\"percent_sum={total:.1f}% (partial)\"\n    else:\n        return 0.0\n"}, {"type": "code", "name": "Monetary and Duration Plausibility", "description": "Check presence and plausibility of at least one monetary value (for total cost) and at least one duration value (with time units).", "weight": 0.3, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = ''\n    try:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = ''\n        if not text:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                text = ''\n        if not text:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = ''\n    except Exception:\n        text = ''\n\n    if not text:\n        return 0.0\n\n    score = 0.0\n    fb = []\n\n    # Monetary values\n    money_matches = re.findall(r\"\\$\\s*([0-9][0-9,]*(?:\\.[0-9]{2})?)\", text)\n    money_vals = []\n    for m in money_matches:\n        try:\n            money_vals.append(float(m.replace(',', '')))\n        except Exception:\n            pass\n    if money_vals:\n        max_val = max(money_vals)\n        if 0 < max_val < 1_000_000_000:\n            score += 0.15\n            fb.append(f\"money_ok(max={max_val:.2f})\")\n        else:\n            fb.append(\"money_out_of_range\")\n    else:\n        fb.append(\"money_missing\")\n\n    # Duration values with units\n    dur_matches = re.findall(r\"(\\d+(?:\\.\\d+)?)\\s*(day|days|hour|hours|hr|hrs)\\b\", text, flags=re.IGNORECASE)\n    durations = []\n    for val, unit in dur_matches:\n        try:\n            num = float(val)\n            # Normalize hours to days for plausibility check (not used further)\n            if unit.lower() in ['hour', 'hours', 'hr', 'hrs']:\n                num = num / 24.0\n            durations.append(num)\n        except Exception:\n            pass\n    if durations:\n        max_d = max(durations)\n        if 0 < max_d < 1000:  # extremely lenient bound\n            score += 0.15\n            fb.append(f\"duration_ok(max_days_equiv={max_d:.2f})\")\n        else:\n            fb.append(\"duration_out_of_range\")\n    else:\n        fb.append(\"duration_missing\")\n\n    return min(score, 0.3), \", \".join(fb)\n"}, {"type": "llm_judge", "name": "Counts vs. Percentages Cross-Check", "description": "Verify that the incidents-per-supplier counts and the percentage-by-supplier chart are mutually consistent (rankings and totals).", "weight": 2.0, "judge_prompt": "Evaluate internal consistency between the counts-by-supplier and the percentage-by-supplier visuals:\n- Do the top suppliers by count also have the largest percentages in the percent chart?\n- If the total number of incidents is visible, do the counts sum to that total (\u00b15%)?\n- Do the shown percentages plausibly correspond to the counts (e.g., if supplier A has double the incidents of supplier B, the percentage should be roughly double)?\n\nScoring:\n- 2.0: Clear alignment of rankings; counts sum to total (\u00b15%); percentages consistent with counts.\n- 1.0: Mostly consistent with minor discrepancies (rank order off by one or small arithmetic mismatch).\n- 0.0: Major inconsistencies (rankings inverted, totals don\u2019t sum, or percentages clearly mismatch counts).", "expectation": "Counts and percentages tell the same story and sum/scale correctly."}, {"type": "llm_judge", "name": "Resolution Time Statistics Separation", "description": "Confirm presence of overall average resolution time with units and separate stats for RMAs and Work Orders; check plausibility and labeling clarity.", "weight": 2.0, "judge_prompt": "Check the resolution time reporting:\n- Is there an overall average time with units (days/hours)?\n- Are separate statistics shown for RMAs and for Work Orders, with clear labels?\n- Do values appear plausible (e.g., not 0 unless justified; typically within 0\u2013120 days for warehouse incidents unless otherwise explained)?\n- If overall and subgroup averages are shown, is the overall reasonably between the subgroup values (or otherwise explained)?\n\nScoring:\n- 2.0: All present, well-labeled, units shown, and values plausible/consistent.\n- 1.0: Present but minor issues (missing units OR borderline plausibility OR unclear labeling).\n- 0.0: Missing separation or clearly implausible numbers.", "expectation": "Overall and per-type averages clearly labeled with units and reasonable values."}, {"type": "llm_judge", "name": "Cost and Scope Integrity", "description": "Validate that the total cost is clearly presented, scope matches the year, and any supplier breakdowns reconcile to the total.", "weight": 2.0, "judge_prompt": "Assess the cost and scope presentation:\n- Is a total cost of resolving incidents shown for the specified year?\n- If a by-supplier cost breakdown is shown, do the parts approximately add up to the total (\u00b15\u201310%)?\n- Is the time scope (this year or a specific year) explicitly stated and used consistently across slides?\n\nScoring:\n- 2.0: Clear total for the correct scope; breakdown sums closely to total; scope consistent.\n- 1.0: Minor inconsistencies (e.g., unclear scope phrasing or small arithmetic gaps) but generally correct.\n- 0.0: Total cost missing, wrong scope, or breakdown does not reconcile.", "expectation": "A clearly scoped annual total cost, with any breakdowns reconciling to that total."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Communication Quality and Actionability", "description": "Holistic quality assessment of the presentation for a management audience.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Executive Synthesis and Recommendations", "description": "Quality of the final summary and recommendations grounded in incident patterns.", "weight": 0.75, "judge_prompt": "Evaluate the final summary/takeaways and recommendations:\n- Are takeaways clearly derived from recurring themes in incident descriptions (e.g., specific failure modes, packaging issues, supplier-specific defects)?\n- Are there at least two concrete supplier-facing actions and two internal warehouse/process actions?\n- Are recommendations specific, measurable where possible, and prioritized by impact/feasibility?\n\nScoring:\n- 0.75: Clear, data-grounded, actionable and prioritized recommendations.\n- 0.4: Some actionable content but generic or weakly linked to data.\n- 0.0: Vague or not grounded in the analysis.", "expectation": "A concise, data-driven summary with concrete, prioritized actions."}, {"type": "llm_judge", "name": "Visual Communication and Clarity", "description": "Professional presentation of visuals, labels, and readability.", "weight": 0.75, "judge_prompt": "Assess visual quality:\n- Are charts/tables legible with clear titles, axis labels, and data labels where appropriate?\n- Are chart types appropriate (e.g., bar for counts, pie/100% bar for shares)?\n- Is formatting consistent (fonts, colors, legends) and accessible (sufficient contrast)?\n\nScoring:\n- 0.75: Professional, clear visuals that communicate effectively.\n- 0.4: Generally readable with minor clarity issues.\n- 0.0: Poorly labeled or confusing visuals.", "expectation": "Clean, well-labeled visuals that fit the data story."}, {"type": "llm_judge", "name": "Audience Fit and Structure", "description": "Slide flow, conciseness, and tone for management stakeholders.", "weight": 0.75, "judge_prompt": "Evaluate structure and tone:\n- Logical flow from context to findings to implications and actions.\n- Concise slides (ideally 8\u201315), minimal clutter, and executive-friendly tone.\n- Clear navigation (headings, section dividers) and consistent formatting.\n\nScoring:\n- 0.75: Strong flow, concise, and executive-ready.\n- 0.4: Adequate but could be more concise or structured.\n- 0.0: Disorganized or verbose.", "expectation": "A concise, logically structured deck suitable for management review."}, {"type": "llm_judge", "name": "Actionability and Risk Awareness", "description": "Does the deck translate insights into prioritized actions and acknowledge risks/limitations?", "weight": 0.75, "judge_prompt": "Assess actionability:\n- Are top suppliers/issues prioritized by frequency, cost, or time impact?\n- Are next steps, owners, and timelines suggested where appropriate?\n- Are risks/limitations and data caveats acknowledged (e.g., missing fields, outliers, reporting biases)?\n\nScoring:\n- 0.75: Clear prioritization, next steps, and risk awareness.\n- 0.4: Some actions but limited prioritization or caveat handling.\n- 0.0: No clear actions or awareness of limitations.", "expectation": "Prioritized actions with awareness of risks and constraints."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "a74ead3b-f67d-4b1c-9116-f6bb81b29d4f", "rubric": {"category_name": "Government \u2014 Child, Family, and School Social Workers: Session 13 & 14 Presentation Deliverables", "rationale": "This rubric enforces a self-documenting, file-based delivery of two session-specific slide decks. Stage 1 (LLM-only) mandates exact structural shape so downstream verification is trivial. Stage 2 mixes light code checks (file presence/types, minimal size, session identifiers) with LLM correctness checks focused on fidelity to the Nurturing Parenting Program for Families in Substance Abuse Treatment and Recovery. Stage 3 applies holistic quality criteria (accessibility, visual design, engagement, and cultural/ethical sensitivity). Code rules are limited, flexible, and worth substantially less than LLM rules, per guidance.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate", "description": "LLM-only gate verifying the presence of two separate session presentations with required sections and basic structure to enable verification. Failure zeros the entire category.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.1, "rules": [{"type": "llm_judge", "name": "Presentation Format and Structure Gate (LLM)", "description": "Verify two distinct, session-specific presentation files exist (Session 13 and Session 14), each with a clear structure enabling verification.", "weight": 3.0, "judge_prompt": "You are evaluating whether the candidate delivered the required STRUCTURE ONLY. Use the rendered files (slides/pages) in the output. Do NOT judge content quality yet.\n\nCheck for BOTH of the following deliverables:\nA) One deck for Session 13\nB) One deck for Session 14\n\nAcceptable formats: PowerPoint (.pptx or .ppt) or PDF exports of slides. They must be two separate files (not a single combined deck unless it clearly splits into two distinct sections with separate title slides and wrap-ups; prefer two files).\n\nFor EACH deck (Session 13 and Session 14), check the presence of REQUIRED sections:\n1) Title slide: includes the program name or reference (e.g., Nurturing Parenting Program for Families in Substance Abuse Treatment and Recovery), the session number (13 or 14), and a session title.\n2) Icebreaker slide/activity: an opening engagement or warm-up appropriate for a recovery setting.\n3) Key Session Points section: several slides featuring the main teaching points for that session (bulleted, readable).\n4) Practice/skills slide(s): activity, role-play, reflection, or home practice task aligned to the session.\n5) Wrap-up slide: summary of key takeaways, next steps/home practice, and reminder of weekly cadence (90 minutes total session length recommendation is acknowledged or implied in pacing/slide count).\n\nAdditional STRUCTURAL expectations:\n- Visually engaging but neutral imagery (non-stigmatizing, family-friendly). At least some slides should include images or visual elements.\n- Accessible language (plain, concise) visible across slides.\n- Each deck reasonably sized for a ~90-minute session (typical range: ~10\u201318 slides; be flexible if content is clearly structured and complete).\n\nScoring (STRUCTURE ONLY):\n- 3.0: Two separate decks present (13 and 14), and BOTH decks include all required sections (1\u20135) and show neutral imagery and accessible text.\n- 2.5: Both decks present with minor omissions (e.g., imagery sparse OR slide count somewhat outside range) but all core sections (1\u20135) are present for both.\n- 2.1: Both decks present but ONE required section missing or clearly underdeveloped in one deck, while the other is complete.\n- 1.5: Only one deck present OR both present but several required sections missing across decks.\n- 0.0: Not slide/PDF format, or no session-specific decks, or structure not discernible.\n\nOnly evaluate presence/structure and basic sectioning per above. Do not assess correctness or quality beyond structural presence.", "expectation": "Two separate slide decks (13 and 14), each with title, icebreaker, key points, practice, and wrap-up; neutral images and accessible language visible."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Fidelity Verification", "description": "Verify the deliverables are the right two session decks, plausibly aligned to the program manual and constraints for families in recovery, with minimal technical adequacy. Mix light code checks with LLM judgment. Code rules carry substantially less weight than LLM rules.", "is_required": true, "max_points": 9.0, "min_score_to_pass": 4.5, "rules": [{"type": "code", "name": "Two Presentations Present and Valid Types", "description": "Checks that at least two presentation-like files exist (ppt/pptx/pdf). Partial credit if only one.", "weight": 0.8, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Ensures there are at least two outputs that look like presentation files (pptx/ppt/pdf).\n    Returns up to 0.8 points.\n    \"\"\"\n    W = 0.8\n    try:\n        outputs = context.get_all_outputs() or []\n    except Exception:\n        return 0.0, \"Unable to access outputs\"\n\n    valid = []\n    for r in outputs:\n        try:\n            path = context.files.get_path(r.id)\n            suf = path.suffix.lower()\n        except Exception:\n            suf = \"\"\n        if suf in [\".pptx\", \".ppt\", \".pdf\"]:\n            valid.append((r, suf))\n\n    n = len(valid)\n    if n >= 2:\n        return W, f\"Found {n} presentation-like files\"\n    if n == 1:\n        return W * 0.5, \"Only one presentation-like file found\"\n    return 0.0, \"No presentation-like files found\""}, {"type": "code", "name": "Distinct Sessions Identified (13 and 14)", "description": "Attempts to identify that sessions 13 and 14 are each represented, using text extraction for PDFs/DOCX where possible or filenames otherwise. Full credit when both sessions are present in distinct files.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Attempts to confirm both Session 13 and Session 14 are present, ideally in distinct files.\n    Returns up to 0.6 points.\n    \"\"\"\n    W = 0.6\n    try:\n        outputs = context.get_all_outputs() or []\n    except Exception:\n        return 0.0, \"Unable to access outputs\"\n\n    candidates = []\n    for r in outputs:\n        try:\n            path = context.files.get_path(r.id)\n            suf = path.suffix.lower()\n        except Exception:\n            suf = \"\"\n            path = None\n        if suf in [\".pptx\", \".ppt\", \".pdf\", \".docx\"]:\n            candidates.append((r, suf, path))\n\n    if not candidates:\n        return 0.0, \"No candidate files\"\n\n    # Map file -> which sessions detected\n    file_sessions = {}\n    for r, suf, path in candidates:\n        text = \"\"\n        try:\n            if suf == \".pdf\":\n                text = context.files.read_pdf_text(r.id) or \"\"\n            elif suf == \".docx\":\n                text = context.files.read_docx_text(r.id) or \"\"\n            else:\n                # No reliable PPTX text extraction available; fallback to filename\n                text = (path.name if path else \"\")\n        except Exception:\n            text = (path.name if path else \"\")\n\n        low = str(text).lower()\n        has13 = bool(re.search(r\"session\\s*13\\b|\\b13\\b\", low))\n        has14 = bool(re.search(r\"session\\s*14\\b|\\b14\\b\", low))\n        sessions = set()\n        if has13:\n            sessions.add(13)\n        if has14:\n            sessions.add(14)\n        file_sessions[r.id] = sessions\n\n    # Aggregate\n    files_with_13 = [fid for fid, s in file_sessions.items() if 13 in s]\n    files_with_14 = [fid for fid, s in file_sessions.items() if 14 in s]\n\n    if files_with_13 and files_with_14:\n        # Prefer distinct files\n        distinct = any(fid13 != fid14 for fid13 in files_with_13 for fid14 in files_with_14)\n        if distinct:\n            return W, \"Session 13 and 14 identified in distinct files\"\n        else:\n            return W * 0.7, \"Both sessions detected but appear in the same file\"\n    elif files_with_13 or files_with_14:\n        return W * 0.5, \"Only one session (13 or 14) detected\"\n    else:\n        return 0.0, \"Could not identify session numbers in files or filenames\""}, {"type": "code", "name": "Reasonable File Size (Non-empty Decks)", "description": "Guards against empty/corrupted files by checking file sizes exceed minimal thresholds appropriate to format.", "weight": 0.6, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Checks minimal file sizes for up to two presentation files. Awards 0.3 each when above threshold.\n    \"\"\"\n    W = 0.6\n    try:\n        outputs = context.get_all_outputs() or []\n    except Exception:\n        return 0.0, \"Unable to access outputs\"\n\n    # Gather up to two likely presentation files\n    pres = []\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            suf = p.suffix.lower()\n        except Exception:\n            continue\n        if suf in [\".pptx\", \".ppt\", \".pdf\"]:\n            pres.append((p, suf))\n    pres = pres[:2]\n\n    if not pres:\n        return 0.0, \"No presentation files to size-check\"\n\n    def min_threshold(suf):\n        if suf in [\".pptx\", \".ppt\"]:\n            return 50 * 1024  # 50 KB\n        if suf == \".pdf\":\n            return 20 * 1024  # 20 KB\n        return 10 * 1024\n\n    ok = 0\n    details = []\n    for p, suf in pres:\n        try:\n            size = p.stat().st_size\n        except Exception:\n            size = 0\n        thr = min_threshold(suf)\n        if size >= thr:\n            ok += 1\n            details.append(f\"{p.name}: {size}B >= {thr}B\")\n        else:\n            details.append(f\"{p.name}: {size}B < {thr}B\")\n\n    score = (ok / max(1, len(pres))) * W\n    return score, \"; \".join(details) or \"Size check complete\""}, {"type": "llm_judge", "name": "Session 13 Fidelity to Manual", "description": "Check that the Session 13 deck\u2019s objectives, key points, and activities plausibly align with the Nurturing Parenting Program for Families in Substance Abuse Treatment and Recovery manual.", "weight": 2.6, "judge_prompt": "Evaluate ONLY the Session 13 deck. Using your knowledge of parenting programs for families in substance use treatment/recovery and the Nurturing Parenting Program\u2019s philosophy (nurturing, empathy, nonviolent discipline, parent\u2013child bonding, recovery-informed practice), judge whether the deck plausibly aligns with the official Session 13 content and intent. You do not need to verify exact module titles; check for reasonable fidelity:\n\nEvidence of fidelity might include:\n- Clear learning objectives relevant to nurturing parenting in recovery (e.g., empathy, communication, boundaries, stress/anger management, relapse risk awareness in parenting, safe routines).\n- Key session points that fit the program\u2019s tone and scope (non-judgmental, strengths-based, attachment-focused, nonviolent discipline strategies).\n- At least one concrete practice or skill-building activity (role-play, reflection, home practice) appropriate for parents in recovery.\n- References to reunification context (tracking attendance/completion as applicable) and safety.\n\nScoring:\n- 2.6: Strong fidelity across objectives, key points, and practice; recovery- and trauma-informed; suitable for reunification context.\n- 1.7: Generally aligned but one area thin or generic (e.g., objectives vague OR limited practice detail).\n- 0.9: Partially aligned; multiple gaps or off-scope content.\n- 0.0: Misaligned or session content not discernible.", "expectation": "A Session 13 deck that clearly reflects Nurturing Parenting Program values with concrete, recovery-appropriate concepts and practice."}, {"type": "llm_judge", "name": "Session 14 Fidelity to Manual", "description": "Check that the Session 14 deck\u2019s objectives, key points, and activities plausibly align with the manual and recovery-focused parenting goals.", "weight": 2.6, "judge_prompt": "Evaluate ONLY the Session 14 deck. Using your knowledge of parenting programs for families in substance use treatment/recovery and the Nurturing Parenting Program\u2019s philosophy, judge whether the deck plausibly aligns with the official Session 14 content and intent. You do not need exact module titles; check for reasonable fidelity:\n\nEvidence of fidelity might include:\n- Learning objectives relevant to nurturing parenting and recovery (e.g., problem-solving, family routines/consistency, coping skills, emotional coaching, co-regulation, safe discipline).\n- Key points that reinforce nonviolent, nurturing strategies and attachment/bonding.\n- At least one concrete practice or skill-building activity appropriate for parents in recovery.\n- References to progress toward reunification and safety planning as appropriate.\n\nScoring:\n- 2.6: Strong fidelity across objectives, key points, and practice; recovery- and trauma-informed; supports reunification.\n- 1.7: Generally aligned but one area thin or generic.\n- 0.9: Partially aligned; multiple gaps or off-scope content.\n- 0.0: Misaligned or session content not discernible.", "expectation": "A Session 14 deck that clearly reflects program values with concrete, recovery-appropriate concepts and practice."}, {"type": "llm_judge", "name": "Reunification and Documentation Readiness", "description": "Check if decks support court/agency documentation needs and weekly cadence (e.g., attendance/completion tracking, home practice).", "weight": 0.9, "judge_prompt": "Across both decks, check for elements that support reunification documentation and weekly cadence:\n- Clear session identifiers (13 and 14) and titles on title slides.\n- A spot to track attendance/completion or a statement of completion for each session.\n- A defined home practice/reflection task suitable for documentation.\n- Acknowledgement of weekly cadence and/or 90-minute facilitation plan.\n\nScoring:\n- 0.9: All present across both decks.\n- 0.6: Most present; one minor element missing or implied only.\n- 0.3: Limited evidence; multiple elements missing.\n- 0.0: Not addressed.", "expectation": "Session IDs, completion/attendance cues, and home practice are visible to support reunification documentation."}, {"type": "llm_judge", "name": "Trauma- and Recovery-Informed Appropriateness", "description": "Language and activities should use person-first, non-stigmatizing, culturally sensitive, and safety-conscious framing for families in recovery.", "weight": 0.9, "judge_prompt": "Across both decks, assess whether language and activities are trauma-informed and recovery-informed:\n- Person-first, non-stigmatizing terminology (e.g., \u201cperson in recovery,\u201d avoid labeling).\n- Emphasis on safety planning, triggers, coping supports, and compassionate accountability.\n- Content avoids shaming and promotes strengths, empowerment, and attachment.\n\nScoring:\n- 0.9: Consistently appropriate language and framing across slides.\n- 0.6: Mostly appropriate; minor lapses or missing explicit safety cues.\n- 0.3: Noticeable lapses or occasional stigmatizing phrases.\n- 0.0: Stigmatizing/judgmental tone or unsafe guidance.", "expectation": "Consistently person-first, strengths-based content with explicit safety- and recovery-aware framing."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Holistic Quality Assessment", "description": "Professionalism, accessibility, engagement, and ethical suitability of both decks for a 5-year-old\u2019s parent in recovery, preparing for reunification.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Visual Design and Accessibility", "description": "Assess readability, visual balance, neutral imagery, and accessibility (plain language, slide readability, color contrast as visible).", "weight": 2.0, "judge_prompt": "Assess both decks for overall visual and accessibility quality:\n- Clear hierarchy, readable fonts, sufficient contrast (as visible), concise bullets.\n- Neutral, family-friendly imagery that avoids stigma; visuals support understanding.\n- Avoids dense text walls; consistent, professional styling.\n\nScoring:\n- 2.0: Professional, accessible design; visuals aid comprehension.\n- 1.3: Generally good; minor issues (occasional dense slides or weak contrast).\n- 0.7: Mixed quality; several readability/design issues.\n- 0.0: Poor readability, distracting or inappropriate visuals.", "expectation": "Clean, consistent slide design with neutral imagery and readable text."}, {"type": "llm_judge", "name": "Clarity and Audience Appropriateness", "description": "Language and examples should fit a broad audience (approx. 6th\u20138th grade reading level), with concrete, parent-friendly explanations.", "weight": 2.0, "judge_prompt": "Evaluate clarity and audience fit for a parent in recovery with a 5-year-old child:\n- Plain language, minimal jargon, concrete examples for daily parenting.\n- Actionable steps parents can try at home; clear definitions of key concepts.\n\nScoring:\n- 2.0: Very clear, concrete, parent-friendly language throughout.\n- 1.3: Mostly clear with occasional jargon/vagueness.\n- 0.7: Several unclear sections or abstract language.\n- 0.0: Largely confusing or technical.", "expectation": "Plain, actionable language suitable for parents in recovery."}, {"type": "llm_judge", "name": "Engagement and Facilitation Support", "description": "Check for interactive elements (icebreakers, activities), facilitator notes or cues, timing hints, and smooth flow across slides.", "weight": 2.0, "judge_prompt": "Assess how well the decks support facilitation and engagement:\n- Icebreakers and interactive/practice activities included and feasible.\n- Suggested prompts, facilitator notes/cues, or timing hints present.\n- Logical flow from objectives to content to practice to wrap-up.\n\nScoring:\n- 2.0: Strong engagement activities and clear facilitation cues; excellent flow.\n- 1.3: Some engagement and cues; minor flow gaps.\n- 0.7: Limited engagement or sparse cues; choppy flow.\n- 0.0: Minimal interactivity; unclear flow.", "expectation": "Interactive, facilitator-friendly decks with smooth progression and cues."}, {"type": "llm_judge", "name": "Cultural and Ethical Sensitivity", "description": "Assesses cultural humility, family diversity representation, privacy/confidentiality reminders, and appropriateness for south Florida context.", "weight": 2.0, "judge_prompt": "Evaluate whether the decks demonstrate cultural humility and ethical practice:\n- Inclusive language and imagery reflecting family diversity; privacy/confidentiality reminders where appropriate.\n- Sensitivity to south Florida context (e.g., awareness of diverse cultures/languages; flexible examples).\n- Avoids legal advice; frames court/reunification processes respectfully.\n\nScoring:\n- 2.0: Strong cultural humility and ethical cues throughout.\n- 1.3: Generally sensitive; minor gaps.\n- 0.7: Some inclusivity but several oversights.\n- 0.0: Insensitive or ethically concerning elements.", "expectation": "Respectful, inclusive, and ethically sound presentation style."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "74d6e8b0-f334-4e7e-af55-c095d5d4d1a6", "rubric": {"category_name": "MenoHelp HT Prescribing Guidelines (Virtual Menopause Care)", "rationale": "This rubric enforces a self-documenting, verifiable structure for a clinical guideline document in Word/PDF format, then verifies clinical correctness against contemporary menopause HT standards, and finally assesses professional quality and usability for a virtual care startup context. Stage 1 is a hard gate with LLM-only structural checks. Stage 2 blends small, robust code checks (file-text parsing) with higher-weight LLM clinical verification. Stage 3 evaluates overall quality and operational readiness.", "max_total_score": 32.0, "stages": [{"name": "Stage 1 \u2013 Shape Enforcement Gate (Document Structure and Required Sections)", "description": "LLM-only gate to ensure the output is a DOCX (preferred) or PDF document with the exact structure that enables subsequent verification. Only presence/format is scored here, not clinical accuracy.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.4, "rules": [{"type": "llm_judge", "name": "Document Format and Structural Completeness", "description": "Check that the candidate output is a professionally formatted DOCX (preferred) or PDF document with all required sections and tables to enable verification.", "weight": 8.0, "judge_prompt": "You are evaluating a clinical guideline document for MenoHelp (virtual menopause care). Only check document format and presence of required sections/tables. Do not judge clinical correctness yet.\n\nPASS-CRITICAL FILE FORMAT:\n- Strongly preferred: DOCX (Word). PDF acceptable with minor penalty if structurally complete.\n- Must be a document (not Excel/CSV/code/plain text). At least 6 pages or equivalent length.\n- Professional formatting: page headers/footers or title page, headings hierarchy, and pagination.\n\nREQUIRED STRUCTURE (flexible header names allowed if clearly equivalent):\n1) Title and Versioning\n   - Title includes MenoHelp and HT Prescribing Guidelines (or equivalent)\n   - Version/Date, Authors/Reviewers, intended audience, update cadence\n2) Table of Contents\n3) Purpose and Scope\n4) Evidence Base and Methods (sources consulted: societies, textbooks, reviews)\n5) Patient Eligibility for Virtual HT & Exclusions (triage to virtual vs in-person)\n6) Contraindications and Risk Stratification\n   - Absolute vs Relative, with risk factors list\n7) Clinical Evaluation & Baseline Screening\n   - History/ROS, physical elements feasible virtually, screenings/labs\n8) Treatment Options & Regimens (Systemic, Local, Nonhormonal)\n   - Dosing tables for estrogen and progestogen, forms/routes\n9) Initiation & Titration Algorithm\n   - Flowchart/decision tree or stepwise protocol\n10) Monitoring & Follow-up\n   - Schedule, safety checks, bleeding management\n11) Special Populations/Complex Cases\n   - Prior breast cancer, VTE history, migraines with aura, ASCVD risk, hepatic disease\n12) Telemedicine Operations\n   - Consent, documentation templates, escalation/referral triggers\n13) Patient Education & Informed Consent (template or checklist)\n14) Appendices\n   - Dosing tables, contraindication checklist, adverse effect management\n15) References section\n   - Literature cited in a consistent style; at least ~10 citations\n\nSCORING (return a number 0 to 8):\n- 8.0: DOCX or well-structured PDF; all 15 sections present with clear headers; dosing tables visible; references section present with approx \u226510 citations.\n- 7.0\u20137.9: All core sections present but minor omissions (e.g., missing one appendix item or slightly thin TOC); PDF instead of DOCX allowed here.\n- 6.4\u20136.9: Mostly complete (\u22642 missing sections) and document format is correct; references included but may be short (<10 items) or TOC missing.\n- 4.0\u20136.3: Several missing sections (3\u20135) or weak formatting (no headings/TOC) but still a document with some structure.\n- 0.1\u20133.9: Document exists but lacks most required sections or structure.\n- 0.0: Not a DOCX/PDF document or essentially no structure.\n\nOnly evaluate presence and structure, not clinical correctness.", "expectation": "A DOCX guideline with all sections present, dosing tables, and a references list (~10+)."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness Verification (Clinical Safety, Content Coverage)", "description": "Verify clinical correctness and completeness, focusing on safety-critical HT principles for virtual menopause care. Combines small code checks (text parsing) with higher-weight LLM judgment.", "is_required": false, "max_points": 16.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Anchor Sources and Citation Coverage (Code)", "description": "Checks for presence of key anchor sources and an adequate volume of citations.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = None\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0\n    if not text:\n        return 0.0\n    t = text.lower()\n    # Anchor societies/landmarks\n    anchors = [\n        \"north american menopause society\", \"nams\",\n        \"american college of obstetricians and gynecologists\", \"acog\",\n        \"endocrine society\",\n        \"nice\", \"national institute for health and care excellence\",\n        \"uspstf\",\n        \"women's health initiative\", \"whi\",\n        \"australasian menopause society\"\n    ]\n    found_anchors = len({a for a in anchors if a in t})\n    anchor_score = min(found_anchors / 4, 1.0)  # need ~4 distinct anchors for full credit\n\n    # Approximate citation density\n    bracket_count = len(re.findall(r\"\\[\\d+\\]\", t))\n    year_count = len(re.findall(r\"\\b(201[5-9]|202[0-5])\\b\", t))\n    doi_count = len(re.findall(r\"\\bdoi\\s*:\\s*\", t))\n    approx_refs = max(bracket_count, doi_count, year_count // 2)\n    if approx_refs >= 12:\n        refs_score = 1.0\n    elif approx_refs >= 8:\n        refs_score = 0.8\n    elif approx_refs >= 5:\n        refs_score = 0.5\n    elif approx_refs >= 2:\n        refs_score = 0.3\n    else:\n        refs_score = 0.0\n\n    overall = 0.5 * anchor_score + 0.5 * refs_score\n    return overall * 1.5"}, {"type": "code", "name": "Dosing Specificity and Regimen Structure (Code)", "description": "Checks for presence of specific medications, routes/forms, dosage units, and regimen language (continuous/cyclic/titration).", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = None\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0\n    if not text:\n        return 0.0\n    t = text.lower()\n\n    meds = [\n        \"estradiol\", \"conjugated estrogens\", \"cee\", \"micronized progesterone\",\n        \"progesterone\", \"medroxyprogesterone\", \"mpa\", \"norethindrone\",\n        \"levonorgestrel\", \"dydrogesterone\"\n    ]\n    forms = [\"patch\", \"transdermal\", \"oral\", \"gel\", \"spray\", \"vaginal\", \"cream\", \"ring\", \"tablet\"]\n    regimen_terms = [\"continuous\", \"cyclic\", \"sequential\", \"titrate\", \"titration\", \"stepwise\"]\n\n    found_meds = sum(1 for m in meds if m in t)\n    found_forms = sum(1 for f in forms if f in t)\n    found_reg = sum(1 for r in regimen_terms if r in t)\n\n    # Dosage units\n    dose_matches = re.findall(r\"\\b\\d+(?:\\.\\d+)?\\s?(mg|mcg)\\b\", t)\n    dosage_count = len(dose_matches)\n\n    meds_score = min(found_meds / 5, 1.0)           # need ~5 meds\n    forms_score = min(found_forms / 4, 1.0)         # need ~4 forms\n    if dosage_count >= 8:\n        dosage_score = 1.0\n    elif dosage_count >= 4:\n        dosage_score = 0.6\n    elif dosage_count >= 1:\n        dosage_score = 0.3\n    else:\n        dosage_score = 0.0\n    regimen_score = 1.0 if found_reg >= 1 else 0.0\n\n    overall = 0.4 * meds_score + 0.3 * forms_score + 0.2 * dosage_score + 0.1 * regimen_score\n    return overall * 1.5"}, {"type": "llm_judge", "name": "Clinical Alignment and Safety Fundamentals (LLM)", "description": "Assesses whether core HT safety principles and indications/contraindications align with contemporary, mainstream guidelines (e.g., NAMS/ACOG).", "weight": 6.0, "judge_prompt": "Evaluate the document for alignment with mainstream menopause HT guidance, focusing on safety-critical fundamentals. Consider the following items as reference points (flexible wording acceptable):\n- Progestogen required for endometrial protection in patients with an intact uterus using systemic estrogen; not required with low-dose local (vaginal) estrogen.\n- Route selection: transdermal estradiol preferred in patients with elevated VTE risk, hypertriglyceridemia, gallbladder disease risk, or migraine with aura.\n- Timing: most favorable benefit-risk if started in age <60 or within 10 years since menopause onset; older/remote menopause requires individualized risk assessment.\n- Contraindications: unexplained vaginal bleeding; active or history of estrogen-sensitive breast cancer (systemic HT generally contraindicated; shared decision may be noted for select oncologic contexts); active or history of VTE, stroke, or coronary disease; active liver disease; known pregnancy; known thrombophilia (often relative); poorly controlled hypertension (relative).\n- Management of abnormal uterine bleeding while on HT (evaluation and pause criteria).\n- Recognition that low-dose vaginal estrogen is generally safe with minimal systemic absorption and may be used for GSM when systemic HT is not indicated.\n\nSCORING (0\u20136):\n- 5.5\u20136.0: Accurately covers all bullets with clear, unambiguous guidance and correct nuance/disclaimers.\n- 4.0\u20135.4: Minor gaps but overall safe and aligned with major guidelines.\n- 2.0\u20133.9: Several omissions or ambiguous/unsafe statements.\n- 0.0\u20131.9: Misaligned or unsafe guidance contradicting mainstream standards.\nProvide a brief rationale and cite where in the document you found supporting content (section/page).", "expectation": "Clear, accurate articulation of core safety rules: uterus needs progestogen with systemic estrogen, route/timing principles, solid contraindications, and GSM/local estrogen guidance."}, {"type": "llm_judge", "name": "Evaluation, Screening, Monitoring Protocols (LLM)", "description": "Checks whether intake, baseline assessment, required screenings, labs policy, and follow-up are clinically appropriate and feasible for virtual care.", "weight": 3.5, "judge_prompt": "Assess the thoroughness and correctness of the evaluation and monitoring protocols for a virtual menopause HT service. Look for:\n- Intake history/ROS including vasomotor/GSM symptoms, menstrual history, CVD/VTE/breast cancer risk factors, medication history.\n- Baseline screenings: age-appropriate mammography and cervical cancer screening per guidelines; ASCVD risk assessment; pregnancy considerations; when to request in-person vitals vs self-reported.\n- Labs policy: recognizes that routine labs are not universally required for HT initiation; targeted labs only when indicated (e.g., thyroid if symptoms; lipids/ASCVD risk as needed).\n- Follow-up cadence (e.g., 6\u201312 weeks after initiation/titration, then periodic), including safety checks (BP, VTE symptoms) and management of side effects.\n- Clear criteria for when to pause HT and refer in-person.\n\nSCORING (0\u20133.5):\n- 3.0\u20133.5: Comprehensive, clinically sound, virtual-feasible protocols with clear thresholds/escalation.\n- 2.0\u20132.9: Mostly correct but with modest gaps or unclear thresholds.\n- 0.5\u20131.9: Incomplete or impractical for virtual setting.\n- 0.0\u20130.4: Unsafe or largely missing.\nInclude brief justification and references to locations in the document.", "expectation": "A practical, safe, virtual-appropriate evaluation/monitoring plan with clear escalation triggers."}, {"type": "llm_judge", "name": "Regimens, Dosing, and Algorithms Actionability (LLM)", "description": "Evaluates whether dosing tables and algorithms are actionable, complete, and reflect common regimens with titration guidance.", "weight": 3.5, "judge_prompt": "Review the treatment section for actionable content:\n- Dosing tables with typical starting doses and ranges for oral and transdermal estradiol, conjugated estrogens; local vaginal estrogen products; progestogens (micronized progesterone continuous/cyclic, MPA, etc.).\n- Clear initiation and titration steps for vasomotor symptoms (e.g., start low, reassess in 6\u201312 weeks, titrate based on symptom relief/side effects).\n- Nonhormonal options (SSRIs/SNRIs, gabapentin, oxybutynin) included with typical dosing ranges.\n- Decision tree/flowchart or explicit stepwise algorithm that maps intake \u2192 eligibility/contraindications \u2192 regimen selection \u2192 monitoring \u2192 when to hold/refer.\n\nSCORING (0\u20133.5):\n- 3.0\u20133.5: Complete dosing tables and clear, stepwise/flowchart algorithms; includes nonhormonal options.\n- 2.0\u20132.9: Mostly actionable but minor omissions or unclear steps.\n- 0.5\u20131.9: Sparse dosing detail or vague algorithm.\n- 0.0\u20130.4: Largely missing or unusable.\nProvide brief notes and pointers to sections/pages.", "expectation": "Well-structured tables and algorithms enabling consistent prescribing and titration."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Quality and Operational Readiness", "description": "Holistic assessment of professional presentation, usability for clinicians, and operationalization for a virtual care startup.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Usability", "description": "Clarity and professional formatting, including navigability for busy clinicians.", "weight": 2.0, "judge_prompt": "Evaluate the document\u2019s professional polish and usability:\n- Clear hierarchy of headings, consistent styles, page numbers, and a navigable TOC with page references.\n- Visual aids (tables, flowcharts) are legible and referenced in text.\n- Templates/checklists (e.g., consent, documentation) are clearly delineated and ready to use.\n\nScore 0\u20132:\n- 1.6\u20132.0: Highly professional, easy to navigate and use.\n- 1.0\u20131.5: Generally good; minor issues.\n- 0.1\u20130.9: Noticeable formatting/navigability problems.\n- 0.0: Very poor or chaotic formatting.", "expectation": "A polished, navigable guideline with high clinical usability."}, {"type": "llm_judge", "name": "Audience Fit and Clarity", "description": "Appropriateness for clinicians (NP/PA/MD/DO) in telehealth context; clarity and brevity where needed.", "weight": 2.0, "judge_prompt": "Judge whether the writing is clear, concise, and appropriate for clinicians practicing virtual menopause care:\n- Jargon and abbreviations defined at first use.\n- Stepwise instructions concise yet complete; avoids unnecessary digressions.\n- Includes scope boundaries for low-to-moderate risk virtual care vs referral.\n\nScore 0\u20132:\n- 1.6\u20132.0: Excellent clarity and audience fit.\n- 1.0\u20131.5: Generally clear; a few dense or ambiguous areas.\n- 0.1\u20130.9: Often unclear or mismatched to audience.\n- 0.0: Unusable due to ambiguity.", "expectation": "Clear, clinician-oriented guidance with defined scope boundaries."}, {"type": "llm_judge", "name": "Virtual Care Operationalization", "description": "Strength of operational details enabling safe, compliant virtual delivery.", "weight": 2.0, "judge_prompt": "Assess how well the guideline operationalizes virtual delivery:\n- Informed consent language suitable for telemedicine; documentation templates (smart phrases/checklists).\n- Privacy/security and licensure considerations acknowledged (e.g., HIPAA, state practice/prescribing constraints) at least at a policy note level.\n- Triage/escalation pathways, red flags, and in-person referral logistics are explicit.\n\nScore 0\u20132:\n- 1.6\u20132.0: Strong operational detail enabling immediate implementation.\n- 1.0\u20131.5: Adequate with minor gaps.\n- 0.1\u20130.9: Incomplete or vague operational guidance.\n- 0.0: Missing operational components.", "expectation": "Actionable operational instructions specific to virtual care."}, {"type": "llm_judge", "name": "Governance and Maintenance Plan", "description": "Version control, ownership, and continuous improvement of the guideline.", "weight": 2.0, "judge_prompt": "Check for governance elements:\n- Versioning, authorship, effective date, and next review date.\n- Change log or revision history.\n- Ownership (who maintains) and process for updates (e.g., annual review or upon major guideline updates).\n\nScore 0\u20132:\n- 1.6\u20132.0: Complete governance with clear ownership and review cadence.\n- 1.0\u20131.5: Present but could be more explicit.\n- 0.1\u20130.9: Sparse governance.\n- 0.0: None present.", "expectation": "Clear governance metadata and a sustainable maintenance plan."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "bf68f2ad-eac5-490a-adec-d847eb45bd6f", "rubric": {"category_name": "Manufacturing \u2013 MIG Welding Catch-up Plan (Supervisor)", "rationale": "Task Type: Mixed (Pattern C). The deliverable is a structured Excel plan plus a brief, manager-ready textual summary. Stage 1 uses an LLM gate to strictly enforce a self-documenting Excel shape with clear sections and columns and the presence of a brief summary (either as a separate file or a 'Manager Summary' sheet). Stage 2 mixes lightweight code checks (bounds, arithmetic identities, feasibility against capacity) with LLM verification (policy logic, consequences of ramp-down, and summary\u2013plan alignment). Code weights are kept much lower than LLM weights. Stage 3 provides a holistic quality assessment focusing on professional presentation, operational realism, and managerial communication.", "max_total_score": 15.5, "stages": [{"name": "Stage 1 \u2013 Structure & Format Gate (LLM ONLY)", "description": "Enforce exact output structure enabling verification: an Excel plan starting at Week 4 with specific tables/sections, plus a brief manager-facing summary.", "is_required": true, "max_points": 1.0, "min_score_to_pass": 0.7, "rules": [{"type": "llm_judge", "name": "Structured Output Format Requirement", "description": "Check that the candidate produced a valid Excel catch-up plan with mandatory sheets/sections and a brief summary that can be sent to a manager.", "weight": 1.0, "judge_prompt": "You are validating FORMAT and STRUCTURE only (not correctness). Check the candidate outputs. The expected deliverables are:\n\nA) One Excel spreadsheet (XLSX) with this structure:\n\nRequired Sheets (names can vary slightly; be flexible):\n1) Main plan sheet named like 'Catch-up Plan' or 'Plan' or 'Schedule' starting at Week 4.\n   - A visible table covering continuous weeks from Week 4 onward with at least 6 weeks of rows.\n   - Required columns (names may vary slightly; judge by meaning):\n     \u2022 Week\n     \u2022 Scheduled Demand (hrs)\n     \u2022 Starting Backlog / Past Due at Start (hrs) or Starting Balance\n     \u2022 Planned Regular Hours (hrs)\n     \u2022 Planned Overtime Hours (hrs)\n     \u2022 Total Planned Production Hours (hrs) (or Planned Hours / Total Planned)\n     \u2022 Ending Backlog / Ending Balance (hrs) (or Cumulative Balance End)\n     \u2022 Workweek Days (#)\n     \u2022 Shift Length (hrs) (10-hr shifts typical)\n     \u2022 Notes/Assumptions (optional)\n   - The plan must clearly begin at Week 4.\n\n2) Assumptions & Policy sheet (or similarly named) containing:\n   - Team production rate: 30 standard hours per day\n   - Weekly hour definitions: Regular = 40 hrs/week (4\u00d710), Max = 60 hrs/week (OT > 40)\n   - Buffer target policy (numeric target in hours) and criteria to ramp from 6 days \u2192 5 days \u2192 4 days\n   - A short Methodology/Assumptions text explaining how the plan links demand, capacity, backlog, overtime, and ramp-down triggers\n\n3) Scenario or Consequence illustration (may be its own sheet named like 'Scenarios'/'Ramp-Down Simulation'/'What-If', or a clearly labeled section on the plan sheet):\n   - Must illustrate the consequences of reducing days (6\u21925\u21924) without a corresponding drop in demand (e.g., backlog rebound or longer time to buffer).\n\nB) A brief manager-facing textual summary, no more than a few sentences:\n   - Either provided as a separate file (MD/DOCX/PDF) or included as a 'Manager Summary' section/sheet inside the Excel file.\n   - It should succinctly state the recommended catch-up approach and when to reduce to 5 days and then to 4 days.\n\nScoring (STRUCTURE ONLY):\n- 1.0: Excel present in correct format with all required sheets/sections and required columns; plan starts at Week 4; scenario/consequence content present; brief summary present (separate file or Excel sheet).\n- 0.8: Excel present with main plan and assumptions/policy; most required columns present; brief summary present; scenario/consequence content present OR clearly described in notes; minor omissions (e.g., Shift Length column missing).\n- 0.5: Excel plan exists but missing major required parts (e.g., no assumptions/policy or no week-by-week plan starting at Week 4) OR no brief summary.\n- 0.0: Not an Excel deliverable or lacks the core plan table; cannot verify structure.\n\nOnly assess presence/format. Do NOT judge calculation correctness or quality.", "expectation": "Excel with a Week 4\u2013onward catch-up plan table, an assumptions/policy sheet, a scenario/consequence illustration, and a brief manager summary (file or sheet)."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness & Verification (Mixed)", "description": "Verify arithmetic consistency, capacity bounds, backlog logic, and alignment between plan, policy, and summary.", "is_required": true, "max_points": 7.5, "min_score_to_pass": 3.75, "rules": [{"type": "code", "name": "Arithmetic and Capacity Consistency", "description": "Checks whether Total Planned \u2248 Regular + Overtime, respects capacity limits from Workweek Days \u00d7 30 std hrs/day, and weekly hours \u2264 60.", "weight": 0.75, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        # Find the first spreadsheet output\n        outputs = getattr(context, 'get_all_outputs', lambda: [])() or []\n        spreadsheet = None\n        primary = context.get_primary_output()\n        if primary and getattr(primary, 'is_spreadsheet', False):\n            spreadsheet = primary\n        else:\n            for r in outputs:\n                if getattr(r, 'is_spreadsheet', False):\n                    spreadsheet = r\n                    break\n        if not spreadsheet:\n            return 0.0, 'No spreadsheet found.'\n\n        # Identify plan sheet (prefer names containing plan/catch/schedule/week)\n        xls_path = context.files.get_path(spreadsheet.id)\n        try:\n            xls = pd.ExcelFile(xls_path)\n            sheet_names = [str(s) for s in xls.sheet_names]\n        except Exception:\n            # Fallback single read\n            sheet_names = ['Sheet1']\n        def pick_plan_sheet(names):\n            lower = [n.lower() for n in names]\n            keys = ['plan','catch','schedule','week']\n            for k in keys:\n                for i, n in enumerate(lower):\n                    if k in n:\n                        return names[i]\n            return names[0]\n        sheet = pick_plan_sheet(sheet_names)\n        df = pd.read_excel(xls_path, sheet_name=sheet)\n        if df.empty:\n            return 0.0, 'Plan sheet empty.'\n\n        # Normalize columns\n        cols = [str(c).strip() for c in df.columns]\n        df.columns = cols\n        lower_cols = [c.lower() for c in cols]\n        def find_col(cands):\n            for i, c in enumerate(lower_cols):\n                for cand in cands:\n                    if all(k in c for k in (cand if isinstance(cand, (list, tuple)) else [cand])):\n                        return cols[i]\n            return None\n\n        week_col = find_col([['week']])\n        demand_col = find_col([['demand']])\n        start_backlog_col = find_col([['starting','backlog'], ['starting','balance'], ['past','due'], ['start','backlog'], ['start','balance']])\n        reg_col = find_col([['regular'], ['base','hours']])\n        ot_col = find_col([['overtime'], ['ot']])\n        total_col = find_col([['total','planned'], ['planned','hours'], ['planned','production'], ['total','production']])\n        end_backlog_col = find_col([['ending','backlog'], ['ending','balance'], ['ending','cumulative'], ['end','backlog'], ['end','balance']])\n        days_col = find_col([['workweek','days'], ['days','week'], ['days']])\n        shift_col = find_col([['shift','length'], ['shift','hrs'], ['shift']])\n\n        # Coerce numeric where possible\n        def to_num(s):\n            try:\n                return pd.to_numeric(s, errors='coerce')\n            except Exception:\n                return pd.Series([np.nan]*len(df))\n\n        reg = to_num(df[reg_col]) if reg_col in df else None\n        ot = to_num(df[ot_col]) if ot_col in df else None\n        tot = to_num(df[total_col]) if total_col in df else None\n        days = to_num(df[days_col]) if days_col in df else None\n\n        # If total missing but reg and ot exist, synthesize\n        if tot is None and reg is not None and ot is not None:\n            tot = reg.add(ot, fill_value=0)\n\n        # Subscores\n        n_rows = max(len(df), 1)\n        tol = 0.5  # hours tolerance\n\n        # 1) Total \u2248 Regular + OT\n        sub1 = 1.0\n        if (reg is not None) and (ot is not None) and (tot is not None):\n            diff = (reg.fillna(0) + ot.fillna(0)) - tot.fillna(0)\n            ok = diff.abs() <= tol\n            sub1 = ok.mean() if len(ok) else 0.0\n        elif tot is not None:\n            # No components to verify against; partial credit\n            sub1 = 0.6\n        else:\n            sub1 = 0.0\n\n        # 2) Capacity bound: Planned \u2264 Days * 30 std hrs/day\n        sub2 = 1.0\n        if (tot is not None) and (days is not None):\n            cap = days.fillna(0) * 30.0\n            ok = tot.fillna(0) <= (cap + tol)\n            sub2 = ok.mean() if len(ok) else 0.0\n        elif tot is not None:\n            sub2 = 0.7  # partial if days missing\n        else:\n            sub2 = 0.0\n\n        # 3) Weekly upper limit: Planned \u2264 60 hours/week\n        sub3 = 1.0\n        if tot is not None:\n            ok = tot.fillna(0) <= (60.0 + tol)\n            sub3 = ok.mean() if len(ok) else 0.0\n        else:\n            sub3 = 0.0\n\n        score = float(np.clip((sub1 + sub2 + sub3) / 3.0, 0.0, 1.0))\n        return score, f\"Arithmetic/Capacity subscores: total_vs_components={sub1:.2f}, capacity_bound={sub2:.2f}, weekly_limit={sub3:.2f}\"\n    except Exception as e:\n        return 0.0, f'Error: {e}'"}, {"type": "code", "name": "Backlog Progression and Ramp-Down Timing", "description": "Checks that backlog reaches zero (caught up) within the plan horizon and that day-count reductions (6\u21925\u21924) do not occur before backlog is cleared; also sanity-checks against extreme values.", "weight": 0.75, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        outputs = getattr(context, 'get_all_outputs', lambda: [])() or []\n        spreadsheet = None\n        primary = context.get_primary_output()\n        if primary and getattr(primary, 'is_spreadsheet', False):\n            spreadsheet = primary\n        else:\n            for r in outputs:\n                if getattr(r, 'is_spreadsheet', False):\n                    spreadsheet = r\n                    break\n        if not spreadsheet:\n            return 0.0, 'No spreadsheet found.'\n\n        xls_path = context.files.get_path(spreadsheet.id)\n        try:\n            xls = pd.ExcelFile(xls_path)\n            sheet_names = [str(s) for s in xls.sheet_names]\n        except Exception:\n            sheet_names = ['Sheet1']\n        def pick_plan_sheet(names):\n            lower = [n.lower() for n in names]\n            keys = ['plan','catch','schedule','week']\n            for k in keys:\n                for i, n in enumerate(lower):\n                    if k in n:\n                        return names[i]\n            return names[0]\n        sheet = pick_plan_sheet(sheet_names)\n        df = pd.read_excel(xls_path, sheet_name=sheet)\n        if df.empty:\n            return 0.0, 'Plan sheet empty.'\n\n        cols = [str(c).strip() for c in df.columns]\n        df.columns = cols\n        lower_cols = [c.lower() for c in cols]\n        def find_col(cands):\n            for i, c in enumerate(lower_cols):\n                for cand in cands:\n                    if all(k in c for k in (cand if isinstance(cand, (list, tuple)) else [cand])):\n                        return cols[i]\n            return None\n\n        week_col = find_col([['week']])\n        demand_col = find_col([['demand']])\n        start_backlog_col = find_col([['starting','backlog'], ['starting','balance'], ['past','due'], ['start','backlog'], ['start','balance']])\n        total_col = find_col([['total','planned'], ['planned','hours'], ['planned','production'], ['total','production']])\n        end_backlog_col = find_col([['ending','backlog'], ['ending','balance'], ['ending','cumulative'], ['end','backlog'], ['end','balance']])\n        days_col = find_col([['workweek','days'], ['days','week'], ['days']])\n\n        def to_num(s):\n            try:\n                return pd.to_numeric(s, errors='coerce')\n            except Exception:\n                return pd.Series([np.nan]*len(df))\n\n        week = to_num(df[week_col]) if week_col in df else pd.Series(range(1, len(df)+1))\n        demand = to_num(df[demand_col]) if demand_col in df else None\n        start_b = to_num(df[start_backlog_col]) if start_backlog_col in df else None\n        tot = to_num(df[total_col]) if total_col in df else None\n        end_b = to_num(df[end_backlog_col]) if end_backlog_col in df else None\n        days = to_num(df[days_col]) if days_col in df else None\n\n        # Try to compute ending backlog if missing\n        if end_b is None and (start_b is not None) and (demand is not None) and (tot is not None):\n            end_b = start_b.add(demand, fill_value=0).sub(tot, fill_value=0)\n\n        subs = []\n        msgs = []\n\n        # A) Backlog reaches zero (caught up) check\n        subA = 0.0\n        if end_b is not None:\n            min_end = float(end_b.min(skipna=True)) if len(end_b) else np.inf\n            if np.isfinite(min_end):\n                subA = 1.0 if (min_end <= 0.0) else 0.0\n                msgs.append(f\"min_ending_backlog={min_end:.2f}\")\n        elif start_b is not None and demand is not None and tot is not None:\n            # already covered; else partial\n            subA = 0.2\n        subs.append(subA)\n\n        # B) Do not reduce days before backlog cleared\n        subB = 1.0\n        if (days is not None) and (end_b is not None):\n            # First week where days <=5 (i.e., reduction from 6)\n            try:\n                # heuristics: assume initial days ~6 if present\n                first_reduce_idx = None\n                for i in range(len(days)):\n                    d = days.iloc[i]\n                    if pd.notna(d) and d <= 5:\n                        first_reduce_idx = i\n                        break\n                zero_idx = None\n                for i in range(len(end_b)):\n                    eb = end_b.iloc[i]\n                    if pd.notna(eb) and eb <= 0:\n                        zero_idx = i\n                        break\n                if first_reduce_idx is not None and (zero_idx is None or first_reduce_idx < zero_idx):\n                    subB = 0.2  # penalize early reduction\n            except Exception:\n                subB = 0.7\n        elif days is None:\n            subB = 0.6  # partial if day data missing\n        subs.append(subB)\n\n        # C) Weekly hours sanity (\u226460) reinforcing check if total available\n        subC = 1.0\n        if tot is not None:\n            ok = tot.fillna(0) <= 60.5\n            subC = ok.mean() if len(ok) else 0.0\n        subs.append(subC)\n\n        score = float(np.clip(np.nanmean(subs), 0.0, 1.0))\n        feedback = f\"Backlog zero check={subA:.2f}; Ramp-down timing={subB:.2f}; Weekly hours sanity={subC:.2f}. \" + (\"; \".join(msgs) if msgs else \"\")\n        return score, feedback\n    except Exception as e:\n        return 0.0, f'Error: {e}'"}, {"type": "llm_judge", "name": "Policy Alignment and Criteria Presence", "description": "Assesses whether the plan explicitly states buffer targets and clear criteria for reducing from 6\u21925\u21924 days, consistent with constraints (30 std hrs/day, 40 regular, up to 60 max, OT > 40).", "weight": 2.0, "judge_prompt": "Review the Excel plan (including the Assumptions/Policy sheet or section). Verify:\n- It explicitly states a numeric buffer target (in hours).\n- It defines clear criteria for when to move from 6 days to 5 days and from 5 days to 4 days (return to regular time), consistent with being caught up (no past due) and maintaining the buffer.\n- It acknowledges the constraints: team produces 30 standard hours/day, regular = 40 hrs/week (4\u00d710), overtime is >40, max availability = 60 hrs/week.\n\nScoring:\n- 2.0: Clear numeric buffer target and explicit, feasible criteria for both 6\u21925 and 5\u21924 transitions; constraints are correctly recognized and referenced.\n- 1.0: Criteria present but incomplete or buffer target not numeric/unclear; minor inconsistencies with constraints.\n- 0.0: Criteria missing or contradictory to constraints.", "expectation": "A precise buffer target with explicit ramp-down criteria aligned to the given operating constraints."}, {"type": "llm_judge", "name": "Consequences of Reducing Days Without Demand Drop", "description": "Checks that the plan illustrates consequences (e.g., backlog rebound) when reducing days prematurely or without demand reduction.", "weight": 2.0, "judge_prompt": "Evaluate whether the Excel plan includes a scenario, what-if, or clearly labeled section demonstrating the consequences of reducing days (e.g., from 6 to 5 or 4) without a corresponding drop in demand.\n\nLook for a scenario sheet/section or annotations that show how backlog/buffer changes under earlier reductions.\n\nScoring:\n- 2.0: Includes a clear scenario/what-if analysis or explicit section that quantitatively shows backlog or buffer outcomes under reduced days without demand drop.\n- 1.0: Mentions consequences qualitatively but lacks clear quantitative illustration.\n- 0.0: No illustration or discussion of consequences.", "expectation": "Quantified illustration (table/section) of how premature day-count reduction affects backlog/buffer."}, {"type": "llm_judge", "name": "Summary\u2013Plan Alignment", "description": "Validates that the brief textual summary matches the Excel plan\u2019s recommendations and timing.", "weight": 2.0, "judge_prompt": "Review the brief summary (as a separate file or in a 'Manager Summary' sheet) and the Excel plan. Check that the summary:\n- States the recommended catch-up approach at a high level.\n- Indicates roughly when the team can reduce to 5 days and later to 4 days, consistent with the plan\u2019s timeline and buffer logic.\n- Is concise (a few sentences) and not contradictory to the Excel schedule.\n\nScoring:\n- 2.0: Concise, accurate, and consistent with the plan\u2019s timeline and buffer thresholds.\n- 1.0: Mostly consistent with minor discrepancies or vague timing.\n- 0.0: Inconsistent or missing.", "expectation": "A few-sentence summary accurately reflecting the plan\u2019s ramp-down sequence and timing."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Holistic Quality Assessment", "description": "Professionalism, clarity, feasibility, and managerial usefulness of the deliverables.", "is_required": false, "max_points": 7.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Excel Communication and Readability", "description": "Assesses formatting, clear labeling, units (hours), legible tables, and a professional look-and-feel.", "weight": 2.0, "judge_prompt": "Judge the Excel for professional presentation:\n- Clear headers, units in hours, and consistent formatting\n- Readable columns without truncation or hidden critical content\n- Appropriate use of highlighting or notes to emphasize ramp-down weeks and buffer achievement\n\nScoring:\n- 2.0: Highly professional, clear, and easy to interpret.\n- 1.0: Adequate but some clutter, inconsistencies, or hard-to-read elements.\n- 0.0: Poorly formatted or confusing.", "expectation": "Well-formatted, clearly labeled Excel with readable tables and units."}, {"type": "llm_judge", "name": "Feasibility and Risk Management", "description": "Evaluates whether the plan is operationally realistic and minimizes sustained overtime while meeting backlog goals.", "weight": 2.0, "judge_prompt": "Assess the operational realism of the plan:\n- Uses overtime strategically to catch up, then reduces sustained overtime\n- Achieves backlog clearance and builds a buffer within a reasonable number of weeks\n- Identifies risks (e.g., demand volatility) and uses policies to mitigate (e.g., buffer before ramp-down)\n\nScoring:\n- 2.0: Realistic timeline, prudent overtime, and clear risk-mitigating policies.\n- 1.0: Generally feasible but missing some risk considerations or overly aggressive/slow.\n- 0.0: Unrealistic or ignores key constraints.", "expectation": "Pragmatic path to caught-up status with a reasonable buffer and reduced overtime."}, {"type": "llm_judge", "name": "Operational Practicality and Workforce Considerations", "description": "Considers practicality of shifts/days, avoiding burnout, respecting constraints, and smooth transitions.", "weight": 1.5, "judge_prompt": "Evaluate how the plan handles workforce practicality:\n- Smooth transitions when moving 6\u21925\u21924 days\n- Respects 10-hour shifts and max 60 hours/week; avoids abrupt changes or excessive OT\n- Notes any worker impact and mitigations (rest, scheduling stability)\n\nScoring:\n- 1.5: Thoughtful, practical, and considerate of workforce impacts.\n- 0.8: Moderately practical with minor concerns.\n- 0.0: Impractical or ignores workforce considerations.", "expectation": "A practical schedule that respects constraints and avoids excessive fatigue."}, {"type": "llm_judge", "name": "Manager-Facing Summary Quality", "description": "Clarity, brevity, and actionability of the few-sentence summary for a manager audience.", "weight": 1.5, "judge_prompt": "Judge the brief summary:\n- 2\u20134 concise sentences, clear recommendation, timing for ramp-downs, and confidence level\n- Actionable tone suitable for emailing a manager; references the attached Excel\n\nScoring:\n- 1.5: Clear, crisp, managerial, and actionable.\n- 0.8: Understandable but somewhat vague or wordy.\n- 0.0: Unclear, too long, or missing.", "expectation": "A succinct, manager-appropriate summary that references the attached plan and key timing."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "cebf301e-5ea7-41ae-b117-ad8f43e7ac22", "rubric": {"category_name": "Customer Portal Design Document (CleanTech CTO)", "rationale": "Three-stage, self-documenting rubric tailored for a CTO-level design doc. Stage 1 is a hard gate that enforces DOCX format and a verifiable section layout enabling automated checks. Stage 2 mixes lightweight code rules (keyword/structural presence) with higher-weight LLM rules (cross-referencing requirements, architecture, and delivery correctness). Stage 3 assesses professional quality, feasibility under a six-week constraint, and actionability for a senior team.", "max_total_score": 22.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate", "description": "Hard gate ensuring the output is a DOCX design document with required structure and length so that verification is possible.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Document Format Gate (DOCX + Length)", "description": "Verify the candidate output is a Word document (DOCX), professionally formatted, and approximately 2\u20133 pages.", "weight": 1.0, "judge_prompt": "You are validating the SHAPE ONLY of the submitted output.\n\nCheck the following:\n1) File format is a Word document (DOCX). Do NOT accept PDF, Markdown, or plain text.\n2) Document length: approximately 2\u20133 pages (roughly 600\u20131200 words). Be flexible with page count if formatting affects pagination, but the content should be within this range.\n3) Professional document structure (headings, sections, bullet lists or tables).\n\nScoring (0 to 1.0):\n- 1.0: Is DOCX AND approx. 2\u20133 pages AND professionally structured.\n- 0.7: Is DOCX AND approx. 2\u20133 pages but structure is minimal OR slightly under/over length but still close (500\u20131400 words).\n- 0.4: Is DOCX but far from length expectation OR structure is very weak.\n- 0.0: Not a DOCX file.\n\nOnly check presence/format/length shape, not content quality or correctness.", "expectation": "A DOCX file of about 2\u20133 pages with visible headings and basic professional formatting."}, {"type": "llm_judge", "name": "Required Sections Gate (Structure Presence)", "description": "Verify presence of required section headers and basic structural elements enabling verification in later stages.", "weight": 3.0, "judge_prompt": "You are validating STRUCTURE ONLY (not content correctness). The document must be a CTO-level design for a Customer Portal. Check for clearly labeled sections (be flexible with exact wording). Accept close synonyms.\n\nRequired sections (aim for at least 8 of 10):\n1) Purpose & Goals (e.g., \"Purpose\", \"Goals\", \"Objectives\", \"Overview\")\n2) Scope & High-Level Functional Requirements (e.g., \"Scope\", \"Functional Requirements\", \"MVP Scope\")\n3) Architecture & Integration (e.g., \"Architecture\", \"System Design\", \"Integration Points\")\n4) Authentication & Security (e.g., \"Auth\", \"Security\", \"TOTP\", \"Access Control\")\n5) Data & Storage (e.g., \"Data Model\", \"PostgreSQL\", \"Object Storage/S3\")\n6) PDF Export (explicit mention of in-browser export)\n7) Mobile & Responsiveness (e.g., \"Responsive Design\", \"Mobile Support\")\n8) CI/CD & Infrastructure as Code (e.g., \"GitHub Actions\", \"IaC\", \"Terraform\")\n9) Framework & Repo Strategy (e.g., \"React Framework Recommendation\", \"Monorepo vs New Repo\")\n10) Roadmap/Milestones (for six-week launch) AND Risks & Open Questions (both should appear; can be separate or combined sections)\n\nAlso check for at least one of:\n- A bullet list (e.g., risks or requirements)\n- A small table (e.g., decision matrix or requirement table)\n\nScoring (0 to 3.0):\n- 3.0: 8\u201310 required items present with clear headers AND at least one bullet list or table.\n- 2.2: 7 required items present AND has bullets or a table.\n- 1.4: 5\u20136 required items present; bullets/tables optional.\n- 0.6: 3\u20134 required items present.\n- 0.0: Fewer than 3 required items present, or structure not discernible.\n\nOnly check structural presence, not correctness or depth.", "expectation": "A clearly sectioned document including the listed areas, plus at least one bullet list or small table."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness Verification", "description": "Verify the document\u2019s decisions align with explicit requirements and are internally consistent. Combines deterministic code checks with higher-weight LLM judgment.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Core Requirements Mentioned", "description": "Check if critical requirement themes are explicitly mentioned: TOTP, in-browser PDF export, strict access control, object storage (S3), PostgreSQL, responsive/mobile support, session tokens/cookies with expiration.", "weight": 0.7, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception as e:\n        return 0.0, f\"Failed to read DOCX: {e}\"\n    t = text.lower()\n\n    checks = {\n        'totp': any(k in t for k in [\"totp\", \"time-based one-time\", \"one-time password\"]),\n        'pdf_export': any(k in t for k in [\"in-browser pdf\", \"client-side pdf\", \"pdf export\", \"html-to-pdf\", \"print to pdf\"]),\n        'access_control': any(k in t for k in [\"access control\", \"authorization\", \"each customer sees only\", \"row-level\", \"rbac\", \"scope per customer\", \"multi-tenant isolation\"]),\n        'object_storage': any(k in t for k in [\"s3\", \"object storage\", \"blob storage\"]),\n        'postgres': any(k in t for k in [\"postgres\", \"postgresql\"]),\n        'responsive': any(k in t for k in [\"responsive\", \"mobile\", \"breakpoints\", \"mobile-first\"]),\n        'session_expiry': any(k in t for k in [\"session\", \"cookie\", \"jwt\", \"token\"]) and any(k in t for k in [\"expire\", \"expiration\", \"ttl\", \"duration\"]),\n    }\n\n    score = sum(1 for v in checks.values() if v) / len(checks)\n    missing = [k for k, v in checks.items() if not v]\n    return score, f\"Missing: {', '.join(missing)}\" if missing else \"OK\"\n"}, {"type": "code", "name": "Platform & Pipeline Mentions", "description": "Check mentions of Node.js/TypeScript API, containers or serverless, GitHub Actions CI/CD, and Infrastructure-as-Code.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        t = context.files.read_docx_text(output.id).lower()\n    except Exception:\n        return 0.0\n\n    node_ts = (\"node\" in t or \"node.js\" in t) and (\"typescript\" in t or \"ts\" in t)\n    deploy = any(k in t for k in [\"container\", \"docker\", \"kubernetes\", \"serverless\", \"lambda\", \"cloud run\", \"functions\"])\n    gha = any(k in t for k in [\"github actions\", \"gh actions\", \"ci/cd\", \"pipeline\"])\n    iac = any(k in t for k in [\"terraform\", \"pulumi\", \"cdk\", \"cloudformation\", \"infrastructure as code\", \"iac\"])\n\n    checks = [node_ts, deploy, gha, iac]\n    score = sum(1 for c in checks if c) / len(checks)\n    return score\n"}, {"type": "code", "name": "Integration & Repo Strategy Mentions", "description": "Check for integration with existing sales system and repository/framework recommendations.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        t = context.files.read_docx_text(output.id).lower()\n    except Exception:\n        return 0.0\n\n    integration = (any(k in t for k in [\"integration\", \"integrate\", \"integration points\"]) and any(k in t for k in [\"api\", \"rest\", \"express\", \"sales admin\"])) or any(k in t for k in [\"webhook\", \"event\", \"sync\"]) \n    repo = any(k in t for k in [\"monorepo\", \"polyrepo\", \"separate repo\", \"new repo\", \"repo strategy\", \"nx\", \"turborepo\", \"pnpm workspace\", \"yarn workspaces\"])\n    framework = any(k in t for k in [\"next.js\", \"nextjs\", \"remix\", \"vite\", \"create react app\", \"cra\"])\n\n    checks = [integration, repo, framework]\n    score = sum(1 for c in checks if c) / len(checks)\n    return score\n"}, {"type": "llm_judge", "name": "Requirements Alignment and Completeness", "description": "Verify the document explicitly addresses all mandatory requirements with concrete decisions and acceptance criteria where appropriate.", "weight": 3.2, "judge_prompt": "Evaluate how well the document covers the mandatory requirements with concrete choices. Look for explicit statements about: React standalone portal; TOTP for initial auth; deferral of social login; strict per-customer access control; proposal metadata in PostgreSQL and assets in object storage (e.g., S3); in-browser PDF export; responsive/mobile support; session tokens/cookies with reasonable expiration; scalability considerations; GitHub Actions for CI/CD; repo strategy recommendation; React framework recommendation; integration points with existing sales system.\n\nScoring (0 to 3.2):\n- 3.2: All items addressed explicitly with clear decisions and basic acceptance criteria or testability hints.\n- 2.4: Most items (\u226511 of 13) addressed; a few are implied but not explicit.\n- 1.6: About half addressed; several missing or ambiguous.\n- 0.8: Few addressed; mostly vague.\n- 0.0: Largely ignores the stated requirements.\n\nFocus on coverage and explicitness, not prose quality.", "expectation": "A crisp mapping from each requirement to a concrete decision (e.g., framework, auth method, storage choices) with minimal ambiguity."}, {"type": "llm_judge", "name": "Architecture & Integration Correctness", "description": "Check whether the proposed architecture and integration points are coherent and feasible with the given stack and constraints.", "weight": 2.5, "judge_prompt": "Assess if the architecture is coherent and feasible:\n- React portal front end; Node.js/TypeScript API (Express or similar) via containers or serverless.\n- PostgreSQL for proposal metadata; S3/object storage for assets.\n- TOTP-based auth flow at launch; strict authorization boundaries to ensure a user only sees their own proposals.\n- In-browser PDF export (client-side technique referenced, not purely server-side).\n- Clear integration points with existing sales admin REST API (e.g., endpoints, data flow, webhooks, or shared services) and data synchronization considerations.\n\nScoring (0 to 2.5):\n- 2.5: Architecture cleanly aligns to stack, includes specific integration mechanics and authorization boundary details.\n- 1.7: Mostly correct; integration or auth boundary details are light but present.\n- 0.9: Some correct elements but important gaps or contradictions.\n- 0.0: Incoherent or contradicts key constraints (e.g., no TOTP at launch, server-only PDF export with no client mention, etc.).", "expectation": "A diagram-level narrative tying React, API, DB, object storage, and existing REST API together with auth/authorization boundaries."}, {"type": "llm_judge", "name": "Delivery & DevOps Correctness", "description": "Check CI/CD, IaC, and environment/deployment details sufficient for a real team to implement quickly.", "weight": 2.3, "judge_prompt": "Evaluate delivery/operations details:\n- GitHub Actions workflows for build, test, lint, and deploy.\n- Infrastructure-as-code (e.g., Terraform/CDK) with environment separation (dev/stage/prod) and secrets management.\n- Deployment target (container platform or serverless) with rationale tied to six-week constraint.\n- Automated testing coverage at least smoke/E2E for MVP.\n\nScoring (0 to 2.3):\n- 2.3: Solid, actionable CI/CD and IaC details with environment/risk considerations.\n- 1.6: Adequate but missing some environment or testing clarity.\n- 0.8: Minimal signals; unclear automation.\n- 0.0: Lacks CI/CD or IaC considerations.", "expectation": "A pragmatic CI/CD plan using GitHub Actions, plus IaC with environments and basic testing to support a six-week MVP."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professional Assessment", "description": "Holistic evaluation of clarity, prioritization for a six-week MVP, professional tone, and strategic foresight.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Actionability for Senior Engineering Team", "description": "Is the document concise, explicit on decisions, and easily translatable into tickets?", "weight": 2.0, "judge_prompt": "Judge how actionable the doc is for a senior team ready to execute. Look for explicit decisions, tight scoping, and clear breakdowns that map naturally to tickets (auth, data, integration, CI/CD, PDF export, mobile/responsive, etc.). Avoid verbose tutorials.\n\nScoring (0 to 2.0):\n- 2.0: Highly actionable and concise; decisions and immediate next steps are obvious.\n- 1.3: Generally actionable; a few ambiguities remain.\n- 0.7: Some direction, but several decisions unclear.\n- 0.0: Not actionable for implementation.", "expectation": "A crisp, decision-oriented doc that a senior team can break into tickets with minimal clarification."}, {"type": "llm_judge", "name": "Feasibility within Six Weeks & Prioritization", "description": "Does the doc present a realistic MVP scope, phased roadmap, and trade-offs to hit the six-week launch?", "weight": 2.0, "judge_prompt": "Evaluate whether the plan is realistic for a six-week MVP. Look for: prioritized scope, explicit cutlines, dependencies, and a brief milestone/timeline. The plan should balance ambition with feasibility.\n\nScoring (0 to 2.0):\n- 2.0: Realistic, phased MVP with clear priorities and cutlines.\n- 1.3: Mostly feasible; prioritization present but could be tighter.\n- 0.7: Vague feasibility; risky scope without clear cutlines.\n- 0.0: Unreasonable or no prioritization.", "expectation": "A tight MVP, with descoped later items (e.g., social logins) and a simple timeline."}, {"type": "llm_judge", "name": "Risks and Open Questions Quality", "description": "Assess whether key risks and open questions are surfaced and useful for de-risking execution.", "weight": 2.0, "judge_prompt": "Assess the quality of risks and open questions. Look for specific items tied to auth, security, integration with the existing sales system, data privacy/PII, PDF export reliability, mobile experience, and scaling. Prefer prioritized lists with mitigation ideas.\n\nScoring (0 to 2.0):\n- 2.0: Specific, prioritized risks with mitigations and clear open questions.\n- 1.3: Reasonable list but missing prioritization or mitigations.\n- 0.7: Generic or superficial.\n- 0.0: Absent or not useful.", "expectation": "A focused list that anticipates real delivery risks and clarifies unknowns."}, {"type": "llm_judge", "name": "Professional Presentation and Clarity", "description": "Evaluate clarity, organization, and professionalism appropriate for executive/engineering stakeholders.", "weight": 2.0, "judge_prompt": "Evaluate overall presentation: clarity, organization, consistent terminology, effective use of headings, bullets, and any small tables. The tone should be professional and concise.\n\nScoring (0 to 2.0):\n- 2.0: Clear, well-structured, professional throughout.\n- 1.3: Generally clear with minor issues.\n- 0.7: Some structural/clarity issues.\n- 0.0: Poorly presented or confusing.", "expectation": "A polished internal design doc suitable for executives and senior engineers."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "ab81b076-e5d8-473a-9bdb-7ea7c38f6ebc", "rubric": {"category_name": "Automotive Parts Check-In SOP (Wholesale Dealers)", "rationale": "This rubric enforces a self-documenting, verifiable PDF SOP for dealership parts check-in. Stage 1 uses an LLM-only gate to require a precise document structure that enables verification. Stage 2 mixes lightweight code checks (keywords, structure cues) with LLM judges for procedural correctness and cross-referenced logic. Stage 3 assesses professional quality, clarity, and practical usefulness for dealership staff.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate", "description": "LLM-only gate verifying the deliverable is a 1\u20133 page PDF with the exact section structure needed to later verify correctness. If this gate fails, the category score is 0.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Document Format and Structure Requirements (Automotive Parts Check-In SOP)", "description": "Verify the output is a properly structured PDF SOP with all required sections and visual guidance placeholders for verification. Only check structure/presence, not quality or correctness.", "weight": 4.0, "judge_prompt": "You are evaluating whether the candidate output satisfies the required STRUCTURE for a dealership Parts Check-In SOP. Only check presence/format, NOT content quality or calculation correctness.\n\nFormat requirements:\n- Must be a PDF document (not DOCX/Word, not plain text, not Excel)\n- Length: 1\u20133 pages total\n- Professionally formatted with clear section headers and readable layout\n\nRequired sections and elements (be flexible with exact names):\n1) Title block and metadata on page 1:\n   - Document title indicating Parts Check-In SOP or Procedure\n   - Version/date and dealership applicability (e.g., \u201cAll Dealers\u201d or similar)\n2) Overview / Purpose & Scope\n3) Roles & Responsibilities (who performs which steps)\n4) Stock Orders \u2014 Step-by-Step Procedure:\n   - Numbered steps from delivery truck arrival to system confirmation/posting\n   - Includes put-away/binning step and inventory update\n5) Critical Orders (e.g., VOR/Emergency) \u2014 Step-by-Step Procedure:\n   - Numbered steps from delivery to confirmation, distinct from stock flow\n   - Priority handling called out (e.g., expedite, immediate confirmation)\n6) Handling Common Issues section with subitems for ALL of the following:\n   - Damaged parts\n   - Missing items/shortages\n   - Bill of Lading (BOL) discrepancies\n   Each subitem must include brief steps or a checklist.\n7) Communication with Manufacturer\u2019s Parts Distribution Center (PDC):\n   - Who to contact, how to contact, what info to include (e.g., order no., part no., photos)\n8) Visual guidance for documenting damage:\n   - At least one figure/photo/diagram OR a clearly labeled placeholder (e.g., \u201cPhoto: Damaged packaging with circled dent\u201d) with caption or annotations describing what to show\n9) Final Confirmation & System Update section:\n   - Explicitly states confirming receipt in DMS/parts system and handling of adjustments\n\nOptional but encouraged:\n- A simple template or checklist table (e.g., Check-in Log) with fields like Part Number, Qty Ordered/Received, Condition, Discrepancy Notes, Action, Initials/Date.\n\nScoring (STRUCTURE only):\n- 4.0: PDF, 1\u20133 pages, and all 9 required elements present (Sections 1\u20139). Optional template not required.\n- 3.5: PDF, 1\u20133 pages, missing only ONE minor element among Sections 2, 3, 8, or 9.\n- 3.0: PDF, 1\u20133 pages, missing ONE core procedural section among Sections 4\u20137 OR visual guidance is missing.\n- 2.0: PDF but 1\u20133 core sections missing, or length outside 1\u20133 pages while otherwise structured.\n- 1.0: PDF but only a few sections present (\u22643) or structure very unclear.\n- 0.0: Not a PDF OR no recognizable required structure.\n\nOnly check PRESENCE and FORMAT. Do not judge content quality, correctness, or dealership-specific accuracy.", "expectation": "A 1\u20133 page PDF SOP with clearly labeled sections for stock and critical orders, issues handling, PDC communication, and at least one visual or labeled placeholder."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Verification", "description": "Verify procedural correctness, distinctions between order types, issue-handling workflows, and traceability using mixed code checks and LLM judges. Stage is scored but not a hard gate.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Mentions and distinguishes Stock vs Critical Orders", "description": "Checks that PDF text references both stock and critical orders in proximity to the word 'order'.", "weight": 0.25, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = ''\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = ''\n    if not text:\n        return 0.0\n    t = text.lower()\n    stock = re.search(r'(stock[^\\n\\r]{0,60}order|order[^\\n\\r]{0,60}stock)', t) is not None\n    critical = re.search(r'(critical[^\\n\\r]{0,60}order|order[^\\n\\r]{0,60}critical|vor|emergency order)', t) is not None\n    return 0.25 if (stock and critical) else 0.0"}, {"type": "code", "name": "Step-by-step structure present", "description": "Detects presence of numbered steps (e.g., '1.' or 'Step 1'). Rewards if at least 6 steps across document.", "weight": 0.25, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = ''\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = ''\n    if not text:\n        return 0.0\n    t = text.lower()\n    # Count common step patterns\n    patterns = [r'^\\s*\\d+\\.', r'^\\s*step\\s*\\d+\\b']\n    count = 0\n    for line in t.splitlines():\n        if any(re.search(p, line) for p in patterns):\n            count += 1\n    # Partial credit up to 6\n    score = min(count / 6.0, 1.0) * 0.25\n    return score"}, {"type": "code", "name": "Issues coverage keywords (damaged, missing, BOL)", "description": "Checks presence of handling for damaged parts, missing items, and Bill of Lading discrepancies.", "weight": 0.25, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = ''\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = ''\n    if not text:\n        return 0.0\n    t = text.lower()\n    hits = 0\n    if re.search(r'damag', t):\n        hits += 1\n    if re.search(r'\\bmissing\\b|shortage', t):\n        hits += 1\n    if re.search(r'\\bbill of lading\\b|\\bbol\\b', t):\n        hits += 1\n    return (hits / 3.0) * 0.25"}, {"type": "code", "name": "Visual guidance cues present", "description": "Rewards mention of visuals/annotations (figure/image/photo/diagram/screenshot/caption/annotated).", "weight": 0.25, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = ''\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = ''\n    if not text:\n        return 0.0\n    t = text.lower()\n    if re.search(r'figure|image|photo|diagram|screenshot|caption|annotat', t):\n        return 0.25\n    return 0.0"}, {"type": "llm_judge", "name": "End-to-end procedural completeness for both order types", "description": "Checks whether the SOP provides complete, logically ordered steps from delivery arrival to system confirmation for both stock and critical orders, including put-away/binning and inventory posting.", "weight": 3.0, "judge_prompt": "Evaluate the PDF\u2019s procedural completeness for BOTH Stock Orders and Critical Orders. Consider only correctness and completeness, not formatting.\n\nRequirements for FULL credit:\n- For Stock Orders: A clear, numbered sequence from delivery receipt \u2192 verification against packing list/BOL \u2192 quantity and condition check \u2192 discrepancy handling path \u2192 binning/put-away \u2192 inventory update/posting in DMS \u2192 confirmation/close-out.\n- For Critical Orders (e.g., VOR/Emergency): Similar sequence with explicit priority handling (expedite, immediate staging/hand-off to service/advisor) and prompt system confirmation.\n- Logical order (no missing critical steps) and unambiguous transitions between steps.\n\nScoring guide:\n- 3.0: Both flows complete and logically ordered end-to-end.\n- 2.0: One minor step missing or unclear in one flow.\n- 1.0: Significant gaps in one flow or both flows partially complete.\n- 0.0: Only one flow present or major gaps in both.", "expectation": "Two complete, clearly ordered procedures covering delivery through confirmation for stock and critical orders."}, {"type": "llm_judge", "name": "Clear distinctions between Stock vs Critical handling", "description": "Assesses whether distinctions are explicit and operationally meaningful (e.g., prioritization, staffing, staging, time targets).", "weight": 2.0, "judge_prompt": "Assess whether the SOP explicitly distinguishes Stock Orders vs Critical Orders in a way that affects operations. Look for priority handling (e.g., expedite, immediate staging), staffing/role differences, time targets, and any exceptions in checks/put-away.\n\nScoring:\n- 2.0: Distinctions are explicit, practical, and appear in steps (not just in a note).\n- 1.0: Distinctions mentioned but vague or not operationalized.\n- 0.0: No meaningful distinction beyond labels.", "expectation": "Operationally meaningful, explicit differences in handling stock vs critical orders."}, {"type": "llm_judge", "name": "Issue handling workflows and PDC communication", "description": "Evaluates clarity of damaged, missing, and BOL discrepancy handling, including documentation (photos/tags), quarantine, and communication to PDC with required info and timelines.", "weight": 2.0, "judge_prompt": "Check whether the SOP provides actionable workflows for: (a) damaged parts, (b) missing items/shortages, (c) BOL/packing list discrepancies. Look for:\n- Documentation steps (photos with annotations, damage tags/labels, discrepancy notes)\n- Quarantine/segregation procedures for damaged/incorrect items\n- Initiating PDC communication with specific info (order number, part numbers, quantities, photos, deadline/SLAs, ticket/case IDs)\n\nScoring:\n- 2.0: All three issues have clear steps, documentation, quarantine, and PDC comms with specifics.\n- 1.0: Partially covered (1\u20132 issues strong; others vague or missing details).\n- 0.0: Minimal or generic mention without clear steps.", "expectation": "Each issue type has concrete steps, clear documentation guidance, and defined PDC communication details."}, {"type": "llm_judge", "name": "Traceability and recordkeeping adequacy", "description": "Checks whether the SOP ensures traceability via a log/checklist or equivalent DMS recording directions with key fields.", "weight": 2.0, "judge_prompt": "Evaluate whether the SOP provides sufficient traceability and recordkeeping. Look for either:\n- A check-in log template or checklist (table or bullet list) with fields such as: Order #, BOL #, Part #, Description, Qty Ordered, Qty Received, Condition, Discrepancy Type, Action Taken, Photos/Refs, User Initials, Date/Time; OR\n- Clear instructions on recording equivalent data in the DMS with the same key fields.\n\nScoring:\n- 2.0: Robust template or explicit DMS recording instructions with most key fields.\n- 1.0: Basic checklist or partial fields; traceability possible but limited.\n- 0.0: No meaningful traceability guidance.", "expectation": "A practical log template or clear DMS recording steps that capture essential identifiers and actions."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Practicality Assessment", "description": "Holistic LLM assessment of clarity, usability, professionalism, and visual effectiveness for dealership parts staff.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity, organization, and readability", "description": "Assesses whether the SOP is easy to follow with clear headings, numbering, and layout appropriate for 1\u20133 pages.", "weight": 1.5, "judge_prompt": "Evaluate clarity and organization for a quick-use SOP. Consider headings, numbering, whitespace, and succinct wording. Is it easy to scan and follow within 1\u20133 pages?\nScoring: 1.5=excellent clarity and organization; 1.0=good with minor issues; 0.5=some confusion or clutter; 0.0=hard to follow.", "expectation": "A concise, well-structured SOP that is easy to scan and follow."}, {"type": "llm_judge", "name": "Practical applicability and actionability", "description": "Determines if staff could implement the procedure immediately without SME guidance, including role assignments and time cues.", "weight": 2.0, "judge_prompt": "Assess the SOP\u2019s practicality: Are roles assigned to steps? Are time cues or priorities stated where needed (especially for critical orders)? Are tools/forms referenced? Would a parts counter person or manager implement it without further clarification?\nScoring: 2.0=immediately actionable; 1.0=mostly actionable with a few ambiguities; 0.0=insufficiently actionable.", "expectation": "Actionable steps with roles, time cues, and references to tools/forms where appropriate."}, {"type": "llm_judge", "name": "Professional tone and terminology consistency", "description": "Evaluates tone, consistent terminology (e.g., BOL, DMS, PDC), and absence of confusing jargon.", "weight": 1.0, "judge_prompt": "Evaluate tone and terminology: Is the SOP professional and consistent (BOL, DMS, PDC used correctly)? Avoids manufacturer-specific jargon unless generalized? No contradictions in terms?\nScoring: 1.0=professional and consistent; 0.5=minor inconsistencies; 0.0=unprofessional or inconsistent.", "expectation": "Professional tone with consistent, correct terminology relevant to dealership parts operations."}, {"type": "llm_judge", "name": "Visuals/annotations effectiveness", "description": "Judges whether visuals or placeholders meaningfully support the damage documentation guidance and overall comprehension.", "weight": 1.5, "judge_prompt": "Assess whether included images/placeholders/diagrams are relevant and annotated or captioned to support damage documentation and comprehension. Do callouts/arrows/captions clarify what to capture and how to mark visibility?\nScoring: 1.5=highly supportive and clear; 1.0=present and helpful; 0.5=present but marginal; 0.0=absent or not useful.", "expectation": "At least one well-captioned, relevant visual or placeholder that clearly instructs documentation and marking of damage."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "94925f49-36bc-42da-b45b-61078d329300", "rubric": {"category_name": "Real Estate Buyer Dossier: Elementary Schools + Nearby Homes (Floral Park & New Hyde Park, NY)", "rationale": "Pattern C (Mixed): A professionally formatted PDF report with structured, quantitative school data plus curated nearby listings. Stage 1 uses LLM-only gating to enforce an explicit, verifiable document shape that enables deterministic checks. Stage 2 mixes lightweight code rules (bounds and presence checks) with LLM judges for nuanced verification (citations, proximity, and consistency). Stage 3 assesses overall professional quality, usefulness to the buyer, and compliance. Code rules are intentionally lighter-weight (\u22485x less total weight than LLM within Stage 2).", "max_total_score": 13.0, "stages": [{"name": "Stage 1 \u2014 Structure and Format Gate (LLM-only)", "description": "Gate that enforces a single combined PDF/DOCX dossier with five clearly delimited sections (one per school) and required tables/sections. If this fails, the category is not gradable.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.0, "rules": [{"type": "llm_judge", "name": "Required dossier format and section coverage", "description": "Checks the candidate produced a single combined PDF/DOCX dossier containing sections for all five specified schools with consistent structure.", "weight": 1.2, "judge_prompt": "You are evaluating the SHAPE ONLY (not correctness) of the output document. Accept either PDF or DOCX. Reject plain text, spreadsheets, or multiple unrelated files. Check the following structural requirements:\n\nFormat requirements:\n- A single combined PDF or DOCX dossier that covers ALL FIVE schools.\n- Professionally formatted document (titles, headers, tables). Not a raw note dump.\n- Contains a title page or top-level title indicating purpose (e.g., buyer dossier, school comparison).\n\nPer-school sections (five total):\nMust include separate, clearly labeled sections for each of these schools (be flexible with exact header wording):\n- Floral Park-Bellerose School\n- John Lewis Childs School\n- Hillside Grade School\n- Manor Oaks School\n- Garden City Park School\n\nIn EACH school section, the following subsections must be present (naming can vary slightly):\n1) \"School Snapshot\" (or similar) \u2014 a compact table of key metrics (at least 6 fields), such as: grades served, enrollment, student-teacher ratio, academic ratings/scores, % gifted (if available), average teacher salary, and district name.\n2) \"Nearby Homes\" (or similar) \u2014 a table of listings that meet buyer criteria.\n3) \"Community Reviews\" (or similar) \u2014 brief curated parent/alumni/community notes.\n4) \"Sources & Access Date\" (or similar) \u2014 citations for school data and listing sites including an access date.\n\nScoring:\n- 1.0: All five school sections present AND each section has all four required subsections.\n- 0.7: All five schools present but 1\u20132 sections are missing one required subsection.\n- 0.4: Only 3\u20134 of the five schools present OR most sections missing multiple subsections.\n- 0.0: Not a single combined PDF/DOCX, or fewer than 3 schools covered, or structure grossly missing.\n\nOnly assess presence/structure, not the accuracy of content.", "expectation": "A single PDF/DOCX dossier with five school sections, each containing Snapshot table, Nearby Homes table, Community Reviews, and Sources with access date."}, {"type": "llm_judge", "name": "Tables present and properly labeled", "description": "Verifies that the required tables are present with appropriate column labels for both school snapshot and nearby homes per school section.", "weight": 1.2, "judge_prompt": "Check that tables exist and are labeled appropriately. For EACH of the five school sections, look for:\n\nA) School Snapshot table (or similar) with at least 6 metrics (labels similar to: Grades, Enrollment, Student-Teacher Ratio, Test Scores/Academic Rating, % Gifted, Avg Teacher Salary, District). Minor naming variations acceptable.\n\nB) Nearby Homes table (or similar) with columns analogous to: Address, Distance to School, List Price, Beds, Baths, Sq Ft, Lot Size, Days on Market, Source/Link, Notes. Minor naming variations are acceptable, but it must be obvious that these attributes are covered.\n\nScoring:\n- 1.0: Both required tables present with clear column labels in all five sections.\n- 0.7: Minor omissions (e.g., 1\u20132 sections missing 1\u20132 columns) but overall recognizable and consistent.\n- 0.4: Tables present but poorly labeled or multiple sections lack columns.\n- 0.0: Tables are missing or unlabeled across most sections.\n\nEvaluate table presence/labels only; do not evaluate the correctness of the data.", "expectation": "Each school section contains a clearly labeled School Snapshot table and a clearly labeled Nearby Homes table with the expected columns."}, {"type": "llm_judge", "name": "Page limits and overall structure sanity", "description": "Ensures per-school page count and total length are within expectations and that sections are clearly delineated.", "weight": 0.6, "judge_prompt": "Verify page limits and general structural organization:\n- Each school section should be no more than 10 pages. Combined dossier should be plausibly within 50 pages.\n- Sections should be clearly delineated with headers per school; tables and reviews should be grouped within that section.\n\nScoring:\n- 1.0: All sections within page limits; document length reasonable; clear section delineation.\n- 0.6: Slightly exceeds limit in 1 section or minor structure sprawl, but still workable.\n- 0.2: Multiple sections appear overly long or structure is confusing.\n- 0.0: Grossly exceeds limits or structure is chaotic/unclear.\n\nCheck length/structure only; do not assess content quality.", "expectation": "A concise, well-structured dossier adhering to the 10 pages per school guideline and clearly separated sections."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification: Data Correctness and Plausibility (Mixed)", "description": "Now that the dossier shape is validated, verify correctness and plausibility of data, sources, and listing constraints. Combines light code checks with LLM judgment. Code rules are intentionally lower weight than LLM rules.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Price cap compliance for listings (<= $1,250,000)", "description": "Parse the document text for listing-like dollar amounts near real-estate keywords and check all are <= 1,250,000. Heuristic filters exclude school salary/other non-listing dollars.", "weight": 0.4, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_document:\\n            return 0.0, 'No document output to evaluate.'\\n        text = ''\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_docx_text(output.id)\\n            except Exception:\\n                return 0.0, 'Unable to read text from PDF/DOCX.'\\n        low = text.lower()\\n\\n        price_pat = re.compile(r\"\\$\\s*([0-9]{1,3}(?:,[0-9]{3})+|[0-9]+)(?:\\.\\d{2})?\")\\n        listing_keywords = ['list price', 'asking', 'listing', 'home', 'property', 'mls', 'beds', 'baths', 'sq ft', 'sqft', 'address', 'dom', 'days on market', 'nearby homes']\\n        exclude_keywords = ['salary', 'per student', 'tuition', 'funding', 'budget', 'median income']\\n\\n        matches = list(price_pat.finditer(text))\\n        checked, compliant, violators = 0, 0, []\\n        for m in matches:\\n            start, end = m.span()\\n            window = low[max(0, start-160): min(len(low), end+160)]\\n            if any(kw in window for kw in listing_keywords) and not any(ek in window for ek in exclude_keywords):\\n                # consider this a listing price\\n                val_str = m.group(1).replace(',', '')\\n                try:\\n                    val = int(val_str)\\n                except Exception:\\n                    continue\\n                checked += 1\\n                if val <= 1250000:\\n                    compliant += 1\\n                else:\\n                    violators.append(val)\\n\\n        if checked == 0:\\n            return 0.5, 'No listing-like prices detected; assigning neutral partial credit.'\\n        score = compliant / checked\\n        fb = f'Checked {checked} listing-like prices; compliant={compliant}, violators={violators[:5]}'\\n        return score, fb\\n    except Exception as e:\\n        return 0.0, f'Error in price cap check: {e}'"}, {"type": "code", "name": "School info sources present with access dates", "description": "Detect presence of reputable school info sources (e.g., niche.com, greatschools.org, district sites) and at least one access date pattern.", "weight": 0.3, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_document:\\n            return 0.0, 'No document output.'\\n        text = ''\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_docx_text(output.id)\\n            except Exception:\\n                return 0.0, 'Unable to read text content.'\\n        low = text.lower()\\n        school_domains = ['niche.com', 'greatschools.org', 'usnews.com', 'school', 'district']\\n        found = set()\\n        for d in ['niche.com','greatschools.org','usnews.com']:\\n            if d in low:\\n                found.add(d)\\n        # Also detect any URL containing 'school' or 'district' as supplementary evidence\\n        for url in re.findall(r'https?://[^\\s)]+', low):\\n            if 'school' in url or 'district' in url:\\n                found.add(url.split('/')[2])\\n        date_pat = re.compile(r'(accessed|retrieved|as of)\\s+(on\\s+)?((?:\\w+\\s+\\d{1,2},\\s*\\d{4})|(?:\\d{4}-\\d{2}-\\d{2})|(?:\\d{1,2}/\\d{1,2}/\\d{2,4}))')\\n        has_date = bool(date_pat.search(low))\\n\\n        # Scoring: >=2 distinct school info sources => 0.8; >=1 => 0.5; +0.2 if access date present (cap 1.0)\\n        base = 0.0\\n        if len(found) >= 2:\\n            base = 0.8\\n        elif len(found) >= 1:\\n            base = 0.5\\n        if has_date:\\n            base += 0.2\\n        if base > 1.0:\\n            base = 1.0\\n        fb = f\"School sources detected: {list(found)[:5]}, access_date={has_date}\"\\n        return base, fb\\n    except Exception as e:\\n        return 0.0, f'Error in school sources check: {e}'"}, {"type": "code", "name": "Real estate platform sources present", "description": "Detect presence of reputable listing platforms (OneKey MLS/MLSLI, Realtor, Zillow, Redfin, etc.).", "weight": 0.3, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_document:\\n            return 0.0, 'No document output.'\\n        text = ''\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_docx_text(output.id)\\n            except Exception:\\n                return 0.0, 'Unable to read text content.'\\n        low = text.lower()\\n        platforms = ['mlsli.com','onekeymls.com','realtor.com','zillow.com','redfin.com','trulia.com','streeteasy.com','compass.com','corcoran.com','elliman.com']\\n        found = set(d for d in platforms if d in low)\\n        # Score: >=2 platforms => 1.0; 1 platform => 0.6; else 0.0\\n        score = 1.0 if len(found) >= 2 else (0.6 if len(found) == 1 else 0.0)\\n        fb = f\"Listing platforms detected: {list(found)}\"\\n        return score, fb\\n    except Exception as e:\\n        return 0.0, f'Error in real estate sources check: {e}'"}, {"type": "llm_judge", "name": "School data plausibility and citation integrity", "description": "Judge whether school metrics look plausible and are properly attributed to credible sources with access dates within a reasonable timeframe.", "weight": 1.5, "judge_prompt": "For each school section, inspect the school metrics (enrollment, student-teacher ratio, academic ratings/scores, % gifted if present, teacher salary) and the cited sources.\n\nEvaluate:\n- Do the metrics fall within plausible ranges for an LI (Long Island) elementary school? (e.g., enrollment roughly 100\u20131,500; student-teacher ratio roughly 8\u201320; salaries and ratings within reasonable bounds).\n- Are citations provided for the metrics (e.g., niche.com, district sites), with an access date? Dates should be recent (within the last 24 months if specified).\n- No obvious contradictions within a school section (e.g., ratio inconsistent with enrollment/teacher counts, wildly implausible numbers).\n\nScoring:\n- 1.0\u20131.5: All five schools have plausible metrics AND proper citations + dates; no contradictions.\n- 0.5\u20130.9: Minor gaps (one section missing a date or marginal plausibility issues) but mostly credible.\n- 0.0\u20130.4: Multiple sections lack citations/dates or have implausible data/contradictions.", "expectation": "Plausible school metrics with traceable, recent citations for each school."}, {"type": "llm_judge", "name": "Listings meet criteria and proximity rationale", "description": "Judge whether the listed homes per school meet the stated buyer constraints and include credible proximity details.", "weight": 1.5, "judge_prompt": "Check the Nearby Homes table(s) across all school sections.\n\nCriteria to verify:\n- List prices are at or below $1,250,000.\n- Homes are single-family (or the document explains any exceptions) and located in or very near Floral Park, NY or New Hyde Park, NY.\n- Proximity is explained (distance in miles/minutes or clear neighborhood adjacency) and seems plausible for each school.\n- Each school has at least a few candidate homes (aim for \u22653), unless scarcity is explicitly noted.\n- Sources/links for each property are provided and credible (MLS/major portals).\n\nScoring:\n- 1.0\u20131.5: All or nearly all entries meet criteria with clear proximity info and solid sources.\n- 0.5\u20130.9: One school thin on listings or minor price/proximity/link gaps.\n- 0.0\u20130.4: Multiple violations (over price cap, far locations, or missing source links).", "expectation": "For each school, a small slate of at-or-below-cap single-family properties with plausible proximity notes and working source links."}, {"type": "llm_judge", "name": "District identification and boundary notes", "description": "Ensure the report identifies each school's district and appropriately cautions on attendance boundaries/assignment verification.", "weight": 1.0, "judge_prompt": "Evaluate whether each school\u2019s district is clearly stated and whether the dossier includes a reasonable disclaimer or guidance about school attendance boundaries (e.g., buyers must verify with district; attendance zones can change; address lookup needed). Also check that the document avoids unlawful steering and focuses on objective, sourced information.\n\nScoring:\n- 1.0: District named for each school + clear boundary verification guidance + neutral, objective tone.\n- 0.5: Districts named but weak/no boundary guidance or tone slightly suggestive.\n- 0.0: Missing district info or inappropriate/steering language.", "expectation": "Clear district names and a boundary verification disclaimer; neutral tone."}, {"type": "llm_judge", "name": "Internal consistency across sections", "description": "Judge for conflicting figures, duplicated or mismatched school/listing info, and consistency in units and timeframes across the dossier.", "weight": 1.0, "judge_prompt": "Scan across all five school sections for internal consistency:\n- No contradictory numbers within a school (e.g., two different student-teacher ratios for the same school) or between tables and narrative.\n- Consistent units (miles vs minutes clarified; square feet consistently applied; $ formatting consistent).\n- Timeframes are aligned (e.g., all school metrics refer to the same or similar school year; listing data reflect a common timeframe).\n\nScoring:\n- 1.0: Consistent across the dossier.\n- 0.5: Minor inconsistencies noted.\n- 0.0: Multiple or major contradictions/format inconsistencies that undermine reliability.", "expectation": "No significant contradictions; consistent units/timeframes across sections."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Buyer Usefulness (LLM)", "description": "Professional polish, buyer-centered insight, clarity, and compliance. Not a gate; awards points for a strong, decision-ready dossier.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional presentation and clarity", "description": "Assess layout quality, readability, and navigability for a non-expert homebuyer audience.", "weight": 1.0, "judge_prompt": "Evaluate the dossier\u2019s professional polish: clean layout, readable fonts, consistent headings, well-formatted tables, and an index/contents or clear section headers that make navigation easy. Writing should be concise, free of errors, and understandable to a typical buyer.\n\nScoring:\n- 0.8\u20131.0: Highly professional and easy to read.\n- 0.4\u20130.7: Generally clear but with some formatting or clarity issues.\n- 0.0\u20130.3: Difficult to read, inconsistent formatting, or sloppy presentation.", "expectation": "Clean, consistent, and buyer-friendly presentation."}, {"type": "llm_judge", "name": "Decision support and actionable insight", "description": "Assesses whether the dossier helps the buyer decide among schools and select targets with clear tradeoffs and prioritization.", "weight": 1.0, "judge_prompt": "Does the dossier provide a comparative perspective and clear next steps? Look for: a brief comparison/summary across the five schools, tradeoffs (e.g., academics vs. class sizes), a shortlist of top school(s) and property targets with rationale, and buyer-relevant flags (e.g., taxes, potential commute). Avoid prescriptive language; keep it options-oriented.\n\nScoring:\n- 0.8\u20131.0: Strong comparative insight and prioritized, actionable suggestions.\n- 0.4\u20130.7: Some comparative insights but limited prioritization.\n- 0.0\u20130.3: Little to no decision support.", "expectation": "A succinct comparison and prioritized, buyer-centric recommendations."}, {"type": "llm_judge", "name": "Transparency and reproducibility", "description": "Assesses whether sources, access dates, and methods are transparent so buyers could replicate/refresh the findings.", "weight": 1.0, "judge_prompt": "Check for a brief methodology note describing how school data were sourced and how listings were filtered (price cap, single-family, proximity). Ensure links are included, access dates provided, and any assumptions or limitations stated (e.g., data may change, listings volatile). \n\nScoring:\n- 0.8\u20131.0: Clear methodology + links + dates + limitations.\n- 0.4\u20130.7: Partial methodology or sporadic links/dates.\n- 0.0\u20130.3: Opaque sourcing and methods.", "expectation": "Clear methods, links, dates, and limitations for reproducibility."}, {"type": "llm_judge", "name": "Compliance and neutrality", "description": "Evaluates compliance with fair housing norms, neutral tone, and appropriate disclaimers about school zones and data variability.", "weight": 1.0, "judge_prompt": "Ensure the dossier maintains a neutral, non-discriminatory tone, avoids steering, and includes appropriate disclaimers (verify attendance zones with district; data subject to change; links provided). Any qualitative content (reviews) should be attributed and paraphrased neutrally.\n\nScoring:\n- 0.8\u20131.0: Fully compliant and neutral, with appropriate disclaimers.\n- 0.4\u20130.7: Mostly compliant; minor tone issues.\n- 0.0\u20130.3: Notably noncompliant or suggestive language.", "expectation": "Neutral tone, fair housing compliance, and proper disclaimers."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "ee09d943-5a11-430a-b7a2-971b4e9b01b5", "rubric": {"category_name": "April Month-End Financial Package (Aurisic) - Structured, Verifiable Workbook", "rationale": "This rubric enforces a self-documenting workflow for a senior accountant preparing an April month-end financial package. Stage 1 is a strict LLM-only shape gate requiring a single Excel workbook with a precise structure (TOC as Tab 0, exclusion of CFO-reserved tabs, inclusion and labeling of April tabs). Stage 2 mixes light code rules (deterministic checks on file type, sheet names, and TOC coverage of source files) with heavier LLM verification of cross-references, reconciliations, and inclusion of new April schedules. Stage 3 is a holistic quality assessment focused on professional presentation and executive readiness. Code rules are weighted ~5x less than LLM rules in Stage 2 to reflect their narrower scope, while still ensuring objective, automatable validations.", "max_total_score": 24.0, "stages": [{"name": "Stage 1 - Shape Enforcement Gate (LLM only)", "description": "Mandate exact, verifiable workbook structure enabling downstream verification. Failure zeros the category.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Workbook Structural Requirements (Gate)", "description": "Verify the candidate produced the required Excel package shape for April.", "weight": 4.0, "judge_prompt": "You are validating the STRUCTURE ONLY (not correctness of numbers) of an Excel workbook prepared for April month-end. Use flexible matching for header wording, but enforce the required shape.\n\nRequirements to verify (structure/format only):\n1) Deliverable & Naming\n   - Single Excel workbook (xlsx) named exactly: \"Aurisic_Financials_4-25-1.xlsx\" (be flexible if the platform obscures exact filename, but prefer exact match if visible).\n\n2) Tab Order and Reserved Tabs\n   - Tab 0 must be a Table of Contents (TOC). Accept variations like \"TOC\", \"Table of Contents\", \"Contents\".\n   - CFO-reserved tabs 1, 2, 2a, and 3 must NOT be present in the April workbook.\n   - The working tabs start at 3a and continue onward.\n\n3) TOC Content (Tab 0)\n   - TOC contains a table listing at minimum: Tab #/Index, Tab Name, Source File(s), and a Status/Comments field.\n   - TOC reflects all included April tabs and clearly labels any new April-only tabs as new (e.g., \"NEW\").\n   - The TOC should reference the April source files by their exact names when applicable:\n     \u2022 Accr2011-1.xlsx\n     \u2022 AccrBonus-1.xlsx\n     \u2022 AccrMisc-1.xlsx\n     \u2022 Aurisic_Corp_Payrolls_April_2025-1.xlsx\n     \u2022 Aurisic_Financials_3-25-1.xlsx (as prior template)\n     \u2022 Aurisic_Prepaid_Expenses_4-25-1.xlsx\n     \u2022 AP_TB-1.xlsx\n     \u2022 AR_Accrual-1.xlsx\n     \u2022 Aurisic_Final_TB_4-25-1.txt\n     \u2022 Good Insurance Co - Loan II.xlsx\n     \u2022 Good Insurance Co - Loan.xlsx\n     \u2022 Legal_Dump-1.xlsx\n     \u2022 Outstanding_CKs_4-30-25-1.xlsx\n     \u2022 Payroll-1.xlsx\n     \u2022 PPD1250-1.xlsx\n     \u2022 PPD1251-1.xlsx\n     \u2022 Prof_Fee_Dump-1.xlsx\n     \u2022 Rebates-1.xlsx\n\n4) Working Tabs (3a onward)\n   - Each working tab (3a+) is present and clearly labeled. It should include a brief header block (accept variations) indicating at least: Source(s) used, that it is updated for April 2025, and a short description or purpose.\n   - Any April-only schedules/tabs not present in March (from the provided April sources) are added at the end and identified in the TOC.\n\n5) Template Adherence\n   - The overall structure and tab sequencing take \"Aurisic_Financials_3-25-1.xlsx\" (March) as the primary template for structure, formatting, and tab order; new April-only tabs are appended at the end.\n\nScoring (Structure ONLY):\n- 4.0: All structural requirements met: xlsx workbook; TOC as Tab 0 with required columns; reserved tabs excluded; working tabs 3a+ present; April sources referenced; April-only tabs appended and labeled; follows March template.\n- 3.2: Minor deviations (e.g., slight TOC column naming differences or small TOC omissions) but core shape intact: TOC as Tab 0, reserved tabs excluded, working tabs 3a+ present, April sources mostly referenced.\n- 2.0: Multiple structural issues but still an Excel workbook with a recognizable TOC and some 3a+ tabs updated. Examples: TOC missing key columns, April sources not referenced, or unclear labeling of new tabs.\n- 0.0: Not an Excel workbook; no TOC; or reserved tabs (1, 2, 2a, 3) included; or structure too incomplete to verify.\n\nOnly evaluate structure/format presence, not numerical correctness or reconciliation.", "expectation": "A single .xlsx named Aurisic_Financials_4-25-1.xlsx with Tab 0 as TOC (listing Tab #, Tab Name, Source File(s), Status/Comments), excludes tabs 1,2,2a,3, includes tabs 3a+ updated for April with header blocks, and appends any new April-only tabs at the end and labels them in the TOC."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 - Correctness and Verification (Mixed)", "description": "Now that the workbook is in verifiable shape, check internal consistency, references, and April coverage using light code checks and LLM judgment. Code rules are light; LLM rules assess nuanced verification.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Output Type and TOC Presence (light check)", "description": "Confirm the primary output is an Excel workbook and that a TOC-like first sheet exists.", "weight": 0.6, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheets = xls.sheet_names\n        if not sheets:\n            return 0.0, \"No sheets found\"\n        first = sheets[0].lower()\n        score = 0.0\n        # First sheet should be TOC-like\n        if any(k in first for k in [\"toc\", \"table of contents\", \"contents\"]):\n            score += 0.4\n        # Any sheet named like TOC earns partial if first isn't TOC\n        if score < 0.4:\n            if any(any(k in s.lower() for k in [\"toc\", \"table of contents\", \"contents\"]) for s in sheets):\n                score += 0.2\n        return min(score, 0.6), f\"First sheet: {sheets[0]}\"\n    except Exception as e:\n        return 0.0, f\"Error reading workbook: {e}\""}, {"type": "code", "name": "Reserved Tabs Excluded", "description": "Verify tabs 1, 2, 2a, and 3 are not present (allow fuzzy matching like 'Tab 1').", "weight": 0.5, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        sheets = pd.ExcelFile(path).sheet_names\n        patterns = [r'^\\s*1\\s*$', r'^\\s*2\\s*$', r'^\\s*2a\\s*$', r'^\\s*3\\s*$', r'^\\s*tab\\s*1\\s*$', r'^\\s*tab\\s*2\\s*$', r'^\\s*tab\\s*2a\\s*$', r'^\\s*tab\\s*3\\s*$']\n        bad = []\n        for s in sheets:\n            sl = s.lower().strip()\n            if any(re.match(p, sl) for p in patterns):\n                bad.append(s)\n        return (0.5 if not bad else 0.0, (\"OK\" if not bad else f\"Reserved tabs present: {bad}\"))\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "TOC References April Source Files", "description": "Check TOC text includes the specified April source filenames (exact names, substring match anywhere in TOC sheet). Scores proportionally.", "weight": 0.9, "code": "import re\nimport pandas as pd\n\nFILES = [\n    'Accr2011-1.xlsx',\n    'AccrBonus-1.xlsx',\n    'AccrMisc-1.xlsx',\n    'Aurisic_Corp_Payrolls_April_2025-1.xlsx',\n    'Aurisic_Financials_3-25-1.xlsx',\n    'Aurisic_Prepaid_Expenses_4-25-1.xlsx',\n    'AP_TB-1.xlsx',\n    'AR_Accrual-1.xlsx',\n    'Aurisic_Final_TB_4-25-1.txt',\n    'Good Insurance Co - Loan II.xlsx',\n    'Good Insurance Co - Loan.xlsx',\n    'Legal_Dump-1.xlsx',\n    'Outstanding_CKs_4-30-25-1.xlsx',\n    'Payroll-1.xlsx',\n    'PPD1250-1.xlsx',\n    'PPD1251-1.xlsx',\n    'Prof_Fee_Dump-1.xlsx',\n    'Rebates-1.xlsx'\n]\n\nCANDIDATE_TOC_NAMES = ['toc', 'table of contents', 'contents', 'tab 0']\n\n\ndef _find_toc_sheet(sheet_names):\n    # Prefer first if looks like TOC\n    first = sheet_names[0].lower() if sheet_names else ''\n    if any(k in first for k in CANDIDATE_TOC_NAMES):\n        return sheet_names[0]\n    for s in sheet_names:\n        sl = s.lower()\n        if any(k in sl for k in CANDIDATE_TOC_NAMES):\n            return s\n    # fallback to first\n    return sheet_names[0] if sheet_names else None\n\n\ndef evaluate(workflow, context):\n    out = context.get_primary_output()\n    if not out or not out.is_spreadsheet:\n        return 0.0, \"No spreadsheet\"\n    try:\n        path = context.files.get_path(out.id)\n        xls = pd.ExcelFile(path)\n        sheets = xls.sheet_names\n        if not sheets:\n            return 0.0, \"No sheets\"\n        toc_sheet = _find_toc_sheet(sheets)\n        if not toc_sheet:\n            return 0.0, \"No TOC sheet detected\"\n        df = pd.read_excel(path, sheet_name=toc_sheet, header=None)\n        text = ' '.join(df.astype(str).fillna('').values.flatten()).lower()\n        found = 0\n        missing = []\n        for name in FILES:\n            if name.lower() in text:\n                found += 1\n            else:\n                missing.append(name)\n        ratio = found / max(1, len(FILES))\n        score = ratio * 0.9\n        return score, (f\"Found {found}/{len(FILES)}\" if missing else \"All files referenced\")\n    except Exception as e:\n        return 0.0, f\"Error reading TOC: {e}\""}, {"type": "llm_judge", "name": "Per-Tab Source, April Update, and Header Blocks", "description": "Verify that tabs 3a+ include clear header blocks indicating source(s), April 2025 update, and purpose/description.", "weight": 3.0, "judge_prompt": "Review the workbook\u2019s working tabs (3a onward). Check that a majority of these tabs visibly include a header block indicating at least: (a) the source file(s) used, (b) that the tab is updated for April 2025, and (c) a brief description or purpose. Be flexible on exact wording and location (top area is typical). Do not assess numerical correctness\u2014only presence and clarity of these header elements.\n\nScoring:\n- 3.0: Nearly all 3a+ tabs have clearly visible header blocks with the three elements (source, April update, purpose).\n- 2.0: Most tabs include at least two of the three elements and are reasonably clear.\n- 1.0: Some tabs include partial headers; coverage is inconsistent.\n- 0.0: Little to no evidence of required headers across 3a+ tabs.", "expectation": "Each 3a+ tab includes a header area listing source(s), indicates April 2025, and states the tab\u2019s purpose."}, {"type": "llm_judge", "name": "Reconciliations and Cross-Checks Present", "description": "Check for presence of tie-outs/variance checks to trial balance and internal roll-forwards.", "weight": 3.5, "judge_prompt": "Verify that the workbook includes visible reconciliation or cross-check sections such as: tie-outs to the April trial balance (e.g., referencing \"Aurisic_Final_TB_4-25-1.txt\"), variance checks, roll-forward checks, or reasonableness checks. Look for labeled areas like \"Check\", \"Tie-out\", \"Variance\", \"Reconciliation\", or similar. We only need to see that the checks exist and are used to validate data; do not verify the math.\n\nScoring:\n- 3.5: Reconciliation/tie-out sections are clearly present on the relevant tabs with indications of pass/fail or variance amounts.\n- 2.0: Some checks present but limited in scope or clarity.\n- 1.0: Minimal evidence of checks; mostly ad-hoc references.\n- 0.0: No visible reconciliation or cross-checks.", "expectation": "Clearly labeled reconciliation/tie-out checks exist and reference April data and/or the April trial balance where appropriate."}, {"type": "llm_judge", "name": "New April Schedules Included and Appended", "description": "Confirm April-only schedules/tabs (from provided April sources) are included, appended at the end, and labeled as NEW in TOC.", "weight": 3.5, "judge_prompt": "Confirm that tabs representing April-only schedules (derived from the provided April source files) are appended to the end of the workbook and identified in the TOC (e.g., flagged as \"NEW\" or otherwise indicated). Be flexible on labeling format but require clear identification and end-of-workbook placement.\n\nScoring:\n- 3.5: All identified April-only schedules are appended at the end and clearly flagged in the TOC as new.\n- 2.0: Most are appended and labeled; minor omissions.\n- 1.0: Some new schedules appear but labeling/placement unclear.\n- 0.0: No evidence of newly added April schedules or misplacement.", "expectation": "Any April-only tabs from the listed sources are appended at the end and clearly marked NEW in the TOC."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 - Quality and Professionalism (LLM)", "description": "Holistic assessment of presentation, documentation, and executive readiness.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Consistency", "description": "Assess formatting, readability, and consistency with March template.", "weight": 2.0, "judge_prompt": "Evaluate the professional presentation of the workbook: consistent fonts/styles, clean tab naming, readable tables, freeze panes or similar usability enhancements where appropriate, and overall consistency with the March template structure. Do not check math; focus on presentation quality.\n\nScoring:\n- 2.0: Highly professional and consistent throughout.\n- 1.0: Generally good with some inconsistencies.\n- 0.0: Sloppy, inconsistent, or difficult to read.", "expectation": "Workbook looks professional, legible, and consistent with prior template conventions."}, {"type": "llm_judge", "name": "Documentation and Change Log Quality", "description": "Assess the clarity and completeness of documented changes, assumptions, and methodology.", "weight": 2.0, "judge_prompt": "Review how well the preparer documented changes from March, assumptions, and methodology. Look for a clear change log or notes on each affected tab or a centralized log. The documentation should enable reviewers to understand what changed for April and why.\n\nScoring:\n- 2.0: Comprehensive, clear, and easy to follow change documentation.\n- 1.0: Partial documentation that covers key changes but with gaps.\n- 0.0: Sparse or missing documentation of changes and assumptions.", "expectation": "Clear change logs and notes that explain April updates and any methodological adjustments."}, {"type": "llm_judge", "name": "Issue Flagging and Executive Readiness", "description": "Evaluate whether issues, inconsistencies, or missing info are clearly flagged and summarized for the CFO/executive team.", "weight": 2.0, "judge_prompt": "Determine whether the workbook clearly flags any missing information or inconsistencies and summarizes action items or CFO notifications (e.g., on TOC or a dedicated issues/notes tab). The package should be ready for executive review with clear next steps if needed.\n\nScoring:\n- 2.0: Issues and action items are clearly flagged and summarized; easy for CFO to review.\n- 1.0: Some flags present but incomplete or scattered.\n- 0.0: No clear issue flagging or executive-oriented summaries.", "expectation": "Concise, visible flags and summaries that facilitate CFO decision-making."}, {"type": "llm_judge", "name": "Analytical Rigor and Completeness (Non-numeric)", "description": "Judge perceived thoroughness of updates and inclusion of all relevant April schedules (without redoing math).", "weight": 2.0, "judge_prompt": "Holistically assess whether the workbook appears complete and thorough for April: relevant tabs updated, new April schedules included, and narratives/headers suggest careful work. Do not verify calculations, only completeness and rigor signals.\n\nScoring:\n- 2.0: Appears complete and thorough across tabs.\n- 1.0: Mostly complete with minor omissions.\n- 0.0: Noticeably incomplete or patchy updates.", "expectation": "Workbook appears comprehensive for April with clear signs of rigorous updating."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "aa071045-bcb0-4164-bb85-97245d56287e", "rubric": {"category_name": "Real Estate & Rental: Counter/Rental Clerk \u2013 Service Request + Damage Revenue Analysis", "rationale": "Mixed task (Pattern C): requires a structured Word/DOCX service form and an Excel analysis/report. Stage 1 uses LLM judges only to strictly enforce output shape for both files so verification is possible. Stage 2 mixes light, robust code checks (on the Excel\u2019s raw data sheet and basic linkage) with higher-weight LLM checks for correctness and cross-referencing. Stage 3 evaluates overall professional quality, clarity, and managerial usefulness.", "max_total_score": 16.0, "stages": [{"name": "Stage 1 \u2013 Shape Enforcement Gate (STRUCTURE ONLY)", "description": "MANDATORY gate. Verify the candidate produced BOTH required outputs with the exact, verifiable structure that enables later checks. LLM judges only.", "is_required": true, "max_points": 2.0, "min_score_to_pass": 1.5, "rules": [{"type": "llm_judge", "name": "Service Request Form \u2013 Document Structure", "description": "Check the presence and structure of the Service Request Form document.", "weight": 1.0, "judge_prompt": "You are validating the SHAPE ONLY of a Service Request Form document. Review ALL provided outputs. Identify the Service Request Form \u2013 it must be a DOCX or a PDF (exported from Word is acceptable). Do NOT judge correctness of content here, only structure and presence of key labeled fields.\n\nFormat requirements:\n- Must be a DOCX or PDF (not Excel/CSV/Markdown).\n- Title near top: \u201cService Request Form \u2013 Vehicle Maintenance\u201d (flexible on punctuation/case, but title intent must be clear).\n- Clearly labeled fields/sections (labels may vary slightly):\n  1) Date\n  2) Current Location\n  3) Customer Name\n  4) Rental Agreement No.\n  5) Car Make & Model\n  6) License Plate No.\n  7) Current Mileage\n  8) Damage Description (mention broken left/driver\u2019s rearview mirror)\n  9) Request Type (Repair or Replacement listed as a selectable/marked option)\n  10) Vehicle Status (Available or Out Of Service listed as a selectable/marked option)\n- A simple footer or signature/prepared-by area is optional but not required for full credit.\n\nScoring:\n- 1.0: Valid DOCX/PDF and ALL 10 labeled fields visibly present as fields/lines/entries; title present.\n- 0.7: Valid DOCX/PDF, title present, and at least 8/10 labeled fields present.\n- 0.4: Valid DOCX/PDF, title present, but only 5\u20137 labeled fields present.\n- 0.0: Not DOCX/PDF OR missing title OR fewer than 5 labeled fields.\n\nOnly check presence/format of structure, not whether the values are correct.", "expectation": "A professional-looking DOCX/PDF form with the specified title and fields exists."}, {"type": "llm_judge", "name": "Damage Revenue Report \u2013 Workbook Structure", "description": "Check the presence and structure of the Excel analysis/report.", "weight": 1.0, "judge_prompt": "You are validating the SHAPE ONLY of an Excel Damage Revenue analysis. Review ALL provided outputs and locate the Excel workbook. Do NOT judge numerical correctness here, only structure and presence of required sheets/sections. Flexible on exact phrasing but intent must be clear.\n\nRequired workbook format:\n- File must be XLSX (Excel), not CSV/PDF.\n- Must contain these sheets and sections:\n  A) Sheet named exactly \u201cDamage Revenue Report\u201d (or very close, e.g., \u201cDamage Revenue \u2013 Report\u201d). This sheet must include:\n     1) A clearly labeled Summary section with a visible Total Damage Revenue value.\n     2) A table labeled Revenue by Vehicle Category with two columns: [Vehicle Category | Revenue].\n     3) A table labeled Revenue by Damage Type with two columns: [Damage Type | Revenue] and must include rows for Dent and Scratch.\n     4) An Operational Conclusions section (at least 3 bullet points or sentences).\n  B) Sheet named \u201cData Import\u201d (or very close, e.g., \u201cRaw Data\u201d, \u201cData \u2013 Import\u201d). This sheet must show row-level records with columns for: Date, Rental Agreement No., Vehicle Category, Damage Type, and Revenue (numeric). Column names may vary slightly but all five data fields must be present as visible columns.\n- Optional: charts/visuals are allowed but not required.\n\nScoring:\n- 1.0: XLSX present AND both sheets present with all required sections/tables/fields listed above.\n- 0.6: XLSX present, but missing exactly one required section/table/field among A1\u2013A4 or B (still structurally strong).\n- 0.0: Not XLSX OR missing multiple required elements (e.g., missing Data Import sheet or both breakdown tables).\n\nOnly check structural presence, not the correctness of calculations.", "expectation": "An XLSX with a report sheet housing Summary, two breakdown tables, and Conclusions plus a raw Data Import sheet with the five required columns."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness Verification (Mixed)", "description": "Now verify that the contents are correct and internally consistent. Favor LLM judgment for nuanced checks; code rules perform deterministic validations on the Excel workbook.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Data Import Integrity and Required Columns", "description": "Verify the Excel report contains a usable Data Import sheet with required fields and plausible values.", "weight": 0.9, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    weight = 0.9\n    # Find a spreadsheet output (prefer one with 'damage' or 'report' in name)\n    outputs = context.get_all_outputs()\n    xls_res = None\n    for r in outputs:\n        if getattr(r, 'is_spreadsheet', False):\n            name = (getattr(r, 'name', '') or '').lower()\n            if any(k in name for k in ['damage', 'revenue', 'report']):\n                xls_res = r\n                break\n    if xls_res is None:\n        # fallback: first spreadsheet\n        for r in outputs:\n            if getattr(r, 'is_spreadsheet', False):\n                xls_res = r\n                break\n    if xls_res is None:\n        return 0.0, 'No spreadsheet output found.'\n\n    # Helper: find Data Import sheet flexibly\n    try:\n        file_path = context.files.get_path(xls_res.id)\n        xls = pd.ExcelFile(file_path)\n        sheets = [s for s in xls.sheet_names]\n        # prefer sheet with 'data' in name\n        data_sheet = None\n        candidates = sorted(sheets, key=lambda s: 0 if re.search(r'data', s, re.I) else 1)\n        for s in candidates:\n            if re.search(r'(data\\s*import|raw\\s*data|data)', s, re.I):\n                data_sheet = s\n                break\n        if data_sheet is None and sheets:\n            data_sheet = sheets[0]\n        df = pd.read_excel(file_path, sheet_name=data_sheet)\n    except Exception as e:\n        return 0.0, f'Failed reading Excel/Data Import: {e}'\n\n    if df is None or df.shape[0] == 0:\n        return 0.0, 'Data Import sheet empty or unreadable.'\n\n    # Normalize columns\n    cols_map = {c: str(c).strip().lower() for c in df.columns}\n    rev_syn = ['revenue','amount','charge','damage charge','total amount','total']\n    cat_syn = ['vehicle category','category','vehicle class','class','car category']\n    dmg_syn = ['damage type','type','damage','issue']\n    date_syn = ['date','damage date','record date']\n    ra_syn = ['rental agreement no.','rental agreement','ra no.','ra no','agreement no','agreement']\n\n    def find_col(syns):\n        for c in df.columns:\n            lc = cols_map[c]\n            for s in syns:\n                if s in lc:\n                    return c\n        return None\n\n    score = 0.0\n    feedback_parts = []\n\n    # Required columns\n    c_rev = find_col(rev_syn)\n    c_cat = find_col(cat_syn)\n    c_dmg = find_col(dmg_syn)\n    c_date = find_col(date_syn)\n    c_ra = find_col(ra_syn)\n\n    req_cols = [('Revenue', c_rev), ('Vehicle Category', c_cat), ('Damage Type', c_dmg), ('Date', c_date), ('Rental Agreement No.', c_ra)]\n    cols_ok = sum(1 for _, c in req_cols if c is not None)\n    score += (cols_ok / len(req_cols)) * (weight * 0.6)\n    if cols_ok < len(req_cols):\n        missing = [name for name, c in req_cols if c is None]\n        feedback_parts.append(f\"Missing columns: {', '.join(missing)}\")\n\n    # Revenue numeric and non-negative\n    rev_ok = 0.0\n    if c_rev is not None:\n        series = pd.to_numeric(df[c_rev], errors='coerce')\n        valid_frac = float(series.notna().mean()) if len(series) else 0.0\n        nonneg_frac = float((series.fillna(0) >= 0).mean()) if len(series) else 0.0\n        rev_ok = (valid_frac * 0.5 + nonneg_frac * 0.5)\n        if valid_frac < 0.8:\n            feedback_parts.append('Revenue column has many non-numeric values.')\n        if nonneg_frac < 0.95:\n            feedback_parts.append('Revenue column has negative values.')\n    score += rev_ok * (weight * 0.25)\n\n    # Damage types presence (dent/scratch appear in data)\n    dmg_ok = 0.0\n    if c_dmg is not None:\n        vals = df[c_dmg].astype(str).str.lower().str.strip()\n        has_dent = vals.str.contains('dent').any()\n        has_scratch = vals.str.contains('scratch').any()\n        dmg_ok = 1.0 if (has_dent or has_scratch) else 0.0\n        if not (has_dent or has_scratch):\n            feedback_parts.append('Damage Type column lacks dent/scratch values.')\n    score += dmg_ok * (weight * 0.15)\n\n    # Cap and return\n    score = min(score, weight)\n    feedback = '; '.join(feedback_parts) if feedback_parts else 'Data Import integrity looks good.'\n    return score, feedback"}, {"type": "code", "name": "Report Linkage \u2013 Categories/Types Reflected in Report Sheet", "description": "Check that categories and damage types from Data Import appear on the report sheet, suggesting the breakdown tables were populated.", "weight": 0.6, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    weight = 0.6\n    outputs = context.get_all_outputs()\n    xls_res = None\n    for r in outputs:\n        if getattr(r, 'is_spreadsheet', False):\n            name = (getattr(r, 'name', '') or '').lower()\n            if any(k in name for k in ['damage', 'revenue', 'report']):\n                xls_res = r\n                break\n    if xls_res is None:\n        for r in outputs:\n            if getattr(r, 'is_spreadsheet', False):\n                xls_res = r\n                break\n    if xls_res is None:\n        return 0.0, 'No spreadsheet output found.'\n\n    try:\n        file_path = context.files.get_path(xls_res.id)\n        xls = pd.ExcelFile(file_path)\n        # Locate data sheet\n        data_sheet = None\n        for s in xls.sheet_names:\n            if re.search(r'(data\\s*import|raw\\s*data|data)', s, re.I):\n                data_sheet = s\n                break\n        if data_sheet is None:\n            data_sheet = xls.sheet_names[0]\n        df = pd.read_excel(file_path, sheet_name=data_sheet)\n        # Locate report sheet\n        report_sheet = None\n        for s in xls.sheet_names:\n            if re.search(r'(damage\\s*revenue\\s*report|damage\\s*revenue)', s, re.I):\n                report_sheet = s\n                break\n        if report_sheet is None:\n            # try any sheet with 'report'\n            for s in xls.sheet_names:\n                if re.search(r'report', s, re.I):\n                    report_sheet = s\n                    break\n        if report_sheet is None:\n            return 0.0, 'Report sheet not found.'\n        rpt = pd.read_excel(file_path, sheet_name=report_sheet, header=None).astype(str)\n    except Exception as e:\n        return 0.0, f'Failed reading workbook: {e}'\n\n    # Gather category values and damage type values from data\n    def find_col(df, syns):\n        cmap = {c: str(c).strip().lower() for c in df.columns}\n        for c in df.columns:\n            lc = cmap[c]\n            for s in syns:\n                if s in lc:\n                    return c\n        return None\n\n    c_cat = find_col(df, ['vehicle category','category','vehicle class','class','car category'])\n    c_dmg = find_col(df, ['damage type','type','damage','issue'])\n\n    categories = set()\n    dmg_types = set()\n    if c_cat is not None:\n        categories = set(df[c_cat].dropna().astype(str).str.strip().str.lower().unique())\n    if c_dmg is not None:\n        dmg_types = set(df[c_dmg].dropna().astype(str).str.strip().str.lower().unique())\n\n    rpt_text = ' '.join(rpt.fillna('').values.flatten()).lower()\n\n    # Count matches present in report sheet text\n    cat_matches = sum(1 for c in categories if c and c in rpt_text)\n    dmg_matches = sum(1 for d in dmg_types if d and d in rpt_text)\n\n    # Score: 70% weight for categories appearing, 30% for damage types\n    cat_score = (cat_matches / max(1, len(categories))) if categories else 0.0\n    dmg_score = (dmg_matches / max(1, len(dmg_types))) if dmg_types else 0.0\n    score = (cat_score * 0.7 + dmg_score * 0.3) * weight\n\n    fb = f\"Category labels matched: {cat_matches}/{max(1,len(categories))}; Damage type labels matched: {dmg_matches}/{max(1,len(dmg_types))}.\"\n    return min(score, weight), fb"}, {"type": "llm_judge", "name": "Service Request \u2013 Correctness of Populated Details", "description": "Verify the service form correctly reflects the provided incident details and appropriate selections.", "weight": 2.5, "judge_prompt": "Review the Service Request Form (DOCX/PDF). Now check correctness of the populated values (not just structure):\n\nProvided incident details (should be present and correct):\n- Date: September 18 (today)\n- Current Location: ORD (Chicago O\u2019Hare)\n- Customer Name: Carol Smith\n- Rental Agreement No.: 1809/2025\n- Car Make & Model: Toyota Corolla\n- License Plate No.: LAV-555\n- Current Mileage: 10562\n- Damage Description: Broken left/driver\u2019s rearview mirror (wording may vary but meaning must match)\n- Request Type: Replacement (a broken mirror typically requires replacement, not repair)\n- Vehicle Status: Out Of Service (expected; vehicle should be unavailable until the mirror is fixed)\n\nScoring:\n- 2.5: All above details present and correct; Request Type marked as Replacement; Vehicle Status marked Out Of Service.\n- 1.8: One minor field off/omitted but core fields (RA no., customer, vehicle, damage, request type, status) correct.\n- 1.2: Multiple minor issues or one core field incorrect/missing (e.g., wrong request type or status).\n- 0.0: Major mismatch (significant fields missing/incorrect) or form not found.\n\nBe strict but reasonable about equivalence of phrasing (e.g., \u201cdriver-side mirror broken\u201d acceptable).", "expectation": "All specified values correctly populated; Replacement and Out Of Service selected."}, {"type": "llm_judge", "name": "Excel Report \u2013 Correct Computations and Coverage", "description": "Check that the report actually computes and shows the required summaries/breakdowns based on the underlying data.", "weight": 2.0, "judge_prompt": "Review the Excel workbook. Judge whether the report:\n1) Clearly shows a Total Damage Revenue value that is consistent with the data shown in the workbook (at least facially; you do not need exact recalculation but look for obvious inconsistencies).\n2) Includes a Revenue by Vehicle Category table with reasonable entries and totals.\n3) Includes a Revenue by Damage Type table and that rows for Dent and Scratch are present and plausible.\n4) The Operational Conclusions logically reference patterns visible in the tables (e.g., top categories, which damage type drives revenue).\n\nScoring:\n- 2.0: All four elements are present, numerically formatted, and appear internally consistent.\n- 1.4: Three elements clearly correct; one seems weak or inconsistent.\n- 0.8: Only two elements clearly correct; others unclear.\n- 0.0: Lacks core computations or they conflict with shown data.\n\nUse qualitative judgment; do not require exact numeric agreement if not directly visible.", "expectation": "Totals and breakdowns appear reasonable and linked to shown data; conclusions reference the displayed numbers."}, {"type": "llm_judge", "name": "Methodology/Traceability Notes", "description": "Check for a brief notes/methodology section citing data source and assumptions.", "weight": 1.5, "judge_prompt": "In the Excel workbook, look for a short Notes/Methodology/Assumptions section (anywhere in the report sheet or a separate notes area) that:\n- Cites the data source as the prior workday\u2019s \u201cDamage list.xlsx\u201d.\n- Mentions the processing date context (e.g., aligns to the prior workday) and any light cleaning/assumptions.\n- Clarifies definitions of Vehicle Category and Damage Type used in the tables.\n\nScoring:\n- 1.5: Notes present and cover all three points (source, timing, definitions/assumptions).\n- 1.0: Notes present but miss one point.\n- 0.5: Minimal note referencing source but lacks detail.\n- 0.0: No notes/methodology found.", "expectation": "A concise methodology note tying the report to \u201cDamage list.xlsx\u201d and explaining any assumptions."}, {"type": "llm_judge", "name": "Formatting Correctness (Currency/Legibility)", "description": "Check that revenue fields look like currency and tables are legible with clear labels/units.", "weight": 0.5, "judge_prompt": "Visually inspect the Excel report for basic formatting correctness:\n- Revenue figures formatted as currency (or clearly labeled with $ or USD) in Summary and breakdown tables.\n- Column headers and table labels are readable; totals highlighted or otherwise distinguishable.\n\nScoring:\n- 0.5: Currency/labels clear and legible throughout.\n- 0.3: Mostly clear, minor lapses.\n- 0.0: Poor formatting; hard to tell amounts/units.", "expectation": "Revenue values are clearly currency-formatted; headers and totals are readable."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Quality and Professionalism", "description": "Holistic quality assessment of both the service form and the Excel report for management and maintenance audiences.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation (Both Outputs)", "description": "Assess the overall formatting, readability, and professionalism of the DOCX/PDF and Excel.", "weight": 1.5, "judge_prompt": "Evaluate the professional presentation of both outputs:\n- Service Request Form: clean layout, clear field labels, consistent typography, easy to complete/read.\n- Excel Report: clean sheet names, logical section ordering, clear tables, optional charts well-labeled if present.\n\nScoring:\n- 1.5: Both documents look professional and easy to use.\n- 1.0: Generally good; minor layout/formatting issues.\n- 0.5: Noticeable formatting issues reduce clarity.\n- 0.0: Poorly formatted and hard to follow.", "expectation": "Clean, professional documents suitable for internal operations and management."}, {"type": "llm_judge", "name": "Actionable Operational Insights", "description": "Judge the usefulness of the conclusions for decision-making.", "weight": 1.5, "judge_prompt": "Assess how actionable the Operational Conclusions are:\n- Do they identify clear patterns (e.g., high-revenue categories or dominant damage type)?\n- Do they propose concrete actions (e.g., inspection focus, parts inventory, training, signage)?\n- Are recommendations prioritized and feasible?\n\nScoring:\n- 1.5: Insightful, specific, and actionable recommendations tied to data.\n- 1.0: Some actionable advice but generic or lightly tied to data.\n- 0.5: Vague high-level statements.\n- 0.0: No meaningful conclusions.", "expectation": "Specific, data-linked recommendations that management can act on."}, {"type": "llm_judge", "name": "Audience Appropriateness & Clarity", "description": "Ensure content fits maintenance and management audiences with clear terminology.", "weight": 1.5, "judge_prompt": "Evaluate whether the outputs use appropriate tone and clarity for operational/management audiences:\n- Service Form uses concise, unambiguous labels and damage description.\n- Excel uses plain language, consistent terminology (Vehicle Category, Damage Type), and avoids jargon.\n- Key numbers stand out; no unnecessary clutter.\n\nScoring:\n- 1.5: Very clear for both audiences; terminology consistent.\n- 1.0: Mostly clear with minor inconsistencies.\n- 0.5: Noticeable clarity or terminology issues.\n- 0.0: Confusing or misaligned with audience needs.", "expectation": "Clear, consistent, and concise communication for internal use."}, {"type": "llm_judge", "name": "Completeness & Self-Containment", "description": "Check that someone else could reuse/understand the report and form without extra context.", "weight": 1.5, "judge_prompt": "Assess whether the package is self-contained:\n- Form contains all necessary identifiers for maintenance processing.\n- Excel report includes enough labels/notes to understand scope, period (prior workday), and definitions.\n- Optional: a simple cover note or \u201cRead Me\u201d cell area improves clarity.\n\nScoring:\n- 1.5: Fully self-contained; easy to understand and reuse.\n- 1.0: Mostly complete; small gaps.\n- 0.5: Several missing context elements.\n- 0.0: Not self-contained; requires outside info to interpret.", "expectation": "A complete package with identifiers, scope, and definitions apparent."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "0ed38524-a4ad-405f-9dee-7b2252659aad", "rubric": {"category_name": "Finance & Insurance \u2014 ECID Constituent Feedback Summaries (CSR)", "rationale": "Pattern C (Mixed): Agent must produce two documents (PDFs) derived from an Excel tracking log: a one-page per\u2011district summary and a set of board\u2011meeting talking points. Stage 1 uses LLM-only gates to strictly enforce deliverable shape and document structure. Stage 2 combines lightweight code checks (file presence, token/section heuristics, cross-document consistency) with higher-weight LLM verification (per-district coverage, sourcing, and inter-document alignment). Stage 3 assesses overall professional quality, clarity, and actionability for a board audience. Code rules are intentionally lower weight than LLM rules in Stage 2 (\u22485x lower on average).", "max_total_score": 38.0, "stages": [{"name": "Stage 1 \u2014 Structure & Format Gate (LLM-only)", "description": "Gate that enforces exact deliverable shape so verification is possible: two PDFs produced \u2014 a 1-page constituent summary by district and a talking-points PDF for board discussion. Only checks presence/format/sections, not content quality or correctness.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.5, "rules": [{"type": "llm_judge", "name": "GATE: One-Page District Summary PDF \u2014 Required Structure", "description": "Verify there is a single-page PDF summary with specified sections and per-district structure.", "weight": 3.0, "judge_prompt": "You are evaluating if the candidate produced a properly structured one-page PDF summary of constituent comments by district. Only check structure and presence (not content quality or correctness).\n\nRequirements (be flexible with exact phrasing of headers, but structure must be clear):\n- Format: PDF (not DOCX/Excel). Exactly one page.\n- Professional title on top including ECID and a summary descriptor (e.g., \"ECID Constituent Feedback Summary\"), and a date.\n- A short methodology/source note referencing the ECID Constituent Feedback Tracking Log (or equivalent wording like \"tracking log\").\n- Four district sections (district names may vary, e.g., \"District 1\"/\"North\"/official names). For each district section:\n  \u2022 2\u20133 bullet points summarizing key themes.\n  \u2022 At least one simple metric (e.g., count of comments, % of mentions).\n  \u2022 Optional: a representative short quote in quotation marks or paraphrased evidence (do not penalize if absent when bullets+metrics are present).\n- An \"Overall Themes\" or \"Cross-Cutting Issues\" section.\n- A short \"Actions/Next Steps\" section (3\u20135 bullets).\n- Footer or small note with contact/author.\n\nScoring:\n- 1.0: All required elements present; clearly one page.\n- 0.7: Minor omissions (e.g., missing quote, or contact line) but core sections (title/date, methodology/source note, 4 district sections with bullets+some metric, overall themes, next steps) are present and it fits on one page.\n- 0.4: Several required sections present but missing one or more core elements (e.g., only 2\u20133 districts covered OR no methodology/source note). Still a PDF.\n- 0.0: Not a PDF, not one page, or lacks multiple core sections.\n\nReturn only a numeric score according to the weight.", "expectation": "A single-page PDF with clear per-district sections (4 total), source note, overall themes, and next steps."}, {"type": "llm_judge", "name": "GATE: Board-Meeting Talking Points PDF \u2014 Required Structure", "description": "Verify there is a PDF of talking points suitable for use during the board meeting, with required sections.", "weight": 3.0, "judge_prompt": "Evaluate whether a second PDF exists that serves as board-meeting talking points about constituent concerns. Only check presence/format/sections.\n\nRequirements (naming can vary, e.g., \"Talking Points\" / \"Board Meeting Brief\"):\n- Format: PDF, typically 1\u20132 pages (up to 3 acceptable if concise).\n- Clear header or title indicating Talking Points/Board Meeting.\n- Opening context/frame (purpose, agenda slot, or what the board needs to know).\n- Key messages organized by district and/or cross-cutting themes.\n- A short data snapshot (e.g., counts or simple metrics) aligned with the summary.\n- Q&A or Anticipated Questions section with at least 4 Q/A pairs (or equivalent structured prompts/responses).\n- Recommended responses/mitigations and Next Steps for staff (owners or timeframes optional but preferred).\n\nScoring:\n- 1.0: All listed sections present; PDF length within 1\u20132 pages (3 acceptable if concise).\n- 0.7: Missing one secondary element (e.g., owners/timeframes) but core sections exist.\n- 0.4: Several sections present but missing multiple core elements (e.g., no Q&A or no key messages by district/themes). Still a PDF.\n- 0.0: Not a PDF or mostly missing required sections.\n\nReturn only a numeric score according to the weight.", "expectation": "A concise PDF with talking points: title, context, per-district/cross-cutting key messages, data snapshot, Q&A, and next steps."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness & Consistency)", "description": "Now that the structure exists, verify correctness and internal consistency across both PDFs with a mix of deterministic code checks and higher-level LLM judgment.", "is_required": false, "max_points": 20.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Two PDFs Present", "description": "Verify that at least two PDFs were produced among the outputs.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs() or []\n        pdfs = []\n        for r in outputs:\n            try:\n                text = context.files.read_pdf_text(r.id)\n                if text and isinstance(text, str) and text.strip():\n                    pdfs.append((r, text))\n            except Exception:\n                continue\n        n = len(pdfs)\n        if n >= 2:\n            return (1.0, f\"Found {n} PDF(s).\")\n        elif n == 1:\n            return (0.5, \"Only one PDF found; expected two (summary + talking points).\")\n        else:\n            return (0.0, \"No PDFs found among outputs.\")\n    except Exception as e:\n        return (0.0, f\"Error checking PDFs: {e}\")"}, {"type": "code", "name": "Summary PDF: District Coverage and Metrics Tokens", "description": "Heuristically identify the summary PDF and check for at least four district labels and presence of simple metrics related to comments/feedback, plus a source note referencing the tracking log.", "weight": 1.0, "code": "import re\n\ndef _get_pdfs(context):\n    pdfs = []\n    for r in context.get_all_outputs() or []:\n        try:\n            text = context.files.read_pdf_text(r.id)\n            if text and isinstance(text, str) and text.strip():\n                pdfs.append((r, text))\n        except Exception:\n            pass\n    return pdfs\n\ndef _pick_summary_and_talking(pdfs):\n    scored = []\n    for r, t in pdfs:\n        lt = t.lower()\n        s_score = sum(k in lt for k in [\"summary\", \"constituent feedback\", \"overview\"]) \n        tp_score = sum(k in lt for k in [\"talking points\", \"q&a\", \"questions\", \"board meeting\"])\n        scored.append((r, t, s_score, tp_score, len(t)))\n    summary = None\n    talking = None\n    # Prefer explicit markers\n    cand_summary = [x for x in scored if x[2] > x[3]]\n    cand_talking = [x for x in scored if x[3] > x[2]]\n    if cand_summary:\n        summary = max(cand_summary, key=lambda x: (x[2], -x[4]))\n    if cand_talking:\n        talking = max(cand_talking, key=lambda x: (x[3], x[4]))\n    # Fallbacks\n    if not summary and scored:\n        summary = min(scored, key=lambda x: x[4])  # shorter likely the one-pager\n    if not talking and scored:\n        others = [x for x in scored if x != summary]\n        talking = max(others, key=lambda x: x[4]) if others else summary\n    return (summary[1] if summary else \"\"), (talking[1] if talking else \"\")\n\ndef evaluate(workflow, context):\n    try:\n        pdfs = _get_pdfs(context)\n        if not pdfs:\n            return (0.0, \"No PDFs available to analyze.\")\n        summary_text, _ = _pick_summary_and_talking(pdfs)\n        if not summary_text:\n            return (0.0, \"Could not identify a summary PDF.\")\n        lt = summary_text.lower()\n        # District label extraction\n        labels = set(m.group(0).lower() for m in re.finditer(r\"\\bdistrict\\s+[a-z0-9\\-]+\", lt))\n        district_score = min(len(labels) / 4.0, 1.0)\n        # Metrics near comments/feedback\n        has_metric = bool(re.search(r\"\\b\\d{1,3}(?:,\\d{3})*\\b\\s+(?:comments?|feedback|issues|cases|mentions)\", lt))\n        metric_score = 1.0 if has_metric else 0.0\n        # Source note\n        has_source = (\"tracking log\" in lt) or (\"constitutent feedback tracking log\" in lt) or (\"constituent feedback tracking log\" in lt) or (\"ecid\" in lt and \"log\" in lt and \"tracking\" in lt)\n        source_score = 1.0 if has_source else 0.0\n        score = (district_score + metric_score + source_score) / 3.0\n        fb = f\"District labels found: {len(labels)}; metric token: {has_metric}; source note: {has_source}.\"\n        return (float(score), fb)\n    except Exception as e:\n        return (0.0, f\"Error analyzing summary PDF: {e}\")"}, {"type": "code", "name": "Talking Points PDF: Q&A and Next Steps Tokens", "description": "Heuristically identify the talking-points PDF and check for talking points markers, Q&A presence, and next steps/recommendations markers.", "weight": 1.0, "code": "import re\n\ndef _get_pdfs(context):\n    pdfs = []\n    for r in context.get_all_outputs() or []:\n        try:\n            text = context.files.read_pdf_text(r.id)\n            if text and isinstance(text, str) and text.strip():\n                pdfs.append((r, text))\n        except Exception:\n            pass\n    return pdfs\n\ndef _pick_summary_and_talking(pdfs):\n    scored = []\n    for r, t in pdfs:\n        lt = t.lower()\n        s_score = sum(k in lt for k in [\"summary\", \"constituent feedback\", \"overview\"]) \n        tp_score = sum(k in lt for k in [\"talking points\", \"q&a\", \"questions\", \"board meeting\"])\n        scored.append((r, t, s_score, tp_score, len(t)))\n    summary = None\n    talking = None\n    cand_summary = [x for x in scored if x[2] > x[3]]\n    cand_talking = [x for x in scored if x[3] > x[2]]\n    if cand_summary:\n        summary = max(cand_summary, key=lambda x: (x[2], -x[4]))\n    if cand_talking:\n        talking = max(cand_talking, key=lambda x: (x[3], x[4]))\n    if not summary and scored:\n        summary = min(scored, key=lambda x: x[4])\n    if not talking and scored:\n        others = [x for x in scored if x != summary]\n        talking = max(others, key=lambda x: x[4]) if others else summary\n    return (summary[1] if summary else \"\"), (talking[1] if talking else \"\")\n\ndef evaluate(workflow, context):\n    try:\n        pdfs = _get_pdfs(context)\n        if not pdfs:\n            return (0.0, \"No PDFs available.\")\n        _, talking_text = _pick_summary_and_talking(pdfs)\n        if not talking_text:\n            return (0.0, \"Could not identify a talking-points PDF.\")\n        lt = talking_text.lower()\n        markers = 0\n        # Talking points marker\n        if (\"talking points\" in lt) or (\"board meeting\" in lt) or (\"brief\" in lt):\n            markers += 1\n        # Q&A presence\n        qa_present = (lt.count(\"q:\") >= 1 and lt.count(\"a:\") >= 1) or (lt.count(\"?\") >= 3) or (\"q&a\" in lt)\n        if qa_present:\n            markers += 1\n        # Next steps / Recommendations marker\n        if (\"next steps\" in lt) or (\"recommendations\" in lt) or (\"actions\" in lt):\n            markers += 1\n        score = markers / 3.0\n        return (float(score), f\"Markers present: {markers}/3; Q&A present: {qa_present}.\")\n    except Exception as e:\n        return (0.0, f\"Error analyzing talking points: {e}\")"}, {"type": "code", "name": "Cross-Document District Alignment", "description": "Check that district labels appearing in the summary also appear in the talking points, indicating alignment.", "weight": 1.0, "code": "import re\n\ndef _get_pdfs(context):\n    pdfs = []\n    for r in context.get_all_outputs() or []:\n        try:\n            text = context.files.read_pdf_text(r.id)\n            if text and isinstance(text, str) and text.strip():\n                pdfs.append((r, text))\n        except Exception:\n            pass\n    return pdfs\n\ndef _pick_summary_and_talking(pdfs):\n    scored = []\n    for r, t in pdfs:\n        lt = t.lower()\n        s_score = sum(k in lt for k in [\"summary\", \"constituent feedback\", \"overview\"]) \n        tp_score = sum(k in lt for k in [\"talking points\", \"q&a\", \"questions\", \"board meeting\"])\n        scored.append((r, t, s_score, tp_score, len(t)))\n    summary = None\n    talking = None\n    cand_summary = [x for x in scored if x[2] > x[3]]\n    cand_talking = [x for x in scored if x[3] > x[2]]\n    if cand_summary:\n        summary = max(cand_summary, key=lambda x: (x[2], -x[4]))\n    if cand_talking:\n        talking = max(cand_talking, key=lambda x: (x[3], x[4]))\n    if not summary and scored:\n        summary = min(scored, key=lambda x: x[4])\n    if not talking and scored:\n        others = [x for x in scored if x != summary]\n        talking = max(others, key=lambda x: x[4]) if others else summary\n    return (summary[1] if summary else \"\"), (talking[1] if talking else \"\")\n\ndef _extract_labels(text):\n    lt = (text or \"\").lower()\n    return set(m.group(0).lower() for m in re.finditer(r\"\\bdistrict\\s+[a-z0-9\\-]+\", lt))\n\ndef evaluate(workflow, context):\n    try:\n        pdfs = _get_pdfs(context)\n        if not pdfs:\n            return (0.0, \"No PDFs available.\")\n        summary_text, talking_text = _pick_summary_and_talking(pdfs)\n        if not (summary_text and talking_text):\n            return (0.0, \"Could not identify both summary and talking points.\")\n        s_labels = _extract_labels(summary_text)\n        t_labels = _extract_labels(talking_text)\n        inter = len(s_labels.intersection(t_labels))\n        score = min(inter / 4.0, 1.0)  # aim for 4 distinct district references\n        return (float(score), f\"Common district labels: {inter}; summary={len(s_labels)}; talking={len(t_labels)}.\")\n    except Exception as e:\n        return (0.0, f\"Error checking cross-document alignment: {e}\")"}, {"type": "llm_judge", "name": "Per-District Coverage and Counts Check", "description": "Verify that the summary provides coverage for each of the four districts with clear themes and at least minimal quantitative indication (counts/metrics), and that the talking points reflect those districts.", "weight": 6.0, "judge_prompt": "Using all available outputs, verify the following correctness aspects:\n- The one-page summary includes coverage for each of the four districts (flexible naming allowed), with clear per-district themes. Each district should have at least minimal quantitative indication (e.g., number of comments, share of mentions, or similar).\n- The talking-points PDF reflects the same set of districts (naming may vary but should correspond) and the same general themes.\n\nScoring:\n- 1.0: All four districts are clearly covered in the summary with themes AND a minimal metric per district; the talking points reflect these same districts.\n- 0.7: Three districts fully covered; the fourth is present but missing a metric OR is implied. Talking points mostly reflect the same districts.\n- 0.4: Only two districts have clear coverage/metrics; others are weak.\n- 0.0: District-level coverage is largely missing or non-determinable.\n\nReturn a numeric score only (normalized to this rule\u2019s weight).", "expectation": "Four districts with themes and simple per-district metrics in the summary; the talking points mirror those districts."}, {"type": "llm_judge", "name": "Source Traceability to Tracking Log", "description": "Check that the documents clearly cite the ECID Constituent Feedback Tracking Log (or equivalent wording) as the source/method and provide enough context to trace the data.", "weight": 5.0, "judge_prompt": "Verify that the one-page summary (and preferably also the talking points) includes a methodology/source note that clearly references the ECID Constituent Feedback Tracking Log or an equivalent phrasing (e.g., \"ECID tracking log\"). The goal is traceability, not auditing the actual numbers.\n\nScoring:\n- 1.0: Clear, explicit source/method note in the summary referencing the tracking log; talking points also mention the source or indicate that data align with the summary.\n- 0.6: Clear source in the summary only; talking points do not repeat it.\n- 0.3: Implicit or vague reference to data without clear mention of the log.\n- 0.0: No source/method reference.\n\nReturn a numeric score only.", "expectation": "A clear source note naming the ECID Constituent Feedback Tracking Log (or equivalent wording) for traceability."}, {"type": "llm_judge", "name": "Cross-Document Consistency (Numbers & Themes)", "description": "Ensure that key numbers/themes in the talking points align with the one-page summary (no obvious contradictions).", "weight": 5.0, "judge_prompt": "Compare the one-page summary and the talking points:\n- Do the themes emphasized in the talking points clearly align with those in the summary?\n- If any counts/totals or simple metrics appear in both documents, do they match (or at least do not contradict each other)?\n- Are any district names or categorizations inconsistent or conflicting between the two documents?\n\nScoring:\n- 1.0: Themes and any repeated numbers align across both documents; no contradictions.\n- 0.6: Minor inconsistencies in phrasing but overall alignment.\n- 0.3: Noticeable inconsistencies (e.g., a district emphasized differently with conflicting figures).\n- 0.0: Major contradictions or mismatched districts/numbers.\n\nReturn a numeric score only.", "expectation": "Talking points should echo and not contradict the summary\u2019s districts, themes, and any reported numbers."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality & Fitness for Purpose", "description": "Holistic assessment of professionalism, clarity, and actionability for a board audience.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation & Tone", "description": "Assess formatting, readability, and professional tone appropriate for ECID board materials.", "weight": 3.0, "judge_prompt": "Assess whether both PDFs are professionally presented (clear headings, readable typography/spacing), and whether the tone is neutral, respectful, and suitable for an ECID board audience. Ignore minor typos; focus on overall professionalism.\n\nScore:\n- 1.0: Professional formatting and tone throughout.\n- 0.6: Generally professional with small issues.\n- 0.3: Noticeable formatting or tone issues.\n- 0.0: Unprofessional or hard to read.\n\nReturn a numeric score only.", "expectation": "Clean, structured, and neutral documents appropriate for board review."}, {"type": "llm_judge", "name": "Clarity & Conciseness (One-Page Summary)", "description": "Evaluate whether the summary communicates clearly and concisely within one page, avoiding jargon and dense blocks of text.", "weight": 3.0, "judge_prompt": "Evaluate the one-page summary for clarity and conciseness: Are the sections scannable with effective bullets? Is language plain and accessible? Avoids unnecessary jargon or long paragraphs? Remains within a one-page footprint without feeling cramped.\n\nScore:\n- 1.0: Very clear, concise, and scannable.\n- 0.6: Mostly clear with minor verbosity.\n- 0.3: Somewhat unclear or too verbose/dense.\n- 0.0: Hard to follow.\n\nReturn a numeric score only.", "expectation": "A crisp, scannable one-pager with plain language."}, {"type": "llm_judge", "name": "Actionability for Board Discussion", "description": "Do the materials provide concrete direction for the board and staff (priorities, next steps, owners/timelines when appropriate)?", "weight": 3.0, "judge_prompt": "Assess the actionability of the materials: Do they translate constituent concerns into clear priorities and next steps suitable for a board meeting? Are responsibilities, suggested owners, or time frames indicated where helpful? Are recommendations practical and relevant to ECID operations?\n\nScore:\n- 1.0: Clear, actionable next steps with ownership/timing where appropriate.\n- 0.6: Generally actionable but light on specifics.\n- 0.3: Vague recommendations.\n- 0.0: Little to no actionability.\n\nReturn a numeric score only.", "expectation": "Specific, pragmatic actions and talking points that guide the board conversation."}, {"type": "llm_judge", "name": "Balance, Fairness, and Confidentiality", "description": "Ensure the materials balance interests across districts, avoid bias, and do not expose PII in quotes or examples.", "weight": 3.0, "judge_prompt": "Evaluate whether the documents fairly represent concerns across all districts without undue bias, and avoid personally identifiable information (e.g., full names, addresses) in any quotes or examples. Check that sensitive details are anonymized or generalized.\n\nScore:\n- 1.0: Balanced, fair, and privacy-conscious.\n- 0.6: Minor balance or privacy issues.\n- 0.3: Notable bias or risky specificity.\n- 0.0: Clear bias or PII exposure.\n\nReturn a numeric score only.", "expectation": "Fair, neutral framing with anonymized examples and no PII."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "f84ea6ac-8f9f-428c-b96c-d0884e30f7c7", "rubric": {"category_name": "Research Summary Table \u2013 AI/Automation in Government Administrative Services", "rationale": "Task Type: Document (Pattern B) with a structured, one-page comparison table in a Word/PDF document. Stage 1 uses an LLM judge only to strictly enforce file format and table structure so later verification is trivial. Stage 2 mixes small, resilient code checks (URLs present, post-2020 years detected) with heavier LLM verification of topical relevance, completeness of fields, public accessibility, and recency. Stage 3 uses LLM-only quality criteria to assess professional presentation, clarity, strategic utility, and comparability for decision-making.", "max_total_score": 19.0, "stages": [{"name": "Stage 1 \u2013 Shape Enforcement Gate (Document/Table Structure)", "description": "Gate that enforces exact output structure so verification is possible. Must be a one-page DOCX or PDF containing a single main comparison table with specified columns and minimum five studies.", "is_required": true, "max_points": 1.0, "min_score_to_pass": 0.7, "rules": [{"type": "llm_judge", "name": "Document Format and Table Structure Requirement", "description": "Verify the file is a one-page DOCX/PDF containing a single main table with required columns and row contents for five studies.", "weight": 1.0, "judge_prompt": "You are evaluating only SHAPE/FORMAT, not content quality. Inspect the primary output (DOCX or PDF) and check these exact structural requirements:\n\nFormat requirements:\n- Must be a DOCX or PDF (not spreadsheet, not plain text).\n- The main content must fit on ONE page. A short title/header is acceptable; avoid multi-page content.\n- The one-page output must primarily consist of a single main comparison table.\n\nTable requirements:\n- The table must have at least 3 visible column headers approximating:\n  1) \"Study Information\"\n  2) \"Key Findings\"\n  3) \"Implications for Government\"\n  Accept minor variants like \"Study Details\" / \"Implications for Gov\u2019t\" / \"Policy Implications\".\n- The table must have at least 5 rows (one per study/article).\n- Cells for \"Key Findings\" and \"Implications for Government\" should use point-form/bullets (concise phrases, not long paragraphs).\n\nPer-row Study Information (verify presence, not correctness):\n- Title\n- Author(s)\n- Year in YYYY format\n- Setting or region (e.g., country/agency context)\n- Study goal(s)\n- A public URL (full http/https link) to the article or open source page\n\nScoring:\n- 1.0: Valid DOCX/PDF; single-page; one primary table with the 3 columns (or close synonyms); \u22655 rows; each row\u2019s Study Information contains all listed subfields; findings/implications are bullet/point-form.\n- 0.85: As above but one minor omission (e.g., a single row missing a subfield, or minor overflow in text density but still clearly one-page).\n- 0.7: Mostly correct structure (valid DOCX/PDF, one page, table has the 3 columns) but with multiple minor omissions (e.g., a couple rows missing one subfield each) OR exactly 5 rows but one column header is a near synonym that\u2019s still clearly equivalent.\n- 0.0: Wrong format (not DOCX/PDF); not a single page; lacks the main table; fewer than 5 studies; or missing multiple core elements. Return 0.0 in these cases.\n\nOnly evaluate presence/structure, not the accuracy of content or links.", "expectation": "A one-page DOCX/PDF with a single comparison table having 3 columns (Study Information, Key Findings, Implications for Government) and at least 5 rows. Each Study Information cell includes title, authors, year, setting, goals, and a public URL. Bulleted/point-form content in findings and implications."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness Verification (Content and Compliance)", "description": "Verify that the table truly summarizes five relevant, recent, public articles and that each row contains complete and appropriate information aligned to the task.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Public URLs Present (Heuristic)", "description": "Checks the document text for presence of at least five distinct http(s) URLs, suggesting each study is publicly findable online.", "weight": 0.8, "code": "import re\\n\\n\\ndef evaluate(workflow, context):\\n    \\n    output = context.get_primary_output()\\n    if not output:\\n        return 0.0, \"No output found.\"\\n    if not output.is_document:\\n        return 0.0, \"Primary output is not a document (DOCX/PDF).\"\\n\\n    text = \"\"\\n    # Try DOCX first\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        text = \"\"\\n    # Fallback to PDF\\n    if not text:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            text = \"\"\\n    # Fallback to generic text (e.g., markdown)\\n    if not text:\\n        try:\\n            text = context.files.read_text(output.id)\\n        except Exception:\\n            text = \"\"\\n    if not text:\\n        return 0.0, \"Unable to extract text from document.\"\\n\\n    urls = re.findall(r'https?://[^\\s\\]\\)>,;]+', text)\\n    # Clean common trailing punctuation\\n    urls = [u.rstrip('.,);]') for u in urls]\\n    unique_urls = set([u for u in urls if len(u) > 8])\\n    count = len(unique_urls)\\n\\n    frac = min(count / 5.0, 1.0)\\n    return frac, f\"Detected {count} unique URL(s).\""}, {"type": "code", "name": "Post-2020 Year Detection (Heuristic)", "description": "Parses text to detect publication years and rewards if five or more instances of 2021+ are present, aligning with the 'published after 2020' requirement.", "weight": 1.2, "code": "import re\\n\\n\\ndef evaluate(workflow, context):\\n    \\n    output = context.get_primary_output()\\n    if not output:\\n        return 0.0, \"No output found.\"\\n    if not output.is_document:\\n        return 0.0, \"Primary output is not a document (DOCX/PDF).\"\\n\\n    text = \"\"\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        text = \"\"\\n    if not text:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            text = \"\"\\n    if not text:\\n        try:\\n            text = context.files.read_text(output.id)\\n        except Exception:\\n            text = \"\"\\n    if not text:\\n        return 0.0, \"Unable to extract text from document.\"\\n\\n    # Look for 4-digit years; then count years >= 2021\\n    years = re.findall(r'(?<!\\d)(20\\d{2})(?!\\d)', text)\\n    yrs = [int(y) for y in years]\\n    recent = [y for y in yrs if y >= 2021]\\n    count_recent = len(recent)\\n\\n    frac = min(count_recent / 5.0, 1.0)\\n    return frac, f\"Detected {count_recent} year mentions for 2021+ (total years found: {len(yrs)}).\""}, {"type": "llm_judge", "name": "Topical Relevance and Government Admin Focus", "description": "Confirm that exactly five studies are summarized and that each is clearly about AI/automation in government with relevance to administrative service jobs/functions.", "weight": 2.5, "judge_prompt": "Evaluate the document\u2019s table for topical correctness. Check these items:\\n1) Exactly five studies are included (five rows).\\n2) Each study is clearly about AI and/or automation in the public sector/government context.\\n3) The focus is relevant to administrative service jobs/functions (e.g., clerical, back-office processing, service delivery administration, digital workflows).\\n\\nScoring:\\n- 2.5: Exactly five rows; all five are clearly government/public sector; all address AI/automation with relevance to admin service functions.\\n- 1.5: Five rows but 1 study is weakly related or tangential (e.g., broad e-government without clear AI/automation link, or focuses on non-admin roles).\\n- 0.5: Five rows but 2 studies are off-topic or private-sector only.\\n- 0.0: Fewer than five studies OR majority not about AI/automation in government OR not relevant to administrative functions.", "expectation": "Five government-focused, AI/automation-relevant studies clearly tied to administrative services functions."}, {"type": "llm_judge", "name": "Field Completeness per Row", "description": "Verify each row contains the required Study Information subfields, plus concise bullets for Key Findings and Implications.", "weight": 2.5, "judge_prompt": "For each of the five rows in the table, check that the following are present in the appropriate columns:\\n- Study Information includes: Title, Author(s), Year (YYYY), Setting/Region, Goals, and a public URL.\\n- Key Findings: concise point-form bullets capturing the study\u2019s main results relevant to admin services.\\n- Implications for Government: concise point-form bullets that logically follow from findings.\\n\\nScoring:\\n- 2.5: All five rows include all Study Information subfields and have bullet-point Findings and Implications.\\n- 1.5: Minor omissions (e.g., up to two missing subfields across the entire table) but overall clearly complete.\\n- 0.5: Multiple omissions (e.g., three to five missing subfields across rows) or prose paragraphs instead of bullets in several cells.\\n- 0.0: Systemic missing fields (e.g., most rows missing key subfields) or the columns are not used as intended.", "expectation": "All five rows have complete Study Information (title, authors, year, setting, goals, URL) and bulleted Key Findings and Implications."}, {"type": "llm_judge", "name": "Public Accessibility (Plausibility)", "description": "Assess whether links/domains appear publicly accessible (not paywalled) and that sources seem reachable via open web.", "weight": 1.5, "judge_prompt": "Review the links provided for each study. Judge plausibility of public accessibility using visible URLs/domains (e.g., government sites, university repositories, open-access journals, preprints). You are not required to click, but assess whether the links appear to be open-web accessible as presented.\\n\\nScoring:\\n- 1.5: All five studies include a direct URL that appears publicly accessible (e.g., open-access journal pages, PDF links, .gov/.edu repositories, arXiv/SSRN where applicable).\\n- 0.8: One study\u2019s link seems likely paywalled or indirect (publisher landing page without OA indication).\\n- 0.3: Two studies seem non-public or only via paywalled publishers.\\n- 0.0: Most links missing or clearly paywalled-only references.", "expectation": "Each study includes a direct, plausibly public URL (gov, edu, OA journal, or repository)."}, {"type": "llm_judge", "name": "Recency (Post-2020) and Publication Details", "description": "Confirm that each study is published after 2020 (2021 or later) and includes adequate bibliographic cues (title, authors, year).", "weight": 1.5, "judge_prompt": "Check whether the five studies are published after 2020 (i.e., 2021 or later). Also confirm basic bibliographic cues are shown (title, author, year).\\n\\nScoring:\\n- 1.5: All five are 2021 or later and each has clear title/author/year.\\n- 0.8: One study lacks a clear year or is 2020; others are compliant.\\n- 0.3: Two studies lack years or pre-2021 dates.\\n- 0.0: Three or more do not meet the 2021+ requirement or lack years.", "expectation": "All five studies should indicate year 2021+ with basic bibliographic information."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Quality and Strategic Utility Assessment", "description": "Holistic evaluation of professional presentation, clarity, comparability, and strategic usefulness for government administrative services planning.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Clarity", "description": "Assess one-page layout, readability, consistent formatting, and concise point-form writing.", "weight": 2.0, "judge_prompt": "Evaluate presentation quality: Is the content clearly laid out on one page, with consistent fonts, spacing, and alignment? Are bullets concise, scannable, and free of long paragraphs? Are column widths and row heights reasonable so text isn\u2019t cramped or overflowing?\\n\\nScoring:\\n- 2.0: Highly professional one-page presentation; concise bullets; clean, consistent formatting.\\n- 1.2: Generally clear with minor formatting issues or slightly dense text.\\n- 0.6: Readable but several formatting issues or overly verbose bullets.\\n- 0.0: Poorly formatted, hard to read, or not effectively one page.", "expectation": "Clean, consistent one-page table with concise, scannable bullets."}, {"type": "llm_judge", "name": "Comparability and Structure Consistency", "description": "Judge whether the table supports easy cross-study comparison with consistent fields and terminology.", "weight": 2.0, "judge_prompt": "Assess whether the table enables easy comparison across the five studies: Are the fields consistent across rows? Are similar concepts described with consistent terminology? Do Study Information cells include setting and goals in a comparable manner?\\n\\nScoring:\\n- 2.0: Strong comparability; consistent fields and terminology; rows align well.\\n- 1.2: Mostly comparable with minor inconsistencies.\\n- 0.6: Noticeable inconsistencies that hinder comparison.\\n- 0.0: Disorganized; difficult to compare across studies.", "expectation": "Uniform, comparable entries that make cross-study scanning easy."}, {"type": "llm_judge", "name": "Strategic Usefulness for Government Planning", "description": "Evaluate whether the Implications are actionable and tailored to administrative services planning (skills, process redesign, risks, change management).", "weight": 2.0, "judge_prompt": "Review the Implications for Government for specificity and actionability in the administrative services context. Look for items such as: workforce skill shifts (e.g., data literacy), process redesign/automation opportunities, service quality metrics, risk/ethics/governance, change management/communications, procurement considerations.\\n\\nScoring:\\n- 2.0: Clear, actionable implications tied to admin services planning; meaningful for a 3\u20135 year strategy.\\n- 1.2: Generally useful but some items are vague or generic.\\n- 0.6: Mostly generic or high-level statements with limited applicability.\\n- 0.0: Little to no strategic value for planning.", "expectation": "Actionable implications that inform staffing, process, governance, and implementation planning."}, {"type": "llm_judge", "name": "Accuracy, Balance, and Nuance", "description": "Judge whether the summary reflects the studies faithfully with balanced pros/cons, avoiding overclaiming.", "weight": 2.0, "judge_prompt": "Evaluate whether the Key Findings and Implications appear faithful to the cited studies (as described) and exhibit balance (benefits, limitations, risks). Avoids hype or overclaiming; acknowledges context (data quality, privacy, bias, change risks).\\n\\nScoring:\\n- 2.0: Balanced, accurate reflection with appropriate caveats.\\n- 1.2: Mostly accurate with minor overstatements or missing caveats.\\n- 0.6: Noticeable overclaiming or simplistic takeaways.\\n- 0.0: Misleading or clearly inaccurate summaries.", "expectation": "Balanced, credible synthesis reflecting both opportunities and risks."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "0112fc9b-c3b2-4084-8993-5a4abb1f54f1", "rubric": {"category_name": "Pediatric SOAP Note (Head Injury) - Structure, Correctness, and Quality", "rationale": "This rubric enforces a self-documenting SOAP note for a pediatric head injury visit. Stage 1 (LLM-only) strictly mandates the document structure and sections to enable automated verification. Stage 2 mixes lightweight code checks (vitals plausibility and key elements) with LLM judgments for clinical correctness aligned to adolescent head injury best practices. Stage 3 assesses overall professional quality, clarity, and appropriateness for a 16-year-old patient. Code rules are intentionally narrower in weight than LLM rules to reflect nuanced clinical reasoning evaluation by LLM judges.", "max_total_score": 20.0, "stages": [{"name": "Stage 1: Shape Enforcement Gate (SOAP Structure)", "description": "Confirm the output is a properly structured SOAP note document with required sections and subcomponents that enable verification. LLM-only gate.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 2.0, "rules": [{"type": "llm_judge", "name": "SOAP Structure and Format Compliance", "description": "Verify the candidate output is a valid document file (PDF, DOCX, or Markdown) formatted as a SOAP note with required headers and sub-sections for this case.", "weight": 4.0, "judge_prompt": "You are evaluating whether the submitted output is a properly structured SOAP note for a 16-year-old male head injury visit (date 3/1/2024). Only assess structure/format\u2014not clinical correctness.\n\nAcceptable file formats: PDF, DOCX, or Markdown (.md).\n\nRequired overall structure (be flexible with minor header name variations, e.g., 'S:', 'O:', 'A:', 'P:'):\n1) Clear top header with patient identifiers:\n   - Patient initials (C.S.)\n   - Age (16)\n   - Sex (male)\n   - Date of visit (3/1/2024)\n\n2) Subjective section (\"Subjective\" or \"S\") with labeled subsections:\n   - Chief Complaint (CC)\n   - History of Present Illness (HPI)\n   - Review of Systems (ROS) including at least Neurologic and HEENT elements\n   - Histories: PMH/PSH, Medications, Allergies, Family History, Social History (PMH and PSH may be combined; require at least four of these five labeled items)\n\n3) Objective section (\"Objective\" or \"O\"):\n   - Vitals listed with numeric values (e.g., Temp, HR, BP, RR, Height, Weight)\n   - Physical Exam with subsections covering at least Neurologic and HEENT; other systems as applicable\n\n4) Assessment (\"Assessment\" or \"A\"):\n   - Primary diagnosis statement\n   - At least two differential diagnoses listed (a short differential is acceptable)\n\n5) Plan (\"Plan\" or \"P\") with labeled components:\n   - Diagnostics/Testing (e.g., imaging vs. observation)\n   - Treatment/Medications (symptomatic care acceptable)\n   - Patient/Family Education\n   - Return/ER precautions (red flags)\n   - Follow-up with a timeframe\n\nOptional (do not penalize if missing): coding (ICD-10/CPT), school/sports note, signature block.\n\nScoring:\n- 4.0: All five major sections present and properly labeled; Subjective has CC, HPI, ROS, and \u22654 of the five histories; Objective has vitals with numeric values and PE including Neurologic + HEENT; Assessment shows a primary diagnosis + \u22652 differentials; Plan includes all five components.\n- 3.0: All SOAP sections present and labeled, but missing exactly one minor subcomponent (e.g., only three of the five histories, or Plan missing exactly one component).\n- 2.0: SOAP sections present and labeled, but missing two or more required subcomponents (e.g., no ROS or no vitals, or Plan missing two elements).\n- 0.0: Not a PDF/DOCX/Markdown document OR one of the main SOAP sections (S/O/A/P) is missing entirely.\n\nOnly check structure and labeled presence. Do not judge medical accuracy or thoroughness beyond structural completeness.", "expectation": "A well-formatted SOAP note document with clearly labeled S/O/A/P sections and prescribed subcomponents present to enable automated verification."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Correctness and Clinical Verification", "description": "Verify clinical documentation accuracy and appropriateness against the case. Mix of code checks for concrete items and LLM checks for clinical reasoning and guideline alignment.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Vitals Present and Plausible", "description": "Detect common vital signs in the document text and check they fall within plausible adolescent ranges. Partial credit for number found and plausible.", "weight": 0.8, "code": "import re\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    # Read text from PDF/DOCX/MD\n    text = ''\n    try:\n        if output.is_document:\n            # Try PDF then DOCX\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    text = ''\n        if not text and output.is_text_format:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = ''\n    except Exception:\n        text = ''\n    if not text:\n        return 0.0\n    t = ' '.join(text.split()).lower()\n\n    found = 0\n    total_considered = 0\n\n    # Temperature (assume Fahrenheit if no unit)\n    total_considered += 1\n    m = re.search(r'(temp(?:erature)?[:\\s]*)(\\d{2,3}(?:\\.\\d)?)', t)\n    if m:\n        try:\n            val = float(m.group(2))\n            if 95.0 <= val <= 103.0:\n                found += 1\n        except Exception:\n            pass\n\n    # HR\n    total_considered += 1\n    m = re.search(r'\\b(hr|heart rate)[:\\s]*([0-9]{2,3})\\b', t)\n    if m:\n        try:\n            val = int(m.group(2))\n            if 40 <= val <= 160:\n                found += 1\n        except Exception:\n            pass\n\n    # BP\n    total_considered += 1\n    m = re.search(r'\\b(bp|blood pressure)[:\\s]*(\\d{2,3})\\s*/\\s*(\\d{2,3})\\b', t)\n    if m:\n        try:\n            sys = int(m.group(2)); dia = int(m.group(3))\n            if 80 <= sys <= 160 and 40 <= dia <= 110:\n                found += 1\n        except Exception:\n            pass\n\n    # RR\n    total_considered += 1\n    m = re.search(r'\\b(rr|resp(?:iratory)? rate)[:\\s]*([0-9]{1,2})\\b', t)\n    if m:\n        try:\n            val = int(m.group(2))\n            if 8 <= val <= 30:\n                found += 1\n        except Exception:\n            pass\n\n    # Weight (kg or lbs). If lbs, accept 66-330; if kg, 30-150; if unitless, accept 30-150.\n    total_considered += 1\n    m = re.search(r'\\b(weight)[:\\s]*([0-9]{2,3}(?:\\.\\d+)?)\\s*(kg|kilograms|lbs|pounds)?\\b', t)\n    if m:\n        try:\n            val = float(m.group(2))\n            unit = m.group(3) or ''\n            unit = unit.lower()\n            if unit in ['kg', 'kilograms', '']:\n                if 30 <= val <= 150:\n                    found += 1\n            elif unit in ['lbs', 'pounds']:\n                if 66 <= val <= 330:\n                    found += 1\n        except Exception:\n            pass\n\n    # Height (ft/in or cm)\n    total_considered += 1\n    ok = False\n    m = re.search(r'\\b(height)[:\\s]*([4-7])\\s*[\\'\\'\u2032]\\s*([0-9]{1,2})\\b', t)\n    if m:\n        try:\n            ft = int(m.group(2)); inch = int(m.group(3))\n            if 4 <= ft <= 7 and 0 <= inch <= 11:\n                ok = True\n        except Exception:\n            pass\n    else:\n        m = re.search(r'\\b(height)[:\\s]*([1-2][0-9]{2})\\s*cm\\b', t)\n        if m:\n            try:\n                cm = int(m.group(2))\n                if 120 <= cm <= 210:\n                    ok = True\n            except Exception:\n                pass\n    if ok:\n        found += 1\n\n    # Score: give credit for up to 5 items; allow one missing without severe penalty\n    score = min(1.0, found / 5.0)\n    return max(0.0, min(1.0, score))"}, {"type": "code", "name": "Neurologic Exam Elements Present", "description": "Check for key neurologic exam documentation elements (orientation, cranial nerves, pupils, gait/coordination).", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    text = ''\n    try:\n        if output.is_document:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    text = ''\n        if not text and output.is_text_format:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = ''\n    except Exception:\n        text = ''\n    if not text:\n        return 0.0\n    t = text.lower()\n\n    categories = 0\n    # Orientation\n    if re.search(r'(alert\\s+and\\s+oriented|a&o|a\\&o|oriented\\s*x\\s*[234])', t):\n        categories += 1\n    # Cranial nerves\n    if 'cranial nerve' in t or re.search(r'cn\\s*[ii|2]-?x?i?i?', t):\n        categories += 1\n    # Pupils\n    if re.search(r'(pupils\\s+equal|perrl|perrla|pupil(s)?\\s+reactive)', t):\n        categories += 1\n    # Gait/coordination\n    if re.search(r'(gait|coordination|romberg|heel\\s*walking|toe\\s*walking|finger-to-nose)', t):\n        categories += 1\n\n    return max(0.0, min(1.0, categories / 4.0))"}, {"type": "code", "name": "Plan: Follow-up, Return Precautions, and Activity Restrictions", "description": "Verify that the plan references follow-up timeframe, return/ER precautions, and activity/safety guidance (rest/no sports/helmet/no driving).", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    text = ''\n    try:\n        if output.is_document:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    text = ''\n        if not text and output.is_text_format:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = ''\n    except Exception:\n        text = ''\n    if not text:\n        return 0.0\n    t = text.lower()\n\n    have_fu = bool(re.search(r'(follow[-\\s]?up|recheck|return\\s+visit).{0,40}(hour|day|week|\\b\\d{2,3}\\b)', t))\n    have_prec = any(w in t for w in ['return to er', 'go to er', 'ed immediately', 'emergency', 'red flag', 'seek urgent care', 'worsen'])\n    have_activity = any(w in t for w in ['no sports', 'no contact sports', 'gradual return', 'return-to-play', 'cognitive rest', 'brain rest', 'no driving', 'helmet', 'avoid screens'])\n\n    count = sum([have_fu, have_prec, have_activity])\n    return max(0.0, min(1.0, count / 3.0))"}, {"type": "llm_judge", "name": "Assessment and Imaging Decision Alignment", "description": "Check that assessment and plan align with adolescent minor head injury best practices (e.g., PECARN principles), given this specific case: no LOC, normal neuro exam, mild headache, mild nausea, no vomiting, mechanism = skateboard fall, GCS 15 implied.", "weight": 2.0, "judge_prompt": "Review the SOAP note against the provided case details: 16-year-old male, skateboard fall, head struck pavement, no loss of consciousness, mild persistent headache x2 hours, mild nausea (no vomiting), no photophobia/phonophobia, normal neuro exam except mild heel-walk coordination deficit, vitals within normal ranges in the case description. Evaluate clinical soundness of Assessment and Plan with respect to adolescent head injury guidance (PECARN principles or equivalent):\n\nConsider:\n- Is the primary diagnosis reasonable (e.g., minor head injury/concussion vs. other causes)?\n- Are red flags considered and appropriately absent/present?\n- Is the imaging decision reasonable (typically avoid immediate CT if low risk; observation vs. imaging rationale stated)?\n- Does the plan include appropriate monitoring and return precautions?\n\nScoring:\n- 1.0: Clear, guideline-concordant reasoning (states low-risk features, avoids unnecessary CT or provides strong rationale, includes observation/monitoring advice and red flags).\n- 0.5: Generally reasonable but incomplete or somewhat generic; rationale for imaging/observation not clearly tied to risk features.\n- 0.0: Poor alignment; recommends unnecessary imaging without rationale or misses key safety considerations.\n\nOnly assess correctness and coherence of assessment/plan versus case facts; do not score structure here.", "expectation": "A low-risk head injury assessment with appropriate avoidance of CT or clear rationale, observation advice, and comprehensive return precautions."}, {"type": "llm_judge", "name": "Medication Recommendations and Pediatric Dosing", "description": "Evaluate whether symptomatic treatment recommendations (e.g., acetaminophen/ibuprofen) and dosing guidance are appropriate for a 16-year-old (~56.8 kg). Avoid unsafe meds (e.g., aspirin) and provide safe dosing ranges/frequencies.", "weight": 2.0, "judge_prompt": "Check the Plan for medication guidance appropriateness:\n- Symptomatic analgesia recommended (e.g., acetaminophen and/or ibuprofen) with appropriate pediatric/adolescent dosing ranges (e.g., acetaminophen ~10\u201315 mg/kg q4\u20136h; ibuprofen ~10 mg/kg q6\u20138h; max/day guidance acceptable). For ~56.8 kg adolescent, adult dosing ranges may be acceptable but should be safe and justified.\n- Avoids aspirin and unnecessary sedatives/opioids.\n- Includes basic administration guidance (with food for NSAIDs, avoid duplicate dosing, max daily dose, or similar safety notes).\n\nScoring:\n- 1.0: Safe, specific dosing and frequency appropriate for adolescent weight with key safety notes.\n- 0.5: Symptomatic treatment given but dosing vague or missing one key safety detail.\n- 0.0: Inappropriate/unsafe recommendations (e.g., aspirin) or no medication guidance when clearly indicated.", "expectation": "Safe, weight-appropriate analgesic guidance with dosing and safety notes, avoiding aspirin/opioids."}, {"type": "llm_judge", "name": "History Fidelity to Case Details", "description": "Cross-check that the note\u2019s Subjective and histories accurately capture key case details (mechanism, symptoms, relevant past/family/social history, meds, allergies).", "weight": 2.0, "judge_prompt": "Compare the SOAP note content to the provided case summary. Verify inclusion of these elements in Subjective/Histories:\n- Mechanism: skateboard fall; head hit pavement; no helmet; landed on left side; no loss of consciousness; drove self.\n- Symptoms: persistent headache x2 hours; mild nausea; no vomiting; no sound/light sensitivity; blurry vision attributed to old glasses; denies brain fog or concentration difficulties.\n- Past history: ear tubes (2013), tonsillectomy (2015); intermittent headaches since age 14.\n- Family history: GERD (father), HTN (paternal grandmother), DM2 (maternal grandfather), others noncontributory.\n- Social: denies alcohol/tobacco/drugs.\n- Medications/Allergies: daily multivitamin; NKDA.\n\nScoring:\n- 1.0: Captures mechanism and at least five of the seven bulleted categories above accurately.\n- 0.5: Captures mechanism and at least three categories.\n- 0.0: Misses mechanism or includes major inaccuracies conflicting with the case.", "expectation": "Accurate, case-faithful histories with mechanism and key elements reflected without contradictions."}, {"type": "llm_judge", "name": "Safety and Activity Counseling Completeness", "description": "Evaluate completeness of counseling for a 16-year-old with minor head injury: return-to-play, school accommodations, driving, helmet/safety, and rest guidance.", "weight": 2.0, "judge_prompt": "Assess the Plan\u2019s counseling content for age-appropriate safety guidance:\n- Return-to-play: staged/gradual return to sports; no contact sports until symptom-free.\n- School/cognitive: brief cognitive rest, school accommodations if needed.\n- Driving/activities: avoid driving/heavy machinery until symptom-free.\n- Helmet/safety: future helmet use and injury prevention.\n- Clear red-flag return/ER precautions.\n\nScoring:\n- 1.0: Addresses at least four of the five bullets with clear, actionable guidance.\n- 0.5: Addresses two to three bullets.\n- 0.0: Addresses one or none, or gives unsafe guidance.", "expectation": "Comprehensive, actionable counseling for sports, school, driving, safety, and red flags."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3: Quality and Professional Presentation", "description": "Holistic assessment of clinical writing quality, organization, reasoning clarity, and appropriateness for a pediatric/adolescent patient and family.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Organization and Readability", "description": "Evaluate clarity of structure, formatting, and readability for clinical use.", "weight": 1.5, "judge_prompt": "Assess organization and readability:\n- Clear, scannable headings/subheadings; logical flow.\n- Concise sentences and clinically usable formatting (bullets/tables where appropriate).\n- Minimal redundancy; unambiguous statements.\nScoring: 1.5 = highly organized and readable; 0.75 = mixed clarity with minor issues; 0 = disorganized or hard to follow.", "expectation": "Clean, well-structured clinical note that is quick to scan and easy to understand."}, {"type": "llm_judge", "name": "Clinical Reasoning Clarity", "description": "Evaluate how well the note explains reasoning from history/exam to assessment and plan.", "weight": 1.5, "judge_prompt": "Assess clarity of clinical reasoning from S/O to A/P:\n- States rationale linking symptoms/exam to diagnosis/differential.\n- Justifies imaging/observation decisions.\n- Addresses alternative diagnoses and why they are less likely.\nScoring: 1.5 = clear, justified reasoning; 0.75 = some reasoning present but thin; 0 = reasoning unclear or absent.", "expectation": "A concise but explicit link from findings to diagnoses and management decisions."}, {"type": "llm_judge", "name": "Professional Tone and Documentation Hygiene", "description": "Evaluate tone, use of medical terminology, and documentation hygiene (units, abbreviations, dates).", "weight": 1.5, "judge_prompt": "Assess tone and documentation hygiene:\n- Professional, neutral tone; correct medical terminology; consistent units.\n- Appropriate use of abbreviations and expansions on first use.\n- Includes dates/identifiers where relevant; avoids slang.\nScoring: 1.5 = professional and polished; 0.75 = minor lapses; 0 = unprofessional or inconsistent.", "expectation": "Polished clinical documentation with consistent terminology and units."}, {"type": "llm_judge", "name": "Patient-Centeredness and Communication", "description": "Evaluate whether the note reflects patient/family education, understanding checks, and shared decision making appropriate for an adolescent.", "weight": 1.5, "judge_prompt": "Assess patient-centered communication aspects:\n- Education provided in understandable terms; confirms understanding (teach-back or similar) or documents acceptance/refusal.\n- Age-appropriate guidance for a 16-year-old; involves guardian as appropriate or documents decision-making.\n- Respectful, empathetic phrasing.\nScoring: 1.5 = strongly patient-centered; 0.75 = partially addressed; 0 = not addressed.", "expectation": "Clear, empathetic education and documentation of understanding appropriate for a 16-year-old and family."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "43dc9778-450b-4b46-b77e-b6d82b202035", "rubric": {"category_name": "1040 Preparation Package (Bob & Lisa Smith)", "rationale": "Pattern C (Mixed): The deliverable is a PDF document bundle (Form 1040 plus required schedules) with embedded data. Stage 1 enforces a strict, verifiable structure (single combined PDF with core 1040 pages and a front-matter Filing Summary page). Stage 2 mixes code and LLM checks to verify key correctness and cross-references now that the structure is predictable. Code rules provide deterministic bounds/consistency tests on extracted text, while LLM rules assess nuanced form linkages (schedules flowing to Form 1040, signatures, filing status). Stage 3 evaluates overall professional quality, review-readiness, and e-file appropriateness.", "max_total_score": 30.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (STRUCTURE ONLY)", "description": "LLM-only checks to ensure the output is a single combined PDF return for Tax Year 2024 with Form 1040 pages 1\u20132 and a preparer Filing Summary/Index page that lists included forms/schedules. If this structure is missing, downstream verification is impossible.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 5.5, "rules": [{"type": "llm_judge", "name": "Core 2024 Form 1040 Presence", "description": "Verify the candidate output is a single PDF containing the official IRS Form 1040 for Tax Year 2024 with both pages present and taxpayers' names.", "weight": 5.0, "judge_prompt": "You are evaluating whether the candidate output satisfies the CORE structural requirements for a 2024 Form 1040 return. Only check presence/format, not correctness of numbers.\n\nRequired to award full credit:\n- The output is a single PDF (not Word/Excel/images) containing:\n  1) IRS Form 1040, clearly labeled \u201cForm 1040\u201d and \u201c2024,\u201d page 1 and page 2.\n  2) Taxpayer names: \u201cBob Smith\u201d and \u201cLisa Smith\u201d (or similar like Robert/Elizabeth if clearly same clients) appear on the 1040 header section.\n- Pages look like official IRS forms (proper headings such as \u201cU.S. Individual Income Tax Return\u201d).\n\nFlexible matching guidance:\n- Allow minor naming variations (e.g., Bob/Robert, Lisa/Elizabeth) if evidently the same people.\n- Accept combined PDF where Form 1040 pages might not be first, as long as both pages are present.\n\nScoring (0 to 5):\n- 5: Single PDF; both 1040 pages 1\u20132 are present and show 2024; Bob and Lisa appear.\n- 3: Single PDF; only one 1040 page present or year unclear but likely 2024; or names appear elsewhere in package.\n- 1: A PDF exists but not the 1040 form (e.g., wrong form/year) OR only a partial mock-up not recognizable as IRS 1040.\n- 0: Not a PDF or no recognizable 1040 present.\n\nOnly evaluate structure/presence. Do not assess calculation correctness.", "expectation": "Single PDF including 2024 Form 1040 pages 1 and 2 with Bob and Lisa identified."}, {"type": "llm_judge", "name": "Filing Summary / Index Page Present", "description": "Check for a front-matter Filing Summary page (not filed) that lists included forms/schedules and key top-line figures to enable verification.", "weight": 3.0, "judge_prompt": "Check if the PDF includes a clearly labeled front-matter page before the IRS forms that serves as a \u201cFiling Summary,\u201d \u201cIndex of Included Forms,\u201d or similar. It must be clearly marked as NOT part of the filed return (e.g., \u201cPreparer Summary \u2013 Not Filed\u201d). This page enables verification.\n\nRequired elements for full credit:\n- A distinct summary/index page exists near the beginning.\n- It lists included forms/schedules (e.g., Form 1040 p1\u2013p2, Schedule 1/2/3, A/B/C/D/E, 8879 if present), or explicitly marks non-applicable (N/A) items.\n- It includes a concise key figures box with these fields labeled (values can be blank if unknown): Filing Status; Dependents count; AGI (Line 11); Taxable Income (Line 15); Total Tax (Line 24); Total Payments (Line 33); Refund (Line 35) or Amount You Owe (Line 37).\n\nFlexible matching:\n- Section titles may vary (e.g., \u201cPreparer Cover Sheet,\u201d \u201cReturn Index,\u201d \u201cReview Summary\u201d).\n- Key figures may be shown as placeholders if justified.\n\nScoring (0 to 3):\n- 3: Clear summary/index page present with forms list and labeled key figure fields.\n- 2: Summary present but missing either forms list or key figure block.\n- 1: Some cover text exists but lacks both a clear list and key figures.\n- 0: No summary/index page.\n\nDo not judge numerical accuracy\u2014only structure and presence.", "expectation": "Front-matter page listing included forms/schedules and labeled key figures; marked as not filed."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Cross-Reference Verification", "description": "Now that the structure is enforced, verify correctness and consistency using a mix of code and LLM rules. Code does deterministic checks on text-extracted values; LLM judges nuanced cross-form linkages and filing details.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "PDF + 1040 + 2024 Keyword Check", "description": "Deterministically verify the file is a PDF-like document containing 1040/2024 keywords.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float or (float, str) in [0, 0.8]\n    \"\"\"\n    weight = 0.8\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output resource.\"\n    if not output.is_document:\n        return 0.0, \"Primary output is not a document.\"\n    # Try to extract text from PDF/DOCX\n    text = \"\"\n    feedback = []\n    try:\n        if str(output.extension).lower() == '.pdf' or (hasattr(output, 'mime') and 'pdf' in str(output.mime).lower()):\n            text = context.files.read_pdf_text(output.id) or \"\"\n        else:\n            # Fallback to docx if provided\n            try:\n                text = context.files.read_docx_text(output.id) or \"\"\n            except Exception:\n                text = context.files.read_pdf_text(output.id) or \"\"\n    except Exception as e:\n        return 0.0, f\"Failed to read document text: {e}\"\n    t = text.lower()\n    score = 0.0\n    # Check keywords\n    if 'form 1040' in t:\n        score += 0.3\n    else:\n        feedback.append(\"Missing 'Form 1040' keyword.\")\n    if 'u.s. individual income tax return' in t:\n        score += 0.3\n    else:\n        feedback.append(\"Missing 'U.S. Individual Income Tax Return' heading.\")\n    # Year check\n    if re.search(r'\\b2024\\b', t):\n        score += 0.2\n    else:\n        feedback.append(\"Tax year '2024' not detected.\")\n    return min(score, weight), \"; \".join(feedback) if feedback else \"OK\""}, {"type": "code", "name": "Key Line Amounts: Presence and Basic Relationships", "description": "Try to extract key 1040 line amounts (AGI L11, Taxable Income L15, Total Tax L24, Total Payments L33, Refund L35, Amount Owed L37) from text and check simple relationships.", "weight": 1.2, "code": "import re\n\nAMOUNT_RE = re.compile(r'\\$?\\s*([0-9]{1,3}(?:,[0-9]{3})*|[0-9]+)(?:\\.[0-9]{2})?')\n\nLABELS = {\n    'agi': ['adjusted gross income', 'agi', 'line 11'],\n    'taxable_income': ['taxable income', 'line 15'],\n    'total_tax': ['total tax', 'line 24'],\n    'total_payments': ['total payments', 'line 33'],\n    'refund': ['refund', 'line 35'],\n    'amount_owe': ['amount you owe', 'line 37', 'amount you owe']\n}\n\n\ndef parse_amount_from_lines(lines, keywords):\n    for i, line in enumerate(lines):\n        lo = line.lower()\n        if any(k in lo for k in keywords):\n            # Try amount on same line\n            m = None\n            for m2 in AMOUNT_RE.finditer(line):\n                m = m2\n            if m:\n                try:\n                    return float(m.group(1).replace(',', ''))\n                except Exception:\n                    pass\n            # Try next line\n            if i + 1 < len(lines):\n                for m2 in AMOUNT_RE.finditer(lines[i+1]):\n                    try:\n                        return float(m2.group(1).replace(',', ''))\n                    except Exception:\n                        continue\n    return None\n\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Returns score in [0, 1.2] based on presence and basic numeric relationships.\n    \"\"\"\n    weight = 1.2\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output.\"\n    # Extract text\n    text = \"\"\n    try:\n        if str(output.extension).lower() == '.pdf' or (hasattr(output, 'mime') and 'pdf' in str(output.mime).lower()):\n            text = context.files.read_pdf_text(output.id) or \"\"\n        else:\n            text = context.files.read_docx_text(output.id) or \"\"\n    except Exception as e:\n        return 0.0, f\"Failed to extract text: {e}\"\n    lines = text.splitlines()\n    vals = {k: parse_amount_from_lines(lines, LABELS[k]) for k in LABELS}\n    found = {k: v for k, v in vals.items() if v is not None}\n    feedback = []\n    score = 0.0\n    # Presence of at least three key amounts\n    if len(found) >= 3:\n        score += 0.4\n    else:\n        feedback.append(f\"Only found {len(found)} of 6 key amounts.\")\n    # Non-negativity checks for core\n    for k in ['agi', 'taxable_income', 'total_tax', 'total_payments']:\n        v = vals.get(k)\n        if v is not None and v >= 0:\n            score += 0.1\n        elif v is not None and v < 0:\n            feedback.append(f\"{k} negative: {v}\")\n    # Relationship: taxable_income <= agi (when both present)\n    if vals.get('agi') is not None and vals.get('taxable_income') is not None:\n        if vals['taxable_income'] <= vals['agi'] + 1e-6:\n            score += 0.2\n        else:\n            feedback.append(\"Taxable income exceeds AGI.\")\n    # Refund vs Amount Owed mutually exclusive logic\n    refund = vals.get('refund')\n    owe = vals.get('amount_owe')\n    if refund is not None or owe is not None:\n        # one should be zero-ish if the other is positive\n        if refund is not None and owe is not None:\n            if (refund > 0 and owe <= 1) or (owe > 0 and refund <= 1) or (abs((refund or 0) - (owe or 0)) <= 1):\n                score += 0.2\n            else:\n                feedback.append(\"Refund and Amount Owed both appear positive/inconsistent.\")\n        else:\n            score += 0.1\n    # Cap and return\n    score = min(score, weight)\n    return score, (\"; \".join(feedback) if feedback else \"OK\")"}, {"type": "llm_judge", "name": "Names, Filing Status, and Household Consistency", "description": "Verify Form 1040 shows Bob and Lisa as taxpayers and a coherent filing status (likely Married filing jointly).", "weight": 3.5, "judge_prompt": "Examine the 1040 pages.\nCheck for:\n- Taxpayer names: Bob Smith and Lisa Smith (allow reasonable variants)\n- Filing status checkmark consistent with two taxpayers (e.g., Married filing jointly). If a different filing status is chosen, ensure the rest of the form is consistent (e.g., only one taxpayer listed for Single).\n- If a dependents section exists, ensure it looks coherent (names or count present if referenced in summary; if none claimed, section may be blank).\n\nScoring (0 to 3.5):\n- 3.5: Both names present and filing status consistent with data shown; dependents area coherent.\n- 2.5: Minor inconsistencies (e.g., names present but filing status unclear) but overall coherent.\n- 1.5: Noticeable inconsistency (e.g., Married filing jointly but only one name present) yet still a 1040.\n- 0: Names not present or filing status clearly incompatible with visible data.", "expectation": "Names and filing status align; dependents section not contradictory."}, {"type": "llm_judge", "name": "Schedules Presence and Flow to 1040", "description": "Cross-check that schedules listed in the Filing Summary are present and amounts appear to flow to the correct 1040 lines (high-level reasonableness).", "weight": 3.5, "judge_prompt": "Using the Filing Summary/Index page, verify that listed schedules/forms exist later in the PDF. Then check if the presence of those schedules makes sense with the 1040 line references.\n\nExamples (not exhaustive):\n- Schedule 1 (Additional Income/Adjustments) amounts should feed into 1040 lines 8 or 10.\n- Schedule 2/3 amounts should feed into 1040 lines 23 or 31.\n- Schedule B (interest/dividends) should correspond to 1040 line 2 or 3.\n- Schedule D (capital gains) should correspond to 1040 line 7.\n- Schedule C/E/F should ultimately impact AGI/taxable income.\n\nScoring (0 to 3.5):\n- 3.5: All schedules listed are present; cross-form flows look coherent with references and numbers (no obvious mismatches or missing pages).\n- 2.5: One minor mismatch or ambiguous reference, overall coherent.\n- 1.0: Multiple mismatches or missing listed schedules, but some linkage present.\n- 0: Schedules in summary are absent or flows obviously incorrect.", "expectation": "Listed schedules are present and reasonably flow to the 1040 lines."}, {"type": "llm_judge", "name": "Signatures and Preparer Information", "description": "Check that 1040 page 2 includes taxpayer signature area and preparer details appropriate for review/e-file context.", "weight": 3.0, "judge_prompt": "On 1040 page 2 verify:\n- Taxpayer(s) signature/date area: For e-file, an 8879 may handle signatures; accept notes like \u201cE-file authorization via Form 8879\u201d if signatures not present.\n- Preparer section includes firm name, PTIN/EIN placeholders, address, and date.\n\nScoring (0 to 3):\n- 3: Signatures or valid e-file authorization note present; preparer info filled legibly.\n- 2: Preparer info present but signatures/authorization unclear.\n- 1: Signature and preparer sections largely blank, but page 2 is present.\n- 0: Page 2 missing or sections obviously absent.", "expectation": "Page 2 has signature/authorization context and preparer details appropriate for review."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Review Readiness", "description": "Holistic LLM assessment of presentation quality, clarity for reviewer/client, and e-file readiness. Not about raw correctness; focuses on how well the package communicates and is prepared for professional review.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Presentation and Organization", "description": "Professional look-and-feel, readable, and organized as a single coherent package.", "weight": 2.5, "judge_prompt": "Assess the overall presentation:\n- Single, coherent combined PDF with logical ordering: Filing Summary first, then Form 1040 p1\u2013p2, then schedules.\n- Legibility, consistent typography, no blurry or handwritten fields unless necessary.\n- Clear page headers/labels; bookmarks/table of contents optional but helpful.\n\nScoring (0 to 2.5):\n- 2.5: Clean, legible, logically ordered, easy to navigate.\n- 1.5: Mostly professional with minor formatting issues.\n- 0.5: Disorganized or multiple files stitched poorly, but content present.\n- 0: Chaotic or unreadable.", "expectation": "Clean, legible, logically ordered combined PDF."}, {"type": "llm_judge", "name": "Preparer Summary Clarity for Review", "description": "Quality of the Filing Summary/Index page for reviewer handoff.", "weight": 3.5, "judge_prompt": "Evaluate the Filing Summary/Index (preparer-only) page:\n- Are key figures (Filing Status, Dependents, AGI L11, Taxable Income L15, Total Tax L24, Total Payments L33, Refund/Amount Owed) clearly labeled and filled in (or explicitly N/A)?\n- Does it list included forms/schedules with page references or checkboxes?\n- Does it include brief reviewer notes (e.g., unusual items, assumptions, known missing docs) if applicable?\n\nScoring (0 to 3.5):\n- 3.5: Clear, complete, with helpful notes and page references.\n- 2.5: Mostly clear with minor omissions (e.g., no page refs).\n- 1.0: Minimal summary lacking clarity.\n- 0: No usable summary.", "expectation": "Concise, labeled key figures and a clear list of included forms/schedules with reviewer-friendly notes."}, {"type": "llm_judge", "name": "Compliance and E-file Readiness (Qualitative)", "description": "Holistic sense that the package would meet typical e-file attachment expectations and includes necessary forms for the scenarios depicted.", "weight": 2.5, "judge_prompt": "Provide a qualitative assessment of e-file readiness:\n- For the items shown, does the package include the forms that would typically be required with a 1040 e-file (e.g., Schedules 1\u20133 if applicable, B/D/C/E as needed, 8879 or a note about e-signature authorization)?\n- Are any obvious missing forms apparent from the numbers or cross-references (e.g., interest shown but no Schedule B)?\n\nScoring (0 to 2.5):\n- 2.5: Looks appropriately complete for e-file for the facts shown.\n- 1.5: Minor omissions or ambiguities.\n- 0.5: Multiple likely-missing attachments.\n- 0: Appears non-compliant/missing key forms.", "expectation": "Forms and schedules align with the facts; nothing obviously missing for e-file."}, {"type": "llm_judge", "name": "Data Privacy and PII Handling", "description": "Sensitive info appears only where appropriate; preparer summary avoids unnecessary PII.", "weight": 1.5, "judge_prompt": "Assess whether sensitive PII is handled appropriately:\n- SSNs and bank details appear only on appropriate IRS form fields (not on the preparer summary/index page).\n- If bank info for refund is shown, it is appropriately placed and not overly exposed elsewhere.\n- Redaction is optional; focus on keeping PII limited to proper locations.\n\nScoring (0 to 1.5):\n- 1.5: PII appears only where expected on IRS forms; preparer summary avoids exposing PII.\n- 1.0: Minor PII exposure on summary but not severe.\n- 0.5: Multiple PII exposures that reduce professionalism.\n- 0: Significant PII exposure or leakage.", "expectation": "PII only on appropriate IRS forms; no unnecessary repetition in the summary."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "1b1ade2d-f9f6-4a04-baa5-aa15012b53be", "rubric": {"category_name": "Manufacturing \u2014 Buyers & Purchasing Agents: Agile Sourcing Workflow (Lamp Assemblies)", "rationale": "This rubric enforces a self-documenting, verifiable 2\u20133-page DOCX/PDF that becomes both an executive-ready proposal and a system blueprint for TechSol. Stage 1 (LLM-only) strictly mandates document structure and artifacts (RACI table, modular quotation structure table, defined gates), enabling Stage 2 verification. Stage 2 blends small deterministic code checks (roles/keywords coverage) with higher-weight LLM judgments for correctness of change-handling, traceability, and modular-cost integration. Stage 3 assesses overall quality for executive and technical audiences.", "max_total_score": 24.0, "stages": [{"name": "Stage 1 \u2014 Format & Structural Gate", "description": "LLM-only gate verifying the output is a 2\u20133-page professional DOCX/PDF with the exact structural elements required to enable verification and platform build-out.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.5, "rules": [{"type": "llm_judge", "name": "Document Format and Structure Requirements (Gate)", "description": "Checks the file format, length, and presence of all required sections and structural artifacts enabling verification: modular quotation table, process steps with decision gates, and RACI/approval mapping.", "weight": 6.0, "judge_prompt": "You are evaluating a candidate output for a sourcing workflow redesign in automotive purchasing. Only evaluate STRUCTURE and FORMAT, not content quality.\n\nAcceptable formats: DOCX or PDF only. Reject plain text, spreadsheets, or slides.\nLength: 2\u20133 pages total.\nProfessional formatting: Clear section headers, numbered/bulleted lists where appropriate.\n\nRequired sections (flexible naming allowed if meaning is clear):\n1) Executive Summary (or Overview)\n2) Background & Objectives (or Context & Goals)\n3) Modular Quotation Structure (must describe a plug-and-play model for cost drivers based on features, design elements, child parts, and raw materials). This section must include a small table with columns similar to: [Module/Feature | Cost Driver | UoM | Source | Update Trigger].\n4) Revised Sourcing & Nomination Workflow (end-to-end steps from TRSO initiation through supplier nomination and beyond). Must show a numbered sequence of steps and explicitly named decision gates (e.g., TRSO Sign-off Gate, Shortlist Gate, Nomination Gate, Change-Request Gate).\n5) Approval Layers & RACI (a table mapping roles to key steps/approvals; include at least these roles: ER, Quality, Purchase, Finance, Program Manager, TechSol, Supplier/Vendor). RACI columns or an equivalent approval mapping must be visible.\n6) Post-Nomination Change Handling (how to handle variant additions, feature updates, aesthetic redesigns; include branching/loop-back and re-approval triggers).\n7) Governance & Audit Trail (digital signatures, versioning, communication trail requirements).\n8) Digital Platform Blueprint (or System Blueprint) summarizing entities/objects (e.g., TRSO, Quote Modules, Supplier, Part/Variant, Change Request, Negotiation Record, Approval), workflow states, and key transitions. Bullet list is acceptable.\n\nOptional (helps partial credit if some required elements are borderline): KPIs/SLAs and Implementation/Rollout Roadmap.\n\nScoring instructions (apply to the 6-point weight):\n- 6.0: DOCX/PDF, 2\u20133 pages, all 8 required sections present; includes BOTH tables: (a) Modular Quotation table and (b) RACI/Approval table; numbered steps with named gates.\n- 4.8: DOCX/PDF, 2\u20133 pages, all required sections present but missing either the modular table or the RACI/Approval table (not both).\n- 3.6: DOCX/PDF, 2\u20133 pages, missing exactly one required section (or steps lack explicit named gates) but otherwise structurally sound.\n- 1.8: DOCX/PDF, but fewer than half of required sections OR page count outside 2\u20133 unless content is dense and clearly equivalent in scope.\n- 0.0: Not DOCX/PDF OR clearly under-scoped (e.g., <1.5 pages equivalent) OR lacks most required sections.\n\nBe flexible on exact section titles but strict on presence and structural artifacts (tables, numbered steps, named gates). Output only a numeric score and one-sentence rationale.", "expectation": "A 2\u20133-page DOCX/PDF with clear sections, numbered process steps, named decision gates, a Modular Quotation table, and a RACI/approval mapping table enabling downstream verification."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness Verification", "description": "Checks whether the structured document provides a coherent, verifiable workflow with robust change-handling, traceability, and proper integration of the modular quotation construct. Mix of lightweight code checks and higher-weight LLM judgments.", "is_required": true, "max_points": 10.0, "min_score_to_pass": 5.0, "rules": [{"type": "code", "name": "Stakeholder Role Coverage (Deterministic)", "description": "Verifies the presence of key stakeholder roles mentioned in the document to ensure role completeness for approvals and accountability.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float in [0, 1] or (float, str)\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n    text = \"\"\n    try:\n        if output.extension and output.extension.lower().endswith('.pdf'):\n            text = context.files.read_pdf_text(output.id)\n        else:\n            text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            return 0.0, \"Unable to read document text\"\n    if not text:\n        return 0.0, \"Empty document text\"\n    t = text.lower()\n\n    # Required stakeholders (count presence with synonyms)\n    role_synonyms = {\n        'engineering & research / er': [r'engineering\\s*&\\s*research', r'\\bER\\b', r'R&D', r'engineering team'],\n        'quality': [r'\\bquality\\b', r'quality team', r'qa'],\n        'purchase / procurement': [r'\\bpurchase\\b', r'\\bprocurement\\b', r'purchasing'],\n        'finance / controllers': [r'\\bfinance\\b', r'controller', r'financial controller'],\n        'program manager / pm': [r'program manager', r'\\bPM\\b', r'program management'],\n        'techsol / it': [r'techsol', r'\\bit\\b', r'information technology'],\n        'supplier / vendor': [r'supplier', r'vendor']\n    }\n\n    found = {}\n    for role, patterns in role_synonyms.items():\n        present = any(re.search(pat, t, flags=re.IGNORECASE) for pat in patterns)\n        found[role] = present\n\n    count = sum(1 for v in found.values() if v)\n    score = count / max(1, len(role_synonyms))\n    missing = [k for k, v in found.items() if not v]\n    feedback = f\"Roles found: {count}/{len(role_synonyms)}; Missing: {', '.join(missing) if missing else 'None'}\"\n    return score, feedback"}, {"type": "code", "name": "Modular Quotation Coverage (Deterministic)", "description": "Checks presence of key terms proving a true modular, plug-and-play quote model tied to cost drivers across features, design elements, child parts, and raw materials.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float in [0, 1] or (float, str)\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n    text = \"\"\n    try:\n        if output.extension and output.extension.lower().endswith('.pdf'):\n            text = context.files.read_pdf_text(output.id)\n        else:\n            text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            return 0.0, \"Unable to read document text\"\n    if not text:\n        return 0.0, \"Empty document text\"\n    t = text.lower()\n\n    terms = {\n        'modular': [r'\\bmodular\\b'],\n        'plug-and-play': [r'plug[-\\s]?and[-\\s]?play'],\n        'cost driver(s)': [r'cost\\s*driver'],\n        'feature(s)': [r'\\bfeature(s)?\\b'],\n        'design element(s)': [r'design\\s*element'],\n        'child part(s)': [r'child\\s*part'],\n        'raw material(s)': [r'raw\\s*material'],\n        'module structure/roll-up': [r'module', r'roll[-\\s]?up', r'bom']\n    }\n\n    hits = {}\n    for label, patterns in terms.items():\n        hits[label] = any(re.search(pat, t, flags=re.IGNORECASE) for pat in patterns)\n\n    count = sum(1 for v in hits.values() if v)\n    score = count / max(1, len(terms))\n    missing = [k for k, v in hits.items() if not v]\n    feedback = f\"Modular quote coverage: {count}/{len(terms)}; Missing: {', '.join(missing) if missing else 'None'}\"\n    return score, feedback"}, {"type": "llm_judge", "name": "Change-Handling Flow Completeness (Lamp Assemblies)", "description": "Assesses whether the workflow defines clear, agile mechanisms to handle post-nomination design changes (variant additions, feature updates, aesthetic redesigns), including triggers, branching, re-approvals, and negotiation loops without losing speed.", "weight": 3.0, "judge_prompt": "Evaluate the correctness and completeness of the change-handling design for lamp assemblies (headlamps/tail lamps). Focus on structure and logic, not prose style.\n\nLook for:\n- Explicit triggers for change (variant additions, feature updates, aesthetic redesigns) and linkage to TRSO updates.\n- Clear branching/loop-back steps and decision gates for changes after supplier nomination.\n- Rapid re-quotation using the Modular Quotation Structure (module-level deltas, automatic roll-up) and rules for when renegotiation is required vs. auto-accept.\n- Approvals for price change requests involving Finance controllers, Program Managers, and Purchase, with thresholds/SLAs.\n- Parallelization where appropriate (e.g., conditional approvals, fast-track rules) to reduce timeline impact.\n\nScoring (0\u20133):\n- 3.0: All elements present and logically connected with named gates and specific thresholds/SLA concepts; suitable to implement.\n- 2.0: Most elements present; some thresholds/parallelization unclear but overall flow workable.\n- 1.0: Partial flow; triggers or approvals vague; lacks negotiation/re-quote logic.\n- 0.0: Missing or generic; does not address post-nomination agility.\n\nReturn a numeric score and a brief justification.", "expectation": "A concrete, agile change-management loop with triggers, thresholds, re-quote rules, approvals, and named gates tailored to lamp assemblies."}, {"type": "llm_judge", "name": "Traceability, Audit Trail, and Governance", "description": "Evaluates whether the document ensures full traceability and governance for executive oversight and compliance: versioning, digital signatures, communication trails, and cross-referencing of artifacts.", "weight": 3.0, "judge_prompt": "Assess the document for robust traceability and governance design.\n\nLook for:\n- Unique IDs and versioning for TRSO, Quotes/Modules, Parts/Variants, Change Requests, Approvals.\n- Immutable audit logs for communications, negotiations, and decisions; digital signatures or e-signatures.\n- Cross-referencing/trace linking among artifacts (e.g., CR -> TRSO version -> Quote Module versions -> Approval decision).\n- Visibility for stakeholders (dashboards, notifications, status) and record retention policies.\n\nScoring (0\u20133):\n- 3.0: Strong, explicit mechanisms for IDs, versioning, linked artifacts, and e-signed approvals; clearly implementable.\n- 2.0: Adequate traceability with minor gaps.\n- 1.0: Minimal traceability mentions; unclear implementation.\n- 0.0: Not addressed.\n\nReturn a numeric score and brief rationale.", "expectation": "Clearly defined IDs, versioning, e-signatures, audit logs, and cross-linking of artifacts enabling end-to-end traceability."}, {"type": "llm_judge", "name": "Integration of Modular Quotation Into Core Workflow", "description": "Checks whether the modular quotation structure is truly embedded in the sourcing and change flows, driving negotiation, re-quotation, and analytics.", "weight": 3.0, "judge_prompt": "Assess how well the Modular Quotation Structure is integrated into the end-to-end workflow.\n\nLook for:\n- Quote composed of modules/features/child parts with cost drivers and UoM, supporting plug-and-play updates.\n- Automatic impact calculation and roll-up to total piece price when modules change.\n- Rules linking module changes to re-quote, negotiation triggers, and approvals.\n- Use in supplier comparison, localization analysis, and what-if simulations.\n\nScoring (0\u20133):\n- 3.0: Deep integration; specific rules and examples; clearly drives decisions and negotiations.\n- 2.0: Integrated but some rules vague or incomplete.\n- 1.0: Mentioned but not operationalized.\n- 0.0: Not integrated.\n\nReturn a numeric score and brief rationale.", "expectation": "Modular cost structure actively drives re-quote logic, negotiation, approvals, and reporting throughout the workflow."}], "on_failure_action": "skip_remaining", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Holistic Quality Assessment", "description": "LLM-only professional assessment of clarity, usefulness to CPO and TechSol, strategic value, and presentation quality.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Executive Readiness and Strategic Clarity", "description": "Assesses whether the document provides a compelling, concise, and logically structured narrative suitable for the CPO and executive stakeholders.", "weight": 2.0, "judge_prompt": "Judge the document\u2019s executive suitability. Consider: crisp Executive Summary, clear objectives tied to pain points, explicit benefits (agility, reduced delays, traceability), and a coherent storyline with measurable outcomes.\n\nScore (0\u20132):\n- 2.0: Clear, compelling, and action-oriented with measurable outcomes.\n- 1.0: Generally clear but lacks strong linkage or measurable outcomes.\n- 0.0: Unclear or unfocused for executives.\n\nReturn numeric score and one-sentence justification.", "expectation": "A concise executive story linking problems to the proposed agile workflow with expected impact and outcomes."}, {"type": "llm_judge", "name": "Technical Blueprint Quality for TechSol", "description": "Evaluates whether TechSol can directly translate the document into a build plan: entities, states, roles, permissions, integrations, notifications, and SLAs.", "weight": 2.0, "judge_prompt": "Assess whether the document is ready for TechSol to build from. Look for: well-defined entities and relationships, workflow states and transitions, role-based access and approvals, notification/escation design, SLAs, and any integration touchpoints. Prefer specificity over high-level statements.\n\nScore (0\u20132):\n- 2.0: Sufficiently specific to start design/build with minimal clarification.\n- 1.0: Partially specific; requires significant elaboration.\n- 0.0: Too high-level to implement.\n\nReturn numeric score and brief rationale.", "expectation": "A concrete entity/state model and operational details enabling immediate system design."}, {"type": "llm_judge", "name": "Practicality for Lamp Assemblies and Risk Management", "description": "Assesses whether the solution considers lamp-assembly specifics (aesthetic, safety-critical, frequent changes), risks, mitigations, and KPIs.", "weight": 2.0, "judge_prompt": "Evaluate practicality with lamp assemblies: addresses frequent late-stage changes, supplier design iterations, safety-critical validation, and aesthetic variants. Look for risk identification (timeline slippage, cost volatility, quality slips) with mitigations (thresholds, fast-tracks, PPAP/validation considerations) and KPIs/SLAs.\n\nScore (0\u20132):\n- 2.0: Concrete lamp-specific risks, mitigations, and KPIs are present.\n- 1.0: Some relevant points but incomplete.\n- 0.0: Generic or misses lamp-specific realities.\n\nReturn numeric score and brief rationale.", "expectation": "Lamp-specific risks and controls are integrated with KPIs and SLAs to ensure delivery and quality."}, {"type": "llm_judge", "name": "Presentation, Structure, and Readability", "description": "Assesses professional presentation: clear headers, tables, lists, consistent terminology, and visual clarity.", "weight": 2.0, "judge_prompt": "Assess presentation quality: Are headers and sections clear? Are tables (Modular Quotation, RACI) readable? Are steps numbered, gates named consistently, and terminology consistent? Is the document easy to scan and understand for mixed audiences?\n\nScore (0\u20132):\n- 2.0: Professional, consistent, and easy to scan.\n- 1.0: Adequate with minor inconsistencies.\n- 0.0: Disorganized or hard to follow.\n\nReturn numeric score and a short rationale.", "expectation": "A polished, easy-to-scan document with consistent naming and well-structured visuals and lists."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "11dcc268-cb07-4d3a-a184-c6d7a19349bc", "rubric": {"category_name": "Manufacturing \u2014 Inventory Location Report (Shipping/Receiving/Inventory Clerks)", "rationale": "This rubric enforces a self-documenting Excel workbook that proves cross-referencing between a Daily Receiving Log and the assigned locations (Inv on line). Stage 1 mandates an exact, verification-friendly workbook structure. Stage 2 mixes deterministic code checks (bounds, schema, special item handling) with LLM validation of reconciliation logic and mapping consistency, with LLM rules carrying ~5x the weight of code rules. Stage 3 judges overall usability, professionalism, and clarity for material handlers and operations.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate", "description": "LLM-only gate to ensure the output is an Excel workbook with specific sheets and structures that make verification trivial.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Excel Workbook Requirement", "description": "Verify the candidate output is a single Excel workbook that contains the mandated sheets and tables enabling verification.", "weight": 4.0, "judge_prompt": "You are evaluating whether the submitted output is a valid Excel workbook with the exact structure needed to verify a daily putaway/location report for manufacturing receiving. Only check PRESENCE and STRUCTURE, not correctness of calculations.\n\nFORMAT REQUIREMENTS:\n- File must be an Excel spreadsheet (.xlsx or .xls). Not PDF/DOCX/CSV.\n- Workbook must contain at least these REQUIRED sheets (be flexible with near-equivalent names):\n  1) \"Location Report\" (or similar: \"Putaway Report\", \"Stored Away Report\").\n     Must contain a tabular dataset with visible column headers. Required columns (flexible naming, but must be unambiguous):\n       - Date (move/putaway date)\n       - Item Number (part/SKU)\n       - Item Description\n       - Supplier or Vendor\n       - Receiver/PO or Reference\n       - Qty Received\n       - Qty Moved to Line (or Putaway Qty)\n       - Qty Remaining in Receiving (balance not yet stored away)\n       - UOM\n       - Moved From (staging/phantom WMS location)\n       - Moved To (assigned line location per Inv on line)\n       - Notes (optional but recommended)\n     The table should have at least one data row populated (not just headers).\n\n  2) \"Reconciliation\" (or similar: \"Coverage Summary\", \"Daily Reconcile\").\n     Must include:\n       - A table or section showing, by Item Number, the quantities: From Daily Receiving Log (Qty Received), From Location Report (Qty Moved), and Qty Remaining (not yet moved). Totals row is recommended.\n       - A summary that indicates coverage (e.g., how many items from the receiving log were moved vs. pending), and an Exceptions/Not Moved list or section.\n\n  3) \"Assigned Locations\" (or similar: \"Data Crosswalk\", \"Inv on line Map\").\n     Must include a table mapping Item Number to Assigned Location and Source/Provenance (e.g., \u201cInv on line\u201d). Columns should include at least: Item Number, Assigned Location, and Source/Reference.\n\n  4) \"Method & Assumptions\" (or similar: \"Notes & Audit Trail\").\n     Must include 3+ sentences explaining:\n       - How the cross-reference was performed between Daily Receiving Log and Inv on line.\n       - That \u201cMoved From\u201d are staging/phantom WMS locations.\n       - The special case that only half the quantity of item P11-P09457-01 was received and moved to its line location.\n\n- OPTIONAL (bonus but not required): A \"Pivot Summary\" or charts for operations.\n\nSCORING (out of 4.0):\n- 4.0: Valid Excel + all 4 REQUIRED sheets present with appropriate tables/sections and at least one populated data row in Location Report.\n- 3.0: Valid Excel + required sheets present but a minor omission (e.g., small column gap in Location Report or Reconciliation lacks totals OR minor brevity in Method text); overall still clearly usable for verification.\n- 2.0: Valid Excel but missing exactly one REQUIRED sheet or several required columns/sections on a sheet; structure incomplete for reliable verification.\n- 0.0: Not an Excel file OR missing multiple REQUIRED sheets OR Location Report table not present/populated.\n\nOnly evaluate structure and presence. Do not judge correctness of data or calculations here.", "expectation": "A multi-sheet Excel workbook with Location Report, Reconciliation, Assigned Locations (crosswalk), and Method & Assumptions, in the specified shapes."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness and Consistency)", "description": "Deterministic checks and LLM-based verification that, given the enforced shape, the content is arithmetically consistent, cross-referenced, and treats the special item correctly.", "is_required": true, "max_points": 10.0, "min_score_to_pass": 6.0, "rules": [{"type": "code", "name": "Schema and Quantity Sanity Checks", "description": "Verify spreadsheet presence, locate key columns with fuzzy matching, ensure non-negative numeric quantities, and check that Row-Level Qty Remaining approximately equals Qty Received minus Qty Moved when present.", "weight": 0.6, "code": "import re\\nimport pandas as pd\\nimport numpy as np\\n\\n\\nDEF_NUM_TOL = 1e-6\\n\\n\\ndef _best_sheet_name(sheet_names, candidates):\\n    sn_lower = {s.lower(): s for s in sheet_names}\\n    for s_lower, s_orig in sn_lower.items():\\n        for c in candidates:\\n            if c in s_lower:\\n                return s_orig\\n    return None\\n\\n\\ndef _find_col(df, candidates):\\n    cols = [str(c) for c in df.columns]\\n    low = [c.lower().strip() for c in cols]\\n    for i, c in enumerate(low):\\n        for cand in candidates:\\n            if cand in c:\\n                return cols[i]\\n    return None\\n\\n\\ndef _to_num(s):\\n    return pd.to_numeric(s, errors='coerce')\\n\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_spreadsheet:\\n        return 0.0, 'No spreadsheet primary output'\\n\\n    try:\\n        x_path = context.files.get_path(output.id)\\n        xfile = pd.ExcelFile(x_path)\\n        sheet = _best_sheet_name(xfile.sheet_names, ['location report','putaway','stored away'])\\n        if not sheet:\\n            return 0.2, 'Location Report-like sheet not found'\\n        df = pd.read_excel(x_path, sheet_name=sheet)\\n        if df.empty:\\n            return 0.2, 'Location Report sheet is empty'\\n\\n        # Fuzzy columns\\n        col_item = _find_col(df, ['item number','item','part','sku'])\\n        col_recv = _find_col(df, ['qty received','received qty','received','qty_in'])\\n        col_moved = _find_col(df, ['qty moved','moved to line','putaway qty','to line'])\\n        col_rem = _find_col(df, ['qty remaining','remaining','balance','left in receiving'])\\n\\n        # Basic presence check for key columns\\n        have_core = sum([col_item is not None, col_recv is not None, col_moved is not None])\\n        if have_core < 2:\\n            return 0.3, 'Insufficient key columns found'\\n\\n        score = 0.0\\n        # Non-negative checks\\n        good_nonneg = 1.0\\n        for c in [col_recv, col_moved, col_rem]:\\n            if c is None:\\n                continue\\n            v = _to_num(df[c])\\n            nn = v.dropna()\\n            if len(nn) == 0:\\n                continue\\n            nonneg_ratio = (nn >= -DEF_NUM_TOL).mean()\\n            good_nonneg *= nonneg_ratio if nonneg_ratio>0 else 0\\n        score += 0.4 * good_nonneg\\n\\n        # Balance check: rem \u2248 recv - moved (only for rows where all three exist)\\n        if col_recv and col_moved and col_rem:\\n            recv = _to_num(df[col_recv])\\n            moved = _to_num(df[col_moved])\\n            rem = _to_num(df[col_rem])\\n            mask = recv.notna() & moved.notna() & rem.notna()\\n            ok = ((recv[mask] - moved[mask] - rem[mask]).abs() <= 1e-3).mean() if mask.any() else 0.0\\n            score += 0.6 * ok\\n        else:\\n            score += 0.2  # partial credit if columns partially present\\n\\n        # Clamp to [0,1]\\n        score = max(0.0, min(1.0, score))\\n        return score, 'Schema/quantity sanity score computed'\\n    except Exception as e:\\n        return 0.0, f'Exception reading spreadsheet: {e}'"}, {"type": "code", "name": "Special Item P11-P09457-01 Handling", "description": "Confirm the special item row(s) exist and reflect that only half received was moved to line (i.e., moved equals received; remaining is zero or absent).", "weight": 0.6, "code": "import re\\nimport pandas as pd\\nimport numpy as np\\n\\n\\ndef _best_sheet_name(sheet_names, candidates):\\n    sn_lower = {s.lower(): s for s in sheet_names}\\n    for s_lower, s_orig in sn_lower.items():\\n        for c in candidates:\\n            if c in s_lower:\\n                return s_orig\\n    return None\\n\\n\\ndef _find_col(df, candidates):\\n    cols = [str(c) for c in df.columns]\\n    low = [c.lower().strip() for c in cols]\\n    for i, c in enumerate(low):\\n        for cand in candidates:\\n            if cand in c:\\n                return cols[i]\\n    return None\\n\\n\\ndef _to_num(s):\\n    return pd.to_numeric(s, errors='coerce')\\n\\n\\ndef evaluate(workflow, context):\\n    target_item = 'P11-P09457-01'\\n    output = context.get_primary_output()\\n    if not output or not output.is_spreadsheet:\\n        return 0.0, 'No spreadsheet output'\\n    try:\\n        x_path = context.files.get_path(output.id)\\n        xfile = pd.ExcelFile(x_path)\\n        sheet = _best_sheet_name(xfile.sheet_names, ['location report','putaway','stored away'])\\n        if not sheet:\\n            return 0.0, 'Location Report sheet missing'\\n        df = pd.read_excel(x_path, sheet_name=sheet)\\n        if df.empty:\\n            return 0.0, 'Location Report empty'\\n\\n        col_item = _find_col(df, ['item number','item','part','sku'])\\n        col_recv = _find_col(df, ['qty received','received qty','received','qty_in'])\\n        col_moved = _find_col(df, ['qty moved','moved to line','putaway qty','to line'])\\n        col_rem = _find_col(df, ['qty remaining','remaining','balance','left in receiving'])\\n        col_to = _find_col(df, ['moved to','to location','assigned location','line location','destination'])\\n        if not col_item or not col_recv or not col_moved:\\n            return 0.2, 'Missing key columns for special item check'\\n\\n        mask = df[col_item].astype(str).str.upper().str.contains(target_item.upper(), na=False)\\n        if not mask.any():\\n            return 0.0, 'Special item not found'\\n\\n        sub = df[mask].copy()\\n        recv = _to_num(sub[col_recv]).fillna(0)\\n        moved = _to_num(sub[col_moved]).fillna(0)\\n        rem = _to_num(sub[col_rem]) if col_rem else pd.Series([np.nan]*len(sub))\\n\\n        total_recv = float(recv.sum())\\n        total_moved = float(moved.sum())\\n        total_rem = float(pd.to_numeric(rem, errors='coerce').fillna(0).sum()) if rem is not None else 0.0\\n\\n        # Expect moved == received and rem == 0 (since the half that was received was moved).\\n        cond_moved_eq_recv = abs(total_moved - total_recv) <= 1e-6\\n        cond_rem_zero = abs(total_rem) <= 1e-6 if rem is not None else True\\n        has_to = True\\n        if col_to:\\n            has_to = sub[col_to].astype(str).str.strip().replace({'nan':'','None':''}).ne('').any()\\n\\n        score = 0.0\\n        if cond_moved_eq_recv:\\n            score += 0.6\\n        elif abs(total_moved - total_recv) / (total_recv + 1e-6) <= 0.1:\\n            score += 0.3\\n        if cond_rem_zero:\\n            score += 0.2\\n        if has_to:\\n            score += 0.2\\n        return max(0.0, min(1.0, score)), 'Special item handling scored'\\n    except Exception as e:\\n        return 0.0, f'Exception: {e}'"}, {"type": "code", "name": "Moved From Appears To Be Staging/Phantom", "description": "Check that most Moved From values look like staging/phantom WMS locations (e.g., contain STAGE, DOCK, RCV/REC, TEMP, PHANTOM, INBOUND).", "weight": 0.6, "code": "import re\\nimport pandas as pd\\n\\nPATTERN = re.compile(r'(stage|staging|dock|rcv|recv|rec\\\\b|inbound|temp|phanto)', re.IGNORECASE)\\n\\n\\ndef _best_sheet_name(sheet_names, candidates):\\n    sn_lower = {s.lower(): s for s in sheet_names}\\n    for s_lower, s_orig in sn_lower.items():\\n        for c in candidates:\\n            if c in s_lower:\\n                return s_orig\\n    return None\\n\\n\\ndef _find_col(df, candidates):\\n    cols = [str(c) for c in df.columns]\\n    low = [c.lower().strip() for c in cols]\\n    for i, c in enumerate(low):\\n        for cand in candidates:\\n            if cand in c:\\n                return cols[i]\\n    return None\\n\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_spreadsheet:\\n        return 0.0, 'No spreadsheet output'\\n    try:\\n        x_path = context.files.get_path(output.id)\\n        xfile = pd.ExcelFile(x_path)\\n        sheet = _best_sheet_name(xfile.sheet_names, ['location report','putaway','stored away'])\\n        if not sheet:\\n            return 0.0, 'Location Report sheet missing'\\n        df = pd.read_excel(x_path, sheet_name=sheet)\\n        if df.empty:\\n            return 0.0, 'Location Report empty'\\n        col_from = _find_col(df, ['moved from','from location','staging','source location'])\\n        if not col_from:\\n            return 0.2, 'Moved From column missing'\\n        s = df[col_from].astype(str).str.strip()\\n        if s.empty:\\n            return 0.2, 'Moved From empty'\\n        valid = s.apply(lambda x: bool(PATTERN.search(x)) if x and x.lower()!='nan' else False).mean()\\n        # Score: majority matching earns full, partial otherwise\\n        if valid >= 0.8:\\n            return 1.0, 'Staging pattern strong'\\n        elif valid >= 0.5:\\n            return 0.7, 'Staging pattern moderate'\\n        elif valid > 0.0:\\n            return 0.4, 'Few staging-like sources'\\n        else:\\n            return 0.0, 'No staging-like sources detected'\\n    except Exception as e:\\n        return 0.0, f'Exception: {e}'"}, {"type": "llm_judge", "name": "Cross-Reference and Reconciliation Integrity", "description": "Validate that the Reconciliation sheet clearly ties the Location Report to the Daily Receiving Log: by-item quantities, totals, coverage summary, and exceptions list.", "weight": 3.0, "judge_prompt": "Using the provided Excel workbook, inspect the Reconciliation (or similarly named) sheet and the Location Report sheet. Assess whether the Reconciliation demonstrates a faithful cross-reference to the Daily Receiving Log by including: (a) an item-level table with Qty Received (from Daily Receiving Log), Qty Moved (from Location Report), and Qty Remaining; (b) totals row or summary; (c) a coverage summary indicating how many received items were moved vs. pending; (d) an exceptions or not-moved list. You may skim the Location Report to confirm that the quantities claimed in Reconciliation appear to be consistent and drawn from that sheet. Score based on completeness and clarity of this linkage (structure and internal references), not on external source correctness (since the Daily Receiving Log is not attached here).\n\nScoring (0 to 3.0):\n- 3.0: All elements present (item-level table, totals/summary, coverage, exceptions), with clear labeling and internal consistency with the Location Report.\n- 2.0: Most elements present; minor omissions (e.g., exceptions section thin) but still clearly ties to Location Report.\n- 1.0: Partial reconciliation; unclear linkage or missing multiple elements.\n- 0.0: Reconciliation missing or does not tie to Location Report at all.", "expectation": "A clear, by-item reconciliation linking the Location Report to the Daily Receiving Log with coverage and exceptions."}, {"type": "llm_judge", "name": "Assigned Location Mapping Correctness", "description": "Evaluate whether each item in the Location Report has a clear mapping to its assigned location from the Assigned Locations (crosswalk) sheet, with consistent references to the source (Inv on line).", "weight": 2.8, "judge_prompt": "Check the Location Report sheet against the Assigned Locations (or Data Crosswalk) sheet. For a representative sample of items, verify that each has a corresponding Assigned Location entry and that the Moved To values match the mapped Assigned Location. Confirm the crosswalk indicates a source (e.g., \u201cInv on line\u201d). Be flexible on column names but require unambiguous linkage by Item Number. Do not validate the external source itself; only the internal consistency across sheets.\n\nScoring (0 to 2.8):\n- 2.8: Consistent mapping for essentially all items checked; Moved To aligns with Assigned Location; source noted.\n- 1.8: Mostly consistent with minor mismatches or gaps (e.g., a few items lack crosswalk entries).\n- 0.8: Significant gaps; many items lack mapping or mismatches are common.\n- 0.0: No usable crosswalk or no evidence that Moved To aligns with assigned locations.", "expectation": "Location Report Moved To values match an Assigned Locations mapping that cites Inv on line as source."}, {"type": "llm_judge", "name": "Arithmetic Consistency and Totals", "description": "Assess whether sums and simple arithmetic shown in Reconciliation align with the item-level details in Location Report (e.g., Moved + Remaining \u2248 Received).", "weight": 2.4, "judge_prompt": "Compare totals and simple arithmetic presented in the Reconciliation sheet to the item-level lines in the Location Report. You may calculate approximate sums mentally; perfect precision is not required. Verify that (Qty Moved + Qty Remaining) approximately equals (Qty Received) at item and/or total levels as presented. Note any glaring inconsistencies or missing totals.\n\nScoring (0 to 2.4):\n- 2.4: Totals and arithmetic are consistently aligned; any differences are trivially explained (rounding/format).\n- 1.6: Minor inconsistencies or missing a few totals, but overall reasonable.\n- 0.8: Multiple inconsistencies; confidence in arithmetic is limited.\n- 0.0: Arithmetic does not hold or no totals provided.", "expectation": "Reconciliation math matches the Location Report details at item and total levels."}], "on_failure_action": "skip_remaining", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Operational Usefulness", "description": "Holistic LLM assessment of professionalism, clarity, and operational value for material handlers and production support.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Operational Usefulness for Material Handlers", "description": "Does the workbook make it easy for handlers to pick and move material quickly (clear locations, quantities, filters/sorts)?", "weight": 1.8, "judge_prompt": "Judge how usable the Location Report is for frontline material handlers. Consider: Are Item Number, Moved From (staging), Moved To (assigned/line location), and quantities prominently visible and filterable/sortable? Are dates and references (PO/Receiver) present for traceability? Would a handler quickly know what to pick and where to take it? Avoid assessing numerical correctness; focus on usability and clarity.", "expectation": "A clear, filterable table with obvious Item, From, To, and quantities that supports fast picking/putaway."}, {"type": "llm_judge", "name": "Professional Presentation and Formatting", "description": "Evaluate formatting, legibility, and workbook hygiene (headers, freeze panes, number/date formats, no broken merged cells).", "weight": 1.6, "judge_prompt": "Assess presentation quality: readable headers, consistent number/date formats, appropriate column widths, freeze panes, filters, minimal or no merged cells that would break sorting, and sensible sheet/tab naming. Penalize clutter and messy layouts.", "expectation": "Clean, professional spreadsheets that are easy to read and manipulate operationally."}, {"type": "llm_judge", "name": "Documentation Clarity (Method & Assumptions)", "description": "Quality of the narrative explaining methods, sources, and the special handling of item P11-P09457-01 and phantom staging locations.", "weight": 1.4, "judge_prompt": "Read the Method & Assumptions (or similar) sheet. Judge clarity and completeness: Does it explain the cross-reference steps, cite the Inv on line as the source for assigned locations, state that Moved From are staging/phantom WMS locations, and describe the special case for P11-P09457-01? Is it at least 3 sentences, concise, and operationally relevant?", "expectation": "A concise, 3+ sentence narrative covering method, sources, phantom staging, and the special item."}, {"type": "llm_judge", "name": "Data Hygiene and Consistency", "description": "Check for duplicate lines, missing key fields, inconsistent item code formats, and clear handling of exceptions.", "weight": 1.2, "judge_prompt": "Skim the Location Report and supporting sheets for data hygiene: consistent item code formats, minimal missing key fields (Item, Qty, From/To), no obvious duplicate lines unless justified, and explicit handling of exceptions (e.g., Not Moved list). Do not check the external sources\u2014only internal consistency and cleanliness.", "expectation": "Clean tables with consistent identifiers and explicit exception handling."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "e6429658-4de1-42dd-a9e0-2d2b9b02fb10", "rubric": {"category_name": "Medical Necessity Appeal and Patient Assistance Application (Health Care - Nurse Practitioner)", "rationale": "This rubric enforces a self-documenting, two-deliverable workflow: (1) a 2\u20134 page Word medical necessity appeal letter requesting Vraylar 1.5 mg coverage; (2) a digitally filled AbbVie patient assistance application PDF. Stage 1 uses LLM-only checks to strictly enforce the output shape and formatting so later verification is trivial. Stage 2 mixes light code checks (file presence, key term presence, basic content signals) with stronger LLM verification for clinical and administrative correctness and cross-document consistency. Stage 3 provides holistic quality assessment on tone, clarity, persuasiveness, and compliance.", "max_total_score": 20.0, "stages": [{"name": "Stage 1: Shape and Format Enforcement (Gate)", "description": "Gate that requires BOTH deliverables in correct formats and with required structural sections. LLM-only checks. Failure zeros the category.", "is_required": true, "max_points": 5.0, "min_score_to_pass": 3.5, "rules": [{"type": "llm_judge", "name": "Appeal Letter Shape Gate", "description": "Verify the Word appeal letter exists and follows strict structural requirements enabling verification.", "weight": 2.5, "judge_prompt": "Review all candidate output files in this submission. Determine if there is a professional appeal letter for insurance coverage of Vraylar 1.5 mg that meets ALL of the following STRUCTURAL requirements (do not judge content quality, only presence/format):\n\nFormat requirements:\n- File is a Word document (DOCX preferred). If PDF of the letter is present instead of DOCX, allow partial credit. \n- Length is 2\u20134 pages.\n- Professional letter formatting with date, addressee, subject line, and signature block.\n\nRequired sections/headers or obvious equivalents (be flexible with naming but insist each section is clearly present):\n1) Patient and plan identifiers at top: patient full name, age or DOB, insurance plan name, member ID (if available), denial reference/number (if available).\n2) Subject: Medical necessity appeal for Vraylar 1.5 mg (RE line or header).\n3) Clinical background/diagnosis: clearly states treatment-resistant depression (or equivalent) and brief clinical course.\n4) Treatment history: a structured list or table of prior medication trials with outcomes or adverse effects.\n5) Rationale for continuing Vraylar: explicit justification section.\n6) Financial/access context: states inability to afford out-of-pocket and hardship; request for coverage/tier exception.\n7) Explicit request statement and plan action sought; closing with provider credentials/signature block and contact info.\n8) Attachments/references note (may be a brief line listing enclosures or references).\n\nScoring:\n- 2.5: DOCX present, 2\u20134 pages, all 8 sections present.\n- 2.0: DOCX present, 2\u20134 pages, 6\u20137 sections present.\n- 1.5: DOCX present, 2\u20134 pages, 4\u20135 sections present.\n- 1.0: Letter present but in PDF (not DOCX) OR length outside 2\u20134 pages, with most sections present.\n- 0.0: No identifiable appeal letter, or lacks professional letter structure.", "expectation": "A 2\u20134 page DOCX appeal letter with all required sections clearly delineated."}, {"type": "llm_judge", "name": "AbbVie Application Shape Gate", "description": "Verify the AbbVie patient assistance application PDF exists and is structurally complete for verification.", "weight": 1.5, "judge_prompt": "Review all candidate output files. Determine if there is a digitally filled AbbVie Patient Assistance Application for Vraylar that meets these STRUCTURAL requirements (presence/format only, not judging quality):\n\nFormat requirements:\n- File is a PDF form, completed digitally (typed entries; if signatures are typed names/dates, accept as digital signatures if clearly indicated).\n\nRequired sections present and visibly filled where information is available (leave unknown clinical details blank per instructions):\n1) Patient Information section (name and at least one additional demographic field).\n2) Prescriber/Clinic Information section (provider name, clinic, contact).\n3) Medication Requested section specifying Vraylar (cariprazine) and strength 1.5 mg.\n4) Insurance Information section (insurer name and at least one identifier or note of no coverage/denial).\n5) Income/Household section (values or clearly marked unknown/left blank per instructions).\n6) Authorizations/Signatures section with dates (typed signature acceptable if indicated).\n\nScoring:\n- 1.5: PDF application present with all six sections visible and filled where available.\n- 1.0: PDF present with 4\u20135 required sections filled.\n- 0.5: PDF present with 2\u20133 required sections filled.\n- 0.0: No identifiable AbbVie application PDF or essentially blank form.", "expectation": "A digitally filled AbbVie Patient Assistance Application PDF with key sections populated where information is available."}, {"type": "llm_judge", "name": "File Naming and Deliverable Pairing", "description": "Check that there are exactly two core deliverables with appropriate names and formats.", "weight": 1.0, "judge_prompt": "Review the submission files. Confirm the presence of two distinct deliverables with appropriate names and formats:\n- Appeal letter saved as \u201cVraylar Appeal for RP\u201d (accept minor variations like punctuation or case; extension should be .docx ideally; partial credit if PDF).\n- AbbVie patient assistance application saved as \u201cRP Financial Assistance Application\u201d (PDF).\n\nScoring:\n- 1.0: Both files present, names closely match (minor variations acceptable), and formats match expectations (DOCX for letter, PDF for application). \n- 0.7: Both files present and identifiable by content, but naming deviates (e.g., missing initials) or letter is PDF instead of DOCX.\n- 0.3: Only one of the two files present/identifiable.\n- 0.0: Neither file identifiable.", "expectation": "Two files clearly named/identifiable as the appeal letter (DOCX) and the AbbVie application (PDF)."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Correctness and Cross-Verification", "description": "Verify clinical and administrative correctness and cross-document consistency. Mix of light code checks and LLM judgment.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Two Distinct Files Present (Programmatic Check)", "description": "Programmatically confirm at least one DOCX letter and one PDF application exist among outputs.", "weight": 0.5, "code": "def evaluate(workflow, context):\n    import re\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0, \"No outputs found.\"\n    docx_count = 0\n    pdf_count = 0\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            suf = p.suffix.lower()\n            if r.is_document and suf == '.docx':\n                docx_count += 1\n            if r.is_document and suf == '.pdf':\n                pdf_count += 1\n        except Exception:\n            continue\n    score = 0.5 if (docx_count >= 1 and pdf_count >= 1) else 0.0\n    fb = f\"Found {docx_count} DOCX and {pdf_count} PDF.\"\n    return score, fb"}, {"type": "code", "name": "Appeal Letter Key Elements Present", "description": "Check the letter text includes Vraylar, dose 1.5 mg, and appeal-related terms.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    import re\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0, \"No outputs.\"\n    # Find likely letter: prefer DOCX; fallback to any document with 'appeal'/'vraylar'\n    letter_res = None\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            if r.is_document and p.suffix.lower() == '.docx':\n                letter_res = r\n                break\n        except Exception:\n            continue\n    if letter_res is None:\n        # Fallback: any document mentioning appeal/Vraylar\n        for r in outputs:\n            try:\n                if not r.is_document:\n                    continue\n                text = ''\n                p = context.files.get_path(r.id)\n                if p.suffix.lower() == '.pdf':\n                    text = context.files.read_pdf_text(r.id) or ''\n                elif p.suffix.lower() == '.docx':\n                    text = context.files.read_docx_text(r.id) or ''\n                low = text.lower()\n                if 'appeal' in low and 'vraylar' in low:\n                    letter_res = r\n                    break\n            except Exception:\n                continue\n    if letter_res is None:\n        return 0.0, \"No identifiable appeal letter.\"\n    # Read text\n    p = context.files.get_path(letter_res.id)\n    text = ''\n    try:\n        if p.suffix.lower() == '.docx':\n            text = context.files.read_docx_text(letter_res.id) or ''\n        elif p.suffix.lower() == '.pdf':\n            text = context.files.read_pdf_text(letter_res.id) or ''\n    except Exception:\n        text = ''\n    low = text.lower()\n    checks = 0\n    total = 4\n    if 'vraylar' in low or 'cariprazine' in low:\n        checks += 1\n    if re.search(r\"\\b1\\.?5\\s*mg\\b\", low):\n        checks += 1\n    if 'appeal' in low or 'medical necessity' in low:\n        checks += 1\n    if ('treatment-resistant' in low and 'depress' in low) or 'trd' in low:\n        checks += 1\n    score = (checks/total) * 1.0\n    fb = f\"Letter keyword hits: {checks}/{total}.\"\n    return score, fb"}, {"type": "code", "name": "Application PDF Filled Content Signals", "description": "Check the application PDF contains expected terms indicating it is filled and relevant.", "weight": 0.5, "code": "def evaluate(workflow, context):\n    import re\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0, \"No outputs.\"\n    # Find likely AbbVie application PDF by content\n    app_res = None\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            if not (r.is_document and p.suffix.lower() == '.pdf'):\n                continue\n            txt = context.files.read_pdf_text(r.id) or ''\n            low = txt.lower()\n            if any(k in low for k in ['abbvie', 'patient assistance', 'assistance application', 'pap']):\n                app_res = r\n                break\n        except Exception:\n            continue\n    if app_res is None:\n        return 0.0, \"No identifiable AbbVie application PDF.\"\n    try:\n        txt = context.files.read_pdf_text(app_res.id) or ''\n    except Exception:\n        txt = ''\n    low = txt.lower()\n    signals = 0\n    total = 4\n    if len(low) > 300:\n        signals += 1\n    if 'vraylar' in low or 'cariprazine' in low:\n        signals += 1\n    if 'abbvie' in low or 'patient assistance' in low or 'assistance application' in low:\n        signals += 1\n    if any(k in low for k in ['patient information', 'prescriber', 'provider', 'signature', 'date']):\n        signals += 1\n    score = (signals/total) * 0.5\n    fb = f\"Application content signals: {signals}/{total}.\"\n    return score, fb"}, {"type": "llm_judge", "name": "Clinical Justification and Prior Trials", "description": "Assess whether the letter lays out a coherent clinical rationale for continuing Vraylar, including prior failed/intolerant trials and stability on current therapy.", "weight": 3.0, "judge_prompt": "Evaluate the appeal letter ONLY for clinical correctness and completeness (not overall writing quality). Look for:\n- Patient is a 65-year-old with treatment-resistant depression (or equivalent wording) and has been stable for ~2 years on Vraylar 1.5 mg.\n- A clear, itemized history of prior antidepressant/adjunctive trials, with duration, outcomes, and adverse effects leading to discontinuation or failure.\n- Rationale for continuing Vraylar tied to patient-specific response, functioning, and risk of relapse if changed.\n- Mentions dose (1.5 mg), frequency, and continuity of care.\nScoring:\n- 3.0: All elements present, specific, and internally coherent.\n- 2.0: Most elements present; some specifics missing (e.g., incomplete trial details).\n- 1.0: Minimal mention of prior trials or rationale; vague statements.\n- 0.0: Lacks core clinical rationale or incorrect medication/dose.", "expectation": "Specific, patient-centered clinical argument including prior ineffective/intolerant trials and 2-year stability on Vraylar 1.5 mg."}, {"type": "llm_judge", "name": "Insurance Appeal Essentials", "description": "Verify administrative essentials for a strong medical necessity appeal.", "weight": 2.5, "judge_prompt": "Evaluate the appeal letter for insurance appeal essentials (do not penalize for missing unknowns, but they should be addressed if available):\n- Includes insurer name, member ID, and denial reference/summary (or states that information is unavailable but being appealed).\n- States financial hardship and inability to pay out-of-pocket.\n- Explicitly requests coverage/tier exception or formulary exception for Vraylar 1.5 mg.\n- Notes risk of decompensation if therapy is interrupted or switched; optionally references guidelines or literature.\n- Provides provider credentials and contact information for payer follow-up.\nScoring:\n- 2.5: All essentials addressed clearly.\n- 1.5: Most essentials present; one notable omission.\n- 0.5: Only 1\u20132 essentials present.\n- 0.0: Lacks clear request and payer-facing details.", "expectation": "A payer-ready appeal with identifiers, denial summary, explicit coverage request, hardship note, and provider contact info."}, {"type": "llm_judge", "name": "Cross-Document Consistency", "description": "Check the appeal letter and application align on patient identity and medication details.", "weight": 1.5, "judge_prompt": "Compare the appeal letter and the AbbVie application. Check for consistency of:\n- Patient name (e.g., Robert Palen) and, if present, DOB/age.\n- Medication and dose: Vraylar (cariprazine) 1.5 mg.\n- Prescriber/clinic information.\nMinor formatting differences are acceptable. Score higher when all items align; lower if discrepancies or missing fields.\nScoring:\n- 1.5: All key items consistent across both documents.\n- 1.0: Minor omissions but no contradictions.\n- 0.5: Several missing fields; limited ability to confirm consistency.\n- 0.0: Contradictions between documents (e.g., different patient names/medications).", "expectation": "Letter and application show matching patient and medication details without contradictions."}, {"type": "llm_judge", "name": "Application Completeness (Content)", "description": "Judge whether the AbbVie application is reasonably complete and fillable content is present.", "weight": 1.0, "judge_prompt": "Assess the AbbVie application PDF for reasonable completeness (do not require data not provided in the prompt):\n- Patient info, prescriber info, medication request fields appear filled and legible.\n- Insurance info is provided or a clear note indicates missing/denied coverage.\n- Income/household section is addressed (values or left blank per instructions if unknown).\n- Required patient/prescriber authorization/signatures and dates are present (typed is acceptable if clearly indicated).\nScoring:\n- 1.0: All applicable sections reasonably complete and legible.\n- 0.7: One section notably incomplete.\n- 0.4: Multiple sections incomplete but document is clearly attempted.\n- 0.0: Essentially blank or unusable application.", "expectation": "A legible, reasonably completed application with key sections addressed and signatures/dates included."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3: Holistic Quality Assessment", "description": "Evaluate professional presentation, persuasiveness, clarity, and compliance.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Tone and Structure", "description": "Assess the appeal letter\u2019s tone, formatting, and adherence to professional standards for medical necessity appeals.", "weight": 1.5, "judge_prompt": "Judge the appeal letter\u2019s professionalism: formal tone, clear headings, logical flow, correct 2\u20134 page length, complete contact/signature block, and overall readability for a payer audience. Do not judge clinical content beyond presentation quality.\nScoring:\n- 1.5: Highly professional, polished, and well-structured.\n- 1.0: Professional with minor issues.\n- 0.5: Adequate but rough formatting or organization.\n- 0.0: Informal or poorly structured.", "expectation": "A polished, professionally formatted letter that reads like a standard medical necessity appeal."}, {"type": "llm_judge", "name": "Persuasiveness and Evidence Framing", "description": "Evaluate how convincingly the letter argues for coverage, including framing of risks/benefits and references if included.", "weight": 1.5, "judge_prompt": "Assess persuasiveness: clear articulation of patient benefit on Vraylar, concrete risks of switching, concise synthesis of prior failures/intolerance, and any supportive guideline/literature references (optional but helpful). Consider whether the request is specific and compelling.\nScoring:\n- 1.5: Strong, compelling case tailored to the patient.\n- 1.0: Reasonably persuasive but generic in places.\n- 0.5: Weak or generic rationale.\n- 0.0: Not persuasive.", "expectation": "A compelling, patient-specific argument for continued Vraylar coverage."}, {"type": "llm_judge", "name": "Clarity and Accessibility", "description": "Assess clarity, concision, and ease for payer reviewers to parse key points quickly.", "weight": 1.0, "judge_prompt": "Evaluate clarity: concise sentences, organized sections/bullets for treatment history, key facts surfaced early (age, diagnosis, dose, stability), and minimal jargon. Penalize rambling or buried key facts.\nScoring:\n- 1.0: Clear and concise; easy to scan.\n- 0.7: Mostly clear with minor verbosity.\n- 0.4: Somewhat cluttered; key points harder to find.\n- 0.0: Unclear or disorganized.", "expectation": "Well-organized, concise writing that highlights key payer-relevant facts."}, {"type": "llm_judge", "name": "Ethical and Compliance Considerations", "description": "Ensure appropriate handling of sensitive information and honest disclosure of missing data.", "weight": 1.0, "judge_prompt": "Check for HIPAA-conscious handling (only necessary identifiers), avoidance of speculative or fabricated clinical details, and transparent statements where data are unknown or not provided. Also check that the assistance application uses typed entries legibly and avoids misleading information.\nScoring:\n- 1.0: Fully appropriate and transparent.\n- 0.7: Minor issues.\n- 0.4: Noticeable issues with over-disclosure or conjecture.\n- 0.0: Major compliance concerns or apparent fabrication.", "expectation": "Appropriate, minimal necessary PHI; transparent about unknowns; no fabrication."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "bb863dd9-31c2-4f64-911a-ce11f457143b", "rubric": {"category_name": "IEHK 2017 Quotation (MedWholeGroup \u2192 Inter-Aid)", "rationale": "Self-documenting, staged evaluation that first enforces a strict, verification-ready Excel workbook shape, then verifies correctness with a mix of deterministic code checks and LLM judgement, and finally assesses professional quality and client appropriateness.", "max_total_score": 15.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement (GATE)", "description": "LLM-only gate ensuring the output is an Excel quotation workbook with the exact structural elements needed for verification.", "is_required": true, "max_points": 1.0, "min_score_to_pass": 0.7, "rules": [{"type": "llm_judge", "name": "Structured Quotation Workbook Requirement", "description": "Check that the candidate output is a properly structured Excel quotation for IEHK 2017 modules with required sheets/sections and tables.", "weight": 1.0, "judge_prompt": "You are evaluating whether the submitted file is a valid EXCEL quotation workbook with the required structure for an IEHK 2017 indicative pricing quote from MedWholeGroup to Inter-Aid.\n\nRequirements (be flexible with sheet/section names, but structure must clearly exist):\n\nFormat:\n- Must be an Excel spreadsheet (.xlsx), not PDF/Word/Markdown.\n- Workbook should present a professional quotation layout.\n\nSheet: 'Quotation' (or similar like 'Items', 'Pricing', 'Offer'):\n- Must contain a line-item table with columns equivalent to: [Item Description/Module | Article Number | Quantity | Unit Price (USD) | Line Total (USD) | Shelf Life | Lead Time]. Column names can vary slightly but the meaning must be clear.\n- The table must include the IEHK 2017 modules with quantities: 10 units of the Basic Module and 1 unit each for every other module. (You do not need to verify the exact module list here; just confirm that a row for Basic with qty ~10 exists, and rows for other modules with qty ~1 exist.)\n\nSheet: 'Summary' or 'Cover' (or clearly labeled summary section anywhere):\n- Must show commercial terms and totals, including: EXW (Ex-Works) basis, currency USD, a total amount (subtotal/total), payment terms = 100% prepayment, offer validity = 30 days.\n- Must include identifiers: Project reference 'BO-757820' and Quotation number 'Q6533211'. Client name 'Inter-Aid' should appear on the cover/summary or header.\n\nSheet or Section: 'Notes' / 'Assumptions' / 'References':\n- Must include a helpful WHO IEHK 2017 documentation link (e.g., one of the provided WHO URLs) to explain the kit structure.\n\nScoring:\n- 1.0: Excel file present AND has (a) a line-item table with the required columns, (b) the summary/commercial terms + identifiers, and (c) a WHO reference link section.\n- 0.7: Excel file present AND has (a) line-item table AND (b) summary/commercial terms, but missing the WHO link/reference section.\n- 0.4: Excel file present but missing a major part of the structure (e.g., line-item table missing key columns or summary/terms missing).\n- 0.0: Not an Excel file OR structure is too incomplete to verify.\n\nOnly check presence/structure, not correctness of data or math.", "expectation": "A professionally structured Excel workbook with itemized IEHK 2017 modules, a clear summary of commercial terms, identifiers (Q6533211, BO-757820, Inter-Aid), and a WHO reference link."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness)", "description": "Deterministic checks and LLM judgements to verify compliance with quantities, terms, identifiers, math, and references.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Exact Filename and Spreadsheet Type", "description": "Verify the output is an Excel spreadsheet and the filename exactly matches the required name.", "weight": 0.4, "code": "import re\nfrom pathlib import Path\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, 'No output resource.'\n    score = 0.0\n    feedback = []\n    # Spreadsheet check\n    if getattr(output, 'is_spreadsheet', False):\n        score += 0.2\n        feedback.append('Spreadsheet detected.')\n    else:\n        feedback.append('Not a spreadsheet.')\n    # Filename check\n    try:\n        path = context.files.get_path(output.id)\n        required = 'Quotation Q6533211 - BO-757820 (Inter-Aid).xlsx'\n        if path.name == required:\n            score += 0.2\n            feedback.append('Exact filename matches requirement.')\n        else:\n            feedback.append(f'Filename mismatch: {path.name}')\n    except Exception as e:\n        feedback.append(f'Filename check error: {e}')\n    return min(score, 0.4), '; '.join(feedback)"}, {"type": "code", "name": "Line-Item Table Integrity and Totals", "description": "Detect quantities, unit prices, and line totals; verify extended values and sum consistency within tolerance.", "weight": 0.4, "code": "import re\nimport numpy as np\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not getattr(output, 'is_spreadsheet', False):\n        return 0.0, 'No spreadsheet to validate.'\n    path = context.files.get_path(output.id)\n    feedback = []\n    try:\n        xl = pd.ExcelFile(path)\n        # Prefer likely item sheets\n        candidates = ['quotation', 'items', 'pricing', 'offer']\n        chosen = None\n        for s in xl.sheet_names:\n            sl = s.strip().lower()\n            if any(c in sl for c in candidates):\n                chosen = s\n                break\n        if chosen is None:\n            chosen = xl.sheet_names[0]\n        df = pd.read_excel(path, sheet_name=chosen)\n        if df.empty:\n            return 0.0, 'Item sheet is empty.'\n        cols = [str(c).strip() for c in df.columns]\n        low = [c.lower() for c in cols]\n        def find_col(preds):\n            for i, c in enumerate(low):\n                if any(all(p in c for p in ps) for ps in preds):\n                    return cols[i]\n            return None\n        qty_col = find_col([['qty'], ['quantity']])\n        unit_col = find_col([['unit','price'], ['unitprice'], ['unit','usd'], ['price']])\n        total_col = find_col([['line','total'], ['amount'], ['total']])\n        if not qty_col or not unit_col or not total_col:\n            return 0.0, f'Missing columns: qty={qty_col}, unit={unit_col}, total={total_col}'\n        # Clean numerics\n        def to_num(series):\n            return pd.to_numeric(series.astype(str).str.replace(r'[^0-9.+-]', '', regex=True), errors='coerce')\n        qty = to_num(df[qty_col])\n        unit = to_num(df[unit_col])\n        line = to_num(df[total_col])\n        # Basic sanity\n        if qty.notna().sum() == 0 or unit.notna().sum() == 0 or line.notna().sum() == 0:\n            return 0.0, 'Numeric conversion failed for key columns.'\n        # Compute expected line totals\n        expected = qty * unit\n        # Compare sums with tolerance\n        sum_diff = abs(np.nansum(line) - np.nansum(expected))\n        row_diff = np.nanmean(abs((line - expected).fillna(0)))\n        tol_sum = max(0.01, 0.001 * max(1.0, float(np.nansum(expected))))\n        tol_row = 0.01\n        score = 0.0\n        if sum_diff <= tol_sum:\n            score += 0.2\n            feedback.append('Sum of line totals matches qty*unit within tolerance.')\n        else:\n            feedback.append(f'Sum mismatch: diff={sum_diff:.2f} > tol={tol_sum:.2f}.')\n        if row_diff <= tol_row:\n            score += 0.2\n            feedback.append('Row-level line totals are consistent on average.')\n        else:\n            feedback.append(f'Row avg mismatch {row_diff:.2f} > {tol_row:.2f}.')\n        return min(score, 0.4), '; '.join(feedback)\n    except Exception as e:\n        return 0.0, f'Error validating totals: {e}'"}, {"type": "code", "name": "Basic vs Other Module Quantities", "description": "Check that the Basic Module appears with qty >=10 and that multiple other modules appear with qty=1 each.", "weight": 0.4, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not getattr(output, 'is_spreadsheet', False):\n        return 0.0, 'No spreadsheet to validate.'\n    path = context.files.get_path(output.id)\n    try:\n        xl = pd.ExcelFile(path)\n        candidates = ['quotation', 'items', 'pricing', 'offer']\n        chosen = None\n        for s in xl.sheet_names:\n            sl = s.strip().lower()\n            if any(c in sl for c in candidates):\n                chosen = s\n                break\n        if chosen is None:\n            chosen = xl.sheet_names[0]\n        df = pd.read_excel(path, sheet_name=chosen)\n        if df.empty:\n            return 0.0, 'Item sheet is empty.'\n        cols = [str(c).strip() for c in df.columns]\n        low = [c.lower() for c in cols]\n        # Identify description/module col and qty col\n        def find_desc_col():\n            for i, c in enumerate(low):\n                if any(k in c for k in ['description', 'module', 'item']):\n                    return cols[i]\n            return cols[0]\n        def find_qty_col():\n            for i, c in enumerate(low):\n                if 'qty' in c or 'quantity' in c:\n                    return cols[i]\n            return None\n        desc_col = find_desc_col()\n        qty_col = find_qty_col()\n        if not qty_col:\n            return 0.0, 'Quantity column not found.'\n        df['_desc'] = df[desc_col].astype(str).str.lower()\n        df['_qty'] = pd.to_numeric(df[qty_col], errors='coerce')\n        # Basic rows: contain 'basic'\n        basic_mask = df['_desc'].str.contains('basic', na=False)\n        basic_ok = False\n        if basic_mask.any():\n            basic_qty = df.loc[basic_mask, '_qty'].dropna()\n            basic_ok = (basic_qty >= 10).any()\n        # Other modules: qty == 1 and not basic\n        other_df = df.loc[~basic_mask & df['_qty'].notna()]\n        other_count = other_df.loc[other_df['_qty'] == 1].shape[0]\n        # Require at least a few other modules to infer coverage\n        other_ok = other_count >= 3\n        if basic_ok and other_ok:\n            return 0.4, 'Basic module qty >=10 and at least 3 other modules with qty=1 found.'\n        if basic_ok or other_ok:\n            return 0.2, f'Partial: basic_ok={basic_ok}, other_modules_qty1={other_count}.'\n        return 0.0, 'Did not detect required quantity pattern (basic>=10 and others qty=1).'\n    except Exception as e:\n        return 0.0, f'Error checking module quantities: {e}'"}, {"type": "code", "name": "Shelf Life and Lead Time Populated", "description": "Verify shelf life and lead time columns exist and are mostly populated for line items.", "weight": 0.4, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not getattr(output, 'is_spreadsheet', False):\n        return 0.0, 'No spreadsheet to validate.'\n    path = context.files.get_path(output.id)\n    try:\n        xl = pd.ExcelFile(path)\n        candidates = ['quotation', 'items', 'pricing', 'offer']\n        chosen = None\n        for s in xl.sheet_names:\n            sl = s.strip().lower()\n            if any(c in sl for c in candidates):\n                chosen = s\n                break\n        if chosen is None:\n            chosen = xl.sheet_names[0]\n        df = pd.read_excel(path, sheet_name=chosen)\n        if df.empty:\n            return 0.0, 'Item sheet is empty.'\n        cols = [str(c).strip() for c in df.columns]\n        low = [c.lower() for c in cols]\n        def find_col(keys):\n            for i, c in enumerate(low):\n                if any(k in c for k in keys):\n                    return cols[i]\n            return None\n        shelf_col = find_col(['shelf', 'expiry', 'expiration', 'expir'])\n        lead_col = find_col(['lead time', 'lead-time', 'leadtime', 'lead', 'availability', 'delivery'])\n        score = 0.0\n        notes = []\n        if shelf_col is not None:\n            filled = pd.Series(df[shelf_col]).astype(str).str.strip().replace({'': None}).notna().mean()\n            if filled >= 0.8:\n                score += 0.2\n                notes.append('Shelf life mostly populated.')\n            else:\n                notes.append(f'Shelf life completeness {filled:.0%}.')\n        else:\n            notes.append('Shelf life column missing.')\n        if lead_col is not None:\n            filled = pd.Series(df[lead_col]).astype(str).str.strip().replace({'': None}).notna().mean()\n            if filled >= 0.8:\n                score += 0.2\n                notes.append('Lead time mostly populated.')\n            else:\n                notes.append(f'Lead time completeness {filled:.0%}.')\n        else:\n            notes.append('Lead time column missing.')\n        return min(score, 0.4), '; '.join(notes)\n    except Exception as e:\n        return 0.0, f'Error checking shelf life and lead time: {e}'"}, {"type": "llm_judge", "name": "Commercial Terms Compliance", "description": "Check the workbook explicitly states EXW incoterms, currency USD, payment terms = 100% prepayment, and offer validity = 30 days.", "weight": 1.6, "judge_prompt": "Evaluate the workbook content for commercial terms compliance. Confirm ALL of the following are explicitly present and clearly stated:\n- Incoterms: EXW (Ex-Works / ex-warehouse)\n- Currency: USD\n- Payment terms: 100% prepayment\n- Offer validity: 30 days\n\nScoring:\n- 1.6: All four items explicitly and clearly present\n- 0.8: Two or three items clearly present\n- 0.4: Only one item present\n- 0.0: None present or ambiguous\nExplain briefly which items you found and where (e.g., Summary sheet, footer).", "expectation": "Clear, unambiguous commercial terms matching EXW, USD, 100% prepayment, validity 30 days."}, {"type": "llm_judge", "name": "Project and Client Identification", "description": "Check presence and correctness of key IDs: project ref BO-757820, quotation no. Q6533211, client Inter-Aid, and seller MedWholeGroup.", "weight": 1.6, "judge_prompt": "Check the workbook for identifiers and parties:\n- Project reference: 'BO-757820'\n- Quotation number: 'Q6533211'\n- Client name: 'Inter-Aid'\n- Seller/company name: 'MedWholeGroup'\n\nScoring:\n- 1.6: All four present and consistent across sheets/headers\n- 1.2: Three present\n- 0.8: Two present\n- 0.4: One present\n- 0.0: None found\nNote any mismatches or inconsistencies.", "expectation": "All identifiers correctly stated and consistent."}, {"type": "llm_judge", "name": "WHO IEHK Coverage and Reference Link", "description": "Assess whether the modules listed appear to cover the IEHK 2017 structure (10x Basic, 1x others) and that a WHO IEHK reference link is included.", "weight": 1.6, "judge_prompt": "Review the line-item listing and references:\n- Does the listing appear to include the IEHK 2017 modules structured as: 10 units of the Basic Module and 1 unit of each other module? (You do not need to verify exact WHO composition, but look for breadth and plausible completeness beyond just a couple of modules.)\n- Is there a WHO IEHK 2017 reference link included (e.g., https://iris.who.int/... or https://www.who.int/...)?\n\nScoring:\n- 1.6: Clear presence of 10x Basic and a broad set of other modules at 1x each, plus a WHO link\n- 1.2: Correct quantities pattern but limited other modules listed OR WHO link missing\n- 0.8: Partial modules and quantities with WHO link\n- 0.4: Minimal modules/quantities, even if WHO link is present\n- 0.0: No discernible IEHK structure and no WHO link\nExplain your judgment briefly.", "expectation": "A reasonably comprehensive module list reflecting 10x Basic + 1x others, and a WHO reference link present."}, {"type": "llm_judge", "name": "Summary Totals Presence and EXW Total", "description": "Check that a summary area shows subtotal/total and an EXW total in USD consistent with an itemized quote.", "weight": 1.6, "judge_prompt": "Look for a summary or totals area. Confirm that:\n- A subtotal and/or total is shown\n- The total is identified as EXW and in USD\n- The totals presentation is consistent with an itemized quote (you do not need to recalculate, just check plausibility and clear labeling)\n\nScoring:\n- 1.6: Clear subtotal and final EXW total in USD; presentation is consistent and professional\n- 0.8: Totals present but labeling (EXW/USD) is unclear or only one of subtotal/total is present\n- 0.4: Some numeric total appears but is poorly labeled or ambiguous\n- 0.0: No discernible totals area\nProvide a brief note on what you found.", "expectation": "A clear subtotal and final EXW USD total in a summary section."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality Assessment", "description": "LLM-only holistic assessment of presentation quality, clarity, and client usefulness.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Layout and Readability", "description": "Assess formatting, typography, table clarity, and overall professionalism suitable for a client-facing quotation.", "weight": 1.5, "judge_prompt": "Evaluate the workbook\u2019s professionalism and readability:\n- Clean, consistent formatting; legible fonts\n- Clear table headers and alignment\n- Logical sheet ordering and navigation (Cover/Summary \u2192 Quotation \u2192 Notes)\n- Suitable for sending directly to a client without rework\n\nScoring:\n- 1.5: Highly professional and polished\n- 1.0: Generally professional with minor issues\n- 0.5: Acceptable but needs cleanup\n- 0.0: Poorly formatted or confusing", "expectation": "A polished, client-ready quotation workbook."}, {"type": "llm_judge", "name": "Clarity and Client Guidance", "description": "Assess whether the document explains the IEHK structure at a high level and links to WHO guidance to help the client self-serve understanding.", "weight": 1.5, "judge_prompt": "Assess client guidance:\n- Is there a short explanatory note on IEHK modules vs. full kit to set expectations?\n- Is a WHO link provided and placed where a client would find it helpful?\n- Are any assumptions stated clearly (e.g., EXW excludes transport, validity)?\n\nScoring:\n- 1.5: Clear guidance and helpful reference link(s)\n- 1.0: Adequate guidance with minor gaps\n- 0.5: Minimal guidance\n- 0.0: No meaningful guidance", "expectation": "Concise, helpful guidance and a WHO reference link that orients the client."}, {"type": "llm_judge", "name": "Completeness of Commercial Communication", "description": "Evaluate whether all requested commercial elements are clearly and visibly communicated, avoiding ambiguity.", "weight": 1.5, "judge_prompt": "Check communication completeness for:\n- EXW basis, USD currency\n- 100% prepayment, 30-day validity\n- Totals placement and labeling\n- Project ref BO-757820 and quotation no. Q6533211 visibly associated with the quote\n\nScoring:\n- 1.5: All elements clear and visible\n- 1.0: Minor omissions/ambiguities\n- 0.5: Multiple gaps\n- 0.0: Largely incomplete", "expectation": "All key commercial details are unambiguously stated and easy to find."}, {"type": "llm_judge", "name": "Usefulness for Procurement Decision", "description": "Judge whether the quotation provides enough signal for the client to proceed (indicative pricing per module, availability/lead time, shelf life, and clear next steps/contact).", "weight": 1.5, "judge_prompt": "Assess the practical usefulness of this indicative quote for a procurement decision:\n- Per-module prices visible, itemized and understandable\n- Availability/lead time and shelf life noted per line item\n- Clear next steps or a contact line for questions/PO issuance\n\nScoring:\n- 1.5: Fully useful; client could proceed confidently\n- 1.0: Generally useful with minor gaps\n- 0.5: Limited usefulness\n- 0.0: Not useful for decision-making", "expectation": "A quotation that enables the client to compare modules and plan timelines confidently."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "a99d85fc-eff8-48d2-a7d4-42a75d62f18d", "rubric": {"category_name": "Real Estate Leasing Calculator \u2014 Rent Matrix (Excel)", "rationale": "This rubric enforces a self-documenting, verifiable Excel deliverable for a leasing rent matrix. Stage 1 uses LLM-only shape enforcement to require exact workbook structure so verification becomes trivial. Stage 2 mixes light, robust code checks (parsing, bounds, consistency) with higher-weight LLM judgment for escalation logic and dynamic behavior. Stage 3 assesses professional quality and usability for the intended audience.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Structured Workbook Gate", "description": "LLM-only gate to ensure the output is an Excel workbook with the exact sections, sheets, and tables that enable automated verification.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.5, "rules": [{"type": "llm_judge", "name": "Workbook Format and Sheet Structure", "description": "Verify the deliverable is an Excel workbook with the mandated sheets and high-level structure.", "weight": 3.0, "judge_prompt": "You are validating ONLY the structure/format of the candidate\u2019s output.\nRequirement: The output must be a single Excel workbook (.xlsx) that contains these sheets (flexible on exact naming, but they must clearly correspond):\n- Inputs (or Assumptions/Parameters)\n- Annual Matrix (or Annual Rent Matrix)\n- Monthly Matrix (or Monthly Rent Matrix)\n\nOn the Inputs sheet, verify:\n- Presence of editable variables for Suite Number (Suite #) and Suite Size (in square feet). These should be clearly marked as user-editable (light blue fill requested; be flexible but note presence/intent).\n- A Scenarios input table with rows for three scenarios and columns for: Primary Term (years), Base Rent ($/SF per month), Annual Escalator (%). Values should be editable.\n\nOn the Annual Matrix sheet, verify:\n- A single combined annual table covering all three scenarios with columns at minimum: Scenario, Year, Monthly Rent, $/SF, Annual Base Rent.\n- Rows for each year up to the scenario\u2019s term (up to 10 years max), with blank or no rows beyond the term.\n- A clearly labeled total Gross Lease Value by Scenario at the bottom of the annual breakdown (e.g., a small summary table or subtotal lines).\n- A Notes section present below the annual table (header like \u201cNotes\u201d).\n\nOn the Monthly Matrix sheet, verify:\n- A combined monthly table across all scenarios with columns at minimum: Scenario, Month #, Monthly Rent, $/SF.\n- Up to 120 months (for a 10-year term). The table must be capable of showing 120 lines but should not show extraneous values beyond each scenario\u2019s term (e.g., blank beyond the term).\n- A total lease value by Scenario displayed at the top of this sheet under the title (e.g., a small summary section labeled clearly).\n\nGeneral requirements to check:\n- Three scenarios are clearly delineated and consistently referenced across sheets.\n- Scenario color-coding is present (distinct colors per scenario). Editable input cells are light blue or similarly indicated.\n- The layout is clean and clearly labeled (titles, headers).\n\nScoring:\n- 3.0: All required sheets exist and all structural elements present as specified.\n- 2.0: All sheets exist; one non-critical structural element missing (e.g., color-coding or Notes) but core tables and summaries are present.\n- 1.0: Workbook present but one core component missing or not clearly structured (e.g., missing an entire table on one sheet).\n- 0.0: Not an Excel workbook or multiple major structural elements missing.\nOnly evaluate presence/structure, not correctness of formulas or numbers.", "expectation": "A well-structured .xlsx with clearly named sheets, inputs, annual and monthly combined tables, scenario delineation, and visible totals/notes."}, {"type": "llm_judge", "name": "Table Headers and Field Presence", "description": "Verify column headers and key fields exist to enable deterministic parsing in Stage 2.", "weight": 3.0, "judge_prompt": "Check that column headers needed for verification are clearly present, readable, and aligned to tables in the sheets:\n- Inputs sheet: A Scenarios table with columns (flexible naming but unambiguous): Scenario, Primary Term (years), Base Rent ($/SF per month), Annual Escalator (%). Also verify Suite # and Suite Size (SF) fields are present as editable variables.\n- Annual Matrix sheet: A combined annual table with columns: Scenario, Year, Monthly Rent, $/SF, Annual Base Rent. A clearly labeled section or table that shows Gross Lease Value by Scenario at the bottom of the annual breakdown.\n- Monthly Matrix sheet: A combined monthly table with columns: Scenario, Month #, Monthly Rent, $/SF. A summary (near the top) showing Total Lease Value by Scenario.\n\nBe flexible with exact header wording (e.g., \u201cAnnual Base Rent\u201d vs \u201cAnnual Rent\u201d; \u201c$/SF\u201d vs \u201cRate ($/SF)\u201d ), but the meaning must be clear.\n\nScoring:\n- 3.0: All listed column headers and fields are present and clearly associated with their tables.\n- 2.0: Minor header naming differences but unambiguous; one field slightly unclear yet still parsable.\n- 1.0: Headers/fields partially present; likely ambiguous for parsing.\n- 0.0: Headers/fields largely missing or unclear.\nOnly check presence/clarity of headers, not the correctness of data or formulas.", "expectation": "Clear headers matching intent so automated checks can reliably find scenario, year/month, rents, and totals."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification of Correctness", "description": "Automated and LLM checks to validate dynamic behavior, internal consistency, and reasonableness of the rent matrices.", "is_required": false, "max_points": 9.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Parsable Inputs/Annual/Monthly Tables", "description": "Programmatically locate and parse the Inputs, Annual Matrix, and Monthly Matrix tables with flexible header matching. Confirm 3 distinct scenarios exist.", "weight": 0.8, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output\"\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheet_names = [s for s in xls.sheet_names]\n        def find_sheet(candidates):\n            for s in sheet_names:\n                sl = s.lower()\n                if any(c in sl for c in candidates):\n                    return s\n            return None\n        sh_inputs = find_sheet([\"input\", \"assumption\", \"parameter\"])\n        sh_annual = find_sheet([\"annual\"]) \n        sh_monthly = find_sheet([\"month\"]) \n        if not (sh_inputs and sh_annual and sh_monthly):\n            # Partial score if at least two found\n            found = sum([sh_inputs is not None, sh_annual is not None, sh_monthly is not None])\n            return (found/3.0), f\"Sheets found: inputs={sh_inputs}, annual={sh_annual}, monthly={sh_monthly}\"\n        # Utility to locate header row by required keywords\n        def find_table(df, req_cols_groups):\n            # req_cols_groups: list of lists of synonyms per column\n            for i in range(min(50, len(df))):\n                row = df.iloc[i].astype(str).str.strip().str.lower().tolist()\n                for idx in range(len(row)):\n                    row[idx] = re.sub(r\"\\s+\", \" \", row[idx])\n                ok = True\n                col_map = {}\n                for group in req_cols_groups:\n                    present = False\n                    for syn in group:\n                        if any(syn in c for c in row):\n                            present = True\n                            break\n                    if not present:\n                        ok = False\n                        break\n                if ok:\n                    # Use this row as header\n                    df2 = df.iloc[i+1:].copy()\n                    df2.columns = df.iloc[i].astype(str).tolist()\n                    # drop all-empty columns\n                    df2 = df2.loc[:, [not all(pd.isna(df2[c])) for c in df2.columns]]\n                    return df2\n            return None\n        # Read sheets raw\n        df_in_raw = pd.read_excel(path, sheet_name=sh_inputs, header=None)\n        df_an_raw = pd.read_excel(path, sheet_name=sh_annual, header=None)\n        df_mo_raw = pd.read_excel(path, sheet_name=sh_monthly, header=None)\n        inputs_req = [[\"scenario\"], [\"term\", \"years\", \"year\"], [\"base\", \"rent\", \"/sf\"], [\"escal\", \"%\"]]\n        annual_req = [[\"scenario\"], [\"year\", \"yr\"], [\"monthly\", \"rent\"], [\"$/sf\", \"/sf\", \"rate\"], [\"annual\", \"base\"]]\n        monthly_req = [[\"scenario\"], [\"month\"], [\"monthly\", \"rent\"], [\"$/sf\", \"/sf\", \"rate\"]]\n        df_in = find_table(df_in_raw, inputs_req)\n        df_an = find_table(df_an_raw, annual_req)\n        df_mo = find_table(df_mo_raw, monthly_req)\n        score = 0.0\n        feedback = []\n        if df_in is not None:\n            score += 1/3\n        else:\n            feedback.append(\"Inputs table not found/parsable\")\n        if df_an is not None:\n            score += 1/3\n        else:\n            feedback.append(\"Annual Matrix table not found/parsable\")\n        if df_mo is not None:\n            score += 1/3\n        else:\n            feedback.append(\"Monthly Matrix table not found/parsable\")\n        # Check 3 scenarios if inputs parsed\n        if df_in is not None:\n            # find scenario column\n            scen_col = None\n            for c in df_in.columns:\n                cl = str(c).lower()\n                if \"scenario\" in cl:\n                    scen_col = c\n                    break\n            if scen_col is not None:\n                uniq = df_in[scen_col].dropna().astype(str).str.strip().unique()\n                if len(uniq) >= 3:\n                    score = min(1.0, score)  # keep as ratio, already <=1\n                else:\n                    score *= 0.8\n                    feedback.append(f\"Expected >=3 scenarios, found {len(uniq)}\")\n        return max(0.0, min(1.0, score)), \"; \".join(feedback) if feedback else \"OK\"\n    except Exception as e:\n        return 0.0, f\"Error parsing workbook: {e}\""}, {"type": "code", "name": "Term Bounds Reflected in Monthly Rows", "description": "For each scenario, expected months = term*12 from Inputs. Verify Monthly Matrix shows rows up to expected month and does not extend beyond (no months > expected).", "weight": 0.8, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        def find_sheet(cands):\n            for s in xls.sheet_names:\n                if any(c in s.lower() for c in cands):\n                    return s\n            return None\n        sh_inputs = find_sheet([\"input\", \"assumption\", \"parameter\"]) \n        sh_monthly = find_sheet([\"month\"]) \n        if not (sh_inputs and sh_monthly):\n            return 0.0\n        df_in_raw = pd.read_excel(path, sheet_name=sh_inputs, header=None)\n        df_mo_raw = pd.read_excel(path, sheet_name=sh_monthly, header=None)\n        def find_table(df, req_cols_groups):\n            for i in range(min(50, len(df))):\n                row = df.iloc[i].astype(str).str.strip().str.lower().tolist()\n                row = [re.sub(r\"\\s+\", \" \", x) for x in row]\n                ok = True\n                for group in req_cols_groups:\n                    present = any(any(syn in c for c in row) for syn in group)\n                    if not present:\n                        ok = False\n                        break\n                if ok:\n                    df2 = df.iloc[i+1:].copy()\n                    df2.columns = df.iloc[i].astype(str).tolist()\n                    return df2\n            return None\n        df_in = find_table(df_in_raw, [[\"scenario\"],[\"term\",\"year\"],[\"base\"],[\"escal\"]])\n        df_mo = find_table(df_mo_raw, [[\"scenario\"],[\"month\"],[\"monthly\",\"rent\"]])\n        if df_in is None or df_mo is None:\n            return 0.0\n        # Identify columns\n        def pick_col(cols, keys):\n            for c in cols:\n                cl = str(c).lower()\n                if all(k in cl for k in keys):\n                    return c\n                for k in keys:\n                    if k in cl:\n                        return c\n            return None\n        c_scen_in = pick_col(df_in.columns, [\"scenario\"]) \n        c_term = pick_col(df_in.columns, [\"term\"]) \n        c_scen_mo = pick_col(df_mo.columns, [\"scenario\"]) \n        c_month = pick_col(df_mo.columns, [\"month\"]) \n        c_morent = pick_col(df_mo.columns, [\"monthly\"]) or pick_col(df_mo.columns, [\"rent\"])\n        if not all([c_scen_in, c_term, c_scen_mo, c_month, c_morent]):\n            return 0.0\n        # Build expected months map\n        exp = {}\n        for _, r in df_in[[c_scen_in, c_term]].dropna().iterrows():\n            try:\n                scen = str(r[c_scen_in]).strip()\n                term_years = float(str(r[c_term]).strip().replace('%',''))\n                exp[scen] = int(round(term_years * 12))\n            except:\n                continue\n        if len(exp) == 0:\n            return 0.0\n        # Evaluate per scenario\n        scores = []\n        for scen, exp_months in exp.items():\n            sub = df_mo[df_mo[c_scen_mo].astype(str).str.strip()==scen]\n            if sub.empty:\n                scores.append(0.0)\n                continue\n            months = pd.to_numeric(sub[c_month], errors='coerce').dropna().astype(int)\n            rents = pd.to_numeric(sub[c_morent], errors='coerce')\n            # only consider rows with any numeric month or rent\n            if months.empty:\n                scores.append(0.0)\n                continue\n            max_mon = months.max()\n            over = (months > exp_months).sum()\n            # expected that max month == exp_months and no over\n            sc = 1.0\n            if max_mon != exp_months:\n                sc -= 0.5\n            if over > 0:\n                sc -= 0.5\n            scores.append(max(0.0, sc))\n        if not scores:\n            return 0.0\n        return float(np.mean(scores))\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Annual vs Monthly Totals Match by Scenario", "description": "Sum Annual Base Rent per Scenario vs sum of Monthly Rent per Scenario; they should match (within tolerance).", "weight": 0.4, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        def find_sheet(cands):\n            for s in xls.sheet_names:\n                if any(c in s.lower() for c in cands):\n                    return s\n            return None\n        sh_annual = find_sheet([\"annual\"]) \n        sh_monthly = find_sheet([\"month\"]) \n        if not (sh_annual and sh_monthly):\n            return 0.0\n        df_an_raw = pd.read_excel(path, sheet_name=sh_annual, header=None)\n        df_mo_raw = pd.read_excel(path, sheet_name=sh_monthly, header=None)\n        def find_table(df, req_cols_groups):\n            for i in range(min(60, len(df))):\n                row = df.iloc[i].astype(str).str.strip().str.lower().tolist()\n                row = [re.sub(r\"\\s+\", \" \", x) for x in row]\n                ok = True\n                for group in req_cols_groups:\n                    present = any(any(syn in c for c in row) for syn in group)\n                    if not present:\n                        ok = False\n                        break\n                if ok:\n                    df2 = df.iloc[i+1:].copy()\n                    df2.columns = df.iloc[i].astype(str).tolist()\n                    return df2\n            return None\n        df_an = find_table(df_an_raw, [[\"scenario\"],[\"year\"],[\"monthly\"],[\"$/sf\",\"/sf\",\"rate\"],[\"annual\"]])\n        df_mo = find_table(df_mo_raw, [[\"scenario\"],[\"month\"],[\"monthly\",\"rent\"]])\n        if df_an is None or df_mo is None:\n            return 0.0\n        def pick_col(cols, keys):\n            for c in cols:\n                cl = str(c).lower()\n                if all(k in cl for k in keys):\n                    return c\n                for k in keys:\n                    if k in cl:\n                        return c\n            return None\n        c_scen_an = pick_col(df_an.columns, [\"scenario\"])\n        c_ann = pick_col(df_an.columns, [\"annual\"]) \n        c_scen_mo = pick_col(df_mo.columns, [\"scenario\"]) \n        c_morent = pick_col(df_mo.columns, [\"monthly\"]) or pick_col(df_mo.columns, [\"rent\"]) \n        if not all([c_scen_an, c_ann, c_scen_mo, c_morent]):\n            return 0.0\n        an_tot = df_an.groupby(df_an[c_scen_an].astype(str).str.strip())[c_ann].apply(lambda s: pd.to_numeric(s, errors='coerce').sum(min_count=1)).to_dict()\n        mo_tot = df_mo.groupby(df_mo[c_scen_mo].astype(str).str.strip())[c_morent].apply(lambda s: pd.to_numeric(s, errors='coerce').sum(min_count=1)).to_dict()\n        if not an_tot or not mo_tot:\n            return 0.0\n        scenarios = set(an_tot.keys()).intersection(set(mo_tot.keys()))\n        if not scenarios:\n            return 0.0\n        scores = []\n        for s in scenarios:\n            a = an_tot.get(s, np.nan)\n            m = mo_tot.get(s, np.nan)\n            if pd.isna(a) or pd.isna(m) or a==0:\n                scores.append(0.0)\n                continue\n            rel_err = abs(a - m) / max(1.0, abs(a))\n            scores.append(1.0 if rel_err <= 0.01 else (0.5 if rel_err <= 0.05 else 0.0))\n        return float(np.mean(scores)) if scores else 0.0\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Monthly Rent \u2248 ($/SF \u00d7 Suite Size) \u2014 Spot Check", "description": "Use Suite Size (SF) from Inputs and $/SF values from Annual or Monthly table to check Monthly Rent consistency on a few rows.", "weight": 0.1, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        def find_sheet(cands):\n            for s in xls.sheet_names:\n                if any(c in s.lower() for c in cands):\n                    return s\n            return None\n        sh_inputs = find_sheet([\"input\", \"assumption\", \"parameter\"]) \n        sh_annual = find_sheet([\"annual\"]) \n        if not (sh_inputs and sh_annual):\n            return 0.0\n        df_in = pd.read_excel(path, sheet_name=sh_inputs, header=None)\n        df_an = pd.read_excel(path, sheet_name=sh_annual, header=None)\n        # Find suite size (look for a cell containing 'suite' and 'sf' or 'square feet')\n        suite_sf = None\n        for i in range(min(80, len(df_in))):\n            row = df_in.iloc[i].astype(str).str.lower().tolist()\n            for j, val in enumerate(row):\n                if (\"suite\" in val or \"size\" in val) and (\"sf\" in val or \"square\" in val):\n                    # try next cell right as numeric\n                    for k in range(j+1, min(j+3, len(row))):\n                        try:\n                            v = float(str(df_in.iloc[i, k]).replace(',',''))\n                            if v>0:\n                                suite_sf = v\n                                break\n                        except:\n                            pass\n                if suite_sf is not None:\n                    break\n            if suite_sf is not None:\n                break\n        if suite_sf is None:\n            return 0.3  # partial if cannot find size\n        # Find annual table header\n        def find_table(df, req_cols_groups):\n            for i in range(min(60, len(df))):\n                row = df.iloc[i].astype(str).str.strip().str.lower().tolist()\n                row = [re.sub(r\"\\s+\", \" \", x) for x in row]\n                ok = True\n                for group in req_cols_groups:\n                    present = any(any(syn in c for c in row) for syn in group)\n                    if not present:\n                        ok = False\n                        break\n                if ok:\n                    df2 = df.iloc[i+1:].copy()\n                    df2.columns = df.iloc[i].astype(str).tolist()\n                    return df2\n            return None\n        df_an2 = find_table(df_an, [[\"scenario\"],[\"year\"],[\"monthly\"],[\"$/sf\",\"/sf\",\"rate\"]])\n        if df_an2 is None:\n            return 0.3\n        # Identify columns\n        def pick_col(cols, keys):\n            for c in cols:\n                cl = str(c).lower()\n                if all(k in cl for k in keys):\n                    return c\n                for k in keys:\n                    if k in cl:\n                        return c\n            return None\n        c_rate = pick_col(df_an2.columns, [\"/sf\"]) or pick_col(df_an2.columns, [\"rate\"]) or pick_col(df_an2.columns, [\"$/sf\"]) \n        c_mon = pick_col(df_an2.columns, [\"monthly\"]) or pick_col(df_an2.columns, [\"rent\"]) \n        if not (c_rate and c_mon):\n            return 0.3\n        # Take up to 6 non-null samples across scenarios/years\n        test = df_an2[[c_rate, c_mon]].copy()\n        test[c_rate] = pd.to_numeric(test[c_rate].astype(str).str.replace(',',''), errors='coerce')\n        test[c_mon] = pd.to_numeric(test[c_mon].astype(str).str.replace(',',''), errors='coerce')\n        test = test.dropna().head(6)\n        if test.empty:\n            return 0.3\n        oks = 0\n        for _, r in test.iterrows():\n            expected = r[c_rate] * suite_sf\n            if expected == 0:\n                continue\n            rel_err = abs(r[c_mon] - expected) / max(1.0, abs(expected))\n            if rel_err <= 0.02:\n                oks += 1\n        frac = oks / max(1, len(test))\n        # Map to score between 0 and 1\n        return float(frac)\n    except Exception:\n        return 0.0"}, {"type": "llm_judge", "name": "Escalation Occurs on Anniversary Months", "description": "Check that rents step up once every 12 months per scenario, aligned with the annual escalator, with no mid-year changes.", "weight": 3.0, "judge_prompt": "Evaluate the Monthly Matrix for each of the three scenarios:\n- Confirm the monthly rent is constant within each 12-month year and steps up on months 13, 25, 37, etc. (anniversary months) for as long as the scenario\u2019s term lasts.\n- The sequence should show discrete steps consistent with the stated annual escalator (do not calculate exact percentages; visually confirm reasonable step pattern and frequency of once per year).\n- Verify there are no mid-year step-ups; any increase should align to the 12-month boundaries.\n\nScoring:\n- 3.0: All three scenarios show correct once-per-year step-ups exactly at 12-month intervals for the full term; no mid-year anomalies observed.\n- 2.0: One minor anomaly (e.g., off-by-one display in one scenario) but broadly correct.\n- 1.0: Pattern present in only some scenarios or intermittently correct.\n- 0.0: No evident annual-step pattern or frequent mid-year changes.", "expectation": "Clear 12-month step-up pattern aligned to anniversaries for each scenario."}, {"type": "llm_judge", "name": "Dynamic Conditional Logic (Blanks Beyond Term)", "description": "Confirm that monthly rows beyond each scenario\u2019s term are blank (no spurious zeros/errors), and that the Monthly Matrix total summary is present at top.", "weight": 2.0, "judge_prompt": "Check the Monthly Matrix for dynamic behavior:\n- For the 3-year and 5-year scenarios, verify months beyond their terms are blank (no numbers, errors, or placeholders). The 10-year scenario shows data through month 120.\n- Confirm the sheet displays a total lease value by Scenario at the top under the Monthly Matrix title.\n\nScoring:\n- 2.0: All scenarios correctly suppress values beyond term; total summary by scenario is present and clearly labeled.\n- 1.0: Minor anomalies (e.g., a stray zero or formatting artifact), but the intent is clear and mostly correct.\n- 0.0: Many non-blank extraneous values beyond term or missing top summary.", "expectation": "Clean suppression of values beyond the term and a clear total summary at the top."}, {"type": "llm_judge", "name": "Cross-Checks: Annual vs Monthly Totals and $/SF Logic", "description": "Holistic consistency check that annual and monthly totals align, and $/SF times suite SF matches monthly rent magnitudes.", "weight": 2.0, "judge_prompt": "Perform a qualitative consistency check:\n- For each scenario, the total from the Annual Matrix (Gross Lease Value) should equal the total from the Monthly Matrix summary. Small formatting differences are acceptable; the figures should visibly match.\n- Spot-check that Monthly Rent values approximately equal ($/SF) \u00d7 (Suite Size in SF), recognizing rounding. Do not compute exact formulas; visually verify magnitudes align.\n\nScoring:\n- 2.0: Totals match for all scenarios and the $/SF \u00d7 SF relation looks consistent.\n- 1.0: Mostly consistent with one minor discrepancy.\n- 0.0: Significant mismatches between annual vs monthly totals or obvious inconsistency in $/SF \u00d7 SF relationship.", "expectation": "Visible agreement of totals across views and coherent unit-math between $/SF and monthly rent."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism", "description": "LLM assessment of presentation quality, usability, and stakeholder readiness.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Usability and Clarity for Stakeholders", "description": "Assess labeling, instructions, readability, and ease of use for non-technical leasing stakeholders.", "weight": 1.5, "judge_prompt": "Judge how easy the workbook is for a prospective tenant or leasing stakeholder to understand and use:\n- Are titles, headers, and labels clear and consistent across sheets?\n- Are editable fields clearly indicated (light blue) and grouped logically? Are non-editable areas visually distinct?\n- Is the flow intuitive (Inputs \u2192 Annual view \u2192 Monthly detail)? Any brief instructions included?\n\nScoring:\n- 1.5: Highly intuitive and well-labeled with clear edit cues and brief guidance.\n- 1.0: Generally clear with minor friction points.\n- 0.5: Some confusion or missing cues.\n- 0.0: Difficult to understand/use without guidance.", "expectation": "Clear, intuitive structure with obvious edit points and logical flow."}, {"type": "llm_judge", "name": "Professional Formatting and Visual Design", "description": "Evaluate color-coding for scenarios, consistent styling, print readiness, and overall polish.", "weight": 1.0, "judge_prompt": "Assess professional presentation:\n- Distinct color-coding per scenario used consistently across sheets; a restrained, accessible palette.\n- Clean formatting: alignment, number formats ($, %, thousands separators), freeze panes if helpful, and print areas set sensibly.\n- No clutter: minimal gridlines when not needed; helpful legends/keys if color used.\n\nScoring:\n- 1.0: Professional and consistent formatting throughout.\n- 0.7: Minor inconsistencies but overall professional.\n- 0.4: Mixed formatting quality.\n- 0.0: Poor, distracting, or inconsistent formatting.", "expectation": "Polished, consistent visuals suitable for client-facing sharing."}, {"type": "llm_judge", "name": "Notes and Documentation Quality", "description": "Evaluate the Notes section and any embedded guidance about assumptions, escalation timing, and how to edit inputs.", "weight": 1.5, "judge_prompt": "Review the Notes section (below the Annual Matrix) and any other guidance:\n- Does it explain key assumptions (e.g., escalation applied on anniversaries) and how to modify Inputs?\n- Are any caveats about rounding, display, or blank cells beyond term documented?\n- Is the tone concise and professional?\n\nScoring:\n- 1.5: Clear, useful, and concise documentation.\n- 1.0: Adequate but could be clearer or more complete.\n- 0.5: Minimal.\n- 0.0: No meaningful notes/documentation.", "expectation": "Concise guidance that aids accurate use and interpretation."}, {"type": "llm_judge", "name": "Dynamic Calculator UX and Robustness", "description": "Assess whether the model feels dynamic and robust (e.g., handles different input terms, rates) and communicates this to users.", "weight": 1.0, "judge_prompt": "Consider the overall user experience of the calculator:\n- Does the model appear robust to different inputs (e.g., term changes up to 10 years, escalator variations)?\n- Are any data validation, named ranges, or protections used appropriately (optional but a plus)?\n- Are errors or edge cases handled gracefully (e.g., no #N/A/#DIV/0! showing)?\n\nScoring:\n- 1.0: Clearly robust and dynamic; edge cases handled; minimal risk of user error.\n- 0.7: Generally robust with minor exposure to errors.\n- 0.4: Some fragility visible.\n- 0.0: Brittle with visible errors or poor handling of variations.", "expectation": "A reliable, dynamic tool that feels safe to adjust and reuse."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "a97369c7-e5cf-40ca-99e8-d06f81c57d53", "rubric": {"category_name": "Legal Memo \u2014 Delaware Corporate Law Analysis (Avalon Bancorp)", "rationale": "This rubric enforces a self-documenting, file-based workflow for a Delaware corporate law memo. Stage 1 (LLM-only) strictly gates format and structure so later verification is trivial. Stage 2 mixes lightweight code checks (format, word count, citation presence) with LLM judges for substantive legal accuracy and application. Stage 3 evaluates overall professional quality, clarity, and client usefulness. Code-rule weights are kept ~5x lower than LLM rules on average in Stage 2, per guidance.", "max_total_score": 33.0, "stages": [{"name": "Stage 1 \u2014 Format and Structural Gate (LLM-only)", "description": "Gate that ensures the output is a properly structured legal memo in a verifiable document format with required sections and core citations present. Only checks presence/structure, not substantive correctness.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Structured Legal Memo Requirement", "description": "Verify the candidate produced a professional legal memo in the required file format and with required structural elements, headings, and incorporated authorities.", "weight": 8.0, "judge_prompt": "You are evaluating whether the candidate\u2019s output satisfies STRICT format/structure requirements for a legal memo. Do NOT judge substantive correctness here\u2014only presence and structure. Use the following checklist and scoring.\n\nDocument/Format Requirements:\n- File format: PDF or DOCX (not plain text, not Excel/CSV). At least roughly 1 page and no more than ~3,000 words (LLM-estimate OK).\n- Professional memo heading addressed to the client (Avalon): look for To/From/Date/Re (flexible on exact labels) and the client\u2019s name (e.g., \u201cAvalon\u201d or \u201cAvalon Bancorp\u201d).\n- Clear section headings corresponding to the three issues (flexible wording):\n  1) Authority of the board and enforceability of the stockholders\u2019 agreement with Marcus.\n  2) Fiduciary duty implications for the board in deferring to Marcus\u2019s veto.\n  3) Fiduciary duty implications for Marcus in blocking the deal for personal reasons.\n- Inclusion of a brief scope/assumptions statement acknowledging that (i) Marcus vetoed purely for personal animus and (ii) demand requirement is out of scope.\n- Citations/References: The memo should incorporate at least 6 of the following 10 authorities (flexible on citation format; inline or footnote acceptable):\n  \u2022 DGCL \u00a7 141; DGCL \u00a7 122; DGCL \u00a7 109\n  \u2022 Delaware Senate Bill 313\n  \u2022 West Palm Beach v. Moelis\n  \u2022 McMullin v. Beran\n  \u2022 Kahn v. Lynch\n  \u2022 In re Sears Hometown & Outlet Stores, Inc.\n  \u2022 Voigt v. Metcalf\n  \u2022 Basho Tech. v. Georgetown Basho Investors\n\nScoring (return a single numeric score out of 8):\n- 8: PDF/DOCX; memo heading to Avalon; all three required issue sections present with clear headers; scope/assumptions noted; cites \u22656 listed authorities; within length.\n- 6\u20137: PDF/DOCX; three sections present; minor omission (e.g., heading labels not explicit, scope statement implicit, or only 4\u20135 authorities) but overall structurally compliant and within length.\n- 3\u20135: Valid PDF/DOCX but missing one required issue section OR cites only 1\u20133 authorities OR no visible memo addressing; still recognizably a memo.\n- 0\u20132: Wrong format (not PDF/DOCX) OR missing multiple issue sections OR not a memo OR egregiously over length (e.g., clearly far beyond 3,000 words).", "expectation": "A professional PDF/DOCX memo addressed to Avalon that clearly structures the three requested analyses, states the assumptions/scope, and references most of the listed authorities."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Substantive Verification (Code + LLM)", "description": "Checks substantive correctness and application to facts under Delaware law, with lightweight code checks for objective elements and LLM judges for legal analysis and reasoning.", "is_required": false, "max_points": 15.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Length and Format Compliance (Objective)", "description": "Confirms the output is a document file and within the 3,000-word limit, with basic memo addressing indicators.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float in [0,1] or (float, str)\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = \"\"\n    # Try PDF, then DOCX, then text\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n\n    if not text or not isinstance(text, str):\n        return 0.0\n\n    words = re.findall(r\"\\b\\w+\\b\", text)\n    wc = len(words)\n    if wc == 0:\n        return 0.0\n\n    # Word count score: full credit if <= 3000; slight tolerance up to 3300\n    if wc <= 3000:\n        length_score = 1.0\n    elif wc <= 3300:\n        length_score = max(0.5, 3000.0 / max(1.0, wc))\n    else:\n        length_score = max(0.2, 3000.0 / float(wc))\n\n    tl = text.lower()\n    addressing_hits = 0\n    if \"avalon\" in tl or \"avalon bancorp\" in tl:\n        addressing_hits += 1\n    # Look for memo-style fields\n    memo_fields = sum([1 for k in [\"to:\", \"from:\", \"date:\", \"re:\", \"subject:\"] if k in tl])\n    if memo_fields >= 2:\n        addressing_hits += 1\n\n    addressing_score = addressing_hits / 2.0  # 0, 0.5, or 1.0\n\n    # Final score emphasizes length compliance\n    score = 0.7 * length_score + 0.3 * addressing_score\n    score = max(0.0, min(1.0, score))\n    return score"}, {"type": "code", "name": "Citations to Listed Authorities (Objective)", "description": "Checks for presence of citations to at least six of the ten specified authorities using flexible string/regex matching.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Returns a score in [0,1] proportional to how many of the 10 target authorities are referenced.\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n    if not text:\n        return 0.0\n\n    tl = text.lower()\n\n    patterns = [\n        r\"(dgcl\\s*\u00a7?\\s*141|8\\s*del\\.?\\s*c\\.?\\s*\u00a7\\s*141|section\\s+141\\b)\",\n        r\"(dgcl\\s*\u00a7?\\s*122|8\\s*del\\.?\\s*c\\.?\\s*\u00a7\\s*122|section\\s+122\\b)\",\n        r\"(dgcl\\s*\u00a7?\\s*109|8\\s*del\\.?\\s*c\\.?\\s*\u00a7\\s*109|section\\s+109\\b)\",\n        r\"(senate\\s+bill\\s*313|sb\\s*313|bill\\s*313|delaware\\s+senate\\s+bill\\s*313)\",\n        r\"(moelis)\",\n        r\"(mcmullin)\",\n        r\"(kahn\\s+v\\.?\\s+lynch|lynch\\s*\\b.*\\b\\s*638|638\\s*a\\.?2d\\s*1110)\",\n        r\"(sears\\s+hometown)\",\n        r\"(voigt\\s+v\\.?\\s+metcalf|voigt\\b)\",\n        r\"(basho)\"\n    ]\n\n    hits = 0\n    for pat in patterns:\n        try:\n            if re.search(pat, tl):\n                hits += 1\n        except re.error:\n            continue\n\n    # Score based on fraction of authorities found\n    score = min(1.0, hits / 10.0)\n    return score"}, {"type": "llm_judge", "name": "Board Authority and Enforceability Analysis (DGCL 141/109/122; Moelis; SB 313)", "description": "Assesses whether the memo accurately explains Delaware law on board authority and the enforceability of stockholder agreements limiting board discretion, including the implications of Moelis and Delaware Senate Bill 313.", "weight": 5.0, "judge_prompt": "Evaluate the section analyzing the board\u2019s authority and the enforceability of the stockholders\u2019 agreement. Look for:\n- Correct statement of DGCL \u00a7 141(a)\u2019s vesting of corporate management in the board and limits on contracting away core board authority (e.g., via stockholder agreements), and the roles of DGCL \u00a7\u00a7 109 and 122.\n- Accurate, nuanced use of West Palm Beach v. Moelis regarding provisions like investor approval rights and appointment rights, and when such covenants risk impermissibly constraining the board.\n- Consideration of Delaware Senate Bill 313 and any updates to corporate power to contract (e.g., DGCL \u00a7 122(18) or analogous changes), including limits and how (or whether) they affect Moelis-style analysis.\n- Application to Avalon\u2019s facts: investor appointment of directors/officers and blanket pre-approval/veto of transactions; whether and to what extent those provisions are enforceable.\nScoring (0\u20135, return normalized 0\u20131):\n- 1.0: Legally accurate and balanced; correctly integrates Moelis with DGCL and SB 313; applies reasoning to Avalon\u2019s facts with clear conclusions and caveats.\n- 0.6\u20130.9: Mostly accurate with minor omissions or nuance issues; still applies to facts.\n- 0.3\u20130.5: Partial understanding; cites authorities but misses key limitations or misapplies to facts.\n- 0.0\u20130.2: Materially incorrect or largely non-responsive.", "expectation": "Clear, accurate explanation of the limits of stockholder agreements under Delaware law, integrating Moelis and SB 313, and applying them to Avalon\u2019s specific provisions."}, {"type": "llm_judge", "name": "Board Fiduciary Duties in Deferring to Investor Veto", "description": "Evaluates the analysis of fiduciary duties when the board declines a lucrative deal to comply with an investor\u2019s contractual veto right.", "weight": 4.0, "judge_prompt": "Assess the memo\u2019s treatment of the board\u2019s fiduciary duties where a value-enhancing transaction (10x revenue) is declined to honor an investor\u2019s veto. Look for:\n- Duty of loyalty/care and good faith; risks of abdication of directorial duty or undue deference to a stockholder\u2019s personal interests.\n- Standards of review discussed with appropriate triggers (e.g., business judgment vs. enhanced scrutiny vs. entire fairness if a controller is implicated), and recognition of cleansing doctrines when applicable.\n- Use of relevant authorities (e.g., McMullin v. Beran and related sale-process principles; interplay with Moelis constraints; recognition that adhering to an invalid covenant cannot justify fiduciary breaches).\n- Specific application to facts and identification of litigation exposure (e.g., potential claims for breach of fiduciary duty for rejecting the Velridge deal for reasons unrelated to Avalon\u2019s best interests).\nScoring (0\u20134, return normalized 0\u20131):\n- 1.0: Accurate, nuanced standards and applications; identifies key risks/cases and explains likely scrutiny.\n- 0.6\u20130.9: Generally solid with minor gaps.\n- 0.3\u20130.5: Partial or superficial; standards unclear.\n- 0.0\u20130.2: Mostly incorrect or non-responsive.", "expectation": "A clear, standards-driven fiduciary analysis that applies Delaware doctrines to the board\u2019s decision to defer to Marcus\u2019s veto."}, {"type": "llm_judge", "name": "Marcus\u2019s Fiduciary Status and Conduct", "description": "Assesses whether the memo correctly analyzes when a minority investor with contractual rights may be a controlling stockholder and what fiduciary duties apply; applies those duties to Marcus\u2019s personal-animus veto.", "weight": 4.0, "judge_prompt": "Evaluate the analysis of Marcus\u2019s fiduciary status and conduct. Look for:\n- Correct definition of a \u201ccontroller\u201d (actual control over corporate conduct) even without majority ownership; factors including veto rights, appointment power, and practical domination.\n- Accurate use of Kahn v. Lynch (controller duties and entire fairness), Voigt v. Metcalf (control via contractual leverage), Basho Tech (investor overreach/coercion and control), and In re Sears Hometown (controller conduct in transactional contexts).\n- Application to the facts: whether Marcus\u2019s appointment rights and blanket pre-approval right could render him a controller; analysis of his veto for personal reasons as implicating the duty of loyalty and likely entire fairness if he is a controller.\nScoring (0\u20134, return normalized 0\u20131):\n- 1.0: Thorough, accurate controller analysis with well-supported application to Marcus; addresses alternative views if not a controller.\n- 0.6\u20130.9: Mostly correct with minor omissions.\n- 0.3\u20130.5: Partial/incomplete framework or weak application.\n- 0.0\u20130.2: Materially incorrect.", "expectation": "A precise controller analysis with faithful use of cited cases and clear application to Marcus\u2019s personal-animus veto."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Client Usefulness (LLM)", "description": "Holistic assessment of writing quality, organization, and practical value to the client. Not about correctness of law per se, but presentation and usefulness.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Organization, Clarity, and Professional Tone", "description": "Checks whether the memo is well-structured, readable, and professionally presented for a corporate client.", "weight": 3.0, "judge_prompt": "Evaluate organization and clarity: logical flow, clear headings/subheadings, concise paragraphs, neutral/objective tone, effective topic sentences, and professional formatting suitable for a client legal memo. Consider whether an executive summary or upfront key takeaways are provided (not required, but a plus). Score 0\u20133 (return normalized 0\u20131) based on overall readability and professionalism.", "expectation": "A cleanly organized, concise memo with a professional, neutral tone and easy navigation for a client reader."}, {"type": "llm_judge", "name": "Actionable Guidance and Risk Management", "description": "Assesses whether the memo provides practical next steps, risk framing, and strategic options for Avalon.", "weight": 3.0, "judge_prompt": "Assess the extent to which the memo gives actionable guidance: identifies plausible litigation risks and claims, suggests process improvements (e.g., special committees, independent advice, documenting business rationale), potential remedial steps (e.g., seeking waivers, amendments, or declaratory relief), and outlines strategic options under different controller/non-controller scenarios. Score 0\u20133 (return normalized 0\u20131) based on practicality and specificity of recommendations.", "expectation": "Concrete, prioritized recommendations that Avalon can act on, calibrated to the legal analysis."}, {"type": "llm_judge", "name": "Citation Integration and Transparency of Assumptions", "description": "Checks how well authorities are integrated and whether assumptions/scope limits are transparent.", "weight": 2.0, "judge_prompt": "Evaluate whether the memo integrates citations smoothly (inline or footnotes), uses accurate references (pin cites if provided), and clearly states assumptions/scope (e.g., personal-animus assumption; demand requirement excluded). Score 0\u20132 (return normalized 0\u20131) based on coherence of citation use and clarity about assumptions/limits.", "expectation": "Citations used to support key propositions, with clear assumptions and scope boundaries."}, {"type": "llm_judge", "name": "Client-Focused Communication", "description": "Assesses whether the memo is tailored to Avalon\u2019s needs and avoids unnecessary jargon.", "weight": 2.0, "judge_prompt": "Assess whether the memo is tailored to a corporate client audience (Avalon), avoids unnecessary legalese, defines terms where needed, and keeps conclusions accessible without sacrificing accuracy. Score 0\u20132 (return normalized 0\u20131).", "expectation": "Client-centric communication that is accessible, accurate, and focused on Avalon\u2019s practical concerns."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "40a8c4b1-b169-4f92-a38b-7f79685037ec", "rubric": {"category_name": "Grand Rounds 2025 Schedule (Otolaryngology)", "rationale": "This rubric enforces a self-documenting Excel workbook that makes verification trivial. Stage 1 mandates a precise workbook shape (sheets, grid, log) so that Stage 2 code/LLM checks can deterministically validate constraints (In\u2011Service placement, semiannual All Periop, required topic coverage, holidays). Stage 3 evaluates professional quality and auditability for stakeholders in a medical education context.", "max_total_score": 30.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate", "description": "LLM-only gate. Ensures the output is an Excel workbook with the exact structure required to verify scheduling rules: a properly named schedule sheet with Column C populated for Wednesdays, a Topics & Labs sheet with optional items highlighted if unused, and a Scheduling Log sheet that maps each scheduled session to its source/type.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Workbook and Sheets Structure", "description": "Checks the candidate produced a valid Excel workbook with mandatory sheets and layout that enable verification.", "weight": 4.0, "judge_prompt": "You are evaluating the STRUCTURE ONLY (not correctness) of an Excel workbook for the Otolaryngology Grand Rounds 2025 schedule.\n\nConfirm ALL of the following:\n\nA) File format and identity\n- The output is an Excel workbook (.xlsx), not a PDF/Word/plain text.\n- The workbook name or visible title indicates \u201cGrand Rounds 2025\u201d (flexible on exact punctuation/spacing).\n\nB) Required sheets exist\n- A sheet named exactly \u201c2025 Grand Rounds Schedule\u201d (renamed from a template\u2019s \u201c2025 Blank Schedule\u201d).\n- A sheet named \u201cTopics & Labs\u201d.\n- A sheet named \u201cScheduling Log\u201d.\n\nC) Scheduling Log table exists and is tabular with clear header row, containing at minimum these columns (flexible naming, but the meaning must be clear):\n- Date (per-session calendar date)\n- Topic (the scheduled topic title)\n- Source Type (one of: Scheduled Meeting, Required, Optional)\n- Source Name/Origin (e.g., \u201cScheduled Meetings.docx\u201d, \u201cTopics & Labs\u201d)\n- Quarter (Q1/Q2/Q3/Q4) or a date-based equivalent\n- Notes/Condition Tags (brief note referencing priority/condition when relevant)\n\nScoring:\n- 4.0: All A\u2013C satisfied; headers are clear and the log is obviously usable for verification.\n- 3.0: Excel format + all required sheets present, but Scheduling Log is missing one minor column or its headers are ambiguous.\n- 2.0: Excel format + some required sheets present (at least the schedule sheet), but either the Scheduling Log sheet is missing or clearly not tabular.\n- 0.0: Not an Excel workbook OR missing the \u201c2025 Grand Rounds Schedule\u201d sheet.\n\nOnly assess presence and structure, not the correctness of content.", "expectation": "A clean .xlsx workbook named for 2025 that includes sheets: 2025 Grand Rounds Schedule, Topics & Labs, Scheduling Log (with the specified columns)."}, {"type": "llm_judge", "name": "Schedule Grid and Column C Population", "description": "Checks the schedule sheet\u2019s grid exists and Column C is used for topics in bordered cells (excluding header row).", "weight": 3.0, "judge_prompt": "On the sheet \u201c2025 Grand Rounds Schedule\u201d, inspect the visible grid.\n\nVerify:\n- The sheet appears to be a weekly schedule grid for 2025, with rows representing Wednesday dates (or equivalent per-week entries). Metadata like 7:00\u20139:00 AM may be in the header or a visible location.\n- Column C contains the scheduled session names for all bordered cells that represent active Wednesdays (exclude header row 1). Holidays may be blank or labeled as holiday; do not penalize for holidays being blank.\n- The bordered cells (excluding row 1) in Column C are populated where a session is expected.\n\nScoring:\n- 3.0: Clear weekly grid; Column C is consistently used for session titles across the year; holidays are clearly indicated or left appropriately blank.\n- 2.0: Mostly correct but with noticeable gaps or some misplacement; overall shape still supports verification.\n- 1.0: Grid exists but Column C usage is inconsistent or sparse.\n- 0.0: No recognizable weekly grid OR Column C not used for session titles.", "expectation": "A recognizable weekly grid with Column C containing session titles for the Wednesday sessions, excluding holiday exceptions."}, {"type": "llm_judge", "name": "Unused Optional Highlighting Presence", "description": "Checks that unused optional topics/labs are visibly highlighted in yellow on the Topics & Labs sheet.", "weight": 1.0, "judge_prompt": "On the sheet \u201cTopics & Labs\u201d, check the section/list of Optional items.\n\nVerify:\n- Optional topics/labs that were NOT scheduled are visibly highlighted in yellow.\n- The highlighting is clearly applied to the unused items (cells or rows), not used items.\n\nScoring:\n- 1.0: Clear yellow highlighting on unused optional items; obvious intent and consistency.\n- 0.5: Some highlighting exists but is partial or inconsistent.\n- 0.0: No yellow highlighting for unused optional items is visible.\n\nOnly check that highlighting exists for unused optional items; do not verify exact counts here.", "expectation": "Unused optional items are clearly highlighted yellow on Topics & Labs."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness Verification", "description": "Mixed code + LLM checks to verify scheduling constraints, required inclusions, and cross-references now that the workbook is in a verifiable shape.", "is_required": false, "max_points": 14.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "File Type and Naming Sanity Check", "description": "Verifies the output is a spreadsheet and its name indicates Grand Rounds 2025.", "weight": 0.5, "code": "import os\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No primary output.\"\n    if not output.is_spreadsheet:\n        return 0.0, \"Output is not a spreadsheet (.xlsx).\"\n\n    # Try to infer name from resource; fall back to path\n    name = getattr(output, 'name', '') or ''\n    try:\n        path = context.files.get_path(output.id)\n        if not name:\n            name = os.path.basename(str(path))\n    except Exception:\n        path = None\n\n    lname = (name or '').lower()\n    ok_name = any(k in lname for k in [\n        'grand rounds 2025', 'grand_rounds_2025', 'grandrounds2025',\n        'grand-rounds-2025', 'grandrounds 2025'\n    ])\n    if ok_name:\n        return 1.0, f\"Spreadsheet name looks correct: {name}\"\n    else:\n        return 0.6, f\"Spreadsheet detected, but name may not indicate 2025 grand rounds: {name}\""}, {"type": "code", "name": "In-Service Study Session in Late February (Wednesday)", "description": "Checks that an In-Service Study Session is scheduled on the last or second-to-last Wednesday of February 2025.", "weight": 1.0, "code": "import pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\n\ndef _find_col(df, keys):\n    cols = {c.lower(): c for c in df.columns}\n    for key in keys:\n        for lc, orig in cols.items():\n            if key in lc:\n                return orig\n    return None\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n\n    # Read Scheduling Log\n    try:\n        log_df = context.files.read_excel(output.id, sheet_name='Scheduling Log')\n    except Exception as e:\n        return 0.0, f\"Scheduling Log not readable: {e}\"\n\n    if log_df is None or log_df.empty:\n        return 0.0, \"Scheduling Log empty or missing.\"\n\n    date_col = _find_col(log_df, [\"date\"])\n    topic_col = _find_col(log_df, [\"topic\", \"session\"])\n    if not date_col or not topic_col:\n        return 0.0, \"Required columns (Date, Topic) not found in Scheduling Log.\"\n\n    # Parse dates\n    dates = pd.to_datetime(log_df[date_col], errors='coerce')\n    topics = log_df[topic_col].astype(str).str.lower().fillna(\"\")\n\n    # Compute last two Wednesdays of Feb 2025\n    feb_weds = pd.date_range('2025-02-01', '2025-02-28', freq='W-WED')\n    if len(feb_weds) == 0:\n        return 0.0, \"No Wednesdays found in Feb 2025 calculation.\"\n    last_wed = feb_weds[-1]\n    second_last_wed = feb_weds[-2] if len(feb_weds) >= 2 else None\n\n    # Check if In-Service Study Session appears on one of those dates\n    mask_dates = (dates.dt.date == last_wed.date())\n    if second_last_wed is not None:\n        mask_dates |= (dates.dt.date == second_last_wed.date())\n\n    on_target = log_df[mask_dates]\n    if on_target.empty:\n        return 0.0, \"No session on last/second-to-last Wednesday of Feb 2025.\"\n\n    # Look for in-service session\n    found = False\n    for _, row in on_target.iterrows():\n        t = str(row[topic_col]).lower()\n        if 'in-service' in t and ('study' in t or 'session' in t or 'study session' in t):\n            found = True\n            break\n    return (1.0 if found else 0.0,\n            \"In-Service Study Session found on target date.\" if found else \"Target date lacks In-Service Study Session.\")"}, {"type": "code", "name": "All Periop Semiannual Distribution", "description": "Verifies at least two All Periop meetings occur, ideally split across H1 and H2 of 2025.", "weight": 1.0, "code": "import pandas as pd\n\n\ndef _find_col(df, keys):\n    cols = {c.lower(): c for c in df.columns}\n    for key in keys:\n        for lc, orig in cols.items():\n            if key in lc:\n                return orig\n    return None\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n    try:\n        df = context.files.read_excel(output.id, sheet_name='Scheduling Log')\n    except Exception as e:\n        return 0.0, f\"Scheduling Log not readable: {e}\"\n    if df is None or df.empty:\n        return 0.0, \"Scheduling Log empty.\"\n\n    date_col = _find_col(df, [\"date\"])\n    topic_col = _find_col(df, [\"topic\", \"session\"])\n    if not date_col or not topic_col:\n        return 0.0, \"Missing Date/Topic columns.\"\n\n    dates = pd.to_datetime(df[date_col], errors='coerce')\n    topics = df[topic_col].astype(str).str.lower().fillna(\"\")\n    peri = topics.str.contains('periop') | topics.str.contains('peri-op') | topics.str.contains('all periop')\n    count = peri.sum()\n    if count < 2:\n        return 0.0, f\"Found {int(count)} All Periop sessions (need >=2).\"\n\n    # Check distribution across halves\n    months = dates.dt.month.where(peri, other=pd.NA).dropna()\n    h1 = (months <= 6).any()\n    h2 = (months >= 7).any()\n    if h1 and h2:\n        return 1.0, \"All Periop occurs at least twice and is split across H1/H2.\"\n    else:\n        return 0.5, \"At least two All Periop sessions found, but not split across halves.\""}, {"type": "code", "name": "Required Topics Coverage (Topics & Labs \u2192 Schedule)", "description": "Ensures each Required item on Topics & Labs appears at least once in the schedule.", "weight": 1.0, "code": "import pandas as pd\n\n\ndef _find_col(df, keys):\n    cols = {c.lower(): c for c in df.columns}\n    for key in keys:\n        for lc, orig in cols.items():\n            if key in lc:\n                return orig\n    return None\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n\n    try:\n        topics_df = context.files.read_excel(output.id, sheet_name='Topics & Labs')\n        log_df = context.files.read_excel(output.id, sheet_name='Scheduling Log')\n    except Exception as e:\n        return 0.0, f\"Error reading sheets: {e}\"\n\n    if topics_df is None or topics_df.empty or log_df is None or log_df.empty:\n        return 0.0, \"Missing or empty Topics & Labs or Scheduling Log.\"\n\n    type_col = _find_col(topics_df, [\"type\", \"category\", \"required\"])\n    title_col = _find_col(topics_df, [\"title\", \"topic\", \"name\"])\n    if not type_col or not title_col:\n        return 0.0, \"Topics & Labs lacks clear Type/Title columns.\"\n\n    req_df = topics_df[topics_df[type_col].astype(str).str.lower().str.contains('required')]\n    if req_df.empty:\n        # If no explicit required items listed, award partial credit\n        return 0.6, \"No explicit required items listed.\"\n\n    sched_title_col = _find_col(log_df, [\"topic\", \"session\", \"title\"])\n    if not sched_title_col:\n        return 0.0, \"Scheduling Log lacks Topic/Title column.\"\n\n    scheduled = log_df[sched_title_col].astype(str).str.lower().fillna(\"\").tolist()\n\n    total = 0\n    covered = 0\n    missing = []\n    for _, row in req_df.iterrows():\n        total += 1\n        req_title = str(row[title_col]).strip().lower()\n        if not req_title:\n            continue\n        if any(req_title in s for s in scheduled):\n            covered += 1\n        else:\n            missing.append(req_title)\n\n    if total == 0:\n        return 0.6, \"No parsable required titles.\"\n\n    frac = covered / total\n    feedback = f\"Covered {covered}/{total} required topics. Missing: {', '.join(missing[:5]) + ('...' if len(missing)>5 else '')}\"\n    return frac, feedback"}, {"type": "llm_judge", "name": "Schedule Completeness and Constraints", "description": "Verifies that every Wednesday in 2025 (except holidays) has a scheduled session 7\u20139 AM, MS4 Talks included, and priorities/conditions appear respected.", "weight": 4.0, "judge_prompt": "Examine the \u201c2025 Grand Rounds Schedule\u201d and the \u201cScheduling Log\u201d. Verify the following at a content level:\n- For 2025, each Wednesday has a corresponding schedule entry except on holidays (allow blank or marked holiday rows). The time window 7:00\u20139:00 AM is clearly indicated in the schedule context (header, notes, or per-entry time).\n- MS4 Talks are included somewhere in the year (can be grouped or scattered, consistent with departmental practice) and labeled clearly.\n- The distribution and ordering of sessions appears to adhere to the linked priorities/conditions (e.g., required items scheduled with precedence; reasonable sequencing). You may infer compliance from notes/tags in the Scheduling Log.\n\nScoring:\n- 4.0: All Wednesdays covered except holidays; 7\u20139 AM noted; MS4 Talks included; ordering/distribution plausibly matches priorities/conditions.\n- 3.0: Minor gaps/ambiguities (e.g., a few unclear weeks or time notation) but overall compliant.\n- 2.0: Noticeable gaps or unclear adherence to priorities.\n- 0.0\u20131.0: Many Wednesdays unscheduled or time window missing; priorities largely ignored.", "expectation": "Near-complete Wednesday coverage with 7\u20139 AM context, MS4 Talks present, and a plausible alignment with stated priorities/conditions."}, {"type": "llm_judge", "name": "Unused Optional Highlight Integrity", "description": "Checks that unused optional topics highlighted in yellow are truly unused (i.e., do not appear in the schedule).", "weight": 3.0, "judge_prompt": "Cross-compare the \u201cTopics & Labs\u201d sheet and the scheduled entries (via the \u201c2025 Grand Rounds Schedule\u201d and/or the \u201cScheduling Log\u201d).\n\n- Confirm that items highlighted in yellow as unused optional topics do NOT appear in the schedule.\n- If any highlighted-as-unused item appears scheduled, deduct significantly.\n\nScoring:\n- 3.0: All highlighted unused optionals are indeed unscheduled.\n- 2.0: One minor inconsistency.\n- 1.0: Several inconsistencies.\n- 0.0: Highlighting is misleading or mostly incorrect.", "expectation": "No yellow-highlighted optional item appears in the scheduled sessions."}, {"type": "llm_judge", "name": "Source Mapping and Required Inclusions", "description": "Validates that Scheduling Log source mapping is coherent and that mandated items (e.g., All Periop, MS4 Talks, School of Medicine required topics) are represented with correct source types.", "weight": 3.0, "judge_prompt": "Review the \u201cScheduling Log\u201d and overall workbook for these checks:\n- Each scheduled row has a clear Source Type (Scheduled Meeting, Required, Optional) and a source origin (e.g., \u201cScheduled Meetings.docx\u201d, \u201cTopics & Labs\u201d).\n- Mandated categories are represented: semiannual All Periop meetings, MS4 Talks blocks, and School of Medicine required topics (from Topics & Labs marked as Required).\n- The Source Type tagging looks consistent with the origin of the item.\n\nScoring:\n- 3.0: Clear mapping for nearly all entries; all mandated categories are present and correctly tagged.\n- 2.0: Mostly mapped correctly; one mandated category is ambiguous or tagging has minor errors.\n- 1.0: Multiple mapping errors or a mandated category appears missing/unclear.\n- 0.0: Little or no coherent source mapping.\n\nNote: Judge for internal consistency within the workbook; do not require external files.", "expectation": "Coherent source mapping with all mandated categories present and correctly tagged."}, {"type": "llm_judge", "name": "Holiday Handling (Spot Check)", "description": "Spot-checks that obvious holidays (e.g., New Year\u2019s Day 2025, a Wednesday) are not scheduled or are marked as holidays.", "weight": 1.0, "judge_prompt": "Spot-check holiday handling on the 2025 schedule. For example, New Year\u2019s Day 2025 falls on a Wednesday. Confirm that day is not scheduled for a regular session or is clearly labeled as a holiday.\n\nScoring:\n- 1.0: Clear holiday handling for the spot check(s).\n- 0.5: Ambiguous handling.\n- 0.0: Clearly scheduled a normal session on a known holiday without indication.", "expectation": "New Year\u2019s Day is not treated as a normal session; holidays are visibly respected."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism", "description": "Holistic LLM assessment of presentation quality, clarity, and auditability for stakeholders.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Readability", "description": "Evaluates formatting, clarity of headers, consistent styles, and ease of reading the schedule.", "weight": 2.0, "judge_prompt": "Assess overall professionalism of the workbook:\n- Clear titles, headers, and consistent formatting (fonts, borders, alignment).\n- Legible weekly grid and unambiguous session names.\n- Helpful touches like freeze panes, legends/keys for colors, or page setup.\n\nScoring:\n- 2.0: Highly professional and easy to read.\n- 1.0: Generally readable with minor inconsistencies.\n- 0.0: Cluttered, inconsistent, or hard to read.", "expectation": "Clean, consistent formatting suitable for distribution to clinicians and trainees."}, {"type": "llm_judge", "name": "Auditability and Change Traceability", "description": "Judges whether the workbook makes it easy to audit decisions and track changes (e.g., notes/condition tags, clear source mapping).", "weight": 3.0, "judge_prompt": "Review whether the workbook is self-documenting:\n- The Scheduling Log includes concise notes/condition tags referencing the applicable priority/condition.\n- Cross-references from schedule entries to their source are easy to trace.\n- Optional: presence of a brief README/Notes or a version/date stamp to clarify finalization.\n\nScoring:\n- 3.0: Excellent auditability and traceability.\n- 2.0: Generally good with minor gaps.\n- 1.0: Limited traceability; difficult to audit rationale.\n- 0.0: Little/no support for auditing decisions.", "expectation": "A reviewer can quickly see why each session is placed and from which source it came."}, {"type": "llm_judge", "name": "Educational Balance and Distribution", "description": "Assesses the balance of topics across quarters, avoiding clustering and ensuring a sensible cadence (labs vs talks).", "weight": 2.0, "judge_prompt": "Evaluate whether the schedule reflects thoughtful distribution:\n- Reasonable mix across quarters (Q1\u2013Q4) without heavy clustering of similar content.\n- Labs and talks interleaved sensibly; mandated items spaced appropriately (e.g., All Periop semiannually).\n\nScoring:\n- 2.0: Balanced and pedagogically sensible across the year.\n- 1.0: Some clustering or balance issues but acceptable.\n- 0.0: Poor balance or questionable cadence.", "expectation": "A well-paced, varied schedule across all four quarters."}, {"type": "llm_judge", "name": "Communication Completeness", "description": "Checks for small but valuable touches that aid end users (legends, notes to presenters, holiday callouts).", "weight": 1.0, "judge_prompt": "Look for communication supports that improve usability:\n- Legends/keys for color highlighting (e.g., unused optionals in yellow).\n- Holiday callouts or footnotes.\n- Any brief instructions to presenters/administrators.\n\nScoring:\n- 1.0: Helpful supporting notes/legends present.\n- 0.5: Minimal supporting notes.\n- 0.0: None present where clearly helpful.", "expectation": "Clear legends and notes that make the workbook self-explanatory."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "1b9ec237-bf9c-41f9-8fa9-0e685fcd93c6", "rubric": {"category_name": "HTN Lecture Presentation (Nurse Practitioner)", "rationale": "This rubric enforces a verifiable, self-documenting presentation deliverable. Stage 1 (LLM-only) mandates an inspectable slide deck shape (PDF with Notes Pages or PPTX plus PDF), with specific slide-level structure enabling downstream verification. Stage 2 mixes lightweight code checks (pattern/coverage) with heavier LLM judges for clinical correctness and internal consistency based on AHA guidance. Stage 3 assesses instructional quality and professional presentation for nursing students.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 Gate \u2014 Structured Presentation Format", "description": "Shape enforcement for a verifiable slide deck that includes all required sections, a single pre-test MCQ, case study, speaker notes, and references; \u226420 slides.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Presentation Shape and Structural Completeness", "description": "Verify the deliverable is a slide presentation in a verifiable format with required sections, \u226420 slides, speaker notes, and references.", "weight": 4.0, "judge_prompt": "You are evaluating whether the candidate produced a properly structured presentation deliverable that is easy to verify. Acceptable formats:\n- Preferred: A single PDF export of the slides WITH speaker notes visible (Notes Pages view or similar), OR\n- PPTX plus an accompanying PDF (slides or Notes Pages) that lets you see slide content and notes.\n\nCheck all of the following shape/structure requirements. Be flexible with exact slide titles, but the content must clearly exist as distinct slides/sections. The total number of slides must be 20 or fewer.\n\nRequired structural elements:\n1) A title slide.\n2) A single pre-test multiple-choice question (MCQ) on its own slide near the beginning, with 4 options (A\u2013D). The answer should not be revealed on that same slide.\n3) Definition of hypertension.\n4) Pathophysiology of hypertension.\n5) Risk factors (including modifiable and non-modifiable).\n6) Clinical signs and symptoms.\n7) Diagnostic methods (how HTN is diagnosed) AND a clearly labeled slide for AHA blood pressure categories/stages.\n8) An illustration that demonstrates how blood pressure is measured (e.g., correct cuff placement/positioning). This must be an actual image/diagram, not just text.\n9) Treatment options divided into pharmacologic and non-pharmacologic interventions (these can be separate slides or clearly separated sections).\n10) Patient education strategies.\n11) One case study featuring a patient with risk factors including smoking and a family history of cardiovascular disease (CVD), used to apply learning.\n12) A summary or key takeaways slide.\n13) Final references slide with properly formatted citations.\n14) Speaker notes present where necessary to support instructional delivery (at minimum for: pathophysiology, AHA stages, treatment, and case study slides). Notes must be visible in the PDF (Notes Pages) or otherwise clearly provided.\n\nSlide limit: 20 slides maximum (including title and references). If a PDF is used, page count should correspond to slide count (Notes Pages view is acceptable as long as the slides and notes are clearly visible and the total slides are \u226420).\n\nScoring:\n- 4.0: Acceptable format (PDF with notes pages OR PPTX+PDF); \u226420 slides; all required elements (1\u201314) clearly present.\n- 3.0: Acceptable format; \u226420 slides; missing exactly one required element OR speaker notes missing for one of the required note-bearing slides listed in item 14.\n- 2.0: Acceptable format but missing two to three required elements and/or no visible speaker notes.\n- 0.0: Wrong format (not PDF/PPTX, or cannot view slides/notes), OR more than 20 slides, OR missing four or more required elements.\n\nOnly assess structure/format and presence; do not judge medical correctness or quality here.", "expectation": "A \u226420-slide presentation in PDF (with notes) or PPTX+PDF, containing all specified sections, the pre-test MCQ, the case study, and a references slide; speaker notes visible for key slides."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Clinical Verification", "description": "Verify medical correctness and completeness of key clinical content (AHA staging, diagnostics, treatment, case application).", "is_required": true, "max_points": 10.0, "min_score_to_pass": 5.0, "rules": [{"type": "code", "name": "AHA Staging Signals Present (Numeric/Label Consistency)", "description": "Check extracted text for presence of AHA stage labels and characteristic BP thresholds (e.g., 120/80, 130/80, 140/90, 180/120). Partial credit if most are present.", "weight": 0.7, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float or (float, str)\n    \"\"\"\n    # Gather readable text from any accompanying PDF/DOCX/MD to avoid penalizing PPTX-only if PDF provided.\n    outputs = context.get_all_outputs() or []\n    if not outputs:\n        return 0.0, \"No outputs found.\"\n\n    combined_texts = []\n    for res in outputs:\n        try:\n            if getattr(res, 'is_document', False):\n                try:\n                    t = context.files.read_pdf_text(res.id)\n                    if t:\n                        combined_texts.append(t)\n                        continue\n                except Exception:\n                    pass\n                try:\n                    t = context.files.read_docx_text(res.id)\n                    if t:\n                        combined_texts.append(t)\n                        continue\n                except Exception:\n                    pass\n            if getattr(res, 'is_text_format', False):\n                try:\n                    t = context.files.read_text(res.id)\n                    if t:\n                        combined_texts.append(t)\n                except Exception:\n                    pass\n        except Exception:\n            continue\n\n    if not combined_texts:\n        return 0.0, \"No readable document text (PDF/DOCX/MD) found.\"\n\n    text = \" \\n\".join(combined_texts).lower()\n\n    # Look for stage/category labels and key numeric thresholds\n    label_hits = 0\n    label_targets = [\n        'normal', 'elevated', 'stage 1', 'stage i', 'stage one',\n        'stage 2', 'stage ii', 'stage two', 'hypertensive crisis', 'crisis'\n    ]\n    for lab in label_targets:\n        if lab in text:\n            label_hits += 1\n\n    # Numeric signals commonly associated with AHA stages\n    nums = ['120', '80', '130', '140', '180', '90']\n    num_hits = 0\n    for n in nums:\n        if re.search(r'\\b' + re.escape(n) + r'\\b', text):\n            num_hits += 1\n\n    # Score: need at least some labels and numbers; partial credit based on coverage\n    label_score = min(label_hits / 6.0, 1.0)  # expect ~6+ labels variants matched\n    num_score = min(num_hits / 5.0, 1.0)      # expect ~5+ numeric anchors\n    score = 0.7 * (0.6 * label_score + 0.4 * num_score)\n\n    feedback = f\"Labels matched: {label_hits}, Numbers matched: {num_hits}\"\n    return score, feedback"}, {"type": "code", "name": "Interventions Coverage (Pharm + Non-Pharm)", "description": "Detect presence of first-line pharmacologic classes and lifestyle strategies. Partial credit based on breadth.", "weight": 0.7, "code": "import re\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs() or []\n    if not outputs:\n        return 0.0, \"No outputs found.\"\n\n    combined_texts = []\n    for res in outputs:\n        try:\n            if getattr(res, 'is_document', False):\n                try:\n                    t = context.files.read_pdf_text(res.id)\n                    if t:\n                        combined_texts.append(t)\n                        continue\n                except Exception:\n                    pass\n                try:\n                    t = context.files.read_docx_text(res.id)\n                    if t:\n                        combined_texts.append(t)\n                        continue\n                except Exception:\n                    pass\n            if getattr(res, 'is_text_format', False):\n                try:\n                    t = context.files.read_text(res.id)\n                    if t:\n                        combined_texts.append(t)\n                except Exception:\n                    pass\n        except Exception:\n            continue\n\n    if not combined_texts:\n        return 0.0, \"No readable document text found.\"\n\n    text = \" \\n\".join(combined_texts).lower()\n\n    pharm = [\n        'thiazide', 'chlorthalidone', 'hydrochlorothiazide',\n        'ace inhibitor', 'ace-i', 'acei', 'lisinopril', 'enalapril',\n        'arb', 'losartan', 'valsartan',\n        'calcium channel blocker', 'ccb', 'amlodipine', 'diltiazem',\n        'beta blocker', 'metoprolol', 'atenolol'\n    ]\n    lifestyle = [\n        'dash diet', 'dietary approaches to stop hypertension',\n        'sodium', 'salt reduction', 'na+', 'low-sodium',\n        'exercise', 'physical activity', 'aerobic',\n        'weight loss', 'bmi', 'weight reduction',\n        'alcohol moderation', 'limit alcohol',\n        'smoking cessation', 'quit smoking', 'tobacco cessation',\n        'stress management', 'sleep', 'cpap', 'sleep apnea'\n    ]\n\n    pharm_hits = sum(1 for k in pharm if k in text)\n    life_hits = sum(1 for k in lifestyle if k in text)\n\n    # Expect at least 2 pharmacologic class signals and 3 lifestyle signals\n    pharm_score = min(pharm_hits / 4.0, 1.0)\n    life_score = min(life_hits / 5.0, 1.0)\n    score = 0.7 * (0.55 * pharm_score + 0.45 * life_score)\n\n    feedback = f\"Pharm hits: {pharm_hits}, Lifestyle hits: {life_hits}\"\n    return score, feedback"}, {"type": "code", "name": "Case Study Presence and Risk Factors", "description": "Check for a case study referencing smoking and family history of cardiovascular disease, with application elements.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs() or []\n    if not outputs:\n        return 0.0, \"No outputs found.\"\n\n    combined_texts = []\n    for res in outputs:\n        try:\n            if getattr(res, 'is_document', False):\n                try:\n                    t = context.files.read_pdf_text(res.id)\n                    if t:\n                        combined_texts.append(t)\n                        continue\n                except Exception:\n                    pass\n                try:\n                    t = context.files.read_docx_text(res.id)\n                    if t:\n                        combined_texts.append(t)\n                        continue\n                except Exception:\n                    pass\n            if getattr(res, 'is_text_format', False):\n                try:\n                    t = context.files.read_text(res.id)\n                    if t:\n                        combined_texts.append(t)\n                except Exception:\n                    pass\n        except Exception:\n            continue\n\n    if not combined_texts:\n        return 0.0, \"No readable document text found.\"\n\n    text = \" \\n\".join(combined_texts).lower()\n\n    has_case = ('case' in text) or ('scenario' in text)\n    has_smoking = 'smok' in text  # matches smoking/smoker\n    fam_hist_patterns = ['family history', 'fhx', 'family hx']\n    has_fhx = any(p in text for p in fam_hist_patterns)\n    cvd_terms = ['cardiovascular disease', 'cvd', 'coronary', 'heart disease', 'mi', 'stroke']\n    has_cvd = any(p in text for p in cvd_terms)\n    application_terms = ['assessment', 'plan', 'management', 'treatment', 'education', 'counsel']\n    has_application = any(p in text for p in application_terms)\n\n    hits = sum([has_case, has_smoking, has_fhx and has_cvd, has_application])\n    # Score progressively: 0, 0.2, 0.4, 0.6\n    score = 0.6 * (hits / 4.0)\n    feedback = f\"case:{has_case}, smoking:{has_smoking}, fhx_cvd:{has_fhx and has_cvd}, application:{has_application}\"\n    return score, feedback"}, {"type": "llm_judge", "name": "Clinical Accuracy \u2014 AHA Stages and Treatment Alignment", "description": "Evaluate correctness of AHA categories and whether treatment guidance aligns with contemporary AHA/ACC recommendations.", "weight": 4.0, "judge_prompt": "Evaluate the clinical correctness of the AHA blood pressure categories and treatment alignment. Consider ACC/AHA guidance (e.g., 2017 guideline and subsequent updates).\n\nCheck:\n- Correct AHA category names and thresholds (Normal, Elevated, Stage 1 HTN, Stage 2 HTN, Hypertensive Crisis). Thresholds typically include values like <120/<80 (Normal), 120\u2013129 and <80 (Elevated), 130\u2013139 or 80\u201389 (Stage 1), \u2265140 or \u226590 (Stage 2), and hypertensive crisis around \u2265180 and/or \u2265120. Allow for correct, guideline-consistent phrasing.\n- Initial pharmacologic therapy recommendations align with guidance (e.g., thiazide diuretics, ACE inhibitors, ARBs, or CCBs as first-line; considerations for CKD/diabetes; specific considerations for Black adults; avoidance of ACEi+ARB duplication).\n- Blood pressure targets are reasonable for adults (often <130/80 for many with confirmed HTN, context-dependent).\n- Internal consistency between stated goals, stages, and proposed treatment intensity.\n\nScoring:\n- 4.0: All categories and thresholds correct; treatment and targets align with guidance; no significant inaccuracies.\n- 3.0: One minor inaccuracy or omission but generally correct and aligned.\n- 2.0: Multiple inaccuracies or unclear alignment, but core ideas mostly present.\n- 1.0: Major inaccuracies in thresholds or treatment alignment.\n- 0.0: Fundamentally incorrect staging or unsafe/unsupported treatment guidance.", "expectation": "Accurate AHA staging and treatment recommendations consistent with accepted guidelines; coherent targets and step-up therapy rationale."}, {"type": "llm_judge", "name": "Diagnostics and BP Measurement Technique", "description": "Verify diagnostic approach and the accuracy of the BP measurement illustration/instructions.", "weight": 4.0, "judge_prompt": "Assess whether diagnostic methods and the BP measurement technique are accurately presented.\n\nLook for:\n- Proper BP measurement steps (rest 5 minutes, seated, back supported, feet flat, arm supported at heart level, correct cuff size, avoid caffeine/exercise/smoking 30 minutes prior, empty bladder, no talking, multiple readings/visits, validated device).\n- Illustration/diagram showing proper cuff placement and positioning (an actual image, not only text), clearly labeled.\n- Diagnostic criteria explained (use of averaged readings across occasions; confirmation/ABPM/HBPM as applicable; white-coat/masked HTN concepts acceptable if mentioned).\n- Alignment between diagnostic criteria and the AHA staging described elsewhere.\n\nScoring:\n- 4.0: All key measurement steps correct; clear illustration; diagnostics well explained and consistent.\n- 3.0: Minor omissions but overall accurate and consistent.\n- 2.0: Multiple omissions or unclear/confusing depiction; partial consistency.\n- 1.0: Significant inaccuracies in measurement or diagnostics.\n- 0.0: Incorrect or misleading measurement guidance; no illustration when one is required.", "expectation": "Clear, accurate measurement instructions with a visible illustration; consistent and correct diagnostic guidance."}], "on_failure_action": "skip_remaining", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Instructional and Presentation Quality", "description": "Holistic assessment of pedagogy, clarity, accessibility, and professional standards for nursing students.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Instructional Design and Flow", "description": "Checks logical sequencing, introduction of objectives, smooth transitions, and effective summary.", "weight": 2.0, "judge_prompt": "Evaluate the instructional design for nursing students: presence of clear learning objectives (explicit or implicit), logical flow from definition \u2192 pathophysiology \u2192 risks \u2192 signs/symptoms \u2192 diagnostics \u2192 treatment \u2192 patient education \u2192 case \u2192 summary, and the effective use of the pre-test to prime learning. Are transitions coherent and does the case apply concepts effectively? Score 0\u20132 with partial credit.", "expectation": "Well-sequenced, objective-driven flow culminating in applied learning via the case and a concise summary."}, {"type": "llm_judge", "name": "Speaker Notes Utility and Accuracy", "description": "Assess whether notes add instructional value without contradicting slides.", "weight": 1.5, "judge_prompt": "Review the speaker notes. Are they present and substantive on key slides (pathophysiology, AHA stages, treatment, case study)? Do they provide clarifications, cues, and clinical nuance without introducing inaccuracies or contradictions? Score 0\u20131.5 with partial credit.", "expectation": "Concise, clinically sound notes that enhance delivery and anticipate learner questions."}, {"type": "llm_judge", "name": "Visual Design and Accessibility", "description": "Evaluate readability, visual clarity, and accessibility for learners.", "weight": 1.5, "judge_prompt": "Assess slide design for professional readability: adequate font sizes, consistent layout, uncluttered visuals, meaningful use of color, sufficient contrast, and legible charts/illustrations (including the BP measurement image). Consider accessibility practices (e.g., avoiding color-only cues, describing visuals in notes). Score 0\u20131.5 with partial credit.", "expectation": "Clean, consistent visuals with strong readability and accessible design choices."}, {"type": "llm_judge", "name": "References Quality and Academic Rigor", "description": "Evaluate reference slide for currency and proper formatting.", "weight": 1.0, "judge_prompt": "Examine the references slide. Are sources recent and authoritative (e.g., ACC/AHA guidelines, peer-reviewed journals, reputable organizations)? Are citations properly formatted and specific (titles, authors, years/DOIs/URLs)? Do in-slide claims appear to be traceable to these sources? Score 0\u20131 with partial credit.", "expectation": "Properly formatted, current, authoritative references (e.g., ACC/AHA 2017 guideline and updates) supporting major claims."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "ae0c1093-5ea8-4b84-a81e-53ebf7a4321d", "rubric": {"category_name": "Undercover Operations Guide and Observation Form (Retail Investigation)", "rationale": "This rubric enforces a self-documenting, verification-friendly structure for two professional PDFs: a Guide and an Observation Form for undercover employee evaluation in retail settings. Stage 1 (LLM-only) strictly gates format and structural completeness so later checks are trivial. Stage 2 mixes lightweight code checks (existence, key text presence) with LLM judgments (layout, internal consistency, legal/ethical guardrails). Stage 3 provides a holistic quality assessment focusing on professional presentation, field usability, and appropriateness for Columbia, SC retail investigations.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (Documents Present and Properly Structured)", "description": "LLM-only gate to ensure the deliverables exist in the exact verifiable structure that enables later checks. Accepts either two separate PDFs or one combined PDF with both titled sections.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Document Format and Structure Requirements (Gate)", "description": "Verify the presence and structure of two required PDFs (or a single combined PDF with both sections). Only check structure/presence, not content quality.", "weight": 4.0, "judge_prompt": "You are evaluating whether the candidate provided the required deliverables in a verifiable structure. Review all provided outputs.\n\nFormat acceptance:\n- Accept EITHER:\n  A) Two separate PDFs:\n     1) \"Undercover Operations Guide: Employee Evaluation\"\n     2) \"Undercover Observation Form\"\n  OR\n  B) One combined PDF containing both documents as clearly separated sections with the exact titles above, each beginning on its own page.\n\nRequirements to verify:\n1) For the Guide (whether standalone PDF or a section within a combined PDF):\n   - Title appears exactly or near-exactly as: \"Undercover Operations Guide: Employee Evaluation\" at or near the top of the first page of the guide section.\n   - Contains a clearly labeled section heading: \"Purpose\" (or \"Purpose and Objectives\").\n   - In that Purpose section, the stated objective must explicitly convey: to discreetly observe and assess employee behavior within the organization (minor paraphrase acceptable: synonyms like \"covert/discreetly,\" \"observe,\" \"assess,\" and \"employee behavior\" should be present together).\n   - Contains clearly labeled operational sections enabling verification later (flexible wording allowed). Look for these labeled headings:\n     \u2022 Roles and/or Cover Identity\n     \u2022 Pre-Deployment Planning or Preparation\n     \u2022 Observation Protocol(s) and Documentation Procedures\n     \u2022 Evidence Handling / Chain of Custody\n     \u2022 Legal and Ethical Considerations\n     \u2022 Reporting and Debrief\n\n2) For the Observation Form (whether standalone PDF or a section within a combined PDF):\n   - Title appears exactly or near-exactly as: \"Undercover Observation Form\" at or near the top of the first page of the form section.\n   - The form includes page-level headers/fields suitable for handwriting, such as: Date, Time, Location, Investigator/Observer, Assignment/Target (flexible labels allowed).\n   - Critically: Under each header/field, there must be three solid lines intended for handwritten notes. This pattern should be visible wherever header fields appear. Visual verification required (lines, not merely underscores in text).\n   - The body should include labeled blocks (flexible wording) for common undercover notes (e.g., Observations, Interactions, Incidents, Evidence/Photos, Summary/Next Steps), each followed by three solid lines for handwriting.\n\nScoring (0\u20134):\n- 4.0: Valid PDF format(s) + Both documents present (either as two PDFs or a combined PDF) + Guide has Purpose with the stated objective + All listed guide sections labeled + Form has required title + headers with three solid lines + body blocks with three solid lines.\n- 3.0: Valid PDF format(s) + Both documents present + Guide has Purpose with objective + Most guide sections present (missing 1) + Form shows headers with three solid lines but body blocks may be partially incomplete OR vice versa (minor structural omissions only).\n- 2.0: Valid format but only one of the two documents present OR Guide lacks a clearly labeled Purpose with the required objective OR the Form lacks the required three solid lines under headers.\n- 1.0: PDFs provided but titles are incorrect/unclear and multiple structural elements are missing for one or both documents.\n- 0.0: Not a PDF deliverable OR no recognizable guide/form structures present.\n\nOnly evaluate structure and presence of elements needed for later verification. Do not judge writing quality or legal accuracy here.", "expectation": "A cleanly structured set of PDFs (or a single combined PDF) containing both the Guide with a Purpose and operational sections, and the Form with visible three solid lines under headers and in note blocks."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness Verification (Structure and Content Validity)", "description": "Now that structural shape is enforced, verify correctness with a mix of light code checks and deeper LLM review: existence of both documents or combined PDF, presence of the required Purpose objective, and that the form is visually suitable with three solid lines under headers and note blocks.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Two PDFs Present or Combined with Both Sections", "description": "Verify that the outputs include both the Guide and the Form as separate PDFs, or a single combined PDF containing both titled sections.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float (score) or tuple[float, str]\n    \"\"\"\n    outputs = context.get_all_outputs() or []\n    if not outputs:\n        return 0.0, \"No outputs found.\"\n\n    # Collect PDFs and their texts\n    pdf_resources = []\n    for r in outputs:\n        try:\n            path = context.files.get_path(r.id)\n            is_pdf = str(path).lower().endswith('.pdf') if path else False\n        except Exception:\n            is_pdf = False\n        if getattr(r, 'is_document', False) and is_pdf:\n            pdf_resources.append(r)\n\n    if not pdf_resources:\n        return 0.0, \"No PDF documents found.\"\n\n    title_guide = \"undercover operations guide: employee evaluation\"\n    title_form = \"undercover observation form\"\n\n    def pdf_text(res):\n        try:\n            return (context.files.read_pdf_text(res.id) or \"\").lower()\n        except Exception:\n            return \"\"\n\n    texts = [pdf_text(r) for r in pdf_resources]\n\n    # Check separate PDFs\n    has_guide_pdf = any(title_guide in t for t in texts)\n    has_form_pdf = any(title_form in t for t in texts)\n\n    # Check combined PDF possibility\n    combined_ok = any((title_guide in t and title_form in t) for t in texts)\n\n    if (has_guide_pdf and has_form_pdf) or combined_ok:\n        return 0.4, \"Both documents present (separate or combined).\"\n\n    if has_guide_pdf or has_form_pdf:\n        return 0.2, \"Only one document detected (guide or form).\"\n\n    return 0.0, \"Required titles not detected in PDFs.\""}, {"type": "code", "name": "Guide Purpose Objective Text Check", "description": "Confirm the Guide contains a Purpose section that states the objective to discreetly observe and assess employee behavior within the organization (allowing minor paraphrase).", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs() or []\n    if not outputs:\n        return 0.0\n\n    title_guide = \"undercover operations guide: employee evaluation\"\n\n    def read_text(res):\n        try:\n            return (context.files.read_pdf_text(res.id) or \"\").lower()\n        except Exception:\n            return \"\"\n\n    guide_texts = []\n    for r in outputs:\n        try:\n            path = context.files.get_path(r.id)\n            if getattr(r, 'is_document', False) and str(path).lower().endswith('.pdf'):\n                txt = read_text(r)\n                if title_guide in txt or (\"undercover operations guide\" in txt and \"employee\" in txt):\n                    guide_texts.append(txt)\n        except Exception:\n            continue\n\n    # If not found, consider any combined PDF text\n    if not guide_texts:\n        for r in outputs:\n            try:\n                path = context.files.get_path(r.id)\n                if getattr(r, 'is_document', False) and str(path).lower().endswith('.pdf'):\n                    txt = read_text(r)\n                    if \"undercover operations guide\" in txt and \"undercover observation form\" in txt:\n                        guide_texts.append(txt)\n                        break\n            except Exception:\n                continue\n\n    if not guide_texts:\n        return 0.0\n\n    best = guide_texts[0]\n\n    # Purpose section heuristic: presence of 'purpose' heading\n    has_purpose_heading = 'purpose' in best\n\n    # Objective paraphrase: look for discreet/covert + observe + assess + employee + behavior\n    tokens_present = 0\n    if re.search(r\"\\b(discreet|discreetly|covert|covertly)\\b\", best):\n        tokens_present += 1\n    if re.search(r\"\\b(observe|observation|monitor)\\b\", best):\n        tokens_present += 1\n    if re.search(r\"\\b(assess|evaluate|evaluation)\\b\", best):\n        tokens_present += 1\n    if re.search(r\"\\b(employee|staff|associate)\\b\", best):\n        tokens_present += 1\n    if re.search(r\"\\b(behavior|conduct|actions)\\b\", best):\n        tokens_present += 1\n\n    # Direct phrase check for exact objective\n    direct_phrase = \"discreetly observe and assess employee behavior\"\n    has_direct = direct_phrase in best\n\n    score = 0.0\n    if has_purpose_heading and (tokens_present >= 4 or has_direct):\n        score = 0.6\n    elif has_purpose_heading and tokens_present >= 3:\n        score = 0.4\n    elif tokens_present >= 3:\n        score = 0.2\n\n    return score"}, {"type": "llm_judge", "name": "Guide Operational Consistency and Required Sections", "description": "Check the Guide includes and aligns the required operational sections: Roles/Cover Identity, Pre-Deployment Planning, Observation & Documentation Protocols, Evidence Handling/Chain of Custody, Legal/Ethical Considerations, Reporting & Debrief. Verify headings are present and logically consistent.", "weight": 2.5, "judge_prompt": "Evaluate the Guide portion of the deliverables (standalone PDF or a section in a combined PDF). Only assess whether the required sections are present and internally consistent. Do not evaluate writing quality.\n\nVerify presence of clearly labeled headings (flexible wording allowed):\n- Roles and/or Cover Identity\n- Pre-Deployment Planning or Preparation\n- Observation Protocols and Documentation Procedures\n- Evidence Handling / Chain of Custody\n- Legal and Ethical Considerations\n- Reporting and Debrief\n\nAlso verify that the Observation/Documentation section references the use of the \"Undercover Observation Form\" (or a form) for note-taking and that the Reporting section references how observations transition to client-ready reports.\n\nScoring (0\u20132.5):\n- 2.5: All headings present and logically consistent; cross-reference to the Observation Form and to Reporting is explicit.\n- 1.5: Most headings present (missing 1) or cross-reference is only implicit.\n- 0.5: Several headings missing (missing 2 or more) or sections are present but inconsistent/conflicting.\n- 0.0: Headings largely absent or not a Guide section.", "expectation": "A Guide with all required operational headings and explicit linkage to the Observation Form and reporting flow."}, {"type": "llm_judge", "name": "Observation Form Layout Verification (Headers + Three Solid Lines)", "description": "Visually confirm the Observation Form has headers with three solid lines under each header, and body note blocks also using three solid lines suitable for handwriting; ideally consistent across pages.", "weight": 2.5, "judge_prompt": "Focus only on the Observation Form (standalone PDF or the form section in a combined PDF). Verify layout suitability for handwritten notes.\n\nCheck the following:\n- Title near the top: \"Undercover Observation Form\" (allow minor variations).\n- Presence of header fields such as Date, Time, Location, Investigator/Observer, Assignment/Target (flexible labels allowed).\n- Under each visible header field: exactly three solid horizontal lines intended for handwriting (not dotted; visually solid). This pattern should be visible consistently wherever header fields appear.\n- Body includes labeled note blocks (e.g., Observations, Interactions, Incidents, Evidence/Photos, Summary/Next Steps), each followed by three solid lines for handwriting.\n- Preferably present on multiple pages or clearly repeatable per page.\n\nScoring (0\u20132.5):\n- 2.5: Title present + headers present + three solid lines under headers + body blocks with three solid lines; consistent across pages.\n- 1.5: Title present + headers present; three-line pattern mostly present but with minor inconsistencies or only in some sections/pages.\n- 0.5: Title present but header three-line pattern or body lines are largely missing.\n- 0.0: No recognizable Observation Form layout.", "expectation": "A visually verifiable form with three solid lines under headers and in note blocks for handwritten use."}, {"type": "llm_judge", "name": "Legal/Ethical Guardrails Presence (Correctness)", "description": "Confirm the Guide includes legal/ethical guardrails typical for undercover retail investigations in South Carolina (e.g., privacy expectations, recording/consent considerations, no entrapment, trespass, chain of custody). Assess presence/coverage, not jurisdictional accuracy.", "weight": 2.0, "judge_prompt": "Review the Guide for legal and ethical guardrails relevant to undercover work in Columbia, South Carolina retail settings. Do not judge exact legal accuracy; instead verify that appropriate guardrail topics are explicitly covered.\n\nLook for these topics (flexible wording allowed):\n- Privacy expectations on business premises; store policy compliance; limits on back-of-house access.\n- Recording/consent considerations (e.g., one-party consent and audio/video caution); no illegal surveillance.\n- No entrapment or inducement; adherence to company policies and applicable laws.\n- Trespass and refusal-to-leave protocols.\n- Evidence handling, documentation integrity, and chain of custody.\n\nScoring (0\u20132.0):\n- 2.0: Guide explicitly addresses most of the above (4\u20135 items) with clear guardrail language.\n- 1.0: Addresses some items (2\u20133) but others are missing.\n- 0.5: Mentions legal/ethical items only in passing.\n- 0.0: No meaningful legal/ethical guardrails present.", "expectation": "Clear legal/ethical constraints and cautions are present in the Guide for SC retail undercover work."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism Assessment", "description": "Holistic LLM assessment of presentation, usability, and strategic value for field use in Columbia, SC retail undercover operations.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Readability", "description": "Assess overall professional formatting, readability, and organization across both documents (typography, headings, spacing, page numbers, consistency).", "weight": 2.0, "judge_prompt": "Evaluate the overall professional quality and readability of the Guide and the Observation Form. Consider: consistent headings, typography, spacing/margins, page numbers or footers, and overall visual clarity. Do not nitpick wording; focus on professional presentation and ease of reading/printing.\n\nScoring (0\u20132):\n- 2.0: Highly professional, consistent, printer-friendly; easy to navigate.\n- 1.0: Generally readable with minor inconsistencies.\n- 0.5: Noticeable formatting issues but still usable.\n- 0.0: Poorly formatted or hard to read.", "expectation": "Clean, consistent, and professional presentation suitable for client delivery and field use."}, {"type": "llm_judge", "name": "Clarity and Actionability of the Guide", "description": "Assess whether the Guide is clear, step-by-step, and immediately actionable (checklists, workflows, examples) for undercover employee evaluation in retail settings.", "weight": 2.0, "judge_prompt": "Assess how actionable the Guide is for undercover employee evaluation in a retail environment. Consider presence of:\n- Step-by-step workflows from planning to reporting\n- Checklists or bullet lists for deployment and observation\n- Practical examples or scenarios (e.g., point-of-sale behavior, cash handling, merchandise shrink)\n- Clear role expectations and decision points\n\nScoring (0\u20132):\n- 2.0: Clear workflows and checklists; practical and immediately usable.\n- 1.0: Mostly clear but lacks depth or examples.\n- 0.5: Some usable content but gaps limit field application.\n- 0.0: Vague or conceptual; not actionable.", "expectation": "A guide that operationalizes undercover evaluation with concrete steps and checklists."}, {"type": "llm_judge", "name": "Context Fit: Columbia, SC Retail Investigations", "description": "Assess appropriateness for Columbia, SC retail context (e.g., store environments, typical retail risks, cultural/legal awareness).", "weight": 2.0, "judge_prompt": "Evaluate how well the deliverables reflect the Columbia, South Carolina retail context. Consider:\n- Applicability to common retail environments (cash wrap, fitting rooms, stockroom, POS systems)\n- Typical risks: employee theft, sweethearting, refund fraud, under-ringing, backdoor shrink\n- Local awareness signals (e.g., references to SC practices, store policies, or general regional considerations)\n\nScoring (0\u20132):\n- 2.0: Strong fit for Columbia, SC retail; concrete, relevant coverage.\n- 1.0: Generally applicable but light on retail/SC specifics.\n- 0.5: Generic with minimal retail relevance.\n- 0.0: Poor contextual fit.", "expectation": "Retail-specific guidance suitable for Columbia, SC investigations."}, {"type": "llm_judge", "name": "Form Field Usability and Reusability", "description": "Assess whether the Observation Form is field-usable (ample space, logical flow) and reusable (clear labels, repeatable pages, room for signatures).", "weight": 2.0, "judge_prompt": "Evaluate the Observation Form for practical field usability and reusability.\nConsider: logical flow from headers to observations/incidents; ample handwriting space; signature/date lines; page numbers; repeatable format across pages; optional checkboxes or prompts that aid consistency.\n\nScoring (0\u20132):\n- 2.0: Highly usable and reusable; clear flow; sufficient space; signatures; repeatable pages.\n- 1.0: Usable with minor improvements needed (space/flow/labels).\n- 0.5: Barely usable (cramped/unclear flow) but workable.\n- 0.0: Not practically usable in the field.", "expectation": "A form that supports efficient, repeatable handwritten documentation in the field."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "46b34f78-6c06-4416-87e2-77b6d8b20ce9", "rubric": {"category_name": "Energy HY Strategy Memo (H1 2025) \u2014 Finance & Insurance: Financial and Investment Analysts", "rationale": "Pattern C (Mixed): A professional DOCX report with embedded analytical content and tables. Stage 1 is an LLM-only hard gate enforcing an exact structure that makes later verification trivial. Stage 2 blends light code checks (URLs, constraint mentions, issuer sections, time focus) with heavier LLM checks on consistency, issuer suitability, constraints alignment, and actionability. Stage 3 is a holistic LLM quality assessment for executive readability, structure, tone, and usefulness for trading and sales MDs/Directors.", "max_total_score": 30.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (LLM only)", "description": "Strict structural and format requirements to make verification trivial. If the document is not a DOCX strategy memo with the required sections and tables, evaluation stops and the category is zeroed.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 5.5, "rules": [{"type": "llm_judge", "name": "Structured DOCX Strategy Memo Requirement", "description": "Check that the candidate output is a DOCX report (\u226410 pages) with required sections and minimally-required tables enabling verification.", "weight": 8.0, "judge_prompt": "You are checking ONLY structure and format (not content quality or correctness). Review the candidate output. Confirm all of the following:\n\nFORMAT\n- The file is a Microsoft Word document (.docx), not PDF/Excel/plain text.\n- Length is no more than 10 pages.\n- Professionally formatted with clear section headers and at least one table.\n\nREQUIRED SECTION HEADERS (flexible naming OK if clearly equivalent):\n1) Executive Summary (or Overview) \u2014 appears near the beginning\n2) Energy Market Overview \u2014 with subsections for Oil and Natural Gas\n3) Issuer Analysis \u2014 two distinct subsections:\n   - Oil issuer analysis (explicitly indicates the issuer/company and focuses on oil)\n   - Natural gas issuer analysis (explicitly indicates the issuer/company and focuses on gas)\n   Each issuer analysis must include a clearly labeled \u201cBond Snapshot\u201d (or similar) table.\n4) Strategy Recommendations \u2014 with two clear subsections:\n   - Trading Strategy (H1 2025)\n   - Sales Strategy (H1 2025)\n5) Portfolio Constraints Compliance (or Constraints Summary) \u2014 explicit tie-back to:\n   - Max 20% HY allocation\n   - HY duration 3\u20135 years\n   - Diversification across fixed income products\n6) Appendix (or Sources) \u2014 optional, but if present should list public data sources\n\nMINIMALLY-REQUIRED TABLES (flexible but must be clearly present):\nA) In the Energy Market Overview: a \u201cKey Indicators\u201d table (or similar) with columns akin to: [Indicator | Latest Value | 1M change | 3M change | Source/Date]. It must include at least these rows: WTI (or front-month), Brent (or front-month), Henry Hub, and WTI\u2013Brent spread. (Additional rows like rig count, inventories, volatility, crack spreads are optional.)\nB) In each issuer analysis (both oil and gas): a \u201cBond Snapshot\u201d table (or similar) with columns akin to: [Issuer | Bond/CUSIP(or Ticker) | Coupon | Maturity | Price | Yield/YTW | OAS/Spread | Duration (Mod) | Rating | Notes/Thesis]. Column names can vary but these data elements must be visibly present.\nC) In Portfolio Constraints Compliance: a small \u201cConstraint Check\u201d table (or similar) summarizing how each constraint is met.\n\nCITATIONS\n- A Sources/References list or inline footnotes with public links (URLs) to data.\n\nSCORING (return a single numeric score from 0 to 8):\n- 8.0: DOCX, \u226410 pages, all required sections present, Key Indicators table present, both Bond Snapshot tables present, Constraint Check table present, and sources/URLs present.\n- 6.0\u20137.5: DOCX, \u226410 pages, all required sections present but missing one minor structural element (e.g., one table incomplete, or Appendix/Sources present but not fully formatted).\n- 4.0\u20135.5: DOCX with some sections present but missing one required section or one required issuer analysis/table; or lacks the Key Indicators table.\n- 0\u20133.5: Wrong format (not DOCX), over 10 pages, or multiple required sections/tables missing.\n\nOnly assess presence and structure. Do NOT judge analytical correctness or quality.", "expectation": "A \u226410-page DOCX memo with all mandated sections and tables, clearly labeled and easy to locate."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness & Consistency)", "description": "With the structure enforced, verify factual/analytical consistency, constraints alignment, and basic completeness. Mix light code checks with deeper LLM reasoning. Code rules collectively carry lower weight than LLM rules.", "is_required": false, "max_points": 14.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Public Source Citations (URLs) Present", "description": "Count unique public URLs in the document text to verify use of public data sources.", "weight": 1.0, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_document:\\n            return 0.0, 'No document output.'\\n        text = ''\\n        try:\\n            text = context.files.read_docx_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_pdf_text(output.id)\\n            except Exception:\\n                text = ''\\n        if not text:\\n            return 0.0, 'Unable to read document text.'\\n        urls = set(re.findall(r'https?://[^\\s)\\]>]+', text))\\n        count = len(urls)\\n        # Heuristic: 3+ URLs = full credit; 2 = 0.67; 1 = 0.33; else 0\\n        if count >= 3:\\n            score = 1.0\\n        elif count == 2:\\n            score = 0.67\\n        elif count == 1:\\n            score = 0.33\\n        else:\\n            score = 0.0\\n        return score, f'Found {count} unique URLs.'\\n    except Exception as e:\\n        return 0.0, f'Error: {e}'"}, {"type": "code", "name": "Constraints Mentioned and Addressed", "description": "Verify mentions of all three constraints: 20% HY cap, HY duration 3\u20135 years, and diversification across fixed income products.", "weight": 0.8, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_document:\\n            return 0.0\\n        text = ''\\n        try:\\n            text = context.files.read_docx_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_pdf_text(output.id)\\n            except Exception:\\n                text = ''\\n        if not text:\\n            return 0.0\\n        lt = text.lower()\\n        # HY 20% limit (look for numeric or words)\n        hy_limit = ('20%' in lt) or ('twenty percent' in lt) or ('twenty-percent' in lt)\\n        # Duration window 3-5 years\n        dur_window = bool(re.search(r'3\\s*[-\u2013to]\\s*5\\s*(year|yr|years)', lt)) or ('3 to 5 year' in lt)\\n        # Diversification mention\n        divers = ('diversif' in lt) or ('across fixed income' in lt) or ('mix of fixed income' in lt)\\n        hits = sum([hy_limit, dur_window, divers])\\n        return (hits/3.0)*0.8, f'Constraints hits: {hits}/3'\\n    except Exception as e:\\n        return 0.0, f'Error: {e}'"}, {"type": "code", "name": "Issuer Sections + Key Bond Metrics Mentioned", "description": "Check for presence of oil and gas issuer analysis sections and key bond metrics terms (yield, duration, spread).", "weight": 0.8, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_document:\\n            return 0.0\\n        text = ''\\n        try:\\n            text = context.files.read_docx_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_pdf_text(output.id)\\n            except Exception:\\n                text = ''\\n        if not text:\\n            return 0.0\\n        lt = text.lower()\\n        oil_section = bool(re.search(r'(issuer|company)\\s*analysis[^\\n]*oil', lt)) or ('oil issuer' in lt) or ('oil company' in lt)\\n        gas_section = bool(re.search(r'(issuer|company)\\s*analysis[^\\n]*(natural\\s*gas|gas)', lt)) or ('gas issuer' in lt) or ('natural gas company' in lt)\\n        metrics = any(w in lt for w in ['yield', 'ytw', 'duration', 'spread', 'oas'])\\n        hits = sum([oil_section, gas_section, metrics])\\n        return (hits/3.0)*0.8, f'Sections/metrics hits: {hits}/3'\\n    except Exception as e:\\n        return 0.0, f'Error: {e}'"}, {"type": "code", "name": "H1 2025 Timeframe Emphasis", "description": "Verify the document references the H1 2025 timeframe explicitly (e.g., H1 2025, Q1/Q2 2025, or month references in 2025).", "weight": 0.4, "code": "import re\\n\\nMONTHS = '(january|february|march|april|may|june)'\\n\\ndef evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_document:\\n            return 0.0\\n        text = ''\\n        try:\\n            text = context.files.read_docx_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_pdf_text(output.id)\\n            except Exception:\\n                text = ''\\n        if not text:\\n            return 0.0\\n        lt = text.lower()\\n        patterns = [r'h1\\s*2025', r'first\\s*half\\s*of\\s*2025', r'q1\\s*2025', r'q2\\s*2025', rf'{MONTHS}[^\\n,]*2025']\\n        focused = any(re.search(p, lt) for p in patterns)\\n        return 0.4 if focused else 0.0\\n    except Exception as e:\\n        return 0.0, f'Error: {e}'"}, {"type": "llm_judge", "name": "Market Data Recency and Consistency", "description": "Check the market overview references recent (2024\u20132025) events and plausible public indicators (WTI/Brent, spreads, Henry Hub) with coherent narrative and sources.", "weight": 3.5, "judge_prompt": "Evaluate whether the Energy Market Overview is consistent and recent enough to guide H1 2025 decisions. Specifically check:\n- References to 2024\u20132025 events or conditions affecting oil and natural gas (e.g., volatility, inventories, rig counts, geopolitical tensions).\n- Presence of key indicators (WTI/Brent prices or front-month futures, WTI\u2013Brent spread, Henry Hub) and a coherent explanation of their implications.\n- Citations to public sources appear plausible and appropriately linked to claims.\n- Narrative consistency between Oil and Natural Gas subsections (no contradictions about directionality or drivers).\nScore 0 to 3.5 based on completeness and consistency. Do not just reward length; reward relevance and internal consistency with recent timeframe.", "expectation": "Timely, consistent overview citing public indicators with clear implications for H1 2025."}, {"type": "llm_judge", "name": "Issuer Selection Validity (Oil and Gas) and HY Focus", "description": "Assess whether the two issuer analyses appropriately represent one oil and one natural gas issuer, with HY bond focus, 3\u20135 year duration, and coherent valuation metrics.", "weight": 3.0, "judge_prompt": "Review both issuer analyses. Check:\n- One issuer is clearly oil-focused; the other clearly natural-gas-focused.\n- Bonds analyzed are high-yield (sub-IG) and within a 3\u20135 year duration window (or clearly justified if modified duration proxy used).\n- Bond snapshots present plausible fields: price, yield/YTW, OAS/spread, coupon, maturity, rating, duration.\n- The qualitative thesis and catalysts align with the metrics (no contradictions).\nScore 0 to 3.0 based on completeness and internal coherence.", "expectation": "Two distinct issuers (oil and gas), HY bonds with 3\u20135Y duration, coherent thesis tied to metrics."}, {"type": "llm_judge", "name": "Strategy vs. Portfolio Constraints Alignment", "description": "Verify that trading and sales recommendations are feasible under the portfolio\u2019s constraints, with risk controls and sizing logic.", "weight": 2.5, "judge_prompt": "Evaluate the Trading Strategy and Sales Strategy sections for:\n- Explicit linkage to constraints: max 20% HY allocation, HY duration 3\u20135 years, diversification across fixed income products.\n- Position sizing or exposure changes that would keep the portfolio within constraints.\n- Risk controls (stops, hedges, scenario contingencies) appropriate for HY energy bonds.\n- No contradictions between trading and sales directions.\nScore 0 to 2.5 based on feasibility and alignment with constraints.", "expectation": "Actionable strategies that respect HY cap, duration band, and diversification, with concrete risk controls."}, {"type": "llm_judge", "name": "Actionability and Measurable Triggers", "description": "Check that recommendations include concrete entry/exit levels, triggers, and monitoring metrics for H1 2025.", "weight": 2.0, "judge_prompt": "Assess how actionable the recommendations are. Look for:\n- Entry/exit levels or spread/yield targets for bonds or hedges.\n- Measurable triggers (e.g., inventory thresholds, curve moves, volatility regimes) tied to H1 2025.\n- Monitoring cadence and data sources.\nScore 0 to 2.0 based on specificity and measurability.", "expectation": "Clear, measurable actions with levels/triggers suitable for H1 2025 execution."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism", "description": "Holistic assessment of clarity, persuasion, and usefulness for MD/Director audience on the energy desk.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Executive Summary Quality", "description": "Judges whether the executive summary crisply captures the thesis, key indicators, issuer choices, and top actions.", "weight": 2.0, "judge_prompt": "Evaluate the Executive Summary for brevity and impact:\n- Clear H1 2025 thesis for oil and gas.\n- The most important indicators and drivers.\n- The two issuers named and why they were chosen.\n- Top 2\u20133 trading and sales actions with expected effect on returns and constraints.\nScore 0 to 2.0 based on clarity and succinctness.", "expectation": "A concise, high-signal summary that a busy MD can absorb in under a minute."}, {"type": "llm_judge", "name": "Clarity, Structure, and Readability", "description": "Assesses document organization, headers, flow, grammar, and use of tables/figures to aid comprehension.", "weight": 2.0, "judge_prompt": "Judge clarity and readability:\n- Logical flow between sections, informative headers, minimal redundancy.\n- Grammar, syntax, and professional formatting.\n- Tables/figures used where they add clarity (e.g., Key Indicators, Bond Snapshots).\nScore 0 to 2.0 based on how easy the memo is to read and navigate.", "expectation": "Well-structured, clean writing with tables that enhance clarity."}, {"type": "llm_judge", "name": "Audience Appropriateness and Tone", "description": "Checks that the memo addresses MD/Director-level needs with a professional sell-side tone.", "weight": 2.0, "judge_prompt": "Evaluate whether the memo is suitable for MD/Director readers:\n- Professional, precise tone; avoids jargon without explanation.\n- Focuses on decision-useful insights for trading and client engagement.\n- Distills complex analysis into key implications for P&L.\nScore 0 to 2.0 based on suitability for senior desk leadership.", "expectation": "Professional, decision-focused tone tailored to senior trading/sales leadership."}, {"type": "llm_judge", "name": "Usefulness of Visuals and Tables", "description": "Rates whether included tables/visuals materially help decision-making (beyond mere presence).", "weight": 2.0, "judge_prompt": "Consider whether visuals/tables materially aid decisions:\n- Key Indicators table highlights what matters and is readable.\n- Bond Snapshot tables make valuation comparison straightforward.\n- Any additional visuals (if present) are legible and not cluttered.\nScore 0 to 2.0 based on contribution to decision clarity.", "expectation": "Readable, decision-oriented visuals/tables that support swift judgments."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "c9bf9801-9640-45fa-8166-1ab01f2d98e4", "rubric": {"category_name": "DGHT Formal Mentorship Program Guide - Structured, Verifiable, and Professional Delivery", "rationale": "Pattern B (Document). This rubric enforces a self-documenting, file-based guide with explicit structure first (Stage 1, LLM-only gate), then verifies correctness with mixed LLM and code rules (Stage 2), and finally assesses professional quality (Stage 3). Stage 2 code rules focus on deterministic checks (existence of separate DOCX templates, presence of required keywords, timeline coverage), while LLM rules judge nuanced alignment, consistency, and best practices. Weighting favors LLM rules for complex evaluations, with code rules contributing ~5x less on average.", "max_total_score": 31.0, "stages": [{"name": "Stage 1 \u2014 Format and Structural Gate (LLM ONLY)", "description": "Gate: The main output must be a well-structured DOCX/PDF guide containing all required sections and the Appendix listing/linking required templates. No content quality is judged here\u2014only presence, structure, and format.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Document Format and Required Structure Present", "description": "Checks the final deliverable\u2019s file type and the presence of all required sections and appendix elements as a structured, professional guide.", "weight": 8.0, "judge_prompt": "You are validating ONLY the presence and structural completeness of the submitted guide. Do not assess content quality or correctness yet.\n\nCheck the PRIMARY output file for:\n\nFormat Requirements:\n- Must be a DOCX or PDF document (prefer DOCX). Not a spreadsheet or plain text.\n- Length: at least 5 pages (be flexible if content is clearly comprehensive and sectioned).\n- Professional structure with clear headings and subheadings.\n\nRequired Sections (flexible on exact header names; look for equivalent terms):\n1) Introduction and Program Overview\n2) Program Purpose and Goals\n3) Definitions and Value of Mentoring\n4) Eligibility Criteria and Application Process\n5) Matching Procedures\n6) Roles and Responsibilities of Mentors and Mentees (both parties explicitly addressed)\n7) Key Program Requirements (e.g., mentorship roadmap, monthly meetings/cadence, IDPs)\n8) Training Components (skills-building sessions and networking events)\n9) Supporting Documentation Expectations (e.g., progress reports, evaluations)\n10) Program Timeline with monthly milestones and deliverables (aimed at an 8-month cycle)\n\nAppendix Requirement:\n- An appendix titled like \u201cDocuments and Templates\u201d (or similar) that clearly references and labels:\n  \u2022 Mentor Application (DOCX, linked)\n  \u2022 Mentee Application (DOCX, linked)\n  \u2022 Mentorship Roadmap Template (DOCX, linked)\n  \u2022 4-month Evaluation Form\n  \u2022 8-month Evaluation Form\n- You cannot verify link functionality here; just check that each item appears listed and presented as a link or clearly marked for linkage.\n\nCredits/Acknowledgment:\n- Acknowledgment that the guide was inspired by NCIPC\u2019s Mentoring Program (flexible wording).\n\nBranding Note (informational, not required to pass):\n- If visible, CDC/DGHT branding and tone are appropriate. Do NOT fail if branding elements are not visible.\n\nScoring Guide (0\u20138):\n- 8: Valid DOCX/PDF, >=5 pages, and ALL required sections + Appendix items + NCIPC acknowledgment present.\n- 7: Valid format, most sections present, Appendix includes all 3 DOCX templates and both evaluation forms, and NCIPC acknowledgment present; only 1 minor structural element missing or mislabeled.\n- 5\u20136: Valid format, but missing 2\u20133 required sections OR Appendix missing 1\u20132 required items OR NCIPC acknowledgment missing.\n- 3\u20134: Valid format but missing multiple core sections; Appendix incomplete (3+ items missing) and/or structure unclear.\n- 0: Not DOCX/PDF OR extremely short (e.g., <2\u20133 pages) OR lacks clear section structure.\n\nOnly evaluate PRESENCE/STRUCTURE, not the correctness or quality of the content.", "expectation": "A DOCX (preferred) or PDF guide with all required sections, a properly labeled appendix listing the three linked templates plus 4- and 8-month evaluation forms, and an NCIPC acknowledgment."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification: Correctness and Cross-Reference (Mixed Code + LLM)", "description": "Now verify correctness, consistency, and program readiness using a mix of deterministic code checks and LLM reasoning. Code rules perform file and keyword/timeline checks; LLM rules ensure logical completeness, alignment, and best practices.", "is_required": false, "max_points": 15.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Separate Templates Exist and Are DOCX", "description": "Verify that the three required templates exist as separate DOCX files among outputs: Mentor Application, Mentee Application, and Mentorship Roadmap Template.", "weight": 1.2, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        primary = context.get_primary_output()\n        if not primary or not primary.is_document:\n            return 0.0, \"Primary output not a document.\"\n        outputs = context.get_all_outputs() or []\n        # Exclude primary\n        others = [r for r in outputs if r.id != primary.id]\n        # DOCX heuristic by extension\n        docx_files = []\n        for r in others:\n            try:\n                p = context.files.get_path(r.id)\n                if p.suffix.lower() == '.docx':\n                    docx_files.append((r, p))\n            except Exception:\n                continue\n        found_mentor = False\n        found_mentee = False\n        found_roadmap = False\n        for r, p in docx_files:\n            name = p.name.lower()\n            text = ''\n            try:\n                text = (context.files.read_docx_text(r.id) or '').lower()\n            except Exception:\n                text = ''\n            corpus = name + '\\n' + text\n            if ('mentor' in corpus and 'application' in corpus):\n                found_mentor = True\n            if ('mentee' in corpus and 'application' in corpus):\n                found_mentee = True\n            if (('roadmap' in corpus and ('template' in corpus or 'mentorship' in corpus)) or 'mentorship roadmap' in corpus):\n                found_roadmap = True\n        hits = sum([found_mentor, found_mentee, found_roadmap])\n        score = (hits / 3.0) * 1.2\n        feedback = f\"Found templates \u2014 Mentor: {found_mentor}, Mentee: {found_mentee}, Roadmap: {found_roadmap}.\"\n        return score, feedback\n    except Exception as e:\n        return 0.0, f\"Error checking templates: {e}\""}, {"type": "code", "name": "Timeline Covers 8 Months", "description": "Check the main document includes a month-by-month timeline referencing months 1 through 8 (numeric or word forms).", "weight": 0.6, "code": "import re\n\nNUM_WORDS = {1:'one',2:'two',3:'three',4:'four',5:'five',6:'six',7:'seven',8:'eight'}\n\ndef extract_text(context, res):\n    text = ''\n    try:\n        p = context.files.get_path(res.id)\n        if p.suffix.lower() == '.pdf':\n            text = context.files.read_pdf_text(res.id) or ''\n        else:\n            text = context.files.read_docx_text(res.id) or ''\n    except Exception:\n        try:\n            text = context.files.read_docx_text(res.id) or ''\n        except Exception:\n            text = ''\n    return text\n\ndef evaluate(workflow, context):\n    try:\n        primary = context.get_primary_output()\n        if not primary or not primary.is_document:\n            return 0.0, \"Primary output not a document.\"\n        text = extract_text(context, primary).lower()\n        if not text:\n            return 0.0, \"Could not read document text.\"\n        months_found = set()\n        for i in range(1,9):\n            patterns = [\n                rf\"\\bmonth\\s*{i}\\b\",\n                rf\"\\bmonth\\s*[-:\u2013\u2014]?\\s*{i}\\b\",\n                rf\"\\bmonth\\s*(?:{NUM_WORDS[i]})\\b\",\n                rf\"\\bmonths?\\s*{i}\\b\",\n                rf\"\\bmonths?\\s*(?:{NUM_WORDS[i]})\\b\",\n            ]\n            if any(re.search(p, text) for p in patterns):\n                months_found.add(i)\n        score = (len(months_found) / 8.0) * 0.6\n        return score, f\"Months detected: {sorted(list(months_found))}\"\n    except Exception as e:\n        return 0.0, f\"Error checking timeline: {e}\""}, {"type": "code", "name": "Core Program Requirements Keywords", "description": "Verify presence of key operational elements: IDPs, monthly meetings/cadence, progress/evaluation checkpoints, matching, eligibility, and training/networking.", "weight": 0.6, "code": "import re\n\ngroups = {\n    'idp': [r'idp', r'individual\\s+development\\s+plan'],\n    'monthly_meetings': [r'monthly\\s+meeting', r'meet\\s+monthly', r'monthly\\s+check-?in', r'monthly\\s+cadence'],\n    'progress_eval': [r'progress\\s+report', r'progress\\s+update', r'evaluation', r'4-?month', r'four-?month', r'8-?month', r'eight-?month'],\n    'matching': [r'matching', r'pairing', r'match\\s+process'],\n    'eligibility': [r'eligibility', r'eligible', r'criteria'],\n    'training_networking': [r'skills-?building', r'training', r'networking\\s+event']\n}\n\ndef extract_text(context, res):\n    text = ''\n    try:\n        p = context.files.get_path(res.id)\n        if p.suffix.lower() == '.pdf':\n            text = context.files.read_pdf_text(res.id) or ''\n        else:\n            text = context.files.read_docx_text(res.id) or ''\n    except Exception:\n        try:\n            text = context.files.read_docx_text(res.id) or ''\n        except Exception:\n            text = ''\n    return text\n\ndef evaluate(workflow, context):\n    try:\n        primary = context.get_primary_output()\n        if not primary or not primary.is_document:\n            return 0.0\n        text = extract_text(context, primary).lower()\n        hits = 0\n        missing = []\n        for key, pats in groups.items():\n            found = any(re.search(p, text) for p in pats)\n            if found:\n                hits += 1\n            else:\n                missing.append(key)\n        score = (hits / len(groups)) * 0.6\n        fb = f\"Groups present: {hits}/{len(groups)}. Missing: {', '.join(missing) if missing else 'None'}.\"\n        return score, fb\n    except Exception as e:\n        return 0.0, f\"Error checking keywords: {e}\""}, {"type": "code", "name": "NCIPC Acknowledgment Present", "description": "Confirm the guide acknowledges NCIPC\u2019s Mentoring Program as inspiration.", "weight": 0.3, "code": "def evaluate(workflow, context):\n    try:\n        primary = context.get_primary_output()\n        if not primary or not primary.is_document:\n            return 0.0\n        # Try DOCX then PDF\n        text = ''\n        try:\n            text = (context.files.read_docx_text(primary.id) or '')\n        except Exception:\n            try:\n                text = (context.files.read_pdf_text(primary.id) or '')\n            except Exception:\n                text = ''\n        lt = text.lower()\n        ok = ('ncipc' in lt) and ('acknowled' in lt or 'inspir' in lt or 'credit' in lt)\n        return (0.3 if ok else 0.0), (\"NCIPC acknowledgment found.\" if ok else \"NCIPC acknowledgment not found.\")\n    except Exception as e:\n        return 0.0, f\"Error checking NCIPC acknowledgment: {e}\""}, {"type": "code", "name": "CDC/DGHT Context Present", "description": "Check for CDC and DGHT mentions appropriate to internal context.", "weight": 0.2, "code": "def evaluate(workflow, context):\n    try:\n        primary = context.get_primary_output()\n        if not primary or not primary.is_document:\n            return 0.0\n        text = ''\n        try:\n            text = (context.files.read_docx_text(primary.id) or '')\n        except Exception:\n            try:\n                text = (context.files.read_pdf_text(primary.id) or '')\n            except Exception:\n                text = ''\n        lt = text.lower()\n        signals = 0\n        if 'cdc' in lt or 'centers for disease control and prevention' in lt:\n            signals += 1\n        if 'dght' in lt or 'division of global hiv & tb' in lt or 'division of global hiv and tb' in lt:\n            signals += 1\n        # Give credit if the full division name appears\n        if 'division of global hiv' in lt:\n            signals += 1\n        score = (min(signals, 3) / 3.0) * 0.2\n        return score, f\"Branding mentions detected: {signals}/3\"\n    except Exception as e:\n        return 0.0, f\"Error checking CDC/DGHT mentions: {e}\""}, {"type": "code", "name": "Appendix References Specific Items", "description": "Confirm the Appendix includes references to the five required items (mentor app, mentee app, roadmap template, 4- and 8-month evaluations).", "weight": 0.1, "code": "import re\n\ndef extract_text(context, res):\n    try:\n        p = context.files.get_path(res.id)\n        if p.suffix.lower() == '.pdf':\n            return context.files.read_pdf_text(res.id) or ''\n        return context.files.read_docx_text(res.id) or ''\n    except Exception:\n        try:\n            return context.files.read_docx_text(res.id) or ''\n        except Exception:\n            return ''\n\ndef evaluate(workflow, context):\n    try:\n        primary = context.get_primary_output()\n        if not primary or not primary.is_document:\n            return 0.0\n        t = extract_text(context, primary).lower()\n        has_appendix = ('appendix' in t) or ('documents and templates' in t) or ('templates' in t)\n        items = {\n            'mentor application': ('mentor' in t and 'application' in t),\n            'mentee application': ('mentee' in t and 'application' in t),\n            'roadmap template': (('roadmap' in t) and ('template' in t)),\n            '4-month evaluation': (('4-month' in t) or ('four-month' in t) or ('4 month' in t)),\n            '8-month evaluation': (('8-month' in t) or ('eight-month' in t) or ('8 month' in t)),\n        }\n        hits = sum(1 for v in items.values() if v)\n        score = (1.0 if has_appendix else 0.0) * (hits/5.0) * 0.1\n        return score, f\"Appendix present: {has_appendix}. Items found: {hits}/5\"\n    except Exception as e:\n        return 0.0, f\"Error checking Appendix items: {e}\""}, {"type": "llm_judge", "name": "Section Completeness and Internal Consistency", "description": "Evaluate whether each required section provides specific, usable details and that the pieces align coherently (eligibility\u2192application\u2192matching\u2192roles\u2192requirements\u2192timeline).", "weight": 3.0, "judge_prompt": "Evaluate the guide for completeness and internal consistency of required sections:\n- Eligibility criteria are clear and measurable; application steps are actionable (what, when, where/how to submit).\n- Matching procedures logically follow from eligibility and application data (e.g., role, goals, skills, constraints).\n- Roles and responsibilities for both mentors and mentees are specific, including time commitments and boundaries.\n- Program requirements specify monthly meeting cadence and use of IDPs/roadmaps.\n- Supporting documentation expectations (progress reports and evaluations) describe timing and required content.\n- The program timeline (target 8 months) aligns with these requirements and includes month-by-month milestones and deliverables.\nScore higher if the cross-references are clear (e.g., sections refer to the Appendix templates by name).", "expectation": "A coherent, end-to-end program flow with concrete, cross-referenced instructions and timelines."}, {"type": "llm_judge", "name": "Training Components: Skills-Building and Networking", "description": "Check that training and networking components are defined with examples, learning objectives, or logistics, and are spaced appropriately across the timeline.", "weight": 3.0, "judge_prompt": "Review the training components. Are there clearly defined skills-building sessions (topics, objectives, or formats) and networking events (purpose, participants, format)? Are these components placed appropriately in the timeline (e.g., kickoff, mid-point, close)? Are participation expectations and preparation requirements specified?", "expectation": "Training and networking elements are purposeful, scheduled, and actionable for participants."}, {"type": "llm_judge", "name": "Best Practices, Inclusion, and Safeguards", "description": "Assess integration of mentoring best practices, inclusive language, and participant safeguards.", "weight": 3.0, "judge_prompt": "Evaluate whether the guide incorporates mentoring best practices and inclusive language: confidentiality, psychological safety, boundaries, goal-setting, feedback norms, accessibility and accommodations, DEIA considerations, power dynamics, conflict resolution/escalation paths, and anti-harassment/anti-discrimination safeguards. Score higher if these are explicit and operationalized (e.g., in roles, agreements, or training).", "expectation": "A clear, inclusive, and safe mentoring framework aligned with workforce development best practices."}, {"type": "llm_judge", "name": "Alignment to CDC/DGHT Context and Acknowledgments", "description": "Verify appropriate internal tone/branding, correct references to CDC/DGHT, and proper NCIPC acknowledgment in credits.", "weight": 3.0, "judge_prompt": "Judge alignment to CDC/DGHT context: correct use of names (CDC; Division of Global HIV & TB, DGHT), internal audience tone, and consistent branding style if visible. Confirm that NCIPC\u2019s Mentoring Program is acknowledged appropriately in credits. Score based on correctness and clarity of alignment and acknowledgment.", "expectation": "Tone and references fit CDC/DGHT internal context; NCIPC acknowledgment is respectfully included."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism (LLM)", "description": "Holistic assessment of presentation quality, clarity, and usability for mentors and mentees across the 8-month cycle.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Formatting and Readability", "description": "Evaluate visual hierarchy, consistent headings, spacing, lists, and overall readability. Icons/callouts optional but rewarded.", "weight": 2.0, "judge_prompt": "Assess professional presentation: consistent heading styles, logical section hierarchy, white space and spacing, readable lists/tables, and use of visual aids (icons, callouts) where appropriate. Is the document visually scannable and easy to navigate (TOC or section numbering helpful but optional)?", "expectation": "Clean, consistent formatting that supports quick comprehension."}, {"type": "llm_judge", "name": "Clarity for Both Mentors and Mentees", "description": "Determine how clearly guidance addresses both audiences with actionable steps and expectations.", "weight": 2.0, "judge_prompt": "Does the guide speak clearly to both mentors and mentees, with explicit expectations, steps, and timelines for each? Look for role-specific instructions, checklists, and examples that make it easy to follow.", "expectation": "Clear, role-specific guidance that minimizes ambiguity for both mentors and mentees."}, {"type": "llm_judge", "name": "Actionability and Implementation Readiness", "description": "Assess whether the guide can be used immediately to run the program with minimal additional clarification.", "weight": 2.0, "judge_prompt": "Could DGHT implement the program using this guide alone? Consider presence of checklists, timelines, communications touchpoints, escalation/issue-resolution paths, contact info, and references to templates. Score higher if the guide includes practical tools (e.g., checklists, sample agendas, email templates).", "expectation": "A ready-to-implement guide with practical tools and clear processes."}, {"type": "llm_judge", "name": "Polish and Coherence", "description": "Evaluate writing quality, consistency of tone, absence of errors, and smooth flow.", "weight": 2.0, "judge_prompt": "Rate overall polish: professional tone aligned to CDC\u2019s internal style, grammar/spelling, concise prose, coherent flow between sections, and consistent terminology (e.g., mentor/mentee, program terms).", "expectation": "A polished, coherent document suitable for internal distribution."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "99ac6944-4ec6-4848-959c-a460ac705c6f", "rubric": {"category_name": "Mobile IEM Rig Proposal (Touring Band) \u2014 Staged Evaluation", "rationale": "Task Type: Mixed (Pattern C). The deliverable is a PDF report (document) that must embed visual artifacts (PNG signal-flow diagram and PNG of an Excel cost spreadsheet) and include structured budget analysis and sourcing links. Stage 1 uses LLM-only gating to enforce a strict, self-documenting structure that makes later verification trivial. Stage 2 mixes code rules (lightweight, deterministic checks for budget and links) with LLM judges (functional compliance, completeness, and routing correctness). Stage 3 focuses on overall professional quality, practicality, and usability for the target audience (touring band, IEM tech). Code-rule weighting is ~5x less than LLM rules in Stage 2 as required.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Format & Structure Gate (LLM only)", "description": "Strict shape enforcement for a self-documenting deliverable. Verifies the output is a PDF with all required sections, embedded images, tables, and the last-page spreadsheet PNG. No correctness checks here\u2014only presence and structure.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured PDF Requirements Met", "description": "Check the candidate\u2019s output is a PDF document with all required sections, links, tables, and embedded PNG images as specified.", "weight": 4.0, "judge_prompt": "You are evaluating a candidate\u2019s deliverable for strict STRUCTURE ONLY. Do not assess correctness or quality here\u2014only whether the required elements exist and are clearly identifiable in the rendered PDF. Be flexible with section naming, but all required content types must be present.\n\nRequired format and elements:\n- File format: PDF (not Word/Excel/plain text).\n- Length: at least 3 pages.\n- First page includes a clear title and an Executive Summary/Overview.\n- A section summarizing constraints/requirements explicitly references: two singers only, analogue mixing board required (onboard digital effects acceptable), RF IEMs, total budget \u2264 $3,000, portability, side-stage/FOH placement flexibility, and exclusion of helical UHF antennas (already owned).\n- A System Overview section identifying major components with web links to:\n  1) a professional-grade RF IEM system suitable for two singers (2 packs total)\n  2) an analogue mixing board with onboard compression, reverb, and delay\n- A Bill of Materials (BOM) or Cost Breakdown section with a table-like structure that includes, in some form: Item/Description, Link/Source, Quantity, Unit Price (USD), Extended Price.\n- A Wiring and Signal Flow section that includes an embedded PNG image of a simple signal-flow diagram.\n- A Setup/Operation section describing the IEM tech\u2019s mixing/monitoring location flexibility and quick setup considerations (portable surface/stand/case mentioned).\n- A Budget Summary section stating Total Budget and Total Estimated Cost in USD.\n- The last page of the PDF contains a PNG image of an Excel spreadsheet analysis showing a full cost breakdown with columns for item, price, quantity, totals, and a visible total budget and total estimated cost in USD.\n\nScoring guide (structure only):\n- Full credit: All required elements present, PDF format, 3+ pages, two distinct PNG images (signal-flow and spreadsheet on the last page).\n- ~80%: Minor omissions (e.g., one small structural element missing) but core sections and both PNGs present.\n- ~50%: Multiple required sections or one of the PNG images missing, but still a PDF with partial structure.\n- ~20%: PDF exists but major structural gaps (e.g., missing BOM table and/or no links and/or no last-page spreadsheet image).\n- 0: Not a PDF OR fewer than 3 pages OR missing both the signal-flow PNG and the last-page spreadsheet PNG.\n\nOnly evaluate structure and presence. Do not check math correctness, product suitability, or budget accuracy at this stage.", "expectation": "A multi-page PDF including the specified sections, embedded PNG signal-flow diagram, and a last-page PNG of a spreadsheet cost breakdown table with totals."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness & Compliance)", "description": "Now that the structure is confirmed, verify factual and functional compliance. Mix small deterministic code checks with higher-weight LLM judgment for nuanced technical validation.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 4.0, "rules": [{"type": "code", "name": "Budget Under $3,000 (Detected in PDF Text)", "description": "Parse PDF text to find a stated Total Estimated Cost (not the budget declaration) and verify it is \u2264 $3,000 USD.", "weight": 0.7, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document output to parse.\"\n        # Extract text from PDF or DOCX\n        text = \"\"\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                return 0.0, \"Could not read document text.\"\n        if not text:\n            return 0.0, \"Empty document text.\"\n        low = text.lower()\n\n        # Identify lines containing total/estimated cost while excluding budget-only mentions\n        lines = [l for l in low.splitlines() if l.strip()]\n        candidates = []\n        money_re = re.compile(r\"\\$\\s*([0-9]{1,3}(?:,[0-9]{3})*(?:\\.[0-9]{2})?|[0-9]+(?:\\.[0-9]{2})?)\")\n        for i, line in enumerate(lines):\n            if any(k in line for k in [\"total estimated\", \"estimated total\", \"grand total\", \"total cost\", \"total estimate\", \"subtotal\", \"sum total\"]):\n                if \"budget\" in line:\n                    continue\n                for m in money_re.finditer(line):\n                    amt = m.group(1).replace(\",\", \"\")\n                    try:\n                        val = float(amt)\n                        candidates.append((val, line))\n                    except:\n                        pass\n        # If none found above, try broader search for any 'total' line not marked as budget\n        if not candidates:\n            for i, line in enumerate(lines):\n                if \"total\" in line and \"budget\" not in line:\n                    for m in money_re.finditer(line):\n                        amt = m.group(1).replace(\",\", \"\")\n                        try:\n                            val = float(amt)\n                            candidates.append((val, line))\n                        except:\n                            pass\n        # Also detect presence of a budget statement for partial credit if no cost found\n        has_budget_decl = any((\"budget\" in l and money_re.search(l)) for l in lines)\n        if not candidates:\n            if has_budget_decl:\n                return 0.21 * 0.7, \"Budget stated but could not find a Total Estimated Cost number.\"  # ~0.147\n            return 0.0, \"Could not find a Total Estimated Cost number.\"\n        # Heuristic: choose the LAST non-budget total as the final cost\n        final_cost, final_line = candidates[-1]\n        if final_cost <= 3000:\n            return 0.7, f\"Detected Total Estimated Cost ${final_cost:.2f} within budget.\"\n        else:\n            return 0.2 * 0.7, f\"Detected Total Estimated Cost ${final_cost:.2f} exceeds $3000.\"\n    except Exception as e:\n        return 0.0, f\"Error in budget check: {e}\""}, {"type": "code", "name": "Product Links Presence and Retailer Credibility", "description": "Ensure at least two HTTP(S) links exist and that at least one appears to be from a recognized retailer/manufacturer. Preferably one link relates to an IEM system and one to a mixing board.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document to parse for links.\"\n        text = \"\"\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                return 0.0, \"Could not read document text.\"\n        low = text.lower()\n        urls = re.findall(r\"https?://[^\\s)]+\", text)\n        url_count = len(urls)\n        retailers = [\n            \"sweetwater\", \"bhphotovideo\", \"guitarcenter\", \"zzounds\", \"thomann\",\n            \"amazon\", \"musiciansfriend\", \"reverb.com\", \"sennheiser\", \"shure\",\n            \"yamaha\", \"behringer\", \"mackie\", \"soundcraft\", \"roland\"\n        ]\n        has_retail = any(any(r in u.lower() for r in retailers) for u in urls)\n        # Heuristic: try to see if nearby text suggests 'iem' and 'mix' for two different links\n        score = 0.0\n        fb = []\n        if url_count >= 2:\n            score += 0.3\n            fb.append(f\"Found {url_count} URLs.\")\n        if has_retail:\n            score += 0.1\n            fb.append(\"Recognized retailer/manufacturer domain present.\")\n        # Keyword proximity check\n        iem_related = any(\"iem\" in low[max(0, low.find(u.lower())-100):low.find(u.lower())+100] or \"in-ear\" in low[max(0, low.find(u.lower())-100):low.find(u.lower())+100] for u in urls if low.find(u.lower()) != -1)\n        mixer_related = any(\"mixer\" in low[max(0, low.find(u.lower())-100):low.find(u.lower())+100] or \"mixing board\" in low[max(0, low.find(u.lower())-100):low.find(u.lower())+100] for u in urls if low.find(u.lower()) != -1)\n        if iem_related:\n            score += 0.05\n            fb.append(\"At least one URL appears near IEM context.\")\n        if mixer_related:\n            score += 0.05\n            fb.append(\"At least one URL appears near mixer context.\")\n        # Cap score at weight 0.5\n        score = min(score, 0.5)\n        return score, \"; \".join(fb) if fb else \"No strong link evidence.\"\n    except Exception as e:\n        return 0.0, f\"Error in link check: {e}\""}, {"type": "code", "name": "Two IEM Packs and Independent Mixes Mentioned", "description": "Check the text mentions two IEM packs/bodypacks and independent/separate mixes (e.g., dual-mono or L/R assignment).", "weight": 0.3, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document to parse.\"\n        text = \"\"\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                return 0.0, \"Could not read document text.\"\n        low = text.lower()\n        score = 0.0\n        fb = []\n        # Evidence of 2 packs\n        if re.search(r\"\\b(2|two)\\s+(iem\\s+)?(packs?|body\\s*packs?)\\b\", low):\n            score += 0.15\n            fb.append(\"Mentions two IEM packs/bodypacks.\")\n        # Evidence of independent mixes\n        if re.search(r\"(independent|separate)\\s+mix(es)?\", low) or \"dual mono\" in low or re.search(r\"mix\\s*1\\b.*mix\\s*2\\b\", low, re.S):\n            score += 0.1\n            fb.append(\"Mentions independent/separate mixes.\")\n        # Evidence of L/R assignment\n        if (\"left\" in low and \"right\" in low and \"assign\" in low) or \"l/r\" in low:\n            score += 0.05\n            fb.append(\"Mentions L/R assignment (dual-mono or stereo split).\")\n        score = min(score, 0.3)\n        return score, \"; \".join(fb) if fb else \"No explicit evidence of two packs with independent mixes.\"\n    except Exception as e:\n        return 0.0, f\"Error detecting packs/mixes: {e}\""}, {"type": "llm_judge", "name": "Functional Compliance with Requirements", "description": "Evaluate whether the proposed rig functionally meets all constraints: analogue mixer w/ onboard compression, reverb, delay; only two vocal mic inputs; two independent IEM mixes via RF; mobility and placement flexibility; excludes helical antennas; supports live monitoring by IEM tech.", "weight": 2.3, "judge_prompt": "Review the PDF content and assess FUNCTIONAL COMPLIANCE against these requirements:\n- Mixer must be analogue (not digital), but onboard digital effects are acceptable; onboard compression, reverb, and delay are present and used for the two vocal channels.\n- Inputs: exactly the two on-stage dynamic vocal microphones are specified as inputs (not a full band mix). \n- RF IEM system provides two independent mixes for two singers using two packs (e.g., dual-mono or two transmit channels) and is described clearly.\n- The rig is mobile, quick to deploy, and can be located side-stage, at FOH, or further away; a portable surface/stand/tray/case is specified for setup versatility.\n- Excludes helical UHF antennas in the BOM (since they are already owned), but acknowledges RF planning/line-of-sight considerations.\n\nScoring guidance:\n- Full: Clearly and accurately meets ALL above points with coherent descriptions of how they\u2019re achieved.\n- 75%: Meets most points with minor ambiguity or one missing detail.\n- 50%: Partial compliance; multiple unclear or missing aspects.\n- 25%: Major gaps; only a few elements addressed.\n- 0: Fundamentally noncompliant (e.g., digital desk without analogue requirement, not RF, lacks two packs, etc.).", "expectation": "A workable analogue-mixer-based IEM rig for two singers with onboard compression/reverb/delay and two independent mixes via RF, operationally mobile with a specified portable surface."}, {"type": "llm_judge", "name": "BOM Completeness and Within-Budget Design", "description": "Evaluate whether the Bill of Materials includes all necessary components (IEM transmitter + 2 packs, analogue mixer with onboard effects, cabling/adapters, power, mounting/case/portable surface) with realistic pricing and credible sourcing links, staying within $3,000.", "weight": 2.0, "judge_prompt": "Inspect the BOM/Cost sections and links. Judge whether:\n- Core items are present: RF IEM system with two packs; analogue mixer with onboard compression, reverb, and delay; all required cables/adapters; power supplies/strips; any mounting/rack/case; a portable surface/stand/tray.\n- Links are to credible, professional retailers/manufacturers; items appear to be available and appropriate.\n- Prices appear realistic for 2023 US market. The total estimated cost is under $3,000.\n\nScoring guidance:\n- Full: Comprehensive BOM with credible links and realistic prices; total clearly under budget.\n- 75%: Minor omissions or slight ambiguity, but feasible and under budget.\n- 50%: Noticeable gaps (missing key cables/accessories or unclear pricing), or budget borderline.\n- 25%: Major omissions or unrealistic pricing; budget risk.\n- 0: Fails to list essential components or obviously exceeds budget.", "expectation": "A complete, sourced BOM with realistic prices, including all accessories and a portable surface solution, staying under $3,000."}, {"type": "llm_judge", "name": "Signal Flow Correctness (Diagram + Text)", "description": "Assess whether the wiring/signal flow is correct for two vocals through analogue processing to two independent IEM mixes, and whether the diagram PNG and text align.", "weight": 1.4, "judge_prompt": "Evaluate the signal-flow diagram (PNG) and accompanying text for correctness and clarity:\n- Two dynamic vocal mics to analogue mixer channels; channel compression applied; sends to onboard reverb/delay and returns handled appropriately.\n- Two independent monitor mixes (e.g., Aux 1 and Aux 2) feeding an RF IEM transmitter solution correctly (stereo L/R split or dual-channel) to two packs.\n- Clear indication of how packs are configured (e.g., L-only vs R-only for dual-mono, or separate transmitters) and how the IEM tech monitors.\n- Diagram and text agree on routing and labeling.\n\nScoring guidance: Full = accurate, unambiguous routing; 75% = minor omissions; 50% = some ambiguity; 25% = significant errors; 0 = incorrect or missing routing.", "expectation": "A clear, technically correct path from mics to analogue mixer with compression and FX, to two separate IEM mixes feeding two packs, matching the diagram."}, {"type": "llm_judge", "name": "Operational Readiness & RF Considerations", "description": "Check whether the plan addresses real-world deployment concerns such as frequency legality and coordination for US 2023, line-of-sight, battery strategy, spares, and quick setup workflow across varied venues.", "weight": 0.8, "judge_prompt": "Assess mentions of practical considerations: US-legal RF bands for 2023 (avoid restricted ranges), frequency coordination/scan advice, line-of-sight and antenna placement basics (without adding helical antennas), battery strategy for packs, spares, power distribution, labeling, and a quick setup/teardown workflow suitable for side-stage/FOH flexibility.\n\nScore higher for concrete, accurate, and actionable advice tied to the proposed gear.", "expectation": "Shows awareness of US RF rules in 2023, basic coordination, battery/power/logistics, and a succinct, actionable setup workflow."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality & Professionalism", "description": "Holistic assessment of presentation quality, clarity, practicality, and usefulness for the intended audience (band + IEM tech).", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Presentation and Readability", "description": "Evaluate overall document organization, clarity, grammar, visual quality (images/tables readable), and navigation.", "weight": 2.5, "judge_prompt": "Judge the professionalism of presentation: clear structure and headings, concise writing, correct grammar, readable tables/figures (no pixelated PNGs), consistent terminology, and navigable flow. Score higher if it looks like a polished, client-ready document.", "expectation": "A cleanly formatted, well-written, and visually clear PDF suitable for client review."}, {"type": "llm_judge", "name": "Costing Transparency and Sourcing Quality", "description": "Assess whether per-item price, quantity, extended totals, and grand totals are easy to find and cross-check, and whether sources are reputable.", "weight": 2.0, "judge_prompt": "Evaluate how transparent and auditable the costs are: clear itemization, quantities, unit prices, extended totals, and a well-labeled grand total; links to reputable retailers; any notes on pricing date or variability. Score higher for unambiguous, cross-checkable costing.", "expectation": "An auditable cost breakdown with clear math and credible sourcing."}, {"type": "llm_judge", "name": "Practicality and Portability for Touring", "description": "Judge whether the proposal is practical for touring: portability, protection, power, quick setup, and real-world constraints.", "weight": 2.0, "judge_prompt": "Assess practicality for mid-sized venues/festivals: portability (cases, weight/size awareness), protection (rack/case), power distribution and cable management, battery/recharge plan, redundancy/spares, labeling, and a checklist for quick setup. Score higher for realistic, field-ready details.", "expectation": "A field-ready plan with cases, power, cable management, and a quick setup checklist suitable for varied venues."}, {"type": "llm_judge", "name": "Audience Appropriateness and Actionability", "description": "Determine if the document serves as a one-stop, actionable summary for the band and IEM tech.", "weight": 1.5, "judge_prompt": "Does the document function as a single, actionable reference? Look for a succinct executive summary, clear component list, links, signal-flow diagram, final cost page, and step-by-step setup/operation notes. Score higher if a crew member could execute without further clarification.", "expectation": "A one-stop, actionable guide enabling quick deployment by the IEM tech and singers."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "eb54f575-93f9-408b-b9e0-f1208a0b6759", "rubric": {"category_name": "Government \u2014 First-Line Supervisors of Police and Detectives | Patrol Rifle Procurement Report", "rationale": "This rubric enforces a self-documenting, executive-ready PDF report with explicit calculation steps and a defensible caliber recommendation. Stage 1 is a strict LLM-only gate that mandates a PDF with the exact five-section structure, a precise title, and the presence of verifiable calculation and justification elements. Stage 2 mixes lightweight code checks (numeric/signaling cues) with heavier LLM verification for correctness, consistency with assumptions (50% personal rifles, 15% buffer, end-of-2026 certification), and ballistic justification aligned to FBI protocols. Stage 3 evaluates professional quality, clarity, and command utility.", "max_total_score": 12.0, "stages": [{"name": "Stage 1 \u2014 Structure & Format Gate (LLM only)", "description": "Validate the output is a professionally formatted PDF report with the required title and five required sections, containing the necessary structural elements that enable verification in later stages.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.1, "rules": [{"type": "llm_judge", "name": "PDF Format, Title, and Section Structure", "description": "Check that the candidate output is a PDF with the exact structural contract necessary to enable verification.", "weight": 3.0, "judge_prompt": "You are evaluating whether the submitted output satisfies strict STRUCTURAL requirements. Do NOT judge correctness or quality; only verify presence and format. Be flexible with close synonyms for headers, but all five sections must be present.\n\nConfirm ALL of the following:\n1) File format is PDF (not Word, not plain text, not Excel).\n2) Title on the first page exactly reads: \"Procurement of New Duty Rifles for Departmental Issuance\" (allow minor punctuation/case deviations if clearly the same title).\n3) Document length is at least 2 pages and professionally formatted for executive review.\n4) Contains ALL FIVE required sections as top-level headings, in this general order (allow close synonyms but not missing content):\n   - Executive Summary \u2014 includes both the total number of rifles to be procured and the selected caliber stated explicitly in this section.\n   - Introduction \u2014 mentions authorized staffing up to 750 officers, that actual staffing is currently below authorized, the plan that all officers will be rifle-certified by end of 2026, and the purpose of equipping all certified officers.\n   - Rifle Quantity Analysis \u2014 shows a step-by-step calculation breakdown that uses: projected staffing (or an explicit staffing assumption), current/assumed personal rifle carry rate (~50%), and an explicit 15% operational buffer; includes a final computed total quantity.\n   - Terminal Ballistics Evaluation & Caliber Justification \u2014 presents a terminal ballistics comparison that references FBI ballistic testing protocols and typical police engagement distances; recommends a single caliber.\n   - Conclusion & Final Recommendation \u2014 restates both the recommended number of rifles and the chosen caliber.\n\nScoring:\n- 3.0: PDF format + exact title + all five sections present with the specified content requirements (numbers/caliber in Executive Summary; step-by-step math with 15% buffer in Analysis; FBI protocol referenced and single caliber recommended in Ballistics; final restatement in Conclusion). Professionally formatted, 2+ pages.\n- 2.5: Minor deviations (e.g., slightly different section names) but all required content elements are clearly present.\n- 2.0: Missing one required content element within the sections (e.g., Executive Summary lacks either the number or the caliber; or Analysis lacks explicit 15% buffer) but otherwise correct structure.\n- 1.0: Multiple required elements missing or unclear, but still a PDF with some of the required sections.\n- 0.0: Not a PDF, missing multiple sections, or lacks essential structure so that verification is not possible.\n\nOnly evaluate STRUCTURE and PRESENCE, not accuracy of numbers or the quality of analysis.", "expectation": "A professionally formatted PDF with the exact title and five clearly labeled sections containing the specified structural elements, ready for verification."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness & Verification", "description": "Verify the quantitative calculation, consistency with assumptions and timeline, and rigor of the ballistic justification. Mix of lightweight code checks and LLM judgment.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 3.0, "rules": [{"type": "code", "name": "Quantity & 15% Buffer Signaling", "description": "Detect presence of an explicit recommended total rifle count and mention of a 15% buffer in the PDF text.", "weight": 0.5, "code": "import re\\n\\n\ndef evaluate(workflow, context):\\n    \\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0, \"No document output detected.\"\\n\\n    # Prefer PDF text; fall back to DOCX if needed\\n    text = \"\"\\n    try:\\n        text = context.files.read_pdf_text(output.id) or \"\"\\n    except Exception:\\n        try:\\n            text = context.files.read_docx_text(output.id) or \"\"\\n        except Exception:\\n            text = \"\"\\n\\n    if not text:\\n        return 0.0, \"Unable to extract text from document.\"\\n\\n    t = re.sub(r\"\\s+\", \" \", text).lower()\\n\\n    # Detect 15% buffer mention\\n    buffer_patterns = [r\"15\\s*%\", r\"15 percent\", r\"fifteen percent\", r\"0\\.15\"]\\n    has_buffer = any(re.search(p, t) for p in buffer_patterns)\\n\\n    # Detect explicit recommended total count of rifles\\n    qty = None\\n    qty_patterns = [\\n        r\"(recommend(?:ation)?|procure|purchase|acquire|buy|order)[^\\n]{0,60}?(\\d{2,5})\\s+(?:patrol|duty)?\\s*rifles?\",\\n        r\"(total|final)\\s+(?:number|qty|quantity)\\s+(?:of\\s+)?rifles[^\\n]{0,20}?(\\d{2,5})\",\\n        r\"recommend(?:ation)?[^\\n]{0,80}?(\\d{2,5})\\s+(?:rifles|units)\",\\n        r\"(\\d{2,5})\\s+(?:new\\s+)?(?:patrol|duty)?\\s*rifles[^\\n]{0,30}?(?:to\\s+)?(?:procure|purchase|acquire)\"\\n    ]\\n    for p in qty_patterns:\\n        m = re.search(p, t)\\n        if m:\\n            try:\\n                # Last capturing group should be the number\\n                num = int(m.groups()[-1])\\n                if 10 <= num <= 10000:  # plausible bounds\\n                    qty = num\\n                    break\\n            except Exception:\\n                continue\\n\\n    score = 0.0\\n    details = []\\n    if qty is not None:\\n        score += 0.5\\n        details.append(f\"Detected recommended quantity: {qty}\")\\n    else:\\n        details.append(\"No clear recommended quantity detected.\")\\n\\n    if has_buffer:\\n        score += 0.5\\n        details.append(\"15% buffer mention detected.\")\\n    else:\\n        details.append(\"No explicit 15% buffer mention detected.\")\\n\\n    return score, \"; \".join(details)\\n"}, {"type": "code", "name": "Caliber Mention and AR-15/SBR Compatibility Signal", "description": "Check that the document names a plausible rifle caliber and references AR-15 platform and/or short-barreled compatibility.", "weight": 0.7, "code": "import re\\n\\n\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0, \"No document output detected.\"\\n\\n    text = \"\"\\n    try:\\n        text = context.files.read_pdf_text(output.id) or \"\"\\n    except Exception:\\n        try:\\n            text = context.files.read_docx_text(output.id) or \"\"\\n        except Exception:\\n            text = \"\"\\n\\n    if not text:\\n        return 0.0, \"Unable to extract text from document.\"\\n\\n    t = text.lower()\\n\\n    caliber_patterns = [\\n        r\"5\\.56x45\", r\"5\\.56 nato\", r\"5\\.56mm\", r\"\\.223\\s*rem\", r\"\\.223\\s*remington\",\\n        r\"\\.300\\s*blk\", r\"300\\s*blk\", r\"\\.300\\s*blackout\", r\"7\\.62x39\", r\"6\\.8\\s*spc\",\\n        r\"9mm\", r\"9x19\", r\"7\\.62x51\", r\"\\.308\\s*win\"\\n    ]\\n    has_caliber = any(re.search(p, t) for p in caliber_patterns)\\n\\n    platform_patterns = [\\n        r\"ar-15\", r\"ar15\", r\"m4\", r\"short-?barrel\", r\"sbr\", r\"10\\.3\\\"\", r\"11\\.5\\\"\", r\"12\\.5\\\"\", r\"carbine\"\\n    ]\\n    has_platform = any(re.search(p, t) for p in platform_patterns)\\n\\n    score = 0.0\\n    details = []\\n\\n    if has_caliber:\\n        score += 0.6\\n        details.append(\"Caliber mention detected.\")\\n    else:\\n        details.append(\"No clear caliber mention detected.\")\\n\\n    if has_platform:\\n        score += 0.4\\n        details.append(\"AR-15/SBR/platform compatibility mention detected.\")\\n    else:\\n        details.append(\"No explicit AR-15/SBR/platform compatibility mention detected.\")\\n\\n    return score, \"; \".join(details)\\n"}, {"type": "llm_judge", "name": "Quantity Methodology Consistency Check", "description": "Assess whether the step-by-step quantity calculation is logically consistent and correctly incorporates projected staffing, ~50% personal rifle carry rate, and the explicit 15% buffer, yielding a coherent final number.", "weight": 2.0, "judge_prompt": "Evaluate the Rifle Quantity Analysis for correctness and internal consistency. Consider:\n- Does it clearly state a staffing assumption or projection (<= 750) and use it in calculations?\n- Does it account for ~50% of officers currently carrying personal rifles, showing how departmental issuance needs are derived from that?\n- Is the 15% operational buffer explicitly applied to the issuance quantity and shown in the math (e.g., spares for training/maintenance)?\n- Do rounding decisions (up/down) make sense for procurement (e.g., whole rifles, reasonable spare pool)?\n- Is the final recommended total consistent with the described steps?\n\nScoring guidance:\n- Full credit: Clear, step-by-step math with transparent assumptions, correct application of ~50% carry and 15% buffer, and a final number that matches the steps.\n- Partial: Minor ambiguity in one element (e.g., rounding or personal rifle application) but broadly consistent.\n- Minimal: Vague or missing one or more critical inputs (staffing, 50% carry, 15% buffer) or final number does not match steps.\n- None: No coherent calculation or contradictions.", "expectation": "A transparent, reproducible calculation yielding a final number aligned with staffing, personal rifle rates, and buffer."}, {"type": "llm_judge", "name": "Ballistics Rigor and Caliber Justification", "description": "Check that the ballistic comparison references FBI ballistic protocols, addresses typical police engagement distances, barrier performance, over-penetration, cost/practicality, and recommends a single caliber compatible with short-barreled AR-15 platforms.", "weight": 1.6, "judge_prompt": "Evaluate the Terminal Ballistics Evaluation & Caliber Justification section:\n- Does it explicitly reference FBI ballistic testing protocols (e.g., gel tests, barrier protocols)?\n- Does it discuss typical patrol engagement distances and performance at those ranges?\n- Does it address barrier penetration (e.g., auto glass, doors, interior walls) and over-penetration risks?\n- Are cost and practical considerations (availability, logistics, recoil/controllability) weighed?\n- Is a single caliber recommended, with explicit compatibility for short-barreled AR-15-style rifles?\n\nScore higher when the analysis is objective, cites or mirrors credible data trends, and the recommendation logically follows from the comparison.", "expectation": "A data-driven, FBI-protocol-aligned justification selecting one caliber suitable for SBR AR-15 platforms."}, {"type": "llm_judge", "name": "Operational Alignment and Timeline Integration", "description": "Verify that the recommendation aligns with department staffing realities and the 2026 certification timeline, and accounts for officers with personal rifles and inventory buffer for training/maintenance.", "weight": 1.2, "judge_prompt": "Assess whether the report:\n- Recognizes authorization up to 750 officers and that current staffing is below that level, using a reasonable staffing assumption/projection.\n- Integrates the goal of universal rifle certification by end of 2026 into the quantity recommendation.\n- Accounts for officers who currently carry personally owned rifles and limits issuance accordingly.\n- Explicitly includes inventory to support training and maintenance (15% buffer) and explains its operational purpose.\n\nScore higher if the logic is consistent across Introduction, Rifle Quantity Analysis, and Conclusion, and the Final Recommendation restates both number and caliber clearly.", "expectation": "A coherent, timeline-aware, operationally grounded recommendation that is consistent across sections."}], "on_failure_action": "skip_remaining", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality & Command Utility", "description": "Holistic assessment of executive suitability, organization, clarity, and command utility for decision-making.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Executive Tone and Clarity", "description": "Professional tone, concise executive summary, and clear writing appropriate for command staff.", "weight": 1.0, "judge_prompt": "Evaluate whether the document is suitable for executive review:\n- Is the tone formal, objective, and professional throughout?\n- Is the Executive Summary concise and decision-focused, clearly stating the number and caliber?\n- Is language clear and free of ambiguity or jargon without explanation?\nAssign higher scores for crisp, direct, and professional communication.", "expectation": "Concise, professional language with a decision-focused executive summary."}, {"type": "llm_judge", "name": "Organization and Navigability", "description": "Section flow, headings, and internal references that make the report easy to navigate and understand.", "weight": 0.8, "judge_prompt": "Assess the organization:\n- Are the required sections clearly labeled and well-separated with headings/subheadings?\n- Is there logical flow between sections (Intro -> Analysis -> Ballistics -> Conclusion)?\n- Are tables/figures (if any) labeled and referenced in text?\nScore higher if the document is easy to skim and key findings are easy to locate.", "expectation": "Clear structure that supports rapid executive comprehension."}, {"type": "llm_judge", "name": "Actionability and Command Utility", "description": "Quality of the final recommendation and usefulness for procurement decision-making.", "weight": 0.8, "judge_prompt": "Evaluate decision support:\n- Does the Conclusion & Final Recommendation restate the number and caliber and provide a clear go-forward action (e.g., approve procurement quantity, next steps, training/rollout considerations)?\n- Are key operational risks or constraints briefly noted (e.g., supply chain, training throughput, maintenance)?\n- Would a Chief or command staff find this sufficient to make a decision or request minimal follow-up?\nScore higher when recommendations are crisp, defensible, and actionable.", "expectation": "An actionable, defensible recommendation suitable for immediate command decision."}, {"type": "llm_judge", "name": "Evidence Use and Credibility", "description": "Use of objective data and credible references to support claims.", "weight": 0.4, "judge_prompt": "Assess whether claims (especially in the ballistics section) are supported by objective data or credible references (e.g., FBI ballistic testing protocols, published law enforcement data/trends). Explicit citations are not required, but clearer references and alignment to recognized standards should score higher.", "expectation": "Objective, standards-aligned evidence supporting the caliber choice and analysis."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "d3d255b2-f5f2-4841-9f62-2083ec9ef3da", "rubric": {"category_name": "Seller Offer Evaluation Report (Real Estate Sales Agent)", "rationale": "Pattern B (Document). The rubric enforces a self-documenting PDF report with a specific section layout and structured elements (tables and clearly labeled sections) so verification is trivial. Stage 1 is an LLM-only gate to ensure the exact structure exists. Stage 2 mixes light code checks (text extraction and deterministic validations) with heavier LLM judgment for cross-references and reasonableness (weighted ~5x more than code). Stage 3 performs a holistic quality review focused on professional tone, clarity, actionability, and strategic value for a seller client.", "max_total_score": 26.0, "stages": [{"name": "Stage 1 \u2014 Document Shape and Structure Gate (LLM only)", "description": "MANDATORY structural gate. Ensures the output is a 2\u20133 page, client-facing report in PDF (preferred) or DOCX with the exact sections and tables required for verification.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.2, "rules": [{"type": "llm_judge", "name": "Document Format and Required Sections", "description": "Check that the output is a professionally formatted 2\u20133 page PDF (preferred) or DOCX with all required sections and structured elements.", "weight": 6.0, "judge_prompt": "You are verifying strict STRUCTURE ONLY (not content quality). Review the candidate output and score based on whether it is a professionally formatted seller-facing report with the exact elements below. Be flexible with section names if clearly equivalent.\n\nFormat requirements:\n- File must be a PDF (preferred) or DOCX document (not plain text, not Excel). \n- Length must be approximately 2\u20133 pages. \n- Professional layout with clear section headers and readable formatting.\n\nRequired sections and structured elements (all must be visible and clearly delineated):\n1) Header/Introduction on page 1: identify property and list price ($525,000) and purpose of report. \n2) Offer Summary section containing a table (or a clearly laid out bullet list that reads like a table) with fields: Buyer type (cash), Offer price ($500,000), Condition (As-Is), Contingencies (None), Closing timeline (30 days). Earnest money and special terms are optional but count positively if present. \n3) Market Analysis Review: a narrative section referencing the home being slightly overpriced and time-on-market; references to comps/CMA or pricing context are acceptable. \n4) Negotiation Strategy: a dedicated section explaining approach (risks/benefits, leverage from cash/as-is, speed-to-close). \n5) Recommended Counteroffer: a dedicated section that includes a numeric counteroffer price and any key terms (e.g., timeline, earnest money suggestion, proof of funds, allocation of closing costs, confirm as-is). \n6) Next Steps / Response Guidance: actionable guidance for the seller, including a response deadline or timeline.\n\nScoring:\n- 6.0: PDF, 2\u20133 pages, all six required sections present; Offer Summary presented as table-like structure; Recommended Counteroffer includes numeric price and key terms; professional formatting throughout.\n- 5.0: DOCX (2\u20133 pages) with all required sections, or PDF with all required sections but missing one minor field (e.g., earnest money) in the Offer Summary; still clearly structured and professional.\n- 4.0: Valid PDF/DOCX of at least 2 pages with 4\u20135 of the required sections present, but missing one major structural element (e.g., no Offer Summary table OR no explicit Recommended Counteroffer section).\n- 2.5: Valid PDF/DOCX but only 2\u20133 sections present OR lacks clear structural separation (e.g., Offer Summary not tabular/bullet-structured and counteroffer not clearly labeled).\n- 0.0: Wrong format (not PDF/DOCX) OR fewer than 2 pages OR not a seller-facing report at all.\n\nDo NOT evaluate correctness of numbers or strategy\u2014only presence and structure.", "expectation": "A 2\u20133 page PDF with the six sections above, including an Offer Summary table and a clearly labeled Recommended Counteroffer with a numeric price."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Consistency Verification (Mixed)", "description": "Now that structure is guaranteed, verify factual capture of offer terms, plausibility of the counteroffer, and consistency of reasoning with the market context and cash/as-is conditions.", "is_required": false, "max_points": 13.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Offer Terms Captured (Price, Cash, As-Is, No Contingencies, 30-Day Close)", "description": "Verify the text includes the key offer terms: list price $525,000, offer $500,000, cash buyer, as-is, no contingencies, and closing in ~30 days. Partial credit for each element present.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float or (float, str)\n    \"\"\"\n    weight = 0.8\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n    try:\n        text = context.files.read_pdf_text(output.id) if output.ext.lower() == '.pdf' else context.files.read_docx_text(output.id)\n    except Exception:\n        return 0.0, \"Could not extract document text\"\n    if not text:\n        return 0.0, \"Empty document text\"\n    low = text.lower()\n\n    def has_money(val):\n        # match $525,000 or 525,000 or 525k\n        patterns = [\n            rf\"\\$?{val:,}\",\n            rf\"\\b{val}\\b\",\n        ]\n        if val == 525000:\n            patterns += [r\"\\b525k\\b\", r\"\\b$?525,?000\\b\"]\n        if val == 500000:\n            patterns += [r\"\\b500k\\b\", r\"\\b$?500,?000\\b\"]\n        return any(re.search(p, low) for p in patterns)\n\n    checks = []\n    # List price 525,000\n    checks.append(has_money(525000) or (\"list\" in low and (\"525\" in low and \"000\" in low)))\n    # Offer 500,000\n    checks.append(has_money(500000) or (\"offer\" in low and (\"500\" in low and \"000\" in low)))\n    # Cash buyer\n    checks.append(\"cash\" in low)\n    # As-is condition\n    checks.append(re.search(r\"as[-\\s]?is\", low) is not None)\n    # No contingencies (flexible)\n    contingency_terms = [r\"no contingenc\", r\"non[-\\s]?contingent\", r\"waiv(?:e|es|ed) contingenc\", r\"no (?:financing|appraisal|inspection) contingenc\"]\n    checks.append(any(re.search(t, low) for t in contingency_terms))\n    # 30-day closing\n    close_30 = re.search(r\"\\b30\\b[^\\n]{0,40}\\bday\", low) or re.search(r\"\\b30[-\\s]?day\", low) or (\"thirty\" in low and \"day\" in low)\n    checks.append(close_30 is not None)\n\n    score = weight * (sum(1 for c in checks if c) / len(checks))\n    feedback = f\"Offer term checks met: {sum(1 for c in checks if c)}/{len(checks)}\"\n    return score, feedback"}, {"type": "code", "name": "Counteroffer Price Presence and Plausibility", "description": "Detect a recommended counteroffer price near keywords and check it is plausible (between $500k and $525k). If the agent recommends accepting $500k instead of countering, award full credit.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 0.6\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n    try:\n        text = context.files.read_pdf_text(output.id) if output.ext.lower() == '.pdf' else context.files.read_docx_text(output.id)\n    except Exception:\n        return 0.0, \"Could not extract document text\"\n    if not text:\n        return 0.0, \"Empty document text\"\n    low = text.lower()\n\n    # If they clearly advise acceptance at $500,000 (no counter), give full credit\n    accept_patterns = [r\"recommend(?:ing)? acceptance\", r\"advise (?:the )?seller to accept\", r\"we (?:recommend|suggest) acceptance\"]\n    mentions_offer = (\"500,000\" in text) or (\"$500,000\" in text) or re.search(r\"\\b500k\\b\", low)\n    if any(re.search(p, low) for p in accept_patterns) and mentions_offer:\n        return weight, \"Recommends acceptance at $500k\"\n\n    # Find dollar amounts near counteroffer keywords\n    money_re = re.compile(r\"\\$\\s?([0-9]{1,3}(?:,[0-9]{3})+|[0-9]{5,6})|\\b([0-9]{3})k\\b\", re.I)\n    key_words = [\"counter\", \"counteroffer\", \"counter-offer\", \"propose\", \"recommend\", \"suggest\"]\n    best = None\n    for m in money_re.finditer(text):\n        start = m.start()\n        window = low[max(0, start-120): start+120]\n        if any(k in window for k in key_words):\n            amt = m.group(1) or m.group(2)\n            try:\n                if m.group(2):\n                    val = int(m.group(2)) * 1000\n                else:\n                    val = int(amt.replace(',', ''))\n                best = val\n                break\n            except Exception:\n                continue\n\n    if best is None:\n        return 0.2, \"No explicit counter price found near keywords; minimal credit\"\n\n    # Plausibility bands\n    if 500000 <= best <= 525000:\n        return weight, f\"Counter price plausible: ${best:,}\"\n    elif 490000 <= best <= 535000:\n        return weight * 0.5, f\"Counter price marginally plausible: ${best:,}\"\n    else:\n        return 0.1, f\"Counter price out of expected range: ${best:,}\""}, {"type": "code", "name": "Market Context Referenced (Overpricing/DOM/Repairs/Concessions)", "description": "Check that the negotiation discussion references market context: overpricing, time-on-market (DOM), minor repairs/as-is, and concessions/credits. Partial credit for coverage of these themes.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 0.6\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n    try:\n        text = context.files.read_pdf_text(output.id) if output.ext.lower() == '.pdf' else context.files.read_docx_text(output.id)\n    except Exception:\n        return 0.0, \"Could not extract document text\"\n    if not text:\n        return 0.0, \"Empty document text\"\n    low = text.lower()\n\n    groups = {\n        'overpricing': [r\"overpriced\", r\"priced above market\", r\"over market\", r\"price reduction\"],\n        'dom': [r\"days on market\", r\"\\bdom\\b\", r\"months on market\"],\n        'repairs': [r\"minor repairs\", r\"as[-\\s]?is\", r\"deferred maintenance\"],\n        'concessions': [r\"concession\", r\"seller credit\", r\"closing cost credit\", r\"price concession\"],\n    }\n\n    hits = 0\n    for k, pats in groups.items():\n        if any(re.search(p, low) for p in pats):\n            hits += 1\n    score = weight * (hits / len(groups))\n    return score, f\"Context themes covered: {hits}/{len(groups)}\""}, {"type": "llm_judge", "name": "Cross-Reference Consistency of Offer Terms", "description": "Ensure the report\u2019s Offer Summary and narrative consistently reflect: cash buyer, as-is, no contingencies, closing in ~30 days, and list/offer prices without contradictions.", "weight": 3.5, "judge_prompt": "Evaluate consistency of offer terms across the Offer Summary and narrative sections. Specifically check for alignment on: (a) cash buyer, (b) as-is condition, (c) no contingencies, (d) closing timeline ~30 days, and (e) list price $525,000 versus offer price $500,000. Identify contradictions (e.g., references to financing/appraisal contingencies for a cash, non-contingent offer). Score higher when all terms are consistently and accurately represented across sections and tables.", "expectation": "No contradictions; consistent mention of cash, as-is, no contingencies, ~30 days, and the correct prices."}, {"type": "llm_judge", "name": "Market-Based Pricing Logic", "description": "Judge whether the report uses the market analysis context (overpriced status, time on market, comps) to justify a negotiation stance and counteroffer recommendation.", "weight": 3.0, "judge_prompt": "Assess whether the Market Analysis Review and negotiation narrative use market context to justify the recommended price/action: notes that the property is slightly overpriced, has been on the market for months, and addresses minor repairs/as-is. Looks for concrete tie-ins to comps, pricing trends, or DOM to justify either accepting $500k or countering (e.g., $505\u2013$515k). Award higher scores when the rationale is explicit and coherent.", "expectation": "Clear linkage from market context to pricing guidance (accept or counter within a market-justified band)."}, {"type": "llm_judge", "name": "Negotiation Strategy Suitability (Seller\u2019s Best Interest)", "description": "Evaluate the suitability and balance of the negotiation strategy for a seller wanting a quick sale without undue pressure.", "weight": 2.5, "judge_prompt": "Evaluate whether the strategy advises in the seller\u2019s best interest without pressuring: articulates pros/cons of accepting vs countering, highlights benefits of cash/as-is/no contingencies (speed, certainty), acknowledges risks of waiting (further price cuts, prolonged DOM), and suggests a response path that preserves leverage (e.g., request proof of funds, earnest money, clear response deadline). Higher score for balanced, fiduciary-aligned guidance.", "expectation": "Balanced, fiduciary-aligned strategy with clear pros/cons and a recommended path."}, {"type": "llm_judge", "name": "Counteroffer Terms Completeness and Feasibility", "description": "Check that the recommended counteroffer includes a realistic price and key terms (timeline, earnest money suggestion, proof of funds, as-is), without introducing infeasible conditions.", "weight": 2.0, "judge_prompt": "Review the Recommended Counteroffer section for completeness and feasibility. Does it include: a specific counter price, confirmation of as-is condition, a realistic closing timeline (around 30 days or faster), a reasonable earnest money suggestion, and request for proof of funds? The terms should be operationally feasible (no appraisal financing contingencies for a cash buyer, no impractical timelines). Higher scores for clearly actionable, realistic terms aligned to a quick close.", "expectation": "Actionable counteroffer terms: price, as-is confirmation, timeline, earnest money, proof of funds; all feasible for a cash close."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Holistic Quality Assessment (LLM)", "description": "Professionalism, clarity, actionability, and client-facing quality of the report.", "is_required": false, "max_points": 7.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Tone and Client-Facing Clarity", "description": "Assess tone, clarity, and appropriateness for a seller client; concise narrative style suitable for a 2\u20133 page report.", "weight": 2.0, "judge_prompt": "Judge whether the report uses a professional, respectful, and supportive tone appropriate for a seller client. Writing should be concise, clear, and easy to follow in a 2\u20133 page narrative. Higher scores for plain language that aids decision-making without jargon or pressure.", "expectation": "Professional, concise, and client-friendly prose."}, {"type": "llm_judge", "name": "Structure and Readability", "description": "Evaluate the readability, use of headers, table for Offer Summary, and overall visual organization.", "weight": 1.5, "judge_prompt": "Evaluate whether the document is well-structured: clear headers, logical flow, readable formatting, and use of a table or clearly structured bullets for the Offer Summary. Award higher scores if sectioning makes quick scanning easy and important details are surfaced prominently.", "expectation": "Clean, scannable structure with clear headers and structured Offer Summary."}, {"type": "llm_judge", "name": "Strategic Insight and Value-Add", "description": "Evaluate depth of insight beyond restating facts\u2014e.g., weighing scenarios, noting leverage, suggesting walk-away thresholds or likely buyer responses.", "weight": 2.0, "judge_prompt": "Assess whether the report provides meaningful strategic insight: weighs accept vs counter scenarios, discusses likely buyer responses to counters in the 500\u2013515k range, articulates leverage (cash/as-is, speed), and may suggest a walk-away threshold. Higher scores for nuanced, practical guidance that helps the seller decide quickly.", "expectation": "Nuanced, practical strategic guidance that adds decision-making value."}, {"type": "llm_judge", "name": "Actionability and Next Steps", "description": "Evaluate whether the report ends with clear, actionable next steps and timing, making it easy for the seller to proceed.", "weight": 1.5, "judge_prompt": "Check whether the report provides a clear call to action and next steps (e.g., choose accept/counter path, response deadline, documents to gather, confirm proof of funds, set escrow timeline). Higher scores for specificity and immediate usability by the seller.", "expectation": "Specific, actionable next steps aligned with a quick close."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "c44e9b62-7cd8-4f72-8ad9-f8fbddb94083", "rubric": {"category_name": "Government \u2014 Administrative Services Managers \u2014 FTE Reduction Package", "rationale": "This rubric enforces a self-documenting, mixed-output package: a revised org chart (PDF), an FTE report (Excel), and a briefing note (DOCX). Stage 1 is an LLM-only gate that requires exact, verifiable structure across all three deliverables to make downstream verification easy. Stage 2 mixes light code checks (bounds and structural consistency) with heavier LLM cross-file verification (attrition logic, supervisor-specific reductions, principle alignment). Stage 3 evaluates overall professional quality and stakeholder readiness. Code rules are weighted lower than LLM rules (~5x less on average) to reflect the nuanced judgment needed for cross-document consistency and policy alignment.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (ALL THREE DELIVERABLES)", "description": "Gate: Confirm the package includes all three deliverables in the exact, verifiable formats with required structures so that verification is possible. LLM-only per philosophy.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.5, "rules": [{"type": "llm_judge", "name": "Org Chart PDF \u2014 Required Structure Present", "description": "Check there is a revised organizational chart delivered as a PDF with clearly indicated reductions.", "weight": 2.0, "judge_prompt": "You will see multiple candidate outputs. Identify the revised organizational chart deliverable and verify all of the following shape requirements:\n\nFormat requirements:\n- The org chart is provided as a PDF (not Word, not Excel, not images embedded in other files).\n- It visually represents the Administrative Support Services Branch with hierarchical structure, including Central and Regional groupings.\n- The line \u201cRegional Support Services Supervisor\u201d (or close variant) is visibly present in the chart.\n\nStructural requirements:\n- Boxes/nodes represent positions (assume one FTE per box unless a number in parentheses is shown).\n- Positions planned for reduction are visually highlighted (e.g., color/outline/strikethrough, legend explaining highlight, or explicit labels like \u201creduction\u201d).\n- The chart header/title or caption indicates it is a revised chart for next fiscal year or otherwise clearly distinguishes it from the current chart.\n\nScoring:\n- 2.0: PDF format and all structural requirements are present (hierarchy, correct branch, Regional Support Services Supervisor visible, and reductions clearly highlighted/legend).\n- 1.0: PDF present with hierarchy and branch, but reduction highlighting or legend is unclear/implicit.\n- 0.0: Not a PDF, or not an org chart, or missing key branch/supervisor structure.\n\nOnly check presence/format, not the correctness of the reductions.", "expectation": "A professional PDF org chart for the Administrative Support Services Branch with clear visual highlighting of reductions and the Regional Support Services Supervisor visible."}, {"type": "llm_judge", "name": "FTE Report Excel \u2014 Required Sheets and Columns", "description": "Check there is an Excel-based FTE plan with required sheet(s) and columns.", "weight": 2.0, "judge_prompt": "From the outputs, locate the Excel/Spreadsheet deliverable that is the updated FTE Report. Verify the following shape requirements (be flexible with near-synonyms for names):\n\nFormat requirements:\n- File is an Excel workbook (.xlsx preferred) with at least one main data sheet for the plan.\n\nRequired data sheet (name like \u201cFTE Plan\u201d, \u201cFTE Report\u201d, or similar):\n- Contains a tabular dataset with these logical columns (flexible naming):\n  1) Position Title\n  2) Unit/Supervisor (e.g., Team, Unit, Manager, Reporting Line)\n  3) Current FTE (baseline/current year)\n  4) Planned FTE (next fiscal year)\n  5) FTE Change (number)\n  6) FTE Change (%)\n- A visible grand total row or a separate small summary section on the sheet showing total current FTE, total planned FTE, and total change.\n\nOptional but helpful:\n- A second sheet named like \u201cSummary\u201d or \u201cAssumptions/Methodology\u201d that briefly documents how reductions were derived (do not penalize if missing).\n\nScoring:\n- 2.0: Excel with a clear main data sheet containing the six logical columns and a visible total/summary.\n- 1.0: Excel with the main sheet present but one of the six logical columns OR the totals/summary is missing.\n- 0.0: Not Excel, or no clear main data sheet with position-level current vs planned FTEs.\n\nDo not judge numerical correctness, only structure/presence.", "expectation": "An .xlsx file with a main plan sheet containing position-level Current vs Planned FTEs, Unit/Supervisor, and change columns, plus totals."}, {"type": "llm_judge", "name": "Briefing Note DOCX \u2014 Required Sections Present", "description": "Check there is a briefing note in Word (DOCX) format with required sections and references to planning principles.", "weight": 2.0, "judge_prompt": "Identify the briefing note deliverable and verify the following shape requirements:\n\nFormat and length requirements:\n- File is a Word document (DOCX). PDF is not acceptable for this deliverable.\n- At least 1 full page; professional briefing note formatting (title/header and sections with headings).\n\nRequired sections (header names can vary slightly):\n1) Title/Heading indicating it is a Briefing Note.\n2) Background/Context.\n3) Proposed Reductions (or Analysis) describing the planned next-fiscal-year changes.\n4) Alignment with Budget Planning Principles \u2014 must explicitly reference the provided principles, with specific mention of \u201cPrinciple #7\u201d (or \u201cPrinciple Seven\u201d).\n5) Risks and Mitigations (or Impacts and Mitigation).\n6) Implementation Timeline (or Next Steps).\n7) Approvals/Distribution (or Contacts/Sign-off) section.\n\nScoring:\n- 2.0: DOCX with all 7 sections, including an explicit reference to Principle #7 in the principles section.\n- 1.0: DOCX with at least 5 of the 7 sections, including a principles section but missing explicit Principle #7 mention.\n- 0.0: Not a DOCX, or missing major briefing note structure (\u22644 sections).\n\nOnly check presence/format, not the quality of the content.", "expectation": "A DOCX briefing note with the listed sections and explicit reference to Principle #7 in the principles alignment section."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification of Correctness and Cross-File Consistency", "description": "Now verify that the structured package is internally consistent and implements the requested reductions: \u22654% overall reduction, 10% reduction under Regional Support Services Supervisor, and attrition/vacancy details correctly reflected. Code rules do bounds/structure checks; LLM rules verify nuanced cross-file logic and policy alignment.", "is_required": false, "max_points": 9.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Excel totals show \u22654% reduction and valid planned FTEs", "description": "Parse the Excel plan and verify planned FTEs are non-negative, not exceeding current totals, and that total reduction is at least 4%.", "weight": 1.2, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    WEIGHT = 1.2\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0, \"No outputs found.\"\n\n    # Helper functions\n    def norm(s):\n        return re.sub(r\"\\s+\", \" \", str(s).strip().lower())\n\n    def looks_current(col):\n        c = norm(col)\n        return (\"current\" in c or \"baseline\" in c) and (\"fte\" in c or \"headcount\" in c or c.endswith(\"current\"))\n\n    def looks_planned(col):\n        c = norm(col)\n        return (\"planned\" in c or \"target\" in c or \"budget\" in c or \"next\" in c or \"fy\" in c) and (\"fte\" in c or \"headcount\" in c or c.startswith(\"planned\"))\n\n    feedback = []\n    best_score = 0.0\n\n    for res in outputs:\n        if not getattr(res, 'is_spreadsheet', False):\n            continue\n        try:\n            path = context.files.get_path(res.id)\n            xlf = pd.ExcelFile(path)\n        except Exception as e:\n            feedback.append(f\"Failed to open spreadsheet: {e}\")\n            continue\n\n        for sheet in xlf.sheet_names:\n            try:\n                df = pd.read_excel(path, sheet_name=sheet)\n            except Exception as e:\n                feedback.append(f\"Read error {sheet}: {e}\")\n                continue\n            if df is None or df.empty or df.shape[1] < 2:\n                continue\n\n            cols = list(df.columns)\n            current_candidates = [c for c in cols if looks_current(c)]\n            planned_candidates = [c for c in cols if looks_planned(c)]\n            if not current_candidates or not planned_candidates:\n                continue\n\n            current_col = current_candidates[0]\n            planned_col = planned_candidates[0]\n\n            cur = pd.to_numeric(df[current_col], errors='coerce')\n            pla = pd.to_numeric(df[planned_col], errors='coerce')\n\n            # Filter to rows that have at least one of cur/pla defined\n            mask = cur.notna() | pla.notna()\n            cur = cur[mask].fillna(0)\n            pla = pla[mask].fillna(0)\n\n            total_current = float(cur.sum())\n            total_planned = float(pla.sum())\n\n            if total_current <= 0:\n                feedback.append(f\"Sheet '{sheet}': Non-positive total current FTE {total_current}.\")\n                continue\n\n            if (pla < 0).any():\n                feedback.append(f\"Sheet '{sheet}': Found negative planned FTE values.\")\n                # Still evaluate reduction but penalize score\n\n            reduction = (total_current - total_planned) / total_current\n            parts = []\n            if total_planned <= total_current + 1e-6:\n                parts.append(0.5)\n            else:\n                feedback.append(f\"Sheet '{sheet}': Planned total exceeds current total ({total_planned:.2f} > {total_current:.2f}).\")\n\n            if reduction >= 0.04:\n                parts.append(0.5)\n            elif reduction >= 0.03:\n                parts.append(0.35)\n                feedback.append(f\"Sheet '{sheet}': Reduction {reduction:.2%} < 4% threshold.\")\n            elif reduction >= 0.02:\n                parts.append(0.2)\n                feedback.append(f\"Sheet '{sheet}': Reduction {reduction:.2%} well below 4% threshold.\")\n            else:\n                parts.append(0.0)\n                feedback.append(f\"Sheet '{sheet}': Reduction {reduction:.2%} insufficient.\")\n\n            # Penalty if any negatives in planned\n            if (pla < 0).any():\n                penalty = 0.1\n            else:\n                penalty = 0.0\n\n            score = max(0.0, min(WEIGHT, WEIGHT * (sum(parts)) - penalty))\n            fb = f\"Sheet '{sheet}': current={total_current:.2f}, planned={total_planned:.2f}, reduction={reduction:.2%}.\"\n            feedback.append(fb)\n            best_score = max(best_score, score)\n\n    if best_score == 0.0 and feedback == []:\n        feedback = [\"No suitable spreadsheet with recognizable Current/Planned FTE columns found.\"]\n\n    return best_score, \" | \".join(feedback)[:800]"}, {"type": "code", "name": "Regional Support Services Supervisor reduced ~10% in Excel", "description": "Verify that rows under the Regional Support Services Supervisor (or equivalent) reflect approximately a 10% planned reduction.", "weight": 0.8, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    WEIGHT = 0.8\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0, \"No outputs found.\"\n\n    def norm(s):\n        return re.sub(r\"\\s+\", \" \", str(s).strip().lower())\n\n    def looks_current(col):\n        c = norm(col)\n        return (\"current\" in c or \"baseline\" in c) and (\"fte\" in c or \"headcount\" in c or c.endswith(\"current\"))\n\n    def looks_planned(col):\n        c = norm(col)\n        return (\"planned\" in c or \"target\" in c or \"budget\" in c or \"next\" in c or \"fy\" in c) and (\"fte\" in c or \"headcount\" in c or c.startswith(\"planned\"))\n\n    def looks_unit(col):\n        c = norm(col)\n        keys = [\"unit\", \"team\", \"supervisor\", \"manager\", \"report\", \"section\", \"line\", \"department\"]\n        return any(k in c for k in keys)\n\n    best_score = 0.0\n    fb_notes = []\n\n    for res in outputs:\n        if not getattr(res, 'is_spreadsheet', False):\n            continue\n        try:\n            path = context.files.get_path(res.id)\n            xlf = pd.ExcelFile(path)\n        except Exception as e:\n            fb_notes.append(f\"Failed to open spreadsheet: {e}\")\n            continue\n\n        for sheet in xlf.sheet_names:\n            try:\n                df = pd.read_excel(path, sheet_name=sheet)\n            except Exception as e:\n                fb_notes.append(f\"Read error {sheet}: {e}\")\n                continue\n            if df is None or df.empty or df.shape[1] < 2:\n                continue\n\n            cols = list(df.columns)\n            unit_cols = [c for c in cols if looks_unit(c)]\n            if not unit_cols:\n                continue\n            current_candidates = [c for c in cols if looks_current(c)]\n            planned_candidates = [c for c in cols if looks_planned(c)]\n            if not current_candidates or not planned_candidates:\n                continue\n            current_col = current_candidates[0]\n            planned_col = planned_candidates[0]\n\n            # Aggregate rows where unit/supervisor looks like Regional Support Services (flexible)\n            region_mask = None\n            for ucol in unit_cols:\n                colvals = df[ucol].astype(str).str.lower()\n                mask = colvals.str.contains(\"regional\") & colvals.str.contains(\"support\")\n                if region_mask is None:\n                    region_mask = mask\n                else:\n                    region_mask = region_mask | mask\n\n            if region_mask is None or not region_mask.any():\n                continue\n\n            cur = pd.to_numeric(df.loc[region_mask, current_col], errors='coerce').fillna(0)\n            pla = pd.to_numeric(df.loc[region_mask, planned_col], errors='coerce').fillna(0)\n            total_current = float(cur.sum())\n            total_planned = float(pla.sum())\n            if total_current <= 0:\n                fb_notes.append(f\"Sheet '{sheet}': Regional group has non-positive total current FTE.\")\n                continue\n\n            reduction = (total_current - total_planned) / total_current\n            # Full credit if reduction within 8%-12%, partial if within 5%-15%\n            if 0.08 <= reduction <= 0.12:\n                score = WEIGHT\n            elif 0.05 <= reduction <= 0.15:\n                score = WEIGHT * 0.6\n            elif reduction > 0:\n                score = WEIGHT * 0.3\n            else:\n                score = 0.0\n            fb_notes.append(f\"Sheet '{sheet}': Regional reduction={reduction:.2%} (current={total_current:.2f}, planned={total_planned:.2f}).\")\n            best_score = max(best_score, score)\n\n    if best_score == 0.0:\n        fb = \"Could not locate Regional Support Services group with recognizable current/planned FTEs.\"\n    else:\n        fb = \" | \".join(fb_notes)[:800]\n\n    return best_score, fb"}, {"type": "llm_judge", "name": "Attrition and vacancies correctly applied by position/line", "description": "Verify the plan reflects the specified attrition/vacancy reductions across the correct reporting lines.", "weight": 2.5, "judge_prompt": "Using the Excel FTE report and the briefing note together, check that the following attrition- and vacancy-based reductions are explicitly captured and reflected in the planned FTE numbers:\n\n- Under Central Services Supervisor: 3 Data Clerk positions are planned retirements/leaves (treated as reductions).\n- Under IT Support Services Supervisor: 1 Clerk II and 1 Data Clerk are going on leave, and 1 additional Data Clerk position is vacant and not to be backfilled (total 3 positions affected under IT Support).\n- Under Policy Supervisor: 1 Facilitator resignation (reduced).\n\nWhat to check:\n1) The briefing note or Excel (or both) explicitly identify these positions and lines of reporting as the basis for reductions (by title and supervisor/unit).\n2) The Excel planned FTE counts for these positions show reductions consistent with the described events (e.g., planned FTE lower than current for those titles/lines).\n3) The logic (leaves/vacancy/resignation) is described as occurring before end of current fiscal year.\n\nScoring:\n- 2.5: All three reporting lines and positions are clearly identified, and Excel planned counts reflect these reductions.\n- 1.2: Two of the three lines/position sets are correctly reflected, or all three are mentioned but one is not reflected in planned counts.\n- 0.5: Only one element is correctly captured (either narrative or counts) with obvious gaps.\n- 0.0: The attrition/vacancy elements are not captured.\n\nPrefer the Excel evidence for numeric reflection; use the briefing note for narrative confirmation and timing.", "expectation": "Excel and briefing note together show the specified attrition/vacancy reductions tied to the correct supervisors and titles."}, {"type": "llm_judge", "name": "Org Chart highlights match Excel reductions", "description": "Check that visually highlighted reductions in the org chart correspond to the positions and counts reduced in the Excel plan.", "weight": 2.5, "judge_prompt": "Compare the revised org chart PDF with the Excel FTE report:\n- Do highlighted/marked-for-reduction positions in the org chart correspond to positions that have lower planned FTE in the Excel?\n- Are central and regional changes consistent (e.g., regional layer shows aggregate reduction consistent with the 10% expectation)?\n- Are there any highlighted reductions in the chart that are not reflected in Excel (or vice versa)?\n\nScoring:\n- 2.5: Strong cross-consistency \u2014 highlights align with position-level reductions in Excel and regional reduction is broadly consistent with the ~10% target.\n- 1.2: Mostly consistent with minor discrepancies (e.g., 1-2 positions or unclear highlighting).\n- 0.5: Partial alignment but noticeable inconsistencies.\n- 0.0: No apparent alignment between the chart and Excel.\n\nFocus on cross-file consistency of what is reduced, not the artistic quality of the chart.", "expectation": "Org chart highlights match position-level reductions reported in Excel, including the regional slice."}, {"type": "llm_judge", "name": "Budget Planning Principles explicitly used (incl. Principle #7)", "description": "Verify that the briefing note explicitly references the Budget Planning Principles and provides specific alignment, emphasizing Principle #7.", "weight": 2.0, "judge_prompt": "In the briefing note (DOCX), verify:\n- There is an explicit section aligning the proposed reductions with the provided Budget Planning Principles.\n- Principle #7 is explicitly named or referenced (e.g., \u201cPrinciple #7\u201d, \u201cPrinciple Seven\u201d).\n- The narrative explains how the approach (e.g., using attrition, maintaining service continuity, minimizing risk) aligns to these principles.\n\nScoring:\n- 2.0: Explicit reference to the principles and to Principle #7 with a clear explanation of alignment.\n- 1.0: General principles mentioned but no clear or specific mention of Principle #7, or explanation is cursory.\n- 0.0: No visible reference to the principles.\n\nJudge alignment quality at a high level; detailed quality will be assessed in Stage 3.", "expectation": "A clear alignment section mentioning Principle #7 and how the plan adheres to it."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism", "description": "Holistic assessment of presentation quality, clarity, and strategic value for leadership decision-making.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional packaging and readability", "description": "Assess file naming clarity, document professionalism, and readability across deliverables.", "weight": 1.0, "judge_prompt": "Evaluate overall professionalism:\n- Clear, descriptive file names (e.g., include Branch and date/version) and coherent packaging.\n- Legible layouts: org chart readable at typical zoom; Excel has clear headers/freeze panes/number formats; briefing note with clean headings.\n- Minimal typos, consistent terminology (e.g., FTE, position titles).\n\nScoring:\n- 1.0: Very professional and easy to navigate.\n- 0.5: Generally acceptable with minor issues.\n- 0.0: Sloppy or confusing.", "expectation": "Clean, readable, professionally packaged deliverables."}, {"type": "llm_judge", "name": "Briefing note clarity and usefulness for leaders", "description": "Assess how well the briefing note communicates the rationale, impacts, and decisions needed.", "weight": 1.5, "judge_prompt": "Assess the briefing note for leadership usefulness:\n- Clear problem framing, concise background.\n- Specific, actionable proposed reductions with rationale (attrition-first approach, service continuity).\n- Risks/impacts and mitigations are concrete and proportionate.\n- Timeline/next steps are clear with milestones (budget cycle, HR actions).\n\nScoring:\n- 1.5: Highly clear, actionable, and decision-ready.\n- 0.8: Adequate but missing some specificity.\n- 0.0: Vague or not decision-ready.", "expectation": "A crisp, action-oriented briefing note suitable for senior leadership."}, {"type": "llm_judge", "name": "Strategic coherence and stakeholder readiness", "description": "Evaluate whether the plan is coherent across central and regional operations and anticipates stakeholders (HR, unions, regions).", "weight": 1.5, "judge_prompt": "Considering all deliverables, assess:\n- Coherence of reductions across central vs regional; avoids over-concentration of risk.\n- Consideration of stakeholder processes: HR staffing actions, union notices, change management, communications.\n- Alignment to government budget cycle timing and approvals.\n\nScoring:\n- 1.5: Strong coherence and stakeholder readiness.\n- 0.8: Partial consideration; some gaps.\n- 0.0: Little to no consideration.", "expectation": "A coherent plan aligned to operational realities and stakeholder processes."}, {"type": "llm_judge", "name": "Visual clarity of revised org chart", "description": "Assess whether the org chart clearly communicates changes.", "weight": 1.0, "judge_prompt": "Judge the clarity of the org chart:\n- Changes are clearly indicated (legend/notes, color/outline differences).\n- Reporting lines remain clear after reductions; no ambiguous orphan boxes.\n- Titles and groupings are legible and consistent with the Excel nomenclature.\n\nScoring:\n- 1.0: Very clear and easy to interpret.\n- 0.5: Mostly clear with minor issues.\n- 0.0: Hard to interpret or misleading.", "expectation": "An org chart where reductions and structure are immediately understandable."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "38889c3b-e3d4-49c8-816a-3cc8e5313aba", "rubric": {"category_name": "Music Production: Instrumental + Stems Delivery (Info Sector \u2022 Audio/Video Tech)", "rationale": "Self-documenting rubric tailored for a music-producer deliverable. Stage 1 forces a verifiable Delivery Manifest document (PDF/DOCX) that enumerates the ZIP package contents and embeds visual proofs (tempo/keys/section markers). Stage 2 mixes deterministic code checks on the ZIP/WAV specs and LLM cross-checks against the Manifest and proof images. Stage 3 assesses professional readiness and suitability for a recording session. Code rules are kept lightweight versus LLM judges per guideline.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2013 Delivery Manifest Structure Gate (LLM only)", "description": "Gate: The primary output must be a professionally formatted Delivery Checklist & Session Manifest (PDF or DOCX) that makes verification trivial. This enables code checks in Stage 2 against the ZIP audio package.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Delivery Manifest Format & Sections", "description": "Verify the primary output is a PDF/DOCX Delivery Checklist & Session Manifest with all required sections and embedded proof images. This is a structure/format check only.", "weight": 4.0, "judge_prompt": "You are checking ONLY the structure and presence of elements in the primary output document (PDF/DOCX). Do not assess the quality of the music or correctness of claims yet.\n\nConfirm the following for the primary document:\n\nFormat requirements:\n- The primary output is a PDF or DOCX (not XLS/CSV/MD/ZIP).\n- At least 2 pages.\n- Professional layout with clear section headers.\n\nRequired sections (flexible naming allowed):\n1) Project Overview: Must clearly state song title 'Deja Vu', 140 BPM, main key G Major, and bridge key Ab Major.\n2) Section/Arrangement Map: A timeline map listing section labels and timestamps, specifically including the bridge from 1:22 to 1:49.\n3) File Inventory (ZIP Package Index): A table listing the ZIP filename and internal files including:\n   - One Master Track WAV\n   - Four stem WAVs: Guitars, Synths, Bass, Bridge\n   - For each file: duration, sample rate (48 kHz), bit depth (24-bit float or equivalent), and channel count.\n4) Instrumentation & Sound Design Notes: Brief description of guitars, synths, bass; references to bossa-influenced groove and modern/punchy feel; mention of any synth models (e.g., DX7, Prophet 5, ARP 2600, MiniMoog) and time-based effects used.\n5) Drum Sync Statement: An explicit note that all instrumentation is tightly synchronized to the provided 'DRUM REFERENCE TRACK.WAV'.\n6) Sampling & Rights Statement: A short compliance note referencing the Tracklib sampling guide URL: https://www.tracklib.com/blog/music-sampling-guide\n7) Embedded Proof Images (screenshots from DAW or tools):\n   - Tempo proof showing 140 BPM\n   - Section markers showing the bridge window 1:22\u20131:49\n   - Key signature or MIDI/piano-roll/key map evidencing G Major main sections and Ab Major bridge\n\nScoring (structure only):\n- 4.0: Valid PDF/DOCX, >=2 pages, and all 7 required elements are present and clear\n- 3.0: Valid PDF/DOCX, >=2 pages, and only one minor element missing (e.g., one proof image) while the rest are present\n- 2.0: Valid PDF/DOCX but 2\u20133 required elements missing OR weakly labeled such that inventory/sections are unclear\n- 0.0: Not PDF/DOCX, <2 pages, or missing multiple core sections (e.g., no inventory or no section map)\n\nOnly check presence/structure; do not judge correctness or audio quality.", "expectation": "A well-structured Manifest in PDF/DOCX with clear sections, file inventory, and embedded proof images that enable verification."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification (Code + LLM)", "description": "Now verify the deliverable: confirm ZIP contents and WAV technical specs via code; cross-check Manifest claims, proofs, and alignment with task requirements via LLM.", "is_required": true, "max_points": 10.0, "min_score_to_pass": 6.0, "rules": [{"type": "code", "name": "ZIP Structure and Required WAV Files Present", "description": "Check there is a ZIP output and it contains exactly one master WAV and at least the four required stems (Guitars, Synths, Bass, Bridge). Flexible naming allowed (case-insensitive substring match).", "weight": 0.8, "code": "import re, json, pandas as pd, numpy as np\nimport zipfile, io\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    # Find the ZIP among all outputs\n    zip_res = None\n    for res in context.get_all_outputs():\n        path = context.files.get_path(res.id)\n        if str(path).lower().endswith('.zip'):\n            zip_res = res\n            break\n    if zip_res is None:\n        return 0.0, \"No ZIP file found among outputs.\"\n\n    try:\n        zip_path = context.files.get_path(zip_res.id)\n        with zipfile.ZipFile(zip_path, 'r') as zf:\n            names = [n for n in zf.namelist() if not n.endswith('/')]\n            wavs = [n for n in names if n.lower().endswith('.wav')]\n            if not wavs:\n                return 0.0, \"ZIP contains no WAV files.\"\n            lower_wavs = [w.lower() for w in wavs]\n            # Master identification: filename contains 'master'\n            master_files = [w for w in lower_wavs if 'master' in w]\n            # Stem identification by keyword\n            def find_by_kw(kw):\n                return [w for w in lower_wavs if kw in w]\n            guitars = find_by_kw('guitar')\n            synths = find_by_kw('synth')\n            bass = find_by_kw('bass')\n            bridge = find_by_kw('bridge')\n\n            score = 0.0\n            feedback = []\n\n            if len(master_files) >= 1:\n                score += 0.3\n            else:\n                feedback.append(\"No master WAV identified (filename containing 'master').\")\n\n            present = 0\n            present += 1 if guitars else 0\n            present += 1 if synths else 0\n            present += 1 if bass else 0\n            present += 1 if bridge else 0\n            # Each stem present worth 0.1 up to 0.4\n            score += 0.1 * present\n            if present < 4:\n                missing = []\n                if not guitars: missing.append('Guitars')\n                if not synths: missing.append('Synths')\n                if not bass: missing.append('Bass')\n                if not bridge: missing.append('Bridge')\n                feedback.append(\"Missing stems: \" + \", \".join(missing))\n\n            # Bonus up to max weight if there are only one master and at least 4 total WAVs\n            if score > 0.8:\n                score = 0.8\n            return score, \"; \".join(feedback) if feedback else \"ZIP contains master and required stems.\"\n    except Exception as e:\n        return 0.0, f\"Error reading ZIP: {e}\""}, {"type": "code", "name": "WAV Technical Specs and Durations", "description": "Validate WAVs are 48 kHz and 24-bit (3 bytes/sample) or float (commonly 32-bit, 4 bytes/sample), master duration \u2248 2:17, and stems align in length with master.", "weight": 0.8, "code": "import re, json, pandas as pd, numpy as np\nimport zipfile, io, wave, contextlib\n\ndef evaluate(workflow, context):\n    # Locate ZIP\n    zip_res = None\n    for res in context.get_all_outputs():\n        if str(context.files.get_path(res.id)).lower().endswith('.zip'):\n            zip_res = res\n            break\n    if zip_res is None:\n        return 0.0, \"No ZIP found.\"\n\n    try:\n        zip_path = context.files.get_path(zip_res.id)\n        with zipfile.ZipFile(zip_path, 'r') as zf:\n            wav_names = [n for n in zf.namelist() if n.lower().endswith('.wav')]\n            if not wav_names:\n                return 0.0, \"No WAVs in ZIP.\"\n\n            # Identify master and stems\n            lower_map = {n.lower(): n for n in wav_names}\n            masters = [n for n in lower_map if 'master' in n]\n            if not masters:\n                return 0.0, \"No master WAV found for spec checks.\"\n            master_name = lower_map[masters[0]]\n\n            def read_wav_info(name):\n                data = zf.read(name)\n                bio = io.BytesIO(data)\n                with contextlib.closing(wave.open(bio, 'rb')) as wf:\n                    sr = wf.getframerate()\n                    sw = wf.getsampwidth()\n                    ch = wf.getnchannels()\n                    nf = wf.getnframes()\n                    dur = nf / float(sr) if sr else 0.0\n                return dict(name=name, sr=sr, sw=sw, ch=ch, nf=nf, dur=dur)\n\n            infos = []\n            for n in wav_names:\n                try:\n                    infos.append(read_wav_info(n))\n                except Exception as e:\n                    # If any wav cannot be read, count as failure for that file\n                    infos.append(dict(name=n, sr=0, sw=0, ch=0, nf=0, dur=0.0, err=str(e)))\n\n            # Compute scores\n            score = 0.0\n            fb = []\n\n            # Sample rate: require all to be 48000\n            all_sr_ok = all(i.get('sr') == 48000 for i in infos if i.get('sr'))\n            if all_sr_ok:\n                score += 0.3\n            else:\n                fb.append(\"Not all WAVs are 48 kHz.\")\n\n            # Bit depth: accept 24-bit PCM (3 bytes) or 32-bit float (4 bytes) across files\n            all_width_ok = all(i.get('sw') in (3, 4) for i in infos if i.get('sw') is not None)\n            if all_width_ok:\n                score += 0.2\n            else:\n                fb.append(\"Some WAVs are not 24-bit PCM (3 bytes) or 32-bit float (4 bytes).\")\n\n            # Duration target: master around 137s (2:17)\n            master_info = None\n            for i in infos:\n                if i['name'] == master_name:\n                    master_info = i\n                    break\n            if master_info and master_info['dur'] > 0:\n                md = master_info['dur']\n                # Full credit if within \u00b13s, partial if within \u00b17s\n                if abs(md - 137.0) <= 3.0:\n                    score += 0.2\n                elif abs(md - 137.0) <= 7.0:\n                    score += 0.1\n                else:\n                    fb.append(f\"Master duration off target (actual ~{md:.1f}s).\")\n            else:\n                fb.append(\"Master duration unreadable.\")\n\n            # Alignment: stems approximately same length as master (within 1% frames)\n            if master_info and master_info['nf'] > 0:\n                nm = master_info['nf']\n                mismatch = []\n                for i in infos:\n                    if i['name'] == master_name:\n                        continue\n                    nf = i.get('nf', 0)\n                    if nm == 0 or nf == 0:\n                        mismatch.append(i['name'])\n                        continue\n                    if abs(nf - nm) / float(nm) > 0.01:\n                        mismatch.append(i['name'])\n                if not mismatch:\n                    score += 0.1\n                else:\n                    fb.append(\"Stems not aligned in length to master: \" + \", \".join(mismatch))\n\n            # Cap at weight\n            if score > 0.8:\n                score = 0.8\n            return score, \"; \".join(fb) if fb else \"All specs OK (48k, 24/32-bit, duration, alignment).\"\n    except Exception as e:\n        return 0.0, f\"Error reading WAV specs: {e}\""}, {"type": "code", "name": "Manifest Metadata Consistency", "description": "Parse the primary PDF/DOCX text and verify presence of critical metadata: BPM=140, G Major main, Ab Major bridge, bridge 1:22\u20131:49, drum reference sync mention, 48k/24-bit float claim, synth models, Tracklib link.", "weight": 0.4, "code": "import re, json, pandas as pd, numpy as np\n\ndef evaluate(workflow, context):\n    # Read primary document text (PDF or DOCX or MD)\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No primary output.\"\n\n    text = \"\"\n    try:\n        if output.is_document:\n            # Try PDF first, then DOCX\n            path = context.files.get_path(output.id)\n            p = str(path).lower()\n            if p.endswith('.pdf'):\n                text = context.files.read_pdf_text(output.id) or \"\"\n            elif p.endswith('.docx'):\n                text = context.files.read_docx_text(output.id) or \"\"\n            else:\n                # If not PDF/DOCX, try generic\n                text = context.files.read_text(output.id) or \"\"\n        else:\n            text = context.files.read_text(output.id) or \"\"\n    except Exception:\n        text = \"\"\n\n    t = text.lower()\n    score = 0.0\n    fb = []\n\n    # BPM 140\n    if ('140' in t and 'bpm' in t) or re.search(r'\\b140\\s*bpm\\b', t):\n        score += 0.08\n    else:\n        fb.append('Missing explicit 140 BPM.')\n\n    # Keys\n    if 'g major' in t:\n        score += 0.08\n    else:\n        fb.append('Missing G Major (main sections).')\n    if ('ab major' in t) or ('a\u266d major' in t) or ('a-flat major' in t) or ('a flat major' in t):\n        score += 0.08\n    else:\n        fb.append('Missing Ab Major (bridge).')\n\n    # Bridge window timings\n    if ('1:22' in t and '1:49' in t) or re.search(r'1\\s*:\\s*22.*1\\s*:\\s*49', t, re.DOTALL):\n        score += 0.08\n    else:\n        fb.append('Missing bridge timing 1:22\u20131:49.')\n\n    # Drum reference sync mention\n    if 'drum reference' in t or 'drum track' in t or 'drum reference track.wav' in t:\n        score += 0.04\n    else:\n        fb.append('No explicit drum reference sync statement.')\n\n    # Technical spec claim 48 kHz and 24-bit float (accept variants)\n    sr_ok = ('48 khz' in t) or ('48khz' in t) or ('48000' in t)\n    bd_ok = ('24-bit' in t) or ('24 bit' in t)\n    fl_ok = 'float' in t\n    if sr_ok and bd_ok and fl_ok:\n        score += 0.04\n    else:\n        fb.append('Missing explicit 48 kHz / 24-bit float export claim.')\n\n    # Synth models mention (any of these)\n    synth_tokens = ['dx7','prophet 5','prophet-5','arp 2600','arp2600','minimoog','mini moog']\n    if any(tok in t for tok in synth_tokens):\n        score += 0.06\n    else:\n        fb.append('No mention of synth model choices (DX7/Prophet 5/ARP 2600/MiniMoog).')\n\n    # Tracklib link mention\n    if 'tracklib.com' in t and 'music-sampling-guide' in t:\n        score += 0.04\n    else:\n        fb.append('Missing Tracklib sampling guide reference/link.')\n\n    if score > 0.4:\n        score = 0.4\n    return score, \"; \".join(fb) if fb else 'Manifest metadata present.'"}, {"type": "llm_judge", "name": "Proof Artifacts Validate Tempo/Keys/Sections", "description": "Using the Manifest\u2019s embedded images, visually confirm there is evidence for 140 BPM, correct bridge timing, and key signatures for main/bridge.", "weight": 3.0, "judge_prompt": "Open the primary PDF/DOCX and examine embedded images/screenshots. Based on what you SEE:\n- Is there a Tempo proof image clearly indicating 140 BPM?\n- Is there a Section/Arrangement screenshot showing a bridge region from 1:22 to 1:49?\n- Is there visible evidence (piano roll, key signature annotations, MIDI or analysis view) of G Major for main sections and Ab Major for the bridge?\n\nScoring:\n- 3.0: All three visual proofs are clearly present and correctly labeled (140 BPM; bridge 1:22\u20131:49; keys G Major main and Ab Major bridge)\n- 2.0: Two of the three visual proofs clearly present\n- 1.0: Only one proof present or multiple are ambiguous\n- 0.0: No usable visual proofs\n\nJudge only visual evidence in the document; do not rely on text claims alone.", "expectation": "Clear, legible screenshots that explicitly show 140 BPM, the bridge time window, and key signatures as claimed."}, {"type": "llm_judge", "name": "Musical Intent and Instrumentation Compliance (Evidence-Based)", "description": "Check the Manifest\u2019s narrative and any labeled figures for evidence that the arrangement aligns with the requested style and instrumentation.", "weight": 3.0, "judge_prompt": "Using the Manifest text and labeled figures (not listening to audio), check for evidence that:\n- The groove is described as uptempo, bright, tightly looped, bossa-influenced, with a crisp, modern, punchy beat.\n- Instrumentation includes Guitars, Synths, and Bass (explicitly noted) and a Bridge section is arranged/described.\n- Time-based effects usage is described (e.g., delay, reverb, chorus, stereo widening) to create a driving feel.\n- Mentions of synth models (e.g., DX7, Prophet 5, ARP 2600, MiniMoog) are included as inspirations or sources.\n\nScoring:\n- 3.0: Strong evidence on all bullets (clear, specific descriptions)\n- 2.0: Most bullets covered with reasonable specificity\n- 1.0: Only minimal or vague coverage of style/instrumentation/effects\n- 0.0: No substantive evidence of requested musical intent", "expectation": "Concise but specific descriptions that show the producer designed for the requested style and instruments."}, {"type": "llm_judge", "name": "Recording Session Readiness", "description": "Assess whether the deliverable appears ready for a recording engineer: stems clearly labeled, shared technical specs, alignment instructions, and sampling compliance statement.", "weight": 2.0, "judge_prompt": "From the Manifest, judge the handoff readiness for a recording session:\n- Are stems clearly labeled (Guitars, Synths, Bass, Bridge) and described as starting at the same time and same length as the master?\n- Are export specs stated (48 kHz, 24-bit float), and does the File Inventory include durations/channels per file?\n- Is there an explicit drum sync/alignment statement referencing the provided DRUM REFERENCE TRACK.WAV?\n- Is there a sampling/copyright compliance note referencing the Tracklib guide URL?\n\nScoring:\n- 2.0: All four readiness elements clearly documented\n- 1.0: Two\u2013three elements documented\n- 0.0: Zero\u2013one elements documented", "expectation": "A professional handoff with unambiguous file labels, specs, alignment notes, and sampling compliance."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Quality and Professionalism (LLM)", "description": "Holistic assessment of the handoff quality, clarity, and professional standards as evidenced in the Manifest and organization of deliverables.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Organization and Clarity of Handoff", "description": "Evaluate the overall organization, labeling, and clarity of instructions for the recording engineer.", "weight": 2.0, "judge_prompt": "Assess the overall organization and clarity:\n- Are sections logically ordered with clear headers and navigable structure?\n- Is the File Inventory easy to read (tables, consistent naming, durations/specs visible)?\n- Are any additional notes (e.g., count-in, head/tail silence, click reference) specified to ease the session?\n\nScoring:\n- 2.0: Exceptionally organized and clear\n- 1.0: Generally clear with minor issues\n- 0.0: Disorganized or confusing handoff", "expectation": "Clean, consistent formatting with a clear, concise file inventory and actionable notes."}, {"type": "llm_judge", "name": "Production Rationale and Creative Cohesion", "description": "Judge the quality of the production notes: do they present a coherent creative intent aligned with the brief?", "weight": 2.0, "judge_prompt": "Evaluate the Production Notes/Instrumentation sections:\n- Do they explain how the arrangement achieves an uptempo, bright, bossa-influenced groove with a modern punchy feel?\n- Are choices for guitars, synths, bass, and time-based effects coherently justified (e.g., why DX7 plucks vs Prophet pads; bass sound design intentions; guitar rhythmic patterns)?\n- Is there a clear plan for dynamics and transitions into/out of the 1:22\u20131:49 bridge in Ab Major?\n\nScoring:\n- 2.0: Strong, coherent rationale tied to the brief\n- 1.0: Partial rationale, somewhat generic\n- 0.0: Weak or missing rationale", "expectation": "A concise yet thoughtful rationale tying sound choices and structure to the artist brief."}, {"type": "llm_judge", "name": "Technical Professionalism", "description": "Judge professional presentation: technical specs completeness, terminology accuracy, and risk mitigation notes.", "weight": 2.0, "judge_prompt": "Assess technical professionalism:\n- Are specs (sample rate, bit depth, channels) consistently stated and accurate across the document?\n- Are potential risks (e.g., clipping/headroom, latency compensation, phase in multi-mic guitars) anticipated with brief mitigations?\n- Are references/citations (e.g., Tracklib link) properly formatted?\n\nScoring:\n- 2.0: Highly professional and precise\n- 1.0: Mostly professional with minor inconsistencies\n- 0.0: Lacks professional rigor", "expectation": "Consistent specs, correct terminology, brief but useful risk notes, and clean references."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "2fa8e956-7b35-4c13-95dc-027f02be318b", "rubric": {"category_name": "Real Estate & Rental \u2013 Concierge Deliverable: Napa Valley Winery List (Shareable DOCX)", "rationale": "Pattern B (Document). We enforce a strict, self-documenting Word/PDF structure that makes verification trivial: a concise, well-formatted, image-enhanced recommendations document tailored to an ultra-high-net-worth member. Stage 1 (LLM-only) mandates the exact document shape, page limit, footer, and fielded winery entries. Stage 2 mixes light code checks (field presence and bounds) with LLM verification (tasting availability, proximity, sources). Stage 3 evaluates professional quality, layout, and actionability.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2013 Format & Structure Gate (LLM-only)", "description": "Gate: Enforce exact output shape for a shareable, concise, fielded DOCX/PDF deliverable with footer, photo, and per-winery required fields.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.2, "rules": [{"type": "llm_judge", "name": "Document Format, Page Limit, and Title Block", "description": "Checks the file is a professional document (prefer DOCX), has 1\u20134 pages, a prominent title, and an intro sentence/paragraph.", "weight": 1.0, "judge_prompt": "You are evaluating a concierge deliverable for a UHNW member. Inspect the candidate output directly (rendered). Determine if:\n\n- File format is DOCX (preferred) or PDF (acceptable). Not plain text/spreadsheet.\n- Length: no more than 4 pages; at least 1 page.\n- A clear, prominent title present on page 1 (e.g., \u201cNapa Valley Wineries\u201d or similar).\n- A short introduction (1\u20133 sentences) explaining the list\u2019s purpose and criteria (within one-hour drive; tastings; variety of grape types).\n\nScoring:\n- 1.0: DOCX or PDF, 1\u20134 pages, clear title on p1, intro present.\n- 0.7: PDF (not DOCX) but otherwise fully compliant.\n- 0.4: Valid format and page limit, but missing either title or intro.\n- 0.0: Wrong format (not DOCX/PDF) OR exceeds 4 pages OR missing both title and intro.\n\nOnly check presence/format, not content accuracy.", "expectation": "A 1\u20134 page DOCX with a clear title and brief intro."}, {"type": "llm_judge", "name": "Footer and Winery Entry Structure", "description": "Confirms required footer text and that each winery entry contains all mandated fields in a consistent, scannable layout.", "weight": 1.2, "judge_prompt": "Check that the document includes:\n\n- Footer: present on pages; must contain the exact footer text \u201cNapa Valley Wineries\u201d. Font requirement will be checked in a different rule, but ensure the text and placement as footer, not body.\n- Winery Recommendations section: a structured list or table with one entry per winery.\n- For each winery entry, the following fields must be clearly identifiable (via labels or a consistent field order):\n  1) Name\n  2) Grape varieties offered\n  3) 1\u20132 sentence description\n  4) Visiting hours\n  5) Address\n  6) Phone number\n  7) Distance from The Westin Verasa Napa (in miles)\n  8) Estimated drive time (in minutes)\n\nBe flexible on exact label wording (e.g., \u201cVarietals\u201d vs \u201cGrape Varieties\u201d, \u201cHours\u201d vs \u201cVisiting Hours\u201d), but the structure must be obvious and repeatable per entry.\n\nScoring:\n- 1.2: Footer with exact text is present as a footer; winery entries are consistently structured and all 8 fields are present for the majority of entries.\n- 0.8: Footer present but not perfectly in footer area OR 1 missing field across most entries.\n- 0.4: Footer missing OR multiple required fields missing across entries.\n- 0.0: Footer missing AND winery entries are not structured (e.g., narrative only).", "expectation": "Footer titled \u201cNapa Valley Wineries\u201d and a consistent, labeled block or table per winery with all fields."}, {"type": "llm_judge", "name": "Image and Typography Compliance (Visual)", "description": "Checks presence of a relevant royalty-free Napa vineyards photo and required typography/color usage.", "weight": 0.8, "judge_prompt": "Visually verify:\n\n- A relevant royalty-free photo of Napa Valley vineyards is included (not a logo or unrelated image). Preferably with small attribution/citation if included.\n- Typography:\n  \u2022 Black body text in Georgia at approx. 9 pt.\n  \u2022 Grape varieties rendered in purple text, Georgia ~9 pt.\n  \u2022 Footer text \u201cNapa Valley Wineries\u201d in Georgia ~14 pt.\n\nBe practical: font family and sizes should be clearly close to the spec; minor deviations are acceptable only if Georgia-like serif is obviously used and the visual size hierarchy is clear.\n\nScoring:\n- 0.8: Vineyard photo present + body text appears Georgia ~9 pt black + grape varieties clearly in purple Georgia ~9 pt + footer appears Georgia ~14 pt.\n- 0.5: Photo present + most typography correct but one element (e.g., purple varieties or footer size) is off.\n- 0.2: Photo present but typography largely non-compliant.\n- 0.0: No relevant photo OR typography clearly non-compliant across the board.", "expectation": "A Napa vineyards image and visible Georgia font styling; purple varieties; larger Georgia footer."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness Verification (Mixed)", "description": "Verify factual and structural correctness enabled by Stage 1 shape: required fields present, plausible distances/times (\u22641 hour), tasting availability, variety of grapes, sources, and hotel anchor.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Field Coverage by Entry (Labels/Patterns)", "description": "Programmatically checks for presence of key fields across entries using robust pattern matching on the document text.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float score in [0, 0.4] with feedback\n    \"\"\"\n    weight = 0.4\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n\n    # Read text from DOCX or PDF\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text:\n        return 0.0, \"Unable to extract text\"\n\n    txt = text.lower()\n\n    # Estimate number of entries by phone count (most robust signal)\n    phone_re = re.compile(r\"(?:\\+1\\s*)?(?:\\(\\d{3}\\)|\\d{3})[\\s\\.-]?\\d{3}[\\s\\.-]?\\d{4}\")\n    phones = phone_re.findall(text)\n    entry_est = len(phones)\n\n    # Fallback: if no phones, estimate by 'address' labels or 'distance' tokens\n    if entry_est == 0:\n        dist_tokens = re.findall(r\"\\b\\d{1,2}(?:\\.\\d+)?\\s?(?:mi|miles)\\b\", txt)\n        entry_est = max(entry_est, len(dist_tokens))\n    if entry_est == 0:\n        # If still zero, try counting 'winery' occurrences\n        entry_est = txt.count('winery')\n\n    if entry_est == 0:\n        return 0.0, \"Could not identify any winery entries\"\n\n    # Field presence signals\n    dist_count = len(re.findall(r\"\\b\\d{1,2}(?:\\.\\d+)?\\s?(?:mi|miles)\\b\", txt))\n    time_count = len(re.findall(r\"\\b\\d{1,3}\\s?(?:min|mins|minutes)\\b\", txt))\n    hours_count = sum([\n        txt.count('hours'), txt.count('visiting hours'), txt.count('open '), txt.count('opening hours')\n    ])\n    address_count = len(re.findall(r\"\\bCA\\s?\\d{5}\\b\", text))  # CA ZIPs\n    grape_count = sum([txt.count('grape'), txt.count('varietal'), txt.count('varieties')])\n\n    # Normalize counts to entries (cap at entry_est)\n    def coverage(cnt):\n        return min(cnt, entry_est) / max(1, entry_est)\n\n    cov_dist = coverage(dist_count)\n    cov_time = coverage(time_count)\n    cov_hours = coverage(hours_count)\n    cov_addr = coverage(address_count)\n    cov_grape = 1.0 if grape_count >= entry_est else coverage(grape_count)\n\n    # Average coverage across key fields\n    avg_cov = (cov_dist + cov_time + cov_hours + cov_addr + cov_grape) / 5.0\n\n    return weight * avg_cov, f\"Entries\u2248{entry_est}; coverage(dist={cov_dist:.2f}, time={cov_time:.2f}, hours={cov_hours:.2f}, addr={cov_addr:.2f}, grape={cov_grape:.2f})\""}, {"type": "code", "name": "Bounds Check: Distance and Drive Time \u2264 60", "description": "Checks that most listed distances and drive times are within a one-hour drive window.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Returns a score in [0, 0.4] based on fraction of distances <= 60 miles and drive times <= 60 minutes.\n    \"\"\"\n    weight = 0.4\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n\n    # Extract text\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text:\n        return 0.0, \"Unable to extract text\"\n\n    txt = text.lower()\n\n    # Distances in miles\n    dist_vals = [float(m) for m in re.findall(r\"\\b(\\d{1,2}(?:\\.\\d+)?)\\s?(?:mi|miles)\\b\", txt)]\n    # Drive times in minutes\n    time_vals = [int(m) for m in re.findall(r\"\\b(\\d{1,3})\\s?(?:min|mins|minutes)\\b\", txt)]\n\n    if not dist_vals and not time_vals:\n        return 0.0, \"No distances or times found\"\n\n    ok_dist = sum(1 for v in dist_vals if v <= 60)\n    ok_time = sum(1 for v in time_vals if v <= 60)\n\n    frac_dist = ok_dist / len(dist_vals) if dist_vals else 0.0\n    frac_time = ok_time / len(time_vals) if time_vals else 0.0\n\n    # Combine (average). If only one type present, use that fraction.\n    if dist_vals and time_vals:\n        combined = (frac_dist + frac_time) / 2.0\n    elif dist_vals:\n        combined = frac_dist\n    else:\n        combined = frac_time\n\n    return weight * max(0.0, min(1.0, combined)), f\"Within-bounds: dist={frac_dist:.2f}, time={frac_time:.2f}\""}, {"type": "llm_judge", "name": "Tasting Availability and Grape Variety Diversity", "description": "Verifies that each winery offers tasting experiences and that grape varieties are meaningfully varied (reds and whites, multiple varietals).", "weight": 1.6, "judge_prompt": "Check the winery entries to confirm:\n- Each selected winery explicitly offers tasting experiences (e.g., mentions \u201ctasting\u201d, \u201ctasting room\u201d, tastings by appointment/reservation).\n- Grape varieties are provided for each, and across the list represent meaningful diversity (reds and whites; multiple varietals, not repeating the same one everywhere).\nScoring:\n- 1.6: All wineries clearly offer tastings, and varieties show strong diversity across entries.\n- 1.0: Minor omissions (1\u20132 entries unclear on tastings) OR diversity is adequate but somewhat repetitive.\n- 0.5: Several entries lack tasting info OR varieties are sparse/monotonous.\n- 0.0: Tastings not established for most entries and varieties missing/very thin.", "expectation": "Every entry states tastings, and the set spans multiple varietals (e.g., Cab, Pinot, Merlot, Chardonnay, Sauvignon Blanc, etc.)."}, {"type": "llm_judge", "name": "Hotel Anchor, Proximity Credibility, and One-Hour Constraint", "description": "Confirms that distances/times are anchored from The Westin Verasa Napa and are plausible within a one-hour radius.", "weight": 1.6, "judge_prompt": "Confirm the document:\n- References The Westin Verasa Napa as the origin for distance/drive time.\n- Lists distances (miles) and drive times (minutes) that are plausible for Napa Valley surroundings and mostly within 60 minutes.\n- Avoids far-away regions clearly beyond an hour in typical conditions.\nScoring:\n- 1.6: Westin Napa explicitly referenced as the starting point; all or nearly all entries are plausibly within an hour.\n- 1.0: Westin is implied but not explicit; or 1\u20132 entries look borderline beyond an hour.\n- 0.5: Several entries seem implausibly distant or origin unclear.\n- 0.0: No anchor to Westin Napa and many entries beyond an hour.", "expectation": "Distances clearly stated from The Westin Verasa Napa; entries constrained to ~\u226460 min."}, {"type": "llm_judge", "name": "Sourcing and Google Maps Methodology Mention", "description": "Validates presence of reputable sources for winery info and that Google Maps is cited for distance/time estimation.", "weight": 1.0, "judge_prompt": "Assess whether the document:\n- Cites reputable sources (e.g., Napa Valley/Visit Napa Valley, SF Chronicle, The Family Travel Guy, or similar) either per entry or in a references section.\n- Mentions Google Maps as the source for distances/drive times.\nScoring:\n- 1.0: Clear sources for winery details AND explicit Google Maps mention for distances/times.\n- 0.6: Sources present but Google Maps not clearly mentioned, or vice versa.\n- 0.3: Vague or minimal sourcing.\n- 0.0: No sources and no Google Maps reference.", "expectation": "Named reputable sources and a note like \u201cDistances/drive times via Google Maps.\u201d"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Holistic Quality Assessment", "description": "Professional presentation, readability, and utility for an ultra-high-net-worth member making decisions quickly.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Readability", "description": "Evaluates tone, clarity, and concision appropriate for UHNW concierge work.", "weight": 0.7, "judge_prompt": "Judge the overall professionalism and readability:\n- Polished, concierge-grade tone; concise yet informative.\n- Clear 1\u20132 sentence descriptions that differentiate wineries.\n- Minimal clutter; headings and spacing aid skimmability.\nScoring: 0.7 excellent; 0.5 good; 0.3 fair; 0.0 poor.", "expectation": "Concise, polished, skimmable descriptions suitable for UHNW clientele."}, {"type": "llm_judge", "name": "Layout and Formatting Consistency", "description": "Assesses visual consistency, alignment, and adherence to the specified color usage for varieties.", "weight": 0.5, "judge_prompt": "Evaluate:\n- Consistent entry layout (labels or table formatting).\n- Logical ordering (e.g., by distance) and strong visual alignment/spacing.\n- Purple used only for grape varieties; other text remains black.\n- The 1\u20134 page length feels balanced and not overcrowded.\nScoring: 0.5 excellent; 0.3 mixed; 0.1 weak; 0.0 poor.", "expectation": "Clean, consistent layout with restrained color usage and balanced density."}, {"type": "llm_judge", "name": "Actionability for Decision-Making", "description": "Checks that the document enables quick comparison and easy next steps.", "weight": 0.5, "judge_prompt": "Assess actionability:\n- Comparison-friendly (consistent fields, sorting, or grouping helpful to decide).\n- Contact details are complete and obvious for booking.\n- Optional but positive: reservation notes, fees, or highlights.\nScoring: 0.5 strong; 0.3 adequate; 0.1 minimal; 0.0 not actionable.", "expectation": "Easy to compare and act: clear fields, ordering, and complete contacts."}, {"type": "llm_judge", "name": "Image Relevance and Attribution Quality", "description": "Rates how well the included photo supports the document and whether attribution (if present) is discrete and clear.", "weight": 0.3, "judge_prompt": "Evaluate the vineyard photo\u2019s contribution:\n- Strong relevance and quality (sharp, on-theme).\n- If attribution/citation is provided, it is concise and unobtrusive.\nScoring: 0.3 strong; 0.2 decent; 0.1 weak; 0.0 irrelevant/absent.", "expectation": "A high-quality vineyard photo with tasteful, optional attribution."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "46bc7238-3501-4839-b989-e2bd47853676", "rubric": {"category_name": "Real Estate Broker \u2013 QSR Tenant Outreach Playbook (Miami, FL)", "rationale": "This rubric enforces a self-documenting, file-based deliverable: a 5\u20138 page PDF/DOCX playbook with mandated sections, visuals, and a flyer template. Stage 1 (LLM-only) strictly gates structure/format so later verification is trivial. Stage 2 mixes light, robust code checks (text presence/coverage) with higher-weight LLM judgment for nuanced correctness. Stage 3 holistically evaluates professionalism, strategy, repeatability, and actionability for a leasing team in Miami targeting QSR tenants.", "max_total_score": 28.0, "stages": [{"name": "Stage 1 \u2013 Shape Enforcement Gate", "description": "LLM-only structural validation of the deliverable\u2019s format, page count, and required sections to ensure verifiable shape.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Structured Playbook Format Requirement", "description": "Verify the candidate produced a 5\u20138 page PDF/DOCX playbook with the exact sections and assets required.", "weight": 8.0, "judge_prompt": "You are evaluating whether the submitted output is a properly structured playbook document with all required components. You can see rendered pages and images.\n\nCheck the following strictly as structural/format requirements (do not judge writing quality yet):\n\nFormat and Length:\n- The file must be a PDF or DOCX (not Excel, not plain text).\n- The document must be between 5 and 8 pages inclusive.\n- Professional, readable layout.\n\nVisuals Requirement:\n- A cover page must include a stock photo of a shopping center.\n- Each page (including the cover) should include at least one free stock photo or relevant retail/QSR visual. Be flexible: images may be stock-like, royalty-free, or clearly illustrative. Reject obviously watermarked or missing images.\n\nRequired Sections (headers can vary; be flexible with synonyms):\n1) Executive Summary & Property Highlights (on early pages)\n2) Overview of target QSR tenant categories (fast casual, coffee/breakfast, pizza, subs, chicken/wings, smoothies/health)\n3) Sample cold call and email scripts tailored specifically to QSR prospects\n4) Outreach cadence and follow-up strategy (multi-channel: email, call, LinkedIn, site visit)\n5) A one-page flyer template example for prospective tenants (property overview, highlights, contact info)\n6) Next Steps\n\nProperty Context Presence (as visible in document):\n- The property: 5,000 SF end cap at 123 Dade County Rd, Miami, FL, shadow-anchored by Publix, strong visibility.\n\nScoring (return a single numeric score out of 8):\n- 8: Valid PDF/DOCX, 5\u20138 pages, cover page with stock photo, images on every page, and all six required sections present; property context appears.\n- 6\u20137: Valid format and length; minor misses only (e.g., one page missing an image OR one required section lightly combined/renamed but clearly present); property context appears.\n- 4\u20135: Valid format but missing multiple required elements (e.g., 1\u20132 sections absent, or several pages lack images); property context only partially present.\n- 1\u20133: Document exists but wrong length, major structural gaps, or visuals largely missing.\n- 0: Not a PDF/DOCX, or fewer than 5 pages, or completely wrong format.\n\nOnly evaluate structure, presence, and format\u2014not content quality or correctness.", "expectation": "A 5\u20138 page PDF/DOCX with cover image, images on each page, all required sections, and property context explicitly referenced."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness Verification", "description": "Mixed verification of content correctness and completeness given the enforced structure. Light code rules cover deterministic text presence; LLM judges assess nuanced QSR-specific tailoring and workflow coherence.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "QSR Category Coverage (Code)", "description": "Verify the playbook text mentions all target QSR categories (fast casual, coffee/breakfast, pizza, subs, chicken/wings, smoothies/health).", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n\n    text = \"\"\n    try:\n        if output.extension.lower() == '.pdf':\n            text = context.files.read_pdf_text(output.id)\n        else:\n            text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = \"\"\n\n    t = text.lower()\n\n    groups = [\n        [\"fast casual\"],\n        [\"coffee\", \"breakfast\"],\n        [\"pizza\"],\n        [\"subs\", \"sub\", \"sandwich\", \"sandwiches\"],\n        [\"chicken\", \"wings\"],\n        [\"smoothie\", \"smoothies\", \"health\", \"healthy\"],\n    ]\n\n    covered = 0\n    for keys in groups:\n        if any(k in t for k in keys):\n            covered += 1\n\n    score = covered / len(groups)\n    feedback = f\"Covered {covered}/{len(groups)} QSR categories.\"\n    return score, feedback"}, {"type": "code", "name": "Property-Specific Details Present (Code)", "description": "Check the document text references the property address/context and key attributes: Miami, FL; 5,000 SF end cap; Publix; visibility.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n\n    def read_text():\n        try:\n            if output.extension.lower() == '.pdf':\n                return context.files.read_pdf_text(output.id)\n            else:\n                return context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                return context.files.read_docx_text(output.id)\n            except Exception:\n                return \"\"\n\n    t = read_text().lower()\n\n    checks = []\n    # Address and city/state\n    checks.append(bool(re.search(r\"\\b123\\s+dade\\s+county\\s+rd\\b\", t)))\n    checks.append(\"miami\" in t)\n    checks.append(re.search(r\"\\bfl\\b|florida\", t) is not None)\n    # Size and type\n    checks.append(bool(re.search(r\"5[,\\s]?000\\s*sf\", t)))\n    checks.append(\"end cap\" in t or \"endcap\" in t)\n    # Anchor and visibility\n    checks.append(\"publix\" in t)\n    checks.append(\"visibility\" in t or \"exposure\" in t)\n\n    found = sum(1 for c in checks if c)\n    score = found / len(checks)\n    return score, f\"Found {found}/{len(checks)} property-specific details.\""}, {"type": "code", "name": "Outreach Channels & Cadence Mention (Code)", "description": "Confirm mentions of multi-channel outreach and cadence concepts: email, call/phone, LinkedIn, site visit/tour, follow-up/cadence/sequence.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n\n    def read_text():\n        try:\n            if output.extension.lower() == '.pdf':\n                return context.files.read_pdf_text(output.id)\n            else:\n                return context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                return context.files.read_docx_text(output.id)\n            except Exception:\n                return \"\"\n\n    t = read_text().lower()\n\n    channels = [\n        (\"email\", [\"email\"]),\n        (\"call/phone\", [\"call\", \"phone\"]),\n        (\"linkedin\", [\"linkedin\"]),\n        (\"site visit/tour\", [\"site visit\", \"tour\", \"site tour\"]),\n    ]\n\n    concepts = [\n        (\"follow-up/cadence\", [\"follow-up\", \"follow up\", \"cadence\", \"sequence\"]),\n    ]\n\n    hits = 0\n    total = len(channels) + len(concepts)\n\n    for name, keys in channels + concepts:\n        if any(k in t for k in keys):\n            hits += 1\n\n    score = hits / total\n    return score, f\"Mentioned {hits}/{total} outreach channels/concepts.\""}, {"type": "code", "name": "Scripts and Flyer Template Elements Present (Code)", "description": "Validate presence of sample cold call and email scripts and a flyer template with property overview, highlights, and contact info (email/phone).", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n\n    def read_text():\n        try:\n            if output.extension.lower() == '.pdf':\n                return context.files.read_pdf_text(output.id)\n            else:\n                return context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                return context.files.read_docx_text(output.id)\n            except Exception:\n                return \"\"\n\n    t = read_text().lower()\n\n    # Scripts presence\n    call_script = (\"cold call\" in t and \"script\" in t) or (\"call script\" in t)\n    email_script = (\"email script\" in t) or (\"sample email\" in t)\n\n    # Flyer template elements\n    flyer_template = (\"flyer\" in t and \"template\" in t)\n    prop_overview = (\"property overview\" in t) or (\"overview\" in t)\n    highlights = (\"highlights\" in t)\n\n    email_pat = re.compile(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\")\n    phone_pat = re.compile(r\"(\\+1[\\s\\-.]?)?(\\(?\\d{3}\\)?[\\s\\-.]?\\d{3}[\\s\\-.]?\\d{4})\")\n\n    has_contact = (\"contact\" in t) and (bool(email_pat.search(t)) or bool(phone_pat.search(t)))\n\n    checks = [call_script, email_script, flyer_template, prop_overview, highlights, has_contact]\n    hits = sum(1 for c in checks if c)\n    score = hits / len(checks)\n    return score, f\"Scripts/flyer elements present: {hits}/{len(checks)}.\""}, {"type": "llm_judge", "name": "QSR-Tailored Scripts and Objection Handling (LLM)", "description": "Assess whether sample call/email scripts are tailored to QSR prospects and preempt common QSR buildout concerns.", "weight": 3.0, "judge_prompt": "Evaluate the scripts specifically for QSR relevance and completeness.\nLook for: mention of drive-thru potential/constraints, venting/hood/grease trap/grease interceptor, HVAC/tonnage, parking/stacking/queue, patio or outdoor seating, hours of operation, delivery/pickup lanes, co-tenancy/Publix shadow anchor synergy, exclusives/conflicts, NNN/TI/Free Rent summary, and realistic next-step asks (tour, LOI).\nScoring (0\u20133):\n- 3: Scripts clearly QSR-specific, anticipate multiple QSR objections, include persuasive value props tied to Publix/end cap/visibility, and end with strong CTAs.\n- 2: Mostly QSR-specific with some objections addressed; CTAs present but could be stronger.\n- 1: Generic retail scripts with few QSR elements.\n- 0: Scripts missing or not tailored to QSR.", "expectation": "Clear, QSR-focused scripts with objection handling and CTAs aligned to touring and LOI progression."}, {"type": "llm_judge", "name": "Outreach Cadence Coherence & Multi-Channel (LLM)", "description": "Judge whether the cadence is sequenced, multi-channel, and durable as a repeatable system.", "weight": 3.0, "judge_prompt": "Evaluate the outreach cadence/follow-up as a repeatable process.\nExpect: multi-channel sequence (email, call, LinkedIn, site visit), clear timing (days/weeks), touch counts (e.g., 8\u201312 touches over 3\u20134 weeks), personalization at scale, CRM logging/next-tasking, A/B subject lines, branching based on responses, and booking site tours.\nScoring (0\u20133):\n- 3: Detailed schedule with channel mix, timing, branching logic, CRM tracking, and tour conversion steps.\n- 2: Reasonable sequence and channels with some timing and tracking details.\n- 1: Minimal steps, vague timing, or missing channels.\n- 0: Cadence absent.", "expectation": "A concrete, time-bound multi-channel cadence usable by junior agents."}, {"type": "llm_judge", "name": "Flyer Template Completeness for QSR Leasing (LLM)", "description": "Assess whether the one-page flyer template includes essential leasing and QSR-relevant fields.", "weight": 2.5, "judge_prompt": "Assess the flyer template example for completeness and QSR relevance. Look for: property overview, key highlights, co-tenants including Publix, site plan/map or callout placeholders, access/visibility notes, traffic counts/daytime population placeholders, QSR-specific specs (vent/hood/grease capacity, potential drive-thru, patio), basic lease economics placeholders (NNN, TI, base rent), and a prominent contact block (name, phone, email). Optional: QR code/link, disclaimer.\nScoring (0\u20132.5):\n- 2.5: Includes all core items and QSR-relevant specs with a clean, fillable layout.\n- 1.5\u20132.0: Most core items present; minor omissions.\n- 0.5\u20131.0: Partial/incomplete; lacks several essentials.\n- 0: Not included or generic with no placeholders.", "expectation": "A fillable, market-ready flyer template aligned to QSR tenant needs."}, {"type": "llm_judge", "name": "Visuals Present and Relevant (LLM)", "description": "Confirm stock photos/visuals appear on every page, are relevant, and not watermarked.", "weight": 1.5, "judge_prompt": "Check that each page contains at least one relevant visual (e.g., shopping center, QSR, storefronts, food imagery). Reject obvious watermarks. Prefer consistent style and relevance to retail/QSR.\nScoring (0\u20131.5):\n- 1.5: Visuals on every page, relevant, consistent, and unwatermarked.\n- 1.0: Minor gaps (e.g., one page lacks a visual) or mixed relevance.\n- 0.5: Multiple pages without visuals or weak relevance.\n- 0: Visuals largely missing or watermarked.", "expectation": "Every page has an on-theme, unwatermarked visual."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Holistic Quality Assessment", "description": "Professionalism, strategic value, repeatability, and actionability for a junior leasing team executing in Miami\u2019s QSR-demand submarket.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation & Clarity (LLM)", "description": "Assess tone, clarity, structure, and usability as a professional broker playbook.", "weight": 2.0, "judge_prompt": "Evaluate the overall professionalism: tone, grammar, clarity, page layout, headings, and readability for junior agents. Is the document easy to navigate and reference during execution? Are sections concise yet complete?\nScoring (0\u20132):\n- 2: Highly professional, polished, and easy to use.\n- 1: Adequate but with minor clarity/organization issues.\n- 0: Poorly written or disorganized.", "expectation": "A polished, clear, and navigable playbook."}, {"type": "llm_judge", "name": "Strategic Depth for Miami QSR Leasing (LLM)", "description": "Evaluate market-aware insight relevant to Miami/Dade and Publix shadow-anchored centers.", "weight": 2.0, "judge_prompt": "Assess whether the playbook shows strategic understanding of leasing QSR space in Miami/Dade: Publix shadow-anchor dynamics, neighborhood center positioning, visibility, trade area notes, daytime population/traffic (even as placeholders), competitive set considerations, and typical permitting/buildout timelines at a high level.\nScoring (0\u20132):\n- 2: Strong, context-aware strategy tailored to Miami/Publix-anchored neighborhood centers.\n- 1: Some strategy but generic.\n- 0: Little to no strategic insight.", "expectation": "Context-aware strategy that reflects Miami QSR dynamics and anchor synergies."}, {"type": "llm_judge", "name": "Repeatable Systemization (LLM)", "description": "Judge whether the playbook provides checklists, templates, KPIs, and role-based steps enabling repeatable execution.", "weight": 2.0, "judge_prompt": "Look for reusable systems: checklists (prospecting, qualification, tour), templates (emails, call talk tracks, flyer), CRM fields/definitions, KPIs (touches/week, meeting set rate, tour-to-LOI), and role clarity for juniors vs. senior broker.\nScoring (0\u20132):\n- 2: Clear, reusable system with checklists/templates/KPIs and role clarity.\n- 1: Partially systemized; some reusable elements present.\n- 0: Ad hoc; few reusable components.", "expectation": "A repeatable, process-driven playbook with measurable KPIs."}, {"type": "llm_judge", "name": "Actionability & Next Steps (LLM)", "description": "Assess whether Next Steps are concrete and assignable with timelines to drive leasing outcomes.", "weight": 2.0, "judge_prompt": "Evaluate whether Next Steps are specific, time-bound, and assignable. Look for a 30/60/90-day outline or similar, task ownership (junior vs. senior), scheduling of tours, LOI preparation steps, and follow-up hygiene.\nScoring (0\u20132):\n- 2: Clear, time-bound actions with owners and milestones.\n- 1: Some direction but limited timelines/ownership.\n- 0: Vague or generic suggestions.", "expectation": "A clear execution plan with timelines and ownership for the team."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "e21cd746-404d-4602-b9d2-01d2812c5b87", "rubric": {"category_name": "Last\u2011Mile Logistics M&A Overview Slides (Finance & Investment Analysis)", "rationale": "This rubric enforces a self\u2011documenting, client\u2011ready 1\u20135 slide PDF deck that summarizes key private last\u2011mile delivery players and public comps with valuation multiples. Stage 1 is a strict LLM gate focused on structure and format (PDF slides, required tables/sections). Stage 2 mixes deterministic code checks (numeric/methodology plausibility) with LLM verifications (sector fit, completeness, and cross\u2011consistency). Stage 3 assesses overall client\u2011readiness, strategic value, and clarity, ensuring the deliverable is not only correct but also usable in an M&A advisory context.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Format and Structural Gate (Slides)", "description": "LLM-only gate to ensure the output is a PDF slide deck (1\u20135 pages) with the exact sections/tables required to enable verification. If this fails, the entire category is zeroed.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "PDF Slide Format and Count", "description": "Verify deliverable is a PDF with 1\u20135 slides, slide-like formatting, and clear slide titles.", "weight": 2.0, "judge_prompt": "You are evaluating whether the candidate output is a valid PDF slide deck that meets format constraints.\n\nCheck the following:\n1) File format: Must be a PDF (not Word/Excel/plain text).\n2) Page count: 1\u20135 pages (inclusive). Treat each PDF page as a slide.\n3) Slide formatting cues: Look for slide titles, layout consistent with presentation slides, and not a dense report.\n\nScoring:\n- 2.0: PDF format AND 1\u20135 pages AND slide-like formatting with clearly visible slide titles.\n- 1.0: PDF format AND 1\u20135 pages but slide formatting/titles are unclear or inconsistent.\n- 0.0: Not a PDF OR page count <1 or >5.\nOnly evaluate format/count/slide-like presentation. Do not assess content beyond confirming it appears as slides.", "expectation": "A 1\u20135 page PDF that visually reads like slides with clear titles."}, {"type": "llm_judge", "name": "Required Slide Structure and Tables Present", "description": "Verify that the slides contain the mandated sections and tables to enable verification in later stages.", "weight": 2.0, "judge_prompt": "You are evaluating the presence of required sections/tables in a 1\u20135 slide PDF deck for an M&A overview in last-mile delivery.\n\nRequired elements (flexible with naming but structure must be present):\nA) Title/Overview slide (on page 1) \u2013 includes objective/scope/context (e.g., purpose: identify private last\u2011mile players and public valuation comps).\nB) Private Players Overview \u2013 a structured table listing companies with columns or clearly labeled fields that map to: Company, Business Description, Latest Valuation, Funding to Date, Key Investors, Key Customers. This can span across 1\u20132 slides but must exist as a structured table, not just prose.\nC) Public Comparables Overview \u2013 a structured table with valuation context that maps to: Ticker, Company, (Region/Segment optional), EV/Revenue, EV/EBITDA, and P/E (other fields like Market Cap, Revenue, EBITDA are optional). Multiples must be explicitly visible per company or summarized.\nD) Valuation Summary/Benchmarking \u2013 a slide or section that summarizes/visualizes how public comps are valued (e.g., bar/summary table/range bands) and ties back to the comps table (can be on same slide as C if space is tight).\nE) Methodology & Sources (optional but strongly preferred) \u2013 assumptions, data dates (e.g., LTM/FY), and sources cited.\n\nScoring:\n- 2.0: A\u2013D all present in a structured manner; E present or clearly integrated (e.g., footers) \u2013 full credit.\n- 1.5: A\u2013D present but E missing.\n- 1.0: Missing exactly one of A\u2013D or tables are present but not clearly structured (e.g., missing column labels yet still obviously a table).\n- 0.0: Missing multiple required elements (A\u2013D) or content is unstructured prose, not tables.\nOnly check presence/structure, not correctness of values.", "expectation": "A compact slide deck with clear tables for private players and public comps, plus a valuation summary; optional methodology/sources."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Verification", "description": "Verify the deliverable\u2019s factual structure and numeric plausibility using a mix of code and LLM checks. Focus on presence and reasonableness of valuation multiples, sector/peer fit, and data transparency.", "is_required": true, "max_points": 10.0, "min_score_to_pass": 6.5, "rules": [{"type": "code", "name": "Multiples Extracted and Plausible", "description": "Programmatically detect EV/Revenue, EV/EBITDA, and P/E figures in the PDF text and check if at least one value for each metric exists and falls within plausible ranges.", "weight": 1.2, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float in [0,1] normalized (engine applies weight)\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id)\n        if not text:\n            return 0.0\n        tl = text.lower()\n        # Helper to extract numbers near a keyword\n        def extract_near_keywords(keywords, window=80, allow_x=True):\n            vals = []\n            for kw in keywords:\n                for m in re.finditer(re.escape(kw), tl):\n                    start = max(0, m.start()-window)\n                    end = min(len(tl), m.end()+window)\n                    snippet = tl[start:end]\n                    # capture like 12.3x or 12.3\n                    if allow_x:\n                        nums = re.findall(r\"([0-9]+(?:\\.[0-9]+)?)\\s*x\", snippet)\n                        vals += [float(n) for n in nums]\n                    # also capture bare numbers when no 'x'\n                    nums2 = re.findall(r\"([0-9]+(?:\\.[0-9]+)?)\", snippet)\n                    vals += [float(n) for n in nums2]\n            return vals\n        rev_keys = [\"ev/revenue\", \"ev / revenue\", \"revenue multiple\", \"ev/sales\", \"sales multiple\"]\n        ebitda_keys = [\"ev/ebitda\", \"ev / ebitda\", \"ebitda multiple\"]\n        pe_keys = [\"p/e\", \"pe\", \"price/earnings\", \"price to earnings\"]\n        rev_vals = extract_near_keywords(rev_keys)\n        ebitda_vals = extract_near_keywords(ebitda_keys)\n        pe_vals = extract_near_keywords(pe_keys, allow_x=True)\n        # plausible ranges\n        def in_range(values, lo, hi):\n            return [v for v in values if lo <= v <= hi]\n        rev_ok = len(in_range(rev_vals, 0.1, 15.0)) > 0\n        ebitda_ok = len(in_range(ebitda_vals, 1.0, 40.0)) > 0\n        pe_ok = len(in_range(pe_vals, 2.0, 120.0)) > 0\n        # If none of the metrics found at all, return 0\n        found_any = (len(rev_vals) > 0) + (len(ebitda_vals) > 0) + (len(pe_vals) > 0)\n        if found_any == 0:\n            return 0.0\n        score_bits = 0\n        total = 3\n        score_bits += 1 if (len(rev_vals) > 0 and rev_ok) else (0.5 if len(rev_vals) > 0 else 0)\n        score_bits += 1 if (len(ebitda_vals) > 0 and ebitda_ok) else (0.5 if len(ebitda_vals) > 0 else 0)\n        score_bits += 1 if (len(pe_vals) > 0 and pe_ok) else (0.5 if len(pe_vals) > 0 else 0)\n        return score_bits / total\n    except Exception:\n        return 0.0\n"}, {"type": "code", "name": "Methodology, Dates, and Sources Presence", "description": "Check for transparency markers like sources and dating of data (LTM/FY/Q) to support verification.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float in [0,1] normalized (engine applies weight)\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id)\n        if not text:\n            return 0.0\n        tl = text.lower()\n        has_source = bool(re.search(r\"\\bsource[s]?\\b|http[s]?://|www\\.\", tl))\n        has_date = bool(re.search(r\"\\bas of\\b|\\bltm\\b|\\bttm\\b|\\bfy\\s?20\\d{2}\\b|\\bq[1-4]\\s?20\\d{2}\\b|\\b20(2[0-9]|3[0-9])\\b\", tl))\n        if has_source and has_date:\n            return 1.0\n        if has_source or has_date:\n            return 0.5\n        return 0.0\n    except Exception:\n        return 0.0\n"}, {"type": "llm_judge", "name": "Sector-Fit and Public Comps Appropriateness", "description": "Assess whether public comps are genuinely relevant last-mile/logistics peers and that multiples are stated for those peers.", "weight": 3.0, "judge_prompt": "Evaluate whether the public comparables listed are appropriate peers for last-mile delivery/logistics. Consider inclusions like UPS, FedEx, DSV, XPO, GXO, Deutsche Post DHL, SF Holding/SF Express, JD Logistics, Royal Mail/IDS, MercadoLibre Logistics unit references, or other regional last-mile delivery operators. Flexibility is allowed for geography, but they must be logistics/delivery focused. Confirm that valuation multiples (EV/Revenue, EV/EBITDA, P/E) are explicitly shown for the comps (per company or summarized with clear mapping).\n\nScoring:\n- 3.0: Comps are clearly relevant logistics/delivery names AND EV/Revenue, EV/EBITDA, and P/E are explicitly shown for the comp set (per name or clearly summarized) with no mismatches.\n- 2.0: Comps mostly relevant (minor stretch) AND at least two of the three multiples are explicitly shown.\n- 1.0: Some relevant comps but list is weak/too tech/retail-heavy OR only one multiple shown.\n- 0.0: Comps not relevant to delivery/logistics or no valuation multiples are visible.\nProvide brief reasoning.", "expectation": "A credible set of delivery/logistics peers with explicit multiples coverage."}, {"type": "llm_judge", "name": "Private Players Coverage and Specificity", "description": "Check that private last-mile players are listed with the requested details and are realistic choices for M&A scanning.", "weight": 3.0, "judge_prompt": "Evaluate the private last-mile delivery players section for coverage and specificity. Look for at least 5 named private companies and the following details: business description, latest valuation, funding to date, key investors, key customers. These can be in table columns or clearly labeled fields per company. Names should be plausible active private players in last-mile/logistics (e.g., regional parcel/courier networks, gig delivery platforms, micro-fulfillment/last-mile specialists). Be tolerant of regional diversity.\n\nScoring:\n- 3.0: \u22655 private companies with all five requested details for most entries; information appears specific and not generic filler.\n- 2.0: 4\u20135 companies with most details present; minor gaps or a couple generic entries.\n- 1.0: 2\u20133 companies or details mostly missing/generic.\n- 0.0: <2 companies or no usable details.\nProvide brief reasoning.", "expectation": "A robust private target list with concrete, company-specific details."}, {"type": "llm_judge", "name": "Linkage Between Public Multiples and Private Context", "description": "Assess whether the deck connects public comps valuation context to private targets (e.g., range benchmarking, how public valuation informs private expectations).", "weight": 1.5, "judge_prompt": "Evaluate whether the slides draw a clear connection between public comps valuation multiples and the private target landscape. This can include range charts, median/mean tables, or commentary like \u201cTier-1 last-mile networks trade at X\u2013Y EV/EBITDA; asset-light platforms trade at A\u2013B EV/Revenue,\u201d and how these relate to private targets\u2019 stage/margins.\n\nScoring:\n- 1.5: Clear, explicit linkage (range/median and narrative) tying public comps to private valuation context.\n- 1.0: Some linkage or partial commentary but not fully explicit.\n- 0.5: Weak, generic high-level statements.\n- 0.0: No linkage.\nProvide brief reasoning.", "expectation": "A concise bridge from public valuation benchmarks to private targets."}, {"type": "llm_judge", "name": "Transparency of Assumptions and Data Timeliness", "description": "Confirm assumptions, dates (e.g., LTM or as-of date), and sources are visible and coherent with the presented values.", "weight": 1.5, "judge_prompt": "Check that the deck discloses key assumptions and timing of data (e.g., LTM vs FY, as-of date like April 2025 or a recent quarter) and lists credible sources (company filings, S&P Capital IQ, Bloomberg, PitchBook, CB Insights, press releases). Evaluate internal coherence: do the disclosures match the timeframe implied by numbers?\n\nScoring:\n- 1.5: Clear dating (LTM/FY/as-of) and credible sources; coherence with presented numbers.\n- 1.0: Some dating and/or sources but incomplete; mostly coherent.\n- 0.5: Minimal markers with unclear coherence.\n- 0.0: No assumptions/dates/sources visible.\nProvide brief reasoning.", "expectation": "Clear, credible sources and time markers consistent with values."}], "on_failure_action": "skip_remaining", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Client-Readiness", "description": "Holistic LLM assessment of presentation quality, insightfulness, and suitability for senior client audiences.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Executive Clarity and Slide Craft", "description": "Assess whether the deck is clean, concise, and visually professional (titles, spacing, charts/tables readable).", "weight": 1.5, "judge_prompt": "Evaluate overall slide craftsmanship: crisp titles, logical hierarchy, readable charts/tables, minimal clutter, and consistent styling suitable for client distribution.\n\nScoring:\n- 1.5: Highly professional and polished; excellent readability.\n- 1.0: Generally professional with minor issues.\n- 0.5: Mixed quality; noticeable readability/layout issues.\n- 0.0: Poor formatting; not client-ready.", "expectation": "Client-ready slides with clear hierarchy and readability."}, {"type": "llm_judge", "name": "Strategic Insight and M&A Relevance", "description": "Judge the strength of insights (segmentation, value drivers, risks) and how it frames potential M&A moves.", "weight": 2.5, "judge_prompt": "Assess whether the content goes beyond listing data, offering insights such as: segmentation (asset-heavy vs asset-light; parcel vs on-demand), margin/scale drivers, regulatory or labor considerations, and how these shape M&A angles (tuck-ins, capability expansion, geographic coverage). Look for concise commentary supporting a banker\u2019s recommendation stance.\n\nScoring:\n- 2.5: Strong, actionable insights tailored to last-mile M&A.\n- 1.5: Some useful insights but not fully developed.\n- 0.5: Minimal high-level commentary.\n- 0.0: No strategic insight.", "expectation": "Actionable banker-grade insight tied to M&A theses."}, {"type": "llm_judge", "name": "Narrative Flow and Brevity (\u22645 slides)", "description": "Evaluate coherence across slides with clear flow from problem to comps to implications, within the slide limit.", "weight": 1.0, "judge_prompt": "Assess whether the story flows: objective \u2192 private landscape \u2192 public comps \u2192 valuation implications \u2192 next steps, all within \u22645 slides without feeling rushed or fragmented.\n\nScoring:\n- 1.0: Smooth, coherent narrative within limits.\n- 0.5: Adequate flow with minor gaps.\n- 0.0: Disjointed or exceeds scope/slide constraint.", "expectation": "Concise, coherent narrative in \u22645 slides."}, {"type": "llm_judge", "name": "Actionability and Next Steps", "description": "Check if the deck proposes clear next steps for advisory engagement (e.g., data room ask, outreach, diligence).", "weight": 1.0, "judge_prompt": "Evaluate whether the deck ends with or includes concrete next steps tailored to the client (e.g., refine target list, initiate confidential outreach, request data, build operating/valuation model, timeline). Generic boilerplate scores lower than tailored recommendations.\n\nScoring:\n- 1.0: Clear, tailored next steps.\n- 0.5: Generic or lightly tailored.\n- 0.0: None provided.", "expectation": "Specific, relevant next steps that advance the M&A conversation."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "55ddb773-23a4-454c-8704-d432fe1b99d9", "rubric": {"category_name": "Community Association Violation Inspection Questionnaire Form", "rationale": "This rubric enforces a self-documenting, verifiable PDF questionnaire form for a master HOA\u2019s inspection vendor. Stage 1 (LLM-only) strictly gates structure and format so later checks are trivial. Stage 2 mixes light code checks (string/bounds/style presence) with heavier LLM verification of coverage and clarity. Stage 3 holistically assesses professional quality and practical usability for boards, sub associations, and inspectors. Code rules are intentionally lower-weight than LLM rules per guidance.", "max_total_score": 30.0, "stages": [{"name": "Stage 1 \u2013 Format and Structural Gate (LLM-only)", "description": "Gate that the output is a properly structured, fillable-style questionnaire document (PDF preferred) with all required sections and fields to enable verification. Do NOT judge content quality here\u2014only presence/structure/format.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Valid PDF form with core header fields", "description": "Checks document format and presence of all header fields with lined spaces and Y/N circle options.", "weight": 3.0, "judge_prompt": "You are checking only FORMAT and STRUCTURAL PRESENCE, not content quality.\n\nPass criteria: The output is a PDF (preferred) or DOCX document that appears as a professional questionnaire form. It must include clearly labeled, lined or fillable spaces for ALL of the following header fields:\n- Sub Association Name\n- Type of Association (e.g., SFH/Condo/Townhouse/Phase)\n- Gate/Access Codes\n- CAM Name and Phone Number\n- Number of Homes\n- Community fines (Y/N) with explicit instruction to \u201cCircle one\u201d or clear radio/checkbox indicators\n- Picture requirement for violations (Y/N) with explicit instruction to \u201cCircle one\u201d or clear radio/checkbox indicators\n\nScoring guidance (structure only):\n- 3.0: PDF or DOCX form AND all listed fields present with lined/blank areas and explicit Y/N circle (or radio/checkbox) options for both fines and picture requirement\n- 2.0: Valid document and 1\u20132 minor header elements missing or unlabeled; Y/N options present for both items\n- 1.0: Valid document but multiple header fields missing; at least one Y/N option present\n- 0.0: Not a document, wrong format, or missing most core header fields\n\nBe flexible with exact wording (e.g., \u201cAssociation Type\u201d vs \u201cType of Association\u201d). Evaluate only structure/presence.", "expectation": "A PDF questionnaire with labeled lines/blanks for all header fields and explicit Y/N circle options for fines and picture requirement."}, {"type": "llm_judge", "name": "Required section layout present", "description": "Checks that the document contains the necessary major sections with appropriate structural elements.", "weight": 3.0, "judge_prompt": "Assess only presence/structure (not correctness):\nConfirm the document includes these sections (accept reasonable synonyms):\n1) Instructions or Overview (purpose of the form and how to complete)\n2) Violations Types Catalog (listing violation categories with space under each for qualifying questions/details)\n3) Architectural Regulations section (each item/question on its own line with blank spaces for association-specific rules)\n4) Community-Specific Additions (a block of several blank lines to add unique rules)\n5) Submission/Contact block (where to return the form or contact info)\n\nAlso confirm the Y/N fields instruct \u201cCircle one\u201d or use clearly indicated yes/no selection controls.\n\nScoring:\n- 3.0: All 5 sections present and clearly delineated with appropriate lines/blanks beneath category items\n- 2.0: 4/5 sections present\n- 1.0: 3/5 sections present\n- 0.0: Fewer than 3 sections or no clear structure\n", "expectation": "All five sections are visible with clear headers or equivalent labels and fillable/blank space where applicable."}, {"type": "llm_judge", "name": "Fillability elements and field labeling", "description": "Checks that the form uses lined spaces/checkboxes/radio circles and clear labels to enable easy completion.", "weight": 2.0, "judge_prompt": "Evaluate whether the form visibly supports being filled out by a sub association:\n- Lined/blank fields (e.g., underscores or boxes) appear under prompts requiring written input\n- Y/N choices are indicated as radio/circles/checkboxes with nearby instructions (e.g., \u201cCircle one\u201d)\n- Category items (especially violations and architectural) have one or more dedicated lines beneath for details\n\nScoring:\n- 2.0: Clear fill-in lines and selection controls across all relevant fields\n- 1.0: Mostly present but inconsistent in some sections\n- 0.0: Little to no evidence of fillable structure\n", "expectation": "Consistent lined spaces and clear yes/no selection controls across the form."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness and Coverage Verification", "description": "Now verify that the content is appropriately complete and operational for inspections. Use code for deterministic presence checks and LLM for nuanced coverage and cross-references.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Header field strings present (deterministic check)", "description": "Confirms the presence of key header strings and Y/N tokens in the text layer of the document.", "weight": 0.8, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float in [0, weight]\n    \"\"\"\n    weight = 0.8\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    # Try reading PDF then DOCX then text\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n    if not text:\n        return 0.0\n    t = text.lower()\n    required = [\n        ['sub association name', 'association name'],\n        ['type of association', 'association type'],\n        ['access code', 'gate code', 'gate/access'],\n        ['cam name', 'community association manager'],\n        ['phone', 'phone number', 'tel'],\n        ['number of homes', 'total homes', 'home count'],\n        ['fines', 'community fines'],\n        ['picture requirement', 'photo requirement', 'pictures required']\n    ]\n    hits = 0\n    total = len(required)\n    for group in required:\n        if any(g in t for g in group):\n            hits += 1\n    # Require also some Y/N markers in document\n    yn_present = any(x in t for x in ['y/n', 'yes/no'])\n    ratio = (hits / total)\n    # Give small bonus if yn marker present\n    score = ratio\n    if yn_present:\n        score = min(1.0, score + 0.1)\n    return max(0.0, min(weight, score * weight))"}, {"type": "code", "name": "Yes/No selection cues near fines and pictures", "description": "Verifies that Y/N or Yes/No appears near \u2018fines\u2019 and \u2018picture requirement\u2019 references.", "weight": 0.6, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    \\\"\\\"\\n    Returns a score in [0, weight] based on detection of Y/N or Yes/No cues\\n    near mentions of fines and picture requirements.\\n    \\\"\\\"\\n    weight = 0.6\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n    text = ''\\n    try:\\n        text = context.files.read_pdf_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_docx_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_text(output.id)\\n            except Exception:\\n                return 0.0\\n    if not text:\\n        return 0.0\\n    t = text.lower()\\n    # Build windows around keywords\\n    def has_yn_near(keyword):\\n        for m in re.finditer(re.escape(keyword), t):\\n            start = max(0, m.start()-80)\\n            end = min(len(t), m.end()+80)\\n            window = t[start:end]\\n            if ('y/n' in window) or ('yes/no' in window) or ('circle' in window and ('yes' in window or 'no' in window)):\\n                return True\\n        return False\\n    fines_ok = has_yn_near('fine') or has_yn_near('fines') or has_yn_near('community fines')\\n    pics_ok = has_yn_near('picture requirement') or has_yn_near('photo requirement') or has_yn_near('pictures required') or has_yn_near('photo required')\\n    score = 0.0\\n    if fines_ok:\\n        score += 0.5\\n    if pics_ok:\\n        score += 0.5\\n    # Normalize to weight (max raw = 1.0)\\n    return min(weight, score * weight) / 1.0"}, {"type": "code", "name": "Architectural section and adequate blank lines", "description": "Checks for an Architectural section and that it appears to contain multiple fill-in lines.", "weight": 0.6, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    weight = 0.6\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n    text = ''\\n    try:\\n        text = context.files.read_pdf_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_docx_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_text(output.id)\\n            except Exception:\\n                return 0.0\\n    if not text:\\n        return 0.0\\n    t = text.lower()\\n    # Architectural section presence\n    arch_idx = t.find('architectural')\n    if arch_idx == -1:\n        return 0.0\n    window = t[arch_idx: arch_idx + 2500]\n    # Count occurrences of typical line markers\n    line_markers = len(re.findall(r'_{3,}', window)) + len(re.findall(r'\\[\\s*\\]', window)) + len(re.findall(r'\\.{5,}', window))\n    # Score: 0 lines -> 0; 1-2 -> 0.5; >=3 -> 1.0\n    if line_markers >= 3:\n        ratio = 1.0\n    elif line_markers >= 1:\n        ratio = 0.5\n    else:\n        ratio = 0.0\n    return min(weight, ratio * weight)"}, {"type": "code", "name": "Community-specific additions block present", "description": "Checks for a section inviting association-specific additions with several blank lines.", "weight": 0.5, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    weight = 0.5\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n    text = ''\\n    try:\\n        text = context.files.read_pdf_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_docx_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_text(output.id)\\n            except Exception:\\n                return 0.0\\n    if not text:\\n        return 0.0\\n    t = text.lower()\\n    # Look for heading synonyms\n    has_heading = any(h in t for h in [\n        'community-specific', 'association-specific', 'additional items', 'other items', 'other rules', 'notes for this community'\n    ])\n    # Look for multiple line markers nearby\n    lines = len(re.findall(r'_{3,}', t)) + len(re.findall(r'\\.{5,}', t))\n    if has_heading and lines >= 3:\n        ratio = 1.0\n    elif has_heading and lines >= 1:\n        ratio = 0.6\n    elif has_heading:\n        ratio = 0.4\n    else:\n        ratio = 0.0\n    return min(weight, ratio * weight)"}, {"type": "llm_judge", "name": "Coverage of key violation categories", "description": "Evaluates whether the Violations Types Catalog broadly covers typical HOA categories and appears usable by an inspection vendor.", "weight": 3.5, "judge_prompt": "Check if the form\u2019s Violations Types Catalog covers a broad range of typical HOA violation categories and sub-association variations. Examples include (flexible list): exterior maintenance, landscaping, trash/recycling, parking/vehicles, commercial vehicles, signage, noise/nuisance, pets/animal control, leasing/rental restrictions, short-term rentals, amenities (pool/clubhouse/fitness), common area use, architectural modifications, fence/shed/patio, satellite/antennas, lighting, mailbox, holiday decor/seasonal items. Also consider phase-specific differences.\n\nScore:\n- 3.5: Broad coverage across most categories, clearly organized for inspectors\n- 2.5: Good coverage with some gaps\n- 1.5: Limited coverage; several common categories missing\n- 0.0: Very few categories present\n\nDo not penalize for exact wording; assess breadth and relevance.", "expectation": "A comprehensive, inspector-usable catalog spanning most typical HOA violation categories."}, {"type": "llm_judge", "name": "Qualifying questions/details present under each type", "description": "Checks that each violation type includes sub-questions/prompts for specifics that inspectors need to apply rules consistently.", "weight": 3.0, "judge_prompt": "For the listed violation categories, confirm that under each category there are qualifying questions/prompts and dedicated blank lines to capture specifics such as: thresholds/dimensions (e.g., height/size), frequency/duration, location constraints (front/back/common area), allowed times/days, vehicle types and limits, grace/cure periods, photo requirements, repeat offense handling, and references to governing documents.\n\nScore:\n- 3.0: Most categories include several relevant qualifying prompts and fill-in lines\n- 2.0: Some categories have qualifiers; others are sparse\n- 1.0: Few qualifiers overall\n- 0.0: No meaningful qualifiers beneath categories\n", "expectation": "Each violation type has tailored qualifying prompts with space to fill association-specific parameters."}, {"type": "llm_judge", "name": "Architectural regulations enumerated on separate lines", "description": "Verifies that architectural items/questions are each listed on their own line with extra blank lines for additional architectural notes.", "weight": 2.0, "judge_prompt": "Inspect the Architectural Regulations section. It should: (1) list specific architectural items/questions each on its own line (e.g., paint colors, roofing, fences, doors, windows, solar, landscaping changes, hardscape, play equipment), and (2) include several extra blank lines at the end for association-specific architectural items.\n\nScore:\n- 2.0: Clear per-item lines and several extra blanks\n- 1.0: Some items present but not consistently separated or few/no extra blanks\n- 0.0: No meaningful architectural breakdown\n", "expectation": "A well-structured architectural section with discrete lines and additional blank lines."}, {"type": "llm_judge", "name": "Inspector usability and picture requirement clarity", "description": "Ensures the form communicates picture evidence requirements and is operational for inspection workflows.", "weight": 1.0, "judge_prompt": "Confirm that the form clearly indicates whether photos are required for violations via a Y/N choice and, if photos are required, that expectation language is present or implied (e.g., space to note photo standards or a checkbox). Also verify that instructions make the form usable by an inspection vendor (e.g., guidance to use as a checklist/reference).\n\nScore:\n- 1.0: Photo requirement choice is clear and inspector-oriented instructions are present\n- 0.5: Photo choice is clear but little/no instruction\n- 0.0: Ambiguous or missing photo requirement\n", "expectation": "Clear Y/N photo requirement and minimal inspector-facing guidance."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Holistic Quality Assessment", "description": "Professional quality, clarity, and practical usability for boards, sub associations, and inspectors.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional formatting and readability", "description": "Assesses page layout, headings hierarchy, consistent typography, and clean spacing.", "weight": 3.0, "judge_prompt": "Evaluate professional presentation for a formal HOA form:\n- Clear title and section headers\n- Consistent fonts, sizes, and spacing\n- Logical page breaks and margins suitable for print\n- Legible labels and adequate contrast\n\nScore 0\u20133 based on overall professional polish and readability.", "expectation": "A professional-looking form with clear hierarchy and consistent formatting."}, {"type": "llm_judge", "name": "Form usability and completion experience", "description": "Assesses whether a sub association can easily complete the form without confusion.", "weight": 3.0, "judge_prompt": "Consider:\n- Adequate space for answers under each prompt\n- Consistent use of lines/checkboxes/radios\n- Minimal ambiguity in where to write responses\n- Obvious flow from top to bottom with helpful brief instructions\n\nScore 0\u20133 based on ease of completion and absence of ambiguity.", "expectation": "Intuitive flow and sufficient space to respond throughout."}, {"type": "llm_judge", "name": "Clarity and non-ambiguity of language", "description": "Assesses whether prompts avoid vague terms and include clarifying cues for thresholds, locations, and time windows.", "weight": 2.0, "judge_prompt": "Judge the clarity of prompts and qualifiers:\n- Avoids vague terms without context (e.g., \"excessive,\" \"improper\")\n- Includes clarifying cues (dimensions, durations, locations)\n- Uses plain, direct language suitable for non-experts\n\nScore 0\u20132 based on clarity and specificity.", "expectation": "Clear, plain language with embedded clarifying cues to reduce ambiguity."}, {"type": "llm_judge", "name": "Printability and administrative readiness", "description": "Assesses whether the document is ready for distribution and archival.", "weight": 2.0, "judge_prompt": "Evaluate:\n- Page count is reasonable; margins suitable for print\n- Presence of footer/header with association name/date/version or space for it\n- Contact/submission info is visible\n\nScore 0\u20132 based on readiness for printing/distribution.", "expectation": "Ready-to-print document with headers/footers or placeholders and clear submission info."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "5ad0c554-a7a2-48cd-b41a-ebc1bff4a9de", "rubric": {"category_name": "Real Estate Buyer Brochure (Sarasota, FL)", "rationale": "This rubric enforces a self-documenting, verifiable brochure deliverable for first-time home buyers in Sarasota, FL. Stage 1 (LLM-only) strictly mandates a DOCX/PDF brochure shape with clearly labeled sections for the five buyer milestones, an explicit 2024 NAR settlement/Buyer\u2019s Broker Agreement notice, visual elements, and contact/CTA. Stage 2 mixes lightweight code checks (structure signals, local/financing mentions) with heavier LLM checks (correctness of NAR requirement, proper cross-referencing of \"132 Things REALTORS Do for Buyers,\" and milestone coverage). Stage 3 assesses professional quality, readability for first-time buyers, localization to Sarasota/gated communities, and persuasive CTA. Code rules are intentionally lower weight than LLM rules, reflecting their narrow, deterministic focus.", "max_total_score": 15.0, "stages": [{"name": "Stage 1 Gate \u2013 Document Format and Structure Requirements", "description": "Mandatory brochure shape and sections so downstream verification is trivial. LLM-only gate.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.0, "rules": [{"type": "llm_judge", "name": "Brochure Shape and Structural Completeness", "description": "Check brochure is a DOCX/PDF, at least 2 pages (double-sided), with required sections, NAR Buyer\u2019s Broker Agreement note, visuals, and contact/CTA.", "weight": 3.0, "judge_prompt": "You are checking ONLY format and structural presence (not content quality or correctness). Review the candidate output and assess if it satisfies ALL of the following brochure shape requirements. Be flexible with exact wording of headers, but confirm each element is clearly present and identifiable.\n\nFormat Requirements:\n- File type: Must be a DOCX or PDF (brochure/flyer style). Not plain text, not spreadsheet.\n- Length: At least 2 pages (double-sided) or a clear two-panel layout equating to a double-sided brochure.\n- Visual appeal: Include at least one visual element (e.g., photo, icon, banner, logo, divider, or decorative shape) beyond plain text.\n\nRequired Sections (Headers may vary, but intent must be clear and separated as their own sections):\n1) Buyer consultation\n2) The home search process\n3) Pre-offer details\n4) The offer process\n5) Contract to closing\n\nMandatory Compliance Notice:\n- A clearly visible note explaining the 2024 NAR settlement requirement that a signed Buyer\u2019s Broker Agreement is needed before an agent can show or tour properties. This must be called out distinctly (e.g., a sidebar, callout box, or bolded note) so buyers can\u2019t miss it.\n\nReferences to Source Material:\n- For each of the five sections, include at least 1 bullet/point that explicitly references or identifies relevant items from \"132 Things REALTORS Do for Buyers\" (the linked reference). References can be by brief description and/or item numbers (e.g., \u201cFrom 132 Things\u2026 #14\u201d). Be flexible about exact citation styling.\n\nContact and CTA:\n- A contact block with at least one of: phone number or email address.\n- A clear call-to-action inviting buyers to schedule a consultation and/or to sign the Buyer\u2019s Broker Agreement before touring.\n\nScoring:\n- 3.0: All required elements present (correct format, 2+ pages, visuals, all 5 sections, NAR requirement callout, per-section reference to the 132 Things list, and contact+CTA).\n- 2.0: Valid format + 2+ pages + visuals + at least 4 of 5 sections + NAR requirement callout + contact+CTA. Minor issues with references (e.g., some sections reference 132 Things but not all).\n- 1.0: Valid format + 2+ pages, but missing multiple required sections and/or missing NAR callout or references to 132 Things; still looks like a brochure with contact+CTA.\n- 0.0: Wrong format (not DOCX/PDF), only 1 page, or missing most structural elements.\n\nOnly evaluate presence/shape, not the factual correctness of the content.", "expectation": "A two-page (or equivalent) DOCX/PDF brochure with five clearly labeled milestone sections, a prominent 2024 NAR Buyer\u2019s Broker Agreement notice, per-section references to the 132 Things list, at least one visual, and a contact/CTA block."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification of Coverage and Correctness", "description": "Verify factual coverage and essential compliance details using mixed LLM and lightweight code checks.", "is_required": false, "max_points": 7.5, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Milestone Headings Present (Fuzzy)", "description": "Check presence of the five milestone sections via fuzzy keyword search in the document text.", "weight": 0.5, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns: float or (float, str)\n    \"\"\"\n    import re\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output or wrong type\"\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0, \"Unable to read document text\"\n    tl = text.lower()\n\n    buckets = {\n        'buyer_consultation': [\n            'buyer consultation', 'initial consultation', 'strategy session', 'first-time buyer consultation', 'introductory consultation'\n        ],\n        'home_search': [\n            'home search', 'search process', 'finding homes', 'property search', 'touring homes', 'neighborhood research'\n        ],\n        'pre_offer': [\n            'pre-offer', 'before you write an offer', 'offer prep', 'pre offer', 'getting ready to offer'\n        ],\n        'offer_process': [\n            'offer process', 'making an offer', 'submit offer', 'writing an offer', 'negotiation'\n        ],\n        'contract_to_closing': [\n            'contract to closing', 'from contract to close', 'under contract', 'escrow', 'closing process', 'due diligence', 'inspection and appraisal'\n        ],\n    }\n\n    count = 0\n    details = []\n    for key, kws in buckets.items():\n        found = any(kw in tl for kw in kws)\n        if found:\n            count += 1\n            details.append(f\"{key}: ok\")\n        else:\n            details.append(f\"{key}: missing\")\n\n    score = count / 5.0\n    return score, f\"Sections found: {count}/5; details: {', '.join(details)}\""}, {"type": "code", "name": "Buyer\u2019s Broker Agreement and 2024/NAR Mention", "description": "Verify that the brochure references a Buyer\u2019s Broker Agreement in the context of the 2024 NAR settlement/rule change and indicates it is required before touring/showings.", "weight": 0.5, "code": "def evaluate(workflow, context):\n    import re\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output or wrong type\"\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0, \"Unable to read document\"\n    tl = text.lower()\n\n    # Buyer broker agreement keywords\n    bba_terms = [\n        \"buyer broker agreement\", \"buyer\u2019s broker agreement\", \"buyers broker agreement\", \n        \"buyer brokerage agreement\", \"buyer representation agreement\"\n    ]\n    has_bba = any(term in tl for term in bba_terms)\n\n    # NAR/2024/settlement context and requirement language\n    context_terms = [\"2024\", \"nar\", \"national association of realtors\", \"settlement\", \"rule change\"]\n    verbs = [\"require\", \"required\", \"must\", \"cannot\", \"prohibit\", \"prohibited\"]\n\n    has_context = any(ct in tl for ct in context_terms)\n    has_requirement_verb = any(v in tl for v in verbs)\n\n    # Award partial if BBA is mentioned; full if BBA + (context + requirement verb)\n    if has_bba and has_context and has_requirement_verb:\n        return 1.0, \"BBA + 2024/NAR/settlement context + requirement language found\"\n    elif has_bba:\n        return 0.66, \"BBA mentioned without sufficient 2024/NAR/requirement context\"\n    else:\n        return 0.0, \"No clear Buyer\u2019s Broker Agreement reference detected\""}, {"type": "code", "name": "Contact, Local Cues, and Financing Signals", "description": "Check for contact info (phone/email), Sarasota/Florida locality, and financing/pre-approval terms.", "weight": 0.5, "code": "def evaluate(workflow, context):\n    import re\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output or wrong type\"\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0, \"Unable to read document\"\n    tl = text.lower()\n\n    # Contact info\n    has_email = re.search(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\", text) is not None\n    has_phone = re.search(r\"(\\+?1[\\s\\-\\.]*)?\\(?\\d{3}\\)?[\\s\\-\\.]?\\d{3}[\\s\\-\\.]?\\d{4}\", text) is not None\n    contact_ok = has_email or has_phone\n\n    # Local cues\n    local_ok = (\"sarasota\" in tl) or (\"florida\" in tl) or (\" fl \" in tl) or (\" fl.\" in tl) or (\" fl,\" in tl)\n\n    # Financing/pre-approval\n    finance_terms = [\"pre-approval\", \"preapproval\", \"mortgage\", \"lender\", \"financing\", \"down payment\", \"closing costs\"]\n    finance_ok = any(term in tl for term in finance_terms)\n\n    satisfied = sum([1 if contact_ok else 0, 1 if local_ok else 0, 1 if finance_ok else 0])\n    score = satisfied / 3.0\n    feedback = f\"Contact:{contact_ok}, Local:{local_ok}, Financing:{finance_ok}\"\n    return score, feedback"}, {"type": "llm_judge", "name": "Correct Referencing of \u2018132 Things REALTORS Do for Buyers\u2019", "description": "Verify that each milestone section includes at least one relevant, correctly attributed item from the 132 Things list and that the items make sense for the section.", "weight": 2.4, "judge_prompt": "Evaluate whether the brochure correctly references and uses items from \"132 Things REALTORS Do for Buyers\" across the five milestone sections. You are checking content correctness and relevance (not just presence):\n\nChecks:\n- For each of the five sections (Buyer consultation; Home search process; Pre-offer details; Offer process; Contract to closing), at least one bullet/point explicitly ties to a relevant task from the 132 Things list (by description and/or item number). Be flexible with citation format (e.g., \u201cFrom 132 Things\u2026 #14\u201d).\n- The cited item(s) are appropriate for that section (e.g., lender referral, needs analysis in consultation; showing scheduling/logistics in search; CMA and comps in pre-offer; negotiation and contingencies in offer; inspections, appraisal, title/escrow coordination in contract-to-close).\n- No obviously incorrect attributions (e.g., referencing title work in the consultation section only, with nothing related to consultation).\n\nScoring:\n- 2.4: All five sections include at least one relevant, correctly attributed item from the list.\n- 1.6: 3\u20134 sections properly reference relevant items; minor mismatches elsewhere.\n- 0.8: 1\u20132 sections reference relevant items or references are mostly generic/vague.\n- 0.0: No clear references or references are largely irrelevant/misplaced.", "expectation": "Each milestone section cites at least one relevant, specific item from the 132 Things list, with appropriate placement."}, {"type": "llm_judge", "name": "Accuracy of 2024 NAR Settlement Requirement", "description": "Validate the accuracy and clarity of the Buyer\u2019s Broker Agreement requirement and how it applies before touring/showing homes.", "weight": 2.0, "judge_prompt": "Check the brochure\u2019s explanation of the 2024 NAR settlement requirement regarding Buyer\u2019s Broker Agreements:\n\n- It should clearly state that buyer agents are prohibited from showing/touring properties without a signed Buyer\u2019s Broker Agreement.\n- It should frame this as a policy change effective in 2024 due to the NAR settlement (wording can vary but must be accurate in essence).\n- It should indicate what buyers need to do (e.g., sign the agreement before touring) and may include a brief, non-legal advisory tone (optional but helpful).\n- It should avoid legal misstatements (e.g., claiming unrelated requirements) or misleading/incomplete instructions.\n\nScoring:\n- 2.0: Accurate, clear, and actionable explanation of the requirement.\n- 1.2: Mostly correct but missing a key detail (e.g., timing or the reason tied to NAR settlement) or vague on action steps.\n- 0.6: Mentioned but confusing or partially inaccurate.\n- 0.0: Not mentioned or materially inaccurate.", "expectation": "A concise, accurate callout explaining the 2024 buyer-broker agreement requirement before showings, tied to the NAR settlement."}, {"type": "llm_judge", "name": "Coverage and Actionability Across All Milestones", "description": "Check that each milestone section contains actionable, concrete steps suited for first-time buyers.", "weight": 1.6, "judge_prompt": "Assess whether the brochure covers all five milestones with actionable, concrete guidance appropriate for first-time home buyers:\n\n- Each section should have 2\u20135 concise bullets or similar that describe practical steps (e.g., needs analysis, pre-approval, MLS search, setting showings, CMA/comps, offer terms, contingencies, inspections, appraisal, title/escrow coordination, walk-through, closing logistics).\n- Content should be specific enough to guide a first-time buyer through what happens at that stage.\n- Avoids major gaps (e.g., no mention of inspections or appraisal in contract-to-close) and avoids redundancies across sections that make the flow unclear.\n\nScoring:\n- 1.6: All five sections have clear, practical bullets/steps.\n- 1.0: One section is thin/vague or missing concrete actions.\n- 0.5: Two sections thin/vague or one section missing.\n- 0.0: Three or more sections thin/vague or multiple sections missing actions.", "expectation": "All five milestones include concise, practical steps that guide first-time buyers through the process."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Quality and Professionalism", "description": "Holistic quality assessment: design, readability, localization, and persuasion.", "is_required": false, "max_points": 4.5, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Design and Visual Appeal", "description": "Assess professional brochure design and visual hierarchy.", "weight": 1.2, "judge_prompt": "Judge the brochure\u2019s design quality:\n- Professional visual hierarchy: clear headings, subheadings, and spacing; logical flow suited to a brochure.\n- Use of visuals (photos/icons/shapes) that support comprehension and aesthetics without clutter.\n- Print-friendly layout appropriate for a double-sided handout (reasonable margins, contrast, and legibility).\n\nScoring:\n- 1.2: Clean, professional design with effective visuals and hierarchy.\n- 0.8: Generally good but with minor layout/contrast issues or slightly cluttered visuals.\n- 0.4: Adequate but plain or somewhat disorganized.\n- 0.0: Poorly formatted, cluttered, or visually confusing.", "expectation": "A polished, print-ready brochure with clear hierarchy and tasteful visuals."}, {"type": "llm_judge", "name": "Clarity and Accessibility for First-Time Buyers", "description": "Evaluate readability and clarity for non-expert audiences.", "weight": 1.2, "judge_prompt": "Assess how accessible the brochure is for first-time buyers:\n- Plain-language explanations (minimal jargon; if used, terms like escrow, contingencies, appraisal are briefly clarified).\n- Bulleted, scannable content with short sentences and clear takeaways.\n- Consistent tone that is supportive and informative.\n\nScoring:\n- 1.2: Very clear, user-friendly throughout.\n- 0.8: Mostly clear with minor jargon or density.\n- 0.4: Mixed clarity; several jargon-heavy or long-winded sections.\n- 0.0: Confusing or overly technical.", "expectation": "Simple, scannable language tailored to first-time buyers."}, {"type": "llm_judge", "name": "Localization and Relevance to Sarasota/Gated Communities", "description": "Check localization to Sarasota, FL and gated community considerations.", "weight": 1.0, "judge_prompt": "Evaluate localization:\n- References to Sarasota/Florida context (e.g., neighborhoods, coastal considerations, flood zones, hurricane/wind insurance, HOA/amenities, CDDs, local lenders/inspectors) where appropriate.\n- Relevance to gated communities with amenities (e.g., HOA rules/fees, amenity access, visitor policies, security, clubhouse/pool/fitness notes).\n\nScoring:\n- 1.0: Clear localization and relevant gated community points incorporated.\n- 0.6: Some localization or gated-community points, but limited/incomplete.\n- 0.3: Minimal localization; generic content with slight references.\n- 0.0: No meaningful localization.", "expectation": "Meaningful Sarasota/Florida and gated community context woven into the guidance."}, {"type": "llm_judge", "name": "CTA Strength and Professionalism", "description": "Assess the persuasiveness of calls-to-action and professional presentation.", "weight": 1.1, "judge_prompt": "Review calls-to-action and professional cues:\n- Clear invitation to schedule a consultation and to sign the Buyer\u2019s Broker Agreement before touring.\n- Prominent, accurate contact block (name, phone/email; brokerage/branding helpful).\n- Professional tone; optional helpful disclaimers (e.g., not legal advice) are acceptable if concise.\n\nScoring:\n- 1.1: Strong, persuasive CTA with professional presentation and contact clarity.\n- 0.7: CTA present but less prominent or contact block incomplete.\n- 0.3: Weak CTA and/or hard-to-find contact info.\n- 0.0: No meaningful CTA or contact info.", "expectation": "A compelling CTA and clear contact presence that supports conversion."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "f1be6436-ffff-4fee-9e66-d550291a1735", "rubric": {"category_name": "ACP-IMM 2026 Estimated Costs Document (Medical Secretaries)", "rationale": "Mixed task: a Word document with embedded evidence (screenshots) plus itemized calculations and roll-ups. Stage 1 strictly enforces the self-documenting structure so that downstream verification is possible. Stage 2 mixes light code checks (text extraction, presence of required strings, dates, and naming) with heavier LLM verification (math roll-ups, lodging split logic, timeline constraints, taxes/fees inclusion) per the 5x weighting guidance. Stage 3 provides holistic quality assessment for professionalism, clarity, evidence traceability, and instruction compliance.", "max_total_score": 17.0, "stages": [{"name": "Stage 1: Shape Enforcement Gate (Document Structure and Evidence Presence)", "description": "LLM-only gate to ensure the candidate output is a DOCX file named as specified and contains the exact sections, screenshots, and structural elements required to enable verification.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.1, "rules": [{"type": "llm_judge", "name": "Structured DOCX with Required Sections, Dates, and Screenshots", "description": "Verify the document is a Word file named correctly, contains all mandated sections with dates in headers, embedded screenshots, and itemized details enabling verification.", "weight": 3.0, "judge_prompt": "You are evaluating a candidate output for a logistics cost estimate document for two physicians attending ACP-IMM 2026.\n\nOnly assess SHAPE/STRUCTURE and EVIDENCE PRESENCE. Do not judge calculation correctness here.\n\nRequirements to check:\n1) File/Format:\n   - Must be a Word document (DOCX), not PDF/Excel/Text.\n   - Target filename: \"2026 ACP-IMM Estimated Costs.docx\" (allow minor punctuation/spacing variations, but it must be clearly the intended title in the file name or on the cover/title).\n\n2) Required Sections with dates in header parentheses (e.g., \"(6/1/25)\"):\n   - Registration (date)\n   - Travel and Transportation (date)\n   - Lodging (date)\n   - Total Costs (date)\n   For any screenshot captured on a different date than compilation, the header or section should state both the screenshot capture date and the compilation date.\n\n3) Evidence (embedded screenshots) and itemization:\n   - Registration section: embedded screenshot of ACP-IMM 2026 registration pricing page; brief summary with itemized registration cost for Dr. Smith and Dr. Doe and a subtotal/total for registration.\n   - Travel and Transportation section: \n        a) Flights: embedded screenshots for each physician\u2019s economy flights (airline, dates, depart/arrive cities and times, and cost visible). \n        b) Ground rides: embedded screenshots of routes and costs for airport-to-hotel on arrival and hotel-to-airport on departure for each physician. \n        c) Brief summary listing per-physician costs and total for this section.\n   - Lodging section: \n        a) A list identifying hotels within ~3-block radius of the ACP-IMM venue, with indication of 4- or 5-star ratings.\n        b) Embedded screenshot of the selected cheapest 4- or 5-star option\u2019s 3-night price for proxy dates (July 17\u201320).\n        c) A brief summary explaining room-sharing and proportional split (Dr. Smith 3 nights, Dr. Doe 2 nights), and listing per-physician lodging costs and total.\n   - Total Costs section: a 2-column layout/table with columns labeled \"Dr. Smith\" and \"Dr. Doe\" that includes at minimum the following rows: \"$2k Department Funding\", \"Total Cost\" (rolled up from prior sections), and \"Remaining Cost\" (Total Cost minus $2,000). Remaining Cost should be visually highlighted: green if positive (department covered fully), red if negative (amount to be taken from discretionary fund). Text labels must be clearly present; color highlight should be visually apparent if possible in DOCX.\n\nScoring (STRUCTURE ONLY):\n- 3.0: DOCX clearly named as specified; all four sections present; each section has date in header; all required screenshots embedded; itemized details present in each section; Total Costs table with two columns, required rows, and visible red/green highlight on Remaining Cost.\n- 2.5: DOCX with correct sections and screenshots, but one minor structural omission (e.g., date missing for one section OR color highlight missing but labels present).\n- 2.0: DOCX present but missing one major requirement (e.g., a section lacks its screenshot OR Total Costs table not clearly two-column by physician OR no dates on two sections).\n- 1.0: DOCX present but multiple major structural issues (e.g., several sections missing screenshots; unclear sectioning; no Total Costs structure).\n- 0.0: Not a DOCX, or missing multiple sections/evidence such that verification is impossible.\n\nReturn a single numeric score according to the rubric above.", "expectation": "A properly structured, evidence-rich DOCX with all required sections, dates, screenshots, and the Total Costs two-column layout enabling verification."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Verification (Calculations, Policy Constraints, Consistency)", "description": "Now that the structure is correct, verify correctness and internal consistency using a mix of code rules for deterministic checks and LLM judges for nuanced validation.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Registration Evidence and Pricing Consistency", "description": "Check that registration prices correspond to ACP member rates for ACP-IMM 2026 and the numbers in the summary match the screenshot values.", "weight": 1.6, "judge_prompt": "Evaluate the Registration section only.\nConfirm:\n- Screenshots clearly show ACP-IMM 2026 registration pricing for ACP members (both physicians are members).\n- The per-physician registration costs in the summary match the screenshot (within reasonable rounding).\n- A registration subtotal/total is shown and is consistent with the per-physician amounts.\nScoring:\n- 1.6: Clear member pricing evidence; per-physician and total amounts accurately match the screenshots.\n- 1.0: Mostly consistent but minor mismatch or unclear labeling (e.g., small rounding or ambiguous member category) with otherwise credible evidence.\n- 0.5: Weak evidence or partially missing details; prices present but not clearly tied to ACP-IMM 2026 member rates.\n- 0.0: No usable registration evidence or clear mismatch with screenshot values.", "expectation": "Accurate member-rate pricing with screenshot match and correct per-physician totals."}, {"type": "llm_judge", "name": "Travel Timeline and Ground Transport Completeness", "description": "Verify flight plans (economy) and ground transportation for both physicians, respecting Dr. Doe\u2019s schedule constraint, with coherent routing and costs.", "weight": 1.6, "judge_prompt": "Evaluate the Travel and Transportation section.\nConfirm:\n- Each physician has economy flight details with screenshots showing airline, dates, departure/arrival cities and times, and prices.\n- Ground transportation screenshots and costs are provided for airport-to-hotel on arrival and hotel-to-airport on departure for each physician.\n- Dr. Jane Doe is only staying for the first two days and has an obligation on April 18 at 3pm; the plan should accommodate a timely return or explicitly justify feasibility.\n- Per-physician and total travel/transportation costs are listed and consistent with screenshot values (within rounding tolerance).\nScoring:\n- 1.6: Complete and coherent flights plus ground legs for both physicians; schedule constraint addressed; prices align with screenshots.\n- 1.0: Mostly complete; minor gaps (e.g., one timing ambiguity or small price mismatch) but overall feasible.\n- 0.5: Significant omissions (e.g., one ground leg missing) or unclear feasibility for Dr. Doe\u2019s constraint.\n- 0.0: Inadequate details or mismatches preventing verification.", "expectation": "Complete economy flight and ground transport plans per physician, aligned with Dr. Doe\u2019s constraint."}, {"type": "llm_judge", "name": "Lodging Split, Taxes/Fees Inclusion, and Selection Rationale", "description": "Verify that a nearby 4/5-star hotel was chosen as the cheapest option with proper screenshot evidence, taxes/fees included, and cost split 3 vs 2 nights.", "weight": 1.6, "judge_prompt": "Evaluate the Lodging section.\nConfirm:\n- The document lists hotels within about a 3-block radius and identifies a 4- or 5-star option selected as the cheapest among them (reasonable claim with evidence).\n- There is a screenshot showing a 3-night total for proxy dates July 17\u201320, including all taxes/fees/surcharges.\n- The cost is split proportionally by nights: Dr. Smith 3 nights, Dr. Doe 2 nights; the per-physician lodging subtotals reflect this logic.\n- A total lodging cost is shown and is consistent with the screenshot (within rounding tolerance) and the split.\nScoring:\n- 1.6: Clear selection rationale and evidence; taxes/fees included; split math correct and totals consistent.\n- 1.0: Mostly correct but minor ambiguity (e.g., star rating not explicitly shown) or small numeric discrepancy.\n- 0.5: Evidence present but unclear selection rationale or taxes/fees not obviously included.\n- 0.0: No usable lodging evidence or split logic absent/incorrect.", "expectation": "Cheapest suitable 4/5-star option with taxes/fees included and correct 3:2 night split."}, {"type": "llm_judge", "name": "Total Roll-up, Funding Subtraction, and Visual Indicators", "description": "Verify the per-physician roll-ups, the $2k department funding subtraction, and correct red/green highlighting based on Remaining Cost sign.", "weight": 1.7, "judge_prompt": "Evaluate the Total Costs section.\nConfirm:\n- For each physician, Total Cost equals Registration + Flights + Ground Transportation + Lodging from the earlier sections (within rounding tolerance).\n- Subtract $2,000 department funding per physician to compute Remaining Cost.\n- If Remaining Cost is positive (department covers fully), visually highlight green; if negative (needs discretionary funds), highlight red.\n- Labels \"$2k Department Funding\", \"Total Cost\", and \"Remaining Cost\" are present under two columns for Dr. Smith and Dr. Doe.\nScoring:\n- 1.7: All math checks out; subtraction correct; appropriate color highlights; labels/columns clear.\n- 1.2: Minor rounding or formatting inconsistencies but overall correct.\n- 0.6: Some mismatches or missing highlight color; structure present but unreliable totals.\n- 0.0: No credible roll-up or funding subtraction visible.", "expectation": "Accurate, clearly labeled per-physician roll-up with correct funding subtraction and color-coding."}, {"type": "code", "name": "File Naming Compliance (.docx expected)", "description": "Checks that the primary output is a DOCX with an appropriate filename indicating \"2026 ACP-IMM Estimated Costs\".", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float or (float, str)\n    \"\"\"\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No primary output.\"\n    try:\n        p = context.files.get_path(output.id)\n        name = p.name.lower()\n    except Exception:\n        name = \"\"\n    score = 0.0\n    fb = []\n    if name.endswith('.docx'):\n        score += 0.2\n        fb.append(\"DOCX detected.\")\n    else:\n        # small partial if it's at least a document\n        try:\n            if getattr(output, 'is_document', False):\n                score += 0.1\n                fb.append(\"Document detected but not DOCX.\")\n            else:\n                fb.append(\"Not a document.\")\n        except Exception:\n            fb.append(\"Type unknown.\")\n    norm = name.replace('_',' ').replace('-',' ')\n    if '2026 acp imm estimated costs' in norm.replace('.', '').replace('  ', ' '):\n        score += 0.3\n        fb.append(\"Filename matches expected title.\")\n    elif 'acp' in norm and 'imm' in norm and 'estimated' in norm and 'cost' in norm:\n        score += 0.2\n        fb.append(\"Filename partially matches expected keywords.\")\n    return min(score, 0.5), \"; \".join(fb)"}, {"type": "code", "name": "Section Headers with Dates Present", "description": "Checks for presence of the four required section headers and a date in parentheses near each header.", "weight": 0.5, "code": "import re\n\ndef _read_text(context, output):\n    # Try DOCX, then PDF, then plain text\n    try:\n        return context.files.read_docx_text(output.id)\n    except Exception:\n        pass\n    try:\n        return context.files.read_pdf_text(output.id)\n    except Exception:\n        pass\n    try:\n        return context.files.read_text(output.id)\n    except Exception:\n        return \"\"\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output.\"\n    text = _read_text(context, output)\n    low = text.lower()\n    headers = [\n        'registration',\n        'travel and transportation',\n        'lodging',\n        'total costs'\n    ]\n    # count headers found\n    found = sum(1 for h in headers if h in low)\n    # date pattern in parentheses near header, e.g., (6/1/25) or (06/01/2025)\n    date_rx = re.compile(r\"(registration|travel and transportation|lodging|total costs)\\s*\\(([^)]{4,40})\\)\", re.I)\n    date_hits = len(date_rx.findall(text))\n    score = 0.0\n    fb = []\n    if found == 4:\n        score += 0.25\n        fb.append(\"All headers present.\")\n    else:\n        score += max(0, (found/4.0) * 0.25)\n        fb.append(f\"Headers found: {found}/4.\")\n    # Give up to 0.25 for dates (>=3 good hits full, else proportional)\n    if date_hits >= 3:\n        score += 0.25\n        fb.append(\"Dates present with headers.\")\n    elif date_hits > 0:\n        score += (date_hits/3.0) * 0.25\n        fb.append(f\"Dates present for {date_hits} header(s).\")\n    else:\n        fb.append(\"No header dates detected.\")\n    return min(score, 0.5), \"; \".join(fb)"}, {"type": "code", "name": "Funding and Remaining Cost Language", "description": "Checks that both physicians are named and that $2k department funding and Remaining Cost are mentioned.", "weight": 0.5, "code": "import re\n\ndef _read_text(context, output):\n    try:\n        return context.files.read_docx_text(output.id)\n    except Exception:\n        pass\n    try:\n        return context.files.read_pdf_text(output.id)\n    except Exception:\n        pass\n    try:\n        return context.files.read_text(output.id)\n    except Exception:\n        return \"\"\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output.\"\n    t = _read_text(context, output)\n    low = t.lower()\n    score = 0.0\n    fb = []\n    if ('dr. sarah smith' in low or 'dr. smith' in low) and ('dr. jane doe' in low or 'dr. doe' in low):\n        score += 0.2\n        fb.append(\"Both physicians named.\")\n    else:\n        fb.append(\"One or both physician names missing.\")\n    if 'remaining cost' in low:\n        score += 0.15\n        fb.append(\"'Remaining Cost' label present.\")\n    else:\n        fb.append(\"'Remaining Cost' label missing.\")\n    # $2k department funding variants\n    has_2k = any(s in low for s in [\n        '$2k', '2k department', '$2,000', '2000', '2,000'\n    ]) and ('department' in low or 'funding' in low)\n    if has_2k:\n        score += 0.15\n        fb.append(\"$2k department funding mentioned.\")\n    else:\n        fb.append(\"$2k department funding not detected.\")\n    return min(score, 0.5), \"; \".join(fb)"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3: Quality and Professionalism", "description": "Holistic assessment of presentation, clarity, traceability, and appropriateness for internal stakeholders.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Formatting and Readability", "description": "Assess overall organization, formatting, and readability suitable for a hospital department audience.", "weight": 1.5, "judge_prompt": "Evaluate the document\u2019s professional presentation:\n- Clear, consistent headings and typography; logical flow; appropriate use of lists/tables.\n- The Total Costs table is easy to scan by physician; numbers are formatted as currency; highlights are visually clear.\n- Minimal typos; consistent units and date formats.\nScoring:\n- 1.5: Highly professional, clean, and easy to read.\n- 1.0: Generally good with minor issues.\n- 0.5: Noticeable formatting or readability problems.\n- 0.0: Poorly formatted or confusing.", "expectation": "Professional, legible formatting with clear tables and currency formatting."}, {"type": "llm_judge", "name": "Evidence Traceability and Reproducibility", "description": "Evaluate whether the document enables re-checking the numbers later via dates, screenshots, and source cues.", "weight": 1.5, "judge_prompt": "Assess evidence traceability:\n- Each section header includes a date; if screenshot date differs from compilation date, both are stated.\n- Screenshots are legible and include source identifiers (page titles, provider names, or URLs if visible).\n- Enough detail is present to reproduce the search/booking (airline names, flight numbers or times, hotel names/addresses, ride providers/routes).\nScoring:\n- 1.5: Fully traceable and reproducible.\n- 1.0: Mostly traceable; minor gaps.\n- 0.5: Limited traceability.\n- 0.0: Weak or missing evidence.", "expectation": "Legible screenshots with dates and source cues sufficient to reproduce results."}, {"type": "llm_judge", "name": "Actionability for Booking", "description": "Assess whether the information is directly usable to proceed with bookings with minimal clarification.", "weight": 1.5, "judge_prompt": "Assess actionability for next steps:\n- Flights: clear airline, dates, cities, times, and prices for each physician; economy class indicated.\n- Ground transport: providers, approximate durations, and prices are clear per leg.\n- Lodging: hotel name, address, star rating, and booking total with taxes/fees included.\n- Any assumptions or alternatives are briefly noted.\nScoring:\n- 1.5: Immediately actionable.\n- 1.0: Mostly actionable; small clarifications needed.\n- 0.5: Significant gaps would delay booking.\n- 0.0: Not actionable.", "expectation": "Details sufficient for a secretary to book without additional research."}, {"type": "llm_judge", "name": "Instruction Compliance and Constraints", "description": "Evaluate adherence to all instructions, including shared room, 3 vs 2 night split, and department funding framing.", "weight": 1.5, "judge_prompt": "Assess compliance:\n- Room-sharing explicitly applied; lodging costs split 3 nights (Smith) vs 2 nights (Doe).\n- Dr. Doe\u2019s limited attendance (first two days) and April 18, 3pm obligation reflected in travel plan.\n- Department funding of $2,000 per physician shown; Remaining Cost highlighted correctly according to sign.\n- All taxes/fees/surcharges included in estimates.\nScoring:\n- 1.5: Fully compliant with clear references to constraints.\n- 1.0: Mostly compliant with minor omissions.\n- 0.5: Partial compliance with notable gaps.\n- 0.0: Non-compliant with key instructions.", "expectation": "Clear demonstration of all constraints and policy elements in the final document."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "e996036e-8287-4e7f-8d0a-90a57cb53c45", "rubric": {"category_name": "Wholesale Trade \u2013 First-Line Sales Supervisors: Terms Scenario Plan (Cosmetics \u2192 New Retail Account)", "rationale": "Pattern C (Mixed). The task requires an Excel-based financial scenario model plus a written executive summary and a comparative visual, all inside the same workbook. Stage 1 uses an LLM-only structural gate to enforce a very specific workbook shape that makes verification trivial. Stage 2 mixes lightweight code checks (bounds, formula sanity) with LLM cross-reference checks tied to the mandated structure. Stage 3 uses LLM for holistic quality: executive readiness, strategic rationale, and presentation/design.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2013 Structured Workbook Gate", "description": "LLM-only gate enforcing the exact Excel structure needed for verification. If shape is wrong, scoring for entire category is zeroed.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Workbook Shape and Required Sections", "description": "Verify the candidate produced an Excel scenario model with required sheets, tables, chart, and summary paragraph.", "weight": 4.0, "judge_prompt": "You are the gatekeeper for STRUCTURE ONLY (not correctness). Inspect the primary output. Confirm it is an Excel workbook with the following elements. Be flexible on exact sheet and section names, but strict about presence and structure.\n\nFormat requirements:\n- It must be an Excel workbook (not PDF/Word/CSV).\n- Content must be readable with multiple sheets and visible tables.\n\nRequired sheets/sections inside the workbook:\n1) Sheet named \"Scenario Analysis\" (or similar like \"Scenarios\", \"Scenario Model\") containing a primary comparison table with columns covering at least:\n   - Scenario (three scenarios)\n   - Retailer Margin (%)\n   - Payment Terms (Net 30 or Net 60)\n   - Marketing Allowance (% of shipments at retail value)\n   - Wholesale Revenue (derived from shipments and margin)\n   - Net Wholesale Revenue (after marketing allowance)\n   Also present on this sheet (or an adjacent/linked sheet): a visual chart comparing favorability/performance across the three scenarios (e.g., by Net Wholesale Revenue or similar KPI). The chart must clearly compare the scenarios.\n\n2) Sheet named \"Quarterly Projections\" (or similar like \"Quarterly Schedule\", \"Q1\u2013Q4 Plan\") with a table that includes:\n   - A Quarter dimension (Q1, Q2, Q3, Q4)\n   - Projected Retail Sales by quarter that total to $200,000 for Year 1\n   - Projected Shipments (Retail Value) by quarter that total to $225,000 for Year 1\n   Totals should be visible in this sheet.\n\n3) Sheet named \"Assumptions & Inputs\" (or similar) showing the key inputs used:\n   - Retailer margin default 40% with willingness up to 50%\n   - Payment terms options: Net 30 (default) and Net 60\n   - Marketing allowance up to 4% of shipments (retail value)\n   - MSRP adherence with retailer responsible for markdowns\n\n4) Sheet named \"Executive Summary\" or \"Summary\" that contains a written 5\u20136 sentence paragraph for leadership which:\n   - Selects a preferred scenario\n   - Explains how it balances profitability, company goals (brand awareness), and retailer cash flow concerns\n   - Notes considerations/compromises\n   This paragraph must be inside the Excel file (in a cell range or text box).\n\nScoring (Structure only, do not judge numerical correctness):\n- 4.0: All four elements present with clear tables, the scenario table includes all listed columns, a comparative chart is present, and the summary paragraph is in the workbook.\n- 3.0: One non-core item missing or noticeably incomplete (e.g., assumptions sheet light but present), but scenario table, quarterly projections with totals, chart, and summary paragraph are present.\n- 1.5: Multiple required elements missing (e.g., chart missing and summary missing), but some scenario table and quarterly table exist.\n- 0.0: Not an Excel workbook or the required structure is largely missing.\n\nOnly evaluate PRESENCE and STRUCTURE. Do not judge calculation accuracy or narrative quality.", "expectation": "A multi-sheet Excel model with a scenario table, quarterly projections, assumptions, a comparative chart, and a 5\u20136 sentence executive summary paragraph embedded in the workbook."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification: Numbers, Logic, and Cross-Consistency", "description": "Now that the structure is confirmed, verify correctness and internal consistency using a mix of code rules (deterministic checks) and LLM rules (reasoning/cross-reference).", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Quarterly Totals Match Assumptions", "description": "Check quarterly tables total to $200,000 Retail Sales and $225,000 Shipments (Retail Value).", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef _to_number(x):\n    if pd.isna(x):\n        return np.nan\n    s = str(x).strip().lower().replace(',', '')\n    pct = None\n    if s.endswith('%'):\n        try:\n            pct = float(s[:-1]) / 100.0\n        except:\n            pct = None\n    if pct is not None:\n        return pct\n    s = re.sub(r'[^0-9\\.-]', '', s)\n    try:\n        return float(s)\n    except:\n        return np.nan\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheet_candidates = [s for s in xls.sheet_names if re.search(r'quarter', s, re.I)]\n        if not sheet_candidates:\n            # fallback: search for any sheet that has a 'Quarter' column\n            sheet_candidates = xls.sheet_names\n        sales_ok = False\n        ship_ok = False\n        for s in sheet_candidates:\n            try:\n                df = pd.read_excel(path, sheet_name=s)\n            except Exception:\n                continue\n            # Try to identify columns\n            cols = [str(c).strip().lower() for c in df.columns]\n            df2 = df.copy()\n            for c in df2.columns:\n                df2[c] = df2[c].apply(_to_number)\n            # Find sales and shipments-like columns\n            sales_cols = [c for c in cols if ('sale' in c and 'retail' in c) or ('projected' in c and 'sale' in c)]\n            ship_cols = [c for c in cols if ('ship' in c and 'retail' in c) or ('projected' in c and 'ship' in c)]\n            # If not found, try looser matching\n            if not sales_cols:\n                sales_cols = [c for c in cols if 'sale' in c]\n            if not ship_cols:\n                ship_cols = [c for c in cols if 'ship' in c]\n            # Sum numeric values\n            if sales_cols:\n                total_sales = 0.0\n                for c in sales_cols:\n                    total_sales += pd.to_numeric(df2[c], errors='coerce').sum(skipna=True)\n                # If they included a total row, summing Q1-Q4 may be duplicated; try to detect if Q1..Q4 present\n                # Prefer sum over rows labeled Q1..Q4 if present\n                try:\n                    qmask = df.astype(str).applymap(lambda v: str(v).strip().upper() in ['Q1','Q2','Q3','Q4']).any(axis=1)\n                    if qmask.any():\n                        total_sales = pd.to_numeric(df2.loc[qmask, sales_cols].sum(axis=0), errors='coerce').sum()\n                except Exception:\n                    pass\n                if abs(total_sales - 200000.0) <= 0.02 * 200000.0:\n                    sales_ok = True\n            if ship_cols:\n                total_ship = 0.0\n                for c in ship_cols:\n                    total_ship += pd.to_numeric(df2[c], errors='coerce').sum(skipna=True)\n                try:\n                    qmask = df.astype(str).applymap(lambda v: str(v).strip().upper() in ['Q1','Q2','Q3','Q4']).any(axis=1)\n                    if qmask.any():\n                        total_ship = pd.to_numeric(df2.loc[qmask, ship_cols].sum(axis=0), errors='coerce').sum()\n                except Exception:\n                    pass\n                if abs(total_ship - 225000.0) <= 0.02 * 225000.0:\n                    ship_ok = True\n            if sales_ok and ship_ok:\n                break\n        score = 0.0\n        if sales_ok:\n            score += 0.25\n        if ship_ok:\n            score += 0.25\n        return score\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Scenario Inputs Within Allowed Ranges", "description": "Margins within 40\u201350%, payment terms only Net 30/Net 60, marketing allowance between 0\u20134%.", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef _to_number(x):\n    if pd.isna(x):\n        return np.nan\n    s = str(x).strip().lower().replace(',', '')\n    if s.endswith('%'):\n        try:\n            return float(s[:-1]) / 100.0\n        except:\n            return np.nan\n    s = re.sub(r'[^0-9\\.-]', '', s)\n    try:\n        return float(s)\n    except:\n        return np.nan\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        scenario_sheet = None\n        for s in xls.sheet_names:\n            if re.search(r'scenario', s, re.I):\n                scenario_sheet = s\n                break\n        if scenario_sheet is None:\n            return 0.0\n        df = pd.read_excel(path, sheet_name=scenario_sheet)\n        cols = [str(c).strip().lower() for c in df.columns]\n        colmap = {cname: idx for idx, cname in enumerate(cols)}\n        # Find likely columns\n        def find_col(keywords):\n            for i, c in enumerate(cols):\n                if all(k in c for k in keywords):\n                    return i\n            return None\n        margin_col = find_col(['margin'])\n        pay_col = find_col(['pay'])\n        mkt_col = find_col(['market']) or find_col(['allow'])\n        checks = 0\n        passed = 0\n        if margin_col is not None:\n            checks += 1\n            margins = df.iloc[:, margin_col].apply(_to_number).dropna()\n            if not margins.empty and margins.between(0.40, 0.50, inclusive='both').all():\n                passed += 1\n        if pay_col is not None:\n            checks += 1\n            terms = df.iloc[:, pay_col].astype(str).str.lower().str.replace(' ', '')\n            valid = terms.apply(lambda t: t in ['net30','net60']).all()\n            if valid:\n                passed += 1\n        if mkt_col is not None:\n            checks += 1\n            mkt = df.iloc[:, mkt_col].apply(_to_number).dropna()\n            if not mkt.empty and mkt.ge(0).all() and mkt.le(0.04).all():\n                passed += 1\n        if checks == 0:\n            return 0.0\n        return 0.5 * (passed / checks)\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Wholesale Revenue Formula Sanity", "description": "Wholesale Revenue \u2248 Shipments (Retail Value) \u00d7 (1 \u2212 Retailer Margin) for each scenario (tolerance 2%).", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef _to_number(x):\n    if pd.isna(x):\n        return np.nan\n    s = str(x).strip().lower().replace(',', '')\n    if s.endswith('%'):\n        try:\n            return float(s[:-1]) / 100.0\n        except:\n            return np.nan\n    s = re.sub(r'[^0-9\\.-]', '', s)\n    try:\n        return float(s)\n    except:\n        return np.nan\n\ndef _find_total_shipments(path):\n    try:\n        xls = pd.ExcelFile(path)\n        for s in xls.sheet_names:\n            if re.search(r'quarter', s, re.I):\n                dfq = pd.read_excel(path, sheet_name=s)\n                cols = [str(c).strip().lower() for c in dfq.columns]\n                df2 = dfq.copy()\n                for c in df2.columns:\n                    df2[c] = df2[c].apply(_to_number)\n                ship_cols = [c for c in cols if ('ship' in c and 'retail' in c) or ('projected' in c and 'ship' in c)]\n                if not ship_cols:\n                    ship_cols = [c for c in cols if 'ship' in c]\n                if ship_cols:\n                    # Prefer Q1..Q4 rows\n                    try:\n                        qmask = dfq.astype(str).applymap(lambda v: str(v).strip().upper() in ['Q1','Q2','Q3','Q4']).any(axis=1)\n                        if qmask.any():\n                            return pd.to_numeric(df2.loc[qmask, ship_cols].sum(axis=0), errors='coerce').sum()\n                    except Exception:\n                        pass\n                    return pd.to_numeric(df2[ship_cols], errors='coerce').sum(axis=None, skipna=True)\n    except Exception:\n        pass\n    return np.nan\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        total_ship = _find_total_shipments(path)\n        if not np.isfinite(total_ship) or total_ship <= 0:\n            return 0.0\n        xls = pd.ExcelFile(path)\n        scenario_sheet = None\n        for s in xls.sheet_names:\n            if re.search(r'scenario', s, re.I):\n                scenario_sheet = s\n                break\n        if scenario_sheet is None:\n            return 0.0\n        df = pd.read_excel(path, sheet_name=scenario_sheet)\n        cols = [str(c).strip().lower() for c in df.columns]\n        # locate columns\n        def find_col(matchers):\n            for i, c in enumerate(cols):\n                if all(m in c for m in matchers):\n                    return i\n            return None\n        margin_col = find_col(['margin'])\n        wr_col = find_col(['wholesale','revenue']) or find_col(['wholesale'])\n        if margin_col is None or wr_col is None:\n            return 0.0\n        margins = df.iloc[:, margin_col].apply(_to_number)\n        ws_revs = df.iloc[:, wr_col].apply(_to_number)\n        valid_rows = margins.notna() & ws_revs.notna()\n        if not valid_rows.any():\n            return 0.0\n        diffs = []\n        for m, wr in zip(margins[valid_rows], ws_revs[valid_rows]):\n            expected = total_ship * (1 - m)\n            if expected == 0:\n                diffs.append(np.inf)\n            else:\n                diffs.append(abs(wr - expected) / abs(expected))\n        if not diffs:\n            return 0.0\n        within = [d <= 0.02 for d in diffs if np.isfinite(d)]\n        if not within:\n            return 0.0\n        frac = sum(within) / len(within)\n        return 0.5 * frac\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Net Wholesale Revenue Includes Marketing Allowance", "description": "Net Wholesale Revenue \u2248 Wholesale Revenue \u2212 (Shipments \u00d7 Marketing Allowance %) for each scenario (tolerance 2%).", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef _to_number(x):\n    if pd.isna(x):\n        return np.nan\n    s = str(x).strip().lower().replace(',', '')\n    if s.endswith('%'):\n        try:\n            return float(s[:-1]) / 100.0\n        except:\n            return np.nan\n    s = re.sub(r'[^0-9\\.-]', '', s)\n    try:\n        return float(s)\n    except:\n        return np.nan\n\ndef _find_total_shipments(path):\n    try:\n        xls = pd.ExcelFile(path)\n        for s in xls.sheet_names:\n            if re.search(r'quarter', s, re.I):\n                dfq = pd.read_excel(path, sheet_name=s)\n                cols = [str(c).strip().lower() for c in dfq.columns]\n                df2 = dfq.copy()\n                for c in df2.columns:\n                    df2[c] = df2[c].apply(_to_number)\n                ship_cols = [c for c in cols if ('ship' in c and 'retail' in c) or ('projected' in c and 'ship' in c)]\n                if not ship_cols:\n                    ship_cols = [c for c in cols if 'ship' in c]\n                if ship_cols:\n                    try:\n                        qmask = dfq.astype(str).applymap(lambda v: str(v).strip().upper() in ['Q1','Q2','Q3','Q4']).any(axis=1)\n                        if qmask.any():\n                            return pd.to_numeric(df2.loc[qmask, ship_cols].sum(axis=0), errors='coerce').sum()\n                    except Exception:\n                        pass\n                    return pd.to_numeric(df2[ship_cols], errors='coerce').sum(axis=None, skipna=True)\n    except Exception:\n        pass\n    return np.nan\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        total_ship = _find_total_shipments(path)\n        if not np.isfinite(total_ship) or total_ship <= 0:\n            return 0.0\n        xls = pd.ExcelFile(path)\n        scenario_sheet = None\n        for s in xls.sheet_names:\n            if re.search(r'scenario', s, re.I):\n                scenario_sheet = s\n                break\n        if scenario_sheet is None:\n            return 0.0\n        df = pd.read_excel(path, sheet_name=scenario_sheet)\n        cols = [str(c).strip().lower() for c in df.columns]\n        def find_col(matchers):\n            for i, c in enumerate(cols):\n                if all(m in c for m in matchers):\n                    return i\n            return None\n        mkt_col = find_col(['market']) or find_col(['allow'])\n        wr_col = find_col(['wholesale','revenue']) or find_col(['wholesale'])\n        net_col = find_col(['net','wholesale','revenue']) or find_col(['net','revenue'])\n        if mkt_col is None or wr_col is None or net_col is None:\n            return 0.0\n        mkt = df.iloc[:, mkt_col].apply(_to_number)\n        wr = df.iloc[:, wr_col].apply(_to_number)\n        net = df.iloc[:, net_col].apply(_to_number)\n        valid_rows = mkt.notna() & wr.notna() & net.notna()\n        if not valid_rows.any():\n            return 0.0\n        diffs = []\n        for a, w, n in zip(mkt[valid_rows], wr[valid_rows], net[valid_rows]):\n            expected = w - total_ship * a\n            if expected == 0:\n                diffs.append(np.inf)\n            else:\n                diffs.append(abs(n - expected) / (abs(expected) if expected != 0 else 1.0))\n        within = [d <= 0.02 for d in diffs if np.isfinite(d)]\n        if not within:\n            return 0.0\n        frac = sum(within) / len(within)\n        return 0.5 * frac\n    except Exception:\n        return 0.0"}, {"type": "llm_judge", "name": "Cash Flow Timing Reflected by Payment Terms", "description": "Verify the model shows cash flow timing based on Net 30 vs Net 60 and ties receipts to shipment timing.", "weight": 2.67, "judge_prompt": "Check the workbook for clear cash flow timing that changes with payment terms:\n- There should be a schedule or depiction linking shipment timing (e.g., by quarter) to cash receipts, with Net 30 vs Net 60 visibly shifting when cash is received.\n- Look for labels like Cash Flow, AR aging, Receipts, or Collection schedule.\n- Receipts should align to shipment periods plus 30 or 60 days (or next quarter) in a plausible way.\n\nScoring:\n- 2.67: Clear cash flow schedule exists, explicitly sensitive to Net 30 vs Net 60, and aligns to shipment timing.\n- 1.33: Some cash flow depiction exists but the linkage to terms or quarters is unclear or partially shown.\n- 0.0: No cash flow timing or it ignores payment terms.", "expectation": "A visible per-period receipts schedule that meaningfully shifts with Net 30 vs Net 60."}, {"type": "llm_judge", "name": "Scenario Comparison Visual and KPI Alignment", "description": "Assess if the chart meaningfully compares the three scenarios and reflects the stated KPI(s) of favorability (e.g., Net Wholesale Revenue).", "weight": 2.67, "judge_prompt": "Evaluate the comparative chart in the workbook:\n- It should compare all three scenarios side-by-side.\n- The primary metric(s) used for the chart should reflect favorability, typically Net Wholesale Revenue (or similarly appropriate KPI).\n- Labels, axes, and legend should make it obvious which scenario is better according to the chosen KPI.\n\nScoring:\n- 2.67: Chart clearly compares all three scenarios; KPI is appropriate (e.g., Net Wholesale Revenue) and labeling is clear.\n- 1.33: Chart compares scenarios but KPI or labeling causes some ambiguity.\n- 0.0: Chart missing or does not compare scenarios meaningfully.", "expectation": "A clean bar/column chart (or similar) with clear labels comparing scenario results on Net Wholesale Revenue or similar."}, {"type": "llm_judge", "name": "Narrative Summary: Choice and Rationale", "description": "Check the 5\u20136 sentence executive summary: identifies the preferred scenario, balances profitability, brand awareness, and retailer concerns, and notes compromises.", "weight": 2.66, "judge_prompt": "Review the executive summary paragraph in the workbook:\n- It must be 5\u20136 sentences.\n- It must explicitly identify the selected scenario.\n- It must explain why the choice balances: (a) profitability for the brand, (b) company objectives like brand awareness, and (c) potential retailer concerns (cash flow, new expansion risk).\n- It should mention at least two variables among: retailer margin, payment terms, marketing allowance, and describe any compromises (e.g., higher retailer margin in exchange for marketing exposure).\n\nScoring:\n- 2.66: Meets all points clearly and specifically.\n- 1.33: Partially meets (e.g., identifies scenario but skimps on one dimension or variables).\n- 0.0: Missing, too short/long, or does not address the required dimensions.", "expectation": "A concise 5\u20136 sentence executive-ready paragraph naming the scenario and covering profitability, brand awareness leverage, retailer cash flow, and trade-offs."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Quality and Executive Readiness", "description": "Holistic assessment of professionalism, clarity, and strategic value for leadership.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Structure and Clarity", "description": "Evaluate workbook organization, labeling, and readability for an executive audience.", "weight": 1.5, "judge_prompt": "Judge professional presentation:\n- Sheets and sections are clearly named and logically ordered.\n- Tables have clear headers, currency/percentage formats, and no obvious clutter or hidden gotchas.\n- Inputs vs calculations vs outputs are distinguishable.\n\nScoring: 1.5 excellent; 0.75 adequate with minor issues; 0 poor.", "expectation": "Clean, well-labeled workbook that an executive can navigate quickly."}, {"type": "llm_judge", "name": "Strategic Insight and Risk Handling", "description": "Assess the strength of rationale regarding profitability, retailer cash flow risk, and brand awareness activations.", "weight": 1.5, "judge_prompt": "Evaluate strategic depth:\n- Does the narrative and model reflect understanding of the retailer\u2019s cash flow risks (Net 60 implications) and the brand\u2019s growth goal (awareness via retailer-led activations)?\n- Are trade-offs articulated (e.g., higher margin in exchange for stronger co-marketing, or Net 30 to mitigate retailer risk)?\n- Are recommendations grounded in the model\u2019s outputs?\n\nScoring: 1.5 strong and nuanced; 0.75 somewhat generic; 0 superficial.", "expectation": "A thoughtful recommendation balancing margin, terms, and marketing allowance with retailer partnership dynamics."}, {"type": "llm_judge", "name": "Actionability for Leadership", "description": "Determine if the output provides a clear decision and next steps.", "weight": 1.5, "judge_prompt": "For leadership actionability, check:\n- A clear preferred scenario and rationale.\n- Any conditions or thresholds (e.g., cap marketing allowance at X%, require Y-type retailer activations).\n- Suggested next steps or approval ask.\n\nScoring: 1.5 actionable and specific; 0.75 somewhat actionable; 0 not actionable.", "expectation": "A crisp recommendation with conditions/asks that leadership can approve."}, {"type": "llm_judge", "name": "Visual Communication Quality", "description": "Evaluate the effectiveness of the chart(s) and visual cues.", "weight": 1.5, "judge_prompt": "Assess visual clarity:\n- Chart choice fits the comparison (e.g., bar/column) and uses clear labels, units, and legend.\n- Colors and formatting aid interpretation; no misleading scales.\n- Visual aligns with the numeric table and narrative conclusion.\n\nScoring: 1.5 strong; 0.75 acceptable; 0 confusing or missing.", "expectation": "A polished chart that cleanly communicates scenario favorability without ambiguity."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "feb5eefc-39f1-4451-9ef9-bffe011b71dd", "rubric": {"category_name": "GRAT vs CRAT Comparative Analysis (Finance & Insurance \u2014 Personal Financial Advisor)", "rationale": "This rubric uses a self-documenting, staged approach. Stage 1 is a hard gate (LLM-only) that enforces a precise PDF structure so later verification is trivial. Stage 2 mixes deterministic code checks (lightweight, ~1/5 the weight of LLM) with LLM judges to verify correctness of mechanics, tax implications, scenario alignment with client facts (2015 law, $16M proceeds, age 62, married), and internal consistency. Stage 3 evaluates professional quality, clarity, and actionability for a client-facing deliverable. All resources are file-based; code rules read files via the provided context API.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (PDF Structure & Required Sections)", "description": "LLM-only gate to ensure the deliverable is a PDF (\u226412 pages) with the exact structure needed to enable verification.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Document Format and Structural Completeness (PDF, \u226412 pages, required sections)", "description": "Check format, page limit, and presence of all mandated sections/tables to make verification possible.", "weight": 4.0, "judge_prompt": "You are evaluating whether the submitted deliverable satisfies STRICT structure/format requirements. Examine the file you received.\n\nRequirements (STRUCTURE ONLY; do not judge quality or correctness):\n1) File format and length:\n   - Must be a PDF (not Word, not Excel). If not PDF, score 0.\n   - No more than 12 pages. Prefer at least 3 pages for professionalism.\n2) Required sections with clear headers (flexible naming allowed if obviously equivalent):\n   - Executive Summary\n   - Client Profile & Objectives (facts must match: 62-year-old, married, sold business for about $16,000,000; intent to reduce estate tax, benefit children, consider philanthropy)\n   - GRAT Overview (purpose/intent, key mechanics, how funded, typical duration/term)\n   - CRAT Overview (purpose/intent, key mechanics, how funded, typical duration/term)\n   - Tax Implications, Advantages, Disadvantages, and Risks (for both GRAT and CRAT)\n   - Scenario Illustration(s) for this client (at least one scenario per trust or a combined comparative scenario)\n   - Comparative Analysis (side-by-side comparison table of key dimensions like goals fit, tax effects, term, payout mechanics, remainder beneficiaries, risks)\n   - Recommendation (clear, professional conclusion naming which option best reduces estate tax exposure for the client \u2014 or neither \u2014 with rationale)\n   - Assumptions & Disclosures (disclaimer and key assumptions; the 2015 estate tax context should be shown here or nearby)\n3) Artifacts to enable verification:\n   - At least one side-by-side comparison table between GRAT and CRAT.\n   - A clearly labeled assumptions block listing the 2015 estate tax exemptions (~$5.43M individual, ~$10.86M married) and 40% estate tax rate (presence only; correctness will be judged later).\n\nScoring guidance (map to the rule\u2019s full weight):\n- Full credit: PDF format, \u226412 pages, and all listed sections + artifacts present.\n- Minor omissions (e.g., one subsection missing OR assumptions block present but not grouped under a header): award 75\u201390%.\n- Several omissions (e.g., missing comparison table or missing scenario illustrations): award 40\u201370%.\n- Wrong format (not PDF) or grossly incomplete (\u22642 pages or missing multiple core sections): 0\u201320%.\n\nOnly check presence/structure, not the accuracy of content.", "expectation": "A client-ready PDF (\u226412 pages) with all required sections, a comparison table, a scenario aligned to the client facts, and an assumptions/disclosures section."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness Verification (Mechanics, Tax, Scenario Consistency)", "description": "Verifies that content is factually grounded, aligned with 2015 rules, and applies to the client\u2019s facts. Mix of code (deterministic) and LLM (nuanced).", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Trusts Mentioned and Document Readability", "description": "Confirm the file is a readable document and that both GRAT and CRAT are explicitly mentioned.", "weight": 0.3, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Ensures we can extract text and both trust names appear.\n    Returns a score in [0,1]; framework scales by weight.\n    \"\"\"\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    if not (output.is_document or output.is_text_format):\n        return 0.0\n    text = ''\n    # Try PDF, then DOCX, then plaintext\n    for reader in (context.files.read_pdf_text, context.files.read_docx_text, context.files.read_text):\n        try:\n            text = reader(output.id)\n            if text:\n                break\n        except Exception:\n            continue\n    if not text or not text.strip():\n        return 0.0\n    t = text.lower()\n    found_grat = 'grat' in t or 'grantor retained annuity trust' in t\n    found_crat = 'crat' in t or 'charitable remainder annuity trust' in t\n    score = 0.0\n    score += 0.5 if found_grat else 0.0\n    score += 0.5 if found_crat else 0.0\n    return score"}, {"type": "code", "name": "Key Figures and Client Facts Present (2015 thresholds, rate, sale amount, age, marital status)", "description": "Check presence of essential numeric facts and client context: 2015 exemptions (~$5.43M/$10.86M), 40% estate tax rate, $16,000,000 sale proceeds, age 62, and married status.", "weight": 0.6, "code": "import re\n\ndef _any(pats, text):\n    return any(re.search(p, text, flags=re.I) for p in pats)\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Looks for: $16,000,000, age 62, married status, 2015, $5.43M, $10.86M, 40%.\n    Partial credit per item found. Returns ratio [0,1].\n    \"\"\"\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    if not (output.is_document or output.is_text_format):\n        return 0.0\n    text = ''\n    for reader in (context.files.read_pdf_text, context.files.read_docx_text, context.files.read_text):\n        try:\n            text = reader(output.id)\n            if text:\n                break\n        except Exception:\n            continue\n    if not text:\n        return 0.0\n    t = text\n    checks = 0\n    total = 7\n    # Sale amount ~$16,000,000\n    if _any([r\"\\$?16[, ]?0{6}\", r\"\\$?16\\,?000\\,?000\", r\"\\b16\\s*million\\b\"], t):\n        checks += 1\n    # Age 62\n    if _any([r\"\\bage\\s*:?\\s*62\\b\", r\"\\b62\\s*years? old\\b\", r\"\\b\\(62\\)\\b\"], t):\n        checks += 1\n    # Married status\n    if _any([r\"\\bmarried\\b\", r\"\\bspouse\\b\"], t):\n        checks += 1\n    # Year 2015 context\n    if _any([r\"\\b2015\\b\"], t):\n        checks += 1\n    # Individual exemption ~$5.43M\n    if _any([r\"5\\.?43\\s*m\", r\"\\$?5,?430,?000\"], t):\n        checks += 1\n    # Married exemption ~$10.86M\n    if _any([r\"10\\.?86\\s*m\", r\"\\$?10,?860,?000\"], t):\n        checks += 1\n    # Estate tax rate 40%\n    if _any([r\"40\\s*%\", r\"40\\s*percent\"], t):\n        checks += 1\n    return checks / float(total)"}, {"type": "code", "name": "Tax Topics Coverage Keywords", "description": "Verify the document touches on core tax topics likely relevant to GRAT/CRAT: estate tax, gift tax, income tax, charitable deduction, basis/step-up, and 7520 rate.", "weight": 0.3, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Keyword coverage check across critical tax topics: estate tax, gift tax, income tax,\n    charitable deduction, basis/step-up, 7520. Returns proportion found.\n    \"\"\"\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    if not (output.is_document or output.is_text_format):\n        return 0.0\n    text = ''\n    for reader in (context.files.read_pdf_text, context.files.read_docx_text, context.files.read_text):\n        try:\n            text = reader(output.id)\n            if text:\n                break\n        except Exception:\n            continue\n    if not text:\n        return 0.0\n    t = text.lower()\n    keywords = [\n        'estate tax',\n        'gift tax',\n        'income tax',\n        'charitable deduction',\n        'basis',  # includes step-up basis discussions\n        '7520'    # section 7520 rate reference\n    ]\n    found = sum(1 for k in keywords if k in t)\n    return found / len(keywords)"}, {"type": "llm_judge", "name": "Trust Mechanics Accuracy (Funding, Term, Annuity, Remainders)", "description": "Assess whether the document correctly explains how GRATs and CRATs work, including funding sources, annuity structure, typical durations, and beneficiaries.", "weight": 2.0, "judge_prompt": "Evaluate the accuracy of the explanations of GRAT and CRAT mechanics. Look for:\n- GRAT: irrevocable trust, grantor retains fixed annuity for a set term, valuation based on Section 7520 rate, potential to minimize gift via near zeroed-out GRAT, remainder to beneficiaries (children or trust for them), funding mechanics, investment performance vs hurdle rate, mortality risk if grantor dies during term.\n- CRAT: irrevocable trust, pays fixed annuity to non-charitable beneficiary (e.g., client/spouse) for life or term, remainder to charity, annuity percentage fixed at inception, must satisfy IRS tests (e.g., 10% remainder requirement; exhaustion/probability tests), valuation relies on Section 7520 rate, funding with appreciated assets allows gain deferral in trust, charitable deduction for present value of the remainder.\n- Include how each is funded and typical duration options.\nScoring guidance: Full credit if both trusts are described correctly and completely; reduce for material omissions (e.g., no mention of term, annuity nature, remainder structure, or valuation concepts).", "expectation": "Clear, materially correct descriptions of both GRAT and CRAT mechanics, including funding, term/annuity structure, and remainder beneficiaries."}, {"type": "llm_judge", "name": "Tax Implications and Risk Coverage", "description": "Assess correctness and completeness of the tax discussion and key risks.", "weight": 2.0, "judge_prompt": "Evaluate whether tax implications and risks are accurately discussed:\n- GRAT tax: gift tax valuation of remainder (zeroed-out potential), grantor-trust income tax treatment to the grantor during term, estate inclusion risk if death during term, impact of investment returns relative to the 7520 rate.\n- CRAT tax: charitable income tax deduction rules (PV of remainder, AGI limits in general terms), taxation of annuity payments under tier rules (conceptually ok), capital gains deferral inside CRAT, estate consequences for remainder going to charity (less for heirs), 10% remainder test, payout percentage constraints.\n- Risks: mortality risk (GRAT), performance risk (GRAT not clearing the hurdle), CRAT compliance tests and payout rigidity, legislative/interest rate sensitivity.\nScoring guidance: Full credit for accurate, balanced coverage; deduct for misunderstandings or missing material risks.", "expectation": "Accurate, balanced coverage of income, gift, and estate tax consequences plus key risks for both GRAT and CRAT."}, {"type": "llm_judge", "name": "Scenario Application to Client (2015 law, $16M, age 62, married)", "description": "Check that scenario(s) concretely apply to the client\u2019s facts and show estate tax reduction dynamics.", "weight": 1.4, "judge_prompt": "Assess whether the scenario(s):\n- Use the client\u2019s facts (age ~62, married, sold company for ~$16,000,000, 2015 law context) and reasonably illustrate how each trust functions.\n- Demonstrate directional impact on estate tax exposure for children (e.g., shifting appreciation with GRAT; charitable remainder with CRAT reducing taxable estate, trade-offs for heirs).\n- Are internally consistent (numbers, logic) even if simplified.\nScoring guidance: Full credit if tailored and consistent; partial if generic or missing key client facts; low if mismatched or internally inconsistent.", "expectation": "A tailored, internally consistent scenario for each trust (or a combined comparative scenario) that aligns to the client facts and demonstrates estate tax impact."}, {"type": "llm_judge", "name": "Recommendation Consistency With Objectives", "description": "Evaluate whether the final recommendation is clearly stated and follows from the analysis with the client\u2019s goals in mind.", "weight": 1.4, "judge_prompt": "Assess the recommendation:\n- Does it clearly choose GRAT, CRAT, a combination, or neither, and explicitly connect to the goal of reducing estate tax exposure for the client\u2019s children while acknowledging philanthropic preferences?\n- Is it consistent with the analysis of pros/cons and risks (e.g., GRAT mortality and performance risk; CRAT charitable remainder effects and payout constraints)?\n- Does it acknowledge client context (age 62, married, 2015 environment) and make sense under those conditions?\nScoring guidance: Full credit for a clear, defensible, and context-aware recommendation; partial for vague or weakly supported recommendations; low if contradictory to the analysis.", "expectation": "A clear, defensible recommendation tied to the client\u2019s goals and the comparative analysis presented."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism", "description": "Holistic assessment of presentation, clarity, strategic value, and actionability for a client-facing advisory report.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Compliance", "description": "Assess formatting, readability, and professional standards for a client deliverable.", "weight": 2.0, "judge_prompt": "Evaluate professional presentation: clear headings, logical flow, readable tables/figures, appropriate disclaimers, and sourcing/assumptions identified. The document should look like a client-ready PDF and comply with typical RIA/CFP standards (no promises of guaranteed outcomes). Score higher for polished, consistent formatting and properly labeled comparison tables.", "expectation": "Polished, well-structured, client-ready PDF with clear sections, readable tables, and appropriate disclaimers."}, {"type": "llm_judge", "name": "Clarity and Accessibility", "description": "Evaluate whether complex trust and tax concepts are explained clearly for a lay client.", "weight": 2.0, "judge_prompt": "Assess clarity for a non-expert reader: jargon defined (e.g., 7520 rate, annuity, remainder), acronyms expanded (GRAT/CRAT), concise explanations, and helpful summaries. Look for clear, digestible explanations and avoidance of unnecessary complexity.", "expectation": "Plain-English explanations with defined terms, good summaries, and logical progression."}, {"type": "llm_judge", "name": "Strategic Insight and Nuance", "description": "Judge the depth of strategic thinking beyond mechanics and taxes.", "weight": 2.0, "judge_prompt": "Evaluate whether the analysis shows insight beyond mechanics: addresses trade-offs between benefiting children vs. charity; flexibility; interest rate sensitivity; mortality/sequence-of-returns risk; interactions with broader estate plan (e.g., portability, spousal considerations, alternatives). Reward nuanced, balanced strategic framing that helps decision-making.", "expectation": "Insightful, balanced perspective that contextualizes GRAT vs CRAT within the client\u2019s broader goals and constraints."}, {"type": "llm_judge", "name": "Actionability and Next Steps", "description": "Assess whether the report provides concrete next steps for implementation.", "weight": 2.0, "judge_prompt": "Evaluate whether the deliverable includes clear next steps: decision criteria, information required (e.g., 7520 rate at funding, target annuity rate, desired term, charitable beneficiaries), timeline, coordination with estate attorney/CPA, and risk monitoring items. Score higher if it includes a succinct action plan/checklist.", "expectation": "Clear, prioritized next steps and coordination plan so the client knows how to proceed."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "5f6c57dd-feb6-4e70-b152-4969d92d1608", "rubric": {"category_name": "Finance & Insurance \u2014 Financial Managers \u2014 Standardized Branch/Regional Reporting Package (Excel)", "rationale": "This rubric enforces a self-documenting, highly structured Excel deliverable with five specific worksheets, dropdown-driven branch/All views, and verifiable tables for income statements, trends, rankings, regional comparisons, and KPIs. Stage 1 (LLM-only) strictly gates workbook structure so later checks are trivial. Stage 2 mixes lightweight code checks (bounds, arithmetic identity, fuzzy sheet detection) with heavier LLM consistency reviews. Stage 3 evaluates professional quality, usability, and executive readiness.", "max_total_score": 32.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (Workbook/Sheets/Structure)", "description": "Gate: Verify the candidate produced a single Excel workbook with exactly the required worksheets and within-sheet structures that enable verification. Do not assess calculation correctness.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Workbook and Sheet Presence", "description": "Confirm the output is a single Excel workbook with five distinct worksheets for the requested components. Flexible on exact sheet names, but each must map clearly to the requested function.", "weight": 4.0, "judge_prompt": "You are checking ONLY FORMAT/STRUCTURE (not correctness of numbers). Inspect the candidate output. Confirm it is a valid Excel workbook with FIVE separate worksheets corresponding to each required component. Flexible matching of sheet/tab names is allowed, but all five functional areas must be clearly identifiable.\n\nRequired 5 worksheets (names may vary, but functions must match):\n1) Income Statement Comparison (M23 vs M24 monthly, and FY2023 vs FY2024 with YoY variances)\n2) Monthly Trended Income Statement (2024 trend; include FY 2023 and FY 2024 full-year statements)\n3) Branch Ranking (10 branches ranked by: YoY % sales growth, 2024 ARPU, Sales $ per headcount, YoY % gross margin growth, YoY % order growth)\n4) Regional Comparison (Regions A\u2013G vs Revenue, SG&A, Allocations, EBITDA for 2023 and 2024)\n5) Metrics/KPI (Efficiency, Volume, Profitability metrics for 2024 monthly and yearly)\n\nScoring (0\u20134):\n- 4: Excel file with all 5 functions clearly present on separate sheets.\n- 3: Excel file with 4 of 5 present.\n- 2: Excel file with 3 of 5 present.\n- 1: Excel file with only 1\u20132 present.\n- 0: Not an Excel workbook, or structure unclear/missing most sheets.\nOnly check presence and mapping to functions; do not judge numeric correctness.", "expectation": "A single Excel workbook with five clearly identifiable worksheets matching the five requested components."}, {"type": "llm_judge", "name": "Within-Sheet Structural Requirements + Branch Selector Control", "description": "Check each sheet has required tables/columns/sections and a visible Branch/All selector (dropdown) to toggle branch vs aggregate views. Do not check correctness of values, only presence and structure.", "weight": 4.0, "judge_prompt": "Evaluate ONLY STRUCTURE and CONTROL PRESENCE. For each of the five worksheets, verify that the required sections/tables/columns are present and that a Branch/All selector (clearly labeled, with a visible dropdown-style control) exists on each sheet so users can pick a branch or an aggregate company view. Be flexible on exact naming, but the structural elements must be clear to a human reviewer.\n\nGeneral control requirement on ALL sheets:\n- A clearly labeled selector at the top (e.g., \"Branch:\" or \"Entity:\"), with a dropdown control and an option for an aggregate view (e.g., \"All Company\" or \"All Branches\"). The currently selected branch or All should be visible on the sheet.\n\nSheet-specific structural requirements:\n1) Income Statement Comparison:\n   - Rows grouped into canonical P&L buckets (e.g., Revenue, COGS, SG&A, EBITDA).\n   - Monthly comparison: columns for M23 and M24, plus MoM variance columns in dollars and percent.\n   - Annual comparison: columns for FY2023 and FY2024, plus YoY variance columns in dollars and percent.\n   - Variance columns are clearly labeled with $ and % indications.\n2) Monthly Trended Income Statement (2024):\n   - Columns covering M13\u2013M24 for 2024.\n   - Includes FY 2023 and FY 2024 full-year summaries (e.g., at right or as a separate totaled section).\n3) Branch Ranking (10 branches):\n   - Table with exactly 10 branch rows (or the top-10 clearly indicated) and columns for:\n     \u2022 YoY % sales growth\n     \u2022 2024 ARPU\n     \u2022 Sales $ per headcount\n     \u2022 YoY % gross margin growth (gross margin = revenue \u2212 COGS)\n     \u2022 YoY % order growth\n   - A rank column (1\u201310) or separate ranking columns per metric.\n4) Regional Comparison (Regions A\u2013G):\n   - Region rows A through G.\n   - Columns for both 2023 and 2024 showing Revenue, SG&A, Allocations, and EBITDA.\n5) 2024 Metrics/KPI Sheet (monthly and yearly):\n   - Efficiency: (1) Implementation Headcount Hours per Implementation Headcount, (2) Revenue per Direct Labor Headcount, (3) Revenue per Sales Headcount, (4) Expenses per Total Headcount.\n   - Volume: (1) Backlog Turn Rate % (Revenue Units \u00f7 Project Backlog Units), (2) Backlog Days (Project Backlog Units \u00f7 Units Closed per Day).\n   - Profitability: (1) ARPU, (2) COGS per Rev Unit, (3) EBITDA as % of Total Revenue.\n   - Each metric reported monthly for 2024 and a 2024 full-year value.\n\nScoring (0\u20134):\n- 4: All sheets contain the specified structures AND a visible Branch/All selector.\n- 3: Minor omissions (e.g., one variance column missing, or one sheet lacks the visible selector) but overall structure is in place.\n- 2: Multiple structural gaps across sheets but at least half of requirements are visible.\n- 1: Only a few structural elements are present; selectors mostly missing.\n- 0: Lacks most required structures and/or no visible selectors.\nDo NOT check the arithmetic; only presence and labeling of elements.", "expectation": "Each sheet shows the expected tables/columns and includes a visible Branch/All dropdown-style selector."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness and Consistency)", "description": "Verify core logic and plausibility based on the enforced shape. Use code for deterministic checks and LLM for nuanced cross-checks. Mixed rules with heavier weight on LLM judges.", "is_required": false, "max_points": 16.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Workbook Sheet Detection (Fuzzy)", "description": "Detects required sheets by fuzzy name matching to ensure the intended structure is present. Grants partial credit if most are found.", "weight": 1.2, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output detected.\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheets = [s for s in xls.sheet_names]\n        sheets_lower = [s.lower() for s in sheets]\n        # Expected functional areas with keyword options (flexible)\n        expectations = {\n            'income': ['income', 'p&l', 'profit', 'statement'],\n            'trended': ['trend', 'trended', 'monthly'],\n            'ranking': ['rank', 'ranking', 'leaderboard'],\n            'regional': ['region', 'regional'],\n            'metrics': ['metric', 'dashboard', 'kpi']\n        }\n        found = {}\n        for key, kws in expectations.items():\n            found[key] = any(any(kw in s for kw in kws) for s in sheets_lower)\n        found_count = sum(1 for v in found.values() if v)\n        score = (found_count / 5.0) * 1.2\n        missing = [k for k,v in found.items() if not v]\n        fb = f\"Sheets found: {found_count}/5. Missing functional areas: {', '.join(missing) if missing else 'None'}. Sheets: {sheets}\"\n        return float(score), fb\n    except Exception as e:\n        return 0.0, f\"Error reading Excel: {e}\""}, {"type": "code", "name": "Regional EBITDA Arithmetic Identity", "description": "On the Regional Comparison sheet, verify EBITDA \u2248 Revenue \u2212 SG&A \u2212 Allocations within tolerance on available rows.", "weight": 0.8, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef _find_sheet_name(sheet_names):\n    names_lower = [s.lower() for s in sheet_names]\n    for i, s in enumerate(names_lower):\n        if 'region' in s:\n            return sheet_names[i]\n    return None\n\ndef _first_match(cols_lower, predicates):\n    # predicates: list of lambdas that take (col_lower) -> bool; return first matching index\n    for i, c in enumerate(cols_lower):\n        if all(pred(c) for pred in predicates):\n            return i\n    return None\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheet = _find_sheet_name(xls.sheet_names)\n        if not sheet:\n            return 0.0, \"Regional sheet not found by fuzzy match.\"\n        df = context.files.read_excel(output.id, sheet_name=sheet)\n        if df is None or df.empty:\n            return 0.0, \"Regional sheet is empty.\"\n        # Normalize columns\n        cols = [str(c) for c in df.columns]\n        cols_lower = [c.lower() for c in cols]\n        def has(substr):\n            return lambda c: substr in c\n        # Try to find columns (allow variants and year suffixes)\n        # Prefer columns containing explicit measure names regardless of year tokens\n        idx_rev = None\n        idx_sga = None\n        idx_alloc = None\n        idx_ebitda = None\n        candidates = list(range(len(cols_lower)))\n        def pick_first(*terms):\n            for i, c in enumerate(cols_lower):\n                if all(t in c for t in terms):\n                    return i\n            return None\n        # broader match\n        for i,c in enumerate(cols_lower):\n            if idx_rev is None and 'revenue' in c and 'per' not in c:\n                idx_rev = i\n            if idx_sga is None and ('sg&a' in c or 'sga' in c or 'sg and a' in c):\n                idx_sga = i\n            if idx_alloc is None and 'alloc' in c:\n                idx_alloc = i\n            if idx_ebitda is None and 'ebitda' in c:\n                idx_ebitda = i\n        required_idxs = [idx_rev, idx_sga, idx_alloc, idx_ebitda]\n        if any(i is None for i in required_idxs):\n            return 0.0, f\"Missing one or more columns (rev:{idx_rev}, sga:{idx_sga}, alloc:{idx_alloc}, ebitda:{idx_ebitda}).\"\n        # Coerce numerics\n        rev = pd.to_numeric(df.iloc[:, idx_rev], errors='coerce')\n        sga = pd.to_numeric(df.iloc[:, idx_sga], errors='coerce')\n        alloc = pd.to_numeric(df.iloc[:, idx_alloc], errors='coerce')\n        ebitda = pd.to_numeric(df.iloc[:, idx_ebitda], errors='coerce')\n        mask = rev.notna() & sga.notna() & alloc.notna() & ebitda.notna()\n        if mask.sum() == 0:\n            return 0.0, \"No comparable numeric rows to validate EBITDA identity.\"\n        lhs = ebitda[mask].values\n        rhs = (rev[mask] - sga[mask] - alloc[mask]).values\n        # tolerance: 1% of |rev| or absolute 1e-6\n        tol = np.maximum(0.01 * np.abs(rev[mask].values), 1e-6)\n        ok = np.abs(lhs - rhs) <= tol\n        prop_ok = float(np.mean(ok)) if ok.size > 0 else 0.0\n        score = prop_ok * 0.8\n        fb = f\"Validated {int(mask.sum())} rows. Proportion within tolerance: {prop_ok:.2f}.\"\n        return score, fb\n    except Exception as e:\n        return 0.0, f\"Error validating EBITDA identity: {e}\""}, {"type": "code", "name": "KPI Metrics Sanity Bounds (2024)", "description": "On the Metrics/KPI sheet, verify that key metrics are within plausible ranges (e.g., EBITDA %, ARPU, Backlog metrics, revenue/expense per headcount). Partial credit by proportion within bounds.", "weight": 1.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\nMETRIC_DEFS = [\n    {\"name\":\"EBITDA %\", \"must_contain\":[\"ebitda\"], \"also\":[\"%\",\"margin\"], \"lower\":-1.0, \"upper\":1.0},\n    {\"name\":\"ARPU\", \"must_contain\":[\"arpu\"], \"also\":[], \"lower\":0.0, \"upper\":1e9},\n    {\"name\":\"COGS per Rev Unit\", \"must_contain\":[\"cogs\",\"per\",\"unit\"], \"also\":[], \"lower\":0.0, \"upper\":1e9},\n    {\"name\":\"Backlog Turn Rate\", \"must_contain\":[\"backlog\",\"turn\"], \"also\":[], \"lower\":0.0, \"upper\":200.0},\n    {\"name\":\"Backlog Days\", \"must_contain\":[\"backlog\",\"day\"], \"also\":[], \"lower\":0.0, \"upper\":10000.0},\n    {\"name\":\"Expenses per Total Headcount\", \"must_contain\":[\"expense\",\"per\",\"head\"], \"also\":[], \"lower\":0.0, \"upper\":1e7},\n    {\"name\":\"Revenue per Direct Labor Headcount\", \"must_contain\":[\"revenue\",\"per\",\"direct\"], \"also\":[\"head\"], \"lower\":0.0, \"upper\":1e8},\n    {\"name\":\"Revenue per Sales Headcount\", \"must_contain\":[\"revenue\",\"per\",\"sales\"], \"also\":[\"head\"], \"lower\":0.0, \"upper\":1e8},\n    {\"name\":\"Impl Hours per Impl Headcount\", \"must_contain\":[\"hour\",\"per\",\"impl\"], \"also\":[\"head\"], \"lower\":0.0, \"upper\":10000.0},\n]\n\ndef _find_metrics(cols_lower):\n    mapping = {}\n    for m in METRIC_DEFS:\n        indices = []\n        for i, c in enumerate(cols_lower):\n            if all(term in c for term in m['must_contain']) and all(extra in c for extra in m.get('also', [])):\n                indices.append(i)\n        mapping[m['name']] = indices\n    return mapping\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        # find metrics sheet\n        sheet = None\n        for s in xls.sheet_names:\n            sl = s.lower()\n            if 'metric' in sl or 'kpi' in sl or 'dashboard' in sl:\n                sheet = s\n                break\n        if not sheet:\n            return 0.0, \"Metrics/KPI sheet not found.\"\n        df = context.files.read_excel(output.id, sheet_name=sheet)\n        if df is None or df.empty:\n            return 0.0, \"Metrics sheet empty.\"\n        cols = [str(c) for c in df.columns]\n        cols_lower = [c.lower() for c in cols]\n        mapping = _find_metrics(cols_lower)\n        total_checks = 0\n        ok_checks = 0\n        details = []\n        for m in METRIC_DEFS:\n            idxs = mapping.get(m['name'], [])\n            if not idxs:\n                details.append(f\"Missing metric cols for {m['name']}\")\n                continue\n            for idx in idxs:\n                series = pd.to_numeric(df.iloc[:, idx], errors='coerce').dropna()\n                if series.empty:\n                    continue\n                vals = series.values\n                lower = m['lower']\n                upper = m['upper']\n                within = np.logical_and(vals >= lower, vals <= upper)\n                prop = float(np.mean(within)) if within.size > 0 else 0.0\n                ok_checks += prop\n                total_checks += 1.0\n                details.append(f\"{m['name']} col '{cols[idx]}' prop_in_bounds={prop:.2f}\")\n        if total_checks == 0:\n            return 0.0, \"No recognizable KPI columns found.\"\n        score = (ok_checks / total_checks) * 1.0\n        fb = \"; \".join(details)\n        return float(score), fb\n    except Exception as e:\n        return 0.0, f\"Error validating KPI bounds: {e}\""}, {"type": "llm_judge", "name": "Income Statement Variance Sign and Label Consistency (Spot Checks)", "description": "Visually verify that MoM $/% variances exist on M23 vs M24, YoY $/% exist on FY2023 vs FY2024, and that increases in revenue are shown as positive while increases in COGS/SG&A are shown as negative. Spot-check a few rows for reasonableness and number formatting (percent displayed as %).", "weight": 5.0, "judge_prompt": "Inspect the Income Statement Comparison sheet. Perform spot checks for consistency and formatting:\n- Confirm columns for M23 and M24 exist (2024 months) and that MoM variance columns exist in both $ and % with clear labels.\n- Confirm columns for FY2023 and FY2024 exist and that YoY variance columns exist in both $ and %.\n- Spot-check 2\u20133 lines: when M24 > M23 for a revenue line, the MoM $ variance shows positive; when M24 > M23 for an expense line (COGS or SG&A), the MoM $ variance shows negative. Percent variances should use percent formatting.\n- Spot-check that YoY variance sign conventions follow the same rule for revenue vs expenses.\n\nScoring (0\u20135):\n- 5: All variance columns present and labeled; spot checks consistent for both revenue and expenses; % formatted.\n- 3\u20134: Minor label/format issues or one inconsistent line, overall correct.\n- 1\u20132: Several inconsistencies or missing either $ or % variances.\n- 0: Variance structure missing or signs broadly incorrect.", "expectation": "Clearly labeled MoM and YoY $/% variances with correct sign conventions and percent formats."}, {"type": "llm_judge", "name": "Branch Ranking Completeness and Plausibility", "description": "Verify that the Branch Ranking sheet shows 10 branches with ranks and the five required metrics, with plausible value formats (%, $, per-head figures).", "weight": 4.0, "judge_prompt": "Inspect the Branch Ranking sheet:\n- Confirm there are 10 branch entries (or a clearly labeled top-10) and a ranking column 1\u201310 or equivalent ranking indicators.\n- Columns for the five metrics must be present: (1) YoY % sales growth, (2) 2024 ARPU, (3) Sales $ per headcount, (4) YoY % gross margin growth (gross margin = revenue \u2212 COGS), (5) YoY % order growth.\n- Check that percentages are formatted as %, dollar values as currency or standard numeric.\n- Spot-check plausibility: ARPU should be positive; growth rates typically between \u2212100% and +500% unless justified.\n\nScoring (0\u20134):\n- 4: All metrics present for 10 branches with ranks; formats look correct; values plausible.\n- 3: Minor format or counting issues but overall correct.\n- 1\u20132: Missing metrics or unclear ranking; several implausible values.\n- 0: Not a valid ranking table or missing most metrics.", "expectation": "A complete 10-branch ranking table with the five metrics, ranks, and sensible formatting/values."}, {"type": "llm_judge", "name": "Regional Comparison and EBITDA Identity (Visual)", "description": "Visually verify the regional table includes Regions A\u2013G and for both 2023 and 2024 shows Revenue, SG&A, Allocations, and EBITDA with EBITDA consistent with Revenue \u2212 SG&A \u2212 Allocations in a couple of examples.", "weight": 4.0, "judge_prompt": "Inspect the Regional Comparison sheet:\n- Confirm regions A through G are present as rows.\n- Confirm both years 2023 and 2024 are represented with columns for Revenue, SG&A, Allocations, and EBITDA.\n- Visually pick 1\u20132 lines and verify that EBITDA looks consistent with Revenue \u2212 SG&A \u2212 Allocations (within rounding).\n\nScoring (0\u20134):\n- 4: All regions present, both years shown, and EBITDA arithmetic appears consistent on spot checks.\n- 3: Minor omissions/format issues but generally correct.\n- 1\u20132: Missing a year or measure; arithmetic looks off.\n- 0: Not a valid regional comparison as specified.", "expectation": "Regions A\u2013G with two-year measures and visibly consistent EBITDA arithmetic."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism", "description": "Holistic assessment of presentation quality, usability, documentation, and executive readiness. Focus on clarity, formatting, and scalability, not just correctness.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Design, Layout, and Usability", "description": "Assesses professional formatting, frozen headers, consistent number formats ($, %, units), readable fonts, and clear labeling of selectors. Navigation ease across sheets.", "weight": 2.0, "judge_prompt": "Evaluate the workbook\u2019s professional polish and usability:\n- Consistent fonts, clear headers, frozen panes for large tables.\n- Correct numeric formats applied (currency for $, percent for % metrics, integers where appropriate).\n- Branch/All selector is clearly labeled and placed consistently across sheets.\n- Ease of navigation and readability for senior management.\nScoring (0\u20132): 2 = polished and easy to use; 1 = acceptable but with minor issues; 0 = cluttered/unclear formatting.", "expectation": "Clean, consistent formatting with correct number formats and easy usability."}, {"type": "llm_judge", "name": "Executive Readiness and Comparability", "description": "Evaluates whether the package enables rapid comparison across branches/regions/years and highlights what matters to executives.", "weight": 2.0, "judge_prompt": "Judge how executive-ready this package is:\n- Do the views make cross-branch/region and cross-year comparisons obvious (e.g., clear totals, YoY summaries, and ranks)?\n- Are the key KPIs surfaced prominently and grouped logically (Efficiency, Volume, Profitability)?\n- Are totals/subtotals clearly labeled and easy to find?\nScoring (0\u20132): 2 = highly executive-friendly; 1 = fair; 0 = hard to compare or interpret quickly.", "expectation": "A package that supports quick executive comparisons and decision-making."}, {"type": "llm_judge", "name": "Documentation and Instructions", "description": "Checks for a brief instructions/notes area or sheet describing how to use the dropdowns, data refresh, and any assumptions. Not strictly required but improves maintainability.", "weight": 2.0, "judge_prompt": "Look for brief documentation (an Instructions/Notes sheet or clearly labeled notes) explaining:\n- How to use the Branch/All selector and any other controls.\n- What time periods M1\u2013M24 represent (M1\u2013M12 = 2023, M13\u2013M24 = 2024).\n- Any assumptions or definitions (e.g., ARPU, Gross Margin = Revenue \u2212 COGS).\nScoring (0\u20132): 2 = clear instructions/notes; 1 = minimal hints; 0 = no documentation.", "expectation": "Concise instructions/notes improving user adoption and clarity."}, {"type": "llm_judge", "name": "Consistency and Maintainability", "description": "Assesses consistent naming conventions, repeatable formulas (no hard-coded totals where formulas should be), and model scalability.", "weight": 2.0, "judge_prompt": "Evaluate consistency and maintainability:\n- Consistent naming of sheets, columns, and measures across the workbook.\n- Reusable formulas instead of hard-coded values where calculations are expected.\n- Structure that would scale to more branches/regions without redesign.\nScoring (0\u20132): 2 = consistent and maintainable; 1 = some inconsistencies; 0 = brittle or ad hoc design.", "expectation": "A consistent, formula-driven workbook that scales and is easy to maintain."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "0353ee0c-18b5-4ad3-88e8-e001d223e1d7", "rubric": {"category_name": "PACT Act Presumptives Consolidated Guide (Veteran-Facing PDF)", "rationale": "This is a document-heavy task (Pattern B). The rubric enforces a strict, verifiable document shape first (Stage 1, LLM-only), then verifies correctness and traceability via mixed LLM and light code checks (Stage 2), and finally assesses overall usability and professional quality for the veteran audience (Stage 3). Stage 2 weights favor LLM judgment, with code rules providing precise, deterministic validations (e.g., hyperlink density, key header presence).", "max_total_score": 30.0, "stages": [{"name": "Stage 1 \u2013 Format and Structure Gate", "description": "Gate that enforces an exact, verifiable document structure enabling later verification. LLM-only per guidelines.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Document Format, Length, and Front Matter", "description": "Checks that the candidate output is a professional PDF/DOCX with essential front-matter that enables verification.", "weight": 3.0, "judge_prompt": "You are validating the output document's format and front matter. Check these strictly, but be flexible with similar wording:\n\nFormat requirements:\n- File is a PDF or DOCX (not spreadsheet, not plain text)\n- Length \u2265 8 pages\n- Professional formatting (clear headings, page numbers, consistent styles)\n\nFront-matter requirements:\n- Cover page with a clear title referencing PACT Act, presumptive exposures/conditions, and veterans (e.g., \u201cPACT Act Presumptive Conditions and Exposures \u2013 Veteran Guide\u201d)\n- A visible date or version label (e.g., \u201cUpdated: Month YYYY\u201d or \u201cVersion X.Y\u201d)\n- Table of Contents (TOC)\n- \u201cHow to Use This Guide\u201d or similar section describing navigation and purpose\n\nScoring:\n- 3.0: All format + all front-matter present\n- 2.0: Valid format + length ok + 3/4 front-matter items\n- 1.0: Valid format + length ok + 2/4 front-matter items\n- 0.5: Valid format but length < 8 pages OR only 1 front-matter item\n- 0.0: Not PDF/DOCX or unprofessional single-page/fragment with no meaningful front-matter\n\nOnly check presence/format; do not judge content quality.", "expectation": "A professional PDF/DOCX with cover, date/version, TOC, and How-to-Use section."}, {"type": "llm_judge", "name": "Required Sections, Tables, and Appendices Present", "description": "Checks for the mandated structural sections, catalogs, and appendices that enable downstream verification.", "weight": 5.0, "judge_prompt": "Check if the document contains ALL of the following structural elements. Be flexible with naming, but the section intent must be clear. Look for headers, tables, and appendices. Do not assess content accuracy here\u2014only presence and structure.\n\nCore sections required:\n1) Executive Summary or Overview (veteran-facing purpose)\n2) Eligibility Overview & Key Concepts (explains \u201cpresumptive,\u201d PACT Act scope)\n3) Presumptive Exposures Catalog (grouped categories, e.g., Agent Orange, burn pits, radiation, Gulf War-related, water contamination); for each category: a subtable that lists eligible service locations and service dates\n4) Presumptive Conditions Catalog split into:\n   4a) Cancer Conditions (table)\n   4b) Non-Cancer Conditions (table)\n   Each table should have columns or clearly labeled information for: Condition, Exposure Category, Eligible Service Location(s), Eligible Service Date(s), Notes/Footnotes, Source/Citation\n5) Service Location/Date Matrix or Mapping that cross-references exposures \u2192 locations \u2192 dates\n6) Quick Lookup Index (e.g., alphabetical index of conditions or exposures)\n7) Sources/References section with a compiled list of URLs or citations (expect many links; the clinic\u2019s prior sheet had 19 links)\n8) Revision History & Versioning (e.g., version log or update notes)\n\nAppendices required:\nA) Appendix A \u2013 Master Crosswalk Table combining Exposures, Conditions, Eligible Service Location(s), Eligible Service Date(s), Branch/Theater (if applicable), Notes, Source URL\nB) Appendix B \u2013 Glossary/Definitions\nC) Appendix C \u2013 Acronyms\n\nScoring:\n- 5.0: All core sections (1\u20138) and all appendices (A\u2013C) present with tables/labels where specified\n- 4.0: Missing exactly one item (from core or appendices)\n- 3.0: Missing two items\n- 2.0: Missing three items\n- 1.0: Valid doc but only a subset (\u2264 half) of the above present\n- 0.0: Largely missing structure (wrong format, or only 1\u20132 sections with no appendices)\n\nOnly check structure and presence, not correctness.", "expectation": "All listed sections and appendices exist, with identifiable headers and tables enabling verification."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification of Completeness and Traceability", "description": "Now verify that the document is comprehensive, cross-referenced, and traceable to sources without inventing new information.", "is_required": true, "max_points": 12.0, "min_score_to_pass": 6.0, "rules": [{"type": "code", "name": "Hyperlink Density and Source Footprint", "description": "Deterministically check that the document contains a substantial number of URLs (indicating traceability to the reference links).", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Returns a score in [0, 1] based on number of unique URLs found.\n    \"\"\"\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output resource.\"\n    if not output.is_document and not output.is_text_format:\n        return 0.0, \"Output is not a document/text format.\"\n\n    text = \"\"\n    try:\n        if output.is_document:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                text = \"\"\n            if not text:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    text = \"\"\n        if not text and output.is_text_format:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    except Exception:\n        text = \"\"\n\n    if not text:\n        return 0.0, \"Could not extract text from document.\"\n\n    urls = set(re.findall(r\"https?://[^\\s)\\]>\\]]+\", text))\n    n = len(urls)\n\n    # Scoring tiers: encourage >=19 unique links\n    if n >= 19:\n        score = 1.0\n    elif n >= 10:\n        score = 0.6\n    elif n >= 5:\n        score = 0.3\n    else:\n        score = 0.0\n\n    return score, f\"Found {n} unique URLs.\""}, {"type": "code", "name": "Master Crosswalk Columns Presence (Appendix A)", "description": "Check that Appendix A (or equivalent) exists and that expected column/header keywords appear somewhere in the document text.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Returns a score in [0, 1] based on presence of Appendix A and key header terms.\n    \"\"\"\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output resource.\"\n    if not output.is_document and not output.is_text_format:\n        return 0.0, \"Output is not a document/text format.\"\n\n    text = \"\"\n    try:\n        if output.is_document:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                text = \"\"\n            if not text:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    text = \"\"\n        if not text and output.is_text_format:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    except Exception:\n        text = \"\"\n\n    if not text:\n        return 0.0, \"Could not extract text from document.\"\n\n    lt = text.lower()\n    has_appendix = (\"appendix a\" in lt)\n\n    # Look for key header-like terms that would label the master table\n    keys = [\"condition\", \"exposure\", \"toxic exposure\", \"service location\", \"location\", \"service date\", \"dates\", \"branch\", \"theater\", \"notes\", \"source\", \"url\", \"link\"]\n    hits = sum(1 for k in keys if k in lt)\n\n    # Scoring: require Appendix A mention and at least 6 distinct header keywords\n    if has_appendix and hits >= 8:\n        score = 1.0\n    elif has_appendix and hits >= 6:\n        score = 0.7\n    elif has_appendix and hits >= 4:\n        score = 0.4\n    elif hits >= 6:\n        score = 0.3\n    else:\n        score = 0.0\n\n    return score, f\"Appendix A: {has_appendix}; header keyword hits: {hits}.\""}, {"type": "code", "name": "Cancer vs. Non-Cancer Conditions Sections", "description": "Verify both cancer and non-cancer presumptive condition sections are present (by keyword heuristics).", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Returns a score in [0, 1] indicating presence of cancer and non-cancer sections.\n    \"\"\"\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output resource.\"\n    if not output.is_document and not output.is_text_format:\n        return 0.0, \"Output is not a document/text format.\"\n\n    text = \"\"\n    try:\n        if output.is_document:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                text = \"\"\n            if not text:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    text = \"\"\n        if not text and output.is_text_format:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    except Exception:\n        text = \"\"\n\n    if not text:\n        return 0.0, \"Could not extract text from document.\"\n\n    lt = text.lower()\n    has_cancer = (\"cancer\" in lt)\n    has_non_cancer = (\"non-cancer\" in lt) or (\"noncancer\" in lt) or (\"non cancer\" in lt)\n\n    if has_cancer and has_non_cancer:\n        return 1.0, \"Cancer and non-cancer both detected.\"\n    elif has_cancer or has_non_cancer:\n        return 0.5, \"Only one of cancer/non-cancer detected.\"\n    else:\n        return 0.0, \"Neither cancer nor non-cancer detected.\""}, {"type": "llm_judge", "name": "Traceability and Source-Linked Entries", "description": "Spot-check that entries in the catalogs and crosswalks are source-cited and traceable, with exposure \u2192 condition \u2192 location/date mapping present.", "weight": 3.5, "judge_prompt": "Assess traceability. Sample across the Presumptive Exposures Catalog, Presumptive Conditions (cancer and non-cancer), and Appendix A (Master Crosswalk). For a small but diverse sample (\u22485 entries across sections):\n- Each sampled item should show or reference a specific source/citation (URL or numbered reference) nearby or in a source column/footnote.\n- Each sampled condition maps to at least one exposure AND has eligible service location(s) AND date(s) recorded (either in the catalog table or Appendix A).\n- Cross-references are consistent (e.g., the condition links back to the same exposure and service period across sections).\n\nScoring:\n- 3.5: All sampled items have explicit source links/citations and complete exposure-location-date mapping; cross-references consistent\n- 2.5: Minor gaps (e.g., 1 missing date or link) but generally traceable and consistent\n- 1.5: Several gaps (e.g., \u22652 items missing sources or mapping); some inconsistencies\n- 0.5: Little traceability; most sampled items lack sources or mapping\n- 0.0: No visible citations and no reliable mapping\n\nFocus on verifiable presence of citations and mappings, not content accuracy beyond what\u2019s obvious.", "expectation": "Every catalog/crosswalk entry sampled is source-cited and fully mapped to exposure, location(s), and date(s)."}, {"type": "llm_judge", "name": "Coverage of Major Presumptive Exposure Categories", "description": "Checks that the exposures catalog covers the major PACT Act exposure groupings veterans expect to see.", "weight": 2.5, "judge_prompt": "Evaluate whether the Presumptive Exposures Catalog clearly covers the major exposure categories commonly associated with PACT Act presumptives, each with service locations and date ranges:\n- Examples of major categories: Agent Orange/herbicides, burn pits/airborne hazards, radiation-related, Gulf War-related toxic exposures, contaminated water (e.g., base-specific), and other notable category groupings if applicable.\n- Do not require exact names; accept close equivalents. The key is that several major groupings are distinctly presented with their eligible locations and date windows.\n\nScoring:\n- 2.5: At least 5 distinct major exposure groupings with location/date details\n- 1.8: 4 groupings\n- 1.0: 3 groupings\n- 0.5: 2 groupings\n- 0.0: Only 0\u20131 grouping or not clearly categorized\n\nOnly assess presence and categorization\u2014do not verify the scientific or legal correctness of the categories.", "expectation": "A clear, categorized exposures section covering \u22655 major exposure groups with locations/dates."}, {"type": "llm_judge", "name": "Internal Consistency and De-duplication", "description": "Checks that entries are not contradictory or obviously duplicated across sections and appendices.", "weight": 1.5, "judge_prompt": "Check for internal consistency and obvious duplication across sections:\n- Spot-check if the same condition appears multiple times with conflicting exposure, location, or date details.\n- Verify that catalog entries match Appendix A crosswalk for the sampled items.\n- Look for repeated, redundant rows without value (e.g., exact duplicate condition-exposure pairs repeated).\n\nScoring:\n- 1.5: No contradictions; de-duplication is evident; crosswalk matches catalogs for samples\n- 1.0: Minor redundancy but no material contradictions\n- 0.5: Several redundant entries or minor inconsistencies\n- 0.0: Contradictory or chaotic listings that undermine reliability\n\nFocus on consistency rather than completeness.", "expectation": "No contradictions; minimal redundancy; crosswalk aligns with catalogs."}, {"type": "llm_judge", "name": "No New Information Beyond Sources", "description": "Ensures the document compiles and organizes source content without inventing new eligibility rules or conditions.", "weight": 2.0, "judge_prompt": "Determine if the document stays within the bounds of referenced sources:\n- Does the document refrain from asserting new presumptive conditions, dates, or locations that are not clearly tied to a source citation?\n- Are any advisory statements clearly framed as navigational guidance rather than new policy?\n- If the document includes summaries, are they faithful (no obvious extrapolation beyond the cited links)?\n\nScoring:\n- 2.0: No evidence of invented or unsourced eligibility information\n- 1.0: One questionable or weakly sourced statement\n- 0.0: Multiple invented or unsourced claims about presumptives/eligibility\n\nJudge conservatively; penalize only when clear.", "expectation": "All eligibility details are sourced; no invented presumptives or dates/locations."}], "on_failure_action": "skip_remaining", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Quality and Usability Assessment", "description": "Holistic evaluation of clarity, usability in clinic operations, and professional polish for a veteran audience.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Veteran-Centered Clarity and Navigation", "description": "Assesses readability, plain-language explanations, and navigability for veterans.", "weight": 3.0, "judge_prompt": "Assess clarity for a veteran audience:\n- Plain-language explanations of key terms (presumptive, eligibility, service periods)\n- Clear navigation: TOC works; headers, page numbers, and internal references/links function\n- Quick cues: callouts, tips, or icons that help veterans find information quickly\n\nScoring:\n- 3.0: Very clear, easy to navigate, strong plain language\n- 2.0: Generally clear with minor friction\n- 1.0: Some clarity issues; navigation is clunky\n- 0.0: Confusing or hard to navigate\n\nFocus on user experience, not legal precision.", "expectation": "Clear, plain-language guide with intuitive navigation."}, {"type": "llm_judge", "name": "Operational Usefulness for Clinics", "description": "Evaluates whether staff can use the document to streamline intake and counseling.", "weight": 3.0, "judge_prompt": "Assess operational usefulness:\n- Practical workflows: quick triage steps or checklists (e.g., \u201cHow to Use\u201d summarizing steps to check presumptives)\n- Crosswalk enables fast verification of exposure \u2192 condition \u2192 location/date\n- Minimal redundancy; information architecture reduces clicks/time\n\nScoring:\n- 3.0: Highly actionable for clinic workflows; quick to verify eligibility\n- 2.0: Useful but could be better organized for speed\n- 1.0: Limited operational value\n- 0.0: Not practically useful\n\nPrioritize clinical workflow efficiency.", "expectation": "Actionable structure enabling quick eligibility checks."}, {"type": "llm_judge", "name": "Accessibility and Inclusivity Considerations", "description": "Checks for readability and access features appropriate to a wide veteran audience.", "weight": 2.0, "judge_prompt": "Review accessibility:\n- Readable fonts and sufficient contrast\n- Logical heading hierarchy for screen readers\n- Avoids dense jargon; includes Glossary/Acronyms\n- Contact or help resources (e.g., VA contact info) where appropriate\n\nScoring:\n- 2.0: Strong accessibility practices throughout\n- 1.0: Some attention to accessibility\n- 0.0: Little/no accessibility consideration\n\nDo not require formal compliance testing; look for clear indicators.", "expectation": "Readable, accessible layout with helpful references and glossary."}, {"type": "llm_judge", "name": "Professional Polish and Version Control", "description": "Assesses visual consistency, error-free writing, and maintenance signals.", "weight": 2.0, "judge_prompt": "Evaluate professionalism:\n- Consistent styles (fonts, tables), minimal typos\n- Clear ownership (e.g., clinic/system name), date/versioning, and revision history\n- Visual organization (tables aligned, captions/footnotes used consistently)\n\nScoring:\n- 2.0: Highly professional\n- 1.0: Generally professional with minor issues\n- 0.0: Noticeably unpolished or inconsistent\n\nFocus on presentation, not content accuracy.", "expectation": "Polished, consistent, and versioned document ready for distribution."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "1d4672c8-b0a7-488f-905f-9ab4e25a19f7", "rubric": {"category_name": "International Index Correlation Analysis (Excel + PDF)", "rationale": "Pattern C (Mixed): This task requires a structured analytical workbook (Excel) plus a professional PDF report. Stage 1 uses LLM-only shape enforcement to mandate an exact, self-documenting Excel structure and a well-structured PDF report. Stage 2 blends light, robust code checks (bounds, symmetry, date range, reproducibility of correlations) with heavier LLM cross-referencing and methodological checks; LLM rules carry ~5x the weight of code rules. Stage 3 applies holistic LLM quality assessment for professionalism, clarity, and strategic value.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (LLM-only)", "description": "Verify the deliverables exist in the exact structures that enable verification: Excel workbook with specified sheets/sections and a structured PDF analysis. No calculation correctness here\u2014only presence and structure. This is a hard gate.", "is_required": true, "max_points": 5.0, "min_score_to_pass": 4.0, "rules": [{"type": "llm_judge", "name": "Excel Workbook Structure (Returns + Correlation Matrix)", "description": "Check the Excel file has the required sheets and layout to make verification possible.", "weight": 2.5, "judge_prompt": "You are validating ONLY the structure/presence of the Excel analysis (not correctness). Inspect all outputs and identify one Excel workbook (.xlsx). Evaluate the following flexible but explicit structure:\n\nRequired overall:\n- File type: Excel (.xlsx)\n- Contains two analysis-oriented sheets with clear names (flexible naming accepted):\n  \u2022 Returns sheet: acceptable names include 'Returns', 'Monthly Returns', 'Historical Returns', 'Return Data'\n  \u2022 Correlation sheet: acceptable names include 'Correlation Matrix', 'Correlations', 'Return Correlation'\n\nReturns sheet must show:\n- A visible header row.\n- A Date column with monthly observations for the period May 2024 through April 2025 (12 consecutive months). Dates should visibly correspond to month-end or monthly closes (exact day can vary but must be one per month).\n- One column per index for the following nine indices (flexible naming accepted, e.g., abbreviations or common variants):\n  1) MSCI EM (Emerging Markets)\n  2) MSCI ACWI IMI\n  3) MSCI World\n  4) MSCI EM ex China\n  5) MSCI EAFE\n  6) MSCI China\n  7) MSCI India\n  8) MSCI EM Latin America\n  9) MSCI AC Asia Pacific ex Japan\n- Numeric data visible in the index columns (either monthly returns or price levels\u2014both acceptable for structure purposes).\n- A brief source and method note visible on the sheet (e.g., a small block of text or note) that mentions: MSCI as the data source and the timeframe (May 2024\u2013Apr 2025). Including the MSCI URL is preferred but optional for Stage 1.\n\nCorrelation sheet must show:\n- A square matrix table with the same nine indices as both row and column labels (names can be reasonably similar). The diagonal should be present (values on the main diagonal). Heatmap formatting optional.\n- A short note on the sheet indicating that correlations are computed on monthly returns for the May 2024\u2013Apr 2025 period (wording can vary).\n\nCross-sheet consistency:\n- The set of indices in the correlation matrix matches those on the returns sheet (allow minor name variants).\n\nScoring (0\u20132.5):\n- 2.5: Excel present; both sheets present and clearly labeled; returns sheet has Date + 9 index columns; 12-month coverage; correlation sheet has a 9\u00d79 labeled matrix; brief source/method notes present; indices consistent across sheets.\n- 2.0: Excel present; both sheets present and usable; one minor element missing (e.g., brief source/method note or small naming mismatch) but structure still clearly supports verification.\n- 1.0: Excel present but only one core sheet present or several required elements missing (e.g., fewer than 9 indices or unclear date coverage).\n- 0.0: No usable Excel or structure is fundamentally wrong (missing both required sheets or unreadable tables).\n\nOnly evaluate structure/presence, not calculation correctness.", "expectation": "A well-structured Excel workbook with a Returns sheet (Date + 9 indices, monthly coverage May 2024\u2013Apr 2025) and a Correlation Matrix sheet (9\u00d79 labeled matrix), plus brief source/method notes."}, {"type": "llm_judge", "name": "PDF Report Structure and Sections", "description": "Check the PDF report exists and contains the required sections and professional formatting.", "weight": 2.5, "judge_prompt": "You are validating ONLY the structure/presence of the PDF analysis (not correctness). Inspect all outputs and identify a PDF (.pdf). Evaluate the following:\n\nFormat requirements:\n- File type: PDF.\n- At least 2 pages; professional formatting with headings and paragraphs.\n\nRequired sections (headers may vary slightly but must be clearly present):\n1) Executive Summary or Overview (on the first page)\n2) Methodology and Data (should mention MSCI as source and the May 2024\u2013Apr 2025 window)\n3) Findings or Correlation Highlights (identifies high and low correlations among the specified indices)\n4) Diversification and Overlap (discusses where exposures overlap and how to diversify)\n5) Portfolio Implications (risk management, strategic adjustments, recommendations, and next steps)\n6) Conclusion\nOptional but preferred: Appendix or visual/table (e.g., correlation heatmap or table snippet) and brief references/citations.\n\nScoring (0\u20132.5):\n- 2.5: Valid PDF; 2+ pages; clearly labeled sections covering all 6 required areas; professional layout.\n- 2.0: Valid PDF; 2+ pages; 5 of 6 sections present with clear headers; overall professional.\n- 1.0: Valid PDF; 2+ pages; only 3\u20134 sections present or poorly labeled.\n- 0.0: Not a PDF or fewer than 2 pages or no clear sections.\n\nOnly evaluate presence/structure, not content quality.", "expectation": "A professional multi-page PDF with all required sections, clearly headed and organized."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Verification", "description": "Now that structure is present, verify correctness and internal consistency using code (deterministic checks) and LLM (reasoning and cross-reference). Code rules are light; LLM rules carry more weight.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 4.0, "rules": [{"type": "code", "name": "Excel: Sheet and Index Coverage", "description": "Confirm the Excel workbook exists; identify Returns and Correlation sheets; verify that at least 8\u20139 expected indices are present (flexible naming).", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    def find_excel_resource():\n        primary = context.get_primary_output()\n        if primary and primary.is_spreadsheet:\n            return primary\n        for r in context.get_all_outputs():\n            if r.is_spreadsheet:\n                return r\n        return None\n\n    def normalize(s):\n        return re.sub(r\"[^a-z0-9]+\",\" \", str(s).lower()).strip()\n\n    expected = {\n        'MSCI EM (Emerging Markets)': [\"msci em\",\"msci emerging markets\"],\n        'MSCI ACWI IMI': [\"msci acwi imi\",\"acwi imi\",\"all country world index imi\"],\n        'MSCI World': [\"msci world\"],\n        'MSCI EM ex China': [\"msci em ex china\",\"msci emerging markets ex china\",\"msci em ex-cn\",\"msci em exchina\"],\n        'MSCI EAFE': [\"msci eafe\"],\n        'MSCI China': [\"msci china\"],\n        'MSCI India': [\"msci india\"],\n        'MSCI EM Latin America': [\"msci em latin america\",\"msci emerging markets latin america\",\"msci em latam\",\"msci em latin am\"],\n        'MSCI AC Asia Pacific ex Japan': [\"msci ac asia pacific ex japan\",\"msci ac asia pac ex japan\",\"msci ac asia pacific ex-japan\",\"msci ac ap ex japan\",\"msci ac asia pac ex jpn\",\"msci ac asia pac ex japan\"]\n    }\n\n    def best_match(name):\n        n = normalize(name)\n        for canon, pats in expected.items():\n            for p in pats:\n                if normalize(p) in n or n in normalize(p):\n                    return canon\n        return None\n\n    try:\n        xls_res = find_excel_resource()\n        if not xls_res:\n            return 0.0, \"No Excel file found.\"\n        xls_path = context.files.get_path(xls_res.id)\n        xls = pd.ExcelFile(xls_path)\n        sheet_names = [s for s in xls.sheet_names]\n        # Identify returns and correlation sheets by name\n        returns_candidates = [s for s in sheet_names if re.search(r\"return|price\", s, re.I)]\n        corr_candidates = [s for s in sheet_names if re.search(r\"correl\", s, re.I)]\n        returns_sheet = returns_candidates[0] if returns_candidates else (sheet_names[0] if sheet_names else None)\n        corr_sheet = corr_candidates[0] if corr_candidates else (sheet_names[1] if len(sheet_names) > 1 else None)\n        if not returns_sheet or not corr_sheet:\n            return 0.2, f\"Excel present but could not clearly identify both Returns and Correlation sheets. Found: {sheet_names}\"\n        df_ret = pd.read_excel(xls_path, sheet_name=returns_sheet)\n        # Count index columns matched\n        matched = set()\n        for col in df_ret.columns:\n            m = best_match(col)\n            if m:\n                matched.add(m)\n        count = len(matched)\n        # Score proportionally: 9 => full, 8 => ~0.44, etc.\n        score = min(1.0, count/9.0) * 0.5\n        fb = f\"Matched {count}/9 expected indices on returns sheet.\"\n        return score, fb\n    except Exception as e:\n        return 0.0, f\"Error reading Excel: {e}\""}, {"type": "code", "name": "Date Range and Monthly Coverage", "description": "Validate that the Returns sheet has a Date column covering May 2024 through April 2025 with roughly monthly frequency and 12 periods.", "weight": 0.4, "code": "import re\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\ndef evaluate(workflow, context):\n    def find_excel_resource():\n        primary = context.get_primary_output()\n        if primary and primary.is_spreadsheet:\n            return primary\n        for r in context.get_all_outputs():\n            if r.is_spreadsheet:\n                return r\n        return None\n\n    def find_returns_sheet(xls):\n        for s in xls.sheet_names:\n            if re.search(r\"return|price\", s, re.I):\n                return s\n        return xls.sheet_names[0] if xls.sheet_names else None\n\n    def parse_dates(df):\n        # Try common date columns\n        candidates = [c for c in df.columns if re.search(r\"date|month\", str(c), re.I)]\n        if not candidates:\n            # try first column if it looks like date\n            c = df.columns[0]\n            candidates = [c]\n        for c in candidates:\n            try:\n                d = pd.to_datetime(df[c], errors='coerce')\n                if d.notna().sum() >= 6:  # at least half\n                    return d.dropna()\n            except Exception:\n                continue\n        return pd.Series([], dtype='datetime64[ns]')\n\n    try:\n        xls_res = find_excel_resource()\n        if not xls_res:\n            return 0.0, \"No Excel file found.\"\n        xls_path = context.files.get_path(xls_res.id)\n        xls = pd.ExcelFile(xls_path)\n        sheet = find_returns_sheet(xls)\n        if not sheet:\n            return 0.05, \"No plausible returns sheet found.\"\n        df = pd.read_excel(xls_path, sheet_name=sheet)\n        dates = parse_dates(df)\n        if dates.empty:\n            return 0.05, \"No parsable date column found.\"\n        # Unique months\n        months = dates.dt.to_period('M').dropna().unique()\n        n_months = len(months)\n        # Range checks\n        min_m = dates.min()\n        max_m = dates.max()\n        # Expectations: May 2024 to April 2025 (inclusive)\n        lower = datetime(2024,5,1)\n        upper = datetime(2025,4,30)\n        in_range = (min_m >= lower - pd.Timedelta(days=7)) and (max_m <= upper + pd.Timedelta(days=7))\n        # Score components\n        score = 0.0\n        if n_months >= 10:\n            score += 0.2\n        if n_months >= 12:\n            score += 0.1\n        if in_range:\n            score += 0.1\n        # cap at weight\n        score = min(score, 0.4)\n        fb = f\"Months detected: {n_months}; date span: {min_m.date() if pd.notna(min_m) else 'NA'} to {max_m.date() if pd.notna(max_m) else 'NA'}.\"\n        return score, fb\n    except Exception as e:\n        return 0.0, f\"Error validating dates: {e}\""}, {"type": "code", "name": "Correlation Matrix Properties (Bounds, Symmetry, Diagonal)", "description": "Check that the correlation matrix is numeric, within [-1, 1], approximately symmetric, and diagonal near 1.", "weight": 0.2, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    def find_excel_resource():\n        primary = context.get_primary_output()\n        if primary and primary.is_spreadsheet:\n            return primary\n        for r in context.get_all_outputs():\n            if r.is_spreadsheet:\n                return r\n        return None\n\n    try:\n        xls_res = find_excel_resource()\n        if not xls_res:\n            return 0.0, \"No Excel file found.\"\n        xls_path = context.files.get_path(xls_res.id)\n        xls = pd.ExcelFile(xls_path)\n        corr_sheet = None\n        for s in xls.sheet_names:\n            if re.search(r\"correl\", s, re.I):\n                corr_sheet = s\n                break\n        if not corr_sheet:\n            return 0.05, \"No correlation sheet found.\"\n        df = pd.read_excel(xls_path, sheet_name=corr_sheet, header=0)\n        # Drop fully empty rows/cols\n        df = df.dropna(how='all').copy()\n        df = df.loc[:, df.notna().any(axis=0)]\n        # Try to coerce first column as index if it looks like labels\n        if df.shape[1] > 1:\n            df = df.set_index(df.columns[0])\n        # Keep numeric-only columns\n        df_num = df.apply(pd.to_numeric, errors='coerce')\n        # Drop non-numeric labeled rows/cols\n        df_num = df_num.dropna(how='all', axis=0)\n        df_num = df_num.dropna(how='all', axis=1)\n        if df_num.shape[0] < 3 or df_num.shape[0] != df_num.shape[1]:\n            return 0.05, f\"Matrix not square or too small: {df_num.shape}\"\n        # Bounds check\n        vals = df_num.values\n        if vals.size == 0:\n            return 0.0, \"Empty matrix.\"\n        bounds_ok = (np.nanmin(vals) >= -1.05) and (np.nanmax(vals) <= 1.05)\n        # Symmetry and diagonal\n        diff = np.nanmean(np.abs(vals - vals.T))\n        diag = np.diag(vals)\n        diag_err = np.nanmean(np.abs(diag - 1)) if diag.size else 1.0\n        score = 0.0\n        if bounds_ok:\n            score += 0.08\n        if diff <= 0.1:\n            score += 0.07\n        if diag_err <= 0.1:\n            score += 0.05\n        score = min(score, 0.2)\n        fb = f\"bounds_ok={bounds_ok}, symmetry_mae={diff:.3f}, diag_mae={diag_err:.3f}\"\n        return score, fb\n    except Exception as e:\n        return 0.0, f\"Error validating correlation matrix: {e}\""}, {"type": "code", "name": "Reproduce Correlations from Returns", "description": "Recompute the correlation matrix from the returns sheet (or from price levels using pct_change) and compare to the provided matrix. Score by mean absolute difference (MAD).", "weight": 0.4, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    def find_excel_resource():\n        primary = context.get_primary_output()\n        if primary and primary.is_spreadsheet:\n            return primary\n        for r in context.get_all_outputs():\n            if r.is_spreadsheet:\n                return r\n        return None\n\n    def normalize(s):\n        return re.sub(r\"[^a-z0-9]+\",\" \", str(s).lower()).strip()\n\n    expected = {\n        'MSCI EM (Emerging Markets)': [\"msci em\",\"msci emerging markets\"],\n        'MSCI ACWI IMI': [\"msci acwi imi\",\"acwi imi\",\"all country world index imi\"],\n        'MSCI World': [\"msci world\"],\n        'MSCI EM ex China': [\"msci em ex china\",\"msci emerging markets ex china\",\"msci em ex-cn\",\"msci em exchina\"],\n        'MSCI EAFE': [\"msci eafe\"],\n        'MSCI China': [\"msci china\"],\n        'MSCI India': [\"msci india\"],\n        'MSCI EM Latin America': [\"msci em latin america\",\"msci emerging markets latin america\",\"msci em latam\",\"msci em latin am\"],\n        'MSCI AC Asia Pacific ex Japan': [\"msci ac asia pacific ex japan\",\"msci ac asia pac ex japan\",\"msci ac asia pacific ex-japan\",\"msci ac ap ex japan\",\"msci ac asia pac ex jpn\",\"msci ac asia pac ex japan\"]\n    }\n\n    def best_match(name):\n        n = normalize(name)\n        best = None\n        for canon, pats in expected.items():\n            for p in pats:\n                if normalize(p) in n or n in normalize(p):\n                    best = canon\n        return best\n\n    try:\n        xls_res = find_excel_resource()\n        if not xls_res:\n            return 0.0, \"No Excel file found.\"\n        xls_path = context.files.get_path(xls_res.id)\n        xls = pd.ExcelFile(xls_path)\n        # Identify sheets\n        returns_sheet = None\n        corr_sheet = None\n        for s in xls.sheet_names:\n            if re.search(r\"correl\", s, re.I):\n                corr_sheet = s\n            if re.search(r\"return|price\", s, re.I):\n                returns_sheet = s\n        if not returns_sheet or not corr_sheet:\n            return 0.0, \"Could not identify both returns and correlation sheets.\"\n        df_ret = pd.read_excel(xls_path, sheet_name=returns_sheet)\n        df_corr = pd.read_excel(xls_path, sheet_name=corr_sheet)\n        # Clean correlation matrix: set first col as index if needed\n        df_corr = df_corr.dropna(how='all')\n        df_corr = df_corr.loc[:, df_corr.notna().any(axis=0)]\n        if df_corr.shape[1] > 1:\n            df_corr = df_corr.set_index(df_corr.columns[0])\n        df_corr_num = df_corr.apply(pd.to_numeric, errors='coerce')\n        df_corr_num = df_corr_num.dropna(how='all', axis=0)\n        df_corr_num = df_corr_num.dropna(how='all', axis=1)\n        # Map column names in returns to canonical\n        col_map = {}\n        for c in df_ret.columns:\n            m = best_match(c)\n            if m:\n                col_map[c] = m\n        # Keep only mapped numeric columns\n        df_vals = df_ret[list(col_map.keys())].apply(pd.to_numeric, errors='coerce')\n        # Drop date-like columns if included\n        df_vals = df_vals.loc[:, df_vals.columns]\n        # Decide if values look like returns or prices\n        med_abs = np.nanmedian(np.abs(df_vals.values)) if df_vals.size else np.nan\n        if np.isnan(med_abs):\n            return 0.0, \"No numeric data to compute correlations.\"\n        # If median abs is large, treat as prices -> compute pct_change\n        if med_abs > 2.0:\n            df_use = df_vals.pct_change().dropna(how='all')\n        else:\n            df_use = df_vals.copy()\n        # Compute correlations\n        corr_calc = df_use.corr(method='pearson')\n        # Canonicalize axes\n        corr_calc.columns = [col_map[c] for c in corr_calc.columns]\n        corr_calc.index = corr_calc.columns\n        # Canonicalize provided matrix labels by best match\n        def canonize_index(idx):\n            out = []\n            for x in idx:\n                m = best_match(x)\n                out.append(m if m else normalize(x))\n            return out\n        df_corr_num.columns = canonize_index(df_corr_num.columns)\n        df_corr_num.index = canonize_index(df_corr_num.index)\n        # Align\n        common = sorted(set(corr_calc.columns).intersection(set(df_corr_num.columns)))\n        if len(common) < 4:\n            return 0.05, f\"Too few common indices to compare: {len(common)}\"\n        a = corr_calc.loc[common, common].astype(float)\n        b = df_corr_num.loc[common, common].astype(float)\n        # Mean absolute difference excluding NaNs\n        mad = np.nanmean(np.abs(a.values - b.values))\n        if np.isnan(mad):\n            return 0.05, \"MAD is NaN.\"\n        # Scoring\n        if mad <= 0.05:\n            score = 0.4\n        elif mad <= 0.10:\n            score = 0.3\n        elif mad <= 0.20:\n            score = 0.15\n        else:\n            score = 0.05\n        return score, f\"Correlation MAD={mad:.3f} over {len(common)} indices\"\n    except Exception as e:\n        return 0.0, f\"Error reproducing correlations: {e}\""}, {"type": "llm_judge", "name": "Methodology and Source Consistency", "description": "PDF should clearly state data source (MSCI), timeframe (May 2024\u2013Apr 2025), frequency (monthly), and how returns/correlations were computed; should align with the Excel sheets.", "weight": 2.0, "judge_prompt": "Evaluate the PDF\u2019s methodology and source description and check consistency with the Excel workbook:\n- Does the PDF explicitly cite MSCI as the data source and describe the timeframe as May 2024\u2013April 2025 and monthly frequency?\n- Does it explain how returns were derived (e.g., total return series or computed from prices) and how Pearson correlations were calculated at the monthly level?\n- Is this consistent with what you see in the Excel workbook (Returns sheet and Correlation Matrix sheet notes)?\nScoring (0\u20132.0):\n- 2.0: Clear, explicit methodology (source, window, frequency, calculation steps) that matches Excel.\n- 1.0: Method covered but missing one key element (e.g., frequency or explicit window) or minor inconsistency with Excel.\n- 0.0: Methodology absent or contradictory to Excel.", "expectation": "Methodology section explicitly cites MSCI, monthly returns, specified window, and Pearson correlation; consistent with workbook."}, {"type": "llm_judge", "name": "Findings Cross-Reference With Numbers", "description": "Check that the PDF references specific correlation facts that match the Excel matrix (e.g., strongest/weakest pairs) and provides at least 3 concrete, correct references.", "weight": 2.5, "judge_prompt": "Cross-check the PDF\u2019s stated findings against the Excel correlation matrix:\n- Identify at least three concrete statements in the PDF citing specific relationships (e.g., \u201cMSCI World\u2013EAFE ~0.9\u201d, \u201cEM ex China\u2013China is lower than EM\u2013World\u201d, etc.).\n- Verify these statements are directionally and roughly numerically consistent with the Excel matrix (values do not have to match to the second decimal but should be close).\nScoring (0\u20132.5):\n- 2.5: Three or more correct, specific cross-references that align with the matrix (strongest/weakest pairs, regional patterns).\n- 1.5: One to two correct references or mostly directional but light on specifics.\n- 0.0: Claims do not match the matrix or no specific numerical references.", "expectation": "At least three correctly cross-referenced statements linking the narrative to matrix values."}, {"type": "llm_judge", "name": "Interpretation and Plausibility of Correlation Patterns", "description": "Judge whether the interpretation of strong/weak correlations and regional overlap is coherent and plausible given the matrix.", "weight": 1.0, "judge_prompt": "Evaluate whether the narrative accurately interprets correlation structure:\n- Do identified high-correlation clusters (e.g., developed markets: MSCI World/EAFE/ACWI IMI) and lower-correlation pairs (e.g., EM ex China vs. China or LatAm vs. Asia ex Japan) match the observed matrix patterns?\n- Are explanations plausible (e.g., overlapping constituents, factor exposures, USD/commodity sensitivity, regional policy/rate regimes)?\nScoring (0\u20131.0):\n- 1.0: Clear, plausible, and well-grounded interpretation consistent with matrix.\n- 0.5: Mixed\u2014some accurate interpretations, some tenuous.\n- 0.0: Interpretation contradicts the matrix or is generic hand-waving.", "expectation": "Interpretations align with matrix clusters and plausible economic rationale."}, {"type": "llm_judge", "name": "Diversification and Portfolio Implications", "description": "Assess whether the diversification ideas and portfolio recommendations are specific and grounded in the correlation results (risk management, strategic adjustments, and next steps).", "weight": 1.0, "judge_prompt": "Evaluate the portfolio guidance:\n- Do recommended adjustments (e.g., tilts, sleeves, hedges, regional rebalancing, factor overlays, currency hedging) logically follow from correlation findings?\n- Are risk management steps and next actions concrete (e.g., monitoring triggers, stress tests, rebalance bands, targeted exposures)?\nScoring (0\u20131.0):\n- 1.0: Specific, actionable recommendations clearly tied to the matrix.\n- 0.5: Partially actionable or loosely connected to findings.\n- 0.0: Vague or not connected to the analysis.", "expectation": "Actionable, correlation-grounded recommendations and next steps."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism", "description": "Holistic assessment of communication quality, usefulness, and professional standards for institutional audiences.", "is_required": false, "max_points": 7.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Writing Quality and Structure", "description": "Clarity, organization, and professionalism of the PDF report for an institutional audience.", "weight": 2.0, "judge_prompt": "Assess the PDF\u2019s writing quality:\n- Is the narrative concise, well-structured, and logically flows from context to methodology to findings to implications?\n- Are headings, charts/tables, and typography professional and consistent?\nScoring (0\u20132.0):\n- 2.0: Highly professional, clear, and well-organized.\n- 1.0: Generally clear with some organizational or stylistic issues.\n- 0.0: Unclear, disorganized, or unprofessional.", "expectation": "A clear, professionally structured report suitable for CIO-level review."}, {"type": "llm_judge", "name": "Strategic Insight and Actionability", "description": "Depth of insight and usefulness of recommendations for portfolio construction and risk management.", "weight": 2.0, "judge_prompt": "Evaluate strategic value:\n- Are insights nuanced and do they translate into tangible portfolio decisions (sizing, hedging, rebalancing cadence, diversification targets)?\n- Does the report anticipate trade-offs and implementation considerations?\nScoring (0\u20132.0):\n- 2.0: Strong, actionable insights with clear implementation guidance.\n- 1.0: Reasonable insights but light on specificity.\n- 0.0: Superficial or not decision-useful.", "expectation": "Insightful, decision-useful guidance for portfolio construction."}, {"type": "llm_judge", "name": "Visuals and Interpretability", "description": "Use and readability of tables/visuals (e.g., correlation heatmap/table) to support understanding.", "weight": 1.5, "judge_prompt": "Assess visuals:\n- Are tables/heatmaps labeled, legible, and correctly oriented?\n- Do visuals aid comprehension (e.g., color scale, ordering/clustering)?\nScoring (0\u20131.5):\n- 1.5: Visuals significantly enhance understanding.\n- 0.8: Adequate but could be improved.\n- 0.0: Missing or confusing visuals.", "expectation": "Clear visuals that make correlation structure easy to grasp."}, {"type": "llm_judge", "name": "Transparency and Reproducibility", "description": "Disclosure of assumptions, limitations, currency/return type, and references/citations to enable replication.", "weight": 1.5, "judge_prompt": "Evaluate transparency:\n- Does the report note return type (price vs. total return), currency base, any adjustments, and limitations (e.g., short sample)?\n- Are data sources and links cited such that another analyst could reproduce the work?\nScoring (0\u20131.5):\n- 1.5: Thorough disclosures and citations enabling replication.\n- 0.8: Partial disclosures; some gaps remain.\n- 0.0: Opaque\u2014insufficient for replication.", "expectation": "Clear disclosures and reproducibility cues (return type, currency, limitations, citations)."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "b9665ca1-4da4-4ff9-86f2-40b9a8683048", "rubric": {"category_name": "Manufacturing | Industrial Engineering \u2014 Safety Circuit Schematic (E\u2011Stop, Reset, Safety Relay)", "rationale": "This rubric enforces a self-documenting, verifiable 1-page PDF schematic for an E\u2011Stop safety circuit using the specified Automation Direct safety relay. Stage 1 (LLM-only) is a strict shape/structure gate that makes later verification trivial. Stage 2 mixes light code checks (string presence from PDF text) with heavier LLM verification of wiring logic and cross-references. Stage 3 evaluates professional quality, readability, and technician usability. Code rules carry much lower weight than LLM rules per guidance.", "max_total_score": 30.0, "stages": [{"name": "Stage 1 \u2014 Shape and Structural Compliance Gate", "description": "LLM-only gate ensuring the deliverable is a 1-page landscape PDF schematic (11x17) with IEC symbols and all required labeled elements present so later verification is possible. No calculation or correctness checking here\u2014only presence/structure/format.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Structured PDF Schematic Requirements", "description": "Check the candidate output against strict format and structural requirements. Only check presence/format/layout\u2014not electrical correctness.", "weight": 8.0, "judge_prompt": "You are evaluating whether the submitted file is a properly structured 1-page schematic PDF with all required labeled sections to enable verification. Do not judge correctness of wiring yet\u2014only the presence of required elements and format. Use visual inspection of the PDF and any extractable text.\n\nFormat requirements:\n- Must be a PDF (not Word/Excel/image) consisting of exactly 1 page.\n- Page orientation: Landscape; Paper size: 11x17 in (Tabloid, approx. 279 x 432 mm).\n- Use IEC-style symbols for: safety relay, emergency stop pushbuttons, pushbuttons, contactors/loads, terminals.\n- Clear title block present with at least: Title field set to exactly \"E-Stop Circuit\".\n- Line/connector width visually within the 0.625\u20130.875 mm range (approximate visual check acceptable).\n- Component spacing appears to be at least 5 mm between distinct symbols/components (approximate visual check acceptable).\n\nRequired structural content (only presence and labeling; don\u2019t verify electrical logic):\n1) Safety relay: Automation Direct LG 5925-48-61-24 (or close variant string like \"LG5925-48-61-24\"). Must show and label pins: A1, A2 (power), S11, S12, S22 (E\u2011Stop channels), S33, S34 (reset), 13, 14 (safety output). Show ES.24V+ to A1(+) and ES.24V- to A2(-).\n2) E-Stop devices: Four E\u2011Stops labeled ES0, ES1, ES2, ES3. Each shown as a 3-channel device (2NC + 1NO). Show the NO channel labels ES1.SIG, ES2.SIG, ES3.SIG for the button boxes (ES0\u2019s NO channel may be omitted if not specified). For ES0 (cabinet) show the four wire labels ES0.K1-1, ES0.K1-2, ES0.K2-1, ES0.K2-2. Similar K1/K2 wire label conventions indicated for ES1\u2013ES3.\n3) E\u2011Stop series loop presence: Show the two-channel series path(s) placed between S11, S12, and S22. Do not check logic; only confirm something labeled as a two-channel E\u2011Stop series connection exists touching these pins.\n4) Manual reset: A normally open reset pushbutton drawn across S33\u2013S34 and labeled Reset.S33 and Reset.S34 (or near these terminals).\n5) Safety output loads: Show the following loads connected to normally open pin 14 (via contact(s)) and the corresponding pin 13 tied to 24V GND and labeled ES.13: ES.1SD-, ES.SD-, ES.3-, ES.6-, ES.10-, actuator soft start valve, ES.STIR.\n6) Operator pushbuttons & indicators: Show STOP buttons: Enclosure stop with labels STP.DI (contact) and STP.IND (pilot). Button boxes BB1/BB2/BB3 with BB1.STP/BB1.IND, BB2.STP/BB2.IND, BB3.STP/BB3.IND. START button on enclosure labeled STR.DI and STR.IND. Indicate parallel connection of STOP buttons (presence only).\n7) Enable pushbutton present on enclosure (label may be \"ENABLE\" or similar) indicating it removes E\u2011Stop condition (presence only).\n\nScoring guidance (8 max):\n- 8: PDF; 1 page; landscape 11x17; title block with exact Title \"E-Stop Circuit\"; IEC symbols; line width and spacing approximately correct; all 7 structural content groups visibly present and labeled.\n- 6\u20137: PDF; 1 page; landscape 11x17; title block correct; IEC symbols; minor omissions such as missing one or two labeling items (e.g., one load name or one ESx.K1/K2 label set), or line width/spacing a bit off but still readable.\n- 3\u20135: PDF format but missing multiple structural groups or page setup issues (not landscape or size not 11x17), or title block incomplete; still shows some core elements (safety relay with some pins, E\u2011Stops, reset).\n- 1\u20132: PDF but largely incomplete or not a schematic (e.g., no recognizable relay/E\u2011Stop content).\n- 0: Not a PDF, more than 1 page, or entirely wrong content.\n\nOnly assess presence and structure; do not judge electrical correctness or safety logic in this stage.", "expectation": "A single-page, landscape 11x17 PDF schematic with IEC symbols, a correct title block, and visibly labeled relay pins, E\u2011Stops (with K1/K2, SIG), reset across S33\u2013S34, safety output loads on 14 with 13 to GND, and required operator pushbuttons/indicators."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness Verification (Wiring Logic + Label Consistency)", "description": "Now verify correctness and consistency of the safety circuit using mixed checks. Code rules lightly confirm key text labels exist. LLM judges assess wiring topology, two-channel E\u2011Stop loops, reset behavior, outputs to loads, and separation of safety vs. standard controls.", "is_required": false, "max_points": 14.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Critical Labels Present in PDF Text", "description": "Parse PDF text to confirm presence of essential device, pin, and wire labels enabling verification. Soft match; score proportionally.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float in [0, weight] or (float, str)\n    \"\"\"\n    weight = 0.8\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n    t = text.lower()\n    # Allow minor variations in relay model string\n    required_terms = [\n        'lg 5925-48-61-24', 'lg5925-48-61-24',\n        'a1', 'a2', 's11', 's12', 's22', 's33', 's34', '13', '14',\n        'es.24v+', 'es.24v-', 'reset.s33', 'reset.s34', 'es.13',\n        'es1.sig', 'es2.sig', 'es3.sig',\n        'es0.k1-1', 'es0.k1-2', 'es0.k2-1', 'es0.k2-2'\n    ]\n    hits = 0\n    for term in required_terms:\n        if term in t:\n            hits += 1\n    score = (hits / len(required_terms)) * weight\n    return score"}, {"type": "code", "name": "Loads and Safety Output Mentions", "description": "Confirm listed downstream loads and safety output pin references appear in text; score proportionally.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 0.8\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n    t = text.lower()\n    required_terms = [\n        'es.1sd-', 'es.sd-', 'es.3-', 'es.6-', 'es.10-', 'es.stir',\n        'soft start valve', 'soft-start valve',\n        'pin 14', '14',  # presence of 14 often repeated; keep flexible\n        'pin 13', '13',  # flexible\n        '24v gnd', 'gnd', 'ground'\n    ]\n    # Count unique hits across variations\n    found = set()\n    for term in required_terms:\n        if term in t:\n            found.add(term)\n    # Deduplicate soft start variants and 13/14 variants into conceptual groups\n    groups = [\n        {'soft start valve', 'soft-start valve'},\n        {'es.1sd-'}, {'es.sd-'}, {'es.3-'}, {'es.6-'}, {'es.10-'}, {'es.stir'},\n        {'pin 14', '14'}, {'pin 13', '13'}, {'24v gnd', 'gnd', 'ground'}\n    ]\n    hits = 0\n    for grp in groups:\n        if any(term in found for term in grp):\n            hits += 1\n    score = (hits / len(groups)) * weight\n    return score"}, {"type": "code", "name": "Operator Pushbuttons and Indicators Labels", "description": "Check presence of STOP/START/ENABLE labels and required parallel stop labels; score proportionally.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 0.8\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n    t = text.lower()\n    required_terms = [\n        'stp.di', 'stp.ind',\n        'bb1.stp', 'bb1.ind', 'bb2.stp', 'bb2.ind', 'bb3.stp', 'bb3.ind',\n        'str.di', 'str.ind',\n        'enable'  # allow generic enable label\n    ]\n    hits = sum(1 for term in required_terms if term in t)\n    score = (hits / len(required_terms)) * weight\n    return score"}, {"type": "llm_judge", "name": "E\u2011Stop Two-Channel Series Loop Correctness", "description": "Verify that four E\u2011Stops (ES0\u2013ES3) each with two NC channels (K1 and K2) are wired as two separate series loops returning to S12 and S22 respectively, with no cross\u2011fault monitoring and the NO SIG channels isolated to PLC (ESx.SIG).", "weight": 2.9, "judge_prompt": "Evaluate the E\u2011Stop loop topology for correctness (not just presence):\n- Four E\u2011Stops ES0, ES1, ES2, ES3 each shown with 2 NC channels (K1 and K2) and one NO channel (SIG). The K1 channel series loop should run from S11 back to S12 (or equivalent convention), and the K2 series loop should run from S11 (or S21 feed if depicted) back to S22. Cross-connections between channels (cross-fault monitoring) should NOT be present since the requirement is \"without cross fault monitoring\".\n- Confirm all four devices appear in both series loops (i.e., each loop includes ES0..ES3 NC contacts in series) and that the NO SIG channels ES1.SIG, ES2.SIG, ES3.SIG are NOT used in the safety chain and are only indicated for PLC monitoring.\n- ES0 cabinet device should show K1/K2 wire labels (ES0.K1-1/2, ES0.K2-1/2). Similar conventions for ES1\u2013ES3 are acceptable.\nScoring (2.9 max):\n- 2.9: Both loops clearly correct; all four E\u2011Stops included; SIGs isolated from safety; no cross\u2011fault monitoring; labeling consistent.\n- 2.0\u20132.5: Mostly correct; minor ambiguity in loop routing or missing one minor label.\n- 1.0\u20131.9: Partially correct; at least one loop incomplete or a SIG misused; unclear inclusion of all four devices.\n- 0: Major errors (single-channel only; cross-coupled loops; SIG used in safety path).", "expectation": "Two separate NC series loops ES0\u2013ES3 each, returning to S12 and S22 respectively, SIG channels isolated from the safety loop."}, {"type": "llm_judge", "name": "Manual Reset Circuit Correctness", "description": "Verify that the manual reset is a momentary NO across S33\u2013S34, not self-latching or bypassing safety, appropriately labeled Reset.S33 and Reset.S34.", "weight": 2.9, "judge_prompt": "Check the reset circuit implementation:\n- Reset pushbutton must be normally open and placed across S33\u2013S34 only.\n- It should be momentary (not a maintained bypass). There should be no self-holding across the safety output that would defeat manual reset.\n- Labels Reset.S33 and Reset.S34 (or very clear equivalents) are present.\nScoring (2.9 max):\n- 2.9: Clear NO momentary reset across S33\u2013S34 with correct labels, no unsafe bypass.\n- 2.0\u20132.5: Slight ambiguity (e.g., symbol unclear) but appears correct.\n- 1.0\u20131.9: Reset present but wiring questionable or mislabeled.\n- 0: Incorrect reset (maintained, bypasses safety, or wired to wrong terminals).", "expectation": "A momentary NO reset across S33\u2013S34 with proper labels and no unsafe latching."}, {"type": "llm_judge", "name": "Safety Outputs De\u2011energize Loads", "description": "Verify that 14 (NO) feeds all listed loads and 13 is to 24V GND (ES.13), such that an E\u2011Stop de\u2011energizes Indexer1/2, heaters, soft\u2011start valve, and stir motor.", "weight": 2.9, "judge_prompt": "Check the safety output section:\n- Pin 14 (NO safety output) should feed the downstream loads: ES.1SD-, ES.SD-, ES.3-, ES.6-, ES.10-, actuator soft start valve, ES.STIR (or clearly equivalent naming) so that opening the safety relay removes power to each.\n- Pin 13 (ES.13) should be tied to 24V GND; overall wiring should show that an E\u2011Stop opens the circuit to de\u2011energize all listed loads.\nScoring (2.9 max):\n- 2.9: All listed loads visibly controlled via 14; 13 to GND; de\u2011energization on E\u2011Stop is unambiguous.\n- 2.0\u20132.5: Minor omission/ambiguity (one label missing but clearly included).\n- 1.0\u20131.9: Several loads missing or unclear power path.\n- 0: Loads not controlled by safety output or 13 not to GND.", "expectation": "All specified loads are on the 14 NO path; 13 tied to 24V GND; cutting safety opens power to all loads."}, {"type": "llm_judge", "name": "Operator Controls Separation and Parallel Stop Buttons", "description": "Verify STOP buttons are in parallel (as specified), START and indicators are non-safety control-level, and Enable pushbutton removes E\u2011Stop condition while being electromechanically monitored.", "weight": 2.9, "judge_prompt": "Evaluate operator control wiring semantics:\n- STOP buttons (enclosure and BB1\u2013BB3) should be shown in parallel as specified. Labels: STP.DI and STP.IND on enclosure; BB1/2/3.STP and BB1/2/3.IND on button boxes.\n- START (STR.DI, STR.IND) should be a standard control (not in the safety loop).\n- Enable pushbutton present on enclosure; functionally depicted to remove the E\u2011Stop condition and monitored electromechanically (presence and separation from software-only control).\nScoring (2.9 max):\n- 2.9: Clear parallel STOPs; START/indicators clearly non-safety; Enable present and appropriately shown.\n- 2.0\u20132.5: Mostly correct; small ambiguities in depiction but intent clear.\n- 1.0\u20131.9: Partial correctness with notable ambiguity (e.g., STOPs not clearly parallel).\n- 0: Miswired semantics (STOPs in series with safety loop or START inside safety).", "expectation": "STOPs parallel at control level; START and indicators non-safety; Enable present and electromechanically monitored."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Presentation and Professional Quality", "description": "Holistic LLM assessment of drafting quality, IEC symbol fidelity, labeling consistency, and technician usability for assembly and troubleshooting.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity, Readability, and Drafting Quality", "description": "Assess legibility, line weights, spacing, and label clarity to ensure the drawing can be read and built from.", "weight": 2.0, "judge_prompt": "Assess the schematic\u2019s drafting quality:\n- Text legibility, consistent fonts/sizes; wire labels readable and near conductors.\n- Line/connector width appears within 0.625\u20130.875 mm and consistent.\n- Spacing between components \u2265 5 mm; no overcrowding; good alignment and routing.\n- Terminal numbering and wire routing are neat and unambiguous.\nScore 0\u20132 accordingly.", "expectation": "Neat, legible, consistent line weights, adequate spacing, and clear labels."}, {"type": "llm_judge", "name": "IEC Symbols and Conventions", "description": "Evaluate adherence to IEC symbol conventions for safety relay, pushbuttons, contacts, contactors, and terminals; correct use of series/parallel symbols.", "weight": 2.0, "judge_prompt": "Evaluate symbol fidelity and conventions:\n- IEC symbols for safety relay, E\u2011Stops (2NC + 1NO), pushbuttons, contactors/loads, terminals.\n- Series vs. parallel representation correct and consistent with IEC drafting.\n- Proper designation of NO/NC contacts by symbol, not just text.\nScore 0\u20132.", "expectation": "Correct IEC symbols used consistently with proper NO/NC depiction and series/parallel conventions."}, {"type": "llm_judge", "name": "Title Block and Documentation Completeness", "description": "Beyond the required Title, check for professional metadata and helpful notes/legends that aid traceability.", "weight": 2.0, "judge_prompt": "Assess the title block and documentation:\n- Title block contains Title: \"E-Stop Circuit\" (required) plus helpful fields (e.g., drawing number, rev, date, author, sheet 1/1) if present.\n- Presence of legend/notes referencing the safety relay model and voltage (24V), terminal naming conventions (ES.*), and any safety notes.\nScore 0\u20132. Give full credit if professionally documented even if minimal but clear.", "expectation": "Professional title block and concise notes/legend that improve traceability."}, {"type": "llm_judge", "name": "Technician Usability (Build and Troubleshoot)", "description": "Assess whether an assembly tech could wire and troubleshoot from the drawing with minimal ambiguity.", "weight": 2.0, "judge_prompt": "Evaluate technician usability:\n- Clear device tags (ES0\u2013ES3, BB1\u2013BB3, enclosure) and wire labels (ESx.K1-1/2, ES.24V+, ES.24V-, ES.13) positioned near terminals.\n- Terminal blocks and reference to panel/enclosure locations are clear.\n- Cross-references or callouts for multi-contact devices, and indication of PLC-only signals for SIG and indicators.\n- Overall, can a technician build and troubleshoot confidently from this single page?\nScore 0\u20132.", "expectation": "Unambiguous device tags, wire labels near terminals, clear terminal blocks/callouts, and PLC-only signals delineated."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "83d10b06-26d1-4636-a32c-23f92c57f30b", "rubric": {"category_name": "Audit Testing: AFC Risk Metrics Sampling and Verification", "rationale": "This rubric enforces a self-documenting workflow for audit sampling of Anti-Financial Crime (AFC) risk metrics. Stage 1 uses an LLM-only gate to mandate a strict Excel structure that makes verification trivial. Stage 2 mixes lightweight deterministic code checks (count, variance math, criteria coverage) with LLM judgment on methodology and criteria alignment, with LLM rules carrying higher weight. Stage 3 assesses professional quality and audit-readiness of the workpapers.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2013 Structured Output Gate (LLM only)", "description": "Gate: Verify the candidate produced a properly structured Excel workbook that enables verification.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.1, "rules": [{"type": "llm_judge", "name": "Workbook and Tabs Presence", "description": "Output must be a valid Excel workbook named 'Sample' (file name flexible) with the required tabs.", "weight": 1.2, "judge_prompt": "You are validating only the presence and structure (not correctness) of the delivered workbook.\n\nCheck the PRIMARY OUTPUT file:\n- It MUST be an Excel spreadsheet (.xlsx or .xls). Reject if not a spreadsheet.\n- It MUST contain at least two tabs/sheets with these roles (be flexible with names):\n  1) Selected Sample tab (acceptable names: 'Selected Sample', 'Sample', 'Audit Sample', 'Sample Selection').\n  2) Sample Size Calculation tab (acceptable names: 'Sample Size Calculation', 'Sample Size', 'Workings', 'Sample Size Calc').\n\nScoring:\n- 1.2 = Spreadsheet format AND both tab roles present (names may vary as above)\n- 0.6 = Spreadsheet format AND exactly one of the required tab roles present\n- 0.0 = Not a spreadsheet OR neither required tab role present\n\nOnly assess existence and structure, not content correctness.", "expectation": "An Excel file with two tabs fulfilling the roles: Selected Sample and Sample Size Calculation."}, {"type": "llm_judge", "name": "Selected Sample Tab Structure", "description": "Selected sample tab must include copied Population columns with variance in column J and sample indicator in column K.", "weight": 1.2, "judge_prompt": "On the 'Selected Sample' (or similarly named) tab, verify STRUCTURE ONLY (not correctness):\n- It displays a tabular dataset with visible header row (copied from Population).\n- It includes columns for Q2 2024 and Q3 2024 values (originally columns H and I). Exact header text may vary (e.g., 'Q2', 'Q3', 'Q2 2024', 'Q3 2024').\n- There is a column capturing the quarter-on-quarter variance in column J position (be flexible if columns shifted, but there must be an explicit 'Variance' or 'Change' column adjacent to Q2/Q3). \n- There is a sample indicator column in column K position that marks selected rows with the value '1'. Be flexible with exact header name (e.g., 'Sample', 'Selected', 'Audit Sample'); the key is that selected rows are clearly flagged with 1.\n\nScoring:\n- 1.2 = Table present with headers + Q2/Q3 columns + a distinct Variance column + a Sample indicator column with 1s marking selections\n- 0.8 = Table and headers + Q2/Q3 columns + either Variance or Sample indicator column present (but not both)\n- 0.4 = Table and headers present but missing both Variance and Sample indicator columns\n- 0.0 = No recognizable Selected Sample tab structure\n\nDo not evaluate math or criteria coverage; only structural presence.", "expectation": "A clearly structured Selected Sample tab showing Q2/Q3 values, a variance column, and sampled rows marked with 1."}, {"type": "llm_judge", "name": "Sample Size Calculation Tab Structure", "description": "Calculation tab must show a 90% confidence, 10% tolerable error sample size with visible workings and final sample size.", "weight": 0.6, "judge_prompt": "On the 'Sample Size Calculation' (or similarly named) tab, verify STRUCTURE ONLY (not correctness):\n- The tab includes labeled inputs/assumptions for the sample size (e.g., Confidence Level 90%, Tolerable Error 10%; Z-value; assumed p or variability; optional population size N).\n- It shows visible step-by-step workings (not hidden) and arrives at a clearly stated final required sample size (an integer n or similar label).\n\nScoring:\n- 0.6 = Inputs/assumptions shown + step-by-step workings + a clearly labeled final sample size\n- 0.3 = Some elements present (e.g., assumptions and a final number) but missing visible steps\n- 0.0 = No recognizable sample size calculation content\n\nOnly check presence/structure, not whether the math is correct.", "expectation": "A tab documenting assumptions, workings, and final sample size for 90% CL and 10% tolerable error."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness and Coverage Verification", "description": "Verify calculations, counts, and coverage against requirements using code checks and LLM reasoning.", "is_required": true, "max_points": 5.0, "min_score_to_pass": 2.5, "rules": [{"type": "code", "name": "Sample Count Matches Calculated Size", "description": "Compare the count of sampled rows (indicator=1) against the final sample size stated in the calculation tab.", "weight": 0.3, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef _find_sheet_name(xls, keywords):\n    for name in xls.sheet_names:\n        lname = str(name).lower()\n        if any(k in lname for k in keywords):\n            return name\n    return None\n\ndef _read_sheet(context, res_id, sheet_name):\n    try:\n        return context.files.read_excel(res_id, sheet_name=sheet_name)\n    except Exception:\n        # Try by index if name fails\n        try:\n            return context.files.read_excel(res_id, sheet_name=0)\n        except Exception:\n            return None\n\ndef _get_excel_fileinfo(context, res_id):\n    # Get sheet names via pandas directly for robustness\n    try:\n        path = context.files.get_path(res_id)\n        xls = pd.ExcelFile(path)\n        return xls\n    except Exception:\n        return None\n\ndef _find_indicator_col(df):\n    if df is None or df.empty:\n        return None\n    cols = list(df.columns)\n    # Prefer columns mentioning sample/selected/audit\n    for i, c in enumerate(cols):\n        lc = str(c).strip().lower()\n        if any(k in lc for k in [\"sample\", \"selected\", \"audit\"]):\n            return c\n    # Try 11th column (column K) if exists\n    if len(cols) >= 11:\n        return cols[10]\n    # Fallback: column with mostly 0/1/yes/no/y/n\n    def is_indicator_series(s):\n        total = len(s)\n        if total == 0:\n            return False\n        vals = s.dropna().astype(str).str.strip().str.lower()\n        allowed = set([\"0\",\"1\",\"y\",\"yes\",\"n\",\"no\",\"true\",\"false\"])\n        if len(vals) == 0:\n            return False\n        return (vals.isin(allowed).mean() > 0.8)\n    for c in cols:\n        try:\n            if is_indicator_series(df[c]):\n                return c\n        except Exception:\n            continue\n    return None\n\ndef _count_sampled(df, ind_col):\n    if df is None or ind_col is None:\n        return None\n    vals = df[ind_col]\n    def to_bool(v):\n        if pd.isna(v):\n            return False\n        s = str(v).strip().lower()\n        return s in [\"1\",\"y\",\"yes\",\"true\"] or (s == \"1.0\")\n    return int(vals.map(to_bool).sum())\n\ndef _extract_stated_sample_size(df_calc):\n    # Search for a clearly labeled final sample size anywhere in the calc tab\n    if df_calc is None or df_calc.empty:\n        return None\n    # Try row-wise labels\n    for _, row in df_calc.iterrows():\n        joined = \" \".join([str(x) for x in row.values])\n        if re.search(r\"sample\\s*size|final\\s*sample|required\\s*sample|n\\s*=\", joined, flags=re.I):\n            nums = re.findall(r\"\\d+\", joined)\n            if nums:\n                try:\n                    return int(nums[-1])\n                except Exception:\n                    pass\n    # Try entire sheet text blob\n    text = \"\\n\".join([\" \".join([str(x) for x in r]) for r in df_calc.values])\n    m = re.search(r\"(final|required)?\\s*sample\\s*size\\D*(\\d+)\", text, flags=re.I)\n    if m:\n        try:\n            return int(m.group(2))\n        except Exception:\n            pass\n    # Try standalone integer near 'n'\n    m2 = re.search(r\"\\bn\\s*=\\s*(\\d+)\", text, flags=re.I)\n    if m2:\n        try:\n            return int(m2.group(1))\n        except Exception:\n            pass\n    return None\n\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float (score) or tuple[float, str]\n    \"\"\"\n    weight = 0.3\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output\"\n    try:\n        xls = _get_excel_fileinfo(context, output.id)\n        if not xls:\n            return 0.0, \"Could not read Excel file\"\n        # Identify sheets\n        sample_sheet = _find_sheet_name(xls, [\"selected sample\",\"sample selection\",\"audit sample\",\"selected\",\"sample\"]) or xls.sheet_names[0]\n        calc_sheet = _find_sheet_name(xls, [\"sample size calculation\",\"sample size\",\"workings\",\"calc\"]) or (xls.sheet_names[1] if len(xls.sheet_names) > 1 else xls.sheet_names[0])\n        df_sample = context.files.read_excel(output.id, sheet_name=sample_sheet)\n        df_calc = context.files.read_excel(output.id, sheet_name=calc_sheet)\n        ind_col = _find_indicator_col(df_sample)\n        sampled_n = _count_sampled(df_sample, ind_col)\n        stated_n = _extract_stated_sample_size(df_calc)\n        if sampled_n is None or stated_n is None:\n            return 0.0, f\"Could not determine sampled count ({sampled_n}) or stated sample size ({stated_n})\"\n        diff = abs(sampled_n - stated_n)\n        if diff <= 1:\n            return weight, f\"Sampled {sampled_n} vs stated {stated_n}: OK\"\n        elif diff <= 2:\n            return weight*0.5, f\"Sampled {sampled_n} vs stated {stated_n}: Slight mismatch\"\n        else:\n            return 0.0, f\"Sampled {sampled_n} vs stated {stated_n}: Mismatch\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Variance Computation Sanity Check", "description": "Verify the variance column matches either absolute change (Q3-Q2) or percent change ((Q3-Q2)/Q2) for most rows.", "weight": 0.3, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef _find_sheet_name(xls, keywords):\n    for name in xls.sheet_names:\n        if any(k in str(name).lower() for k in keywords):\n            return name\n    return None\n\ndef _get_excel_fileinfo(context, res_id):\n    try:\n        path = context.files.get_path(res_id)\n        return pd.ExcelFile(path)\n    except Exception:\n        return None\n\ndef _find_cols(df):\n    # Find Q2, Q3, and variance columns by header hints; fallback to positions H, I, J (~columns 7,8,9)\n    q2_col = None\n    q3_col = None\n    var_col = None\n    cols = list(df.columns)\n    lc = [str(c).lower() for c in cols]\n    for i, c in enumerate(lc):\n        if (\"q2\" in c and \"2024\" in c) or (\"q2\" in c and \"2024\" not in c) or (\"2024 q2\" in c):\n            q2_col = cols[i]\n        if (\"q3\" in c and \"2024\" in c) or (\"q3\" in c and \"2024\" not in c) or (\"2024 q3\" in c):\n            q3_col = cols[i]\n        if any(k in c for k in [\"variance\",\"var\",\"change\",\"delta\"]):\n            var_col = cols[i]\n    # Fallbacks by index if needed\n    if q2_col is None and len(cols) > 7:\n        q2_col = cols[7]\n    if q3_col is None and len(cols) > 8:\n        q3_col = cols[8]\n    if var_col is None and len(cols) > 9:\n        var_col = cols[9]\n    return q2_col, q3_col, var_col\n\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float or (float, str)\n    \"\"\"\n    weight = 0.3\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output\"\n    try:\n        xls = _get_excel_fileinfo(context, output.id)\n        if not xls:\n            return 0.0, \"Could not open Excel\"\n        sample_sheet = _find_sheet_name(xls, [\"selected sample\",\"sample selection\",\"audit sample\",\"selected\",\"sample\"]) or xls.sheet_names[0]\n        df = context.files.read_excel(output.id, sheet_name=sample_sheet)\n        if df is None or df.empty:\n            return 0.0, \"Empty Selected Sample sheet\"\n        q2_col, q3_col, var_col = _find_cols(df)\n        if any(c is None for c in [q2_col, q3_col, var_col]):\n            return 0.0, f\"Missing columns: Q2={q2_col}, Q3={q3_col}, Var={var_col}\"\n        # Keep numeric rows only\n        q2 = pd.to_numeric(df[q2_col], errors='coerce')\n        q3 = pd.to_numeric(df[q3_col], errors='coerce')\n        var = pd.to_numeric(df[var_col].astype(str).str.replace('%','', regex=False), errors='coerce')\n        # Normalize possible percent-as-100 units\n        var_norm = var.copy()\n        # Heuristic: if median absolute var > 1 and < 200, treat as percent in [0-100]\n        if np.nanmedian(np.abs(var_norm)) > 1 and np.nanmedian(np.abs(var_norm)) < 200:\n            var_norm = var_norm / 100.0\n        # Compute models\n        abs_change = q3 - q2\n        with np.errstate(divide='ignore', invalid='ignore'):\n            pct_change = (q3 - q2) / q2.replace({0: np.nan})\n        # Compare with tolerance\n        mask = (~q2.isna()) & (~q3.isna()) & (~var_norm.isna())\n        if mask.sum() == 0:\n            return 0.0, \"No comparable rows\"\n        abs_match = (np.abs(var_norm[mask] - abs_change[mask]) <= (1e-6 + 0.005 * (np.abs(q3[mask]) + np.abs(q2[mask]) + 1))).mean()\n        pct_match = (np.abs(var_norm[mask] - pct_change[mask]) <= 0.01).mean()  # 1% tolerance\n        best = max(abs_match, pct_match)\n        if best >= 0.8:\n            return weight, f\"Variance model match >=80% (best={best:.2f})\"\n        elif best >= 0.5:\n            return weight*0.5, f\"Variance model match >=50% (best={best:.2f})\"\n        else:\n            return 0.0, f\"Low variance match (best={best:.2f})\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Criteria Coverage Hits (Data-Driven)", "description": "Check sampled rows include required criteria examples: high variance, specific entities, risk metrics A1/C1, zero-zero rows, business lines, countries, and division coverage.", "weight": 0.3, "code": "import re\nimport pandas as pd\nimport numpy as np\n\nENTITIES = [\n    \"CB Cash Italy\",\n    \"CB Correspondent Banking Greece\",\n    \"IB Debt Markets Luxembourg\",\n    \"CB Trade Finance Brazil\",\n    \"PB EMEA UAE\",\n]\nBUSINESSES = [\"Trade Finance\", \"Correspondent Banking\"]\nCOUNTRIES = [\"Cayman Islands\", \"Pakistan\", \"UAE\"]\n\n\ndef _find_sheet_name(xls, keywords):\n    for name in xls.sheet_names:\n        if any(k in str(name).lower() for k in keywords):\n            return name\n    return None\n\n\ndef _get_excel_fileinfo(context, res_id):\n    try:\n        path = context.files.get_path(res_id)\n        return pd.ExcelFile(path)\n    except Exception:\n        return None\n\n\ndef _find_indicator_col(df):\n    cols = list(df.columns)\n    for c in cols:\n        lc = str(c).lower()\n        if any(k in lc for k in [\"sample\", \"selected\", \"audit\"]):\n            return c\n    if len(cols) >= 11:\n        return cols[10]\n    # Fallback: 0/1 column\n    for c in cols:\n        try:\n            vals = df[c].dropna().astype(str).str.strip().str.lower()\n            if len(vals) > 0 and (vals.isin([\"0\",\"1\",\"y\",\"yes\",\"n\",\"no\",\"true\",\"false\"]).mean() > 0.8):\n                return c\n        except Exception:\n            continue\n    return None\n\n\ndef _detect_q_cols(df):\n    q2_col = None\n    q3_col = None\n    for c in df.columns:\n        lc = str(c).lower()\n        if \"q2\" in lc:\n            q2_col = c\n        if \"q3\" in lc:\n            q3_col = c\n    # Fallback positions H and I (7,8)\n    cols = list(df.columns)\n    if q2_col is None and len(cols) > 7:\n        q2_col = cols[7]\n    if q3_col is None and len(cols) > 8:\n        q3_col = cols[8]\n    return q2_col, q3_col\n\n\ndef _row_texts(df):\n    # Concatenate all string-like columns for fuzzy searching per row\n    text_cols = [c for c in df.columns if df[c].dtype == object]\n    joined = df[text_cols].astype(str).apply(lambda r: \" \".join(r.values.astype(str)), axis=1)\n    return joined.str.lower()\n\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float or (float, str)\n    \"\"\"\n    weight = 0.3\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output\"\n    try:\n        xls = _get_excel_fileinfo(context, output.id)\n        if not xls:\n            return 0.0, \"Could not open Excel\"\n        sample_sheet = _find_sheet_name(xls, [\"selected sample\",\"sample selection\",\"audit sample\",\"selected\",\"sample\"]) or xls.sheet_names[0]\n        df = context.files.read_excel(output.id, sheet_name=sample_sheet)\n        if df is None or df.empty:\n            return 0.0, \"Empty Selected Sample sheet\"\n        ind_col = _find_indicator_col(df)\n        if ind_col is None:\n            return 0.0, \"No sample indicator column detected\"\n        # Filter sampled rows\n        vals = df[ind_col]\n        def is_sel(v):\n            s = str(v).strip().lower()\n            return s in [\"1\",\"y\",\"yes\",\"true\",\"1.0\"]\n        mask = vals.map(is_sel)\n        sdf = df[mask]\n        if sdf.empty:\n            return 0.0, \"No sampled rows\"\n        txt = _row_texts(sdf)\n        # Criteria checks\n        hits = []\n        # High variance >20% (try percent var from any column labeled variance; fallback using Q2/Q3)\n        q2_col, q3_col = _detect_q_cols(sdf)\n        high_var_hit = False\n        # Try existing variance column\n        var_cols = [c for c in sdf.columns if any(k in str(c).lower() for k in [\"variance\",\"var\",\"change\",\"delta\"])]\n        if var_cols:\n            v = pd.to_numeric(sdf[var_cols[0]].astype(str).str.replace('%','', regex=False), errors='coerce')\n            # Treat >20 either as 20 (percent) or 0.2\n            high_var_hit = ((v >= 20) | (v/100.0 >= 0.2) | (v >= 0.2)).any()\n        if not high_var_hit and q2_col is not None and q3_col is not None:\n            q2 = pd.to_numeric(sdf[q2_col], errors='coerce')\n            q3 = pd.to_numeric(sdf[q3_col], errors='coerce')\n            with np.errstate(divide='ignore', invalid='ignore'):\n                pct = (q3 - q2) / q2.replace({0: np.nan})\n            high_var_hit = (np.abs(pct) >= 0.2).any()\n        hits.append(high_var_hit)\n        # Entities: require at least one row for EACH listed entity\n        for ent in ENTITIES:\n            hits.append(txt.str.contains(re.escape(ent.lower())).any())\n        # Metrics A1 or C1 (word boundary)\n        hits.append(txt.str.contains(r\"\\b(a1|c1)\\b\").any())\n        # Zero-zero rows (Q2=0 and Q3=0)\n        zero_zero = False\n        if q2_col is not None and q3_col is not None:\n            q2 = pd.to_numeric(sdf[q2_col], errors='coerce')\n            q3 = pd.to_numeric(sdf[q3_col], errors='coerce')\n            zero_zero = ((q2.fillna(1) == 0) & (q3.fillna(1) == 0)).any()\n        hits.append(zero_zero)\n        # Businesses presence\n        for b in BUSINESSES:\n            hits.append(txt.str.contains(re.escape(b.lower())).any())\n        # Countries presence\n        for ctry in COUNTRIES:\n            hits.append(txt.str.contains(re.escape(ctry.lower())).any())\n        # Division/Sub-Division coverage: at least 2 unique values in any column containing 'division' or 'sub'\n        div_cols = [c for c in sdf.columns if any(k in str(c).lower() for k in [\"division\",\"sub-division\",\"subdivision\",\"sub division\"])]\n        div_ok = False\n        if div_cols:\n            counts = [(sdf[c].astype(str).str.strip().str.lower().nunique()) for c in div_cols]\n            div_ok = any(n >= 2 for n in counts)\n        hits.append(div_ok)\n        score_frac = np.mean([1.0 if h else 0.0 for h in hits]) if hits else 0.0\n        # Scale: full weight if >=0.8 of criteria satisfied; half if >=0.5; else proportional\n        if score_frac >= 0.8:\n            return weight, f\"Criteria coverage strong ({score_frac:.2f})\"\n        elif score_frac >= 0.5:\n            return weight*0.5, f\"Criteria coverage moderate ({score_frac:.2f})\"\n        else:\n            return weight*score_frac*0.5, f\"Criteria coverage weak ({score_frac:.2f})\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "llm_judge", "name": "Methodology: Confidence and Error Application", "description": "Verify the calculation tab correctly applies 90% CL (Z\u22481.645) and 10% tolerable error, with appropriate assumptions (e.g., p=0.5) and optional finite population correction if N is provided.", "weight": 1.4, "judge_prompt": "On the 'Sample Size Calculation' tab, assess if the methodology is correct and reasonable:\n- Uses 90% confidence (Z \u2248 1.645) and 10% tolerable error (e \u2248 0.10) explicitly.\n- States and justifies key assumption(s) such as p=0.5 (worst case) or uses data-driven p.\n- If a population size N is documented, a finite population correction (FPC) is applied; if N is missing, base (infinite population) formula is acceptable.\n\nScoring:\n- 1.4 = Correct use of Z\u22481.645 and e=10%, reasonable p assumption, clear steps; applies FPC if N provided\n- 0.7 = Minor issues (e.g., slight Z rounding or missing justification) but outcome is directionally correct\n- 0.0 = Major methodological errors (wrong confidence level or error rate, or no coherent calculation)\n\nProvide brief reasoning in your evaluation.", "expectation": "Sound sample size methodology reflecting 90% CL and 10% error with transparent assumptions and FPC when applicable."}, {"type": "llm_judge", "name": "Variance and Threshold Emphasis", "description": "Check that quarter-on-quarter variance is computed correctly and that >20% changes are prioritized in the sample.", "weight": 1.4, "judge_prompt": "On the Selected Sample tab:\n- Verify that the variance column reflects quarter-on-quarter change between Q2 and Q3 (absolute or percentage). Be flexible on exact formula choice but it must be consistently derived from Q2 and Q3.\n- Confirm that the sample includes metrics with >20% changes (in magnitude), and that items with exceptionally large changes are emphasized among selections.\n\nScoring:\n- 1.4 = Variance computed consistently and samples clearly include/prioritize >20% changes\n- 0.7 = Variance exists but selection does not clearly emphasize >20% changes, or emphasis is weak\n- 0.0 = Variance appears inconsistent or selection ignores large changes\n\nExplain briefly how you determined emphasis and consistency from the sheet content.", "expectation": "Consistent variance computation and visible inclusion/emphasis of >20% changes in the sample."}, {"type": "llm_judge", "name": "Criteria Inclusion and Coverage", "description": "Evaluate whether all specified sampling criteria are met across the selected rows at least once.", "weight": 1.3, "judge_prompt": "Assess sampled rows (marked with '1') against the listed criteria. Each criterion should be satisfied by at least one sampled row:\n- Include metrics with >20% variance (emphasize exceptionally large changes)\n- Include metrics from entities: CB Cash Italy; CB Correspondent Banking Greece; IB Debt Markets Luxembourg; CB Trade Finance Brazil; PB EMEA UAE (at least one sampled row for each named entity)\n- Include metrics A1 and C1 (at least one each)\n- Include rows where Q2 and Q3 are zero\n- Include entries from Trade Finance and Correspondent Banking businesses (at least one each)\n- Include metrics from Cayman Islands, Pakistan, and UAE (at least one each)\n- Ensure coverage across Divisions and sub-Divisions (at least two distinct values evident across those fields)\n\nScoring:\n- 1.3 = All criteria satisfied across the sample\n- 0.9 = Most criteria met (misses 1-2 items)\n- 0.5 = About half of criteria met\n- 0.0 = Few/none of the criteria are met\n\nBe explicit about which criteria were satisfied vs missing based on the visible data.", "expectation": "All criteria evidenced at least once among sampled rows."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Workpaper Quality and Professionalism", "description": "Holistic assessment of presentation quality, traceability, and audit-readiness.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Formatting and Clarity", "description": "Assess readability, consistent headers, clear labeling of columns (Q2/Q3/Variance/Sample), and clean presentation.", "weight": 0.6, "judge_prompt": "Evaluate the workbook for professional presentation:\n- Clear, consistent headers (Q2/Q3/Variance/Sample marked), readable number formats, appropriate use of percent format where applicable.\n- No obvious formatting issues (misaligned headers, truncated text, unreadable cells).\n\nScoring:\n- 0.6 = Professional and clear throughout\n- 0.3 = Generally readable with minor issues\n- 0.0 = Poorly formatted or confusing\n\nProvide a brief note on formatting quality.", "expectation": "Clean, professional tabular presentation with unambiguous labels and legible formatting."}, {"type": "llm_judge", "name": "Traceability and Documentation", "description": "Check for audit traceability cues: source references to 'Population', notes/assumptions, date/time, and versioning or sign-off placeholders.", "weight": 0.5, "judge_prompt": "Assess whether the workbook supports audit traceability:\n- References the source ('Population' data) and any transformations applied.\n- Notes/assumptions captured on both tabs as relevant.\n- Optional but valued: date/time, preparer/reviewer fields, versioning.\n\nScoring:\n- 0.5 = Clear traceability with notes and helpful metadata\n- 0.3 = Some traceability elements present\n- 0.0 = No traceability or documentation\n\nExplain briefly the traceability elements you observed.", "expectation": "Workpapers that a reviewer can follow without guesswork, with source references and assumptions recorded."}, {"type": "llm_judge", "name": "Reproducibility and Formula Integrity", "description": "Prefer formulas over hard-coded values for variance and calculations where feasible; minimal hidden logic.", "weight": 0.5, "judge_prompt": "Evaluate whether key calculations are formula-driven and reproducible:\n- Variance column uses formulas tied to Q2/Q3 (not hard-coded values), where Excel display allows you to infer this.\n- Sample size workings show explicit formulas or step calculations rather than only final numbers.\n\nScoring:\n- 0.5 = Clear evidence of formula-driven calculations and reproducibility\n- 0.3 = Mixed (some formulas, some hard-coded)\n- 0.0 = Mostly hard-coded or opaque calculations\n\nAdd brief justification.", "expectation": "Transparent, formula-based computations that can be rechecked by another auditor."}, {"type": "llm_judge", "name": "Fitness for Audit Use", "description": "Assess whether the workbook is directly usable as an audit workpaper, including filters, summaries, and clear sample identification.", "weight": 0.4, "judge_prompt": "Consider whether this workbook is ready for audit testing execution:\n- Sampled rows are easy to filter/identify, with a clear sample count and possibly a brief summary line.\n- Any helpful summaries (e.g., count by criterion, or a coverage checklist) are present.\n\nScoring:\n- 0.4 = Fully ready and convenient for execution\n- 0.2 = Usable with minor adjustments\n- 0.0 = Hard to use for audit execution\n\nProvide a short comment on usability.", "expectation": "A reviewer/tester can immediately use the workbook to perform audit testing with minimal adjustments."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "5e2b6aab-f9fb-4dd6-a1a5-874ef1743909", "rubric": {"category_name": "Rugged Flashlight \u201cToasty\u201d Concept CAD Package", "rationale": "This rubric enforces a self-documenting, file-based deliverable set for a concept-phase mechanical design suitable for CNC prototyping. Stage 1 is an LLM-only gate that mandates exact deliverable shape (STEP models, assembly/sub-assembly PDFs with BOMs, exploded/assembled views, ANSI B landscape title blocks). Stage 2 mixes lightweight code checks (file presence and basic PDF text heuristics) with heavier LLM verification of requirement coverage and manufacturability. Stage 3 performs a holistic quality review of presentation, ergonomics, and documentation clarity.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Deliverable Shape Enforcement (GATE)", "description": "Verify the CAD/drawing package has the exact structure required to enable verification: STEP models for components/assembly/sub-assemblies, and ANSI B landscape assembly and sub-assembly PDFs with BOMs, exploded/assembled views, title block with tolerances, scale, and balloons. If structure is not present, evaluation stops and score is 0.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.5, "rules": [{"type": "llm_judge", "name": "CAD Package Structural Completeness", "description": "Check presence and structural completeness of required file types and drawing contents.", "weight": 4.0, "judge_prompt": "You are validating only the STRUCTURE of the deliverables for a rugged flashlight concept called \"Toasty.\" Review ALL provided files. Acceptable 3D formats: STEP (.stp/.step). 2D drawings must be PDF.\n\nRequired STRUCTURE (do not judge quality):\n- 3D CAD: STEP models provided for all significant components, the main assembly, and any sub-assemblies. If there are more than ~5 component STEP files, a ZIP containing the STEP files is acceptable. If you cannot open the ZIP, assume it contains the STEP files if its name suggests it and separate PDFs are also provided outside the ZIP.\n- 2D PDFs: Final assembly drawing AND each sub-assembly drawing are provided (component drawings not required at this phase). Each PDF must:\n  \u2022 Be ANSI B landscape (11\u00d717 in) format (flexibly accept cues like \u201cANSI B\u201d, \u201cSize B\u201d, or \u201c11x17\u201d).\n  \u2022 Include a professional title block with tolerance specifications.\n  \u2022 Show the drawing scale.\n  \u2022 Include BOTH an assembled view and an exploded view with balloons.\n  \u2022 Include a BOM table that has a column for material type.\n\nScoring (STRUCTURE ONLY):\n- 4.0: All the above present and clearly identifiable across the file set.\n- 3.0: Minor omissions (e.g., scale or tolerance line missing on one PDF) but overall structure is present.\n- 2.0: Clear gaps (e.g., missing sub-assembly PDFs or missing assembled/exploded views), but some structural artifacts exist.\n- 1.0: Only partial structure present (e.g., only STEP or only PDFs, not both).\n- 0.0: Wrong formats or major missing items (no PDFs or no STEP/ZIP evidence).", "expectation": "A file set that clearly contains STEP models for components/assembly/sub-assemblies and ANSI B landscape assembly/sub-assembly PDFs with title block, tolerances, scale, BOM with material, balloons, assembled and exploded views."}, {"type": "llm_judge", "name": "File Organization and Packaging Rule Compliance", "description": "Check that file organization follows instructions and ZIP packaging rule.", "weight": 2.0, "judge_prompt": "Check the file organization and packaging for the following structural requirements (do not judge content quality):\n- STEP models are provided as individual files; if there are more than ~5 component STEP files, a ZIP is present that packages the STEP files. PDFs should NOT be inside that ZIP.\n- PDFs for the final assembly and sub-assemblies are provided as standalone PDF files.\n- File naming is reasonably clear (e.g., includes assembly/sub-assembly identifiers) so that assembly vs. sub-assembly is inferable.\n\nScoring:\n- 2.0: Clear, organized file set; ZIP used appropriately when many STEP files; PDFs are separate; names are sensible.\n- 1.0: Mostly organized, minor confusion or unclear naming; ZIP usage nearly correct.\n- 0.0: Disorganized or violates packaging instruction (e.g., PDFs in ZIP or no ZIP despite many component STEP files).", "expectation": "Clearly separated STEP and PDF deliverables; appropriate ZIP-only for STEP files when component count is high; intelligible file names."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness and Feasibility)", "description": "Now that the structure is valid, verify the design addresses key requirements and is CNC-prototypable. Combines light code checks with deeper LLM assessment. Code rules are intentionally low-weight relative to LLM rules.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Deliverable File-Type Presence (STEP/ZIP + PDF)", "description": "Check that at least one PDF drawing exists and either STEP files or a ZIP is present.", "weight": 0.4, "code": "def evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs()\n        if not outputs:\n            return 0.0, \"No outputs.\"\n        pdf_count = 0\n        step_count = 0\n        zip_present = False\n        for r in outputs:\n            try:\n                p = str(context.files.get_path(r.id)).lower()\n            except Exception:\n                p = (getattr(r, 'name', '') or '').lower()\n            if p.endswith('.pdf'):\n                pdf_count += 1\n            elif p.endswith('.step') or p.endswith('.stp'):\n                step_count += 1\n            elif p.endswith('.zip'):\n                zip_present = True\n        # Scoring\n        if pdf_count >= 1 and (step_count >= 1 or zip_present):\n            return 0.4, f\"PDFs: {pdf_count}, STEPs: {step_count}, ZIP: {zip_present}\"\n        if (pdf_count >= 1) or (step_count >= 1 or zip_present):\n            return 0.2, f\"Partial presence \u2014 PDFs: {pdf_count}, STEPs: {step_count}, ZIP: {zip_present}\"\n        return 0.0, f\"Missing required types \u2014 PDFs: {pdf_count}, STEPs: {step_count}, ZIP: {zip_present}\"\n    except Exception as e:\n        return 0.0, f\"Error checking files: {e}\""}, {"type": "code", "name": "ZIP Packaging Heuristic for Many Components", "description": "If there are more than 5 STEP files, a ZIP should be present. If no STEP files are visible but a ZIP is present, assume packaged correctly (partial credit).", "weight": 0.4, "code": "def evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs()\n        if not outputs:\n            return 0.0\n        step_count = 0\n        zip_present = False\n        for r in outputs:\n            try:\n                p = str(context.files.get_path(r.id)).lower()\n            except Exception:\n                p = (getattr(r, 'name', '') or '').lower()\n            if p.endswith('.step') or p.endswith('.stp'):\n                step_count += 1\n            elif p.endswith('.zip'):\n                zip_present = True\n        if step_count > 5 and zip_present:\n            return 0.4, f\"ZIP present for many STEP files (count={step_count}).\"\n        if step_count > 5 and not zip_present:\n            return 0.0, f\"More than 5 STEP files (count={step_count}) but no ZIP present.\"\n        if step_count == 0 and zip_present:\n            return 0.2, \"No visible STEP files but ZIP present \u2014 assume packaged STEPs.\"\n        return 0.4, f\"STEP count acceptable without ZIP (count={step_count}).\"\n    except Exception as e:\n        return 0.0, f\"Error evaluating ZIP packaging: {e}\""}, {"type": "code", "name": "PDF Drawing Heuristics: BOM and Views", "description": "Heuristic text check that at least one PDF includes a BOM with material and both assembled/exploded view labels.", "weight": 0.4, "code": "def evaluate(workflow, context):\n    import re\n    try:\n        outputs = context.get_all_outputs()\n        pdf_ids = []\n        for r in outputs:\n            try:\n                p = str(context.files.get_path(r.id)).lower()\n            except Exception:\n                p = (getattr(r, 'name', '') or '').lower()\n            if p.endswith('.pdf'):\n                pdf_ids.append(r.id)\n        if not pdf_ids:\n            return 0.0, \"No PDFs to inspect.\"\n        text_blob = \"\\n\".join([context.files.read_pdf_text(pid) or \"\" for pid in pdf_ids[:4]])\n        t = text_blob.lower()\n        # BOM terms\n        bom_hits = 0\n        for term in [\"bom\", \"bill of materials\", \"material\", \"item\", \"qty\", \"quantity\"]:\n            if term in t:\n                bom_hits += 1\n        bom_score = min(bom_hits / 4.0, 1.0)  # require ~4 hits for full credit\n        # View terms\n        view_hits = 0\n        for term in [\"exploded\", \"assembly\", \"assembled\"]:\n            if term in t:\n                view_hits += 1\n        view_score = min(view_hits / 2.0, 1.0)  # need at least 2 tokens\n        score = 0.4 * 0.5 * (bom_score + view_score)\n        return score, f\"BOM hits={bom_hits}, View hits={view_hits}\"\n    except Exception as e:\n        return 0.0, f\"Error reading PDF text: {e}\""}, {"type": "llm_judge", "name": "Requirement Coverage Evidence", "description": "Check if drawings/notes explicitly address key functional requirements.", "weight": 1.8, "judge_prompt": "Check the PDFs and any visible model callouts for explicit evidence that the following requirements are addressed. You are verifying ALIGNMENT, not detailed physics:\n- Two 18650 cells in series, user-replaceable in the field, without tools, while wearing gloves (e.g., tailcap/compartment design, captive elements, large knurls, quarter-turn bayonet, glove-friendly latch).\n- Water ingress protection (e.g., O-rings, gaskets, IP rating notes, thread sealing, compression features).\n- Thermal safety from \u221220\u00b0C to +40\u00b0C (e.g., thermal path/finning/material notes, derating/thermal mass, warnings/usage constraints).\n- Power switch operable with gloves (e.g., large shrouded button, paddle, proud geometry).\n- Body grip features (knurling, scallops, overmold notes) and an interchangeable metal belt clip.\n\nScoring:\n- 1.8: Clear, explicit evidence for all or nearly all items.\n- 1.2: Most items addressed explicitly; one weak/implicit.\n- 0.6: Some items addressed; multiple unclear or missing.\n- 0.0: Little to no evidence linking design to these requirements.", "expectation": "Drawings/notes that explicitly call out battery replacement method, sealing, thermal considerations, glove-friendly switch, grip features, and interchangeable belt clip."}, {"type": "llm_judge", "name": "CNC Prototypability and Material Choice", "description": "Assess whether parts appear manufacturable via common CNC methods and materials meet lightweight, corrosion-resistant goals.", "weight": 1.8, "judge_prompt": "Evaluate manufacturability and materials from drawings/models and notes:\n- Parts split and features suggest CNC-friendly geometry (no impossible undercuts without splits; reasonable wall thicknesses; accessible toolpaths).\n- Fasteners/threads/O-ring grooves are specified or standardizable.\n- Materials meet lightweight, corrosion-resistant intent (e.g., 6061-T6 Al, 7075 Al with coating, Ti, stainless hardware; mention of anodize or passivation okay).\n- Tolerances appropriate for prototype machining.\n\nScoring:\n- 1.8: Strong CNC feasibility and appropriate materials/tolerances.\n- 1.2: Mostly feasible; minor concerns or vague notes.\n- 0.6: Significant manufacturability concerns or unclear materials.\n- 0.0: Not realistically CNC-prototypable or materials unsuitable.", "expectation": "Reasonable splits and features for CNC; materials like anodized aluminum or titanium; clear tolerances and standard features."}, {"type": "llm_judge", "name": "Assembly and Serviceability", "description": "Assess whether assembly structure and exploded views logically support field battery replacement with gloves and without tools.", "weight": 1.7, "judge_prompt": "Using the exploded/assembled views and notes, judge whether the assembly structure logically supports:\n- Easy opening/closing of the battery compartment without tools while wearing gloves.\n- Captive elements/seals that are not easily lost in the field.\n- Clear sequence evident from exploded view and BOM.\n\nScoring:\n- 1.7: Clear service-friendly design; glove use and tool-less operation are obvious.\n- 1.1: Generally serviceable; some steps may be fiddly with gloves.\n- 0.6: Ambiguous or likely tool-required.\n- 0.0: No credible path to tool-less, gloved replacement.", "expectation": "Exploded view and notes make the service sequence obvious, with glove-friendly and captive features."}, {"type": "llm_judge", "name": "Drawing Standards Compliance", "description": "Confirm drawings show ANSI B landscape, professional title block with tolerances, and a visible scale indicator.", "weight": 1.5, "judge_prompt": "Check the PDFs for evidence of:\n- ANSI B landscape format (accept \u201cANSI B\u201d, \u201cSize B\u201d, or \u201c11x17\u201d cues).\n- A professional title block with general tolerance specifications.\n- A visible drawing scale indicator.\n\nScoring:\n- 1.5: All three elements clearly visible.\n- 1.0: Two of three evident.\n- 0.5: Only one evident.\n- 0.0: None evident.", "expectation": "Each assembly/sub-assembly PDF clearly indicates size, title block with tolerances, and scale."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Holistic Quality Assessment", "description": "Evaluate overall professionalism, clarity, and strategic value of the concept package for a non-expert customer relying on engineering guidance.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Presentation and Readability", "description": "Quality of views, callouts, balloons, and overall drawing clarity for a customer audience.", "weight": 1.5, "judge_prompt": "Judge overall presentation quality:\n- Are views well-chosen and legible? Are balloons and callouts clear and consistently placed?\n- Is text readable with logical hierarchy? Are sections easy for a non-expert to follow?\n\nScoring: 1.5 excellent; 1.0 good; 0.5 fair; 0.0 poor.", "expectation": "Clean, legible drawings with clear balloons/callouts suitable for a non-expert."}, {"type": "llm_judge", "name": "Ergonomics and Ruggedization Insight", "description": "Quality of ergonomic features and rugged design considerations beyond mere presence.", "weight": 1.5, "judge_prompt": "Assess the depth of ergonomic and ruggedization thinking:\n- Glove-operable switch ergonomics and protection from accidental activation.\n- Grip surfaces and overall handling with wet/cold hands.\n- Rugged features (e.g., sealing redundancy, impact protection, belt clip robustness/interchangeability).\n\nScoring: 1.5 strong insight; 1.0 decent; 0.5 minimal; 0.0 none.", "expectation": "Thoughtful ergonomic details and robust features appropriate for harsh environments."}, {"type": "llm_judge", "name": "Traceability and Communication", "description": "How well the package communicates how requirements are met (without exhaustive reports).", "weight": 1.5, "judge_prompt": "Evaluate communication and traceability quality:\n- Are specific requirements clearly linked to features (e.g., brief notes, callouts near features)?\n- Are assumptions/limits stated (e.g., thermal notes, IP targets)?\n\nScoring: 1.5 clear traceability; 1.0 partial; 0.5 minimal; 0.0 absent.", "expectation": "Concise notes or tables that map requirements to features and assumptions."}, {"type": "llm_judge", "name": "CAD Organization and Naming", "description": "Professional organization of files and naming conventions that aid understanding and reuse.", "weight": 1.5, "judge_prompt": "Judge CAD and file organization quality:\n- Sensible assembly/sub-assembly breakdown.\n- File names reflect content and version/iteration if applicable.\n- Consistency across models and drawings.\n\nScoring: 1.5 excellent; 1.0 good; 0.5 fair; 0.0 poor.", "expectation": "Clear hierarchy and consistent naming that makes navigation straightforward."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "1bff4551-1d54-4e37-b2e0-d5c3f2ea4a45", "rubric": {"category_name": "Government \u2014 Recreation Workers | Celestial Solstice Rock & Roll Set List (Black Artists)", "rationale": "Self-documenting rubric enforcing a PDF-based, verifiable set list with structured sections and links. Stage 1 mandates an inspection-ready shape. Stage 2 mixes deterministic code checks with LLM validation focused on coverage, collection cross-references, suitability, and appropriateness. Stage 3 assesses professional presentation, educational merit, narrative flow, and operational actionability for organizers and band members.", "max_total_score": 25.0, "stages": [{"name": "Stage 1 \u2014 Format and Structural Gate (PDF Set List Spec)", "description": "Output must be a properly structured PDF set list enabling verification and performance preparation.", "is_required": true, "max_points": 5.0, "min_score_to_pass": 3.5, "rules": [{"type": "llm_judge", "name": "PDF Format and Required Structure Present", "description": "Verify the deliverable is a single PDF with the exact structure required to enable verification and performance use.", "weight": 5.0, "judge_prompt": "You are evaluating whether the candidate output satisfies the required FORMAT and STRUCTURE for a professional set list PDF. Only check presence/structure, not content quality or correctness.\n\nRequired output format:\n- Must be a PDF document (not Word/Docx, not spreadsheet, not plain text)\n- At least 2 pages OR clearly structured with headings and a multi-row table\n\nRequired sections and elements (be flexible with similar names):\n1) Title/Header: Includes event name (Celestial Solstice) and program focus (Black artists in rock & roll).\n2) Program Overview (or Summary/Overview): Brief purpose, audience, and stated runtime target (~45 minutes). Can be a paragraph or bullet list.\n3) Set List (as a table): A single, primary table that includes rows for each song and the following columns (allow close variants in naming):\n   - Order/#\n   - Song Title\n   - Artist/Band\n   - Year (or Release Year)\n   - Sub-genre (or Style)\n   - Era/Decade\n   - Context/Why Included (short paragraph 1\u20134 sentences)\n   - YouTube Link (URL)\n   - Instrumentation Notes (or Band Notes)\n   - Explicit Content? (Yes/No)\n   - Collection Evidence (e.g., NMAAHC link, object ID, or citation)\n   The original song \u201cFistful of Flyers\u201d by vocalist \u201crex\u201d must appear as a row in the table.\n4) Duration Summary: Either per-song durations or an estimated/assumed per-song duration AND a visible Total Program Duration (targeting ~45 minutes).\n5) Sources & Collection Cross-References: Links or citations supporting YouTube entries and NMAAHC collection presence (https://nmaahc.si.edu/ or evidence of collection object/search references).\n\nScoring:\n- 5.0: PDF format AND all required sections present with a clearly labeled set list table including the listed columns (minor naming variations OK) and the required original song. Duration summary and sources present.\n- 4.0: PDF format, set list table present with most columns (may miss 1\u20132 of the non-core columns like Instrumentation Notes or Explicit flag), and the original song present; duration and sources present.\n- 3.5: PDF format, set list table present but missing 2\u20133 required elements OR duration summary weak; original song present. (This is the minimum to pass.)\n- 2.0: PDF format but missing the set list table or missing the original song entry; or missing multiple core sections.\n- 0.0: Not a PDF OR structure too incomplete to verify.\n\nOnly check presence and structure. Do not assess correctness of music history, links, or playability.", "expectation": "A professionally structured PDF with all specified sections, a multi-column set list table, visible total duration near 45 minutes, sources, and inclusion of \u201cFistful of Flyers\u201d by rex."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification: Compliance, Coverage, and Suitability", "description": "Now verify factual, structural, and policy compliance based on the mandated shape. Combines code checks with LLM judgment.", "is_required": true, "max_points": 14.0, "min_score_to_pass": 7.0, "rules": [{"type": "code", "name": "Mandatory Original Song Included", "description": "Checks that the PDF text includes both the song title \u201cFistful of Flyers\u201d and the vocalist name \u201crex\u201d.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output\"\n    if not output.is_document:\n        return 0.0, \"Primary output is not a document\"\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0, \"Unable to read document text\"\n    tl = text.lower()\n    has_title = \"fistful of flyers\" in tl\n    has_rex = re.search(r\"\\brex\\b\", tl) is not None\n    if has_title and has_rex:\n        return 1.5, \"Found required original song and artist name\"\n    elif has_title or has_rex:\n        return 0.75, \"Partially found original song/artist (one missing)\"\n    else:\n        return 0.0, \"Missing required original song by rex\""}, {"type": "code", "name": "Duration Feasibility and Song Count", "description": "Checks for reasonable total duration (~45 min) and a feasible number of songs with YouTube links to cover 45 minutes.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output\"\n    if not output.is_document:\n        return 0.0, \"Primary output is not a document\"\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0, \"Unable to read document text\"\n\n    tl = text.lower()\n    # Estimate song count via YouTube links\n    links = re.findall(r'https?://(?:www\\.)?(?:youtube\\.com|youtu\\.be)/\\S+', tl)\n    n = len(links)\n    # Song count score\n    if 9 <= n <= 13:\n        count_score = 1.0\n    elif 7 <= n <= 8 or 14 <= n <= 15:\n        count_score = 0.5\n    else:\n        count_score = 0.0\n\n    # Duration score: look for explicit total duration near 45 minutes\n    dur_score = 0.0\n    # Try patterns like \"Total Duration: 45 minutes\"\n    m = re.search(r\"total\\s*(?:program\\s*)?duration\\s*[:\\-]?\\s*(\\d{1,2})\\s*(?:minutes|min|m)\\b\", tl)\n    minutes = None\n    if m:\n        try:\n            minutes = int(m.group(1))\n        except Exception:\n            minutes = None\n    # Fallback: pattern like \"45 min\" anywhere\n    if minutes is None:\n        m2 = re.search(r\"\\b(\\d{2})\\s*(?:minutes|min|m)\\b\", tl)\n        if m2:\n            try:\n                minutes = int(m2.group(1))\n            except Exception:\n                minutes = None\n    if minutes is not None:\n        if 40 <= minutes <= 50:\n            dur_score = 1.0\n        elif 35 <= minutes <= 55:\n            dur_score = 0.5\n        else:\n            dur_score = 0.0\n    # Final score averages both subchecks\n    final = 1.5 * ((count_score + dur_score) / 2.0)\n    feedback = f\"YouTube links: {n}; duration minutes: {minutes}; sub-scores (count={count_score}, duration={dur_score})\"\n    return final, feedback"}, {"type": "llm_judge", "name": "Collection Cross-Reference Validity", "description": "Most acts should be represented in the NMAAHC collection; verify that per-row evidence is provided and plausible.", "weight": 4.0, "judge_prompt": "Evaluate whether the set list credibly cross-references the NMAAHC collection for MOST acts.\n\nEvidence to look for:\n- A column like \u201cCollection Evidence\u201d per row with a URL to nmaahc.si.edu OR an object ID/citation that clearly refers to NMAAHC holdings.\n- A Sources section with a mapping between acts and specific NMAAHC search result URLs or collection object pages.\n- Wording that indicates the act is represented in the Institute\u2019s (on-loan) collection.\n\nScoring guide:\n- 4.0: Clear, row-level NMAAHC evidence for \u226570% of acts; links or IDs look specific and relevant; Sources section present.\n- 3.0: Evidence for ~50\u201369% of acts or some rows reference plausible collection presence; minor omissions or a few vague links.\n- 2.0: Evidence for ~30\u201349% of acts; many rows lack specifics; Sources sparse.\n- 1.0: Evidence for <30% of acts; largely generic, non-specific links.\n- 0.0: No credible NMAAHC cross-references.\n\nJudge for plausibility and coverage; do not deeply verify each link\u2019s content.", "expectation": "Row-level NMAAHC links or citations for a strong majority of the acts, plus a Sources section."}, {"type": "llm_judge", "name": "Sub-Genre and Era Diversity", "description": "Songs should span multiple rock sub-genres and eras/decades, demonstrating breadth of Black artists\u2019 contributions.", "weight": 4.0, "judge_prompt": "Assess diversity across sub-genres and eras/decades in the set list table.\n\nLook for:\n- At least 5 distinct sub-genres/styles (e.g., early rock & roll, garage rock, psychedelic, hard rock, funk rock, punk, new wave, alt/indie, metal, pop-rock, etc.).\n- Coverage across at least three distinct decades (e.g., 1950s/60s, 70s/80s, 90s/00s/10s+).\n- The contextual notes indicating why each selection matters historically or musically.\n\nScoring guide:\n- 4.0: \u22655 sub-genres and \u22653 decades well represented; contexts substantively connect each song to its sub-genre/era significance.\n- 3.0: 4 sub-genres and \u22653 decades OR \u22655 sub-genres but only 2 decades; contexts mostly adequate.\n- 2.0: 3 sub-genres and 2 decades; contexts thin.\n- 1.0: Minimal variety (\u22642 sub-genres or 1\u20132 decades); contexts very weak.\n- 0.0: Little to no discernible variety; missing contexts.", "expectation": "A balanced selection showing multiple sub-genres and at least three decades with clear context for each track."}, {"type": "llm_judge", "name": "Suitability for Standard Band and Audience Appropriateness", "description": "Confirm the set is playable by a standard band with limited rehearsal and appropriate for general audiences (no heavy profanity).", "weight": 3.0, "judge_prompt": "Evaluate two aspects:\n1) Playability for a standard band: Do Instrumentation Notes (or similar) indicate feasible arrangements for lead guitar, rhythm guitar, bass, drums, keys, and vocal? Are there any selections that obviously require extraordinary instrumentation or extended rehearsal beyond typical capabilities? Look for notes about key changes, simplified arrangements, or rehearsal guidance.\n2) Audience appropriateness: Titles and notes should not indicate heavy profanity; explicit content flags should be No for most selections or appropriately addressed with clean versions.\n\nScoring guide:\n- 3.0: Clear instrumentation notes; all songs obviously playable by a standard band; no heavy profanity (or clean versions specified).\n- 2.0: Mostly playable; 1\u20132 songs may need minor adaptation; no heavy profanity issues (or clean alternatives provided).\n- 1.0: Several songs appear challenging without extra instruments/rehearsal; ambiguous profanity handling.\n- 0.0: Set contains obviously inappropriate or unplayable selections for the specified band.\n\nDo not nitpick advanced technique; focus on practicality with limited rehearsal and general audience norms.", "expectation": "Practical band notes and explicit content screening that support quick adoption by a standard band."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality: Educational Value and Program Readiness", "description": "Holistic assessment of presentation quality, educational merit, narrative flow, and actionability for organizers and performers.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Clarity", "description": "Assesses whether the PDF is professional, scannable, and ready to distribute to organizers and band members.", "weight": 1.5, "judge_prompt": "Judge visual and structural professionalism: clear headings, consistent table formatting, readable fonts, and logical layout. Links should be visibly clickable/complete. The document should appear ready for direct distribution without reformatting.\nScoring: 1.5 = highly professional and clear; 1.0 = generally professional with minor inconsistencies; 0.5 = adequate but cluttered or inconsistent; 0.0 = unprofessional or confusing.", "expectation": "Polished PDF with clean tables, consistent styles, and clear links."}, {"type": "llm_judge", "name": "Context Quality and Accuracy", "description": "Evaluates the historical/musical context notes for each song: clarity, concision, and factual plausibility.", "weight": 1.5, "judge_prompt": "Review the per-song context/why-included notes. Are they concise (1\u20134 sentences), factually plausible, and do they tie each track to Black contributions to rock (innovation, influence, milestones)? Watch for obvious factual errors or generic filler.\nScoring: 1.5 = concise, specific, and historically/musically insightful across most songs; 1.0 = generally solid with occasional vagueness; 0.5 = mostly generic or shallow; 0.0 = misleading or largely missing.", "expectation": "Short, accurate rationales linking each selection to broader Black rock history."}, {"type": "llm_judge", "name": "Narrative Flow and Audience Engagement", "description": "Assesses sequencing and pacing for a 45-minute evening program with general audiences.", "weight": 1.5, "judge_prompt": "Evaluate the sequencing for an engaging arc (e.g., chronological sweep, thematic mini-sets, dynamic changes). Consider pacing (tempos, energy, variety) appropriate for a continuous 45-minute program.\nScoring: 1.5 = purposeful, engaging arc with good pacing; 1.0 = mostly coherent with minor pacing issues; 0.5 = disjointed or flat; 0.0 = incoherent flow.", "expectation": "A deliberate sequence that balances eras, styles, and energy for audience engagement."}, {"type": "llm_judge", "name": "Actionability for Rehearsal and Logistics", "description": "Determines whether the set list enables immediate rehearsal planning and show execution.", "weight": 1.5, "judge_prompt": "Assess whether the document contains sufficient details for immediate action: per-song keys/tempos (if provided), instrumentation/arrangement notes, explicit content flags/clean version notes, and a clear total runtime. Bonus for cues like transitions or medley notes.\nScoring: 1.5 = directly actionable with minimal follow-up; 1.0 = generally actionable but missing a few helpful details; 0.5 = needs additional clarification; 0.0 = not actionable as-is.", "expectation": "Contains enough concrete info for band and organizers to proceed to rehearsal scheduling and content vetting."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "650adcb1-ed19-4f88-8117-77640f7b94b6", "rubric": {"category_name": "Government \u2022 Recreation Workers \u2014 Winter Intern Schedule (Ski & Snowboard School)", "rationale": "This rubric forces a self-documenting Excel schedule that is easy to verify. Stage 1 uses LLM gate checks to mandate a precise workbook shape (six tabs, per-month calendar tables, a color-coded key, and per-month coverage exception sections). Stage 2 mixes light code checks (sheet presence and time-off content) with LLM verification of application fidelity (key usage, requested days off applied, coverage exceptions accuracy). Stage 3 assesses overall usability, professionalism, and stakeholder readiness. Code rules are intentionally lightweight (\u22485x less weight than LLM in Stage 2).", "max_total_score": 24.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement (Gate)", "description": "LLM-only gate to enforce exact workbook structure that makes verification trivial. If this fails, the entire category is zeroed.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.0, "rules": [{"type": "llm_judge", "name": "Workbook and Tabs Structure", "description": "Check the output is a single Excel workbook with exactly six tabs and required month tab structures.", "weight": 3.0, "judge_prompt": "You are validating the SHAPE (not correctness) of an Excel schedule workbook. Examine the candidate output.\n\nFormat requirements:\n- Must be an Excel spreadsheet (XLSX preferred).\n- Must contain exactly six tabs total:\n  1) December 2025\n  2) January 2026\n  3) February 2026\n  4) March 2026\n  5) April 2026\n  6) Time Off Requests\n  \u2022 Be flexible with exact casing and minor variations (e.g., \"Dec 2025\" acceptable), but all five months (Dec\u2013Apr) and a distinct Time Off Requests tab must be present.\n\nPer-month tab structure (for each month tab):\n- Must include a clearly labeled tabular schedule section with columns (or very close variants):\n  [Date | Day (or Weekday) | Adam Blake | Dustin Herman | Katie Montgomery]\n- Each day in that month should have a row in the table (calendar-grid may exist in addition, but a tabular section with these columns must be present to enable verification).\n- On the same month tab, include a short section titled \"Coverage Exceptions\" or similar, listing dates in that month where fewer than two interns are scheduled to work (presence only, not accuracy yet).\n\nScoring:\n- 3.0: All six tabs present; each month tab includes the required tabular columns and a visible \"Coverage Exceptions\" section; a separate \"Time Off Requests\" tab exists.\n- 1.5\u20132.5: Minor deviations (e.g., slight column header variations, or 1 month missing the Coverage Exceptions section), but overall required shape is present.\n- 0.0\u20131.0: Not Excel, missing multiple required tabs, or missing the tabular per-month schedule columns in most months.", "expectation": "A navigable Excel with 5 monthly tabs and 1 time-off tab. Each month has a tabular daily schedule with intern columns and a Coverage Exceptions section."}, {"type": "llm_judge", "name": "Legend / Key Presence and Coding Spec", "description": "Check for the required color-coding key on the first month tab.", "weight": 2.0, "judge_prompt": "Check the first monthly sheet (December 2025 or equivalent) for a clearly labeled Legend/Key that documents the required coding:\n- (A) Working: green fill + an \"X\" in the cell\n- (B) Scheduled Day Off: orange fill + the word \"off\" in the cell\n- (C) Requested Day Off: red fill + the words \"Requested Day Off\" in the cell\n\nBe flexible on header wording (e.g., \"Legend\", \"Key\"). The key must show all three statuses with the color and text conventions.\n\nScoring:\n- 2.0: Legend/Key present and clearly documents all three codes with stated color + text conventions.\n- 1.0: Key present but missing one code or not explicit about either color or text for one status.\n- 0.0: No usable key found on the first month tab.", "expectation": "A visible legend on the first sheet describing A/B/C with exact color+text conventions."}, {"type": "llm_judge", "name": "Time Off Requests Tab Shape", "description": "Check the dedicated Time Off Requests tab exists and is structured as a simple table.", "weight": 1.0, "judge_prompt": "Confirm there is a dedicated tab for time off (e.g., \"Time Off Requests\"). It should contain a simple table with columns similar to:\n- Intern Name\n- Date (preferably as real dates; ISO or MM/DD/YYYY acceptable)\n- Type (should indicate Requested Day Off)\n- Notes/Reason (free text)\n\nWe only check that a tabular structure exists with rows for requests; not the accuracy.\n\nScoring:\n- 1.0: Tab exists and presents a recognizable table with at least Intern Name and Date columns, plus a type/notes field.\n- 0.5: Tab exists with a table but columns are unclear or dates not obviously tabular.\n- 0.0: No distinct time off tab or no tabular data visible.", "expectation": "A clearly labeled Time Off Requests tab with a table listing requests, names, and dates."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness Verification", "description": "Now verify correctness and cross-references, mixing code and LLM rules. Code rules are light; LLM rules handle nuanced checks.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Workbook Sheets Present (Flexible Match)", "description": "Code check that an Excel workbook exists and includes five month tabs (Dec 2025\u2013Apr 2026) and a time off tab (name contains both 'time' and 'off'). Partial credit if most are present.", "weight": 1.0, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output found.\"\n    try:\n        fp = context.files.get_path(output.id)\n        xls = pd.ExcelFile(fp)\n        names = [str(s).strip() for s in xls.sheet_names]\n        low = [s.lower() for s in names]\n        # Month detection (flexible: abbrev or full)\n        month_aliases = {\n            ('december', 'dec'): '2025',\n            ('january', 'jan'): '2026',\n            ('february', 'feb'): '2026',\n            ('march', 'mar'): '2026',\n            ('april', 'apr'): '2026',\n        }\n        found_months = 0\n        used_idx = set()\n        for aliases, yr in month_aliases.items():\n            hit = False\n            for i, n in enumerate(low):\n                if i in used_idx:\n                    continue\n                if any(a in n for a in aliases):\n                    # Year optional (accept either way)\n                    hit = True\n                    used_idx.add(i)\n                    break\n            if hit:\n                found_months += 1\n        # Time off sheet detection\n        has_timeoff = any(('time' in n and 'off' in n) for n in low)\n        total_hits = found_months + (1 if has_timeoff else 0)\n        score = total_hits / 6.0\n        feedback = f\"Detected {found_months}/5 month sheets and time-off tab={'yes' if has_timeoff else 'no'}.\"\n        return score, feedback\n    except Exception as e:\n        return 0.0, f\"Error reading spreadsheet: {e}\""}, {"type": "code", "name": "Time Off Requests Content \u2014 Expected Dates by Intern", "description": "Code check that the Time Off Requests sheet includes the required four dates per intern (flexible date formats). Partial credit for partial matches.", "weight": 1.0, "code": "import re\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\nREQ = {\n    'adam blake': ['2025-12-25', '2026-03-16', '2026-03-17', '2026-04-01'],\n    'dustin herman': ['2026-03-10', '2026-03-11', '2026-03-12', '2026-03-13'],\n    'katie montgomery': ['2025-12-31', '2026-01-01', '2026-04-04', '2026-04-05'],\n}\n\n# Accept either YYYY-MM-DD or MM/DD/YYYY (and Excel date formats)\n\ndef to_date_set(values):\n    out = set()\n    for v in values:\n        if pd.isna(v):\n            continue\n        s = str(v).strip()\n        # Try parse\n        dt = pd.to_datetime(s, errors='coerce', utc=False)\n        if pd.isna(dt):\n            # Try common formats explicitly\n            for fmt in (\"%Y-%m-%d\", \"%m/%d/%Y\", \"%m/%d/%y\"):\n                try:\n                    dt = datetime.strptime(s, fmt)\n                    break\n                except Exception:\n                    pass\n        if not pd.isna(dt):\n            out.add(dt.date())\n    return out\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output found.\"\n    fp = context.files.get_path(output.id)\n    try:\n        xls = pd.ExcelFile(fp)\n        # Find time-off sheet\n        sheet_name = None\n        for n in xls.sheet_names:\n            ln = str(n).lower()\n            if 'time' in ln and 'off' in ln:\n                sheet_name = n\n                break\n        if sheet_name is None:\n            return 0.0, \"No 'Time Off' sheet detected.\"\n        df = pd.read_excel(fp, sheet_name=sheet_name)\n        # Identify likely columns\n        cols = [str(c) for c in df.columns]\n        lowcols = [c.lower() for c in cols]\n        intern_col_idx = next((i for i,c in enumerate(lowcols) if 'intern' in c or 'name' in c), None)\n        date_col_idx = next((i for i,c in enumerate(lowcols) if 'date' in c), None)\n        matches = 0\n        required = sum(len(v) for v in REQ.values())  # 12 total\n        if intern_col_idx is not None and date_col_idx is not None:\n            # Row-wise evaluation\n            df_local = df.copy()\n            # Normalize intern names\n            def norm_name(x):\n                return str(x).strip().lower()\n            df_local['__intern__'] = df_local.iloc[:, intern_col_idx].map(norm_name)\n            date_values = df_local.iloc[:, date_col_idx]\n            df_local['__date__'] = pd.to_datetime(date_values, errors='coerce').dt.date\n            # Fallback: try to parse strings if NaT\n            mask_nat = df_local['__date__'].isna()\n            if mask_nat.any():\n                parsed = []\n                for s in date_values[mask_nat].astype(str).tolist():\n                    d = pd.NaT\n                    for fmt in (\"%Y-%m-%d\", \"%m/%d/%Y\", \"%m/%d/%y\"):\n                        try:\n                            d = datetime.strptime(s.strip(), fmt).date()\n                            break\n                        except Exception:\n                            continue\n                    parsed.append(d)\n                df_local.loc[mask_nat, '__date__'] = parsed\n            # Build mapping\n            got = {}\n            for _, r in df_local.iterrows():\n                nm = r['__intern__']\n                d = r['__date__']\n                if pd.isna(d) or nm == '' or nm == 'nan':\n                    continue\n                got.setdefault(nm, set()).add(d)\n            # Count matches\n            for intern, dates in REQ.items():\n                need = {pd.to_datetime(x).date() for x in dates}\n                have = got.get(intern, set())\n                matches += len(need & have)\n            score = matches / required\n            return score, f\"Matched {matches}/{required} requested dates by intern in Time Off tab.\"\n        else:\n            # Fallback: coarse text search across the whole sheet\n            all_text = '\\n'.join([\\\n                ' '.join([str(x) for x in row]) for row in df.fillna('').values.tolist()])\n            all_text_low = all_text.lower()\n            # Check if each intern name and date string appear anywhere\n            def variants(d):\n                dt = pd.to_datetime(d)\n                return {dt.strftime('%Y-%m-%d'), dt.strftime('%m/%d/%Y')}\n            for intern, dates in REQ.items():\n                for d in dates:\n                    ok = (intern in all_text_low) and any(v in all_text_low for v in variants(d))\n                    if ok:\n                        matches += 1\n            score = matches / required\n            return score, f\"Text-scan matched {matches}/{required} intern-date pairs (fallback).\"\n    except Exception as e:\n        return 0.0, f\"Error validating Time Off Requests: {e}\""}, {"type": "llm_judge", "name": "Calendar Completeness and Key Adherence", "description": "LLM check that each month tab covers all dates in the month and uses the key conventions (green X=working, orange off, red Requested Day Off) in the schedule table.", "weight": 4.0, "judge_prompt": "Check every month tab (Dec 2025\u2013Apr 2026) for the following:\n1) The tabular schedule section includes every date of that month (one row per date) with columns for Date, Day/Weekday, and each intern.\n2) The schedule cells use the key correctly:\n   - Working: green background + an \"X\"\n   - Scheduled Day Off: orange background + the word \"off\"\n   - Requested Day Off: red background + the words \"Requested Day Off\"\n3) Cells should not mix statuses (e.g., an \"X\" on a red cell is incorrect). Minor formatting variance is acceptable if the meaning is unambiguous and consistent with the key.\n\nScoring guidance:\n- 4.0: All months complete and consistently follow the key.\n- 2.5\u20133.5: Minor gaps or occasional misapplications of the key in 1\u20132 months.\n- 1.0\u20132.0: Repeated gaps or inconsistent coding in several months.\n- 0.0: Months lack full date coverage or key usage is absent/incorrect broadly.", "expectation": "A complete daily schedule per month with consistent application of the color/text key."}, {"type": "llm_judge", "name": "Requested Days Off Applied Correctly", "description": "LLM cross-check that each intern\u2019s requested dates are correctly marked as Requested Day Off (red) on the relevant month tabs and not scheduled to work.", "weight": 3.0, "judge_prompt": "Cross-reference the \"Time Off Requests\" tab with the monthly schedules and verify application for each intern:\n- Adam Blake: 12/25/2025; 3/16/2026; 3/17/2026; 4/1/2026\n- Dustin Herman: 3/10/2026; 3/11/2026; 3/12/2026; 3/13/2026\n- Katie Montgomery: 12/31/2025; 1/1/2026; 4/4/2026; 4/5/2026\n\nEach of these dates for the respective intern should be marked as Requested Day Off (red cell with the words \"Requested Day Off\") and should not show them working that day. Minor label variants like \"Request Off\" are acceptable if clearly equivalent.\n\nScoring:\n- 3.0: All requested dates correctly marked for all interns.\n- 2.0: 1\u20132 individual dates incorrectly marked.\n- 1.0: Several dates misapplied or missing.\n- 0.0: Widespread failures to apply requested days or interns scheduled to work on their requested days.", "expectation": "All 12 requested dates are red and labeled appropriately for the corresponding intern columns."}, {"type": "llm_judge", "name": "Coverage Exceptions Listed and Matched", "description": "LLM verifies that the Coverage Exceptions section on each month tab accurately lists days with fewer than two interns scheduled to work.", "weight": 3.0, "judge_prompt": "For each monthly tab, check the \"Coverage Exceptions\" section (or similarly titled area) that lists dates where fewer than two interns are scheduled to work. Compare a sample of dates in the list against the actual schedule table on that tab:\n- Confirm that days in the exceptions list indeed have fewer than two interns marked as working (green X).\n- Also spot-check that obvious days with fewer than two interns are not missing from the list.\n\nScoring guidance:\n- 3.0: Lists are present on all months and accurately reflect the schedule table (no obvious misses or false positives).\n- 2.0: Minor inaccuracies (e.g., 1\u20132 mismatches across all months).\n- 1.0: Several mismatches or missing lists for one month.\n- 0.0: Missing exception lists or lists do not correspond to the schedules.", "expectation": "Each month\u2019s exceptions list corresponds to dates with <2 interns scheduled, without obvious errors."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Usability Assessment", "description": "Holistic professional quality, usability, and stakeholder readiness.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Readability", "description": "Assess formatting quality: clear headers, consistent fonts, readable color contrast, frozen panes or repeated headers for long tables, and general polish.", "weight": 1.5, "judge_prompt": "Evaluate the workbook\u2019s professional presentation:\n- Clear tab names, headers, and consistent fonts.\n- Adequate color contrast (green/orange/red easily distinguishable and legible with the text inside).\n- Frozen header row or otherwise easy to read long lists of dates.\n- Clean alignment, no clutter, and usable print layout if printed.\nScore higher for polished, standardized formatting and readability throughout.", "expectation": "Clean, consistent formatting with excellent readability across all tabs."}, {"type": "llm_judge", "name": "Stakeholder Clarity and Navigability", "description": "Evaluate how easily interns and the Program Director can find information (e.g., schedule lookups, exceptions, key).", "weight": 1.5, "judge_prompt": "Assess whether an intern or Program Director can quickly understand schedules and exceptions:\n- Is the Legend/Key easy to find and unambiguous?\n- Can a user quickly find an intern\u2019s schedule for a given date?\n- Are Coverage Exceptions easy to locate on each month?\n- Are dates and weekdays clearly displayed?\nScore higher if the workbook is intuitive and quick to navigate for its intended audience.", "expectation": "Clear, intuitive layout enabling quick lookups for interns and leadership."}, {"type": "llm_judge", "name": "Schedule Balance and Policy Adherence (If Possible)", "description": "Holistic judgment of the 5-on/2-off cadence and fairness across interns, acknowledging the \"if possible\" caveat.", "weight": 1.5, "judge_prompt": "Consider the overall rhythm and fairness of the schedules:\n- Do interns generally follow a 5-days-on, 2-days-off pattern with consecutive days off when feasible?\n- Is there a reasonable balance of weekends/holidays among the interns?\n- Are time-off requests integrated without creating undue imbalance elsewhere?\nDo not require perfection; reward thoughtful, fair patterns across the winter.", "expectation": "A generally fair rotation reflecting a 5-on/2-off pattern with sensible consecutive days off."}, {"type": "llm_judge", "name": "Documentation and Notes Quality", "description": "Quality of notes, reasons, and any supporting context (e.g., Time Off Requests notes, brief comments under Coverage Exceptions).", "weight": 1.5, "judge_prompt": "Review the clarity and usefulness of notes and supporting context:\n- Are reasons/notes present for time-off entries?\n- Are Coverage Exceptions optionally annotated with a short rationale or hint for mitigation (e.g., suggest coverage)?\n- Is terminology consistent (e.g., same status labels used everywhere)?\nScore higher for concise, helpful notes and consistent terminology.", "expectation": "Useful, concise notes that enhance clarity and support internal coordination."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "f5d428fd-b38e-41f0-8783-35423dab80f6", "rubric": {"category_name": "Bahamas 7-Day Yacht Itinerary (Concierge)", "rationale": "This rubric enforces a self-documenting, two-page PDF itinerary with day-by-day sections, photos, and credible sources so verification is easy. Stage 1 (LLM-only) mandates exact document shape and required structural elements. Stage 2 mixes lightweight code checks (presence of day sections, interests, citations, dining venues) with LLM correctness checks (photos per day with credits, feasibility of routing, activity/dining specificity, coverage of preferred destinations). Stage 3 provides a holistic quality assessment for luxury clientele, balancing family-friendly activities, pacing, and professional polish.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Structure and Format Gate", "description": "LLM-only gate to verify the candidate produced a properly structured, two-page PDF itinerary with seven day-by-day sections, each paired with a royalty-free image and a reference/credits section.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "File Format, Length, and Core Layout", "description": "Verify the output is a two-page PDF with clear itinerary layout and day-by-day labeling.", "weight": 2.0, "judge_prompt": "You are checking ONLY the format and presence of required structural elements (not content quality). Review the candidate\u2019s primary output.\n\nRequirements:\n- Must be a PDF file (not DOCX, not image-only without text, not PPT/Excel).\n- Exactly 2 pages in length.\n- Professionally formatted itinerary with a clear title (e.g., trip title and dates/duration) and a brief trip overview/introduction on page 1.\n- Seven distinct, clearly labeled sections for each day (e.g., \u201cDay 1: \u2026\u201d through \u201cDay 7: \u2026\u201d). Day labels must be visible in the PDF.\n\nScoring (0 to 2.0):\n- 2.0: PDF format, exactly 2 pages, visible title + overview, and exactly seven clearly labeled day sections.\n- 1.5: PDF and exactly 2 pages; title present; day labels present but not all clearly formatted OR the overview is missing.\n- 1.0: PDF and exactly 2 pages; some day labels missing/ambiguous but at least 5 labeled day sections visible.\n- 0.5: PDF but wrong page count OR minimal itinerary structure (\u22644 day sections labeled).\n- 0.0: Not a PDF OR no visible day-by-day labeling.", "expectation": "A two-page PDF with a title, brief intro, and seven clearly labeled day sections."}, {"type": "llm_judge", "name": "Section Structure: Images and References", "description": "Verify each day section contains a short paragraph and an accompanying royalty-free image, plus a references/credits section for sources and images.", "weight": 2.0, "judge_prompt": "Check ONLY structural presence (not content accuracy). Review the primary PDF.\n\nRequired elements:\n- For each of the 7 day sections: a concise description paragraph of 3\u20134 sentences AND at least one image placed near that day\u2019s content.\n- A visible references/credits section at the end (or per-page footer) that includes: (a) travel research sources (e.g., Lonely Planet, Nassau Paradise Island, Bahamas.com, Travel + Leisure, etc.) and (b) image credits that explicitly name legitimate royalty-free platforms (e.g., Unsplash, Pexels, Pixabay, Wikimedia Commons) with attribution lines and/or links.\n\nScoring (0 to 2.0):\n- 2.0: All 7 day sections each have a 3\u20134 sentence paragraph AND at least one image; a references/credits section lists both research sources and image credits with platform names/links.\n- 1.5: All 7 days have text and images, but references/credits are partial (e.g., missing links or incomplete platform naming) OR 1 day is missing an image but references are complete.\n- 1.0: At least 5 days have both text and images; references/credits exist but are minimal.\n- 0.5: Fewer than 5 days have images or the description paragraphs are missing in many sections; references/credits are missing or extremely incomplete.\n- 0.0: No images per day and no references/credits section.", "expectation": "Seven day sections each with a short paragraph and image, plus a references/credits section listing research sources and royalty-free image attributions."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Compliance Verification", "description": "Verify factual/structural correctness enabled by Stage 1\u2019s shape: day coverage, interests alignment, citations/credits to royalty-free platforms, feasibility of the route, and inclusion of requested destinations.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Seven Day Sections Present (Text Check)", "description": "Checks the PDF/DOCX text for labeled day sections: Day 1 through Day 7.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output:\n            return 0.0, \"No output.\"\n        if not output.is_document:\n            return 0.0, \"Primary output is not a document.\"\n        text = \"\"\n        try:\n            if output.filename.lower().endswith('.pdf'):\n                text = context.files.read_pdf_text(output.id) or \"\"\n            elif output.filename.lower().endswith('.docx'):\n                text = context.files.read_docx_text(output.id) or \"\"\n            else:\n                text = context.files.read_text(output.id) or \"\"\n        except Exception as e:\n            return 0.0, f\"Failed to extract text: {e}\"\n        t = text.lower()\n        days_found = set()\n        for m in re.finditer(r\"\\bday\\s*([1-7])\\b\", t):\n            days_found.add(m.group(1))\n        score = len(days_found) / 7.0\n        missing = [str(i) for i in range(1,8) if str(i) not in days_found]\n        feedback = f\"Days found: {sorted(days_found)}; Missing: {missing}\"\n        return max(0.0, min(1.0, score)), feedback\n    except Exception as e:\n        return 0.0, f\"Exception: {e}\""}, {"type": "code", "name": "Interests Coverage Keywords", "description": "Checks for the family\u2019s interest keywords across the document: swimming, snorkeling, jet skiing, paddleboarding, fishing, and dining.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document output.\"\n        try:\n            if output.filename.lower().endswith('.pdf'):\n                text = context.files.read_pdf_text(output.id) or \"\"\n            elif output.filename.lower().endswith('.docx'):\n                text = context.files.read_docx_text(output.id) or \"\"\n            else:\n                text = context.files.read_text(output.id) or \"\"\n        except Exception as e:\n            return 0.0, f\"Failed to extract text: {e}\"\n        t = text.lower()\n        groups = {\n            'swimming': [r'\\bswim', r'\\bswimming'],\n            'snorkeling': [r'\\bsnorkel', r'\\bsnorkeling', r'\\bsnorkelling'],\n            'jet skiing': [r'jet\\s*-?ski', r'jetski'],\n            'paddleboarding': [r'paddleboard', r'stand-?up\\s*paddle', r'\\bsup\\b'],\n            'fishing': [r'\\bfish', r'\\bfishing'],\n            'dining': [r'\\bdining\\b', r'\\brestaurant', r'fine\\s+dining']\n        }\n        present = []\n        for key, patterns in groups.items():\n            found = any(re.search(p, t) for p in patterns)\n            if found:\n                present.append(key)\n        score = len(present) / len(groups)\n        missing = [k for k in groups.keys() if k not in present]\n        feedback = f\"Interest areas present: {present}; Missing: {missing}\"\n        return max(0.0, min(1.0, score)), feedback\n    except Exception as e:\n        return 0.0, f\"Exception: {e}\""}, {"type": "code", "name": "Citations and Image Credit Platforms", "description": "Checks for presence of travel research sources and royalty-free image platforms with links/credits.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document output.\"\n        try:\n            if output.filename.lower().endswith('.pdf'):\n                text = context.files.read_pdf_text(output.id) or \"\"\n            elif output.filename.lower().endswith('.docx'):\n                text = context.files.read_docx_text(output.id) or \"\"\n            else:\n                text = context.files.read_text(output.id) or \"\"\n        except Exception as e:\n            return 0.0, f\"Failed to extract text: {e}\"\n        t = text.lower()\n        # Travel research sources\n        travel_sources = ['lonely planet', 'nassau paradise island', 'bahamas.com', 'travel + leisure', 'travel and leisure', 'travel+leisure']\n        ts_count = sum(1 for s in travel_sources if s in t)\n        ts_score = min(ts_count / 2.0, 1.0)  # need at least 2\n        # Image platforms (royalty-free)\n        image_platforms = ['unsplash', 'pexels', 'pixabay', 'wikimedia commons', 'flickr', 'creative commons']\n        ip_count = sum(1 for s in image_platforms if s in t)\n        # Expect explicit platform names present; approximate target ~7 credits for 7 days\n        ip_score = min(ip_count / 7.0, 1.0)\n        # Links present\n        url_count = len(re.findall(r'https?://\\S+', text))\n        link_score = min(url_count / 3.0, 1.0)\n        score = (ts_score + ip_score + link_score) / 3.0\n        feedback = f\"Travel sources matched: {ts_count}; Image platform mentions: {ip_count}; URLs: {url_count}\"\n        return max(0.0, min(1.0, score)), feedback\n    except Exception as e:\n        return 0.0, f\"Exception: {e}\""}, {"type": "code", "name": "Dining Venues Mentioned", "description": "Checks for presence of specific dining venue names or dining callouts across the itinerary.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document output.\"\n        try:\n            if output.filename.lower().endswith('.pdf'):\n                text = context.files.read_pdf_text(output.id) or \"\"\n            elif output.filename.lower().endswith('.docx'):\n                text = context.files.read_docx_text(output.id) or \"\"\n            else:\n                text = context.files.read_text(output.id) or \"\"\n        except Exception as e:\n            return 0.0, f\"Failed to extract text: {e}\"\n        # Count likely venue names that end with food-service terms\n        pattern = r\"\\b([A-Z][A-Za-z&'\\s]{2,})(Restaurant|Cafe|Bistro|Grill|Bar|Lounge)\\b\"\n        names = set(m.group(0) for m in re.finditer(pattern, text))\n        count = len(names)\n        # Fallback: generic dining mentions\n        generic_count = len(re.findall(r\"\\brestaurant|dining|dinner|lunch\\b\", text, flags=re.IGNORECASE))\n        if count == 0 and generic_count >= 5:\n            score = 0.5\n            feedback = f\"No specific venue names matched; generic dining mentions: {generic_count}\"\n        else:\n            score = min(count / 5.0, 1.0)\n            feedback = f\"Dining venues matched: {count} ({', '.join(list(names)[:5])})\"\n        return max(0.0, min(1.0, score)), feedback\n    except Exception as e:\n        return 0.0, f\"Exception: {e}\""}, {"type": "llm_judge", "name": "Images per Day and Credit Compliance", "description": "Visually confirm each day has an image and that image credits name legitimate royalty-free platforms with attribution/links.", "weight": 2.0, "judge_prompt": "Visually inspect the PDF. For each of the 7 day sections:\n- Confirm at least one image is present near the day description.\n- Confirm that image credits/attributions are visible in the document (either adjacent to the image, per-day, or in a consolidated references section) and explicitly name royalty-free platforms (e.g., Unsplash, Pexels, Pixabay, Wikimedia Commons) and/or include links.\n\nScoring (0 to 2.0):\n- 2.0: All 7 days have images and clear royalty-free credits with platform names and links.\n- 1.5: All days have images; credits exist but are partial (e.g., some missing links or 1\u20132 missing platform names).\n- 1.0: 1\u20132 days missing images or credits; most others are properly credited.\n- 0.5: 3\u20134 days missing images or credits.\n- 0.0: 5+ days missing images or no visible credits at all.", "expectation": "Every day has an image and credited to a legitimate royalty-free platform with attribution/links."}, {"type": "llm_judge", "name": "Activity and Dining Specificity (3\u20134 sentences/day)", "description": "Check that each day\u2019s paragraph is 3\u20134 sentences and includes concrete activities and at least one named dining venue.", "weight": 2.0, "judge_prompt": "For each day section (Day 1\u2013Day 7), check the description paragraph length and specificity.\nRequirements per day:\n- A concise paragraph of 3\u20134 sentences (not bullet-only).\n- Mentions at least one activity aligned to interests (swimming/snorkeling/jet skiing/paddleboarding/fishing) and at least one named dining venue (restaurant/bar/grill/cafe) with recognizable or plausible names for the Bahamas.\n\nScoring (0 to 2.0):\n- 2.0: All 7 days meet both sentence-length and specificity requirements.\n- 1.5: 1 day slightly short/long or missing a dining venue/activity mention; others compliant.\n- 1.0: 2\u20133 days fail length or specificity criteria.\n- 0.5: 4\u20135 days fail.\n- 0.0: 6\u20137 days fail.", "expectation": "Seven concise, specific paragraphs with activities and named dining venues."}, {"type": "llm_judge", "name": "Itinerary Feasibility and Route Logic (Yacht)", "description": "Assess whether the day-to-day sequence is geographically coherent and feasible for a 7-day yacht trip in the Bahamas.", "weight": 2.0, "judge_prompt": "Evaluate the geographic and logistical feasibility for a 7-day yacht trip. Consider:\n- Logical routing between islands/keys (e.g., Nassau \u2194 Rose Island \u2194 Eleuthera/Harbour Island \u2194 Exumas such as Highbourne/Staniel), minimizing excessive backtracking.\n- Reasonable cruising legs for a family-friendly pace (time for activities and dining each day).\n- Feasible anchorage/marina options implicitly indicated by destinations.\n\nScoring (0 to 2.0):\n- 2.0: Clear, efficient routing suitable for a 7-day family yacht trip; daily legs appear reasonable.\n- 1.5: Mostly logical with a minor inefficiency but still feasible.\n- 1.0: Noticeable inefficiencies or tight days but still possible.\n- 0.5: Poorly routed with unrealistic legs or major backtracking.\n- 0.0: Logistically implausible for 7 days.", "expectation": "A coherent route suitable for a 7-day family yacht itinerary with feasible daily legs."}, {"type": "llm_judge", "name": "Requested Destination Coverage", "description": "Check inclusion of the client\u2019s requested places (some or all of: Nassau, Harbour Island, Eleuthera, Staniel Cay, Highbourne Cay, Rose Island/Nassau).", "weight": 1.6, "judge_prompt": "Verify that the itinerary includes some or all of these destinations: Nassau, Harbour Island, Eleuthera, Staniel Cay, Highbourne Cay, Rose Island/Nassau.\n\nScoring (0 to 1.6):\n- 1.6: Includes 5\u20136 of the listed destinations.\n- 1.2: Includes 4 of the listed destinations.\n- 0.8: Includes 3 of the listed destinations.\n- 0.4: Includes 2 of the listed destinations.\n- 0.0: Includes 0\u20131 of the listed destinations.", "expectation": "Include at least four of the specified destinations in the 7-day plan."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Client Fit", "description": "Holistic LLM assessment of polish, clarity, luxury positioning, and family suitability.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Writing Quality and Tone for UHNWI", "description": "Evaluate clarity, concision, luxury tone, and freedom from errors.", "weight": 1.5, "judge_prompt": "Assess writing quality: clarity, concision appropriate for a two-page limit, luxury-oriented yet warm family tone, and grammatical/spelling accuracy.\nScoring (0 to 1.5):\n- 1.5: Elegant, concise, error-free, and on-brand for luxury clientele.\n- 1.0: Minor issues but professional and polished overall.\n- 0.5: Noticeable awkwardness or several errors.\n- 0.0: Poorly written or error-ridden.", "expectation": "Polished, concise, error-free writing with a luxury yet family-friendly tone."}, {"type": "llm_judge", "name": "Visual Layout and Readability", "description": "Evaluate visual balance, readability on two pages, and professional formatting.", "weight": 1.5, "judge_prompt": "Assess the layout: balanced use of white space, images sized appropriately, consistent headings, legible fonts, and good scan-ability across exactly two pages.\nScoring (0 to 1.5):\n- 1.5: Highly readable and well-balanced professional layout.\n- 1.0: Generally readable with minor layout issues.\n- 0.5: Cluttered or inconsistent formatting.\n- 0.0: Hard to read or unprofessional layout.", "expectation": "Clean, balanced, and professional layout suitable for a two-page deliverable."}, {"type": "llm_judge", "name": "Family-Friendliness and Luxury Positioning", "description": "Evaluate whether activities/dining are age-appropriate for 7- and 9-year-olds while maintaining luxury standards.", "weight": 1.5, "judge_prompt": "Judge alignment with the family profile: age-appropriate activities for children (7 and 9), safe and comfortable pacing, and upscale dining suitable for a luxury clientele.\nScoring (0 to 1.5):\n- 1.5: Excellent alignment\u2014age-appropriate, safe-feeling, and upscale.\n- 1.0: Mostly aligned with minor mismatches.\n- 0.5: Several choices not suited for kids or not luxury.\n- 0.0: Poor alignment.", "expectation": "A refined, family-appropriate plan that still feels ultra-premium."}, {"type": "llm_judge", "name": "Pacing, Variety, and Practicality", "description": "Evaluate daily balance of activity/rest, variety across days, and subtle practicality cues (marinas/anchorage, timing).", "weight": 1.5, "judge_prompt": "Assess pacing and practicality: balance of relaxation and activities, variety (beaches, snorkeling, water sports, dining), and subtle logistical awareness (time to move between stops, plausible rhythm for a family).\nScoring (0 to 1.5):\n- 1.5: Excellent balance and practical flow across all seven days.\n- 1.0: Generally good with a few tight or repetitive days.\n- 0.5: Weak balance or repetitive; practicality concerns.\n- 0.0: Impractical pacing or poor variety.", "expectation": "A balanced, varied, and practical seven-day flow for a yacht holiday."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "4d61a19a-8438-4d4c-9fc2-cf167e36dcd6", "rubric": {"category_name": "Retail Ops: Promotion Projection Form + Training Deck", "rationale": "Mixed deliverables: an Excel template for promotion projections and a training deck to drive adoption. Stage 1 strictly enforces structure for both artifacts (LLM-only). Stage 2 verifies correctness and cross-file consistency using a mix of code (for spreadsheet structure/fields) and LLM judges (for process alignment and cross-references). Stage 3 evaluates professional quality and readiness for rollout.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 GATE: Shape and Structure Enforcement", "description": "LLM-only gate. Both artifacts must exist with the exact, verifiable structure required to enable automated checks.", "is_required": true, "max_points": 5.0, "min_score_to_pass": 4.0, "rules": [{"type": "llm_judge", "name": "Excel Template Structure: 'Promotion Projection Form Template'", "description": "Verify the Excel file exists and contains the mandated sheets and sections with required columns.", "weight": 3.0, "judge_prompt": "You are evaluating whether the candidate produced a properly structured Excel template for store promotion projections.\n\nCheck ALL outputs to find an Excel file named like \u201cPromotion Projection Form Template\u201d (flexible on exact casing/spaces). If multiple spreadsheets exist, choose the one that best fits the described structure.\n\nRequired FORMAT:\n- Must be an Excel file (.xlsx).\n- Workbook title/name should include: \"Promotion Projection Form Template\" (flexible on exact punctuation).\n\nRequired SHEETS and STRUCTURE:\n1) Sheet: \"Form\" (or similarly named: \"Promotion Form\", \"Promo Form\", \"Projection Form\")\n   - A tabular section intended to be shared with stores.\n   - Columns must cover, in clear headers (flexible naming):\n     \u2022 Promotion identifier or name\n     \u2022 Promotion start date\n     \u2022 Promotion end date\n     \u2022 Product / item description\n     \u2022 SKU or PLU\n     \u2022 Regular price\n     \u2022 Promo price (or sale price)\n     \u2022 Discount type (e.g., % off, BOGO)\n     \u2022 Historical context (e.g., previous/last promo date and units and/or sales)\n     \u2022 Merchandising notes\n     \u2022 Store Projected Units (store-editable)\n     \u2022 Store Sign-off (at minimum Name/Initials and Date) (store-editable)\n   - The template should distinguish store-editable fields (e.g., instructions, color, note). Stores are expected to edit ONLY \"Projected Units\" and \"Sign-off\" fields.\n\n2) Sheet: \"Guidance\" (or similarly named: \"Instructions\", \"How To\", \"Overview\")\n   - Brief instructions for Regional Leadership on completing the form prior to sharing with stores, and a note that the file will live in SharePoint folder \"Promotions\".\n\nOptional but encouraged:\n- Sheet: \"History\" / \"Previous Promotions\" with historical reference rows OR a dedicated historical section on the Form.\n\nScoring:\n- 3.0: Excel exists with a \"Form\" sheet including all required column types, plus a \"Guidance/Instructions\" sheet. Store-editable fields clearly identified (by label or visual note).\n- 2.2: Excel exists with \"Form\" sheet covering most required columns (1\u20132 minor elements missing) and a \"Guidance/Instructions\" sheet.\n- 1.5: Excel exists with a viable \"Form\" sheet but missing the Guidance sheet OR missing 3\u20134 required columns.\n- 0.7: Excel exists but structure is largely incomplete (missing many required columns) or store-editable fields not present/identifiable.\n- 0.0: No suitable Excel file found or wrong format.\n\nOnly evaluate presence/structure, not correctness of data.", "expectation": "An .xlsx with a Form sheet including the specified fields and a Guidance/Instructions sheet. Store-editable fields limited to Projected Units and Sign-off."}, {"type": "llm_judge", "name": "Training Deck Structure: 'Promo Projection Form'", "description": "Verify the PowerPoint or PDF deck exists with the required training flow and under 8 slides.", "weight": 2.0, "judge_prompt": "You are evaluating whether the candidate produced a concise training deck for Meat Team Leaders.\n\nCheck ALL outputs for a slide deck titled \"Promo Projection Form\". Accept .pptx or .pdf exports. If multiple decks, choose the best fit.\n\nRequired FORMAT:\n- PPTX or PDF of slides.\n- Fewer than 8 slides total.\n\nRequired CONTENT (flexible titles, but clear coverage):\n1) Title: \"Promo Projection Form\" or equivalent.\n2) What/Why: What the tool is and why we\u2019re using it (store inclusion, right-sizing promo quantities).\n3) Where/Access: How stores will get the form and where it lives (SharePoint folder named \"Promotions\").\n4) What stores fill out: Explicitly call out store-editable fields: \"Projected Units\" and \"Sign-off\" only.\n5) Process flow: How the process will work (who fills what, when, and how submission/collection occurs).\n6) Sample of the form with mock data so they can see how it looks when filled out (screenshot or embedded example).\n7) Recap and a slide that leaves room for discussion or questions (Q&A).\n\nScoring:\n- 2.0: Valid deck with <8 slides covering all 7 content items (flexible naming).\n- 1.5: Valid deck with <8 slides covering 5\u20136 of the items.\n- 1.0: Valid deck with <8 slides covering 3\u20134 items.\n- 0.5: Valid deck but only 1\u20132 items or exceeds slide limit.\n- 0.0: No suitable deck or wrong format.\n\nOnly check presence/coverage and slide count, not content quality.", "expectation": "A concise deck (<8 slides) that explains what/why, where to find the form, what fields stores fill, process, shows a sample, and ends with recap/Q&A."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Correctness and Cross-File Verification", "description": "Verify correctness, consistency, and feasibility using code for deterministic checks and LLM for nuanced evaluation.", "is_required": false, "max_points": 7.5, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Excel Sheets Present (Form + Guidance)", "description": "Programmatically confirm an Excel file exists with a Form-like sheet and a Guidance/Instructions-like sheet.", "weight": 0.5, "code": "import re\\nimport pandas as pd\\n\\ndef evaluate(workflow, context):\\n    try:\\n        # Find the most relevant spreadsheet output\\n        excel_res = None\\n        for r in context.get_all_outputs():\\n            try:\\n                if getattr(r, 'is_spreadsheet', False):\\n                    name = (getattr(r, 'name', '') or '').lower()\\n                    if 'promotion' in name and 'form' in name:\\n                        excel_res = r\\n                        break\\n            except Exception:\\n                continue\\n        if excel_res is None:\\n            # fallback to any spreadsheet\\n            for r in context.get_all_outputs():\\n                if getattr(r, 'is_spreadsheet', False):\\n                    excel_res = r\\n                    break\\n        if excel_res is None:\\n            return 0.0\\n\\n        path = context.files.get_path(excel_res.id)\\n        xls = pd.ExcelFile(path)\\n        sheet_names = [s.lower() for s in xls.sheet_names]\\n        has_form = any(any(k in s for k in ['form', 'projection']) for s in sheet_names)\\n        has_guidance = any(any(k in s for k in ['guidance', 'instruction', 'how to', 'how-to', 'overview']) for s in sheet_names)\\n\\n        score = 0.0\\n        if has_form:\\n            score += 0.35\\n        if has_guidance:\\n            score += 0.15\\n        # Cap to weight\\n        return min(score, 0.5)\\n    except Exception:\\n        return 0.0\\n"}, {"type": "code", "name": "Required Columns Coverage on Form Sheet", "description": "Check that the Form sheet includes the key fields (fuzzy matching on column headers).", "weight": 0.5, "code": "import re\\nimport pandas as pd\\n\\nREQUIRED_FIELDS = [\\n    ('promotion id/name', ['promo id', 'promotion id', 'promotion', 'promo name', 'name', 'title']),\\n    ('start date', ['start date', 'start', 'begin']),\\n    ('end date', ['end date', 'end', 'finish']),\\n    ('product/item', ['product', 'item', 'description']),\\n    ('sku/plu', ['sku', 'plu', 'upc']),\\n    ('regular price', ['regular price', 'reg price', 'list price', 'base price']),\\n    ('promo price', ['promo price', 'sale price', 'discounted price', 'deal price']),\\n    ('discount type', ['discount type', 'offer type', 'mechanic', 'bogo', '% off']),\\n    ('historical date', ['last promo date', 'previous promo date', 'prior promo date', 'history date']),\\n    ('historical units/sales', ['last units', 'previous units', 'prior units', 'units last promo', 'sales last promo', 'history units', 'history sales']),\\n    ('merch notes', ['merchandising notes', 'merch notes', 'display notes', 'placement notes']),\\n    ('store projected units', ['projected units', 'store projection', 'store units', 'forecast units']),\\n    ('sign-off name', ['sign-off', 'sign off', 'approved by', 'manager name', 'initials']),\\n    ('sign-off date', ['sign-off date', 'sign off date', 'approval date', 'signed date'])\\n]\\n\\ndef _get_form_sheet_name(xls):\\n    names = [s for s in xls.sheet_names]\\n    low = [s.lower() for s in names]\\n    # Prefer sheets with 'form' or 'projection' in name\\n    for i, s in enumerate(low):\\n        if 'form' in s or 'projection' in s:\\n            return names[i]\\n    # fallback to first sheet\\n    return names[0] if names else None\\n\\ndef evaluate(workflow, context):\\n    try:\\n        excel_res = None\\n        for r in context.get_all_outputs():\\n            if getattr(r, 'is_spreadsheet', False):\\n                excel_res = r\\n                break\\n        if not excel_res:\\n            return 0.0\\n        path = context.files.get_path(excel_res.id)\\n        xls = pd.ExcelFile(path)\\n        form_sheet = _get_form_sheet_name(xls)\\n        if not form_sheet:\\n            return 0.0\\n        # Try header inference\\n        try:\\n            df = pd.read_excel(path, sheet_name=form_sheet, header=0)\\n            headers = [str(c).strip().lower() for c in df.columns]\\n        except Exception:\\n            # Fallback: read without header and infer first non-empty row as headers\\n            raw = pd.read_excel(path, sheet_name=form_sheet, header=None)\\n            headers = []\\n            for i in range(min(5, len(raw))):\\n                row = [str(x).strip().lower() for x in list(raw.iloc[i].values)]\\n                if sum(1 for x in row if x and x != 'nan') >= 4:\\n                    headers = row\\n                    break\\n        header_text = ' | '.join(headers)\\n        covered = 0\\n        for label, keys in REQUIRED_FIELDS:\\n            found = any(k in header_text for k in keys)\\n            if found:\\n                covered += 1\\n        coverage_ratio = covered / max(len(REQUIRED_FIELDS), 1)\\n        # Map coverage to score (>=75% => full; linear otherwise)\\n        target = 0.75\\n        if coverage_ratio >= target:\\n            return 0.5\\n        else:\\n            return max(0.0, 0.5 * (coverage_ratio / target))\\n    except Exception:\\n        return 0.0\\n"}, {"type": "code", "name": "Basic Plausibility Checks (if sample rows exist)", "description": "If any data rows are present on the Form, check price and date plausibility; otherwise award partial credit for a clean, empty template.", "weight": 0.5, "code": "import re\\nimport pandas as pd\\nimport numpy as np\\n\\nPRICE_COLS = [['regular price', 'reg price', 'base price', 'list price'],\\n              ['promo price', 'sale price', 'discounted price', 'deal price']]\nDATE_START_KEYS = ['start date', 'start', 'begin']\nDATE_END_KEYS = ['end date', 'end', 'finish']\nUNITS_KEYS = ['projected units', 'store projection', 'store units', 'forecast units']\n\ndef find_col(cols, keys):\n    for i, c in enumerate(cols):\n        lc = str(c).lower()\n        if any(k in lc for k in keys):\n            return i\n    return None\n\ndef evaluate(workflow, context):\n    try:\n        excel_res = None\n        for r in context.get_all_outputs():\n            if getattr(r, 'is_spreadsheet', False):\n                excel_res = r\n                break\n        if not excel_res:\n            return 0.0\n        path = context.files.get_path(excel_res.id)\n        xls = pd.ExcelFile(path)\n        sheet = None\n        for s in xls.sheet_names:\n            if 'form' in s.lower() or 'projection' in s.lower():\n                sheet = s\n                break\n        sheet = sheet or xls.sheet_names[0]\n        df = pd.read_excel(path, sheet_name=sheet, header=0)\n        if df.empty or len(df.columns) == 0:\n            # No data rows: neutral partial credit for a template\n            return 0.25\n        cols = list(df.columns)\n        # Identify columns\n        reg_idx = find_col(cols, PRICE_COLS[0])\n        promo_idx = find_col(cols, PRICE_COLS[1])\n        start_idx = find_col(cols, DATE_START_KEYS)\n        end_idx = find_col(cols, DATE_END_KEYS)\n        units_idx = find_col(cols, UNITS_KEYS)\n        score = 0.0\n        # Price plausibility\n        if reg_idx is not None and promo_idx is not None:\n            reg = pd.to_numeric(df.iloc[:, reg_idx], errors='coerce')\n            promo = pd.to_numeric(df.iloc[:, promo_idx], errors='coerce')\n            mask = reg.notna() & promo.notna()\n            if mask.sum() > 0:\n                ok = (promo[mask] <= reg[mask] + 1e-9).mean()\n                score += 0.2 * float(ok)\n        # Date plausibility\n        if start_idx is not None and end_idx is not None:\n            sd = pd.to_datetime(df.iloc[:, start_idx], errors='coerce')\n            ed = pd.to_datetime(df.iloc[:, end_idx], errors='coerce')\n            mask = sd.notna() & ed.notna()\n            if mask.sum() > 0:\n                ok = (sd[mask] <= ed[mask]).mean()\n                score += 0.2 * float(ok)\n        # Units column empty or non-negative\n        if units_idx is not None:\n            units_raw = df.iloc[:, units_idx]\n            # either blank (template) or non-negative numeric\n            numeric = pd.to_numeric(units_raw, errors='coerce')\n            blank_ratio = units_raw.isna().mean()\n            nonneg_ratio = (numeric.dropna() >= 0).mean() if numeric.notna().any() else 1.0\n            score += 0.1 * float(max(blank_ratio, nonneg_ratio))\n        # If no checks contributed (e.g., columns missing), award small partial credit if template-like\n        if score == 0.0:\n            return 0.2\n        return min(score, 0.5)\n    except Exception:\n        return 0.0\n"}, {"type": "llm_judge", "name": "Cross-File Alignment: Deck matches Template fields and edit-scope", "description": "Deck\u2019s sample form and instructions should reflect the same fields as the Excel Form sheet and emphasize that only Projected Units and Sign-off are store-editable.", "weight": 2.0, "judge_prompt": "Compare the training deck with the Excel template you identified in Stage 1. Focus on alignment between what the deck shows and what the template contains.\n\nChecks:\n- Does the deck\u2019s sample (screenshot or reproduced table) show the same key fields as the Excel Form (dates, product, SKU/PLU, regular and promo price, historical info, merch notes)?\n- Does the deck explicitly state that stores should only fill Projected Units and Sign-off, matching the template\u2019s intent?\n- Are any additional fields incorrectly indicated as editable in the deck (which would conflict with the template)?\n\nScoring:\n- 2.0: Clear alignment across fields and edit-scope; no contradictions.\n- 1.3: Mostly aligned with minor omissions or ambiguities.\n- 0.7: Partial alignment; notable discrepancies or unclear edit-scope.\n- 0.0: Misaligned or no usable comparison.\n", "expectation": "Deck visuals and text match the Excel Form fields and edit-scope with no contradictions."}, {"type": "llm_judge", "name": "Process and Access Accuracy", "description": "Deck must clearly explain access via SharePoint Promotions folder and outline the submission workflow.", "weight": 2.0, "judge_prompt": "Evaluate whether the deck clearly explains how stores obtain the form and how the process works.\n\nChecks:\n- Explicit reference to SharePoint, with the folder named \u201cPromotions\u201d.\n- How stores will receive the form (e.g., link or shared folder) and where to submit/return it (or how it\u2019s collected).\n- Roles and timing: who fills what, when, due dates/timeframe.\n\nScoring:\n- 2.0: All items clearly covered.\n- 1.3: Minor gaps (e.g., timing not specific) but overall workable.\n- 0.7: Vague on access or process flow.\n- 0.0: Missing or incorrect.\n", "expectation": "A clear, actionable access and workflow explanation tied to the SharePoint Promotions folder."}, {"type": "llm_judge", "name": "Template Content Completeness", "description": "Excel Form contains all required informational fields for Regional completion and clearly marked store-editable fields.", "weight": 2.0, "judge_prompt": "Assess the Excel template content itself for completeness relative to requirements.\n\nChecks on the Form sheet:\n- Includes promotion dates, product/item, SKU/PLU, regular price, promo price, discount type, historical context (at least prior date and units/sales), and merchandising notes.\n- Clearly contains store-editable fields: Projected Units and Sign-off (Name/Date), and these are identified as the only store-editable fields.\n- Guidance/Instructions sheet indicates that the template will live on SharePoint in the \u201cPromotions\u201d folder.\n\nScoring:\n- 2.0: All required elements present and clear.\n- 1.3: Minor omissions (1\u20132 fields) or slightly unclear editable labeling.\n- 0.7: Several omissions or ambiguous editable fields.\n- 0.0: Largely incomplete or incorrect.\n", "expectation": "Form has all specified fields plus clear store-editable indicators, with guidance mentioning SharePoint \u2018Promotions\u2019 folder."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3: Professional Quality and Adoption Readiness", "description": "Holistic LLM assessment of presentation quality, clarity, and change-management effectiveness.", "is_required": false, "max_points": 7.5, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity and Professional Presentation", "description": "Assess formatting, readability, and overall professionalism of both the template and the deck.", "weight": 2.5, "judge_prompt": "Evaluate overall professionalism:\n- Excel: Clear headers, readable layout, sensible column order, optional features like frozen header row/filters, concise instructions. Visual cue for store-editable fields is a plus.\n- Deck: Clean layout, appropriate fonts/colors, minimal clutter, under 8 slides as required.\n\nScoring:\n- 2.5: Highly professional; easy to read and use.\n- 1.7: Generally good with minor layout/clarity issues.\n- 1.0: Adequate but somewhat messy or hard to follow.\n- 0.0: Poorly formatted or confusing.\n", "expectation": "Clean, readable, professional layouts that facilitate quick understanding."}, {"type": "llm_judge", "name": "Training Effectiveness", "description": "Determine if the deck effectively teaches how to use the form, including a realistic sample and Q&A space.", "weight": 2.5, "judge_prompt": "Consider whether the training would work in a real session:\n- Step-by-step or clearly sequenced guidance from access to completion to submission.\n- Sample of the form contains mock data that realistically demonstrates how fields are filled.\n- Ends with a concise recap and explicit Q&A or discussion prompt.\n\nScoring:\n- 2.5: Excellent: clear flow, strong sample, and explicit recap/Q&A.\n- 1.7: Good but missing one element or light on detail.\n- 1.0: Basic coverage; multiple elements thin or missing.\n- 0.0: Not effective as training material.\n", "expectation": "A concise, clear training flow with a realistic example and a proper close-out."}, {"type": "llm_judge", "name": "Adoption and Change-Management Readiness", "description": "Evaluate whether the materials support adoption across stores: rationale, roles, timing, and contacts.", "weight": 2.5, "judge_prompt": "Assess adoption readiness:\n- Rationale/benefits for stores (why this helps right-size quantities and includes their input).\n- Roles and responsibilities are clear (Regional vs. Stores).\n- Timelines/due dates or cadence implied (e.g., weekly, per-promo) and where to ask for help (contact info).\n\nScoring:\n- 2.5: Strong change-management framing; roles/timing/contacts are clear.\n- 1.7: Adequate with minor gaps.\n- 1.0: Vague on roles or timing; benefits unclear.\n- 0.0: Missing key adoption elements.\n", "expectation": "Materials that make it easy for stores to adopt the process with clarity on why, who, when, and how to get help."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "1137e2bb-bdf9-4876-b572-f29b7de5e595", "rubric": {"category_name": "Wholesale Trade \u2022 Order Clerks \u2014 SKU-Level Order Audit (Price Mismatch & Case Pack)", "rationale": "Mixed-output task: an Excel audit workbook plus a brief management-facing Word/PDF summary. Stage 1 mandates a strict, self-documenting Excel shape (line-level audit + SKU/PO summary) and presence of a summary document. Stage 2 verifies correctness via focused code checks (flag logic, totals) and LLM cross-checks (pivot structure, sample consistency, doc alignment). Stage 3 evaluates professional quality and actionability for management and order operations stakeholders.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (LLM-only)", "description": "Output must include a structured Excel audit workbook and a brief summary document with specific, verifiable sections. Only structure/presence is evaluated here (no correctness).", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Excel + Summary Doc Present with Required Sheets/Columns", "description": "Verify the candidate provided BOTH: (1) an Excel workbook with a line-level audit sheet and a SKU/PO error summary sheet, and (2) a brief summary doc (DOCX or PDF). Check structure and presence only.", "weight": 4.0, "judge_prompt": "You are validating the SHAPE ONLY (not correctness) of the candidate outputs. Review ALL files the candidate returned.\n\nPass criteria (structure only):\nA) Excel workbook present with BOTH of the following sheets (names can vary slightly; be flexible with synonyms like 'Line Audit'/'Validation' and 'Error Summary'/'SKU Error Summary'/'Pivot'):\n  1) Line-level audit sheet (e.g., 'Line Audit', 'Line-Level Audit', 'Validation', 'Audit'). It must include columns covering:\n     - SKU (item identifier)\n     - PO Number\n     - Ordered Units\n     - Entered Unit Price\n     - Expected Unit Price\n     - UOM (Unit Order Multiple / Unit of Measure)\n     - Case Pack\n     - Ship-to Location\n     - Added error fields:\n       \u2022 Price Mismatch Error (per-line flag)\n       \u2022 Case Pack Error (per-line flag)\n       \u2022 Errors Total (sum of the two flags)\n       \u2022 Error Type Summary (text describing error type per line)\n     Columns can have near-equivalent names; judge flexibly as long as intent is clear and all fields are represented.\n  2) Error summary sheet (e.g., 'Error Summary', 'SKU Error Summary', 'Pivot', 'Summary by SKU') that aggregates errors at the SKU level and allows drill-down to PO level (e.g., a pivot with rows SKU then PO, or separate tables enabling SKU\u2192PO navigation). It must show measures for Price Mismatch Errors, Case Pack Errors, and Total Errors.\n\nB) Brief narrative summary document present as DOCX or PDF (not plain text), approximately 0.5\u20131.5 pages, with clear sections for: Overview/Context, Findings/Error Types observed, and Recommendations/Next Steps.\n\nScoring (structure only):\n- 4.0: Excel workbook includes BOTH required sheets with the required fields/sections AND the summary doc (DOCX/PDF) is present with the three sections.\n- 3.0: Excel workbook has both sheets and fields but the summary doc is missing, or the doc is present but one minor column/section is missing while overall structure is still clearly usable for verification.\n- 2.0: Excel exists but missing one required sheet (line audit or summary) OR multiple key columns from the line audit are absent.\n- 0.0: No valid Excel workbook, or the structure is unusable; OR only a text/markdown summary without a DOCX/PDF.\n\nDo NOT judge correctness or content quality here\u2014only the presence and structure needed for later verification.", "expectation": "An Excel workbook with a line-level audit including specified columns, a SKU/PO error summary sheet with required metrics, plus a DOCX/PDF brief summary with Overview, Findings, and Recommendations."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification of Calculations and Cross-Checks", "description": "Now that the structure is valid, verify the correctness of flags, totals, and summary aggregation using code checks and LLM cross-checks.", "is_required": true, "max_points": 10.0, "min_score_to_pass": 5.0, "rules": [{"type": "code", "name": "Column Presence & Data Sanity (basic)", "description": "Locate the Excel audit sheet and verify key columns are present and data looks plausible (UOM values, non-negative quantities/prices).", "weight": 0.4, "code": "import re\nimport json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    # Find a spreadsheet output\n    ss = None\n    for r in context.get_all_outputs():\n        try:\n            if r.is_spreadsheet:\n                ss = r\n                break\n        except Exception:\n            continue\n    if not ss:\n        return 0.0, 'No spreadsheet output found.'\n\n    # Load sheet names\n    try:\n        xls_path = context.files.get_path(ss.id)\n        xls = pd.ExcelFile(xls_path)\n        sheet_names = [s for s in xls.sheet_names]\n    except Exception as e:\n        return 0.0, f'Could not open Excel: {e}'\n\n    # Heuristic to find line-level audit sheet\n    def pick_audit_sheet(names):\n        keys = ['audit','validation','line','lines','detail','check']\n        for n in names:\n            ln = n.lower()\n            if any(k in ln for k in keys):\n                return n\n        return names[0] if names else None\n\n    audit_sheet = pick_audit_sheet(sheet_names)\n    if not audit_sheet:\n        return 0.0, 'No sheets found.'\n\n    try:\n        df = pd.read_excel(xls_path, sheet_name=audit_sheet)\n    except Exception as e:\n        return 0.0, f'Failed reading audit sheet: {e}'\n\n    # Normalize columns\n    cols = [str(c) for c in df.columns]\n    norm = {c: re.sub(r'[^a-z0-9]+','', str(c).lower()) for c in cols}\n\n    def find_col(cands):\n        candsn = [re.sub(r'[^a-z0-9]+','', c.lower()) for c in cands]\n        # exact\n        for orig, n in norm.items():\n            if n in candsn:\n                return orig\n        # contains\n        for orig, n in norm.items():\n            if any(cn in n for cn in candsn):\n                return orig\n        return None\n\n    required_map = {\n        'sku': ['sku','item','itemnumber','style','skunumber'],\n        'po': ['ponumber','po','purchaseorder','po#','poid'],\n        'ordered_units': ['orderedunits','qtyordered','orderqty','quantity','orderedqty'],\n        'entered_price': ['enteredunitprice','invoiceunitprice','customerprice','billedunitprice','unitpriceentered','unitprice'],\n        'expected_price': ['expectedunitprice','internalprice','listprice','masterprice','standardunitprice','stdprice'],\n        'uom': ['uom','unitordermultiple','unitofmeasure','orderuom'],\n        'case_pack': ['casepack','caseqty','packsize','casequantity','csqty'],\n        'ship_to': ['shiptolocation','shipto','destination','shiptoaddr','shiptoid'],\n        'price_err': ['pricemismatcherror','priceerror','priceflag','pricemismatch'],\n        'case_err': ['casepackerror','casepackflag','packmultipleerror','caseerror'],\n        'total_err': ['errorstotal','totalerrors','errorcount','errors'],\n        'err_text': ['errortypesummary','errorsummary','errortype','issuesummary']\n    }\n\n    found = {k: find_col(v) for k,v in required_map.items()}\n\n    # Score components: column coverage + data plausibility\n    needed_keys = ['sku','po','ordered_units','entered_price','expected_price','uom','case_pack','ship_to','price_err','case_err','total_err','err_text']\n    have_cols = sum(1 for k in needed_keys if found.get(k) is not None)\n    col_score = have_cols / len(needed_keys)\n\n    # Data plausibility checks (light): UOM values and non-negative numeric\n    plaus = 0.0\n    plaus_checks = 0\n    try:\n        if found['uom']:\n            plaus_checks += 1\n            u = df[found['uom']].astype(str).str.lower().str.replace(r'[^a-z]','', regex=True)\n            good = u.isin(['ea','case']).mean()\n            plaus += good\n    except Exception:\n        pass\n    try:\n        if found['ordered_units']:\n            plaus_checks += 1\n            ou = pd.to_numeric(df[found['ordered_units']], errors='coerce')\n            plaus += (ou.dropna()>=0).mean() if len(ou.dropna())>0 else 0\n    except Exception:\n        pass\n    try:\n        if found['entered_price']:\n            plaus_checks += 1\n            ep = pd.to_numeric(df[found['entered_price']], errors='coerce')\n            plaus += (ep.dropna()>=0).mean() if len(ep.dropna())>0 else 0\n    except Exception:\n        pass\n    try:\n        if found['expected_price']:\n            plaus_checks += 1\n            xp = pd.to_numeric(df[found['expected_price']], errors='coerce')\n            plaus += (xp.dropna()>=0).mean() if len(xp.dropna())>0 else 0\n    except Exception:\n        pass\n\n    plaus_score = (plaus / plaus_checks) if plaus_checks>0 else 0\n\n    # Final score weighted between column coverage and plausibility\n    score = 0.6*col_score + 0.4*plaus_score\n    return max(0.0, min(0.4, 0.4*score)), f'Cols found: {have_cols}/{len(needed_keys)}; UOM/numeric plausibility: {round(plaus_score,2)}'"}, {"type": "code", "name": "Price Mismatch Flag Consistency", "description": "Check that the Price Mismatch Error flag matches whether Entered Unit Price differs from Expected Unit Price (with small tolerance for rounding).", "weight": 0.7, "code": "import re\nimport json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    ss = None\n    for r in context.get_all_outputs():\n        try:\n            if r.is_spreadsheet:\n                ss = r\n                break\n        except Exception:\n            continue\n    if not ss:\n        return 0.0, 'No spreadsheet.'\n\n    try:\n        xls_path = context.files.get_path(ss.id)\n        xls = pd.ExcelFile(xls_path)\n        names = xls.sheet_names\n        def pick(names):\n            keys = ['audit','validation','line','detail','check']\n            for n in names:\n                if any(k in n.lower() for k in keys):\n                    return n\n            return names[0]\n        sheet = pick(names)\n        df = pd.read_excel(xls_path, sheet_name=sheet)\n    except Exception as e:\n        return 0.0, f'Failed to read audit sheet: {e}'\n\n    norm = {c: re.sub(r'[^a-z0-9]+','', str(c).lower()) for c in df.columns}\n    def find(cands):\n        candsn = [re.sub(r'[^a-z0-9]+','', c.lower()) for c in cands]\n        for orig, n in norm.items():\n            if n in candsn:\n                return orig\n        for orig, n in norm.items():\n            if any(cn in n for cn in candsn):\n                return orig\n        return None\n\n    ep_col = find(['enteredunitprice','invoiceunitprice','customerprice','billedunitprice','unitpriceentered','unitprice'])\n    xp_col = find(['expectedunitprice','internalprice','listprice','masterprice','standardunitprice','stdprice'])\n    flag_col = find(['pricemismatcherror','priceerror','priceflag','pricemismatch'])\n\n    if not (ep_col and xp_col and flag_col):\n        return 0.0, 'Missing columns for price check.'\n\n    ep = pd.to_numeric(df[ep_col], errors='coerce')\n    xp = pd.to_numeric(df[xp_col], errors='coerce')\n    raw_flag = df[flag_col]\n\n    # Normalize flags to 0/1\n    def to01(s):\n        if pd.isna(s):\n            return 0\n        if isinstance(s,(int,float)):\n            return 1 if s and not pd.isna(s) else 0\n        sv = str(s).strip().lower()\n        return 1 if sv in ['1','y','yes','true','t','error'] else 0\n\n    flag = raw_flag.apply(to01)\n\n    # Compute expected mismatch with small tolerance\n    tol_abs = 0.005\n    tol_rel = 0.001\n    close = np.isclose(ep, xp, rtol=tol_rel, atol=tol_abs)\n    expected_flag = (~close).astype(int)\n\n    valid = (~ep.isna()) & (~xp.isna())\n    if valid.sum() == 0:\n        return 0.0, 'No valid price rows to compare.'\n\n    agree = (flag[valid].astype(int) == expected_flag[valid]).mean()\n    score = float(agree) * 0.7\n    return max(0.0, min(0.7, score)), f'Price flag agreement on valid rows: {round(float(agree)*100,1)}% (tol abs {tol_abs}, rel {tol_rel})'"}, {"type": "code", "name": "Case Pack Flag Consistency (UOM=CASE)", "description": "For rows with UOM='CASE' and valid Case Pack > 0, verify Case Pack Error flag equals (Ordered Units % Case Pack != 0). UOM='EA' should not trigger Case Pack Error.", "weight": 0.7, "code": "import re\nimport json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    ss = None\n    for r in context.get_all_outputs():\n        try:\n            if r.is_spreadsheet:\n                ss = r\n                break\n        except Exception:\n            continue\n    if not ss:\n        return 0.0, 'No spreadsheet.'\n\n    try:\n        xls_path = context.files.get_path(ss.id)\n        xls = pd.ExcelFile(xls_path)\n        names = xls.sheet_names\n        def pick(names):\n            keys = ['audit','validation','line','detail','check']\n            for n in names:\n                if any(k in n.lower() for k in keys):\n                    return n\n            return names[0]\n        sheet = pick(names)\n        df = pd.read_excel(xls_path, sheet_name=sheet)\n    except Exception as e:\n        return 0.0, f'Failed to read audit sheet: {e}'\n\n    norm = {c: re.sub(r'[^a-z0-9]+','', str(c).lower()) for c in df.columns}\n    def find(cands):\n        candsn = [re.sub(r'[^a-z0-9]+','', c.lower()) for c in cands]\n        for orig, n in norm.items():\n            if n in candsn:\n                return orig\n        for orig, n in norm.items():\n            if any(cn in n for cn in candsn):\n                return orig\n        return None\n\n    ou_col = find(['orderedunits','qtyordered','orderqty','quantity','orderedqty'])\n    uom_col = find(['uom','unitordermultiple','unitofmeasure','orderuom'])\n    cp_col = find(['casepack','caseqty','packsize','casequantity','csqty'])\n    flag_col = find(['casepackerror','casepackflag','packmultipleerror','caseerror'])\n\n    if not (ou_col and uom_col and cp_col and flag_col):\n        return 0.0, 'Missing columns for case pack check.'\n\n    ou = pd.to_numeric(df[ou_col], errors='coerce')\n    cp = pd.to_numeric(df[cp_col], errors='coerce')\n    uom = df[uom_col].astype(str).str.lower().str.replace(r'[^a-z]','', regex=True)\n\n    # Normalize flags to 0/1\n    def to01(s):\n        if pd.isna(s):\n            return 0\n        if isinstance(s,(int,float)):\n            return 1 if s and not pd.isna(s) else 0\n        sv = str(s).strip().lower()\n        return 1 if sv in ['1','y','yes','true','t','error'] else 0\n\n    flag = df[flag_col].apply(to01)\n\n    mask_case = (uom == 'case') & (~ou.isna()) & (~cp.isna()) & (cp > 0)\n    if mask_case.sum() == 0:\n        return 0.0, 'No valid CASE rows to compare.'\n\n    # Expected error if ordered units not divisible by case pack\n    rem = (ou[mask_case] % cp[mask_case]).fillna(np.nan)\n    expected_flag = (rem != 0).astype(int)\n\n    agree = (flag[mask_case].astype(int) == expected_flag).mean()\n    score = float(agree) * 0.7\n    return max(0.0, min(0.7, score)), f'Case Pack flag agreement on CASE rows: {round(float(agree)*100,1)}% (divisibility rule)'"}, {"type": "code", "name": "Errors Total = Price Mismatch + Case Pack", "description": "Verify that the Errors Total equals the sum of the two error flags for rows with valid data.", "weight": 0.2, "code": "import re\nimport json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    ss = None\n    for r in context.get_all_outputs():\n        try:\n            if r.is_spreadsheet:\n                ss = r\n                break\n        except Exception:\n            continue\n    if not ss:\n        return 0.0, 'No spreadsheet.'\n\n    try:\n        xls_path = context.files.get_path(ss.id)\n        xls = pd.ExcelFile(xls_path)\n        names = xls.sheet_names\n        def pick(names):\n            keys = ['audit','validation','line','detail','check']\n            for n in names:\n                if any(k in n.lower() for k in keys):\n                    return n\n            return names[0]\n        sheet = pick(names)\n        df = pd.read_excel(xls_path, sheet_name=sheet)\n    except Exception as e:\n        return 0.0, f'Failed to read audit sheet: {e}'\n\n    norm = {c: re.sub(r'[^a-z0-9]+','', str(c).lower()) for c in df.columns}\n    def find(cands):\n        candsn = [re.sub(r'[^a-z0-9]+','', c.lower()) for c in cands]\n        for orig, n in norm.items():\n            if n in candsn:\n                return orig\n        for orig, n in norm.items():\n            if any(cn in n for cn in candsn):\n                return orig\n        return None\n\n    p_col = find(['pricemismatcherror','priceerror','priceflag','pricemismatch'])\n    c_col = find(['casepackerror','casepackflag','packmultipleerror','caseerror'])\n    t_col = find(['errorstotal','totalerrors','errorcount','errors'])\n\n    if not (p_col and c_col and t_col):\n        return 0.0, 'Missing error columns.'\n\n    def to01(s):\n        if pd.isna(s):\n            return 0\n        if isinstance(s,(int,float)):\n            return 1 if s and not pd.isna(s) else 0\n        sv = str(s).strip().lower()\n        return 1 if sv in ['1','y','yes','true','t','error'] else 0\n\n    p = df[p_col].apply(to01)\n    c = df[c_col].apply(to01)\n    t = pd.to_numeric(df[t_col], errors='coerce').fillna(0).astype(int)\n\n    valid = (~pd.isna(p)) & (~pd.isna(c)) & (~pd.isna(t))\n    if valid.sum() == 0:\n        return 0.0, 'No valid rows to compare.'\n\n    agree = (t[valid] == (p[valid].astype(int) + c[valid].astype(int))).mean()\n    score = float(agree) * 0.2\n    return max(0.0, min(0.2, score)), f'Total Errors equals sum on {round(float(agree)*100,1)}% of rows.'"}, {"type": "llm_judge", "name": "SKU/PO Summary Aggregation Structure & Drill-Down", "description": "Check the Error Summary sheet shows SKU-level aggregation with drill-down to PO level and includes columns/measures for Price Mismatch, Case Pack, and Total Errors.", "weight": 2.5, "judge_prompt": "Examine the Excel workbook\u2019s summary sheet (e.g., 'Error Summary', 'SKU Error Summary', 'Pivot'). Assess whether:\n- There is a SKU-level aggregation of errors and a clear way to drill down to PO-level details (e.g., nested rows in a pivot, or a linked view). Be flexible with naming but require clarity of SKU\u2192PO navigation.\n- The table has measures/columns for Price Mismatch Errors, Case Pack Errors, and Total Errors, and totals appear internally consistent (e.g., Total \u2248 sum of the two components).\n\nScoring:\n- 2.5: Clear SKU aggregation with PO-level drill-down AND all three metrics present and laid out clearly.\n- 1.5: SKU aggregation present but PO drill-down unclear OR one metric missing/unclear.\n- 0.5: Attempted summary but lacks SKU focus or is too ambiguous; metrics not clearly separated.\n- 0.0: No usable summary sheet for aggregation/drill-down.", "expectation": "A pivot or summary that groups by SKU with nested PO rows and shows Price Mismatch, Case Pack, and Total Errors."}, {"type": "llm_judge", "name": "Per-line Error Type Summary matches flags/data (spot-check)", "description": "Sample several lines in the audit sheet and verify the Error Type Summary text agrees with the flag columns and visible data context.", "weight": 2.0, "judge_prompt": "Open the line-level audit sheet. Spot-check multiple rows (3\u20135 across different SKUs/POs):\n- If Price Mismatch Error=1, does Error Type Summary mention price mismatch or similar language and align with Entered vs Expected Unit Price values on that row?\n- If Case Pack Error=1, does the summary mention case pack/multiple violation consistent with UOM and Case Pack versus Ordered Units?\n- If both flags are 0, the summary should indicate no issues.\n\nScore based on agreement between text and flags/data:\n- 2.0: Strong agreement across checks; descriptions are accurate and specific.\n- 1.0: Mixed; generally aligned but some vague or slightly inconsistent wording.\n- 0.0: Frequent mismatches or misleading summaries.", "expectation": "Row-level summaries reflect the actual flags and underlying values."}, {"type": "llm_judge", "name": "Summary Doc Alignment with Excel Findings", "description": "Check that the DOCX/PDF summary references both error types, highlights high-error SKUs consistent with the Excel summary, and proposes where to start.", "weight": 1.7, "judge_prompt": "Review the summary document (DOCX/PDF) and the Excel summary:\n- Does the document discuss both Price Mismatch and Case Pack error types?\n- Does it identify SKUs with the highest error frequency consistent with the top items in the Excel SKU summary?\n- Does it recommend where to begin remediation (e.g., specific SKUs, pricing tables, UOM/case pack settings) tied to the observed patterns?\n\nScoring:\n- 1.7: Clearly references both categories, aligns with Excel\u2019s high-error SKUs, and gives actionable starting points.\n- 0.8: Mentions both categories but alignment to Excel or actionability is weak.\n- 0.0: Omits one category or lacks alignment and recommendations.", "expectation": "The brief summary accurately reflects the Excel findings and offers actionable next steps tied to observed SKUs."}, {"type": "llm_judge", "name": "Reasonableness of Aggregations & Totals (sanity)", "description": "Judge whether totals and subtotals in the Excel summary look reasonable and consistent (e.g., Total Errors roughly equals Price+Case).", "weight": 1.1, "judge_prompt": "Look at the Excel summary/pivot totals and subtotals:\n- Do Total Errors values appear to be the sum (or clear aggregation) of Price Mismatch and Case Pack Errors for SKUs and overall totals?\n- Do subtotals/overall totals avoid obvious inconsistencies (e.g., negative counts, totals smaller than components)?\n\nScoring:\n- 1.1: Totals and subtotals consistently make sense across the summary.\n- 0.6: Mostly sensible with minor anomalies.\n- 0.0: Clear inconsistencies in totals or relationships.", "expectation": "Totals/subtotals should be coherent; Total Errors should equal or clearly derive from its components."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Communication", "description": "Holistic assessment of professionalism, clarity, and usefulness for management and order operations stakeholders.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Excel Workbook Professionalism & Usability", "description": "Evaluate readability, labeling, and usability of the Excel deliverable for operations/finance users.", "weight": 2.0, "judge_prompt": "Assess the Excel workbook for professional presentation and usability:\n- Clear headers and consistent naming for columns and sheets; filters or pivot fields are easy to understand.\n- Any helper notes/instructions are concise; visuals (e.g., conditional formatting) help identify errors without clutter.\n- The structure supports repeatability (e.g., added columns formula-driven, pivot refreshable).\n\nScoring:\n- 2.0: Professional, clean, and easy to use.\n- 1.0: Adequate but with clutter or confusing labels.\n- 0.0: Hard to read or disorganized.", "expectation": "A clean, labeled workbook with sensible structure that a colleague can use immediately."}, {"type": "llm_judge", "name": "Actionability of Recommendations", "description": "Judge whether recommendations in the summary doc are specific, prioritized, and feasible.", "weight": 2.0, "judge_prompt": "Review the summary document recommendations:\n- Are they specific (e.g., fix pricing table for SKUs X/Y; enforce UOM=CASE multiples; add validation rule in OMS)?\n- Are they prioritized based on error frequency/severity and retailer compliance risk?\n- Are next steps and owners (Order Management, Finance, IT) reasonably implied or stated?\n\nScoring:\n- 2.0: Specific, prioritized, and practical recommendations.\n- 1.0: Somewhat actionable but vague or not prioritized.\n- 0.0: Generic or not actionable.", "expectation": "Concrete, prioritized steps tied to the observed error patterns and stakeholders."}, {"type": "llm_judge", "name": "Clarity for Management Audience", "description": "Evaluate whether the brief clearly explains the problem, findings, and impact for management.", "weight": 1.0, "judge_prompt": "Assess the DOCX/PDF brief for clarity:\n- Clear Overview/Context explaining the issue (invoice vs internal price inconsistency; UOM/case pack rules) without jargon.\n- Findings summarize both error categories and their operational/financial impact (billing errors, short shipping, fines).\n- Concise, to the point (\u22480.5\u20131.5 pages), suitable for management.\n\nScoring:\n- 1.0: Clear, concise, and management-ready.\n- 0.5: Understandable but verbose or missing some context.\n- 0.0: Unclear or inappropriate for the audience.", "expectation": "A concise, management-ready summary of the issue, findings, and implications."}, {"type": "llm_judge", "name": "Comprehensiveness & Risk Coverage", "description": "Determine if the work addresses both error classes, edge cases, and retailer compliance risk.", "weight": 1.0, "judge_prompt": "Evaluate comprehensiveness:\n- Both Price Mismatch and Case Pack errors are covered thoroughly.\n- Edge cases acknowledged (e.g., UOM='EA' can ship individually despite case pack; rounding tolerance on prices).\n- Mentions compliance implications and prioritizes high-risk SKUs/retailers.\n\nScoring:\n- 1.0: Thorough and risk-aware.\n- 0.5: Mostly covered but limited edge-case or risk discussion.\n- 0.0: Superficial coverage.", "expectation": "Both error categories and key edge cases discussed, with attention to compliance risks."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "6d2c8e55-fe20-45c6-bdaf-93e676868503", "rubric": {"category_name": "Journal Club Scheduling and Materials Preparation (Internal Medicine)", "rationale": "Pattern C (Mixed). The deliverables are a structured Excel schedule plus document-style artifacts (nine article PDFs and an email draft). Stage 1 uses LLM-only gates to enforce a very specific, verifiable output shape that enables deterministic checks in Stage 2. Stage 2 blends light, robust code checks (dates, spacing, filenames) with higher-weight LLM cross-referencing (holiday avoidance, topical/recency compliance, email accuracy). Stage 3 evaluates professional quality and usability for a supervising physician.", "max_total_score": 11.7, "stages": [{"name": "Stage 1 \u2014 Structured Output Gate", "description": "LLM-only gate to enforce exact deliverable structure so verification is trivial.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.1, "rules": [{"type": "llm_judge", "name": "Deliverables Presence and Naming", "description": "Verify presence and basic naming of all required deliverables.", "weight": 1.2, "judge_prompt": "You are verifying the structure and presence of deliverables for a medical journal club task. Check ALL outputs in the workspace, not just the primary file.\n\nREQUIRED DELIVERABLES (presence + naming only; do not judge content quality):\n1) One Excel file named exactly \"Journal Club Schedule.xlsx\" (XLSX). If the name is very close (e.g., minor punctuation/case differences), allow partial credit but it MUST be an Excel file.\n2) Exactly nine article PDFs in total, with filenames that indicate the relevant month (acceptable: Oct/October, Nov/November, Dec/December). At least 3 PDFs per month.\n3) An email draft document as DOCX or PDF addressed to Dr. John Smith (can be a draft; must look like an email ready to send). The document should have a recognizable subject line (e.g., includes \"Journal Club\") and a body.\n\nSCORING:\n- 1.0: All three deliverable categories present; Excel named exactly; nine PDFs with month labels (3 per month); email draft is DOCX/PDF with subject and body.\n- 0.8: All categories present; Excel present but name slightly off OR month labeling incomplete but still clearly 3 PDFs per month.\n- 0.5: Two categories present (e.g., Excel and email) but PDFs missing or fewer than nine; OR Excel not in XLSX format.\n- 0.2: Only one category present or major format errors (e.g., only a markdown email, no Excel).\n- 0.0: Required deliverables missing or wrong formats.\n\nOnly evaluate presence/format and obvious naming. Do not assess content quality or correctness in this rule.", "expectation": "All three deliverable types exist: the precisely named Excel schedule, 9 month-labeled article PDFs, and a DOCX/PDF email draft."}, {"type": "llm_judge", "name": "Excel Structure Requirements", "description": "Verify the internal structure of the Journal Club Schedule.xlsx for downstream verification.", "weight": 1.8, "judge_prompt": "Open the Excel file \"Journal Club Schedule.xlsx\" and verify it follows this structure. Be flexible with sheet names but strict about structural presence.\n\nREQUIRED SHEETS/SECTIONS:\nA) Sheet: \"Journal Club Schedule\" (or similarly named: \"Schedule\" or \"Journal Club\")\n   - A single table with columns (headers visible):\n     \u2022 Month (October, November, December)\n     \u2022 Topic (must match: Oct = common causes of autonomic neuropathy; Nov = treatment of long COVID; Dec = role of dietitians in preventative care)\n     \u2022 Date (calendar date)\n     \u2022 Day (weekday name)\n     \u2022 Time (should show 6\u20138 pm in some clear format)\n     \u2022 Location/Room\n     \u2022 Conflict Check/Notes (a cell/column indicating holiday/conference check was performed)\n   - Exactly three rows, one per month (Oct/Nov/Dec), in the correct topical order.\n\nB) Sheet: \"Article Log\" (or similarly named: \"Articles\" / \"Article List\")\n   - A table with 9 rows (3 per month) and columns (headers visible):\n     \u2022 Month\n     \u2022 Topic\n     \u2022 Article Title\n     \u2022 Journal\n     \u2022 Year (publication year within last 10 years)\n     \u2022 Access Type (PDF or Link)\n     \u2022 URL (if link-only)\n     \u2022 Filename (if a PDF attachment)\n\nC) Optional but encouraged: A sheet resembling \"Room Availability\" (or similar) with the relevant slots marked as booked for Journal Club. Presence yields minor consideration but is NOT required for full credit.\n\nSCORING:\n- 1.8: Sheets A and B present with clear tables and all listed columns; A has three rows for Oct/Nov/Dec with correct topics in order; time and weekday fields visible; B has 9 rows and all columns.\n- 1.4: Sheets A and B present but one minor element missing (e.g., missing Conflict Check column OR small header naming variation where intent is still clear).\n- 0.9: Only one required sheet present or both present but key structure missing (e.g., Article Log lacks Year or Access fields, or Schedule missing Topic/Date columns).\n- 0.4: Excel exists but lacks the required structured tables.\n- 0.0: Not an Excel file or no recognizable structure.\n\nDo not judge calculation correctness or cross-file consistency here\u2014only the structural presence that enables verification.", "expectation": "Excel contains a schedule sheet and an article log sheet with the specified columns and rows, enabling deterministic checks later."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness Verification", "description": "Mixed code + LLM checks to verify correctness, consistency, and policy adherence.", "is_required": true, "max_points": 5.2, "min_score_to_pass": 2.6, "rules": [{"type": "code", "name": "Date Spacing and Weekday Preference", "description": "Check schedule dates are >=21 days apart, are weekdays (Mon\u2013Fri), and reflect weekday preference (Wed > Thu > Tue > Mon > Fri).", "weight": 0.35, "code": "import re\nimport pandas as pd\nimport numpy as np\nfrom datetime import timedelta\n\ndef _find_schedule_excel(context):\n    candidates = []\n    for r in context.get_all_outputs():\n        try:\n            if getattr(r, 'is_spreadsheet', False):\n                name = getattr(r, 'name', '') or ''\n                if name:\n                    lname = name.lower()\n                    if 'journal' in lname and 'schedule' in lname:\n                        return r\n                candidates.append(r)\n        except Exception:\n            continue\n    return candidates[0] if candidates else None\n\n\ndef _read_schedule_df(context, xls_res):\n    try:\n        path = context.files.get_path(xls_res.id)\n        xls = pd.ExcelFile(path)\n        sheet = None\n        for s in xls.sheet_names:\n            ls = s.lower()\n            if 'schedule' in ls or ('journal' in ls and 'club' in ls):\n                sheet = s\n                break\n        if sheet is None:\n            sheet = xls.sheet_names[0]\n        df = pd.read_excel(path, sheet_name=sheet)\n        return df\n    except Exception:\n        return None\n\n\ndef _get_date_series(df):\n    # Try to locate a date column flexibly\n    date_col = None\n    for c in df.columns:\n        if 'date' in str(c).lower():\n            date_col = c\n            break\n    if date_col is None:\n        # Try to coerce any column with parseable dates\n        for c in df.columns:\n            s = pd.to_datetime(df[c], errors='coerce')\n            if s.notna().sum() >= 3:\n                return s\n        return pd.Series([], dtype='datetime64[ns]')\n    return pd.to_datetime(df[date_col], errors='coerce')\n\n\ndef _weekday_scores(dts):\n    # Preference: Wed=5, Thu=4, Tue=3, Mon=2, Fri=1\n    # Monday=0 ... Sunday=6 in pandas\n    mapping = {0:2, 1:3, 2:5, 3:4, 4:1, 5:0, 6:0}\n    scores = []\n    for d in dts:\n        if pd.isna(d):\n            scores.append(0)\n        else:\n            scores.append(mapping.get(d.weekday(), 0))\n    return scores\n\n\ndef evaluate(workflow, context):\n    xls = _find_schedule_excel(context)\n    if not xls:\n        return 0.0, \"No schedule Excel found.\"\n    df = _read_schedule_df(context, xls)\n    if df is None or df.empty:\n        return 0.0, \"Could not read schedule sheet.\"\n\n    # Extract date series\n    ds = _get_date_series(df)\n    ds = ds.dropna().unique()\n    try:\n        dts = sorted(pd.to_datetime(ds, errors='coerce'))\n        # Limit to three unique dates if more\n        if len(dts) >= 3:\n            dts = dts[:3]\n    except Exception:\n        return 0.0, \"Failed to parse dates.\"\n\n    if len(dts) < 3:\n        return 0.2, \"Fewer than 3 valid dates detected.\"\n\n    # Spacing check (>=21 days apart consecutively)\n    spacing_ok = True\n    spacings = []\n    for i in range(1, len(dts)):\n        delta = (dts[i] - dts[i-1]).days\n        spacings.append(delta)\n        if delta < 21:\n            spacing_ok = False\n\n    # Weekday check\n    weekdays_ok = all(d.weekday() < 5 for d in dts)\n\n    # Preference score\n    pref_scores = _weekday_scores(dts)\n    avg_pref = np.mean(pref_scores) if pref_scores else 0\n\n    score = 0.0\n    if spacing_ok:\n        score += 0.5\n    if weekdays_ok:\n        score += 0.3\n    # Reward better than neutral preference (avg >=3) with 0.2, otherwise partial 0.1 if weekdays but low pref\n    if avg_pref >= 3:\n        score += 0.2\n    elif weekdays_ok:\n        score += 0.1\n\n    score = min(score, 1.0)\n    fb = f\"Spacing OK: {spacing_ok}, Weekdays OK: {weekdays_ok}, Avg pref score: {avg_pref:.2f}, spacings: {spacings}\"\n    return score, fb"}, {"type": "code", "name": "Article PDFs Count and Month Labeling", "description": "Verify there are 9 PDF articles with at least 3 labeled for each month (Oct/Nov/Dec) in filenames.", "weight": 0.35, "code": "import re\nfrom pathlib import Path\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0, \"No outputs found.\"\n    month_counts = {\"oct\":0, \"nov\":0, \"dec\":0}\n    total_pdfs = 0\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            if p.suffix.lower() == \".pdf\":\n                total_pdfs += 1\n                name = p.name.lower()\n                if any(k in name for k in [\"oct\", \"october\"]):\n                    month_counts[\"oct\"] += 1\n                if any(k in name for k in [\"nov\", \"november\"]):\n                    month_counts[\"nov\"] += 1\n                if any(k in name for k in [\"dec\", \"december\"]):\n                    month_counts[\"dec\"] += 1\n        except Exception:\n            continue\n    # We expect exactly 9 article PDFs, but allow >=9 in case extra supporting PDFs are included\n    # Scoring: fraction of required per-month counts met, weighted by total count present\n    per_month_fraction = sum(min(month_counts[m], 3)/3 for m in [\"oct\",\"nov\",\"dec\"]) / 3.0\n    total_ok = 1.0 if total_pdfs >= 9 else min(total_pdfs/9.0, 1.0)\n    score = min(per_month_fraction, total_ok)\n    fb = f\"Total PDFs: {total_pdfs}, Month counts: {month_counts}\"\n    return score, fb"}, {"type": "llm_judge", "name": "Holiday/Conference Conflict Avoidance Documentation", "description": "Check that the chosen dates explicitly document conflict checks against holidays, conferences, or department events and avoid blocked dates.", "weight": 1.5, "judge_prompt": "Open the Excel schedule sheet and verify the Conflict Check/Notes area shows explicit evidence that the author consulted a holiday/conference/events list (e.g., references to a file named \"Holiday-Conference-Event-Dates.docx\" or a clear statement that the check was performed). Then, look at the selected dates: ensure they are not obviously on major US holidays in Oct\u2013Dec (e.g., Thanksgiving Day or Christmas Day), and that no conflicts are noted.\n\nSCORING:\n- 1.5: Explicit conflict-check documentation and no obvious major-holiday conflicts; notes are clear and specific (e.g., citing the file or listing dates checked).\n- 1.0: Conflict check indicated but documentation is minimal or generic (e.g., \"Checked holidays\"), no obvious conflicts.\n- 0.5: No documentation of conflict checking OR ambiguous notes; dates still appear reasonable (no clear conflict).\n- 0.0: Evident scheduling on a major holiday or a note indicating a conflict was ignored.\n\nUse only the materials provided in the outputs to judge documentation and obvious conflicts. Do not rely on external browsing.", "expectation": "Conflict checks are explicitly documented and scheduled dates avoid major holidays."}, {"type": "llm_judge", "name": "Article Compliance: Peer-Reviewed, Recent, Relevant, Accessible", "description": "Verify 3 articles per month, within 10 years, peer-reviewed journal articles, topically relevant, and accessible (PDF attached or a working link saved in a PDF).", "weight": 1.5, "judge_prompt": "Inspect the nine article PDFs (or link-PDFs). For each month/topic:\n- October: Common causes of autonomic neuropathy\n- November: Treatment of long COVID\n- December: Role of dietitians in preventative care\n\nFor each article, check:\n1) It appears to be a peer-reviewed journal article (e.g., journal name on first page, typical structure with abstract, references).\n2) Publication year within the last 10 years relative to 2025 (i.e., 2015 or later).\n3) Topically relevant to the assigned month.\n4) Fully accessible content in the PDF (not just a paywalled landing page). If the source is only accessible online, a PDF should contain the live link and enough metadata to access publicly without login.\n\nAlso compare with the Excel \"Article Log\" to ensure titles/journals/years match the attached PDFs.\n\nSCORING:\n- 1.5: All 9 articles meet all criteria and match the Article Log.\n- 1.2: Minor issues (e.g., 1 mismatch or 1 article borderline on relevance/format) but overall meets the standard.\n- 0.8: Several issues (e.g., 2\u20133 articles older than 10 years or weak relevance or missing in the log) but at least 6 solid.\n- 0.4: Many issues; only 3\u20135 meet criteria.\n- 0.0: Few or none meet criteria; mostly inaccessible or not peer-reviewed.\n", "expectation": "Nine solid, recent, peer-reviewed, topic-relevant, accessible articles with entries in the Article Log."}, {"type": "llm_judge", "name": "Email Accuracy and Attachment Cross-Reference", "description": "Verify the email draft is ready for Dr. John Smith, accurately summarizes the schedule and articles, and references attachments correctly.", "weight": 1.5, "judge_prompt": "Open the email draft (DOCX or PDF) and verify:\n- Addressed to Dr. John Smith (name appears in the salutation or addressing line) and includes a clear subject line about Journal Club.\n- Body summarizes the finalized schedule: dates, times (6\u20138 pm), locations, and topics for Oct/Nov/Dec. These must match the Excel schedule exactly.\n- Lists the 9 articles (titles/journals) or clearly references the Article Log, and states that PDFs are attached.\n- Professional tone and a clear call to action (review/approval request).\n\nSCORING:\n- 1.5: Accurate cross-references to Excel; all dates/times/locations/topics match; nine articles referenced; attachments mentioned; professional and ready to send.\n- 1.2: Minor discrepancies (e.g., formatting differences or small omission) but overall accurate and professional.\n- 0.8: Multiple omissions (e.g., missing a location or not listing all articles) but still usable.\n- 0.4: Major mismatches between email and Excel or unclear request.\n- 0.0: Not an email draft or lacks essential elements.\n", "expectation": "A professional, accurate email draft that mirrors the Excel schedule and references all article attachments."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality Assessment", "description": "LLM-only holistic quality checks for professionalism, clarity, and usability.", "is_required": false, "max_points": 3.5, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Formatting", "description": "Assess the professional formatting of the Excel schedule and email.", "weight": 1.0, "judge_prompt": "Evaluate overall presentation quality:\n- Excel: Clear headers, readable date/time formats, consistent capitalization, and clean layout. Minimal clutter; filters or table formatting used helpfully.\n- Email: Professional tone, concise structure (greeting, purpose, schedule summary, articles/attachments, next steps, signature with contact info).\n\nSCORING:\n- 1.0: Highly professional; clean, consistent formatting in both Excel and email.\n- 0.7: Generally professional with minor inconsistencies.\n- 0.4: Adequate but somewhat messy or inconsistent.\n- 0.1: Poorly formatted and unprofessional.\n", "expectation": "Polished, clinic-ready materials with clear, consistent formatting."}, {"type": "llm_judge", "name": "Citation Clarity and Completeness", "description": "Assess clarity and completeness of citations in the Article Log and email.", "weight": 0.8, "judge_prompt": "Check that article references in the Article Log and/or email include sufficient bibliographic detail for identification: title, journal, year, authors (where available), and link/DOI if relevant. Consistency of style is preferred but not mandatory.\n\nSCORING:\n- 0.8: Complete and consistent citations across all 9 entries.\n- 0.6: Mostly complete; a few minor omissions.\n- 0.3: Several incomplete citations; still identifiable.\n- 0.0: Sparse or unclear citations that impede identification.\n", "expectation": "Each article is readily identifiable from the provided citation details."}, {"type": "llm_judge", "name": "File Organization and Naming Hygiene", "description": "Assess whether files are well-named and organized for easy review.", "weight": 0.9, "judge_prompt": "Evaluate whether filenames and organization are reviewer-friendly:\n- Article PDFs include month in the filename and a concise, identifiable title.\n- No ambiguous or duplicate filenames.\n- The Excel file has the exact required name.\n- Optional: logical grouping or clear list provided in the email for easy opening.\n\nSCORING:\n- 0.9: Excellent organization and naming hygiene; effortless to review.\n- 0.6: Generally good; minor inconsistencies.\n- 0.3: Disorganized or confusing naming; still usable with effort.\n- 0.0: Disorganized to the point of hindering review.\n", "expectation": "Files are intuitively named and easy to navigate."}, {"type": "llm_judge", "name": "Proactive Coordination and Usability", "description": "Assess how well the materials facilitate next steps and real-world use.", "weight": 0.8, "judge_prompt": "Look for helpful touches that make coordination easier:\n- Email includes clear next steps (e.g., \u201cPlease review/approve by [date]\u201d).\n- Time zone clarity if relevant; confirms 6\u20138 pm protected academic time.\n- Notes about room AV needs, parking, or access if applicable.\n- Clear indication of conflicts checked and any contingency dates.\n\nSCORING:\n- 0.8: Strong proactive details that smooth coordination.\n- 0.6: Some helpful details present.\n- 0.3: Minimal helpful detail; still functional.\n- 0.0: Lacks essential next steps or clarity.\n", "expectation": "Materials anticipate reviewer needs and smooth the path to approval and distribution."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "ef8719da-18e5-4bfe-b986-399652d77376", "rubric": {"category_name": "Science News Pitch \u2013 Space Advertising Ban (Journalism)", "rationale": "Pattern B (Document task). The deliverable must be an editable Word document (DOCX) that follows a strict, verifiable section structure so evaluators can reliably check compliance, correctness, and quality. Stage 1 is an LLM-only gate enforcing the exact document shape and required sections. Stage 2 mixes lightweight code checks (file/text properties, counts, presence) with higher-weight LLM verification of factual completeness and reporting plan soundness. Stage 3 uses LLM assessment for overall editorial quality, persuasiveness, and appropriateness for a general audience.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2013 Shape & Format Gate (LLM only)", "description": "Hard gate ensuring the pitch is a DOCX with all required sections and structural elements that enable verification.", "is_required": true, "max_points": 2.0, "min_score_to_pass": 1.5, "rules": [{"type": "llm_judge", "name": "Document Format and Structural Completeness", "description": "Verify that the submission is an editable Word document (DOCX) with all required sections and basic constraints.", "weight": 2.0, "judge_prompt": "You are evaluating whether the candidate output follows the REQUIRED format and structure for a journalism pitch. Only assess presence/format, not content quality.\n\nRequired Format:\n- Must be a DOCX (editable Word). Not PDF, not plain text, not Google Docs link.\n- Pitch length: no more than 1,000 words for the main body (exclude References/Links section if clearly separated).\n\nRequired Sections (flexible with close synonyms):\n1) \u201cWorking Headline\u201d (acceptable variants: Headline, Proposed Headline, Working Title)\n2) \u201cStory Structure\u201d (acceptable variants: Structure, Narrative Flow, Angles of Inquiry)\n3) \u201cBackground\u201d (acceptable variants: History, Context)\n4) \u201cSources to Contact\u201d (acceptable variants: Sources, Source List, Stakeholders to Interview)\n5) \u201cWhy Now\u201d (acceptable variants: Timeliness, Newsworthiness)\n6) \u201cTimeline\u201d (acceptable variants: Reporting Timeline, Draft Timeline, Schedule)\n7) \u201cReferences\u201d (acceptable variants: Links, Hyperlinks, Sources Used)\n\nHyperlink Requirement:\n- At least 4 hyperlinks to freely accessible resources, with at least 2 from the provided list (domains include: theweek.com, gizmodo.com, spacenews.com, emarketer.com, latimes.com, thehustle.co, campaignasia.com, orbitaltoday.com). Links may appear in text or in the References section.\n\nScoring (0.0\u20132.0):\n- 2.0: DOCX; all 7 sections present with recognizable headers; main body \u22641,000 words; \u22654 hyperlinks with \u22652 from the provided list.\n- 1.5: DOCX; all sections present but one minor deficiency (e.g., only 3\u20134 links or only 1 from the list, or body is slightly over the limit to \u22641,100 words).\n- 1.0: DOCX; missing exactly one required section or multiple minor deficiencies (e.g., links missing and slight word overage), but still mostly structured.\n- 0.5: DOCX but only 3\u20135 sections present.\n- 0.0: Not a DOCX, or severely incomplete structure (\u22642 sections), or body clearly exceeds ~1,200 words.\n\nOnly judge presence/format/structure and link counts. Do not judge content accuracy or quality here.", "expectation": "A properly structured DOCX pitch with all required sections, body \u22641,000 words, and enough hyperlinks (including at least two from the provided list)."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification (Content and Plan Checks)", "description": "Now that the file shape is correct, verify factual completeness, timeliness, balance, and the specificity of the reporting plan. Mix code rules (lightweight, deterministic checks) with higher-weight LLM rules (nuanced verification).", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Header Presence (Fuzzy)", "description": "Check presence of required section headers (fuzzy match).", "weight": 0.25, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float score in [0, weight] or (score, feedback)\n    \"\"\"\n    weight = 0.25\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n    text = \"\"\n    try:\n        # Prefer DOCX text\n        path = context.files.get_path(output.id)\n        if path.suffix.lower() == \".docx\":\n            text = context.files.read_docx_text(output.id)\n        else:\n            # Fallbacks if not DOCX\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_text(output.id)\n                except Exception:\n                    text = \"\"\n    except Exception:\n        text = \"\"\n    if not text:\n        return 0.0, \"Unable to read document text\"\n\n    low = text.lower()\n    patterns = {\n        'working_headline': r\"working\\s+headline|headline|working\\s+title\",\n        'story_structure': r\"story\\s+structure|structure|narrative\\s+flow|angles\\s+of\\s+inquiry|angles\",\n        'background': r\"background|history|context\",\n        'sources': r\"sources\\s+to\\s+contact|sources|source\\s+list|stakeholders\\s+to\\s+interview\",\n        'why_now': r\"why\\s+now|timeliness|newsworthiness\",\n        'timeline': r\"timeline|schedule|draft\\s+timeline|reporting\\s+timeline\",\n        'references': r\"references|links|hyperlinks|sources\\s+used|bibliography\"\n    }\n    found = 0\n    for k, pat in patterns.items():\n        if re.search(pat, low):\n            found += 1\n    score = (found / len(patterns)) * weight\n    return score, f\"Found {found}/7 required headers\"\n"}, {"type": "code", "name": "Word Count (Body \u2264 1000)", "description": "Count words in main body (before References/Links) and score based on compliance with \u22641,000 words.", "weight": 0.25, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 0.25\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n    text = \"\"\n    try:\n        path = context.files.get_path(output.id)\n        if path.suffix.lower() == \".docx\":\n            text = context.files.read_docx_text(output.id)\n        else:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                text = context.files.read_text(output.id)\n    except Exception:\n        text = \"\"\n    if not text:\n        return 0.0, \"Unable to read text\"\n\n    low = text.lower()\n    # Split off references/links section if present\n    split_points = [m.start() for m in re.finditer(r\"\\b(references|links|hyperlinks|sources\\s+used|bibliography)\\b\", low)]\n    body_text = text\n    if split_points:\n        body_text = text[:split_points[0]]\n    words = re.findall(r\"\\b\\w+\\b\", body_text)\n    wc = len(words)\n    if wc <= 1000:\n        return weight, f\"Body word count OK: {wc}\"\n    elif wc <= 1100:\n        return weight * 0.5, f\"Slightly over limit: {wc}\"\n    else:\n        return 0.0, f\"Over limit: {wc}\"\n"}, {"type": "code", "name": "Hyperlink Count and Source Domains", "description": "Verify \u22654 hyperlinks total and at least 2 from the provided list of domains.", "weight": 0.25, "code": "import re\nfrom urllib.parse import urlparse\n\ndef evaluate(workflow, context):\n    weight = 0.25\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n    try:\n        path = context.files.get_path(output.id)\n        if path.suffix.lower() == \".docx\":\n            text = context.files.read_docx_text(output.id)\n        else:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                text = context.files.read_text(output.id)\n    except Exception:\n        return 0.0, \"Unable to read text\"\n\n    urls = set(re.findall(r\"https?://[^\\s)\\]>]+\", text))\n    provided_domains = {\n        'theweek.com','gizmodo.com','spacenews.com','emarketer.com',\n        'latimes.com','thehustle.co','campaignasia.com','orbitaltoday.com'\n    }\n    total = len(urls)\n    from_list = 0\n    for u in urls:\n        try:\n            netloc = urlparse(u).netloc.lower()\n            # Reduce to domain\n            parts = netloc.split(':')[0].split('.')\n            domain = '.'.join(parts[-2:]) if len(parts) >= 2 else netloc\n            if domain in provided_domains:\n                from_list += 1\n        except Exception:\n            pass\n    # Scoring: 0.5*weight for >=4 links; +0.5*weight for >=2 from list\n    score = 0.0\n    if total >= 4:\n        score += weight * 0.5\n    if from_list >= 2:\n        score += weight * 0.5\n    return score, f\"Links: {total} total; {from_list} from provided list\"\n"}, {"type": "code", "name": "Source Diversity in \u2018Sources\u2019 Section", "description": "Check that the \u2018Sources\u2019 section lists multiple stakeholder categories (aiming for balance).", "weight": 0.25, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 0.25\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n    try:\n        path = context.files.get_path(output.id)\n        if path.suffix.lower() == \".docx\":\n            text = context.files.read_docx_text(output.id)\n        else:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                text = context.files.read_text(output.id)\n    except Exception:\n        return 0.0, \"Unable to read text\"\n    low = text.lower()\n\n    # Extract sources section (best-effort)\n    start = re.search(r\"(sources\\s+to\\s+contact|sources|source\\s+list|stakeholders\\s+to\\s+interview)\", low)\n    if not start:\n        return 0.0, \"No sources section found\"\n    end = re.search(r\"(why\\s+now|timeliness|newsworthiness|timeline|schedule|references|links|hyperlinks|sources\\s+used|bibliography)\", low[start.end():])\n    if end:\n        section = low[start.end(): start.end()+end.start()]\n    else:\n        section = low[start.end():]\n\n    categories = {\n        'astronomers': r\"astronomer|astronomy|astrophysicist|observator(y|ies)\",\n        'dark_sky_advocates': r\"dark-?sky|ida|international\\s+dark\\s+sky|light\\s+pollution\",\n        'commercial_space': r\"commercial|company|startup|satellite\\s+operator|space\\s+company|advertising|ad\\s+industry|brand\",\n        'regulators_international': r\"regulator|copuos|united\\s+nations|u\\.n\\.|fcc|faa|esa|ministry|policymaker|policy\\s+maker|government\",\n        'environmental_scientists': r\"environmental\\s+scientist|ecologist|environmental\\b|sustainability|ecology\",\n        'ethics_legal': r\"ethicist|ethics|legal|law|lawyer|international\\s+law\"\n    }\n    found = 0\n    hit = []\n    for name, pat in categories.items():\n        if re.search(pat, section):\n            found += 1\n            hit.append(name)\n    # Scoring: 4+ categories = full; 3 = 0.8; 2 = 0.5; else 0\n    if found >= 4:\n        score = weight\n    elif found == 3:\n        score = weight * 0.8\n    elif found == 2:\n        score = weight * 0.5\n    else:\n        score = 0.0\n    return score, f\"Source categories found: {found} ({', '.join(hit)})\"\n"}, {"type": "llm_judge", "name": "Background and Policy Coverage", "description": "Does the pitch provide key background, including history of space advertising attempts and relevant policy/regulatory developments (e.g., COPUOS/UN bodies, national regulators)?", "weight": 1.33, "judge_prompt": "Evaluate the pitch for completeness of BACKGROUND and POLICY/REGULATORY coverage.\nLook for:\n- History of space advertising (e.g., 1990s billboard concepts, prior attempts, evolving technology like reflective satellites or laser displays).\n- Key risks/concerns raised by astronomers/dark-sky advocates/environmental scientists (light pollution, astronomy interference, debris/orbital congestion, cultural/aesthetic impacts).\n- Relevant policy/regulatory context (e.g., UN COPUOS or other UN forums; national regulators like FCC/FAA/ESA equivalents; professional society positions; calls for bans/moratoria).\n- Use of citations/hyperlinks to reputable sources (ideally including some of the provided articles), but you are judging coverage, not exact citations.\n\nScoring (0\u20131.33):\n- 1.33: Clear, accurate history and multiple policy angles (international + national), with concrete examples and risks articulated.\n- 0.9: Generally solid background with minor gaps (e.g., only one regulatory level or thin historical detail).\n- 0.45: Superficial background; limited policy context; risks barely mentioned.\n- 0.0: Missing or erroneous background/policy coverage.", "expectation": "A concise yet thorough background that places the current debate in historical and regulatory context."}, {"type": "llm_judge", "name": "Timeliness and News Hook (\u2018Why Now\u2019)", "description": "Assess whether the pitch persuasively explains why this story is timely and newsworthy at this moment.", "weight": 1.33, "judge_prompt": "Evaluate the \u2018Why Now\u2019 section (and related references) for timeliness:\n- Does it tie to recent demonstrations by space companies exploring visible-from-Earth advertising (reflective satellites, laser-based displays)?\n- Does it cite or reference current actions/debates by astronomers/professional bodies (e.g., calls for bans, appeals to COPUOS/UN) or other concrete developments?\n- Is the news hook compelling and specific (not generic)?\n\nScoring (0\u20131.33):\n- 1.33: Strong, specific news hook tied to recent events/demos and active policy/professional moves; clear urgency.\n- 0.9: Plausible timeliness with some specificity.\n- 0.45: Vague timeliness; generic rationale.\n- 0.0: No meaningful timeliness justification.", "expectation": "A clear, specific, recent news peg that justifies covering this now."}, {"type": "llm_judge", "name": "Reporting Plan and Narrative Structure", "description": "Check whether the pitch lays out a concrete reporting plan and narrative flow with angles of inquiry and balance.", "weight": 1.34, "judge_prompt": "Assess the \u2018Story Structure\u2019 and \u2018Sources\u2019 sections for a robust reporting plan:\n- Are the angles of inquiry clearly enumerated (e.g., science/astronomy impact; light pollution; orbital debris risk; policy/regulatory dimensions; commercial motivations)?\n- Is there an outline of narrative flow (lead, nut graf, body sections, possible scenes/visuals/data/examples)?\n- Are there concrete interview targets by category to ensure balance (astronomers, dark-sky advocates, commercial space representatives, regulators/international bodies, environmental scientists, ethicists/legal)?\n- Are key questions the reporting will answer articulated?\n\nScoring (0\u20131.34):\n- 1.34: Detailed structure and plan with clear angles, balanced sources, and actionable reporting steps.\n- 0.9: Solid but missing some specificity (e.g., light on scenes/questions or one stakeholder group).\n- 0.45: Minimal outline; little sense of narrative or balance.\n- 0.0: No tangible plan or structure.", "expectation": "A concrete, balanced outline with clear angles and reporting steps."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Quality and Editorial Merit", "description": "Holistic LLM assessment of writing quality, persuasion, audience fitness, and professionalism.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Persuasiveness to Editor", "description": "Does the pitch convincingly argue that this story is important and compelling for a general audience?", "weight": 0.75, "judge_prompt": "Judge the overall persuasive strength of the pitch to an editor:\n- Clear articulation of why this matters to the public (control of the sky, cultural/aesthetic value, science impacts, regulation/public-interest questions).\n- Strong nut graf framing the core tension and stakes.\n- Distinctive angle vs. generic coverage.\n\nScoring (0\u20130.75):\n- 0.75: Highly persuasive and focused.\n- 0.5: Moderately persuasive.\n- 0.25: Weakly persuasive.\n- 0.0: Not persuasive.", "expectation": "A crisp, convincing case for why this story matters now."}, {"type": "llm_judge", "name": "Audience Accessibility and Clarity", "description": "Assess clarity, jargon control, and accessibility for a general readership.", "weight": 0.75, "judge_prompt": "Evaluate clarity and accessibility:\n- Minimal jargon, or jargon is explained (e.g., COPUOS, LEO, light pollution terms).\n- Clear, concise prose with logical flow.\n- Suitable for a general audience, not only specialists.\n\nScoring (0\u20130.75):\n- 0.75: Very clear and accessible.\n- 0.5: Mostly clear, some jargon or density.\n- 0.25: Hard to follow.\n- 0.0: Unclear/inaccessible.", "expectation": "Readable, well-structured prose appropriate for a broad audience."}, {"type": "llm_judge", "name": "Fairness and Ethics", "description": "Check for balance, avoidance of undue sensationalism, acknowledgment of uncertainty, and ethical framing.", "weight": 0.75, "judge_prompt": "Evaluate fairness and ethical framing:\n- Does the pitch plan to represent multiple stakeholders fairly (astronomers, industry, regulators, advocates)?\n- Avoids sensational claims; acknowledges uncertainties and limits.\n- Notes potential conflicts of interest or bias considerations if relevant.\n\nScoring (0\u20130.75):\n- 0.75: Clearly balanced and ethically framed.\n- 0.5: Generally fair with minor issues.\n- 0.25: Noticeable bias or sensational framing.\n- 0.0: One-sided or ethically problematic.", "expectation": "A balanced, responsible approach consistent with professional journalism."}, {"type": "llm_judge", "name": "Professional Presentation and Timeline Realism", "description": "Evaluate organization, formatting polish, grammar, and whether the proposed timeline is realistic for a reported story.", "weight": 0.75, "judge_prompt": "Assess professionalism and timeline:\n- Clean organization with labeled sections; minimal typos/grammar issues.\n- Timeline includes a concrete draft submission date or timeframe and is realistic for a reported news piece with multiple interviews.\n\nScoring (0\u20130.75):\n- 0.75: Professional and realistic.\n- 0.5: Minor issues.\n- 0.25: Messy or somewhat unrealistic.\n- 0.0: Unprofessional or no viable timeline.", "expectation": "Polished presentation and a feasible schedule for reporting and drafting."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "1752cb53-5983-46b6-92ee-58ac85a11283", "rubric": {"category_name": "Manufacturing \u2022 First-Line Supervisors \u2022 Week One Test Plan (Wire Extrusion)", "rationale": "This rubric enforces a self-documenting, verifiable Excel deliverable for a Week One Test Plan to validate two new presses (Press 1 and Press 2). Stage 1 mandates a specific spreadsheet structure so verification is trivial. Stage 2 mixes precise code checks (lightweight, robust) with LLM judgment for operational correctness and cross-references. Stage 3 evaluates professional quality and stakeholder usefulness. Code rules are intentionally lower-weight than LLM rules per guidance.", "max_total_score": 22.0, "stages": [{"name": "Stage 1 \u2014 STRUCTURE GATE (Shape Enforcement)", "description": "LLM-only gate verifying that the output is an Excel workbook with the required sheets and sections to support validation planning for two presses. If this stage fails, the entire category is zeroed.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.5, "rules": [{"type": "llm_judge", "name": "Required Excel Structure Present", "description": "Verify the candidate produced a well-structured Excel workbook for the Week One Test Plan with all required sheets/sections to enable verification.", "weight": 6.0, "judge_prompt": "You are evaluating whether the candidate output is an Excel workbook with the exact structural elements required for a Week One Test Plan to validate two new extrusion presses (Press 1 and Press 2). Only check structure/presence, not correctness of numbers.\n\nRequirements (be flexible with exact names, but the content must clearly match):\nFormat:\n- The file must be an Excel workbook (.xlsx or .xls). Not PDF/DOCX/CSV.\n- The workbook must be readable and contain multiple sheets.\n\nRequired Sheets and Core Content:\n1) Main Plan sheet (name like: \"Week One Test Plan\", \"Test Plan - Week 1\", or similar):\n   - Header area with: facility/department or plan title, week dates, author/preparer.\n   - Two clearly labeled sub-sections for equipment: Press 1 Plan and Press 2 Plan.\n   - Each press sub-section must include a tabular plan with columns similar to: [Day/Date | SKU | Run Qty | Planned Hours | Setup/Changeover Time | Operator(s)].\n   - A totals or summary row/section for each press or a combined total on the sheet.\n\n2) Labor Plan sheet (name like: \"Labor Plan\", \"Staffing\", or similar):\n   - Table including columns similar to: [Team Member | Skill/Rank | Role | Assigned Press | Shift/Hours | Overtime].\n\n3) Material Plan sheet (name like: \"Material Plan\", \"Materials & Purchased Parts\", or similar):\n   - Table including columns similar to: [SKU | Raw Material | Purchased Parts | Qty Required | On-Hand | Shortage/Buy Qty | Supplier or WO].\n   - Should reference SKUs that appear in the schedule.\n\n4) Tooling/Changeover sheet (name like: \"Tooling\", \"Changeover\", or similar):\n   - Table with columns similar to: [Press | From SKU | To SKU | Tooling | Changeover Time (min) | Date/Sequence].\n\n5) Assumptions & Rules sheet (name like: \"Assumptions\", \"Guidelines & Notes\", or similar):\n   - Bulleted text that states assumptions and rules used. Should mention that both presses have equal capacity and that this plan supports validation runs. If the template mentions populating yellow cells only, the notes should acknowledge that constraint.\n\nOptional (improves score but not required):\n- Validation Summary sheet (e.g., acceptance criteria, checkpoints for Maintenance/Quality/Engineering).\n\nScoring (0 to 6):\n- 6.0: Excel file present AND all 5 required sheets with the specified structures clearly present.\n- 5.0: Excel + missing exactly one required sheet OR a required sheet is present but missing a minor structural element (e.g., one column type).\n- 3.0: Excel + only 3 required sheets substantially present.\n- 1.5: Excel + only 2 required sheets present OR structure is very incomplete.\n- 0.0: Not an Excel workbook OR fewer than 2 required sheets.\n\nOnly evaluate presence/structure. Do not check calculations or quality.", "expectation": "A navigable .xlsx with: Main Plan (Press 1/2 tables), Labor Plan, Material Plan, Tooling/Changeover, and Assumptions & Rules; optional Validation Summary."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness & Consistency)", "description": "Now that structure is enforced, verify operational correctness/consistency using a mix of code checks (deterministic) and LLM checks (cross-references and reasonableness).", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Key Labels Present (Presses and SKUs)", "description": "Confirm the workbook references both Press 1 and Press 2 and includes SKU labels across sheets.", "weight": 0.5, "code": "import pandas as pd\nimport numpy as np\nimport re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        path = context.files.get_path(output.id)\n        xl = pd.ExcelFile(path)\n        found = {\"press 1\": False, \"press 2\": False, \"sku\": False}\n        for sheet in xl.sheet_names:\n            try:\n                df = xl.parse(sheet, header=None, dtype=str)\n            except Exception:\n                continue\n            text = \" \".join(df.fillna(\"\").astype(str).values.ravel()).lower()\n            for key in found.keys():\n                if key in text:\n                    found[key] = True\n        score = (sum(1 for v in found.values() if v) / 3.0) * 0.5\n        feedback = f\"Found: {found}\"\n        return score, feedback\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Hours Plausibility Check", "description": "If any 'hour' columns exist, their totals should be positive and within a plausible weekly test window (1\u2013336 hours across all entries). Partial credit if positive but outside range.", "weight": 0.5, "code": "import pandas as pd\nimport numpy as np\nimport re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n    try:\n        path = context.files.get_path(output.id)\n        xl = pd.ExcelFile(path)\n        plausible_found = False\n        positive_found = False\n        sums = []\n        for sheet in xl.sheet_names:\n            try:\n                df = xl.parse(sheet)\n            except Exception:\n                continue\n            cols = [c for c in df.columns if isinstance(c, str) and 'hour' in c.lower()]\n            for c in cols:\n                series = pd.to_numeric(df[c], errors='coerce')\n                total = float(series.dropna().sum())\n                sums.append(total)\n                if total > 0:\n                    positive_found = True\n                if 1 <= total <= 336:\n                    plausible_found = True\n        if plausible_found:\n            return 0.5, f\"Hour totals plausible. Sums: {sums}\"\n        if positive_found:\n            return 0.25, f\"Hours positive but outside 1\u2013336. Sums: {sums}\"\n        return 0.0, f\"No hour columns with positive totals. Sums: {sums}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Cross-Sheet SKU Consistency", "description": "SKUs listed in Material Plan should overlap with SKUs scheduled on the Main Plan/Press sheets.", "weight": 0.5, "code": "import pandas as pd\nimport numpy as np\nimport re\n\ndef extract_skus(df):\n    skus = set()\n    # Prefer columns with 'sku' in header\n    for c in df.columns:\n        if isinstance(c, str) and 'sku' in c.lower():\n            vals = df[c].astype(str).str.strip()\n            for v in vals:\n                if v and v.lower() != 'nan' and v.lower() != 'sku':\n                    skus.add(v)\n    # Fallback: look for cells in first few columns that look like alphanum identifiers\n    if not skus:\n        sample = df.iloc[:, :min(4, df.shape[1])].astype(str).values.ravel()\n        for v in sample:\n            v2 = v.strip()\n            if len(v2) >= 3 and any(ch.isdigit() for ch in v2) and any(ch.isalpha() for ch in v2):\n                skus.add(v2)\n    return skus\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n    try:\n        path = context.files.get_path(output.id)\n        xl = pd.ExcelFile(path)\n        material_skus = set()\n        schedule_skus = set()\n        for sheet in xl.sheet_names:\n            try:\n                df = xl.parse(sheet)\n            except Exception:\n                continue\n            lname = sheet.lower()\n            if any(k in lname for k in ['material', 'purchased', 'bom']):\n                material_skus |= extract_skus(df)\n            if any(k in lname for k in ['plan', 'schedule', 'press', 'main']):\n                schedule_skus |= extract_skus(df)\n        inter = material_skus & schedule_skus\n        if not material_skus or not schedule_skus:\n            # Partial credit if at least one set exists\n            score = 0.25 if (material_skus or schedule_skus) else 0.0\n            return score, f\"Material SKUs: {len(material_skus)} | Schedule SKUs: {len(schedule_skus)} | Overlap: {len(inter)}\"\n        # Score by overlap coverage of scheduled SKUs (cap at 1.0)\n        coverage = len(inter) / max(1, len(schedule_skus))\n        score = max(0.0, min(1.0, coverage)) * 0.5\n        return score, f\"Overlap {len(inter)}/{len(schedule_skus)}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Changeover Data Present", "description": "Confirm a Tooling/Changeover sheet has measurable changeover/setup times.", "weight": 0.5, "code": "import pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n    try:\n        path = context.files.get_path(output.id)\n        xl = pd.ExcelFile(path)\n        found = False\n        details = []\n        for sheet in xl.sheet_names:\n            lname = sheet.lower()\n            if any(k in lname for k in ['tool', 'change', 'setup']):\n                try:\n                    df = xl.parse(sheet)\n                except Exception:\n                    continue\n                cols = [c for c in df.columns if isinstance(c, str) and any(k in c.lower() for k in ['change', 'setup', 'min'])]\n                if cols:\n                    # look for any positive numeric in these columns\n                    vals = []\n                    for c in cols:\n                        s = pd.to_numeric(df[c], errors='coerce').dropna()\n                        vals.extend(s.tolist())\n                    if any(v > 0 for v in vals):\n                        found = True\n                        details.append((sheet, cols[:3]))\n        return (0.5 if found else 0.0, f\"Changeover present: {found} {details}\")\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "llm_judge", "name": "Capacity Balance Across Presses", "description": "Check that work is reasonably balanced across Press 1 and Press 2 (unless explicitly justified in Assumptions).", "weight": 3.0, "judge_prompt": "Check whether planned runs are distributed across both presses. Consider total planned hours, number of SKUs per press, and whether one press is idle or overloaded without justification in the Assumptions & Rules. Minor imbalance is fine, but all or nearly all work on only one press should be penalized unless explicitly justified (e.g., tooling availability or sequencing rationale).\n\nScoring (0\u20133):\n- 3.0: Clear, reasonable balance across the two presses OR explicit, credible justification for imbalance.\n- 2.0: Some imbalance but both presses used; limited rationale.\n- 1.0: Major imbalance (e.g., >80% on one press) and weak/no justification.\n- 0.0: Only one press used with no justification.", "expectation": "Both presses have runs planned; totals roughly within the same order of magnitude or justified by assumptions."}, {"type": "llm_judge", "name": "Labor Coverage and Feasibility", "description": "Evaluate if staffing aligns with scheduled runs (roles/skills per press, shifts/hours coverage, no obvious double-booking).", "weight": 2.5, "judge_prompt": "Using the Labor Plan and Main Plan schedules, assess whether operator(s) and roles are assigned appropriately to each scheduled run, with plausible hours and shifts. Check for:\n- Presence of operators on each planned run\n- Roles/skills consistent with assigned press or task\n- No obvious double-booking of the same person in overlapping times\n- Reasonable total hours for the week per person (not obviously excessive)\n\nScoring (0\u20132.5):\n- 2.5: Staffing clearly covers all runs with appropriate skills and feasible hours.\n- 1.5: Minor gaps or potential overlaps but largely feasible.\n- 0.5: Significant gaps or probable double-booking.\n- 0.0: Little to no alignment between staffing and schedule.", "expectation": "Each run shows assigned operators; total hours are plausible; no person is clearly scheduled in two places at once."}, {"type": "llm_judge", "name": "Material Sufficiency vs. Schedule", "description": "Check whether the Material Plan covers the scheduled SKUs with required quantities and purchased parts.", "weight": 2.5, "judge_prompt": "Cross-check the Material Plan with the scheduled SKUs:\n- For each scheduled SKU, does the Material Plan list required raw materials and purchased parts?\n- Are required quantities, on-hand levels, and buy/shortage quantities provided and plausible?\n- Are there obvious missing items or shortages that would block the planned runs?\n\nScoring (0\u20132.5):\n- 2.5: Material coverage complete and quantities appear adequate.\n- 1.5: Minor gaps or unclear quantities for a small number of SKUs.\n- 0.5: Multiple SKUs lack material coverage or substantial shortages not addressed.\n- 0.0: Material plan does not align with the schedule.", "expectation": "Every scheduled SKU appears in the Material Plan with quantities and shortages clearly identified; plan appears executable."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism", "description": "Holistic assessment of presentation quality, stakeholder usefulness, clarity, and risk planning.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation & Readability", "description": "Evaluate formatting, clarity of tables, headers, and overall readability for busy plant stakeholders.", "weight": 1.5, "judge_prompt": "Assess the professional presentation of the workbook:\n- Clear, consistent headers and units\n- Easy-to-read tables (frozen panes, filters, or sensible layout acceptable)\n- Logical sheet naming and navigation\n- Minimal clutter; unused cells left blank as instructed\n\nScoring (0\u20131.5):\n- 1.5: Highly professional and easy to navigate.\n- 1.0: Generally clear with minor issues.\n- 0.5: Noticeable formatting issues reduce readability.\n- 0.0: Poorly formatted and hard to follow.", "expectation": "Well-formatted workbook with clear headers and legible tables; easy to navigate."}, {"type": "llm_judge", "name": "Stakeholder Alignment (Maint/Quality/Eng)", "description": "Does the plan include what each stakeholder needs to execute validation?", "weight": 1.5, "judge_prompt": "Judge whether the workbook supports Maintenance, Quality, and Engineering needs:\n- Maintenance: tooling and changeover details with times/sequence\n- Quality: validation checkpoints, sampling/acceptance criteria (ideally in a Validation Summary or Assumptions sheet)\n- Engineering: SKUs, run quantities, process parameters/notes where applicable\n\nScoring (0\u20131.5):\n- 1.5: All three stakeholder needs are addressed clearly.\n- 1.0: Two stakeholders covered well; one is light.\n- 0.5: Only one stakeholder covered well.\n- 0.0: Largely misses stakeholder needs.", "expectation": "Includes specifics for tooling/changeover, quality validation criteria, and engineering run details."}, {"type": "llm_judge", "name": "Risk, Constraints, and Contingencies", "description": "Assess identification of risks (e.g., shortages, downtime, skill gaps) and mitigation plans or buffers.", "weight": 1.5, "judge_prompt": "Evaluate whether the plan anticipates execution risks and includes mitigations:\n- Identifies material shortages, tooling conflicts, or staffing constraints\n- Includes buffers or sequencing to reduce changeover time\n- Notes contingency plans (e.g., alternate operators/materials)\n\nScoring (0\u20131.5):\n- 1.5: Clear risks with practical mitigations.\n- 1.0: Some risks noted; mitigations partial.\n- 0.5: Risks mentioned without viable mitigations.\n- 0.0: No risks or mitigations addressed.", "expectation": "Explicit risks and mitigations that make the plan executable under real-world conditions."}, {"type": "llm_judge", "name": "Clarity of Assumptions & Instructions", "description": "Evaluate whether assumptions, constraints, and update/use instructions are clear.", "weight": 1.5, "judge_prompt": "Review the Assumptions & Rules (and any summary notes):\n- States that both presses have equal capacity and that this is for initial validation runs\n- Mentions the constraint to populate only yellow cells (or acknowledges template usage)\n- Provides how to update or extend the plan after Week One\n- Dates, author, and versioning are clear\n\nScoring (0\u20131.5):\n- 1.5: Clear, complete assumptions and instructions.\n- 1.0: Mostly clear with a minor omission.\n- 0.5: Several omissions reduce clarity.\n- 0.0: Assumptions/instructions largely missing.", "expectation": "Concise, explicit assumptions and guidance suitable for handoff to the plant manager."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "90edba97-74f0-425a-8ff6-8b93182eb7cb", "rubric": {"category_name": "Dialysis Monthly Lab Tracker Completion and Protocol Application", "rationale": "This rubric enforces a self-documenting, verifiable Excel workbook that captures monthly dialysis patient labs and applied standing orders. Stage 1 (LLM-only) mandates a strict workbook shape that enables automated checks. Stage 2 mixes lightweight code rules (bounds, mappings, keyword matches) with heavier LLM judgment for protocol adherence and cross-referencing. Stage 3 assesses professional quality, clarity, and auditability for clinical use.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (LLM-only)", "description": "Verify the output is an Excel workbook with the exact structure required to enable verification: a tracker sheet with specific columns, a change log, and summary/methodology sheets. Only structural presence and format are checked here, not correctness of data or orders.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.0, "rules": [{"type": "llm_judge", "name": "Workbook and Sheet Structure", "description": "Confirms the candidate output is an Excel workbook with required sheets.", "weight": 2.5, "judge_prompt": "You are evaluating the STRUCTURE ONLY. Open the candidate output (you will see rendered sheet images). Decide if it is an Excel workbook with the following sheets (flexible naming allowed as long as meaning is clear):\n\nRequired sheets:\n1) Primary tracker sheet named exactly \"Monthly Tracker- Patient Lab Results\" OR a near-equivalent like \"Monthly Tracker\" or \"Patient Lab Results\" that clearly serves as the tracker\n2) \"Change Log\" (or \"Orders Log\" / \"Medication Changes\")\n\nOptional but preferred:\n3) \"Summary\" (per-patient rollup) and\n4) \"Methodology/References\" (notes citing Patient Lab Reports and Standing Order Protocols)\n\nScoring:\n- 2.5: Excel + both required sheets present (primary tracker + change log)\n- 1.5: Excel + only the primary tracker sheet present\n- 0.0: Not Excel OR missing the primary tracker sheet\n\nOnly check presence/format, not content correctness.", "expectation": "An .xlsx workbook with at least the tracker and the change log sheets."}, {"type": "llm_judge", "name": "Tracker Columns and Monthly Layout", "description": "Checks the primary tracker sheet has required columns and a monthly layout suitable for 12 months per patient.", "weight": 2.0, "judge_prompt": "Check the primary tracker sheet. Be flexible with column name variations, but the sheet must clearly include these fields as visible columns:\n- Patient Name\n- MRN\n- Provider (physician)\n- Month\n- Kt/V (dialysis adequacy)\n- HGB or Hemoglobin (for anemia)\n- Serum Calcium (mg/dL)\n- Serum Phosphorus (mg/dL)\n- Albumin (g/dL)\n- Notes/Comments\n- Orders/Changes Applied (free-text is fine)\n\nLayout requirement: For each named patient, there should be a monthly structure (ideally Jan\u2013Dec) with at least one row per month per patient. If months are encoded numerically (1\u201312) or as dates, that is acceptable as long as it is obviously monthly.\n\nScoring:\n- 2.0: All required columns present + clearly monthly rows for each patient (intended for 12 months)\n- 1.0: Most required columns present (missing 1\u20132) and monthly intent is clear\n- 0.0: Lacking core columns or no clear monthly structure\n\nOnly check presence/structure, not correctness.", "expectation": "A well-labeled table with the listed clinical data columns and an Orders/Changes column, in monthly rows."}, {"type": "llm_judge", "name": "Patient Roster and Provider Mapping Present", "description": "Confirms all specified patients are present and assigned a provider column, enabling later verification.", "weight": 1.5, "judge_prompt": "On the tracker, verify these seven patients are present by name, and there is a Provider column that maps them to their physicians. The expected mapping is:\n- Dr. Joe: Cash Stonewater, Fred Fintmore\n- Dr. Johnson: Betty Brite, Tina Lee Bell\n- Dr. Lee: Eric Bird, Homer Sandson\n- Dr. Michael: Jessica Rashmore\n\nBe flexible about exact capitalization/spelling variants that are clearly the same names. The Provider field can be like \"Dr. Joe\" or just \"Joe\" as long as the physician identity is clear.\n\nScoring:\n- 1.5: All seven patients present and a Provider column exists\n- 0.8: 5\u20136 patients present and Provider column exists\n- 0.0: Fewer than 5 patients present OR no Provider column\n\nOnly check presence, not correctness of clinical data.", "expectation": "All listed patients appear in the tracker with an associated provider column."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness Verification (Mixed: Code + LLM)", "description": "Now that the structure is correct, check data plausibility and protocol adherence. Code rules do deterministic checks (bounds, mappings, keyword presence) while LLM judges verify nuanced protocol logic and cross-references.", "is_required": true, "max_points": 9.0, "min_score_to_pass": 4.0, "rules": [{"type": "code", "name": "Data Plausibility and Completeness (bounds + months)", "description": "Checks that tracker has plausible numeric ranges for labs and broadly complete monthly coverage per patient.", "weight": 0.3, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef _normalize_col(c):\n    c = str(c).strip().lower()\n    c = re.sub(r\"[^a-z0-9/ ]+\", \"\", c)\n    return c\n\ndef _find_col(cols, candidates):\n    for col in cols:\n        cl = _normalize_col(col)\n        for cand in candidates:\n            if cand in cl:\n                return col\n    return None\n\ndef _get_tracker_df(context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return None, \"No spreadsheet output\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        best_df = None\n        best_score = -1\n        for sn in xls.sheet_names:\n            try:\n                df = pd.read_excel(path, sheet_name=sn)\n            except Exception:\n                continue\n            cols = list(df.columns)\n            norm = [_normalize_col(c) for c in cols]\n            # heuristic: count of expected fields found\n            expect_sets = [\n                [\"patient\", \"name\"],\n                [\"mrn\"],\n                [\"provider\"],\n                [\"month\"],\n                [\"kt/v\", \"ktv\"],\n                [\"hgb\", \"hemoglobin\"],\n                [\"calcium\"],\n                [\"phosphorus\", \"phos\"],\n                [\"albumin\"],\n                [\"order\", \"change\"],\n            ]\n            score = 0\n            for ex in expect_sets:\n                if any(any(e in n for e in ex) for n in norm):\n                    score += 1\n            if score > best_score:\n                best_score = score\n                best_df = df.copy()\n        return best_df, None\n    except Exception as e:\n        return None, f\"Error reading spreadsheet: {e}\"\n\ndef evaluate(workflow, context):\n    df, err = _get_tracker_df(context)\n    if df is None or len(df) == 0:\n        return 0.0, err or \"Tracker sheet not found or empty\"\n\n    # Map columns\n    cols = list(df.columns)\n    c_patient = _find_col(cols, [\"patient name\", \"patient\"]) or _find_col(cols, [\"patient\"])\n    c_mrn = _find_col(cols, [\"mrn\"]) or _find_col(cols, [\"medical record\"]) \n    c_provider = _find_col(cols, [\"provider\", \"physician\", \"doctor\"]) \n    c_month = _find_col(cols, [\"month\", \"mo\", \"mnth\"]) \n    c_ktv = _find_col(cols, [\"kt/v\", \"ktv\"]) \n    c_hgb = _find_col(cols, [\"hgb\", \"hemoglobin\"]) \n    c_ca = _find_col(cols, [\"calcium\", \"ca\"]) \n    c_phos = _find_col(cols, [\"phosphorus\", \"phos\"]) \n    c_alb = _find_col(cols, [\"albumin\", \"alb\"]) \n\n    required = [c_patient, c_mrn, c_provider, c_month, c_ktv, c_hgb, c_ca, c_phos, c_alb]\n    if any(c is None for c in required):\n        # Missing core columns\n        return 0.2, \"Missing one or more core columns\"\n\n    # Convert numeric columns\n    def to_num(x):\n        try:\n            return pd.to_numeric(x, errors='coerce')\n        except Exception:\n            return pd.Series([np.nan]*len(x))\n\n    n_ktv = to_num(df[c_ktv])\n    n_hgb = to_num(df[c_hgb])\n    n_ca = to_num(df[c_ca])\n    n_phos = to_num(df[c_phos])\n    n_alb = to_num(df[c_alb])\n\n    # Bounds (plausible clinical ranges)\n    rng_checks = []\n    def frac_in_range(series, lo, hi):\n        s = series.dropna()\n        if len(s) == 0:\n            return 0.0\n        return float(((s >= lo) & (s <= hi)).mean())\n\n    rng_checks.append(frac_in_range(n_ktv, 0.3, 2.5))\n    rng_checks.append(frac_in_range(n_hgb, 6.0, 18.0))\n    rng_checks.append(frac_in_range(n_ca, 6.0, 12.0))\n    rng_checks.append(frac_in_range(n_phos, 2.0, 10.0))\n    rng_checks.append(frac_in_range(n_alb, 2.0, 5.5))\n\n    rng_score = np.mean(rng_checks) if len(rng_checks) else 0.0\n\n    # Month coverage heuristic: for known patients, do we have near-annual rows?\n    # Count unique months per patient; accept >=10 as broadly complete\n    try:\n        pm = df.groupby(c_patient)[c_month].nunique(dropna=True)\n        if len(pm) > 0:\n            coverage = (pm >= 10).mean()\n        else:\n            coverage = 0.0\n    except Exception:\n        coverage = 0.0\n\n    # Combine: require both presence and plausibility; weight plausibility higher\n    score = 0.4*coverage + 0.6*rng_score\n    score = float(max(0.0, min(1.0, score)))\n    return score, f\"Ranges OK: {rng_score:.2f}, Coverage: {coverage:.2f}\""}, {"type": "code", "name": "Provider Mapping Correctness", "description": "Validates that each named patient is assigned to the correct provider based on the task instructions.", "weight": 0.3, "code": "import re\nimport pandas as pd\n\ndef _normalize(s):\n    return re.sub(r\"\\s+\", \" \", str(s).strip().lower())\n\nEXPECTED = {\n    \"cash stonewater\": \"joe\",\n    \"fred fintmore\": \"joe\",\n    \"betty brite\": \"johnson\",\n    \"tina lee bell\": \"johnson\",\n    \"eric bird\": \"lee\",\n    \"homer sandson\": \"lee\",\n    \"jessica rashmore\": \"michael\",\n}\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output\"\n    try:\n        path = context.files.get_path(output.id)\n        # Try to find a sheet with Provider and Patient columns\n        xl = pd.ExcelFile(path)\n        best = None\n        best_score = -1\n        for sn in xl.sheet_names:\n            try:\n                df = pd.read_excel(path, sheet_name=sn)\n            except Exception:\n                continue\n            cols = [_normalize(c) for c in df.columns]\n            has_patient = any(\"patient\" in c for c in cols)\n            has_provider = any(any(x in c for x in [\"provider\",\"physician\",\"doctor\"]) for c in cols)\n            if has_patient and has_provider:\n                score = sum([has_patient, has_provider])\n                if score > best_score:\n                    best_score = score\n                    best = df\n        if best is None or best.empty:\n            return 0.0, \"No suitable tracker sheet with provider/patient columns\"\n\n        # Identify columns\n        def find_col(cands):\n            for c in best.columns:\n                lc = _normalize(c)\n                for k in cands:\n                    if k in lc:\n                        return c\n            return None\n        c_patient = find_col([\"patient name\",\"patient\"]) or find_col([\"patient\"]) \n        c_provider = find_col([\"provider\",\"physician\",\"doctor\"]) \n        if c_patient is None or c_provider is None:\n            return 0.0, \"Missing patient or provider column\"\n\n        # Evaluate mapping matches\n        matches = 0\n        total = 0\n        for _, row in best.iterrows():\n            pname = _normalize(row.get(c_patient, \"\"))\n            prov = _normalize(row.get(c_provider, \"\"))\n            if not pname:\n                continue\n            for exp_name, exp_doc in EXPECTED.items():\n                if pname == exp_name:\n                    total += 1\n                    if exp_doc in prov:\n                        matches += 1\n                    break\n        if total == 0:\n            return 0.0, \"Expected patients not found by name\"\n        ratio = matches/total\n        return float(ratio), f\"Provider matches: {matches}/{total} ({ratio:.0%})\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Standing Orders Trigger \u2192 Orders Text Match", "description": "For rows that meet triggers (HGB<10, Ca 7.9\u20138.4, Kt/V<1.2, Phos thresholds with provider-specific binders), checks that Orders/Changes mention the correct medication/action keywords.", "weight": 0.3, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef nrm(s):\n    return re.sub(r\"\\s+\", \" \", str(s).strip().lower())\n\nDOC_FOR_PATIENT = {\n    \"cash stonewater\": \"joe\",\n    \"fred fintmore\": \"joe\",\n    \"betty brite\": \"johnson\",\n    \"tina lee bell\": \"johnson\",\n    \"eric bird\": \"lee\",\n    \"homer sandson\": \"lee\",\n    \"jessica rashmore\": \"michael\",\n}\n\nRENVELA_KEYS = [\"renvela\", \"sevelamer\"]\nPHOSLO_KEYS = [\"phoslo\", \"calcium acetate\"]\nARANESP_KEYS = [\"aranesp\"]\nTUMS_KEYS = [\"tums\"]\nREPEAT_KEYS = [\"repeat\", \"recheck\", \"re test\", \"re-test\"]\nLAB_KEYS = [\"lab\", \"labs\"]\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output\"\n    try:\n        path = context.files.get_path(output.id)\n        xl = pd.ExcelFile(path)\n        tracker = None\n        best = -1\n        for sn in xl.sheet_names:\n            try:\n                df = pd.read_excel(path, sheet_name=sn)\n            except Exception:\n                continue\n            cols = [nrm(c) for c in df.columns]\n            score = sum([any(k in c for c in cols) for k in [\"patient\",\"mrn\",\"provider\",\"month\",\"kt\",\"hgb\",\"calcium\",\"phos\",\"albumin\",\"order\",\"change\"]])\n            if score > best:\n                best = score\n                tracker = df\n        if tracker is None or tracker.empty:\n            return 0.0, \"Tracker sheet not found\"\n\n        # Columns\n        def fcol(cands):\n            for c in tracker.columns:\n                lc = nrm(c)\n                if any(k in lc for k in cands):\n                    return c\n            return None\n        c_patient = fcol([\"patient name\",\"patient\"]) or fcol([\"patient\"]) \n        c_provider = fcol([\"provider\",\"physician\",\"doctor\"]) \n        c_ktv = fcol([\"kt/v\",\"ktv\",\"kt v\"]) \n        c_hgb = fcol([\"hgb\",\"hemoglobin\"]) \n        c_ca = fcol([\"calcium\",\"ca\"]) \n        c_phos = fcol([\"phosphorus\",\"phos\"]) \n        c_orders = fcol([\"order\",\"change\"]) \n        if any(x is None for x in [c_patient, c_provider, c_ktv, c_hgb, c_ca, c_phos, c_orders]):\n            return 0.0, \"Missing columns needed for trigger checks\"\n\n        # Prepare\n        def tonum(s):\n            try:\n                return pd.to_numeric(s, errors='coerce')\n            except Exception:\n                return pd.Series([np.nan]*len(s))\n        ktv = tonum(tracker[c_ktv])\n        hgb = tonum(tracker[c_hgb])\n        ca = tonum(tracker[c_ca])\n        phos = tonum(tracker[c_phos])\n\n        # Evaluate rows\n        total_triggers = 0\n        satisfied = 0\n        for idx, row in tracker.iterrows():\n            pname = nrm(row.get(c_patient, \"\"))\n            prov = nrm(row.get(c_provider, \"\"))\n            order_txt = nrm(row.get(c_orders, \"\"))\n            v_ktv = ktv.iloc[idx] if idx < len(ktv) else np.nan\n            v_hgb = hgb.iloc[idx] if idx < len(hgb) else np.nan\n            v_ca = ca.iloc[idx] if idx < len(ca) else np.nan\n            v_phos = phos.iloc[idx] if idx < len(phos) else np.nan\n\n            # Determine provider expected for binder rules\n            exp_doc = None\n            for k,v in DOC_FOR_PATIENT.items():\n                if pname == k:\n                    exp_doc = v\n                    break\n            if exp_doc is None:\n                # fall back: infer from provider cell\n                if \"joe\" in prov: exp_doc = \"joe\"\n                elif \"johnson\" in prov: exp_doc = \"johnson\"\n                elif \"lee\" in prov: exp_doc = \"lee\"\n                elif \"michael\" in prov: exp_doc = \"michael\"\n\n            # HGB < 10 -> Aranesp\n            if pd.notna(v_hgb) and v_hgb < 10.0:\n                total_triggers += 1\n                if any(k in order_txt for k in ARANESP_KEYS):\n                    satisfied += 1\n\n            # Calcium 7.9\u20138.4 -> TUMS\n            if pd.notna(v_ca) and (7.9 <= v_ca <= 8.4):\n                total_triggers += 1\n                if any(k in order_txt for k in TUMS_KEYS):\n                    satisfied += 1\n\n            # Kt/V < 1.2 -> repeat labs\n            if pd.notna(v_ktv) and v_ktv < 1.2:\n                total_triggers += 1\n                if (any(k in order_txt for k in REPEAT_KEYS) and any(k in order_txt for k in LAB_KEYS)):\n                    satisfied += 1\n\n            # Phosphorus binder rules by provider\n            if pd.notna(v_phos):\n                # Dr Joe/Johnson \u2192 Renvela for 5.6\u20137.4; continue 4.0\u20135.5 (we only detect initiation keywords)\n                if exp_doc in (\"joe\",\"johnson\") and (5.6 <= v_phos <= 7.4):\n                    total_triggers += 1\n                    if any(k in order_txt for k in RENVELA_KEYS):\n                        satisfied += 1\n                # Dr Michael/Lee \u2192 Phoslo for 5.5\u20137.4\n                if exp_doc in (\"michael\",\"lee\") and (5.5 <= v_phos <= 7.4):\n                    total_triggers += 1\n                    if any(k in order_txt for k in PHOSLO_KEYS):\n                        satisfied += 1\n\n        if total_triggers == 0:\n            return 0.0, \"No identifiable trigger events to evaluate\"\n        ratio = satisfied/total_triggers\n        return float(max(0.0, min(1.0, ratio))), f\"Matched {satisfied}/{total_triggers} trigger\u2192order cases ({ratio:.0%})\"    \n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "llm_judge", "name": "Protocol Logic Adherence (LLM)", "description": "Checks whether orders applied each month align with standing protocols for anemia, mineral metabolism, adequacy, and provider-specific phosphorus binders.", "weight": 3.5, "judge_prompt": "Open the workbook. Using the tracker and change log, sample at least 2 months for at least 3 different patients (include both Dr. Joe/Johnson and Dr. Lee/Michael groups).\n\nVerify the following logic is correctly applied where lab values trigger them:\n- HGB < 10.0 \u2192 Start Aranesp 10 mcg IVP each treatment (or reduce to twice weekly if explicitly indicated by a dosing adjustment table)\n- Serum calcium 7.9\u20138.4 mg/dL \u2192 TUMS 2 tabs by mouth, 3x/week\n- Kt/V < 1.2 \u2192 Order to repeat lab work in one month\n- Provider-specific phosphorus binder initiations when phosphorus is elevated:\n  \u2022 Dr. Joe & Dr. Johnson \u2192 Initiate Renvela 800 mg with meals for 5.6\u20137.4 mg/dL\n  \u2022 Dr. Michael & Dr. Lee \u2192 Initiate Phoslo 667 mg, 2 tabs with each meal for 5.5\u20137.4 mg/dL\n\nScoring guidance (3.5 max):\n- 3.5: All sampled trigger cases correctly handled with appropriate, specific orders\n- 2.5: Minor omissions (e.g., 1 missed trigger) but overall correct\n- 1.5: Multiple missed/incorrect triggers or wrong binder by provider\n- 0.5: Orders mostly not aligned with labs\n- 0.0: No evidence of protocol application\n\nJudge only correctness of logic application, not stylistic quality.", "expectation": "Orders correctly match triggers for a representative sample of months/patients."}, {"type": "llm_judge", "name": "Order Specificity and Dosing Correctness (LLM)", "description": "Evaluates whether orders specify drug, dose, route, and frequency according to the described standing orders.", "weight": 2.5, "judge_prompt": "Review Orders/Changes and the Change Log. Check that orders include the following specifics when applicable:\n- Aranesp: \"10 mcg IVP each treatment\" (frequency downgrade to twice weekly only if protocol indicates)\n- TUMS: \"2 tabs by mouth 3x/week\"\n- Renvela: \"800 mg with meals\" (titration every two weeks acceptable if mentioned)\n- Phoslo: \"667 mg, 2 tablets with each meal\" (increases q2 weeks acceptable if mentioned)\n- Repeat labs after Kt/V<1.2: clearly states repeat timeframe (one month)\n\nScoring guidance (2.5 max):\n- 2.5: Orders consistently include drug, dose, route, and frequency with correct values\n- 1.7: Mostly specific but occasional missing element (e.g., missing route or frequency)\n- 0.8: Frequently missing key specifics or wrong doses\n- 0.0: Orders lack dosage/frequency clarity\n\nFocus on specificity and correctness, not formatting.", "expectation": "Medication orders include explicit dose, route, and frequency as per protocols."}, {"type": "llm_judge", "name": "Consistency: Change Log \u2194 Tracker (LLM)", "description": "Assesses whether the change log entries correspond to the same months and actions recorded on the tracker.", "weight": 2.1, "judge_prompt": "Cross-check the Change Log with the tracker for a few patients and months. Do the logged changes (drug, dose, rationale) match the Orders/Changes recorded on the tracker in the same month? Look for mismatches, missing entries, or contradictions.\n\nScoring guidance (2.1 max):\n- 2.1: Entries are consistently mirrored between both sheets\n- 1.3: Minor discrepancies but mostly consistent\n- 0.6: Multiple mismatches or missing log entries\n- 0.0: Log and tracker are not aligned\n\nDo not judge stylistic quality here\u2014only factual consistency between sheets.", "expectation": "Change Log and tracker agree for the same months and actions."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism (LLM)", "description": "Holistic assessment of presentation, clarity for clinical use, and auditability to support care and compliance.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Presentation and Usability", "description": "Evaluates readability, labeling, and ease of use for clinicians.", "weight": 1.5, "judge_prompt": "Assess the workbook\u2019s professional presentation and usability:\n- Clear headers, legible fonts, consistent units (mg/dL, g/dL)\n- Obvious monthly grouping per patient; easy to scan\n- Optional niceties: filters, frozen header row, consistent date/month formatting\n\nScoring (1.5 max):\n- 1.5: Clean, professional, easy to navigate; units consistent\n- 1.0: Generally clear with minor issues\n- 0.5: Cluttered or inconsistent but usable\n- 0.0: Hard to read or confusing layout", "expectation": "A clean, clinician-friendly tracker with consistent labels and units."}, {"type": "llm_judge", "name": "Clinical Communication Clarity", "description": "Assesses clarity of Orders/Changes and rationales so another nurse could safely follow them.", "weight": 1.5, "judge_prompt": "Review Orders/Changes and any rationales/notes:\n- Are instructions unambiguous (drug, dose, route, frequency, duration/next steps)?\n- Rationales cite lab values or protocol rules (e.g., \u201cHGB 9.8 \u2192 start Aranesp 10 mcg IVP each treatment\u201d).\n- Avoids ambiguous shorthand that could cause error.\n\nScoring (1.5 max):\n- 1.5: Clear, complete instructions with rationales tied to labs/protocols\n- 1.0: Mostly clear with occasional ambiguity\n- 0.5: Frequent ambiguity or missing rationale\n- 0.0: Unsafe or unclear instructions", "expectation": "Orders are actionable and rationales link to the triggering labs."}, {"type": "llm_judge", "name": "Auditability and Traceability", "description": "Determines if the workbook supports audit/compliance needs.", "weight": 1.0, "judge_prompt": "Check that entries are traceable:\n- Each change references the month and the triggering lab(s)\n- Patient names and MRNs consistently present\n- Methodology/References sheet (if present) cites source docs (Patient Lab Reports, Standing Order Protocols)\n\nScoring (1.0 max):\n- 1.0: Fully traceable with clear references\n- 0.6: Mostly traceable with minor gaps\n- 0.3: Traceability inconsistent\n- 0.0: Not auditable", "expectation": "A clear trail from labs \u2192 protocol \u2192 order with patient identifiers."}, {"type": "llm_judge", "name": "Data Integrity and Completeness", "description": "Checks for missing values and internal consistency across months.", "weight": 1.0, "judge_prompt": "Scan for missing or inconsistent data:\n- Minimal blanks in core lab fields (Kt/V, HGB, Calcium, Phosphorus, Albumin)\n- Units consistent across months\n- No contradictions (e.g., Aranesp order without low HGB anywhere)\n\nScoring (1.0 max):\n- 1.0: Data appears complete and consistent\n- 0.6: Some gaps but generally sound\n- 0.3: Frequent gaps/inconsistencies\n- 0.0: Data largely incomplete or contradictory", "expectation": "Core lab fields filled and consistent; orders make sense given the data."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "0e386e32-df20-4d1f-b536-7159bc409ad5", "rubric": {"category_name": "PrivateCrypMix Full-Stack Implementation (Code + Manifest)", "rationale": "This rubric enforces a self-documenting delivery: a professionally formatted Implementation Manifest (PDF/DOCX) as the primary output plus a ZIP archive of the complete codebase among task outputs. Stage 1 gates on the manifest\u2019s exact structure to make verification trivial. Stage 2 mixes light-weight code checks (ZIP structure, presence of key artifacts) with heavier LLM checks for technical correctness and consistency against the brief. Stage 3 evaluates overall professional quality, security depth, developer experience, and UX clarity. Code rule weights are ~5x lower than LLM rules in Stage 2, aligning with the philosophy that LLMs judge complex correctness while code performs deterministic checks.", "max_total_score": 24.0, "stages": [{"name": "Stage 1 \u2014 Manifest Shape Enforcement (GATE)", "description": "Primary output must be a professional Implementation Manifest (PDF or DOCX) with required sections that make verification possible. NO code checks here.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Implementation Manifest Format and Sections", "description": "Check the primary output is a PDF/DOCX Implementation Manifest with exact structural requirements enabling verification.", "weight": 4.0, "judge_prompt": "You are evaluating ONLY the primary output file. Verify it is a professionally formatted PDF or DOCX Implementation Manifest for the PrivateCrypMix project (Polygon + Aave + Connext + zkSNARKs). Do not judge correctness here; only check presence and structure.\n\nFormat requirements:\n- File type: PDF or DOCX (not Markdown, not plain text, not Excel)\n- Minimum length: 4 pages (or equivalent in DOCX)\n- Clear section headers and professional formatting\n\nRequired sections (be flexible with naming but require substance):\n1) Executive Summary / Overview\n2) Architecture Overview (frontend dApp, smart contracts, zk circuits/proofs, cross-chain/Connext, optional relayer)\n3) Repository Directory Structure & File Inventory (readable tree plus mapping components\u2192paths)\n4) Setup & Build Instructions (contracts compile/deploy, circuits setup/proving keys or precomputed assets, frontend install/run, relayer config)\n5) Environment & Configuration (.env variables list and descriptions)\n6) Smart Contracts Details (per-contract responsibilities, key storage, events, main functions; fixed denominations and lock-up parameter enforcement)\n7) ZK Proof System (commitments/nullifiers, Merkle set, verifier contract, proof inputs/outputs)\n8) Cross-Chain Flow (Connext usage: xcall/receiver, asset/data bridging, failure handling outline)\n9) Yield Integration (Aave: deposit/withdraw of underlying, aToken handling, accrual model)\n10) Test Plan & How to Run (unit/integration tests: deposit/withdraw, waiting period, zk proof verification, Aave mock/simulation, Connext mock)\n11) Security Considerations & Threat Model (reentrancy, double-spend/nullifier reuse, linkage/metadata leaks, bridge risks, griefing/MEV, parameter risks)\n12) UX & Anonymity Delay (fixed-size deposit rationale, delay communication in UI, countdown/state handling)\n13) Appendix (ABIs or references, API endpoints if any, example env template)\n\nScoring (structure-only):\n- 4.0: Valid PDF/DOCX and all sections 1\u201312 present with a clear Appendix (13)\n- 3.5: Valid format with at most 1 missing minor section (e.g., Appendix) and all core sections 1\u201311 present\n- 2.5: Valid format but missing 2\u20133 sections overall\n- 1.0: Valid format but only 4\u20135 sections or very sparse structure\n- 0.0: Wrong format OR fewer than 4 pages OR lacks most required sections\n\nOnly evaluate presence/structure, not technical accuracy.", "expectation": "A clearly structured, multi-section Implementation Manifest in PDF/DOCX enabling automated and human verification."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness Verification (Mixed)", "description": "Now verify technical correctness and completeness against the brief using a mix of deterministic code checks on the ZIP and LLM checks on the Manifest. Code rules are low-weight; LLM rules carry most points.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "ZIP Archive Presence and Structure", "description": "Locate a .zip among outputs; verify core folders/files exist: contracts, frontend, circuits/zk, tests, config, README.", "weight": 0.7, "code": "import re\nimport json\nimport zipfile\nfrom pathlib import Path\n\ndef evaluate(workflow, context):\n    try:\n        zips = []\n        for res in context.get_all_outputs():\n            try:\n                p = context.files.get_path(res.id)\n                if str(p).lower().endswith('.zip'):\n                    zips.append((res, p))\n            except Exception:\n                continue\n        if not zips:\n            return 0.0\n        # Use the first ZIP found\n        zip_path = zips[0][1]\n        try:\n            with zipfile.ZipFile(zip_path, 'r') as zf:\n                names = zf.namelist()\n        except Exception:\n            return 0.0\n        low = [n.lower() for n in names]\n        def has_dir(keyword):\n            kw = keyword.lower().strip('/') + '/'\n            return any(kw in n for n in low)\n        def has_file_suffix(suffix):\n            suf = suffix.lower()\n            return any(n.lower().endswith(suf) for n in names)\n        def has_any_file_substring(substrs):\n            return any(any(sub in n for sub in substrs) for n in low)\n        checks = []\n        # Core directories\n        checks.append(has_dir('contracts'))\n        checks.append(has_dir('test') or has_dir('tests'))\n        # Frontend directory common names\n        checks.append(has_dir('frontend') or has_dir('app') or has_dir('dapp') or has_dir('web'))\n        # ZK/circuits\n        checks.append(has_dir('circuits') or has_dir('zk') or any(n.lower().endswith('.circom') for n in low))\n        # Config files\n        checks.append(has_any_file_substring(['hardhat.config', 'foundry.toml']))\n        # Readme/manifest inside zip\n        checks.append(has_any_file_substring(['readme.md', 'readme.pdf', 'readme.txt', 'manifest.md', 'manifest.pdf']))\n        # Optional relayer/back-end (counts as +0.5 if present)\n        relayer_present = has_dir('relayer') or has_dir('backend') or has_dir('server')\n        score = sum(1.0 if c else 0.0 for c in checks)\n        score += 0.5 if relayer_present else 0.0\n        # Max core = 6, optional = 0.5 -> cap at 6.5\n        score = min(score, 6.5)\n        return max(0.0, min(1.0, score / 6.5))\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Build/Test Scripts Presence", "description": "Within ZIP, ensure package.json files define useful scripts: build/compile, test, deploy/start.", "weight": 0.6, "code": "import json\nimport zipfile\n\ndef evaluate(workflow, context):\n    try:\n        # Find a zip\n        zip_path = None\n        for res in context.get_all_outputs():\n            try:\n                p = context.files.get_path(res.id)\n                if str(p).lower().endswith('.zip'):\n                    zip_path = p\n                    break\n            except Exception:\n                continue\n        if not zip_path:\n            return 0.0\n        with zipfile.ZipFile(zip_path, 'r') as zf:\n            pkg_files = [n for n in zf.namelist() if n.lower().endswith('package.json')]\n            if not pkg_files:\n                return 0.0\n            found = set()\n            for pf in pkg_files:\n                try:\n                    data = json.loads(zf.read(pf).decode('utf-8', errors='ignore'))\n                except Exception:\n                    continue\n                scripts = data.get('scripts', {}) or {}\n                s_keys = set(scripts.keys())\n                # look for common script intents\n                if {'build','compile'} & s_keys: found.add('build')\n                if 'test' in s_keys: found.add('test')\n                if {'deploy','deploy:dev','deploy:testnet'} & s_keys: found.add('deploy')\n                if {'start','dev','serve'} & s_keys: found.add('start')\n                if {'lint'} & s_keys: found.add('lint')\n            # Require at least build/compile and test; others add credit\n            needed = {'build','test'}\n            score = 0.0\n            if needed.issubset(found):\n                score = 0.6\n                # bonus up to 1.0 with deploy/start/lint\n                extras = len({'deploy','start','lint'} & found)\n                score = min(1.0, 0.6 + 0.2 * extras)\n            else:\n                # partial if either build or test present\n                score = 0.3 if (('build' in found) or ('test' in found)) else 0.0\n            return max(0.0, min(1.0, score))\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Solidity Features and Integrations Signals", "description": "Scan .sol files for pragma, events, nullifier/commitment/Merkle, Aave and Connext references, Verifier, fixed denominations, and withdrawal delay enforcement.", "weight": 0.7, "code": "import re\nimport zipfile\n\ndef evaluate(workflow, context):\n    try:\n        # Find zip\n        zip_path = None\n        for res in context.get_all_outputs():\n            try:\n                p = context.files.get_path(res.id)\n                if str(p).lower().endswith('.zip'):\n                    zip_path = p\n                    break\n            except Exception:\n                continue\n        if not zip_path:\n            return 0.0\n        with zipfile.ZipFile(zip_path, 'r') as zf:\n            sol_files = [n for n in zf.namelist() if n.lower().endswith('.sol')]\n            if not sol_files:\n                return 0.0\n            text = ''\n            total_read = 0\n            for sf in sol_files:\n                try:\n                    chunk = zf.read(sf)[:200000]  # limit per file\n                    text += '\\n' + chunk.decode('utf-8', errors='ignore')\n                    total_read += len(chunk)\n                except Exception:\n                    continue\n                if total_read > 1500000:\n                    break\n            low = text.lower()\n            flags = []\n            flags.append(bool(re.search(r'pragma\\s+solidity', low)))\n            flags.append(('event deposit' in low) or ('event deposited' in low))\n            flags.append(('event withdrawal' in low) or ('event withdraw' in low))\n            flags.append(('nullifier' in low) and ('commitment' in low))\n            flags.append(('merkle' in low) or ('roots' in low and 'tree' in low))\n            flags.append(('iverifier' in low) or ('verifier.sol' in low) or re.search(r'contract\\s+verifier', low))\n            # Aave indicators\n            flags.append(('aave' in low) or ('ipool' in low) or ('lendingpool' in low) or ('atoken' in low))\n            # Connext indicators\n            flags.append(('iconnext' in low) or ('ixreceiver' in low) or ('xcall' in low))\n            # Fixed denomination enforcement heuristics\n            fixed = ('denomination' in low) or ('fixed' in low) or re.search(r'require\\s*\\(\\s*amount\\s*[!=><]', low) or ('allowed' in low and 'denominations' in low)\n            flags.append(bool(fixed))\n            # Time delay enforcement (lock-up)\n            delay = (('withdrawaldelay' in low) or ('lock' in low) or ('unlock' in low)) and ('block.timestamp' in low)\n            flags.append(bool(delay))\n            score = sum(1 for f in flags if f)\n            return max(0.0, min(1.0, score / float(len(flags) if flags else 1)))\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "ZK Artifacts and Tooling Signals", "description": "Check for circuits and proving/verifier artifacts and circom/snarkjs references.", "weight": 0.5, "code": "import json\nimport zipfile\n\ndef evaluate(workflow, context):\n    try:\n        zip_path = None\n        for res in context.get_all_outputs():\n            try:\n                p = context.files.get_path(res.id)\n                if str(p).lower().endswith('.zip'):\n                    zip_path = p\n                    break\n            except Exception:\n                continue\n        if not zip_path:\n            return 0.0\n        with zipfile.ZipFile(zip_path, 'r') as zf:\n            names = zf.namelist()\n            low = [n.lower() for n in names]\n            flags = []\n            # Files indicating circuits\n            flags.append(any(n.endswith('.circom') for n in low))\n            flags.append(any(n.endswith('.r1cs') for n in low))\n            flags.append(any(n.endswith('.zkey') for n in low))\n            flags.append(any('verifier.sol' in n for n in low))\n            # Tooling in package.json\n            has_circom = False\n            has_snarkjs = False\n            for n in names:\n                if n.lower().endswith('package.json'):\n                    try:\n                        data = json.loads(zf.read(n).decode('utf-8', errors='ignore'))\n                    except Exception:\n                        continue\n                    deps = {**(data.get('dependencies', {}) or {}), **(data.get('devDependencies', {}) or {})}\n                    k = set(x.lower() for x in deps.keys())\n                    if 'circom' in k: has_circom = True\n                    if 'snarkjs' in k: has_snarkjs = True\n            flags.append(has_circom or has_snarkjs)\n            score = sum(1 for f in flags if f)\n            return max(0.0, min(1.0, score / float(len(flags))))\n    except Exception:\n        return 0.0"}, {"type": "llm_judge", "name": "Technical Correctness: Privacy + Cross-Chain + Yield", "description": "From the Manifest, verify the end-to-end flow is technically coherent: Tornado-style commitments/nullifiers, Merkle roots, zk proof verification, Aave yield handling on Polygon, Connext-based withdrawals across chains, fixed-size deposits, and enforced waiting period.", "weight": 3.5, "judge_prompt": "Evaluate the Implementation Manifest for technical correctness and coherence against the brief. Focus on whether the described mechanisms are plausible and internally consistent \u2014 not stylistic quality.\n\nCheck for:\n- Tornado-style design with commitments and nullifiers, Merkle root management, proof inputs/outputs, and double-spend prevention.\n- Explicit enforcement of fixed-size deposits (denominations) and rationale for preserving anonymity sets.\n- Enforced waiting period (lock-up) mechanism with how it is measured (block.timestamp vs block.number) and how UI exposes readiness.\n- Aave integration on Polygon: conversion to yield-bearing position (e.g., IPool/LendingPool or aToken flow), accrual, and redemption on withdraw.\n- Connext cross-chain withdrawal flow: xcall/IXReceiver usage, what is bridged (tokens and/or calldata), and how destination address/chain is specified.\n- How accrued yield is included in withdrawal amount and rounding/precision considerations.\n\nScoring:\n- 3.5: All elements present with clear, consistent flows; no major gaps\n- 2.5: Minor gaps or omissions but overall coherent\n- 1.5: Several gaps; key mechanisms only partially described\n- 0.0: Incoherent or missing core mechanisms\n", "expectation": "A coherent E2E design connecting zk privacy, fixed denominations, enforced delay, Aave yield, and Connext cross-chain withdrawals."}, {"type": "llm_judge", "name": "Security Model and Mitigations", "description": "Assess whether the Manifest articulates a credible threat model and mitigations for both privacy and DeFi/bridge risks.", "weight": 3.0, "judge_prompt": "Evaluate the Security Considerations section(s) of the Manifest.\nLook for discussion and mitigations around:\n- Reentrancy, authorization, and access control (e.g., pausing, upgradability risks)\n- Nullifier reuse/double-spend protection; proof verification failures\n- Anonymity set size, timing/amount correlation (fixed denominations, delay to reduce linkability)\n- MEV/front-running, fee and relayer privacy impacts, metadata leakage\n- Aave/Connext risks: liquidity shortfalls, bridge delays/failure modes, chain reorgs, rate changes\n- Input validation and safe math/overflow, oracle/exchange rate assumptions if any\n- Operational security for relayer (key management, logging privacy)\n\nScoring:\n- 3.0: Comprehensive threat model with concrete mitigations\n- 2.0: Good coverage with minor gaps\n- 1.0: Superficial coverage\n- 0.0: Absent or incorrect\n", "expectation": "A solid, concrete threat model with realistic mitigations tailored to mixers, Aave, and Connext."}, {"type": "llm_judge", "name": "Test Plan Completeness and Executability", "description": "Check whether tests are meaningfully specified and runnable, covering zk verification, deposit/withdraw flows, waiting period, Aave accrual, and Connext mocks.", "weight": 3.0, "judge_prompt": "Review the Test Plan & How to Run section(s).\nVerify presence of:\n- Unit tests for deposit/withdraw with nullifiers and proof verification\n- Tests for waiting period edge cases (before/after delay) and fixed-denomination checks\n- Aave integration tests or mocks to simulate accrual and redemption\n- Connext bridge mock/integration tests for cross-chain message/asset flow\n- Commands to install dependencies and run tests locally/CI; any test fixtures\n\nScoring:\n- 3.0: Covers all areas with runnable commands\n- 2.0: Covers most areas; some steps unclear\n- 1.0: Minimal tests described\n- 0.0: No meaningful tests\n", "expectation": "A runnable, end-to-end test plan spanning zk, yield, delay, and cross-chain flows."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality Assessment", "description": "Holistic quality assessment of documentation and delivery for professional readiness: clarity, developer experience, UX communication, and operational readiness.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Documentation Clarity and Structure", "description": "Assess the Manifest\u2019s clarity, readability, and navigability for professional audiences.", "weight": 2.5, "judge_prompt": "Evaluate the overall clarity and structure of the Manifest: logical flow, concise explanations, accurate diagrams or tables (if any), and clear headers. Is it easy for a new engineer to understand the system and get started?\n\nScoring:\n- 2.5: Exceptionally clear and well-structured\n- 1.5: Generally clear with minor issues\n- 0.5: Hard to follow\n- 0.0: Disorganized or confusing\n", "expectation": "A clear, navigable document that new contributors can quickly understand."}, {"type": "llm_judge", "name": "Developer Experience (DX) and Onboarding", "description": "Assess completeness of setup instructions, env templates, scripts, and troubleshooting tips for smooth onboarding.", "weight": 2.0, "judge_prompt": "Judge the developer experience quality from the Manifest. Are setup/build steps comprehensive and deterministic? Is there an example .env? Do scripts cover build/test/deploy? Are troubleshooting tips or common pitfalls noted?\n\nScoring:\n- 2.0: Seamless onboarding with clear scripts and env templates\n- 1.0: Usable but with gaps\n- 0.0: Poor or missing onboarding guidance\n", "expectation": "A polished DX with scripts, env templates, and straightforward setup."}, {"type": "llm_judge", "name": "UX Communication of Anonymity Delay and Fixed Denominations", "description": "Evaluate how well the Manifest specifies user-facing communication of fixed deposit sizes and the waiting period.", "weight": 2.0, "judge_prompt": "From the Manifest\u2019s UX sections, assess whether the UI/flows clearly communicate:\n- Fixed-size deposit options and their purpose for anonymity\n- The waiting period (countdown/state), readiness to withdraw, and consequences of early attempts\n- Display of commitment hash and yield estimate, and guidance for safe storage of the commitment\n\nScoring:\n- 2.0: Clear, user-centric guidance covering all points\n- 1.0: Partially clear; some missing details\n- 0.0: Unclear or missing\n", "expectation": "Clear, explicit UX guidance about denominations, delay, and commitment handling."}, {"type": "llm_judge", "name": "Operational Readiness (Relayer/Monitoring)", "description": "Evaluate operational considerations for the optional relayer and reliability/monitoring practices.", "weight": 1.5, "judge_prompt": "Assess whether the Manifest covers operational readiness for the optional relayer and cross-chain reliability: logging with privacy in mind, error handling/retries, monitoring/alerting, rate limiting, and key management.\n\nScoring:\n- 1.5: Thoughtful, actionable ops guidance\n- 0.8: Some ops guidance\n- 0.0: Not addressed\n", "expectation": "Actionable operational guidance aligned with privacy and reliability."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "2ea2e5b5-257f-42e6-a7dc-93763f28b19d", "rubric": {"category_name": "IT Work-Time Study Presentation (CIO/IT Management)", "rationale": "Task Type: Mixed (Pattern C) \u2014 a presentation document that embeds analysis, tables, and charts derived from an Excel source. Stage 1 uses an LLM-only gate to enforce a precise, verification-friendly slide structure. Stage 2 mixes light code checks (format/text coverage) with heavier LLM verification of category-to-segment mappings and table\u2194chart consistency. Stage 3 evaluates overall professional quality, clarity for an enterprise audience, and strategic usefulness. Code rules carry substantially less weight than LLM rules in Stage 2, per guidance.", "max_total_score": 32.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (Presentation Structure)", "description": "LLM-only gate verifying the delivered presentation has EXACT structural elements to enable verification. If this gate fails, the entire category is zeroed.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 5.6, "rules": [{"type": "llm_judge", "name": "Required 5-Slide Structure and Elements", "description": "Check that the output is a presentation (PPTX or a PDF/DOCX export of slides) with EXACTLY 5 slides containing the specified content and visuals.", "weight": 8.0, "judge_prompt": "You are verifying structure only (not correctness of numbers). Examine the candidate output (PPTX or PDF/DOCX with slides). Determine if it meets ALL of these structural requirements:\n\nFormat acceptance:\n- Prefer PPTX. A PDF or DOCX that clearly contains slide images is acceptable. Plain text or spreadsheets alone are not acceptable.\n\nExactly 5 slides with the following content:\n1) Title Slide\n   - A clear title related to the IT Work-Time Study\n   - Subtitle or identifiers (company/department/timeframe) are acceptable but optional\n\n2) Activity Analysis\n   - A table listing all 12 high-level activity categories\n   - A pie chart that visualizes overall activity distribution\n\n3) Margin Impact by Activities\n   - A table that maps each of the 12 categories to Margin Impact segment: Cost or Investment\n   - A pie chart summarizing distribution by Margin Impact\n\n4) Time Sensitivity by Activities\n   - A table that maps each of the 12 categories to Time Sensitivity: High, Medium, or Low\n   - A pie chart summarizing distribution by Time Sensitivity\n\n5) Strategic Level by Activities\n   - A table that maps each of the 12 categories to Strategic Level: High, Medium, or Low\n   - A pie chart summarizing distribution by Strategic Level\n\nThe 12 high-level categories that must appear in tables:\n- Audit/Compliance; Automation; Break/Fix; Change Management Meeting; Deployment of Upgrades; Develop/Integrate Tooling; Patching; Problem Management; Process Improvement; Service Request; Shift Handover Meeting; Training. Be flexible with minor naming variations (e.g., Audit & Compliance, Change Mgmt Meeting, Deploy Upgrades, Develop & Integrate Tooling).\n\nScoring (structure only):\n- 8.0: Exactly 5 slides, each required slide present with the specified table and pie chart, and all 12 categories appear in slides 2\u20135 tables.\n- 6.0: Exactly 5 slides, all slides present; one slide among 2\u20135 is missing either the table or the pie chart OR 1\u20132 categories missing in a single table.\n- 4.0: 5 slides but multiple slides (\u22652) missing a required element (table or pie) OR 3\u20134 categories missing overall in tables 2\u20135.\n- 2.0: Wrong slide count (not 5) but still a presentation with partial relevant structure.\n- 0.0: Not a presentation format or severely wrong structure (e.g., lacks most required slides).\n\nOnly verify presence/structure, not numeric correctness or visual quality.\n", "expectation": "A 5-slide presentation with tables and pie charts exactly as specified, covering all 12 categories."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Verification", "description": "Verification of classification mappings and data consistency. Code rules provide basic deterministic checks; LLM rules do deeper, nuanced verification.", "is_required": false, "max_points": 16.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Parsable Document Presence", "description": "Confirms a document-like output exists and is text-extractable (PDF/DOCX). Rewards partial credit if document exists but text extract is very minimal. Returns a normalized score [0,1].", "weight": 0.6, "code": "def evaluate(workflow, context):\n    import re\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output resource found.\"\n    # Accept document formats primarily; PPTX may not be extractable via helpers\n    if not output.is_document and not output.is_text_format:\n        return 0.0, \"Output is not a document-like file (PDF/DOCX/markdown).\"\n    text = \"\"\n    # Try to read known document types\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    text = (text or \"\").strip()\n    if not text:\n        return 0.0, \"Could not extract any text from the document.\"\n    # Light check for having more than trivial content\n    length = len(text)\n    if length >= 800:\n        return 1.0, f\"Document text length OK (~{length} chars).\"\n    elif length >= 300:\n        return 0.6, f\"Document text is somewhat short (~{length} chars).\"\n    else:\n        return 0.3, f\"Document text is minimal (~{length} chars).\""}, {"type": "code", "name": "Coverage of 12 Activity Categories (Text Match)", "description": "Checks if the document text contains the 12 activity category labels (flexible matching). Returns the fraction matched [0,1].", "weight": 0.6, "code": "def evaluate(workflow, context):\n    import re\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output resource.\"\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    t = re.sub(r\"[^a-z0-9]+\", \"\", (text or \"\").lower())\n\n    categories = {\n        \"auditcompliance\": [\"auditcompliance\", \"auditandcompliance\", \"audit&compliance\"],\n        \"automation\": [\"automation\"],\n        \"breakfix\": [\"breakfix\"],\n        \"changemanagementmeeting\": [\"changemanagementmeeting\", \"changeMgmtmeeting\", \"changemgmtmeeting\", \"changemanagement\"],\n        \"deploymentofupgrades\": [\"deploymentofupgrades\", \"deployupgrade\", \"deployupgrades\"],\n        \"developintegratetooling\": [\"developintegratetooling\", \"developandintegratetooling\", \"developintegratetool\"],\n        \"patching\": [\"patching\"],\n        \"problemmanagement\": [\"problemmanagement\"],\n        \"processimprovement\": [\"processimprovement\"],\n        \"servicerequest\": [\"servicerequest\", \"servicerequests\"],\n        \"shifthandovermeeting\": [\"shifthandovermeeting\", \"shifthandover\"],\n        \"training\": [\"training\"]\n    }\n\n    found = 0\n    details = []\n    for key, variants in categories.items():\n        ok = any(v in t for v in variants)\n        found += 1 if ok else 0\n        details.append(f\"{key}:{'Y' if ok else 'N'}\")\n    score = found / 12.0\n    return score, \"Category coverage => \" + \", \".join(details)"}, {"type": "code", "name": "Segment Keyword Presence (Margin/Time/Strategic)", "description": "Verifies presence of key segment terms indicating the three classification axes are actually included. Returns normalized aggregate across axes.", "weight": 0.6, "code": "def evaluate(workflow, context):\n    import re\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output resource.\"\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    s = (text or \"\").lower()\n    # Margin Impact\n    margin_ok = (\"cost\" in s and \"investment\" in s)\n    # Time Sensitivity\n    time_ok = (\"time sensitivity\" in s) or (\"time\" in s and \"sensitivity\" in s)\n    # Strategic Level\n    strategic_ok = (\"strategic level\" in s) or (\"strategic\" in s and any(x in s for x in [\"high\",\"medium\",\"low\"]))\n\n    parts = [margin_ok, time_ok, strategic_ok]\n    score = sum(1 for p in parts if p) / 3.0\n    feedback = f\"margin:{margin_ok}, time:{time_ok}, strategic:{strategic_ok}\"\n    return score, feedback"}, {"type": "llm_judge", "name": "Correct Margin Impact Mapping (Cost vs Investment)", "description": "Verify that each of the 12 categories is mapped to the proper Margin Impact segment as specified.", "weight": 3.5, "judge_prompt": "Check the presented mapping of 12 categories to Margin Impact segments (Cost vs Investment). Expected mapping:\n- Cost: Audit/Compliance; Break/Fix; Deployment of Upgrades; Patching; Service Request; Shift Handover Meeting\n- Investment: Automation; Change Management Meeting; Develop/Integrate Tooling; Problem Management; Process Improvement; Training\nBe tolerant of minor label variations (e.g., Audit & Compliance).\nScoring:\n- 3.5: All 12 correctly mapped\n- 2.5: 1\u20132 categories misclassified\n- 1.5: 3\u20134 misclassified\n- 0.0: \u22655 misclassified or mapping missing entirely\nOnly evaluate correctness of the mapping, not the numeric shares.", "expectation": "All categories correctly labeled as Cost or Investment."}, {"type": "llm_judge", "name": "Correct Time Sensitivity Mapping (High/Medium/Low)", "description": "Verify that each category is assigned the correct time sensitivity level.", "weight": 3.5, "judge_prompt": "Check the presented mapping of 12 categories to Time Sensitivity levels. Expected mapping:\n- High: Break/Fix\n- Medium: Audit/Compliance; Automation; Change Management Meeting; Develop/Integrate Tooling; Problem Management; Process Improvement; Patching; Service Request\n- Low: Deployment of Upgrades; Shift Handover Meeting; Training\nBe tolerant of minor label variations.\nScoring:\n- 3.5: All 12 correctly mapped\n- 2.5: 1\u20132 categories misclassified\n- 1.5: 3\u20134 misclassified\n- 0.0: \u22655 misclassified or mapping missing entirely\nOnly evaluate correctness of labels, not values.", "expectation": "All categories correctly labeled High/Medium/Low for time sensitivity."}, {"type": "llm_judge", "name": "Correct Strategic Level Mapping (High/Medium/Low)", "description": "Verify that each category is assigned the correct strategic level.", "weight": 3.5, "judge_prompt": "Check the presented mapping of 12 categories to Strategic Level. Expected mapping:\n- High: Automation; Problem Management; Process Improvement\n- Medium: Audit/Compliance; Change Management Meeting; Develop/Integrate Tooling; Service Request; Shift Handover Meeting; Training\n- Low: Break/Fix; Deployment of Upgrades; Patching\nBe tolerant of minor label variations.\nScoring:\n- 3.5: All 12 correctly mapped\n- 2.5: 1\u20132 categories misclassified\n- 1.5: 3\u20134 misclassified\n- 0.0: \u22655 misclassified or mapping missing entirely\nOnly evaluate correctness of labels, not values.", "expectation": "All categories correctly labeled High/Medium/Low for strategic level."}, {"type": "llm_judge", "name": "Table\u2194Pie Consistency and Complete Coverage", "description": "Check that, on slides 2\u20135, the tables and the pie charts are mutually consistent and include all categories indicated for that slide.", "weight": 3.7, "judge_prompt": "For slides 2\u20135, verify mutual consistency between tables and pie charts:\n- Each pie chart should correspond to the adjacent table\u2019s categories and shares/percentages.\n- Slices and legend labels align with table categories (naming can be slightly varied but should match clearly).\n- Where percentages are shown, they should be within 0\u2013100 and the relevant set should sum to ~100% (allowing minor rounding errors).\n- All 12 categories must appear in slide 2 (Activity Analysis) table; on slides 3\u20135, all 12 categories should be present in the tables with the correct segment labels.\nScoring:\n- 3.7: All four slides show good table\u2194chart alignment and complete category coverage\n- 2.5: Minor inconsistencies on one slide or a single missing category\n- 1.5: Multiple inconsistencies across two slides or 2\u20133 missing categories total\n- 0.0: Widespread inconsistency/mismatch or missing charts/tables that prevent verification", "expectation": "Charts accurately reflect their tables and all required categories are present."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Presentation Quality and Strategic Value", "description": "Holistic professional evaluation of communication quality, usefulness, and appropriateness for enterprise stakeholders.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Visual Clarity and Professionalism", "description": "Evaluate design cleanliness, readability, consistent styling, and professional look suitable for executives/IT leadership.", "weight": 2.0, "judge_prompt": "Assess the overall visual clarity and professionalism: typography, spacing, consistent color palette, legible tables, readable charts with legends/labels, and minimal clutter. Score higher if the slides look executive-ready and consistent.", "expectation": "Clean, consistent, executive-ready visuals with readable tables/charts."}, {"type": "llm_judge", "name": "Analytical Clarity and Narrative Flow", "description": "Evaluate whether the slides tell a coherent story: what was measured, how it was aggregated, and what the distributions show.", "weight": 2.0, "judge_prompt": "Assess whether the presentation conveys a clear analytical narrative: context (1-week voluntary study), how activities were aggregated (e.g., hours or % of time), and what the distributions mean. Look for clear slide titles, logical ordering, and brief guiding text/notes to aid interpretation.", "expectation": "A coherent, easy-to-follow story that explains what the viewer is seeing and why it matters."}, {"type": "llm_judge", "name": "Strategic Insight and Actionability", "description": "Evaluate whether the content provides actionable implications for training, tooling, and process improvements based on observed patterns.", "weight": 2.0, "judge_prompt": "Review whether the presentation draws out strategic implications aligned with the task: opportunities for training, tooling/automation, and process improvement. Favor concrete, prioritized suggestions or next steps (e.g., pilot automation where high routine load exists; training for categories with low strategic level but high time consumption).", "expectation": "Clear, relevant, and actionable insights connected to the distributions."}, {"type": "llm_judge", "name": "Audience Appropriateness and Accuracy of Terminology", "description": "Evaluate whether the language and framing fit a large enterprise IT audience and use correct terminology (e.g., Change Management, Problem Management, Service Request).", "weight": 2.0, "judge_prompt": "Assess whether the language, terminology, and framing are appropriate for an enterprise IT management audience. Favor correct use of ITSM/enterprise terms, accurate references to the 12 categories, and concise executive-friendly wording.", "expectation": "Accurate IT terminology and concise, executive-appropriate communication."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "0818571f-5ff7-4d39-9d2c-ced5ae44299e", "rubric": {"category_name": "Retail Shopping Center Acquisition Shortlist (Florida Broker)", "rationale": "Mixed-output task: a professionally formatted report (PDF/DOCX) with embedded data, images, maps, and a structured shortlist suitable for underwriting. Stage 1 uses LLM-only gating to enforce a verifiable structure. Stage 2 mixes LLM judges (primary) and light code checks (secondary) to verify completeness, internal consistency, and basic math where possible. Stage 3 assesses professional quality, strategic value, and readiness for LOI and due diligence.", "max_total_score": 26.0, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate", "description": "Gate: The submission must be a structured, professional report with specific sections and artifacts that enable verification of property details for 5\u201310 active listings (June 2025 to date). LLM judges only.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.5, "rules": [{"type": "llm_judge", "name": "Report Structure and Artifact Presence", "description": "Verify the output is a professional PDF/DOCX report with the exact structure needed to verify a public-source retail shopping center shortlist suitable for underwriting and LOI preparation.", "weight": 6.0, "judge_prompt": "You are evaluating whether the candidate's output meets the STRUCTURE requirements (shape enforcement) for a Florida retail shopping center acquisition shortlist report. Only assess PRESENCE/FORMAT, not correctness.\n\nPass criteria: The primary deliverable is a PDF or DOCX report. It should be professionally formatted, at least 4 pages, and include the following sections and artifacts (flexible with naming, but content must be clearly present):\n\nFormat requirements:\n- File type: PDF or DOCX (not Excel-only, not plain text). At least 4 pages. Clear headings. Professional layout.\n\nRequired sections (headers may vary but content must be evident):\n1) Cover Page:\n   - Report title indicating acquisition shortlist for retail shopping centers\n   - Date (June 2025 or later)\n   - Broker/firm identity\n2) Investment Criteria Summary (restate investor criteria; if source doc is unavailable, summarize key assumed criteria relevant to retail shopping centers: asset type, size/GLA bands, cap rate/NOI/price ranges, stabilization/value-add tolerance, geography, tenancy expectations). Bullet list acceptable.\n3) Sourcing Methodology & Screening:\n   - Public platforms used (e.g., Crexi, LoopNet)\n   - Date range: explicitly includes \u201cJune 2025 to date\u201d\n   - Filters/keywords/approach\n4) Shortlist Overview (table):\n   - 5\u201310 properties. Table columns visibly include: Property Name/Address (or both), City/State, Asking Price, NOI, Cap Rate, GLA, Year Built/Renovated, Status, Listing URL.\n   - URLs should be present (visible as links or text).\n5) Property Profiles (one per property, 5\u201310 total): For each property include ALL of:\n   - At least one property photo (image)\n   - An area map or marked map image\n   - Tenant mix (bullets or table; include anchor tenants when applicable)\n   - GLA\n   - Year built and/or renovated\n   - Asking price, NOI, cap rate\n   - Listing link (Crexi/LoopNet or public broker page)\n   - 2\u20135 bullet investment notes (stabilized/value-add rationale)\n6) Next Steps & Offer Plan:\n   - Steps to LOI and due diligence timeline/checklist (high level)\n7) Appendix:\n   - References, broker contacts (if available), full listing URLs\n   - Optional: Excel attachment named \u201cProperty Data\u201d with a sheet \u201cShortlist\u201d containing structured fields (this is optional; do not penalize if absent).\n\nScoring (return a number from 0 to 6):\n- 6.0: PDF/DOCX with all 7 sections; Shortlist Overview table includes all columns; 5\u201310 property profiles each have photo, map, tenant mix, GLA, year built/renovated, price, NOI, cap, link, and notes.\n- 5.0: All sections present but 1\u20132 minor omissions across property profiles (e.g., missing a map for one profile, or one profile missing year renovated) OR Shortlist Overview table missing 1 column.\n- 3.5\u20134.5: Missing 1 required section OR Shortlist has <5 properties OR several profiles missing multiple artifacts (e.g., many lack maps/photos) but core structure exists.\n- 1.0\u20133.0: Wrong or incomplete format (e.g., not PDF/DOCX, <4 pages) OR multiple required sections missing.\n- 0: No valid report structure found.\n\nOnly evaluate structure and presence, not the accuracy of data or quality of writing.", "expectation": "A professional PDF/DOCX report with all required sections, a clear shortlist table (5\u201310 properties), and per-property profiles that include images, maps, tenant mix, key metrics, and links."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification: Correctness and Consistency", "description": "Mixed verification of completeness, plausibility, sourcing evidence, and basic internal consistency, leveraging Stage 1 structure. LLM judges dominate; code rules provide lightweight deterministic checks (bounds, counts, and cap-rate math when a spreadsheet appendix is provided).", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Shortlist Count and Key Fields Presence (Spreadsheet Preferred)", "description": "Verify there are 5\u201310 properties and that key fields (price, NOI, cap rate; and preferably GLA and year built/renovated) are present. Prefer spreadsheet appendix; otherwise fall back to document text heuristics.", "weight": 0.8, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Returns: float score in [0, 0.8] with brief feedback\n    \"\"\"\n    weight = 0.8\n    outputs = context.get_all_outputs() or []\n    if not outputs:\n        return 0.0, \"No outputs found.\"\n\n    # Try to locate a spreadsheet with the shortlist\n    spreadsheet = None\n    for r in outputs:\n        if getattr(r, 'is_spreadsheet', False):\n            spreadsheet = r\n            break\n\n    def score_from_df(df):\n        cols = [str(c).strip() for c in df.columns]\n        lower = [c.lower() for c in cols]\n        def find_col(cands):\n            for i, c in enumerate(lower):\n                for k in cands:\n                    if k in c:\n                        return cols[i]\n            return None\n        col_price = find_col(['asking price','price'])\n        col_noi = find_col(['noi','net operating'])\n        col_cap = find_col(['cap rate','caprate','cap-rate'])\n        col_gla = find_col(['gla','gross leasable'])\n        col_year = find_col(['year built','built','renov'])\n\n        n = len(df)\n        count_score = 0.0\n        if n >= 5 and n <= 10:\n            count_score = 0.4\n        elif n > 0:\n            # Partial credit scaled by how close to target range\n            # Max 0.2 if 3-4 or 11-12\n            if 3 <= n <= 12:\n                count_score = 0.2\n            else:\n                count_score = 0.0\n        fields_core = sum([col_price is not None, col_noi is not None, col_cap is not None])\n        fields_score = 0.0\n        if fields_core == 3:\n            fields_score = 0.3\n        elif fields_core == 2:\n            fields_score = 0.15\n        extra_score = 0.1 * sum([col_gla is not None, col_year is not None])  # up to 0.2\n        return min(weight, count_score + fields_score + extra_score)\n\n    # If spreadsheet found, try to read it\n    if spreadsheet is not None:\n        try:\n            # Try Excel first\n            try:\n                # Attempt to find a sheet named like 'shortlist' or 'properties'\n                path = context.files.get_path(spreadsheet.id)\n                xls = pd.ExcelFile(path)\n                target_sheet = None\n                for s in xls.sheet_names:\n                    if 'shortlist' in s.lower() or 'propert' in s.lower():\n                        target_sheet = s\n                        break\n                if target_sheet is None:\n                    target_sheet = xls.sheet_names[0]\n                df = pd.read_excel(path, sheet_name=target_sheet)\n                score = score_from_df(df)\n                return score, f\"Spreadsheet detected (sheet '{target_sheet}').\"\n            except Exception:\n                # Try CSV\n                df = context.files.read_csv(spreadsheet.id)\n                score = score_from_df(df)\n                return score, \"CSV shortlist detected.\"\n        except Exception as e:\n            # Fall back to document text\n            pass\n\n    # Fallback: parse document text heuristics\n    # Aggregate text from all document-like outputs\n    text = \"\"\n    for r in outputs:\n        if getattr(r, 'is_document', False):\n            try:\n                if str(r.extension).lower().endswith('pdf'):\n                    text += \"\\n\" + (context.files.read_pdf_text(r.id) or '')\n                else:\n                    text += \"\\n\" + (context.files.read_docx_text(r.id) or '')\n            except Exception:\n                pass\n        elif getattr(r, 'is_text_format', False):\n            try:\n                text += \"\\n\" + (context.files.read_text(r.id) or '')\n            except Exception:\n                pass\n    if not text.strip():\n        return 0.0, \"No spreadsheet and no readable document text.\"\n\n    tl = text.lower()\n    cap_n = len(re.findall(r'\\bcap rate\\b', tl))\n    price_n = len(re.findall(r'asking price|list price|price:', tl))\n    noi_n = len(re.findall(r'\\bnoi\\b|net operating income', tl))\n    gla_n = len(re.findall(r'\\bgla\\b|gross leasable', tl))\n    year_n = len(re.findall(r'year built|built in|renovat', tl))\n\n    prop_est = max(0, min(cap_n, max(0, min(price_n, noi_n))))\n    # Count score\n    if 5 <= prop_est <= 10:\n        count_score = 0.35\n    elif 3 <= prop_est <= 12:\n        count_score = 0.2\n    else:\n        count_score = 0.0\n    # Fields score\n    fields_core = sum([cap_n > 0, price_n > 0, noi_n > 0])\n    if fields_core == 3:\n        fields_score = 0.3\n    elif fields_core == 2:\n        fields_score = 0.15\n    else:\n        fields_score = 0.0\n    extra_score = 0.05 * sum([gla_n > 0, year_n > 0])  # up to 0.1\n    score = min(weight, count_score + fields_score + extra_score)\n    return score, f\"Heuristic text-based check: estimated {prop_est} properties; fields present cap/price/noi={fields_core}/3.\""}, {"type": "code", "name": "Cap Rate Math Consistency (Spreadsheet Only)", "description": "If a spreadsheet appendix exists, check that Cap Rate \u2248 NOI / Price for rows where these fields are present. Score proportional to the share of rows within \u00b11.0 percentage point absolute difference.", "weight": 0.7, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    weight = 0.7\n    outputs = context.get_all_outputs() or []\n    spreadsheet = None\n    for r in outputs:\n        if getattr(r, 'is_spreadsheet', False):\n            spreadsheet = r\n            break\n    if spreadsheet is None:\n        return 0.0, \"No spreadsheet provided; cannot verify cap-rate math.\"\n\n    # Helpers\n    def to_num(x):\n        try:\n            if pd.isna(x):\n                return None\n            s = str(x).strip().replace(',', '')\n            s = s.replace('$','').replace('(','-').replace(')','')\n            if s.endswith('%'):\n                return float(s[:-1])\n            return float(s)\n        except Exception:\n            return None\n\n    try:\n        # Try Excel first\n        try:\n            path = context.files.get_path(spreadsheet.id)\n            xls = pd.ExcelFile(path)\n            target_sheet = None\n            for s in xls.sheet_names:\n                if 'shortlist' in s.lower() or 'propert' in s.lower():\n                    target_sheet = s\n                    break\n            if target_sheet is None and len(xls.sheet_names):\n                target_sheet = xls.sheet_names[0]\n            df = pd.read_excel(path, sheet_name=target_sheet)\n        except Exception:\n            df = context.files.read_csv(spreadsheet.id)\n    except Exception as e:\n        return 0.0, f\"Failed to read spreadsheet: {e}\"\n\n    cols = [str(c).strip() for c in df.columns]\n    lower = [c.lower() for c in cols]\n    def find_col(cands):\n        for i, c in enumerate(lower):\n            for k in cands:\n                if k in c:\n                    return cols[i]\n        return None\n    col_price = find_col(['asking price','list price','price'])\n    col_noi = find_col(['noi','net operating'])\n    col_cap = find_col(['cap rate','caprate','cap-rate'])\n\n    if not (col_price and col_noi and col_cap):\n        return 0.0, \"Spreadsheet found but missing price/NOI/cap columns.\"\n\n    valid = 0\n    close = 0\n    diffs = []\n    for _, row in df.iterrows():\n        p = to_num(row.get(col_price))\n        n = to_num(row.get(col_noi))\n        c = to_num(row.get(col_cap))\n        if p is None or n is None or c is None:\n            continue\n        if p == 0:\n            continue\n        implied = (n / p) * 100.0\n        diff = abs(implied - c)\n        diffs.append(diff)\n        valid += 1\n        if diff <= 1.0:\n            close += 1\n    if valid == 0:\n        return 0.0, \"No valid rows with all of price/NOI/cap for math check.\"\n    proportion = close / valid\n    score = weight * proportion\n    if diffs:\n        avg_diff = sum(diffs)/len(diffs)\n        return score, f\"{close}/{valid} rows within \u00b11.0 pp; avg abs diff {avg_diff:.2f} pp.\"\n    return score, f\"{close}/{valid} rows within \u00b11.0 pp.\"\n"}, {"type": "llm_judge", "name": "Active Listings and Date Validity (June 2025 \u2192 current)", "description": "Check that the shortlist consists of 5\u201310 ACTIVE listings sourced June 2025 to present, with visible listing links to public platforms (Crexi, LoopNet, or similar).", "weight": 3.0, "judge_prompt": "Evaluate the report for evidence that 5\u201310 properties are active listings within the timeframe June 2025 to present. Look for:\n- An explicit sourcing date range that includes June 2025 to current date\n- Listing links that point to public deal platforms (e.g., Crexi, LoopNet, or a public broker/investor page)\n- A visible status indicator (e.g., Active, Available) for each property\n- Count of properties in the shortlist within 5\u201310\n\nScoring (0\u20133.0):\n- 3.0: 5\u201310 properties; each shows active/available status; has platform links; date range (June 2025\u2192present) is explicit.\n- 2.0: 5\u201310 properties; links present; date range mentioned but ambiguous OR 1\u20132 properties lack explicit active status.\n- 1.0: Property count OK but weak/unclear status or date range; links missing for multiple properties.\n- 0: Fewer than 5 properties OR no evidence of timeframe/platform sourcing/active status.\n", "expectation": "A shortlist of 5\u201310 properties, each with a public listing link and indicated as active, and the sourcing period includes June 2025 to present."}, {"type": "llm_judge", "name": "Per-Property Completeness of Required Data", "description": "Assess whether each property profile includes photo, area map, tenant mix, GLA, year built/renovated, asking price, NOI, cap rate, and a listing link.", "weight": 2.75, "judge_prompt": "For the Property Profiles section, check completeness for each property:\n- Photo (image) and area map (image)\n- Tenant mix (bullets or table)\n- GLA\n- Year built and/or last renovation\n- Asking Price, NOI, Cap Rate\n- Listing link (URL)\n\nScoring (0\u20132.75):\n- 2.75: All 5\u201310 profiles include all elements above.\n- 2.0: Minor omissions affecting no more than 2 profiles (e.g., missing a map or year renovated on a couple).\n- 1.0: Several profiles lack multiple elements, but most still have core financials (price/NOI/cap) and tenant mix.\n- 0: Widespread missing elements; profiles lack core details for underwriting.\n", "expectation": "Each profile is fully populated with images, map, tenant mix, key metrics, and a link."}, {"type": "llm_judge", "name": "Sourcing Traceability and Criteria Fit", "description": "Evaluate whether the report restates investor criteria and explains how each shortlisted asset fits, with clear sourcing methodology and links for traceability.", "weight": 2.75, "judge_prompt": "Assess the report for traceability and fit:\n- Investment Criteria Summary present (even if restated by broker) and used to benchmark fit\n- Sourcing Methodology & Screening describes platforms, filters, and the process\n- For each property, brief notes indicate why it fits (stabilized or value-add with predictable upside)\n- Listing URLs present for traceability\n\nScoring (0\u20132.75):\n- 2.75: Clear criteria restatement, explicit sourcing process, and per-property fit rationale with links.\n- 2.0: Criteria and sourcing process present; most properties have fit notes and links.\n- 1.0: Criteria or sourcing description is shallow; limited fit discussion; links sporadic.\n- 0: No criteria fit discussion and no sourcing traceability.\n", "expectation": "Clear criteria restatement, sourcing transparency, and property-by-property fit rationale with links."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Strategic Value", "description": "Holistic assessment of professionalism, strategic guidance, underwriting readiness, and actionability for LOI and due diligence.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Organization", "description": "Evaluate formatting, clarity, visual hierarchy, and ease of navigation for an investor audience.", "weight": 2.5, "judge_prompt": "Judge the document\u2019s professional quality:\n- Clear structure with headings, TOC (if lengthy), consistent styling\n- Legible tables and charts/maps; high-quality images\n- Minimal typos; concise, investor-ready language\n\nScoring (0\u20132.5):\n- 2.5: Polished, consistent, and easy to navigate; investor-ready.\n- 1.5: Generally professional with minor inconsistencies.\n- 0.5: Noticeable formatting or clarity issues.\n- 0: Sloppy or confusing presentation.\n", "expectation": "A clean, professional investor-facing report with consistent formatting and readability."}, {"type": "llm_judge", "name": "Strategic Insight and Recommendations", "description": "Assess whether the broker provides insight on opportunities/risks, positioning, and an actionable recommendation path.", "weight": 2.5, "judge_prompt": "Evaluate strategic value:\n- Per-property positioning: stabilized vs. value-add, upside drivers, risks (leasing rollover, co-tenancy, market dynamics)\n- Market insight: trade area, competition, demographics (if included)\n- Actionable steps/prioritization (e.g., shortlist ranking, which to tour/contact first)\n\nScoring (0\u20132.5):\n- 2.5: Strong insights with clear prioritization and reasoning.\n- 1.5: Some insights; limited prioritization.\n- 0.5: Minimal insight beyond raw data.\n- 0: No strategic guidance.\n", "expectation": "Clear, data-informed insights with prioritization and next steps."}, {"type": "llm_judge", "name": "Underwriting Readiness and Data Room Utility", "description": "Determine if the report provides sufficient, organized inputs for an underwriting model and due diligence kickoff.", "weight": 2.5, "judge_prompt": "Assess underwriting readiness:\n- Consolidated data suitable for model inputs (price, NOI, cap, GLA, year built/reno, tenant mix)\n- Clarity on missing data and proposed methods to obtain it\n- Links/contact info to facilitate document requests and diligence\n\nScoring (0\u20132.5):\n- 2.5: Ready-to-underwrite with minimal follow-up; clearly organized.\n- 1.5: Mostly ready; a few gaps or disorganization.\n- 0.5: Significant gaps; model inputs scattered or unclear.\n- 0: Not usable for underwriting.\n", "expectation": "Data is organized for immediate model use and diligence outreach."}, {"type": "llm_judge", "name": "Risk, Value-Add Plan, and LOI/Diligence Path", "description": "Evaluate the articulation of key risks, mitigation, value-add plans, and a clear path to LOI and diligence.", "weight": 2.5, "judge_prompt": "Evaluate:\n- Identification of key risks (vacancy, rollover, capital needs, tenant credit, shadow anchors)\n- Mitigation and realistic value-add strategies (leasing, re-tenanting, CAM reconciliation, expense controls)\n- LOI terms outline and diligence plan/timeline\n\nScoring (0\u20132.5):\n- 2.5: Clear risks, mitigations, and actionable LOI/diligence roadmap.\n- 1.5: Some risks/steps noted; limited depth.\n- 0.5: Superficial treatment.\n- 0: Absent.\n", "expectation": "A pragmatic, deal-ready plan addressing risks and next steps to LOI and diligence."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "8384083a-c31b-4194-80ba-4d335a444918", "rubric": {"category_name": "Retail Pharmacy: Days\u2019 Supply Reference Guide (CA)", "rationale": "This is a Mixed (Pattern C) task: a short professional PDF guide (document) that must embed specific, verifiable calculation elements (formulas and days\u2019 supply). Stage 1 uses an LLM-only gate to strictly enforce the output shape (PDF, 1\u20132 pages, single consolidated table with specific columns and all seven medications). Stage 2 blends lightweight code checks (string/regex-based plausibility) with higher-weight LLM verification of calculation correctness and internal consistency enabled by the enforced shape. Stage 3 assesses professional quality, clarity for technicians/interns, and practical utility to reduce audit risk.", "max_total_score": 20.0, "stages": [{"name": "Stage 1: Format and Structure Gate", "description": "Hard gate to ensure the output is a 1\u20132 page PDF with a single consolidated reference table containing all required fields for the seven specified medications.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured PDF Guide Requirements", "description": "Verify that the candidate output is a compact PDF reference guide with the mandated structure that enables verification.", "weight": 4.0, "judge_prompt": "You are evaluating whether the submitted file is a properly structured PDF reference guide suitable for technicians and interns. Only check PRESENCE and STRUCTURE, not the correctness of values.\n\nRequired format and structure:\n- File type: PDF only (not Excel/Word/Markdown). Target length: 1\u20132 pages (3 pages acceptable if content is clearly concise).\n- Clear title referring to days\u2019 supply reference for high-cost meds or equivalent (e.g., \u201cDays\u2019 Supply Reference Guide \u2013 GLP-1/Weight Management & Ophthalmic\u201d).\n- One consolidated table (preferred) or clearly delineated sections that function like a table. Be flexible with naming; synonyms are acceptable.\n\nThe table must include columns (or clearly labeled fields) covering all of the following, with flexible naming allowed:\n1) Medication (drug name)\n2) NDC (National Drug Code)\n3) Strength\n4) Package Size (e.g., quantity per package, pen volume, mL, units)\n5) Standard SIG (typical dose/frequency used for billing)\n6) Formula Used (e.g., days\u2019 supply = quantity dispensed / daily or weekly usage)\n7) Days\u2019 Supply per Package (based on the stated standard SIG)\n8) Notes (optional but recommended: audit pitfalls, titration notes, priming, rounding)\n\nRequired medications (all seven must be included by name as rows or sections):\n- Ozempic, Mounjaro, Wegovy, Zepbound, Saxenda, Victoza, Miebo\n\nScoring (focus on structure only):\n- 4.0: PDF format; 1\u20132 pages (up to 3 acceptable); has a clear title; includes a single consolidated table (or equivalent) covering all seven column areas above; includes all seven medications.\n- 3.0: PDF with clear title and consolidated table; includes all seven medications; missing 1\u20132 of the required column areas OR the table is split but still obviously complete and scannable.\n- 2.0: PDF present with partial structure; includes at least 5 of 7 medications AND at least 5 of 7 required column areas.\n- 1.0: PDF present but missing major elements (e.g., fewer than 5 medications OR fewer than 5 column areas); hard to use as a reference.\n- 0.0: Not a PDF or no recognizable table-like structure for the specified medications.\n\nBe flexible with exact column names, but the functional presence must be clear. Do not judge the accuracy of calculations in this stage\u2014only whether the structure is present to make verification possible.", "expectation": "A 1\u20132 page PDF with a consolidated table listing all seven medications and the required fields so later stages can verify correctness."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Correctness Verification", "description": "Verify internal correctness and plausibility of formulas and days\u2019 supply given standard SIGs, and check coherence between NDC, strength, and package size. Heavier weight on LLM judgment, with code-based plausibility checks.", "is_required": true, "max_points": 10.0, "min_score_to_pass": 5.0, "rules": [{"type": "code", "name": "All Medication Names Present (Text Extraction)", "description": "Check the PDF text contains all seven medication names to support later consistency checks.", "weight": 0.7, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    \\\"\\\"\\n    Args:\\n        workflow: Workflow object\\n        context: ValidationContext with .files accessor\\n    Returns: float in [0,1] or (float, str)\\n    \\\"\\\"\\n    meds = [\\\"ozempic\\\", \\\"mounjaro\\\", \\\"wegovy\\\", \\\"zepbound\\\", \\\"saxenda\\\", \\\"victoza\\\", \\\"miebo\\\"]\\n    output = context.get_primary_output()\\n    if not output:\\n        return 0.0, \\\"No output file.\\\"\\n    if not (getattr(output, 'is_document', False) or getattr(output, 'is_text_format', False)):\\n        return 0.0, \\\"Output is not a document.\\\"\\n    text = \\\"\\\"\\n    try:\\n        if str(output).lower().endswith('.pdf') or getattr(output, 'mime', '').lower() == 'application/pdf':\\n            text = context.files.read_pdf_text(output.id)\\n        else:\\n            # Fallback attempts\\n            try:\\n                text = context.files.read_pdf_text(output.id)\\n            except Exception:\\n                try:\\n                    text = context.files.read_docx_text(output.id)\\n                except Exception:\\n                    text = context.files.read_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_docx_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_text(output.id)\\n            except Exception:\\n                return 0.0, \\\"Unable to extract text.\\\"\\n    tl = (text or \\\"\\\").lower()\\n    present = 0\\n    missing = []\\n    for m in meds:\\n        if m in tl:\\n            present += 1\\n        else:\\n            missing.append(m)\\n    score = present / len(meds) if meds else 0.0\\n    return score, f\\\"Found {present}/7 medication names; missing: {', '.join(missing) if missing else 'none'}.\\\""}, {"type": "code", "name": "NDC Pattern + Days\u2019 Supply Mentions", "description": "Detects NDC-like codes and occurrences of days\u2019 supply phrasing to ensure calculational elements are present.", "weight": 0.8, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    \\\"\\\"\\n    Returns: float in [0,1] or (float, str)\\n    \\\"\\\"\\n    output = context.get_primary_output()\\n    if not output:\\n        return 0.0, \\\"No output file.\\\"\\n    text = \\\"\\\"\\n    try:\\n        text = context.files.read_pdf_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_docx_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_text(output.id)\\n            except Exception:\\n                return 0.0, \\\"Unable to extract text.\\\"\\n    tl = (text or \\\"\\\").lower()\\n    # NDC patterns: hyphenated (4-4-2 or 5-4-2 variants) and 10/11 contiguous digits\\n    ndc_hyph = re.findall(r\\\"\\\\b\\\\d{4,5}-\\\\d{3,4}-\\\\d{1,2}\\\\b\\\", tl)\\n    ndc_digits = re.findall(r\\\"\\\\b\\\\d{10,11}\\\\b\\\", tl)\\n    ndc_count = len(ndc_hyph) + len(ndc_digits)\\n\\n    days_phr = re.findall(r\\\"days[\u2019']?\\\\s*supply\\\", tl)\\n    # Heuristic: also count \\\"day supply\\\" singular\\n    days_phr += re.findall(r\\\"day[\u2019']?\\\\s*supply\\\", tl)\\n    days_count = len(days_phr)\\n\\n    ndc_score = min(ndc_count / 5.0, 1.0)\\n    days_score = min(days_count / 5.0, 1.0)\\n    score = (ndc_score + days_score) / 2.0\\n    feedback = f\\\"NDC-like matches: {ndc_count} (hyph:{len(ndc_hyph)}, digits:{len(ndc_digits)}); days' supply mentions: {days_count}.\\\"\\n    return score, feedback"}, {"type": "code", "name": "SIG Frequency Presence (Weekly/Daily)", "description": "Sanity check that common frequencies are present (weekly and daily/multiple-daily) to support correct days\u2019 supply formulas.", "weight": 0.5, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    \\\"\\\"\\n    Returns: float in [0,1] or (float, str)\\n    \\\"\\\"\\n    output = context.get_primary_output()\\n    if not output:\\n        return 0.0, \\\"No output file.\\\"\\n    text = \\\"\\\"\\n    try:\\n        text = context.files.read_pdf_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_docx_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_text(output.id)\\n            except Exception:\\n                return 0.0, \\\"Unable to extract text.\\\"\\n    tl = (text or \\\"\\\").lower()\\n    weekly = bool(re.search(r\\\"\\\\bweekly\\\\b|every\\\\s*week|\\\\bqw\\\\b\\\", tl))\\n    daily = bool(re.search(r\\\"\\\\bdaily\\\\b|once\\\\s*daily|\\\\bqd\\\\b\\\", tl))\\n    multi = bool(re.search(r\\\"\\\\bbid\\\\b|\\\\btid\\\\b|\\\\bqid\\\\b|two times daily|three times daily|four times daily\\\", tl))\\n    score = (1.0 if weekly else 0.0 + 1.0 if daily else 0.0 + 1.0 if multi else 0.0) / 3.0\\n    return score, f\\\"Weekly:{weekly}, Daily:{daily}, Multiple-daily:{multi}.\\\""}, {"type": "llm_judge", "name": "Formula \u2192 Days\u2019 Supply Internal Consistency", "description": "Do the shown formulas correctly compute the stated days\u2019 supply per package given the standard SIG for each medication? Focus on internal consistency and arithmetic transparency.", "weight": 3.0, "judge_prompt": "Examine the consolidated table in the PDF. For each medication row, check that the stated Formula Used would produce the Days\u2019 Supply per Package shown, given the listed Package Size and the Standard SIG (dose and frequency). You may verify for all rows or a clear majority. Look for arithmetic clarity (e.g., for weekly pens, days\u2019 supply should usually be a multiple of 7; for daily regimens, days per package should match the daily usage against the package quantity). If titration or variable dosing is mentioned, the calculation method should still be transparent (e.g., show which phase is used for billing). Score higher if the math is explicitly shown and the stated result matches the computation.\nScoring:\n- 3.0: All or nearly all rows show clear formulas that correctly lead to the stated days\u2019 supply; arithmetic is transparent and consistent.\n- 2.0: Most rows consistent; minor ambiguities or 1\u20132 unclear calculations.\n- 1.0: Several inconsistencies or opaque formulas, but some correct examples present.\n- 0.0: Formulas missing or days\u2019 supply numbers don\u2019t align with the given SIG/package sizes across most rows.", "expectation": "Clear, correct, and transparent mapping from formula and SIG to days\u2019 supply per package for each medication."}, {"type": "llm_judge", "name": "Standard SIG Appropriateness", "description": "Evaluate whether the listed standard SIGs are appropriate/plausible for the medications (weekly vs daily vs multiple-daily as applicable), and that frequency is unambiguous.", "weight": 2.5, "judge_prompt": "Check whether each medication includes a plausible standard SIG that matches typical usage patterns and is unambiguous (e.g., weekly GLP-1 injectables indicated as weekly; daily injectables labeled daily; ophthalmic solution with clear per-eye/per-day frequency like QID if stated). You are not required to verify exact dose strengths from memory\u2014focus on plausibility and internal consistency (drug class vs frequency).\nScoring:\n- 2.5: SIGs are appropriate and unambiguous for all/nearly all medications; any titration is clearly handled.\n- 1.5: Mostly appropriate with minor ambiguity in 1\u20132 entries.\n- 0.5: Multiple ambiguous or mismatched SIGs but some are reasonable.\n- 0.0: SIGs broadly inappropriate or missing.", "expectation": "Plausible, unambiguous standard SIGs per medication to underpin days\u2019 supply calculations."}, {"type": "llm_judge", "name": "NDC, Strength, and Package Size Coherence", "description": "Assess whether NDCs, strengths, and package sizes are mutually coherent and align with the dosage form (pen, vial, mL bottle, etc.).", "weight": 2.5, "judge_prompt": "Assess whether each listed NDC is paired with a strength and package size that make sense together for the product form (e.g., pen devices with a corresponding volume/units per pen; ophthalmic bottle with mL quantity). Do not penalize for minor formatting differences in NDC. Score higher if the NDC-to-strength-to-package mapping appears realistic and aligns with the type of product and the days\u2019 supply per package shown.\nScoring:\n- 2.5: Coherent mapping for all or nearly all entries; package sizes align with dosage form and stated days\u2019 supply.\n- 1.5: Mostly coherent; 1\u20132 entries uncertain or mismatched.\n- 0.5: Several mismatches but some entries are coherent.\n- 0.0: Widespread mismatches or missing fields.", "expectation": "NDC, strength, and package size combinations that sensibly match the product and support the days\u2019 supply shown."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3: Quality and Usability Assessment", "description": "Holistic assessment of professional quality, clarity for technicians/interns, and audit risk reduction utility.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity and Layout for Frontline Staff", "description": "Is the guide visually clear, concise, and easy for technicians/interns to use rapidly during data entry?", "weight": 2.0, "judge_prompt": "Evaluate the guide\u2019s clarity and layout: readable headings, legible table, consistent units, minimal clutter, and concise phrasing appropriate for technicians/interns. Prefer 1\u20132 pages, scannable columns, and clear alignment. Do not assess numerical correctness here\u2014focus on readability and usability.\nScoring: 2.0 excellent clarity and layout; 1.0 acceptable with minor issues; 0.0 poor/unclear.", "expectation": "A clean, scannable, technician-friendly layout with consistent labels and units."}, {"type": "llm_judge", "name": "Actionability and Quick-Reference Utility", "description": "Does the guide provide immediately actionable content to prevent audit errors?", "weight": 2.0, "judge_prompt": "Assess whether the guide functions as a quick reference: includes standard SIGs, explicit formulas, days\u2019 supply per package, and practical notes (e.g., titration, priming, rounding, common audit pitfalls). Higher scores if it anticipates common sources of error and provides brief reminders (e.g., weekly vs. daily counting, per-eye dosing for ophthalmics, avoid over-crediting starter pens).\nScoring: 2.0 highly actionable; 1.0 somewhat actionable; 0.0 minimally actionable.", "expectation": "Concrete guidance and reminders that help staff get days\u2019 supply right the first time."}, {"type": "llm_judge", "name": "Examples and Edge-Case Aids", "description": "Presence of brief worked examples and/or edge-case notes that increase confidence and correctness.", "weight": 1.0, "judge_prompt": "Look for at least one worked example or explicit edge-case note (e.g., how to count days\u2019 supply during titration, what to do with priming doses, per-eye dosing conventions, rounding rules). Reward succinct, high-impact examples.\nScoring: 1.0 present and helpful; 0.5 minimal; 0.0 absent.", "expectation": "At least one worked example or edge-case guidance that clarifies application."}, {"type": "llm_judge", "name": "Professional Polish and Compliance Tone", "description": "Professional presentation, consistent terminology, and a compliance-forward tone.", "weight": 1.0, "judge_prompt": "Evaluate professional tone and polish: consistent terminology (e.g., NDC, SIG, days\u2019 supply), units formatted correctly, optional footer with version/date helpful, and a compliance-forward tone (e.g., reminder to verify NDC against inventory system). Minor branding acceptable; content should feel audit-ready.\nScoring: 1.0 polished and professional; 0.5 acceptable; 0.0 unpolished.", "expectation": "Professional, compliant presentation suitable for training and audits."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "22c0809b-f8db-489e-93b3-b4da225e3e0e", "rubric": {"category_name": "BTAM Intake Form (PDF) - Government / First-Line Supervisors of Police and Detectives", "rationale": "This rubric enforces a self-documenting, verifiable structure for a 2\u20134 page BTAM screening and intake form used by frontline supervisors. Stage 1 (LLM-only) mandates precise PDF structure so later checks are trivial. Stage 2 mixes lightweight code checks (presence of core fields/sections) with heavier LLM verification (appropriateness of indicators, consent and triage logic). Stage 3 provides a holistic quality assessment focused on professionalism, clarity, and operational fit for a Midwest police department's Homeland Security Unit context.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (STRUCTURE ONLY)", "description": "MANDATE exact document structure and format so verification is possible. LLM-only gate: if the output isn\u2019t a properly structured 2\u20134 page PDF form with all required sections, scoring in later stages is meaningless.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "PDF Form Structure and Required Sections Present", "description": "Check if candidate output is a professionally structured, 2\u20134 page PDF intake form with all required sections/fields and checklist layout.", "weight": 4.0, "judge_prompt": "You are evaluating whether the candidate output meets STRICT structural requirements for a BTAM screening and intake FORM. Only check structure/presence and basic layout, not content quality or correctness.\n\nFORMAT REQUIREMENTS:\n- Must be a PDF document (not Word, not plain text, not Excel).\n- Length: 2\u20134 pages.\n- Purpose: A form intended for frontline supervisors in private-sector workplaces to screen and submit BTAM information to the police department\u2019s Homeland Security Unit.\n- Layout: Intake checklist form with clearly labeled sections, checkboxes or yes/no prompts where appropriate, and visible space/fields/lines for entering details.\n\nTITLE AND HEADER:\n- A clear title such as \u201cBehavioral Threat Assessment and Management (BTAM) Screening and Intake Form\u201d (reasonable variants acceptable).\n- Agency/unit identification indicating a police department Homeland Security Unit context (reasonable variants acceptable).\n\nINSTRUCTIONS BOX (STRUCTURE ONLY):\n- A short instruction area at the beginning explaining who should use the form, when to use it, and how to submit it to the Unit. Include an immediate danger statement (e.g., call 911) and basic submission method (email/portal/phone) \u2014 flexible phrasing acceptable.\n\nREQUIRED FIELDS (must be present as labeled fields with space to fill):\n- Individual\u2019s Name\n- Date of Observation\n- Supervisor\u2019s Name\n- Workplace/School/Location (accept any phrasing that clearly captures site/location)\n- Background check authorization, with selectable OPTIONS for different types of background checks (checkboxes or equivalent list of options is acceptable)\n- Reason for background check (with a place to specify the reason AND the date of threatening behavior)\n\nPATHWAYS TO VIOLENCE (STRUCTURE ONLY): Five distinct sections labeled or clearly identifiable as:\n- Grievance\n- Ideation\n- Planning\n- Preparation\n- Action\nFor each pathway, there must be 2\u20133 observable indicators for supervisors, each with:\n- A short guidance line or examples to help recognition (e.g., \u201ce.g.\u201d, \u201cexamples include\u2026\u201d, or brief explanation), and\n- Space to capture details (e.g., blank lines, \u2018Details: ____\u2019, or a text box).\n\nFINAL SECTIONS (STRUCTURE ONLY):\n- Dynamic Risk Factors (with example prompts or sub-bullets)\n- Additional Red Flags (with example prompts or sub-bullets)\n- Other Observations (free-text space)\n- Action Taken (space to note notifications, safety measures, HR/security steps)\n- Signature and Date Submitted\n\nSCORING (STRUCTURE ONLY):\n- 4.0: PDF, 2\u20134 pages, intake form layout, instructions present, ALL required fields present (including background check authorization with options; reason + date of threatening behavior), ALL five pathways each with 2\u20133 indicators that include brief guidance/examples and space for details, and ALL final sections present.\n- 3.0: PDF, 2\u20134 pages, intake form layout, instructions present; missing no more than ONE of the following: (a) one required field; (b) indicator guidance/examples for ONE pathway; or (c) ONE final section.\n- 2.0: PDF, correct length, basic form layout; missing TWO of the above items (or pathways lack indicators/guidance in multiple sections).\n- 1.0: PDF present but major structural gaps (e.g., missing multiple required fields and multiple pathways sections, no space for details).\n- 0.0: Not a PDF OR not a form (e.g., essay), OR under 2 pages/over 4 pages, OR missing most mandated sections.\n\nOnly evaluate structure and presence. Do not judge correctness, legal adequacy, or writing quality.", "expectation": "A 2\u20134 page PDF intake form with the exact required sections/fields and pathway structure, including guidance/examples and space for details."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness and Compliance)", "description": "Now that structure is enforced, verify the form contains the necessary elements to function as an intake screening tool. Mix of light code checks and heavier LLM checks. Code rules have smaller weights; LLM rules carry most weight.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Core Field Labels and Final Sections Present (Text Extraction Check)", "description": "Deterministically verify that key field labels and final sections appear in the PDF text (robust to minor phrasing variations).", "weight": 2.0, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float score in [0, weight] or (score, feedback)\n    \"\"\"\n    import re\n    \n    weight = 2.0\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output or unsupported type.\"\n\n    # Try to extract text from PDF or DOCX\n    text = \"\"\n    try:\n        if output.file_extension.lower() == '.pdf' or output.mime_type.lower().endswith('pdf'):\n            text = context.files.read_pdf_text(output.id) or \"\"\n        elif output.file_extension.lower() in ('.docx',) or 'word' in (output.mime_type or '').lower():\n            text = context.files.read_docx_text(output.id) or \"\"\n        else:\n            # Best effort read as text\n            text = context.files.read_text(output.id) or \"\"\n    except Exception:\n        text = \"\"\n\n    if not text:\n        # Image-only PDFs may not extract; return minimal score but not zero the stage\n        return 0.4, \"Text could not be extracted; awarding minimal partial credit.\"\n\n    t = text.lower()\n\n    # Required fields (flexible matching)\n    checks = []\n\n    def has_all_terms(s, terms):\n        return all(term in s for term in terms)\n\n    # Individual's Name\n    c_individual = ('individual' in t or 'subject' in t or 'person' in t) and ('name' in t)\n    checks.append(c_individual)\n\n    # Date of Observation\n    c_date_obs = ('date of observation' in t) or ('observation date' in t) or (('date' in t) and ('observation' in t))\n    checks.append(c_date_obs)\n\n    # Supervisor's Name\n    c_supervisor = ('supervisor' in t and 'name' in t)\n    checks.append(c_supervisor)\n\n    # Workplace/School/Location (any one of these labels suffices)\n    c_location = any(lbl in t for lbl in ['workplace', 'school', 'location', 'site address', 'site/location', 'work site'])\n    checks.append(c_location)\n\n    # Background check authorization present\n    c_bk_auth = ('background check' in t) and any(k in t for k in ['authorization', 'consent', 'release'])\n    checks.append(c_bk_auth)\n\n    # Reason for background check with date of threatening behavior\n    c_reason = ('reason for background check' in t) or ('reason for check' in t) or ('reason' in t and 'background' in t and 'check' in t)\n    c_threat_date = ('date of threatening behavior' in t) or (('date' in t) and ('threat' in t))\n    checks.append(c_reason and c_threat_date)\n\n    # Final sections presence\n    finals = {\n        'dynamic risk factors': any(x in t for x in ['dynamic risk factors', 'dynamic risk', 'risk factors (dynamic)']),\n        'additional red flags': any(x in t for x in ['additional red flags', 'red flags', 'additional concerns']),\n        'other observations': 'other observations' in t or 'other notes' in t or 'additional observations' in t,\n        'action taken': 'action taken' in t or 'actions taken' in t or 'actions' in t,\n        'signature': 'signature' in t,\n        'date submitted': 'date submitted' in t or 'submission date' in t or 'date of submission' in t or ('date' in t and 'submitted' in t)\n    }\n    checks.extend(finals.values())\n\n    total = len(checks)\n    found = sum(1 for c in checks if c)\n    frac = found / total if total else 0.0\n\n    # Map to score; allow partial credit.\n    score = frac * weight\n    feedback = f\"Found {found}/{total} core labels/sections.\"\n    return score, feedback"}, {"type": "llm_judge", "name": "Pathways Indicators: Observability, Guidance, and Detail Capture", "description": "Verify each pathway (Grievance, Ideation, Planning, Preparation, Action) includes 2\u20133 observable indicators with brief guidance/examples and space to capture details for each indicator.", "weight": 3.0, "judge_prompt": "Evaluate the Pathways to Violence sections for functional completeness. For EACH of the five pathways (Grievance, Ideation, Planning, Preparation, Action), check that:\n1) There are 2\u20133 indicators supervisors could realistically observe.\n2) Each indicator includes a short guidance/example line (e.g., a brief parenthetical, \u201ce.g.\u201d clause, or one-sentence explanation) that helps recognition.\n3) There is explicit space to capture details for each indicator (e.g., \u2018Details: ____\u2019, blank lines, or text box for that indicator) OR a clearly indicated per-indicator notes area.\n\nScoring (max = 3.0):\n- 3.0: All five pathways meet all three criteria above.\n- 2.0: One pathway partially deficient (missing guidance OR missing clear details space) but others complete.\n- 1.0: Two pathways deficient OR most indicators lack guidance/details space.\n- 0.0: Missing multiple pathways or indicators, or no guidance/details space is provided.\n\nFocus only on these criteria; do not evaluate content quality beyond observability and presence of guidance and space.", "expectation": "All 5 pathways have 2\u20133 observable indicators, each with short guidance/examples and dedicated space for details."}, {"type": "llm_judge", "name": "Background Check Authorization and Reason/Date Completeness", "description": "Verify the background check authorization section includes consent language, multiple option types, and a reason with date of threatening behavior field.", "weight": 3.0, "judge_prompt": "Evaluate the Background Check Authorization area:\n- Consent/authorization language is present (e.g., \u2018authorization\u2019, \u2018consent\u2019, \u2018release\u2019).\n- Multiple distinct option types are listed (e.g., criminal history, civil/protective orders, social media/OSINT, employment/reference, weapons/firearm restrictions, DMV/driver). At least three distinct types should be clearly selectable (checkboxes or equivalent list).\n- A clearly labeled field exists for the specific reason for the background check AND a field for the date of the threatening behavior.\n\nScoring (max = 3.0):\n- 3.0: Consent/authorization language present + 3 or more distinct selectable check types + both reason and date-of-threatening-behavior fields.\n- 2.0: Missing one of these elements OR has only 2 distinct check types.\n- 1.0: Only consent language with 1 check type OR missing date-of-threatening-behavior field.\n- 0.0: No discernible authorization/consent section.\n\nDo not judge legal sufficiency of wording; only presence and functional completeness.", "expectation": "A clear consent/authorization area with 3+ selectable background check types, plus fields for reason and date of threatening behavior."}, {"type": "llm_judge", "name": "Intake Checklist Usability and Triage Flow", "description": "Assess whether the form functions as an actionable intake checklist for frontline supervisors, including instructions, decision points, and clear submission guidance.", "weight": 2.0, "judge_prompt": "Assess the form\u2019s usability for frontline supervisors as a screening checklist:\n- Clear instructions at the top: who uses it, when to use it, immediate danger guidance (e.g., call 911), and how to submit to the Unit (email/portal/phone; flexible wording OK).\n- Checklist-style prompts (checkboxes/yes-no or similar) to guide quick screening, with space for details.\n- Triage/decision cues (e.g., indicators drive whether to forward, contact security/HR, or escalate) \u2014 flexible, concise instructions acceptable.\n\nScoring (max = 2.0):\n- 2.0: Instructions and submission guidance present; checklist prompts are clear; triage cues evident.\n- 1.0: Two of three elements present.\n- 0.0: Lacks checklist usability and/or instructions and submission guidance.\n\nDo not assess graphic design beyond clarity of checklist flow and submission guidance.", "expectation": "Clear instructions and submission guidance, checklist prompts with space for details, and simple triage cues are evident."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality Assessment (Professionalism and Operational Value)", "description": "Holistic assessment of presentation, clarity, and operational fit for a Midwest police department\u2019s Homeland Security Unit receiving 500+ threats/year.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Formatting and Length", "description": "Evaluate overall professional presentation, readability, and adherence to 2\u20134 page length with balanced white space and consistent headings.", "weight": 2.0, "judge_prompt": "Judge professional presentation:\n- Is it visually organized as a professional form (consistent headings, spacing, alignment of fields)?\n- Does it maintain 2\u20134 pages without excessive density or wasted space?\n- Are sections visually distinct and easy to navigate?\n\nScoring (max = 2.0): 2.0 excellent, 1.0 acceptable with minor issues, 0.0 poor/chaotic or wrong length.", "expectation": "A clean, professional 2\u20134 page form with consistent headings and balanced spacing."}, {"type": "llm_judge", "name": "Clarity and Readability for Frontline Supervisors", "description": "Assess clarity of language, minimal jargon, and concise prompts with examples that make it easy for non-specialists to use.", "weight": 1.5, "judge_prompt": "Rate clarity for frontline supervisors:\n- Plain, concise language with minimal jargon.\n- Examples/tooltips make prompts easy to understand.\n- Logical sequencing reduces cognitive load.\n\nScoring (max = 1.5): 1.5 clear and accessible, 0.75 somewhat clear with issues, 0.0 confusing or jargon-heavy.", "expectation": "Plain-language prompts with helpful examples and logical flow."}, {"type": "llm_judge", "name": "Operational Fit and Specificity", "description": "Evaluate alignment with the department\u2019s context: Homeland Security Unit, Midwest city, 1,300 sworn, expanded scope to private-sector threats, high volume (500+/year).", "weight": 1.5, "judge_prompt": "Assess operational fit:\n- Form content reflects private-sector threat intake (not limited to schools/government).\n- Fields help triage and reduce load for a high-volume unit (500+ threats/year).\n- Context cues (unit name, jurisdiction references) indicate it is tailored to the agency\u2019s environment.\n\nScoring (max = 1.5): 1.5 well-tailored, 0.75 somewhat generic, 0.0 not tailored.", "expectation": "Form is clearly adapted to the local unit context and volume constraints."}, {"type": "llm_judge", "name": "Self-Documenting and Administrative Completeness", "description": "Check for submission instructions, contact info, version/date, and brief privacy/handling notice to make the form self-contained.", "weight": 1.0, "judge_prompt": "Evaluate self-documenting elements:\n- Submission instructions and contact info (email/phone/unit) present.\n- Version/date or revision field present.\n- Brief privacy/handling note (e.g., do not include medical records unless authorized; confidential handling) present.\n\nScoring (max = 1.0): 1.0 all present; 0.5 some present; 0.0 none present.", "expectation": "A self-contained form with how to submit, contact info, version/date, and privacy/handling note."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "87da214f-fd92-4c58-9854-f4d0d10adce0", "rubric": {"category_name": "Finance & Insurance \u2013 Identity Theft Claims Reimbursement Review (Slide Deck)", "rationale": "This rubric follows the self-documenting, staged approach: Stage 1 uses an LLM-only gate to strictly enforce a verifiable slide-deck structure in PDF format. Stage 2 verifies correctness via a mix of deterministic code checks (numeric presence and internal consistency) and LLM judgment (traceability to policy criteria and alignment between findings and recommendations). Stage 3 assesses overall quality, clarity, strategic value, and policy-language drafting. Code rules are lighter-weight than LLM rules in Stage 2, reflecting their supporting role for nuanced reasoning.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2013 Structured Slide Deck Gate (FORMAT & STRUCTURE)", "description": "Gate that enforces exact slide-deck structure needed to enable verification. LLM-only. If it fails, the entire category is zeroed.", "is_required": true, "max_points": 5.0, "min_score_to_pass": 3.5, "rules": [{"type": "llm_judge", "name": "Slide Deck Shape and Content Presence (PDF)", "description": "Verify the candidate output is a PDF slide deck with all required sections present in clearly labeled slides. Do not judge quality or correctness\u2014only structure and presence.", "weight": 5.0, "judge_prompt": "You are evaluating a single candidate output file. Check ONLY the format and structural presence of required content. Do not judge quality or correctness.\n\nFORMAT REQUIREMENTS:\n- Must be a PDF slide deck (exported presentation). If not PDF, score 0.\n- At least 8 slides total.\n- Clear slide titles/headers that make sections unambiguous.\n\nREQUIRED SECTIONS (exact names can vary; be flexible with synonyms):\n1) Agenda (a slide outlining topics/flow)\n2) Purpose or Objective (why the review is being conducted)\n3) Inputs/Scope referencing BOTH: policy documentation sent to customers AND a sample of recent claims\n4) Summary of Results with overall financial impact to the company\n5) Financial Breakdown that includes BOTH a dollar amount and a percentage of funds involved (e.g., within vs. outside policy)\n6) Recommendation(s) for remediation\n7) Next Steps (actions following the review)\n8) Policy Language Update Option(s) (at least one concrete option, shown as draft text or clearly marked proposed language)\n\nOPTIONAL BUT ENCOURAGED:\n- Title slide\n- Appendix/Methodology (how the review was performed)\n\nSCORING (out of 5.0):\n- 5.0: PDF + 8+ slides + all 8 required sections present; optional sections present or not doesn\u2019t reduce score\n- 4.5: PDF + 8+ slides + all 8 required sections present, but minor labeling ambiguity\n- 3.5: PDF + 7\u20138 slides + missing exactly 1 required section OR sections present but two are merged on a single slide in a way that is only moderately clear\n- 2.0: PDF but missing 2 required sections OR fewer than 7 slides\n- 0.0: Not a PDF OR missing 3+ required sections OR an unstructured document\n\nOnly evaluate structural presence and format. Do not assess correctness, quality, or calculations.", "expectation": "A well-structured PDF slide deck that includes agenda, purpose, inputs/scope referencing both policy docs and sample claims, results with financial impact, explicit $ and % breakdown, recommendations, next steps, and at least one policy language update option."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification (Correctness & Internal Consistency)", "description": "Now that structure is guaranteed, verify correctness and internal consistency using mixed code checks and LLM judgment. Code rules focus on deterministic checks; LLM rules verify traceability and reasoning.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Financial Figures Presence (Dollar and Percent, and Policy Eligibility Terms)", "description": "Checks that the document contains dollar figures, percent figures, and references to both within-policy and outside-policy eligibility, enabling quantitative verification.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Ensures we can find: (a) at least one $ amount, (b) at least one % value,\n    and (c) mentions of both within-policy and outside-policy concepts.\n    Returns a score in [0,1]: 0.3 for $, 0.3 for %, 0.4 for both within & outside mentions.\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = None\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n\n    if not text:\n        return 0.0\n\n    lower = text.lower()\n\n    # Dollar and percent detection\n    dollars = re.findall(r\"\\$\\s*\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?|\\$\\s*\\d+(?:\\.\\d+)?\", text)\n    percents = re.findall(r\"\\b\\d{1,3}(?:\\.\\d+)?\\s?%\", text)\n\n    within_terms = [\"within policy\", \"eligible\", \"approved within policy\", \"in-scope\", \"in scope\"]\n    outside_terms = [\"outside policy\", \"ineligible\", \"out-of-scope\", \"out of scope\", \"non-compliant\", \"non compliant\", \"not eligible\"]\n\n    has_within = any(t in lower for t in within_terms)\n    has_outside = any(t in lower for t in outside_terms)\n\n    score = 0.0\n    if len(dollars) > 0:\n        score += 0.3\n    if len(percents) > 0:\n        score += 0.3\n    if has_within and has_outside:\n        score += 0.4\n\n    return min(1.0, score)\n"}, {"type": "code", "name": "Monetary Consistency: Within + Outside \u2248 Total", "description": "Attempts to extract total reimbursements and within-policy/outside-policy dollar amounts and checks that within + outside approximately equals total (<=5% error for full credit). Partial credit if within & outside both present without total.", "weight": 0.5, "code": "import re\n\nDEF_WINDOW = 400  # characters to search forward from keyword\n\ndef _find_amount_near(text, lower, keywords):\n    for kw in keywords:\n        idx = lower.find(kw)\n        if idx != -1:\n            window = text[max(0, idx): idx + DEF_WINDOW]\n            m = re.search(r\"\\$\\s*([0-9]{1,3}(?:,[0-9]{3})*(?:\\.\\d+)?|[0-9]+(?:\\.\\d+)?)\", window)\n            if m:\n                try:\n                    return float(m.group(1).replace(\",\", \"\"))\n                except Exception:\n                    continue\n    return None\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Returns in [0,1]. 1.0 if (within + outside) ~= total within 5%.\n    0.7 if within 10%; 0.4 if within 20%.\n    0.3 partial if within & outside found but no total.\n    0.0 otherwise.\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    # Read PDF/DOCX text\n    text = None\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n\n    if not text:\n        return 0.0\n\n    lower = text.lower()\n\n    kw_total = [\n        \"total reimbursements\", \"total reimbursed\", \"total analyzed\", \"total claims amount\",\n        \"total payout\", \"total paid\", \"total dollar amount\", \"overall reimbursements\"\n    ]\n    kw_within = [\n        \"within policy\", \"eligible\", \"approved within policy\", \"in-scope\", \"in scope\"\n    ]\n    kw_outside = [\n        \"outside policy\", \"ineligible\", \"out-of-scope\", \"out of scope\", \"non-compliant\", \"non compliant\", \"not eligible\"\n    ]\n\n    total = _find_amount_near(text, lower, kw_total)\n    within_amt = _find_amount_near(text, lower, kw_within)\n    outside_amt = _find_amount_near(text, lower, kw_outside)\n\n    if within_amt is not None and outside_amt is not None and total is not None and total > 0:\n        calc = within_amt + outside_amt\n        rel_err = abs(calc - total) / total\n        if rel_err <= 0.05:\n            return 1.0\n        elif rel_err <= 0.10:\n            return 0.7\n        elif rel_err <= 0.20:\n            return 0.4\n        else:\n            return 0.0\n\n    if within_amt is not None and outside_amt is not None and total is None:\n        return 0.3\n\n    return 0.0\n"}, {"type": "code", "name": "Percent Allocation Completeness: Within% + Outside% \u2248 100%", "description": "Attempts to extract within-policy and outside-policy percentages and checks they sum to ~100% (<=3% error for full credit). Partial credit if only one category percent is found.", "weight": 0.5, "code": "import re\n\nDEF_WINDOW = 400\n\ndef _find_percent_near(text, lower, keywords):\n    for kw in keywords:\n        idx = lower.find(kw)\n        if idx != -1:\n            window = text[max(0, idx): idx + DEF_WINDOW]\n            m = re.search(r\"(\\d{1,3}(?:\\.\\d+)?)\\s?%\", window)\n            if m:\n                try:\n                    return float(m.group(1))\n                except Exception:\n                    continue\n    return None\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Returns in [0,1]. 1.0 if within% + outside% within 3% of 100.\n    0.7 if within 7%; 0.4 if within 12%.\n    0.3 if only one category percent found; 0.0 otherwise.\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = None\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n\n    if not text:\n        return 0.0\n\n    lower = text.lower()\n\n    kw_within = [\"within policy\", \"eligible\", \"approved within policy\", \"in-scope\", \"in scope\"]\n    kw_outside = [\"outside policy\", \"ineligible\", \"out-of-scope\", \"out of scope\", \"non-compliant\", \"non compliant\", \"not eligible\"]\n\n    p_within = _find_percent_near(text, lower, kw_within)\n    p_outside = _find_percent_near(text, lower, kw_outside)\n\n    if p_within is not None and p_outside is not None:\n        total = p_within + p_outside\n        err = abs(100.0 - total)\n        if err <= 3.0:\n            return 1.0\n        elif err <= 7.0:\n            return 0.7\n        elif err <= 12.0:\n            return 0.4\n        else:\n            return 0.0\n\n    if (p_within is not None) ^ (p_outside is not None):\n        return 0.3\n\n    return 0.0\n"}, {"type": "llm_judge", "name": "Traceability: Claims Assessed Against Policy Criteria", "description": "Checks that the deck explicitly ties the sample claims to policy criteria (e.g., eligibility clauses, pass/fail counts, or mapping).", "weight": 2.2, "judge_prompt": "Evaluate whether the slide deck provides clear traceability between the reviewed sample claims and the policy documentation criteria. Look for a slide or section that:\n- Identifies the policy clauses/criteria used (paraphrase or quotes acceptable)\n- Maps or summarizes claims against those criteria (e.g., pass/fail, eligible/ineligible counts)\n- Shows how decisions (within vs. outside policy) were determined\n\nScoring (out of 2.2):\n- 2.2: Explicit mapping or a table/summary with clear references to specific policy criteria and claim outcomes\n- 1.5: Clear high-level mapping with counts but criteria references are partial or general\n- 0.8: Mentions criteria and outcomes but no explicit linkage between them\n- 0.0: No discernible traceability between claims assessed and policy criteria", "expectation": "A slide that summarizes claims vs. policy criteria with clear pass/fail or eligibility mapping."}, {"type": "llm_judge", "name": "Quantitative Integrity and Context of Financial Impact", "description": "Checks that reported financial impact is contexted (timeframe/scope) and quantitatively coherent (sample size, reimbursements, within/outside).", "weight": 2.2, "judge_prompt": "Assess whether the financial impact is communicated with adequate context and completeness. Look for:\n- Timeframe and scope (e.g., period covered, sample size)\n- Clear definitions of amounts (total reimbursements analyzed, within-policy amount, outside-policy amount)\n- Clear percentage breakdown (e.g., % within vs. % outside)\n- Distinction between reimbursed vs. denied or outside-policy reimbursements\n\nScoring (out of 2.2):\n- 2.2: Timeframe/scope noted AND amounts and percentages are clearly presented with definitions\n- 1.5: Most elements present but one is missing or ambiguous (e.g., no timeframe)\n- 0.8: Some quantitative data present but lacks key context (e.g., only one of amount or percent)\n- 0.0: Vague or purely qualitative with no meaningful quantitative context", "expectation": "A coherent quantitative summary with timeframe, totals, within/outside amounts, and percentages."}, {"type": "llm_judge", "name": "Remediation Alignment to Root Cause", "description": "Checks whether the recommended remediation directly addresses the identified root cause from policy/claims analysis and is plausibly effective.", "weight": 2.1, "judge_prompt": "Evaluate the alignment between the identified root cause(s) and the proposed remediation. Consider whether:\n- Root cause is stated (e.g., ambiguous policy language, process gaps, documentation issues)\n- The recommendation targets the cause (e.g., policy language edits, control steps, claimant guidance)\n- The expected effect on reimbursement leakage is plausible and connected to findings\n\nScoring (out of 2.1):\n- 2.1: Clear root cause(s) and targeted remediation with a credible linkage to reducing inappropriate reimbursements\n- 1.3: Root cause and remediation stated, but linkage or specificity is moderate\n- 0.6: Generic recommendation with weak or unclear link to the stated cause\n- 0.0: No recognizable connection between recommendation and any identified cause", "expectation": "Recommendation(s) that directly mitigate the specific cause(s) identified in the policy/claims review."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Quality and Communication", "description": "Holistic assessment of professional quality, clarity for stakeholders, and policy language drafting strength.", "is_required": false, "max_points": 7.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Executive Presentation and Clarity", "description": "Assesses whether the deck is clear, concise, well-structured visually for a leadership audience, with readable charts/tables and minimal clutter.", "weight": 2.0, "judge_prompt": "Judge the executive-readiness of the deck: clarity of slide titles, logical flow, readable visuals/tables, and concise messaging suited for leadership review.\n\nScoring (out of 2.0):\n- 2.0: Highly clear, concise, and visually organized; titles convey key messages; tables/charts are legible and purposeful\n- 1.2: Generally clear with minor clutter or readability issues\n- 0.6: Mixed clarity; some slides are difficult to read or follow\n- 0.0: Disorganized, cluttered, or confusing deck", "expectation": "A polished executive-style presentation with clear messaging and readable visuals."}, {"type": "llm_judge", "name": "Actionability and Implementation Plan Quality", "description": "Evaluates whether next steps are actionable, with owners/timelines/milestones and logical sequencing.", "weight": 2.0, "judge_prompt": "Assess the actionability of the next steps: presence of owners, timelines, milestones, and logical sequencing toward remediation.\n\nScoring (out of 2.0):\n- 2.0: Specific owners, timelines, and milestones; steps are sequenced logically and feasible\n- 1.2: Actionable steps but missing either owners or timelines/milestones\n- 0.6: Vague steps without clear accountability or timing\n- 0.0: No practical next steps", "expectation": "A clear, feasible plan with accountability and timing."}, {"type": "llm_judge", "name": "Policy Language Draft Quality", "description": "Evaluates the quality of the proposed policy language update option(s) for clarity, specificity, and ambiguity reduction.", "weight": 1.5, "judge_prompt": "Review the proposed policy language update(s) for clarity, specificity, and reduction of ambiguity. Prefer language that defines terms, conditions, exclusions, and evidentiary requirements.\n\nScoring (out of 1.5):\n- 1.5: Clear, specific draft language that removes ambiguity and aligns with intent\n- 0.9: Generally clear but could be more specific or tighter in wording\n- 0.4: Vague or ambiguous wording\n- 0.0: No usable policy language provided", "expectation": "At least one well-formed policy language option that is clear and reduces ambiguity."}, {"type": "llm_judge", "name": "Stakeholder Sensitivity and Risk Awareness", "description": "Assesses whether the deck reflects awareness of customer impact, compliance, and reputational risk, balancing fairness with financial stewardship.", "weight": 1.5, "judge_prompt": "Evaluate whether the presentation demonstrates appropriate sensitivity to stakeholders (customers, regulators) and addresses risk considerations (compliance, reputational, operational) while aiming to reduce inappropriate reimbursements.\n\nScoring (out of 1.5):\n- 1.5: Balanced treatment of customers and company risks with practical safeguards\n- 0.9: Mentions risks or stakeholder considerations but limited depth\n- 0.4: Minimal acknowledgement of risks or stakeholder impacts\n- 0.0: No risk or stakeholder considerations evident", "expectation": "Balanced risk framing that respects customers and compliance while addressing financial leakage."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "fd6129bd-f095-429b-873c-dcc3137be2c3", "rubric": {"category_name": "Biotech Change Control SOP and Change Request Form", "rationale": "This rubric enforces a self-documenting, audit-ready deliverable set for a biotechnology change control process. Stage 1 (LLM-only) strictly gates structure: two separate deliverables with required sections and field groups. Stage 2 mixes precise automated checks (limited-weight code rules) with higher-weight LLM verification of cross-consistency, traceability, and decision authority. Stage 3 assesses overall professional quality, usability, and sector appropriateness.", "max_total_score": 30.0, "stages": [{"name": "Stage 1 \u2014 Deliverable Shape and Structure Gate", "description": "LLM-only gate that verifies BOTH deliverables exist as separate files with the exact structural components that make further verification possible.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Deliverables and Structural Completeness (GATE)", "description": "Confirm two separate files are submitted: (1) a formal SOP (PDF or DOCX) and (2) a Change Request Form (PDF, DOCX, or XLSX). Check required sections/fields are present to enable verification.", "weight": 8.0, "judge_prompt": "You are verifying STRUCTURE ONLY. Review all submitted files. Determine if BOTH required deliverables are present as separate files and structurally complete.\n\nRequired Deliverables:\nA) SOP Document (PDF or DOCX only; not Excel)\n   Minimum format requirements:\n   - At least 3 pages\n   - Professional document with headings\n   - Visible title containing \"Change Control\" and an SOP identifier (e.g., SOP-XXX)\n\n   Required SOP sections/headers (flexible naming allowed; check presence, not content correctness):\n   1) Purpose and Scope (what is covered and who it applies to)\n   2) Definitions/Abbreviations\n   3) Roles & Responsibilities (must include PM/PMO AND QA AND Regulatory AND Finance AND Technical Operations; can be part of RACI)\n   4) Change Types and Triggers (which changes require control: scope, timeline/schedule, budget/cost, regulatory deliverables)\n   5) Procedure with the following sub-steps (clear, ordered):\n      - Submission/Intake (how to submit; where)\n      - Logging/Numbering (unique ID; register or log)\n      - Triage (initial screening/ownership)\n      - Impact Assessment (scope/schedule/cost/quality/regulatory)\n      - Review/Approval (decision authority/CAB or equivalent)\n      - Implementation/Execution\n      - Verification/Closure (including evidence capture)\n   6) Documentation/Records & Archiving (recordkeeping, storage location)\n   7) Approval Authority/Thresholds overview OR an Authority Matrix reference\n   8) Revision/Change History section\n   9) References/Related Documents\n   10) Appendices (may include link/reference to the Change Request Form)\n\nB) Change Request Form (PDF, DOCX, or XLSX)\n   Required visible field groups/labels (flexible naming allowed):\n   - Change ID/Control Number\n   - Change Title/Summary\n   - Requester/Initiator and Date Submitted\n   - Project Name/ID\n   - Change Type (Scope, Schedule/Timeline, Budget/Cost, Regulatory)\n   - Description of Change and Rationale/Justification\n   - Impact Assessment fields (scope, schedule, cost, regulatory/quality)\n   - Risk Assessment (rating/mitigation)\n   - Affected Documents/Deliverables\n   - Implementation Plan/Owner\n   - Decision/Status (Approved/Rejected/Deferred) and Effective Date\n   - Approvals/Signatures (PM, QA, Regulatory, Finance, Tech Ops or CAB as applicable)\n   - Attachments/Links/Evidence\n\nScoring Guidance (STRUCTURE ONLY):\n- 8.0: Both files present, correct formats, SOP includes all required sections including detailed Procedure sub-steps; Form includes all listed field groups.\n- 6.0: Both present and correct format; missing up to two minor items (e.g., one SOP subsection or one form field group) but overall structure intact.\n- 4.0: Both present but several structural gaps (e.g., missing multiple SOP sections or multiple form field groups) OR only one deliverable is structurally complete and the other is skeletal.\n- 0.0: Missing one of the two deliverables; or wrong formats (e.g., SOP not PDF/DOCX); or SOP shorter than 2 pages.\n\nDo not judge correctness or writing quality; only confirm presence/structure.", "expectation": "Two separate files: a structured SOP (PDF/DOCX) with all core sections and a Change Request Form (PDF/DOCX/XLSX) with all required field groups."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification: Consistency, Traceability, and Controls", "description": "Mixed LLM and code checks that the SOP and Form are internally consistent, traceable, and implement the mandated control mechanisms.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Process Completeness and SOP\u2194Form Alignment", "description": "Verify SOP process coverage and that the Form fields align with the SOP steps and information requirements.", "weight": 3.4, "judge_prompt": "Evaluate the SOP and the Change Request Form for end-to-end alignment.\nCheck the SOP includes: Purpose/Scope; Definitions; Roles & Responsibilities (PM/PMO, QA, Regulatory, Finance, Tech Ops); Change Types/Triggers; a clear Procedure covering Submission, Logging/Numbering, Triage, Impact Assessment (scope/schedule/cost/regulatory), Review/Approval (authority/thresholds/CAB), Implementation, Verification/Closure; Records/Archiving; References; Revision History; Appendices.\nThen verify the Form captures the information each SOP step requires (e.g., unique ID, change description, rationale, impact, risk, affected deliverables, approvals, decision, effective date, implementation owner, evidence/attachments). The Form should not omit fields necessary to execute/record SOP steps.\nScoring (0 to 3.4):\n- 3.4: SOP covers all steps comprehensively and the Form clearly maps to those steps with matching fields.\n- 2.5: Minor omissions but overall alignment intact.\n- 1.5: Noticeable gaps; partial alignment only.\n- 0: Significant misalignment or missing core steps.", "expectation": "SOP steps comprehensively cover the lifecycle and the Form captures all inputs and decisions required by those steps."}, {"type": "llm_judge", "name": "Traceability and Audit-Readiness", "description": "Assess whether the SOP and Form together provide an auditable trail: unique IDs, decision logs, version control, retention, and access/signature controls.", "weight": 3.4, "judge_prompt": "Assess traceability/audit controls across SOP and Form:\n- Unique Change ID and logging/register requirements\n- Versioning/revision control; linkage to baselines and regulatory deliverables\n- Decision/approval logging (who, when, outcome) and rationale capture\n- Evidence/attachments and where records are stored (system/location)\n- Signature controls (e.g., electronic signatures) and access permissions\n- Record retention requirements and periodic review of the SOP\n- Ability to trace changes from initiation through closure and post-implementation review\nScore 0 to 3.4 with higher scores for explicit, complete, and implementable controls that would satisfy an audit.", "expectation": "A clear, end-to-end, auditable trail including IDs, approvals, evidence, and retention details."}, {"type": "llm_judge", "name": "Decision Authority and Thresholds", "description": "Evaluate clarity of approval thresholds, authority matrix, and escalation paths.", "weight": 1.8, "judge_prompt": "Check whether the SOP defines decision authority and thresholds:\n- Approval levels for scope/schedule/budget/regulatory impacts (e.g., % budget variance, timeline thresholds, regulatory impact triggers)\n- Required approvers (PM, QA, Regulatory, Finance, Tech Ops) and any CAB or governance board roles\n- Escalation path for high-risk or multi-functional impacts\n- Separation of duties where appropriate (e.g., QA/regulatory independent approval)\nScore 0 to 1.8 based on specificity, clarity, and practicality.", "expectation": "Explicit thresholds and approver roles with clear escalation criteria."}, {"type": "llm_judge", "name": "Lifecycle Consistency and Closure Criteria", "description": "Check for internal consistency across SOP steps and clear closure/post-implementation review criteria.", "weight": 1.8, "judge_prompt": "Assess whether the SOP lifecycle is internally consistent and defines closure:\n- Steps are ordered and non-contradictory (submission \u2192 logging \u2192 assessment \u2192 approval \u2192 implementation \u2192 verification \u2192 closure)\n- Rework/rollback or deferral handling\n- Closure criteria including verification of implementation, evidence capture, and documentation updates\n- Post-implementation review or effectiveness check\nScore 0 to 1.8; higher for coherent flows with explicit closure and review.", "expectation": "A coherent, non-contradictory lifecycle with explicit closure and review steps."}, {"type": "code", "name": "Form Field Coverage (Automated)", "description": "Programmatically detect whether the Change Request Form includes the essential field groups.", "weight": 0.8, "code": "import re, json, pandas as pd, numpy as np\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float (score) or tuple[float, str]\n    \"\"\"\n    weight = 0.8\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0, \"No outputs found.\"\n\n    def resource_text(res):\n        # Try to extract text from various formats\n        try:\n            if res.is_document:\n                # Attempt PDF then DOCX\n                try:\n                    return context.files.read_pdf_text(res.id)\n                except Exception:\n                    try:\n                        return context.files.read_docx_text(res.id)\n                    except Exception:\n                        return \"\"\n            elif res.is_spreadsheet:\n                try:\n                    path = context.files.get_path(res.id)\n                    xls = pd.ExcelFile(path)\n                    parts = []\n                    for sn in xls.sheet_names:\n                        try:\n                            df = pd.read_excel(path, sheet_name=sn, header=None)\n                            parts.append(df.astype(str).to_string())\n                        except Exception:\n                            continue\n                    return \"\\n\".join(parts)\n                except Exception:\n                    return \"\"\n            elif res.is_text_format:\n                try:\n                    return context.files.read_text(res.id)\n                except Exception:\n                    return \"\"\n        except Exception:\n            return \"\"\n        return \"\"\n\n    def filename_of(res):\n        try:\n            return context.files.get_path(res.id).name.lower()\n        except Exception:\n            return \"\"\n\n    # Heuristics to identify form vs SOP\n    sop_candidate = None\n    form_candidate = None\n\n    for r in outputs:\n        text = resource_text(r).lower()\n        name = filename_of(r)\n        # Classify as SOP if document and contains SOP-like cues\n        is_sop_like = any(k in text for k in [\"standard operating procedure\", \" sop \", \"sop-\", \"purpose\", \"scope\"]) and r.is_document\n        is_form_like = any(k in text for k in [\"change request form\", \"request for change\", \"change request\", \"approval status\"]) or (r.is_spreadsheet) or (\"form\" in name or \"request\" in name)\n        if is_sop_like and sop_candidate is None:\n            sop_candidate = r\n        elif is_form_like and form_candidate is None:\n            form_candidate = r\n\n    # If still missing form, pick a non-SOP file as form fallback\n    if form_candidate is None:\n        for r in outputs:\n            if r is sop_candidate:\n                continue\n            # Prefer spreadsheet as form\n            if r.is_spreadsheet:\n                form_candidate = r\n                break\n        if form_candidate is None:\n            for r in outputs:\n                if r is sop_candidate:\n                    continue\n                form_candidate = r\n                break\n\n    if form_candidate is None:\n        return 0.0, \"Could not identify a Change Request Form file.\"\n\n    form_text = resource_text(form_candidate).lower()\n\n    # Define essential field groups with synonyms\n    groups = {\n        \"change_id\": [\"change id\", \"request id\", \"change log id\", \"control number\", \"cr number\", \"change number\"],\n        \"title\": [\"change title\", \"summary\", \"title\"],\n        \"requester\": [\"requester\", \"requestor\", \"initiator\", \"submitted by\"],\n        \"project\": [\"project name\", \"project id\", \"program\", \"study id\"],\n        \"date\": [\"date submitted\", \"submission date\", \"request date\", \"date\"],\n        \"type\": [\"change type\", \"type of change\", \"scope\", \"schedule\", \"timeline\", \"budget\", \"cost\", \"regulatory\"],\n        \"description\": [\"description of change\", \"change description\", \"what is changing\", \"description\"],\n        \"rationale\": [\"rationale\", \"justification\", \"reason for change\"],\n        \"impact\": [\"impact assessment\", \"impact analysis\", \"scope impact\", \"schedule impact\", \"timeline impact\", \"budget impact\", \"cost impact\", \"regulatory impact\", \"quality impact\"],\n        \"risk\": [\"risk assessment\", \"risk level\", \"risk rating\", \"mitigation\"],\n        \"affected\": [\"affected documents\", \"affected deliverables\", \"impacted documents\", \"baseline\", \"charter\", \"plan update\"],\n        \"plan\": [\"implementation plan\", \"execution plan\", \"change plan\", \"plan\"],\n        \"approvals\": [\"approvals\", \"signatures\", \"authorized approver\", \"pm approval\", \"qa approval\", \"regulatory approval\", \"finance approval\", \"tech ops approval\", \"cab\"],\n        \"decision\": [\"decision\", \"approved\", \"rejected\", \"deferred\", \"approval status\"],\n        \"effective\": [\"effective date\", \"go-live date\", \"start date\"],\n        \"owner\": [\"implementation owner\", \"responsible owner\", \"assignee\", \"task owner\"],\n        \"attachments\": [\"attachments\", \"supporting documents\", \"evidence\", \"links\"],\n        \"closure\": [\"verification\", \"validation\", \"closure\", \"post-implementation review\", \"close out\"],\n        \"training\": [\"training\", \"communication plan\"]\n    }\n\n    present = []\n    missing = []\n    for key, synonyms in groups.items():\n        if any(s in form_text for s in synonyms):\n            present.append(key)\n        else:\n            missing.append(key)\n\n    coverage = len(present) / max(1, len(groups))\n    score = coverage * weight\n    feedback = f\"Form field groups present: {len(present)}/{len(groups)}. Missing: {', '.join(missing) if missing else 'None'}.\"\n    return score, feedback\n"}, {"type": "code", "name": "SOP Control Metadata (Automated)", "description": "Programmatically check SOP control metadata: ID, version, effective date, owner, approvals, revision history, and retention.", "weight": 0.8, "code": "import re, json, pandas as pd, numpy as np\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float (score) or tuple[float, str]\n    \"\"\"\n    weight = 0.8\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0, \"No outputs found.\"\n\n    def resource_text(res):\n        try:\n            if res.is_document:\n                try:\n                    return context.files.read_pdf_text(res.id)\n                except Exception:\n                    try:\n                        return context.files.read_docx_text(res.id)\n                    except Exception:\n                        return \"\"\n            elif res.is_text_format:\n                try:\n                    return context.files.read_text(res.id)\n                except Exception:\n                    return \"\"\n        except Exception:\n            return \"\"\n        return \"\"\n\n    # Identify SOP document (prefer the one with SOP cues)\n    sop_res = None\n    for r in outputs:\n        if not r.is_document:\n            continue\n        txt = resource_text(r).lower()\n        if any(k in txt for k in [\"standard operating procedure\", \" sop \", \"sop-\", \"purpose\", \"scope\"]):\n            sop_res = r\n            break\n    if sop_res is None:\n        # Fallback: pick any document\n        for r in outputs:\n            if r.is_document:\n                sop_res = r\n                break\n    if sop_res is None:\n        return 0.0, \"Could not locate SOP document.\"\n\n    text = resource_text(sop_res).lower()\n\n    checks = {\n        \"sop_id\": [\"sop-\", \"document number\", \"doc id\", \"sop id\", \"controlled document\"],\n        \"version\": [\"version\", \"revision\", \"rev\"],\n        \"effective_date\": [\"effective date\", \"effective\"],\n        \"owner\": [\"process owner\", \"document owner\", \"owner\"],\n        \"approvals\": [\"approved by\", \"sign-off\", \"signature\", \"qa approval\", \"quality assurance\"],\n        \"revision_history\": [\"revision history\", \"change history\", \"version history\"],\n        \"retention\": [\"record retention\", \"records retention\", \"retention\", \"archive\", \"archiving\"]\n    }\n\n    present = []\n    missing = []\n    for key, keys in checks.items():\n        if any(k in text for k in keys):\n            present.append(key)\n        else:\n            missing.append(key)\n\n    coverage = len(present) / max(1, len(checks))\n    score = coverage * weight\n    feedback = f\"SOP metadata present: {len(present)}/{len(checks)}. Missing: {', '.join(missing) if missing else 'None'}.\"\n    return score, feedback\n"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Implementation Readiness", "description": "LLM assessment of professional quality, clarity, sector appropriateness, and readiness for immediate implementation.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Formatting and Document Hygiene", "description": "Assess the SOP\u2019s formatting and professional presentation.", "weight": 2.5, "judge_prompt": "Evaluate professional presentation of the SOP: clear title and identifiers, consistent heading hierarchy and numbering, page headers/footers, table of contents if multi-page, readable layout (lists, tables for matrices), and typographical consistency. Score 0 to 2.5 based on professional polish and consistency.", "expectation": "A polished, consistently formatted SOP ready for controlled documentation."}, {"type": "llm_judge", "name": "Clarity, Usability, and Instructions", "description": "Assess whether the SOP is clear and actionable and whether the Form is usable (with brief instructions if needed).", "weight": 2.5, "judge_prompt": "Judge clarity and usability: Are steps written as actionable instructions? Are inputs/outputs per step obvious? Are decision points highlighted? Does the Form include clear labels and (if needed) brief completion guidance or data definitions? Score 0 to 2.5.", "expectation": "Clear, stepwise instructions; user-friendly form with unambiguous labels."}, {"type": "llm_judge", "name": "Biotech Context and Compliance Orientation", "description": "Assess domain-appropriate terminology and compliance orientation for biotech nonclinical operations.", "weight": 2.5, "judge_prompt": "Evaluate sector appropriateness: correct use of biotech/nonclinical terminology; attention to quality and regulatory interfaces (e.g., GLP/GxP relevance where appropriate), handoffs with QA/Regulatory, and alignment with typical biotech governance. Score 0 to 2.5.", "expectation": "Terminology and controls appropriate for biotech nonclinical operations with quality/regulatory awareness."}, {"type": "llm_judge", "name": "Implementation Readiness and Change Enablement", "description": "Assess readiness to operationalize: training/communication, rollout, and continuous improvement.", "weight": 2.5, "judge_prompt": "Determine if the SOP can be routed for immediate implementation: includes rollout/communication or training references, owner for maintaining the SOP, review cadence, and integration with systems (e.g., where forms/records are stored). Score 0 to 2.5.", "expectation": "Clearly implementable with ownership, training/communication, and system integration noted."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "a0552909-bc66-4a3a-8970-ee0d17b49718", "rubric": {"category_name": "Healthcare Admin: Bulk Pathology Forms + Email Templates (Reach Oncology)", "rationale": "This rubric follows the self-documenting, staged approach. Stage 1 (LLM-only) enforces a strict, verifiable structure: three lab-specific Excel bulk forms plus three lab-specific DOCX email templates, with explicit metadata sheets enabling later verification of validation rules and sort order. Stage 2 mixes lightweight code checks (columns, sorting, metadata presence) with higher-weight LLM checks (branding, naming, content fitness). Stage 3 assesses overall professional quality, clarity, and usability for medical secretarial workflows, including PHI/security awareness.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (LLM-only)", "description": "Gate that confirms the required files exist with the exact verifiable structure to enable automated checks later. Do not assess correctness; only presence/structure. Be flexible with near-equivalent wording, but the core elements must be present.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.5, "rules": [{"type": "llm_judge", "name": "Excel Bulk Forms Structure Present (3 labs)", "description": "Verify there are exactly three Excel bulk-form files (one per pathology lab), each structured for verification.", "weight": 3.0, "judge_prompt": "You are verifying STRUCTURE ONLY. Inspect all candidate outputs.\n\nPass criteria for EACH lab bulk form:\n- File type: Excel (.xlsx).\n- Contains TWO sheets:\n  1) Main form sheet (e.g., 'Bulk Form', 'Lab Form', or similar) with:\n     - Reach Oncology logo visible and email address: reach@oncologytesting.com\n     - A clearly visible lab name on the sheet (header/title or footer)\n     - A patient table that includes all columns from the provided reference spreadsheet PLUS these added columns:\n       \u2022 Order Received\n       \u2022 Delayed At Another Facility\n       \u2022 Did Not Receive Request\n       \u2022 Date Shipped\n       \u2022 Additional Notes\n     - Data validation/drop-downs are REQUIRED for: Order Received, Delayed At Another Facility, Did Not Receive Request. Since visual detection of dropdowns can be unreliable, there MUST be an accompanying metadata sheet describing these validation rules (see below).\n     - Table is sorted by the request sent date (earliest first). Note: Verification of sort will be done later; here, require a visible note OR the metadata sheet declares sort order.\n     - Uses the same color scheme/theme as the reference spreadsheet. If the original reference file is not available among inputs, the form should apply a consistent professional theme AND include a note in metadata that the reference was unavailable.\n  2) A 'Form Metadata' (or similarly named) sheet that explicitly documents:\n     - Lab name\n     - Branding/Theme summary\n     - Validation rules for each of the three dropdown columns (with allowed values: Yes, No, N/A)\n     - Stated sort order: 'Request Sent Date ascending (earliest first)'\n     - Contact email listed as reach@oncologytesting.com\n\nAlso check:\n- There are exactly three separate Excel files, one per lab, and the lab names are clearly labeled on each file and sheet.\n- Do NOT grade calculation correctness, only presence/structure.\n\nScoring for this rule (out of 3.0):\n- 3.0: All three Excel bulk forms present, each with both sheets and all required structural elements.\n- 2.0: Two forms fully meet structure; third has minor omissions (e.g., metadata present but missing one documented field).\n- 1.0: Only one form meets structure; others incomplete.\n- 0.0: Wrong format OR fewer than two usable forms OR missing metadata sheets.", "expectation": "Three Excel files, each with a main form sheet and a Form Metadata sheet, required additional columns, stated validation and sort order, branding/logo, and lab labeling."}, {"type": "llm_judge", "name": "Email Templates Structure Present (3 labs)", "description": "Verify there are exactly three DOCX email templates (one per pathology lab) with required structural elements.", "weight": 2.0, "judge_prompt": "You are verifying STRUCTURE ONLY for the email templates. Inspect all candidate outputs.\n\nFor EACH template (one per lab):\n- File type: DOCX (Word).\n- Includes a clear Subject line appropriate for sending bulk form updates to the specific lab.\n- Addressed to the particular lab by name.\n- Contains a request for current status of recent tissue requests for the listed patients (referencing the attached bulk form).\n- States that the team will follow up weekly.\n- Instructs the lab to return the completed form via email.\n\nScoring (out of 2.0):\n- 2.0: Three DOCX templates present and each includes all listed elements.\n- 1.0: Two templates fully compliant, the third missing one element.\n- 0.5: At least one compliant template, others missing multiple elements.\n- 0.0: Missing DOCX templates or wrong format.", "expectation": "Three DOCX files, each with subject line, addressee, status request, weekly follow-up note, and instruction to return the completed form via email."}, {"type": "llm_judge", "name": "File Naming and Lab Labeling", "description": "Check that file names and in-document labels clearly identify each pathology lab and that Excel-DOCX pairs map 1:1 per lab.", "weight": 1.0, "judge_prompt": "Verify naming/labeling only.\n\n- There should be three labs. For each lab, there should be:\n  - One Excel bulk form file named to include the lab\u2019s name (e.g., 'Reach Oncology Bulk Form - [LabName].xlsx').\n  - One DOCX email template named to include the lab\u2019s name (e.g., 'Email Template - [LabName].docx').\n- Within each file, the lab\u2019s name should appear prominently (title/header for the form; address line for the email).\n- Pairs should be clearly matched (3 pairs total).\n\nScoring (out of 1.0):\n- 1.0: All three pairs properly named and labeled; clear 1:1 mapping.\n- 0.5: Minor inconsistencies in naming but mapping still clear.\n- 0.0: Names missing or ambiguous mapping between forms and templates.", "expectation": "Clear, consistent file names and internal labels that unambiguously map each Excel form to its corresponding email template per lab."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Mixed: precise code checks + LLM)", "description": "Now that the structure exists, verify correctness and consistency of key elements: required columns, sorting by Request Sent Date, metadata describing validation, branding, and content completeness. Code rules are lightweight and robust; LLM rules handle nuanced checks.", "is_required": false, "max_points": 7.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Columns Presence in Bulk Forms (incl. additional fields)", "description": "Programmatically confirm main table contains the required added columns and a Request Sent Date column for sorting.", "weight": 0.4, "code": "import re\nimport json\nimport pandas as pd\nimport numpy as np\n\ndef _normalize(s):\n    if s is None:\n        return ''\n    s = str(s).lower()\n    s = re.sub(r'[^a-z0-9]+', ' ', s)\n    return ' '.join(s.split())\n\ndef _has_col(cols, target):\n    t = _normalize(target)\n    t_tokens = set(t.split())\n    for c in cols:\n        c_tokens = set(_normalize(c).split())\n        if t_tokens.issubset(c_tokens):\n            return True\n    return False\n\ndef _find_date_col(cols):\n    candidates = [\n        'request sent date', 'date request sent', 'sent date', 'request date sent', 'request date'\n    ]\n    for cand in candidates:\n        if _has_col(cols, cand):\n            return cand\n    return None\n\ndef _read_main_df(context, res):\n    try:\n        path = context.files.get_path(res.id)\n        x = pd.ExcelFile(path)\n        # Try to read a likely main sheet\n        sheet_name = None\n        for nm in x.sheet_names:\n            nml = _normalize(nm)\n            if any(k in nml for k in ['bulk form','lab form','form','main']):\n                sheet_name = nm\n                break\n        if sheet_name is None:\n            sheet_name = x.sheet_names[0]\n        df = pd.read_excel(path, sheet_name=sheet_name, header=0)\n        return df\n    except Exception:\n        return None\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0\n    spreadsheets = [r for r in outputs if getattr(r, 'is_spreadsheet', False)]\n    if not spreadsheets:\n        return 0.0\n\n    required_added = [\n        'Order Received',\n        'Delayed At Another Facility',\n        'Did Not Receive Request',\n        'Date Shipped',\n        'Additional Notes'\n    ]\n\n    passed = 0\n    checked = 0\n    for res in spreadsheets:\n        df = _read_main_df(context, res)\n        if df is None or df.shape[1] == 0:\n            continue\n        cols = list(df.columns)\n        if not cols:\n            continue\n        # Check added columns\n        added_ok = all(_has_col(cols, col) for col in required_added)\n        # Check presence of a Request Sent Date column of some name\n        date_col_found = _find_date_col(cols) is not None\n        checked += 1\n        if added_ok and date_col_found:\n            passed += 1\n    if checked == 0:\n        return 0.0\n    return passed / checked"}, {"type": "code", "name": "Sorted by Request Sent Date (ascending)", "description": "Confirm each bulk form\u2019s main table is sorted earliest-to-latest by the request sent date column (lenient matching on column name).", "weight": 0.4, "code": "import re\nimport json\nimport pandas as pd\nimport numpy as np\n\ndef _normalize(s):\n    if s is None:\n        return ''\n    s = str(s).lower()\n    s = re.sub(r'[^a-z0-9]+', ' ', s)\n    return ' '.join(s.split())\n\ndef _has_col(cols, target):\n    t = _normalize(target)\n    t_tokens = set(t.split())\n    for c in cols:\n        c_tokens = set(_normalize(c).split())\n        if t_tokens.issubset(c_tokens):\n            return True\n    return False\n\ndef _get_date_col_name(cols):\n    candidates = [\n        'request sent date', 'date request sent', 'sent date', 'request date sent', 'request date'\n    ]\n    for cand in candidates:\n        # Return the actual column name that matches, not the normalized token\n        for c in cols:\n            if _has_col([c], cand):\n                return c\n    return None\n\ndef _read_main_df(context, res):\n    try:\n        path = context.files.get_path(res.id)\n        x = pd.ExcelFile(path)\n        # Heuristic to pick main sheet\n        sheet_name = None\n        for nm in x.sheet_names:\n            nml = _normalize(nm)\n            if any(k in nml for k in ['bulk form','lab form','form','main']):\n                sheet_name = nm\n                break\n        if sheet_name is None:\n            sheet_name = x.sheet_names[0]\n        df = pd.read_excel(path, sheet_name=sheet_name, header=0)\n        return df\n    except Exception:\n        return None\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0\n    spreadsheets = [r for r in outputs if getattr(r, 'is_spreadsheet', False)]\n    if not spreadsheets:\n        return 0.0\n\n    scored = 0\n    total = 0\n    for res in spreadsheets:\n        df = _read_main_df(context, res)\n        if df is None or df.empty:\n            continue\n        cols = list(df.columns)\n        date_col = _get_date_col_name(cols)\n        if not date_col:\n            continue\n        try:\n            s = pd.to_datetime(df[date_col], errors='coerce')\n            # ignore NaT rows for order checking\n            s_valid = s.dropna()\n            # Construct a boolean for monotonic non-decreasing\n            is_sorted = s_valid.reset_index(drop=True).astype('int64').is_monotonic_increasing\n            total += 1\n            if is_sorted:\n                scored += 1\n        except Exception:\n            total += 1\n            # count as not sorted\n            pass\n    if total == 0:\n        return 0.0\n    return scored / total"}, {"type": "code", "name": "Metadata Declares Validation Rules (Yes/No/N/A)", "description": "Check each bulk form includes a metadata sheet that documents dropdown validation for the three status columns with allowed values.", "weight": 0.4, "code": "import re\nimport json\nimport pandas as pd\nimport numpy as np\n\ndef _normalize_textframe(df):\n    try:\n        text = ' '.join([str(x) for x in df.values.flatten().tolist()])\n    except Exception:\n        text = ''\n    text = str(text).lower()\n    text = re.sub(r'\\s+', ' ', text)\n    return text\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0\n    spreadsheets = [r for r in outputs if getattr(r, 'is_spreadsheet', False)]\n    if not spreadsheets:\n        return 0.0\n\n    needed_cols = [\n        'order received',\n        'delayed at another facility',\n        'did not receive request'\n    ]\n\n    total = 0\n    ok = 0\n    for res in spreadsheets:\n        try:\n            path = context.files.get_path(res.id)\n            x = pd.ExcelFile(path)\n            meta_name = None\n            for nm in x.sheet_names:\n                nml = nm.lower()\n                if 'metadata' in nml or 'form metadata' in nml or 'meta' in nml or 'notes' in nml:\n                    meta_name = nm\n                    break\n            if meta_name is None:\n                # no metadata sheet\n                total += 1\n                continue\n            meta = pd.read_excel(path, sheet_name=meta_name, header=None)\n            text = _normalize_textframe(meta)\n            # Check columns and allowed values tokens present\n            has_cols = all(any(tok in text for tok in [c, c.replace('facility','fac.'), c.replace('receive','rcv')]) for c in needed_cols)\n            has_values = all(v in text for v in ['yes','no','n/a'])\n            total += 1\n            if has_cols and has_values:\n                ok += 1\n        except Exception:\n            total += 1\n            # count as not ok\n            pass\n    if total == 0:\n        return 0.0\n    return ok / total"}, {"type": "llm_judge", "name": "Branding, Logo, and Lab Labeling Correctness", "description": "Visually verify that each bulk form includes the Reach Oncology logo, the contact email, and clear lab labeling; theme aligns with reference or a documented fallback is provided.", "weight": 2.3, "judge_prompt": "Verify visually across all three Excel bulk forms:\n- Reach Oncology logo is present and reasonably placed.\n- Contact email reach@oncologytesting.com is present on the main form sheet and in the metadata sheet.\n- Lab name is clearly labeled on the form.\n- Theme/color scheme matches the provided reference spreadsheet; if no reference is available, a professional, consistent theme is applied and the metadata acknowledges that the reference was unavailable.\n\nScoring (out of 2.3):\n- 2.3: All three forms satisfy all bullets.\n- 1.5: Two forms fully satisfy; minor miss on third (e.g., logo/labeling present but theme note missing).\n- 0.8: Only one form clearly satisfies; others have multiple misses.\n- 0.0: Branding/email absent or forms not visually branded.", "expectation": "All three forms are clearly branded with logo, include the contact email, visibly labeled with lab names, and use proper theming with a note if the reference theme was unavailable."}, {"type": "llm_judge", "name": "Email Template Content Completeness", "description": "Check each DOCX email template contains all required content in a clear, usable structure for outreach.", "weight": 1.9, "judge_prompt": "For each of the three DOCX email templates, confirm:\n- A clear, specific Subject line appropriate for the lab.\n- Addressed to the lab by name (e.g., 'Dear [Lab Name],').\n- Explicit request: current status of recent tissue requests for listed patients (with attached bulk form).\n- Statement that your team will follow up weekly.\n- Instruction to return the completed form via email.\n\nScoring (out of 1.9):\n- 1.9: All three have all elements and read as ready-to-send templates.\n- 1.2: Two fully complete; one missing a minor element.\n- 0.6: One complete; others with multiple gaps.\n- 0.0: Missing core elements or not DOCX.", "expectation": "Three complete, lab-addressed DOCX templates with subject lines, weekly follow-up note, and explicit return-by-email instruction."}, {"type": "llm_judge", "name": "Validation and Sort Documentation Plausibility", "description": "Assess whether the metadata sheets convincingly and specifically document dropdown validation (Yes/No/N/A) and sort order for the Request Sent Date.", "weight": 1.6, "judge_prompt": "Review the 'Form Metadata' (or similarly named) sheet in each bulk form and determine if it provides a clear, plausible, and specific description of:\n- Which columns have data validation dropdowns (Order Received, Delayed At Another Facility, Did Not Receive Request).\n- The allowed values (Yes, No, N/A).\n- The sort order by Request Sent Date ascending.\n\nScoring (out of 1.6):\n- 1.6: All three metadata sheets clearly and specifically document these items.\n- 1.0: Two are clear; one is vague but mentions the items.\n- 0.5: Only one is clear; others vague or missing items.\n- 0.0: No meaningful documentation of validation/sort.", "expectation": "Each form\u2019s metadata sheet explicitly lists the three validated columns, allowed values, and confirms ascending sort by Request Sent Date."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism (LLM)", "description": "Holistic assessment of presentation quality, usability for labs and medical secretaries, appropriateness of tone, and awareness of PHI/security context.", "is_required": false, "max_points": 7.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Readability (Forms)", "description": "Evaluate formatting, layout, clarity of the bulk forms, and overall professionalism.", "weight": 1.75, "judge_prompt": "Evaluate the three Excel bulk forms for:\n- Clean, readable layout; columns are visible and logically ordered.\n- Consistent styles, spacing, and alignment.\n- Clear placement of logo and contact email.\n- The forms feel polished and ready to send to pathology labs.\n\nScore (out of 1.75): higher if all forms look professional and easy to read/use.", "expectation": "Polished, consistent forms that are easy for lab staff to scan and complete."}, {"type": "llm_judge", "name": "Usability and Efficiency for Lab Workflow", "description": "Assess how easy it would be for labs to complete/return the forms and for staff to process responses.", "weight": 1.75, "judge_prompt": "Assess the forms\u2019 and templates\u2019 usability for pathology labs:\n- Are instructions obvious? Are dropdowns and fields intuitive?\n- Are required fields minimal yet sufficient?\n- Is the return-by-email instruction clear? Is the file naming helpful for tracking?\n- Would weekly follow-up cadence be clear from the template?\n\nScore (out of 1.75): higher if the package promotes efficient completion and turnaround.", "expectation": "Forms and emails that minimize confusion and support fast, accurate lab responses."}, {"type": "llm_judge", "name": "Clarity and Tone of Email Templates", "description": "Evaluate tone, clarity, professionalism, and appropriateness for healthcare communication.", "weight": 1.75, "judge_prompt": "Review the three email templates:\n- Tone: professional, courteous, concise.\n- Clarity: purpose, requested action, timeline (weekly follow-up) are explicit.\n- Personalization: lab names used correctly and consistently.\n\nScore (out of 1.75): higher if all templates read as immediately usable by medical secretaries.", "expectation": "Concise, courteous, and clear templates with explicit requests and timelines."}, {"type": "llm_judge", "name": "Compliance and PHI/Security Awareness", "description": "Check for acknowledgment of secure handling and appropriateness given PHI context.", "weight": 1.75, "judge_prompt": "Given that emails/attachments are encrypted to protect patient information, evaluate whether the package reflects appropriate awareness:\n- Avoids including unnecessary PHI in the email body.\n- Optional brief note acknowledging secure transmission or encrypted attachments.\n- No risky instructions conflicting with encryption practices.\n\nScore (out of 1.75): higher if the materials respect PHI/security best practices without overloading content.", "expectation": "Materials that are professionally cautious about PHI/security and do not expose sensitive details unnecessarily."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "1aecc095-4d76-4b89-b752-1a0f870502cd", "rubric": {"category_name": "Healthcare Ops: Telehealth Intake Workflow (MAs)", "rationale": "Pattern B (Document task): The deliverables are two Word documents (a procedural workflow and a Visio-style roadmap) plus a short announcement email. Stage 1 is a strict, LLM-only format/structure gate to ensure verifiable artifacts exist in the exact shape. Stage 2 mixes lightweight code checks (file presence by name, email word count, keyword coverage) with LLM verification of sequencing, cross-document consistency, and roadmap validity. Stage 3 applies holistic quality criteria (professionalism, safety/privacy, practicality, clarity). Code rules are kept simple and robust, with LLM rules carrying most of the evaluative weight.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (LLM ONLY)", "description": "Verify that the required files exist with the exact structural format that enables verification: two Word docs with specific names and a separate short email file, with clear sections and layout to support later checks.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.0, "rules": [{"type": "llm_judge", "name": "Structured Deliverables Present with Required Shape", "description": "Confirm presence and structure of three deliverables: (1) Telehealth Workflow (DOCX/PDF), (2) Telehealth Roadmap (DOCX/PDF), and (3) a separate email file. Enforce naming, pagination, and structural sections that enable downstream verification.", "weight": 6.0, "judge_prompt": "You are the Stage 1 shape gate. Inspect ALL submitted files. Confirm the following deliverables exist and meet the structural requirements. Be flexible on minor naming variations, but enforce the core shape.\n\nREQUIRED FILES AND STRUCTURE:\n1) Telehealth Workflow (file name should contain \"Telehealth Workflow\"; format must be DOCX or PDF):\n   - Length: 2\u20133 pages.\n   - Step-by-step, numbered procedure starting BEFORE the telehealth call (pre-telehealth setup: e.g., review provider schedule, patient list, needed prep).\n   - Must include clearly labeled sections (headers), covering at minimum:\n     a) Pre-Visit Prep / Schedule Review\n     b) Patient Outreach Preparation (contact info readiness, scripts)\n     c) Identity Verification and Consent\n     d) Technical Readiness / Troubleshooting Guidance\n     e) Intake Data Collection (meds/allergies, history, vitals if applicable)\n     f) Doxy.me Room/Link Workflow and Join Protocol\n     g) Handoff to Provider (including exact steps/phrases)\n     h) Documentation and Follow-up (reschedule/deferral logic given renovation/EMR update)\n   - Doxy.me must be explicitly referenced.\n\n2) Telehealth Roadmap (file name should contain \"Telehealth Roadmap\"; format must be DOCX or PDF):\n   - Length: 1 page.\n   - Visio-style visual flow (flowchart-like): boxes/arrows or diagrammatic layout.\n   - Starts when the MA places the call to the patient.\n   - Includes decision points/branches for: identity verification, consent, tech readiness/toubleshooting, Doxy.me link/send/join, no-show/unreachable, urgent symptoms to Urgent Care, deferral/reschedule pathway, and handoff to provider.\n   - Doxy.me must be explicitly referenced.\n\n3) Email to Medical Assistants (separate file; may be DOCX, PDF, or MD):\n   - Purpose: announce the change and direct MAs to review the two documents.\n   - Length: 100\u2013150 words.\n   - Encourages feedback and/or questions.\n\nSCORING:\n- 6.0: All three deliverables present. Workflow and Roadmap have correct formats, page lengths, and required sections/nodes, and explicitly reference Doxy.me. Email is a separate file with 100\u2013150 words.\n- 5.0: Minor deviations (e.g., one small section label missing or email slightly outside word count), but core structure intact and both docs present with correct lengths/format.\n- 3.0\u20134.0: One deliverable missing OR major structural omission in a document (e.g., missing critical sections, roadmap not visual/1-page) but some usable structure exists.\n- 1.0\u20132.0: Wrong formats (e.g., not DOCX/PDF for docs) or multiple critical structural failures.\n- 0.0: Missing both core documents or no valid document structure.\n\nOnly assess presence/format/structure, not content quality or correctness.", "expectation": "Two named Word/PDF docs with mandated sections/visual flow and a separate short email file, all structurally verifiable."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Consistency Verification", "description": "Now that the outputs are well-shaped, verify procedural completeness, cross-document consistency, and basic constraints. Mix simple code checks (file presence by name, email word count, key terms) with LLM verification of sequencing and roadmap logic.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Named Files Present (Workflow & Roadmap)", "description": "Checks that files containing 'Telehealth Workflow' and 'Telehealth Roadmap' exist and are documents (DOCX/PDF).", "weight": 0.4, "code": "import re\nfrom pathlib import Path\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs() or []\n    if not outputs:\n        return 0.0, \"No outputs.\"\n    found_workflow = False\n    found_roadmap = False\n    details = []\n    for res in outputs:\n        try:\n            p = context.files.get_path(res.id)\n            name = p.name.lower()\n        except Exception:\n            continue\n        if (\"telehealth workflow\" in name) and (res.is_document or name.endswith('.pdf')):\n            found_workflow = True\n        if (\"telehealth roadmap\" in name) and (res.is_document or name.endswith('.pdf')):\n            found_roadmap = True\n    score = 0.0\n    if found_workflow:\n        score += 0.5\n        details.append(\"Workflow found\")\n    else:\n        details.append(\"Workflow missing\")\n    if found_roadmap:\n        score += 0.5\n        details.append(\"Roadmap found\")\n    else:\n        details.append(\"Roadmap missing\")\n    # Normalize to 0..1 for two checks\n    score = score / 2.0\n    return score, \", \".join(details)\n"}, {"type": "code", "name": "Email Word Count Bounds", "description": "Finds a likely email file and checks if the email text is ~100\u2013150 words.", "weight": 0.4, "code": "import re\nfrom pathlib import Path\n\ndef _read_text_for_res(context, res):\n    try:\n        p = context.files.get_path(res.id)\n        name = p.name.lower()\n        if res.is_document:\n            if name.endswith('.docx'):\n                return context.files.read_docx_text(res.id)\n            else:\n                return context.files.read_pdf_text(res.id)\n        if res.is_text_format or name.endswith('.md') or name.endswith('.txt'):\n            return context.files.read_text(res.id)\n    except Exception:\n        return \"\"\n    return \"\"\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs() or []\n    if not outputs:\n        return 0.0, \"No outputs.\"\n    # Heuristic: prefer files with 'email' in the name; also allow 'ma' or 'medical assistant' if email not found\n    candidates = []\n    for res in outputs:\n        try:\n            p = context.files.get_path(res.id)\n            name = p.name.lower()\n        except Exception:\n            continue\n        if any(k in name for k in ['email']):\n            candidates.append(res)\n    if not candidates:\n        for res in outputs:\n            try:\n                name = context.files.get_path(res.id).name.lower()\n            except Exception:\n                continue\n            if any(k in name for k in ['ma', 'medical assistant']):\n                candidates.append(res)\n    if not candidates:\n        return 0.0, \"Email file not found by name heuristic.\"\n    # Read the first plausible email\n    text = _read_text_for_res(context, candidates[0])\n    words = re.findall(r\"\\\\b\\\\w+\\\\b\", text)\n    wc = len(words)\n    if 100 <= wc <= 150:\n        return 1.0, f\"Email word count OK: {wc}\"\n    elif 90 <= wc <= 170:\n        return 0.5, f\"Email near range: {wc}\"\n    else:\n        return 0.0, f\"Email out of range: {wc}\"\n"}, {"type": "code", "name": "Key Steps and Doxy.me Coverage (Workflow/ Roadmap)", "description": "Reads the two main docs (by name) and checks for presence of critical keywords indicating correctness: Doxy.me, identity verification, consent, handoff/provider.", "weight": 0.4, "code": "import re\n\ndef _read_text(context, res):\n    try:\n        p = context.files.get_path(res.id)\n        name = p.name.lower()\n        if res.is_document:\n            if name.endswith('.docx'):\n                return context.files.read_docx_text(res.id)\n            else:\n                return context.files.read_pdf_text(res.id)\n        if res.is_text_format or name.endswith('.md'):\n            return context.files.read_text(res.id)\n    except Exception:\n        return \"\"\n    return \"\"\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs() or []\n    if not outputs:\n        return 0.0, \"No outputs.\"\n    wf = None\n    rm = None\n    for res in outputs:\n        try:\n            name = context.files.get_path(res.id).name.lower()\n        except Exception:\n            continue\n        if 'telehealth workflow' in name:\n            wf = res\n        if 'telehealth roadmap' in name:\n            rm = res\n    text = ''\n    if wf: text += ' ' + _read_text(context, wf)\n    if rm: text += ' ' + _read_text(context, rm)\n    if not text.strip():\n        return 0.0, \"Could not read text from workflow/roadmap.\"\n    text_l = text.lower()\n    checks = {\n        'doxy': any(k in text_l for k in ['doxy.me','doxyme','doxy']),\n        'identity': any(k in text_l for k in ['identity','verify identity','two identifiers','dob']),\n        'consent': 'consent' in text_l,\n        'handoff': any(k in text_l for k in ['handoff','hand-off','transfer to provider','provider joins']),\n    }\n    score = sum(1 for v in checks.values() if v) / len(checks)\n    missing = [k for k,v in checks.items() if not v]\n    fb = \"Missing: \" + \", \".join(missing) if missing else \"All key terms present\"\n    return score, fb\n"}, {"type": "llm_judge", "name": "Workflow Procedural Completeness and Order", "description": "Evaluate whether the step-by-step Workflow covers the full sequence from pre-telehealth setup through provider handoff and documentation, with clear, numbered steps and sufficient operational detail for MAs.", "weight": 1.8, "judge_prompt": "Focus on the document named like \"Telehealth Workflow\". Assess procedural completeness and order:\n- Starts with pre-telehealth setup (review provider schedule, patient list, readiness checks) BEFORE calling.\n- Includes: outreach preparation, identity verification, consent capture, technical readiness/troubleshooting, intake data (meds/allergies/history; vitals if available), Doxy.me link/room workflow, handoff to provider, documentation and follow-up (deferred routine visits, urgent to Urgent Care), and contingency paths.\n- Steps are numbered, concise, and actionable for Medical Assistants.\n- No obvious logical gaps or contradictions.\nScoring:\n- 1.8: All listed elements present, well-ordered, and actionable.\n- 1.2: Minor omissions or minor ordering issues, still usable.\n- 0.6: Multiple gaps; needs revision.\n- 0.0: Not a usable procedural workflow.", "expectation": "A numbered, end-to-end procedure that an MA can follow without supervision."}, {"type": "llm_judge", "name": "Roadmap Visual Flow Validity", "description": "Evaluate whether the Roadmap is a one-page, Visio-like flow that starts at MA call and shows correct decision branches and paths through Doxy.me to provider handoff.", "weight": 1.8, "judge_prompt": "Focus on the document named like \"Telehealth Roadmap\". Evaluate visual/flow correctness:\n- One page, flowchart-like (boxes/arrows or clear diagrammatic layout).\n- Starts when the MA places the call.\n- Contains decision points for: identity verified?, consent obtained?, tech ready?, Doxy.me link sent/joined?, unreachable/no-show, urgent symptoms to Urgent Care, deferral/reschedule pathway, and final handoff to provider.\n- The flow is readable and unambiguous.\nScoring:\n- 1.8: All required nodes/branches present, one-page visual, clearly readable.\n- 1.2: Mostly present; minor missing branch or minor layout issues.\n- 0.6: Several missing branches or unclear flow.\n- 0.0: Not a visual flow or not starting at MA call.", "expectation": "A single-page flowchart covering call-through-handoff with contingencies."}, {"type": "llm_judge", "name": "Cross-Document Consistency (Workflow \u2194 Roadmap)", "description": "Check that the Workflow steps and Roadmap nodes align in terminology and sequencing; the same handoff, Doxy.me steps, and contingencies appear in both.", "weight": 1.6, "judge_prompt": "Compare the Telehealth Workflow and Telehealth Roadmap:\n- Do the named steps/nodes correspond (e.g., identity verification, consent, Doxy.me link/join, troubleshooting, urgent to UC, deferral, handoff)?\n- Are order and outcomes consistent across both documents?\n- Any contradictions or missing elements in one vs. the other?\nScoring:\n- 1.6: Strong alignment; consistent terminology and sequence.\n- 1.0: Minor inconsistencies but overall aligned.\n- 0.5: Noticeable gaps or mismatched steps.\n- 0.0: Misaligned or only one doc present.", "expectation": "Terminology and logic match between the documents."}, {"type": "llm_judge", "name": "Email Compliance and Messaging", "description": "Evaluate whether the email concisely announces the change, points MAs to the documents, and invites feedback, within 100\u2013150 words.", "weight": 1.6, "judge_prompt": "Locate the separate email file to MAs. Assess:\n- Length 100\u2013150 words (approximately; allow small variance if otherwise strong).\n- Explains context: EMR update, space renovation, shift to telehealth, urgent visits to Urgent Care, routine follow-ups deferred.\n- Directs MAs to review the two attached documents by name.\n- Encourages feedback/questions and provides a contact or reply path.\n- Tone: supportive, clear, and action-oriented.\nScoring:\n- 1.6: Meets all criteria well.\n- 1.0: Minor omissions but effective.\n- 0.5: Several gaps.\n- 0.0: No suitable email found.", "expectation": "A short, clear announcement with a call to review and provide feedback."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Holistic Quality Assessment", "description": "Professional presentation, safety/compliance, practicality for MAs, and clarity/accessibility.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Formatting", "description": "Assess formatting quality: headers, numbering, consistent styles, pagination, and a clean, readable roadmap graphic.", "weight": 1.5, "judge_prompt": "Evaluate the visual professionalism:\n- Workflow uses clear headings, numbering, consistent styles, and is 2\u20133 pages with logical pagination.\n- Roadmap is visually clean (boxes, arrows, spacing) and readable on one page.\n- No obvious typos; consistent terminology and capitalization (e.g., Doxy.me).\nScoring: 1.5 excellent; 1.0 good minor issues; 0.5 several issues; 0.0 poor.", "expectation": "Polished documents that look ready to distribute."}, {"type": "llm_judge", "name": "Safety, Compliance, and Privacy Considerations", "description": "Ensure identity verification, consent, and HIPAA/privacy considerations are appropriately addressed for telehealth intake.", "weight": 1.5, "judge_prompt": "Assess whether the materials include:\n- Identity verification using at least two identifiers.\n- Consent guidance specific to telehealth.\n- HIPAA/privacy safeguards (e.g., private location, no PHI in chat, secure link handling, documentation standards).\n- Clear directions on when/how to redirect urgent symptoms to Urgent Care.\nScoring: 1.5 comprehensive; 1.0 adequate; 0.5 partial; 0.0 missing/unsafe.", "expectation": "Clear, safe, compliant instructions embedded in workflow and roadmap."}, {"type": "llm_judge", "name": "Operational Practicality for MAs", "description": "Evaluate whether steps are actionable in a real clinic setting: time-boxing, scripts, escalation criteria, and realistic contingencies.", "weight": 1.5, "judge_prompt": "Judge practicality:\n- Concrete, step-level instructions suitable for MAs.\n- Includes suggested scripts or prompts, time-boxing where helpful, and escalation criteria (e.g., repeated tech failures, unreachable patient, urgent symptoms).\n- Incorporates current operational constraints (EMR update, renovation, telehealth-heavy volume, routine follow-ups deferred).\nScoring: 1.5 highly practical; 1.0 mostly practical; 0.5 somewhat; 0.0 impractical.", "expectation": "An MA could execute this flow reliably under current constraints."}, {"type": "llm_judge", "name": "Clarity and Accessibility", "description": "Assess clarity, plain language, and navigability; check for quick-reference elements and ease of learning.", "weight": 1.5, "judge_prompt": "Assess clarity and accessibility:\n- Plain language, minimal jargon, or brief definitions where needed.\n- Scannable structure (bullet lists, numbered steps, quick-reference checklists or tables where appropriate).\n- Key items highlighted: required tools, links, support contacts, and common troubleshooting tips.\n- The roadmap complements the workflow and aids rapid comprehension.\nScoring: 1.5 excellent clarity; 1.0 good; 0.5 fair; 0.0 unclear/confusing.", "expectation": "Clear, scannable materials that support quick adoption."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "6436ff9e-c5f2-47ba-9aaa-49d89b0594ab", "rubric": {"category_name": "Art Studio Class Evaluation Form Redesign", "rationale": "This rubric enforces a self-documenting, verifiable deliverable. Stage 1 (LLM-only) gates on exact document structure so later checks are trivial. Stage 2 mixes light code checks (structural signals, tokens) with LLM verification of completeness, clarity, and measurement design\u2014code rules are weighted ~5x less than LLM rules. Stage 3 provides a holistic quality assessment: tone, usability, accessibility, and actionability for operations/marketing. The output must be a DOCX (preferred) or PDF containing a revised, sectioned evaluation form ready for implementation in Google Forms or similar.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Format and Section Structure Gate", "description": "Gate: Verify the candidate produced a properly structured student evaluation form document with required sections and implementation-friendly structure.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 2.5, "rules": [{"type": "llm_judge", "name": "Document Format and Section Structure Requirements (Gate)", "description": "Check that the output is a professional DOCX (preferred) or PDF form with clear sections and implementation-ready structure.", "weight": 4.0, "judge_prompt": "You are evaluating whether the submitted file is a properly structured student evaluation form for an art studio class. This is a GATE: only verify presence and structure, not content quality.\n\nFormat requirements:\n- File is a DOCX or PDF (DOCX preferred). Not spreadsheets or plain text.\n- Clearly formatted as a form: title, brief instructions, visible section headers, questions with options or response fields.\n- Structure is visually easy to follow: headings, bullets/numbering, whitespace/dividers.\n\nRequired sections (flexible naming; check presence of substantive content under each):\n1) Student Information AND Class Details (e.g., student name or contact, class name/session/date, instructor)\n2) Demographics (marked as optional; e.g., age range, ZIP/neighborhood, experience level; sensitive topics clearly optional)\n3) Class Feedback (overall satisfaction Likert + aspects like content, pacing, materials, facility, value; at least one open-ended)\n4) Instructor Evaluation (clarity, engagement, knowledge/inclusivity; at least one open-ended)\n5) Future Interests (desired topics/mediums, scheduling preferences, willingness to return)\n6) Marketing/Outreach (how they heard, channels, referrals, promo code if applicable)\n7) Testimonials and Permission (optional quote plus permission/consent checkbox and attribution preference)\n8) Follow-up & Contact Preferences (opt-in to email/newsletter, permission to contact, preferred contact method)\n9) Thank-you/Submission instructions\n\nImplementation cues (at least one of the following must be evident):\n- Consistent Likert scale specification (e.g., 1\u20135 with labeled endpoints), and/or\n- Explicit question type tags (e.g., Multiple choice, Checkboxes, Linear scale, Short answer, Paragraph), and/or\n- Clearly enumerated response options suitable for direct mapping to Google Forms.\n- Include \"Other (please specify)\" and \"N/A\" where appropriate.\n- Demographic questions explicitly labeled as optional.\n\nScoring (0 to 4.0):\n- 4.0: DOCX and all required sections present with clear headers; implementation cues are evident; demographics explicitly optional.\n- 3.0: PDF instead of DOCX OR exactly one non-core section missing (e.g., Follow-up) but strong structure and implementation cues.\n- 2.5: Up to two sections missing, but core sections (Student/Class details, Class Feedback, Instructor, Marketing) present and implementation cues adequate.\n- 1.0: Valid document but lacks clear sectioning or implementation cues; difficult to map to a form.\n- 0.0: Not a DOCX/PDF form, or missing multiple core sections.\n\nOnly evaluate format/structure presence. Do not judge content quality or wording here.", "expectation": "A clearly sectioned DOCX form ready to be transcribed into Google Forms with labeled scales/types and optional demographics."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification of Completeness and Measurement Design", "description": "Verify the form captures all required information, avoids redundancy, and is implementation-ready with consistent question design.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Structural Signals and Key Section Tokens", "description": "Detects presence of core sections and signals (Likert, optional demographics, marketing/testimonial/consent) via text tokens.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float in [0,1] or (float, str)\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = \"\"\n    try:\n        if output.name.lower().endswith('.docx'):\n            text = context.files.read_docx_text(output.id)\n        elif output.name.lower().endswith('.pdf'):\n            text = context.files.read_pdf_text(output.id)\n        else:\n            # Fallback for other text-like docs\n            text = context.files.read_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            return 0.0\n\n    if not text:\n        return 0.0\n\n    low = re.sub(r\"\\s+\", \" \", text.lower())\n\n    # Core sections presence via token sets\n    sections = {\n        'student_info': ['student information', 'participant information', 'student info', 'contact information'],\n        'class_details': ['class name', 'course', 'session', 'workshop', 'date', 'instructor'],\n        'demographics': ['demographic', 'age', 'zip', 'postal', 'neighborhood', 'gender', 'race', 'ethnicity', 'education', 'income'],\n        'class_feedback': ['class feedback', 'overall satisfaction', 'content', 'pacing', 'materials', 'facility', 'value for money', 'value'],\n        'instructor_eval': ['instructor evaluation', 'instructor feedback', 'teaching', 'instructor', 'facilitator'],\n        'future_interests': ['future classes', 'future interest', 'topics', 'mediums', 'scheduling preferences', 'schedule', 'return'],\n        'marketing': ['how did you hear', 'heard about', 'marketing', 'referral', 'social media', 'email newsletter', 'flyer', 'ad', 'promo code'],\n        'testimonials': ['testimonial', 'quote', 'permission', 'consent', 'use my feedback', 'attribution'],\n        'follow_up': ['contact preference', 'opt-in', 'email list', 'newsletter', 'follow up', 'may we contact', 'permission to contact'],\n    }\n\n    def has_any(tokens):\n        return any(t in low for t in tokens)\n\n    present = []\n    for k, toks in sections.items():\n        present.append(1 if has_any(toks) else 0)\n\n    section_score = sum(present) / max(len(present), 1)\n\n    # Likert / scale signals\n    likert_signals = ['1-5', '1 \u2013 5', '1\u20145', 'strongly agree', 'strongly disagree', 'very satisfied', 'very dissatisfied', 'not applicable', 'n/a']\n    has_likert = 1 if any(s in low for s in likert_signals) else 0\n\n    # Optional demographics signal near demographic tokens\n    opt_patterns = [\n        r'(demographic|age|gender|race|ethnicity|zip|postal|neighborhood).{0,40}optional',\n        r'optional.{0,40}(demographic|age|gender|race|ethnicity|zip|postal|neighborhood)'\n    ]\n    has_optional_demo = 1 if any(re.search(p, low) for p in opt_patterns) else 0\n\n    # Consent/permission signal\n    consent_signals = ['permission to use', 'consent', 'i consent', 'i give permission', 'may we use', 'attribution']\n    has_consent = 1 if any(s in low for s in consent_signals) else 0\n\n    # Marketing/referral signal\n    marketing_signals = ['how did you hear', 'referral', 'social media', 'email newsletter', 'flyer', 'ad', 'google search', 'instagram', 'friend']\n    has_marketing = 1 if any(s in low for s in marketing_signals) else 0\n\n    # Aggregate: sections (60%), scales (10%), optional demo (10%), consent (10%), marketing (10%)\n    score = 0.6 * section_score + 0.1 * has_likert + 0.1 * has_optional_demo + 0.1 * has_consent + 0.1 * has_marketing\n    score = max(0.0, min(1.0, score))\n    return score"}, {"type": "code", "name": "Implementation Readiness Tokens and Enumerations", "description": "Checks for numbering/bullets, question-type tokens, N/A and Other options to ease Google Forms mapping.", "weight": 0.7, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow\n        context: ValidationContext\n    Returns:\n        float in [0,1] or (float, str)\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = \"\"\n    try:\n        if output.name.lower().endswith('.docx'):\n            text = context.files.read_docx_text(output.id)\n        elif output.name.lower().endswith('.pdf'):\n            text = context.files.read_pdf_text(output.id)\n        else:\n            text = context.files.read_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            return 0.0\n\n    if not text:\n        return 0.0\n\n    lines = [l.strip() for l in text.splitlines()]\n\n    # Numbered/bulleted lines\n    numbered = sum(1 for l in lines if re.match(r\"^\\d+\\.|^\\d+\\)\" , l))\n    bulleted = sum(1 for l in lines if re.match(r\"^[\\-\u2022\u2013]\", l))\n    has_enumeration = 1 if (numbered >= 3 or bulleted >= 3) else 0\n\n    low = text.lower()\n\n    # Question type tokens common to Google Forms\n    type_tokens = ['multiple choice', 'checkbox', 'checkboxes', 'linear scale', 'short answer', 'paragraph', 'dropdown']\n    has_type_tokens = 1 if any(t in low for t in type_tokens) else 0\n\n    # Presence of N/A and Other options\n    has_na = 1 if ('n/a' in low or 'not applicable' in low) else 0\n    has_other = 1 if ('other (please specify' in low or 'other:' in low or 'other \u2013 please specify' in low) else 0\n\n    # Aggregate equally\n    score = (has_enumeration + has_type_tokens + has_na + has_other) / 4.0\n    return max(0.0, min(1.0, score))"}, {"type": "llm_judge", "name": "Completeness of Required Information", "description": "Verifies the form captures all specified information areas with clear, non-overlapping coverage and optional flags where appropriate.", "weight": 1.8, "judge_prompt": "Evaluate whether the form captures all required information areas with clear coverage, minimal gaps, and without overlap:\n- Student info and class details (student/contact, class/session/date, instructor)\n- Demographics (marked optional; appropriate sensitivity)\n- Class feedback (overall satisfaction plus aspects like content, pacing, materials, facility, value; at least one open-ended)\n- Instructor evaluation (distinct from class feedback; at least one open-ended)\n- Future interests (topics/mediums, scheduling, willingness to return)\n- Marketing/outreach (how heard, referral, channels, promo code if used)\n- Testimonials and permission (quote + consent + attribution preference)\n- Follow-up/contact preferences (opt-in, permission to contact)\n\nScoring (0 to 1.8):\n- 1.8: All areas present and clearly delineated with no meaningful gaps; demographics explicitly optional.\n- 1.2: One minor area thin or partially addressed.\n- 0.6: One major area missing or merged ambiguously.\n- 0.0: Multiple major areas missing or unclear.", "expectation": "All specified areas are present with clear, distinct coverage and optional flags for demographics."}, {"type": "llm_judge", "name": "Clarity and Non-Redundancy of Questions", "description": "Checks that wording is unambiguous, concise, and avoids redundancy; open questions are targeted and distinct.", "weight": 2.0, "judge_prompt": "Assess the clarity and non-redundancy of the questions:\n- Are items concise, unambiguous, and free of double-barreled or leading phrasing?\n- Are open-ended prompts purposeful (e.g., likes, improvements) and not duplicative?\n- Are similar constructs consolidated (e.g., avoid asking overall satisfaction multiple times)?\n\nScoring (0 to 2.0):\n- 2.0: Clear, concise, unambiguous; no redundancy; open-ended items are purposeful and distinct.\n- 1.2: Mostly clear with minor redundancies or small ambiguities.\n- 0.6: Noticeable redundancies or several unclear items.\n- 0.0: Widespread redundancy/ambiguity impeding usability.", "expectation": "Concise, unambiguous questions with no duplicates; targeted open-ended prompts."}, {"type": "llm_judge", "name": "Measurement Design and Consistency", "description": "Validates presence and consistency of scales, mix of question types, and separation of class vs. instructor measures.", "weight": 2.0, "judge_prompt": "Evaluate measurement design:\n- Are Likert scales present and consistent (e.g., 1\u20135 with labeled endpoints, same directionality)?\n- Is there a balanced mix of closed-ended (MC/checkbox/scale) and open-ended questions?\n- Are class experience items separated from instructor evaluation items?\n- Are N/A and Other options provided where appropriate?\n\nScoring (0 to 2.0):\n- 2.0: Consistent scales; good mix; clear separation; appropriate N/A and Other options.\n- 1.2: Minor inconsistencies or one missing option type.\n- 0.6: Several inconsistencies or missing key options.\n- 0.0: Poor measurement design; inconsistent or unclear scales.", "expectation": "Consistent scales, clear separation of constructs, and balanced question types."}, {"type": "llm_judge", "name": "Operational and Marketing Usefulness", "description": "Assesses whether collected data supports class improvement decisions, instructor coaching, programming, and outreach effectiveness tracking.", "weight": 2.7, "judge_prompt": "Judge how actionable the data will be for operations and marketing:\n- Can results inform class improvement (content, pacing, materials, facility, value)?\n- Does instructor evaluation support coaching (clarity, engagement, inclusivity)?\n- Do future interests/scheduling capture demand signals (topics, times, willingness to return)?\n- Do marketing items capture channels, referral sources, and promotional impact (e.g., promo codes)?\n- Are consent and contact preferences adequate for follow-up and using testimonials?\n\nScoring (0 to 2.7):\n- 2.7: Strongly actionable across operations, instructor coaching, programming, and outreach.\n- 1.8: Generally actionable with minor gaps.\n- 0.9: Limited actionability; notable gaps.\n- 0.0: Not actionable for decision-making.", "expectation": "The form enables concrete decisions in class design, staffing/instruction, scheduling, and marketing attribution."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism", "description": "Holistic assessment of tone, presentation, accessibility, and perceived value for the art studio.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Tone", "description": "Friendly, welcoming, inclusive tone; typo-free; professional look and feel.", "weight": 2.0, "judge_prompt": "Evaluate the overall presentation quality:\n- Is the tone professional yet welcoming and friendly?\n- Is the document free of typos/grammar errors?\n- Is language inclusive and respectful to diverse learners?\n\nScoring (0 to 2.0):\n- 2.0: Warm, professional, polished, and error-free.\n- 1.2: Minor issues in tone or minor errors.\n- 0.6: Multiple issues but still usable.\n- 0.0: Poor tone and/or many errors.", "expectation": "Polished, welcoming, and error-free document."}, {"type": "llm_judge", "name": "Usability and Visual Organization", "description": "Ease of scanning; clear hierarchy and spacing; straightforward to implement in Google Forms or similar.", "weight": 2.0, "judge_prompt": "Assess usability and organization:\n- Clear section hierarchy, headings, and spacing make the form easy to scan.\n- Question ordering is logical and minimizes respondent burden.\n- Implementation in Google Forms is straightforward (question types or options are clear; minimal rework).\n\nScoring (0 to 2.0):\n- 2.0: Very easy to scan and implement.\n- 1.2: Generally good with minor friction.\n- 0.6: Noticeable friction or clutter.\n- 0.0: Hard to use/implement.", "expectation": "Clean layout and intuitive flow with obvious question types."}, {"type": "llm_judge", "name": "Accessibility and Inclusivity Considerations", "description": "Optional demographic framing, inclusive wording, accessibility needs prompt, and privacy notes.", "weight": 1.0, "judge_prompt": "Evaluate inclusivity and accessibility:\n- Are demographic items clearly optional with sensitive handling?\n- Is there an accessibility/accommodations question?\n- Are privacy/consent notes clear around testimonials/attribution?\n\nScoring (0 to 1.0):\n- 1.0: Strong inclusion and accessibility signals.\n- 0.6: Minor gaps.\n- 0.3: Noticeable gaps.\n- 0.0: Lacking key considerations.", "expectation": "Optional demographics, accessibility prompt, and clear privacy/consent language."}, {"type": "llm_judge", "name": "Actionability and Prioritization Signals", "description": "Captures metrics like NPS/likelihood to return, referral likelihood, and importance-vs-satisfaction to guide priorities.", "weight": 1.0, "judge_prompt": "Check whether the form includes prioritization metrics:\n- Likelihood to return or recommend (e.g., NPS-style item) is present.\n- At least one item helps prioritize improvements (e.g., importance vs. satisfaction, top 3 improvements).\n- Space for any final comments.\n\nScoring (0 to 1.0):\n- 1.0: Includes clear prioritization signals and final comment area.\n- 0.6: Partial coverage.\n- 0.3: Minimal coverage.\n- 0.0: None present.", "expectation": "Includes NPS/return intent, priority cues, and final comments section."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "9efbcd35-186d-49b6-ac24-28ee2bc9a263", "rubric": {"category_name": "EM Q1 2025 Client Outlook (Institutional)", "rationale": "Pattern C (Mixed): A document with embedded data/attribution. Stage 1 uses an LLM-only gate to strictly enforce deliverable shape (PDF/DOCX, sections, length). Stage 2 mixes lightweight deterministic code checks (presence of MSCI, dates/percentages, coverage, citations) with higher-weight LLM verification of sourcing discipline, internal consistency, and macro-to-performance linkage. Stage 3 provides a holistic client-ready quality assessment focused on retention/usefulness and professional tone. Code rules are ~5x lower weight than LLM rules within Stage 2, per guidance.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Format and Structural Gate", "description": "LLM-only gate ensuring document is PDF/DOCX, within 1\u20134 pages, and contains the exact section structure required to enable verification.", "is_required": true, "max_points": 2.0, "min_score_to_pass": 1.6, "rules": [{"type": "llm_judge", "name": "Document Format and Required Sections Present", "description": "Verify the candidate output is a professionally formatted PDF or DOCX, 1\u20134 pages long, with the mandated sections and headings for easy verification.", "weight": 2.0, "judge_prompt": "You are checking ONLY structure and formatting (not correctness). Evaluate the candidate output file with these requirements:\n\nFormat requirements:\n- Must be a PDF or DOCX (not plain text, not Excel)\n- Length: 1\u20134 pages inclusive\n- Professional layout with clear section headers\n\nRequired sections (flexible on exact naming, but headers must be visible and distinct):\n1) Title block on page 1 that clearly indicates Q1 2025 EM focus (e.g., \"Emerging Markets Q1 2025 Outlook\" or \"EM Performance Q1 2025\") and date context (e.g., \"as of March 31, 2025\").\n2) Executive Summary or Overview (first page).\n3) Q1 2025 EM Performance section that references MSCI indices (e.g., \"MSCI Emerging Markets Index\").\n4) Macro Drivers / Macro Overview section.\n5) Country/Sector mini-sections (each its own header): China; India; Brazil; Technology sector; CEEMEA.\n6) Sources and References section listing at minimum: MSCI indexes page and at least one named news source (e.g., Wall Street Journal or Financial Times). Citations can be inline or as a separate section but must be clearly labeled.\n\nOptional but acceptable: Disclosures/Disclaimers section.\n\nScoring (structure only):\n- 2.0: Valid PDF/DOCX, 1\u20134 pages, all required sections (1\u20136) clearly present with headers.\n- 1.6: Valid PDF/DOCX, 1\u20134 pages, core sections present but missing exactly one of the required subsections in item 5 OR the Sources/References in item 6.\n- 1.0: Valid PDF/DOCX but either outside length OR missing two required items, yet still includes at least three of the required subsections in item 5 and either the performance or macro section.\n- 0.0: Wrong format (not PDF/DOCX), or completely fails section structure, or far outside length.\n\nOnly assess presence/format of structure, not the accuracy of content.", "expectation": "A 1\u20134 page PDF/DOCX with clear headers: Title, Executive Summary, Q1 2025 EM Performance (MSCI referenced), Macro Drivers, five mini-sections (China, India, Brazil, Technology, CEEMEA), and a Sources/References list including MSCI and at least one major outlet."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification: Sourcing, Coverage, and Consistency", "description": "Now that the structure is verified, assess correctness and verifiability: sourcing discipline, timeframe alignment, coverage completeness, and logical linkage from macro drivers to observed performance.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Mentions MSCI and Index Identifiers", "description": "Checks for presence of MSCI and common index identifiers to ensure performance is framed on MSCI indices.", "weight": 0.3, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n    text = \"\"\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            text = \"\"\\n    if not text:\\n        return 0.0\\n    t = text.lower()\\n    has_msci = 'msci' in t\\n    index_terms = [\\n        'msci emerging markets', 'msci em', 'msci world', 'msci acwi',\\n        'msci china', 'msci india', 'msci brazil', 'msci em index',\\n        'msci world index', 'msci acwi index', 'index'\\n    ]\\n    has_index = any(term in t for term in index_terms) and has_msci\\n    score_frac = 0.0\\n    if has_msci:\\n        score_frac += 0.5\\n    if has_index:\\n        score_frac += 0.5\\n    return score_frac * 0.3"}, {"type": "code", "name": "Q1 2025 Timeframe and Percentages Present", "description": "Verifies the document references Q1 2025 (or equivalent phrasing) and includes percentage return figures.", "weight": 0.3, "code": "import re\\n\\nQ1_PATTERNS = [\\n    r\"\\\\bq1\\\\s*20?25\\\\b\",\\n    r\"\\\\b1q\\\\s*20?25\\\\b\",\\n    r\"\\\\bfirst\\\\s+quarter\\\\b.*\\\\b20?25\\\\b\",\\n    r\"\\\\bjan(uary)?\\\\b.*\\\\b20?25\\\\b\",\\n    r\"\\\\bmar(ch)?\\\\b.*\\\\b20?25\\\\b\"\\n]\\nPCT_PATTERN = r\"[-+]?(?:\\\\d{1,2}(?:\\\\.\\\\d+)?|\\\\d{1,3}(?:\\\\.\\\\d+)?)[ ]?%\"\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n    text = \"\"\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            text = \"\"\\n    if not text:\\n        return 0.0\\n    t = text.lower()\\n    has_q1 = any(re.search(p, t, flags=re.IGNORECASE|re.DOTALL) for p in Q1_PATTERNS)\\n    has_pct = re.search(PCT_PATTERN, t) is not None\\n    if has_q1 and has_pct:\\n        return 0.3\\n    if has_q1 or has_pct:\\n        return 0.15\\n    return 0.0"}, {"type": "code", "name": "Coverage of Required Topics (China, India, Brazil, Tech, CEEMEA, Macro)", "description": "Checks presence of required topic areas using fuzzy keyword matching.", "weight": 0.3, "code": "import re\\n\\ndef _has_ceemea(t: str) -> bool:\\n    if 'ceemea' in t:\\n        return True\\n    signals = 0\\n    for kw in ['eastern europe', 'central and eastern europe', 'middle east', 'africa']:\\n        if kw in t:\\n            signals += 1\\n    return signals >= 2\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n    text = \"\"\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            text = \"\"\\n    if not text:\\n        return 0.0\\n    t = text.lower()\\n    checks = {\\n        'china': ('china' in t),\\n        'india': ('india' in t),\\n        'brazil': ('brazil' in t),\\n        'technology': any(k in t for k in ['technology', 'tech sector', 'information technology']),\\n        'ceemea': _has_ceemea(t),\\n        'macro': any(k in t for k in ['macro', 'macroeconomic', 'macro overview', 'macro landscape'])\\n    }\\n    covered = sum(1 for v in checks.values() if v)\\n    frac = covered / 6.0\\n    return frac * 0.3"}, {"type": "code", "name": "Named Citations Present (MSCI + Major News)", "description": "Ensures there is an explicit mention of MSCI and at least one recognized news source (WSJ/FT).", "weight": 0.3, "code": "import re\\n\\nNEWS_TERMS = ['wall street journal', 'wsj', 'financial times', 'ft ']\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n    text = \"\"\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            text = \"\"\\n    if not text:\\n        return 0.0\\n    t = text.lower()\\n    has_msci = 'msci' in t or 'msci.com' in t\\n    has_news = any(term in t for term in NEWS_TERMS)\\n    if has_msci and has_news:\\n        return 0.3\\n    if has_msci or has_news:\\n        return 0.15\\n    return 0.0"}, {"type": "llm_judge", "name": "MSCI-Sourced Q1 Performance Framing", "description": "Judge whether performance statements are clearly based on MSCI indices, labeled as index-level, and time-bounded to Q1 2025 (as of March 31, 2025).", "weight": 1.6, "judge_prompt": "Evaluate the document for proper performance framing:\n- Do stated returns clearly reference MSCI indices (e.g., MSCI Emerging Markets, MSCI World/ACWI) and read as index-level, not fund returns?\n- Are dates/timeframes explicit and consistent with Q1 2025 (ideally phrased as \"as of March 31, 2025\")?\n- Are currency bases (USD vs local) or price/total return context indicated or reasonably implied?\n\nScoring:\n- 1.6: MSCI index basis is explicit throughout; returns are clearly index-level; Q1 2025 dating is consistent and unambiguous; context (currency/return type) is noted or reasonably implied.\n- 0.8: Mostly clear MSCI/index framing with minor ambiguity or occasional missing labels.\n- 0.0: Lacks clear MSCI/index basis or timeframe labeling; confusing or misleading performance framing.", "expectation": "Consistent, explicit MSCI index references; Q1 2025 dating; no misrepresentation of fund vs index returns."}, {"type": "llm_judge", "name": "Country/Sector Driver Coherence", "description": "Judge whether China, India, Brazil, Technology, and CEEMEA sections each summarize performance context and key macro/micro drivers with specific references.", "weight": 1.6, "judge_prompt": "For each mini-section (China, India, Brazil, Technology sector, CEEMEA):\n- Is there a brief performance recap (directional or indexed) tied to Q1 2025?\n- Are 1\u20132 key drivers (policy, earnings, flows, commodities, rates/FX, geopolitics) identified and tied to cited sources?\n- Are statements specific rather than generic, avoiding contradictions across sections?\n\nScoring:\n- 1.6: All five sections are covered with clear, source-tied drivers and coherent linkages to Q1 performance.\n- 0.8: Most sections are adequate; one or two are thin or generic.\n- 0.0: Sections missing or largely generic with no clear driver linkage.", "expectation": "Each required section pairs a concise performance recap with 1\u20132 specific, sourced drivers."}, {"type": "llm_judge", "name": "Macro Drivers Linked to EM Outcomes", "description": "Judge whether the macro overview covers major Q1 2025 drivers (rates/DM central banks, USD, commodities, geopolitics) and explicitly links them to EM equity performance and EM vs DM dispersion.", "weight": 1.6, "judge_prompt": "Assess the Macro Drivers / Macro Overview section:\n- Are key macro themes addressed (rates/central banks, USD moves, commodity trends like oil/metals, geopolitics)?\n- Does the text explicitly connect these drivers to EM equity performance and relative performance vs DM (where cited)?\n- Are claims tied to the cited sources and consistent with the performance section?\n\nScoring:\n- 1.6: Comprehensive macro coverage with explicit, well-sourced linkage to EM outcomes and EM vs DM context.\n- 0.8: Partial coverage or weak linkage; some claims lack sourcing.\n- 0.0: Minimal macro coverage; no clear linkage to EM outcomes.", "expectation": "Clear macro narrative that ties back to EM equity performance with sources."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Client Readiness", "description": "Holistic assessment of clarity, usefulness for institutional clients, and professionalism.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Client-Centric Narrative and Retention Orientation", "description": "Assesses whether the document acknowledges EM challenges, rebuilds trust, and frames a credible outlook for retention.", "weight": 0.5, "judge_prompt": "Evaluate if the document: (1) acknowledges the 10-year EM underperformance context empathetically; (2) frames a constructive yet realistic outlook; (3) provides balanced risk/reward framing that supports client retention without overpromising.", "expectation": "Balanced, empathetic tone with credible reasons to stay engaged in EM."}, {"type": "llm_judge", "name": "Clarity, Structure, and Concision", "description": "Assesses readability, flow, and adherence to the 4-page constraint.", "weight": 0.5, "judge_prompt": "Judge clarity and organization: clear headers, logical flow from Executive Summary to details, concise prose suitable for busy institutional readers, and adherence to the \u22644-page constraint.", "expectation": "Crisp, well-structured, and concise writing suitable for institutional clients."}, {"type": "llm_judge", "name": "Actionable Takeaways and Outlook", "description": "Assesses presence of succinct takeaways and what to watch next.", "weight": 0.5, "judge_prompt": "Check for a brief set of takeaways (e.g., bullets) and a forward-looking outlook for the next quarter (with appropriate caveats) that a client could use in discussions.", "expectation": "Clear takeaways and near-term watchlist with appropriate caveats."}, {"type": "llm_judge", "name": "Professional Tone and Compliance Sensitivity", "description": "Assesses professional tone, absence of inappropriate promises, and basic disclaimers where needed.", "weight": 0.5, "judge_prompt": "Evaluate tone and professionalism: consistent terminology (e.g., defines EM/DM/CEEMEA), no guarantees or inappropriate recommendations, and presence of a reasonable disclaimer (e.g., informational, not investment advice) appropriate for institutional materials.", "expectation": "Professional, compliant tone with reasonable disclaimers and defined terms."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "8c823e32-537c-42b2-84ba-635d63c2853a", "rubric": {"category_name": "Government | First-Line Supervisors of Police and Detectives | UAS Policy Development", "rationale": "This rubric enforces a self-documenting, evidence-first workflow for a police UAS (drone) operations policy. Stage 1 mandates a precise, professional PDF/DOCX structure enabling reliable verification. Stage 2 blends lightweight code checks (for objective keyword/structure coverage) with LLM judges (for nuanced legal and operational correctness). Stage 3 assesses overall professional quality, clarity, enforceability, and operational readiness. Code rules are weighted ~5x less on average than LLM judges in Stage 2, reflecting their narrower scope.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Format and Structural Gate (MANDATORY)", "description": "LLM-only gate ensuring the output is a professionally formatted PDF/DOCX policy with a complete header block and required sections that enable verification. No content quality checks here\u2014only structure and presence.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.25, "rules": [{"type": "llm_judge", "name": "Format and Header Block Presence", "description": "Validates document format and existence of a professional header block with required fields on the first page.", "weight": 1.5, "judge_prompt": "You are evaluating whether the candidate output is in the correct format and has the required header block. Only check structure and presence\u2014not content quality.\n\nRequirements:\n- File format: Prefer PDF. DOCX acceptable. Not plain text, not Excel.\n- Length: At least 2 pages.\n- Professional formatting: clear headings, readable typography, structured sections.\n- Header block on first page with clearly labeled fields (accept reasonable variants):\n  \u2022 Title (e.g., \u201cGeneral Manual Procedure: Unmanned Aircraft System (UAS) Operations\u201d or similar)\n  \u2022 Referenced Files/References or Authorities\n  \u2022 Responsible Office (e.g., Policy Development Unit, Chief\u2019s Office, or equivalent)\n  \u2022 Related Procedures (cross-references to other manual sections/policies)\n\nScoring (0\u20131.5):\n- 1.5: PDF, \u22652 pages, professional formatting, and all four header fields present.\n- 1.2: DOCX (not PDF) but otherwise meets all requirements; OR PDF with 3/4 header fields.\n- 0.8: Valid PDF/DOCX, \u22652 pages, professional formatting, but only 2/4 header fields.\n- 0.4: Valid PDF/DOCX, <2 pages OR poor formatting but header exists (\u22652 fields).\n- 0.0: Wrong file type OR no header block (\u22641 field) OR obviously not a formal policy.\n\nBe flexible with exact labels and capitalization. Return a score only based on these structural checks.", "expectation": "A professionally formatted PDF with a first-page header showing the title, references, responsible office, and related procedures."}, {"type": "llm_judge", "name": "Required Sections and Use-Case Subsections Present", "description": "Checks the presence of all required top-level sections and the specified subsections under Operational Guidance.", "weight": 1.5, "judge_prompt": "Evaluate only the presence of required sections/subsections, not their quality.\n\nRequired top-level sections (accept minor variations in naming):\n1) Purpose\n2) Policy (or Statement of Policy)\n3) Definitions\n4) Authorized Users (or Authorization/Qualifications)\n5) Prohibited Uses\n6) Operational Guidance (or Operations/Procedures)\n7) Training (or Training & Certification)\n\nOperational Guidance must include clearly labeled subsections for these use cases:\nA) High-Risk Emergency Deployment (firearms/armed suspect scenarios)\nB) Rapid Response (e.g., staged/airborne deployment posture)\nC) Vehicle Pursuit Support\nD) Tactical Team Integration (e.g., SWAT/ERT/SRT support)\n\nScoring (0\u20131.5):\n- 1.5: All 7 top-level sections present AND all 4 required subsections within Operational Guidance present.\n- 1.2: Missing exactly one of the 11 required items (either one top-level section or one required subsection).\n- 0.9: Missing exactly two required items.\n- 0.5: Missing three required items.\n- 0.0: Missing four or more required items OR structure is not clearly sectioned.\n\nCheck only presence and labeling. Do not judge content correctness.", "expectation": "A clearly sectioned policy document with all required sections and the four specified Operational Guidance subsections."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification: Legal, Policy, and Operational Correctness", "description": "Mixture of code and LLM rules. Code rules perform deterministic coverage checks on regulatory and use-case keywords. LLM judges evaluate legal alignment (FAA), civil liberties safeguards, and authorization/oversight coherence.", "is_required": false, "max_points": 9.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Regulatory and FAA Alignment \u2014 Keyword Coverage", "description": "Checks for presence of key FAA/regulatory concepts indicating proper coverage (e.g., Part 107, COA, Remote ID, VLOS/BVLOS, LAANC). Returns a fractional score based on coverage.", "weight": 0.75, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float in [0,1] or (float, str)\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No primary document output to evaluate.\"\n\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0, \"Unable to read document text.\"\n\n    if not text:\n        return 0.0, \"Document text empty or unreadable.\"\n\n    t = text.lower()\n\n    groups = [\n        [\"faa\"],\n        [\"part 107\", \"14 cfr part 107\", \"cfr part 107\", \"title 14 cfr 107\"],\n        [\"remote id\", \"rid\"],\n        [\"laanc\", \"low altitude authorization\", \"airspace authorization\"],\n        [\"certificate of authorization\", \"coa\"],\n        [\"remote pilot in command\", \"rpic\"],\n        [\"visual observer\", \"vo\"],\n        [\"visual line of sight\", \"vlos\"],\n        [\"beyond visual line of sight\", \"bvlos\"],\n        [\"night operations\", \"anti-collision\", \"anticollision\", \"night flight\"],\n        [\"public aircraft\", \"public aircraft operations\", \"40125\"],\n    ]\n\n    found = 0\n    missing_terms = []\n    for grp in groups:\n        if any(term in t for term in grp):\n            found += 1\n        else:\n            missing_terms.append(grp[0])\n\n    ratio = found / len(groups)\n    feedback = f\"Regulatory coverage ratio: {found}/{len(groups)}. Missing exemplars: {', '.join(missing_terms[:5])}\"\n    return ratio, feedback\n"}, {"type": "code", "name": "Operational Use-Case Coverage \u2014 Keyword Presence", "description": "Verifies presence of required operational use cases and related concepts (high-risk firearms calls, rapid response, vehicle pursuits, tactical team integration, and deconfliction with air support).", "weight": 0.75, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float in [0,1] or (float, str)\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No primary document output to evaluate.\"\n\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0, \"Unable to read document text.\"\n\n    if not text:\n        return 0.0, \"Document text empty or unreadable.\"\n\n    t = text.lower()\n\n    groups = [\n        [\"high-risk\", \"high risk\", \"armed\", \"firearm\", \"shots fired\"],\n        [\"rapid response\", \"pre-position\", \"preposition\", \"staged\", \"airborne\", \"overwatch\"],\n        [\"vehicle pursuit\", \"pursuit\", \"fleeing vehicle\", \"tracking\"],\n        [\"tactical team\", \"swat\", \"ert\", \"srt\", \"tactical operations\"],\n        [\"deconfliction\", \"helicopter\", \"air support\", \"aviation\"],\n    ]\n\n    found = 0\n    missing = []\n    for grp in groups:\n        if any(term in t for term in grp):\n            found += 1\n        else:\n            missing.append(grp[0])\n\n    ratio = found / len(groups)\n    feedback = f\"Use-case coverage ratio: {found}/{len(groups)}. Missing exemplars: {', '.join(missing)}\"\n    return ratio, feedback\n"}, {"type": "llm_judge", "name": "FAA/Legal Alignment and Operational Accuracy", "description": "Assesses whether the policy accurately aligns with FAA rules (Part 107 or PAO/COA), includes Remote ID, airspace authorization (e.g., LAANC), VLOS/VO requirements, night ops, and does not misstate authorities.", "weight": 2.5, "judge_prompt": "Evaluate the document for legal/regulatory correctness and completeness related to FAA and airspace rules. Consider:\n- Correct use of Part 107 or Public Aircraft Operations/COA when applicable to police operations.\n- Remote Pilot in Command (RPIC) responsibilities and Visual Observer (VO) usage.\n- VLOS vs. BVLOS limits, including any waiver/COA conditions.\n- Night operations requirements (e.g., anti-collision lighting) consistent with current FAA guidance.\n- Airspace authorization (e.g., LAANC) and deconfliction with manned aircraft.\n- Remote ID compliance.\n- No claims of authority that contradict FAA regulations or state/federal laws.\n\nScoring (0\u20132.5):\n- 2.5: Accurate, complete, no material errors.\n- 1.7: Generally accurate, minor omissions (e.g., mentions VLOS but omits Remote ID specifics).\n- 1.0: Mixed\u2014some correct points but notable gaps/ambiguities.\n- 0.3: Significant misunderstandings or missing key elements.\n- 0.0: Legally incorrect or conflicts with FAA rules.", "expectation": "Clear, accurate articulation of FAA/airspace constraints and responsibilities with no material errors."}, {"type": "llm_judge", "name": "Civil Liberties, Privacy, and Prohibited Uses Sufficiency", "description": "Evaluates whether prohibited uses and privacy safeguards are robust and consistent with best practices and constitutional constraints.", "weight": 2.5, "judge_prompt": "Assess the clarity and sufficiency of civil liberties protections and prohibited uses. Consider presence and adequacy of:\n- Prohibitions on general surveillance or monitoring First Amendment activity without lawful purpose or authorization.\n- Guidance on search/seizure, warrants, exigent circumstances, consent, and applicable state/federal privacy laws.\n- Data management: collection minimization, retention schedules, access controls, CJIS where applicable, audit logs, public records handling.\n- Prohibition on weaponization of UAS and harassment/discrimination safeguards.\n- Transparency/community trust measures (e.g., public reporting, complaint process) if referenced.\n\nScoring (0\u20132.5):\n- 2.5: Comprehensive, specific, and aligned with constitutional and best-practice constraints.\n- 1.7: Generally strong with minor omissions.\n- 1.0: Partial\u2014important safeguards missing or vague.\n- 0.3: Minimal protections; mostly perfunctory.\n- 0.0: Absent or permissive of unconstitutional uses.", "expectation": "A clear, enforceable prohibited uses list with explicit privacy and constitutional safeguards."}, {"type": "llm_judge", "name": "Authorization, Roles, Accountability, and Documentation", "description": "Checks clarity of who may deploy drones, required approvals, chain of command, logs/after-action documentation, and oversight/audit mechanisms.", "weight": 2.5, "judge_prompt": "Evaluate how well the policy defines authorization and accountability. Look for:\n- Role definitions (e.g., UAS Program Manager/Coordinator, RPIC, VO, Watch Commander, Tactical Commander).\n- Clear deployment authorization criteria (routine vs. emergency), supervisor approvals, and notifications.\n- Required documentation: mission logs, evidence handling/chain-of-custody, deconfliction notes, and after-action reporting.\n- Oversight/audit cadence, compliance monitoring, corrective actions.\n\nScoring (0\u20132.5):\n- 2.5: Roles/approvals are explicit; documentation and oversight processes are clear and complete.\n- 1.7: Mostly clear with minor gaps.\n- 1.0: Partially defined; key elements vague or missing.\n- 0.3: Poorly defined roles and accountability.\n- 0.0: No meaningful authorization or oversight framework.", "expectation": "Unambiguous roles/approvals, strong documentation and audit/oversight requirements."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism", "description": "Holistic LLM assessment of presentation quality, enforceability, operational practicality, and readiness for inclusion in the General Police Manual.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Formatting and Manual-Style Consistency", "description": "Assesses whether the document mirrors professional police manual style (structure, typography, headings, tables/appendices if used) and is ready for internal review.", "weight": 2.0, "judge_prompt": "Judge the professional presentation quality and consistency with police manual style. Consider:\n- Clear, consistent headings and numbering; logical flow; clean typography.\n- Inclusion of standard manual features (e.g., references list, related procedures, version/effective date if present).\n- Overall readability and professional polish consistent with referenced agencies.\n\nScoring (0\u20132):\n- 2.0: Fully professional and consistent with established manual style; ready for review.\n- 1.3: Minor formatting issues but overall professional.\n- 0.7: Passable but inconsistent styling or formatting gaps.\n- 0.0: Poorly formatted or unprofessional.", "expectation": "Professional, consistent formatting aligned with typical police general orders/manuals."}, {"type": "llm_judge", "name": "Clarity and Enforceability of Policy Language", "description": "Evaluates clarity, precision, and enforceability (use of shall/shall not, clear definitions, unambiguous directives).", "weight": 2.0, "judge_prompt": "Assess clarity and enforceability of the policy language:\n- Are terms defined and used consistently?\n- Are directives unambiguous (shall/shall not vs. suggestive language)?\n- Is the policy legally sound and written so officers at all levels can follow it?\n\nScoring (0\u20132):\n- 2.0: Clear, precise, and enforceable throughout.\n- 1.3: Minor ambiguities; generally enforceable.\n- 0.7: Several ambiguities; uneven enforceability.\n- 0.0: Unclear and not enforceable.", "expectation": "Direct, unambiguous, and precise policy language with consistent definitions."}, {"type": "llm_judge", "name": "Operational Practicality and Tactical Utility", "description": "Assesses whether guidance is actionable for rapid response, high-risk calls, pursuits, and tactical operations, including safety/deconfliction and community safety considerations.", "weight": 2.0, "judge_prompt": "Evaluate operational practicality and tactical value:\n- Are deployment steps and decision criteria actionable for line supervisors and RPICs?\n- Does it address safety, deconfliction with helicopters/airspace users, and risk mitigation?\n- Does it meaningfully improve rapid response and high-risk incident outcomes while supporting community safety?\n\nScoring (0\u20132):\n- 2.0: Highly actionable with strong tactical and safety guidance.\n- 1.3: Generally useful; some gaps.\n- 0.7: Limited practical guidance; many gaps.\n- 0.0: Not practically useful.", "expectation": "Clear step-by-step or checklist-like operational guidance enabling safe, effective deployment."}, {"type": "llm_judge", "name": "Implementation Readiness (Training, Currency, Maintenance, Records)", "description": "Assesses whether the policy is ready for adoption, including training/certification, currency requirements, equipment maintenance, recordkeeping, and integration with existing systems.", "weight": 2.0, "judge_prompt": "Assess implementation readiness:\n- Training/certification standards for RPICs/VOs; currency/recurrent training.\n- Maintenance/inspection protocols for UAS; firmware/Remote ID readiness.\n- Recordkeeping: mission logs, retention, evidence handling; integration with CAD/RMS/CJIS as applicable.\n- Clear cross-references to related procedures.\n\nScoring (0\u20132):\n- 2.0: Fully ready for adoption with specific requirements.\n- 1.3: Mostly ready; minor omissions.\n- 0.7: Partially ready; notable gaps.\n- 0.0: Not adoption-ready.", "expectation": "Explicit training/currency, maintenance, and records processes that integrate with existing systems."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "05389f78-589a-473c-a4ae-67c61050bfca", "rubric": {"category_name": "Supplier Escalation Email + Replacement Supplier Analysis (Manufacturing Procurement)", "rationale": "This is a mixed task: a 1-page termination/escalation email (document) plus a 2\u20133 page analysis report (document) with embedded financial/risk analysis based on vendor quotations. Stage 1 is a strict LLM-only gate enforcing two separate DOCX/PDF deliverables and precise structure that makes downstream verification trivial. Stage 2 blends light code checks (textual and numeric consistency) with heavier LLM judging for contractual accuracy, INR treatment, and financial/risk reasoning. Stage 3 evaluates professional quality, tone/relationship management, and strategic clarity. Code rules are intentionally lower-weight than LLM rules (~5x less on average) and focus on deterministic checks enabled by the Stage 1 structure.", "max_total_score": 12.0, "stages": [{"name": "Stage 1 Gate \u2014 Structured Deliverables Present", "description": "LLM-only shape enforcement. Requires two separate documents: (1) a 1-page email to Juvoxa leadership with mandatory elements; (2) a 2\u20133 page report to the CPO with required sections, tables, and INR-based analysis scaffolding.", "is_required": true, "max_points": 2.0, "min_score_to_pass": 1.5, "rules": [{"type": "llm_judge", "name": "Mandatory Structure and Format Check (Email + Report)", "description": "Verify there are TWO separate files in valid DOCX or PDF formats: a 1-page professional email and a 2\u20133 page analysis report with specified sections/tables enabling verification.", "weight": 2.0, "judge_prompt": "You are verifying SHAPE ONLY (not correctness) of a submission that should include TWO separate documents in DOCX or PDF. Examine all provided outputs. Score based on these requirements:\n\nA) EMAIL TO SUPPLIER LEADERSHIP (1 page max):\n- Valid format: DOCX or PDF\n- Addressed appropriately: To: Mr. Colin Hartwell (CEO of Juvoxa), CC: Juvoxa design head and relationship manager\n- Clear Subject line\n- Body includes ALL of the following elements:\n  1) Summary of ongoing development issues: specifically notes 4 consecutive crash test failures and the resulting 2-month delay\n  2) References breach of purchase contract/obligations\n  3) Firmly communicates decision to terminate Juvoxa\u2019s nomination for Model A AND all future programs\n  4) Formal request for return of 30% of tooling and development costs paid upfront\n  5) Professional, firm tone acknowledging the partnership while noting erosion of confidence\n- Professional closing and sender identification from Banyan Crest Automotive procurement\n\nB) REPORT TO CPO (2\u20133 pages):\n- Valid format: DOCX or PDF\n- Sections with clear headers (flexible naming allowed, but all functional content must be present):\n  1) Executive Summary (or Overview)\n  2) Supplier Failure Summary (or Background/Context) \u2014 covering Juvoxa\u2019s crash-test failures and timeline impact\n  3) Assumptions and Source Data \u2014 includes an explicit table for vendor quotation figures used for Autonexis Lighting and Vendrax Components. The table should include at minimum: Vendor, Unit Price (INR), Tooling Cost (INR), Volume/MOQ assumption, Lead Time (weeks). If any FX is involved (Autonexis is overseas), state the FX rate used and how conversion to INR is applied. If the original 'Model A HL quotes' file is not available, the section should still show a clearly labeled assumptions table used for calculations.\n  4) Comparative Analysis \u2014 cost comparison and risk discussion broken into sub-parts:\n     - Cost Analysis (tooling, per-unit cost at stated volumes, total cost impact)\n     - Lead Time and Timeline Recovery (explicitly cites lead times)\n     - Forex Exposure for Autonexis (explicit rate and sensitivity noted)\n  5) Financial Impact/Total Cost of Ownership Summary \u2014 includes at least one summary table of INR totals and a step-by-step calculation log or breakdown that shows how totals are derived\n  6) Recommendation \u2014 a specific nomination choice between Autonexis Lighting and Vendrax Components based on cost, risk, timeline, and strategic alignment\n- All costs expressed in INR\n- At least one comparison table and one TCO/summary table are visible\n\nScoring (shape only):\n- 2.0: Both files present as separate DOCX/PDFs; email meets all bullet requirements and is ~1 page; report is 2\u20133 pages and contains all required sections, tables, and explicit INR treatment/FX assumptions if applicable.\n- 1.7: Minor omissions but all core items present (e.g., email missing CC line formatting, or report missing one non-critical sub-bullet like explicit sensitivity but still has FX rate and INR tables).\n- 1.3: One major required element missing in either document (e.g., email lacks explicit 30% tooling refund request OR report lacks the Assumptions/Source Data table or Recommendation section).\n- 0.8: Multiple major elements missing across documents or wrong formats/pages (e.g., only one file provided, or report not 2\u20133 pages, or email not clearly an email).\n- 0.0: Not DOCX/PDF, or documents missing, or content clearly not matching the requested deliverables.\n\nOnly evaluate PRESENCE/STRUCTURE, not whether the numbers or arguments are correct.", "expectation": "Two separate DOCX/PDF files: 1-page email with specified mandatory elements, and a 2\u20133 page report with all required sections, at least one comparison table and one TCO table, and all costs in INR."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification and Internal Consistency", "description": "Now that the outputs are in the right shape, verify internal consistency, presence of key contractual and financial elements, INR treatment, and reasoning coherence.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Email Essential Clauses Detected", "description": "Programmatically check the email text includes Juvoxa, Colin Hartwell, termination language, and the 30% tooling refund request.", "weight": 0.35, "code": "import re\n\ndef evaluate(workflow, context):\n    def get_all_docs():\n        outs = context.get_all_outputs() or []\n        return [r for r in outs if getattr(r, 'is_document', False)]\n\n    def extract_text(res):\n        text = ''\n        try:\n            text = context.files.read_docx_text(res.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(res.id)\n            except Exception:\n                try:\n                    if getattr(res, 'is_text_format', False):\n                        text = context.files.read_text(res.id)\n                except Exception:\n                    text = ''\n        return text or ''\n\n    docs = get_all_docs()\n    if not docs:\n        return 0.0\n\n    texts = [extract_text(d) for d in docs]\n    if not texts:\n        return 0.0\n\n    # Heuristic: email likely the shorter document\n    lengths = [len(t.split()) for t in texts]\n    try:\n        email_idx = lengths.index(min(lengths))\n    except ValueError:\n        email_idx = 0\n    email_text = (texts[email_idx] or '').lower()\n\n    checks = 0\n    total = 4\n\n    if 'juvoxa' in email_text:\n        checks += 1\n    if 'colin hartwell' in email_text or 'mr. colin hartwell' in email_text:\n        checks += 1\n    # termination language\n    if any(x in email_text for x in ['terminate', 'termination', 'terminating', 'cancel', 'cessation']):\n        checks += 1\n    # 30% tooling refund request\n    has_30 = '30%' in email_text or re.search(r'\\b30\\s*percent', email_text)\n    has_tooling = 'tooling' in email_text or 'development cost' in email_text or 'development costs' in email_text\n    if has_30 and has_tooling:\n        checks += 1\n\n    return checks / total"}, {"type": "code", "name": "Report Currency, Vendors, Risk, Numerics, and Recommendation", "description": "Programmatically check the report text includes INR currency, both vendors, lead time and forex/currency risk terms, sufficient numerics, and an explicit recommendation mentioning one vendor.", "weight": 0.35, "code": "import re\n\ndef evaluate(workflow, context):\n    def get_all_docs():\n        outs = context.get_all_outputs() or []\n        return [r for r in outs if getattr(r, 'is_document', False)]\n\n    def extract_text(res):\n        text = ''\n        try:\n            text = context.files.read_docx_text(res.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(res.id)\n            except Exception:\n                try:\n                    if getattr(res, 'is_text_format', False):\n                        text = context.files.read_text(res.id)\n                except Exception:\n                    text = ''\n        return text or ''\n\n    docs = get_all_docs()\n    if len(docs) < 1:\n        return 0.0\n\n    texts = [extract_text(d) for d in docs]\n    if not texts:\n        return 0.0\n\n    # Heuristic: report likely the longer document\n    lengths = [len(t.split()) for t in texts]\n    try:\n        report_idx = lengths.index(max(lengths))\n    except ValueError:\n        report_idx = 0\n    report = (texts[report_idx] or '').lower()\n\n    score = 0\n    total = 6\n\n    # Vendors present\n    if 'autonexis' in report:\n        score += 1\n    if 'vendrax' in report:\n        score += 1\n    # INR currency present\n    if 'inr' in report or '\u20b9' in report:\n        score += 1\n    # Lead time and forex/currency risk mentioned\n    if 'lead time' in report:\n        score += 1\n    if any(x in report for x in ['forex', 'fx', 'currency risk', 'exchange rate']):\n        score += 1\n    # Numerics and explicit recommendation including a vendor name\n    nums = re.findall(r'\\b\\d[\\d,]*(?:\\.\\d+)?\\b', report)\n    has_reco = 'recommend' in report and ('autonexis' in report or 'vendrax' in report)\n    if len(nums) >= 5 and has_reco:\n        score += 1\n\n    return score / total"}, {"type": "code", "name": "Length Heuristics (1-page Email, 2\u20133 page Report)", "description": "Approximate lengths via word counts: email ~150\u2013450 words; report ~600\u20131500 words. Partial credit if only one meets bounds.", "weight": 0.3, "code": "def evaluate(workflow, context):\n    def get_all_docs():\n        outs = context.get_all_outputs() or []\n        return [r for r in outs if getattr(r, 'is_document', False)]\n\n    def extract_text(res):\n        text = ''\n        try:\n            text = context.files.read_docx_text(res.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(res.id)\n            except Exception:\n                try:\n                    if getattr(res, 'is_text_format', False):\n                        text = context.files.read_text(res.id)\n                except Exception:\n                    text = ''\n        return text or ''\n\n    docs = get_all_docs()\n    if len(docs) < 2:\n        return 0.0\n\n    texts = [extract_text(d) for d in docs]\n    lengths = [len((t or '').split()) for t in texts]\n    if not lengths:\n        return 0.0\n\n    email_words = min(lengths)\n    report_words = max(lengths)\n\n    email_ok = 150 <= email_words <= 450\n    report_ok = 600 <= report_words <= 1500\n\n    if email_ok and report_ok:\n        return 1.0\n    elif email_ok or report_ok:\n        return 0.5\n    else:\n        return 0.0"}, {"type": "llm_judge", "name": "Email Contractual Decisiveness and Professionalism", "description": "Email accurately states failures and delays, cites breach, announces termination for Model A and future programs, and requests 30% tooling refund with a firm yet respectful tone.", "weight": 2.0, "judge_prompt": "Evaluate ONLY the email document(s). Score for correctness/completeness of contractual communication:\n- Clearly summarizes the 4 consecutive crash-test failures and resulting 2-month delay\n- Explicitly cites breach of purchase contract/obligations and accountability concerns\n- Unambiguously communicates termination of Juvoxa\u2019s nomination for Model A AND future programs\n- Formally requests return of 30% of tooling and development costs already paid upfront\n- Uses a firm, professional tone acknowledging the partnership and erosion of confidence; avoids inflammatory language\nScoring:\n- 2.0: Meets all bullets clearly and professionally\n- 1.5: Minor omissions/wording ambiguity but decision and refund are clear\n- 1.0: Several elements weak or missing (e.g., failure history or breach not explicit)\n- 0.0: Missing core decision or refund request, or tone inappropriate", "expectation": "A decisive, professional termination email with explicit breach citation and 30% tooling refund request."}, {"type": "llm_judge", "name": "Financial Analysis Coherence and INR Treatment", "description": "Report shows coherent, reproducible INR-based analysis with FX assumptions for Autonexis, lead-time risk, and TCO summary.", "weight": 2.0, "judge_prompt": "Evaluate the REPORT. Check coherence and internal consistency of the financial analysis:\n- All costs presented in INR, with explicit FX rate used for Autonexis (if foreign currency mentioned) and clear conversion\n- Shows unit cost \u00d7 volume calculations and tooling cost treatment (e.g., upfront or amortization) leading to totals\n- Lead time impact quantified or explicitly translated into timeline recovery risk considerations (e.g., buffer, premium, or risk narrative with quantified impact if provided)\n- Forex exposure for Autonexis addressed with at least a basic sensitivity (e.g., +/- X% rate change effect) or clearly justified risk premium\n- Includes a TCO or financial impact summary table and a brief calculation log/breakdown that ties numbers to totals\nScore:\n- 2.0: All items present and calculations are consistent and traceable\n- 1.5: Minor gaps (e.g., sensitivity shallow) but overall consistent and reproducible\n- 1.0: Significant gaps or unclear math; partial INR treatment\n- 0.0: No coherent financial analysis or INR handling", "expectation": "A reproducible INR-based analysis including FX assumptions, lead-time risk, and a TCO summary table."}, {"type": "llm_judge", "name": "Recommendation Justification (Cost, Risk, Timeline, Strategy)", "description": "Recommendation is specific and well-reasoned between Autonexis and Vendrax across cost, risk, timeline recovery, and strategic alignment.", "weight": 1.0, "judge_prompt": "Evaluate the REPORT\u2019s recommendation section:\n- Names the chosen replacement supplier (Autonexis Lighting or Vendrax Components)\n- Justifies the choice across: cost (including tooling and unit price), risk (lead time, forex), timeline recovery (feasibility to protect launch), and strategic alignment with Banyan Crest procurement goals\n- Avoids generic next-step lists; focuses on a decisive selection rationale\nScoring:\n- 1.0: Clear, well-argued choice covering all four dimensions\n- 0.7: Mostly complete with minor gaps\n- 0.4: Vague or missing 1\u20132 dimensions\n- 0.0: No specific recommendation or unsupported choice", "expectation": "A decisive recommendation grounded in cost, risk, timeline, and strategy."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Communication", "description": "Assess writing quality, executive readiness, tone, and transparency of assumptions and calculations.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Executive Readiness and Formatting", "description": "Documents are professionally formatted, easy to read, and sized appropriately (email ~1 page, report 2\u20133 pages).", "weight": 1.0, "judge_prompt": "Assess overall professionalism:\n- Email is cleanly formatted as an executive email, ~1 page\n- Report is 2\u20133 pages with clear headers, logical flow, and at least one table for comparison and one for TCO/summary\n- Visual clarity and layout support quick executive consumption\nScore 0.0\u20131.0 accordingly.", "expectation": "Polished, executive-ready email and report with clear layout and tables."}, {"type": "llm_judge", "name": "Clarity, Structure, and Accessibility", "description": "Clear sectioning, concise language; avoids unnecessary task lists in the conclusion; uses tables/figures to make comparisons accessible.", "weight": 1.0, "judge_prompt": "Evaluate clarity and structure:\n- Sections are well-labeled; content is concise and logically ordered\n- Uses tables/figures to make comparisons easy to grasp\n- Conclusion provides a specific recommendation without devolving into generic next-steps lists\nScore 0.0\u20131.0 based on overall clarity and accessibility.", "expectation": "Concise, well-structured writing with accessible comparisons and a focused conclusion."}, {"type": "llm_judge", "name": "Tone and Relationship Management", "description": "Email maintains firm yet respectful tone acknowledging partnership; report stays objective and professional.", "weight": 1.0, "judge_prompt": "Assess tone and relationship management:\n- Email: firm, professional, acknowledges partnership while conveying loss of confidence; avoids inflammatory language\n- Report: objective, balanced assessment of both vendors without bias or disparagement\nScore 0.0\u20131.0 accordingly.", "expectation": "Firm, respectful email and objective report tone throughout."}, {"type": "llm_judge", "name": "Self-Documentation and Traceability", "description": "Assumptions and data sources are transparent; calculations are traceable; FX assumptions (if used) are explicit.", "weight": 1.0, "judge_prompt": "Evaluate self-documentation:\n- Assumptions and source data are transparently documented (especially if the 'Model A HL quotes' file wasn\u2019t accessible)\n- FX assumptions, if applicable, are explicit and used consistently\n- Calculation steps or logs make totals traceable without guessing\nScore 0.0\u20131.0 based on transparency and traceability.", "expectation": "Clear assumptions table, explicit FX rate, and a brief calculation log enabling reproducibility."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "7ed932dd-244f-4d61-bf02-1bc3bab1af14", "rubric": {"category_name": "Wholesale Trade \u2022 Sales Rep (Alcoholic Beverages) \u2022 Inventory Continuity Model and Shipment Plan", "rationale": "This rubric enforces a self-documenting Excel model that proves correct inventory coverage through end-of-July, integrates upcoming shipments, and produces an actionable shipment recommendation for the distributor. Stage 1 mandates an auditable workbook structure. Stage 2 verifies correctness via targeted code checks plus LLM cross-referencing. Stage 3 assesses professional quality and actionability for distributor stakeholders.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 STRUCTURE GATE (Excel Model Shape Enforcement)", "description": "LLM-only gate that mandates the exact Excel workbook structure required for downstream verification. If this shape is missing, the entire category is unscorable.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.5, "rules": [{"type": "llm_judge", "name": "Required Workbook Structure Present", "description": "Check that the candidate output is a single Excel workbook with the mandated sheets/sections, labeled tables, and highlight conventions to enable verification.", "weight": 6.0, "judge_prompt": "You are validating ONLY the presence and structure of an Excel workbook for an inventory continuity and shipment recommendation model. Do NOT judge correctness of calculations here.\n\nFormat requirements:\n- The output must be an Excel spreadsheet (XLSX). Not PDF, not Word, not CSV alone.\n- Workbook must be self-contained and readable (no screenshots-only content).\n\nRequired structure (be flexible with exact names, but required functions must exist):\n\nA) Input data present in one of these acceptable patterns:\n  Option A (preferred \u2014 single sheet): A sheet named like \"Inputs\" or \"Data Import\" that clearly contains THREE labeled tables/sections:\n    1) Current Inventory & Rate of Sale by SKU (columns resembling: SKU, Product, Current Cases (on hand), Rate of Sale (daily or weekly \u2014 but unit stated))\n    2) Upcoming Shipments (columns resembling: SKU, Arrival Date, Quantity (cases or pallets), Source/Notes)\n    3) Pallet to Case Conversion (columns resembling: SKU or Product/Family and Cases per Pallet)\n  Option B (separate sheets): Three separate sheets that fulfill the same three data needs, with sheet names similar to:\n    - \"Current Inventory\" (or similar)\n    - \"Upcoming Shipments\" (or similar)\n    - \"Pallet Conversion\" (or similar)\n\nB) Inventory Model sheet:\n  - A sheet named like \"Inventory Model\", \"Model\", or \"Calculations\" that includes:\n    - An Assumptions section stating the as-of date and the planning horizon end as July 31 (year visible).\n    - A main calculation table with columns resembling (names may vary slightly but intent must be clear):\n      \u2022 SKU\n      \u2022 Current Cases\n      \u2022 Rate of Sale (cases/day or clearly stated unit)\n      \u2022 Days of Inventory on Hand\n      \u2022 Projected Out-of-Stock Date\n      \u2022 Delivered Days of Inventory (post-shipments) or coverage indicator through July 31\n\nC) Recommended Shipments sheet:\n  - A sheet named like \"Recommended Shipments\", \"Shipment Plan\", or \"Proposed Shipments\" containing a clearly labeled table with columns resembling:\n      \u2022 SKU\n      \u2022 Cases Needed (or Equivalent)\n      \u2022 Pallets Needed (rounded up to whole pallets)\n      \u2022 Required Delivery Date\n      \u2022 Notes/Reason\n  - Visual highlighting conventions are present and explained (a legend or note) for:\n      \u2022 Rows where pallet quantities have been rounded up\n      \u2022 Rows where earlier delivery than the current schedule is required\n\nScoring:\n- 6.0: Excel format AND all three areas (Inputs, Inventory Model, Recommended Shipments) present with the specified tables/sections, plus visible highlighting and a brief legend/note explaining the highlight meanings.\n- 4.5: Excel format AND all three areas present, but the highlight legend is missing OR one minor structural element is unclear (e.g., planning horizon phrasing not explicit but clearly implies end-of-July).\n- 3.0: Excel format AND two of the three areas present (e.g., missing pallet conversion or missing recommended shipments table).\n- 1.5: Excel format but only one required area present.\n- 0.0: Not an Excel file OR structure is largely missing.\n\nOnly check presence/structure and formatting conventions, not correctness of math.", "expectation": "An auditable Excel model with Inputs, Inventory Model, and Recommended Shipments sheets, including labeled tables and highlight conventions."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness and Consistency)", "description": "Verify that calculations and logic are internally consistent and align with the task\u2019s intent to maintain stock through end of July. Mix of code rules (deterministic checks) and LLM judgment (cross-references, nuanced logic).", "is_required": false, "max_points": 9.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Days-on-Hand Math Consistency", "description": "Validate that Days of Inventory roughly equals Current Cases divided by Rate of Sale for rows with valid data on the Inventory Model sheet.", "weight": 0.7, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output\"\n    try:\n        fp = context.files.get_path(output.id)\n        xls = pd.ExcelFile(fp)\n        sheet_names = [s for s in xls.sheet_names]\n        # pick model-like sheet\n        def pick_model_sheet(names):\n            keys = ['model','inventory','calc','calculation','forecast','coverage']\n            for n in names:\n                ln = n.lower()\n                if any(k in ln for k in keys):\n                    return n\n            return names[0] if names else None\n        model_sheet = pick_model_sheet(sheet_names)\n        if not model_sheet:\n            return 0.0, \"No model sheet found\"\n        df = pd.read_excel(fp, sheet_name=model_sheet)\n        if df.empty:\n            return 0.0, \"Model sheet empty\"\n        cols = {str(c).strip(): c for c in df.columns}\n        lowmap = {k.lower(): v for k, v in cols.items()}\n        def find_col(patterns):\n            for k,v in lowmap.items():\n                if all(p in k for p in patterns):\n                    return v\n            return None\n        # Try multiple heuristics\n        curr_col = find_col(['current','case']) or find_col(['on','hand']) or find_col(['inventory'])\n        rate_col = find_col(['rate','sale']) or find_col(['velocity']) or find_col(['daily'])\n        days_col = find_col(['days','hand']) or find_col(['days','inventory']) or find_col(['doh'])\n        if not curr_col or not rate_col or not days_col:\n            return 0.0, f\"Missing necessary columns (current:{bool(curr_col)}, rate:{bool(rate_col)}, days:{bool(days_col)})\"\n        cur = pd.to_numeric(df[curr_col], errors='coerce')\n        rate = pd.to_numeric(df[rate_col], errors='coerce')\n        days = pd.to_numeric(df[days_col], errors='coerce')\n        mask = (rate > 0) & (cur >= 0) & days.notna()\n        total = int(mask.sum())\n        if total == 0:\n            return 0.0, \"No comparable rows (rate>0 and days present)\"\n        exp_days = cur[mask] / rate[mask]\n        got_days = days[mask]\n        # tolerance: max(0.5 days, 5%)\n        tol = np.maximum(0.5, 0.05 * exp_days.abs())\n        ok = (got_days - exp_days).abs() <= tol\n        score = float(ok.mean())\n        return score, f\"Rows passing: {ok.sum()}/{total}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Pallet Conversion and Rounding-Up Correctness", "description": "Check that Pallets Needed equals ceil(Cases Needed / Cases per Pallet) using the pallet conversion table if present, otherwise infer a reasonable cases-per-pallet from data.", "weight": 0.7, "code": "import re\nimport pandas as pd\nimport numpy as np\nimport math\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output\"\n    try:\n        fp = context.files.get_path(output.id)\n        xls = pd.ExcelFile(fp)\n        sheets = xls.sheet_names\n        # Find recommended shipments sheet\n        def pick_reco_sheet(names):\n            keys = ['recommend','ship','plan','proposed']\n            for n in names:\n                ln = n.lower()\n                if any(k in ln for k in keys):\n                    return n\n            return None\n        reco_sheet = pick_reco_sheet(sheets)\n        if not reco_sheet:\n            return 0.0, \"No recommended shipments sheet found\"\n        dfr = pd.read_excel(fp, sheet_name=reco_sheet)\n        if dfr.empty:\n            return 0.0, \"Recommended shipments sheet empty\"\n        cols = {str(c).strip(): c for c in dfr.columns}\n        lowmap = {k.lower(): v for k, v in cols.items()}\n        def find_col(patterns):\n            for k,v in lowmap.items():\n                if all(p in k for p in patterns):\n                    return v\n            return None\n        sku_col = find_col(['sku']) or find_col(['product','code']) or find_col(['item'])\n        cases_col = (find_col(['cases','need']) or find_col(['qty','case']) or find_col(['quantity','case']) or find_col(['cases']))\n        pallet_col = find_col(['pallet'])\n        if not cases_col or not pallet_col:\n            return 0.0, \"Missing cases or pallet column\"\n        # Get conversion sheet if present\n        def pick_conv_sheet(names):\n            keys = ['convert','pallet','cases']\n            for n in names:\n                ln = n.lower()\n                if any(k in ln for k in keys) and n != reco_sheet:\n                    return n\n            return None\n        conv_sheet = pick_conv_sheet(sheets)\n        cpp_default = None\n        sku_cpp = {}\n        if conv_sheet:\n            dfc = pd.read_excel(fp, sheet_name=conv_sheet)\n            if not dfc.empty:\n                # Find cases per pallet\n                c_cols = [c for c in dfc.columns if isinstance(c,str) and ('per' in c.lower() or 'case' in c.lower()) and 'pallet' in c.lower()]\n                sku_c = [c for c in dfc.columns if isinstance(c,str) and ('sku' in c.lower() or 'item' in c.lower() or 'product' in c.lower())]\n                if c_cols:\n                    ccol = c_cols[0]\n                    vals = pd.to_numeric(dfc[ccol], errors='coerce')\n                    v = vals.dropna()\n                    if not v.empty:\n                        cpp_default = float(v.median())\n                        if sku_c:\n                            kcol = sku_c[0]\n                            for _, row in dfc.iterrows():\n                                key = str(row[kcol]).strip() if pd.notna(row[kcol]) else None\n                                val = pd.to_numeric(row[ccol], errors='coerce')\n                                if key and pd.notna(val) and val > 0:\n                                    sku_cpp[key] = float(val)\n        # If not present, infer cpp from data\n        cases = pd.to_numeric(dfr[cases_col], errors='coerce')\n        pallets = pd.to_numeric(dfr[pallet_col], errors='coerce')\n        if cpp_default is None and pallets.notna().any() and cases.notna().any():\n            ratio = (cases / pallets).replace([np.inf, -np.inf], np.nan)\n            cpp_default = float(np.nanmedian(ratio)) if np.isfinite(np.nanmedian(ratio)) else None\n        # Compute checks\n        comparable = pallets.notna() & cases.notna() & (cases >= 0)\n        if comparable.sum() == 0:\n            return 0.0, \"No comparable rows for pallet check\"\n        def get_cpp(idx):\n            if sku_col is not None:\n                key = str(dfr.loc[idx, sku_col]).strip()\n                if key in sku_cpp:\n                    return sku_cpp[key]\n            return cpp_default\n        ok = []\n        for idx in dfr.index[comparable]:\n            cpp = get_cpp(idx)\n            c = float(cases.loc[idx])\n            p = float(pallets.loc[idx])\n            if cpp is None or cpp <= 0:\n                # cannot validate this row\n                continue\n            expected = math.ceil(c / cpp)\n            # pallets should be integer and match ceiling\n            row_ok = (abs(p - expected) < 1e-6) and (abs(p - round(p)) < 1e-6)\n            ok.append(row_ok)\n        if len(ok) == 0:\n            return 0.0, \"No rows with usable conversion to validate\"\n        score = float(np.mean(ok))\n        return score, f\"Rows passing: {int(np.sum(ok))}/{len(ok)}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Required Delivery Date vs OOS Date Logic", "description": "For SKUs with a projected out-of-stock date, ensure the required delivery date in Recommended Shipments is on/before that OOS date (where both are present).", "weight": 0.6, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output\"\n    try:\n        fp = context.files.get_path(output.id)\n        xls = pd.ExcelFile(fp)\n        sheets = xls.sheet_names\n        def pick_model_sheet(names):\n            keys = ['model','inventory','calc','calculation','forecast','coverage']\n            for n in names:\n                if any(k in n.lower() for k in keys):\n                    return n\n            return None\n        def pick_reco_sheet(names):\n            keys = ['recommend','ship','plan','proposed']\n            for n in names:\n                if any(k in n.lower() for k in keys):\n                    return n\n            return None\n        ms = pick_model_sheet(sheets)\n        rs = pick_reco_sheet(sheets)\n        if not ms or not rs:\n            return 0.0, \"Missing model or recommended shipments sheet\"\n        dfm = pd.read_excel(fp, sheet_name=ms)\n        dfr = pd.read_excel(fp, sheet_name=rs)\n        if dfm.empty or dfr.empty:\n            return 0.0, \"Empty model or recommended shipments\"\n        # Find columns\n        def find_col(df, patterns):\n            for c in df.columns:\n                cl = str(c).lower()\n                if all(p in cl for p in patterns):\n                    return c\n            return None\n        sku_m = find_col(dfm, ['sku']) or find_col(dfm, ['item']) or find_col(dfm, ['product','code'])\n        oos_m = find_col(dfm, ['out','stock']) or find_col(dfm, ['oos'])\n        sku_r = find_col(dfr, ['sku']) or find_col(dfr, ['item']) or find_col(dfr, ['product','code'])\n        req_r = find_col(dfr, ['required','date']) or find_col(dfr, ['delivery','date'])\n        if not oos_m or not req_r or not sku_m or not sku_r:\n            return 0.0, \"Missing SKU/OOS/Required Date columns\"\n        # Build OOS per SKU\n        dfm2 = dfm[[sku_m, oos_m]].copy()\n        dfm2[oos_m] = pd.to_datetime(dfm2[oos_m], errors='coerce')\n        oos_map = {}\n        for _, row in dfm2.dropna(subset=[sku_m]).iterrows():\n            sku = str(row[sku_m]).strip()\n            oos = row[oos_m]\n            if pd.notna(oos):\n                # keep earliest OOS if duplicates\n                if (sku not in oos_map) or (pd.notna(oos) and oos < oos_map[sku]):\n                    oos_map[sku] = oos\n        # Compare to required dates\n        dfr2 = dfr[[sku_r, req_r]].copy()\n        dfr2[req_r] = pd.to_datetime(dfr2[req_r], errors='coerce')\n        checks = []\n        for _, row in dfr2.dropna(subset=[sku_r, req_r]).iterrows():\n            sku = str(row[sku_r]).strip()\n            req = row[req_r]\n            oos = oos_map.get(sku, np.nan)\n            if isinstance(oos, pd.Timestamp) and pd.notna(oos):\n                checks.append(req <= oos)\n        if len(checks) == 0:\n            return 0.0, \"No comparable SKU rows with both OOS and required dates\"\n        score = float(np.mean(checks))\n        return score, f\"Rows passing: {int(np.sum(checks))}/{len(checks)}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "llm_judge", "name": "Uses Upcoming Shipments to Build Delivered Coverage", "description": "Check that the model incorporates the upcoming shipments schedule to compute delivered days of inventory or coverage through July 31 for each SKU.", "weight": 3.0, "judge_prompt": "Review the workbook, focusing on the Inventory Model and Inputs/Upcoming Shipments. Determine if the model clearly uses the upcoming shipments to compute delivered coverage (e.g., delivered days of inventory or explicit coverage through July 31). Evidence includes:\n- A calculation section or formulas referencing shipment arrival dates and quantities per SKU.\n- A timeline or logic showing how inbound quantities extend coverage.\n- Results reflected in a Delivered Days of Inventory column or a clear coverage-through-July-31 indicator.\nScoring:\n- 3.0: Clear, consistent incorporation of shipments across SKUs; logic is transparent.\n- 2.0: Generally incorporated, but some SKUs lack clarity or the linkage is partially implicit.\n- 1.0: Weak incorporation; shipments listed but not actually applied to extend coverage.\n- 0.0: No discernible use of upcoming shipments in the coverage calculations.", "expectation": "Shipments are mechanically applied to extend inventory coverage, not just listed."}, {"type": "llm_judge", "name": "Coverage Through End of July", "description": "Judge whether, for each SKU, the model either (a) shows coverage through July 31, or (b) proposes additional shipments with required dates to prevent stockouts.", "weight": 2.5, "judge_prompt": "Evaluate if the model actually ensures inventory continuity through July 31:\n- For SKUs with adequate delivered coverage: clearly shows coverage through July 31 (or beyond).\n- For SKUs at risk: Recommended Shipments table provides cases/pallets and a required delivery date that prevents the identified stockout.\n- The logic appears consistent across SKUs (no obvious missed at-risk SKUs left without recommendations).\nScoring:\n- 2.5: Consistent and complete coverage handling across SKUs.\n- 1.5: Mostly correct with a few probable misses or unclear cases.\n- 0.5: Several gaps; many at-risk SKUs not addressed.\n- 0.0: No apparent mapping from risk to recommendations.", "expectation": "Every at-risk SKU has an actionable shipment recommendation dated before stockout, or the model proves coverage."}, {"type": "llm_judge", "name": "Highlighting Compliance (Rounding and Earlier Delivery)", "description": "Confirm that the Recommended Shipments sheet visually highlights rows where pallets were rounded up and rows requiring earlier-than-scheduled delivery, with a short legend explaining the highlights.", "weight": 1.5, "judge_prompt": "Check the Recommended Shipments sheet for visual cues:\n- Rows where pallet quantities were rounded up are clearly highlighted (e.g., color fill, icons), and a note/legend explains this.\n- Rows where earlier delivery than current schedule is needed are highlighted distinctly and explained.\nScoring:\n- 1.5: Both highlight cases visible and explained with a legend.\n- 1.0: Highlights present but legend or distinction between cases is unclear.\n- 0.5: Only one of the two highlight types is present.\n- 0.0: No usable highlighting for these cases.", "expectation": "Action-driving highlights are visible and explained so the distributor knows what to prioritize."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism", "description": "Holistic assessment of presentation quality, clarity for the distributor, auditability, and practical usefulness.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Usability", "description": "Assess formatting quality, readability, filters, freeze panes, consistent units, and easy navigation for a distributor audience.", "weight": 1.5, "judge_prompt": "Evaluate professional polish and usability:\n- Clean layout, readable fonts, consistent number/date formats, clear units (cases/day).\n- Tables are well-labeled, filterable, and sorted or easy to sort; freeze panes where helpful.\n- Sheet naming is intuitive; a brief cover/notes area describes how to use the file.\nScore 1.5 for strong professional usability; 1.0 for decent; 0.5 for basic but rough; 0.0 for confusing or sloppy.", "expectation": "A distributor can quickly interpret and work with the file without confusion."}, {"type": "llm_judge", "name": "Clarity and Actionability of Recommendations", "description": "Judge whether shipment recommendations are directly actionable with clear quantities, dates, and priority signals.", "weight": 1.5, "judge_prompt": "Review the Recommended Shipments sheet. Are the recommendations directly actionable?\n- Each row has SKU, pallets, cases, and a required delivery date.\n- Priority (e.g., earlier-than-scheduled deliveries) is clear from highlights or a priority column.\n- Notes explain assumptions or lead-time context when needed.\nScoring: 1.5 excellent; 1.0 adequate; 0.5 partial; 0.0 not actionable.", "expectation": "The distributor can schedule deliveries from this sheet without further clarification."}, {"type": "llm_judge", "name": "Transparency and Auditability", "description": "Evaluate whether assumptions, data sources, and logic are documented so the distributor can audit the model.", "weight": 1.0, "judge_prompt": "Check for transparency and traceability:\n- Assumptions section includes as-of date and planning horizon (July 31) plus any lead-time or rounding rules.\n- Inputs show clear provenance (labels or notes indicating source tabs).\n- Calculations are explained briefly (comments/notes) so a reviewer could audit.\nScoring: 1.0 strong; 0.7 fair; 0.3 minimal; 0.0 none.", "expectation": "Reviewers can follow the logic from inputs to outputs without guesswork."}, {"type": "llm_judge", "name": "Risk Communication and Exception Handling", "description": "Assess how well the model communicates stockout risks, edge cases (e.g., zero rate-of-sale), and uncertainty.", "weight": 1.0, "judge_prompt": "Evaluate risk communication:\n- At-risk SKUs are clearly flagged with timing (expected OOS date) and severity.\n- Edge cases (e.g., zero rate of sale, tiny volumes) are handled explicitly (e.g., infinite days on hand or excluded with notes).\n- Any uncertainty or data gaps (missing conversions, TBD shipments) are acknowledged.\nScoring: 1.0 strong; 0.7 adequate; 0.3 weak; 0.0 absent.", "expectation": "A reader understands which SKUs are at risk, why, and what actions mitigate that risk."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "d7cfae6f-4a82-4289-955e-c799dfe1e0f4", "rubric": {"category_name": "Wholesale Trade \u2013 Cosmetics Sets Recap (Beutist)", "rationale": "Analytical task producing an Excel recap for planning. Stage 1 (LLM only) enforces a strict, self-documenting Excel shape so verification is trivial. Stage 2 mixes light code checks (math/bounds/structure) with heavier LLM checks (methodology, time windows, inclusion of supply components). Stage 3 assesses presentation quality and strategic usability for national accounts planning. Code rules are weighted ~5x less than LLM rules on average within Stage 2, per guidance.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2013 Shape Enforcement Gate", "description": "LLM-only gate verifying the delivered Excel has the exact, verifiable recap structure required to enable deterministic checks.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Excel Output Format Requirement", "description": "Check that the candidate produced a properly structured Excel recap with required sheets, tables, and fields.", "weight": 6.0, "judge_prompt": "You are evaluating whether the submission is in the correct verifiable SHAPE (structure), not the correctness of numbers.\n\nCheck the PRIMARY output file only. Requirements:\n\nFormat:\n- Must be a valid Excel workbook (.xlsx) with professional tabular layout. Not PDF/DOCX/CSV.\n\nRequired Sheets (flexible naming allowed; judge by content):\n1) Recap (e.g., \"Recap\", \"Summary Recap\", \"Summary\")\n   - A single recap table by Axis and Brand with these labeled columns (allow reasonable name variations and order):\n     \u2022 Axis (Skincare, Makeup, Fragrance)\n     \u2022 Brand (brands under each axis)\n     \u2022 YTD TY Sales ($) through 9/22/2023\n     \u2022 YTD LY Sales ($) through 9/21/2022\n     \u2022 YTD % Change\n     \u2022 Expected Sales from NOW through end of Q1 (preferably Q1 2024; accept if clearly stated as Q1 2023 but must be explicit)\n     \u2022 Inventory OH ($)\n     \u2022 Inventory OO ($)\n     \u2022 Expected Shipments October 2023 ($)\n     \u2022 Expected Shipments Q1 2024 ($)\n     \u2022 Supply vs Expected \u2013 Dollar Difference\n     \u2022 Supply vs Expected \u2013 Percent of Expected (or Coverage % / Variance %)\n     \u2022 Comments (blank placeholder)\n   - Must include totals by Axis and a Grand Total row.\n\n2) Assumptions/Methodology (e.g., \"Assumptions\", \"Methodology\", \"Notes\")\n   - Text explaining at minimum:\n     \u2022 YTD cutoff dates (TY through 9/22/2023; LY through 9/21/2022) OR clear equivalent if different\n     \u2022 Projection window (from now to end of Q1, preferably Q1 2024)\n     \u2022 Statement that projection uses set sales from Q3 2022 through Q1 2023\n     \u2022 Definitions or mapping of Axis and Brands and currency/units\n   - At least 3 sentences of explanation.\n\n3) Inventory Inputs (e.g., \"Inventory\", \"Supply Inputs\") \u2013 OPTIONAL if these inputs are clearly embedded as columns in the Recap table\n   - Table by Axis and Brand showing OH, OO, and expected shipments (Oct 2023, Q1 2024). If this sheet is missing, the Recap must clearly include these fields as columns.\n\nScoring guidance (STRUCTURE ONLY):\n- 6.0: Excel format + Recap sheet with all listed columns + Axis totals and Grand Total + Assumptions/Methodology present with required notes + Inventory Inputs either as a dedicated sheet or clearly integrated as Recap columns.\n- 4.5: Excel + Recap with all core columns but minor labeling differences or missing the optional Inventory Inputs sheet (while inputs exist as columns), and Assumptions/Methodology present.\n- 3.0: Excel + Recap present but missing 1\u20132 core fields (e.g., missing one of the shipment components or missing YTD % Change) OR Assumptions/Methodology thin (<3 sentences) though present.\n- 1.5: Excel present but recap lacks several required fields and/or no totals; Assumptions/Methodology missing.\n- 0.0: Not an Excel workbook OR no recognizable Recap by Axis/Brand.\n\nDo NOT judge calculation correctness or data reasonableness\u2014only presence and format.", "expectation": "An .xlsx workbook with a clearly labeled Recap sheet by Axis and Brand, Assumptions/Methodology text sheet, and either an Inventory Inputs sheet or the inventory inputs embedded as columns in Recap; includes axis totals and a grand total row."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness Verification", "description": "Now that the shape is enforced, verify internal consistency and correctness using code checks and higher-weight LLM reviews for methodology and inclusion logic.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Column Presence (Flexible Matching)", "description": "Check that the Recap sheet contains the expected core columns using fuzzy name matching.", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        # Find recap sheet by fuzzy match\n        recap_sheet = None\n        for s in xls.sheet_names:\n            sl = s.strip().lower()\n            if any(k in sl for k in [\"recap\",\"summary\"]):\n                recap_sheet = s\n                break\n        if recap_sheet is None:\n            # fallback to first sheet\n            recap_sheet = xls.sheet_names[0]\n        df = pd.read_excel(path, sheet_name=recap_sheet)\n        cols = [str(c).strip().lower() for c in df.columns]\n        def has_any(keys):\n            return any(any(k in c for k in keys) for c in cols)\n        required_groups = {\n            'axis': [\"axis\"],\n            'brand': [\"brand\"],\n            'ytd_ty': [\"ytd\", \"ty\"],\n            'ytd_ly': [\"ytd\", \"ly\"],\n            'ytd_pct': [\"ytd\", \"%\", \"change\", \"pct\"],\n            'expected_sales': [\"expected\", \"sales\"],\n            'oh': [\"oh\", \"on-hand\", \"on hand\"],\n            'oo': [\"oo\", \"on-order\", \"on order\"],\n            'oct_ship': [\"oct\", \"october\", \"shipment\"],\n            'q1_2024_ship': [\"q1\", \"2024\", \"shipment\"],\n            'diff_dollar': [\"diff\", \"variance\", \"$\"],\n            'diff_percent': [\"%\", \"percent\", \"coverage\"],\n            'comments': [\"comment\"],\n        }\n        found = 0\n        total = len(required_groups)\n        for k,keys in required_groups.items():\n            if has_any(keys):\n                found += 1\n        score = (found/total) * 0.5\n        return score, f\"Found {found}/{total} core column groups\"\n    except Exception as e:\n        return 0.0, f\"Error reading recap: {e}\""}, {"type": "code", "name": "YTD % Change math check", "description": "Verify YTD % Change \u2248 (YTD TY \u2013 YTD LY) / YTD LY for rows that have both values.", "weight": 0.5, "code": "import pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        recap_sheet = None\n        for s in xls.sheet_names:\n            sl = s.strip().lower()\n            if any(k in sl for k in [\"recap\",\"summary\"]):\n                recap_sheet = s\n                break\n        if recap_sheet is None:\n            recap_sheet = xls.sheet_names[0]\n        df = pd.read_excel(path, sheet_name=recap_sheet)\n        cols = {str(c).strip().lower(): c for c in df.columns}\n        def find_col(keys):\n            for k,v in cols.items():\n                if all(kw in k for kw in keys):\n                    return v\n            return None\n        ty = find_col([\"ytd\",\"ty\"]) or find_col([\"this\",\"year\"]) or find_col([\"2023\",\"ytd\"]) \n        ly = find_col([\"ytd\",\"ly\"]) or find_col([\"last\",\"year\"]) or find_col([\"2022\",\"ytd\"]) \n        pct = find_col([\"ytd\",\"%\"]) or find_col([\"ytd\",\"pct\"]) or find_col([\"ytd\",\"change\"]) \n        if not (ty and ly and pct):\n            return 0.0, \"Missing columns for YTD math\"\n        s_ty = pd.to_numeric(df[ty], errors='coerce')\n        s_ly = pd.to_numeric(df[ly], errors='coerce')\n        s_pct = pd.to_numeric(df[pct], errors='coerce')\n        mask = s_ly.notna() & (s_ly != 0) & s_ty.notna() & s_pct.notna()\n        if mask.sum() == 0:\n            return 0.1, \"Insufficient numeric rows; giving minimal partial\"\n        calc = (s_ty[mask] - s_ly[mask]) / s_ly[mask]\n        diff = (calc - s_pct[mask]).abs()\n        ok = (diff <= 0.02) | ((diff/ (calc.abs()+1e-9)) <= 0.05)\n        frac_ok = ok.mean()\n        return frac_ok * 0.5, f\"YTD %% rows within tolerance: {frac_ok:.2%}\"\n    except Exception as e:\n        return 0.0, f\"Error in YTD check: {e}\""}, {"type": "code", "name": "Supply vs Expected math check", "description": "Check that $ Difference \u2248 (OH + OO + Oct'23 Ship + Q1'24 Ship) \u2013 Expected Sales, and % \u2248 Difference / Expected (or Coverage% \u2248 Supply/Expected).", "weight": 0.5, "code": "import pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        recap_sheet = None\n        for s in xls.sheet_names:\n            sl = s.strip().lower()\n            if any(k in sl for k in [\"recap\",\"summary\"]):\n                recap_sheet = s\n                break\n        if recap_sheet is None:\n            recap_sheet = xls.sheet_names[0]\n        df = pd.read_excel(path, sheet_name=recap_sheet)\n        cols = {str(c).strip().lower(): c for c in df.columns}\n        def f(keys):\n            for k,v in cols.items():\n                if all(kw in k for kw in keys):\n                    return v\n            return None\n        expected = f([\"expected\",\"sales\"]) \n        oh = f([\"oh\"]) or f([\"on\",\"hand\"]) \n        oo = f([\"oo\"]) or f([\"on\",\"order\"]) \n        oct_ship = f([\"oct\"]) or f([\"october\"]) \n        q1_ship = None\n        # prefer explicit 2024 but accept q1 if clear\n        q1_ship = f([\"q1\",\"2024\"]) or f([\"q1\",\"ship\"]) \n        diff_col = f([\"diff\"]) or f([\"variance\"]) or f([\"over\",\"under\"]) \n        pct_col = f([\"%\"]) or f([\"percent\"]) or f([\"coverage\"]) \n        needed = [expected, oh, oo, oct_ship, q1_ship, diff_col, pct_col]\n        if sum(x is not None for x in needed) < 6:  # allow one missing (often coverage vs variance)\n            return 0.1, \"Many columns missing; minimal partial\"\n        def tonum(s):\n            return pd.to_numeric(s, errors='coerce')\n        supply = pd.Series(0.0, index=df.index)\n        for c in [oh, oo, oct_ship, q1_ship]:\n            if c is not None:\n                supply = supply.add(tonum(df[c]).fillna(0), fill_value=0)\n        expv = tonum(df[expected]) if expected else pd.Series(np.nan, index=df.index)\n        diff_true = supply - expv\n        score_parts = []\n        if diff_col is not None:\n            diff_prov = tonum(df[diff_col])\n            mask = diff_true.notna() & diff_prov.notna()\n            if mask.sum()>0:\n                ok = (diff_true[mask] - diff_prov[mask]).abs() <= (0.02*expv[mask].abs().fillna(1))\n                score_parts.append(ok.mean())\n        if pct_col is not None:\n            pct_prov = tonum(df[pct_col])\n            # Try two interpretations: variance% \u2248 diff/expected OR coverage% \u2248 supply/expected\n            with np.errstate(divide='ignore', invalid='ignore'):\n                var_true = diff_true/expv\n                cov_true = supply/expv\n            m1 = var_true.notna() & pct_prov.notna()\n            m2 = cov_true.notna() & pct_prov.notna()\n            ok1 = ((var_true[m1] - pct_prov[m1]).abs() <= 0.03).mean() if m1.sum()>0 else 0\n            ok2 = ((cov_true[m2] - pct_prov[m2]).abs() <= 0.03).mean() if m2.sum()>0 else 0\n            score_parts.append(max(ok1, ok2))\n        if not score_parts:\n            return 0.1, \"No comparable rows; minimal partial\"\n        frac = float(np.mean(score_parts))\n        return frac * 0.5, f\"Supply math consistency: {frac:.2f}\"\n    except Exception as e:\n        return 0.0, f\"Error in supply math check: {e}\""}, {"type": "code", "name": "Comments column blank", "description": "Verify the Comments placeholder column exists and is blank (or NA) across rows.", "weight": 0.3, "code": "import pandas as pd\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        recap_sheet = None\n        for s in xls.sheet_names:\n            sl = s.strip().lower()\n            if any(k in sl for k in [\"recap\",\"summary\"]):\n                recap_sheet = s\n                break\n        if recap_sheet is None:\n            recap_sheet = xls.sheet_names[0]\n        df = pd.read_excel(path, sheet_name=recap_sheet)\n        # locate comments column\n        cand = None\n        for c in df.columns:\n            if 'comment' in str(c).strip().lower():\n                cand = c\n                break\n        if cand is None:\n            return 0.0, \"No comments column\"\n        s = df[cand].astype(str).str.strip()\n        # treat NA, empty, 'nan' as blank\n        blank = (s.eq('')) | (s.str.lower().eq('nan'))\n        frac_blank = blank.mean()\n        return frac_blank * 0.3, f\"Blank comments fraction: {frac_blank:.2%}\"\n    except Exception as e:\n        return 0.0, f\"Error checking comments: {e}\""}, {"type": "code", "name": "Axis totals match brand sums", "description": "Check that Axis-level totals in Recap approximately equal the sum of their brand rows for key numeric columns.", "weight": 0.5, "code": "import pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        recap_sheet = None\n        for s in xls.sheet_names:\n            sl = s.strip().lower()\n            if any(k in sl for k in [\"recap\",\"summary\"]):\n                recap_sheet = s\n                break\n        if recap_sheet is None:\n            recap_sheet = xls.sheet_names[0]\n        df = pd.read_excel(path, sheet_name=recap_sheet)\n        cols = {str(c).strip().lower(): c for c in df.columns}\n        def f(keys):\n            for k,v in cols.items():\n                if all(kw in k for kw in keys):\n                    return v\n            return None\n        axis_col = f([\"axis\"]) \n        brand_col = f([\"brand\"]) \n        ytd_ty = f([\"ytd\",\"ty\"]) or f([\"this\",\"year\"]) \n        expected = f([\"expected\",\"sales\"]) \n        if not axis_col or not brand_col or not expected:\n            return 0.1, \"Missing axis/brand/expected columns\"\n        # Identify total rows by brand label containing 'total'\n        brand_lower = df[brand_col].astype(str).str.lower()\n        total_rows = df[brand_col][brand_lower.str.contains('total')].index\n        if len(total_rows)==0:\n            return 0.2, \"No explicit total rows found\"\n        # Compare for each axis that has a total row\n        score_parts = []\n        for idx in total_rows:\n            axis_val = df.at[idx, axis_col]\n            # sum all rows with same axis and NOT total\n            mask = (df[axis_col]==axis_val) & (~brand_lower.str.contains('total'))\n            sum_expected = pd.to_numeric(df.loc[mask, expected], errors='coerce').sum()\n            tot_expected = pd.to_numeric(df.at[idx, expected], errors='coerce')\n            if pd.notna(tot_expected) and sum_expected>0:\n                ok = abs(tot_expected - sum_expected) <= 0.02*max(1.0, sum_expected)\n                score_parts.append(1.0 if ok else 0.0)\n        if not score_parts:\n            return 0.2, \"No comparable totals; minimal partial\"\n        frac = float(np.mean(score_parts))\n        return frac * 0.5, f\"Axis totals match fraction: {frac:.2%}\"\n    except Exception as e:\n        return 0.0, f\"Error in totals check: {e}\""}, {"type": "llm_judge", "name": "Projection window and basis described correctly", "description": "Check that the Assumptions/Methodology clearly states the projection window (now through end of Q1, preferably Q1 2024) and that the method uses set sales from Q3 2022 through Q1 2023 as the basis.", "weight": 2.5, "judge_prompt": "Open the workbook and read the Assumptions/Methodology/Notes sheet. Check:\n- The projection window is clearly defined from now (late September 2023) through the end of Q1. Prefer Q1 2024; if they used Q1 2023, accept only if clearly justified and consistent across the recap.\n- The basis uses set sales from Q3 2022 through Q1 2023 to project expected sales for the window.\n- The explanation is explicit (not implied), and consistent with what the Recap table labels show.\nScoring:\n- 2.5: Projection window stated (now\u2192Q1 2024) and basis (Q3'22\u2013Q1'23 sets) clearly described and consistent.\n- 1.5: Window or basis is somewhat ambiguous but present; overall consistent.\n- 0.5: Vague mention with unclear dates or no tie to Q3'22\u2013Q1'23.\n- 0.0: No usable projection description.", "expectation": "Explicit statement like: \u201cExpected sales from 2023-09-23 through 2024-03-31 projected using Beutist set sales from Q3\u201922\u2013Q1\u201923.\u201d"}, {"type": "llm_judge", "name": "YTD cutoff dates and definitions", "description": "Confirm YTD TY/LY cutoffs and that \u201csets\u201d are defined as bundles at value price.", "weight": 2.0, "judge_prompt": "Check the Assumptions/Methodology for:\n- YTD this year cutoff at 9/22/2023 and YTD last year cutoff at 9/21/2022 (or an explicitly stated, equivalent pair of dates).\n- A brief definition of what constitutes a \u201cset\u201d for Beutist (bundles of multiple products packaged together at a value price and limited supply) or an equivalent statement.\nScoring:\n- 2.0: Both cutoffs correctly stated and a clear definition of sets.\n- 1.0: Only cutoffs or only the definition is present.\n- 0.0: Neither present.", "expectation": "Clear, dated YTD windows and a definition of sets aligned to the task brief."}, {"type": "llm_judge", "name": "Supply components included in comparison", "description": "Confirm that OH, OO, Oct\u201923 shipments, and Q1\u201924 shipments are included in the supply used for the comparison vs expected sales.", "weight": 1.5, "judge_prompt": "Inspect the Recap and any Inventory Inputs sheet. Determine if the supply used for the comparison includes:\n- On-hand (OH)\n- On-order (OO)\n- Expected shipments in October 2023\n- Expected shipments in Q1 2024\nThe recap should compute a $ difference and % of expected using these components. Scoring:\n- 1.5: All four components included and clearly part of the supply vs expected calcs.\n- 0.8: Three components included.\n- 0.3: One or two components included.\n- 0.0: Components not identifiable.", "expectation": "Supply total = OH + OO + Oct\u201923 shipments + Q1\u201924 shipments; used to compute $ and % comparison vs expected sales."}, {"type": "llm_judge", "name": "Axis totals and Grand Total present and coherent", "description": "Verify that axis-level totals and a grand total are present and appear coherent with the brand-level table.", "weight": 1.7, "judge_prompt": "Look at the Recap sheet. Confirm:\n- Totals by Axis (Skincare, Makeup, Fragrance) are labeled and present.\n- A Grand Total row exists.\n- Totals are plausibly aligned with the brand rows (no obvious omissions like missing a brand in the sum).\nScoring:\n- 1.7: Axis totals and a grand total present and clearly coherent.\n- 0.9: Totals present but labeling or placement is unclear.\n- 0.3: Only grand total or only some axis totals present.\n- 0.0: No totals.", "expectation": "Clearly labeled subtotal rows for each axis and a final grand total row on the Recap."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Presentation and Strategic Quality", "description": "LLM-only holistic assessment of clarity, usability, and planning value for national accounts.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity and readability", "description": "Assess whether the recap is easy to read and interpret for account planning.", "weight": 1.0, "judge_prompt": "Evaluate the Recap\u2019s readability: clear column labels (with date windows), frozen header row, consistent currency format, thousands separators, and logical sorting (by Axis then Brand). Are negatives and percentages formatted consistently? Scoring: 1.0 excellent readability; 0.6 adequate; 0.3 poor; 0.0 confusing.", "expectation": "Professionally formatted table with legible labels and consistent number formats."}, {"type": "llm_judge", "name": "Actionability for Q1 2024 planning", "description": "Judge whether the recap makes gaps/surpluses vs expected sales obvious and decision-ready.", "weight": 1.0, "judge_prompt": "Does the recap make it easy to see where supply is short or long (e.g., coverage %, color bands/conditional formatting, clear $ difference signs)? Would a planning manager quickly spot where to secure more sets or reallocate promotions? Scoring: 1.0 very actionable; 0.6 somewhat; 0.3 minimal; 0.0 not actionable.", "expectation": "Clear variance/coverage indicators that highlight risks/opportunities by Axis/Brand."}, {"type": "llm_judge", "name": "Audience appropriateness", "description": "Evaluate suitability for national accounts and management stakeholders.", "weight": 1.0, "judge_prompt": "Consider tone and structure: limited jargon, clear Axis/Brand breakdown, comments column left blank as requested, and concise notes in Assumptions. Scoring: 1.0 fully appropriate; 0.6 mostly; 0.3 somewhat; 0.0 inappropriate.", "expectation": "Structured for cross-functional planning reviews with minimal friction."}, {"type": "llm_judge", "name": "Documentation quality (Assumptions)", "description": "Judge the completeness and usefulness of the methodology notes.", "weight": 1.0, "judge_prompt": "Review the Assumptions/Methodology sheet: Are time windows, data sources, exclusions, and limitations stated clearly in \u22653 sentences? Is the projection link to Q3\u201922\u2013Q1\u201923 evident? Scoring: 1.0 thorough and clear; 0.6 adequate; 0.3 superficial; 0.0 missing.", "expectation": "Concise, clear methodology that enables reuse and auditability."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "6074bba3-7e3a-4b1c-b8c6-a15bb6695c3b", "rubric": {"category_name": "Real Estate CMA (Duplex) \u2014 Broker Deliverable", "rationale": "Mixed-pattern deliverable: a client-facing PDF report with embedded, structured market data and visuals. Stage 1 enforces a strict, self-documenting CMA structure that enables reliable verification. Stage 2 blends deterministic code checks (counts, ordering, presence of key facts) with LLM cross-checks (appropriateness of comps, graph alignment, rationale coherence). Stage 3 assesses professional quality, clarity, and strategic value for the seller. Code rules use the new context.files API and are defensive with try/except and flexible matching.", "max_total_score": 18.0, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate (PDF CMA)", "description": "This gate enforces an exact, verifiable CMA structure in a single PDF report so Stage 2 can verify correctness. No code rules here \u2014 LLM-only, as it can see layout, tables, and charts.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 2.0, "rules": [{"type": "llm_judge", "name": "CMA PDF Structure Requirements", "description": "Check the candidate output is a professionally formatted CMA PDF with the mandated sections, tables, and charts.", "weight": 4.0, "judge_prompt": "You are evaluating whether the submitted output is a single CMA PDF with the REQUIRED structure for a duplex at 112 Pine Crest Ln, Adairsville, GA 30103. Only check for PRESENCE and STRUCTURE (not correctness of numbers). Be flexible with section titles if clearly equivalent.\n\nRequired format:\n- File type: PDF (not Word/Excel/plain text)\n- Professional report layout (clear headings, tables, and charts) \u2014 at least 3 pages\n\nRequired sections and artifacts (names can vary slightly but must be clearly equivalent):\n1) Cover or Title area on page 1:\n   - Subject address: \"112 Pine Crest Ln, Adairsville, Georgia 30103\"\n   - Property type noted as duplex or 2-unit\n2) Subject Property Summary section:\n   - Location, size, use, total bedrooms/bathrooms (should state 4 bed / 2 bath total)\n   - Lease/occupancy notes (e.g., tenant status or owner-occupancy) if known\n3) Comparable Sales section (5\u201310 sales):\n   - Tabular presentation with rows for each sale\n   - Columns should generally cover: Address, Proximity/Distance, Property Type/Units (duplex/2\u20134 units), Beds/Baths or Unit mix, Living Area (SF), Sale Date, List Price, Sale Price, $/SF (or similar), DOM, Notes/Adjustments, Data Source\n4) Active or Pending Listings section (3\u20135 listings):\n   - Tabular presentation similar to sales comps\n   - Include status (Active/Pending), List Price, $/SF (or similar), DOM\n5) Valuation Summary and Recommendation:\n   - Tiered pricing range table with Low / Mid / High and a clear recommended list price/tier\n   - Brief rationale referencing comps\n6) Charts/Graphs (both required):\n   - List Price vs Sales Price chart (e.g., scatter or bar) based on comps\n   - Days on Market graph (e.g., histogram/line) referencing the same market set\n7) Methodology & Data Sources section:\n   - Notes on sources (Zillow/Redfin/Realtor/Homes.com/public records) and approach/assumptions\n8) Appendix or Supporting Detail (optional but preferred):\n   - Any raw comp printouts, maps, or additional charts\n\nScoring (0 to 4):\n- 4.0: PDF format with all required items (1\u20137) clearly present; appendix/supporting detail present or not (optional).\n- 3.2: PDF format with all required sections except ONE minor omission (e.g., missing one of the two graphs OR 1\u20132 missing non-critical columns in a table) \u2014 tables still present with 5\u201310 sales and 3\u20135 actives.\n- 2.4: PDF format with significant omissions but core CMA present (both comp tables AND valuation section present, but missing one major element like graphs or cover page details).\n- 1.2: PDF present but structure is weak (only one of the two comp tables OR no valuation summary) \u2014 minimal CMA form.\n- 0.0: Not a PDF OR lacks core CMA structure (no comps table, no valuation, or entirely unstructured).\n\nReturn a score from 0 to 4 based on the above. Do not assess numerical correctness here.", "expectation": "A multi-page CMA PDF with clear subject summary, two comps tables with correct row counts, valuation range table, and two charts."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification of Content and Internal Consistency", "description": "Now that the structure is validated, verify factual inclusions and internal consistency. Mix deterministic code checks with LLM judgment for nuanced validation. Code rules have lower aggregate weight than LLM rules.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Subject Property Core Facts Present", "description": "Verify the PDF text includes the exact subject address, duplex designation, and total beds/baths (4 bed / 2 bath).", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        text = ''\n        if output.file_extension.lower().endswith('.pdf'):\n            text = context.files.read_pdf_text(output.id)\n        elif output.file_extension.lower().endswith('.docx'):\n            text = context.files.read_docx_text(output.id)\n        else:\n            return 0.0\n        if not text:\n            return 0.0\n        t = text.lower()\n        # Address presence\n        addr_ok = ('112 pine crest' in t) and ('adairsville' in t) and ('30103' in t)\n        # Duplex designation\n        duplex_ok = any(k in t for k in ['duplex', '2-unit', 'two-unit', '2 unit'])\n        # Beds/baths presence (allow variants like 4bd/2ba)\n        bed_ok = bool(re.search(r'(4\\s*(bed|bedroom)s?|\\b4\\s*bd\\b)', t))\n        bath_ok = bool(re.search(r'(2\\s*(bath|bathroom)s?|\\b2\\s*ba\\b)', t))\n        bb_ok = bed_ok and bath_ok\n        score = (addr_ok + duplex_ok + bb_ok) / 3.0\n        return float(score)\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Comparable Sales Count (5\u201310) Detected", "description": "Count address-like rows within the Comparable Sales section. Partial credit if near target.", "weight": 0.4, "code": "import re\n\ndef _section_text(t, start_keys, end_keys):\n    start = -1\n    for k in start_keys:\n        i = t.find(k)\n        if i != -1:\n            start = i\n            break\n    if start == -1:\n        return ''\n    end = len(t)\n    for k in end_keys:\n        j = t.find(k, start+1)\n        if j != -1:\n            end = min(end, j)\n    return t[start:end]\n\ndef _count_addresses(t):\n    # Heuristic address matcher for US street types\n    pat = re.compile(r'\\b\\d{2,5}\\s+[A-Za-z0-9\\.\\'\\-]+(?:\\s+[A-Za-z0-9\\.\\'\\-]+)*\\s+(St|Street|Rd|Road|Ave|Avenue|Blvd|Boulevard|Ln|Lane|Dr|Drive|Ct|Court|Way|Pkwy|Parkway)\\b', re.I)\n    return len(set(m.group(0) for m in pat.finditer(t)))\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        if output.file_extension.lower().endswith('.pdf'):\n            raw = context.files.read_pdf_text(output.id) or ''\n        elif output.file_extension.lower().endswith('.docx'):\n            raw = context.files.read_docx_text(output.id) or ''\n        else:\n            return 0.0\n        t = raw.lower()\n        section = _section_text(\n            t,\n            start_keys=['comparable sales', 'sales comparables', 'sold comparables', 'closed sales'],\n            end_keys=['active listings', 'active/pending', 'pending listings', 'valuation', 'pricing', 'summary']\n        )\n        if not section:\n            return 0.0\n        cnt = _count_addresses(section)\n        if 5 <= cnt <= 10:\n            return 1.0\n        if 3 <= cnt <= 12:\n            return 0.5\n        return 0.0\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Valuation Range Monotonicity (Low \u2264 Mid \u2264 High)", "description": "Detect low/mid/high valuation amounts and ensure they are in non-decreasing order.", "weight": 0.4, "code": "import re\n\ndef _find_amount_near(label, t, window=80):\n    out = []\n    for m in re.finditer(label, t):\n        s = max(0, m.start()-window)\n        e = min(len(t), m.end()+window)\n        chunk = t[s:e]\n        for num in re.findall(r'\\$?\\s*([0-9]{2,3}(?:[,0-9]{0,3})*(?:\\.\\d{2})?)', chunk):\n            try:\n                val = float(num.replace(',', ''))\n                if val > 0:\n                    out.append(val)\n            except:\n                pass\n    return out\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        if output.file_extension.lower().endswith('.pdf'):\n            raw = context.files.read_pdf_text(output.id) or ''\n        elif output.file_extension.lower().endswith('.docx'):\n            raw = context.files.read_docx_text(output.id) or ''\n        else:\n            return 0.0\n        t = raw.lower()\n        # Try to limit to valuation section if present\n        start = -1\n        for k in ['valuation', 'pricing', 'recommendation', 'summary valuation']:\n            i = t.find(k)\n            if i != -1:\n                start = i\n                break\n        if start != -1:\n            t = t[start: start + 5000]\n        lows = _find_amount_near('low', t)\n        mids = _find_amount_near('mid', t) + _find_amount_near('midpoint', t) + _find_amount_near('target', t)\n        highs = _find_amount_near('high', t)\n        # Pick first seen per label if available\n        vals = []\n        if lows:\n            vals.append(('low', lows[0]))\n        if mids:\n            vals.append(('mid', mids[0]))\n        if highs:\n            vals.append(('high', highs[0]))\n        if len(vals) < 2:\n            return 0.0\n        # Check ordering based on label sequence low->mid->high\n        label_map = {'low':0, 'mid':1, 'high':2}\n        present = sorted(vals, key=lambda x: label_map.get(x[0], 99))\n        ordered = True\n        for i in range(len(present)-1):\n            if present[i][1] > present[i+1][1]:\n                ordered = False\n                break\n        if ordered:\n            return 1.0\n        # Partial if at least non-decreasing for any adjacent pair\n        partial = False\n        for i in range(len(present)-1):\n            if present[i][1] <= present[i+1][1]:\n                partial = True\n        return 0.5 if partial else 0.0\n    except Exception:\n        return 0.0"}, {"type": "llm_judge", "name": "Comparable Set Relevance and Recency", "description": "Are the selected comps appropriate (duplex/2\u20134 unit, nearby submarket) and recent (ideally 6\u201312 months)?", "weight": 2.8, "judge_prompt": "Evaluate the appropriateness of the comparable set in the CMA PDF:\n- Property similarity: duplex or 2\u20134 unit, similar size/mix and condition where stated\n- Geography: immediate submarket or nearby areas reasonably comparable to Adairsville, GA\n- Recency: majority sold within last 6\u201312 months (flexible if inventory is thin but should be justified)\n- Table completeness: key fields like Sale Date, List/Sale Price, $/SF, DOM present for most comps\nScoring (0\u20132.8):\n- 2.8: Comps are clearly similar duplex/2\u20134 units, near subject, mostly within 6\u201312 months, tables complete\n- 1.8: Generally similar and nearby with a few older or less similar comps; reasonable justification\n- 0.9: Several comps are dissimilar or distant; weak time relevance; tables sparse\n- 0.0: Comps largely irrelevant, far, or very outdated; no justifications", "expectation": "A comp set that would withstand client and appraiser scrutiny for a duplex CMA."}, {"type": "llm_judge", "name": "Charts Presence and Data Alignment", "description": "Confirm the List Price vs Sales Price chart and Days on Market chart exist and appear to reflect the same comps/listings used in tables.", "weight": 2.0, "judge_prompt": "Check charts in the PDF for both requirements:\n1) A List Price vs Sales Price chart (scatter, line, or bars)\n2) A Days on Market (DOM) chart\nAssess alignment with tabular comps/listings: labels, counts, and ranges should make sense relative to the tables. Axes should be labeled and values plausible for the market context. Note any obvious mismatches (e.g., chart counts not matching comp counts).\nScoring (0\u20132.0):\n- 2.0: Both charts present, labeled, and consistent with the comps/listings in count and scale\n- 1.2: Both present but one has minor inconsistencies or missing labels\n- 0.6: Only one chart present, or both present but poorly labeled/incongruent\n- 0.0: Charts missing or irrelevant to the included comps/listings", "expectation": "Two charts present and consistent with the listed comps/listings."}, {"type": "llm_judge", "name": "Valuation Rationale and Recommendation Coherence", "description": "Evaluate whether the low/mid/high valuation range and recommendation are well-supported by the evidence.", "weight": 2.0, "judge_prompt": "Review the Valuation Summary/Recommendation section:\n- Is there an explicit low/mid/high range and a specific recommended list price?\n- Does the narrative rationalize the range using the comps (e.g., averages/medians, $/SF range, adjustments for condition/location/DOM)?\n- Is the recommended price within the stated range and consistent with charts and tables?\n- Does the timing (30\u201360 days) align with the DOM evidence and suggested pricing tier?\nScoring (0\u20132.0):\n- 2.0: Clear range and recommendation, well-justified with metrics and comps; time-to-market guidance aligns with DOM and pricing tier\n- 1.2: Range and recommendation present with some justification; minor inconsistencies\n- 0.6: Range/recommendation present but weakly supported or inconsistent with comps\n- 0.0: Range or recommendation missing or clearly unsupported", "expectation": "A defensible price recommendation with coherent, data-backed rationale."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Client Readiness", "description": "Holistic quality assessment of the CMA as a client-facing deliverable: presentation, clarity, credibility, and strategic usefulness.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Presentation and Template Adherence", "description": "Professional formatting, clean tables/charts, readable typography, and consistent use of the CMA template.", "weight": 1.5, "judge_prompt": "Assess visual and structural professionalism: consistent styles, readable tables and charts, logical flow, page headers/footers, and brand/template consistency. Minimal typos and correct units/labels.\nScoring (0\u20131.5):\n- 1.5: Highly professional, consistent, minimal errors\n- 0.9: Generally professional with a few minor issues\n- 0.4: Noticeable inconsistencies or readability issues\n- 0.0: Poorly formatted or difficult to read", "expectation": "Client-ready, polished presentation suitable for ownership."}, {"type": "llm_judge", "name": "Market Insight and Strategic Guidance", "description": "Quality of interpretation, pricing strategy, and seller guidance for a 30\u201360 day listing horizon.", "weight": 1.5, "judge_prompt": "Evaluate the strength of market insight and strategy: Does the CMA interpret signals (e.g., list-to-sale ratios, $/SF bands, DOM distribution, seasonality)? Are trade-offs articulated (pricing aggressively vs. exposure time)? Are negotiation expectations or contingencies (repairs, concessions) addressed appropriately for a duplex?\nScoring (0\u20131.5):\n- 1.5: Insightful and actionable strategy tailored to the duplex and local submarket\n- 0.9: Some useful insight with limited tailoring\n- 0.4: Generic advice with minimal strategic value\n- 0.0: No meaningful strategic guidance", "expectation": "Actionable strategy tied to evidence and seller goals."}, {"type": "llm_judge", "name": "Data Credibility and Citations", "description": "Transparency of data sources and reproducibility.", "weight": 1.5, "judge_prompt": "Assess whether public/third-party sources are cited (Zillow/Redfin/Realtor/Homes.com/public records), dates are indicated, and any adjustments are explained sufficiently to reproduce the logic.\nScoring (0\u20131.5):\n- 1.5: Clear citations and dates; methodology transparent and reproducible\n- 0.9: Sources cited but limited detail\n- 0.4: Sparse or unclear sourcing\n- 0.0: No credible sources cited", "expectation": "Credible sourcing and transparent methodology."}, {"type": "llm_judge", "name": "Clarity, Risks, and Next Steps", "description": "Communication quality and usefulness for client decision-making.", "weight": 1.5, "judge_prompt": "Evaluate clarity of writing and whether the CMA provides explicit next steps, key risks/assumptions, and decision points (e.g., prep work, pricing triggers, timeline). Consider whether a non-expert owner could act on this document.\nScoring (0\u20131.5):\n- 1.5: Clear, concise, with explicit next steps and risk framing\n- 0.9: Mostly clear with some gaps in guidance\n- 0.4: Hard to follow or missing actionable guidance\n- 0.0: Unclear and not decision-useful", "expectation": "Clear recommendations with risks and actionable next steps."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "8077e700-2b31-402d-bd09-df4d33c39653", "rubric": {"category_name": "Materials Heat-Treatment Analysis Report (Quench + Temper) \u2014 AISI 1018 & 1045", "rationale": "This rubric enforces a self-documenting, mixed-output pattern: a PDF report with embedded, verifiable figures/tables derived from provided lab data. Stage 1 is an LLM-only structural gate ensuring the PDF has the exact sections and artifacts needed for verification. Stage 2 mixes lightweight code checks (presence/parameters/figures) with richer LLM judgment on correctness and metallurgical plausibility, weighted ~5x more for LLM rules. Stage 3 assesses professional quality, clarity, and actionability for engineering stakeholders.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Structure and Format Gate (LLM-only)", "description": "Enforce exact document structure and required artifacts to enable verification. This is a hard gate: if missing, the entire category is zeroed.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.0, "rules": [{"type": "llm_judge", "name": "PDF Structure, Sections, and Required Artifacts", "description": "Verify the output is a professionally formatted PDF with the exact sections and embedded artifacts needed for subsequent verification of quench-temper analysis.", "weight": 3.0, "judge_prompt": "You are evaluating whether the candidate output satisfies strict STRUCTURE requirements for a professional engineering report. Only judge presence/format, not content quality.\n\nCheck the following for the single primary output:\n\nFormat requirements:\n- The output must be a PDF (not Word/Excel/plain text).\n- Minimum length: 3 pages.\n- Professional layout with clear section headers and numbered figures/tables.\n\nRequired sections (flexible naming allowed, but meaning must match):\n1) Introduction\n2) Objectives\n3) Experimental Procedure (or Methodology)\n4) Results\n5) Analysis (or Discussion)\n6) Recommendation(s)\n7) Conclusion\n8) Figures and Data (or Figures and Tables, or Figures & Data Descriptions) \u2014 a dedicated section listing figures/tables with captions.\n\nRequired embedded artifacts and elements:\nA) At least two graphs/figures plotting Rockwell HRF vs. soak time for each alloy separately (AISI 1018 at 240\u00b0C; AISI 1045 at 285\u00b0C). Each figure should have labeled axes and units.\nB) At least one table summarizing treatment windows, with columns equivalent to: [Alloy | Temper Temperature (\u00b0C) | Soak Time Range (min) | Peak HRF | Time to Peak (min) | Notes]. Flexible column naming is acceptable if the meaning is clear.\nC) Explicit textual references to both alloys (AISI 1018 and AISI 1045) and to their respective tempering temperatures (approximately 240\u00b0C and 285\u00b0C) within the Procedure/Results/Analysis.\nD) Cross-references to figures/tables inside the text (e.g., \u201csee Figure 1\u201d, \u201cTable 1\u201d).\nE) A brief acknowledgment that direct microstructure images are not provided and that microstructural interpretations are inferred from hardness trends and known metallurgy (can appear in Analysis or Limitations).\nF) A short statement that the work is based on provided laboratory data (e.g., Data.xlsx) and the work request/scope (e.g., Work Request MATL LAB.pdf). Exact file names may vary but the intent must be clear.\n\nScoring (0\u20133 points total):\n- 3.0: PDF format, \u22653 pages, all required sections present, and artifacts A\u2013F all present.\n- 2.5: PDF, \u22653 pages, all sections present, missing exactly one artifact from A\u2013F.\n- 2.0: PDF, \u22653 pages, at least 6 of 8 required sections present AND at least artifacts A and B present.\n- 1.0: PDF, but fewer than 6 sections or missing both A and B.\n- 0.0: Not a PDF, or extremely incomplete structure.\nReturn a single numeric score according to this scale.\n", "expectation": "A complete, cleanly structured PDF report with all mandated sections, figures, and a treatment window summary table, enabling downstream verification."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification of Technical Correctness and Consistency", "description": "Now that the structure is correct, verify technical substance: presence of key parameters, internal consistency, trend identification, and metallurgical plausibility. Mix of code and LLM rules with LLM weighted ~5x more.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Key Terms and Parameters Present (PDF text scan)", "description": "Detect presence of core domain terms and required parameters in the PDF text: Rockwell HRF, the two alloys (AISI 1018/1045), quenching and tempering terminology, and the two temper temperatures (~240\u00b0C and ~285\u00b0C).", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception as e:\n        return 0.0, f\"Failed to read PDF text: {e}\"\n    if not text:\n        return 0.0, \"Empty PDF text.\"\n    t = text.lower()\n    # Patterns for alloys (allow AISI/SAE and spacing/hyphens)\n    patterns = {\n        'rockwell_hrf': r\"rockwell\\s*(h\\s*r\\s*f|hrf)\",\n        'quench': r\"quench\",\n        'temper': r\"temper\",\n        'aisi_1018': r\"(aisi|sae)\\s*[- ]?1018\",\n        'aisi_1045': r\"(aisi|sae)\\s*[- ]?1045\",\n        'temp_240': r\"240\\s*(\u00b0\\s*c|deg\\s*c|c|\\u00b0\\s*c)\",\n        'temp_285': r\"285\\s*(\u00b0\\s*c|deg\\s*c|c|\\u00b0\\s*c)\",\n    }\n    found = {}\n    for k, pat in patterns.items():\n        found[k] = re.search(pat, t) is not None\n    keys = list(patterns.keys())\n    score = sum(1 for k in keys if found[k]) / len(keys)\n    return 0.5 * score, f\"Found: {[k for k in keys if found[k]]}; Missing: {[k for k in keys if not found[k]]}\""}, {"type": "code", "name": "Figures and Tables Referenced (counts)", "description": "Check that the PDF text references at least two figures and at least one table, enabling cross-verification.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception as e:\n        return 0.0, f\"Failed to read PDF text: {e}\"\n    if not text:\n        return 0.0, \"Empty PDF text.\"\n    t = text.lower()\n    # Count references like \"Figure 1\", \"Fig. 2\", \"Table 1\"\n    fig_refs = re.findall(r\"\\b(fig\\.|figure)\\s*\\d+\", t)\n    tab_refs = re.findall(r\"\\btable\\s*\\d+\", t)\n    figs = len(fig_refs)\n    tabs = len(tab_refs)\n    # Scoring: full if >=2 figures and >=1 table; partial otherwise.\n    if figs >= 2 and tabs >= 1:\n        return 0.5, f\"Figure refs: {figs}, Table refs: {tabs}\"\n    partial = 0.25 if (figs >= 1 and tabs >= 1) or (figs >= 2) else 0.0\n    return partial, f\"Figure refs: {figs}, Table refs: {tabs}\""}, {"type": "llm_judge", "name": "Trend Identification and Time-to-Peak Hardness", "description": "Evaluate whether the report correctly identifies and explains the time-to-peak hardness for both alloys and ties it to efficient treatment windows, with clear references to the plotted HRF vs time curves and the summary table.", "weight": 1.5, "judge_prompt": "Review the PDF report\u2019s Results/Analysis/Recommendations sections and the HRF vs time figures.\nAssess the following:\n1) For AISI 1018 (tempered ~240\u00b0C) and AISI 1045 (tempered ~285\u00b0C), does the report explicitly identify the time-to-peak hardness for each alloy? Is this shown or annotated on the figures and also stated in text?\n2) Are the identified times reflected consistently in the Treatment Window Summary table (or equivalent) with plausible soak ranges around the peak and a stated Peak HRF value?\n3) Do the recommendations use these times/ranges to suggest efficient treatment windows (balancing hardness vs time/efficiency)?\nScoring (0\u20131.5):\n- 1.5: Both alloys have clearly identified time-to-peak, consistent between figures, text, and table; recommendations align with these windows.\n- 1.0: Both alloys identified but minor inconsistencies or missing annotation (either figure or table);\n- 0.5: Only one alloy properly treated, or major inconsistency across artifacts;\n- 0.0: No meaningful identification of time-to-peak or windows.", "expectation": "Clear, cross-referenced time-to-peak identification for both alloys, aligned across figures, tables, and recommendations."}, {"type": "llm_judge", "name": "Metallurgical Reasoning Plausibility", "description": "Judge whether the metallurgical explanations are reasonable for quench + temper behavior of low/medium carbon steels, without claiming unavailable micrographs.", "weight": 1.5, "judge_prompt": "Evaluate the metallurgical explanations in Analysis/Discussion:\n- Do they correctly describe expected quench + temper behavior for AISI 1018 (low carbon) vs AISI 1045 (medium carbon)? Examples: 1045 forms martensite on quench and tempers toward tempered martensite with hardness generally decreasing with temper time/temperature; 1018 has limited martensitic hardenability and thus lower hardness response; mention of carbide precipitation during low-temperature tempering is acceptable; no claims of secondary hardening typical of alloy steels unless justified.\n- Do they avoid asserting direct microstructural evidence (since micrographs are not provided) and instead frame microstructural inferences appropriately (e.g., likely phases, grain refinement effects during prior processing, tempering transformations)?\n- Are statements directionally consistent with HRF trends shown (e.g., not claiming hardness increases with extended tempering time if curves show decrease)?\nScoring (0\u20131.5):\n- 1.5: Technically sound explanations for both alloys, appropriately cautious about missing micrographs, and consistent with plotted trends.\n- 1.0: Mostly correct with minor overstatements or slight inconsistencies.\n- 0.5: Partially correct but with notable misconceptions or confusing/conflicting statements.\n- 0.0: Largely incorrect metallurgy or claims unsupported by the figures.", "expectation": "Sound, appropriately cautious metallurgical reasoning consistent with plain carbon steel quench/temper behavior and observed HRF trends."}, {"type": "llm_judge", "name": "Internal Consistency Across Text, Figures, and Tables", "description": "Check that numerical claims and qualitative statements match the visible data in figures/tables, including temperatures and alloy identification.", "weight": 1.0, "judge_prompt": "Scan the report for internal consistency:\n- Do references to temperatures (~240\u00b0C for 1018; ~285\u00b0C for 1045) and alloy names match across sections, figures, and tables?\n- Do the reported peak hardness values and times align between text, figures (annotations/curve maxima), and the summary table?\n- Are units and axis labels consistent and correctly used for HRF and time (e.g., minutes)?\nScoring (0\u20131.0):\n- 1.0: All cross-references and values are consistent and properly labeled.\n- 0.6: Minor inconsistencies or labeling issues.\n- 0.3: Multiple inconsistencies but core story still decipherable.\n- 0.0: Major contradictions or missing labels that prevent verification.", "expectation": "Numerical and naming consistency across all parts; correctly labeled axes and units."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Overall Quality and Communication", "description": "Holistic quality assessment: professionalism, clarity, and decision usefulness for engineering stakeholders.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Readability", "description": "Assess formatting, figures/tables legibility, and overall readability as a professional lab report.", "weight": 0.5, "judge_prompt": "Evaluate presentation quality:\n- Are figures/tables clearly labeled, legible, and properly captioned? Do they follow a consistent numbering scheme?\n- Is the document well-formatted (headings, spacing, page numbers) and easy to navigate?\n- Are grammar and style at a professional standard?\nScore 0\u20130.5 based on overall professionalism and readability.", "expectation": "Clear, professional layout with legible figures/tables and minimal grammatical issues."}, {"type": "llm_judge", "name": "Actionability of Recommendations", "description": "Judge whether recommendations give concrete treatment windows and expected outcomes useful for manufacturing decisions.", "weight": 0.5, "judge_prompt": "Assess the Recommendations section:\n- Does it propose concrete treatment windows (time ranges) for each alloy at the specified tempering temperatures?\n- Are expected HRF ranges or targets provided, along with rationale (e.g., balancing hardness vs risk of embrittlement/fatigue performance)?\n- Are caveats and limitations (e.g., geometry, section thickness, quench medium) noted where applicable?\nScore 0\u20130.5 based on specificity, practicality, and decision usefulness.", "expectation": "Specific, practical windows and expected HRF ranges with rationale and relevant caveats."}, {"type": "llm_judge", "name": "Audience Appropriateness and Clarity", "description": "Ensure explanations suit mechanical/materials engineering stakeholders, defining specialized terms as needed.", "weight": 0.5, "judge_prompt": "Evaluate audience fit:\n- Are technical terms briefly defined or used appropriately for an engineering audience?\n- Is the narrative concise, logically structured, and aligned to stakeholder needs (reliability under fatigue/high-impact loads)?\n- Are claims tied back to the work request/scope?\nScore 0\u20130.5 based on clarity and audience appropriateness.", "expectation": "Concise, well-structured communication tailored to engineering decision-makers."}, {"type": "llm_judge", "name": "Transparency, Assumptions, and Limitations", "description": "Assess whether the report clearly states assumptions, data sources, and limitations (e.g., lack of micrographs).", "weight": 0.5, "judge_prompt": "Check transparency:\n- Are data sources acknowledged (e.g., provided lab spreadsheet) and any data cleaning/aggregation steps described?\n- Are key assumptions and limitations (no microstructure, possible measurement scatter, specimen geometry effects) stated?\n- Is there a brief note on reproducibility (e.g., how figures/tables were generated from the data)?\nScore 0\u20130.5 based on completeness and clarity of transparency elements.", "expectation": "Clear acknowledgment of data sources, assumptions, limitations, and how results were produced."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "a1963a68-1bea-4bb1-b7e0-145c92a57449", "rubric": {"category_name": "Deep Dive Strategy Presentation Evaluation - SuperK-T Korea", "rationale": "This rubric enforces a self-documenting, presentation-first workflow. Stage 1 forces a very specific PDF slide structure so verification is trivial. Stage 2 mixes lightweight code checks (text extraction, numeric/sourcing/timeline presence) with LLM judges for nuanced factuality, feasibility, and regulatory soundness. Stage 3 holistically assesses executive usefulness, clarity, localization, and professional polish. Code rules carry lower weight than LLM rules to reflect their narrower scope.", "max_total_score": 25.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (Structure & Format)", "description": "LLM-only gate to ensure the deliverable is a properly structured PDF slide deck with required sections to allow subsequent verification.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.1, "rules": [{"type": "llm_judge", "name": "Structured Presentation Requirements (PDF Slides)", "description": "Validate that the output is a slide-style PDF with the exact structural elements required for verification.", "weight": 3.0, "judge_prompt": "You are verifying STRUCTURE ONLY (not content quality). Review the candidate output and check the following strictly but with reasonable flexibility on headings:\n\nFormat Requirements:\n- File must be a PDF (not DOCX/Excel/Text).\n- Looks like a presentation: slide pages with titles and bullet points (not a prose report).\n- Total slide count should typically be 8\u201312 pages to accommodate: cover, 5\u20136 core content slides, Q&A, and at least one appendix.\n\nRequired Slides/Sections (allow combining topics across slides as long as all areas are clearly present across the 5\u20136 core slides):\n1) Cover Page: Title indicating SuperK-Taxi Korea deep dive strategy, date around May 2024, and an author/role (e.g., Head of Strategy).\n2) Market Reality & Strategic Imperatives: candid assessment of current position, dominant competitor (e.g., Kakao T), key challenges/opportunities.\n3) Core Growth & Operational Excellence Plan (Supply): concrete plan to boost vehicle/driver supply.\n4) Korean User Experience & Demand Strategy: localization and priority segments to win demand.\n5) Regulatory Landscape & Partnership Plan: overview of KR regulatory context and approach with associations.\n6) H2 2024 Action Plan & KPIs: August\u2013December 2024 milestones, owners, and measurable targets.\n7) Future-Proofing SuperK-Taxi: innovation and sustainability pathways for long-term leadership.\n8) Q&A slide.\n9) Appendix: \u201cData & Sources\u201d slide listing multiple citations (ideally including Korean authorities like the Korea Transportation Safety Authority and the Korea National Joint Conference of Taxi Associations) and any supporting data or assumptions.\n\nScoring (return a score from 0 to the rule weight):\n- 3.0: PDF; slide-like; 5\u20136 core slides present; ALL required areas covered across slides; Q&A; Appendix with sources.\n- 2.4: PDF; slide-like; 5\u20136 core slides; ONE minor element missing (e.g., Q&A or Appendix lightly present without clear sources) but core areas covered.\n- 1.8: PDF; slide-like; missing ONE required area OR only 4 core content slides but others present.\n- 1.0: PDF but significant structural gaps (e.g., prose report style, fewer than 7 total pages, multiple required areas missing).\n- 0.0: Not a PDF OR grossly wrong format/structure.\nDo not judge correctness or quality\u2014only presence/structure.", "expectation": "A PDF slide deck with cover, 5\u20136 clearly titled core slides covering market reality, supply, demand/UX, regulatory plan, H2 2024 action plan & KPIs, future-proofing; plus Q&A and an Appendix with sources."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Evidence, Consistency, and Feasibility)", "description": "Now that the shape is correct, verify the presence of evidence, timelines, sourcing, and logical consistency. Code rules focus on text/number/sourcing checks; LLM judges assess factuality, feasibility, and regulatory realism.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Timeline & H2 2024 Focus Evidenced in Text", "description": "Check PDF text mentions H2 2024 timing and concrete months/quarters (e.g., August 2024, Q3/Q4 2024).", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = ''\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = ''\n    if not text:\n        return 0.0\n    t = text.lower()\n    tokens = [\n        'h2 2024','2h 2024','2h24','h2-2024','h2/2024',\n        'q3 2024','q4 2024','q3-2024','q4-2024',\n        'aug 2024','august 2024','sep 2024','september 2024','oct 2024','october 2024','nov 2024','november 2024','dec 2024','december 2024',\n        'from august 2024','starting august 2024','h2'\n    ]\n    hits = sum(1 for tok in tokens if tok in t)\n    # Score: 0 if none; 50% if 1 hit; 100% if >=2 distinct signals\n    ratio = 1.0 if hits >= 2 else (0.5 if hits == 1 else 0.0)\n    return ratio * 0.6\n"}, {"type": "code", "name": "Quantitative Rigor \u2014 Presence of Metrics/Numbers", "description": "Check for presence of multiple numeric figures and business metrics (%, KRW/\u20a9, counts).", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = ''\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = ''\n    if not text:\n        return 0.0\n    t = text\n    # Find numbers (integers with optional commas or decimals)\n    nums = re.findall(r\"(?<![A-Za-z])[-+]?\\b\\d{1,3}(?:[,\\s]\\d{3})*(?:\\.\\d+)?\\b\", t)\n    perc = len(re.findall(r\"%\", t))\n    currency = len(re.findall(r\"(KRW|\u20a9|won|USD|$)\", t, re.IGNORECASE))\n    metrics_terms = ['kpi','dau','mau','eta','take rate','conversion','retention','churn','completed trips','bookings']\n    m_hits = sum(1 for m in metrics_terms if m in t.lower())\n    # Heuristic scoring\n    score = 0.0\n    if len(nums) >= 10:\n        score += 0.25\n    if perc >= 3:\n        score += 0.2\n    if currency >= 1:\n        score += 0.1\n    if m_hits >= 2:\n        score += 0.25\n    # Cap at 1.0 then scale by weight\n    score = min(score, 1.0)\n    return score * 0.6\n"}, {"type": "code", "name": "Citations & Source Signals Present", "description": "Verify presence of multiple sources, ideally including Korean authorities and associations, and general URL citations.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = ''\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = ''\n    if not text:\n        return 0.0\n    t = text.lower()\n    url_count = len(re.findall(r\"https?://\", t))\n    authority_terms = [\n        'korea transportation safety authority', 'kotsa',\n        'korea national joint conference of taxi associations',\n        'taxi association', 'molit', 'ministry of land',\n        'statistics korea', 'bank of korea', 'kakao mobility', 'kakao t'\n    ]\n    hits = sum(1 for a in authority_terms if a in t)\n    # Require at least 2 distinct authority/source hits OR >=2 URLs\n    ratio = 1.0 if (hits >= 2 or url_count >= 2) else (0.5 if (hits == 1 or url_count == 1) else 0.0)\n    return ratio * 0.8\n"}, {"type": "llm_judge", "name": "Market Reality & Competitive Dynamics Soundness", "description": "Assess whether the market assessment is grounded, names the dominant competitor (Kakao T), quantifies challenges/opportunities, and aligns with public info circa May 2024.", "weight": 2.5, "judge_prompt": "Evaluate the slide content for factual soundness of the market reality and strategic imperatives:\n- Identifies the dominant competitor in KR ride-hailing (Kakao T) and describes competitive dynamics.\n- Provides concrete, sourced figures where appropriate (e.g., market share, active drivers, usage patterns) consistent with public info as of May 2024.\n- Highlights core growth challenges (e.g., supply constraints, user lock-in, switching costs) and realistic opportunities.\nScoring:\n- 2.5: Competitor correctly identified; numbers and dynamics look plausible with sources; candid and balanced.\n- 1.6: Mostly correct but light on quantification or sourcing, or one notable gap.\n- 0.8: Vague, missing key competitor info, or questionable claims.\n- 0.0: Factually incorrect or no meaningful market reality content.", "expectation": "Clear, sourced, and plausible view of KR ride-hailing landscape naming Kakao T and quantifying key dynamics."}, {"type": "llm_judge", "name": "Operational Plan Feasibility \u2014 Supply & Demand", "description": "Check whether supply-side driver plan and demand/UX plan are concrete, balanced, and feasible within constraints.", "weight": 2.5, "judge_prompt": "Assess the feasibility and completeness of the Core Growth & Operational Excellence Plan and Korean user experience plan:\n- Supply: Clear levers (e.g., taxi company partnerships, onboarding funnels, incentives, fleet/EV programs), realistic ramp and risks.\n- Demand/UX: Priority segments, localization for Korean users, key product fixes and go-to-market channels.\n- Shows 30-60-90 day or monthly ramps, resource/budget needs, and mitigations.\nScoring:\n- 2.5: Concrete levers with quantified ramps and risks; balanced supply/demand approach; feasible near-term execution.\n- 1.6: Reasonable but missing quantification or risk/mitigation depth.\n- 0.8: Generic or one-sided (only supply or only demand) with limited feasibility.\n- 0.0: Not actionable or unrelated to KR market.", "expectation": "A back-to-basics but quantified plan to rapidly grow supply and demand with realistic execution details."}, {"type": "llm_judge", "name": "Regulatory & Partnership Strategy Robustness", "description": "Evaluate realism of regulatory analysis and partnership plan with Korean taxi associations and authorities.", "weight": 2.5, "judge_prompt": "Evaluate the regulatory/partnership plan:\n- Accurately references the Korean regulatory environment for ride-hailing/platform taxis; acknowledges constraints.\n- Proposes compliance-first pathways and practical engagements (e.g., taxi associations, municipalities, ministries).\n- Identifies key risks (e.g., policy shifts) and mitigation.\nScoring:\n- 2.5: Specific, realistic, and locally informed regulatory strategy with credible actions and counterparts.\n- 1.6: Generally correct but thin on specifics or partners.\n- 0.8: Vague or partially misaligned with KR context.\n- 0.0: Unrealistic or factually wrong regulatory framing.", "expectation": "A grounded plan engaging associations/regulators with compliance and risk mitigation."}, {"type": "llm_judge", "name": "H2 2024 Action Plan & KPI Coherence", "description": "Check if the H2 2024 plan contains owners, milestones Aug\u2013Dec 2024, and measurable KPIs aligned to strategy.", "weight": 2.5, "judge_prompt": "Review the H2 2024 action plan slide(s):\n- Contains a timeline from August 2024; shows monthly milestones through Q3\u2013Q4 2024.\n- Lists owners and resources; includes concrete KPIs (e.g., active drivers, completed trips, ETA, cancellation rate, conversion, take rate).\n- KPIs align logically with the earlier strategy and are numerically coherent (magnitudes make sense).\nScoring:\n- 2.5: Clear month-by-month milestones, owners, and credible KPIs aligned to strategy.\n- 1.6: Timeline present but light on owners or KPIs.\n- 0.8: Vague milestones or unmeasurable goals.\n- 0.0: No actionable plan or KPIs.", "expectation": "An Aug\u2013Dec 2024 plan with owners and measurable, aligned KPIs."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality Assessment (Executive Fitness & Communication)", "description": "Holistic assessment of strategic value, clarity, localization, and professional presentation quality for executive decision-making.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Executive Usefulness & Strategic Prioritization", "description": "Does the deck drive decisions with clear prioritization, trade-offs, and explicit asks?", "weight": 2.5, "judge_prompt": "Judge the deck\u2019s executive usefulness:\n- Clear prioritization and sequencing of initiatives; highlights trade-offs and resource choices.\n- Explicit asks/decisions for CEO/Regional Head (e.g., budgets, headcount, policy engagement).\n- Concise synthesis suitable for 30\u201345 min discussion.\nScoring: 2.5 excellent; 1.5 mixed; 0.8 weak; 0.0 poor.", "expectation": "Clear priorities and decision asks enabling immediate executive alignment."}, {"type": "llm_judge", "name": "Clarity, Structure, and Slide Craft", "description": "Assess headline clarity, logical flow, and bullet readability within 5\u20136 core slides.", "weight": 2.5, "judge_prompt": "Evaluate presentation clarity:\n- Headlines are action-oriented; bullets are concise and scannable.\n- Logical flow across the 5\u20136 core slides; no redundancy.\n- Charts/tables (if present) are legible and labeled.\nScoring: 2.5 excellent; 1.5 adequate; 0.8 marginal; 0.0 unclear.", "expectation": "Crisp headlines, focused bullets, and a coherent flow."}, {"type": "llm_judge", "name": "Localization & Cultural Fit for Korea", "description": "Evaluate whether the plan is tailored to Korean users, partners, and norms.", "weight": 2.5, "judge_prompt": "Assess localization:\n- Reflects Korean user behaviors, channels (e.g., Naver, Kakao ecosystem), payments, and service expectations.\n- Proposes localized UX/content and on-the-ground partnerships.\n- Language/tone appropriate for Korea; avoidance of generic, Western-centric assumptions.\nScoring: 2.5 strong localization; 1.5 partial; 0.8 light; 0.0 none.", "expectation": "A distinctly Korean strategy leveraging local platforms and norms."}, {"type": "llm_judge", "name": "Professional Polish & Credibility Signals", "description": "Check visual/professional quality, consistency, and credible sourcing/sanity of magnitudes.", "weight": 2.5, "judge_prompt": "Evaluate polish and credibility:\n- Consistent formatting and branding; minimal typos.\n- Sources are cited in Appendix; numbers look sane in magnitude/units.\n- Overall feels board-ready.\nScoring: 2.5 excellent; 1.5 decent; 0.8 rough; 0.0 unprofessional.", "expectation": "Board-ready presentation with consistent style and credible sourcing."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "4c4dc603-c21c-4284-8fb1-1b827c1fddf4", "rubric": {"category_name": "Finance: Product Summary (Project Kenonic)", "rationale": "This rubric enforces a self-documenting, verifiable one-page investor Product Summary in PDF form. Stage 1 (LLM-only) strictly gates structure and presence of required sections so later checks are trivial. Stage 2 mixes lightweight code rules (format/pattern and basic consistency checks) with higher-weight LLM rules for substantive correctness and compliance reasonableness. Stage 3 uses LLM judges to assess professional quality, clarity, and investor usefulness.", "max_total_score": 32.0, "stages": [{"name": "Stage 1 \u2014 One-Page PDF Structure Gate", "description": "LLM-only gate to ensure the candidate output is a single-page PDF with the exact required sections and contact/disclosure elements, enabling verification.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "One-Page PDF Structure and Required Sections", "description": "Verify the output is a one-page PDF with all required sections and elements present in a clear, investor-ready structure.", "weight": 8.0, "judge_prompt": "You are evaluating a candidate output for strict STRUCTURE ONLY (not content quality). Examine the primary output file.\n\nRequirement: It MUST be a single-page PDF that includes a clear title referencing \"Project Kenonic\" and a concise, investor-ready one-page Product Summary. The structure must include clear section headers or equivalent labels for the following nine areas (order and exact header names can vary slightly, but intent must be clear):\n1) Fund Details (mission, high-level objectives)\n2) Problem the fund is solving\n3) Proposed solution\n4) Salient numbers (e.g., target market size, target raise, target IRR, etc.) \u2014 ideally as a compact table or bullet block\n5) Key economics (token supply, valuation methodology and frequency, price per token)\n6) Investment strategy\n7) Dividend distribution strategy\n8) Team profiles (key team members)\n9) LKK Capital contact details including: website (https://www.lkkcapital.com), email (letstalk@lkkcapital.com), phone ((+1) 000 000 111), and a link to disclosures (https://www.lkkcapital.com/disclosures)\n\nFormatting expectations:\n- Single page only (no more than 1 page)\n- PDF format (not Word, not image-only without selectable text OK if legible; still must be a PDF)\n- Professional, scannable layout with headings and a distinct block for salient numbers\n\nScoring (0\u20138):\n- 8: PDF, single page, title references Project Kenonic, all 9 required sections/elements present with visible headers/labels; salient numbers clearly grouped.\n- 6: PDF, single page, title OK, exactly 1 required section/element missing OR the salient-numbers block is present but not clearly grouped.\n- 4: PDF, single page, 2\u20133 required sections/elements missing OR ambiguous labeling for multiple sections.\n- 2: PDF, single page, 4\u20135 sections missing or highly unclear.\n- 0: Not PDF, more than one page, or 6+ sections missing.\n\nImportant: Only judge structure and presence. Do NOT evaluate correctness of numbers or writing quality.", "expectation": "A one-page PDF with all nine specified sections and LKK Capital contact + disclosure link, with a clear title including Project Kenonic and a grouped salient-numbers area."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification: Content Presence, Patterns, and Reasonableness", "description": "Verify the correctness and plausibility of key elements enabled by the structured output. Mix of code rules for deterministic checks and LLM judges for nuanced reasoning. Code rules carry less weight than LLM rules.", "is_required": false, "max_points": 14.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Contact Details and Disclosures Present (Deterministic)", "description": "Check presence of required LKK Capital contact details and disclosures URL using text extraction. Awards partial credit for each element found.", "weight": 0.7, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n    if not text:\n        return 0.0\n    t = text.lower()\n\n    score_parts = 0\n    total_parts = 4\n\n    # Website: lkkcapital.com (allow http/https, with or without www)\n    if re.search(r\"https?://(www\\.)?lkkcapital\\.com\\b\", t) or \"lkkcapital.com\" in t:\n        score_parts += 1\n\n    # Email: letstalk@lkkcapital.com\n    if re.search(r\"\\bletstalk@lkkcapital\\.com\\b\", t):\n        score_parts += 1\n\n    # Phone: (+1) 000 000 111 (allow spaces/formatting)\n    if re.search(r\"\\(\\+1\\)\\s*0{3}\\s*0{3}\\s*111\", t):\n        score_parts += 1\n\n    # Disclosures link: https://www.lkkcapital.com/disclosures\n    if re.search(r\"https?://(www\\.)?lkkcapital\\.com/disclosures\\b\", t):\n        score_parts += 1\n\n    frac = score_parts / total_parts\n    return frac * 0.7"}, {"type": "code", "name": "Numeric Metrics Presence and Format (IRR, Raise, Tokenomics)", "description": "Check that key numeric metrics appear with reasonable formatting: a percent near IRR, a currency raise amount, a token price, and a token supply count. Partial credit per metric found.", "weight": 0.9, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n    if not text:\n        return 0.0\n    t = text\n\n    found = 0\n    total = 4\n\n    # IRR as a percentage near IRR label\n    irr_pattern = re.compile(r\"(?i)(?:target\\s*)?(?:irr|internal\\s*rate\\s*of\\s*return)[^%\\n]{0,40}?(\\d{1,2}(?:\\.\\d+)?\\s?%)\")\n    if irr_pattern.search(t):\n        found += 1\n\n    # Target raise as currency amount (allow M/B labels)\n    raise_pattern = re.compile(r\"(?i)(target\\s*raise|offering\\s*size|target\\s*fund\\s*size|raise)[^\\n$]{0,20}\\$\\s?[\\d,.]+(?:\\s?(?:m|mm|million|b|bn|billion))?\")\n    if raise_pattern.search(t):\n        found += 1\n\n    # Token price\n    price_pattern = re.compile(r\"(?i)(price\\s*per\\s*token|token\\s*price)[^\\n$]{0,20}\\$\\s?[\\d,.]+(?:\\.\\d{1,4})?\")\n    if price_pattern.search(t):\n        found += 1\n\n    # Token supply\n    supply_pattern = re.compile(r\"(?i)token\\s*(?:supply|issuance|float|circulating)[^\\n\\d]{0,20}[\\d,.]+\")\n    if supply_pattern.search(t):\n        found += 1\n\n    frac = found / total\n    return frac * 0.9"}, {"type": "code", "name": "Tokenomics Arithmetic Sanity (Raise \u2248 Supply \u00d7 Price)", "description": "If token supply, token price, and/or target raise are parsable, check that Raise approximately equals Supply \u00d7 Price within a 20\u201330% tolerance. Partial credit if two values exist and are roughly coherent; zero if insufficient data or wildly inconsistent.", "weight": 0.8, "code": "import re\n\ndef _parse_number_with_scale(s: str):\n    s = s.strip().lower()\n    mult = 1.0\n    if any(x in s for x in [\" billion\", \" bn\", \" b \", \"billion\"]):\n        mult = 1_000_000_000.0\n    elif any(x in s for x in [\" million\", \" mm\", \" m \", \"million\"]):\n        mult = 1_000_000.0\n    # Extract numeric\n    m = re.search(r\"[-+]?\\d[\\d,]*(?:\\.\\d+)?\", s)\n    if not m:\n        return None\n    num = float(m.group(0).replace(\",\", \"\"))\n    return num * mult\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n    if not text:\n        return 0.0\n    t = text.lower()\n\n    # Extract first occurrences\n    price_match = re.search(r\"(?i)(?:price\\s*per\\s*token|token\\s*price)[^$\\n]{0,40}\\$\\s*([\\d,]+(?:\\.\\d{1,4})?)\", text)\n    supply_match = re.search(r\"(?i)(?:token\\s*(?:supply|issuance|float|circulating))[^\\n\\d]{0,20}([\\d,]+(?:\\.\\d+)?)\", text)\n    raise_match  = re.search(r\"(?i)(?:target\\s*raise|offering\\s*size|target\\s*fund\\s*size|raise)[^\\n$]{0,40}(\\$\\s*[\\d,]+(?:\\.\\d+)?(?:\\s?(?:m|mm|million|b|bn|billion))?)\", text)\n\n    price = None\n    if price_match:\n        try:\n            price = float(price_match.group(1).replace(\",\", \"\"))\n        except Exception:\n            price = None\n\n    supply = None\n    if supply_match:\n        try:\n            supply = float(supply_match.group(1).replace(\",\", \"\"))\n        except Exception:\n            supply = None\n\n    target_raise = None\n    if raise_match:\n        target_raise = _parse_number_with_scale(raise_match.group(1))\n\n    # Need at least two values to check sanity\n    have = sum(v is not None for v in [price, supply, target_raise])\n    if have < 2:\n        return 0.0\n\n    # If all three present, check raise \u2248 supply * price\n    score = 0.0\n    if price is not None and supply is not None and target_raise is not None:\n        est = price * supply\n        if est <= 0 or target_raise <= 0:\n            score = 0.0\n        else:\n            # 30% tolerance for summary-level rounding\n            low = 0.7 * est\n            high = 1.3 * est\n            score = 0.8 if (low <= target_raise <= high) else 0.2\n    else:\n        # If only two are present, give partial credit for having coherent pair\n        score = 0.4\n\n    return score"}, {"type": "llm_judge", "name": "Section Substance and Completeness", "description": "Assess whether each required section contains substantive, specific content (not just headers or placeholders).", "weight": 3.5, "judge_prompt": "Evaluate the Product Summary for substantive content in each required section:\n- Fund details: states mission and high-level objectives\n- Problem statement: clearly articulated\n- Proposed solution: how the fund addresses the problem\n- Salient numbers: includes concrete figures (target market size, target raise, target IRR, etc.) in a compact block/table\n- Key economics: token supply, valuation methodology and frequency, price per token\n- Investment strategy: asset types, stages, geographies or themes\n- Dividend distribution: frequency/mechanics, source of distributions\n- Team: key member names/roles and brief credentials\n- Contact block: all items present (website, email, phone, disclosures link)\n\nScoring (0\u20133.5):\n- 3.5: All sections are present with specific, non-generic details (at least 1\u20132 sentences or bullets each; salient numbers are concrete).\n- 2.5: One section is thin or generic, others are specific.\n- 1.5: Two\u2013three sections are thin or generic.\n- 0.5: Four or more sections are thin, mostly placeholders.\n- 0: Most sections are headers only or missing substance.", "expectation": "All nine sections include concrete, investor-usable content; the salient numbers are explicit and specific."}, {"type": "llm_judge", "name": "Strategy and Dividend Specificity", "description": "Verify the investment strategy and dividend distribution strategy provide actionable specifics.", "weight": 3.5, "judge_prompt": "Focus on two areas:\n1) Investment Strategy: Does it specify assets (e.g., private credit, venture, real assets), stages, geographies, sectors, and selection/portfolio construction approach? Are risk controls or diversification mentioned?\n2) Dividend Distribution Strategy: Is the distribution frequency or policy stated (e.g., quarterly), source of distributions (e.g., yield/realizations), and mechanics (e.g., on-chain distribution, reinvestment option) described?\n\nScoring (0\u20133.5):\n- 3.5: Both areas include clear, concrete specifics (frequency, asset types, mechanics) sufficient for an investor to understand how returns may be generated and paid.\n- 2.5: One area is very specific; the other is somewhat generic but present.\n- 1.5: Both areas are present but generic.\n- 0.5: One area missing; the other generic.\n- 0: Both areas missing or purely boilerplate.", "expectation": "Actionable details for strategy and a clearly stated dividend distribution cadence/mechanism."}, {"type": "llm_judge", "name": "Reasonableness and Compliance Tone", "description": "Assess whether claims are reasonable, avoid guarantees, and appropriately reference risks/disclosures.", "weight": 4.0, "judge_prompt": "Review the summary for compliance reasonableness:\n- Avoids guaranteed or promissory language about returns (e.g., \"will deliver,\" \"guaranteed\")\n- Uses appropriate cautionary tone for forward-looking statements\n- Mentions risks at least briefly or clearly directs readers to disclosures (link present is sufficient if text directs to it)\n- Describes tokenization benefits without implying risk-free nature\n\nScoring (0\u20134):\n- 4: Fully reasonable and compliant tone; no guarantees; clear pointer to risks/disclosures.\n- 3: Minor promotional phrasing but generally compliant; disclosures referenced.\n- 2: Several aggressive claims or implied guarantees; weak or no mention of risks but disclosures link present.\n- 1: Highly promotional with near-promissory language; risks absent.\n- 0: Explicit guarantees of returns or no disclosures reference at all.", "expectation": "Balanced, compliant tone with explicit avoidance of guarantees and clear reference to disclosures."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism", "description": "Holistic LLM assessment of clarity, usability, and professional presentation for accredited retail investors.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Executive Clarity and Conciseness", "description": "Is the document concise, scannable, and understandable to an accredited retail investor without jargon overload?", "weight": 2.5, "judge_prompt": "Evaluate clarity and conciseness:\n- The summary fits on one page and is easy to scan (headings, bullets)\n- Jargon is minimized or defined; acronyms explained on first use\n- Key takeaways can be grasped in under a minute\n\nScoring (0\u20132.5): 2.5 excellent clarity and brevity; 1.5 moderate; 0.5 poor; 0 very poor.", "expectation": "Highly scannable, plain-English summary with defined terms and succinct phrasing."}, {"type": "llm_judge", "name": "Investor Usefulness and Actionability", "description": "Does the summary surface the most decision-relevant information and indicate next steps or how to engage?", "weight": 2.5, "judge_prompt": "Assess usefulness for a prospective investor:\n- Salient numbers are featured and easy to find\n- Value proposition/problem-solution is crisp and connected to economics\n- Includes clear next steps/contact or platform references for action\n\nScoring (0\u20132.5): 2.5 highly actionable; 1.5 somewhat actionable; 0.5 minimally; 0 not useful.", "expectation": "Clear value, salient economics, and explicit directions to contact or proceed."}, {"type": "llm_judge", "name": "Visual and Structural Professionalism", "description": "Evaluate layout, readability, and polish befitting a top-quartile fund advisory firm.", "weight": 2.5, "judge_prompt": "Evaluate professional presentation:\n- Consistent headings, spacing, and alignment\n- Clean table or block for salient metrics\n- Minimal typos/grammar issues\n- Optional branding/logos acceptable; overall polished look\n\nScoring (0\u20132.5): 2.5 professional; 1.5 decent; 0.5 rough; 0 unprofessional.", "expectation": "Polished, well-structured one-pager with minimal errors."}, {"type": "llm_judge", "name": "Coherence and Internal Consistency", "description": "Check that narrative, numbers, and tokenomics are coherent and non-contradictory.", "weight": 2.5, "judge_prompt": "Assess internal consistency across the page:\n- Problem-solution aligns with investment strategy\n- Salient numbers, token supply, price per token, and target raise do not contradict each other\n- Dividend distribution aligns with purported cash flow sources\n\nScoring (0\u20132.5): 2.5 fully coherent; 1.5 minor inconsistencies; 0.5 notable inconsistencies; 0 contradictory or confusing.", "expectation": "Numbers and narrative align logically with no contradictions."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "4122f866-01fa-400b-904d-fa171cdab7c7", "rubric": {"category_name": "Serverless Contact Form Backend (Terraform + AWS Lambda)", "rationale": "This rubric uses a 3-stage, self-documenting evaluation pipeline tailored to a code/infrastructure deliverable. Stage 1 is a strict LLM-only shape gate ensuring a single ZIP with the exact file set and basic structure so deeper verification becomes feasible. Stage 2 mixes precise code rules (lightweight, deterministic checks on Terraform/Lambda semantics) with heavier LLM judges for end-to-end wiring, behavior, and security validation. Stage 3 assesses overall production readiness, documentation quality, and maintainability.", "max_total_score": 40.0, "stages": [{"name": "Stage 1 \u2014 Deliverable Shape Gate (ZIP + Required Files)", "description": "Gate that enforces the exact deliverable structure to enable verification. LLM judges only.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "ZIP Deliverable Structure", "description": "Verify primary output is a single ZIP archive that contains the required files at the root or within a single top-level folder.", "weight": 5.0, "judge_prompt": "You are verifying the structure of a candidate submission for a serverless backend deliverable. The primary output MUST be a single ZIP archive containing the following required files (either at the root or in a single top-level folder):\n\n- main.tf (Terraform)\n- variables.tf (Terraform)\n- outputs.tf (Terraform)\n- exports.js (Lambda handler for Node.js 18)\n- README.md (Markdown documentation)\n\nBe flexible with exact folder structure (root or one folder) and Markdown file case (README.md vs README.MD). However, the handler file must be named exports.js. Optional extra files (e.g., .gitignore, LICENSE) are fine.\n\nScoring:\n- 5.0: Primary output is a ZIP and includes all 5 required files (names match, exports.js present).\n- 3.0: ZIP present but exactly one required file missing OR handler file named differently but clearly equivalent (e.g., index.js with README explicitly instructing zipping exports.js \u2014 only if clearly sufficient to run as specified).\n- 1.0: ZIP present but multiple required files missing or badly misnamed (cannot reasonably proceed with verification).\n- 0.0: Not a ZIP archive or no evidence of required files.\n\nOnly assess structure/presence, not content correctness.", "expectation": "A single ZIP with main.tf, variables.tf, outputs.tf, exports.js, and README.md present (root or single folder)."}, {"type": "llm_judge", "name": "README Structure Presence", "description": "Check README exists inside the ZIP and includes clear sections for prerequisites and setup steps.", "weight": 3.0, "judge_prompt": "Within the ZIP, locate README.md (or README.MD). Verify it contains at minimum:\n- A Prerequisites/Requirements section (domain/hosted zone and verified emails are out-of-scope but mentioned; placeholders acceptable; reCAPTCHA keys substituted later).\n- Clear setup steps including: packaging Lambda code (e.g., zip exports.js.zip exports.js), Terraform commands (terraform init/fmt/validate/apply/destroy), and instructions to retrieve outputs (API URL).\n\nScoring:\n- 3.0: README present with both sections (Prereqs + Setup) and includes packaging and Terraform steps and how to get API URL.\n- 2.0: README present, has the sections but missing one expected step (e.g., packaging or outputs retrieval).\n- 1.0: README present but very sparse/minimal; unclear setup.\n- 0.0: README not present.\n\nDo not judge technical correctness here\u2014only presence and structural completeness.", "expectation": "README.md includes prerequisites and step-by-step setup with Lambda packaging and Terraform commands."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness and Wiring)", "description": "Deep verification of Terraform and Lambda implementation. Mix of code checks (deterministic) and LLM judgment (holistic wiring, behavior, security).", "is_required": true, "max_points": 20.0, "min_score_to_pass": 10.0, "rules": [{"type": "code", "name": "Terraform: API + Lambda Wiring", "description": "Detect core API Gateway + Lambda constructs and Node.js 18 runtime in Terraform (.tf files).", "weight": 1.0, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Checks that Terraform declares:\n    - aws_lambda_function with runtime nodejs18.x\n    - aws_api_gateway_rest_api\n    - aws_api_gateway_resource with path_part ~ contact-us\n    - aws_api_gateway_method with POST\n    - aws_api_gateway_integration\n    - aws_api_gateway_deployment or aws_api_gateway_stage\n    Returns score in [0, weight] proportional to hits.\n    \"\"\"\n    import re\n    from pathlib import Path\n    weight = 1.0\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output\"\n    try:\n        p = context.files.get_path(output.id)\n        if p.suffix.lower() != \".zip\":\n            return 0.0, \"Primary output is not a ZIP\"\n        import zipfile\n        with zipfile.ZipFile(p, 'r') as z:\n            tf_texts = []\n            for name in z.namelist():\n                if name.lower().endswith('.tf'):\n                    try:\n                        tf_texts.append(z.read(name).decode('utf-8', errors='ignore'))\n                    except Exception:\n                        pass\n            if not tf_texts:\n                return 0.0, \"No .tf files found in ZIP\"\n            text = \"\\n\".join(tf_texts)\n            checks = {\n                'lambda_function': bool(re.search(r'resource\\s+\"aws_lambda_function\"', text)),\n                'runtime_node18': bool(re.search(r'runtime\\s*=\\s*\"?nodejs18\\\\.x\"?', text)),\n                'rest_api': bool(re.search(r'resource\\s+\"aws_api_gateway_rest_api\"', text)),\n                'resource_contact': bool(re.search(r'aws_api_gateway_resource[\\s\\S]*?path_part\\s*=\\s*\"?contact-?us\"?', text, re.IGNORECASE)),\n                'method_post': bool(re.search(r'resource\\s+\"aws_api_gateway_method\"[\\s\\S]*?http_method\\s*=\\s*\"POST\"', text)),\n                'integration': bool(re.search(r'resource\\s+\"aws_api_gateway_integration\"', text)),\n                'deploy_or_stage': bool(re.search(r'resource\\s+\"aws_api_gateway_(deployment|stage)\"', text)),\n            }\n            hit_count = sum(1 for v in checks.values() if v)\n            total = len(checks)\n            score = weight * (hit_count / total)\n            return score, f\"Hits: {hit_count}/{total} \u2014 \" + \", \".join([k for k,v in checks.items() if v])\n    except Exception as e:\n        return 0.0, f\"Error reading ZIP: {e}\""}, {"type": "code", "name": "Terraform: SES Resources + IAM Permissions", "description": "Verify SES domain/template resources and minimal IAM permissions incl. CloudWatch Logs.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Checks presence of SES + IAM constructs in Terraform (.tf files):\n    - aws_ses_template\n    - aws_ses_domain_identity\n    - aws_ses_domain_dkim or aws_ses_domain_mail_from\n    - aws_ses_email_identity (for recipients)\n    - IAM policy allows ses:SendTemplatedEmail or ses:SendEmail\n    - IAM policy allows logs:CreateLogGroup/CreateLogStream/PutLogEvents\n    Returns proportional score.\n    \"\"\"\n    import re\n    weight = 1.0\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    try:\n        p = context.files.get_path(output.id)\n        if p.suffix.lower() != \".zip\":\n            return 0.0, \"Not a ZIP\"\n        import zipfile\n        with zipfile.ZipFile(p, 'r') as z:\n            tf_texts = []\n            for name in z.namelist():\n                if name.lower().endswith('.tf'):\n                    try:\n                        tf_texts.append(z.read(name).decode('utf-8', errors='ignore'))\n                    except Exception:\n                        pass\n            if not tf_texts:\n                return 0.0, \"No .tf files\"\n            text = \"\\n\".join(tf_texts)\n            checks = {\n                'ses_template': bool(re.search(r'resource\\s+\"aws_ses_template\"', text)),\n                'ses_domain_identity': bool(re.search(r'resource\\s+\"aws_ses_domain_identity\"', text)),\n                'ses_dkim_or_mailfrom': bool(re.search(r'resource\\s+\"aws_ses_domain_(dkim|mail_from)\"', text)),\n                'ses_email_identity': bool(re.search(r'resource\\s+\"aws_ses_email_identity\"', text)),\n                'iam_ses_send': bool(re.search(r'ses:(SendTemplatedEmail|SendEmail)', text)),\n                'iam_logs': bool(re.search(r'logs:(CreateLogGroup|CreateLogStream|PutLogEvents)', text)),\n            }\n            hit_count = sum(1 for v in checks.values() if v)\n            total = len(checks)\n            score = weight * (hit_count / total)\n            return score, f\"Hits: {hit_count}/{total} \u2014 \" + \", \".join([k for k,v in checks.items() if v])\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Variables and Outputs Defined", "description": "Check variables.tf includes key variables and outputs.tf exposes API URL.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Variables expected (flexible names accepted via substring):\n    - region\n    - domain\n    - lambda name (lambda_name or similar)\n    - recipients (primary/admin or recipients)\n    - api route/path (api_route or route or path)\n    - stage name (stage or stage_name)\n    - captcha secret (captcha_secret or recaptcha_secret)\n    - tags\n    Outputs expected to reveal execute-api URL or invoke_url.\n    \"\"\"\n    import re\n    weight = 1.0\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    try:\n        p = context.files.get_path(output.id)\n        if p.suffix.lower() != \".zip\":\n            return 0.0, \"Not a ZIP\"\n        import zipfile\n        vars_text = \"\"\n        outs_text = \"\"\n        with zipfile.ZipFile(p, 'r') as z:\n            for name in z.namelist():\n                lname = name.lower()\n                try:\n                    if lname.endswith('variables.tf'):\n                        vars_text = z.read(name).decode('utf-8', errors='ignore')\n                    if lname.endswith('outputs.tf'):\n                        outs_text = z.read(name).decode('utf-8', errors='ignore')\n                except Exception:\n                    pass\n        if not vars_text:\n            return 0.0, \"variables.tf missing or unreadable\"\n        var_hits = 0\n        var_checks = {\n            'region': 'region' in vars_text,\n            'domain': 'domain' in vars_text,\n            'lambda_name': ('lambda_name' in vars_text) or ('lambda' in vars_text and 'name' in vars_text),\n            'recipients': ('recipient' in vars_text) or ('recipients' in vars_text),\n            'api_route': ('api_route' in vars_text) or ('route' in vars_text) or ('path' in vars_text),\n            'stage': ('stage_name' in vars_text) or re.search(r'variable\\s+\"stage\"', vars_text) is not None,\n            'captcha_secret': ('captcha_secret' in vars_text) or ('recaptcha' in vars_text),\n            'tags': 'tags' in vars_text,\n        }\n        var_hits = sum(1 for v in var_checks.values() if v)\n        out_hit = 0\n        if outs_text:\n            if ('execute-api' in outs_text.lower()) or ('invoke_url' in outs_text.lower()) or ('amazonaws.com' in outs_text.lower() and 'https://' in outs_text.lower()):\n                out_hit = 1\n        total_checks = len(var_checks) + 1\n        score = weight * ((var_hits + out_hit) / total_checks)\n        return score, f\"Variable hits: {var_hits}/{len(var_checks)}, outputs: {out_hit}/1\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Lambda Handler Semantics (exports.js)", "description": "Verify Node.js code uses AWS SDK v3 SES client, validates reCAPTCHA, checks required fields, and returns correct HTTP status codes.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Checks in exports.js:\n    - Uses '@aws-sdk/client-ses' (v3) and SendTemplatedEmailCommand or SendEmailCommand\n    - Verifies Google reCAPTCHA via siteverify (via https or fetch)\n    - Validates required fields: firstName, lastName, email, subject, message, captcha token\n    - Returns statusCode 200/400/500 appropriately\n    - Uses process.env for template/region/recipients/captcha secret\n    \"\"\"\n    import re\n    weight = 1.0\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    try:\n        p = context.files.get_path(output.id)\n        if p.suffix.lower() != \".zip\":\n            return 0.0, \"Not a ZIP\"\n        import zipfile\n        with zipfile.ZipFile(p, 'r') as z:\n            js_name = None\n            for name in z.namelist():\n                if name.lower().endswith('exports.js'):\n                    js_name = name\n                    break\n            if not js_name:\n                return 0.0, \"exports.js not found\"\n            code = z.read(js_name).decode('utf-8', errors='ignore')\n            checks = {\n                'sdk_v3': ('@aws-sdk/client-ses' in code) or re.search(r\"require\\(\\s*'@aws-sdk/client-ses'\\s*\\)\", code) is not None,\n                'ses_cmd': ('SendTemplatedEmailCommand' in code) or ('SendEmailCommand' in code),\n                'recaptcha_verify': ('siteverify' in code) and (('https://www.google.com/recaptcha/api/siteverify' in code) or ('fetch' in code) or ('https' in code)),\n                'required_fields': all(x in code for x in ['firstName','lastName','email','subject','message']) and ('captcha' in code or 'token' in code),\n                'status_codes': ('statusCode' in code and ('200' in code and '400' in code and '500' in code)),\n                'env_usage': ('process.env' in code and any(x in code for x in ['TEMPLATE','RECIPIENT','REGION','CAPTCHA'])),\n            }\n            hit_count = sum(1 for v in checks.values() if v)\n            total = len(checks)\n            score = weight * (hit_count / total)\n            return score, f\"Lambda checks {hit_count}/{total}: \" + \", \".join([k for k,v in checks.items() if v])\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "llm_judge", "name": "End-to-End Wiring and Behavior Compliance", "description": "Evaluate whether Terraform correctly wires API Gateway -> Lambda with environment variables, and Lambda meets behavior specs.", "weight": 4.0, "judge_prompt": "Open the ZIP and review Terraform and exports.js. Check end-to-end wiring and behavior:\n- API Gateway REST API exposes a POST route at /contact-us and deploys to a versioned stage (e.g., v1).\n- Integration connects the POST method to the Lambda function.\n- Lambda environment variables are provided for SES template name, region, recipients (primary and admin), and captcha secret.\n- Lambda accepts JSON body with required fields, validates reCAPTCHA via Google siteverify, and on success sends email via SES to primary with a copy to admin.\n- Returns proper API responses: 200 on success; 400 for validation failures (missing fields or failed captcha); 500 for unexpected errors (e.g., SES failure).\n\nScoring:\n- 4.0: All above aspects clearly implemented and consistent.\n- 3.0: Minor gaps (e.g., slight env var/name mismatch) but overall compliant.\n- 2.0: Some key elements present but missing one core link (e.g., route path mismatch or env vars not wired).\n- 1.0: Mostly incomplete wiring; difficult to deploy as-is.\n- 0.0: Not implementable.\n\nBase judgment on actual code/TF forms, not intentions.", "expectation": "Fully wired, deployable API: /contact-us POST -> Lambda, env vars set, reCAPTCHA check, SES send to both recipients, correct status codes."}, {"type": "llm_judge", "name": "Security and SES Readiness", "description": "Assess IAM least-privilege, secret handling, and SES domain/template setup adequacy.", "weight": 4.0, "judge_prompt": "Open Terraform. Judge security posture and SES readiness:\n- IAM role for Lambda grants least privilege: SES send actions (prefer SendTemplatedEmail) and CloudWatch Logs; no broad wildcards beyond necessity.\n- CAPTCHA secret not hardcoded in code; provided via environment/variables.\n- SES setup includes: domain identity with DKIM and MAIL FROM records, SES template, and placeholder-verified identities for primary and admin recipients.\n- No sensitive real domains/emails or real reCAPTCHA secrets are embedded; placeholders used.\n\nScoring:\n- 4.0: Strong least-privilege IAM; secrets via variables; SES resources complete with DKIM + MAIL FROM + template; placeholders used correctly.\n- 3.0: Minor overbreadth or small SES omissions but acceptable.\n- 2.0: Noticeable gaps (e.g., missing DKIM or lax IAM) but partially usable.\n- 1.0: Weak security posture or SES largely incomplete.\n- 0.0: Insecure or missing SES essentials.", "expectation": "Least-privilege IAM + full SES domain/template setup with placeholders; secrets not hardcoded."}, {"type": "llm_judge", "name": "Outputs and Deployment Flow", "description": "Check Terraform outputs include the fully qualified API URL and README\u2019s deployment flow is accurate and usable.", "weight": 4.0, "judge_prompt": "Confirm that:\n- outputs.tf exposes the fully qualified API invoke URL (e.g., https://{rest_api_id}.execute-api.{region}.amazonaws.com/v1/contact-us) or equivalent.\n- README provides clear, accurate steps: packaging the Lambda (zip exports.js.zip exports.js), terraform init/fmt/validate/apply, and retrieval of outputs.\n- Parameterization allows swapping placeholders for production (variables for domain, recipients, stage, route, captcha secret, tags).\n\nScoring:\n- 4.0: Output URL exposed; README steps correct and reproducible; parameterization clear.\n- 3.0: Mostly correct with minor omissions.\n- 2.0: Some steps unclear or outputs incomplete.\n- 1.0: Hard to follow; outputs not provided.\n- 0.0: No usable deployment guidance or outputs.", "expectation": "Usable deployment runbook and API URL in outputs."}, {"type": "llm_judge", "name": "Lambda Email Composition and Routing", "description": "Ensure the Lambda uses an SES template inserting form details and sends to primary with admin copy.", "weight": 4.0, "judge_prompt": "Examine exports.js and Terraform SES template:\n- There is an SES template that interpolates form fields (firstName, lastName, email, subject, message).\n- Lambda populates the template data correctly and addresses primary recipient with admin CC or a second destination.\n- Handles SES errors and maps them to a 500 response.\n\nScoring:\n- 4.0: Clear template + correct population; both recipients addressed; error handling present.\n- 3.0: Template or routing slightly imperfect but functionally correct.\n- 2.0: Basic send without template or missing a recipient.\n- 1.0: Incorrect composition or routing.\n- 0.0: No viable SES send path.", "expectation": "Templated SES email including form details routed to both recipients; error handling in place."}], "on_failure_action": "skip_remaining", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism", "description": "Holistic assessment of production readiness, maintainability, and documentation clarity.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Production-Readiness and Operational Quality", "description": "Evaluate timeouts/memory, logging/metrics, and resilience aspects.", "weight": 3.0, "judge_prompt": "Assess production readiness:\n- Lambda configuration sensible (timeout, memory) and logs are enabled via CloudWatch.\n- Basic resilience: input validation, clear error messages, and structured logging.\n- Optional but positive: retries, reasonable timeouts for reCAPTCHA call, handling of network failures.\n\nScoring:\n- 3.0: Strong production posture across these dimensions.\n- 2.0: Adequate with minor gaps.\n- 1.0: Minimal operational considerations.\n- 0.0: Lacks basic production readiness.", "expectation": "Reasonable Lambda settings, logging, and robust handling of common failures."}, {"type": "llm_judge", "name": "Security Hardening and Best Practices", "description": "Evaluate adherence to security best practices beyond SES/IAM basics.", "weight": 3.0, "judge_prompt": "Beyond core IAM/SES checks, consider:\n- No secrets in code; variables/TF used.\n- CORS configuration (if present) is reasonable for a public form endpoint.\n- Principle of least privilege consistently applied.\n- No dangerous wildcards or public exposure of sensitive data in logs.\n\nScoring:\n- 3.0: Strong security hygiene and thoughtful hardening.\n- 2.0: Generally good with small issues.\n- 1.0: Some questionable practices.\n- 0.0: Poor security hygiene.", "expectation": "Good security hygiene with minimal risk exposure."}, {"type": "llm_judge", "name": "Documentation Clarity and Developer Experience", "description": "Assess README clarity, placeholders, and end-to-end onboarding speed.", "weight": 3.0, "judge_prompt": "Judge documentation quality:\n- Clear prerequisites and parameterization with safe placeholders for domains/emails/keys.\n- Step-by-step instructions with commands, expected outputs, and troubleshooting notes.\n- Consistent naming, formatting, and tips (terraform fmt/validate included).\n\nScoring:\n- 3.0: Excellent clarity; quick to onboard.\n- 2.0: Clear but missing minor details.\n- 1.0: Usable but thin or ambiguous.\n- 0.0: Insufficient documentation.", "expectation": "Well-structured README enabling quick, low-friction deployment."}, {"type": "llm_judge", "name": "Maintainability and Parameterization", "description": "Evaluate code/TF organization, variables, and ease of adaptation to production.", "weight": 3.0, "judge_prompt": "Consider how maintainable and adaptable the solution is:\n- Terraform uses variables/locals/modules sensibly; tagging present.\n- Code is clean, modular, and readable with comments where helpful.\n- Swapping placeholders for production is straightforward (variables for domain, recipients, stage, route, captcha secret, tags; SES template name configurable).\n\nScoring:\n- 3.0: Highly maintainable and easy to adapt.\n- 2.0: Generally good with minor refactoring needed.\n- 1.0: Works but brittle or under-parameterized.\n- 0.0: Hard to maintain or adapt.\n", "expectation": "Clean structure and sufficient parameterization for production use."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "61717508-4df7-41be-bf97-318dfb2475c0", "rubric": {"category_name": "Finance/Insurance \u2014 CSR Training Deck: Elder Abuse/Financial Exploitation", "rationale": "Pattern B (Document). The deliverables are two PDFs: a ~10-page quick training deck and a second PDF with three realistic role-play scenarios. Stage 1 strictly enforces the exact file/section shape so downstream verification is trivial. Stage 2 mixes light code checks (file presence, keyword/structure signals) with higher-weight LLM verification of legal accuracy and response playbook correctness. Stage 3 holistically assesses usability, tone, and practical value for first-call trainees.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Format & Structural Gate (LLM only)", "description": "Gate: Verify both required PDFs exist and contain the mandated, verifiable section structure enabling downstream checks. Do NOT judge content quality or legal correctness here.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.1, "rules": [{"type": "llm_judge", "name": "Two-PDF Deliverables with Required Structure", "description": "Verify presence and structure of BOTH required PDFs: (A) the quick training deck (~10 pages) and (B) the role-play scenarios pack (3 scenarios).", "weight": 3.0, "judge_prompt": "Review ALL provided candidate files. This task requires exactly TWO separate PDFs: (A) a quick training deck aimed at new CSR hires, and (B) a role-play scenarios pack with three fictional mutual fund accounts. This is a structural check only \u2014 do NOT assess correctness or quality beyond presence/format.\n\nRequired Deliverable A: Quick Training Deck (PDF)\nFormat/length: PDF, approximately 8\u201312 pages (accept ~7\u201314 if clearly substantive). Professional slide-style layout; clear headers.\nMust visibly include these labeled sections (flexible on exact titles, but intent must be clear):\n1) Title + Purpose/Why it matters (overview that exploitation isn\u2019t always obvious)\n2) Quick alignment: What is elder abuse/financial exploitation (very brief plain-English descriptions)\n3) Red flags to watch for during calls, organized (e.g., Behavioral, Transaction, Account/Access changes) with concrete examples (e.g., unauthorized withdrawals, coercion, sudden third-party involvement)\n4) Signals during calls (e.g., caller dynamics, pressure, coached answers, third party speaking over client)\n5) Senior Safe Act \u2014 in plain words (2\u20134 bullets; no legalese)\n6) FINRA Rule 2165 \u2014 in plain words (2\u20134 bullets; e.g., temporary hold authority, scope)\n7) Call handling flow \u2014 step-by-step (what to ask, verify, document, who to notify)\n8) Escalation & documentation steps (when/how to escalate; temporary holds/trusted contact; internal handoffs)\n9) Helpful phrases/scripts (do/do not; neutral, supportive language)\n10) One-page checklist or tear-sheet summary at end (a consolidated reminder page)\n\nRequired Deliverable B: Role-Play Scenarios Pack (PDF)\nFormat/length: PDF, typically 3\u20137 pages total.\nMust contain EXACTLY THREE distinct fictional mutual fund account scenarios. For each scenario, ensure:\n- An account snapshot/table (e.g., Account holder name, age, account type, holdings/balance, recent activity, contact history)\n- A concise narrative describing the situation (e.g., niece calling on behalf; sudden large redemptions)\n- A short list of explicit red flags\n- What to ask (questions/probes) and What to do next (actions), referencing practical steps (e.g., consult 2165 hold procedure, check trusted contact, escalate path)\n\nScoring guidance (apply proportionally; return a single score for this rule):\n- 3.0: Both PDFs present and clearly contain all required sections/elements above (minor wording variations OK). Deck \u224810 pages and scenario pack includes 3 complete scenarios.\n- 2.5: Both PDFs present; one minor section missing or mislabeled, but structure is still verifiable; scenarios all present.\n- 2.0: Both PDFs present but multiple required sections missing in one PDF OR scenarios incomplete (e.g., only 2 scenarios).\n- 1.0: Only one of the two PDFs is present or recognizable as such.\n- 0.0: Wrong format (not PDFs) or missing both deliverables.\nOnly check presence/format/structure, not accuracy or quality.", "expectation": "Two separate PDFs with the exact structural elements outlined, enabling later verification of laws, escalation flow, and scenarios."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness & Verification (Mixed)", "description": "Now verify correctness and fitness-for-use given the mandated structure. Code rules do deterministic checks; LLM judges assess legal accuracy and procedural soundness.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Deliverables Detectable and Properly Classified", "description": "Programmatically detect two separate document outputs and classify likely Deck vs Scenario Pack using text signals.", "weight": 0.3, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs()\n    except Exception:\n        outputs = []\n    if not outputs:\n        return 0.0, \"No outputs found.\"\n\n    # Collect document texts\n    docs = []\n    for r in outputs:\n        try:\n            if hasattr(r, 'is_document') and r.is_document:\n                txt = ''\n                # Try PDF first\n                try:\n                    txt = context.files.read_pdf_text(r.id) or ''\n                except Exception:\n                    pass\n                # Fallback to DOCX\n                if not txt:\n                    try:\n                        txt = context.files.read_docx_text(r.id) or ''\n                    except Exception:\n                        pass\n                if txt:\n                    docs.append((r, txt.lower()))\n        except Exception:\n            continue\n\n    if len(docs) < 2:\n        return 0.0, \"Fewer than two document outputs with readable text.\"\n\n    def scenario_signal(text):\n        score = 0\n        if any(k in text for k in [\"scenario\", \"role play\", \"role-play\", \"case study\", \"mock\"]):\n            score += 1\n        score += len(re.findall(r\"\\b(scenario|case)\\s*(?:#|no\\.|study|)\\s*(\\d+|one|two|three|a|b|c)\\b\", text))\n        if \"mutual fund\" in text:\n            score += 1\n        if \"account holder\" in text or \"account\" in text:\n            score += 1\n        return score\n\n    ranked = sorted(docs, key=lambda x: scenario_signal(x[1]), reverse=True)\n    scenario_doc = ranked[0]\n    deck_doc = ranked[1] if len(ranked) > 1 else None\n\n    scenario_found = scenario_signal(scenario_doc[1]) >= 2\n    deck_found = deck_doc is not None and any(k in deck_doc[1] for k in [\"training\", \"deck\", \"elder\", \"exploitation\", \"call handling\", \"senior safe act\", \"finra rule 2165\"]) \n\n    found_count = int(scenario_found) + int(deck_found)\n    score = (found_count / 2.0) * 0.3\n    fb = []\n    fb.append(f\"Detected documents: {len(docs)}\")\n    fb.append(\"Scenario pack detected\" if scenario_found else \"Scenario pack not confidently detected\")\n    fb.append(\"Deck detected\" if deck_found else \"Deck not confidently detected\")\n    return score, \"; \".join(fb)\n"}, {"type": "code", "name": "Regulatory Mentions and Scenario Count", "description": "Check that the deck mentions both the Senior Safe Act and FINRA Rule 2165. Check that the scenarios pack mentions mutual funds and appears to include at least three scenarios.", "weight": 0.3, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs()\n    except Exception:\n        outputs = []\n    docs = []\n    for r in outputs:\n        try:\n            if hasattr(r, 'is_document') and r.is_document:\n                txt = ''\n                try:\n                    txt = context.files.read_pdf_text(r.id) or ''\n                except Exception:\n                    pass\n                if not txt:\n                    try:\n                        txt = context.files.read_docx_text(r.id) or ''\n                    except Exception:\n                        pass\n                if txt:\n                    docs.append((r, txt.lower()))\n        except Exception:\n            continue\n\n    if not docs:\n        return 0.0, \"No readable documents.\"\n\n    def scenario_markers(text):\n        # Count common scenario markers\n        count = 0\n        count += len(re.findall(r\"\\bscenario\\s*(?:#|no\\.|)\\s*(\\d+|one|two|three)\\b\", text))\n        count += len(re.findall(r\"\\bcase\\s*study\\s*(\\d+|one|two|three)\\b\", text))\n        count += len(re.findall(r\"\\baccount\\s*(?:\\#)?\\s*(\\d+|one|two|three)\\b\", text))\n        return count\n\n    # Heuristic: choose scenario doc by highest markers\n    ranked = sorted(docs, key=lambda x: scenario_markers(x[1]), reverse=True)\n    scenario_text = ranked[0][1] if ranked else ''\n    deck_text = ranked[1][1] if len(ranked) > 1 else ''\n\n    checks = []\n    checks.append(1 if \"senior safe act\" in deck_text else 0)\n    checks.append(1 if (\"2165\" in deck_text or \"finra rule 2165\" in deck_text) else 0)\n    checks.append(1 if \"mutual fund\" in scenario_text else 0)\n    checks.append(1 if scenario_markers(scenario_text) >= 3 else 0)\n\n    passed = sum(checks)\n    score = (passed / 4.0) * 0.3\n    fb = f\"Deck SSA mention: {checks[0]}, Deck 2165 mention: {checks[1]}, Scenario mutual fund: {checks[2]}, Scenario count>=3: {checks[3]}\"\n    return score, fb\n"}, {"type": "llm_judge", "name": "Regulatory Essentials \u2014 Accuracy and Plain Language", "description": "Confirm the deck succinctly and accurately explains the Senior Safe Act and FINRA Rule 2165 in plain language, suitable for non-legal CSR trainees.", "weight": 1.7, "judge_prompt": "Evaluate ONLY the correctness and clarity of the regulatory explanations in the training deck PDF (not the scenarios pack). Look for:\n- Senior Safe Act: correct high-level purpose (encourages reporting; liability protection when trained and acting in good faith), who/what it protects, and that this is a plain-language summary (not legalese).\n- FINRA Rule 2165: correct essence (firms may place temporary holds on disbursements in cases of suspected exploitation for specified customers such as seniors/vulnerable adults), reference to internal process/permissions, and that it\u2019s summarized succinctly in practical terms.\n- No misleading claims (e.g., overbroad authority or misstatements).\n\nScoring (0\u20131 scale, then scaled by weight):\n- 1.0: Both summaries accurate, concise, practical, and plainly written.\n- 0.7: Minor omissions but essentially correct and useful.\n- 0.4: Noticeable inaccuracies or overly legal/abstract; still partially useful.\n- 0.0: Missing or materially incorrect summaries.\nProvide brief feedback citing issues if not full credit.", "expectation": "Correct, succinct, plain-English summaries of Senior Safe Act and FINRA Rule 2165 with no legal overreach."}, {"type": "llm_judge", "name": "Actionable Call-Handling and Escalation Playbook", "description": "Verify that the deck provides a stepwise response framework and escalation path a new CSR can follow in real time; confirm scenarios align with and reinforce these steps.", "weight": 1.7, "judge_prompt": "Evaluate the practicality and completeness of the call-handling and escalation guidance in the training deck, and whether the scenarios pack reinforces it. Look for:\n- Clear step-by-step call flow: clarify the situation; verify identity/authority; gather specifics; probe for red flags; document; consult internal resources; escalate/warm handoff as needed.\n- Concrete triggers and actions tied to policy: e.g., when to consult a supervisor/fraud team, check trusted contact, consider a temporary hold under Rule 2165 (where permitted), and complete required internal forms.\n- Helpful scripts/phrases and a concise end-of-deck checklist/tear-sheet.\n- In the scenarios pack: three realistic cases that map to the playbook (e.g., third-party caller, sudden large redemptions, behavioral pressure), without over-explaining.\n\nScoring (0\u20131 scale, then scaled by weight):\n- 1.0: Clear, actionable steps with scripts and a one-page checklist; scenarios strongly reinforce actions.\n- 0.7: Mostly actionable; minor gaps in steps or mapping to scenarios.\n- 0.4: Partially actionable; key steps or escalation specifics are vague or missing.\n- 0.0: Lacks a usable playbook or scenarios do not align.\nProvide concise feedback if deductions are made.", "expectation": "A practical, followable call flow with escalation guidance and scenarios that clearly apply the playbook."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Presentation & Training Quality (LLM)", "description": "Holistic assessment of clarity, tone, engagement, and practical relevance for first-call trainees.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity and Usability for New Hires", "description": "Is the material easy to follow under live-call pressure? Are instructions concise and scannable?", "weight": 1.0, "judge_prompt": "Assess the overall clarity and usability of BOTH PDFs for a new CSR on their first live calls. Consider: concise headings, scannable bullets, minimal jargon, clear sequencing, and whether a trainee could quickly find what to do next. Score 0\u20131, then scale by weight. Provide one-line feedback if not full credit.", "expectation": "Plain language, scannable structure, and clear next steps suitable for first-call use."}, {"type": "llm_judge", "name": "Engagement and Visual Structure", "description": "Down-to-earth tone and engaging visual organization that aids memory and use.", "weight": 1.0, "judge_prompt": "Evaluate tone and visual organization. Is it down-to-earth (not corporate/legalistic)? Are color and layout used to orient the reader (section breaks, icons, whitespace) without clutter? Is the deck visually coherent and memorable? Score 0\u20131, then scale by weight. Provide one-line feedback if deductions.", "expectation": "Friendly, calm tone with clean visual structure that aids quick comprehension."}, {"type": "llm_judge", "name": "Practical Relevance and Coverage of Realistic Red Flags", "description": "Do examples and scenarios reflect realistic mutual fund customer situations and a range of red flags?", "weight": 1.0, "judge_prompt": "Assess whether the examples and three scenarios feel realistic for mutual fund accounts and cover a range of red flags (e.g., sudden large redemptions, third-party callers, coached answers, coercion/manipulation). Do they connect to recommended actions? Score 0\u20131, then scale by weight. Provide brief feedback if not full credit.", "expectation": "Realistic, varied red flags with clear linkage to actions and escalation steps."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "4b98ccce-9e42-44e9-9115-6fc3e79de288", "rubric": {"category_name": "EMR Transition Data and Correspondence Package (Healthcare Admin)", "rationale": "Mixed-output task: the deliverable must include a specifically named Excel workbook with two mandated sheets and two professionally formatted correspondence letters (PDF/DOCX). Following the self-documenting philosophy, Stage 1 is an LLM-only structural gate mandating exact shape (sheet/tab names, required columns, letters present with HIPAA clause sections and signature blocks). Stage 2 mixes deterministic code checks (columns, sign-offs, keyword presence) with LLM judges for nuanced cross-file consistency and compliance orientation. Stage 3 evaluates overall professionalism, usability, and compliance readiness of the package. Code rules are lighter-weight than LLM rules to reflect nuanced judgment required for healthcare compliance and correspondence quality.", "max_total_score": 27.0, "stages": [{"name": "Stage 1 \u2014 Structural Gate (MANDATORY)", "description": "LLM-only gate ensuring exact deliverable shape: the Excel workbook named PATIENT INCIDENT 007.xlsx with two required sheets and two correspondence files with proper names and structural elements. No content-quality or calculation correctness checks here\u2014presence/format only.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Workbook Presence, Naming, and Sheet Structure", "description": "Verify there is an Excel workbook named PATIENT INCIDENT 007.xlsx (case-insensitive, accept common extension .xlsx). Confirm two sheets exist with these names and structures: Sheet 1: EMR TRANSFER PATIENTS; Sheet 2: Golden Valley EMS DEC. Each sheet must contain a table with required columns and a visible sign-off block beneath the table.", "weight": 3.0, "judge_prompt": "You are checking STRUCTURE ONLY. Examine the candidate deliverables (all files). Confirm:\n\n1) A single Excel workbook exists named \"PATIENT INCIDENT 007.xlsx\" (case-insensitive, ignore minor whitespace/punctuation; extension must be .xlsx).\n2) It contains BOTH sheets named (or obviously equivalent):\n   - \"EMR TRANSFER PATIENTS\"\n   - \"Golden Valley EMS DEC\"\n3) Each of these sheets displays a patient table with these columns clearly labeled (flexible on exact wording, but the meaning must match):\n   - Patient Name (full name)\n   - Medical Record Number (MRN)\n   - Date of Birth (DOB)\n   - Address\n   - Telephone (Phone)\n   - Aliases\n   - Known Relatives (e.g., Next of Kin/Emergency Contact)\n4) Beneath each table there is a visible sign-off area containing the word \"Signed\" and both a Name and an Employee ID (free-form allowed). It must be within the same sheet, below the data.\n\nScoring:\n- 3.0: Workbook name correct, both sheets present with required columns AND sign-off on both sheets.\n- 2.0: Workbook name correct, both sheets present with required columns; sign-off missing on one sheet OR columns clearly missing one minor field.\n- 1.0: Workbook present but only one sheet or columns are significantly incomplete.\n- 0.0: No valid Excel workbook or key sheets missing.\n\nOnly check presence/structure\u2014not content accuracy.", "expectation": "One .xlsx named PATIENT INCIDENT 007.xlsx containing EMR TRANSFER PATIENTS and Golden Valley EMS DEC sheets, both with the specified columns, plus a sign-off block with Name and Employee ID under each table."}, {"type": "llm_judge", "name": "Correspondence Files Presence, Names, and Template Elements", "description": "Verify two correspondence documents exist with correct names (titles) and allowed formats, each including HIPAA clause section and required letter elements per template.", "weight": 3.0, "judge_prompt": "Check STRUCTURE ONLY for the correspondence deliverables. Confirm:\n\n1) Two letters exist as PDF or DOCX files with these exact titles (case-insensitive, ignore extension):\n   - \"DECEASED CORRESPONDENCE 2025\"\n   - \"GENERAL CORRESPONDENCE 2025\"\n2) Each letter includes the following visible elements (flexible headers allowed):\n   - Date\n   - Recipient block (name/department/organization or placeholder)\n   - Subject or Re: line\n   - Body paragraph(s)\n   - A HIPAA clause section (explicitly references HIPAA or privacy protections)\n   - Contact information\n   - Signature block with \"Signed\" plus Name and Employee ID\n\nScoring:\n- 3.0: Both files present (PDF/DOCX), correct names, and all structural elements present in both letters.\n- 2.0: Both files present with correct names, but one missing a minor element (e.g., Subject or Contact info) OR HIPAA mention is present but not clearly sectioned in one letter.\n- 1.0: Only one letter correct, or names/formats mostly wrong but letter-like structure present.\n- 0.0: Letters missing or not in PDF/DOCX.\n\nDo not assess tone or content quality\u2014just presence/format.", "expectation": "Two letters named exactly as specified, each with date, recipient, subject, body, HIPAA clause mention, contact info, and a Signed block with Name and Employee ID."}, {"type": "llm_judge", "name": "Package Completeness and Labeling Check", "description": "Cross-check that the package includes the workbook and both letters, all with exact titles and clear sections to enable verification.", "weight": 2.0, "judge_prompt": "Verify the deliverable SET is complete and clearly labeled. Confirm all three items are present and appropriately titled:\n- Excel workbook: \"PATIENT INCIDENT 007.xlsx\"\n- Deceased letter: \"DECEASED CORRESPONDENCE 2025\" (PDF/DOCX)\n- General letter: \"GENERAL CORRESPONDENCE 2025\" (PDF/DOCX)\n\nAlso confirm each letter visibly references the scenario context (e.g., Golden Valley EMS for deceased handling; EMR transition / records not in the system for general use) as headings or statements\u2014STRUCTURAL presence only.\n\nScoring:\n- 2.0: All three deliverables present with correct naming; both letters include visible scenario references.\n- 1.0: All present but one letter lacks a clear scenario reference.\n- 0.0: One or more deliverables missing or misnamed.\n\nDo not judge wording quality or correctness\u2014only the presence of named items and obvious scenario references.", "expectation": "All required files are present with exact names and minimal scenario references visible so later stages can verify content."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Compliance Verification", "description": "Now that structure is confirmed, verify correctness and consistency across files. Use deterministic code checks (columns, sign-offs, keywords) and LLM judges for nuanced cross-file alignment and compliance orientation.", "is_required": false, "max_points": 9.0, "min_score_to_pass": 4.5, "rules": [{"type": "code", "name": "Workbook Columns Coverage (Both Sheets)", "description": "Programmatically verify both required sheets exist and each includes the required columns (flexible matching of common synonyms). Returns fraction of columns found across both sheets.", "weight": 0.8, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    # Find the Excel workbook, prefer one named PATIENT INCIDENT 007\n    excel_res = None\n    all_spreads = [r for r in context.get_all_outputs() if getattr(r, 'is_spreadsheet', False)]\n    for r in all_spreads:\n        try:\n            p = context.files.get_path(r.id)\n            if p.suffix.lower() in ['.xlsx', '.xls'] and 'patient incident 007' in p.stem.lower():\n                excel_res = r\n                break\n        except Exception:\n            continue\n    if excel_res is None and all_spreads:\n        excel_res = all_spreads[0]\n    if excel_res is None:\n        return 0.0, 'No spreadsheet found.'\n\n    try:\n        p = context.files.get_path(excel_res.id)\n        xls = pd.ExcelFile(p)\n        sheet_names = [s for s in xls.sheet_names]\n    except Exception as e:\n        return 0.0, f'Failed to open Excel: {e}'\n\n    # Identify required sheets (flexible matching)\n    def match_sheet(target, sheets):\n        t = re.sub(r'\\s+', ' ', target.lower()).strip()\n        for s in sheets:\n            sl = re.sub(r'\\s+', ' ', str(s).lower()).strip()\n            if t == sl:\n                return s\n        # contains match fallback\n        for s in sheets:\n            sl = str(s).lower()\n            if t in sl:\n                return s\n        return None\n\n    s1_name = match_sheet('EMR TRANSFER PATIENTS', sheet_names)\n    s2_name = match_sheet('Golden Valley EMS DEC', sheet_names)\n    if not s1_name or not s2_name:\n        return 0.0, 'Required sheets not found.'\n\n    required_fields = {\n        'name': [r'name', r'patient\\s*name', r'full\\s*name'],\n        'mrn': [r'\\bmrn\\b', r'medical\\s*record\\s*number'],\n        'dob': [r'dob', r'date\\s*of\\s*birth'],\n        'address': [r'address'],\n        'phone': [r'phone', r'tele ?phone', r'contact\\s*number'],\n        'aliases': [r'alias', r'aliases', r'also\\s*known\\s*as'],\n        'relatives': [r'known\\s*relatives', r'next\\s*of\\s*kin', r'emergency\\s*contact', r'relatives?']\n    }\n\n    def coverage_for_sheet(sheet):\n        try:\n            df = pd.read_excel(p, sheet_name=sheet, dtype=str)\n        except Exception:\n            return 0.0\n        cols = [re.sub(r'\\s+', ' ', str(c).strip().lower()) for c in df.columns]\n        score = 0\n        total = len(required_fields)\n        for key, pats in required_fields.items():\n            found = False\n            for c in cols:\n                for pat in pats:\n                    if re.search(pat, c):\n                        found = True\n                        break\n                if found:\n                    break\n            if found:\n                score += 1\n        return score / total if total > 0 else 0.0\n\n    cov1 = coverage_for_sheet(s1_name)\n    cov2 = coverage_for_sheet(s2_name)\n    overall = (cov1 + cov2) / 2.0\n    return max(0.0, min(1.0, overall)), f'Columns coverage: {overall:.2f} (S1 {cov1:.2f}, S2 {cov2:.2f})'"}, {"type": "code", "name": "Sign-Off Presence with Employee ID (Both Sheets)", "description": "Detect a sign-off block within each sheet containing the keyword \"Signed\" and an Employee ID pattern. Returns the fraction of sheets meeting the requirement.", "weight": 0.6, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    excel_res = None\n    all_spreads = [r for r in context.get_all_outputs() if getattr(r, 'is_spreadsheet', False)]\n    for r in all_spreads:\n        try:\n            p = context.files.get_path(r.id)\n            if p.suffix.lower() in ['.xlsx', '.xls'] and 'patient incident 007' in p.stem.lower():\n                excel_res = r\n                break\n        except Exception:\n            continue\n    if excel_res is None and all_spreads:\n        excel_res = all_spreads[0]\n    if excel_res is None:\n        return 0.0, 'No spreadsheet found.'\n\n    p = context.files.get_path(excel_res.id)\n    try:\n        xls = pd.ExcelFile(p)\n        sheet_names = xls.sheet_names\n    except Exception as e:\n        return 0.0, f'Failed to read Excel: {e}'\n\n    targets = ['EMR TRANSFER PATIENTS', 'Golden Valley EMS DEC']\n    # Fuzzy match sheet names\n    def match_sheet(target, sheets):\n        t = re.sub(r'\\s+', ' ', target.lower()).strip()\n        for s in sheets:\n            sl = re.sub(r'\\s+', ' ', str(s).lower()).strip()\n            if t == sl:\n                return s\n        for s in sheets:\n            if t in str(s).lower():\n                return s\n        return None\n\n    matched = [m for t in targets if (m := match_sheet(t, sheet_names))]\n    if not matched:\n        return 0.0, 'Required sheets missing.'\n\n    def sheet_has_signoff(sheet):\n        try:\n            df = pd.read_excel(p, sheet_name=sheet, header=None, dtype=str)\n        except Exception:\n            return False\n        text = '\\n'.join([str(v) for v in df.fillna('').values.flatten()])\n        tl = text.lower()\n        if 'signed' not in tl:\n            return False\n        # Look for an employee ID marker and an ID-like token (>=5 alphanum or with dashes)\n        id_marker = re.search(r'(employee\\s*id|emp\\s*id|id)\\s*[:#\\- ]*([A-Za-z0-9\\-]{5,})', text, flags=re.I)\n        return id_marker is not None\n\n    hits = sum(1 for s in matched if sheet_has_signoff(s))\n    denom = len(matched)\n    score = hits / denom if denom else 0.0\n    return score, f'Sign-off found on {hits}/{denom} sheets'"}, {"type": "code", "name": "Letters Existence and HIPAA/Scenario Keyword Check", "description": "Find both letters (PDF/DOCX), extract text, and verify key phrases: HIPAA mention in both; deceased letter references deceased + Golden Valley EMS + authorization/documentation; general letter references declined requests due to EMR transition. Returns averaged fraction.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    docs = [r for r in outputs if getattr(r, 'is_document', False)]\n\n    # Map base title (no ext, lower) to resource\n    by_title = {}\n    for r in docs:\n        try:\n            p = context.files.get_path(r.id)\n            by_title[p.stem.lower()] = r\n        except Exception:\n            continue\n\n    def get_text(res):\n        try:\n            p = context.files.get_path(res.id)\n            if p.suffix.lower() == '.pdf':\n                return context.files.read_pdf_text(res.id) or ''\n            elif p.suffix.lower() in ['.docx', '.doc']:\n                return context.files.read_docx_text(res.id) or ''\n            else:\n                return ''\n        except Exception:\n            return ''\n\n    dec = None\n    gen = None\n    # Try exact, then fuzzy keys\n    for key in list(by_title.keys()):\n        kl = key.lower()\n        if 'deceased correspondence 2025' == kl:\n            dec = by_title[key]\n        if 'general correspondence 2025' == kl:\n            gen = by_title[key]\n    if dec is None:\n        for key in by_title:\n            if 'deceased correspondence 2025' in key:\n                dec = by_title[key]\n                break\n    if gen is None:\n        for key in by_title:\n            if 'general correspondence 2025' in key:\n                gen = by_title[key]\n                break\n\n    score_parts = []\n    feedback = []\n\n    # Deceased letter checks\n    if dec is not None:\n        t = get_text(dec).lower()\n        has_hipaa = 'hipaa' in t or 'privacy rule' in t\n        has_deceased = 'deceas' in t\n        mentions_gv_ems = 'golden valley ems' in t\n        mentions_auth = any(k in t for k in ['authorize', 'authorization', 'authorized documentation', 'legal documentation', 'proof of authority', 'power of attorney'])\n        has_med_rec = 'medical record' in t or 'medical records' in t\n        dec_ok = all([has_hipaa, has_deceased, mentions_gv_ems, mentions_auth, has_med_rec])\n        score_parts.append(1.0 if dec_ok else 0.5 if (has_hipaa and has_deceased) else 0.0)\n        feedback.append(f'Deceased letter: hipaa={has_hipaa}, deceased={has_deceased}, gv_ems={mentions_gv_ems}, auth={mentions_auth}, medrec={has_med_rec}')\n    else:\n        score_parts.append(0.0)\n        feedback.append('Deceased letter not found')\n\n    # General letter checks\n    if gen is not None:\n        t = get_text(gen).lower()\n        has_hipaa = 'hipaa' in t or 'privacy rule' in t\n        mentions_decline = any(k in t for k in ['decline', 'cannot', 'unable', 'not able'])\n        mentions_request = 'record request' in t or 'request for records' in t or 'records request' in t\n        mentions_emr = 'emr' in t or 'electronic medical record' in t or 'electronic record system' in t or 'not in the new emr' in t\n        gen_ok = all([has_hipaa, mentions_decline, mentions_request, mentions_emr])\n        score_parts.append(1.0 if gen_ok else 0.5 if (has_hipaa and mentions_request) else 0.0)\n        feedback.append(f'General letter: hipaa={has_hipaa}, decline={mentions_decline}, req={mentions_request}, emr={mentions_emr}')\n    else:\n        score_parts.append(0.0)\n        feedback.append('General letter not found')\n\n    # Average of two letter scores (each 0..1)\n    score = sum(score_parts) / len(score_parts) if score_parts else 0.0\n    return score, '; '.join(feedback)"}, {"type": "llm_judge", "name": "Deceased Handling Consistency Across Workbook and Letters", "description": "Check whether the approach to deceased patients is consistent: EMS DEC sheet exists for deceased cases; deceased letter states the patient is deceased and requires authorized documentation from Golden Valley EMS to obtain medical records. If no deceased patients appear, the structure is still ready and both letters exist.", "weight": 2.8, "judge_prompt": "Evaluate cross-file consistency for deceased handling:\n- Workbook has the \"Golden Valley EMS DEC\" sheet populated or structurally ready for deceased entries.\n- The \"DECEASED CORRESPONDENCE 2025\" letter explicitly states the patient is deceased AND that Golden Valley EMS must provide authorized documentation to obtain medical records.\n- If no deceased entries are listed, the sheet can be empty but present, and the letter is still included for use.\n\nScoring:\n- 2.8: Clear consistency: sheet present and appropriate; letter contains both the deceased statement and authorized documentation requirement.\n- 1.4: Partial: sheet present but the letter is missing one key element OR vice versa.\n- 0.0: Missing sheet or deceased letter, or contradictions present.", "expectation": "EMS DEC sheet present and the deceased correspondence clearly requires authorization from Golden Valley EMS to obtain records."}, {"type": "llm_judge", "name": "Naming and Save-Conventions Adherence", "description": "Verify file names match exactly the specified titles and year where applicable. Minor case/spacing variance acceptable, but semantic match must be clear.", "weight": 2.0, "judge_prompt": "Check that file names adhere to the specifications:\n- Workbook: \"PATIENT INCIDENT 007.xlsx\"\n- Deceased letter: \"DECEASED CORRESPONDENCE 2025\" (PDF or DOCX)\n- General letter: \"GENERAL CORRESPONDENCE 2025\" (PDF or DOCX)\n\nCase-insensitive and minor whitespace normalization is okay; the semantic content (words, incident number, and year) must match exactly. Score higher when all names clearly match and are unambiguous.\n\nScoring:\n- 2.0: All names match exactly (ignoring case/whitespace) and formats are correct.\n- 1.0: Minor deviations (e.g., small punctuation differences) but clearly the same names.\n- 0.0: One or more deliverables misnamed or wrong format.", "expectation": "All three deliverables present with the exact specified names/years."}, {"type": "llm_judge", "name": "Data Hygiene and Privacy Alignment", "description": "Confirm that letters include HIPAA clauses and do not expose unnecessary PHI beyond what is needed. Workbook includes PHI as required for operations only.", "weight": 2.0, "judge_prompt": "Assess privacy alignment:\n- Both letters visibly include a HIPAA/privacy clause and avoid disclosing unnecessary PHI (no DOB/MRN within letters unless necessary redactions/context provided).\n- Workbook contains the required PHI fields for operational transfer (Name, MRN, DOB, Address, Phone, Aliases, Known Relatives) without gratuitous extraneous details.\n\nScoring:\n- 2.0: Strong privacy alignment: HIPAA clauses present in both letters; letters avoid unnecessary PHI; workbook contains required PHI only.\n- 1.0: Generally aligned but one letter weak on HIPAA mention or minor extra PHI disclosed.\n- 0.0: HIPAA clause missing and/or clear privacy issues.", "expectation": "HIPAA mentioned in both letters; PHI only where required; no gratuitous PHI exposure."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism", "description": "Holistic LLM evaluation of presentation quality, clarity, and readiness for operational use during EMR transition.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Tone and Formatting of Letters", "description": "Evaluate tone, structure, and professionalism of both letters including headers, paragraphing, clarity, and signature blocks.", "weight": 4.0, "judge_prompt": "Evaluate the two letters for professional standards:\n- Clear headers (Date, Recipient, Subject) and organized paragraphs\n- Professional tone suitable for healthcare/EMS correspondence\n- Clear explanation of purpose (deceased handling and general EMR transition decline)\n- Complete signature block with Name, Title (if provided), and Employee ID\n\nScoring:\n- 4.0: Both letters highly professional, well-structured, and error-free.\n- 2.0: Generally professional with minor issues.\n- 0.0: Poorly formatted or unprofessional tone.", "expectation": "Both letters read like professional healthcare administrative correspondence and include clean formatting and signature blocks."}, {"type": "llm_judge", "name": "Workbook Readability and EMR-Readiness", "description": "Assess if spreadsheets are easy to read and ready for EMR import/use: consistent formats, clear headers, sensible ordering, and visible sign-off beneath tables.", "weight": 3.0, "judge_prompt": "Assess the Excel workbook for usability:\n- Column headers clear and consistently formatted\n- DOBs formatted as dates; phone numbers consistently formatted\n- MRNs consistent (fixed width or pattern) where applicable\n- Rows neatly aligned; no obvious truncation or merged-cell issues\n- Sign-off area clearly separated beneath the table on each sheet\n\nScoring:\n- 3.0: Clean, consistent, EMR-ready presentation.\n- 1.5: Mostly readable with some inconsistencies.\n- 0.0: Disorganized and hard to use.", "expectation": "Neat, consistent tables with standard formats and clear sign-off blocks under each table."}, {"type": "llm_judge", "name": "Completeness and Actionability for Operations", "description": "Judge whether the package enables immediate use by Golden Valley EMS and hospital admin teams during partial EMR integration.", "weight": 3.0, "judge_prompt": "Does the package enable action?\n- All required patient fields present to support transfer and EMS billing for deceased where applicable\n- Deceased letter provides clear next steps (authorized documentation) for records access\n- General letter provides clear rationale and next steps for non-deceased/not-in-EMR cases\n\nScoring:\n- 3.0: Fully actionable with clear operational guidance.\n- 1.5: Useful but missing one important detail.\n- 0.0: Not actionable.", "expectation": "The materials let staff proceed confidently with EMS billing (deceased) and manage general record requests during the EMR transition."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "8a7b6fca-60cc-4ae3-b649-971753cbf8b9", "rubric": {"category_name": "Manufacturing \u2022 Industrial Engineering \u2022 Process Map for Parcel Processing (Automation vs Manual)", "rationale": "Task Type: Pattern B (Document). Output: PDF process map. The rubric enforces a self-documenting shape first (Stage 1 LLM-only gate), then verifies logic and correctness with a mix of LLM and light code checks (Stage 2), and finally assesses professional quality and stakeholder usefulness (Stage 3). Code rules read PDF text to confirm presence of required concepts; richer structural and semantic checks rely on LLM judges seeing the rendered diagram.", "max_total_score": 25.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (LLM-only)", "description": "Hard gate verifying the output is a properly structured process map in PDF with mandated, verifiable elements enabling subsequent checks.", "is_required": true, "max_points": 5.0, "min_score_to_pass": 3.5, "rules": [{"type": "llm_judge", "name": "Process Map Structure Gate (PDF and Required Elements)", "description": "Verify the deliverable is a PDF process map with explicit lanes for Automation and Manual, a primary decision split for compatibility, start/end, key tasks, reroute of automation failures to manual, connectors, and a legend/key.", "weight": 5.0, "judge_prompt": "You are evaluating whether the candidate output is a properly structured process map as a PDF that can be verified later. You can see the rendered document.\n\nCheck STRICTLY for structure and presence. Do not grade content quality or correctness here.\n\nFormat Requirements (MANDATORY):\n- File is a PDF (not DOCX/PNG/Excel).\n- Diagrammatic process map is clearly present (not just text). \n- At least one full page diagram; legible shapes and connectors.\n\nStructural Requirements (use flexible matching of labels):\n1) Swimlanes/Lanes:\n   - Two distinct lanes or clearly separated paths: one for Automation (e.g., \"Automation\", \"Automated Sort\", \"Conveyor/AS\") and one for Manual (e.g., \"Manual\", \"Manual Handling\", \"Non-conveyable\"). Lanes should be labeled or unmistakably separated.\n2) Start/End Points:\n   - Explicit Start and End/Finish/Outbound nodes.\n3) Primary Decision for Compatibility:\n   - A decision diamond (or clearly marked decision) that separates automation-compatible vs manual-required items.\n4) Automation Lane Key Steps:\n   - Intake/Unload/Induct\n   - Scan/Identify (barcode or equivalent)\n   - Compatibility check leading to automated routing/sort to outbound\n5) Manual Lane Key Steps:\n   - Intake from decision or exception\n   - Handling steps (e.g., weigh/measure, re-pack, relabel) and manual sort/staging\n   - Outbound handoff\n6) Failure Handling and Reroute:\n   - Explicit handling for automation failures (e.g., jam/exception/overflow) that reroutes into the manual workflow.\n7) Connectors and Flow Direction:\n   - Arrowed connectors showing flow direction and handoffs across the two lanes.\n8) Legend/Key and Title Block:\n   - A legend/key explaining symbols; and a title block including the facility context (e.g., \"Clearbend Logistics Hub\"), plus at least one metadata item (date or version or author).\n\nScoring (return a single score for this rule):\n- 5.0: PDF + all 8 structural requirements clearly present.\n- 4.0: PDF + 6\u20137 structural requirements present (minor omissions only).\n- 3.0: PDF + 4\u20135 structural requirements present (significant omissions but still a recognizable, lane-based process map).\n- 1.5: PDF + only 2\u20133 structural requirements present.\n- 0.5: PDF present but diagram/structure largely missing or unclear.\n- 0.0: Not a PDF or no recognizable process map.\n\nOnly evaluate presence and structure. Do not judge correctness, style, or optimization at this stage.", "expectation": "A one-page or multi-page PDF flow diagram with two lanes (Automation, Manual), start/end, a compatibility decision split, automation steps, manual steps, explicit reroute on automation failure, connectors with arrows, and a legend/key plus a clear title referencing Clearbend Logistics Hub."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification of Logic and Correctness (Mixed LLM + Code)", "description": "Now that shape is enforceably present, verify routing logic, exception handling, data capture, and coordination. Code rules do lightweight text checks; LLM rules assess deeper logic using the rendered diagram.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Decision Logic and Routing Integrity", "description": "Verify that the compatibility decision is well-placed and splits flows correctly, with no dead-ends and clear return to outbound on both lanes.", "weight": 3.0, "judge_prompt": "Evaluate the diagram\u2019s decision logic and routing integrity.\nLook for:\n- A pre-decision scan/identify step feeding the compatibility decision.\n- Decision criteria that plausibly reflect automation compatibility (size/weight/shape/damage/label/ID).\n- Correct split: compatible -> automated process; incompatible -> manual process.\n- No dead-ends; both lanes reach outbound/finish, and exception branches eventually rejoin a valid path.\n\nScoring:\n- 3.0: All points satisfied; placement and routing are unambiguous and logically correct.\n- 2.0: Mostly correct with minor ambiguities (e.g., unclear criteria phrasing or one minor routing ambiguity).\n- 1.0: Significant gaps (e.g., decision before any identification; unclear split; a minor dead-end).\n- 0.0: Decision missing/misplaced; flows incorrect or dead-end significantly.\n\nFocus on logic, not style.", "expectation": "A scan or identification step precedes a compatibility decision; compatible items proceed through automated sort to outbound; incompatible items route to a complete manual workflow; no dead-ends."}, {"type": "llm_judge", "name": "Exception Handling and Reroute Correctness", "description": "Assess whether automation failures (jam/overflow/read failure/damage) are captured and rerouted into manual with appropriate steps and closure.", "weight": 2.5, "judge_prompt": "Evaluate exception handling for automation failures.\nCheck for:\n- Explicit detection points (e.g., jam sensors, misread scans, overflow, damage).\n- Clear reroute from automation to manual (divert/reject/exception handling) with confirmation/scan upon entry to manual.\n- Closure: exceptions are resolved (reworked, relabeled, re-packed) and rejoin outbound without losing traceability.\n\nScoring:\n- 2.5: Comprehensive exception capture with clear reroute to manual, rework steps, and reintegration to outbound.\n- 1.5: Exceptions captured and rerouted, but some rework/closure steps are unclear.\n- 0.5: Vague mention of exceptions with poor reroute clarity.\n- 0.0: No explicit exception handling or reroute.\n", "expectation": "Automation exceptions are visibly diverted to manual; manual path includes rework and confirmation steps; items rejoin outbound cleanly."}, {"type": "llm_judge", "name": "Data Capture and Traceability", "description": "Confirm scanning/identification and system updates occur at key handoffs to preserve traceability across lanes.", "weight": 2.5, "judge_prompt": "Evaluate whether the process ensures data capture and traceability:\n- Scan/identify at induction before decision.\n- System updates (e.g., WMS/WCS/WES) at key transitions: into automation, exception diversion to manual, entry into manual processing, and final outbound.\n- Optional but ideal: re-scan after rework or re-labeling to maintain continuity.\n\nScoring:\n- 2.5: Scans/system updates clearly indicated at all key handoffs across both lanes.\n- 1.5: Most key points covered; one area unclear or missing.\n- 0.5: Sparse indications; traceability weak.\n- 0.0: No discernible data capture strategy.\n", "expectation": "Induction scan precedes decision; system updates at lane entries/transfers; outbound confirmation present."}, {"type": "llm_judge", "name": "Cross-Lane Coordination and WIP Controls", "description": "Assess clarity of handoffs between lanes, staging, and controls to prevent bottlenecks and rework loops.", "weight": 2.0, "judge_prompt": "Evaluate cross-lane coordination:\n- Handoffs between automation and manual are explicit (named transfer points, arrows, staging areas).\n- WIP/staging controls (e.g., exception buffer, manual staging, prioritization) are shown to prevent overflow.\n- Minimal rework loops; loops are purposeful and exit back to main flow.\n\nScoring:\n- 2.0: Clear, well-controlled handoffs with staging and minimal, sensible loops.\n- 1.0: Handoffs present but staging/controls or loop clarity is limited.\n- 0.0: Handoffs unclear; flows likely to create bottlenecks or endless loops.\n", "expectation": "Named transfer points and buffers exist; flow returns to main path without unnecessary rework."}, {"type": "code", "name": "Text Keyword Coverage for Core Elements", "description": "Lightweight check over extracted PDF text for presence of core process concepts (start/end, scan, decision, automation, manual).", "weight": 0.8, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    \\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0, 'No document output'\\n    text = ''\\n    try:\\n        if hasattr(context.files, 'read_pdf_text'):\\n            try:\\n                text = context.files.read_pdf_text(output.id) or ''\\n            except Exception:\\n                text = ''\\n        if not text and hasattr(context.files, 'read_docx_text'):\\n            try:\\n                text = context.files.read_docx_text(output.id) or ''\\n            except Exception:\\n                text = ''\\n    except Exception:\\n        text = ''\\n    if not text:\\n        return 0.0, 'Could not extract text'\\n    t = text.lower()\\n    # Groups of synonyms; count groups that appear at least once\\n    groups = [\\n        ['start', 'begin', 'induct', 'induction', 'inbound'],\\n        ['end', 'finish', 'outbound', 'ship'],\\n        ['scan', 'barcode', 'identify', 'id read'],\\n        ['decision', 'diamond', 'classify', 'classification'],\\n        ['manual', 'manual handling', 'hand processing', 'non-conveyable', 'irregular'],\\n        ['automation', 'automated', 'conveyor', 'auto', 'automated sort']\\n    ]\\n    hits = 0\\n    details = []\\n    for grp in groups:\\n        if any(term in t for term in grp):\\n            hits += 1\\n            details.append(f\"hit:{grp[0]}\")\\n    score = hits / len(groups)\\n    return score, f\"keyword_groups_hit={hits}/{len(groups)}; {', '.join(details)}\""}, {"type": "code", "name": "Presence of Both Lanes or Swimlanes", "description": "Checks text for indications of Automation and Manual lanes/labels.", "weight": 0.6, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n    text = ''\\n    try:\\n        text = context.files.read_pdf_text(output.id) or ''\\n    except Exception:\\n        try:\\n            text = context.files.read_docx_text(output.id) or ''\\n        except Exception:\\n            text = ''\\n    t = text.lower()\\n    has_auto = any(k in t for k in ['automation', 'automated', 'conveyor', 'wcs', 'automated sort'])\\n    has_manual = any(k in t for k in ['manual', 'non-conveyable', 'irregular', 'hand processing', 'manual handling'])\\n    if has_auto and has_manual:\\n        return 1.0, 'Both lanes indicated'\\n    if has_auto or has_manual:\\n        return 0.5, 'Only one lane indicated'\\n    return 0.0, 'No lane indicators found'"}, {"type": "code", "name": "Exception Handling Terminology", "description": "Checks for presence of exception/failure handling terms indicating reroute from automation to manual.", "weight": 0.6, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n    text = ''\\n    try:\\n        text = context.files.read_pdf_text(output.id) or ''\\n    except Exception:\\n        try:\\n            text = context.files.read_docx_text(output.id) or ''\\n        except Exception:\\n            text = ''\\n    t = text.lower()\\n    terms = ['jam', 'overflow', 'exception', 'divert', 'reject', 'reroute', 'breakdown', 'fail', 'incompatible', 'non-conveyable']\\n    count = sum(1 for k in terms if k in t)\\n    if count >= 3:\\n        return 1.0, f'exceptions_terms_found={count}'\\n    if count == 2:\\n        return 0.7, f'exceptions_terms_found={count}'\\n    if count == 1:\\n        return 0.4, f'exceptions_terms_found={count}'\\n    return 0.0, 'No exception terms found'"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism (LLM)", "description": "Holistic assessment of presentation quality, clarity for leadership and cross-functional teams, and actionability for standardization.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Visual Clarity and Professional Presentation", "description": "Professional diagramming: readable typography, consistent symbols, minimal crossings, clean alignment.", "weight": 2.0, "judge_prompt": "Assess visual clarity and professionalism:\n- Readable labels; consistent shape usage for start/end, tasks, and decisions.\n- Connectors with arrows; minimal line crossings; tidy alignment and spacing.\n- Lanes clearly labeled and visually distinct.\n\nScoring:\n- 2.0: Highly professional and clean; excellent readability.\n- 1.0: Generally clear with some clutter/misalignment.\n- 0.0: Difficult to read or inconsistent symbols impede understanding.\n", "expectation": "A clean, legible, professionally formatted process map with standard symbols and clear lanes."}, {"type": "llm_judge", "name": "Audience Appropriateness and Clarity", "description": "Effective communication for leadership and cross-functional alignment, with scope and boundaries obvious.", "weight": 2.0, "judge_prompt": "Assess audience appropriateness:\n- Title conveys scope (e.g., Clearbend Logistics Hub inbound-to-outbound piece flow).\n- Step names are clear and business-readable; acronyms expanded at first use.\n- Scope boundaries evident (what\u2019s included/excluded) via annotations or framing.\n\nScoring:\n- 2.0: Clear, leadership-friendly and alignment-ready.\n- 1.0: Mostly clear; minor jargon or scope ambiguity.\n- 0.0: Confusing labels or unclear scope hinder use by non-authors.\n", "expectation": "Title and labels make the flow obvious to leadership and operations teams."}, {"type": "llm_judge", "name": "Documentation Completeness (Legend, Assumptions, Metadata)", "description": "Legend/key, glossary/assumptions as needed, and versioning metadata (author/date/version).", "weight": 2.0, "judge_prompt": "Check documentation completeness:\n- Legend/key for symbols is present.\n- Assumptions or notes (e.g., compatibility criteria at high level) where helpful.\n- Metadata: author/owner, date, and/or version in a title block.\n\nScoring:\n- 2.0: Legend and metadata present; assumptions/notes appropriately included.\n- 1.0: Missing one element (e.g., legend present but no metadata, or vice versa).\n- 0.0: Lacks both legend and metadata; no helpful notes.\n", "expectation": "Self-contained diagram: a viewer can interpret symbols and context without external docs."}, {"type": "llm_judge", "name": "Actionability for Standardization and Optimization", "description": "Highlights of control points, KPIs, and improvement hooks that make the map actionable for future optimization.", "weight": 2.0, "judge_prompt": "Evaluate actionability:\n- Clear control points/checkpoints (e.g., quality checks, staging/buffers) indicated.\n- References to KPIs or measurement points (e.g., dwell time, read rate, jam rate) or places to capture them.\n- Notes call out known bottlenecks or improvement levers (e.g., buffer sizing, rework minimization).\n\nScoring:\n- 2.0: Multiple clear action hooks and measurement points indicated.\n- 1.0: Some actionable hints but limited coverage.\n- 0.0: No evident guidance for standardization or optimization.\n", "expectation": "Shows where to measure and control, enabling leadership to use the map for improvements."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "ec2fccc9-b7f6-4c73-bf51-896fdb433cec", "rubric": {"category_name": "Editors \u2014 SEO Blog (NFT Photography)", "rationale": "This rubric uses a self-documenting, staged approach. Stage 1 (LLM-only) strictly enforces deliverable shape so later verification is trivial. Stage 2 mixes light code checks (bounds, presence) with higher-weight LLM checks for correctness and coverage. Stage 3 applies holistic quality assessment for tone, readability, SEO craft, and professional polish. Code rules return normalized scores in [0,1]; the framework weights them. LLM judges handle flexible structural and nuanced content checks.", "max_total_score": 25.0, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate (LLM-only)", "description": "Gate: Enforce exact deliverable shape for a Word document SEO blog with specified sections, structure, and artifacts.", "is_required": true, "max_points": 5.0, "min_score_to_pass": 4.0, "rules": [{"type": "llm_judge", "name": "Structured Document Requirements (DOCX blog)", "description": "Check that the submission is a Word document with the exact structural elements required to enable verification.", "weight": 5.0, "judge_prompt": "You are evaluating whether the candidate\u2019s output meets these STRUCTURAL requirements only. Do NOT judge correctness or writing quality; only confirm presence/format. The expected output is a Word document (DOCX) for an SEO blog.\n\nCheck the following:\n1) File Format\n- Must be a DOCX (Word document). PDF or plain text is NOT acceptable for this task.\n\n2) Title and Subheading\n- Title present at the top and matches exactly: \u201cWhat is NFT Photography? An Introductory Guide\u201d.\n- A clear subheading (subtitle) directly under the title.\n\n3) Headings and Structure\n- Uses H2 and H3 headers to break up the text (multiple H2s and at least a few H3s).\n- Includes a clearly labeled section near the end called \u201cWhat\u2019s Next\u201d (or very similar phrasing) that signals a coming deeper exploration including artist interviews and practical demonstrations around minting NFTs for photographers.\n\n4) Formatting Use\n- Bold and italic text are used within paragraphs to emphasize content (not just in headings).\n\n5) Links\n- Includes links to relevant news or explainer articles with descriptive (SEO-friendly) anchor text.\n- Includes links to multiple travel photographers\u2019 NFT collections or their social media profiles (at least 3 distinct artists/links highlighted within the article body).\n\n6) Pull Quote\n- A single \u2018pull quote\u2019 is selected and a caption at the very bottom of the document indicates what the pull quote will be (e.g., a line beginning with \u201cPull Quote:\u201d followed by the quote).\n\n7) Secondary Keywords List\n- After the article copy, there is a list of exactly four additional related secondary keywords chosen via SEO research, labeled under a heading such as \u201cSecondary Keywords\u201d or \u201cSEO Keywords\u201d. These are explicitly listed so the client can record them.\n\n8) Length Target (STRUCTURE ONLY)\n- The article body appears to be approximately 1,500 words (\u00b110%). For Stage 1, just confirm that it is in the expected ballpark visually; detailed counting occurs in Stage 2.\n\nScoring:\n- 5.0: DOCX format AND all elements 2\u20137 present.\n- 4.0: DOCX format AND only one minor omission (e.g., missing explicit H3s OR pull quote caption line OR secondary keyword heading label) but most structure is present.\n- 2.5: DOCX format but multiple structural gaps (missing 2\u20133 key elements from 2\u20137).\n- 0.0: Not DOCX OR missing major structural pieces (e.g., no H2/H3 and no links and no secondary keyword list).\nOnly assess presence/structure, not content quality or correctness.", "expectation": "A properly structured DOCX blog with title, subheading, H2/H3s, links to news and artists, bold/italics usage, pull quote caption at bottom, and a clearly listed set of four secondary keywords after the article."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Verification", "description": "Verify key requirements now that structure is enforced. Mix of code (deterministic checks) and LLM (nuanced content).", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Word Count Within Target Range", "description": "Article body should be ~1,500 words with 10% leeway (1,350\u20131,650 words).", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document output or wrong type\"\n        text = \"\"\n        # Prefer DOCX, fallback to PDF/text if needed\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                text = context.files.read_text(output.id)\n        # Heuristic: treat whole text as article body (Stage 1 handled shape)\n        words = re.findall(r\"\\b[\\w'-]+\\b\", text)\n        wc = len(words)\n        min_wc, max_wc = 1350, 1650\n        score = 1.0 if (wc >= min_wc and wc <= max_wc) else 0.0\n        return score, f\"Word count: {wc} (target 1350\u20131650)\"\n    except Exception as e:\n        return 0.0, f\"Exception in word count check: {e}\""}, {"type": "code", "name": "Primary Keyword Usage: 'NFT photography'", "description": "Ensure the primary keyword appears in the title and multiple times in the body for SEO targeting.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document output or wrong type\"\n        text = \"\"\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                text = context.files.read_text(output.id)\n        low = text.lower()\n        # Title = first non-empty line\n        lines = [ln.strip() for ln in low.splitlines() if ln.strip()]\n        title = lines[0] if lines else \"\"\n        title_has = \"what is nft photography? an introductory guide\" in title\n        # Count occurrences\n        occ = len(re.findall(r\"\\bnft photography\\b\", low))\n        score = 0.0\n        if occ >= 3:\n            score = 0.5\n        if occ >= 6:\n            score = 0.8\n        if title_has:\n            score = min(1.0, score + 0.2)\n        return score, f\"Primary keyword occurrences: {occ}; TitleExact={title_has}\"\n    except Exception as e:\n        return 0.0, f\"Exception in primary keyword check: {e}\""}, {"type": "code", "name": "Secondary Keywords List Present (\u22654) After Article", "description": "Verify a clearly labeled section after the article body listing four related secondary keywords.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document output or wrong type\"\n        text = \"\"\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                text = context.files.read_text(output.id)\n        # Find a section header like \"Secondary Keywords\" or \"SEO Keywords\"\n        m = re.search(r\"^(.*?(secondary|seo)[^\\n]{0,20}keyword[s]?).*$\", text, flags=re.I|re.M)\n        if not m:\n            return 0.0, \"No labeled secondary keywords section found\"\n        # Extract subsequent lines to capture listed items\n        start_idx = m.end()\n        tail = text[start_idx:start_idx+1000]\n        lines = [ln.strip(\"- \u2022* \\t\") for ln in tail.splitlines() if ln.strip()]\n        # Collect potential keyword items from first ~10 lines after header\n        items = []\n        for ln in lines[:10]:\n            parts = [p.strip() for p in re.split(r\",|;|\\u2022|\\u2023|\\||/\", ln) if p.strip()]\n            for p in parts:\n                # Heuristic: keep short phrases (1\u20135 words)\n                if 1 <= len(p.split()) <= 5 and re.search(r\"[A-Za-z]\", p):\n                    items.append(p.lower())\n        uniq = list(dict.fromkeys(items))\n        cnt = len(uniq)\n        score = 1.0 if cnt >= 4 else (0.5 if cnt >= 2 else 0.0)\n        return score, f\"Detected secondary keyword items: {cnt}\"\n    except Exception as e:\n        return 0.0, f\"Exception in secondary keywords check: {e}\""}, {"type": "code", "name": "\"What\u2019s Next\" Section Mentions Interviews and Minting", "description": "Confirm closing section references future content with artist interviews and minting demos.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document output or wrong type\"\n        text = \"\"\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                text = context.files.read_text(output.id)\n        low = text.lower()\n        # Look for a closing section\n        has_next_hdr = re.search(r\"what[\u2019']s next|coming next|next up|what is next\", low)\n        mentions_interviews = \"interview\" in low or \"interviews\" in low\n        mentions_minting = \"mint\" in low or \"minting\" in low\n        if has_next_hdr and mentions_interviews and mentions_minting:\n            return 1.0, \"Has next-section with interviews and minting\"\n        if has_next_hdr and (mentions_interviews or mentions_minting):\n            return 0.5, \"Has next-section but missing either interviews or minting reference\"\n        return 0.0, \"No clear next-section or missing key references\"\n    except Exception as e:\n        return 0.0, f\"Exception in next-section check: {e}\""}, {"type": "llm_judge", "name": "Beginner-Friendly Explanation and Tone", "description": "Checks that the article introduces NFT photography in a friendly, conversational, non-technical manner appropriate for non-web3 readers.", "weight": 3.0, "judge_prompt": "Evaluate the correctness and suitability of the explanation for a beginner audience:\n- Is NFT photography clearly defined without heavy jargon?\n- Are complex ideas explained in plain language with relatable examples?\n- Is the tone friendly and conversational throughout?\n- Are H2/H3 sections used to guide a non-technical reader through the concepts logically?\nScoring:\n- 3.0: Clear, accurate, beginner-friendly explanation with consistently approachable tone.\n- 2.0: Generally friendly and clear, with minor jargon or gaps.\n- 1.0: Some attempt at clarity but too technical or confusing in places.\n- 0.0: Unclear, overly technical, or inaccurate explanation for beginners.", "expectation": "A warm, accessible primer that demystifies NFT photography for non-web3 readers."}, {"type": "llm_judge", "name": "Benefits, Monetization, and Buyer Motivations", "description": "Verify the article correctly explains benefits to photographers/industry, how NFT photographers make money, and why people buy photography NFTs.", "weight": 2.3, "judge_prompt": "Assess the substantive correctness and coverage:\n- Benefits to photographers (ownership, provenance, royalties, community, discovery, new revenue) are articulated accurately.\n- Monetization methods are described (primary sales, secondary royalties, editions, licensing, drops, collaborations), with practical context for photographers.\n- Buyer motivations are covered (supporting artists, collecting scarce works, utility or access, social signaling, curation).\n- Claims are reasonable and not misleading.\nScoring:\n- 2.3: Thorough, accurate coverage with concrete, relevant examples.\n- 1.5: Mostly accurate with minor omissions.\n- 0.8: Superficial or partially accurate.\n- 0.0: Inaccurate or missing core points.", "expectation": "Correct, concrete explanation of how photographers earn and why collectors buy, with realistic benefits."}, {"type": "llm_judge", "name": "Artist Highlights and Relevance (Travel Photographers)", "description": "Check that multiple travel photographers\u2019 NFT collections are highlighted with links to their collections or social profiles.", "weight": 2.0, "judge_prompt": "Evaluate whether the article highlights several travel photographers and links to their NFT collections or social media profiles:\n- At least three distinct travel photographers are mentioned.\n- Each highlighted artist is relevant (travel photography context) and the inclusion feels grounded.\n- Links to their collection pages or social profiles are present and appear appropriate.\nScoring:\n- 2.0: \u22653 relevant travel photographers highlighted with appropriate links.\n- 1.2: 2 relevant artists linked.\n- 0.6: Only 1 artist linked or relevance is weak.\n- 0.0: No credible artist highlights/links.", "expectation": "Several relevant travel photographers are credibly highlighted with working links."}, {"type": "llm_judge", "name": "External News/Explainer Links with SEO-Friendly Anchors", "description": "Verify the presence and suitability of links to relevant news/explainer articles using descriptive anchor text.", "weight": 1.7, "judge_prompt": "Check:\n- There are multiple links (\u22653) to relevant news or explainer articles about NFTs/photography.\n- Anchor text is descriptive and SEO-friendly (not just \u201cclick here\u201d).\n- Links are contextually integrated (placed where they support claims or provide background).\nScoring:\n- 1.7: 3+ relevant links, good anchors, natural integration.\n- 1.0: 2 good links or mixed anchor quality.\n- 0.5: Only 1 passable link.\n- 0.0: No relevant external links with descriptive anchors.", "expectation": "Multiple relevant news/explainer links with strong, descriptive anchor text integrated naturally."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Holistic Quality Assessment", "description": "LLM-only qualitative review of presentation, SEO craft, audience fit, and professional polish.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Readability, Flow, and Accessibility", "description": "Is the article engaging, logically organized, and easy for beginners to follow?", "weight": 2.5, "judge_prompt": "Judge the overall readability and flow:\n- Clear organization with H2/H3 guiding a logical journey.\n- Smooth transitions and structure that keeps a reader engaged.\n- Jargon explained or avoided; examples used to clarify.\n- Friendly, conversational tone maintained throughout.\nScore 0.0\u20132.5 accordingly.", "expectation": "A smooth, coherent, approachable read for non-technical audiences."}, {"type": "llm_judge", "name": "SEO Craft and On-Page Optimization Quality", "description": "Quality of SEO execution beyond mere presence: keyword integration, headings, subheading, and formatting emphasis.", "weight": 2.0, "judge_prompt": "Evaluate on-page SEO quality:\n- Primary keyword in title; natural integration of primary and secondary keywords without stuffing.\n- Effective use of H2/H3 hierarchy; subheading supports the title and includes value.\n- Appropriate use of bold/italic emphasis to highlight skimmable insights.\n- Clear pull quote that could be used in marketing or as a callout.\nScore 0.0\u20132.0 based on overall execution quality.", "expectation": "Well-structured SEO writing with natural keyword use and helpful formatting."}, {"type": "llm_judge", "name": "Brand/Use-Case Alignment and Strategic Value", "description": "Does the article align with the client context (travel photography app moving into NFTs) and provide practical value?", "weight": 1.5, "judge_prompt": "Assess strategic alignment:\n- Connects NFT photography concepts to travel photography use-cases.\n- Appropriately references high-end platforms like SuperRare (curated galleries) in a way consistent with client context.\n- Offers practical tips or perspective that builds trust and primes readers for deeper future content.\nScore 0.0\u20131.5 accordingly.", "expectation": "Content feels tailored to a travel photography app expanding into NFT collectibles on curated platforms."}, {"type": "llm_judge", "name": "Professional Polish (Grammar, Clarity, Anchors)", "description": "Final-quality polish: grammar, clarity, consistent anchors, and overall professionalism.", "weight": 2.0, "judge_prompt": "Check for:\n- Clean grammar and style; minimal typos.\n- Consistent, descriptive anchor text; sensible linking approach.\n- Overall professional presentation suitable for publication.\nScore 0.0\u20132.0 accordingly.", "expectation": "Publication-ready quality with minimal edits required."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "b1a79ce1-86b0-41fb-97dc-9206dfd7b044", "rubric": {"category_name": "Music Video Moodboard (PNG) \u2014 Producers and Directors", "rationale": "This rubric enforces a self-documenting deliverable for a visual moodboard by requiring a PNG collage plus a companion manifest document that exposes palette, references, and creative rationale in a verifiable structure. Stage 1 (LLM-only) is a strict shape gate so the artifacts are evaluable. Stage 2 mixes lightweight code checks (counts, regex) with heavier LLM verification of alignment and coherence. Stage 3 holistically judges professional quality, originality, and production practicality.", "max_total_score": 40.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate", "description": "Confirm the output is a PNG moodboard plus a companion manifest document with clearly labeled sections. This is a structural gate only.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Moodboard PNG + Manifest Structure", "description": "Verify the presence and structure of the required artifacts.", "weight": 8.0, "judge_prompt": "You are evaluating SHAPE ONLY (not content quality). Check the candidate outputs you can view.\n\nRequired artifacts:\n1) A PNG moodboard image (collage) that visibly includes:\n   - A clearly arranged set of reference images (at least 6 distinct visuals)\n   - A color palette area (5\u20138 swatches) with visible labels or hex codes near/under swatches\n   - Some brief annotations/labels tying references to categories (e.g., Wardrobe, Set/Art, Lighting, Camera/Framing, Makeup/Hair). Exact wording can vary but intent should be clear from labels.\n\n2) A companion manifest document (PDF, DOCX, or Markdown) titled with words like \u201cMoodboard Manifest,\u201d \u201cBoard Notes,\u201d or \u201cLookbook Notes\u201d that contains the following clearly labeled sections (flexible naming allowed):\n   - Overview/Summary (at least 3 sentences) describing the concept\n   - Color Palette with 5\u20138 hex codes (e.g., #112233)\n   - References/Sources list (at least 6 items; URLs or source names acceptable)\n   - Visual Motifs and Department Notes covering at least three of: Wardrobe, Set/Art Direction, Lighting/Cinematography, Makeup/Hair\n   - Do/Don\u2019t (or Exclusions/Constraints) section\n   - Credits/Licensing/Attribution note\n\nFormat constraints:\n- The moodboard must be a PNG image file (not JPEG, not PDF).\n- The companion manifest must be a separate readable document (PDF/DOCX/MD). It should be at least ~300 words total.\n\nScoring (structure only):\n- 8.0: PNG with collage + visible palette swatches, labels, and annotations AND companion manifest present with all listed sections.\n- 6.0: PNG OK as above AND manifest present but missing exactly one section OR palette swatches present but without visible labels/hex on the PNG while the manifest lists proper hex codes.\n- 4.0: PNG present with collage and palette swatches but companion manifest missing OR manifest present but PNG is not clearly a collage or lacks a palette area.\n- 0.0: No PNG, wrong image format, or outputs don\u2019t match the required structure.\n\nOnly assess presence/structure. Do not judge quality or thematic fit here.", "expectation": "A PNG moodboard collage plus a companion manifest document with the specified sections and visible palette/annotations on the image."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness & Consistency)", "description": "Check that the deliverables align with the creative brief and include verifiable elements (palette codes, references count, and section coverage).", "is_required": false, "max_points": 20.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Presence of PNG moodboard with reasonable file size", "description": "Verify there is at least one PNG image file and it appears substantive (not a tiny placeholder).", "weight": 1.0, "code": "def evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs()\n    except Exception:\n        outputs = []\n    pngs = []\n    for r in outputs:\n        try:\n            if getattr(r, 'is_image', False):\n                p = context.files.get_path(r.id)\n                if str(p).lower().endswith('.png'):\n                    pngs.append((r, p))\n        except Exception:\n            continue\n    if not pngs:\n        return 0.0\n    # Choose the largest PNG by file size\n    try:\n        sizes = []\n        for r, p in pngs:\n            try:\n                sizes.append(p.stat().st_size)\n            except Exception:\n                sizes.append(0)\n        max_size = max(sizes) if sizes else 0\n    except Exception:\n        max_size = 0\n    # Heuristic thresholds (bytes)\n    if max_size >= 100_000:  # >= ~100KB\n        return 1.0\n    if max_size >= 30_000:   # between ~30KB and 100KB\n        return 0.5\n    return 0.0"}, {"type": "code", "name": "Palette contains 5\u20138 HEX colors in manifest", "description": "Extract text from any companion document and count unique HEX color codes.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    def extract_all_text():\n        texts = []\n        try:\n            outputs = context.get_all_outputs()\n        except Exception:\n            outputs = []\n        for r in outputs:\n            try:\n                if getattr(r, 'is_text_format', False):\n                    texts.append(context.files.read_text(r.id))\n                elif getattr(r, 'is_document', False):\n                    path = context.files.get_path(r.id)\n                    suf = str(path.suffix).lower()\n                    if suf == '.pdf':\n                        try:\n                            texts.append(context.files.read_pdf_text(r.id))\n                        except Exception:\n                            pass\n                    elif suf == '.docx':\n                        try:\n                            texts.append(context.files.read_docx_text(r.id))\n                        except Exception:\n                            pass\n            except Exception:\n                continue\n        return \"\\n\".join(texts)\n\n    text = extract_all_text()\n    if not text.strip():\n        return 0.0\n    hexes = set(re.findall(r\"#[0-9A-Fa-f]{6}\", text))\n    n = len(hexes)\n    if 5 <= n <= 8:\n        return 1.0\n    if 3 <= n <= 4:\n        return 0.5\n    return 0.0"}, {"type": "code", "name": "References list has sufficient sources/links", "description": "Check that at least 6 references are present by counting URLs in the manifest text.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    def extract_all_text():\n        texts = []\n        try:\n            outputs = context.get_all_outputs()\n        except Exception:\n            outputs = []\n        for r in outputs:\n            try:\n                if getattr(r, 'is_text_format', False):\n                    texts.append(context.files.read_text(r.id))\n                elif getattr(r, 'is_document', False):\n                    path = context.files.get_path(r.id)\n                    suf = str(path.suffix).lower()\n                    if suf == '.pdf':\n                        try:\n                            texts.append(context.files.read_pdf_text(r.id))\n                        except Exception:\n                            pass\n                    elif suf == '.docx':\n                        try:\n                            texts.append(context.files.read_docx_text(r.id))\n                        except Exception:\n                            pass\n            except Exception:\n                continue\n        return \"\\n\".join(texts)\n\n    text = extract_all_text()\n    if not text.strip():\n        return 0.0\n    urls = re.findall(r\"https?://[^\\s)]+\", text)\n    n = len(urls)\n    if n >= 6:\n        return 1.0\n    if n >= 4:\n        return 0.6\n    if n >= 2:\n        return 0.3\n    return 0.0"}, {"type": "code", "name": "Manifest includes required section keywords", "description": "Fuzzy match for key sections to ensure broad coverage in the manifest.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    def extract_all_text():\n        texts = []\n        try:\n            outputs = context.get_all_outputs()\n        except Exception:\n            outputs = []\n        for r in outputs:\n            try:\n                if getattr(r, 'is_text_format', False):\n                    texts.append(context.files.read_text(r.id))\n                elif getattr(r, 'is_document', False):\n                    path = context.files.get_path(r.id)\n                    suf = str(path.suffix).lower()\n                    if suf == '.pdf':\n                        try:\n                            texts.append(context.files.read_pdf_text(r.id))\n                        except Exception:\n                            pass\n                    elif suf == '.docx':\n                        try:\n                            texts.append(context.files.read_docx_text(r.id))\n                        except Exception:\n                            pass\n            except Exception:\n                continue\n        return \"\\n\".join(texts)\n\n    text = extract_all_text().lower()\n    if not text.strip():\n        return 0.0\n    buckets = [\n        any(k in text for k in ['overview', 'summary', 'concept']),\n        any(k in text for k in ['palette', 'color palette', 'colour palette']),\n        any(k in text for k in ['references', 'sources', 'credits']),\n        any(k in text for k in ['motif', 'motifs', 'themes', 'visual motifs']),\n        any(k in text for k in ['lighting', 'cinematography', 'camera']),\n        any(k in text for k in [\"do don't\", \"do/don't\", 'exclusions', 'constraints', \"don'ts\", 'dos'])\n    ]\n    score = sum(1 for b in buckets if b)\n    return min(1.0, score / 5.0)  # require at least 5/6 for full credit; 4/6=0.8, etc."}, {"type": "llm_judge", "name": "Alignment with Brief (Mood, Tone, Themes)", "description": "Verify the moodboard expresses the slow-building orchestral, dramatic, elegant feel with tension and vulnerability, and the theme of outer beauty vs inner conflict.", "weight": 4.0, "judge_prompt": "Using the PNG moodboard and the companion manifest, judge alignment with this brief: slow-building ballad with orchestral elements; dramatic and emotional; elegant but tense and vulnerable; rich, theatrical visual style; central contrast of outer beauty vs inner conflict. Consider palette choices, imagery, textures, lighting cues, and annotations that indicate how contrast is portrayed (e.g., mirrors, fractured reflections, pristine vs distressed textures).\n\nScoring:\n- 4.0: Strong, clear alignment across multiple elements (palette, imagery, annotations). The contrast theme is visually and conceptually evident.\n- 2.5: Generally aligned but with notable gaps or mixed signals; the contrast theme is present but not strongly supported.\n- 1.0: Weak or superficial alignment; few elements relate to the brief.\n- 0.0: Misaligned with the brief.", "expectation": "The board should feel elegant and theatrical, with visible tension/vulnerability cues and a clear visual metaphor for outer vs inner conflict."}, {"type": "llm_judge", "name": "Color Palette Appropriateness and Coherence", "description": "Assess whether the palette supports the brief and is cohesive across the board and manifest.", "weight": 4.0, "judge_prompt": "Evaluate the palette shown on the PNG and documented in the manifest. Does it:\n- Contain 5\u20138 purposeful colors with HEX codes in the manifest?\n- Support an elegant, dramatic, theatrical mood (e.g., deep jewel tones, muted metallics, rich neutrals) without garish clashes?\n- Show thoughtful contrast to express outer beauty vs inner conflict (e.g., polished vs distressed tones)?\n\nScoring:\n- 4.0: Highly coherent, on-brief palette with clear rationale/contrast in evidence.\n- 2.5: Mostly coherent; some colors feel off-brief or redundant.\n- 1.0: Incoherent or mismatched to the brief.\n- 0.0: No usable palette.", "expectation": "A refined, theatrical palette with clearly documented HEX codes and visible swatches that reinforce the story."}, {"type": "llm_judge", "name": "Motifs, Departments, and Cinematic Direction Fit", "description": "Check if motifs and department notes (wardrobe, set/art, lighting/camera, makeup/hair) translate the brief into filmable direction.", "weight": 4.0, "judge_prompt": "From the PNG annotations and manifest notes, assess whether there are concrete, filmable ideas across departments (wardrobe, set/art direction, lighting/cinematography, makeup/hair). Look for theatrical staging ideas (e.g., curtains, spotlight, haze), materials (velvet, satin, glass), and camera/lighting notes (chiaroscuro, push-ins, dolly, practicals), that express elegance with tension/vulnerability.\n\nScoring:\n- 4.0: Clear, department-specific guidance with practical cinematic cues aligned to the brief.\n- 2.5: Some useful cues, but spotty or generic.\n- 1.0: Minimal usable direction.\n- 0.0: No actionable guidance.", "expectation": "Actionable, department-specific motifs and notes that map the brief to production decisions."}, {"type": "llm_judge", "name": "Reference Cohesion and Attribution", "description": "Determine if references are stylistically cohesive, on-brief, and reasonably attributed.", "weight": 4.0, "judge_prompt": "Review the collage and manifest references. Judge whether the references are stylistically cohesive (era, production value), appropriate to the brief, and reasonably attributed (URLs or identifiable sources). Penalize if references are random, brand/logo-heavy without relevance, or uncredited.\n\nScoring:\n- 4.0: Cohesive, on-brief references with clear attribution.\n- 2.5: Mostly cohesive; some weak or uncredited items.\n- 1.0: Incoherent or poorly matched references.\n- 0.0: References absent or unusable.", "expectation": "A concise, cohesive set of references that support the concept and include clear sources."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality Assessment", "description": "Holistic evaluation of professional presentation, originality, storytelling, and production practicality.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Art Direction Clarity and Storytelling", "description": "Does the board communicate a clear story arc or emotional journey consistent with the track?", "weight": 3.0, "judge_prompt": "Assess whether the moodboard communicates a coherent visual narrative or emotional progression appropriate for a slow-building, orchestral ballad. Consider whether the sequence of images, annotations, and palette choices suggest a beginning, build, and climax or other emotional arc.\n\nScoring:\n- 3.0: Compelling, clear visual story.\n- 2.0: Some sense of story but diffuse.\n- 1.0: Minimal narrative clarity.\n- 0.0: No apparent storytelling.", "expectation": "A clear emotional arc conveyed through image selection, palette, and annotations."}, {"type": "llm_judge", "name": "Production Practicality", "description": "Judge whether ideas are achievable in a typical music video production context.", "weight": 3.0, "judge_prompt": "Evaluate the practicality of the proposed sets, wardrobe, lighting, and camera ideas for a music video. Prefer ideas that are impactful yet achievable with reasonable resources (e.g., controlled locations, rentals, stagecraft). Penalize if the concepts require implausible VFX or inaccessible locations without alternatives.\n\nScoring:\n- 3.0: Highly actionable and feasible.\n- 2.0: Mostly feasible with minor issues.\n- 1.0: Questionable feasibility.\n- 0.0: Impractical.", "expectation": "Cinematic yet achievable directions with awareness of constraints."}, {"type": "llm_judge", "name": "Originality and Taste Level", "description": "Evaluate distinctiveness and tastefulness of the vision while fitting the brief.", "weight": 3.0, "judge_prompt": "Consider whether the visual strategy shows originality and refined taste while fitting the dramatic, elegant brief. Avoid clich\u00e9s unless subverted purposefully. Penalize derivative, trend-chasing collages lacking a cohesive point of view.\n\nScoring:\n- 3.0: Distinctive, tasteful, and on-brief.\n- 2.0: Competent but familiar.\n- 1.0: Derivative or unfocused.\n- 0.0: Poor taste or incoherent.", "expectation": "A refined, distinctive take that enhances the song\u2019s mood."}, {"type": "llm_judge", "name": "Presentation Polish", "description": "Assess layout clarity, legibility of labels, and overall professionalism of the PNG and manifest.", "weight": 3.0, "judge_prompt": "Judge whether the PNG collage is neatly arranged (consistent margins, readable labels, clean palette swatches) and the manifest is well-formatted (clear headings, consistent typography, error-free). Consider export quality (resolution) and readability.\n\nScoring:\n- 3.0: Professional polish throughout.\n- 2.0: Minor issues.\n- 1.0: Noticeable roughness.\n- 0.0: Low-quality presentation.", "expectation": "A crisp, legible PNG and a clean, well-structured manifest."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "cd9efc18-d14a-4f69-8531-5d178a08084d", "rubric": {"category_name": "Texas Last Will and Testament \u2014 Drafting (Estate Planning)", "rationale": "This rubric enforces a self-documenting, verifiable legal document. Stage 1 mandates a precise Texas will structure in a PDF (8\u201311 pages). Stage 2 mixes deterministic code checks (names, roles, dates, trust parameters) with LLM cross-references for Texas-specific compliance and logical correctness. Stage 3 assesses drafting quality, organization, and professional suitability for client execution in Texas.", "max_total_score": 30.0, "stages": [{"name": "Stage 1 \u2014 Structural Format Gate (PDF Texas Will)", "description": "LLM-only gate ensuring the output is a properly structured Texas Last Will and Testament in a PDF with all required sections and execution formalities to enable verification.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Required Document Shape and Sections (Texas Will)", "description": "Verify the candidate output is a professionally formatted PDF will with the mandated Texas sections, roles, and execution pages.", "weight": 8.0, "judge_prompt": "You are evaluating whether the submitted output is a properly structured Texas Last Will and Testament prepared for the client described. Only check PRESENCE/STRUCTURE and format, not legal correctness.\n\nFormat requirements:\n- File format must be PDF (preferred) or DOCX. If both are present, evaluate the PDF. Reject plain text/Excel.\n- Length should be approximately 8\u201311 pages.\n- Professional formatting with clear headings, numbered articles/sections, and adequate spacing.\n\nRequired front-matter and identity:\n- Title: \u201cLast Will and Testament of Grace J. Parsons\u201d (or very close variant).\n- Introductory clause identifying: Grace J. Parsons, residence in Austin, Texas (or Travis County, Texas), marital status married to Thomas A. Parsons.\n- Revocation clause revoking prior wills and codicils.\n\nCore dispositive structure (as clearly headed Articles/Sections):\n- Appointment of Executor: Thomas A. Parsons as Executor; alternate executor: Sarah R. Roberts; independent administration and waiver of bond (or similar Texas phrasing). Executor given sole discretion to distribute tangible personal property.\n- Primary beneficiary clause: Entire estate to spouse if spouse survives Client (survivorship condition articulated).\n- Contingent beneficiaries: If spouse predeceases, residue to children (Timothy S. Parsons and Joshua J. Parsons) in equal shares; if no spouse or descendants survive, distribute in equal shares to Sarah R. Roberts and Howard C. Long.\n- Tangible personal property and general personal property provisions (can be integrated with executor discretion or a separate article).\n- Residuary clause clearly labeled.\n\nTrusts and guardianship:\n- Testamentary trust for minor beneficiaries: specifies minimum distribution age 25 and a maximum trust duration of 21 years (or equivalent RAP-compliant cap), includes spendthrift clause, and trustee discretion (including power to sell/distribute estate property).\n- Trustees: primary trustee Sarah R. Roberts; alternate trustee Howard C. Long.\n- Guardianship: Sarah R. Roberts as primary guardian of the children; Howard C. Long as alternate; temporary local guardian: Michael T. Fisher.\n\nAdministrative/fiduciary provisions:\n- Fiduciary powers (sell, invest, manage, allocate principal/income, employ agents, etc.).\n- Survivorship/simultaneous death provision (e.g., 120-hour rule or similar Texas survivorship language).\n- Tax apportionment/expenses and no-contest clause (optional but common; presence not required).\n\nExecution and self-proving:\n- Execution date indicated as May 13, 2025 (acceptable if shown in attestation and/or self-proving affidavit).\n- Signature block for Testator: Grace J. Parsons.\n- Two witness attestation with named witnesses: Jose P. Harris and Geraldine R. Watson, with signature lines.\n- Notary public block and self-proving affidavit in Texas style, same date as execution; notary designation (e.g., \u201cNotary Public, State of Texas\u201d) and space for seal.\n\nScoring (0\u20138 points):\n- 8: PDF/DOCX, 8\u201311 pages, professional formatting, and ALL required sections/blocks present and clearly labeled.\n- 6\u20137: Correct format and length; one minor section missing or merged ambiguously (e.g., survivorship or explicit waiver of bond), but dispositive scheme, trust, guardianship, and full execution/self-proving blocks are present.\n- 3\u20135: Valid document but missing multiple required structural elements (e.g., trust or guardianship or residuary clause or self-proving affidavit), or lacks clear headings/numbering.\n- 1\u20132: Document present but wrong format/length or largely unstructured; many required sections absent.\n- 0: Not a PDF/DOCX will; or fewer than 2 pages; or entirely missing the core will structure.\n\nOnly evaluate presence/structure. Do not judge correctness or drafting quality here.", "expectation": "A cleanly structured, 8\u201311 page Texas will in PDF with clear articles, identity, executor scheme, primary and contingent dispositions, a minors\u2019 trust with trustee/guardian appointments and spendthrift, fiduciary powers, survivorship, and a complete execution and Texas self-proving affidavit with the specified witnesses and date."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification of Legal Content and Consistency", "description": "Deterministic checks and LLM review for correctness and Texas-specific compliance, leveraging the enforced structure from Stage 1.", "is_required": true, "max_points": 12.0, "min_score_to_pass": 6.0, "rules": [{"type": "code", "name": "Names and Parties Coverage", "description": "Verify presence of all required names and roles anywhere in the document text.", "weight": 0.75, "code": "def evaluate(workflow, context):\n    import re\n    \n    def read_text_from_output():\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return \"\"\n        text = \"\"\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_text(output.id)\n                except Exception:\n                    text = \"\"\n        return text or \"\"\n    \n    text = read_text_from_output()\n    if not text:\n        return 0.0\n    t = text.lower()\n    required_names = [\n        'grace j. parsons',\n        'thomas a. parsons',\n        'sarah r. roberts',\n        'timothy s. parsons',\n        'joshua j. parsons',\n        'howard c. long',\n        'michael t. fisher',\n        'jose p. harris',\n        'geraldine r. watson'\n    ]\n    score = 0\n    for name in required_names:\n        if name in t:\n            score += 1\n    return score / len(required_names)"}, {"type": "code", "name": "Execution Formalities: Date, Witnesses, Notary, Self-Proving", "description": "Check for May 13, 2025 date, both witness names, notary presence, and a self-proving affidavit reference.", "weight": 0.75, "code": "def evaluate(workflow, context):\n    import re\n    def read_text_from_output():\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return \"\"\n        text = \"\"\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_text(output.id)\n                except Exception:\n                    text = \"\"\n        return text or \"\"\n    \n    txt = read_text_from_output()\n    if not txt:\n        return 0.0\n    t = txt.lower()\n    # Date variants: \"May 13, 2025\" or \"13th day of May, 2025\"\n    has_mdy = 'may 13, 2025' in t\n    has_day_of = bool(re.search(r\"\\b13(?:th)?\\s+day\\s+of\\s+may[,\\s]+2025\\b\", t))\n    has_date = has_mdy or has_day_of\n    has_w1 = 'jose p. harris' in t\n    has_w2 = 'geraldine r. watson' in t\n    has_notary = ('notary public' in t) or ('seal' in t) or ('commission expires' in t)\n    has_self_proving = ('self-proving' in t) or ('self proving' in t) or ('texas estates code' in t)\n    checks = [has_date, has_w1, has_w2, has_notary, has_self_proving]\n    return sum(1 for c in checks if c) / len(checks)"}, {"type": "code", "name": "Beneficiary Scheme Logic Present", "description": "Verify primary to spouse, contingent to children equally, and ultimate fallback to Sarah R. Roberts and Howard C. Long in equal shares, with survivorship language.", "weight": 0.75, "code": "def evaluate(workflow, context):\n    import re\n    def read_text_from_output():\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return \"\"\n        text = \"\"\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_text(output.id)\n                except Exception:\n                    text = \"\"\n        return text or \"\"\n    t = (read_text_from_output() or '').lower()\n    if not t:\n        return 0.0\n    # Primary to spouse if survives\n    primary = ('thomas a. parsons' in t) and (('survive' in t) or ('if he survives me' in t) or ('if my spouse survives me' in t)) and (('entire estate' in t) or ('residuary' in t))\n    # Children equal shares contingent\n    children_equal = (('timothy s. parsons' in t) and ('joshua j. parsons' in t)) and (('equal share' in t) or ('equally' in t) or ('share and share alike' in t))\n    # Ultimate fallback equal shares to Sarah and Howard\n    fallback = ('sarah r. roberts' in t) and ('howard c. long' in t) and (('equal share' in t) or ('equally' in t))\n    # Survivorship/simultaneous death language\n    survivorship = ('120-hour' in t) or ('hundred twenty' in t) or ('simultaneous death' in t) or ('survivorship' in t)\n    checks = [primary, children_equal, fallback, survivorship]\n    return sum(1 for c in checks if c) / len(checks)"}, {"type": "code", "name": "Trust, Guardianship, and Fiduciary Powers", "description": "Check testamentary trust parameters, spendthrift, trustee/guardian appointments (including temporary), and executor personal property discretion/power to sell.", "weight": 0.75, "code": "def evaluate(workflow, context):\n    import re\n    def read_text_from_output():\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return \"\"\n        text = \"\"\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_text(output.id)\n                except Exception:\n                    text = \"\"\n        return text or \"\"\n    t = (read_text_from_output() or '').lower()\n    if not t:\n        return 0.0\n    has_trust = ('testamentary trust' in t) or ('trust for minor' in t) or ('trust for my child' in t)\n    has_age_25 = ('age 25' in t) or bool(re.search(r'age\\s+twenty[-\\s]?five', t)) or bool(re.search(r'\\b25\\b', t))\n    has_21_years = ('21 years' in t) or ('twenty-one years' in t) or ('twenty one years' in t)\n    has_spendthrift = 'spendthrift' in t\n    trustee_named = ('sarah r. roberts' in t) and ('trustee' in t)\n    alt_trustee = ('howard c. long' in t) and ('trustee' in t)\n    guardian_named = ('sarah r. roberts' in t) and ('guardian' in t)\n    alt_guardian = ('howard c. long' in t) and ('guardian' in t)\n    temp_guardian = ('michael t. fisher' in t) and ('guardian' in t)\n    discretion = ('discretion' in t) or ('sole discretion' in t)\n    sell_power = ('sell' in t) or ('convey' in t) or ('dispose' in t)\n    executor_personal_prop_discretion = (('executor' in t) and ('personal property' in t) and (('sole discretion' in t) or ('discretion' in t)))\n    checks = [has_trust, has_age_25, has_21_years, has_spendthrift, trustee_named, alt_trustee, guardian_named, alt_guardian, temp_guardian, discretion, sell_power, executor_personal_prop_discretion]\n    return sum(1 for c in checks if c) / len(checks)"}, {"type": "llm_judge", "name": "Texas Formalities and Self-Proving Affidavit Compliance", "description": "Assess whether execution language and self-proving affidavit align with Texas practices (independent executor, waiver of bond, two witnesses, Texas notary, same-day execution).", "weight": 3.0, "judge_prompt": "Review the will\u2019s execution components and fiduciary appointments for Texas compliance.\nCheck for:\n- Independent executor appointment and waiver of bond (or similar Texas independent administration).\n- Two witness attestation with named witnesses (Jose P. Harris and Geraldine R. Watson) and same-date execution.\n- Texas self-proving affidavit style (explicit reference to sworn statements before a Texas notary, notary block with capacity/seal, acknowledgment text). \n- County/state references consistent with Texas (e.g., Travis County or State of Texas).\n\nScoring guidance (0\u20133):\n- 3: All items present and clearly compliant with Texas norms.\n- 2: Minor omissions/ambiguities (e.g., bond waiver phrased indirectly), but overall Texas-compliant.\n- 1: Multiple elements weak or unclear (e.g., affidavit too generic), but still likely usable.\n- 0: Substantially non-compliant or missing key execution/self-proving elements.\n", "expectation": "A clean Texas-style execution with an independent executor without bond and a complete self-proving affidavit before a Texas notary with the required witness statements."}, {"type": "llm_judge", "name": "Dispositive Scheme Coherence and Cross-References", "description": "Evaluate whether the dispositive provisions are internally consistent and correctly reference the named parties and conditions.", "weight": 3.0, "judge_prompt": "Evaluate dispositive logic for internal consistency:\n- Primary: Entire estate to spouse (Thomas A. Parsons) if he survives the testator.\n- Contingent: If spouse predeceases, residue to children (Timothy and Joshua) in equal shares.\n- Ultimate: If no spouse/descendants survive, equal shares to Sarah R. Roberts and Howard C. Long.\n- Confirm a clear residuary clause exists, survivorship is addressed (e.g., 120-hour rule), and there are no contradictions (e.g., conflicting percentages).\n- Verify names and roles match across body, trust, guardianship, and signature blocks.\n\nScoring (0\u20133):\n- 3: Fully coherent with explicit conditions, equal shares stated where required, and no contradictions.\n- 2: Minor drafting ambiguity but overall coherent.\n- 1: Noticeable inconsistencies or unclear fallback mechanics.\n- 0: Dispositive structure missing or contradictory.", "expectation": "A consistent, unambiguous chain of gifts with equal shares clearly defined and aligned names and roles."}, {"type": "llm_judge", "name": "Trust Design and Fiduciary Powers Sufficiency", "description": "Check whether the testamentary trust, guardianship, and fiduciary powers are robust and customary for Texas practice.", "weight": 3.0, "judge_prompt": "Review the trust and administration articles for:\n- Testamentary trust for minor beneficiaries with minimum distribution age 25 and maximum duration of 21 years.\n- Spendthrift provision and clear trustee discretion (including power to sell/distribute property), principal/income allocations, and investment/management powers.\n- Trustee appointment: Sarah R. Roberts (primary), Howard C. Long (alternate); guardianship appointments matching (with temporary local guardian Michael T. Fisher).\n- Waiver of bond and accounting formalities for fiduciaries (typical Texas provisions) and tax/expense handling (apportionment), if present.\n\nScoring (0\u20133):\n- 3: Comprehensive trust/administration coverage with clear, customary fiduciary powers.\n- 2: Generally sufficient with minor power gaps or missing boilerplate.\n- 1: Basic trust present but several key powers/appointments unclear.\n- 0: Inadequate or missing trust/fiduciary framework.", "expectation": "A practical minors\u2019 trust with spendthrift and strong fiduciary powers, consistent appointments, and common Texas administrative provisions."}], "on_failure_action": "skip_remaining", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Presentation", "description": "Holistic quality review of drafting clarity, organization, and professional readiness for client execution.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Drafting Clarity and Readability", "description": "Assess clarity, plain-English style, defined terms, and absence of ambiguity while maintaining professional legal tone.", "weight": 3.0, "judge_prompt": "Evaluate the drafting for clarity and readability:\n- Clear headings, numbering, and cross-references; defined terms used consistently (e.g., \u201cExecutor,\u201d \u201cTrustee,\u201d \u201cResiduary Estate\u201d).\n- Plain-English sentences without ambiguity; avoids contradictions.\n- Proper use of survivorship and conditionals (e.g., \u201cif he survives me,\u201d \u201cif none of my descendants survive me\u201d).\nScoring (0\u20133): 3 excellent clarity; 2 minor issues; 1 several unclear passages; 0 confusing or error-prone.", "expectation": "A clear, precise, professional will that a Texas client could read and understand at a high level."}, {"type": "llm_judge", "name": "Organization, Formatting, and Length Target", "description": "Check professional layout, pagination, signature spacing, and length within 8\u201311 pages.", "weight": 3.0, "judge_prompt": "Review professional presentation:\n- Clean page layout, consistent fonts, margins, and numbering; page headers/footers with pagination helpful but optional.\n- Signature and notary blocks clearly separated with adequate white space and labels.\n- Total length near 8\u201311 pages; avoid excessive boilerplate or brevity that omits essentials.\nScoring (0\u20133): 3 strong professional presentation and target length; 2 generally good with minor layout/length issues; 1 adequate but crowded or too sparse; 0 poor formatting or far off target length.", "expectation": "A polished 8\u201311 page PDF suitable for client execution and record-keeping."}, {"type": "llm_judge", "name": "Practical Completeness for Execution", "description": "Ensure the document is ready-to-sign with all signature blocks, initials (if used), dates, and notary lines present and aligned.", "weight": 2.0, "judge_prompt": "Assess execution readiness:\n- All required signature lines for testator, two witnesses, and notary present with labeled name lines; date fields completed for May 13, 2025.\n- Self-proving affidavit includes notary seal area and commission line; county/state indicated.\n- Any ancillary exhibits (if referenced) are included or removed.\nScoring (0\u20132): 2 fully ready-to-sign; 1 minor fixes required; 0 significant execution readiness gaps.", "expectation": "Ready-to-execute packet with no missing lines or placeholders that would delay signing."}, {"type": "llm_judge", "name": "Professional Suitability and Customary Texas Clauses", "description": "Evaluate inclusion of customary Texas clauses that increase robustness (e.g., independent administration language, no-contest clause, digital assets, tax apportionment).", "weight": 2.0, "judge_prompt": "Check for professional polish via customary Texas clauses:\n- Independent administration language and bond waiver.\n- No-contest clause, digital assets provision, and tax apportionment/expense payment (not mandatory but valuable).\n- Coordination between dispositive articles, trust, and administrative powers shows thoughtful drafting.\nScoring (0\u20132): 2 strong inclusion and coordination; 1 partial; 0 minimal beyond bare essentials.", "expectation": "A well-rounded Texas will including common protective clauses without unnecessary complexity."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "116e791e-890c-42b1-ba90-1db02e8bfd45", "rubric": {"category_name": "Pediatric PACU Nursing Care Plan (Post-op ORIF with Spica Cast)", "rationale": "This rubric enforces a self-documenting, one-page PDF nursing care plan with explicit, verifiable structure so that subsequent verification can be performed reliably. Stage 1 is a strict LLM-only format/structure gate. Stage 2 mixes light code checks (file/text parsing, keyword/bounds) with heavier LLM clinical checks focusing on appropriateness and internal consistency. Stage 3 assesses professional quality, safety, and handoff readiness.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Structure and Format Gate (LLM only)", "description": "Verify that the output is a single-page PDF nursing care plan with exactly three nursing diagnoses, each with an outcome, four assessments, and four interventions, in a clearly labeled and reviewable structure.", "is_required": true, "max_points": 2.0, "min_score_to_pass": 1.5, "rules": [{"type": "llm_judge", "name": "Care Plan Structure and Format Gate", "description": "Check the document format and presence of required sections and counts. Do not judge clinical quality here\u2014only verify structure and format.", "weight": 2.0, "judge_prompt": "You are verifying structure only. Examine the candidate output.\nFormat requirements:\n- Must be a PDF (not DOCX/Word, not text, not Excel).\n- Must be exactly ONE page.\n- Professional layout with a visible title such as \u201cNursing Care Plan\u201d (flexible wording OK).\n- Patient context present somewhere on the page: pediatric (age \u22483), post-op ORIF for right femur with a spica cast. Flexible wording is fine.\n\nContent structure requirements:\n- Exactly three nursing diagnoses (flexible naming allowed, e.g., \u201cAcute Pain\u201d, \u201cRisk for Infection\u201d, \u201cImpaired Physical Mobility\u201d, etc.). They must be clearly delineated as three separate diagnoses/sections.\n- For EACH diagnosis, the following labeled subsections are required:\n  \u2022 Outcome (exactly one outcome)\n  \u2022 Assessments (a list of 4 items)\n  \u2022 Interventions (a list of 4 items)\n- Section labels can use synonyms:\n  \u2022 Outcome: Goal, Expected Outcome, Target\n  \u2022 Assessments: Monitoring, Assess/Monitor, Evaluation Checks\n  \u2022 Interventions: Nursing Actions, Actions, Implementation\n- Lists may be bullets or numbered; tables are acceptable if the counts are unambiguous.\n\nScoring (structure only):\n- 2.0: PDF, one page, clear title, patient context present, and exactly 3 diagnoses each with 1 outcome, 4 assessments, and 4 interventions (correct counts and labeled subsections visible).\n- 1.7: PDF and one page with 3 diagnoses and labeled subsections; minor count deviation (one subsection short by 1 item overall) OR title/patient context phrased unclearly but evidently present.\n- 1.5: PDF and one page with 3 diagnoses but missing or unlabeled subsections in one diagnosis (e.g., no clear labels for Assessments/Interventions) OR counts unclear in one diagnosis.\n- 1.0: PDF and one page but only 2 diagnoses present OR multiple unlabeled/missing subsections making counts unverifiable.\n- 0.0: Not a PDF, not one page, or grossly missing required structure (e.g., fewer than 2 diagnoses).\n\nOnly assess presence/format and counts. Do not evaluate clinical accuracy or quality.", "expectation": "A single-page PDF with a clear title, patient context, and three diagnosis sections. Each diagnosis contains exactly one outcome, four assessments, and four interventions, with labels or an equivalent table making counts obvious."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Clinical Verification", "description": "Now that the output is in the correct shape, verify correctness and clinical appropriateness for a 3-year-old, post-op right femur ORIF with spica cast, PACU recovery, pain management, and infection risk reduction.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 3.0, "rules": [{"type": "code", "name": "Context and Safety Keywords Present", "description": "Verify presence of key pediatric PACU and post-op orthopedic context terms to reduce hallucinations and ensure relevant safety content is included.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float or (float, str)\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n\n    text = \"\"\n    try:\n        # Prefer PDF (as mandated), but fall back to DOCX if needed\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = context.files.read_docx_text(output.id)\n    except Exception:\n        return 0.0, \"Unable to extract text from document.\"\n\n    t = text.lower()\n\n    checks = {\n        \"spica\": bool(re.search(r\"\\bspica\\b\", t)),\n        \"orif\": bool(re.search(r\"\\borif\\b|open reduction internal fixation\", t)),\n        \"pain\": bool(re.search(r\"\\bpain\\b\", t)),\n        \"faces\": bool(re.search(r\"faces\\s*scale|wong\\s*-?\\s*baker|faces\\b\", t)),\n        \"hand_hygiene_or_aseptic\": bool(re.search(r\"hand\\s*hygiene|aseptic\\s*technique|clean\\s*technique|sterile\\s*technique\", t)),\n    }\n\n    per = 0.4 / len(checks)\n    score = sum(1 for v in checks.values() if v) * per\n    missing = [k for k, v in checks.items() if not v]\n    feedback = f\"Missing signals: {', '.join(missing)}\" if missing else \"All key context/safety signals present.\"\n    return score, feedback"}, {"type": "code", "name": "Recognizable Nursing Diagnoses Detected", "description": "Check that at least two recognized, appropriate NANDA-style diagnosis labels appear (fuzzy matching allowed).", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n\n    try:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = context.files.read_docx_text(output.id)\n    except Exception:\n        return 0.0, \"Unable to extract text from document.\"\n\n    t = text.lower()\n\n    # Recognizable, appropriate diagnoses for this scenario (synonyms included)\n    patterns = [\n        r\"acute\\s+pain\",\n        r\"risk\\s*(for|of)\\s*infection|infection\\s*risk\",\n        r\"impaired\\s*physical\\s*mobility|limited\\s*mobility\",\n        r\"(risk\\s*(for|of)\\s*)?impaired\\s*skin\\s*integrity|skin\\s*breakdown\",\n        r\"risk\\s*(for|of)\\s*peripheral\\s*neurovascular\\s*dysfunction|neurovascular\\s*impairment|ineffective\\s*peripheral\\s*tissue\\s*perfusion\",\n        r\"risk\\s*(for|of)\\s*constipation\",\n        r\"risk\\s*(for|of)\\s*delayed\\s*surgical\\s*recovery\",\n    ]\n\n    found = set()\n    for i, pat in enumerate(patterns):\n        if re.search(pat, t):\n            found.add(i)\n\n    count = len(found)\n    # Score relative to the expected 3 diagnoses (cap at 3)\n    ratio = min(count, 3) / 3.0\n    score = ratio * 0.4\n    return score, f\"Recognized diagnoses: {count}.\""}, {"type": "llm_judge", "name": "Clinical Appropriateness of Diagnoses", "description": "Evaluate whether the three diagnoses are clinically appropriate for a 3-year-old in PACU post-ORIF with spica cast (e.g., Acute Pain, Risk for Infection, Impaired Mobility/Neurovascular/Skin Integrity).", "weight": 1.3, "judge_prompt": "Assess whether the three nursing diagnoses are appropriate for a pediatric (age 3) post-op right femur ORIF with a spica cast in PACU/inpatient care. Consider typical priorities: acute pain, infection risk, mobility/neurovascular/skin integrity, constipation risk, etc. Scoring:\n- 1.3: All 3 diagnoses are highly appropriate and prioritized reasonably.\n- 0.9: 2 are clearly appropriate; 1 is somewhat less relevant but acceptable.\n- 0.5: Only 1 is clearly appropriate.\n- 0.0: Diagnoses are largely inappropriate for this case.", "expectation": "Diagnoses align with post-op pediatric orthopedic priorities (pain, infection risk, mobility/neurovascular/skin)."}, {"type": "llm_judge", "name": "Outcomes \u2014 Measurable, Time-bound, Aligned", "description": "Check that each diagnosis has exactly one outcome that is measurable, time-bound, and aligned to the diagnosis and pediatric context (e.g., FACES score reduction, intact neurovascular status, no fever).", "weight": 1.3, "judge_prompt": "For each diagnosis, review the single outcome. Are outcomes measurable (e.g., numeric scales/criteria like FACES \u22643/10, cap refill <2s, VS within range), time-bound (e.g., within 30\u201360 min for pain, within shift/24h for others), and aligned with the diagnosis? Scoring:\n- 1.3: All three outcomes are specific, measurable, time-bound, and clinically aligned.\n- 0.9: Two are strong; one is vague or missing time/measures.\n- 0.5: Only one is adequate.\n- 0.0: Outcomes are vague, not measurable, or misaligned.", "expectation": "Each diagnosis includes one SMART-style outcome aligned to pediatrics (e.g., FACES pain target, neurovascular perfusion criteria)."}, {"type": "llm_judge", "name": "Assessments \u2014 Completeness and Frequency", "description": "Verify that each diagnosis lists four assessments with relevant pediatric postoperative monitoring (neurovascular checks, cast skin assessment, pain reassessment, VS trends) and reasonable frequency/parameters.", "weight": 1.3, "judge_prompt": "Assess whether each diagnosis has 4 relevant assessments. Look for pediatric post-op elements like: neurovascular checks (color, warmth, movement, sensation, cap refill, pulses), cast/skin inspection, pain scale reassessment using FACES, vital signs, signs of infection, intake/output, and specified frequencies (e.g., q1h initially). Scoring:\n- 1.3: All diagnoses include 4 relevant, specific assessments; frequency/parameters are appropriate.\n- 0.9: Generally relevant but one diagnosis has vague or non-specific items or missing frequency.\n- 0.5: Significant gaps (e.g., missing neurovascular or pain reassessment) in at least one diagnosis.\n- 0.0: Assessments are largely irrelevant or dangerously incomplete.", "expectation": "Four targeted assessments per diagnosis emphasizing neurovascular and cast-related monitoring, pain reassessment, infection surveillance."}, {"type": "llm_judge", "name": "Interventions \u2014 Pharmacologic, Nonpharmacologic, and Aseptic Technique", "description": "Check that each diagnosis lists four interventions balancing pharmacologic (e.g., analgesics as ordered) and nonpharmacologic strategies (positioning, distraction, ice per protocol), plus infection prevention (hand hygiene/aseptic technique).", "weight": 1.3, "judge_prompt": "Evaluate the four interventions per diagnosis for appropriateness and balance: pharmacologic pain meds per order with reassessment timing; nonpharmacologic measures suitable for a 3-year-old (distraction, caregiver presence, elevation/positioning compatible with spica cast); infection prevention (hand hygiene, aseptic technique for lines/wounds); safety (dose checks, cast care precautions). Scoring:\n- 1.3: Interventions are comprehensive, balanced, include infection prevention, and specify timing/safety.\n- 0.9: Mostly appropriate; minor omissions (e.g., limited nonpharmacologic detail) but no safety issues.\n- 0.5: Important elements missing (e.g., no hand hygiene mention or no reassessment timing).\n- 0.0: Interventions are inappropriate or unsafe.", "expectation": "Four interventions per diagnosis including medication and nonpharm comfort, with hand hygiene and aseptic technique emphasized where applicable."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Handoff Readiness", "description": "Holistic quality evaluation: professionalism, readability, family-centeredness, safety consistency, and shift-to-shift usability.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and One-Page Readability", "description": "Assess formatting, clarity, and concise presentation suitable for bedside use within one page.", "weight": 0.5, "judge_prompt": "Evaluate professional presentation and readability: clear headers, consistent formatting, concise language, easy scanning at bedside, fits cleanly on one page without overcrowding. Scoring:\n- 0.5: Clean, consistent, highly readable, clearly one-page optimized.\n- 0.3: Generally clear with minor clutter or formatting inconsistencies.\n- 0.1: Hard to scan or cluttered but usable.\n- 0.0: Poorly formatted and difficult to use.", "expectation": "Concise, scannable layout with clear headers and consistent styling."}, {"type": "llm_judge", "name": "Family-Centered and Pediatric-Appropriate Language", "description": "Check that language and plan reflect pediatric needs and caregiver involvement/education.", "weight": 0.5, "judge_prompt": "Does the plan reflect pediatric appropriateness and family-centered care (e.g., caregiver education for cast care, pain reporting, infection signs; child-friendly comfort measures)? Scoring:\n- 0.5: Strong family-centered elements and pediatric-appropriate language.\n- 0.3: Some caregiver education/child-friendly elements present.\n- 0.1: Minimal references to family/child needs.\n- 0.0: Absent or inappropriate for pediatrics.", "expectation": "Incorporates caregiver teaching and child-friendly comfort strategies."}, {"type": "llm_judge", "name": "Updateability and Handoff Readiness", "description": "Evaluate whether the plan can be easily updated each shift and supports consistent handoff.", "weight": 0.5, "judge_prompt": "Assess whether the plan supports shift-to-shift updates and handoffs: clear outcomes/criteria, reassessment intervals, and optional fields or instructions for date/time, initials, or progress notes. Scoring:\n- 0.5: Explicit update cues (e.g., reassessment times, checkboxes/fields) enabling consistent updates.\n- 0.3: Generally updateable but missing explicit cues.\n- 0.1: Limited update guidance.\n- 0.0: Not conducive to shift updates.", "expectation": "Contains explicit cues or structures to facilitate per-shift updates and tracking progress."}, {"type": "llm_judge", "name": "Safety and Consistency Check", "description": "Ensure there are no unsafe recommendations or internal contradictions; aligns with standard pediatric post-op ortho care.", "weight": 0.5, "judge_prompt": "Check for safety and internal consistency: no contraindicated actions (e.g., compromising cast integrity), correct pain reassessment timing after meds, appropriate infection control statements, consistent terminology, no contradictions. Scoring:\n- 0.5: Fully safe and consistent with pediatric post-op ortho standards.\n- 0.3: Minor issues but overall safe.\n- 0.1: Notable inconsistencies but not unsafe.\n- 0.0: Unsafe or contradictory guidance.", "expectation": "No unsafe advice; internally consistent and aligned with standard care."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "9a0d8d36-6233-4c76-9107-0d1f783c7340", "rubric": {"category_name": "ISO vs NSO Tax Presentation (Executive Client Education)", "rationale": "This is a Mixed task (Pattern C): a client-ready slide deck (document) that must also show step-by-step calculations (analysis). Stage 1 strictly enforces a verifiable structure: a slide-style PDF/DOCX plus a companion Excel workbook with specified sheets and tables. Stage 2 verifies correctness using a mix of lightweight code checks on the workbook (bounds and structural consistency) and LLM judgment for nuanced tax treatment and logical consistency. Stage 3 evaluates overall communication quality, professionalism, and client appropriateness for an executive HNW audience.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Structure Gate (Presentation + Workbook)", "description": "LLM-only gate to enforce exact output shape: a short slide-style presentation (PDF/DOCX) plus a companion Excel model with specified sheets and tables enabling verification. Failure zeros the category.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.5, "rules": [{"type": "llm_judge", "name": "Structured Deck + Calculation Workbook Present", "description": "Verify the candidate produced BOTH: (1) a slide-style presentation (PDF or DOCX) and (2) an Excel workbook with specific sheets/tables needed for verification. Only check structure and presence, not calculation correctness.", "weight": 6.0, "judge_prompt": "You are evaluating whether the submission satisfies the REQUIRED output shape. Look at all provided files.\n\nPASS REQUIREMENTS (be flexible on exact wording, but structure must be clear):\nA) Presentation Format (Primary deliverable)\n- Must be a slide-style deck exported as PDF or a DOCX that clearly contains slide-like pages. PowerPoint exported to PDF is acceptable.\n- Length target: 6\u201312 slides/pages.\n- Slide titles are visible and professional.\n- Required sections/slides present (names can vary slightly):\n  1) Title slide (client-safe, includes topic and advisor/bank name)\n  2) ISO vs NSO Overview/Differences (definition and key distinctions)\n  3) ISO Step-by-Step Example (with hypothetical numbers)\n  4) NSO Step-by-Step Example (with hypothetical numbers)\n  5) Tax Implications by Event (exercise vs sale; call out AMT for ISO)\n  6) Net Proceeds Comparison (side-by-side table summarizing ISO vs NSO)\n  7) Assumptions & Inputs (shares, strike price, FMV/current price, tax rates used; holding period assumptions)\n  8) Disclaimers & Next Steps (not tax advice; consult tax professional)\n- The deck should reference a companion Excel workbook for detailed calculations (e.g., \u201cSee Option Tax Model.xlsx\u201d).\n\nB) Calculation Workbook (Excel)\n- An attached Excel file exists (XLSX preferred) containing the following sheets (names can be flexible but must be clearly equivalent):\n  - \"Inputs\" sheet: a small table with columns like [Parameter | Value | Notes], including rows for: Number of Shares, Strike/Exercise Price, Current FMV/Market Price, Federal ordinary income rate, Long-term capital gains rate, AMT rate (or AMT parameters), and any state tax if used.\n  - \"ISO Calculation\" sheet: step-by-step table with columns like [Step | Quantity | Value per Share | Total | Notes]. Should include rows for: Exercise Cost, Bargain Element, AMT Adjustment, Sale Proceeds, Basis, Qualifying vs Disqualifying disposition logic or labels.\n  - \"NSO Calculation\" sheet: step-by-step table similar to ISO, including rows for: Ordinary Income at Exercise, Withholding/Payroll (FICA/Medicare), Exercise Cost, Sale Proceeds, Basis, Capital Gain/Loss at sale.\n  - \"Summary\" sheet: side-by-side comparison table of key outputs with columns similar to [Metric | ISO | NSO], including Net Proceeds, Taxes, Exercise Cost, and Sale Proceeds.\n\nSCORING (Structure only; do not judge correctness):\n- 6.0: Presentation (PDF/DOCX) meets format + all 8 slide sections present AND Excel workbook exists with all four sheets and appropriate tables.\n- 4.5: Presentation valid with at least 6 of 8 required slide sections AND Excel workbook exists with at least 3 of the 4 sheets.\n- 3.0: Presentation valid but only 4\u20135 required slide sections OR Excel workbook exists with only 2 of 4 sheets.\n- 0.0: Not a PDF/DOCX deck, fewer than 4 sections, OR no Excel workbook present.\n\nOnly evaluate structural presence and format, not the accuracy of any content or numbers.", "expectation": "A concise, professional slide deck (PDF/DOCX) plus a companion Excel workbook with Inputs, ISO Calculation, NSO Calculation, and Summary sheets enabling verification."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness and Consistency)", "description": "Now that the structure is present, verify correctness of tax treatment and numerical/logical consistency across the deck and workbook. Mix of deterministic code checks and LLM judgment. Code rules carry lower weight than LLM rules.", "is_required": true, "max_points": 9.0, "min_score_to_pass": 4.5, "rules": [{"type": "code", "name": "Workbook Present with Expected Sheets", "description": "Confirm an Excel workbook is included among outputs and has expected sheets: Inputs, ISO Calculation, NSO Calculation, and Summary (flexible names). Partial credit if some are present.", "weight": 0.6, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    # Find any spreadsheet among outputs (prefer last task outputs)\n    outputs = getattr(context, 'get_all_outputs', lambda: [])()\n    spreadsheet = None\n    for res in reversed(outputs):\n        try:\n            if getattr(res, 'is_spreadsheet', False):\n                spreadsheet = res\n                break\n        except Exception:\n            continue\n    if spreadsheet is None:\n        # As fallback, try primary output if it's a spreadsheet\n        primary = context.get_primary_output()\n        if primary and getattr(primary, 'is_spreadsheet', False):\n            spreadsheet = primary\n    if spreadsheet is None:\n        return (0.0, \"No spreadsheet workbook found among outputs.\")\n\n    try:\n        path = context.files.get_path(spreadsheet.id)\n        xl = pd.ExcelFile(path)\n        sheet_names = [s.lower() for s in xl.sheet_names]\n    except Exception as e:\n        return (0.0, f\"Failed to open spreadsheet: {e}\")\n\n    def has_sheet_like(names, keywords):\n        for n in names:\n            for kw in keywords:\n                if kw in n:\n                    return True\n        return False\n\n    expected = {\n        'inputs': ['input', 'assumption'],\n        'iso': ['iso', 'incentive'],\n        'nso': ['nso', 'non-qualified', 'nonqualified'],\n        'summary': ['summary', 'compare', 'comparison']\n    }\n\n    present = 0\n    missing = []\n    for key, kws in expected.items():\n        if has_sheet_like(sheet_names, kws):\n            present += 1\n        else:\n            missing.append(key)\n\n    score = present / 4.0  # normalized 0-1\n    feedback = f\"Found {present}/4 expected sheets. Missing: {', '.join(missing) if missing else 'none'}.\"\n    return (score, feedback)\n"}, {"type": "code", "name": "Summary Net Proceeds Within Plausible Bounds", "description": "Using Inputs and Summary sheets, check that ISO/NSO Net Proceeds are non-negative and do not exceed max sale proceeds (shares * FMV) when available. Partial credit if only one side can be validated.", "weight": 0.6, "code": "import re\nimport math\nimport pandas as pd\nimport numpy as np\n\ndef _to_number(x):\n    if isinstance(x, (int, float, np.number)) and pd.notnull(x):\n        return float(x)\n    if isinstance(x, str):\n        m = re.findall(r\"[-+]?[0-9]*\\.?[0-9]+\", x.replace(',', ''))\n        if m:\n            try:\n                return float(m[0])\n            except Exception:\n                return None\n    return None\n\ndef _find_sheet(xl, keywords):\n    for name in xl.sheet_names:\n        low = name.lower()\n        if any(kw in low for kw in keywords):\n            return name\n    return None\n\ndef evaluate(workflow, context):\n    outputs = getattr(context, 'get_all_outputs', lambda: [])()\n    spreadsheet = None\n    for res in reversed(outputs):\n        try:\n            if getattr(res, 'is_spreadsheet', False):\n                spreadsheet = res\n                break\n        except Exception:\n            continue\n    if spreadsheet is None:\n        primary = context.get_primary_output()\n        if primary and getattr(primary, 'is_spreadsheet', False):\n            spreadsheet = primary\n    if spreadsheet is None:\n        return (0.0, \"No workbook to verify numeric plausibility.\")\n\n    try:\n        path = context.files.get_path(spreadsheet.id)\n        xl = pd.ExcelFile(path)\n    except Exception as e:\n        return (0.0, f\"Failed to open workbook: {e}\")\n\n    # Locate Inputs sheet\n    inputs_name = _find_sheet(xl, ['input', 'assumption'])\n    shares = None\n    fmv = None\n    if inputs_name:\n        try:\n            df_in = pd.read_excel(path, sheet_name=inputs_name)\n            # Assume first two columns are parameter and value\n            if df_in.shape[1] >= 2:\n                params = df_in.iloc[:,0].astype(str).str.lower()\n                vals = df_in.iloc[:,1]\n                for i, p in enumerate(params):\n                    if any(k in p for k in ['share', 'quantity', 'qty', 'units']):\n                        shares = _to_number(vals.iloc[i]) or shares\n                    if any(k in p for k in ['fmv', 'market', 'current price', 'sale price', 'fair']):\n                        fmv = _to_number(vals.iloc[i]) or fmv\n        except Exception:\n            pass\n\n    max_sale = None\n    if shares is not None and fmv is not None and shares >= 0 and fmv >= 0:\n        max_sale = shares * fmv\n\n    # Find Summary sheet\n    summary_name = _find_sheet(xl, ['summary', 'compare', 'comparison'])\n    if not summary_name:\n        return (0.0, \"No Summary sheet found for net proceeds check.\")\n\n    try:\n        df = pd.read_excel(path, sheet_name=summary_name)\n    except Exception as e:\n        return (0.0, f\"Failed to read Summary sheet: {e}\")\n\n    # Try to identify ISO/NSO net proceeds in common layouts\n    iso_net = None\n    nso_net = None\n\n    cols_lower = [str(c).lower() for c in df.columns]\n\n    # Pattern A: wide format with 'iso' and 'nso' columns and a metric label column\n    if any('iso' in c for c in cols_lower) and any('nso' in c for c in cols_lower):\n        # label column likely first\n        label_col = df.columns[0]\n        iso_col = [c for c in df.columns if 'iso' in str(c).lower()][0]\n        nso_col = [c for c in df.columns if 'nso' in str(c).lower()][0]\n        labels = df[label_col].astype(str).str.lower()\n        mask = labels.str.contains('net') & labels.str.contains('proceed')\n        candidates = df[mask]\n        if not candidates.empty:\n            iso_net = _to_number(candidates[iso_col].iloc[0])\n            nso_net = _to_number(candidates[nso_col].iloc[0])\n    \n    # Pattern B: long/narrow with a 'metric' column and separate rows for ISO/NSO values\n    if iso_net is None or nso_net is None:\n        # Try to find rows that contain 'net' and 'proceed' and columns with iso/nso\n        # Or columns like 'type' and 'value'\n        metric_col = None\n        for i, c in enumerate(df.columns):\n            if 'metric' in str(c).lower() or 'item' in str(c).lower() or 'label' in str(c).lower():\n                metric_col = c\n                break\n        if metric_col is not None:\n            mask = df[metric_col].astype(str).str.lower().str.contains('net') & df[metric_col].astype(str).str.lower().str.contains('proceed')\n            sub = df[mask]\n            if not sub.empty:\n                # try find iso/nso columns\n                iso_cols = [c for c in df.columns if 'iso' in str(c).lower()]\n                nso_cols = [c for c in df.columns if 'nso' in str(c).lower()]\n                val_cols = [c for c in df.columns if 'value' in str(c).lower() or 'amount' in str(c).lower()]\n                type_cols = [c for c in df.columns if 'type' in str(c).lower() or 'option' in str(c).lower()]\n                if iso_cols and nso_cols:\n                    iso_net = _to_number(sub.iloc[0][iso_cols[0]]) if iso_net is None else iso_net\n                    nso_net = _to_number(sub.iloc[0][nso_cols[0]]) if nso_net is None else nso_net\n                elif type_cols and val_cols:\n                    # look for iso/nso rows in type col\n                    tcol = type_cols[0]\n                    vcol = val_cols[0]\n                    for _, row in sub.iterrows():\n                        t = str(row[tcol]).lower()\n                        if 'iso' in t and iso_net is None:\n                            iso_net = _to_number(row[vcol])\n                        if 'nso' in t and nso_net is None:\n                            nso_net = _to_number(row[vcol])\n\n    found = sum(x is not None for x in [iso_net, nso_net])\n    if found == 0:\n        return (0.0, \"Could not locate ISO/NSO net proceeds in Summary sheet.\")\n\n    # Plausibility checks\n    def ok(v):\n        if v is None or not np.isfinite(v):\n            return False\n        if v < 0:\n            return False\n        if max_sale is not None and v > max_sale * 1.01:  # small tolerance\n            return False\n        return True\n\n    iso_ok = ok(iso_net) if iso_net is not None else False\n    nso_ok = ok(nso_net) if nso_net is not None else False\n\n    # scoring: 1.0 both OK, 0.5 only one OK, else 0\n    if iso_ok and nso_ok:\n        return (1.0, f\"ISO and NSO net proceeds plausible. max_sale={max_sale if max_sale is not None else 'n/a'}\")\n    if iso_ok or nso_ok:\n        return (0.5, f\"Only one side plausible. ISO_ok={iso_ok}, NSO_ok={nso_ok}, max_sale={max_sale if max_sale is not None else 'n/a'}\")\n    return (0.0, \"Found net proceeds but values are out of plausible bounds.\")\n"}, {"type": "llm_judge", "name": "Tax Treatment Accuracy (ISO vs NSO)", "description": "Check that the deck correctly describes tax treatment differences: NSO ordinary income and payroll taxes at exercise with withholding; ISO no regular tax at exercise but AMT adjustment may apply; sale taxation depends on holding period (qualifying vs disqualifying disposition) with proper capital gain/ordinary income characterization.", "weight": 2.6, "judge_prompt": "Evaluate the tax treatment explanations across the deck (and any notes/appendix). Look for the following correctness elements:\n- NSO: Ordinary income recognized at exercise equal to bargain element; subject to payroll taxes (FICA/Medicare) and income tax withholding; basis adjustment for sale; subsequent sale generally capital gain/loss from sale price vs basis.\n- ISO: No regular income tax at exercise; bargain element is an AMT preference item potentially triggering AMT; at sale: qualifying disposition (holding period met) yields long-term capital gains on sale price minus exercise price; disqualifying disposition recharacterizes some or all as ordinary income; basis rules are correctly noted.\n- Distinguish clearly between tax at exercise vs tax at sale for both ISO and NSO.\n- Any mention of state tax should be reasonable if included; if omitted, not penalized. Do not require precise rates.\nScoring:\n- 1.0: Contains multiple material errors or conflates ISO/NSO treatment.\n- 2.0: Mostly correct but misses one key nuance (e.g., AMT for ISO or payroll withholding for NSO).\n- 2.6: Accurate on all key points listed above and clearly distinguishes timing and character of income.\nOnly evaluate conceptual correctness, not numerical values.", "expectation": "Accurate, unambiguous explanation of ISO and NSO tax timing and characterization, including AMT for ISO and withholding/payroll for NSO."}, {"type": "llm_judge", "name": "Step-by-Step Calculation Logic Coherence", "description": "Ensure the step-by-step examples use consistent inputs and logically flow from assumptions to exercise to sale for both ISO and NSO.", "weight": 2.6, "judge_prompt": "Review the step-by-step calculation slides and any referenced tables. Check that:\n- The same hypothetical inputs (shares, strike, FMV) are used for both ISO and NSO examples unless explicitly justified.\n- Steps are in logical order: Inputs -> Exercise (cost, tax at exercise if applicable) -> Sale (proceeds, basis, gain characterization) -> Taxes at sale -> Net proceeds.\n- Quantities and units make sense (per-share vs total) and totals are consistent within the presentation.\n- If the deck references the workbook, the numbers shown on slides should match or clearly tie to workbook figures.\nScoring:\n- 1.0: Steps are confusing or inconsistent; mismatched inputs; unclear totals.\n- 2.0: Mostly coherent but with one noticeable inconsistency or missing step.\n- 2.6: Fully coherent, consistent, and traceable from inputs through to net proceeds for both ISO and NSO.\nDo not re-check tax law; focus on internal logic and consistency.", "expectation": "Clear, consistent step-by-step flow for ISO and NSO using the same inputs, with traceable totals that tie to the workbook."}, {"type": "llm_judge", "name": "Net Proceeds Comparison Correctness and Labeling", "description": "Verify the comparison table correctly identifies key line items and the resulting net proceeds for ISO and NSO, with appropriate labels and sign conventions.", "weight": 2.6, "judge_prompt": "Inspect the comparison table (Summary) in the deck. Verify that:\n- It includes key items: Sale Proceeds (or Gross Proceeds), Exercise Cost, Taxes (broken out or aggregated), and Net Proceeds for both ISO and NSO.\n- Labels are clear and sign conventions are correct (costs/taxes reduce net proceeds).\n- The Net Proceeds values align logically with the listed items (no obvious sign mistakes or internal contradictions on the slide).\n- Any notes about withholding timing (NSO) vs AMT considerations (ISO) are placed reasonably.\nScoring:\n- 1.0: Missing multiple key items or mislabeled causing confusion.\n- 2.0: Mostly correct but one labeling/sign or mapping issue.\n- 2.6: Correct, clearly labeled, and internally consistent for both ISO and NSO.\nDo not evaluate graphic design; focus on correctness and clarity of the comparison content.", "expectation": "A side-by-side comparison with clearly labeled items culminating in correctly signed Net Proceeds for ISO and NSO."}], "on_failure_action": "skip_remaining", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Client Readiness", "description": "Holistic assessment of presentation quality for an executive HNW client: clarity, professionalism, usefulness, and appropriateness.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Audience Appropriateness and Clarity", "description": "Executive-friendly clarity: defines terms, minimizes jargon, and emphasizes what matters to the client (net proceeds, timing, risks).", "weight": 1.25, "judge_prompt": "Assess whether the deck is easily understandable by an executive client:\n- Defines key terms (ISO, NSO, FMV, strike price, AMT) succinctly.\n- Minimizes jargon or explains it clearly.\n- Emphasizes practical takeaways: timing of taxes, cash flow for exercise, and net proceeds.\n- Uses plain-English captions and concise bullets.\nScore 0.0\u20131.25 based on clarity and client orientation.", "expectation": "Plain-English explanations with defined terms and a focus on practical implications for an executive client."}, {"type": "llm_judge", "name": "Professional Formatting and Slidecraft", "description": "Visual professionalism: slide structure, consistency, readability, and brevity appropriate for a short in-person briefing.", "weight": 1.25, "judge_prompt": "Evaluate formatting and slidecraft:\n- Consistent layout, readable fonts, appropriate slide count (6\u201312), and professional tone.\n- Effective use of white space, charts/tables, and minimal wall-of-text.\n- Clear slide titles and logical flow from overview \u2192 examples \u2192 comparison \u2192 next steps.\nScore 0.0\u20131.25 based on professional presentation quality.", "expectation": "Clean, consistent slides with readable typography and clear flow suitable for in-person discussion."}, {"type": "llm_judge", "name": "Actionability and Next Steps", "description": "Provides useful next steps tailored to vesting timeline and tax planning (e.g., AMT awareness, withholding planning, liquidity planning).", "weight": 1.25, "judge_prompt": "Assess whether the deck offers actionable guidance:\n- Notes that options are not vested for a year; frames content as education and planning ahead.\n- Highlights planning items: AMT impact for ISO, withholding/cash needs for NSO, liquidity for exercise, holding period tracking for ISO qualifying disposition, and coordinating with a tax professional.\n- Suggests next steps and decision considerations without rendering individualized tax advice.\nScore 0.0\u20131.25 based on actionability and prudence.", "expectation": "Clear, non-prescriptive next steps reflecting vesting timeline and tax planning considerations."}, {"type": "llm_judge", "name": "Compliance, Disclaimers, and Context", "description": "Checks for appropriate disclaimers and context-awareness (e.g., mention of tax professional; optional note on state taxes) without giving specific personal advice.", "weight": 1.25, "judge_prompt": "Evaluate compliance tone:\n- Includes a general disclaimer (educational, not tax/legal advice) and suggests consulting a qualified tax professional.\n- Avoids personal recommendations; presents scenarios and comparisons.\n- Optional: reasonable acknowledgement of state taxes (e.g., Missouri) if referenced; absence should not be penalized heavily.\nScore 0.0\u20131.25 based on compliance-appropriate language and non-prescriptive tone.", "expectation": "Professional disclaimers and prudent language suitable for a bank advisor context."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "ce864f41-8584-49ba-b24f-9c9104b47bf0", "rubric": {"category_name": "Workload Distribution Tracker - Project Management Specialists", "rationale": "This rubric enforces a self-documenting Excel workbook that makes verification trivial, then validates correctness with a mix of deterministic code checks and LLM cross-referencing. Stage 1 uses LLM-only gates to mandate a precise workbook structure and presence of brief written answers. Stage 2 focuses on correctness: capacity math with 15% admin reserve, utilization formulas, department roll-ups, and project budget variance checks\u2014weighted heavily toward LLM judges for nuanced threshold application and cross-sheet consistency. Stage 3 evaluates professional quality: clarity, actionable insight, reproducibility, and executive appropriateness.", "max_total_score": 20.0, "stages": [{"name": "Stage 1: Shape Enforcement Gate", "description": "LLM-only gate that mandates a single Excel workbook with specific sheets and a brief written answer (either as a sheet or separate document). No correctness checks\u2014only structure/presence.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.0, "rules": [{"type": "llm_judge", "name": "Workbook Structure and Supporting Answers Present", "description": "Verify the output is an Excel workbook with all required sheets and that brief answers to the 3 questions are provided (either as a sheet or a separate doc).", "weight": 3.0, "judge_prompt": "You are checking only FORMAT and STRUCTURE (not correctness). Inspect the candidate outputs.\n\nRequired primary deliverable:\n- A single Excel workbook (XLSX) that contains the following sheets (be flexible with similar names):\n  1) \"Stakeholder Registry\" \u2014 employee list and capacities\n  2) \"Timekeeping Analysis\" \u2014 per-employee hours and utilization\n  3) \"Department Utilization\" \u2014 per-department net capacity, hours, utilization, deviation from 100%, status\n  4) \"Project Budget vs Actuals\" \u2014 per-project allocated vs actual hours, variance, status\n  5) \"Assumptions & Methodology\" \u2014 text detailing assumptions (FT=40h/wk, PT=20h/wk, monthly basis, 15% admin reserve) and process\n  6) An \"Answers\" sheet (e.g., named \"Answers\", \"Summary\", \"Executive Summary\", or \"Outputs & Answers\") that briefly answers the 3 questions\n\nAcceptable alternative to item 6:\n- A separate PDF/DOCX containing brief answers to the 3 questions (if this is present, the workbook may omit the Answers sheet).\n\nExpected content structure (names may vary slightly; judge based on visible headers/tables):\n- Stakeholder Registry: columns for Employee Name, Department, Role/Position, Employment Type (FT/PT), Estimated Monthly Capacity (Gross Hours), Admin/Overhead Reserve (~15%), Net Capacity Hours\n- Timekeeping Analysis: Employee, Department, Employment Type, Capacity (Gross), Admin Reserve, Net Capacity, Logged Hours in March, Utilization Rate (%), Status flag\n- Department Utilization: Department, Net Capacity Hours, Logged Hours, Utilization %, Deviation from 100%, Status (Within/Over/Under)\n- Project Budget vs Actuals: Project Name, Allocated Hours (Budget), Actual Hours (March), Variance (Actual - Allocated), Status (Over/Within)\n- Assumptions & Methodology: brief narrative covering monthly hours baseline, FT/PT mapping, 15% admin reserve, data sources/mapping, and any cleaning steps\n- Answers: 3 concise responses addressing departments, individuals, and projects exceeding budgeted hours\n\nScoring (STRUCTURE only):\n- 3.0: Excel workbook present AND contains all 5 analysis sheets AND either an Answers sheet or a separate PDF/DOCX with the answers\n- 2.5: Excel workbook present with 5 analysis sheets but answers are missing OR answers present but one analysis sheet missing\n- 2.0: Excel workbook present with at least 4 of 5 required analysis sheets AND answers present somewhere\n- 0.0: Not an Excel workbook OR missing multiple required sheets OR no answers provided in any form\n\nOnly judge presence and structural completeness. Do not judge data accuracy or quality.", "expectation": "A well-structured Excel workbook with the specified sheets, plus brief written answers either as a sheet or a separate PDF/DOCX."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Correctness Verification", "description": "Verify capacity math, utilization calculations, roll-ups, and budget variance logic using a mix of code checks and LLM cross-references. Code rules handle deterministic math checks; LLM judges validate threshold application and cross-sheet consistency.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Stakeholder Registry Count and Columns Plausibility", "description": "Checks the Stakeholder Registry sheet exists, has ~23 employees, and includes key columns. Verifies 15% admin reserve and net capacity calculations are plausible.", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"Primary output is not a spreadsheet.\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheet_names = [s for s in xls.sheet_names]\n        lower_map = {s.lower(): s for s in sheet_names}\n        # Fuzzy find stakeholder sheet\n        target = None\n        for s in sheet_names:\n            sl = s.lower()\n            if any(k in sl for k in [\"stakeholder\", \"registry\", \"employees\", \"employee list\", \"staff\"]):\n                target = s\n                break\n        if target is None:\n            return 0.0, \"No Stakeholder Registry-like sheet found.\"\n        df = pd.read_excel(path, sheet_name=target)\n        if df.shape[0] == 0:\n            return 0.0, \"Stakeholder sheet is empty.\"\n        cols = [str(c).strip().lower() for c in df.columns]\n        def find_col(keys):\n            for i,c in enumerate(cols):\n                if any(k in c for k in keys):\n                    return df.columns[i]\n            return None\n        c_name = find_col([\"name\", \"employee\"])\n        c_dept = find_col([\"department\", \"dept\"])\n        c_role = find_col([\"role\", \"position\", \"title\"])\n        c_type = find_col([\"employment\", \"type\", \"status\", \"full\", \"part\", \"ft\", \"pt\"])\n        # Capacity columns\n        c_gross = None\n        c_admin = None\n        c_net = None\n        for i,c in enumerate(cols):\n            if (\"cap\" in c or (\"hour\" in c and (\"month\" in c or \"est\" in c or \"capacity\" in c))) and (\"net\" not in c and \"avail\" not in c):\n                c_gross = df.columns[i] if c_gross is None else c_gross\n            if any(k in c for k in [\"admin\", \"overhead\", \"reserve\", \"non-bill\", \"15\"]):\n                c_admin = df.columns[i] if c_admin is None else c_admin\n            if any(k in c for k in [\"net\", \"avail\"]):\n                c_net = df.columns[i] if c_net is None else c_net\n        score = 0.0\n        feedback = []\n        # Row count and uniqueness\n        n = df.shape[0]\n        if c_name is not None:\n            unique_names = df[c_name].dropna().astype(str).str.strip().str.lower().nunique()\n        else:\n            unique_names = 0\n        if n >= 23 and unique_names >= 23:\n            score += 0.25\n            feedback.append(f\"Row/unique name count OK: {n} rows, {unique_names} unique.\")\n        else:\n            feedback.append(f\"Row/unique name count low or duplicate: {n} rows, {unique_names} unique.\")\n        # Column presence\n        needed_cols_ok = sum(x is not None for x in [c_name, c_dept, c_role, c_type])\n        if needed_cols_ok >= 3:\n            score += 0.15\n            feedback.append(\"Core columns present (name/department/role/type).\")\n        else:\n            feedback.append(\"Missing some core columns (name/department/role/type).\")\n        # Admin ~15% of gross and net = gross - admin\n        consistency_checks = 0\n        consistency_pass = 0\n        tol = 0.12  # allow \u00b112% relative error on admin calc\n        if c_gross is not None and c_admin is not None:\n            g = pd.to_numeric(df[c_gross], errors='coerce')\n            a = pd.to_numeric(df[c_admin], errors='coerce')\n            mask = g.notna() & a.notna() & (g > 0)\n            if mask.sum() > 0:\n                ratio = (a[mask] / g[mask]).clip(lower=0, upper=10)\n                pass_ratio = (ratio.sub(0.15).abs() <= 0.15*tol).mean()  # ~\u00b11.8pp band\n                consistency_checks += 1\n                if pass_ratio >= 0.7:\n                    score += 0.05\n                    consistency_pass += 1\n                    feedback.append(f\"Admin reserve ~15% for >=70% rows (pass rate {pass_ratio:.0%}).\")\n                else:\n                    feedback.append(f\"Admin reserve deviates from 15% (pass rate {pass_ratio:.0%}).\")\n        if c_gross is not None and c_admin is not None and c_net is not None:\n            g = pd.to_numeric(df[c_gross], errors='coerce')\n            a = pd.to_numeric(df[c_admin], errors='coerce')\n            ncap = pd.to_numeric(df[c_net], errors='coerce')\n            mask = g.notna() & a.notna() & ncap.notna()\n            if mask.sum() > 0:\n                ok = (ncap[mask] - (g[mask] - a[mask])).abs() <= np.maximum(1.0, 0.02*np.maximum(g[mask],1))\n                pass_rate = ok.mean()\n                score += 0.05 if pass_rate >= 0.7 else 0.0\n                feedback.append(f\"Net capacity \u2248 Gross - Admin for {pass_rate:.0%} rows.\")\n        # Cap score within [0, 0.5]\n        score = float(max(0.0, min(0.5, score)))\n        return score, \"; \".join(feedback)\n    except Exception as e:\n        return 0.0, f\"Exception during Stakeholder Registry checks: {e}\""}, {"type": "code", "name": "Utilization Formula Consistency (Employee Level)", "description": "Verifies Utilization \u2248 Logged Hours / Net Capacity for most employees and values lie in plausible bounds.", "weight": 0.7, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"Not a spreadsheet.\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        # Find Timekeeping Analysis sheet\n        target = None\n        for s in xls.sheet_names:\n            sl = s.lower()\n            if any(k in sl for k in [\"timekeeping\", \"analysis\", \"utilization\", \"allocation\"]):\n                target = s\n                break\n        if target is None:\n            return 0.0, \"No Timekeeping Analysis-like sheet found.\"\n        df = pd.read_excel(path, sheet_name=target)\n        cols = [str(c).strip().lower() for c in df.columns]\n        def pick(keys):\n            for i,c in enumerate(cols):\n                if any(k in c for k in keys):\n                    return df.columns[i]\n            return None\n        c_net = None\n        c_logged = None\n        c_util = None\n        for i,c in enumerate(cols):\n            if (\"net\" in c and (\"cap\" in c or \"capacity\" in c)) or (\"available\" in c and \"hour\" in c):\n                c_net = df.columns[i]\n            if any(k in c for k in [\"logged\", \"actual hour\", \"march hour\", \"total hour\", \"hours worked\"]):\n                c_logged = df.columns[i]\n            if any(k in c for k in [\"util\", \"alloc\", \"rate\", \"%\"]):\n                c_util = df.columns[i] if c_util is None else c_util\n        if c_net is None or c_logged is None:\n            return 0.0, \"Missing net capacity or logged hours columns.\"\n        net = pd.to_numeric(df[c_net], errors='coerce')\n        logged = pd.to_numeric(df[c_logged], errors='coerce')\n        mask = net.notna() & (net > 0) & logged.notna()\n        if mask.sum() == 0:\n            return 0.0, \"No valid rows to check utilization.\"\n        inferred = (logged[mask] / net[mask]).replace([np.inf, -np.inf], np.nan)\n        plaus = inferred.between(0.0, 1.5)\n        score = 0.0\n        fb = []\n        # Plausibility check\n        plaus_rate = plaus.mean()\n        if plaus_rate >= 0.8:\n            score += 0.3\n        fb.append(f\"Plausible utilization for {plaus_rate:.0%} rows.\")\n        # Consistency with provided utilization column (if present)\n        if c_util is not None:\n            util = pd.to_numeric(df[c_util].astype(str).str.replace('%','').str.strip(), errors='coerce')\n            util = util/100.0 if util.max() > 1.5 else util\n            umask = mask & util.notna()\n            if umask.sum() > 0:\n                diff = (util[umask] - inferred[umask]).abs()\n                pass_rate = (diff <= 0.03).mean()  # within 3 pp\n                if pass_rate >= 0.7:\n                    score += 0.4\n                fb.append(f\"Utilization matches formula within 3pp for {pass_rate:.0%} rows.\")\n        # Cap at 0.7\n        score = float(max(0.0, min(0.7, score)))\n        return score, \"; \".join(fb)\n    except Exception as e:\n        return 0.0, f\"Exception during Utilization checks: {e}\""}, {"type": "code", "name": "Department Roll-up Consistency", "description": "Validates that department-level net capacity and logged hours sum to employee-level values within tolerance.", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"Not a spreadsheet.\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        # Find sheets\n        sh_emp = None\n        sh_dept = None\n        for s in xls.sheet_names:\n            sl = s.lower()\n            if sh_emp is None and any(k in sl for k in [\"timekeeping\", \"analysis\", \"utilization\", \"allocation\"]):\n                sh_emp = s\n            if sh_dept is None and any(k in sl for k in [\"department\", \"dept\"]):\n                sh_dept = s\n        if sh_emp is None or sh_dept is None:\n            return 0.0, \"Missing Timekeeping Analysis or Department Utilization sheet.\"\n        de = pd.read_excel(path, sheet_name=sh_emp)\n        dd = pd.read_excel(path, sheet_name=sh_dept)\n        # Identify columns\n        def pick(df, keys):\n            cols = [str(c).strip().lower() for c in df.columns]\n            for i,c in enumerate(cols):\n                if any(k in c for k in keys):\n                    return df.columns[i]\n            return None\n        cdept_e = pick(de, [\"department\", \"dept\"])\n        cnet_e = pick(de, [\"net\", \"avail\"]) or pick(de, [\"cap\", \"capacity\"])  # fallback\n        clog_e = pick(de, [\"logged\", \"actual hour\", \"total hour\", \"march hour\", \"hours worked\"])\n        cdept_d = pick(dd, [\"department\", \"dept\"])\n        cnet_d = pick(dd, [\"net capacity\", \"net\", \"capacity\"]) \n        clog_d = pick(dd, [\"logged\", \"actual hour\", \"total hour\", \"hours\"]) \n        if None in [cdept_e, cnet_e, clog_e, cdept_d, cnet_d, clog_d]:\n            return 0.0, \"Could not identify necessary columns for aggregation.\"\n        # Aggregate employee-level\n        emp = de[[cdept_e, cnet_e, clog_e]].copy()\n        emp[cnet_e] = pd.to_numeric(emp[cnet_e], errors='coerce')\n        emp[clog_e] = pd.to_numeric(emp[clog_e], errors='coerce')\n        agg = emp.groupby(cdept_e, dropna=False).sum(numeric_only=True)\n        # Prepare department-level\n        dep = dd[[cdept_d, cnet_d, clog_d]].copy()\n        dep[cnet_d] = pd.to_numeric(dep[cnet_d], errors='coerce')\n        dep[clog_d] = pd.to_numeric(dep[clog_d], errors='coerce')\n        merged = dep.merge(agg.reset_index(), left_on=cdept_d, right_on=cdept_e, how='inner', suffixes=('_dept', '_emp'))\n        if merged.shape[0] == 0:\n            return 0.0, \"No overlapping departments to compare.\"\n        tol = 0.03\n        # Compare\n        net_ok = (merged[f\"{cnet_d}_dept\"] - merged[f\"{cnet_e}_emp\"]).abs() <= np.maximum(2.0, tol*np.maximum(merged[f\"{cnet_e}_emp\"].abs(), 1))\n        log_ok = (merged[f\"{clog_d}_dept\"] - merged[f\"{clog_e}_emp\"]).abs() <= np.maximum(2.0, tol*np.maximum(merged[f\"{clog_e}_emp\"].abs(), 1))\n        pass_rate = (net_ok & log_ok).mean()\n        score = float(max(0.0, min(0.5, 0.5 * pass_rate)))\n        return score, f\"Dept roll-up consistency pass rate: {pass_rate:.0%}\"\n    except Exception as e:\n        return 0.0, f\"Exception during department roll-up check: {e}\""}, {"type": "code", "name": "Project Variance Logic", "description": "Checks that Variance \u2248 Actual - Allocated and that Status aligns with sign of variance in the Project Budget vs Actuals sheet.", "weight": 0.3, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"Not a spreadsheet.\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        target = None\n        for s in xls.sheet_names:\n            sl = s.lower()\n            if any(k in sl for k in [\"project\", \"budget\", \"actual\", \"variance\"]):\n                target = s\n                break\n        if target is None:\n            return 0.0, \"No Project Budget vs Actuals-like sheet found.\"\n        df = pd.read_excel(path, sheet_name=target)\n        cols = [str(c).strip().lower() for c in df.columns]\n        def pick(keys):\n            for i,c in enumerate(cols):\n                if all(k in c for k in keys):\n                    return df.columns[i]\n            for i,c in enumerate(cols):\n                if any(k in c for k in keys):\n                    return df.columns[i]\n            return None\n        calloc = pick([\"alloc\", \"hour\"]) or pick([\"budget\"]) or pick([\"planned\", \"hour\"]) \n        cact = pick([\"actual\", \"hour\"]) or pick([\"logged\", \"hour\"]) \n        cvar = pick([\"var\"]) or pick([\"delta\"]) \n        cstat = pick([\"status\"]) or pick([\"flag\"]) \n        if None in [calloc, cact, cvar]:\n            return 0.0, \"Missing Allocated/Actual/Variance columns.\"\n        a = pd.to_numeric(df[calloc], errors='coerce')\n        b = pd.to_numeric(df[cact], errors='coerce')\n        v = pd.to_numeric(df[cvar], errors='coerce')\n        mask = a.notna() & b.notna() & v.notna()\n        if mask.sum() == 0:\n            return 0.0, \"No valid rows for variance check.\"\n        diff = (b[mask] - a[mask]) - v[mask]\n        var_ok = (diff.abs() <= np.maximum(1.0, 0.02*np.maximum(b[mask].abs(),1))).mean()\n        score = 0.2 * var_ok  # up to 0.2\n        fb = [f\"Variance arithmetic pass rate {var_ok:.0%}\"]\n        if cstat is not None:\n            stat = df[cstat].astype(str).str.lower()\n            over_mask = (b[mask] > a[mask])\n            # Consider status words\n            over_words = [\"over\", \"exceed\", \"exceeded\", \"above\"]\n            within_words = [\"within\", \"ok\", \"on\", \"under\", \"below\"]\n            stat_sub = stat[mask]\n            ok_over = stat_sub[over_mask].apply(lambda x: any(w in x for w in over_words)).mean() if over_mask.sum() > 0 else 1.0\n            ok_within = stat_sub[~over_mask].apply(lambda x: any(w in x for w in within_words) or not any(w in x for w in over_words)).mean() if (~over_mask).sum() > 0 else 1.0\n            score += 0.1 * min(ok_over, ok_within)  # up to 0.1\n            fb.append(f\"Status alignment over/within: over={ok_over:.0%}, within={ok_within:.0%}\")\n        score = float(max(0.0, min(0.3, score)))\n        return score, \"; \".join(fb)\n    except Exception as e:\n        return 0.0, f\"Exception during project variance logic check: {e}\""}, {"type": "llm_judge", "name": "Assumptions Logical and Documented", "description": "Checks that the Assumptions & Methodology sheet clearly states FT/PT basis, monthly hours basis, and applies 15% admin reserve to determine net capacity used in utilization.", "weight": 2.5, "judge_prompt": "Check the workbook for an \"Assumptions & Methodology\" sheet (or similarly named) and verify it documents:\n- FT=40 hours/week, PT=20 hours/week (or an explicitly stated monthly hour basis that derives from these weekly norms)\n- The monthly capacity basis used for March (e.g., weeks/month or other explicit monthly assumption)\n- That 15% of time is reserved for admin/overhead and excluded from utilization calculations (net capacity)\n- A brief description of data mapping/cleaning and how hours were aggregated\nAlso check in the analysis sheets that the net capacity columns and utilization appear consistent with these assumptions (e.g., admin reserve is deducted before utilization).\n\nScoring:\n- 2.5: All items documented and visible evidence that net capacity reflects 15% admin in calculations\n- 1.5: Most items documented but minor gaps or unclear application\n- 0.5: Minimal assumptions noted; unclear application to calculations\n- 0.0: Assumptions missing or contradictory to sheet calculations", "expectation": "Clear, explicit assumptions and visible application of the 15% admin reserve to net capacity used in utilization."}, {"type": "llm_judge", "name": "Thresholds Applied to Departments and Individuals", "description": "Validates that status flags for departments and individuals follow the specified thresholds.", "weight": 2.5, "judge_prompt": "Using the analysis sheets:\n- Departments: In the Department Utilization sheet, confirm that the status/flag uses the target of 100% utilization with an acceptable band of \u00b15 percentage points. Departments above 105% should be flagged as overutilized; below 95% as underutilized; within the band as on target.\n- Individuals: In the Timekeeping Analysis sheet, confirm that individuals below 60% are marked underutilized and above 90% are marked overutilized/at risk of burnout; others are within range.\n\nScoring:\n- 2.5: Threshold rules are applied consistently across the sheet(s)\n- 1.5: Mostly applied with a few inconsistencies\n- 0.5: Sporadic or unclear application\n- 0.0: Thresholds not used or incorrectly defined", "expectation": "Department and individual status flags consistently reflect the \u00b15pp dept target and the <60% / >90% individual thresholds."}, {"type": "llm_judge", "name": "Projects Cross-Referenced with Budget and Flags Correct", "description": "Checks that Project Budget vs Actuals uses budget allocations and flags any project that exceeded allocated hours.", "weight": 2.0, "judge_prompt": "Review the Project Budget vs Actuals sheet:\n- Confirm it lists per-project allocated hours (from budget) and actual hours (from timekeeping) for March\n- Confirm variance (Actual - Allocated) is shown and projects with variance > 0 are clearly flagged as over/ exceeded\n- Spot-check a few rows to see if the flag aligns with the numbers\n\nScoring:\n- 2.0: Allocated vs Actuals are clearly reported and flags align with variance across the sheet\n- 1.0: Mostly correct with minor inconsistencies\n- 0.0: Missing allocations/actuals or flags do not align with variance", "expectation": "Clear project-level variance and correct overage flags."}, {"type": "llm_judge", "name": "Answers Accurately Summarize Findings", "description": "Evaluates the brief responses for completeness and correctness relative to the workbook findings.", "weight": 1.0, "judge_prompt": "Locate the brief answers (either in an \"Answers\"-like sheet or a separate PDF/DOCX). Check that they succinctly answer the 3 questions:\n1) Department-level utilization risks (\u00b15pp around 100%)\n2) Individual-level under/overutilization (<60% and >90%)\n3) Projects exceeding monthly allocated hours\nAnswers should reference the analysis, be concise (a few sentences/bullets), and align with the actual flags/tables in the workbook.\n\nScoring:\n- 1.0: All three questions answered succinctly and align with workbook findings\n- 0.5: Partially complete or minor misalignment\n- 0.0: Missing or inconsistent with the workbook", "expectation": "Three concise, accurate answers aligned with the workbook's results."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3: Quality Assessment", "description": "Holistic evaluation of presentation quality, strategic insight, and executive appropriateness. LLM-only assessment.", "is_required": false, "max_points": 7.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity and Professional Presentation", "description": "Assess formatting, readability, and professional polish of the workbook.", "weight": 2.0, "judge_prompt": "Evaluate the workbook's professionalism:\n- Clear sheet/tab names and section headers\n- Well-labeled columns with units (% vs hours), readable number formats, and consistent precision\n- Use of conditional formatting or clear status indicators for flags\n- No obvious broken references or placeholder text\n\nScoring:\n- 2.0: Highly professional, easy to read and navigate\n- 1.0: Adequate but some inconsistencies or readability issues\n- 0.0: Poorly organized or unprofessional appearance", "expectation": "A clean, well-formatted workbook with clear labels and readable numbers."}, {"type": "llm_judge", "name": "Actionable Insight and Risk Communication", "description": "Assesses whether the deliverable highlights material risks and suggests practical next steps.", "weight": 2.0, "judge_prompt": "Review whether the deliverable translates analysis into action:\n- Highlights departments/individuals at risk and prioritizes the most material issues\n- Provides brief, practical recommendations (e.g., rebalance workloads, adjust allocations, hire/contract, cross-train)\n- Communicates implications succinctly for leadership decisions\n\nScoring:\n- 2.0: Clear, actionable insights and recommendations\n- 1.0: Some insight but limited actionability\n- 0.0: Largely descriptive with no practical guidance", "expectation": "Concise, prioritized risks with practical recommendations."}, {"type": "llm_judge", "name": "Reproducibility and Auditability", "description": "Evaluates transparency: can someone trace inputs to outputs and reproduce the calculations?", "weight": 1.5, "judge_prompt": "Assess the transparency of methods:\n- Assumptions & Methodology is specific enough to reproduce the model (capacity basis, 15% admin, cleaning steps)\n- Formulas are visible/traceable (not opaque or hidden), and logic is consistent across tabs\n- Clear links/key fields to map stakeholders, timekeeping, and projects\n\nScoring:\n- 1.5: Highly reproducible and auditable\n- 0.75: Partially reproducible with some gaps\n- 0.0: Opaque and not reproducible", "expectation": "Clear documentation and visible, consistent formulas."}, {"type": "llm_judge", "name": "Executive Appropriateness and Brevity", "description": "Assesses whether the brief answers and presentation are appropriate for a CEO audience.", "weight": 1.5, "judge_prompt": "Evaluate the executive suitability of the brief answers and overall summary:\n- Concise, jargon-light summaries that answer the three questions directly\n- Emphasis on implications and decisions rather than technical detail\n- Tone and structure suitable for leadership review\n\nScoring:\n- 1.5: Excellent executive focus and brevity\n- 0.75: Adequate but could be tighter or more executive-focused\n- 0.0: Not appropriate for executive consumption", "expectation": "Crisp, decision-oriented communication appropriate for a CEO."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "8f9e8bcd-6102-40da-ab76-23f51d8b21fa", "rubric": {"category_name": "Retail GM \u2014 Bridal Sales Objection-Handling Training (DOCX/PDF)", "rationale": "This rubric enforces a self-documenting, verifiable Word/PDF training document for bridal sales objection handling. Stage 1 uses an LLM gate to mandate exact sections and structural elements that enable verification. Stage 2 mixes lightweight code checks (text extraction for concrete inclusions and counts) with higher-weight LLM judges for correctness and alignment. Stage 3 provides a holistic quality review focused on professional utility for a bridal retail sales team.", "max_total_score": 22.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (Document Structure)", "description": "Gate: Verify the candidate output is a properly structured DOCX/PDF training document with all required sections and structural elements present. Do not judge content quality or correctness here.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Document Format and Required Sections Present", "description": "Check the document format and presence of all required sections and key structural elements enabling verification.", "weight": 4.0, "judge_prompt": "You are verifying ONLY structure and format (not quality or correctness). Inspect the candidate output.\n\nFormat requirements:\n- Must be a DOCX or PDF document (not spreadsheet, not plain text file).\n- At least 2 pages in length.\n- Professionally formatted with clear section headers.\n\nRequired visible sections (flexible naming allowed if clearly equivalent):\n1) \"Overview\" \u2014 states why objection handling matters and lists the most common objections.\n2) \"Types of Objections\" \u2014 contains the five types with descriptions and examples:\n   - price (cost/budget)\n   - need (necessity/relevance)\n   - urgency (time frame)\n   - trust (company/product confidence)\n   - authority (needs to check with partner/parent/friend)\n3) \"Core Strategies to Overcoming the Objection\" \u2014 a practical, actionable framework.\n4) \"Let's Practice\" \u2014 includes common objections mapped to their type and a suggested response. Preferably a list or table showing the mapping.\n5) \"Conclusion\" \u2014 a brief recap of the training purpose.\n6) \"Homework\" \u2014 instructs salesperson to track at least 6 objections over a week including: the objection, the type, how they responded, and whether it resulted in a purchase. Also includes a due date line and a line for the salesperson to print their name.\n\nScoring (0 to 4):\n- 4.0: DOCX/PDF, >=2 pages, professional formatting, and all six required sections present with the specified structural elements (including the Homework lines for due date and printed name, and practice mapping of objection \u2192 type \u2192 response).\n- 3.0: Correct format/length + 5/6 sections present (none of the missing pieces are the Homework lines or the Let's Practice mapping). Minor header naming variations acceptable.\n- 2.0: Correct format/length + 4/6 sections present.\n- 1.0: Correct format/length but only 2\u20133 sections present.\n- 0.0: Wrong format (not DOCX/PDF), <2 pages, or fewer than 2 sections.\n\nOnly check presence and structural format. Do not evaluate correctness or quality of content.", "expectation": "A 2\u20136 page, professional DOCX/PDF with all required sections and structural elements clearly labeled to enable verification."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Compliance Verification", "description": "Now that structure is enforced, verify correctness, completeness, and alignment to the brief. Mix code-based checks for concrete inclusions with higher-weight LLM checks for nuanced evaluation.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "All Five Objection Types Detected in Types Section", "description": "Verify the 'Types of Objections' section includes and describes the five required types (price, need, urgency, trust, authority) with flexible keyword matching.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n    if not text:\n        return 0.0\n    lower = text.lower()\n\n    def extract_section(txt, start_key, end_keys):\n        s = txt.find(start_key)\n        if s == -1:\n            return txt  # fallback to entire doc\n        e_positions = [txt.find(k) for k in end_keys if txt.find(k) != -1]\n        e = min(e_positions) if e_positions else len(txt)\n        return txt[s:e]\n\n    types_section = extract_section(\n        lower,\n        \"types of objections\",\n        [\n            \"core strategies to overcoming the objection\",\n            \"core strategies to overcoming objections\",\n            \"core strategies\",\n            \"let's practice\",\n            \"lets practice\",\n            \"conclusion\",\n            \"homework\",\n        ],\n    )\n\n    keywords = {\n        'price': [\"price\", \"cost\", \"budget\"],\n        'need': [\"need\", \"necessity\", \"relevance\"],\n        'urgency': [\"urgency\", \"time frame\", \"timeline\", \"timing\"],\n        'trust': [\"trust\", \"confidence\", \"credibility\", \"reputation\"],\n        'authority': [\"authority\", \"decision-maker\", \"decision maker\", \"partner\", \"parent\", \"friend\", \"approval\"],\n    }\n\n    found = 0\n    for _, kws in keywords.items():\n        if any(k in types_section for k in kws):\n            found += 1\n    return found / 5.0"}, {"type": "code", "name": "Homework Specificity & Tracking Requirements Present", "description": "Check that the Homework section requires tracking at least 6 objections over a week with fields (objection, type, response, purchase result) and includes a due date line and a print-name line.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n    if not text:\n        return 0.0\n    lower = text.lower()\n\n    def extract_section(txt, start_key, end_keys):\n        s = txt.find(start_key)\n        if s == -1:\n            return txt\n        e_positions = [txt.find(k) for k in end_keys if txt.find(k) != -1]\n        e = min(e_positions) if e_positions else len(txt)\n        return txt[s:e]\n\n    hw = extract_section(lower, \"homework\", [\"conclusion\", \"appendix\", \"supporting data\", \"references\"])  # typical end bounds\n\n    # Content requirements\n    qty_ok = bool(re.search(r\"(at least\\s*6|six)\\s+objection\", hw))\n    fields_ok = all(k in hw for k in [\"objection\", \"type\", \"respond\", \"purchase\"])  # respond/response, purchase/result\n\n    # Line placeholders\n    due_ok = (\"due date\" in hw) or bool(re.search(r\"due\\s*:\\s*\", hw))\n    name_ok = (\"print name\" in hw) or (\"printed name\" in hw) or bool(re.search(r\"name\\s*:\\s*\", hw))\n\n    structural = (1.0 if due_ok else 0.5 if (\"due\" in hw) else 0.0) + (1.0 if name_ok else 0.5 if (\"name\" in hw) else 0.0)\n    structural /= 2.0\n\n    content = 0.0\n    if qty_ok and fields_ok:\n        content = 1.0\n    elif qty_ok or fields_ok:\n        content = 0.6\n    else:\n        content = 0.0\n\n    return (structural + content) / 2.0"}, {"type": "code", "name": "Let's Practice Contains Multiple Mapped Pairs", "description": "Verify the 'Let's Practice' section includes multiple objection\u2192type\u2192response mappings (target \u22656).", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n    if not text:\n        return 0.0\n    lower = text.lower()\n\n    def extract_section(txt, start_key, end_keys):\n        s = txt.find(start_key)\n        if s == -1:\n            return txt\n        e_positions = [txt.find(k) for k in end_keys if txt.find(k) != -1]\n        e = min(e_positions) if e_positions else len(txt)\n        return txt[s:e]\n\n    prac = extract_section(lower, \"let's practice\", [\"conclusion\", \"homework\", \"summary\"]).replace(\"lets practice\", \"let's practice\")\n\n    # Count occurrences as a proxy for pairs\n    n_obj = len(re.findall(r\"\\bobjection\\b\\s*[:\\-]\", prac)) + len(re.findall(r\"\\bobjection\\b\", prac))\n    n_resp = len(re.findall(r\"\\bresponse\\b\\s*[:\\-]\", prac)) + len(re.findall(r\"\\bresponse\\b\", prac))\n    has_type = bool(re.search(r\"\\btype\\b\", prac)) or bool(re.search(r\"\\b(category|classification)\\b\", prac))\n\n    approx_pairs = min(n_obj, n_resp)\n    # Scoring: full at 6+, partial at 4\u20135, minimal at 2\u20133\n    if approx_pairs >= 6 and has_type:\n        return 1.0\n    elif approx_pairs >= 4 and has_type:\n        return 0.75\n    elif approx_pairs >= 2:\n        return 0.5\n    else:\n        return 0.0"}, {"type": "llm_judge", "name": "Practice Pair Quality and Coverage", "description": "Assess whether Let's Practice includes realistic, bridal-specific objections spanning all five types, each with type and an effective suggested response.", "weight": 2.8, "judge_prompt": "Evaluate the \"Let's Practice\" section for practical coverage and correctness.\n\nExpectations:\n- Includes at least 6 distinct practice items.\n- Each item clearly labels the objection's TYPE (price, need, urgency, trust, authority) and provides a tailored suggested response/script.\n- Coverage spans all five types overall (not necessarily evenly, but none should be completely absent unless there are 6+ examples and four+ types strongly covered).\n- Objections and responses are realistic for a bridal store (e.g., budget constraints, need to check with partner/parent, timeline before the wedding, designer/fit/alterations concerns, trust in quality/reviews).\n- Responses model empathy, probing questions, value reinforcement, options (payment plans, alterations, lead times), and a gentle close or next step.\n\nScoring (0\u20132.8):\n- 2.8: \u22656 items, all five types represented, each with clear type label and effective, bridal-specific response.\n- 2.1: \u22656 items, 4/5 types represented, responses mostly effective and bridal-specific.\n- 1.4: 4\u20135 items with type labels and generally appropriate responses; some gaps in coverage or specificity.\n- 0.7: 2\u20133 items or missing type labels; responses generic or weak.\n- 0.0: Fewer than 2 items or no responses.", "expectation": "A robust, realistic practice set that labels types and provides effective, bridal-relevant responses across all objection categories."}, {"type": "llm_judge", "name": "Strategies Framework Soundness", "description": "Assess whether the Core Strategies section provides a clear, ethical, step-by-step framework suitable for bridal sales objection handling.", "weight": 2.8, "judge_prompt": "Evaluate the \"Core Strategies to Overcoming the Objection\" section for clarity, completeness, and suitability for a bridal retail context.\n\nLook for:\n- A step-by-step framework (e.g., LAER/LAARC, Feel\u2013Felt\u2013Found, Empathize\u2013Probe\u2013Isolate\u2013Address\u2013Confirm\u2013Close).\n- Guidance that is ethical and customer-centric (empathy, active listening, clarifying questions, isolating the true objection, offering options aligned to the bride/couple's priorities, confirming resolution, appropriate closing or next step).\n- Practical tools: prompts/scripts, phrases to use, branching tactics for each type (price/need/urgency/trust/authority), notes on using store-specific levers (alterations, timelines, trunk shows, designer guarantees, payment plans, holds/appointments).\n- Common pitfalls to avoid.\n\nScoring (0\u20132.8):\n- 2.8: Clear, stepwise framework with scripts/templates and bridal-specific applications for all five types; ethical and actionable.\n- 2.1: Solid framework with some scripts/examples and coverage of most types; generally actionable.\n- 1.4: Basic guidance; partial framework or limited specificity; few scripts.\n- 0.7: Vague tips with little structure; minimal practicality.\n- 0.0: Missing or unusable framework.", "expectation": "A clear, bridal-relevant objection-handling framework with actionable steps and example language."}, {"type": "llm_judge", "name": "Context Alignment and Business Relevance", "description": "Verify the document references the observed conversion-rate decline, ties training to overcoming objections, and frames outcomes for individual reps and store performance.", "weight": 2.8, "judge_prompt": "Check whether the document aligns to the provided business context:\n- References the decline in closing conversion rate across new and seasoned team members.\n- States that poor objection handling is the observed root cause and that the training addresses this.\n- Connects benefits to individual sales performance and overall store KPIs (e.g., close rate, appointment-to-purchase conversion, average order value, revenue).\n- Optionally suggests how progress will be tracked (e.g., objection log review, weekly coaching huddles).\n\nScoring (0\u20132.8):\n- 2.8: Clearly references the decline, root cause, and expected impact on both individual and store KPIs; proposes tracking.\n- 2.1: Mentions decline and ties training to improved outcomes; light on KPI/measurement details.\n- 1.4: Vague acknowledgement of issues; minimal business linkage.\n- 0.7: Weak or implied linkage only.\n- 0.0: No alignment to the stated context.", "expectation": "An explicitly business-aligned document that ties training to conversion improvement at both individual and store levels."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Holistic Quality Assessment", "description": "Assess professional quality, usability, ethics, and suitability for bridal retail sales training.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity, Organization, and Readability", "description": "Professional presentation: clear headings, scannable layout, appropriate length, and coherent flow for quick team adoption.", "weight": 2.0, "judge_prompt": "Evaluate clarity and organization:\n- Clear hierarchy of headings matching the required sections; consistent formatting.\n- Scannable lists/tables; callouts or examples where helpful.\n- Appropriate length (about 2\u20136 pages) for a brief training.\n- Logical flow from Overview \u2192 Types \u2192 Strategies \u2192 Practice \u2192 Conclusion \u2192 Homework.\n\nScore 0\u20132:\n- 2.0: Very clear, well-structured, scannable, appropriate length, excellent flow.\n- 1.3: Generally clear, minor issues in flow/formatting/length.\n- 0.7: Some structure but hard to scan or follow.\n- 0.0: Poorly organized and hard to read.", "expectation": "A clean, readable training doc with consistent headings and scannable sections."}, {"type": "llm_judge", "name": "Audience Fit and Coaching Tone", "description": "Assess whether the tone and content suit bridal sales consultants and feel supportive, inclusive, and confidence-building.", "weight": 2.0, "judge_prompt": "Evaluate tone and audience fit:\n- Supportive, coach-like, and confidence-building; avoids shaming.\n- Empathetic language suited to sensitive bridal decisions and diverse couples/families.\n- Practical tips a bridal consultant can apply on the floor today.\n\nScore 0\u20132:\n- 2.0: Highly supportive, empathetic, and tailored to bridal consultants.\n- 1.3: Generally appropriate with minor tone/gap issues.\n- 0.7: Generic sales tone with limited empathy.\n- 0.0: Inappropriate or manipulative tone.", "expectation": "A coaching-oriented, empathetic tone tailored to bridal retail consultants."}, {"type": "llm_judge", "name": "Actionability and Enablement", "description": "Assess whether the document equips reps with scripts, prompts, and next steps for immediate use, including practice guidance.", "weight": 2.0, "judge_prompt": "Evaluate actionability:\n- Includes scripts/phrases, role-play prompts, or a quick-reference checklist.\n- Provides specific next steps for practice (e.g., pair role-play, manager coaching huddle, objection log usage).\n- Clear call-to-action in Conclusion and well-defined Homework instructions.\n\nScore 0\u20132:\n- 2.0: Highly actionable with scripts, prompts, and clear next steps.\n- 1.3: Mostly actionable; some elements missing.\n- 0.7: Limited immediate utility.\n- 0.0: Not actionable.", "expectation": "Concrete scripts, prompts, and clear practice steps that enable immediate on-floor use."}, {"type": "llm_judge", "name": "Ethics and Customer Experience", "description": "Ensure the guidance is ethical, respects budgets/relationships, and prioritizes a positive bridal experience.", "weight": 2.0, "judge_prompt": "Evaluate ethics and customer experience:\n- Avoids pressure/manipulation; respects budgets and the need to consult partners/family.\n- Encourages informed, confident decisions; transparent about policies (e.g., alterations timelines, ordering windows) where relevant.\n- Reinforces building trust and long-term reputation.\n\nScore 0\u20132:\n- 2.0: Strong ethical grounding and CX focus throughout.\n- 1.3: Generally ethical with minor gaps.\n- 0.7: Some pressuring elements or unclear boundaries.\n- 0.0: Pushy or manipulative guidance.", "expectation": "Customer-centric, ethical guidance that builds trust and long-term store reputation."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "ed2bc14c-99ac-4a2a-8467-482a1a5d67f3", "rubric": {"category_name": "Tenant Retention Strategy (Property Management)", "rationale": "This rubric enforces a self-documenting, verifiable 1\u20132 page memo (DOCX/PDF) that clearly structures data-backed insights from exit survey feedback into actionable retention tactics. Stage 1 is an LLM-only gate that mandates exact sections and artifacts to enable verification. Stage 2 blends light-weight code checks (structure, signals, bounds) with heavier LLM judgments (coherence, alignment, reasonableness). Stage 3 evaluates overall professional quality, strategic value, localization, and measurability. Code rules rely on robust text parsing from DOCX/PDF using the provided file APIs.", "max_total_score": 14.0, "stages": [{"name": "Stage 1 \u2013 Structured Memo Gate (Shape Enforcement)", "description": "Mandatory format and structure requirements for a concise, 1\u20132 page business memo in DOCX or PDF, with specific sections enabling verification.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 2.0, "rules": [{"type": "llm_judge", "name": "Document Format and Required Sections Present", "description": "Checks that the output is a professionally formatted 1\u20132 page memo (DOCX or PDF) with specific, verifiable sections and elements required for downstream verification.", "weight": 4.0, "judge_prompt": "You are checking only structure/format (not correctness). Review the candidate\u2019s output file. Confirm it is a business memo (professional format) and that it contains the required sections and elements below. Be flexible on exact section titles, but the content must clearly map to each requirement.\n\nFormat requirements:\n- File must be DOCX or PDF (not plain text, not Excel)\n- Approximately 1\u20132 pages (concise memo)\n- Professional memo structure (title, headings/subheadings, bullets or short tables where appropriate)\n\nRequired sections and elements (all must be present for full credit):\n1) Title/Header: Includes a clear title such as \u201cTenant Retention Strategy\u201d and property context (Harborview Flats, Stamford, 200 units).  \n2) Departure Reasons Analysis:  \n   - Mentions the source: \u201cExit Survey Feedback.xlsx\u201d (or \u201cExit Survey Feedback\u201d if file extension omitted).  \n   - Presents a simple, visible categorization summary into around five reason categories (e.g., rent increase too high, lack of community, maintenance, location/commute, amenities/parking/noise). A small table or clear bullet list is acceptable.  \n   - Clearly identifies the top two departure reasons and provides a brief explanation of their meaning/implications.  \n   - Includes a brief \u201cMethodology\u201d note describing how raw comments were categorized.\n3) Tiered Renewal Offer Structure:  \n   - Three distinct tiers included: Early Bird (~90 days out), Standard (~60 days out), and a Month-to-Month premium.  \n   - Each tier lists what the resident receives or pays (e.g., discount, credit, rate increase/premium) and any eligibility/triggers.  \n4) Communication Plan:  \n   - Timeline for 90/60/30-day renewal notifications.  \n   - Draft content elements: subject lines and 1\u20132 sentence snippets per touchpoint.  \n5) Community Engagement Initiatives:  \n   - Two low-cost, high-impact resident events tailored to property amenities and seasonality (e.g., use lounge, grills, front lawn; summer timing).  \n   - Include brief logistics and a rough budget range per event.\n\nOptional but encouraged (not required for full credit): brief KPI/owner table (e.g., targets, owners, timeline) and a short implementation note.\n\nScoring:\n- 4.0: Valid DOCX/PDF; memo-like and concise; all 5 required sections present with clear headers; each section includes the specific elements listed.\n- 3.0: Valid format; mostly memo-like; 1 minor element missing across the five sections (e.g., missing brief methodology or missing budget ranges for events).\n- 2.0: Valid format; 1 full required section missing OR multiple key elements missing across sections.\n- 1.0: Valid format but only 1\u20132 of the required sections present; not memo-like.\n- 0.0: Not DOCX/PDF or lacks memo structure entirely.", "expectation": "A concise DOCX/PDF memo with all required sections, including a visible categorization of exit reasons, tiered offers, communication timeline with drafts, and two tailored resident events."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification of Substance (Mixed Code + LLM)", "description": "Checks for key structural signals, data-use indications, and alignment between identified reasons and proposed tactics. Code rules do light, robust parsing; LLM judges handle nuanced reasoning.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 3.0, "rules": [{"type": "code", "name": "Document Type and Length (Word Count)", "description": "Verifies the candidate output is a document and approximately 1\u20132 pages by word count.", "weight": 0.3, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float (score) or tuple[float, str]\n    \"\"\"\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output file provided.\"\n    if not output.is_document:\n        return 0.0, \"Output is not a document (DOCX/PDF).\"\n\n    text = \"\"\n    fb = []\n    try:\n        # Prefer DOCX text if possible\n        if str(output.extension).lower() == \".docx\":\n            text = context.files.read_docx_text(output.id)\n        elif str(output.extension).lower() == \".pdf\":\n            text = context.files.read_pdf_text(output.id)\n        else:\n            # Fallback attempt\n            text = context.files.read_docx_text(output.id)\n            if not text:\n                text = context.files.read_pdf_text(output.id)\n    except Exception as e:\n        return 0.0, f\"Could not extract text from document: {e}\"\n\n    if not text:\n        return 0.0, \"Empty document text.\"\n\n    words = re.findall(r\"\\b\\w+\\b\", text)\n    wc = len(words)\n\n    # Heuristic mapping: 1 page ~ 350-500 words depending on formatting\n    # Full credit: 350\u2013900 words; Partial: 250\u20131100 words\n    if 350 <= wc <= 900:\n        score = 0.3\n        note = f\"Word count {wc} within ideal 1\u20132 page range.\"\n    elif 250 <= wc <= 1100:\n        score = 0.15\n        note = f\"Word count {wc} near acceptable range; borderline for 1\u20132 pages.\"\n    else:\n        score = 0.0\n        note = f\"Word count {wc} outside expected 1\u20132 page range.\"\n    return score, note"}, {"type": "code", "name": "Timeline and Tiers Signals", "description": "Checks for presence of 90/60/30-day timeline references, and that month-to-month premium language and numeric incentives are present.", "weight": 0.3, "code": "import re\n\ndef _extract_text(context, output):\n    text = \"\"\n    try:\n        if str(output.extension).lower() == \".docx\":\n            text = context.files.read_docx_text(output.id)\n        elif str(output.extension).lower() == \".pdf\":\n            text = context.files.read_pdf_text(output.id)\n    except Exception:\n        text = \"\"\n    return text or \"\"\n\ndef near_numeric(text, anchors, window=80):\n    t = text.lower()\n    for a in anchors:\n        for m in re.finditer(re.escape(a.lower()), t):\n            start = max(0, m.start()-window)\n            end = min(len(t), m.end()+window)\n            snippet = t[start:end]\n            if re.search(r\"[\\$%]|\\b(percent|credit|discount|off|increase|premium|surcharge)\\b\", snippet):\n                return True\n    return False\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = _extract_text(context, output)\n    t = text.lower()\n\n    has_90 = bool(re.search(r\"\\b90\\s*[-\u2013]?\\s*(day|days)\\b|\\b90d\\b|\\b90-day\\b\", t))\n    has_60 = bool(re.search(r\"\\b60\\s*[-\u2013]?\\s*(day|days)\\b|\\b60d\\b|\\b60-day\\b\", t))\n    has_30 = bool(re.search(r\"\\b30\\s*[-\u2013]?\\s*(day|days)\\b|\\b30d\\b|\\b30-day\\b\", t))\n\n    # Month-to-month premium near numeric terms\n    mtm_match = re.search(r\"month[-\\s]?to[-\\s]?month|\\bmtm\\b\", t)\n    mtm_numeric = False\n    if mtm_match:\n        s = max(0, mtm_match.start()-80)\n        e = min(len(t), mtm_match.end()+80)\n        snip = t[s:e]\n        mtm_numeric = bool(re.search(r\"premium|surcharge|additional|\\+|[\\$%]\", snip))\n\n    # Numeric incentives near early/90 and standard/60\n    early_numeric = near_numeric(t, [\"early\", \"early bird\", \"90\", \"90-day\", \"90 days\"])\n    std_numeric = near_numeric(t, [\"standard\", \"regular\", \"60\", \"60-day\", \"60 days\"])\n\n    checks = [has_90, has_60, has_30, mtm_match is not None, mtm_numeric, early_numeric, std_numeric]\n    passed = sum(bool(x) for x in checks)\n\n    # 7 checks total: full if >=6, half if >=4\n    if passed >= 6:\n        return 0.3, \"Timeline and tier signals comprehensive.\"\n    elif passed >= 4:\n        return 0.15, \"Timeline/tier signals partially present.\"\n    else:\n        return 0.0, \"Missing key timeline/tier indicators.\""}, {"type": "code", "name": "Property Context Anchoring", "description": "Verifies that the memo anchors to Harborview Flats in Stamford, CT, references 200 units, and mentions at least one on-site amenity relevant to initiatives.", "weight": 0.3, "code": "import re\n\ndef _get_text(context, output):\n    try:\n        if str(output.extension).lower() == \".docx\":\n            return context.files.read_docx_text(output.id) or \"\"\n        elif str(output.extension).lower() == \".pdf\":\n            return context.files.read_pdf_text(output.id) or \"\"\n    except Exception:\n        return \"\"\n    return \"\"\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    t = _get_text(context, output).lower()\n    if not t:\n        return 0.0\n\n    has_harborview = \"harborview\" in t\n    has_stamford = \"stamford\" in t\n    has_200 = bool(re.search(r\"\\b200\\b|200-unit|200 units\", t))\n\n    amenities = [\"resident lounge\", \"lounge\", \"grill\", \"gas grill\", \"work-from-home\", \"front lawn\", \"lawn\", \"parking lot\"]\n    has_amenity = any(a in t for a in amenities)\n\n    score = 0.0\n    parts = [has_harborview, has_stamford, has_200, has_amenity]\n    score = (sum(1 for p in parts if p) / 4.0) * 0.3\n\n    return score"}, {"type": "code", "name": "Exit Survey Analysis Signals", "description": "Checks for signals that exit survey feedback was analyzed and top two reasons were identified with brief methodology.", "weight": 0.3, "code": "import re\n\ndef _text(context, output):\n    try:\n        if str(output.extension).lower() == \".docx\":\n            return context.files.read_docx_text(output.id) or \"\"\n        elif str(output.extension).lower() == \".pdf\":\n            return context.files.read_pdf_text(output.id) or \"\"\n    except Exception:\n        return \"\"\n    return \"\"\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    t = _text(context, output).lower()\n    if not t:\n        return 0.0\n\n    # Mention of source and analysis\n    has_source = (\"exit survey\" in t) or (\"feedback.xlsx\" in t) or (\"xlsx\" in t and \"exit\" in t)\n    has_method = any(w in t for w in [\"methodology\", \"categor\", \"coded\", \"approach\", \"how we grouped\"])  # categor covers categorize/categorization\n    # Top reasons statement\n    has_top_two = bool(re.search(r\"top\\s*(two|2)\\s*reasons|top\\s*reasons\", t))\n\n    categories = [\n        \"rent increase\", \"lack of community\", \"maintenance\", \"noise\", \"location\", \"parking\", \"safety\", \"management responsiveness\", \"amenities\", \"commute\"\n    ]\n    cat_count = sum(1 for c in categories if c in t)\n\n    score = 0.0\n    if has_source:\n        score += 0.09\n    if has_method:\n        score += 0.09\n    if has_top_two:\n        score += 0.06\n    # Reward listing multiple plausible categories up to 0.06\n    score += min(cat_count, 3) * 0.02\n\n    return min(score, 0.3)"}, {"type": "llm_judge", "name": "Alignment: Reasons \u2192 Offers, Communications, Events", "description": "Assesses whether the identified top-two departure reasons are explicitly addressed by the tiered offers, communication plan, and engagement initiatives with a clear line-of-sight.", "weight": 1.2, "judge_prompt": "Evaluate the memo for alignment between the identified top-two reasons residents leave and the proposed actions. Specifically:\n- Does the Tiered Renewal Offer Structure directly target those reasons (e.g., price-sensitive residents get economic relief; community-related issues get community-building value)?\n- Do the 90/60/30 communications reinforce the value propositions aimed at those top reasons?\n- Do the two resident events explicitly support mitigation of those top reasons (e.g., belonging, connection, amenity activation)?\nScoring:\n- 1.2: Clear, explicit linkage from each top reason to offers, messaging, and events; rationale is stated or obvious.\n- 0.8: Mostly aligned with minor gaps.\n- 0.4: Weak or implicit alignment; missing coverage for one top reason.\n- 0.0: No discernible alignment.", "expectation": "Each top reason has a corresponding tactical response across offers, communications, and events."}, {"type": "llm_judge", "name": "Tiered Incentive Logic and Reasonableness", "description": "Evaluates whether Early Bird and Standard offers are meaningfully differentiated and whether Month-to-Month is appropriately disincentivized.", "weight": 1.2, "judge_prompt": "Review the tiered offer structure:\n- Is the Early Bird (\u224890 days) meaningfully better than Standard (\u224860 days)?\n- Is Month-to-Month priced with a clear premium vs. renewal (e.g., surcharge or higher rent) to encourage commitment?\n- Are incentives reasonable for a Stamford, CT 200-unit property (e.g., modest credits/discounts, not extreme giveaways)?\nScoring:\n- 1.2: Clear differentiation, reasonable incentive sizes, and appropriate MTM premium.\n- 0.8: Mostly reasonable; small inconsistencies.\n- 0.4: Minimal differentiation or unclear premium.\n- 0.0: Illogical or self-defeating incentives.", "expectation": "Early Bird > Standard in value; MTM has a clear premium; numbers are plausible for the market."}, {"type": "llm_judge", "name": "Communication Plan Quality and Tone", "description": "Judges clarity and professionalism of 90/60/30-day drafts and whether tone matches resident-friendly communications.", "weight": 1.2, "judge_prompt": "Assess the communication plan:\n- Are the 90/60/30 messages clear and progressively increase urgency while staying helpful?\n- Do they include subject lines and brief draft snippets?\n- Is the tone professional, courteous, and resident-friendly (consistent with typical renewal letters if a specific reference is not provided)?\nScoring:\n- 1.2: Clear, well-staged drafts with professional tone.\n- 0.8: Adequate drafts with minor tone or clarity issues.\n- 0.4: Sparse or generic drafts; tone mediocre.\n- 0.0: Missing drafts or unprofessional tone.", "expectation": "Succinct subject lines and 1\u20132 sentence drafts per touchpoint; tone is professional and service-oriented."}, {"type": "llm_judge", "name": "Data Use and Categorization Rigor", "description": "Assesses whether the memo shows a concrete categorization of exit survey feedback and clearly identifies the top two reasons with brief meaning/implication.", "weight": 1.2, "judge_prompt": "Check the Departure Reasons Analysis:\n- Is there a visible categorization into about five reasons (table or bullet list acceptable)?\n- Are the top two reasons clearly identified and briefly interpreted (what they mean for the property)?\n- Is a short methodology note provided indicating how comments were grouped?\nScoring:\n- 1.2: Clear categories with top two and concise interpretation; methodology present.\n- 0.8: Mostly complete; minor missing details.\n- 0.4: Vague categorization or missing interpretation.\n- 0.0: No real categorization or top-two identification.", "expectation": "A small table or bullet list of categories, top-two called out, and a 1\u20132 sentence methodology."}], "on_failure_action": "skip_remaining", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Holistic Quality Assessment", "description": "Evaluates professionalism, strategic feasibility, localization to the property and season, and measurability of the plan.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Memo Quality and Clarity", "description": "Overall memo polish, structure, readability, and concision within 1\u20132 pages.", "weight": 1.5, "judge_prompt": "Evaluate the memo\u2019s professional quality: clear title and headings, logical flow, concise writing, effective bullets/tables, minimal jargon, and overall readability. Is it plausibly 1\u20132 pages and visually memo-like?\nScoring:\n- 1.5: Highly professional, clear, and concise.\n- 1.0: Generally professional with minor issues.\n- 0.5: Adequate but cluttered or hard to follow.\n- 0.0: Poorly structured or unclear.", "expectation": "Concise, well-structured memo with clear headings and well-formatted lists/tables."}, {"type": "llm_judge", "name": "Strategic Impact and Feasibility", "description": "Assesses whether the plan can plausibly lift retention by ~10% in 6 months and considers cost/benefit and execution realism.", "weight": 1.0, "judge_prompt": "Judge the strategic strength of the plan: Are tactics likely to influence a ~10% retention lift in 6 months? Are incentives/events cost-conscious and feasible? Are there brief considerations of ROI or tradeoffs?\nScoring:\n- 1.0: Strong, plausible impact with feasible execution and basic ROI awareness.\n- 0.7: Reasonable but not compelling; limited feasibility details.\n- 0.3: Unclear impact or weak feasibility.\n- 0.0: Impractical or unlikely to move retention.", "expectation": "Plausible, feasible levers with cost-aware framing and realistic impact."}, {"type": "llm_judge", "name": "Localization and Compliance Awareness", "description": "Evaluates tailoring to Harborview Flats (amenities, Stamford/seasonality) and awareness of fair housing/compliance-friendly language.", "weight": 0.8, "judge_prompt": "Assess localization: Are recommendations tailored to Harborview\u2019s lounge/grills/work-from-home zones/front lawn and Stamford summer season? Do they avoid discriminatory language and reflect fair housing/compliance awareness (neutral, inclusive phrasing)?\nScoring:\n- 0.8: Strong property/season tailoring and compliance-aware.\n- 0.5: Some tailoring; minor compliance tone issues.\n- 0.2: Generic with minimal tailoring.\n- 0.0: Inappropriate or non-tailored.", "expectation": "Property-specific ideas leveraging amenities and inclusive, compliant tone."}, {"type": "llm_judge", "name": "KPIs and Implementation Readiness", "description": "Checks for measurable KPIs, owners, and timeline to track progress during the 6-month period.", "weight": 0.7, "judge_prompt": "Does the memo define measurable KPIs (e.g., retention rate, renewal conversion, email open/click rates, event attendance), assign owners, and outline a simple timeline for the next 6 months?\nScoring:\n- 0.7: Clear KPIs, ownership, and simple timeline.\n- 0.4: Some KPIs or timeline, but incomplete.\n- 0.2: Vague metrics without ownership or timeline.\n- 0.0: No measurement plan.", "expectation": "A brief KPI/owner/timeline note or small table sufficient to operationalize tracking."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "02aa1805-c658-4069-8a6a-02dec146063a", "rubric": {"category_name": "Green Hydrogen Project \u2013 Illinois EPA Well Screening (Project Management)", "rationale": "Mixed-output task: a structured Excel workbook drives verifiable screening logic, and a professional email summarizes and recommends top options. Stage 1 is a strict LLM-only gate enforcing the exact deliverable shapes (two-sheet workbook + email). Stage 2 mixes deterministic code checks (sheet/column presence, boolean filter logic, subset integrity, coverage of required systems) with LLM verification of interpretation and alignment. Stage 3 evaluates overall professional quality, clarity, and usefulness for a manager. Code rules are lighter-weight than LLM rules, per guidance.", "max_total_score": 17.0, "stages": [{"name": "Stage 1 \u2013 Structured Deliverables Gate", "description": "Gate: Verify the candidate produced BOTH the required Excel workbook with two specific tabs and the manager email document with highlighted recommendations. Only structure is checked here, not correctness of data or logic.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Workbook Structure Present (2 Tabs, Specific Columns)", "description": "Check that an Excel workbook is present with two sheets: an 'All Wells' sheet (or similarly named) containing all extracted wells with all required columns plus helper filter columns; and a 'Potential Wells' sheet (or similarly named) containing only wells meeting all criteria, with the same core columns.", "weight": 2.5, "judge_prompt": "You are evaluating the STRUCTURE of the submitted outputs (not the correctness of values). Look through the provided files. Confirm the presence and structure of an Excel workbook with BOTH of these sheets:\n\nSheet A: \"All Wells\" (accept flexible naming like: All Wells, All_Wells, All Wells Extracted, Master Wells)\n- Must include a single tab that contains ALL extracted wells from the listed Illinois systems.\n- Must include the following core columns (flexible naming OK, but the meaning should be clear):\n  \u2022 Water system\n  \u2022 Well ID\n  \u2022 Well Description\n  \u2022 Status\n  \u2022 Depth\n  \u2022 Minimum Setback\n  \u2022 Pumpage\n  \u2022 Aquifer Code\n  \u2022 Aquifer Description\n  \u2022 Max Zone\n- Must also include helper/filtering structure for each criterion and an overall flag column to easily filter wells that meet ALL required criteria. Accept flexible names such as: Depth_OK, Aquifer_OK, Active_OK, and a composite like Meets All / Meets Criteria / Selected / Include.\n\nSheet B: \"Potential Wells\" (accept flexible naming like: Potential Wells, Candidates, Shortlist, Screened, Selected, Viable)\n- Must contain ONLY wells that meet all required criteria, using the same core columns as above.\n\nScoring:\n- 2.5: Workbook present and both sheets present, with all core columns on All Wells, helper/flag columns for criteria filtering, and a second sheet containing only the potential wells.\n- 1.8: Workbook present and both sheets present with the core columns, but helper/flag columns for criteria filtering on All Wells are missing or unclear.\n- 1.0: Workbook present but only one of the required sheets is present OR several core columns are missing.\n- 0.0: No Excel workbook found OR sheets/structure are not present as described.\n\nOnly evaluate presence/structure, not data correctness.", "expectation": "Two-sheet Excel workbook with the exact column set and a composite filter column in the first sheet; second sheet shows filtered results only."}, {"type": "llm_judge", "name": "Manager Email Document Present (Recommendations Highlighted)", "description": "Check that a professional email (PDF/DOCX/MD) to the manager exists, explicitly calling out the recommended wells and referencing the attached Excel.", "weight": 1.5, "judge_prompt": "Check for a professional email-style document (PDF, DOCX, or Markdown) addressed to a manager. Confirm:\n- It clearly states the purpose (screening Illinois EPA well data for green hydrogen water sourcing) and references the attached Excel workbook.\n- It highlights or lists the top options (recommended wells) and briefly states why they were selected based on the criteria (depth 160\u2013200; aquifer description includes sand and gravel; not abandoned/inactive/disconnected/emergency/sealed).\n- It uses a professional tone appropriate for a senior project manager.\n\nScoring:\n- 1.5: Email document present, references the Excel attachment, clearly highlights top options, and ties them to the stated criteria.\n- 0.8: Email document present and professional but either does not reference the attachment OR does not clearly tie recommendations to criteria.\n- 0.3: A document exists but is not in email form or is missing key elements (no recommendations or no context).\n- 0.0: No email-like document present.", "expectation": "A professional email to the manager that references the attached workbook and highlights top recommended wells with brief rationale."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness and Verification", "description": "Verify that the workbook implements the criteria correctly, contains the required systems, and that recommendations in the email align with the filtered results. Mix of code (deterministic checks) and LLM (interpretation/alignment).", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Required Columns Identified and Typed", "description": "Confirm the All Wells sheet exists, locate core columns with flexible matching, and check that Depth and Pumpage are numeric/coercible.", "weight": 0.6, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        # Find a spreadsheet output\n        outputs = context.get_all_outputs()\n        xls_res = None\n        for r in outputs:\n            if getattr(r, 'is_spreadsheet', False):\n                xls_res = r\n                break\n        if not xls_res:\n            return 0.0, \"No spreadsheet resource found\"\n\n        fp = context.files.get_path(xls_res.id)\n        try:\n            xls = pd.ExcelFile(fp)\n        except Exception as e:\n            return 0.0, f\"Failed to open Excel: {e}\"\n\n        # Identify sheets by fuzzy names\n        sheet_names = [s for s in xls.sheet_names]\n        ln = [s.lower() for s in sheet_names]\n        all_idx = None\n        pot_idx = None\n        for i, n in enumerate(ln):\n            if (\"all\" in n and \"well\" in n) or (\"master\" in n and \"well\" in n) or (\"extracted\" in n and \"well\" in n):\n                all_idx = i\n            if (\"potential\" in n and \"well\" in n) or (\"candidate\" in n) or (\"shortlist\" in n) or (\"screened\" in n) or (\"selected\" in n) or (\"viable\" in n and \"well\" in n):\n                pot_idx = i\n        if all_idx is None and sheet_names:\n            all_idx = 0  # fallback to first\n        if all_idx is None:\n            return 0.0, \"No All Wells-like sheet\"\n\n        all_df = pd.read_excel(fp, sheet_name=sheet_names[all_idx])\n        cols = [str(c) for c in all_df.columns]\n        lcols = [c.lower() for c in cols]\n\n        def find_col(cands):\n            for i, c in enumerate(lcols):\n                for cand in cands:\n                    if cand in c or c in cand:\n                        return cols[i]\n            return None\n\n        synonyms = {\n            'water_system': ['water system','system name','system','water supply','utility','pws name','pws id','supplier'],\n            'well_id': ['well id','well number','well no','well #','id'],\n            'well_description': ['well description','description','well type','well notes','desc'],\n            'status': ['status','well status','activity','active','operational status'],\n            'depth': ['depth','well depth','total depth'],\n            'minimum_setback': ['minimum setback','min setback','setback','setback distance'],\n            'pumpage': ['pumpage','capacity','gpm','flow','yield','pumping rate','design capacity'],\n            'aquifer_code': ['aquifer code','aq code','aquifer id','aquifer'],\n            'aquifer_description': ['aquifer description','aquifer desc','aquifer name','aquifer type'],\n            'max_zone': ['max zone','maximum zone','zone max','zone']\n        }\n\n        found = {}\n        for k, cands in synonyms.items():\n            found[k] = find_col(cands)\n\n        required_keys = list(synonyms.keys())\n        present = sum(1 for k in required_keys if found.get(k))\n        base_score = present / len(required_keys)\n\n        # Type checks for Depth and Pumpage\n        bonus = 0.0\n        numeric_ok = 0\n        for k in ['depth','pumpage']:\n            col = found.get(k)\n            if col and col in all_df.columns:\n                try:\n                    pd.to_numeric(all_df[col], errors='coerce')\n                    numeric_ok += 1\n                except Exception:\n                    pass\n        if numeric_ok == 2:\n            bonus = 0.15\n        elif numeric_ok == 1:\n            bonus = 0.05\n\n        score = max(0.0, min(1.0, 0.85*base_score + bonus))\n        return score, f\"Columns present: {present}/{len(required_keys)}, numeric_ok={numeric_ok}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Meets-All Criteria Logic Consistency", "description": "Check that the All Wells sheet includes a Meets/Selected-style flag and that its truth aligns with the stated rules: Depth 160\u2013200 inclusive, Aquifer Description contains both 'sand' and 'gravel', and Well Description/Status do not contain disqualifying words.", "weight": 0.6, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs()\n        xls_res = None\n        for r in outputs:\n            if getattr(r, 'is_spreadsheet', False):\n                xls_res = r\n                break\n        if not xls_res:\n            return 0.0, \"No spreadsheet resource found\"\n        fp = context.files.get_path(xls_res.id)\n        xls = pd.ExcelFile(fp)\n        sheet_names = xls.sheet_names\n        # Find All Wells sheet\n        all_sheet = None\n        for s in sheet_names:\n            sl = s.lower()\n            if (\"all\" in sl and \"well\" in sl) or (\"master\" in sl and \"well\" in sl) or (\"extracted\" in sl and \"well\" in sl):\n                all_sheet = s\n        if all_sheet is None and sheet_names:\n            all_sheet = sheet_names[0]\n        if all_sheet is None:\n            return 0.0, \"No All Wells sheet\"\n        df = pd.read_excel(fp, sheet_name=all_sheet)\n        cols = [str(c) for c in df.columns]\n        lcols = [c.lower() for c in cols]\n\n        def find_col(cands):\n            for i, c in enumerate(lcols):\n                for cand in cands:\n                    if cand in c or c in cand:\n                        return cols[i]\n            return None\n\n        depth_col = find_col(['depth','well depth','total depth'])\n        aquifer_desc_col = find_col(['aquifer description','aquifer desc','aquifer name','aquifer type'])\n        desc_col = find_col(['well description','description','well type','well notes','desc'])\n        status_col = find_col(['status','well status','activity','active','operational status'])\n        meets_col = find_col(['meets all','meets criteria','eligible','passes all','selected','include','flag'])\n\n        if not depth_col or not aquifer_desc_col or not desc_col:\n            return 0.0, \"Missing required columns for logic check\"\n        if meets_col is None:\n            # Helper/flag column missing \u2013 low score by design\n            meets_series = None\n        else:\n            meets_series = df[meets_col]\n\n        depth = pd.to_numeric(df[depth_col], errors='coerce')\n        aq = df[aquifer_desc_col].astype(str).str.lower()\n        desc = df[desc_col].astype(str).str.lower()\n        status = df[status_col].astype(str).str.lower() if status_col else pd.Series([\"\"]*len(df))\n\n        bad_words = ['abandoned','inactive','disconnected','emergency','sealed']\n        not_bad = ~desc.fillna('').str.contains('|'.join(bad_words), regex=True)\n        if status_col:\n            not_bad &= ~status.fillna('').str.contains('|'.join(bad_words), regex=True)\n\n        depth_ok = (depth >= 160) & (depth <= 200)\n        aq_ok = aq.str.contains('sand') & aq.str.contains('gravel')\n        should_meet = depth_ok.fillna(False) & aq_ok.fillna(False) & not_bad.fillna(False)\n\n        if meets_series is None:\n            # Without a meets flag, award small credit proportional to presence of qualifying rows\n            prop_ok = float(should_meet.mean()) if len(should_meet) else 0.0\n            score = 0.2 + 0.3*prop_ok  # cap at 0.5\n            return min(1.0, score), \"No Meets flag; partial credit based on inherent criteria match proportion\"\n\n        # Normalize meets flag to boolean\n        m = meets_series.astype(str).str.strip().str.lower()\n        m_bool = m.isin(['true','yes','y','1','selected','include','included','pass','passes','eligible'])\n        # Also consider numeric or explicit booleans\n        try:\n            m_num = pd.to_numeric(meets_series, errors='coerce')\n            m_bool = m_bool | (m_num == 1)\n        except Exception:\n            pass\n\n        comparable = should_meet.notna()\n        if comparable.sum() == 0:\n            return 0.0, \"No comparable rows\"\n        agree = (m_bool == should_meet.fillna(False))\n        accuracy = float(agree.mean())\n        return max(0.0, min(1.0, accuracy)), f\"Agreement accuracy: {accuracy:.2f}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Potential Wells Tab Integrity", "description": "Confirm Potential Wells tab exists, contains only wells that meet all criteria, and entries correspond to All Wells rows.", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs()\n        xls_res = None\n        for r in outputs:\n            if getattr(r, 'is_spreadsheet', False):\n                xls_res = r\n                break\n        if not xls_res:\n            return 0.0, \"No spreadsheet resource found\"\n        fp = context.files.get_path(xls_res.id)\n        xls = pd.ExcelFile(fp)\n        sheet_names = xls.sheet_names\n        # Identify sheets\n        all_sheet = None\n        pot_sheet = None\n        for s in sheet_names:\n            sl = s.lower()\n            if (\"all\" in sl and \"well\" in sl) or (\"master\" in sl and \"well\" in sl) or (\"extracted\" in sl and \"well\" in sl):\n                all_sheet = s\n            if (\"potential\" in sl and \"well\" in sl) or (\"candidate\" in sl) or (\"shortlist\" in sl) or (\"screened\" in sl) or (\"selected\" in sl) or (\"viable\" in sl and \"well\" in sl):\n                pot_sheet = s\n        if all_sheet is None and sheet_names:\n            all_sheet = sheet_names[0]\n        if pot_sheet is None:\n            return 0.0, \"No Potential Wells-like sheet\"\n        df_all = pd.read_excel(fp, sheet_name=all_sheet)\n        df_pot = pd.read_excel(fp, sheet_name=pot_sheet)\n\n        # Column matching\n        def find_col(df, cands):\n            cols = [str(c) for c in df.columns]\n            lcols = [c.lower() for c in cols]\n            for i, c in enumerate(lcols):\n                for cand in cands:\n                    if cand in c or c in cand:\n                        return cols[i]\n            return None\n        ws_col = find_col(df_all, ['water system','system name','system','water supply','utility','pws name'])\n        id_col = find_col(df_all, ['well id','well number','well no','well #','id'])\n        depth_col = find_col(df_all, ['depth','well depth','total depth'])\n        aq_desc_col = find_col(df_all, ['aquifer description','aquifer desc','aquifer name','aquifer type'])\n        desc_col = find_col(df_all, ['well description','description','well type','well notes','desc'])\n        status_col = find_col(df_all, ['status','well status','activity','active','operational status'])\n\n        if not (ws_col and id_col and depth_col and aq_desc_col and desc_col):\n            return 0.0, \"Missing needed columns for integrity check\"\n\n        # Build key for matching\n        all_keys = set((str(a).strip().lower(), str(b).strip().lower()) for a,b in zip(df_all[ws_col], df_all[id_col]))\n        pot_keys = set((str(a).strip().lower(), str(b).strip().lower()) for a,b in zip(df_pot.get(ws_col, df_pot.get(ws_col.title(), df_pot.iloc[:,0])), df_pot.get(id_col, df_pot.get(id_col.title(), df_pot.iloc[:,1]))) )\n\n        # Criteria on All Wells\n        depth = pd.to_numeric(df_all[depth_col], errors='coerce')\n        aq = df_all[aq_desc_col].astype(str).str.lower()\n        desc = df_all[desc_col].astype(str).str.lower()\n        status = df_all[status_col].astype(str).str.lower() if status_col else pd.Series([\"\"]*len(df_all))\n        bad_words = ['abandoned','inactive','disconnected','emergency','sealed']\n        not_bad = ~desc.fillna('').str.contains('|'.join(bad_words), regex=True)\n        if status_col:\n            not_bad &= ~status.fillna('').str.contains('|'.join(bad_words), regex=True)\n        depth_ok = (depth >= 160) & (depth <= 200)\n        aq_ok = aq.str.contains('sand') & aq.str.contains('gravel')\n        should_meet = depth_ok.fillna(False) & aq_ok.fillna(False) & not_bad.fillna(False)\n\n        # Map keys of rows that should meet\n        meet_keys = set((str(ws).strip().lower(), str(i).strip().lower()) for ws, i, ok in zip(df_all[ws_col], df_all[id_col], should_meet) if bool(ok))\n\n        # Metrics\n        if len(df_pot) == 0:\n            return 0.0, \"Potential sheet empty\"\n        subset_ok = len(pot_keys - all_keys) == 0\n        all_meet = len(pot_keys - meet_keys)\n        precision = 1.0 - (all_meet / max(1, len(pot_keys)))\n        recall = len(pot_keys & meet_keys) / max(1, len(meet_keys))\n        score = max(0.0, min(1.0, 0.6*precision + 0.4*recall))\n        if not subset_ok:\n            score *= 0.7\n        return score, f\"precision={precision:.2f}, recall={recall:.2f}, subset_ok={subset_ok}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Coverage of Required Water Systems", "description": "Verify the All Wells sheet includes at least one well for each specified water system: Farmer City, Springerton, Bartlett, Enfield, Crossville, Weldon, Norris City, Waynesville.", "weight": 0.3, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs()\n        xls_res = None\n        for r in outputs:\n            if getattr(r, 'is_spreadsheet', False):\n                xls_res = r\n                break\n        if not xls_res:\n            return 0.0, \"No spreadsheet\"\n        fp = context.files.get_path(xls_res.id)\n        xls = pd.ExcelFile(fp)\n        # Find All Wells sheet\n        all_sheet = None\n        for s in xls.sheet_names:\n            sl = s.lower()\n            if (\"all\" in sl and \"well\" in sl) or (\"master\" in sl and \"well\" in sl) or (\"extracted\" in sl and \"well\" in sl):\n                all_sheet = s\n        if all_sheet is None and xls.sheet_names:\n            all_sheet = xls.sheet_names[0]\n        if all_sheet is None:\n            return 0.0, \"No All Wells sheet\"\n        df = pd.read_excel(fp, sheet_name=all_sheet)\n        # Find water system column\n        cols = [str(c) for c in df.columns]\n        lcols = [c.lower() for c in cols]\n        ws_col = None\n        for i,c in enumerate(lcols):\n            for cand in ['water system','system name','system','water supply','utility','pws name','supplier']:\n                if cand in c or c in cand:\n                    ws_col = cols[i]\n                    break\n            if ws_col:\n                break\n        if ws_col is None:\n            return 0.0, \"No Water System column\"\n        systems = ['Farmer City','Springerton','Bartlett','Enfield','Crossville','Weldon','Norris City','Waynesville']\n        present = 0\n        values = df[ws_col].astype(str).str.lower()\n        for s in systems:\n            sl = s.lower()\n            if values.str.contains(sl, regex=False).any():\n                present += 1\n        score = present / len(systems)\n        return score, f\"Systems covered: {present}/{len(systems)}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "llm_judge", "name": "Criteria Stated and Applied Correctly", "description": "Confirm the criteria are clearly stated somewhere (email and/or workbook) and match the task: Depth 160\u2013200 inclusive; aquifer description includes sand and gravel; exclude abandoned/inactive/disconnected/emergency/sealed.", "weight": 2.2, "judge_prompt": "Check the workbook and the email for an explicit statement of screening criteria. Verify:\n- Depth range is specified as 160\u2013200 (inclusive).\n- Aquifer Description requires both the words sand and gravel (or equivalent phrasing like sand & gravel).\n- Exclusions include abandoned, inactive, disconnected, emergency, or sealed wells (as stated).\n- The Potential Wells sheet appears to reflect these exact criteria (at least superficially consistent with the All Wells filters/flags).\n\nScoring:\n- 2.2: All criteria accurately stated and visibly applied (helper columns or clear logic), with Potential Wells consistent.\n- 1.4: Criteria mostly accurate but one aspect is ambiguous or slightly off (e.g., depth inclusivity unclear) while Potential Wells broadly aligns.\n- 0.7: Criteria vaguely stated or partially incorrect, though Potential Wells seems partly aligned.\n- 0.0: Criteria not stated or clearly incorrect relative to task.", "expectation": "Clear, correct criteria text and visible application aligning with the Potential Wells tab."}, {"type": "llm_judge", "name": "EPA Source Traceability", "description": "Check that the email and/or workbook cites the Illinois EPA SWAP factsheets (URL or system-level references) as the data source.", "weight": 1.3, "judge_prompt": "Verify the presence of source attribution to Illinois EPA Source Water Assessment Program factsheets. Look for:\n- The provided URL or links to system-specific factsheets.\n- Mention of Illinois EPA SWAP or equivalent authoritative description.\n- Basic traceability (which factsheets correspond to which systems) is at least implied.\n\nScoring:\n- 1.3: Clear citation with URL(s) and explicit attribution to Illinois EPA SWAP; ideally references to specific systems.\n- 0.8: General attribution to Illinois EPA SWAP without URLs or specifics.\n- 0.3: Vague data source mention, not clearly tied to Illinois EPA SWAP.\n- 0.0: No data source attribution.", "expectation": "Citations/links to Illinois EPA SWAP factsheets, ideally system-specific."}, {"type": "llm_judge", "name": "Top Options Alignment With Potential Wells", "description": "Ensure the wells highlighted in the email are present in the Potential Wells sheet and logically reflect the criteria.", "weight": 1.3, "judge_prompt": "Compare the top options highlighted in the email against the Potential Wells sheet. Check:\n- Each recommended well appears in the Potential Wells sheet.\n- The recommendations seem plausible given the visible data (depth, aquifer description as sand & gravel, active/not disqualified).\n- The count of highlighted wells matches the narrative, and they belong to the requested systems.\n\nScoring:\n- 1.3: Strong alignment: all recommended wells are in Potential Wells and clearly match the criteria.\n- 0.8: Mostly aligned with minor inconsistencies (e.g., one well uncertain) but generally correct.\n- 0.3: Weak alignment with multiple discrepancies.\n- 0.0: No evident alignment between recommendations and the Potential Wells tab.", "expectation": "Recommendations correspond exactly to the filtered candidates and match the criteria."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Professional Quality and Actionability", "description": "Holistic assessment of communication quality, workbook usability, and usefulness for decision-making.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Email Quality", "description": "Clarity, concision, tone, and direct recommendation to the manager.", "weight": 1.5, "judge_prompt": "Evaluate the email for professional quality appropriate to a senior project manager:\n- Clear purpose and concise context.\n- Direct, prioritized recommendations (e.g., top X wells) and any caveats.\n- Professional tone and formatting; scannable structure (bullets, short paragraphs).\n\nScoring:\n- 1.5: Clear, concise, professional; strong, actionable recommendations.\n- 1.0: Generally professional but somewhat verbose or less focused.\n- 0.5: Understandable but lacks polish or clear prioritization.\n- 0.0: Unprofessional or confusing.", "expectation": "A crisp, executive-ready email with clear recommendations."}, {"type": "llm_judge", "name": "Workbook Usability and Documentation", "description": "Ease of filtering, helper columns, clarity of labels, and basic documentation within the workbook.", "weight": 1.5, "judge_prompt": "Assess workbook usability:\n- Helper columns for each criterion (Depth_OK, Aquifer_OK, Active_OK) and a composite flag present and clearly labeled.\n- Columns are well-named; data is clean and filterable.\n- Light documentation (notes/readme tab, comments, or header text) to explain methodology and how to use filters.\n\nScoring:\n- 1.5: Very usable with clear helper logic and brief documentation.\n- 1.0: Usable and mostly clear; minor labeling or documentation gaps.\n- 0.5: Barely usable; unclear helpers or confusing labels.\n- 0.0: Difficult to use for filtering or understanding.", "expectation": "Well-labeled filters/helpers, and brief instructions or notes."}, {"type": "llm_judge", "name": "Actionability and Next Steps", "description": "Does the work inform next steps (e.g., confirm pump tests, contact utilities, permitting/timeline considerations)?", "weight": 1.0, "judge_prompt": "Evaluate whether the email outlines sensible next steps for due diligence and project planning, for example:\n- Confirming pumpage/yield and water rights/availability with the utility.\n- Checking seasonal variability or setback/regulatory constraints.\n- Laying out immediate actions and owners/timelines.\n\nScoring:\n- 1.0: Specific, pragmatic next steps tailored to the wells/systems.\n- 0.6: Some next steps but generic.\n- 0.2: Vague gestures at follow-up.\n- 0.0: No next steps provided.", "expectation": "Concrete follow-up plan tied to the recommendations."}, {"type": "llm_judge", "name": "Data Integrity and Presentation", "description": "Overall cleanliness, absence of obvious inconsistencies, and presentation quality of tables and figures.", "weight": 1.0, "judge_prompt": "Review the workbook and email for overall data integrity and presentation quality:\n- No obvious typos, duplicated headers, or formatting errors.\n- Consistent units and coherent values (depths and pumpage look reasonable at a glance).\n- Tables are readable and consistently formatted.\n\nScoring:\n- 1.0: Clean and polished; no obvious integrity issues.\n- 0.6: Minor issues not affecting overall understanding.\n- 0.3: Noticeable issues that reduce confidence.\n- 0.0: Poorly presented or error-prone.", "expectation": "Clean, consistent presentation that inspires confidence."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "c357f0e2-963d-4eb7-a6fa-3078fe55b3ba", "rubric": {"category_name": "UAT Test Plan (ProjMGR Tool) - Structured, Verifiable, and High-Quality Excel Deliverable", "rationale": "This rubric enforces a self-documenting, verifiable Excel UAT plan shaped as a single workbook table. Stage 1 (LLM-only) mandates exact structure so later checks are trivial. Stage 2 mixes lightweight code checks (bounds, coverage) with heavier LLM verification of functional/permission coverage aligned to the implementation outline. Stage 3 assesses professional quality and usability for test execution. Code rules carry substantially less weight than LLM judges, reflecting nuanced evaluation needs.", "max_total_score": 21.0, "stages": [{"name": "Stage 1 - Shape Enforcement Gate", "description": "Validates that the output is an Excel UAT plan in a specific, verification-friendly structure. LLM-only per philosophy.", "is_required": true, "max_points": 5.0, "min_score_to_pass": 3.5, "rules": [{"type": "llm_judge", "name": "Workbook and Table Structure Present", "description": "Output must be an Excel workbook with a clearly structured UAT test cases table supporting verification.", "weight": 5.0, "judge_prompt": "You are verifying ONLY SHAPE and STRUCTURE (not content correctness). Inspect the candidate output.\n\nExpectations (be flexible with near-synonyms for headers):\n- Format: Single Excel workbook (.xlsx). Not PDF, DOCX, CSV, or plaintext.\n- Primary sheet name: Prefer one of [\"UAT Plan\", \"Test Cases\", \"UAT\", \"Plan\"]. Accept similar variants.\n- Must contain a single tabular list of test cases with the following REQUIRED columns (header names can vary slightly but intent must match):\n  1) Role (e.g., Viewers, Project Managers, System Admins, Super Admins)\n  2) Module (e.g., Idea Management, Proposal Management, Project Management, Programs, System Administration, IRAD, Cross-functional Testing)\n  3) User Action\n  4) Test Scenario (or equivalent description of steps/conditions)\n  5) Expected Result\n  6) Actual Result (column present and intentionally blank for all rows)\n  7) Test Date (column present and intentionally blank for all rows)\n- Optional but desirable columns (do NOT penalize if missing): Test Case ID/Number, Priority/Severity, Preconditions/Test Data, Browser.\n- Volume: Approximately 80\u2013100 test cases (allow reasonable flexibility if clearly around this range and obviously intended for UAT execution).\n\nScoring (STRUCTURE ONLY):\n- 5.0: Excel workbook with a clear primary sheet; all 7 required columns present; Actual Result and Test Date columns exist and appear blank; ~80\u2013100 cases; roles and modules look populated as columns (not required to validate content).\n- 4.0: Excel workbook and clear table; 6/7 required columns present OR row count modestly outside range (e.g., 70\u2013120) OR one of Actual Result/Test Date columns missing but intent otherwise clear.\n- 2.5: Excel workbook and a table exists but missing 2+ required columns OR significantly outside volume (e.g., <50) OR layout not clearly a test case table.\n- 0.0: Not an Excel workbook OR no usable test cases table.\n\nDo NOT assess calculation or content correctness. Only verify presence, layout, and readiness for verification.", "expectation": "A single .xlsx with a primary sheet containing a UAT table with required columns and ~80\u2013100 rows, with Actual Result and Test Date blank."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 - Correctness and Coverage Verification", "description": "Verifies that the structured UAT plan is complete and aligned to the ProjMGR Tool MVP scope, with bounds checks and coverage validation.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Parsable Spreadsheet and Required Columns Detected", "description": "Confirms spreadsheet is readable and required columns (with flexible synonyms) are present.", "weight": 0.6, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output found.\"\n    try:\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n        preferred = [s for s in xls.sheet_names if str(s).strip().lower() in (\n            'uat plan','test cases','uat','plan')]\n        sheet = preferred[0] if preferred else xls.sheet_names[0]\n        df = pd.read_excel(file_path, sheet_name=sheet)\n    except Exception as e:\n        return 0.0, f\"Failed to read Excel: {e}\"\n\n    cols = [str(c).strip().lower() for c in df.columns]\n\n    def find_col(candidates):\n        for i, c in enumerate(cols):\n            for cand in candidates:\n                if cand in c:\n                    return i\n        return None\n\n    role_idx = find_col(['role'])\n    module_idx = find_col(['module','area','component'])\n    action_idx = find_col(['user action','action'])\n    scenario_idx = find_col(['test scenario','scenario','steps','description'])\n    expected_idx = find_col(['expected'])\n    actual_idx = find_col(['actual'])\n    date_idx = find_col(['test date','date'])\n\n    required = {\n        'role': role_idx,\n        'module': module_idx,\n        'action': action_idx,\n        'scenario': scenario_idx,\n        'expected': expected_idx,\n        'actual': actual_idx,\n    }\n\n    present = sum(1 for v in required.values() if v is not None)\n    base_score = present / 6.0  # 6 required columns\n\n    # small bonus if date column present\n    if date_idx is not None:\n        base_score = min(1.0, base_score + 0.1)\n\n    missing = [k for k, v in required.items() if v is None]\n    fb = f\"Found columns: {present}/6. Missing: {', '.join(missing) if missing else 'None'}.\"\n    return max(0.0, min(1.0, base_score)), fb"}, {"type": "code", "name": "Row Count and Blank Execution Fields", "description": "Checks target volume (approx 80\u2013100) and that Actual Result/Test Date cells are predominantly blank.", "weight": 0.6, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output found.\"\n    try:\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n        preferred = [s for s in xls.sheet_names if str(s).strip().lower() in (\n            'uat plan','test cases','uat','plan')]\n        sheet = preferred[0] if preferred else xls.sheet_names[0]\n        df = pd.read_excel(file_path, sheet_name=sheet)\n    except Exception as e:\n        return 0.0, f\"Failed to read Excel: {e}\"\n\n    # Identify key columns (flexible)\n    cols = [str(c).strip().lower() for c in df.columns]\n    def find_col(cands):\n        for i, c in enumerate(cols):\n            for cand in cands:\n                if cand in c:\n                    return i\n        return None\n\n    role_i = find_col(['role'])\n    module_i = find_col(['module','area','component'])\n    actual_i = find_col(['actual'])\n    date_i = find_col(['test date','date'])\n\n    # Determine candidate rows (non-empty role/module)\n    if role_i is not None:\n        role_series = df.iloc[:, role_i]\n    else:\n        role_series = pd.Series([np.nan]*len(df))\n    if module_i is not None:\n        module_series = df.iloc[:, module_i]\n    else:\n        module_series = pd.Series([np.nan]*len(df))\n\n    mask = (~role_series.astype(str).str.strip().isin(['', 'nan'])) | (~module_series.astype(str).str.strip().isin(['', 'nan']))\n    row_count = int(mask.sum()) if len(df) else 0\n\n    # Score for volume\n    if 80 <= row_count <= 110:\n        volume_score = 1.0\n    elif 70 <= row_count <= 120:\n        volume_score = 0.7\n    elif 50 <= row_count < 70:\n        volume_score = 0.4\n    else:\n        volume_score = 0.0\n\n    # Blankness checks\n    def blank_fraction(series):\n        if series is None:\n            return None\n        s = series.astype(str).str.strip().str.lower()\n        blank = (s == '') | (s == 'nan')\n        denom = max(1, len(series))\n        return float(blank.sum()) / float(denom)\n\n    actual_blank_frac = blank_fraction(df.iloc[:, actual_i]) if actual_i is not None else None\n    date_blank_frac = blank_fraction(df.iloc[:, date_i]) if date_i is not None else None\n\n    blank_score_parts = []\n    if actual_blank_frac is not None:\n        blank_score_parts.append(1.0 if actual_blank_frac >= 0.95 else (0.5 if actual_blank_frac >= 0.80 else 0.0))\n    if date_blank_frac is not None:\n        blank_score_parts.append(1.0 if date_blank_frac >= 0.90 else (0.5 if date_blank_frac >= 0.75 else 0.0))\n\n    blank_score = np.mean(blank_score_parts) if blank_score_parts else 0.5  # neutral if missing columns\n\n    score = max(0.0, min(1.0, 0.6*volume_score + 0.4*blank_score))\n    fb = f\"Rows counted: {row_count}. Actual blank frac: {actual_blank_frac}. Date blank frac: {date_blank_frac}.\"\n    return score, fb"}, {"type": "code", "name": "Role Coverage (Viewers, Project Managers, System Admins, Super Admins)", "description": "Validates that all required roles appear in the Role column (flexible matching).", "weight": 0.4, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output found.\"\n    try:\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n        preferred = [s for s in xls.sheet_names if str(s).strip().lower() in (\n            'uat plan','test cases','uat','plan')]\n        sheet = preferred[0] if preferred else xls.sheet_names[0]\n        df = pd.read_excel(file_path, sheet_name=sheet)\n    except Exception as e:\n        return 0.0, f\"Failed to read Excel: {e}\"\n\n    cols = [str(c).strip().lower() for c in df.columns]\n    role_idx = None\n    for i, c in enumerate(cols):\n        if 'role' in c:\n            role_idx = i\n            break\n    if role_idx is None:\n        return 0.0, \"Role column not found.\"\n\n    roles_series = df.iloc[:, role_idx].astype(str).str.lower()\n\n    targets = {\n        'viewers': ['viewer','read-only','readonly','view only','viewer(s)'],\n        'project managers': ['project manager','pm','project mgr','project managers'],\n        'system admins': ['system admin','sysadmin','org admin','organization admin','system administrator'],\n        'super admins': ['super admin','superadmin','global admin','tenant admin']\n    }\n\n    coverage = {}\n    for key, syns in targets.items():\n        pattern = '|'.join([re.escape(s) for s in syns])\n        coverage[key] = roles_series.str.contains(pattern, regex=True).any()\n\n    covered_count = sum(1 for v in coverage.values() if v)\n    score = covered_count / 4.0\n    missing = [k for k, v in coverage.items() if not v]\n    fb = f\"Role coverage: {covered_count}/4. Missing: {', '.join(missing) if missing else 'None'}.\"\n    return max(0.0, min(1.0, score)), fb"}, {"type": "code", "name": "Module Coverage (All MVP Areas)", "description": "Checks that required modules are represented across the plan (prefer Module column; fallback to Scenario/Expected).", "weight": 0.4, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output found.\"\n    try:\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n        preferred = [s for s in xls.sheet_names if str(s).strip().lower() in (\n            'uat plan','test cases','uat','plan')]\n        sheet = preferred[0] if preferred else xls.sheet_names[0]\n        df = pd.read_excel(file_path, sheet_name=sheet)\n    except Exception as e:\n        return 0.0, f\"Failed to read Excel: {e}\"\n\n    cols = [str(c).strip().lower() for c in df.columns]\n\n    def find_col(cands):\n        for i, c in enumerate(cols):\n            for cand in cands:\n                if cand in c:\n                    return i\n        return None\n\n    module_i = find_col(['module','area','component'])\n    scenario_i = find_col(['test scenario','scenario','steps','description'])\n    expected_i = find_col(['expected'])\n\n    text_sources = []\n    if module_i is not None:\n        text_sources.append(df.iloc[:, module_i].astype(str))\n    if scenario_i is not None:\n        text_sources.append(df.iloc[:, scenario_i].astype(str))\n    if expected_i is not None:\n        text_sources.append(df.iloc[:, expected_i].astype(str))\n\n    if not text_sources:\n        return 0.0, \"No suitable text columns to detect modules.\"\n\n    combined = ('\\n'.join(ts.fillna('').tolist())).lower()\n\n    targets = {\n        'idea management': ['idea management','idea form','promote to proposal','reject idea'],\n        'proposal management': ['proposal management','proposal','promote','hold','reject','approval chain'],\n        'project management': ['project management','milestones','tasks','resources','listing','search','standard','infrastructure'],\n        'programs': ['programs','program management','related projects'],\n        'system administration': ['system administration','system admin','configuration','administration tasks'],\n        'irad': ['irad','issues','risks','actions','decisions'],\n        'cross-functional testing': ['cross-functional','permissions','roles','browser','cross-browser','compatibility']\n    }\n\n    coverage = {}\n    for key, syns in targets.items():\n        pattern = '|'.join([re.escape(s) for s in syns])\n        coverage[key] = re.search(pattern, combined) is not None\n\n    covered = sum(1 for v in coverage.values() if v)\n    score = covered / len(targets)\n    missing = [k for k, v in coverage.items() if not v]\n    fb = f\"Module coverage: {covered}/{len(targets)}. Missing: {', '.join(missing) if missing else 'None'}.\"\n    return max(0.0, min(1.0, score)), fb"}, {"type": "llm_judge", "name": "Functional Coverage by Module", "description": "Verifies that test scenarios reasonably cover the MVP functions per module (Idea, Proposal, Project, Programs, System Admin, IRAD, Cross-functional).", "weight": 2.5, "judge_prompt": "Evaluate if the UAT plan includes realistic, traceable scenarios for each module aligned to the outline:\n1) Idea Management: Idea Form basics; actions: Reject, Promote to Proposal (including validation/mandatory fields).\n2) Proposal Management: actions Promote/Hold/Reject; Proposal Module Summary coverage including Basic Details, Proposal Details, Initial Project Team, Organization, Business Case (Project Description, Business Driver, Business Risk, Additional Comments/Notes), Documents, Dates and Phase Durations.\n3) Project Management: direct creation for Standard and Infrastructure types; add resources/team; define milestones/tasks; listing and search.\n4) Programs: organizing related projects under broader programs.\n5) System Administration: configuration/administration tasks.\n6) IRAD: Issues, Risks, Actions, Decisions life cycles.\n7) Cross-functional Testing: role-based permissions and cross-browser considerations.\n\nScore guidance:\n- 2.5: Clear, substantive coverage with multiple scenarios per module, reflecting the listed functions.\n- 1.7: Most modules reasonably covered; some sub-features thin or missing.\n- 0.8: Several modules underrepresented; coverage feels superficial.\n- 0.0: Little to no alignment with the specified modules/functions.\nFocus on reasonable coverage, not perfect completeness or exact wording.", "expectation": "Multiple scenarios per module reflecting the MVP capabilities and flows."}, {"type": "llm_judge", "name": "Permissions and Negative/Edge Scenarios", "description": "Ensures role-based permissions and negative/edge cases are well represented across roles.", "weight": 2.5, "judge_prompt": "Assess whether the plan includes:\n- Role-based permission checks for Viewers (view-only), Project Managers (own ideas/proposals/projects), System Admins (manage within their organization), Super Admins (full multi-org admin).\n- Negative/edge cases: permission denied, unauthorized attempts, missing mandatory inputs, invalid formats, boundary values, error handling.\n- Scenarios where requirements are both met and not met, with expected results stating accept/deny behavior and validation messages.\n\nScore guidance:\n- 2.5: Strong coverage across all roles with numerous permission restrictions and diverse negative/edge cases.\n- 1.7: Covers most roles and some negative cases but gaps remain (e.g., sparse for one role or few edge conditions).\n- 0.8: Minimal permission checks or negative cases.\n- 0.0: No meaningful permission or edge coverage.", "expectation": "Robust role permission tests and a healthy set of negative/edge scenarios across modules."}, {"type": "llm_judge", "name": "Traceability to Detailed Proposal Elements", "description": "Checks whether scenarios explicitly exercise the Proposal Module Summary sections and related fields.", "weight": 2.0, "judge_prompt": "Look for tests that explicitly trace to the Proposal Module Summary components:\n- Basic Details, Proposal Details, Initial Project Team Members, Organization.\n- Business Case with Project Description, Business Driver, Business Risk, Additional Comments/Notes.\n- Documents upload/management.\n- Dates and Phase Durations.\nAlso check that test cases refer to proposal actions Promote/Hold/Reject in conjunction with these fields.\n\nScoring:\n- 2.0: Most of the above components are directly addressed with clear expected results.\n- 1.2: About half are covered or coverage is implied but not explicit.\n- 0.6: Only a few components are mentioned superficially.\n- 0.0: No clear traceability to these components.", "expectation": "Direct references to Proposal Summary sections with explicit behavior/validation."}, {"type": "llm_judge", "name": "Cross-Browser and Compatibility Coverage", "description": "Verifies that common browsers are included and rendering/interaction issues are considered.", "weight": 1.0, "judge_prompt": "Check if the UAT plan includes cross-browser coverage with at least several of: Chrome, Firefox, Edge, Safari. Scenarios should consider login/session behavior, form controls, uploads, date pickers, and table interactions.\n\nScoring:\n- 1.0: 3\u20134 browsers explicitly covered with relevant UI/interaction checks.\n- 0.6: 2 browsers or generic cross-browser mention with limited scenarios.\n- 0.2: Only a token mention.\n- 0.0: No cross-browser coverage.", "expectation": "Named browsers with concrete UI interaction checks."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 - Professional Quality and Execution Readiness", "description": "Assesses clarity, professionalism, and readiness for execution and defect tracking.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Presentation and Template Hygiene", "description": "Professional formatting and cleanliness for circulation.", "weight": 1.5, "judge_prompt": "Evaluate formatting quality: consistent headers, freeze panes, filters enabled, readable column widths, no obviously broken formulas or stray data, consistent capitalization/grammar, and a clean tab name. The sheet should be ready to share without cleanup.", "expectation": "Neat, consistent, and share-ready Excel layout with filters and freeze panes appropriately set."}, {"type": "llm_judge", "name": "Clarity and Testability of Scenarios", "description": "Tests are atomic, unambiguous, and have verifiable expected results.", "weight": 1.5, "judge_prompt": "Assess clarity: Each test case should describe preconditions/data (if present), a concise user action, and a test scenario/steps that are specific. Expected Result should be observable and testable (e.g., visible message, state change). Avoid vague wording. Prefer one assertion per test or clearly separated assertions.", "expectation": "Clear steps and specific, verifiable expected results; minimal ambiguity."}, {"type": "llm_judge", "name": "Execution Readiness and Result Capture", "description": "The plan should facilitate execution, dating, and defect linkage.", "weight": 1.5, "judge_prompt": "Verify that Actual Result and Test Date are intentionally blank and obviously intended for testers to fill. If present, fields like Test Case ID, Priority/Severity, and a Notes/Defect/Link column should support execution and defect tracking. Check that expected results anticipate pass/fail criteria.", "expectation": "Obvious placeholders for execution with IDs/priority helpful; clear pass/fail expectations."}, {"type": "llm_judge", "name": "Coverage Balance and Risk Focus", "description": "Balanced coverage across modules and roles with emphasis on higher-risk areas.", "weight": 1.5, "judge_prompt": "Judge whether tests are sensibly distributed across modules and roles, with more depth where risk/impact is higher (e.g., permissions, proposal approvals, project creation flows). Redundant or trivial tests should be limited. Look for prioritization indicators or implied prioritization through depth.", "expectation": "Well-balanced set emphasizing higher-risk flows; limited redundancy."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "b3573f20-5d3e-4954-948f-9461fda693d2", "rubric": {"category_name": "Wholesale Trade \u2014 Sales Managers: Brand Data Gathering PDF", "rationale": "This rubric enforces a self-documenting, verifiable structure for a 3-page, text-based PDF questionnaire that sales managers can use to gather brand onboarding data. Stage 1 (LLM-only) strictly gates format and structural shape. Stage 2 mixes lightweight code checks (text extraction, keyword/topic coverage, question/space heuristics) with LLM assessments for correctness and completeness. Stage 3 evaluates overall professional quality and usability for the intended internal audience. Code rules are low-weight relative to LLM rules to reflect their narrower precision checks.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (LLM-only)", "description": "Verify the candidate output is a text-based PDF titled \u201cBrand Data Gathering\u201d, at least 3 pages, organized as a fillable, question-based document with the required sections. Structure only; do not judge content quality.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.1, "rules": [{"type": "llm_judge", "name": "Document Format and Structural Requirements", "description": "Check format, pagination, title, and presence of required sections with question-style prompts and visible answer space.", "weight": 3.0, "judge_prompt": "You are checking ONLY the structure/shape of the submitted output. Do not assess content quality or correctness.\n\nRequirements (all must be evaluated via the rendered PDF):\n\nFormat and Pagination\n- File must be a PDF (not DOCX/Word, not Excel, not images-only scans).\n- Must be primarily text-based (not blank, not a 1-page cover; text should be selectable/legible in the render).\n- Must be at least 3 pages.\n\nTitle\n- The first page must show the title \u201cBrand Data Gathering\u201d near the top. Minor variants like \u201cBrand Data Gathering Form\u201d or \u201cBrand Data Gathering Questionnaire\u201d are acceptable if clearly equivalent.\n\nStructure (Sections and Prompts)\n- The document must be organized into clearly labeled sections (headers or bold lines count; exact capitalization is flexible). Accept synonyms if obviously equivalent.\n- Target section set (allow close synonyms):\n  1) Company Overview\n  2) Product & Packaging\n  3) Pricing & Commercial Terms\n  4) Logistics & Fulfillment\n  5) Compliance & Certifications\n  6) Sales Channels & Policies\n  7) Marketing & Assets\n  8) Operations & Systems\n  9) Onboarding & Contacts\n- Each section should contain multiple question-style prompts (e.g., lines ending with \u201c?\u201d or prompts followed by a colon) that a brand team could answer.\n- There should be visible space to write answers (e.g., blank lines, dotted/underscored lines, or clear whitespace under/after each prompt). Embedded form fields are NOT required.\n- No branding/design elements are required; a simple, readable layout is fine.\n\nScoring (0 to 3):\n- 3.0: PDF format, 3+ pages, correct/clearly equivalent title on page 1, and all 9 sections present with question-style prompts and visible answer space.\n- 2.5: PDF, 3+ pages, correct title, at least 8 of 9 sections present, prompts are question-style with visible answer space.\n- 2.0: PDF, 3+ pages, correct title, at least 6 of 9 sections present, mostly question-style prompts with visible answer space.\n- 1.0: PDF but structural issues (e.g., <3 pages OR only 3\u20135 sections OR limited question-style prompts/answer space).\n- 0.0: Not a PDF, OR <2 pages, OR no clear title, OR not a question-based, fillable structure.\n\nOnly evaluate presence/format/structure, not the substantive quality of the content.", "expectation": "A 3+ page, text-based PDF titled \u201cBrand Data Gathering,\u201d organized into 9 clearly labeled sections with question-style prompts and visible space for answers."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Completeness Verification", "description": "With the shape enforced, verify coverage of critical onboarding/readiness topics, fillability, and internal-use suitability. Combine precise code checks with higher-level LLM judgment.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Topic Coverage via Text Extraction (Keywords & Synonyms)", "description": "Checks presence of key onboarding/readiness topics using flexible keyword groups in the PDF text.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float in [0,1] or (float, str)\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n\n    text = \"\"\n    try:\n        if output.extension.lower() == '.pdf':\n            text = context.files.read_pdf_text(output.id)\n        elif output.extension.lower() in ('.docx', '.doc'):\n            text = context.files.read_docx_text(output.id)\n        else:\n            # Try generic text read for fallback\n            text = context.files.read_text(output.id)\n    except Exception:\n        return 0.0, \"Failed to read document text\"\n\n    if not text or len(text.strip()) < 200:\n        return 0.0, \"Document text too short or unreadable\"\n\n    s = text.lower()\n\n    # Topic groups: any match within a group counts that topic as covered\n    topic_groups = [\n        [\"legal name\", \"tax id\", \"ein\"],\n        [\"upc\", \"gtin\", \"sku\"],\n        [\"case pack\", \"inner pack\"],\n        [\"dimensions\", \"length x width\", \"cube\"],\n        [\"weight\", \"net weight\", \"gross weight\"],\n        [\"msrp\", \"map\", \"retail price\"],\n        [\"moq\", \"minimum order\"],\n        [\"lead time\", \"production lead\"],\n        [\"warehouse\", \"3pl\", \"fulfillment\"],\n        [\"edi\", \"api\", \"order management\"],\n        [\"returns\", \"rma\", \"return policy\"],\n        [\"chargeback\", \"compliance requirements\"],\n        [\"shelf life\", \"expiry\", \"lot code\"],\n        [\"temperature\", \"cold chain\"],\n        [\"incoterms\", \"fob\", \"exw\", \"ddp\", \"dda\"],\n        [\"payment terms\", \"net 30\", \"net-30\"],\n        [\"pallet\", \"ti/hi\", \"tihi\", \"freight class\"],\n        [\"hs code\", \"hts\", \"tariff\"],\n        [\"certification\", \"organic\", \"non-gmo\", \"msds\", \"sds\", \"prop 65\"],\n        [\"primary contact\", \"email\", \"phone\"]\n    ]\n\n    covered = 0\n    for group in topic_groups:\n        if any(term in s for term in group):\n            covered += 1\n\n    total = len(topic_groups)\n    coverage_ratio = covered / total if total else 0.0\n\n    # Full score at ~60%+ coverage, linear scale below that\n    score = min(1.0, coverage_ratio / 0.6)\n    feedback = f\"Covered {covered}/{total} topic groups (ratio={coverage_ratio:.2f}).\"\n    return score, feedback"}, {"type": "code", "name": "Question-Based Prompts and Answer Space Heuristic", "description": "Heuristically checks that the document uses question-style prompts and provides visible space to answer (underscores/dots/checkboxes).", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Returns a normalized score [0,1] indicating presence of question-like prompts and answer space indicators.\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n\n    try:\n        if output.extension.lower() == '.pdf':\n            text = context.files.read_pdf_text(output.id)\n        elif output.extension.lower() in ('.docx', '.doc'):\n            text = context.files.read_docx_text(output.id)\n        else:\n            text = context.files.read_text(output.id)\n    except Exception:\n        return 0.0, \"Failed to read document text\"\n\n    if not text or len(text.strip()) < 200:\n        return 0.0, \"Document text too short or unreadable\"\n\n    # Count question-like prompts: '?' or lines ending with ':'\n    q_marks = text.count('?')\n    colon_lines = 0\n    for line in text.splitlines():\n        if re.search(r\":\\s*$\", line):\n            colon_lines += 1\n\n    q_count = q_marks + colon_lines\n\n    # Answer space indicators: long underscores, dotted lines, or checkboxes\n    underscores = len(re.findall(r\"_{5,}\", text))\n    dotted = len(re.findall(r\"\\.{5,}\", text))\n    checkboxes = len(re.findall(r\"\\[\\s?\\]\", text))\n\n    # Thresholds tuned for a 3-page questionnaire\n    q_ok = q_count >= 20\n    space_ok = (underscores + dotted + checkboxes) >= 5\n\n    # Partial credit: half for prompts, half for space\n    score = 0.0\n    if q_ok:\n        score += 0.5\n    else:\n        # Soft scale if not enough prompts\n        score += min(0.5, q_count / 40.0)\n\n    if space_ok:\n        score += 0.5\n    else:\n        # Soft scale if limited answer indicators\n        score += min(0.5, (underscores + dotted + checkboxes) / 10.0)\n\n    feedback = f\"Prompts detected: {q_count} (q={q_marks}, colons={colon_lines}); answer markers: underscores={underscores}, dotted={dotted}, checkboxes={checkboxes}.\"\n    return max(0.0, min(1.0, score)), feedback"}, {"type": "llm_judge", "name": "Operational & Logistics Coverage", "description": "Judge whether the prompts adequately cover operational capacity, product logistics, and compliance essentials needed for distribution readiness.", "weight": 1.1, "judge_prompt": "Assess whether the questionnaire\u2019s prompts sufficiently cover the following readiness domains. Do not penalize for wording differences; evaluate substance.\n\nKey domains to look for (examples are illustrative, not exhaustive):\n- Company & Contacts: legal entity, primary contacts, support/escalation, time zones.\n- Product Identification: SKU lists, UPC/GTIN, variants, case/inner pack, master carton.\n- Packaging & Specs: unit and case dimensions/weights, pallet/TI-HI, freight class, temperature control if applicable.\n- Logistics & Fulfillment: warehouses/3PL, shipping origins, lead times, MOQs, production capacity, backorder policy.\n- Commercial Terms: MSRP/MAP, wholesale pricing, discounts/brackets, payment terms, incoterms/FOB.\n- Compliance & Documentation: certifications (e.g., organic, non-GMO), MSDS/SDS if applicable, Prop 65, HS/HTS codes, lot/expiry/shelf life, labeling.\n- Systems & Data: EDI/API capability, ASN, order confirmations, tracking, chargeback/compliance acknowledgments.\n- Sales Channels & Policies: channel restrictions, territories, marketplaces, returns/RMA, warranty/damages.\n- Marketing & Assets: brand guidelines, product images, copy, samples policy.\n\nScoring (0 to 1.1):\n- 1.1: Clearly covers most items in each domain with specific, answerable prompts.\n- 0.8: Covers most domains with moderate specificity; a few notable gaps.\n- 0.5: Partial coverage; several domains thin or missing.\n- 0.2: Minimal coverage; many domains absent.\n- 0.0: Largely off-topic or missing these domains.", "expectation": "Prompts span all key operational/logistics/compliance domains with specific, answerable questions."}, {"type": "llm_judge", "name": "Clarity and Fillability of Prompts", "description": "Judge whether prompts are clear, unambiguous, and easy to answer, with visible space for responses.", "weight": 1.1, "judge_prompt": "Evaluate the clarity and ease-of-completion of the questionnaire.\nConsider:\n- Are prompts concise and unambiguous (who/what/when/how much/where)?\n- Are data requests specific (e.g., units, formats, examples)?\n- Is there visible space to respond (lines/whitespace/dotted areas), even without embedded form fields?\n- Is the organization within sections logical so respondents can proceed smoothly?\n\nScoring (0 to 1.1):\n- 1.1: Very clear, specific prompts; obvious room to respond; easy to complete.\n- 0.8: Generally clear; minor ambiguity or occasional tight spacing.\n- 0.5: Mixed clarity; several vague prompts or weak spacing.\n- 0.2: Confusing, inconsistent, or cramped.\n- 0.0: Not fillable as a practical questionnaire.", "expectation": "A clear, question-led layout with obvious places to write answers."}, {"type": "llm_judge", "name": "Internal-Use Readiness Alignment", "description": "Judge whether the document is geared for internal onboarding (not marketing), enabling sales ops and logistics prep.", "weight": 1.0, "judge_prompt": "Assess if the document is appropriate for internal readiness and onboarding, not marketing.\nLook for:\n- Focus on actionable data for internal teams (e.g., SKU lists, specs, lead times, terms, systems integrations).\n- Avoidance of brand fluff/marketing copy; no need for external-facing design.\n- Inclusion of items that drive internal workflows (EDI readiness, ASN/tracking, chargebacks, returns, channel restrictions, warehouse addresses).\n- Practical instructions/context where needed (e.g., units of measure, examples, acceptable formats).\n\nScoring (0 to 1.0):\n- 1.0: Strong internal focus with practical, actionable data requests.\n- 0.7: Mostly internal-focus; a few extraneous or missing operational items.\n- 0.4: Mixed purpose with noticeable gaps.\n- 0.1: Largely off-purpose.\n- 0.0: Not aligned to internal onboarding needs.", "expectation": "Actionable, operations-focused prompts that support distribution readiness."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism", "description": "Holistic assessment of tone, organization, completeness, and usability for brand-side respondents and internal teams.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Tone and Readability", "description": "Assesses professional, neutral tone and readability suitable for B2B onboarding.", "weight": 0.8, "judge_prompt": "Evaluate the tone and readability:\n- Professional, neutral, and respectful tone appropriate for B2B.\n- Avoids jargon unless necessary; defines terms when helpful.\n- Consistent naming/terminology.\nScoring (0 to 0.8): 0.8 excellent; 0.6 good; 0.4 adequate; 0.2 weak; 0.0 poor.", "expectation": "Professional, neutral, consistent language."}, {"type": "llm_judge", "name": "Logical Organization and Flow", "description": "Checks sequencing of sections and internal flow to minimize backtracking.", "weight": 0.8, "judge_prompt": "Assess organization and flow:\n- Sections progress logically (company \u2192 product \u2192 logistics \u2192 terms \u2192 systems \u2192 channels \u2192 marketing \u2192 onboarding).\n- Related prompts grouped together; minimal redundancy.\n- Headings or separators used consistently.\nScoring (0 to 0.8): 0.8 excellent; 0.6 good; 0.4 fair; 0.2 poor; 0.0 disorganized.", "expectation": "A coherent progression that\u2019s easy to follow."}, {"type": "llm_judge", "name": "Completeness and Specificity of Requests", "description": "Evaluates whether prompts elicit the depth needed to operationalize onboarding without follow-up.", "weight": 0.7, "judge_prompt": "Judge completeness and specificity:\n- Prompts request necessary detail (units, formats, examples, ranges) to avoid repeated follow-ups.\n- Covers edge cases where relevant (temperature control, hazardous materials, international shipments/tariffs).\n- Includes placeholders for attachments (e.g., SKU lists, certification PDFs) if appropriate.\nScoring (0 to 0.7): 0.7 excellent; 0.5 good; 0.3 partial; 0.1 minimal; 0.0 poor.", "expectation": "Detailed prompts that reduce back-and-forth."}, {"type": "llm_judge", "name": "Usability for Respondents", "description": "Assesses scannability, spacing, and guidance that makes completion straightforward.", "weight": 0.7, "judge_prompt": "Evaluate usability:\n- Clear section starts; scannable lists; consistent indentation/bullets if used.\n- Adequate space and pacing (not overly dense).\n- Brief cues/instructions where needed (e.g., units, date formats).\nScoring (0 to 0.7): 0.7 excellent; 0.5 good; 0.3 fair; 0.1 minimal; 0.0 poor.", "expectation": "Easy to scan and complete with adequate space and cues."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "5d0feb24-e8b6-4ace-b64f-d5cd1a8b563d", "rubric": {"category_name": "Science Editing - TRAPPIST-1 Article Senior Editor Review", "rationale": "This rubric enforces a self-documenting, verifiable editorial deliverable for a newsroom science edit. Stage 1 (LLM-only) mandates an editable DOCX with tracked changes, inline comments, and specific structural sections to make verification trivial. Stage 2 mixes light code checks (URLs, change-log presence, word-count) with LLM fact-verification and sourcing checks focused on TRAPPIST-1 and the cited arXiv paper. Stage 3 uses LLM to assess editorial quality, readability for a curious non-expert audience, and actionability of feedback. Code rules are kept small in weight relative to LLM judges, consistent with the philosophy that nuanced editorial assessment is best handled by LLM judges.", "max_total_score": 25.0, "stages": [{"name": "Stage 1 \u2014 Format & Structure Gate (LLM-only)", "description": "Gate: The output must be an editable Word document with tracked changes, margin comments, and the required sections that enable verification.", "is_required": true, "max_points": 5.0, "min_score_to_pass": 3.5, "rules": [{"type": "llm_judge", "name": "Editable DOCX with Tracked Changes and Comments", "description": "Verify the candidate output is a DOCX with visible redlined tracked changes and margin comments for feedback.", "weight": 3.0, "judge_prompt": "You are checking the SHAPE ONLY of the candidate output. Do not judge content quality or correctness here.\n\nRequirements:\n- File must be an editable Word document (DOCX preferred). Not PDF.\n- Tracked Changes redlines must be visible: insertions and/or deletions shown in the body text.\n- Margin comments must be present and attached to relevant text spans.\n- The document should clearly be an edit of a TRAPPIST-1 news story (title and/or opening lines reflect the topic).\n\nScoring (0 to 3):\n- 3.0: DOCX with both visible tracked insertions/deletions AND margin comments present; content clearly about TRAPPIST-1.\n- 2.0: DOCX with either tracked changes OR margin comments (but not both), or both are present but extremely sparse; topic appears to be TRAPPIST-1.\n- 1.0: Editable document (DOC/DOCX) but no visible tracked changes or comments; OR a PDF that nevertheless shows comment-like annotations.\n- 0.0: Not a Word document, or content unrelated to TRAPPIST-1.\n\nOnly evaluate structure/format presence, not correctness of edits.", "expectation": "A DOCX showing redlined tracked changes and margin comments on a TRAPPIST-1 article."}, {"type": "llm_judge", "name": "Required Editorial Structure Present", "description": "Check presence of the structural elements that enable verification and traceability.", "weight": 2.0, "judge_prompt": "Check the candidate DOCX for the following structural elements (names can be flexible but intent must be clear):\n\nRequired structural elements:\n1) Document title or headline at top.\n2) An Overall Editor Note near the top (can be a first-page comment or a short paragraph) summarizing priorities (e.g., accuracy focus, novelty angle, word-count guidance).\n3) Inline tracked edits (insertions/deletions) throughout the body text.\n4) Margin comments containing at least some source URLs for science-related edits.\n5) A Change Log section toward the end that summarizes major edits in a line-based or table-like format (pipe-delimited lines acceptable), pairing each change with a reason and (where applicable) a source link.\n6) A References section listing URLs cited in comments and/or body (e.g., arXiv paper and NASA TRAPPIST-1 page).\n\nScoring (0 to 2):\n- 2.0: All six elements clearly present.\n- 1.5: Five elements present (missing any one non-core element except inline tracked edits or comments; those are core).\n- 1.0: Four elements present.\n- 0.5: Two\u2013three elements present.\n- 0.0: One or zero elements present.\n\nDo not assess correctness; only verify presence of these sections/components.", "expectation": "A DOCX with title, editor note, tracked edits, comments with links, a change log, and a references section."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Content Accuracy, Sourcing, and Traceability)", "description": "Now that structure is valid, verify correctness and traceability. Mix light code checks with LLM judgment. Code rules carry small weight relative to LLM rules.", "is_required": true, "max_points": 12.0, "min_score_to_pass": 6.0, "rules": [{"type": "code", "name": "Citations Coverage: arXiv 2401.11815 and NASA TRAPPIST-1", "description": "Detect presence of the core sources as URLs in the DOCX text: the arXiv paper (2401.11815) and NASA TRAPPIST-1 page.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        text = \"\"\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                return 0.0\n        text_low = text.lower()\n        score = 0.0\n        if \"arxiv.org\" in text_low and (\"2401.11815\" in text_low or \"/abs/2401.11815\" in text_low):\n            score += 0.5\n        if \"science.nasa.gov\" in text_low and \"trappist1\" in text_low.replace(\"-\",\"\" ).replace(\"/\",\"\"):\n            score += 0.5\n        return min(1.0, score)\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Change Log Present with Well-Formed Entries", "description": "Check for a 'Change Log' section with at least 3 pipe-delimited entries (e.g., Section | Issue | Action | Source URL), with URLs present.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                return 0.0\n        text_low = text.lower()\n        # Find Change Log block heuristically\n        if \"change log\" not in text_low and \"edit log\" not in text_low:\n            return 0.0\n        lines = [ln.strip() for ln in text.splitlines()]\n        entries = []\n        url_re = re.compile(r\"https?://[\\w\\-\\.\\?/&#%=:+,~]+\", re.I)\n        for ln in lines:\n            if '|' in ln and ln.count('|') >= 3:\n                if url_re.search(ln):\n                    entries.append(ln)\n        if len(entries) >= 3:\n            return 1.0\n        elif len(entries) >= 1:\n            return 0.5\n        else:\n            return 0.0\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "References: URL Count and Domain Diversity", "description": "Ensure multiple credible URLs are included and not all from the same site.", "weight": 0.6, "code": "import re\nfrom urllib.parse import urlparse\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                return 0.0\n        url_re = re.compile(r\"https?://[\\w\\-\\.\\?/&#%=:+,~]+\", re.I)\n        urls = url_re.findall(text)\n        urls = list(dict.fromkeys(urls))  # unique preserve order\n        domains = set()\n        for u in urls:\n            try:\n                d = urlparse(u).netloc.lower()\n                if d:\n                    domains.add(d)\n            except Exception:\n                pass\n        n = len(urls)\n        nd = len(domains)\n        if n >= 6 and nd >= 3:\n            return 1.0\n        elif n >= 4 and nd >= 2:\n            return 0.7\n        elif n >= 2:\n            return 0.4\n        else:\n            return 0.0\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Substantial Editorial Content Length", "description": "Heuristic check that the edited document contains substantial content (word count).", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                return 0.0\n        words = re.findall(r\"\\b\\w+\\b\", text)\n        wc = len(words)\n        if wc >= 500:\n            return 1.0\n        elif wc >= 300:\n            return 0.6\n        elif wc >= 150:\n            return 0.3\n        else:\n            return 0.0\n    except Exception:\n        return 0.0"}, {"type": "llm_judge", "name": "Scientific Accuracy and Non-Overstatement", "description": "Check the edited story and comments for factual accuracy regarding TRAPPIST-1 and avoidance of hype, with links supporting science edits.", "weight": 4.0, "judge_prompt": "Judge the edited DOCX for scientific accuracy and sober framing. Focus on:\n- Core facts about TRAPPIST-1: seven Earth-sized rocky planets, ~40 light-years, ultracool M-dwarf star; which planets are in/near the habitable zone; tidal locking and stellar activity caveats.\n- Ensure no overclaiming about detected atmospheres or life; recent JWST results often set upper limits rather than detections for TRAPPIST-1 b/c/d/e.\n- Alignment with the referenced arXiv:2401.11815 paper: edits/comments should correctly reflect what the study is about and why its methodology is novel or important.\n- For science-related edits, ensure comments include credible source links (e.g., arXiv, NASA pages) supporting corrections/clarifications.\n\nScoring (0 to 4):\n- 4.0: All key facts correct; no hype; paper\u2019s novelty/process explained accurately; science edits are accompanied by appropriate supporting links.\n- 3.0: Mostly accurate; minor imprecision; novelty/process addressed but could be clearer; most science edits supported with links.\n- 2.0: Several inaccuracies or unclear novelty; some edits lack support.\n- 1.0: Frequent inaccuracies/overstatements; weak or missing support.\n- 0.0: Materially incorrect or misleading science; no usable sourcing.\n\nCite specific issues you observed in brief feedback.", "expectation": "Accurate, cautious science framing with linked sources for corrections."}, {"type": "llm_judge", "name": "Novelty and Research Process Emphasis", "description": "Assess whether the edit highlights what is genuinely novel about the study and its forward-looking potential.", "weight": 3.0, "judge_prompt": "Evaluate whether the edit focuses the story on the assignment brief: highlight how THIS research is novel and what it enables next. Look for:\n- Clear articulation of how studying an ultracool, non-Sun-like star (e.g., TRAPPIST-1) changes or expands the search for habitable worlds.\n- Specifics of the paper\u2019s methodology or dataset (e.g., instruments, observing modes, analysis approach) and why it\u2019s new or first-of-its-kind.\n- Forward-looking implications (what it enables for future JWST/HST/ground-based follow-ups or comparative exoplanet climatology).\n\nScoring (0 to 3):\n- 3.0: Novelty and process are explicit, accurate, and central to the narrative; implications are concrete and realistic.\n- 2.0: Novelty/process mentioned but somewhat generic or shallow; implications are present but could be sharper.\n- 1.0: Minimal focus on novelty/process; implications vague.\n- 0.0: No real novelty/process framing.\n\nBe concise but specific in feedback.", "expectation": "A clear, accurate emphasis on what\u2019s new in this study and why it matters."}, {"type": "llm_judge", "name": "Source Relevance and Cross-Reference Coverage", "description": "Check that cited sources are credible, relevant to claims, and include both the arXiv paper and NASA TRAPPIST-1 materials.", "weight": 3.0, "judge_prompt": "Inspect the comments and references:\n- Do they include the arXiv paper (2401.11815) and NASA TRAPPIST-1 pages among others?\n- Are the links placed adjacent to the relevant edits/claims?\n- Are sources credible (peer-reviewed/presS releases/agency pages) and do they support the specific statements being corrected/clarified?\n\nScoring (0 to 3):\n- 3.0: Contains arXiv 2401.11815 and NASA TRAPPIST-1 links; other links are credible and correctly matched to claims.\n- 2.0: Mostly credible and relevant; one of the key links missing or some mismatches.\n- 1.0: Sparse or poorly matched sources; key links missing.\n- 0.0: No usable sources.\n\nProvide a brief rationale with examples.", "expectation": "Credible, relevant links placed where claims are made, covering arXiv and NASA pages."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Editorial Excellence", "description": "Holistic quality assessment of the editorial work, presentation, and audience appropriateness.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity and Accessibility for a Curious Non-Expert", "description": "Assess whether complex concepts (M-dwarfs, habitable zones, tidal locking, atmospheric detection limits) are explained clearly and without misrepresentation.", "weight": 2.0, "judge_prompt": "Judge the edited piece for clarity and accessibility aimed at scientifically-curious non-experts:\n- Jargon is minimized or explained (e.g., M-dwarf, transit spectroscopy, phase curves, habitable zone, upper limits).\n- Explanations are accurate, not oversimplified to the point of being wrong.\n- Logical flow: lede, context, method, findings/limits, implications.\n\nScoring (0 to 2):\n- 2.0: Highly clear and accurate explanations; strong flow; no confusing jargon.\n- 1.0: Generally clear with minor jargon/flow issues.\n- 0.0: Confusing, jargon-heavy, or misleading simplifications.\n\nGive quick, specific notes.", "expectation": "Clear, accurate explanations tailored to an informed lay audience."}, {"type": "llm_judge", "name": "Actionability and Specificity of Editorial Feedback", "description": "Evaluate whether comments provide actionable guidance, concrete prompts, and constructive direction to the reporter.", "weight": 2.5, "judge_prompt": "Focus on the margin comments and any editor note:\n- Are suggestions specific (what to verify, how to re-report, what detail to add or trim)?\n- Do they include concrete prompts or questions the reporter can act on?\n- Are positive remarks included where the reporter did well?\n\nScoring (0 to 2.5):\n- 2.5: Highly actionable, specific, balanced (includes positives), with clear next steps.\n- 1.5: Mostly actionable; some comments generic.\n- 0.5: Vague or largely non-actionable.\n- 0.0: Little to no useful guidance.\n\nOffer brief examples.", "expectation": "Targeted, constructive guidance that the reporter can immediately use."}, {"type": "llm_judge", "name": "Organization and Use of Tracked Changes/Comments", "description": "Assess professionalism of markup: coherent change sets, comments anchored to relevant passages, minimal clutter.", "weight": 1.5, "judge_prompt": "Evaluate the mechanics of editing:\n- Tracked changes are grouped logically; not noisy or contradictory.\n- Comments are attached to the correct spans and reference specific text.\n- The Change Log and References are neatly organized.\n\nScoring (0 to 1.5):\n- 1.5: Professional markup; clean, coherent, and easy to follow.\n- 1.0: Generally good; minor clutter or mis-anchored comments.\n- 0.5: Messy or hard to follow.\n- 0.0: Disorganized markup.\n\nProvide concise feedback.", "expectation": "Clean, professional markup that makes the editor\u2019s intent obvious."}, {"type": "llm_judge", "name": "Tone, Balance, and Editorial Polish", "description": "Evaluate tone (supportive, professional), balance between critique and praise, and overall publishability readiness.", "weight": 2.0, "judge_prompt": "Assess tone and polish:\n- Professional, respectful tone toward the reporter.\n- Balanced critique with acknowledgment of strengths.\n- The edited copy reads close to publishable after revisions suggested (no major unresolved issues left unflagged).\n\nScoring (0 to 2):\n- 2.0: Exemplary tone and polish; balanced and thorough.\n- 1.0: Adequate tone; some rough edges or imbalances.\n- 0.0: Unprofessional or unhelpful tone; low polish.\n\nAdd brief remarks.", "expectation": "Supportive editorial voice with high polish and balanced critique."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "11e1b169-5fb6-4d79-8a83-82ddf4987a85", "rubric": {"category_name": "KY Police Roll Call Quick Reference Guide (2-page PDF)", "rationale": "This rubric enforces a self-documenting workflow: Stage 1 is a strict LLM-only shape gate requiring a two-page PDF with clearly labeled sections and field-oriented sub-bullets. Stage 2 mixes small, robust code checks (keyword/topic coverage and Kentucky anchoring) with heavier LLM correctness reviews for legal substance. Stage 3 assesses professional quality, usability for mid-watch officers, and presentation polish. The structure forces the creator to prove completeness and correctness through a verifiable document layout, enabling deterministic checks and nuanced LLM judging.", "max_total_score": 10.0, "stages": [{"name": "Stage 1: Structure and Format Gate (LLM Only)", "description": "Verify the output is a 2-page PDF quick reference with the exact structural requirements for fast field use by Kentucky patrol officers.", "is_required": true, "max_points": 2.0, "min_score_to_pass": 1.4, "rules": [{"type": "llm_judge", "name": "PDF Shape and Section Presence", "description": "Checks that the output is a properly formatted, 2-page PDF with all required topic sections, labeled sub-bullets, and basic header/footer elements.", "weight": 2.0, "judge_prompt": "You are evaluating the SHAPE ONLY of the candidate output. Do not judge correctness or writing quality. Use what you can see from the rendered file.\n\nRequired format and structure:\n- File must be a PDF (not Word, not text).\n- Must be exactly 2 pages.\n- Professional quick-reference layout suitable for patrol roll call.\n- Title/Header on page 1: includes a clear title (e.g., \"Mid-Watch Roll Call Quick Reference\" or similar), unit/agency name, and date.\n- The body must include EIGHT clearly labeled sections (headings or bold labels):\n  1) Fourth Amendment (Search and Seizure)\n  2) Reasonable Suspicion\n  3) Probable Cause\n  4) Exigent Circumstances\n  5) Terry Stops\n  6) Pat Downs (a.k.a. frisk)\n  7) Protective Sweeps\n  8) KRS 503.090: Use of Physical Force in Law Enforcement\n- For EACH of the eight sections, include at least THREE labeled bullets or sub-points aimed at field use:\n  \u2022 Definition (or What it means)\n  \u2022 Legal Standard/Test (threshold/when it applies)\n  \u2022 Field Cues/Examples (practical indicators/do-don'ts)\n- A short references or sources area at the end (URLs or citations) and a brief internal disclaimer (e.g., training use only, consult policy) are required somewhere in the 2 pages.\n\nFlexible matching guidance:\n- Section names can be close variants (e.g., \"Fourth Amendment\" with subtitle \"Search and Seizure\"; \"Pat-Down/Frisk\").\n- The three sub-bullets can be labeled with close synonyms (e.g., \"Definition\" vs \"Overview\"; \"Legal Standard\" vs \"Threshold\"; \"Field Cues\" vs \"Examples/Checklist\").\n\nScoring (shape only):\n- 2.0: PDF, exactly 2 pages, all 8 sections present with clearly labeled headings; each section shows all three required sub-bullets; includes header (title/unit/date), references, and disclaimer.\n- 1.6: PDF and exactly 2 pages; minor omissions (e.g., 1 section missing any 1 of the 3 sub-bullets OR header missing one element OR references/disclaimer combined but present). All 8 section headings still present.\n- 1.0: PDF and exactly 2 pages, but structure is incomplete (e.g., 1\u20132 sections missing OR multiple sections missing sub-bullets OR header and references/disclaimer missing). Still resembles the required format.\n- 0.0: Not a PDF, or not exactly 2 pages, or lacks multiple required sections making verification impractical.\n\nOnly assess presence/format. Do NOT assess legal accuracy or writing quality.", "expectation": "A two-page PDF with the 8 required sections, each with Definition, Legal Standard/Test, and Field Cues/Examples, plus header, references, and disclaimer."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Legal Correctness and Alignment (Mixed)", "description": "Verify the substantive correctness of legal concepts, with small deterministic code checks and higher-weight LLM legal reviews.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Required Topic Coverage (Text Presence)", "description": "Checks the PDF text contains all eight required topics using flexible keyword matching; scores proportionally by coverage.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float in [0,1] or (float, str)\n    \"\"\"\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document output or wrong type.\"\n        text = \"\"\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                return 0.0, \"Could not read PDF/DOCX text.\"\n        t = text.lower()\n        patterns = {\n            \"fourth_amendment\": [r\"fourth amendment\", r\"4th amendment\"],\n            \"reasonable_suspicion\": [r\"reasonable\\s+suspicion\"],\n            \"probable_cause\": [r\"probable\\s+cause\"],\n            \"exigent\": [r\"exigent\\s+circumstance\" , r\"exigency\"],\n            \"terry\": [r\"terry\\s+stop\", r\"terry v\\. ohio\", r\"terry\\b\"],\n            \"pat_down\": [r\"pat[- ]?down\", r\"frisk\\b\"],\n            \"protective_sweep\": [r\"protective\\s+sweep\"],\n            \"krs_503_090\": [r\"krs\\s*503\\.090\", r\"use of physical force in law enforcement\"]\n        }\n        found = 0\n        missing = []\n        for key, pats in patterns.items():\n            if any(re.search(p, t) for p in pats):\n                found += 1\n            else:\n                missing.append(key)\n        score = found / len(patterns)\n        feedback = f\"Found {found}/8 required topics. Missing: {', '.join(missing) if missing else 'None'}.\"\n        return score, feedback\n    except Exception as e:\n        return 0.0, f\"Error in evaluation: {e}\""}, {"type": "code", "name": "Kentucky Anchoring Signals", "description": "Checks for Kentucky-specific anchoring (state name, KRS, statute number) to ensure local applicability.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float in [0,1] or (float, str)\n    \"\"\"\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document output or wrong type.\"\n        text = \"\"\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                return 0.0, \"Could not read PDF/DOCX text.\"\n        t = text.lower()\n        signals = {\n            \"kentucky\": r\"\\bkentucky\\b|\\bky\\b\",\n            \"krs\": r\"\\bkrs\\b|kentucky\\s+revised\\s+statutes|ky\\.\\s*rev\\.\\s*stat\",\n            \"krs_503_090\": r\"503\\.090\",\n            \"use_of_physical_force_label\": r\"use of physical force\"\n        }\n        hits = {}\n        for name, pat in signals.items():\n            hits[name] = 1 if re.search(pat, t) else 0\n        found = sum(hits.values())\n        total = len(signals)\n        score = found / total\n        missing = [k for k, v in hits.items() if v == 0]\n        feedback = f\"KY anchors found {found}/{total}. Missing: {', '.join(missing) if missing else 'None'}.\"\n        return score, feedback\n    except Exception as e:\n        return 0.0, f\"Error in evaluation: {e}\""}, {"type": "llm_judge", "name": "Core Fourth Amendment Concepts Correctness", "description": "Assesses accuracy and alignment of definitions and thresholds for Reasonable Suspicion, Probable Cause, Exigent Circumstances, Terry Stops, Pat Downs, and Protective Sweeps.", "weight": 2.1, "judge_prompt": "Evaluate the legal correctness (not just presence) of the quick reference guide for the following topics: Reasonable Suspicion, Probable Cause, Exigent Circumstances, Terry Stops, Pat Downs/Frisk, Protective Sweeps, and the general Fourth Amendment (search/seizure) framing. Use standard, well-established doctrine.\n\nLook for:\n- Reasonable Suspicion: articulable facts, totality of circumstances, less than probable cause, allows a brief investigative detention.\n- Probable Cause: fair probability standard, totality, based on facts and reasonable inferences, needed for arrest/search warrants (with exceptions).\n- Exigent Circumstances: immediate needs (e.g., hot pursuit, imminent destruction of evidence, emergency aid). Limits: no pretext; must be objectively reasonable.\n- Terry Stops: brief, investigative detention based on reasonable suspicion; scope and duration limits; no automatic frisk.\n- Pat Down/Frisk: limited to weapons-only outer clothing pat for officer safety; requires reasonable suspicion the person is armed and dangerous; \u201cplain feel\u201d limitations.\n- Protective Sweeps: incident to arrest, quick and limited sweep for people posing danger; confined to spaces immediately adjoining or where a person may be found; articulable facts requirement for beyond-incident areas.\n- Fourth Amendment baseline: reasonableness standard, warrant preference with exceptions; curtilage vs. open fields distinctions are acceptable but optional.\n\nReward correct constraints, thresholds, and scope limits. Penalize conflations (e.g., claiming reasonable suspicion equals probable cause), overstated authorities (e.g., automatic frisks), or omissions of critical limits. Citations to canonical cases (e.g., Terry v. Ohio) are helpful but not required.\n\nScoring:\n- Full credit: Accurate, consistent, and appropriately constrained across all topics.\n- Partial: Minor inaccuracies or omissions that would not materially mislead field officers.\n- Low: Multiple substantive errors or misleading guidance.\nProvide a brief rationale in your feedback.", "expectation": "Definitions and thresholds are accurate, constraints are clear, and scope limits are correctly stated."}, {"type": "llm_judge", "name": "KRS 503.090 and Use-of-Force Standards Accuracy", "description": "Evaluates whether the summary of KRS 503.090 and related use-of-force standards is accurate and consistent with constitutional doctrine (e.g., Graham/ Garner) and Kentucky law.", "weight": 2.1, "judge_prompt": "Judge the correctness of the section on KRS 503.090 (Use of Physical Force in Law Enforcement) and related federal standards. Consider:\n- Correct articulation of KRS 503.090: when physical force and deadly physical force are justified by peace officers; include definitions/thresholds consistent with Kentucky law.\n- Alignment with federal standards: objective reasonableness (Graham v. Connor) and deadly force limits on fleeing felons (Tennessee v. Garner) if discussed.\n- Distinguishing physical force vs deadly physical force; necessity and proportionality concepts as applicable.\n- Any cautionary notes about reporting, medical aid, and policy compliance (not required but beneficial).\n- No misleading expansions of authority (e.g., deadly force solely for non-violent misdemeanors without imminent danger).\n\nScoring:\n- Full credit: Accurate, well-bounded summary aligned with KRS 503.090 and constitutional benchmarks.\n- Partial: Minor omissions or phrasing issues without practical misdirection.\n- Low: Substantive inaccuracies/misstatements of Kentucky law or federal constraints.\nProvide a concise explanation in feedback.", "expectation": "Sound, Kentucky-specific use-of-force summary aligned with KRS 503.090 and constitutional standards."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3: Field Usability and Professional Quality", "description": "Holistic assessment of clarity, usability for mid-watch patrol officers, and presentation professionalism.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity and Field Usability", "description": "Assesses whether content is concise, plain-language, and action-oriented for new patrol officers during mid-watch.", "weight": 1.0, "judge_prompt": "Evaluate clarity and field usability for new mid-watch patrol officers:\n- Plain-language, concise phrasing; minimal jargon or, if present, defined quickly.\n- Actionable bullets/checklists with do/don'ts or field cues.\n- Quick-scan layout (bold keywords, consistent bulleting) that supports on-shift reference.\n- Avoids overlong paragraphs; uses succinct guidance.\nScore higher if officers could plausibly use it during a call. Provide brief feedback with examples.", "expectation": "Concise, actionable, quick-scan guidance tailored to patrol use."}, {"type": "llm_judge", "name": "Organization and Quick-Reference Design", "description": "Assesses structure for rapid retrieval: clear headings, consistent sub-bullets, and visual hierarchy across both pages.", "weight": 1.0, "judge_prompt": "Assess the quick-reference organization and design:\n- Clear, consistent section headings for all eight topics.\n- Each section includes the three sub-bullets (Definition, Legal Standard/Test, Field Cues/Examples) and a consistent visual hierarchy.\n- Logical flow across the two pages; spacing and balance appropriate; key warnings or pitfalls called out.\n- Optional but beneficial: icons, callouts, or highlight styles that improve scanning.\nScore based on how quickly a patrol officer could find what they need. Provide short feedback.", "expectation": "Predictable, consistent layout with strong visual hierarchy for fast scanning."}, {"type": "llm_judge", "name": "Professional Presentation and References", "description": "Evaluates professional tone, agency-facing polish, and inclusion of references and disclaimers.", "weight": 1.0, "judge_prompt": "Evaluate professional presentation:\n- Appropriate tone for an internal roll-call training handout.\n- Header includes title, unit/agency, and date; footer or end section includes references/sources and a brief internal-use disclaimer.\n- Formatting is clean and consistent; no obvious typos.\nScore higher when it looks ready for immediate distribution. Provide brief feedback.", "expectation": "Polished, professional, properly referenced document suitable for roll-call distribution."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "69a8ef86-4e69-4fe2-9168-080f1e978e67", "rubric": {"category_name": "Wholesale Trade \u2013 Sales Managers: Return Authorization (RA) Process and Guidelines", "rationale": "This rubric enforces a self-documenting, two-document deliverable: an internal RA process and an external-facing RA guidelines document. Stage 1 uses LLM-only gating to require exact document structures that make verification trivial. Stage 2 mixes lightweight code checks (detectable, deterministic signals like deadlines and acronym coverage) with higher-weight LLM rules for correctness and cross-document consistency. Stage 3 evaluates professional quality, usability, and strategic value for key accounts (REI, Nordstrom, Dick\u2019s) and internal operations (DC, CS, Finance/AR, EDI/SPS, D365).", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2013 Shape Enforcement Gate (Documents and Structure)", "description": "Gate: Verify the candidate produced two separate documents with the exact structures required to enable verification. LLM-only checks. If this stage fails, the entire category is 0.", "is_required": true, "max_points": 2.5, "min_score_to_pass": 1.8, "rules": [{"type": "llm_judge", "name": "Internal RA Process \u2013 Structure Gate", "description": "Check that a separate DOCX/PDF exists for the internal process with required sections and explicit, step-wise structure.", "weight": 1.5, "judge_prompt": "You are validating the STRUCTURE ONLY of the internal Return Authorization (RA) process document. Consider ALL provided outputs. Identify the internal-facing document (intended for employees). Do not judge content quality here\u2014only presence and structure. Requirements:\n\nFormat and separation:\n- A separate document that is clearly internal-facing.\n- File type: DOCX or PDF.\n\nRequired sections and elements (flexible naming allowed):\n1) Title indicating internal RA process/policy.\n2) Overview/Purpose (why this exists; objectives like accountability, efficiency, error reduction).\n3) Scope (which accounts/returns are covered; e.g., key accounts like REI, Nordstrom, Dick\u2019s).\n4) Roles and Responsibilities (must reference roles like KAM/KAR, CS, Warehouse/DC, Finance/AR, EDI/SPS, D365).\n5) Step-by-Step Process section with numbered steps or a table. Each step clearly includes all three labels/fields:\n   - Action(s)\n   - Timeline/Deadline\n   - Role/Owner\n6) Explicit inclusion of ALL five mandated deadlines somewhere within the process (flexible phrasing allowed, but the specific timeframes must be findable):\n   - 3 days from KAM approving RTV to RA# issued\n   - Returns received at warehouse within 60 days of RA issuance\n   - 14 days for warehouse to provide report of items to CS\n   - Return credit issued within 45 days of warehouse receiving shipment\n   - RA closed internally after 90 days of creation with close/notify logic\n7) Closure Protocol section describing the 90-day closure and notifying the account if not received/credited.\n8) Optional appendix/templates (e.g., RA request template, label example) \u2013 optional.\n\nScoring:\n- 1.5: Separate DOCX/PDF found AND items 1\u20137 present (item 8 optional).\n- 1.2: All required except minor omission (e.g., section naming differs, or appendix missing; or 1 of the 5 deadlines not explicitly visible but the structure is otherwise correct).\n- 0.9: Missing one required section (e.g., no Roles/Responsibilities OR no clear step-by-step structure), OR only 3\u20134 of the 5 deadlines appear.\n- 0.5: Wrong or ambiguous format/structure (e.g., mixed with external, not clearly internal; lacks step-wise structure with Action/Timeline/Role), OR only 1\u20132 deadlines visible.\n- 0.0: No separate internal document found OR not DOCX/PDF.\n\nOnly evaluate presence/structure\u2014not correctness.", "expectation": "A cleanly structured internal DOCX/PDF with explicit stepwise process, roles, and all five deadlines."}, {"type": "llm_judge", "name": "External RA Guidelines \u2013 Structure Gate", "description": "Check that a separate DOCX/PDF exists for the external guidelines with required sections for key accounts.", "weight": 1.0, "judge_prompt": "You are validating the STRUCTURE ONLY of the external-facing RA guidelines. Consider ALL provided outputs. Identify the customer-facing document. Do not judge content quality\u2014only presence and structure. Requirements:\n\nFormat and separation:\n- A separate document that is clearly external-facing for key accounts (e.g., REI, Nordstrom, Dick\u2019s Sporting Goods).\n- File type: DOCX or PDF.\n\nRequired sections and elements (flexible naming allowed):\n1) Title indicating Return Authorization / Returns Guidelines / Policy for Key Accounts.\n2) Who this applies to (audience/scope).\n3) RA Request Requirements (the information customers must provide when requesting an RA; e.g., account info, PO/invoice, order number, SKU/UPC, quantities, reason codes, photos if applicable, ship-from address, contact details; references to SPS/D365 identifiers where relevant).\n4) Packaging and Labeling Requirements (e.g., RA# on outer carton and packing list; one RA per shipment; carton count; packing list contents; condition requirements).\n5) Shipping and Timing Requirements (include the 60-day window to return after RA issuance; carrier/appointment/tracking instructions as applicable; destination/return address guidance).\n6) Non-compliance/Exceptions (e.g., risk of delays, rejections, chargebacks, restocking fees; contingency notes for late or unlabeled returns).\n7) Contact/Support Information.\n8) Optional FAQs or Examples \u2013 optional.\n\nScoring:\n- 1.0: Separate DOCX/PDF found AND items 1\u20137 present (item 8 optional).\n- 0.8: Missing one minor supporting section or detail (e.g., explicit non-compliance notes), but core structure intact.\n- 0.5: Missing 2\u20133 required sections OR unclear separation from internal process.\n- 0.0: No separate external document found OR not DOCX/PDF.\n\nOnly evaluate presence/structure\u2014not correctness.", "expectation": "A cleanly structured external DOCX/PDF outlining request info, labeling, shipping/timing (including 60-day window), non-compliance, and contacts."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification (Correctness and Cross-Checks)", "description": "Now verify correctness and internal consistency of the structured deliverables. Mix lightweight code checks with LLM rules. Code rules have lower weight; LLM rules handle nuanced evaluation.", "is_required": false, "max_points": 4.5, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Acronym Coverage and Definitions Present", "description": "Deterministically check whether the documents include both acronyms and their expansions: KAR = Key Account Representative, CS = Customer Service, D365 = Dynamics 365, SPS = SPS Commerce. Partial credit awarded.", "weight": 0.25, "code": "import re\\n\\n\\ndef evaluate(workflow, context):\\n    \\\"\\\"\\n    Args:\\n        workflow: Workflow object\\n        context: ValidationContext with .files accessor\\n\\n    Returns:\\n        float in [0,1] normalized (engine scales by rule weight)\\n    \\\"\\\"\\n    try:\\n        outputs = context.get_all_outputs()\\n    except Exception:\\n        outputs = []\\n    if not outputs:\\n        return 0.0\\n\\n    def read_text(resource):\\n        # Try PDF first, then DOCX, then text fallback\\n        txt = ''\\n        try:\\n            if hasattr(resource, 'is_document') and resource.is_document:\\n                # Attempt PDF\\n                try:\\n                    txt = context.files.read_pdf_text(resource.id) or ''\\n                except Exception:\\n                    txt = ''\\n                if not txt:\\n                    try:\\n                        txt = context.files.read_docx_text(resource.id) or ''\\n                    except Exception:\\n                        txt = ''\\n            if (not txt) and hasattr(resource, 'is_text_format') and resource.is_text_format:\\n                try:\\n                    txt = context.files.read_text(resource.id) or ''\\n                except Exception:\\n                    txt = ''\\n        except Exception:\\n            txt = ''\\n        return txt\\n\\n    all_text = ''\\n    for r in outputs:\\n        all_text += '\\n' + (read_text(r) or '')\\n    if not all_text.strip():\\n        return 0.0\\n\\n    checks = [\\n        (r\"\\\\bKAR\\\\b\", r\"key\\\\s+account\\\\s+representative\"),\\n        (r\"\\\\bCS\\\\b\", r\"customer\\\\s+service\"),\\n        (r\"\\\\bD365\\\\b\", r\"dynamics\\\\s*365\"),\\n        (r\"\\\\bSPS\\\\b\", r\"sps\\\\s+commerce\"),\\n    ]\\n\\n    found = 0\\n    text_lower = all_text.lower()\\n    for ac_re, exp_re in checks:\\n        if re.search(ac_re, all_text, flags=re.IGNORECASE) and re.search(exp_re, text_lower, flags=re.IGNORECASE):\\n            found += 1\\n\\n    return found / len(checks) if checks else 0.0\\n"}, {"type": "code", "name": "Internal Deadlines Present (3, 60, 14, 45, 90 days)", "description": "Deterministically check that the five mandated deadlines appear in at least one document (ideally the internal process). Scores proportional to count found.", "weight": 0.25, "code": "import re\\n\\n\\ndef evaluate(workflow, context):\\n    \\\"\\\"\\n    Args:\\n        workflow: Workflow object\\n        context: ValidationContext with .files accessor\\n\\n    Returns:\\n        float in [0,1] normalized (engine scales by rule weight)\\n    \\\"\\\"\\n    targets = [\\n        (r\"\\\\b3\\\\s*(business\\\\s*)?day\", '3d'),\\n        (r\"\\\\b60\\\\s*(calendar\\\\s*)?day\", '60d'),\\n        (r\"\\\\b14\\\\s*(business\\\\s*)?day\", '14d'),\\n        (r\"\\\\b45\\\\s*(calendar\\\\s*)?day\", '45d'),\\n        (r\"\\\\b90\\\\s*(calendar\\\\s*)?day\", '90d'),\\n    ]\\n\\n    def read_text(resource):\\n        txt = ''\\n        try:\\n            if hasattr(resource, 'is_document') and resource.is_document:\\n                try:\\n                    txt = context.files.read_pdf_text(resource.id) or ''\\n                except Exception:\\n                    txt = ''\\n                if not txt:\\n                    try:\\n                        txt = context.files.read_docx_text(resource.id) or ''\\n                    except Exception:\\n                        txt = ''\\n            if (not txt) and hasattr(resource, 'is_text_format') and resource.is_text_format:\\n                try:\\n                    txt = context.files.read_text(resource.id) or ''\\n                except Exception:\\n                    txt = ''\\n        except Exception:\\n            txt = ''\\n        return txt\\n\\n    try:\\n        outputs = context.get_all_outputs() or []\\n    except Exception:\\n        outputs = []\\n    docs = [read_text(r) for r in outputs]\\n    if not any(docs):\\n        return 0.0\\n\\n    # Count matches per document; take the doc with the most matches (likely the internal process)\\n    best = 0\\n    for text in docs:\\n        if not text:\\n            continue\\n        tlower = text.lower()\\n        c = 0\\n        for pat, _ in targets:\\n            if re.search(pat, tlower):\\n                c += 1\\n        if c > best:\\n            best = c\\n\\n    return best / len(targets) if targets else 0.0\\n"}, {"type": "llm_judge", "name": "Internal Process \u2013 Correctness and Control Alignment", "description": "Assess whether the internal process addresses the pain points (unlabeled returns, late returns, credit discrepancies) with clear controls and aligned timelines.", "weight": 1.0, "judge_prompt": "Evaluate the INTERNAL process document for correctness and control alignment (not just structure). Check:\n- Do steps introduce controls to mitigate the stated issues (unlabeled returns, late returns, processing delays, credit mismatches)? Examples: mandatory RA labeling at carton and packing slip, intake verification at DC, discrepancy resolution workflow, cutoff/timing controls aligned to SLAs, clear documentation handoffs.\n- Are the five deadlines correctly placed in the flow and logically sequenced? (3d to RA issuance; 60d receipt window; 14d DC-to-CS report; 45d credit issuance; 90d closure protocol.)\n- Are dependencies and inputs/outputs between teams clear (e.g., KAM/KAR \u2192 CS \u2192 DC \u2192 CS/Finance/AR, with SPS/D365 touchpoints)?\n\nScoring:\n- 1.0: Controls comprehensively address issues; deadlines are correctly embedded and sequenced; handoffs clearly defined.\n- 0.7: Mostly correct with minor gaps (e.g., one control or handoff unclear).\n- 0.4: Significant gaps in controls or sequencing; multiple ambiguous handoffs.\n- 0.0: Largely incorrect or missing key controls/dates.", "expectation": "A coherent internal process with strong controls that reduce chargebacks, delays, and mismatches."}, {"type": "llm_judge", "name": "Roles, Ownership, and Handoffs", "description": "Assess clarity of role assignment and accountability across steps, including KAR/KAM, CS, DC/Warehouse, Finance/AR, EDI/SPS, D365 touchpoints.", "weight": 1.0, "judge_prompt": "Evaluate whether each step in the INTERNAL process clearly identifies responsible role/team and handoffs. Look for:\n- Explicit owner per step and clear inputs/outputs.\n- KAR/KAM, CS, Warehouse/DC, Finance/AR responsibilities are unambiguous.\n- SPS/EDI and D365 touchpoints are identified where relevant (e.g., RA creation in CS, upload/logging in D365, EDI notices via SPS).\n\nScoring:\n- 1.0: Ownership and handoffs are explicit and unambiguous throughout.\n- 0.7: Minor ambiguity in one area.\n- 0.4: Multiple steps missing owners or unclear handoffs.\n- 0.0: Roles largely missing or mislabeled.", "expectation": "Stepwise mapping of Action \u2192 Timeline \u2192 Responsible role, with crisp handoffs."}, {"type": "llm_judge", "name": "External Guidelines \u2013 Completeness and Operability", "description": "Evaluate the customer-facing guidelines for completeness and operational clarity.", "weight": 1.0, "judge_prompt": "Assess the EXTERNAL guidelines for:\n- RA Request Requirements: account details, PO/invoice/order, SKU/UPC, quantities, reason codes, photos (if defect), ship-from, contact, SPS/D365 identifiers when relevant.\n- Packaging/Labeling: RA# on outer carton and packing list, one RA per shipment, carton count, condition/packaging standards, packing list details.\n- Shipping/Timing: 60-day return window, carrier/appointment/tracking as needed, destination/return address guidance.\n- Non-compliance: clear consequences (delays, rejection, chargebacks, restocking), exceptions handling.\n- Contacts/support for questions.\n\nScoring:\n- 1.0: All areas covered with clear, actionable guidance.\n- 0.7: Minor omissions or vague elements.\n- 0.4: Multiple missing areas.\n- 0.0: Largely incomplete or unusable.", "expectation": "A practical, actionable set of instructions key accounts can follow without confusion."}, {"type": "llm_judge", "name": "Internal\u2013External Consistency and Systems Integration", "description": "Check that internal and external documents align on timelines/policies and properly reference D365/SPS where appropriate.", "weight": 1.0, "judge_prompt": "Cross-check the INTERNAL process and EXTERNAL guidelines for consistency:\n- Timelines align (e.g., 60-day receipt window, 3/14/45/90-day internal SLAs reflected and not contradicted).\n- Terminology alignment for RA/RTV, labeling, documentation, and credit issuance.\n- Systems integration notes are coherent: CS generates RA#, D365 logging, SPS/EDI acknowledgments or reference numbers are used consistently across docs.\n\nScoring:\n- 1.0: Fully consistent; systems touchpoints aligned and not contradictory.\n- 0.7: Minor inconsistencies or omissions.\n- 0.4: Several inconsistencies; unclear systems integration.\n- 0.0: Contradictory policies/timelines.", "expectation": "Both documents tell the same story and reference systems consistently."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Quality and Professionalism", "description": "Holistic quality assessment of both documents for audience fit, clarity, and strategic value.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Tone and Clarity", "description": "Evaluate writing quality, coherence, and appropriateness for internal staff and key accounts.", "weight": 0.75, "judge_prompt": "Assess both documents for professional tone, clear and concise writing, coherent organization, and audience-appropriate language (internal vs. external). Consider formatting (headings, bullets), consistency in terminology, and absence of ambiguity or jargon without explanation.\n\nScoring: 0.75 excellent; 0.5 good with minor issues; 0.25 fair with notable issues; 0.0 poor/unclear.", "expectation": "Concise, professional documents that are easy to read and navigate."}, {"type": "llm_judge", "name": "Actionability and Usability", "description": "Evaluate whether documents can be used operationally without extra guidance.", "weight": 0.75, "judge_prompt": "Do the documents provide actionable steps and checklists/templates so teams and accounts can execute without additional clarification? Look for tables pairing Action\u2013Timeline\u2013Owner, checklists for RA requests, labeling examples, and clear next steps for exceptions.\n\nScoring: 0.75 highly actionable; 0.5 mostly actionable; 0.25 partially actionable; 0.0 not actionable.", "expectation": "Practical tools (steps, checklists, examples) that reduce ambiguity and rework."}, {"type": "llm_judge", "name": "Risk Mitigation and KPI Orientation", "description": "Assess how well the solution reduces chargebacks, delays, and credit discrepancies, and whether it includes metrics/SLAs for accountability.", "weight": 0.75, "judge_prompt": "Evaluate whether the internal process and external guidelines include controls and monitoring that mitigate risks (unlabeled/late returns, credit mismatches) and propose KPIs/SLAs (e.g., on-time RA issuance, on-time DC reporting, credit issuance cycle time, exception rate). Consider escalation paths and auditability (e.g., logs, D365/SPS references).\n\nScoring: 0.75 strong risk control and KPI focus; 0.5 moderate; 0.25 weak; 0.0 absent.", "expectation": "Clear controls with measurable KPIs/SLAs and escalation paths."}, {"type": "llm_judge", "name": "Scalability to Key Accounts and Systems Fit", "description": "Judge whether the approach scales across large key accounts and integrates with DC/DC scheduling, D365, and SPS/EDI flows.", "weight": 0.75, "judge_prompt": "Assess whether the policies are practical at scale for REI, Nordstrom, Dick\u2019s, etc., considering DC appointments, carton labeling volume, EDI/SPS transactions, and D365 processes. Look for guidance that anticipates variability (multiple POs, mixed shipments), and avoids bottlenecks.\n\nScoring: 0.75 clearly scalable and systems-aware; 0.5 somewhat; 0.25 limited; 0.0 not scalable.", "expectation": "Guidance that works reliably across large retail partners and existing systems."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "c7d83f01-2874-4876-b7fd-52582ec99e1a", "rubric": {"category_name": "American Option Pricing Framework (Quant Research)", "rationale": "Pattern C (Mixed): The deliverable is a Python notebook combining executable code, data/plots, and an executive-style written recommendation. Stage 1 uses an LLM-only gate to enforce a strict, verifiable notebook structure that makes downstream verification trivial. Stage 2 mixes light-weight code rules (structural/keyword/bounds checks over the .ipynb JSON) with heavier LLM judges for technical correctness and comparative methodology assessment. Stage 3 provides a holistic, professional quality review focused on clarity, production relevance, and presentation.", "max_total_score": 30.0, "stages": [{"name": "Stage 1 \u2014 Notebook Shape Enforcement (Gate)", "description": "Enforce exact deliverable structure so that verification is trivial. LLM-only checks: presence of required sections, methods, plots, and summary within a Jupyter notebook.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Structured Notebook Format Requirement", "description": "Check that the primary output is a well-structured Jupyter notebook implementing and comparing American option pricing methods with visual evidence and a final recommendation.", "weight": 8.0, "judge_prompt": "You are evaluating the candidate's primary output. This is a SHAPE-ONLY gate. Do not judge correctness or code quality here\u2014only confirm that the expected structure exists and is clearly labeled.\n\nAcceptable formats (flexible):\n- Preferred: A Jupyter Notebook (.ipynb) showing code cells, markdown sections, and embedded outputs/figures.\n- Also acceptable: A PDF export of the notebook that visibly includes code, outputs, and headings mirroring the required structure.\n\nRequired structure (be flexible on exact headings, but all content must be present and clearly labeled):\n1) Title and Overview\n   - An introductory markdown section stating the objective: building a comprehensive American option pricing framework and comparing multiple methods.\n   - A short list of key parameters/assumptions (e.g., S0, K, r, q/dividend, sigma, T) and scope.\n\n2) Methods Implemented (all three required)\n   - Binomial tree method for American options (e.g., CRR or trinomial) with early exercise logic.\n   - Finite-difference PDE method for American options (e.g., implicit or Crank\u2013Nicolson with free-boundary treatment via penalty/PSOR or equivalent) with boundary conditions stated.\n   - Monte Carlo with Longstaff\u2013Schwartz (LSM) regression for early exercise.\n   Each method must have: a short explanation, a code implementation, and at least one usage example.\n\n3) Validation and Sanity Checks\n   - At least two sanity/validation checks specific to American options (examples: American call without dividends equals European price; no-arbitrage bounds; put\u2013call inequalities; monotonicity w.r.t. key parameters). A short narrative and corresponding code/output.\n\n4) Convergence Study\n   - A convergence analysis for at least two methods (e.g., price vs. steps/grid size), with plots or clearly formatted tables.\n\n5) Runtime Benchmarks\n   - A simple runtime benchmark comparison across methods (e.g., steps/grid sizes vs. measured time) with a table/plot and brief notes about the environment (hardware and/or package versions acceptable in any section).\n\n6) Accuracy Comparison\n   - A side-by-side price comparison table for all implemented methods on at least one test case; optional reference/\"gold standard\" noted if used.\n\n7) Visualizations (minimum three)\n   - Include at least three distinct figures supporting the analysis (e.g., convergence plot, runtime bar/line chart, early-exercise/free-boundary visualization or LSM regression diagnostics).\n\n8) Summary and Recommendation\n   - A final section with key findings and a practical recommendation for production use in a high-performance trading context. The recommendation must name a preferred method (or combination) and justify it.\n\n9) Reproducibility Notes\n   - A short, visible note covering how to re-run the notebook end-to-end, seeds or randomness handling, and environment/hardware notes (version list or brief statement is acceptable).\n\nScoring (apply strictly to PRESENCE/STRUCTURE only):\n- 8.0: All required structural elements are present and clearly labeled; notebook (or faithful export) shows code, outputs, and figures.\n- 6.0\u20137.5: Exactly one required block missing OR present but clearly incomplete (e.g., only 1 validation check, only 2 figures, or runtime study missing environment note). Everything else present.\n- 4.0\u20135.5: Two required blocks missing/incomplete OR one core method (Binomial, FD, or LSM) missing.\n- 1.0\u20133.5: Multiple core elements missing; appears to be a rough draft with little structure.\n- 0.0: Not a notebook (or faithful export of one), or fundamentally wrong format with no identifiable sections.\n\nOnly evaluate the existence and labeling of these elements, not their correctness or quality.", "expectation": "A Jupyter notebook with clearly labeled sections, three implemented American option methods (Binomial, Finite Difference, LSM Monte Carlo), validation checks, at least three supporting plots, convergence and runtime analyses, and a final recommendation."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification of Methodological Correctness and Evidence", "description": "Now verify technical soundness and comparative analysis using mixed code and LLM rules. Code rules perform robust structural/keyword checks over the .ipynb JSON. LLM judges assess correctness, validation depth, and recommendation logic.", "is_required": true, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Notebook Parsable and Methods Coverage (Keywords)", "description": "Parse .ipynb and check for indicative keywords showing all three American methods are implemented (binomial, finite-difference with free boundary treatment, LSM Monte Carlo) and early exercise handling.", "weight": 0.6, "code": "import json, re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No primary output.\"\n    # Attempt to read notebook JSON\n    try:\n        path = context.files.get_path(output.id)\n        if not str(path).lower().endswith('.ipynb'):\n            # Not fatal for Stage 2, but this rule requires a notebook\n            return 0.0, \"Primary output is not a .ipynb notebook.\"\n        nb_text = context.files.read_text(output.id)\n        nb = json.loads(nb_text)\n    except Exception as e:\n        return 0.0, f\"Failed to parse notebook JSON: {e}\"\n\n    cells = nb.get('cells', [])\n    code_text = []\n    md_text = []\n    for c in cells:\n        src = ''.join(c.get('source', [])).lower()\n        if c.get('cell_type') == 'code':\n            code_text.append(src)\n        elif c.get('cell_type') == 'markdown':\n            md_text.append(src)\n    code_s = '\\n'.join(code_text)\n    md_s = '\\n'.join(md_text)\n    all_s = code_s + '\\n' + md_s\n\n    score = 0.0\n    details = []\n    # Binomial/trinomial indicators\n    if any(k in all_s for k in ['binomial', 'crr', 'cox-ross-rubinstein', 'trinomial']):\n        score += 0.15; details.append('Binomial present')\n    # FD indicators (free boundary / PSOR / penalty / crank\u2013nicolson)\n    if any(k in all_s for k in ['finite difference', 'crank', 'cn scheme', 'implicit', 'psor', 'penalty', 'free boundary', 'free-boundary']):\n        score += 0.15; details.append('Finite-difference present')\n    # LSM indicators\n    if any(k in all_s for k in ['longstaff', 'lsm', 'least squares', 'regression basis']):\n        score += 0.15; details.append('LSM present')\n    # Early exercise / american indicators\n    if any(k in all_s for k in ['american', 'early exercise', 'snell']):\n        score += 0.15; details.append('Early-exercise noted')\n\n    # Cap at weight\n    max_w = 0.6\n    if score > max_w:\n        score = max_w\n    fb = ', '.join(details) if details else 'No relevant keywords found'\n    return score, fb"}, {"type": "code", "name": "Visual Evidence and Benchmarks Present", "description": "Check notebook outputs and code for plots/figures and runtime benchmarking (e.g., use of matplotlib/plotly and time/perf_counter/timeit).", "weight": 0.6, "code": "import json\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No primary output.\"\n    try:\n        path = context.files.get_path(output.id)\n        if not str(path).lower().endswith('.ipynb'):\n            return 0.0, \"Not a notebook; cannot inspect cell outputs.\"\n        nb = json.loads(context.files.read_text(output.id))\n    except Exception as e:\n        return 0.0, f\"Failed to parse notebook JSON: {e}\"\n\n    cells = nb.get('cells', [])\n    code_text = []\n    image_count = 0\n    for c in cells:\n        if c.get('cell_type') == 'code':\n            code_text.append(''.join(c.get('source', [])).lower())\n            for out in c.get('outputs', []) or []:\n                data = out.get('data') or {}\n                if isinstance(data, dict) and any(k.startswith('image/') for k in data.keys()):\n                    image_count += 1\n    code_s = '\\n'.join(code_text)\n\n    plot_hits = any(k in code_s for k in ['matplotlib', 'plt.', 'seaborn', 'plotly'])\n    time_hits = any(k in code_s for k in ['timeit', 'perf_counter', 'time.time(', 'process_time'])\n    conv_hits = any(k in code_s for k in ['convergence', 'n_steps', 'nsteps', 'grid size', 'refinement'])\n\n    score = 0.0\n    details = []\n    if plot_hits:\n        score += 0.2; details.append('Plotting libs used')\n    if image_count >= 2:\n        score += 0.2; details.append(f'{image_count} images in outputs')\n    if time_hits:\n        score += 0.1; details.append('Runtime measurement present')\n    if conv_hits:\n        score += 0.1; details.append('Convergence-related code present')\n    if score > 0.6:\n        score = 0.6\n    return score, ', '.join(details) if details else 'No plots/benchmarks detected'"}, {"type": "code", "name": "Validation Checks and Reproducibility Cues", "description": "Look for explicit validation tests (e.g., American call without dividends equals European), no-arbitrage bounds, and randomness seeding for MC.", "weight": 0.6, "code": "import json, re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No primary output.\"\n    try:\n        path = context.files.get_path(output.id)\n        if not str(path).lower().endswith('.ipynb'):\n            return 0.0, \"Not a notebook; cannot inspect validation cues.\"\n        nb = json.loads(context.files.read_text(output.id))\n    except Exception as e:\n        return 0.0, f\"Failed to parse notebook JSON: {e}\"\n\n    cells = nb.get('cells', [])\n    text = []\n    for c in cells:\n        src = ''.join(c.get('source', [])).lower()\n        text.append(src)\n    s = '\\n'.join(text)\n\n    score = 0.0\n    details = []\n    # American call vs European (no dividends)\n    if any(k in s for k in ['american call equals european', 'american call = european', 'call without dividends equals european']):\n        score += 0.2; details.append('American call vs European check')\n    # No-arbitrage / bounds / inequality checks\n    if any(k in s for k in ['no-arbitrage', 'no arbitrage', 'lower bound', 'upper bound', 'put-call', 'put call']):\n        score += 0.2; details.append('Arbitrage/bounds checks')\n    # Seeding randomness\n    if any(k in s for k in ['np.random.seed', 'random.seed', 'rng = np.random.default_rng']):\n        score += 0.2; details.append('Randomness seeded')\n\n    if score > 0.6:\n        score = 0.6\n    return score, ', '.join(details) if details else 'No validation cues or seeding detected'"}, {"type": "llm_judge", "name": "Methodological Correctness for American Options", "description": "Assess whether each method is implemented in a way that is algorithmically appropriate for American options: early exercise in binomial trees; free-boundary or equivalent constraint treatment in finite differences; and LSM regression for Monte Carlo.", "weight": 2.55, "judge_prompt": "Evaluate technical correctness of the three methods specifically for American options:\n- Binomial/Trinomial: Early exercise decision at each node; reasonable handling of dividends (q) if present; stability with increasing steps.\n- Finite Differences: A proper treatment of the American early-exercise constraint (e.g., obstacle problem via penalty/PSOR or projected method), appropriate boundary/terminal conditions, and a stable scheme (implicit/CN) with grid specification.\n- Monte Carlo (LSM): Clear regression/basis functions for continuation value, in-the-money filtering, backward induction, and correct use for American-style early exercise.\n\nScoring:\n- 2.55: All three are correctly adapted to American options with clear, coherent implementations and explanations.\n- 1.7\u20132.2: Minor issues or omissions (e.g., brief but acceptable description of free boundary, limited basis set) but overall correct.\n- 0.8\u20131.6: Noticeable gaps (e.g., FD lacks a proper constraint; MC lacks regression or policy definition; binomial missing exercise logic) but partial correctness.\n- 0.0\u20130.7: Major methodological errors or misapplication (e.g., plain European MC used for American pricing).", "expectation": "All three methods correctly handle early exercise and constraints appropriate to American options."}, {"type": "llm_judge", "name": "Validation Depth and Accuracy Evidence", "description": "Assess whether the notebook provides sound validation: sanity checks, convergence to stable values, and accuracy comparisons against a reference or high-resolution benchmark.", "weight": 2.55, "judge_prompt": "Judge the robustness of validation and accuracy evidence:\n- Sanity checks include American-call-without-dividends equivalence to European, no-arbitrage bounds/inequalities, and parameter monotonicity where appropriate.\n- Convergence studies (e.g., steps/grid refinements) show prices stabilizing; error metrics or at least qualitative error trends are presented.\n- Accuracy comparison table includes multiple methods and, if possible, a reference price (e.g., high-step binomial, high-res FD, or well-known approximation) with transparent assumptions.\n\nScoring:\n- 2.55: Multiple, well-executed validation checks, clear convergence evidence, and a credible accuracy comparison.\n- 1.7\u20132.2: Some validation present but limited depth (e.g., convergence for only one method or missing error quantification).\n- 0.8\u20131.6: Minimal validation; unclear convergence; weak comparison.\n- 0.0\u20130.7: No meaningful validation or evidence of accuracy.", "expectation": "Clear convergence and validation checks with an accuracy comparison across methods."}, {"type": "llm_judge", "name": "Computational Efficiency and Engineering Considerations", "description": "Evaluate whether computational trade-offs and performance engineering relevant to production are addressed and evidenced (runtime plots/tables, vectorization/Numba, complexity discussion).", "weight": 2.55, "judge_prompt": "Assess the computational analysis:\n- Runtime benchmarks across methods and resolutions shown; notes on environment/hardware; commentary on complexity (big-O) or bottlenecks.\n- Evidence or discussion of performance techniques (vectorization, memory patterns, Numba/Cython, parallelism/GPU feasibility) and how they affect American methods differently.\n- Practical implications for latency/throughput in a trading context (e.g., precomputation, caching surfaces, warm-starts for FD/PSOR, batching for LSM).\n\nScoring:\n- 2.55: Strong, well-substantiated performance analysis with actionable engineering insights.\n- 1.7\u20132.2: Reasonable runtime evidence and some discussion; limited depth on engineering.\n- 0.8\u20131.6: Superficial timing or generic comments; little production relevance.\n- 0.0\u20130.7: No meaningful efficiency analysis.", "expectation": "Substantiated runtime comparisons and production-relevant performance insights."}, {"type": "llm_judge", "name": "Recommendation Soundness for Production Use", "description": "Evaluate whether the recommendation identifies a most suitable method (or hybrid) with clear, evidence-based justification aligned to high-performance trading needs.", "weight": 2.55, "judge_prompt": "Review the final recommendation:\n- Names a preferred approach (e.g., high-step binomial for flexibility, FD with PSOR for stability and Greeks, LSM for path-dependent extensions) or a hybrid strategy (e.g., FD for valuation, binomial for sanity checks, LSM for exotics).\n- Justification ties to accuracy, stability, Greeks availability, calibration, latency/throughput, and implementation complexity.\n- Notes limitations and fallback plans (e.g., degradation modes, parameter ranges, discrete dividends handling, stress behavior).\n\nScoring:\n- 2.55: Clear, defensible, and evidence-backed recommendation tailored to production constraints.\n- 1.7\u20132.2: Mostly sound but missing one or two critical considerations (e.g., Greeks or latency).\n- 0.8\u20131.6: Vague or weakly supported recommendation.\n- 0.0\u20130.7: No actionable recommendation.", "expectation": "An explicit, evidence-based method recommendation aligned with trading constraints."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Holistic Quality and Professionalism", "description": "Assess overall clarity, professionalism, and strategic value for a proprietary trading context.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Code Organization and Documentation Quality", "description": "Evaluate readability, modularity, docstrings, type hints/comments, and logical organization (parameter blocks, reusable functions/classes, test cells).", "weight": 2.5, "judge_prompt": "Judge software craftsmanship:\n- Clear modular functions/classes with docstrings and, where appropriate, type hints or inline comments.\n- Logical flow: parameters/assumptions; implementations; tests; results; summary.\n- Cleanliness (PEP 8 style, minimal global state) and avoidance of notebook anti-patterns (hidden state, duplicated cells).\n\nScore 0.0\u20132.5 based on how professional, maintainable, and well-documented the codebase appears.", "expectation": "Well-structured, documented, and maintainable code cells with clear organization."}, {"type": "llm_judge", "name": "Visualization Clarity and Communication", "description": "Assess whether figures are interpretable, well-labeled, and support the narrative (titles, axes, legends, units; readable styling; appropriate scales).", "weight": 2.5, "judge_prompt": "Evaluate the quality of plots/tables:\n- Clear titles, axes labels (including units where relevant), legends, and readable scales.\n- Visuals directly support claims (e.g., convergence slopes, runtime comparisons, exercise boundary depiction).\n- Appropriate use of error bars or reference lines if applicable, and consistent styling.\n\nScore 0.0\u20132.5 based on clarity and communicative effectiveness.", "expectation": "Clean, well-labeled visuals that support the analysis and conclusions."}, {"type": "llm_judge", "name": "Clarity of Narrative and Accessibility", "description": "Evaluate how clearly the notebook explains assumptions, methods, and results for a quant audience under time pressure.", "weight": 2.5, "judge_prompt": "Assess explanatory clarity:\n- Concise, accurate explanations of assumptions and methods.\n- Results are summarized with takeaways; readers can quickly grasp what to use and why.\n- Accessibility: definitions/notation given once; avoids unnecessary jargon; links to resources optional.\n\nScore 0.0\u20132.5 based on clarity and accessibility for a professional quant audience.", "expectation": "A crisp narrative with clear takeaways and minimal friction for expert readers."}, {"type": "llm_judge", "name": "Production Readiness, Risk, and Extensibility", "description": "Evaluate attention to practical concerns: Greeks, dividends, robustness across parameters, failure modes, and pathways to extend the framework.", "weight": 2.5, "judge_prompt": "Judge production-minded completeness:\n- Discussion or evidence of Greeks availability/stability and calibration considerations.\n- Treatment of dividends (continuous q or discrete) and implications for American calls/puts.\n- Robustness across market regimes (vol, rates), numerical stability, and monitoring for drift/mismatch.\n- Extensibility to early-exercise exotics or multi-asset underlyings; fallback and validation harness.\n\nScore 0.0\u20132.5 based on practical completeness and risk-awareness.", "expectation": "Demonstrated awareness of production risks and a path to robust, extensible deployment."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "1a78e076-445e-4c5d-b8ce-387d2fe5e715", "rubric": {"category_name": "Health Care: Evidence-Based Literature Review \u2014 Hypertension Adherence in Older Adults", "rationale": "This rubric enforces a self-documenting workflow for a professional literature review. Stage 1 is a strict, LLM-only gate ensuring the output is a properly structured DOCX/PDF with all required sections and Results subthemes, enabling later verification. Stage 2 mixes targeted code checks (lightweight, deterministic) with higher-weight LLM judgments to verify methodological soundness and factual coverage (prevalence, age variation, morbidity/mortality, financial impact). Stage 3 provides a holistic quality assessment focused on writing clarity, critical appraisal, clinical utility, and professional presentation. Code rules are intentionally lower-weight than LLM rules to reflect their narrower scope.", "max_total_score": 26.0, "stages": [{"name": "Stage 1 \u2014 Format and Structural Gate", "description": "Gate: The submission must be a properly structured research-style document (DOCX or PDF) with all required sections and subthemes to enable verification. No content quality assessed here\u2014only presence/shape.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.0, "rules": [{"type": "llm_judge", "name": "Document Format and Section Structure (Gate)", "description": "Check that the output is a DOCX or PDF research-style document, 10\u201315 pages, with required sections and subthemes, and a References section with no more than 30 entries.", "weight": 6.0, "judge_prompt": "You are evaluating whether the candidate output satisfies the required FORMAT and STRUCTURE for a literature review. Only assess presence/structure, not quality or correctness of content. Be flexible with minor header wording, but the intended sections must be clearly identifiable.\n\nRequirements to check:\n- File format: Must be DOCX or PDF. It should read like a research paper/essay (not an outline, not slides).\n- Length: 10\u201315 pages (judge visually for PDFs; for DOCX, approximate via pagination or overall length). If clearly <10 or clearly >15 pages, it fails length.\n- Professional formatting: Clear section headers and paragraph prose (not bullet-only).\n\nRequired named sections (or close synonyms, in this order or logically close):\n1) \"Factors Affecting Adherence in Hypertension Management\"\n2) \"Search Strategy\"\n3) \"Results\" section WITH four identifiable subthemes as subsections under Results:\n   - Multidisciplinary\n   - Psychosocial\n   - Patient-Centered\n   - Technological Intervention\n4) \"Strengths and Limitations\"\n5) \"Conclusion\"\n6) \"Future Research\"\n7) \"References\" (not more than 30 references)\n\nTopic coverage presence checks (structure-only, not accuracy):\n- The document mentions or dedicates content to: (i) prevalence data, (ii) adherence variations across older age groups, (iii) morbidity/mortality associated with poor adherence, and (iv) financial impact of hypertension management. These can appear within Factors/Results or elsewhere but must be explicitly present.\n- The \"Search Strategy\" section should be written in paragraph form (at least ~3 sentences) and mention databases or sources (e.g., PubMed, CINAHL, Google Scholar, CDC, AHA) and keywords/Boolean terms (flexible, but must be clearly described).\n\nScoring (0.0 to 6.0):\n- 6.0: DOCX/PDF; 10\u201315 pages; all required sections present; Results has all four subtheme subsections; References present with \u226430 entries; all four topic mentions present; Search Strategy present with paragraph description and databases mentioned.\n- 5.0: All required sections present and Results subthemes present; minor deviation (e.g., slight length ambiguity or one topic mention only briefly present) but still clearly 10\u201315 pages and references \u226430.\n- 4.0: One missing structural element (e.g., one Results subtheme missing OR Search Strategy too brief) but otherwise correct format and length; references \u226430.\n- 2.0: Multiple missing sections/subthemes or unclear Results organization OR references clearly >30 OR length out of bounds but still a DOCX/PDF.\n- 0.0: Not DOCX/PDF; or grossly wrong format (outline/slides); or clearly <10 pages; or missing several core sections (e.g., no Results or no References).\n\nOnly evaluate structure/presence. Do not evaluate correctness of facts or writing quality.", "expectation": "A professionally formatted DOCX/PDF, 10\u201315 pages, with all required sections and Results subthemes, plus a capped References section (\u226430)."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification and Evidence Checks", "description": "Now verify methodological adequacy and factual coverage using mixed rules. Code rules do bounded textual checks; LLM rules assess evidentiary correctness and synthesis.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Coverage of Required Analytic Topics", "description": "Verify that the review substantively covers: prevalence in older adults; adherence variation across older age ranges; morbidity/mortality associated with poor adherence; and financial impact of hypertension management, with appropriate citations.", "weight": 2.5, "judge_prompt": "Assess whether the document substantively and accurately addresses all four analytic topics, supported by citations to peer-reviewed or reputable sources:\n1) Prevalence data for hypertension in older adults (provide at least one concrete statistic or trend with a citation).\n2) How treatment adherence varies across older age bands (e.g., young-old vs old-old), with evidence.\n3) Morbidity and mortality implications associated with poor adherence in older adults, with data points or risk estimates and citations.\n4) Financial impact of hypertension management and/or nonadherence (e.g., costs to patients/health systems), with cited figures or estimates.\n\nScoring (0.0\u20132.5):\n- 2.5: All four topics covered with specific, plausible statistics and citations; no major factual errors.\n- 1.7: Three topics are covered well; the fourth is superficial or missing a clear citation.\n- 1.0: Two topics adequately covered; others superficial or missing.\n- 0.3: Only one topic touched superficially.\n- 0.0: None covered meaningfully or obvious factual issues.", "expectation": "All four topics addressed with data and citations."}, {"type": "llm_judge", "name": "Search Strategy Methodological Adequacy", "description": "Evaluate the rigor and clarity of the search strategy (databases, time windows, inclusion/exclusion criteria, keywords/Boolean).", "weight": 2.5, "judge_prompt": "Evaluate the Search Strategy section for methodological adequacy appropriate to an evidence-based literature review targeting older adults and medication adherence in hypertension.\n\nExpectations:\n- Names relevant databases/sources (e.g., PubMed, CINAHL, Google Scholar, CDC, AHA) and a reasonable time range.\n- Clearly states inclusion criteria (terms such as adherence, self-efficacy, hypertension/high blood pressure, older adults, medication adherence) and any exclusion criteria (e.g., non-English, non-peer-reviewed, pediatric).\n- Describes keywords and Boolean operators (e.g., AND/OR combinations) and screening steps.\n- Indicates how many sources were screened/selected or provides a transparent selection rationale.\n\nScoring (0.0\u20132.5):\n- 2.5: Thorough, transparent, and reproducible; databases, criteria, and terms clearly specified.\n- 1.7: Mostly clear but one key element is vague (e.g., no date range or unclear criteria).\n- 1.0: Partial description with multiple gaps.\n- 0.3: Minimal description.\n- 0.0: No meaningful search strategy described.", "expectation": "A transparent, reproducible search strategy with databases, criteria, and Boolean terms."}, {"type": "llm_judge", "name": "Evidence Attribution and Interpretation Accuracy", "description": "Check that key claims are attributed to peer-reviewed/reputable sources and that statistics are interpreted correctly without obvious errors.", "weight": 2.0, "judge_prompt": "Review the Results and supporting sections for whether claims are properly attributed and correctly interpreted:\n- Are specific data points (e.g., prevalence, risk ratios, mortality rates, cost estimates) accompanied by citations?\n- Are there obvious misinterpretations (e.g., confusing association vs causation, misreading percentages, misquoting figures)?\n- Do cited sources appear peer-reviewed or reputable (journals, CDC, AHA), rather than blogs/unverified sources?\n\nScoring (0.0\u20132.0):\n- 2.0: Claims consistently cited; interpretations align with evidence; no major errors.\n- 1.2: Mostly cited and accurate, with minor lapses.\n- 0.6: Several uncited claims or some questionable interpretations.\n- 0.0: Many uncited claims or clear misinterpretations.", "expectation": "Consistent, accurate attribution to peer-reviewed or reputable sources."}, {"type": "llm_judge", "name": "Older Adult Focus and Thematic Synthesis", "description": "Confirm focus on older adults and inclusion of the four subthemes in Results with synthesis of barriers/facilitators and interventions relevant to adherence and self-efficacy.", "weight": 2.0, "judge_prompt": "Evaluate whether the review maintains a clear focus on older adults and synthesizes evidence across the four Results subthemes (Multidisciplinary, Psychosocial, Patient-Centered, Technological Intervention):\n- Do these subsections integrate findings rather than merely listing studies?\n- Do they address adherence, medication adherence, and self-efficacy mechanisms/barriers/facilitators relevant to older adults?\n- Are interventions described with outcomes or effect directions and citations?\n\nScoring (0.0\u20132.0):\n- 2.0: Strong, integrative synthesis across all four subthemes; clear older adult focus and adherence/self-efficacy relevance.\n- 1.2: Good synthesis with minor gaps or one weaker subtheme.\n- 0.6: Superficial synthesis or multiple weak subthemes.\n- 0.0: Little to no synthesis; older adult focus missing.", "expectation": "Integrated synthesis across the four subthemes centered on older-adult adherence and self-efficacy."}, {"type": "code", "name": "Word Count Bounds Check (Length Proxy)", "description": "Approximate 10\u201315 pages via word count on extracted text.", "weight": 1.2, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float or (float, str)\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output.\"\n    text = \"\"\n    try:\n        if output.ext.lower().endswith('.pdf'):\n            text = context.files.read_pdf_text(output.id) or \"\"\n        elif output.ext.lower().endswith('.docx'):\n            text = context.files.read_docx_text(output.id) or \"\"\n        else:\n            return 0.0, \"Unsupported document format.\"\n    except Exception as e:\n        return 0.0, f\"Error reading document: {e}\"\n\n    words = re.findall(r\"\\b\\w+\\b\", text)\n    wc = len(words)\n    # 10\u201315 pages ~ roughly 2500\u20136000 words (broad tolerance for formatting)\n    if 2500 <= wc <= 6000:\n        score = 1.2\n        fb = f\"Word count {wc} within target range.\"\n    elif 2000 <= wc < 2500 or 6001 <= wc <= 7000:\n        score = 0.6\n        fb = f\"Word count {wc} marginally outside target range.\"\n    else:\n        score = 0.0\n        fb = f\"Word count {wc} far outside target range.\"\n    return score, fb"}, {"type": "code", "name": "Inclusion-Terms Presence Check", "description": "Check presence of key inclusion terms relevant to the review focus.", "weight": 0.9, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        if output.ext.lower().endswith('.pdf'):\n            text = context.files.read_pdf_text(output.id) or \"\"\n        else:\n            text = context.files.read_docx_text(output.id) or \"\"\n    except Exception:\n        return 0.0\n    t = text.lower()\n    terms = [\n        'adherence',\n        'self-efficacy',\n        'hypertension',\n        'high blood pressure',\n        'older adults',\n        'medication adherence'\n    ]\n    found = 0\n    for term in terms:\n        if term in t:\n            found += 1\n        else:\n            # try hyphen/space variants\n            term_var = term.replace('-', ' ').replace('  ', ' ')\n            if term_var in t:\n                found += 1\n    ratio = found / len(terms)\n    return ratio * 0.9"}, {"type": "code", "name": "References Count Heuristic (\u226430)", "description": "Heuristically count references in the References section and check they do not exceed 30 and are not too few.", "weight": 1.2, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document.\"\n    try:\n        if output.ext.lower().endswith('.pdf'):\n            text = context.files.read_pdf_text(output.id) or ''\n        else:\n            text = context.files.read_docx_text(output.id) or ''\n    except Exception as e:\n        return 0.0, f\"Read error: {e}\"\n\n    tl = text.lower()\n    idx = tl.rfind('references')\n    if idx == -1:\n        return 0.0, \"No References section found.\"\n    refs_text = text[idx:]\n    # Count lines that look like references: contain a 4-digit year or DOI/URL\n    lines = [ln.strip() for ln in refs_text.splitlines() if ln.strip()]\n    ref_lines = 0\n    year_re = re.compile(r'(19\\d{2}|20\\d{2}|202\\d)')\n    for ln in lines:\n        if year_re.search(ln) or 'doi.org' in ln.lower() or 'http' in ln.lower():\n            ref_lines += 1\n    # Heuristic: references often span multiple lines; approximate unique items\n    # Collapse consecutive lines into items by blank-line boundaries already removed; fall back to counting numerals\n    approx_refs = ref_lines\n    # Score: full if 10\u201330, partial if 5\u20139 or 31\u201335, else 0\n    if 10 <= approx_refs <= 30:\n        return 1.2, f\"Approx. {approx_refs} references within limit.\"\n    elif 5 <= approx_refs < 10 or 31 <= approx_refs <= 35:\n        return 0.6, f\"Approx. {approx_refs} references marginal relative to limit.\"\n    else:\n        return 0.0, f\"Approx. {approx_refs} references outside acceptable bounds.\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Clinical Utility", "description": "Holistic assessment of writing quality, critical appraisal, and clinical usefulness for practice improvement.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Organization, Clarity, and Professional Tone", "description": "Evaluate coherence, flow, clarity, and professional academic tone with minimal grammatical issues.", "weight": 2.0, "judge_prompt": "Assess the document\u2019s overall writing quality: logical flow across sections, clear topic sentences and transitions, professional/academic tone, minimal grammatical or typographical errors, and readability appropriate for clinicians.\n\nScoring (0.0\u20132.0):\n- 2.0: Clear, well-structured, professional writing; minimal errors.\n- 1.2: Generally clear; minor issues.\n- 0.6: Noticeable issues affecting readability.\n- 0.0: Disorganized, unprofessional, or hard to follow.", "expectation": "Clear, professional, and well-structured prose suitable for a clinical audience."}, {"type": "llm_judge", "name": "Clinical Applicability and Actionability", "description": "Judge how effectively the review translates evidence into practical strategies for managing older adults with hypertension in a clinical setting.", "weight": 2.0, "judge_prompt": "Evaluate the practical value for clinicians: Does the review translate findings into actionable strategies or implications for education and practice (e.g., adherence counseling, multidisciplinary coordination, technology-enabled monitoring, addressing psychosocial barriers)? Are recommendations feasible and clearly linked to cited evidence?\n\nScoring (0.0\u20132.0):\n- 2.0: Clear, evidence-linked, feasible strategies for clinical practice.\n- 1.2: Some practical guidance with limited linkage.\n- 0.6: Vague or generic suggestions.\n- 0.0: No practical implications.", "expectation": "Concrete, evidence-linked strategies that can inform clinical practice."}, {"type": "llm_judge", "name": "Critical Appraisal and Identification of Gaps", "description": "Assess depth of critique, balance of strengths/limitations, and articulation of gaps and future research needs.", "weight": 2.0, "judge_prompt": "Assess whether the review goes beyond summary to appraise study quality and limitations; balances strengths and limitations; and clearly articulates gaps specific to adherence in older adults and proposes focused future research directions.\n\nScoring (0.0\u20132.0):\n- 2.0: Thoughtful critique with clear, specific gaps and future research agenda.\n- 1.2: Some critique and gaps but not fully developed.\n- 0.6: Minimal, generic critique.\n- 0.0: No critical appraisal or gaps.", "expectation": "Balanced critique with specific gaps and actionable future research ideas."}, {"type": "llm_judge", "name": "Citation and Formatting Consistency", "description": "Evaluate consistency of citation style and professional presentation of references and in-text citations.", "weight": 2.0, "judge_prompt": "Review in-text citations and the References section for consistency (e.g., APA/AMA style), completeness (authors, year, title, source), and professionalism (consistent formatting, hanging indents or equivalent, no obvious duplicates). Minor style deviations are acceptable; focus on overall consistency and professionalism.\n\nScoring (0.0\u20132.0):\n- 2.0: Consistent and professional throughout.\n- 1.2: Mostly consistent with minor issues.\n- 0.6: Noticeable inconsistencies but usable.\n- 0.0: Poor or missing citation formatting.", "expectation": "Consistent, professional citation formatting suitable for academic work."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "41f6ef59-88c9-4b2c-bcc7-9ceb88422f48", "rubric": {"category_name": "Health Care Admin \u2013 Declined Payments Outreach Kit", "rationale": "Task Type = Mixed (Pattern C): requires an Excel tracking spreadsheet plus a DOCX email template. Stage 1 uses LLM-only gate to enforce exact file/structure so later checks are trivial. Stage 2 mixes light code checks (deterministic structure/content sanity) with heavier LLM verification (interactive Excel features, email content compliance, cross-consistency). Stage 3 uses LLM for holistic quality: professionalism, usability, clarity, and Zendesk macro readiness.", "max_total_score": 18.0, "stages": [{"name": "Stage 1 \u2013 Shape Enforcement Gate", "description": "Gate: Verify BOTH required artifacts exist with the mandated structure so verification is possible.", "is_required": true, "max_points": 2.0, "min_score_to_pass": 1.6, "rules": [{"type": "llm_judge", "name": "Spreadsheet Structural Requirements", "description": "Output must include a one-tab Excel workbook with the specified sheet name and columns, including a test row.", "weight": 1.0, "judge_prompt": "You are evaluating whether the candidate produced the required Excel tracking spreadsheet with the exact structural shape (not checking calculation quality). Use the rendered spreadsheet view.\n\nPass criteria (be flexible with capitalization/spacing but strict on presence):\n- File exists and is an Excel workbook (.xlsx preferred). A single visible tab/sheet is present.\n- Sheet name is \u201cJune 2025 Declined Payments Outreach\u201d (minor variations like spacing/case acceptable; if clearly equivalent, accept).\n- The sheet includes a clearly labeled table with columns covering ALL of the following fields:\n  1) Third Decline Date (a date column)\n  2) Patient Name\n  3) Date of Birth (DOB)\n  4) Email\n  5) Subscription Type\n  6) Emailed? (re: third declined payment)\n  7) Responded?\n  8) Updated Payment Method?\n- There is at least one \u201ctest user\u201d example row filled in to demonstrate how to use the tracker.\n- The sheet is intended for efficient data entry (filters/frozen header/validation may exist, but in Stage 1 only confirm structural presence, not functionality).\n\nScoring:\n- 1.0: Excel file with one tab named as required (or trivially equivalent), all 8 fields present as distinct columns, and a clearly labeled test user/example row.\n- 0.7: Excel file present with one tab and most fields (6\u20137 of 8) plus a test row; or all fields but minor sheet name variation.\n- 0.4: Excel file present but missing multiple required fields OR missing the test row OR wrong/missing sheet name; still clearly the intended tracker.\n- 0.0: No Excel file OR multiple unrelated tabs OR not a tracker matching the described purpose.\n\nOnly evaluate structure and presence; do not assess interactive features here.", "expectation": "A one-tab Excel file named as specified, with the 8 required columns and a test user row."}, {"type": "llm_judge", "name": "Email Template Document Structure", "description": "Output must include a DOCX (or PDF) with an email template that contains all required elements in a copy-pasteable format.", "weight": 1.0, "judge_prompt": "You are evaluating whether the candidate produced the required email template document with the necessary elements, focusing on presence/structure (not style quality). Use the rendered document view.\n\nRequired structure/elements (flexible wording is fine):\n- File is a DOCX or PDF containing a patient-facing email template that can be copy-pasted.\n- Contains a clear subject line appropriate to declined payments.\n- States that the patient\u2019s payment method on file has declined for a third time.\n- Warns that if it declines a fourth time, their subscription will be canceled and they will not receive their medication refill.\n- Provides step-by-step instructions for updating payment method in patient portal, specifically including these steps in order: log in -> go to settings -> go to billing -> click update payment method -> click save after entering new card.\n- Requests the patient to reply to confirm they updated their payment method.\n\nScoring:\n- 1.0: Valid DOCX/PDF containing an email template with all required elements above present and clearly identifiable (subject + third decline + fourth-cancel/no-refill warning + the 5 ordered steps + reply request).\n- 0.7: Valid DOCX/PDF with most elements (missing one minor component such as an exact step label or the reply request), but still clearly the intended template.\n- 0.4: Valid DOCX/PDF but missing multiple required elements; still recognizable as the intended email.\n- 0.0: No DOCX/PDF email template or completely unrelated content.\n\nOnly check presence and structural completeness, not tone or writing quality here.", "expectation": "A Word/PDF email template containing the required warning and the exact step sequence for updating payment method, plus a reply request."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness and Functional Verification", "description": "Now verify correctness and functionality given the enforced shape. Mix of code (deterministic) and LLM (nuanced checks).", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Spreadsheet Columns and Sheet Sanity (Deterministic)", "description": "Verify the spreadsheet exists, the target sheet is present (flexible match), and required columns are detected via fuzzy matching.", "weight": 0.7, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef _normalize(s):\n    if s is None:\n        return \"\"\n    s = str(s).lower()\n    s = re.sub(r\"[^a-z0-9]+\", \" \", s)\n    return re.sub(r\"\\s+\", \" \", s).strip()\n\ndef _find_spreadsheet(context):\n    # Prefer last task outputs; pick first spreadsheet\n    outs = context.get_all_outputs() or []\n    for r in outs[::-1]:\n        try:\n            if getattr(r, 'is_spreadsheet', False):\n                return r\n        except Exception:\n            continue\n    return None\n\ndef _read_target_df(context, res):\n    # Try to find the desired sheet name or close variant\n    path = context.files.get_path(res.id)\n    try:\n        xls = pd.ExcelFile(path)\n        sheet_names = xls.sheet_names\n        # target tokens\n        target = \"june 2025 declined payments outreach\"\n        norm_sheets = {sn: _normalize(sn) for sn in sheet_names}\n        best = None\n        for sn, ns in norm_sheets.items():\n            if ns == _normalize(target):\n                best = sn\n                break\n        if best is None:\n            # fallback: contains key tokens\n            for sn, ns in norm_sheets.items():\n                if all(tok in ns for tok in [\"june\", \"2025\", \"declined\", \"payment\", \"outreach\"]):\n                    best = sn\n                    break\n        if best is None and sheet_names:\n            best = sheet_names[0]\n        df = pd.read_excel(path, sheet_name=best)\n        return df, best\n    except Exception:\n        try:\n            # CSV fallback if provided\n            df = context.files.read_csv(res.id)\n            return df, None\n        except Exception:\n            return None, None\n\ndef evaluate(workflow, context):\n    res = _find_spreadsheet(context)\n    if not res:\n        return 0.0, \"No spreadsheet resource found\"\n    df, sheet = _read_target_df(context, res)\n    if df is None or df.empty:\n        return 0.0, \"Spreadsheet unreadable or empty\"\n\n    cols = [ _normalize(c) for c in list(df.columns) ]\n\n    # Define required column matchers (flexible synonyms)\n    reqs = {\n        'third decline date': [r'decline date', r'3rd decline', r'third decline', r'payment decline date', r'third payment decline'],\n        'patient name': [r'name', r'patient name', r'full name'],\n        'date of birth': [r'dob', r'date of birth', r'birth date'],\n        'email': [r'email', r'e mail'],\n        'subscription type': [r'subscription', r'plan', r'subscription type', r'plan type'],\n        'emailed flag': [r'emailed', r'email sent', r'contacted', r'emailed re third decline'],\n        'responded flag': [r'responded', r'reply', r'replied'],\n        'updated payment flag': [r'updated payment', r'payment updated', r'updated payment method', r'payment method updated', r'billing updated']\n    }\n\n    def has_req(patterns):\n        for p in patterns:\n            p_norm = _normalize(p)\n            for c in cols:\n                if all(tok in c for tok in p_norm.split()):\n                    return True\n        return False\n\n    found = {}\n    for k, pats in reqs.items():\n        found[k] = has_req(pats)\n\n    match_count = sum(1 for v in found.values() if v)\n    total = len(reqs)\n    ratio = match_count / total if total else 0.0\n\n    # Feedback: list missing\n    missing = [k for k, v in found.items() if not v]\n    fb = f\"Columns found: {match_count}/{total}. Missing: {', '.join(missing) if missing else 'None'}. Sheet: {sheet or 'N/A'}.\"\n\n    return max(0.0, min(1.0, ratio)), fb"}, {"type": "code", "name": "Test User Row and Plan Values", "description": "Verify a clearly marked 'test' example row exists and Subscription Type values align to Plan A/B/C.", "weight": 0.8, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef _normalize(s):\n    if s is None:\n        return \"\"\n    s = str(s).lower()\n    s = re.sub(r\"[^a-z0-9]+\", \" \", s)\n    return re.sub(r\"\\s+\", \" \", s).strip()\n\ndef _find_spreadsheet(context):\n    outs = context.get_all_outputs() or []\n    for r in outs[::-1]:\n        try:\n            if getattr(r, 'is_spreadsheet', False):\n                return r\n        except Exception:\n            continue\n    return None\n\ndef _read_df(context, res):\n    try:\n        path = context.files.get_path(res.id)\n        try:\n            # Prefer first/only sheet\n            xls = pd.ExcelFile(path)\n            sheet = xls.sheet_names[0]\n            return pd.read_excel(path, sheet_name=sheet)\n        except Exception:\n            return context.files.read_csv(res.id)\n    except Exception:\n        return None\n\ndef evaluate(workflow, context):\n    res = _find_spreadsheet(context)\n    if not res:\n        return 0.0, \"No spreadsheet resource found\"\n    df = _read_df(context, res)\n    if df is None or df.empty:\n        return 0.0, \"Spreadsheet unreadable or empty\"\n\n    # Detect test row by searching for 'test' in any cell\n    try:\n        df_str = df.astype(str)\n        has_test = df_str.applymap(lambda x: 'test' in str(x).lower()).any().any()\n    except Exception:\n        has_test = False\n\n    # Find subscription column\n    cols_norm = [ _normalize(c) for c in df.columns ]\n    sub_idx = None\n    for i, c in enumerate(cols_norm):\n        if any(tok in c for tok in [\"subscription\", \"plan\"]):\n            sub_idx = i\n            break\n\n    plan_ok = False\n    if sub_idx is not None:\n        try:\n            vals = df.iloc[:, sub_idx].dropna().astype(str).str.strip().str.lower().unique().tolist()\n            allowed = {\"plan a\", \"plan b\", \"plan c\"}\n            # If any row uses an allowed value (e.g., test row), consider acceptable\n            plan_ok = any(v in allowed for v in vals)\n        except Exception:\n            plan_ok = False\n\n    score_parts = 0\n    if has_test:\n        score_parts += 1\n    if plan_ok:\n        score_parts += 1\n\n    score = score_parts / 2.0\n    fb = f\"Test row present: {has_test}. Subscription value acceptable: {plan_ok}.\"\n    return max(0.0, min(1.0, score)), fb"}, {"type": "llm_judge", "name": "Excel Interactivity and Field Types", "description": "Verify the spreadsheet uses data-entry accelerators: dropdown for Plan A/B/C; checkboxes or yes/no validations for Emailed/Responded/Updated; date formatting for Third Decline Date; includes filters/frozen header.", "weight": 3.0, "judge_prompt": "Open the spreadsheet and inspect its interactivity and field types.\nCheck for the following (flexible implementations acceptable):\n- Subscription Type uses a dropdown/pop-up menu constrained to exactly: Plan A, Plan B, Plan C.\n- Emailed?, Responded?, Updated Payment Method? are implemented as checkboxes OR dropdowns with Yes/No (or similar) values.\n- Third Decline Date column is formatted as a date (entries display as dates, not free text).\n- The list is easy to use (e.g., table filters or header row filters visible; header row frozen or clearly demarcated). Do not require every enhancement; judge practicality.\n- A single tab named for June 2025 is used (carry-over check; minor name variation acceptable).\n\nScoring:\n- 3.0: Dropdown for plan with exactly A/B/C present; yes/no checkboxes or dropdowns for status fields; date formatting applied; and basic usability features (filters/frozen header) clearly visible.\n- 2.0: Most items present (e.g., correct plan dropdown and status checkboxes) but missing one convenience feature or minor formatting misses.\n- 1.0: Spreadsheet exists but little/no interactivity (free text everywhere); still recognizable.\n- 0.0: Not a usable tracker or wrong file.", "expectation": "Functional data validation and checkbox/yes-no controls, date formatting, and basic usability (filters/frozen header)."}, {"type": "llm_judge", "name": "Email Content Compliance (Policy-Critical)", "description": "Verify the template contains the required warnings and exact step flow for updating payment method, plus a reply request.", "weight": 2.5, "judge_prompt": "Open the DOCX/PDF email template. Check for the following content elements (wording can vary but intent must be clear):\n- Subject line appropriate to declined payments.\n- States the payment method declined for a third time.\n- Warns that a fourth decline will result in subscription cancellation and no medication refill.\n- Provides clear, ordered steps specifically including: log in -> go to settings -> go to billing -> click update payment method -> click save after entering the new card.\n- Requests the patient reply to confirm they updated their payment method.\n\nScoring:\n- 2.5: All five elements present and unambiguous.\n- 1.7: Missing or weakening one minor element (e.g., reply request is implied but not explicit, or step wording is slightly compressed but order is intact).\n- 0.8: Multiple elements missing/unclear but still obviously the right template.\n- 0.0: Not present or unrelated.", "expectation": "An email template that includes the third-decline notice, fourth-decline cancellation/no-refill warning, the exact 5-step flow, and a reply request."}, {"type": "llm_judge", "name": "Cross-Consistency: Tracker Fields vs. Email Actions", "description": "Ensure the spreadsheet fields align with what the email asks patients to do and what staff need to track.", "weight": 1.0, "judge_prompt": "Consider both artifacts together (spreadsheet + email template):\n- The email asks patients to update payment method and to reply; the spreadsheet should include fields to track \u201cEmailed?\u201d, \u201cResponded?\u201d, and \u201cUpdated Payment Method?\u201d.\n- The email references the third decline; the spreadsheet includes a \u201cThird Decline Date\u201d.\n- Subscription reference in email context fits available plans (A/B/C) represented in the spreadsheet\u2019s Subscription Type.\n\nScoring:\n- 1.0: Clear one-to-one alignment between email actions and spreadsheet fields; nothing critical missing.\n- 0.5: Mostly aligned but one tracking field or reference is missing/misaligned.\n- 0.0: Poor alignment; fields in the tracker don\u2019t support what the email requests.", "expectation": "Strong alignment: the tracker fields map directly to the email\u2019s requested actions and context."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Professional Quality and Readiness", "description": "Holistic quality assessment for tone, usability, and operational readiness in a healthcare setting.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Tone and Patient-Centered Communication", "description": "Assess the email\u2019s tone for clarity, empathy, and appropriateness in a healthcare context.", "weight": 2.0, "judge_prompt": "Evaluate the email template for professional, empathetic, and clear tone suitable for a healthcare setting:\n- Polite, non-accusatory language; avoids shaming.\n- Clear plain-language explanations; concise.\n- Includes a reasonable support/assistance line (e.g., contact info or how to get help if issues persist).\n- Avoids unnecessary PHI; uses placeholders instead of embedding sensitive details.\n\nScoring: 2.0 excellent; 1.3 good with minor issues; 0.7 fair but needs edits; 0.0 poor/unprofessional.", "expectation": "Empathetic, concise, and professional email with clear help options and no unnecessary PHI."}, {"type": "llm_judge", "name": "Spreadsheet Usability & Layout Quality", "description": "Assess formatting that improves speed and accuracy: headers, widths, filters, freeze panes, data types, brief instructions.", "weight": 2.0, "judge_prompt": "Evaluate the spreadsheet for real-world usability:\n- Clear header row, readable column widths, and consistent date format for Third Decline Date.\n- Filters enabled or table format applied; freeze top row if applicable.\n- Brief on-sheet guidance (e.g., a note for how to mark Emailed/Responded/Updated) is a plus.\n- Visual cues (e.g., light shading for header, data validation prompts) helpful but not mandatory.\n\nScoring: 2.0 excellent; 1.3 good; 0.7 passable; 0.0 poor.", "expectation": "Well-formatted tracker with filters/frozen header and consistent date formatting."}, {"type": "llm_judge", "name": "Zendesk Macro Readiness", "description": "Assess whether the email is copy-paste friendly and macro-ready with placeholders and a clean structure.", "weight": 2.0, "judge_prompt": "Judge whether the email template is ready to be copied into Zendesk as a macro:\n- Contains a concise, clear subject line.\n- Uses placeholders (e.g., {{FirstName}}, {{DOB}}) where appropriate; no odd formatting that breaks in plain text.\n- Uses a numbered/bulleted list for the steps (scannable), and a clear CTA to reply.\n- Includes a professional signature block (clinic name/contact) suitable for reuse.\n\nScoring: 2.0 excellent; 1.3 good; 0.7 needs tweaks; 0.0 not macro-ready.", "expectation": "Subject line, placeholders, list formatting, clear CTA, and a reusable signature block."}, {"type": "llm_judge", "name": "Clarity, Accessibility, and Compliance Polish", "description": "Assess overall clarity, scannability, and basic compliance sensibilities (plain language, accessible formatting).", "weight": 2.0, "judge_prompt": "Evaluate across both artifacts for clarity and accessibility:\n- Plain language and logical flow; numbered steps easy to follow.\n- Minimal jargon; avoids walls of text.\n- Avoids embedding sensitive PHI; includes only necessary info/placeholders.\n- Any links (if present) are descriptive; instructions stand alone without requiring links.\n\nScoring: 2.0 excellent; 1.3 good; 0.7 fair; 0.0 poor.", "expectation": "Clear, accessible materials that respect privacy and are easy to execute without extra guidance."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "60221cd0-686e-4a08-985e-d9bb2fa18501", "rubric": {"category_name": "Virginia 2025 Elections Article (Journalism)", "rationale": "Pattern B (Document). The deliverable is a PDF news article aimed at informing Virginia voters about 2025 elections, with a gubernatorial focus. Stage 1 uses an LLM-only gate to enforce the exact output shape (PDF, article structure, presence of dates, voter methods, link at end, no images). Stage 2 mixes code and LLM checks to verify measurable requirements (word count, URL placement, presence of specific 2025 dates, gubernatorial focus, voter participation methods, neutrality). Code rules are precise but light-weight, while LLM rules carry most points for nuanced verification. Stage 3 evaluates overall professional quality for the audience and clarity.", "max_total_score": 12.0, "stages": [{"name": "Stage 1 \u2014 PDF Article Structure Gate", "description": "LLM-only gate ensuring the output is a properly structured PDF news article enabling downstream verification.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.0, "rules": [{"type": "llm_judge", "name": "PDF Article Structure Requirements (Gate)", "description": "Check that the candidate output is a single PDF article with all required structural elements present.", "weight": 3.0, "judge_prompt": "You are evaluating whether the submission satisfies the STRUCTURAL requirements of the task. Only check presence/format, not factual correctness or writing quality.\n\nRequirements to verify (be flexible with exact phrasing of headers):\n1) File format: A valid PDF document (not DOCX, not plain text, not images embedded as separate files).\n2) Length: Article body is between 300 and 500 words (approximate count is fine).\n3) Headline: A clear title/headline appears near the top.\n4) Gubernatorial focus and scope: The piece is an election-themed article centered on Virginia\u2019s 2025 gubernatorial election while acknowledging upcoming 2025 elections.\n5) Upcoming race dates: The article visibly includes specific upcoming election dates (e.g., primary in June 2025 and general election in November 2025). A short list or inline mentions are acceptable.\n6) Voter participation methods: A clearly labeled portion (or clearly separated paragraph) describes how Virginians can vote (e.g., early in-person, mail/absentee, Election Day in-person, etc.). Flexible on section names like \u201cHow to Vote,\u201d \u201cVoting Options,\u201d or similar.\n7) Official website link at the end: The final line (or closing block) contains the Virginia Department of Elections website URL: www.elections.virginia.gov\n8) No pictures/images: The PDF should not contain images or photos.\n9) Neutral presentation: The article reads as informational rather than promotional (surface-level check only; full neutrality reviewed later).\n\nScoring (structure only):\n- 1.0: All required elements present, PDF format, and link placed at the end, with no images.\n- 0.7: One minor element missing or imperfect (e.g., link present but not strictly last line, or voter methods present but not clearly labeled).\n- 0.4: Multiple structural elements missing/unclear (e.g., lacks clear headline or dates or methods) but still a PDF article.\n- 0.0: Not a PDF OR not an article (e.g., wrong format) OR obviously missing most structural requirements.\n\nReturn only a score according to the above. Do not assess factual accuracy or prose quality here.", "expectation": "A one-page PDF news-style article with a headline, 300\u2013500 words, explicit 2025 election dates, clear voting methods section, and the official website URL as the last line; no images."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness Verification", "description": "Verify measurable and requirement-level correctness now that the structure is enforced.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 3.0, "rules": [{"type": "code", "name": "Word Count Within 300\u2013500", "description": "Checks that the article body text is between 300 and 500 words (inclusive), with slight partial credit if very close.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float in [0,1] (normalized score)\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        pass\n    if not text:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            pass\n    if not text:\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            pass\n    if not text:\n        return 0.0\n\n    words = re.findall(r\"\\b\\w[\\w'-]*\\b\", text)\n    n = len(words)\n    if 300 <= n <= 500:\n        return 1.0\n    # partial credit if close\n    if 280 <= n <= 520:\n        return 0.5\n    return 0.0"}, {"type": "code", "name": "Official Website URL Present at the End", "description": "Checks for presence of www.elections.virginia.gov and that it appears as the final non-empty line. Partial credit if present but not last line.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float in [0,1] (normalized score)\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        pass\n    if not text:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            pass\n    if not text:\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            pass\n    if not text:\n        return 0.0\n\n    if \"elections.virginia.gov\" not in text:\n        return 0.0\n\n    lines = [ln.strip() for ln in text.splitlines()]\n    # Find the last non-empty line\n    last_non_empty = next((ln for ln in reversed(lines) if ln), \"\")\n    if re.search(r\"\\b(elections\\.virginia\\.gov)\\b\", last_non_empty, re.IGNORECASE):\n        return 1.0\n    # Partial credit if present anywhere but not last line\n    return 0.5"}, {"type": "llm_judge", "name": "Specific Upcoming 2025 Election Dates Included", "description": "Checks that the article clearly lists or references specific 2025 election dates, including at least one in June and one in November.", "weight": 1.5, "judge_prompt": "Evaluate whether the article explicitly includes specific upcoming election dates for 2025. Look for at least two distinct dates, ideally covering both June (e.g., primary) and November (e.g., general election). Dates may be in prose or a short list. Do not fact-check the dates against external sources; only verify that concrete 2025 dates are present and clearly tied to Virginia elections.\n\nScoring:\n- 1.0: At least two specific 2025 dates are present, including one in June and one in November, clearly identified as Virginia election events.\n- 0.5: Specific dates are present but only one month is covered (e.g., June or November but not both) OR dates are ambiguous.\n- 0.0: No specific 2025 election dates are provided.\nReturn only the score.\n", "expectation": "Clear mention of specific June and November 2025 Virginia election dates."}, {"type": "llm_judge", "name": "Voter Participation Methods Coverage", "description": "Checks that the article clearly explains how Virginians can vote (e.g., early in-person, absentee/mail, Election Day in-person, ballot drop-off), with enough clarity for readers to understand options.", "weight": 1.5, "judge_prompt": "Evaluate whether the article clearly describes methods for Virginia voter participation. Accept equivalent wording. Look for multiple options such as: early in-person voting, absentee/mail voting, Election Day in-person voting, and (if mentioned) drop-off/return options and key steps (e.g., deadlines, ID requirements mentions). Do not fact-check; focus on presence and clarity.\n\nScoring:\n- 1.0: At least two distinct Virginia voting methods are explained clearly, with practical clarity for a general reader.\n- 0.5: Methods are mentioned but minimally or unclearly (e.g., just listed without basic explanation).\n- 0.0: Methods are missing or too vague to be useful.\nReturn only the score.\n", "expectation": "A concise, clear explanation of at least two voting options in Virginia."}, {"type": "llm_judge", "name": "Gubernatorial Focus and Context", "description": "Verifies that the gubernatorial race is a central focus of the piece while situating it within the broader 2025 Virginia elections calendar.", "weight": 1.0, "judge_prompt": "Assess whether the article centers the 2025 Virginia gubernatorial election while still acknowledging the broader sequence of 2025 elections. The governor\u2019s race should be highlighted, referenced early or prominently, and connected to the dates and voter information.\n\nScoring:\n- 1.0: Gubernatorial race is clearly the central focus and well-situated in the 2025 elections context.\n- 0.5: Gubernatorial race is mentioned but not clearly central, or context is weak.\n- 0.0: Little to no gubernatorial focus.\nReturn only the score.\n", "expectation": "Clear gubernatorial focus with contextual links to other 2025 election events."}, {"type": "llm_judge", "name": "Neutral, Non-Advocacy Tone", "description": "Ensures the piece remains impartial and avoids endorsing candidates or positions.", "weight": 1.0, "judge_prompt": "Evaluate the tone for neutrality. The article should be informational and avoid opinionated language, endorsements, or criticism of parties/candidates. It should not express preferences about platforms or urge support for any side.\n\nScoring:\n- 1.0: Fully neutral, no advocacy language.\n- 0.5: Mostly neutral with minor lapses in subjective wording.\n- 0.0: Contains clear advocacy, endorsements, or disparagement.\nReturn only the score.\n", "expectation": "Impartial, news-style tone free of advocacy."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality Assessment", "description": "Holistic evaluation of clarity, usefulness, and journalistic polish for Virginia voters.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity and Organization", "description": "Assesses structure, logical flow, and scannability for a general audience.", "weight": 0.75, "judge_prompt": "Judge the clarity and organization of the article. It should have a clear headline, an effective lead, logical paragraphs, and a coherent flow from dates to voter options, concluding with the official link. Consider whether a busy voter can quickly grasp the key information.\n\nScoring:\n- 1.0: Very clear and well-organized; easy to scan and understand.\n- 0.5: Adequately organized with minor issues.\n- 0.0: Disorganized or confusing.\nReturn only the score.\n", "expectation": "Clear headline, effective lead, logical structure, scannable layout."}, {"type": "llm_judge", "name": "Readability and Journalistic Style", "description": "Evaluates readability, grammar, and a neutral, professional newswriting style appropriate for a local paper.", "weight": 0.75, "judge_prompt": "Assess readability, grammar, and professional tone consistent with a local newspaper. Writing should be concise, active, and free of errors and jargon. Do not re-check strict neutrality (already evaluated), but ensure the style supports objective reporting.\n\nScoring:\n- 1.0: Polished, clean, and professional.\n- 0.5: Minor issues but generally readable.\n- 0.0: Frequent errors or unprofessional style.\nReturn only the score.\n", "expectation": "Clean copy with professional, objective newswriting style."}, {"type": "llm_judge", "name": "Audience Relevance (Virginia Voters) and Source Transparency", "description": "Assesses whether the piece serves Virginia voters with locally-relevant info and clearly points to the Virginia Department of Elections as the authoritative source.", "weight": 0.75, "judge_prompt": "Evaluate if the article is clearly targeted to Virginia voters (references to Virginia-specific processes, offices, and timelines) and whether it transparently cites the Virginia Department of Elections as the authoritative source (beyond merely placing the URL, e.g., referencing the site in-body as the source of dates/methods or directing readers to it for details).\n\nScoring:\n- 1.0: Strong Virginia relevance and explicit source transparency in-body.\n- 0.5: Virginia relevance present but weak source transparency, or vice versa.\n- 0.0: Generic or unclear audience targeting and no clear source attribution.\nReturn only the score.\n", "expectation": "Virginia-focused guidance with explicit attribution to the Department of Elections."}, {"type": "llm_judge", "name": "Completeness and Practical Usefulness", "description": "Checks that the article succinctly covers who/what/when/where/how, enabling readers to take the next step to vote.", "weight": 0.75, "judge_prompt": "Assess whether the article compactly covers the essential questions (who/what/when/where/how) so readers can act. Look for a clear outline of key dates, voting options, and a concrete next step (e.g., check registration, find polling place) pointing to the official site.\n\nScoring:\n- 1.0: Complete and actionable without fluff.\n- 0.5: Mostly complete but missing one actionable element.\n- 0.0: Lacks essential elements or is not actionable.\nReturn only the score.\n", "expectation": "Succinct, actionable guidance that enables next steps for voters."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "11593a50-734d-4449-b5b4-f8986a133fd8", "rubric": {"category_name": "Real Estate Buyer Packet: Massapequa Park Shortlist + Map", "rationale": "Mixed task: document deliverables with embedded structured data. Stage 1 is a strict LLM-only shape gate enforcing two specific PDFs with exact sections/columns/photos. Stage 2 mixes light code checks (bounds/consistency from extracted PDF text) with heavier LLM verification (criteria compliance and cross-referencing list \u2194 map). Stage 3 is holistic quality for presentation and buyer usefulness. Code rules have ~5x less weight than LLM rules on average, and Stage 1 uses only LLM judges per the self-documenting philosophy.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 SHAPE ENFORCEMENT Gate (Document Structure)", "description": "Confirm deliverables exist with the exact structure needed for verification: a 2-page property shortlist PDF with specified columns and a photo per property, and a 1-page map PDF with pins for all properties.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.2, "rules": [{"type": "llm_judge", "name": "Property Shortlist PDF Structure (2 pages, table + photos)", "description": "Verify there is a single two-page PDF that contains the shortlist of less than 15 homes, each with a photo and a tabular row including the required columns.", "weight": 4.5, "judge_prompt": "You are evaluating the STRUCTURE ONLY of the candidate outputs (not correctness of the data). Look for a two-page PDF that serves as the buyer shortlist. Be flexible if file names differ, but confirm these visible structural elements:\n\nFormat requirements:\n- The shortlist is a PDF (not Word/Excel/markdown) and consists of exactly 2 pages.\n- Contains fewer than 15 properties total.\n- Each property has a clearly visible photo (thumbnail or larger) next to or above/below its details.\n\nRequired table columns for each property row (headers can be synonyms; order can vary):\n- Status (e.g., Active)\n- Type (e.g., Single Family Home / House)\n- Price\n- List Date\n- Address (should appear complete enough to identify the home)\n- Bed count\n- Bathroom count\n- Square footage (interior)\n- Lot size\n- Year built\n- $/sqft (explicitly present)\n\nLayout expectations:\n- A tabular/grid layout where each listing presents these fields in a consistent schema.\n- Headers are visible for the required columns.\n- Photos are present for each listing (not just for a few).\n\nScoring (structure only):\n- 4.5: A valid 2-page PDF shortlist is present; fewer than 15 properties; clear table/grid; headers covering all required columns (or obvious close synonyms); each property has a photo.\n- 3.5\u20134.0: Two-page PDF with table/grid and photos; missing 1\u20132 minor columns or headers are present but one column is visibly merged/ambiguous.\n- 2.0\u20133.0: Two-page PDF exists with property rows but table/headers are unclear or 3\u20134 required columns are missing; or some listings lack photos.\n- 0.5\u20131.5: A PDF exists but not clearly two pages, not a table/grid, or photos largely missing; or more than 14 properties presented.\n- 0.0: Shortlist not provided as PDF, or obviously not two pages, or not a property list.\n\nOnly assess structure and presence of elements, not whether the data meets buyer criteria.", "expectation": "A clean, two-page PDF shortlist with <15 rows, each with a photo and the exact set of required columns present as headers."}, {"type": "llm_judge", "name": "Map PDF Presence and Pinning", "description": "Verify there is a separate one-page PDF map showing all shortlisted properties pinned.", "weight": 1.5, "judge_prompt": "Check for a separate one-page PDF that is a map with all shortlisted properties pinned. Be flexible with filename. Confirm the following STRUCTURAL elements:\n- The file is a PDF and appears to be a map (street/area map or similar), 1 page only.\n- Multiple pins/markers appear corresponding to the properties in the shortlist.\n- Some labeling is present (addresses or short labels) or a legend/list referencing the properties; labels can be nearby or in a side panel.\n\nScoring:\n- 1.5: A distinct 1-page PDF map exists with multiple property pins and clear labels/legend linking pins to the shortlist.\n- 1.0: One-page map PDF with pins but labels/legend are minimal or partially unclear.\n- 0.5: A map-like PDF exists but unclear if pins correspond to shortlist properties.\n- 0.0: No one-page map PDF found or it\u2019s clearly not a property pin map.\n\nOnly check presence/structure, not geographic accuracy or completeness vs. the shortlist.", "expectation": "A one-page PDF map with pins for each property and basic labels or a legend."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 VERIFICATION (Correctness & Consistency)", "description": "Now that the shape is correct, verify the content meets buyer criteria and internal consistency. Mix of lightweight code checks and LLM cross-referencing.", "is_required": false, "max_points": 8.4, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Listing Count and Location Heuristic Check", "description": "From the shortlist PDF text, heuristically confirm there are \u226415 listings and that they are in Massapequa Park, NY 11762 (look for city/ZIP tokens).", "weight": 0.7, "code": "import re\nimport json\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs()\n        if not outputs:\n            return 0.0, \"No outputs found\"\n        # Choose likely shortlist PDF as the document with the most extractable text\n        shortlist_text = \"\"\n        shortlist_id = None\n        for r in outputs:\n            if getattr(r, 'is_document', False):\n                try:\n                    t = context.files.read_pdf_text(r.id)\n                except Exception:\n                    t = \"\"\n                if len(t) > len(shortlist_text):\n                    shortlist_text = t\n                    shortlist_id = r.id\n        if not shortlist_text:\n            return 0.0, \"No readable PDF text\"\n        text = shortlist_text\n        # Heuristic listing count: count price occurrences as proxy\n        price_hits = re.findall(r\"\\$\\s?\\d[\\d,]*\", text)\n        est_count = len(price_hits)\n        # Cap overcount by grouping near-duplicate prices: rough unique set\n        unique_prices = set()\n        for m in price_hits:\n            n = re.sub(r\"[^0-9]\", \"\", m)\n            if n:\n                unique_prices.add(n)\n        approx_count = min(len(unique_prices), est_count)\n        # Location tokens\n        has_city = bool(re.search(r\"Massapequa Park\", text, re.I))\n        has_zip = bool(re.search(r\"11762\", text))\n        # Score components\n        score = 0.0\n        # Count <= 15 gets partial credit; if we can't estimate, be lenient\n        if approx_count == 0 and est_count == 0:\n            score += 0.2  # can't detect; don't punish harshly\n        else:\n            score += 0.35 if approx_count <= 15 else 0.0\n        # City/ZIP presence\n        if has_city or has_zip:\n            score += 0.35\n        return min(score, 0.7), f\"approx_count={approx_count}, has_city={has_city}, has_zip={has_zip}\"\n    except Exception as e:\n        return 0.0, f\"error: {e}\""}, {"type": "code", "name": "Price vs $/sqft Consistency", "description": "Check that computed price/sqft roughly matches stated $/sqft for several entries when sqft present.", "weight": 0.7, "code": "import re\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs()\n        if not outputs:\n            return 0.0, \"No outputs\"\n        shortlist_text = \"\"\n        for r in outputs:\n            if getattr(r, 'is_document', False):\n                try:\n                    t = context.files.read_pdf_text(r.id)\n                except Exception:\n                    t = \"\"\n                if len(t) > len(shortlist_text):\n                    shortlist_text = t\n        if not shortlist_text:\n            return 0.0, \"No readable text\"\n        txt = shortlist_text\n        # Extract price values (first N reasonable prices under $1.6M)\n        prices = [int(re.sub(r\"[^0-9]\", \"\", m)) for m in re.findall(r\"\\$\\s?([0-9][0-9,]{2,})\", txt)]\n        prices = [p for p in prices if p < 1600000]\n        # Extract sqft and $/sqft\n        sqft = [int(re.sub(r\"[^0-9]\", \"\", m)) for m in re.findall(r\"(\\d[\\d,]{2,})\\s*(?:sq\\s?ft|sf|sqft)\", txt, flags=re.I)]\n        per_sqft = [int(re.sub(r\"[^0-9]\", \"\", m)) for m in re.findall(r\"\\$\\s*([0-9][\\d,]*)\\s*/\\s*(?:sq\\s?ft|sf|sqft)\", txt, flags=re.I)]\n        # Use up to first 5 triplets if available\n        k = min(len(prices), len(sqft), len(per_sqft), 5)\n        if k == 0:\n            return 0.2, \"Insufficient data to check; giving small credit\"\n        matches = 0\n        for i in range(k):\n            p = prices[i]\n            s = sqft[i] if sqft[i] else None\n            ps = per_sqft[i]\n            if s and s > 0:\n                calc = round(p / s)\n                # allow 12% tolerance due to rounding/fees\n                if abs(calc - ps) <= max(20, int(0.12 * ps)):\n                    matches += 1\n        frac = matches / k\n        return 0.7 * frac, f\"checked={k}, matches={matches}\"\n    except Exception as e:\n        return 0.0, f\"error: {e}\""}, {"type": "llm_judge", "name": "Criteria Compliance (Type, Status, Beds/Baths, Price, Location)", "description": "Judge whether every property on the shortlist meets the buyer\u2019s stated criteria: single family, Active (not pending/contingent/coming soon), 4\u20136 beds, \u22652 baths, price < $1.5M, located in Massapequa Park, NY 11762.", "weight": 3.5, "judge_prompt": "Review the two-page shortlist PDF and evaluate for CRITERIA COMPLIANCE. Using the visible data in the table and listing rows, check:\n- Property type is single family (or equivalent phrasing) for every listing.\n- Status is Active for every listing; no Pending, Contingent, Under Contract, Coming Soon, Temporarily Off Market, etc.\n- Bedrooms between 4 and 6 inclusive; Bathrooms at least 2.\n- Price strictly under $1,500,000.\n- Location is Massapequa Park, NY 11762 (city and/or ZIP visible). Minor formatting variants are acceptable.\n\nScoring:\n- 3.5: All listings clearly satisfy all criteria.\n- 2.5\u20133.0: One minor exception or ambiguous field among the set (e.g., one listing with unclear type or missing bath count but appears compliant).\n- 1.0\u20132.0: Multiple issues or 1\u20132 listings clearly outside criteria.\n- 0.0\u20130.5: Many listings violate criteria or insufficient information to tell for most listings.\n\nUse only the content visible in the shortlist PDF; do not assume or search externally. Provide brief rationale in feedback.", "expectation": "Every listing obviously matches all constraints based on the table values."}, {"type": "llm_judge", "name": "List \u2194 Map Cross-Reference", "description": "Ensure the map shows pins corresponding to the same set of addresses on the shortlist and no obvious extras/omissions.", "weight": 3.5, "judge_prompt": "Compare the two-page shortlist PDF to the one-page map PDF. Assess whether the map\u2019s pins correspond to the properties on the shortlist (addresses or labels). Consider:\n- All listed properties appear represented by a map pin (count should match within 0\u20131).\n- Labels or legend clearly tie the pins to the listings (either address snippets, numbers, or a legend).\n- No obvious extra pins for addresses not on the shortlist.\n\nScoring:\n- 3.5: Pin count matches the list and labels/legend make the correspondence clear; no obvious extras.\n- 2.5\u20133.0: Correspondence is mostly clear; off by one pin or a couple labels are weak.\n- 1.0\u20132.0: Several pins unlabelled/unclear; possible missing/extra pins.\n- 0.0\u20130.5: Map seems unrelated or largely inconsistent with shortlist.\n\nJudge visually; do not enforce geographic accuracy beyond correspondence with the list.", "expectation": "Pins/labels on the map clearly cover all properties from the shortlist with no extras."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 QUALITY (Professionalism and Buyer Usefulness)", "description": "Holistic assessment of presentation quality, clarity, and buyer usefulness beyond raw correctness.", "is_required": false, "max_points": 5.6, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Presentation and Layout Professionalism", "description": "Evaluate visual polish: clean layout, consistent typography, clear headers, aligned columns, and overall readability of the 2-page shortlist.", "weight": 2.0, "judge_prompt": "Assess the 2-page shortlist PDF for professional presentation:\n- Clear title/header indicating area and date.\n- Consistent typography, spacing, and alignment across rows/columns.\n- Column headers are distinct and repeated if needed on page 2.\n- No cluttered or overlapping elements; photos and text are balanced and readable.\n\nScoring:\n- 2.0: Highly professional, clean, consistent, and easy to read.\n- 1.3\u20131.7: Generally professional with minor alignment or spacing issues.\n- 0.6\u20131.1: Readable but noticeable inconsistencies or clutter.\n- 0.0\u20130.5: Unprofessional or hard to read layout.", "expectation": "A polished, well-structured brochure-style shortlist with consistent formatting."}, {"type": "llm_judge", "name": "Photo Quality and Consistency", "description": "Judge whether each property includes a clear photo and that imagery is consistently sized/placed and helpful.", "weight": 1.6, "judge_prompt": "Review photos on the shortlist:\n- Each listing should have at least one photo.\n- Images should be clear enough to recognize the property.\n- Reasonably consistent sizing/placement across listings.\n- No obviously distorted or pixelated images.\n\nScoring:\n- 1.6: All listings have clear, consistently presented photos.\n- 1.0\u20131.3: Minor inconsistencies; 1\u20132 weaker images.\n- 0.4\u20130.8: Several poor/missing images.\n- 0.0\u20130.3: Imagery largely missing or unusable.", "expectation": "Clear, consistent property photos throughout."}, {"type": "llm_judge", "name": "Buyer Usefulness (Prioritization and Clarity)", "description": "Evaluate whether the packet is helpful for a 2-day tour: clarity of key fields, list dates, and any cues that aid prioritization and scheduling.", "weight": 1.5, "judge_prompt": "Assess how useful this packet would be to buyers scheduling a 2-day tour:\n- Key decision fields (price, beds/baths, SQFT, lot, year) are legible and easy to scan.\n- List date is included and readable to help identify new/fresh listings.\n- Ordering or grouping is logical (e.g., by price, date, or neighborhood) even if not explicitly stated.\n- Any brief notes or cues (optional) that aid route planning/time management.\n\nScoring:\n- 1.5: Highly useful and scannable; facilitates quick decisions and tour planning.\n- 1.0\u20131.3: Generally useful with minor friction.\n- 0.4\u20130.8: Some usefulness but hard to scan or missing key cues.\n- 0.0\u20130.3: Not useful for quick decision-making.", "expectation": "A scannable shortlist that makes planning a 2-day viewing straightforward."}, {"type": "llm_judge", "name": "Source Transparency", "description": "Reward inclusion of MLS numbers or source references that increase trust and traceability.", "weight": 0.5, "judge_prompt": "Check whether the shortlist provides MLS numbers, listing IDs, or source references/links (e.g., mention of MLSLI.com) that would help the buyer or agent verify details.\n\nScoring:\n- 0.5: Clear MLS IDs or source references for all or most listings.\n- 0.3\u20130.4: Some IDs or partial sourcing present.\n- 0.1\u20130.2: Minimal/isolated sourcing.\n- 0.0: No sourcing indicators at all.\n\nThis is a quality bonus and not strictly required by the task.", "expectation": "MLS IDs or source references are present for most listings."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "3940b7e7-ec4f-4cea-8097-3ab4cfdcaaa6", "rubric": {"category_name": "Aerospace CFD Flow-Simulation Report (X-Wing Assembly)", "rationale": "Task Type: Mixed (documents with embedded data/analysis) \u2192 Pattern C. Output Format: PDF (preferred) or DOCX; we will extract text to verify. Stage 1 uses LLM-only to enforce strict structure and tables so verification is possible. Stage 2 mixes light code checks (numeric/units/bounds parsed from PDF text) with LLM checks for technical consistency; code rules carry ~5x less weight than LLM rules. Stage 3 is holistic quality. The rubric forces the agent to prove correctness via self-documenting structure: mandated sections, two required tables with specific rows/columns, and explicit metrics and forces.", "max_total_score": 15.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate", "description": "Document format and structural completeness for a CFD flow-simulation report supporting a design review. LLM-only checks to ensure the file is a professional PDF/DOCX with the exact sections and required tables/figures so later verification is trivial.", "is_required": true, "max_points": 2.0, "min_score_to_pass": 1.5, "rules": [{"type": "llm_judge", "name": "Structured CFD Report Requirements (Gate)", "description": "Verify that the output is a PDF/DOCX with all mandated sections, tables, and at least one figure. Only check presence/structure (not correctness).", "weight": 2.0, "judge_prompt": "You are evaluating whether the candidate output is in the correct STRUCTURE to enable verification. Inspect the primary output file.\n\nFormat requirements:\n- Must be a PDF or DOCX (PDF strongly preferred).\n- At least 2 pages.\n- Clearly formatted with headings and tables.\n\nRequired section headers (allow minor variations/plurals/case):\n1) \"Objective\"\n2) \"Simulation environment\" (should describe computational domain and mesh, and may include solver/turbulence model, material properties)\n3) \"Boundary conditions\"\n4) \"Results\"\n5) \"Discussion\"\n6) \"Conclusion\"\n\nRequired figures/tables:\nA) At least one figure with a caption referencing the computational domain and/or mesh (e.g., \"Computational domain\", \"Domain and mesh\", or similar). A simple schematic/render is acceptable.\n\nB) Table 1: Global goal values\n- A clearly titled table (e.g., \"Global goals\", \"Global goal values\").\n- Columns should include labels akin to: [Metric/Goal | Value | Units] (allow equivalent synonyms like Definition/Units).\n- Rows must include (allowing flexible phrasing):\n  \u2022 Lift force (N)\n  \u2022 Drag force (N)\n  \u2022 Peak axial velocity (m/s)\n  \u2022 Either maximum turbulence intensity (%) or turbulent kinetic energy (TKE, m^2/s^2)\n\nC) Table 2: Field variable extrema (min/max)\n- A clearly titled table (e.g., \"Field variable extremes\", \"Min/Max values\").\n- Columns should include: [Variable | Min | Max | Units] (allow equivalent synonyms).\n- The variables listed must include at least SIX items drawn from: density, pressure (static or total), temperature, velocity components (Ux, Uy, Uz or Velocity X/Y/Z or Vx/Vy/Vz), Mach number, relative pressure.\n\nScoring rubric (return a single numeric score based on completeness):\n- 2.0: Valid PDF/DOCX with all 6 required sections, the figure on domain/mesh, both required tables, and the required rows/variables present as specified.\n- 1.7: Valid format with all sections, both tables present, but figure missing OR one required row/variable missing.\n- 1.5: Valid format with all sections present but missing exactly one of: the figure OR one of the two tables; or missing 2 required rows/variables across the tables.\n- 1.0: Valid format but only 4\u20135 of the required section headers present OR both tables present but clearly missing multiple required rows/variables.\n- 0.0: Not PDF/DOCX, less than 2 pages, or missing multiple required sections (\u22643) or both tables.\n\nOnly check PRESENCE/STRUCTURE. Do not judge correctness of numbers, units, or engineering logic.", "expectation": "A concise, professional PDF with the six specific sections, a domain/mesh figure, a Global Goals table (lift, drag, peak axial velocity, and TI% or TKE), and a Field Min/Max table (\u22656 variables from the specified list)."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness & Consistency)", "description": "Now that the shape is correct, verify content correctness and consistency. Use light code checks for numeric/units sanity and LLM checks for technical coherence and traceability.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Global Goals: numeric presence and units", "description": "Parse PDF/DOCX text to find numeric values and reasonable units for lift, drag, peak axial velocity, and either turbulence intensity or TKE.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n    if not text:\n        return 0.0\n    t = text\n    tl = t.lower()\n\n    # Helper to extract first numeric with unit after a label\n    num_pat = r\"([-+]?\\d+(?:\\.\\d+)?(?:[eE][+-]?\\d+)?)\"\n    scored = 0\n    total = 4  # lift, drag, peak axial velocity, and TI or TKE\n\n    # Lift (N, kN, MN)\n    lift_found = False\n    m = re.search(r\"\\b(lift(?:\\s+force)?)\\b[^\\n]{0,50}?\" + num_pat + r\"\\s*(MN|kN|N)\\b\", t, re.IGNORECASE)\n    if m:\n        lift_found = True\n        scored += 1\n\n    # Drag (N, kN, MN)\n    drag_found = False\n    m = re.search(r\"\\b(drag(?:\\s+force)?)\\b[^\\n]{0,50}?\" + num_pat + r\"\\s*(MN|kN|N)\\b\", t, re.IGNORECASE)\n    if m:\n        drag_found = True\n        scored += 1\n\n    # Peak axial velocity (m/s)\n    pav_found = False\n    m = re.search(r\"(peak\\s+axial\\s+velocity|axial\\s+velocity\\s*(peak|max))[^\\n]{0,50}?\" + num_pat + r\"\\s*(m/s|m\\s*/\\s*s|m\\s*s\\^-1)\\b\", t, re.IGNORECASE)\n    if m:\n        pav_found = True\n        scored += 1\n\n    # Turbulence intensity (%) OR TKE (m2/s2)\n    ti_or_tke_found = False\n    m_ti = re.search(r\"(turbulence\\s+intensity|\\bTI\\b)[^\\n]{0,50}?\" + num_pat + r\"\\s*%\\b\", t, re.IGNORECASE)\n    m_tke = re.search(r\"(turbulent\\s+kinetic\\s+energy|\\bTKE\\b)[^\\n]{0,50}?\" + num_pat + r\"\\s*(m\\^?2\\s*/\\s*s\\^?2|m2/s2)\\b\", t, re.IGNORECASE)\n    if m_ti or m_tke:\n        ti_or_tke_found = True\n        scored += 1\n\n    return max(0.0, min(1.0, scored / total))"}, {"type": "code", "name": "Field variable extrema: presence and consistency", "description": "Detect min/max pairs for key variables (density, pressure, temperature, velocity components, Mach, relative pressure). Check that max \u2265 min where both are found.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    # Read text\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n    if not text:\n        return 0.0\n\n    num = r\"([-+]?\\d+(?:\\.\\d+)?(?:[eE][+-]?\\d+)?)\"\n\n    def find_min_max_for(label_pat, txt):\n        # look within 120 chars window around label for min and max\n        flags = re.IGNORECASE | re.DOTALL\n        # Allow tables where Min and Max may appear before/after\n        # Strategy: find label, then search nearby for Min value and Max value\n        for m in re.finditer(label_pat, txt, flags):\n            start = max(0, m.start() - 120)\n            end = min(len(txt), m.end() + 120)\n            window = txt[start:end]\n            min_m = re.search(r\"min[^-\\d]{0,20}\" + num, window, flags)\n            max_m = re.search(r\"max[^-\\d]{0,20}\" + num, window, flags)\n            if min_m and max_m:\n                try:\n                    vmin = float(min_m.group(1))\n                    vmax = float(max_m.group(1))\n                    return vmin, vmax\n                except Exception:\n                    continue\n        return None\n\n    variables = [\n        r\"(density|\\brho\\b)\",\n        r\"((static\\s+)?pressure(?!\\s*coefficient)|\\bp\\b)\",\n        r\"(relative\\s+pressure|gauge\\s+pressure)\",\n        r\"(temperature|\\btemp\\b|\\bT\\b)\",\n        r\"(mach\\s*number|\\bmach\\b)\",\n        r\"(u[_\\s]*x|v[_\\s]*x|velocity\\s*x|\\bvx\\b)\",\n        r\"(u[_\\s]*y|v[_\\s]*y|velocity\\s*y|\\bvy\\b)\",\n        r\"(u[_\\s]*z|v[_\\s]*z|velocity\\s*z|\\bvz\\b)\",\n    ]\n\n    found = 0\n    consistent = 0\n    for pat in variables:\n        mm = find_min_max_for(pat, text)\n        if mm is not None:\n            found += 1\n            vmin, vmax = mm\n            if vmax >= vmin:\n                consistent += 1\n\n    if found == 0:\n        return 0.0\n    # reward both presence and consistency; require at least some coverage\n    coverage = min(1.0, found / 6.0)  # aim for \u22656 variables per Stage 1\n    consistency_score = consistent / max(found, 1)\n    score = 0.5 * coverage + 0.5 * consistency_score\n    return max(0.0, min(1.0, score))"}, {"type": "code", "name": "Unit sanity across variables", "description": "Check that common units are present and paired with relevant variables somewhere in the document: pressure\u2192Pa/kPa/MPa, density\u2192kg/m^3, velocity\u2192m/s, temperature\u2192K/\u00b0C, TI\u2192%.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n    if not text:\n        return 0.0\n\n    tl = text.lower()\n\n    def near(term_pat, unit_pat):\n        # term within 60 chars of unit\n        for m in re.finditer(term_pat, text, flags=re.IGNORECASE):\n            start = max(0, m.start() - 60)\n            end = min(len(text), m.end() + 60)\n            if re.search(unit_pat, text[start:end], flags=re.IGNORECASE):\n                return True\n        return False\n\n    checks = 0\n    ok = 0\n\n    # Pressure units\n    checks += 1\n    if near(r\"pressure|relative\\s+pressure|static\\s+pressure\", r\"\\b(Pa|kPa|MPa)\\b\"):\n        ok += 1\n\n    # Density units\n    checks += 1\n    if near(r\"density|\\brho\\b\", r\"kg\\s*/\\s*m\\^?3|kg\\s*m\\^-3\"):\n        ok += 1\n\n    # Velocity units\n    checks += 1\n    if near(r\"velocity|u[_\\s]*[xyz]|v[_\\s]*[xyz]|\\bv[xyz]\\b\", r\"m\\s*/\\s*s|m/s|m\\s*s\\^-1\"):\n        ok += 1\n\n    # Temperature units\n    checks += 1\n    if near(r\"temperature|\\btemp\\b|\\bT\\b\", r\"\\bK\\b|\u00b0C|deg\\s*C\"):\n        ok += 1\n\n    # Turbulence intensity as %\n    checks += 1\n    if near(r\"turbulence\\s+intensity|\\bTI\\b\", r\"%\"):\n        ok += 1\n\n    return max(0.0, min(1.0, ok / max(checks, 1)))"}, {"type": "code", "name": "Lift/Drag plausibility (signs and ratio)", "description": "If lift and drag are present with units, ensure values are positive magnitudes and the L/D ratio is within a broad plausible range for an airfoil/wing (0.01\u2013500).", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n    if not text:\n        return 0.0\n\n    num = r\"([-+]?\\d+(?:\\.\\d+)?(?:[eE][+-]?\\d+)?)\"\n    m_lift = re.search(r\"\\b(lift(?:\\s+force)?)\\b[^\\n]{0,50}?\" + num + r\"\\s*(MN|kN|N)\\b\", text, re.IGNORECASE)\n    m_drag = re.search(r\"\\b(drag(?:\\s+force)?)\\b[^\\n]{0,50}?\" + num + r\"\\s*(MN|kN|N)\\b\", text, re.IGNORECASE)\n\n    def to_newtons(val_str, unit):\n        try:\n            v = float(val_str)\n        except Exception:\n            return None\n        if unit.lower() == 'n':\n            return v\n        if unit.lower() == 'kn':\n            return v * 1e3\n        if unit.lower() == 'mn':\n            return v * 1e6\n        return None\n\n    if not (m_lift and m_drag):\n        return 0.0\n\n    L = to_newtons(m_lift.group(2), m_lift.group(3))\n    D = to_newtons(m_drag.group(2), m_drag.group(3))\n    if L is None or D is None:\n        return 0.0\n\n    if L <= 0 or D <= 0:\n        return 0.0\n\n    ratio = L / D if D != 0 else 0\n    if 0.01 <= ratio <= 500:\n        return 1.0\n    else:\n        return 0.5"}, {"type": "llm_judge", "name": "BCs and Simulation Environment Completeness", "description": "Check that the Simulation environment and Boundary conditions sections specify domain, mesh, materials, solver/turbulence model, inlets/outlets/walls, and convergence/goal criteria; ensure they are coherent with the Results.", "weight": 1.6, "judge_prompt": "Evaluate the technical completeness and coherence of the Simulation environment and Boundary conditions sections relative to the Results. Specifically verify:\n- Computational domain and mesh are described (domain extents/geometry reference to STEP model, element type, mesh size/refinement strategy).\n- Material properties (e.g., air density/viscosity and temperature model) are stated or referenced.\n- Boundary conditions identify inlets, outlets, walls/symmetry, and any rotating/moving frames if applicable; include values (e.g., velocity, pressure) and reference frames.\n- Solver/turbulence model and convergence/goal criteria are stated (e.g., residual targets, force/goal flattening, mass balance).\n- Internal consistency: BCs and models make sense with reported metrics (e.g., Mach and velocity magnitudes align; subsonic vs. transonic consistent with shocks, etc.).\n\nScoring:\n- 1.6: All bullets present and consistent with Results.\n- 1.2: Minor omissions (one element thin) but overall consistent.\n- 0.8: Multiple omissions or vague values, but still usable.\n- 0.4: Major gaps or inconsistencies.\n- 0.0: Missing most content or incoherent.", "expectation": "Clear, internally consistent environment and BCs with solver/model/convergence info adequate to reproduce the run and interpret results."}, {"type": "llm_judge", "name": "Forces and Key Metrics Coverage", "description": "Verify that Results summarize lift/drag forces with units and report peak axial velocity, maximum turbulence intensity or TKE, and other key quantities; ensure the Global Goals and Min/Max tables match the narrative.", "weight": 1.6, "judge_prompt": "Check the Results section and required tables:\n- Lift and drag forces reported with numeric values and units (N/kN/MN), and referenced in a Global Goals table.\n- Peak axial velocity and either maximum turbulence intensity (%) or TKE (m^2/s^2) are clearly reported.\n- The Global Goals table includes at least the above items; the Min/Max table covers at least six variables (density, pressure, temperature, velocity components, Mach, relative pressure) with plausible units.\n- Tables and narrative are consistent (numbers and units match within rounding; no contradictions).\n\nScoring:\n- 1.6: All items present; tables and text agree.\n- 1.2: One minor mismatch or one missing minor metric.\n- 0.8: Multiple minor issues or one significant omission.\n- 0.4: Major omissions or inconsistencies.\n- 0.0: Metrics largely absent.", "expectation": "Results include forces and key flow metrics with units, and tables accurately reflect the narrative."}, {"type": "llm_judge", "name": "Aerodynamic Interpretation and Phenomena", "description": "Assess whether Discussion correctly interprets implications: lift vs drag tradeoffs, shock formation (if applicable), separation, turbulence, and ties them to the metrics and flow conditions.", "weight": 1.6, "judge_prompt": "Evaluate the Discussion for aerodynamic interpretation quality:\n- Connects forces (L/D) to performance and constraints.\n- Reasonably discusses shock formation (transonic/supersonic indications via Mach), flow separation (pressure gradients, turbulence intensity/TKE), and turbulence characteristics.\n- Uses reported metrics to justify claims (e.g., high TI correlating with separation zones; peak Mach aligning with shocks).\n- No obvious contradictions (e.g., claiming laminar while high TI; claiming subsonic everywhere while Mach>1 appears).\n\nScoring:\n- 1.6: Accurate, well-justified, and relevant interpretation.\n- 1.2: Generally correct with minor gaps.\n- 0.8: Superficial or partially incorrect.\n- 0.4: Mostly incorrect or unsubstantiated.\n- 0.0: Missing or contradictory.", "expectation": "A coherent, technically sound interpretation of flow phenomena tied to the presented metrics."}, {"type": "llm_judge", "name": "Recommendations Traceability and Next Steps", "description": "Assess whether the Conclusion provides preliminary, actionable recommendations linked to the findings and suitable for guiding further optimization.", "weight": 1.6, "judge_prompt": "Review the Conclusion for recommendations:\n- At least 3 concrete, actionable recommendations (e.g., adjust airfoil camber/sweep, modify LE radius or twist, refine mesh near shocks/TE, adjust BCs or turbulence model, add control surfaces, change incidence, or rerun with tighter convergence).\n- Each recommendation is traceable to specific observed results (e.g., reduce separation indicated by high TI near flap; mitigate shock intensity shown by peak Mach and pressure jump).\n- Prioritization or next steps (e.g., run DOE, mesh sensitivity, new BCs, parametric study) are specified.\n\nScoring:\n- 1.6: Clear, traceable, prioritized recommendations.\n- 1.2: Actionable but weak traceability or prioritization.\n- 0.8: Vague or generic suggestions.\n- 0.4: Minimal value.\n- 0.0: None given.", "expectation": "Recommendations clearly map to findings and propose concrete next steps for optimization."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Holistic Quality Assessment", "description": "Professionalism, clarity, and usefulness for an internal design review briefing.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity, Structure, and Conciseness", "description": "Evaluate readability, logical flow, and brevity appropriate for an internal design review.", "weight": 1.25, "judge_prompt": "Assess overall clarity and organization:\n- Logical flow aligns with the required section order; headings are clear.\n- Writing is concise yet complete; minimal redundancy; key points easy to find.\n- Transitions from setup\u2192results\u2192implications\u2192actions are smooth.\n\nScore 1.25 for excellent, 1.0 good, 0.75 fair, 0.5 poor, 0.0 very poor.", "expectation": "Crisp, well-structured narrative tailored to a design review."}, {"type": "llm_judge", "name": "Professional Presentation and Formatting", "description": "Check tables/figures labeling, units consistency, captions, and overall professional polish.", "weight": 1.25, "judge_prompt": "Evaluate presentation:\n- Tables and figures are properly titled, numbered, and referenced in text.\n- Units are consistent throughout; symbols/abbreviations defined (e.g., TI, TKE).\n- Page numbers, fonts, and spacing look professional.\n\nScore 1.25 excellent, 1.0 good, 0.75 fair, 0.5 poor, 0.0 very poor.", "expectation": "Professional formatting with consistent units and properly referenced visuals."}, {"type": "llm_judge", "name": "Audience Appropriateness and Context", "description": "Ensure the report targets the internal design team: necessary detail without overwhelming, context on CAD/CFD inputs, limitations/assumptions stated.", "weight": 1.25, "judge_prompt": "Assess suitability for an internal aerospace design team:\n- Provides sufficient context on the STEP/CAD geometry and CFD dataset used.\n- States key assumptions/limitations (e.g., steady vs. transient, compressibility, turbulence model limits, mesh adequacy).\n- Balances technical depth with brevity.\n\nScore 1.25 excellent, 1.0 good, 0.75 fair, 0.5 poor, 0.0 very poor.", "expectation": "Right level of technical depth and transparency about assumptions for a design review."}, {"type": "llm_judge", "name": "Actionability and Strategic Value", "description": "Judge whether the report would effectively guide immediate next design/analysis actions and decision-making.", "weight": 1.25, "judge_prompt": "Evaluate strategic value:\n- Clear takeaways tied to performance goals (e.g., L/D, shock control, separation management).\n- Concrete next steps and risk/uncertainty notes (e.g., mesh independence pending, sensitivity to BCs).\n- Would this brief enable the design team to act?\n\nScore 1.25 excellent, 1.0 good, 0.75 fair, 0.5 poor, 0.0 very poor.", "expectation": "Actionable insights that enable prioritization of design iterations and analyses."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "3f821c2d-ab97-46ec-a0fb-b8f73c2682bc", "rubric": {"category_name": "Omnichannel Stock & Sales Flow (Wholesale Trade - First-Line Supervisors of Non-Retail Sales Workers)", "rationale": "Task Type = Analytical (Pattern A). The deliverable is an Excel workbook with channel-level stock and sales flows and an omni total, across months Aug\u2013Jan, with LY comparison. Stage 1 is a strict LLM-only gate enforcing the exact workbook/table shape that enables verification. Stage 2 mixes code (deterministic arithmetic/constraint checks on the omni table) and LLM rules (aggregation consistency and plausibility across channels). Stage 3 assesses professional quality, strategic value, and usability for stakeholders. Code rules are kept lightweight and flexible, with most points in LLM judgment as per guidance.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Structured Excel Workbook Gate", "description": "Shape Enforcement: Verify the candidate produced an Excel workbook with the exact, verifiable structure to enable downstream checks. LLM-only gate per instructions.", "is_required": true, "max_points": 5.0, "min_score_to_pass": 3.5, "rules": [{"type": "llm_judge", "name": "Structured Output Format Requirement (Gate)", "description": "Confirm the deliverable is an Excel workbook with the required channel tables, months, rows, side-by-side layout, omni total, and LY comparison in the same shape.", "weight": 5.0, "judge_prompt": "You are evaluating the structure of the candidate's primary output ONLY. Do not judge calculation correctness here. Score purely on presence and structure.\n\nRequirements (be flexible with similar naming):\n- File format: Must be an Excel workbook (.xlsx). If not Excel, score 0.\n- This Season tables: There must be clearly labeled tables for three views: Stores, E-commerce, and Omni (aka Total/All Channels). Accept if these appear on one sheet side-by-side or on a primary sheet with clearly adjacent sections. Omni should be labeled as a total.\n- Columns (months): For each table, columns must include (left-to-right): Aug, Sep, Oct, Nov, Dec, Jan for the Fall season (Aug\u2013Jan). Month labels can be full or abbreviated.\n- Rows (per table): The following rows must be visible and clearly labeled: BOM Inventory $, Retail Sales $, Receipts $, EOM Inventory $, and Turn (Monthly). A Seasonal Turn should also be present (either as a separate row or as a clearly labeled KPI near the table). Turn formatting may be numeric like 3.8x; sales/inventory should be currency-formatted.\n- LY Comparison: Provide LY tables with the SAME shape (same months and row labels) for Stores, E-commerce, and Omni. Headers can be like LY, Last Year, Prior, or 2024.\n- Side-by-side layout: The three channel tables for THIS SEASON should be arranged left-to-right (e.g., Stores | E-commerce | Omni) for easy comparison. LY can be similarly arranged or in a clear adjacent section, but must be formatted the same way as this year for easy visual comparison.\n- Constraints annotation (preferred but not mandatory for full structure): A small header or note somewhere (e.g., a text box or legend) that states: Gross receipt budget (omni) = $675,000; Omni seasonal turn target = 4.0; EOM January (omni) must be \u2264 $200,000; Minimum monthly receipts: Stores \u2265 $10,000; E-commerce \u2265 $6,000.\n\nScoring:\n- 5.0: Excel format AND all three this-season channel tables with required months/rows AND an Omni table AND LY comparison in the same shape AND side-by-side layout is clear. Constraints annotation present.\n- 4.0: Same as 5.0 but missing the constraints annotation (or it's vague), while all tables/structure are correct.\n- 3.0: This-season Stores and E-commerce tables are present with correct rows/columns, but either Omni or LY comparison is missing.\n- 1.5: Only one channel table present or months/rows are clearly incomplete.\n- 0.0: Not an Excel file OR no recognizable tables matching the requested structure.\n\nOnly evaluate presence and structure, not accuracy of numbers or formulas.", "expectation": "A clean .xlsx with three properly structured channel tables for Aug\u2013Jan, an Omni total, and a same-shaped LY comparison, arranged side-by-side for this season."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification of Calculations and Constraints", "description": "Now that the workbook shape is validated, verify arithmetic identities, constraints, and plausibility. Mix of code (deterministic checks on omni) and LLM (aggregation consistency, turn visibility, and constraints by eye).", "is_required": false, "max_points": 9.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Omni Accounting Identity (EOM = BOM + Receipts \u2212 Sales)", "description": "Checks that the omni EOM equals BOM + Receipts \u2212 Sales for Aug\u2013Jan with a small tolerance.", "weight": 0.4, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n    path = context.files.get_path(output.id)\n    try:\n        xf = pd.ExcelFile(path)\n    except Exception as e:\n        return 0.0, f\"Failed to open Excel: {e}\"\n\n    # Pick an omni/total sheet if available; else first sheet\n    sheet_name = None\n    for s in xf.sheet_names:\n        sl = s.lower()\n        if any(k in sl for k in [\"omni\", \"total\", \"summary\", \"all channel\"]):\n            sheet_name = s\n            break\n    if sheet_name is None:\n        sheet_name = xf.sheet_names[0]\n\n    df = pd.read_excel(path, sheet_name=sheet_name, header=None)\n\n    # Helpers\n    def norm(x):\n        try:\n            x = str(x).strip().lower()\n        except Exception:\n            x = \"\"\n        return x\n\n    month_keys = [\n        (\"aug\", [\"aug\", \"august\"]),\n        (\"sep\", [\"sep\", \"sept\", \"september\"]),\n        (\"oct\", [\"oct\", \"october\"]),\n        (\"nov\", [\"nov\", \"november\"]),\n        (\"dec\", [\"dec\", \"december\"]),\n        (\"jan\", [\"jan\", \"january\"]),\n    ]\n\n    def find_month_cols(df):\n        rows_to_scan = min(8, df.shape[0])\n        found = {}\n        for r in range(rows_to_scan):\n            for c in range(df.shape[1]):\n                v = norm(df.iat[r, c])\n                if not v:\n                    continue\n                for key, aliases in month_keys:\n                    if key not in found:\n                        if any(a in v for a in aliases):\n                            found[key] = c\n        # Ensure all 6 months present\n        if all(k in found for k, _ in month_keys):\n            return [found[k] for k, _ in month_keys]\n        return None\n\n    month_cols = find_month_cols(df)\n    if not month_cols:\n        return 0.0, \"Could not locate all month columns Aug\u2013Jan on omni/summary sheet.\"\n\n    def find_row_idx(df, keywords):\n        # Scan top 100 rows and first 8 columns for a cell matching all keywords\n        R = min(100, df.shape[0])\n        C = min(8, df.shape[1])\n        for r in range(R):\n            for c in range(C):\n                v = norm(df.iat[r, c])\n                if v and all(k in v for k in keywords):\n                    return r\n        return None\n\n    def row_series(df, row_idx):\n        vals = []\n        for c in month_cols:\n            vals.append(pd.to_numeric(df.iat[row_idx, c], errors='coerce'))\n        return np.array(vals, dtype=float)\n\n    # Try common label variants\n    r_bom = find_row_idx(df, [\"bom\", \"inventory\"]) or find_row_idx(df, [\"beg\", \"inventory\"]) \n    r_sales = find_row_idx(df, [\"retail\", \"sales\"]) or find_row_idx(df, [\"sales\"]) \n    r_receipts = find_row_idx(df, [\"receipt\"]) \n    r_eom = find_row_idx(df, [\"eom\", \"inventory\"]) or find_row_idx(df, [\"ending\", \"inventory\"]) \n\n    if None in [r_bom, r_sales, r_receipts, r_eom]:\n        return 0.0, \"Missing one or more required rows on omni sheet (BOM, Sales, Receipts, EOM).\"\n\n    bom = row_series(df, r_bom)\n    sales = row_series(df, r_sales)\n    rcpt = row_series(df, r_receipts)\n    eom = row_series(df, r_eom)\n\n    if any(np.isnan(x).any() for x in [bom, sales, rcpt, eom]):\n        return 0.0, \"Non-numeric values detected in required omni rows.\"\n\n    diff = bom + rcpt - sales - eom\n    tol = 1.0  # $1 tolerance\n    ok = np.sum(np.abs(diff) <= tol)\n    score = ok / 6.0\n    return score, f\"Identity satisfied for {ok}/6 months.\""}, {"type": "code", "name": "Omni Receipts vs $675,000 Budget", "description": "Checks if omni receipts total for Aug\u2013Jan is close to the $675,000 budget (within ~1% gets full credit; larger deviations reduce score).", "weight": 0.4, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n    path = context.files.get_path(output.id)\n    try:\n        xf = pd.ExcelFile(path)\n    except Exception as e:\n        return 0.0, f\"Failed to open Excel: {e}\"\n\n    # Pick omni-like sheet or first\n    sheet_name = None\n    for s in xf.sheet_names:\n        sl = s.lower()\n        if any(k in sl for k in [\"omni\", \"total\", \"summary\", \"all channel\"]):\n            sheet_name = s\n            break\n    if sheet_name is None:\n        sheet_name = xf.sheet_names[0]\n\n    df = pd.read_excel(path, sheet_name=sheet_name, header=None)\n\n    def norm(x):\n        try:\n            x = str(x).strip().lower()\n        except Exception:\n            x = \"\"\n        return x\n\n    month_keys = [\n        (\"aug\", [\"aug\", \"august\"]),\n        (\"sep\", [\"sep\", \"sept\", \"september\"]),\n        (\"oct\", [\"oct\", \"october\"]),\n        (\"nov\", [\"nov\", \"november\"]),\n        (\"dec\", [\"dec\", \"december\"]),\n        (\"jan\", [\"jan\", \"january\"]),\n    ]\n\n    def find_month_cols(df):\n        rows_to_scan = min(8, df.shape[0])\n        found = {}\n        for r in range(rows_to_scan):\n            for c in range(df.shape[1]):\n                v = norm(df.iat[r, c])\n                if not v:\n                    continue\n                for key, aliases in month_keys:\n                    if key not in found:\n                        if any(a in v for a in aliases):\n                            found[key] = c\n        if all(k in found for k, _ in month_keys):\n            return [found[k] for k, _ in month_keys]\n        return None\n\n    month_cols = find_month_cols(df)\n    if not month_cols:\n        return 0.0, \"Could not locate all month columns Aug\u2013Jan on omni/summary sheet.\"\n\n    def find_row_idx(df, keywords):\n        R = min(100, df.shape[0])\n        C = min(8, df.shape[1])\n        for r in range(R):\n            for c in range(C):\n                v = norm(df.iat[r, c])\n                if v and all(k in v for k in keywords):\n                    return r\n        return None\n\n    r_receipts = find_row_idx(df, [\"receipt\"]) \n    if r_receipts is None:\n        return 0.0, \"Receipts row not found on omni sheet.\"\n\n    vals = []\n    for c in month_cols:\n        vals.append(pd.to_numeric(df.iat[r_receipts, c], errors='coerce'))\n    rcpt = np.array(vals, dtype=float)\n    if np.isnan(rcpt).any():\n        return 0.0, \"Non-numeric receipts values detected.\"\n\n    total = float(np.nansum(rcpt))\n    budget = 675000.0\n    if budget == 0:\n        return 0.0, \"Budget is zero?\"\n    pct_err = abs(total - budget) / budget\n\n    # Scoring: <=1% error => 1.0; linearly drop to 0 by 5% error\n    if pct_err <= 0.01:\n        score = 1.0\n    elif pct_err >= 0.05:\n        score = 0.0\n    else:\n        score = max(0.0, 1.0 - (pct_err - 0.01) / (0.04))\n    return score, f\"Omni receipts total ${total:,.0f} vs budget ${budget:,.0f} (error {pct_err*100:.2f}%).\""}, {"type": "code", "name": "Omni EOM January \u2264 $200,000", "description": "Checks the omni EOM for January is at or under $200,000 (with a soft penalty if slightly higher).", "weight": 0.3, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n    path = context.files.get_path(output.id)\n    try:\n        xf = pd.ExcelFile(path)\n    except Exception as e:\n        return 0.0, f\"Failed to open Excel: {e}\"\n\n    sheet_name = None\n    for s in xf.sheet_names:\n        sl = s.lower()\n        if any(k in sl for k in [\"omni\", \"total\", \"summary\", \"all channel\"]):\n            sheet_name = s\n            break\n    if sheet_name is None:\n        sheet_name = xf.sheet_names[0]\n\n    df = pd.read_excel(path, sheet_name=sheet_name, header=None)\n\n    def norm(x):\n        try:\n            x = str(x).strip().lower()\n        except Exception:\n            x = \"\"\n        return x\n\n    month_keys = [\n        (\"aug\", [\"aug\", \"august\"]),\n        (\"sep\", [\"sep\", \"sept\", \"september\"]),\n        (\"oct\", [\"oct\", \"october\"]),\n        (\"nov\", [\"nov\", \"november\"]),\n        (\"dec\", [\"dec\", \"december\"]),\n        (\"jan\", [\"jan\", \"january\"]),\n    ]\n\n    def find_month_cols(df):\n        rows_to_scan = min(8, df.shape[0])\n        found = {}\n        for r in range(rows_to_scan):\n            for c in range(df.shape[1]):\n                v = norm(df.iat[r, c])\n                if not v:\n                    continue\n                for key, aliases in month_keys:\n                    if key not in found:\n                        if any(a in v for a in aliases):\n                            found[key] = c\n        if all(k in found for k, _ in month_keys):\n            return [found[k] for k, _ in month_keys]\n        return None\n\n    month_cols = find_month_cols(df)\n    if not month_cols:\n        return 0.0, \"Could not locate Aug\u2013Jan columns on omni sheet.\"\n\n    def find_row_idx(df, keywords):\n        R = min(100, df.shape[0])\n        C = min(8, df.shape[1])\n        for r in range(R):\n            for c in range(C):\n                v = norm(df.iat[r, c])\n                if v and all(k in v for k in keywords):\n                    return r\n        return None\n\n    r_eom = find_row_idx(df, [\"eom\", \"inventory\"]) or find_row_idx(df, [\"ending\", \"inventory\"]) \n    if r_eom is None:\n        return 0.0, \"EOM Inventory row not found on omni sheet.\"\n\n    # January is the 6th month in Aug\u2013Jan sequence\n    jan_col = month_cols[5]\n    jan_val = pd.to_numeric(df.iat[r_eom, jan_col], errors='coerce')\n    if pd.isna(jan_val):\n        return 0.0, \"January EOM value is non-numeric or missing.\"\n\n    cap = 200000.0\n    if jan_val <= cap:\n        return 1.0, f\"Jan EOM ${jan_val:,.0f} within cap.\"\n    # Soft penalty: degrade to 0 by +20% over cap\n    over = (jan_val - cap) / cap\n    if over >= 0.20:\n        return 0.0, f\"Jan EOM ${jan_val:,.0f} exceeds cap by {over*100:.1f}%.\"\n    score = max(0.0, 1.0 - (over / 0.20))\n    return score, f\"Jan EOM ${jan_val:,.0f} exceeds cap; partial credit.\""}, {"type": "code", "name": "Omni Seasonal Turn \u2265 4.0", "description": "Computes omni seasonal turn = Seasonal Sales / (Sum of monthly EOM / 6). Awards full credit at \u22654.0, partial at \u22653.65.", "weight": 0.4, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n    path = context.files.get_path(output.id)\n    try:\n        xf = pd.ExcelFile(path)\n    except Exception as e:\n        return 0.0, f\"Failed to open Excel: {e}\"\n\n    # Pick omni-like sheet\n    sheet_name = None\n    for s in xf.sheet_names:\n        sl = s.lower()\n        if any(k in sl for k in [\"omni\", \"total\", \"summary\", \"all channel\"]):\n            sheet_name = s\n            break\n    if sheet_name is None:\n        sheet_name = xf.sheet_names[0]\n\n    df = pd.read_excel(path, sheet_name=sheet_name, header=None)\n\n    def norm(x):\n        try:\n            x = str(x).strip().lower()\n        except Exception:\n            x = \"\"\n        return x\n\n    month_keys = [\n        (\"aug\", [\"aug\", \"august\"]),\n        (\"sep\", [\"sep\", \"sept\", \"september\"]),\n        (\"oct\", [\"oct\", \"october\"]),\n        (\"nov\", [\"nov\", \"november\"]),\n        (\"dec\", [\"dec\", \"december\"]),\n        (\"jan\", [\"jan\", \"january\"]),\n    ]\n\n    def find_month_cols(df):\n        rows_to_scan = min(8, df.shape[0])\n        found = {}\n        for r in range(rows_to_scan):\n            for c in range(df.shape[1]):\n                v = norm(df.iat[r, c])\n                if not v:\n                    continue\n                for key, aliases in month_keys:\n                    if key not in found:\n                        if any(a in v for a in aliases):\n                            found[key] = c\n        if all(k in found for k, _ in month_keys):\n            return [found[k] for k, _ in month_keys]\n        return None\n\n    month_cols = find_month_cols(df)\n    if not month_cols:\n        return 0.0, \"Could not locate Aug\u2013Jan columns on omni sheet.\"\n\n    def find_row_idx(df, keywords):\n        R = min(100, df.shape[0])\n        C = min(8, df.shape[1])\n        for r in range(R):\n            for c in range(C):\n                v = norm(df.iat[r, c])\n                if v and all(k in v for k in keywords):\n                    return r\n        return None\n\n    r_sales = find_row_idx(df, [\"retail\", \"sales\"]) or find_row_idx(df, [\"sales\"]) \n    r_eom = find_row_idx(df, [\"eom\", \"inventory\"]) or find_row_idx(df, [\"ending\", \"inventory\"]) \n\n    if None in [r_sales, r_eom]:\n        return 0.0, \"Missing Sales or EOM rows on omni sheet.\"\n\n    sales = np.array([pd.to_numeric(df.iat[r_sales, c], errors='coerce') for c in month_cols], dtype=float)\n    eom = np.array([pd.to_numeric(df.iat[r_eom, c], errors='coerce') for c in month_cols], dtype=float)\n\n    if np.isnan(sales).any() or np.isnan(eom).any():\n        return 0.0, \"Non-numeric values in Sales or EOM rows.\"\n\n    seasonal_sales = float(np.nansum(sales))\n    denom = float(np.nansum(eom)) / 6.0 if np.nansum(eom) != 0 else np.nan\n    if denom == 0 or np.isnan(denom):\n        return 0.0, \"Invalid denominator for seasonal turn.\"\n    seasonal_turn = seasonal_sales / denom\n\n    if seasonal_turn >= 4.0:\n        return 1.0, f\"Seasonal Turn {seasonal_turn:.2f} \u2265 4.0.\"\n    if seasonal_turn >= 3.65:\n        return 0.5, f\"Seasonal Turn {seasonal_turn:.2f} \u2265 3.65 but < 4.0.\"\n    return 0.0, f\"Seasonal Turn {seasonal_turn:.2f} < 3.65.\""}, {"type": "llm_judge", "name": "Omni Aggregation Consistency (Channels \u2192 Omni)", "description": "Visually verify that the Omni totals equal the sum of Stores and E-commerce across Sales, Receipts, and Inventory for Aug\u2013Jan, with coherent side-by-side alignment.", "weight": 2.5, "judge_prompt": "Visually inspect the workbook. Check that the Omni/Total (or equivalent) values appear to equal the sum of Stores and E-commerce for each month (Aug\u2013Jan) across key rows: Retail Sales $, Receipts $, EOM Inventory $ (BOM if visible, too). You may use quick mental spot-checks or obvious subtotals. Also check that the three tables are aligned left-to-right for this season so comparisons are straightforward.\n\nScoring:\n- 2.5: Omni matches the sum of Stores + E-comm for all months and all key rows (allowing small rounding differences) and alignment is clear.\n- 1.5: Generally matches with a few minor discrepancies or alignment issues.\n- 0.5: Frequent mismatches or poor alignment make aggregation unclear.\n- 0.0: No visible Omni aggregation or badly mismatched data.", "expectation": "Clear left-to-right tables with Omni values that add up from Stores and E-comm across months for Sales, Receipts, Inventory."}, {"type": "llm_judge", "name": "Constraint Compliance by Eye", "description": "Check visible compliance with key constraints: min monthly receipts by channel, EOM Jan cap, receipts budget magnitude, and seasonal turn target.", "weight": 2.5, "judge_prompt": "Using visual inspection and obvious numbers in the tables:\n- Minimum Receipts per month: Stores \u2265 $10,000; E-commerce \u2265 $6,000 (Aug\u2013Jan). If any month violates this, note it.\n- EOM January (Omni): Should be \u2264 $200,000.\n- Omni receipts budget: Totals around $675,000 for Aug\u2013Jan (within a reasonable proximity, e.g., \u00b11\u20132% is ideal). If a clear total is shown, use it; otherwise, infer from monthly columns.\n- Seasonal Turn (Omni): Should be \u2265 4.0 and be explicitly labeled somewhere (as a row or KPI).\n\nScoring:\n- 2.5: All constraints visibly met (or trivially within rounding), and turn \u2265 4.0 is clearly shown.\n- 1.5: One minor miss (e.g., a single month slightly under min receipts, or small budget variance) but mostly compliant.\n- 0.5: Multiple misses but shows effort toward constraints.\n- 0.0: Major constraint violations (e.g., Jan EOM far above 200k, large budget miss, or turn clearly < 3.65).", "expectation": "Tables show per-month receipts above thresholds, a January EOM under the cap, receipts around $675k total, and an omni seasonal turn at or above 4.0."}, {"type": "llm_judge", "name": "Turn Calculations Presence and Plausibility", "description": "Verify that monthly Turn is present for each table and appears consistent with Sales/Average Inventory; also that a Seasonal Turn is present and aligns with the provided formula.", "weight": 2.5, "judge_prompt": "Confirm that each table (Stores, E-comm, Omni) has a Turn row for each month (Aug\u2013Jan) and that a Seasonal Turn is shown (row or KPI). Do a plausibility spot-check: Turn \u2248 Sales / ((BOM + EOM)/2) for a couple of months, and the Seasonal Turn \u2248 Seasonal Sales / (Sum of EOM / 6). You do not need exact arithmetic; ensure the magnitudes are consistent.\n\nScoring:\n- 2.5: All tables include monthly and seasonal Turn; spot-checks look consistent across multiple months.\n- 1.5: Present but some inconsistencies in a few months or missing in one table.\n- 0.5: Turn rows/seasonal KPI appear incomplete or inconsistent across most months.\n- 0.0: Turn not present or clearly incoherent.", "expectation": "Turn lines exist for all three tables and look consistent with the stated formulas."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism", "description": "Holistic assessment of presentation quality, strategic value, and stakeholder readiness.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Clarity", "description": "Formatting, readability, consistent months/order, number formats, clear headers, and side-by-side comparability.", "weight": 1.5, "judge_prompt": "Assess presentation quality:\n- Clear headers for Stores, E-commerce, and Omni; months in correct order Aug\u2192Jan.\n- Consistent currency formatting for $ rows and sensible formatting for Turn (e.g., 3.9x).\n- Side-by-side layout is tidy with gridlines, bold headers, and spacing that makes comparisons easy.\n- No clutter: unnecessary sheets or extraneous elements do not distract.\n\nScoring: 1.5 = highly professional, consistent, and easy to read; 0.8 = generally clear but with minor formatting gaps; 0.0 = confusing, inconsistent, or poorly formatted.", "expectation": "A clean, well-formatted workbook where comparisons are intuitive and values are easy to read."}, {"type": "llm_judge", "name": "Strategic Inventory Flow and Turn Optimization", "description": "Evaluate whether receipts phasing supports sales while controlling inventory and improving turn vs LY.", "weight": 1.5, "judge_prompt": "Judge the strategy reflected in the flow:\n- Receipts phasing relative to sales peaks (e.g., heavier receipts ahead of high-sales months), without causing excessive EOM spikes.\n- Inventory profile that trends down toward January to respect the cap and reduce carryover.\n- Evident improvement in turn vs LY (e.g., higher monthly turns or a stronger seasonal turn), while supporting the fixed sales plan.\n\nScoring: 1.5 = thoughtful phasing supports sales and controls inventory with visibly improved turns; 0.8 = reasonable phasing with minor inefficiencies; 0.0 = poor phasing leading to overstock or weak turns.", "expectation": "Receipts timing appears deliberate, keeping EOM lean while supporting demand; turn materially improves vs LY."}, {"type": "llm_judge", "name": "Stakeholder Readiness and Self-Documentation", "description": "Assesses whether the model is ready for operational use with a concise KPI summary and light documentation of assumptions/constraints.", "weight": 1.5, "judge_prompt": "Look for stakeholder-ready elements:\n- A small KPI summary box: omni seasonal receipts vs $675k, omni seasonal turn vs 4.0 target and LY, Jan EOM vs $200k, any notable channel highlights.\n- Brief notes on assumptions (e.g., receipt phasing logic, minimum receipt constraints) or a simple legend.\n- Clear cues enabling store/e-comm teams to act (e.g., highlighting key months for allocation/replenishment).\n\nScoring: 1.5 = concise KPI summary plus brief notes that make the file self-explanatory; 0.8 = some summary elements present; 0.0 = lacks summaries/notes, making the file hard to adopt operationally.", "expectation": "A concise summary and brief notes that make the model easily actionable for teams."}, {"type": "llm_judge", "name": "LY Comparison Insightfulness", "description": "Evaluates whether the LY comparison is used meaningfully (beyond just present) to signal improvements or tradeoffs.", "weight": 1.5, "judge_prompt": "Beyond simply showing LY tables, does the workbook make the comparison useful?\n- Same shape/tables allow easy visual comparison.\n- Optional: simple variance indicators or callouts highlighting changes (e.g., higher turn, lower EOM in peak months).\n- Narrative cues (labels/notes) explaining where strategy differs from LY and why.\n\nScoring: 1.5 = LY comparison clearly illuminates improvements/tradeoffs; 0.8 = basic comparison with limited insight; 0.0 = LY present but offers no meaningful insight or is hard to compare.", "expectation": "LY data is leveraged to highlight improvements in turn and inventory positioning versus last year."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "f9f82549-fdde-4462-aff8-e70fba5b8c66", "rubric": {"category_name": "Retail LP: Employee Theft Investigation Artifacts", "rationale": "This rubric enforces a self-documenting, two-artifact deliverable: a PDF flowchart for general procedures and a set of PowerPoint slide documents capturing the prior incident details aligned to each flowchart header. Stage 1 (LLM-only) strictly gates the structural shape. Stage 2 mixes light code checks (file presence, basic text checks in the PDF) with heavier LLM verification of cross-references, factual fidelity to the incident scenario, and procedural soundness. Stage 3 holistically scores professional quality and strategic usefulness for LP investigators across regions/districts. Code rules are low-weight compared to LLM rules as instructed.", "max_total_score": 30.0, "stages": [{"name": "Stage 1: Shape Enforcement Gate", "description": "Gate: Required artifacts and structure must exist to enable verification. LLM-only checks of PDF flowchart structure and PowerPoint alignment with headers.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Flowchart PDF Structural Requirements", "description": "Check the PDF flowchart exists with the required visible title and clear top-level headers/nodes constituting a procedural flow for LP incidents.", "weight": 4.0, "judge_prompt": "You are evaluating the STRUCTURE ONLY. Consider all submitted files. Determine whether there is a PDF flowchart document meeting these requirements:\n\nFormat and Title:\n- Must be a PDF (not Word/Excel/plain text).\n- The visible on-page title on page 1 should read exactly \"Loss Prevention Incident Flowchart\" (allow minor punctuation/case deviations).\n- The PDF file should be named \"Missing Bank Deposits Investigation\" (allow minor separators like underscores/hyphens and the .pdf extension). If filename is not visible, accept if the on-page title is correct and the document content clearly pertains to missing bank deposits.\n\nRequired Flowchart Structure (top-level headers/nodes):\n- Must show a procedural flow with labeled stages. Accept equivalent phrasing. Expect 8\u201312 distinct top-level nodes such as:\n  - Intake/Triage\n  - Deposit Verification & Cash Variance Review\n  - Secure Funds & Audit Controls (dual custody, bag logs)\n  - Exception Thresholds & Alerts\n  - Escalation & Approvals\n  - Evidence Preservation (CCTV, bag logs, paperwork)\n  - Employee Interview Process\n  - Disciplinary/Legal Action\n  - Recovery/Restitution\n  - Reporting & Communication (Corp LP, HR, Legal, Bank)\n  - Prevention/Controls (armored car, drop safe, segregation)\n  - Post-Incident Review & Training\n\nScoring (structure only):\n- 4.0: PDF present, on-page title matches (or trivially close), and 8\u201312 distinct, clearly labeled top-level nodes connected as a flow.\n- 3.0: PDF present, title matches, and at least 6 distinct labeled nodes in a clear flow.\n- 2.0: PDF present, title present but flow is minimal (3\u20135 nodes) or layout unclear.\n- 1.0: A PDF exists but not obviously a flowchart or missing title.\n- 0.0: No valid PDF flowchart.\n\nDo not assess correctness of content, only presence and structural completeness.", "expectation": "A clearly titled PDF flowchart with 8\u201312 labeled nodes in a procedural flow."}, {"type": "llm_judge", "name": "PowerPoint Incident Detail Alignment (Per-Header)", "description": "Check presence of separate PowerPoint document(s) covering the prior incident details aligned to the flowchart headers.", "weight": 4.0, "judge_prompt": "You are evaluating STRUCTURE ONLY for the incident-detail deliverables. Consider all submitted files.\n\nRequirement:\n- For each top-level header/node in the flowchart PDF, there should be a corresponding PowerPoint document containing the prior incident details aligned to that header.\n- Accept either:\n  1) One separate PowerPoint file per header (preferred), or\n  2) A single PowerPoint deck with one clearly titled slide (or section) per header covering the incident details.\n- The materials must be PowerPoint format (.pptx). If slides are provided as PDF exports but clearly contain slide content, award partial credit.\n- Each slide or deck section title should visibly match or closely correspond to a flowchart header.\n\nScoring (structure only):\n- 4.0: Separate .pptx files per header OR a single .pptx with a clearly titled slide for each header; mappings are comprehensive and obvious.\n- 3.0: A single .pptx covers most headers (\u226575%) with clear titles matching flowchart nodes.\n- 2.0: Some PowerPoint materials present but only cover a minority of headers (30\u201374%) or are provided as slide PDFs.\n- 1.0: A PowerPoint exists but mapping to headers is unclear or minimal.\n- 0.0: No PowerPoint material.\n\nDo not evaluate content quality; only presence and alignment to headers.", "expectation": "Either multiple PPTX files (one per header) or one PPTX with a titled slide per header, aligned to the flowchart."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Correctness and Cross-Verification", "description": "Verify the artifacts are internally consistent, reflect the specified incident, and that PowerPoint mapping matches flowchart headers. Mix of light code checks and LLM judgments.", "is_required": false, "max_points": 14.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Required File Types Present", "description": "Check that at least one PDF flowchart document and at least one PowerPoint (.pptx) file are present among outputs.", "weight": 0.5, "code": "import os\nfrom pathlib import Path\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs() or []\n        if not outputs:\n            return 0.0, \"No outputs.\"\n        pdfs = []\n        ppts = []\n        for r in outputs:\n            try:\n                p = context.files.get_path(r.id)\n            except Exception:\n                continue\n            suffix = p.suffix.lower() if p is not None else ''\n            if suffix == '.pdf':\n                pdfs.append(r)\n            if suffix in ['.pptx', '.ppt']:\n                ppts.append(r)\n        score = 0.0\n        feedback = []\n        if pdfs:\n            score += 0.25\n            feedback.append(f\"Found {len(pdfs)} PDF(s).\")\n        else:\n            feedback.append(\"No PDF found.\")\n        if ppts:\n            score += 0.25\n            feedback.append(f\"Found {len(ppts)} PowerPoint file(s).\")\n        else:\n            feedback.append(\"No PowerPoint file found.\")\n        return min(score, 0.5), \"; \".join(feedback)\n    except Exception as e:\n        return 0.0, f\"Error checking file types: {e}\""}, {"type": "code", "name": "Flowchart PDF Title and Key Topic Coverage (Text Check)", "description": "Read the PDF text to confirm the expected title phrase appears and that several key procedural topics are present.", "weight": 0.5, "code": "import re\n\nKEY_TERMS = [\n    'intake', 'triage', 'deposit', 'variance', 'audit', 'dual custody', 'escalation',\n    'evidence', 'cctv', 'interview', 'disciplinary', 'legal', 'restitution',\n    'reporting', 'communication', 'prevention', 'controls', 'post-incident'\n]\n\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs() or []\n    if not outputs:\n        return 0.0, \"No outputs.\"\n    # find a PDF and read text\n    best_text = ''\n    found_pdf = False\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            if p.suffix.lower() == '.pdf':\n                found_pdf = True\n                try:\n                    txt = context.files.read_pdf_text(r.id) or ''\n                except Exception:\n                    txt = ''\n                if len(txt) > len(best_text):\n                    best_text = txt\n        except Exception:\n            continue\n    if not found_pdf:\n        return 0.0, \"No PDF to check.\"\n    text = (best_text or '').lower()\n    title_ok = ('loss prevention incident flowchart' in text)\n    hits = sum(1 for t in KEY_TERMS if t in text)\n    # scoring: title worth 0.2, key terms coverage worth up to 0.3\n    score = 0.0\n    fb = []\n    if title_ok:\n        score += 0.2\n        fb.append('Found expected title phrase.')\n    else:\n        fb.append('Expected title phrase not found in extracted text.')\n    coverage = min(hits / 8.0, 1.0)  # cap coverage at 8 hits\n    score += 0.3 * coverage\n    fb.append(f\"Key topic hits: {hits}.\")\n    return min(score, 0.5), \"; \".join(fb)"}, {"type": "llm_judge", "name": "Incident Fidelity and Anonymization", "description": "Do the incident details accurately reflect the scenario and omit names/store numbers as requested?", "weight": 6.0, "judge_prompt": "Evaluate the combined artifacts (PDF flowchart + PowerPoint materials) for factual fidelity to the described incident and anonymization:\n\nIncident elements that should be clearly reflected in the incident-detail slides:\n- Employee with deposit duties signing out cash deposits; store lacked armored car services.\n- Store Manager took bank deposit bags home instead of placing in the bank\u2019s drop.\n- Manager gambled with the funds; after accumulating winnings over a few days, deposited funds later, causing fluctuations.\n- Incident occurred in one store within a district; used as awareness across Regions/Districts.\n\nAnonymization requirements:\n- Exclude personal names and the specific store number. Use roles/titles only.\n\nScoring:\n- 6.0: All key incident elements are accurately captured in the slides, clearly tied to relevant headers, and names/store numbers are omitted.\n- 4.0: Most elements present (\u226575%) and anonymization is correct.\n- 2.0: Some elements present (30\u201374%) or minor anonymization issues (e.g., one redacted late reference).\n- 0.0: Major incident elements missing/misrepresented or PII/store numbers disclosed.", "expectation": "Slide content mirrors the scenario accurately and remains anonymized (no names/store numbers)."}, {"type": "llm_judge", "name": "Header-to-Slide Mapping Completeness", "description": "Cross-check that each flowchart top-level header has a corresponding slide/document section with incident details.", "weight": 4.0, "judge_prompt": "Using the flowchart PDF as the source of top-level headers, check whether each header is represented in the PowerPoint material:\n- A header is considered represented if there is a slide (or separate PPT file) with a closely matching title and focused incident details for that stage.\n- Allow equivalent phrasing but titles should be obviously corresponding.\n\nScoring:\n- 4.0: \u226590% headers have a 1:1 mapped slide/section with incident details.\n- 3.0: 75\u201389% mapped.\n- 2.0: 50\u201374% mapped.\n- 1.0: 25\u201349% mapped.\n- 0.0: <25% mapped or mapping is unclear.", "expectation": "Every flowchart header is covered by a slide/section with matching title and relevant incident details."}, {"type": "llm_judge", "name": "Procedural Soundness and Control Logic", "description": "Check that the flowchart\u2019s steps reflect sound LP practice and appropriate controls for bank deposit handling and theft investigations.", "weight": 3.0, "judge_prompt": "Assess the flowchart\u2019s procedural logic and controls for loss prevention in the given context. Look for:\n- Segregation of duties, dual custody for deposit preparation/transport, deposit logs, tamper-evident bags, drop safe or armored car alternatives.\n- Daily reconciliation and variance thresholds; exception reporting; CCTV/chain-of-custody evidence capture.\n- Clear escalation, documentation, interview procedures (HR/legal alignment), potential legal/law enforcement engagement, and restitution.\n- Post-incident review and preventive measures (policy updates, training, audits).\n\nScoring:\n- 3.0: Procedures reflect strong, comprehensive LP controls and investigation steps tailored to bank deposit risk.\n- 2.0: Generally sound with minor gaps.\n- 1.0: Several material gaps or ambiguities.\n- 0.0: Procedural logic is weak or inappropriate.", "expectation": "A robust, LP-aligned procedure with clear controls and investigative steps."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3: Presentation and Strategic Quality", "description": "Holistic quality assessment for professionalism, clarity, and strategic usefulness across Regions/Districts.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Clarity", "description": "Visual clarity and professional formatting of the flowchart and slides.", "weight": 2.0, "judge_prompt": "Evaluate professional quality:\n- Flowchart readability: legible labels, clear arrows/decision points, consistent typography/colors, minimal clutter.\n- Slides: concise titles, readable text, consistent style, appropriate visuals.\n- Overall cohesion and accessibility for LP investigators.\n\nScoring: 2.0 excellent, 1.0 adequate, 0.0 poor.", "expectation": "Clean, legible visuals and consistent, professional formatting."}, {"type": "llm_judge", "name": "Actionability for LP Investigators", "description": "Practical utility as an awareness and prevention tool across Regions/Districts.", "weight": 2.0, "judge_prompt": "Assess how actionable the package is for investigators:\n- Clear steps, timelines, decision points, checklists or cues.\n- Guidance on who to notify, when to escalate, and documentation expectations.\n- Portability across Regions/Districts; notes on adapting to local conditions.\n\nScoring: 2.0 strong and actionable, 1.0 somewhat useful, 0.0 weak.", "expectation": "A practical playbook investigators can apply immediately."}, {"type": "llm_judge", "name": "Risk, Legal, and HR Considerations", "description": "Coverage of employee rights, documentation, and legal coordination.", "weight": 2.0, "judge_prompt": "Evaluate inclusion of:\n- Employee interview considerations (Weingarten/representation rights as applicable), HR partnership, documentation standards.\n- Coordination with bank and possible law enforcement.\n- Ethical handling, privacy, and evidence preservation.\n\nScoring: 2.0 comprehensive, 1.0 partial, 0.0 minimal/absent.", "expectation": "Balanced treatment of HR/legal/ethical aspects and documentation rigor."}, {"type": "llm_judge", "name": "Preventive Insights and Metrics", "description": "Quality of prevention guidance and measurability.", "weight": 2.0, "judge_prompt": "Assess prevention content:\n- Control redesigns (e.g., armored car service, drop safe, dual custody, transport policies).\n- Training and communication plans.\n- KPIs/audits for deposit timeliness/variance; follow-up reviews to ensure sustained compliance.\n\nScoring: 2.0 strong with measurable follow-through, 1.0 some prevention ideas, 0.0 weak/none.", "expectation": "Clear preventive measures with metrics and follow-up auditing."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "2696757c-1f8a-4959-8f0d-f5597b9e70fc", "rubric": {"category_name": "VA Servicing Purchase \u2013 Bankruptcy Testing Template (Regulatory Testing Items)", "rationale": "This rubric enforces a self-documenting, file-based deliverable for a professional regulatory testing artifact. Stage 1 mandates a single PDF with a precise, verification-friendly structure. Stage 2 then checks correctness with a balanced mix of code (deterministic structural/content checks) and LLM judges (legal alignment, auditability). Stage 3 evaluates professional quality, tone, and operational usefulness for compliance testing. Code rules are deliberately lighter-weight than LLM rules, reflecting their narrower scope.", "max_total_score": 25.0, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate", "description": "Gate: Output must be a single PDF document with exact sections enabling verification. LLM-only per spec.", "is_required": true, "max_points": 5.0, "min_score_to_pass": 3.5, "rules": [{"type": "llm_judge", "name": "Required PDF Structure Present", "description": "Verify the candidate produced a single PDF that contains the required header and two required sections (each with a test question, citation, and exception statement).", "weight": 5.0, "judge_prompt": "You are validating the SHAPE ONLY of the output (not content quality). Review the candidate's final output.\n\nRequired format and structure:\n- Must be a single PDF document (not DOCX, not text/markdown, not Excel).\n- Title/Header at the top: \u201cVA Servicing Purchase \u2013 Bankruptcy Testing Template\u201d (allow minor punctuation or dash variations like hyphen vs en dash).\n- Exactly two distinct test items, one for each citation:\n  1) Paragraph 9.07(a)(2)(a)\n  2) Paragraph 9.08(c)(3)\n\nFor EACH of the two items, the PDF must clearly include all of the following, visibly labeled (labels may vary slightly but must be unambiguous):\n- A Test Question that targets the requirement; the citation placed immediately after the question\n- A Citation that explicitly references VA Servicer Handbook M26-4, Chapter 9 with the specific paragraph number (e.g., \u201cVA Servicer Handbook M26-4, Ch. 9.07(a)(2)(a)\u201d).\n- An Exception Statement (a narrative block) that would be used when non-compliance is found.\n\nScoring (return a numeric score out of 5; do not comment on content quality):\n- 5.0: Single PDF; correct header; both required items present; each item clearly has a labeled Test Question with citation included immediately after the question; and a labeled Exception Statement.\n- 3.5: Single PDF and correct header; both items present but one label or the exact placement of the citation is slightly off; content is still clearly identifiable and complete.\n- 2.0: Single PDF and header present, but only one complete item (question+citation+exception) is present.\n- 0.0: Not a PDF OR header missing OR items missing/not clearly labeled.\n\nImportant: Only evaluate presence/structure. Do not grade correctness or writing quality.", "expectation": "A single PDF with the exact header and two clearly labeled items (each with a test question, the citation after the question, and an exception statement)."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness Verification", "description": "Checks that the questions and exception statements are properly targeted, auditable, and internally consistent with VA M26-4 Chapter 9 bankruptcy/VASP context.", "is_required": true, "max_points": 14.0, "min_score_to_pass": 8.0, "rules": [{"type": "code", "name": "Structural Markers and Citations Found", "description": "Deterministically verify presence of at least two 'Test Question' and two 'Exception Statement' markers and the specific citations 9.07(a)(2)(a) and 9.08(c)(3).", "weight": 1.0, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output:\\n            return 0.0\\n        text = None\\n        # Prefer PDF text; fall back to DOCX/text if needed\\n        try:\\n            if output.is_document:\\n                # Attempt PDF first\\n                try:\\n                    text = context.files.read_pdf_text(output.id)\\n                except Exception:\\n                    try:\\n                        text = context.files.read_docx_text(output.id)\\n                    except Exception:\\n                        pass\\n        except Exception:\\n            pass\\n        if not text:\\n            try:\\n                text = context.files.read_text(output.id)\\n            except Exception:\\n                return 0.0\\n\\n        t = text.lower()\\n        # Count labeled elements (be flexible)\\n        tq_count = len(re.findall(r\"\\\\btest\\\\s*question\\\\b\", t))\\n        es_count = len(re.findall(r\"\\\\bexception\\\\s*statement\\\\b\", t))\\n\\n        # Citations (exact paragraphs)\\n        c1 = re.search(r\"\\\\b9\\\\.07\\\\(a\\\\)\\\\(2\\\\)\\\\(a\\\\)\\\\b\", t) is not None\\n        c2 = re.search(r\"\\\\b9\\\\.08\\\\(c\\\\)\\\\(3\\\\)\\\\b\", t) is not None\\n        # Also expect general handbook reference\\n        handbook_ref = (\"m26-4\" in t) or (\"va servicer handbook\" in t) or (\"chapter 9\" in t)\\n\\n        parts = []\\n        parts.append(1.0 if tq_count >= 2 else 0.0)\\n        parts.append(1.0 if es_count >= 2 else 0.0)\\n        parts.append(1.0 if (c1 and c2 and handbook_ref) else 0.0)\\n\\n        score = sum(parts) / len(parts)\\n        return max(0.0, min(1.0, score))\\n    except Exception:\\n        return 0.0"}, {"type": "code", "name": "Exception Narrative Minimum Substance", "description": "Verify each Exception Statement has basic narrative substance: at least ~2 sentences and >= 30 words (flexible) for the first two exception blocks found.", "weight": 1.0, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output:\\n            return 0.0\\n        text = None\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_docx_text(output.id)\\n            except Exception:\\n                try:\\n                    text = context.files.read_text(output.id)\\n                except Exception:\\n                    return 0.0\\n\\n        if not text:\\n            return 0.0\\n\\n        # Heuristic: capture text following each 'Exception Statement' label\\n        blocks = []\\n        for m in re.finditer(r\"Exception\\\\s*Statement\\s*[:\\u2013\\-]?\\s*\", text, flags=re.IGNORECASE):\\n            start = m.end()\\n            snippet = text[start:start+1200]  # take a sizeable window\\n            # Stop at next common label-like pattern\\n            stop_match = re.search(r\"\\n\\s*(Test\\\\s*Question|Citation|Exception\\\\s*Statement|Recommendations|Appendix|Section)\\b\", snippet, flags=re.IGNORECASE)\\n            if stop_match:\\n                snippet = snippet[:stop_match.start()]\\n            blocks.append(snippet.strip())\\n            if len(blocks) >= 2:\\n                break\\n\\n        if not blocks:\\n            return 0.0\\n\\n        def score_block(txt):\\n            # Count sentences and words\\n            sentences = re.split(r\"(?<=[\\.!?])\\s+\", txt.strip())\\n            sentences = [s for s in sentences if s.strip()]\\n            words = re.findall(r\"\\b\\w+\\b\", txt)\\n            word_count = len(words)\\n            if len(sentences) >= 2 and word_count >= 30:\\n                return 1.0\\n            elif len(sentences) >= 1 and word_count >= 20:\\n                return 0.6\\n            else:\\n                return 0.0\\n\\n        scores = [score_block(b) for b in blocks[:2]]\\n        # If only one block, average over one\\n        score = sum(scores) / max(1, len(scores))\\n        return max(0.0, min(1.0, score))\\n    except Exception:\\n        return 0.0"}, {"type": "llm_judge", "name": "Legal Alignment with VA M26-4 Chapter 9 (Bankruptcy/VASP Context)", "description": "Check whether each test question targets the correct obligation implied by the cited paragraph and whether the exception statements accurately articulate a failure to meet that obligation in the VA bankruptcy/VASP context.", "weight": 6.0, "judge_prompt": "Assess the substantive correctness of each test item in the PDF:\n\nFor each of the two items (9.07(a)(2)(a) and 9.08(c)(3)) verify:\n- The Test Question is tightly aligned with the cited requirement under VA Servicer Handbook M26-4, Chapter 9, in a bankruptcy and VA Servicing Purchase (VASP) context. The question should target a verifiable servicer obligation (e.g., action, timing, documentation, eligibility/decisioning under streamlined criteria) rather than broad commentary.\n- The Exception Statement describes, in regulatory tone, how the servicer failed to meet the specific obligation referenced by the citation. It should clearly state the expected requirement and the observed deficiency.\n- Internal consistency: the exception text logically relates to the question and the cited paragraph (no contradictions or mismatched topics).\n\nScoring (out of 6):\n- 6.0: Both items are well-aligned, obligation-focused, and internally consistent; exception statements correctly articulate requirement and deficiency.\n- 4.0: One item is strong; the other is somewhat general or partially mismatched but still plausibly aligned.\n- 2.0: Items are mostly generic or misaligned with bankruptcy/VASP expectations; requirements/deficiency link is weak.\n- 0.0: Items are off-topic or contradictory to the cited paragraphs.\n\nJudge only correctness/alignment, not formatting or prose polish.", "expectation": "Each question targets a concrete servicer duty under the cited paragraph; each exception mirrors that duty and explains the failure in bankruptcy/VASP context."}, {"type": "llm_judge", "name": "Auditability and Evidence Orientation", "description": "Confirm the questions are testable with objective criteria and reference appropriate evidence types; exceptions should indicate what evidence would show non-compliance.", "weight": 6.0, "judge_prompt": "Evaluate whether the proposed test questions and exception statements are audit-ready and evidence-oriented:\n- Questions: Are they stated in a way that a tester can answer using standard mortgage servicing records for bankruptcy accounts (e.g., system notes, bankruptcy docket/plan, proof of claim, payment application history, VASP decision/evaluation artifacts, timelines/dates)? Do they imply objective yes/no or clearly checkable criteria?\n- Exceptions: Do they indicate what records or artifacts would substantiate the finding (explicitly or implicitly), and do they distinguish expected vs. observed evidence?\n\nScoring (out of 6):\n- 6.0: Both items are clearly testable with objective criteria and relevant evidence types are clear/implicit; exceptions indicate expected vs observed evidence.\n- 4.0: Mostly testable with minor ambiguity on evidence.\n- 2.0: Vague or subjective; limited linkage to concrete evidence.\n- 0.0: Not auditable; no usable evidence cues.\n\nFocus on testability and evidence orientation, not legal precision.", "expectation": "Both items enable a tester to gather files/records and reach an objective conclusion; exceptions mention expected vs. observed evidence."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality Assessment", "description": "Evaluate tone, clarity, and operational usefulness for a bank\u2019s monthly VA oversight testing.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Regulatory Tone", "description": "Assess whether the writing uses an appropriate regulatory/compliance voice suitable for reporting to stakeholders.", "weight": 1.5, "judge_prompt": "Assess the tone of the PDF content:\n- Is the tone professional, neutral, and regulatory (e.g., uses precise language like \u201cmust/shall,\u201d \u201cfailed to,\u201d \u201cin accordance with,\u201d \u201cpursuant to\u201d)?\n- Is it free of accusatory or emotional language?\n- Would this be acceptable in a bank\u2019s regulatory testing report?\n\nScore 0.0 to 1.5 accordingly, based on overall appropriateness of tone.", "expectation": "Neutral, precise, compliance-focused language appropriate for formal reporting."}, {"type": "llm_judge", "name": "Clarity and Specificity", "description": "Evaluate whether the questions and exception statements are clear, concise, and specific enough for testers and stakeholders to understand without ambiguity.", "weight": 1.5, "judge_prompt": "Evaluate clarity and specificity:\n- Are the test questions concise and unambiguous?\n- Do exception statements clearly state the expected requirement and the observed gap in a way a reader can quickly grasp?\n- Avoids unnecessary jargon; uses precise terms.\n\nScore 0.0 to 1.5 based on clarity and specificity.", "expectation": "Concise, unambiguous items with clearly stated expectations and observed deficiencies."}, {"type": "llm_judge", "name": "Operational Usefulness and Reusability", "description": "Judge whether the artifact can be plugged into a standard monthly testing program with minimal rework.", "weight": 1.5, "judge_prompt": "Assess operational usefulness:\n- Are the items structured so they can be reused month-to-month with minimal edits?\n- Do they include the citation right after each question, as required?\n- Do they naturally map to standard workpapers (criteria, condition, cause, effect, evidence)?\n\nScore 0.0 to 1.5 based on readiness for repeatable operational testing.", "expectation": "Drop-in readiness for monthly testing with citations included after each question."}, {"type": "llm_judge", "name": "Formatting and Document Quality", "description": "Evaluate document-level readability and basic formatting quality beyond Stage 1 structure.", "weight": 1.5, "judge_prompt": "Evaluate formatting and readability:\n- Is the header correctly presented and visually distinct?\n- Are the two items clearly separated and easy to scan (labels, spacing, line breaks)?\n- Minimal typographical errors; consistent punctuation and citation style.\n\nScore 0.0 to 1.5 based on overall presentation quality.", "expectation": "Clean, readable layout with consistent labels and minimal typos."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "3baa0009-5a60-4ae8-ae99-4955cb328ff3", "rubric": {"category_name": "Economic News Article + Chart (World Bank GEP June 2025)", "rationale": "Task Type: Mixed (documents with embedded data/visual). Output Format: One Markdown (.md) article plus one JPG chart image. Stage 1 uses LLM-only shape enforcement so later verification is trivial. Stage 2 blends light code checks (bounds and structural/text heuristics) with heavier LLM verification (factual framing, tone, consistency, and chart-text alignment). Stage 3 assesses professional quality, clarity for a U.S. non-expert audience, and presentation.", "max_total_score": 12.0, "stages": [{"name": "Stage 1 \u2013 Structure Gate (Required)", "description": "LLM-only gate verifying the exact, self-documenting package structure: a 300\u2013500 word Markdown article with a clear title, embedded reference to a JPG chart, and source links to World Bank, Reuters, and/or AP; plus a separate JPG chart file showing World Bank global growth for 2024, 2025, and 2027.", "is_required": true, "max_points": 2.0, "min_score_to_pass": 1.0, "rules": [{"type": "llm_judge", "name": "Output Package Structure Gate", "description": "Check the candidate outputs for required files and structural elements only (do not judge content quality or correctness).", "weight": 2.0, "judge_prompt": "You are checking ONLY the presence and format of required deliverables. Do not assess correctness or writing quality.\n\nRequired package structure:\n1) One primary article as a Markdown (.md) file that:\n   - Has a clear title/headline at the top (e.g., a Markdown H1 or prominent title line).\n   - Is between 300 and 500 words of body copy (not counting URLs-only lines or image alt text/captions).\n   - Mentions the World Bank\u2019s June 2025 Global Economic Prospects (GEP) report.\n   - Includes at least a short \u201cSources\u201d or similar section with links referencing at least two of these domains: worldbank.org, reuters.com, apnews.com.\n   - Embeds a chart image via Markdown image syntax referencing a JPG file (e.g., ![alt](chart.jpg)). A caption or alt text that indicates it\u2019s World Bank global growth for 2024, 2025, and 2027 is expected.\n2) One separate JPG image file (the chart) that visibly shows a simple chart (bars or line) for World Bank global growth across the years 2024, 2025, and 2027. The chart should have:\n   - A readable title like \u201cWorld Bank Global Growth\u201d (or similar),\n   - Year labels for 2024, 2025, 2027,\n   - A y-axis in percent (%) or clearly conveys percentages.\n\nScoring (structure only):\n- 2.0: All above present: Markdown article with title and 300\u2013500 words, sources with at least two required domains, embedded image reference to a JPG, and a separate JPG chart that clearly shows 2024/2025/2027 global growth with a readable title and percent axis or clear percent markings.\n- 1.5: Article and JPG present with embedded reference, but one minor structural element is missing or weak (e.g., caption/alt not explicit, or only one required domain in sources, or chart lacks explicit axis label but still clearly shows percentages and years).\n- 1.0: Correct file types present (a Markdown article and a JPG chart) and image is embedded, but multiple required structural elements missing (e.g., word count outside 300\u2013500 OR sources section absent OR chart missing year labels/title).\n- 0.0: Wrong format (no Markdown article or no JPG chart) OR no image embedding in the article OR article is far shorter than 300 words.\n\nOnly check presence/structure. Do not verify factual accuracy, tone, or numeric correctness.", "expectation": "A .md article (300\u2013500 words) with a clear title, embedded JPG chart, and sources section; plus a separate JPG chart showing 2024/2025/2027 global growth with a readable title and percentage context."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification (Content and Consistency)", "description": "Now that the structure exists, verify correctness and consistency using a mix of deterministic code checks and LLM judgment. Code rules carry small weight; LLM judges carry most weight.", "is_required": false, "max_points": 7.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "File Types and Image Embedding Check", "description": "Verify presence of a Markdown article, a JPG image, and that the Markdown embeds a JPG via Markdown image syntax.", "weight": 0.3, "code": "def evaluate(workflow, context):\n    import re\n    # Find article (text/markdown) and image resources among all outputs\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0, \"No outputs.\"\n\n    article_res = None\n    image_res = None\n\n    for r in outputs:\n        try:\n            if getattr(r, 'is_text_format', False) and not getattr(r, 'is_document', False):\n                # Heuristic: assume markdown if it contains markdown cues\n                text = context.files.read_text(r.id)\n                if text and ('\\n# ' in text or text.strip().startswith('#') or '![' in text or '[' in text and '](' in text):\n                    article_res = r\n            if getattr(r, 'is_image', False):\n                # Prefer JPG/JPEG by checking any markdown reference or filename hint if available\n                image_res = image_res or r\n        except Exception:\n            continue\n\n    if not article_res:\n        return 0.0, \"No Markdown-like article found.\"\n    if not image_res:\n        return 0.0, \"No image resource found.\"\n\n    try:\n        md = context.files.read_text(article_res.id)\n    except Exception:\n        return 0.0, \"Unable to read article text.\"\n\n    # Check for embedded JPG reference in markdown\n    has_md_img = False\n    try:\n        for m in re.finditer(r\"!\\[[^\\]]*\\]\\(([^)]+)\\)\", md, flags=re.IGNORECASE):\n            target = m.group(1)\n            if any(ext in target.lower() for ext in ['.jpg', '.jpeg']):\n                has_md_img = True\n                break\n    except Exception:\n        pass\n\n    score = 0.0\n    feedback = []\n    if article_res:\n        score += 0.1\n    else:\n        feedback.append('Missing article file')\n    if image_res:\n        score += 0.1\n    else:\n        feedback.append('Missing image file')\n    if has_md_img:\n        score += 0.1\n    else:\n        feedback.append('Article lacks embedded JPG image reference')\n\n    return score, '; '.join(feedback) if feedback else 'Article and image present with embedded JPG.'"}, {"type": "code", "name": "Word Count Within 300\u2013500", "description": "Check that the body text (excluding top title line(s)) is between 300 and 500 words.", "weight": 0.3, "code": "def evaluate(workflow, context):\n    import re\n    out = context.get_primary_output()\n    # If primary isn't the article, find one\n    outputs = context.get_all_outputs() or ([] if out is None else [out])\n\n    article_res = None\n    for r in outputs:\n        try:\n            if getattr(r, 'is_text_format', False):\n                txt = context.files.read_text(r.id)\n                if txt and ('\\n# ' in txt or txt.strip().startswith('#')):\n                    article_res = r\n                    break\n        except Exception:\n            continue\n\n    if not article_res:\n        return 0.0, 'No markdown article found.'\n\n    try:\n        md = context.files.read_text(article_res.id)\n    except Exception:\n        return 0.0, 'Unable to read article text.'\n\n    # Remove title/header lines starting with #\n    lines = [ln for ln in md.splitlines() if not ln.strip().startswith('#')]\n    body = '\\n'.join(lines)\n    # Remove image and link URLs to avoid counting them unfairly\n    body = re.sub(r\"!\\[[^\\]]*\\]\\([^)]*\\)\", \" \", body)\n    body = re.sub(r\"\\[[^\\]]*\\]\\([^)]*\\)\", \" \", body)\n    # Collapse whitespace and count words\n    tokens = re.findall(r\"[A-Za-z0-9%\\-$]+\", body)\n    wc = len(tokens)\n\n    if 300 <= wc <= 500:\n        return 0.3, f'Word count OK: {wc}'\n    elif 260 <= wc < 300 or 500 < wc <= 560:\n        return 0.15, f'Word count slightly out of range: {wc}'\n    else:\n        return 0.0, f'Word count out of range: {wc}'"}, {"type": "code", "name": "Source Links Presence", "description": "Verify the article references at least two of the required source domains: worldbank.org, reuters.com, apnews.com.", "weight": 0.3, "code": "def evaluate(workflow, context):\n    import re\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0, 'No outputs.'\n\n    article_text = ''\n    for r in outputs:\n        try:\n            if getattr(r, 'is_text_format', False):\n                article_text = context.files.read_text(r.id) or ''\n                if article_text:\n                    break\n        except Exception:\n            continue\n\n    if not article_text:\n        return 0.0, 'No article text found.'\n\n    domains = ['worldbank.org', 'reuters.com', 'apnews.com']\n    present = [d for d in domains if d in article_text.lower()]\n    n = len(present)\n\n    if n >= 2:\n        return 0.3, f'Source domains present: {present}'\n    elif n == 1:\n        return 0.15, f'Only one source domain present: {present}'\n    else:\n        return 0.0, 'No required source domains present.'"}, {"type": "code", "name": "Topic Coverage Keywords", "description": "Check that the article references World Bank, trade tensions/tariffs, and both the U.S. and China.", "weight": 0.3, "code": "def evaluate(workflow, context):\n    import re\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0, 'No outputs.'\n\n    text = ''\n    for r in outputs:\n        try:\n            if getattr(r, 'is_text_format', False):\n                t = context.files.read_text(r.id) or ''\n                if t:\n                    text = t.lower()\n                    break\n        except Exception:\n            continue\n\n    if not text:\n        return 0.0, 'No article text found.'\n\n    has_wb = 'world bank' in text\n    has_us = any(k in text for k in [' united states', ' u.s.', ' us '])\n    has_china = 'china' in text\n    has_trade = any(k in text for k in ['trade war', 'trade tensions', 'tariff', 'tariffs'])\n\n    hits = sum([has_wb, has_us, has_china, has_trade])\n    score = {4:0.3, 3:0.2, 2:0.1}.get(hits, 0.0)\n    missing = []\n    if not has_wb: missing.append('World Bank')\n    if not has_us: missing.append('U.S.')\n    if not has_china: missing.append('China')\n    if not has_trade: missing.append('trade/tariffs')\n\n    return score, 'Missing: ' + ', '.join(missing) if missing else 'All topical elements present.'"}, {"type": "llm_judge", "name": "Factual Framing and Forecast Coverage", "description": "Evaluate whether the article accurately frames the World Bank\u2019s June 2025 GEP outlook as a downgrade/negative global growth picture linked to trade tensions/tariffs, and whether it summarizes global, U.S., and China forecasts for 2025 (and relevant context). Check that attributions to World Bank/Reuters/AP are present for key figures/claims.", "weight": 2.0, "judge_prompt": "Evaluate the article text for factual framing and coverage (do not penalize small numeric deviations unless clearly wrong). Consider:\n- Does it clearly state that the World Bank\u2019s June 2025 Global Economic Prospects report projects a weaker global growth outlook and link the downgrade to trade tensions/tariffs between the U.S. and China?\n- Does it provide a concise summary of World Bank forecasts (global, U.S., and China), suitable for a quick news brief?\n- Are key claims attributed to World Bank and/or newswires (Reuters/AP)?\n\nScoring:\n- 2.0: Clear negative/downgraded global outlook linked to trade tensions/tariffs, includes global + U.S. + China forecasts with proper attribution.\n- 1.3: Mostly correct framing; one forecast missing or attribution thin.\n- 0.7: Partial/unclear framing; significant omissions (e.g., lacks country breakdowns) or weak attribution.\n- 0.0: Misframed (e.g., suggests strong acceleration) or no relevant forecasts/attribution.", "expectation": "A concise, accurate, attributed summary of the World Bank\u2019s global, U.S., and China outlook with trade/tariff impacts."}, {"type": "llm_judge", "name": "Chart\u2013Text Consistency and Readability", "description": "Check that the JPG chart\u2019s values/years (2024, 2025, 2027) align with the article\u2019s narrative and that the chart is readable (title and percentage context).", "weight": 1.6, "judge_prompt": "Inspect the JPG chart and the article. Check:\n- The chart shows World Bank global growth for 2024, 2025, and 2027.\n- Numbers or trend in the chart are consistent with the text\u2019s stated direction and approximate magnitudes.\n- The chart has a clear title and percent context (axis label or % markings) and is legible.\n\nScoring:\n- 1.6: Years and values align with the text; chart is clear, titled, and percent context is evident.\n- 1.0: Mostly aligns; minor labeling or readability issues.\n- 0.5: Some mismatch or unclear labeling; still somewhat usable.\n- 0.0: Years missing, chart unreadable, or contradicts the text.", "expectation": "A legible chart whose years and values match the article\u2019s narrative."}, {"type": "llm_judge", "name": "Neutrality and Attribution Style", "description": "Assess tone neutrality, balance, and proper attribution (e.g., avoids editorializing, uses neutral verbs, attributes key claims to sources).", "weight": 1.2, "judge_prompt": "Evaluate the article\u2019s tone and sourcing:\n- Neutral, balanced language suitable for a professional news outlet.\n- Avoids loaded or speculative phrasing; uses factual, attributed statements (e.g., \u201cthe World Bank said,\u201d \u201cReuters reported,\u201d \u201cAP reported\u201d).\n\nScoring:\n- 1.2: Consistently neutral and well-attributed.\n- 0.8: Mostly neutral; minor lapses or thin attribution.\n- 0.4: Noticeable editorializing or weak attribution.\n- 0.0: Overtly biased or un-attributed claims dominate.", "expectation": "Balanced, factual tone with clear attributions."}, {"type": "llm_judge", "name": "Clarity for U.S. Non\u2011Expert Audience", "description": "Judge whether the article explains the significance in plain English for a U.S. audience and avoids jargon.", "weight": 1.0, "judge_prompt": "Assess accessibility for a U.S.-based, non-expert audience:\n- Plain-English explanations; minimal jargon.\n- Briefly explains why trade tensions/tariffs and the forecasts matter for U.S. readers (jobs, prices, growth outlook).\n\nScoring:\n- 1.0: Clear, accessible, and relevant to U.S. readers.\n- 0.6: Generally clear with minor jargon or thin relevance.\n- 0.3: Somewhat technical or unclear relevance.\n- 0.0: Jargon-heavy or no U.S. relevance.", "expectation": "Simple, relevant framing for U.S. non-expert readers."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Quality Assessment", "description": "Holistic quality assessment of writing craft, presentation, and visual communication for a professional online news outlet.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Headline and Lede Quality", "description": "Is the headline concise and informative? Does the lede summarize the key news and set context effectively?", "weight": 0.9, "judge_prompt": "Evaluate headline and lede:\n- Headline: clear, specific, reflects the World Bank\u2019s downgraded outlook context.\n- Lede: quickly delivers the main takeaway and context (trade tensions/tariffs, affected regions).\n\nScoring: 0.9 excellent; 0.6 good; 0.3 fair; 0.0 poor.", "expectation": "Clear, specific headline and lede that convey the core news."}, {"type": "llm_judge", "name": "Organization and Flow", "description": "Assess structure, paragraphing, transitions, and logical flow suited to a 300\u2013500 word news brief.", "weight": 0.7, "judge_prompt": "Assess organization and readability:\n- Logical progression from lede to details to broader context.\n- Reasonable paragraph length; smooth transitions.\n\nScoring: 0.7 excellent; 0.5 good; 0.3 fair; 0.0 poor.", "expectation": "Well-structured, easy-to-follow short article."}, {"type": "llm_judge", "name": "Precision and Style", "description": "Check for correct names/dates, consistent units/percentages, grammar/spelling, and crisp newswriting style (AP-like).", "weight": 0.7, "judge_prompt": "Evaluate precision and style:\n- Names/dates accurate; numbers include % or units where relevant.\n- Grammar and spelling clean; concise, AP-like style.\n\nScoring: 0.7 excellent; 0.5 good; 0.3 fair; 0.0 poor.", "expectation": "Clean, precise, professional copy."}, {"type": "llm_judge", "name": "Visual Presentation (Chart Aesthetics and Caption/Alt)", "description": "Evaluate chart aesthetics/legibility and whether the article provides a caption or alt text that explains the chart clearly.", "weight": 0.7, "judge_prompt": "Evaluate the JPG chart\u2019s visual presentation and the article\u2019s accompanying caption/alt:\n- Clear labeling, readable fonts, not cluttered.\n- Caption or alt text in the article that explains what the chart shows and cites the World Bank as source.\n\nScoring: 0.7 excellent; 0.5 good; 0.3 fair; 0.0 poor.", "expectation": "Clean chart with clear caption/alt and source attribution."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "a45bc83b-22f9-4def-8d89-9c5661b2b86f", "rubric": {"category_name": "GCP Migration & Modernization Proposal (Solutions Architect)", "rationale": "This rubric enforces a self-documenting, file-based deliverable set aligned to the task: two Word documents (architecture summary and POC guide) and one PDF architecture diagram. Stage 1 is an LLM-only structural gate mandating exact shapes to enable verification. Stage 2 combines light code checks (presence of critical GCP services/keywords) with LLM verification of cross-document consistency and requirement coverage. Stage 3 assesses overall quality, clarity, and professional readiness for customer review.", "max_total_score": 35.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (Documents + Diagram)", "description": "LLM-only gate to ensure required deliverables and structures exist before any verification.", "is_required": true, "max_points": 10.0, "min_score_to_pass": 7.5, "rules": [{"type": "llm_judge", "name": "Deliverables Presence & Formats", "description": "Verify all required files are present with correct formats: two DOCX documents and one PDF diagram.", "weight": 4.0, "judge_prompt": "Evaluate the candidate outputs (all submitted files):\n\nRequired deliverables and formats:\n- A Word document (DOCX) titled as or clearly functioning as \u201cProposed Architecture Summary\u201d (bulleted style, mirrors the style of a current-architecture bulleted summary).\n- A PDF architecture diagram using official Google Cloud icons (https://cloud.google.com/icons) and following a professional architecture-diagram style.\n- A Word document (DOCX) for a POC plan with step-by-step implementation instructions (each step includes a purpose or clearly explains why it\u2019s done).\n\nScoring:\n- 4.0: All three deliverables present; formats correct (DOCX, PDF) and each document clearly matches its intended purpose.\n- 3.0: All three present but one has format or clear-purpose issues (e.g., steps lack clear purposes OR diagram looks like a generic image without clear GCP iconography).\n- 2.0: Only two deliverables present in correct formats.\n- 1.0: Only one deliverable present in correct format.\n- 0.0: Missing or wrong formats (e.g., no PDF diagram, or documents not in DOCX).", "expectation": "Exactly two DOCX documents (architecture summary and POC) and one PDF diagram are present."}, {"type": "llm_judge", "name": "Architecture Summary Structure (Bulleted)", "description": "Check the architecture summary DOCX has a bulleted, sectioned structure mirroring the provided current-architecture style.", "weight": 3.0, "judge_prompt": "Inspect the architecture summary DOCX. Verify it is a bulleted, sectioned document mirroring a current-architecture summary style, focusing on renamed/updated sections suitable for the proposed GCP design.\n\nRequired structural elements (allow flexible naming):\n- A header or title indicating it is the proposed GCP architecture summary.\n- Bulleted sections covering: Ingress/Networking, Web & App Layer, Data Layer (databases or storage), Static Content hosting, Security & DDoS (L3/L4), High Availability/Scalability, Operations/Monitoring.\n- Bulleted data flow description from external users through networking/load balancing to app/services and data stores.\n\nScoring:\n- 3.0: Clear title and all listed sections present with bullet lists; flow is covered as bullets, not prose-only.\n- 2.0: Most sections present (missing one) and primarily bulleted.\n- 1.0: Some bullets but lacks multiple required sections or is mostly prose.\n- 0.0: Not a bulleted, sectioned summary or wrong file type.", "expectation": "A DOCX with clear title and bullet lists for each key area and an end-to-end data flow."}, {"type": "llm_judge", "name": "POC Plan Structure (Step-by-Step with Purpose)", "description": "Check the POC DOCX has numbered steps and each step includes a purpose/clarification.", "weight": 3.0, "judge_prompt": "Inspect the POC DOCX for step-by-step instructions. Confirm structure and clarity:\n\nRequired structural elements (flexible naming):\n- Sections like Objectives, Scope/Assumptions, Architecture Overview (brief), Implementation Steps, Validation/Test Plan, Rollback/Backout, References.\n- Implementation Steps are numbered and each step either states a purpose (e.g., why it\u2019s needed) or has an immediately adjacent explanatory note.\n- References include official Google Cloud links (any of: cloud.google.com/docs, solutions/web-hosting, load-balancing, dns, cdn, security/products/armor, storage, firestore, sql, or the 13 architectures blog).\n\nScoring:\n- 3.0: Numbered steps, each with a purpose; most of the listed sections present; at least two official Google Cloud links included.\n- 2.0: Numbered steps present with occasional missing purposes; sections mostly present; at least one official link.\n- 1.0: Steps are present but unnumbered or purposes largely missing; sections thin; minimal/no references.\n- 0.0: Lacks a step-by-step section or is not a DOCX.", "expectation": "A DOCX with numbered steps, each clarified by a purpose, plus key sections and official references."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness & Consistency)", "description": "Check correctness of services, alignment with requirements, and cross-document consistency. Includes targeted code-based checks and LLM-based reasoning.", "is_required": false, "max_points": 15.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Deliverable Count & Basic Content Sanity", "description": "Verify at least two DOCX and one PDF exist among outputs and documents mention Google Cloud context.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs()\n        if not outputs:\n            return 0.0, \"No outputs provided.\"\n        docx_ids = [r.id for r in outputs if getattr(r, 'is_document', False) and str(r.name).lower().endswith('.docx')]\n        pdf_ids = [r.id for r in outputs if str(r.name).lower().endswith('.pdf')]\n        score = 0.0\n        if len(docx_ids) >= 2:\n            score += 0.5\n        if len(pdf_ids) >= 1:\n            score += 0.5\n        # Check if any document text mentions Google Cloud concepts\n        combined_text = \"\"\n        for rid in docx_ids:\n            try:\n                combined_text += \"\\n\" + (context.files.read_docx_text(rid) or \"\")\n            except Exception:\n                pass\n        for rid in pdf_ids:\n            try:\n                combined_text += \"\\n\" + (context.files.read_pdf_text(rid) or \"\")\n            except Exception:\n                pass\n        mentions = 0\n        keywords = [\n            'google cloud', 'gcp', 'cloud load balancing', 'cloud armor', 'cloud cdn',\n            'cloud dns', 'cloud storage', 'gke', 'cloud run', 'app engine', 'cloud sql', 'firestore'\n        ]\n        text_l = combined_text.lower()\n        if any(k in text_l for k in keywords):\n            mentions = 1\n        # If structure present but zero content signal, reduce to 0.75 of structural score\n        if (score >= 1.0) and (mentions == 0):\n            return 0.75, \"Formats present but missing clear GCP mentions.\"\n        return min(score, 1.0), \"Checked deliverable counts and GCP mentions.\"\n    except Exception as e:\n        return 0.0, f\"Error during basic sanity check: {e}\""}, {"type": "code", "name": "Key Capability Coverage via Keywords", "description": "Check that documents cover critical requirements with at least one service per capability group (ingress/DDoS, static content, compute modernization, data layer, HA/ops).", "weight": 1.5, "code": "def evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs()\n        if not outputs:\n            return 0.0\n        # Aggregate text from DOCX and PDF\n        text = \"\"\n        for r in outputs:\n            name = (getattr(r, 'name', '') or '').lower()\n            try:\n                if name.endswith('.docx') and getattr(r, 'is_document', False):\n                    text += \"\\n\" + (context.files.read_docx_text(r.id) or \"\")\n                elif name.endswith('.pdf'):\n                    text += \"\\n\" + (context.files.read_pdf_text(r.id) or \"\")\n            except Exception:\n                continue\n        tl = text.lower()\n        if not tl:\n            return 0.0, \"No readable text from documents/PDFs.\"\n        # Capability groups: at least one hit per group\n        groups = {\n            'ddos_ingress': [\n                'cloud armor', 'armor policy', 'l3', 'l4', 'ddos', 'http(s) load balancer', 'external tcp/udp load balancer', 'global load balancer'\n            ],\n            'static_content': [\n                'cloud storage', 'gcs', 'bucket', 'static content', 'cdn', 'cloud cdn'\n            ],\n            'compute_modern': [\n                'gke', 'kubernetes engine', 'managed instance group', 'mig', 'cloud run', 'app engine'\n            ],\n            'data_layer': [\n                'cloud sql', 'postgres', 'mysql', 'sql server', 'firestore', 'spanner'\n            ],\n            'ha_ops': [\n                'multi-zone', 'multi region', 'regional', 'high availability', 'autoscaling', 'health check', 'cloud monitoring', 'cloud logging', 'operations suite'\n            ],\n            'network_core': [\n                'vpc', 'subnet', 'firewall', 'private service connect', 'private ip', 'nat'\n            ],\n            'dns': [\n                'cloud dns'\n            ]\n        }\n        hits = 0\n        total = len(groups)\n        details = []\n        for g, kws in groups.items():\n            if any(k in tl for k in kws):\n                hits += 1\n                details.append(f\"{g}:ok\")\n            else:\n                details.append(f\"{g}:miss\")\n        ratio = hits / total if total else 0\n        score = ratio * 1.5\n        return score, \", \".join(details)\n    except Exception as e:\n        return 0.0, f\"Error during capability coverage check: {e}\""}, {"type": "llm_judge", "name": "Cross-Document Consistency (Summary \u2194 Diagram)", "description": "Verify that the architecture summary and the PDF diagram are consistent in components and data flow.", "weight": 4.0, "judge_prompt": "Review the architecture summary DOCX and the PDF diagram together.\n\nCheck for cross-consistency:\n- Major components mentioned in the summary appear in the diagram (ingress/LB, app tier, data stores, static content/CDN, security controls like Cloud Armor, VPC/subnets).\n- Data flow sequence in the summary (external user \u2192 DNS \u2192 CDN/LB \u2192 app \u2192 data \u2192 responses) matches the diagram\u2019s flows/arrows.\n- The diagram shows GCP-native services corresponding to the summary\u2019s wording.\n\nScoring:\n- 4.0: Strong alignment; components and flows clearly match in both documents.\n- 3.0: Mostly aligned; minor omissions (one piece missing or named differently but understandable).\n- 2.0: Some inconsistencies or missing key elements in either document.\n- 1.0: Weak alignment; diagram and summary largely diverge.\n- 0.0: Little to no correspondence.", "expectation": "Summary and diagram depict the same GCP architecture and data flow."}, {"type": "llm_judge", "name": "Requirements Coverage (Scalability, Static Hosting, L3/L4 DDoS, Security & HA)", "description": "Assess whether the proposed solution meets the stated requirements using appropriate GCP services.", "weight": 4.0, "judge_prompt": "Determine if the proposed architecture satisfies the explicit requirements with suitable GCP services and patterns:\n- Scalability: autoscaling (MIGs, GKE HPA), global/regional load balancing.\n- Static content hosting: Cloud Storage + Cloud CDN (or equivalent), proper caching/invalidations.\n- Layer 3/4 DDoS protection: Cloud Armor policies in front of external LB.\n- Enterprise security: IAM roles/least privilege, Secret Manager, private IP for DB, firewalling, TLS, logging/monitoring, optionally org policies.\n- High availability: multi-zone/regional design, health checks, failover patterns.\n\nScoring:\n- 4.0: All requirement areas are addressed with appropriate GCP services and patterns.\n- 3.0: Most areas addressed; one area lightly covered.\n- 2.0: Multiple areas thin or missing details.\n- 1.0: Mentions but lacks concrete service mapping.\n- 0.0: Does not meet key requirements.", "expectation": "Concrete GCP services mapped to each requirement with coherent rationale."}, {"type": "llm_judge", "name": "POC Feasibility & Testability", "description": "Evaluate if the POC plan is executable and includes validation steps and references to official docs.", "weight": 4.5, "judge_prompt": "Review the POC DOCX for practical feasibility:\n- Steps form an executable sequence (environment setup, networking, LB, compute, data store, static hosting/CDN, Cloud Armor, DNS as relevant) with purposes for each step.\n- Includes validation/test plan (e.g., health checks, failover tests, CDN behavior, WAF rule effectiveness).\n- References link to official Google Cloud documentation for main services used.\n- Includes a rollback/backout or cleanup plan.\n\nScoring:\n- 4.5: Fully executable sequence; clear validations; multiple official references; rollback included.\n- 3.5: Mostly executable; validations present; at least one official reference; partial rollback.\n- 2.5: Executable but thin on validation or references.\n- 1.5: Fragmented steps; minimal validation; unclear feasibility.\n- 0.0: Not realistically executable.", "expectation": "A concrete, testable POC plan that can be followed by an engineer."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality Assessment (Professional Readiness)", "description": "Holistic evaluation of clarity, presentation, and customer readiness.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Technical Soundness & Modernization Strategy", "description": "Assess whether the solution shows solid architectural judgment and a modernization path.", "weight": 2.5, "judge_prompt": "Evaluate the overall technical soundness and modernization strategy:\n- Sensible choice of managed services (Cloud Run/GKE/MIGs, Cloud SQL/Firestore, Cloud CDN, Cloud Armor).\n- Clear rationale for modernization (e.g., stateless web tier, containerization, CI/CD hooks) and migration path.\n- Avoids anti-patterns (single-zone, public DB, lack of health checks).\n\nScore 0.0\u20132.5 based on depth and correctness of architectural reasoning.", "expectation": "A modern, manageable, and secure architecture with a clear modernization path."}, {"type": "llm_judge", "name": "Clarity, Organization, and Audience Appropriateness", "description": "Professional writing quality and suitability for customer stakeholders (technical and delivery leads).", "weight": 2.5, "judge_prompt": "Assess clarity, organization, and tone:\n- Documents have clear headings, logical flow, concise and readable bullets.\n- Terminology is accurate and appropriate for technical stakeholders.\n- Minimal ambiguity; actionable content.\n\nScore 0.0\u20132.5 based on clarity and organization.", "expectation": "Well-structured, concise, and audience-appropriate documents."}, {"type": "llm_judge", "name": "Diagram Quality & Conventions", "description": "Professionalism and readability of the PDF architecture diagram using official GCP iconography.", "weight": 2.5, "judge_prompt": "Evaluate the diagram\u2019s professional quality:\n- Uses official GCP icons; consistent labeling; legible fonts; appropriate grouping (VPC, subnets, regions).\n- Clear data flows and legends/annotations where needed.\n- Visually mirrors professional architecture standards.\n\nScore 0.0\u20132.5 based on visual clarity and adherence to conventions.", "expectation": "A clear, professional GCP diagram with proper iconography and labeling."}, {"type": "llm_judge", "name": "Customer Readiness & Practical Value", "description": "Does the package help the customer move forward confidently?", "weight": 2.5, "judge_prompt": "Judge the overall customer readiness:\n- Materials enable a productive review with technical team and delivery leads.\n- Risks/assumptions are identified; effort sequencing is realistic; environment boundaries clear.\n- POC plan provides measurable outcomes and next steps.\n\nScore 0.0\u20132.5 based on practical usefulness and readiness.", "expectation": "Actionable deliverables that facilitate decision-making and execution."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "afe56d05-dac8-47d7-a233-ad1d035ca5bd", "rubric": {"category_name": "Editors \u2014 Special Reporting Situations Guide (WorldCast)", "rationale": "This rubric enforces a self-documenting, verifiable Word/PDF guide for newly hired journalists. Stage 1 (LLM-only) strictly gates structure and format so later verification is trivial. Stage 2 mixes lightweight code checks (bounds, presence, links) with stronger LLM judgments on ethical correctness and actionability. Stage 3 evaluates overall editorial quality, neutrality, and applicability to an international newsroom.", "max_total_score": 30.0, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate (LLM only)", "description": "Gate: Verify the candidate produced a properly structured DOCX/PDF guide with required title, opening note, sections, and linkable references. No quality judgments, only presence/shape.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 5.5, "rules": [{"type": "llm_judge", "name": "Document Format, Title, and Opening Note Present", "description": "Checks that output is a DOCX or PDF with professional document formatting; exact title present; opening note encouraging escalation to supervisors appears at the beginning; approximate length near requested range.", "weight": 3.5, "judge_prompt": "You are verifying structural conformance only. Examine the primary output.\n\nPASS CRITERIA (structure only):\n- File is a DOCX or PDF (not plain text/Excel).\n- Title appears prominently and reads exactly: \"WorldCast GUIDANCE ON SPECIAL REPORTING SITUATIONS\" (case-insensitive flexibility allowed for capitalization; text must clearly match the phrase).\n- The document begins with a brief note encouraging journalists to raise issues not covered with supervisors and to continue discussing concerns with colleagues/supervisors.\n- Length is approximately 2,200\u20132,300 words (flexible tolerance \u00b110%).\n- Professional document formatting is evident (headings/subheadings, paragraphs, bullets).\n\nScoring:\n- 3.5: All points above present.\n- 2.5\u20133.0: Valid DOCX/PDF and title present and opening note present; length within 2,000\u20132,500 words.\n- 1.0\u20132.0: Valid DOCX/PDF and at least two of: title, opening note, reasonable length, professional formatting.\n- 0: Not DOCX/PDF or missing both title and opening note.\nOnly check presence/shape, not content quality.\n", "expectation": "A properly formatted DOCX/PDF with the exact title, an opening escalation note, and roughly 2,200\u20132,300 words."}, {"type": "llm_judge", "name": "Required Sections and Subheadings", "description": "Checks that all nine specified situations appear as subheadings/sections with clear labeling. Structure only.", "weight": 3.0, "judge_prompt": "Check structure only. Confirm the document contains all nine required sections as clear subheadings or section headers (be flexible with minor naming variants):\n1) Conflicts and combat activities\n2) Terrorism\n3) Hostage and barricade situations\n4) Protests/demonstrations\n5) Criminal activity\n6) Victims\n7) Mass shootings\n8) Suicides\n9) Identifying and interviewing minors\n\nAlso check that each of the nine sections includes explicit Dos and Don\u2019ts (labels such as \"Do:\" and \"Don\u2019t:\" or equivalent phrasing) preferably using bullets or lists.\n\nScoring:\n- 3.0: All nine sections clearly present as headers with visible Do/Don\u2019t guidance in each.\n- 2.0\u20132.5: All nine sections present but 1\u20132 sections lack explicit Do/Don\u2019t labels or lists.\n- 1.0\u20131.5: 7\u20138 sections present or multiple sections lack Do/Don\u2019t structure.\n- 0: Fewer than 7 sections present or sections not identifiable as headers.\nOnly verify structural presence, not correctness of guidance.\n", "expectation": "Nine clearly labeled sections, each with Do/Don\u2019t lists."}, {"type": "llm_judge", "name": "References and Hyperlinks Structure", "description": "Checks that credible references are present with hyperlinks to reputable sources (SPJ and at least some of the provided resources). Structure only.", "weight": 1.5, "judge_prompt": "Check structure only. Verify that the document includes references with hyperlinks to reputable sources. Look for explicit hyperlinks or clearly stated URLs to: SPJ (spj.org) and at least some of: thenewsmanual.net, icfj.org, poynter.org, rcfp.org, unicef.org. Inline links or a references section both acceptable.\n\nScoring:\n- 1.5: Hyperlinked references present to SPJ and at least three of the listed resources.\n- 1.0: Hyperlinked references to SPJ and at least two others.\n- 0.5: Some references/hyperlinks present but not clearly including SPJ.\n- 0: No visible references/hyperlinks.\nOnly verify presence/structure, not citation quality.\n", "expectation": "Visible hyperlinks to SPJ and multiple listed reputable sources."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification of Compliance and Correctness", "description": "Now that the structure is valid, verify key correctness aspects: length bounds, title presence, coverage of sections, and ethical/actionable content aligned with reputable guidance.", "is_required": true, "max_points": 14.0, "min_score_to_pass": 8.0, "rules": [{"type": "code", "name": "Word Count Within Requested Range", "description": "Parses DOCX/PDF text and checks word count against 2,200\u20132,300 target with graded tolerance.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float or (float, str)\n    \"\"\"\n    weight = 0.8\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    # Try to read text from DOCX or PDF\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n    if not text:\n        return 0.0\n\n    words = re.findall(r\"\\b\\w+[\\-']*\\w*\\b\", text)\n    wc = len(words)\n\n    # Scoring bands: full credit at 2200\u20132300; 80% at 2100\u20132400; 50% at 2000\u20132500; else 0\n    score_ratio = 0.0\n    if 2200 <= wc <= 2300:\n        score_ratio = 1.0\n    elif 2100 <= wc <= 2400:\n        score_ratio = 0.8\n    elif 2000 <= wc <= 2500:\n        score_ratio = 0.5\n    else:\n        score_ratio = 0.0\n\n    return weight * score_ratio"}, {"type": "code", "name": "Title and WorldCast Presence Check", "description": "Confirms the exact title phrase is present and that the organization name is referenced early in the document.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 0.6\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n    if not text:\n        return 0.0\n\n    tlow = text.lower()\n    exact_title = \"worldcast guidance on special reporting situations\"\n    has_exact = exact_title in tlow\n\n    # Check early reference to WorldCast in first 600 characters\n    early = tlow[:600]\n    has_worldcast_early = \"worldcast\" in early\n\n    if has_exact and has_worldcast_early:\n        return weight\n    elif has_worldcast_early or has_exact:\n        return weight * 0.6\n    else:\n        return weight * 0.0"}, {"type": "code", "name": "References to Reputable Sources (Domains)", "description": "Detects presence of references/URLs to SPJ and at least some of the listed organizations.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 0.6\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n    if not text:\n        return 0.0\n\n    tlow = text.lower()\n    domains = [\n        \"spj.org\",\n        \"thenewsmanual.net\",\n        \"icfj.org\",\n        \"poynter.org\",\n        \"rcfp.org\",\n        \"unicef.org\",\n    ]\n    present = set(d for d in domains if d in tlow)\n\n    # Require SPJ plus others for higher credit\n    has_spj = \"spj.org\" in present\n    count = len(present)\n\n    if has_spj and count >= 4:\n        return weight\n    elif has_spj and count >= 2:\n        return weight * 0.6\n    elif count >= 2:\n        return weight * 0.4\n    elif count == 1:\n        return weight * 0.2\n    else:\n        return 0.0"}, {"type": "code", "name": "Nine Topics Coverage (Fuzzy Text Match)", "description": "Fuzzy-matches the nine required topic areas in the extracted text to ensure each is addressed explicitly.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 0.6\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n    if not text:\n        return 0.0\n\n    tlow = text.lower()\n    # Each topic with synonyms/variants\n    topics = [\n        [\"conflicts\", \"combat\"],\n        [\"terrorism\", \"terrorist\"],\n        [\"hostage\", \"barricade\"],\n        [\"protest\", \"demonstration\"],\n        [\"criminal\", \"crime\"],\n        [\"victim\"],\n        [\"mass shooting\", \"active shooter\", \"mass-shooting\"],\n        [\"suicide\", \"self-harm\", \"self harm\"],\n        [\"minors\", \"children\", \"youth\", \"under-18\", \"under 18\"],\n    ]\n\n    covered = 0\n    for variants in topics:\n        found = any(v in tlow for v in variants)\n        if found:\n            covered += 1\n\n    ratio = covered / 9.0\n    # Full credit at 9/9, partial linear scaling; below 7/9 cap at 0.5\n    if covered >= 9:\n        return weight\n    elif covered >= 8:\n        return weight * 0.85\n    elif covered >= 7:\n        return weight * 0.7\n    elif covered >= 5:\n        return weight * 0.5\n    else:\n        return weight * 0.0"}, {"type": "llm_judge", "name": "Ethical Correctness vs. SPJ Code and Best Practices", "description": "Checks whether guidance aligns with SPJ Code of Ethics and widely accepted standards: minimize harm, avoid sensationalism, verify information, transparency, and accountability.", "weight": 3.6, "judge_prompt": "Assess correctness against ethical standards. The guidance should:\n- Align with SPJ Code of Ethics (seek truth and report it; minimize harm; act independently; be accountable and transparent).\n- Avoid sensationalism and loaded language; emphasize precise language and verification.\n- Include privacy and dignity protections, especially for victims and minors.\n- Address safety/operational prudence (e.g., live reporting in conflict/hostage situations, withholding sensitive tactical details).\n\nScoring:\n- 3.6: Strong alignment throughout; no contradictions to SPJ; explicit mention of minimizing harm and verification; clear guardrails on sensitive details.\n- 2.5\u20133.0: Generally aligned with minor gaps or a small number of ambiguous points.\n- 1.0\u20132.0: Mixed alignment with notable omissions or occasional conflicting advice.\n- 0: Guidance contradicts core ethics or encourages harmful/sensational practices.\n", "expectation": "Guidance consistently embodies SPJ principles with concrete ethical guardrails."}, {"type": "llm_judge", "name": "Actionable Do/Don\u2019t Guidance per Section", "description": "Evaluates whether each section provides concrete, operational Dos and Don\u2019ts (verbs, examples, specific boundaries) rather than vague platitudes.", "weight": 3.0, "judge_prompt": "Judge the actionability of the Do/Don\u2019t items across all nine sections. Strong outputs include:\n- Clear imperative verbs (e.g., \"Verify\", \"Seek\", \"Do not identify\", \"Avoid broadcasting tactics in real time\").\n- Specifics applicable to the situation (e.g., not naming minors without guardian consent; avoiding method details in suicide stories; avoiding publication of hostage rescue tactics; caution when live-streaming protests).\n- Where appropriate, quick examples or clarifying parentheticals.\n\nScoring:\n- 3.0: All or nearly all sections contain concrete, operational Dos/Don\u2019ts.\n- 2.0\u20132.5: Most sections actionable; a few are generic.\n- 1.0\u20131.5: Many items are vague; limited operational value.\n- 0: Largely generic platitudes without clear actions.\n", "expectation": "Practical, step-by-step Dos and Don\u2019ts tailored to each scenario."}, {"type": "llm_judge", "name": "Legal and Safety Awareness", "description": "Checks inclusion of key legal risk and safety considerations relevant to these scenarios.", "weight": 2.4, "judge_prompt": "Evaluate whether the guide demonstrates awareness of legal and safety risks, including:\n- Legal: defamation/libel, contempt of court, presumption of innocence, privacy and identification (victims and minors), source protection, restrictions during active operations, permissions/consent.\n- Safety: operational security (no revealing tactics/positions in conflicts/hostage incidents), avoiding interference with law enforcement, situational risk assessments for field crews, duty of care.\n- International context: acknowledges variations in laws/jurisdictions; frames specifics as examples where relevant.\n\nScoring:\n- 2.4: Comprehensive legal and safety awareness integrated into relevant sections.\n- 1.5\u20132.0: Good coverage with some gaps.\n- 0.5\u20131.0: Limited or scattered mentions.\n- 0: No meaningful legal/safety guidance.\n", "expectation": "Clear, non-contradictory legal and safety guardrails across sections."}, {"type": "llm_judge", "name": "Minors and Suicide Coverage Specificity", "description": "Checks for recognized best practices for minors and suicide reporting (consent, privacy, language, and method details).", "weight": 2.4, "judge_prompt": "Focus on two areas: minors and suicide coverage.\nExpectations:\n- Minors: do not identify without guardian consent and clear public-interest justification; avoid leading questions; protect privacy and dignity; obtain assent/consent where appropriate; explain power dynamics; avoid showing faces where risky.\n- Suicide: avoid describing methods or publishing locations/notes in detail; use non-stigmatizing language; include help resources when appropriate; avoid simplistic causation; consult reputable guidelines (e.g., WHO-based, or Poynter/SPJ guidance).\n\nScoring:\n- 2.4: Both sections include specific, widely recognized best practices.\n- 1.5\u20132.0: One section strong; the other somewhat generic.\n- 0.5\u20131.0: Both sections present but shallow.\n- 0: Missing or incorrect/harmful guidance.\n", "expectation": "Specific, recognized best practices for minors and suicide reporting are present and correct."}], "on_failure_action": "skip_remaining", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Holistic Quality Assessment", "description": "Professionalism, neutrality, clarity, and usefulness for WorldCast\u2019s international newsroom.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Neutrality and WorldCast Standards", "description": "Assesses tone neutrality and alignment with WorldCast\u2019s \u201cLeast Biased\u201d and \u201cHigh factual reporting\u201d reputation.", "weight": 2.5, "judge_prompt": "Judge whether the document maintains a neutral, non-partisan tone; avoids loaded/sensational language; emphasizes verification and balanced sourcing; and reflects a high-factual, least-biased standard consistent with WorldCast\u2019s brand.\n\nScoring:\n- 2.5: Consistently neutral, careful language throughout.\n- 1.5\u20132.0: Mostly neutral with minor lapses.\n- 0.5\u20131.0: Noticeable bias/sensational phrasing in places.\n- 0: Overt bias or sensationalism.\n", "expectation": "Even-handed language with emphasis on verification and balance."}, {"type": "llm_judge", "name": "Organization and Readability", "description": "Assesses structure, flow, headings, bulleting, and ease of reference under deadline pressure.", "weight": 2.0, "judge_prompt": "Evaluate overall organization and readability: clear hierarchy of headings, scannable lists, consistent formatting, and logical flow. Consider whether a journalist under deadline could quickly find relevant guidance.\n\nScoring:\n- 2.0: Excellent structure; very easy to navigate.\n- 1.0\u20131.5: Generally good with minor issues.\n- 0.5: Disorganized or inconsistent.\n- 0: Very hard to follow.\n", "expectation": "Clear, consistent headings and bullets enabling quick reference."}, {"type": "llm_judge", "name": "International Applicability", "description": "Assesses whether guidance is internationally aware and not narrowly jurisdiction-specific.", "weight": 2.0, "judge_prompt": "Assess whether the guidance is suitable for an international broadcast organization: avoids assuming a single country\u2019s laws/culture; where specific legal references are made, they are framed as examples and emphasize checking local law; language and examples travel well.\n\nScoring:\n- 2.0: Strong global applicability; explicit acknowledgment of jurisdictional differences.\n- 1.0\u20131.5: Generally applicable with a few local assumptions.\n- 0.5: Mostly local/US-centric without caveats.\n- 0: Inappropriate for international use.\n", "expectation": "Guidance usable across jurisdictions with appropriate caveats."}, {"type": "llm_judge", "name": "Practicality and Brevity", "description": "Evaluates concision and usefulness at ~2,200\u20132,300 words without unnecessary filler.", "weight": 1.5, "judge_prompt": "Judge whether the guide is concise yet comprehensive (around 2,200\u20132,300 words), avoids fluff, and provides practical checklists/steps that add value over platitudes.\n\nScoring:\n- 1.5: Concise, practical, and comprehensive.\n- 1.0: Mostly concise/practical with minor padding or gaps.\n- 0.5: Wordy or thin on practical value.\n- 0: Largely filler or missing substance.\n", "expectation": "Crisp, practical guidance at the requested length."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "74ed1dc7-1468-48a8-9071-58775c0d667a", "rubric": {"category_name": "Sales Managers \u2014 ERP Order Type Proposal (Wholesale Trade)", "rationale": "This rubric enforces a self-documenting, verifiable Word document proposal for new ERP order types tailored to a fast-growing sportswear brand's key accounts. Stage 1 (LLM-only) mandates a precise document structure that enables verification. Stage 2 combines lightweight code checks (coverage and anchors) with LLM judges for nuanced correctness (differentiation, lifecycle integrity, feasibility). Stage 3 assesses overall professional quality for the executive audience. Code rules use the provided API and are robust to DOCX/PDF text extraction, though Stage 1 requires DOCX as the contract of delivery.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Format and Structural Gate (LLM only)", "description": "Gate: The proposal must be a DOCX with a precise structure enabling verification. If this fails, the entire category is zeroed.", "is_required": true, "max_points": 5.0, "min_score_to_pass": 3.5, "rules": [{"type": "llm_judge", "name": "Word Document Structure and Completeness Gate", "description": "Verify the candidate produced a DOCX proposal with all required sections and per-type mini-spec structure to enable downstream verification.", "weight": 5.0, "judge_prompt": "You are evaluating whether the submitted output satisfies STRICT structure requirements. Only assess PRESENCE/FORMAT, not content quality or correctness.\n\nRequired FORMAT:\n- Must be a Word document (DOCX), not PDF/Excel/text.\n- At least 3 pages OR approximately 1200+ words.\n- Professional document with clear headings and section hierarchy.\n\nRequired SECTIONS (flexible on exact titles, but meaning must match):\n1) Title and Document Metadata\n   - Title including terms like \"ERP Order Types Proposal\" or similar\n   - Date and/or author/team\n2) Executive Summary (must be on page 1 or 2)\n3) Current State and Challenges\n   - References existing order types: Pre-Order, Re-Order, Bulk\n   - Summarizes key process/reporting pain points\n4) Proposed New Order Types (core section)\n   - At least 4 NEW or changed order types, in addition to Pre-Order/Re-Order/Bulk\n   - For EACH proposed type, there is a structured mini-spec as labeled subsections or a table including:\n     a) Name\n     b) Definition/Purpose\n     c) Eligibility/Triggers (when to use)\n     d) Lifecycle/Statuses (status path and transitions)\n     e) Required ERP Data Fields (e.g., channel, season, delivery window, cancel reason, allocation)\n     f) Reporting/Accounting Impact (e.g., revenue recognition, allocation/inventory implications)\n     g) Rationale (problem solved)\n     h) Edge Cases/Exceptions\n5) Implementation Plan and Governance\n   - Phased rollout, change management/training, owner(s), timeline, risks\n6) Metrics and SLAs\n   - KPIs and measurement plan\n7) Risks and Mitigations (can be combined with Implementation if clearly present)\n8) Appendix/Supporting Material\n   - Glossary (expands KA = Key Accounts, KAM = Key Account Manager, PO = Purchase Order)\n   - Optional: RACI and/or Data Dictionary\n\nScoring (0 to 5):\n- 5.0: DOCX + length ok + all required sections present; \"Proposed New Order Types\" includes \u22654 types each with all mini-spec items (a\u2013h) clearly labeled.\n- 4.0: DOCX + length ok + all major sections present; proposed types section has \u22654 types with minor omissions (missing 1\u20132 mini-spec items across a few types).\n- 3.0: DOCX + length ok + most sections present; proposed types section has \u22653 types with partial mini-spec coverage.\n- 2.0: DOCX but short OR multiple missing sections; proposed types section insufficient (\u22642 types) or poorly structured.\n- 1.0: DOCX with minimal structure; fails to provide the core sections.\n- 0.0: Not a DOCX OR clearly missing most required sections.\n\nOnly evaluate structure/presence and document format; do not judge correctness or writing quality.", "expectation": "A well-structured DOCX proposal with all mandated sections and at least four fully specified new order types."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification of Correctness and Feasibility", "description": "Now that the shape is correct, verify correctness, differentiation, completeness of lifecycle/data, and implementation feasibility. Mix of code checks for anchors and LLM judges for nuanced validation.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Substantive Length and Section Anchor Coverage", "description": "Checks that the document has substantive length and includes key section anchors indicating comprehensive coverage.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns: float score in [0, 0.5] with brief feedback\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output or wrong type.\"\n\n    text = \"\"\n    try:\n        # Prefer DOCX; fallback to PDF text if needed\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = context.files.read_pdf_text(output.id)\n    except Exception:\n        return 0.0, \"Unable to read document text.\"\n\n    if not text or len(text.strip()) == 0:\n        return 0.0, \"Empty document text.\"\n\n    # Length scoring component (max 0.3 of 0.5)\n    words = len(re.findall(r\"\\b\\w+\\b\", text))\n    length_score = 0.0\n    if words >= 1800:\n        length_score = 0.3\n    elif words >= 1200:\n        length_score = 0.25\n    elif words >= 700:\n        length_score = 0.15\n    else:\n        length_score = 0.0\n\n    # Section anchors (max 0.2 of 0.5)\n    anchors = [\n        \"executive summary\",\n        \"current state\",\n        \"challenge\",\n        \"proposed\",\n        \"order type\",\n        \"implementation\",\n        \"metrics\",\n        \"kpi\",\n        \"risk\",\n        \"appendix\",\n    ]\n    lower = text.lower()\n    hit = sum(1 for a in anchors if a in lower)\n    anchor_score = 0.2 * min(hit / 6.0, 1.0)  # up to 6 anchors needed for full credit\n\n    score = length_score + anchor_score\n    feedback = f\"Words={words}, anchors_hit={hit}/10.\"\n    return min(score, 0.5), feedback"}, {"type": "code", "name": "Abbreviation and Glossary Consistency", "description": "Verifies that KA/KAM/PO abbreviations and their expansions are present and that a Glossary/Definitions section exists.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document or wrong type.\"\n    try:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = context.files.read_pdf_text(output.id)\n    except Exception:\n        return 0.0, \"Unable to read document text.\"\n\n    lower = text.lower()\n\n    # Abbreviations\n    has_ka = ' ka ' in f\" {lower} \" or 'ka=' in lower or 'ka:' in lower\n    has_kam = ' kam ' in f\" {lower} \" or 'kam=' in lower or 'kam:' in lower\n    has_po = ' po ' in f\" {lower} \" or 'po=' in lower or 'po:' in lower\n\n    # Expansions (loose match)\n    has_key_account = 'key account' in lower\n    has_key_account_manager = 'key account manager' in lower\n    has_purchase_order = 'purchase order' in lower\n\n    # Glossary presence\n    has_glossary = ('glossary' in lower) or ('definitions' in lower)\n\n    score = 0.0\n    details = []\n\n    # Abbrev presence up to 0.3 (0.1 each)\n    ab_score = 0.0\n    for present, name in [(has_ka,'KA'),(has_kam,'KAM'),(has_po,'PO')]:\n        if present:\n            ab_score += 0.1\n            details.append(f\"{name} present\")\n        else:\n            details.append(f\"{name} missing\")\n\n    # Expansion presence up to 0.15 (0.05 each only if corresponding abbrev present)\n    exp_score = 0.0\n    if has_ka and has_key_account:\n        exp_score += 0.05\n    if has_kam and has_key_account_manager:\n        exp_score += 0.05\n    if has_po and has_purchase_order:\n        exp_score += 0.05\n\n    # Glossary up to 0.05\n    gloss_score = 0.05 if has_glossary else 0.0\n\n    score = ab_score + exp_score + gloss_score\n    return min(score, 0.5), \", \".join(details) + (\"; glossary present\" if has_glossary else \"; glossary missing\")"}, {"type": "code", "name": "KPI and Reporting Anchors", "description": "Checks for presence of KPI/metric language and common supply chain/sales KPIs indicating a measurable reporting plan.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document or wrong type.\"\n    try:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = context.files.read_pdf_text(output.id)\n    except Exception:\n        return 0.0, \"Unable to read document text.\"\n\n    lower = text.lower()\n\n    anchors = [\"kpi\", \"metric\", \"sla\", \"okr\", \"dashboard\", \"reporting\", \"visibility\"]\n    kpi_terms = [\n        \"fill rate\", \"on-time\", \"on time\", \"otif\", \"cancellation rate\", \"cancel rate\",\n        \"backorder\", \"lead time\", \"revenue recognition\", \"inventory accuracy\",\n        \"forecast accuracy\", \"service level\", \"gross margin\", \"net sales\", \"gross sales\"\n    ]\n\n    anchor_hits = sum(1 for a in anchors if a in lower)\n    kpi_hits = sum(1 for k in kpi_terms if k in lower)\n\n    # Require at least some anchor context\n    if anchor_hits == 0 and kpi_hits == 0:\n        return 0.0, \"No KPI/reporting anchors detected.\"\n\n    # Score up to 0.5: 0.2 for anchors, 0.3 for KPI terms\n    score = min(anchor_hits, 5) / 5.0 * 0.2 + min(kpi_hits, 6) / 6.0 * 0.3\n    feedback = f\"anchors={anchor_hits}, kpi_terms={kpi_hits}\"\n    return min(score, 0.5), feedback"}, {"type": "llm_judge", "name": "Differentiation and Problem-Solution Fit", "description": "Each proposed order type is clearly differentiated from Pre-Order/Re-Order/Bulk and addresses specific pain points; explanations show how reporting becomes unambiguous.", "weight": 2.2, "judge_prompt": "Evaluate correctness of the proposal with focus on DIFFERENTIATION and PROBLEM-SOLUTION FIT:\n- For each proposed new order type, is it clearly distinct from existing Pre-Order/Re-Order/Bulk? (naming, use case boundaries, when-to-use rules)\n- Does the rationale tie to specific challenges in current state (e.g., manual manipulation, misreporting, lack of auditability)?\n- Does it explain how the new type improves reporting clarity (e.g., unique statuses, distinct fields, reduced ambiguity)?\n- Are concrete examples or scenarios provided for key accounts (KA/KAM) where applicable?\n\nScoring (0 to 2.2):\n- 2.2: \u22654 types, each well-differentiated with precise rules; strong tie to challenges; clear reporting improvements with examples.\n- 1.6: \u22654 types, mostly differentiated; ties to challenges present but inconsistent; some ambiguity remains.\n- 1.0: 3\u20134 types with partial differentiation and weak challenge linkage.\n- 0.5: \u22642 types or mostly overlapping with existing types; unclear problem fit.\n- 0.0: No meaningful differentiation or tie to challenges.", "expectation": "New order types have crisp boundaries, explicit use criteria, and compelling rationale that solves known reporting issues."}, {"type": "llm_judge", "name": "Lifecycle and Data Completeness for Reporting", "description": "Checks that each proposed type includes lifecycle/status transitions, mandatory ERP fields, and clear reporting/accounting implications enabling unambiguous dashboards and revenue recognition.", "weight": 2.2, "judge_prompt": "Evaluate whether lifecycle and data design is complete and enables reliable reporting:\n- Are lifecycle/status transitions defined per type (including entry/exit criteria, transitions, and stop conditions)?\n- Are mandatory ERP fields specified (e.g., channel, season, delivery window, allocation, cancel reason, split shipment, substitution, priority/hold codes)?\n- Are reporting/accounting impacts described (e.g., revenue recognition timing, inventory allocation, forecasting signals)?\n- Are edge cases/exceptions and validation rules addressed?\n\nScoring (0 to 2.2):\n- 2.2: All types include clear lifecycle, mandatory fields, reporting/accounting implications, and edge cases with validation rules.\n- 1.6: Most types include lifecycle and fields; reporting impacts present but not exhaustive; limited edge case coverage.\n- 1.0: Partial lifecycle/fields; minimal reporting/accounting guidance.\n- 0.5: Largely missing lifecycles/fields; ambiguous reporting implications.\n- 0.0: Not addressed.", "expectation": "Per-type mini-specs define statuses, required fields, and accounting/reporting logic sufficient for unambiguous dashboards."}, {"type": "llm_judge", "name": "Implementation Feasibility, Governance, and Change Management", "description": "Assesses the practicality of rollout, ownership, and risk management across functions.", "weight": 2.1, "judge_prompt": "Evaluate the feasibility and governance of implementation:\n- Is there a phased rollout plan with timeline/milestones and back-compat handling for legacy orders?\n- Are ownership, RACI, and cross-functional roles (Sales Ops, IT/ERP, Finance, Supply Chain, KAMs) defined?\n- Are training, communications, and change management covered?\n- Are risks and mitigations articulated with measurable success criteria (KPIs/SLAs and monitoring plan)?\n\nScoring (0 to 2.1):\n- 2.1: Clear phased plan, RACI/ownership, change mgmt, legacy migration, risks/mitigations, and measurable success criteria.\n- 1.5: Mostly feasible plan and ownership; partial change mgmt or risks; limited measurement detail.\n- 1.0: High-level plan; unclear ownership; light on risks/measurement.\n- 0.5: Vague feasibility with major gaps.\n- 0.0: Not addressed.", "expectation": "An actionable rollout with governance and change management that can realistically be executed across functions."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Executive Readiness", "description": "Holistic quality assessment for leadership audience: clarity, actionability, and presentation.", "is_required": false, "max_points": 7.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Executive Clarity and Persuasion", "description": "Assesses clarity, concise executive summary, and persuasive narrative tailored to leadership and PM stakeholders.", "weight": 2.0, "judge_prompt": "Assess executive communication quality:\n- Is the Executive Summary concise, outcome-oriented, and aligned to business impact (accuracy, speed, visibility)?\n- Is the narrative structured logically with clear takeaways for PMs and leadership?\n- Does it avoid jargon or explain it when necessary?\n\nScoring (0 to 2):\n- 2.0: Compelling, concise, and audience-appropriate with clear takeaways.\n- 1.4: Generally clear with minor verbosity or gaps.\n- 0.8: Mixed clarity; key points buried or technical without explanation.\n- 0.3: Hard to follow.\n- 0.0: Not suitable for executive audience.", "expectation": "A crisp executive summary and clear storyline supporting leadership decisions."}, {"type": "llm_judge", "name": "Actionability and Prioritization", "description": "Evaluates whether the proposal includes prioritized steps, owners, timelines, and acceptance criteria.", "weight": 2.0, "judge_prompt": "Assess actionability:\n- Are actions prioritized with owners and timelines/milestones?\n- Are acceptance criteria or definition of done stated for key steps?\n- Is there a clear path from proposal to execution (e.g., pilot, iterate, scale)?\n\nScoring (0 to 2):\n- 2.0: Concrete, prioritized plan with owners/timelines and clear acceptance criteria.\n- 1.4: Mostly actionable; minor gaps in ownership or acceptance criteria.\n- 0.8: High-level tasks; unclear ownership/timelines.\n- 0.3: Vague suggestions without execution detail.\n- 0.0: No actionable plan.", "expectation": "A prioritized, owner-assigned plan that can start immediately."}, {"type": "llm_judge", "name": "Presentation and Professional Polish", "description": "Checks formatting consistency, use of tables/figures for per-type specs, and readability.", "weight": 1.5, "judge_prompt": "Assess professional presentation:\n- Are headings, numbering, and formatting consistent?\n- Are tables or structured lists used for per-type mini-specs?\n- Is the document easy to scan with clear labels and visuals where helpful?\n\nScoring (0 to 1.5):\n- 1.5: Highly professional, consistent, and scannable with helpful tables/figures.\n- 1.1: Generally polished with minor inconsistencies.\n- 0.7: Mixed formatting; some sections hard to scan.\n- 0.3: Poorly formatted.\n- 0.0: Unprofessional or disorganized.", "expectation": "A clean, consistent document using tables/lists to enhance comprehension."}, {"type": "llm_judge", "name": "Cross-Functional Alignment and Risks", "description": "Evaluates how well the proposal aligns Sales, Finance, Supply Chain, and IT/ERP and anticipates cross-functional risks.", "weight": 1.5, "judge_prompt": "Assess cross-functional alignment:\n- Are impacts on Sales Ops, Finance (revenue recognition), Supply Chain (allocation/inventory), and IT/ERP addressed?\n- Are cross-functional risks and dependencies identified with mitigations?\n\nScoring (0 to 1.5):\n- 1.5: Strong, explicit alignment across functions with clear mitigations.\n- 1.1: Good coverage with minor gaps.\n- 0.7: Partial alignment; several gaps.\n- 0.3: Minimal alignment.\n- 0.0: Not addressed.", "expectation": "Demonstrated understanding of cross-functional needs and risks with mitigations."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "c6269101-fdc8-4602-b345-eac7597c0c81", "rubric": {"category_name": "Manufacturing | Industrial Engineering | Process Capability Study (Brightland Processing Center)", "rationale": "Pattern C (Mixed): The task combines quantitative analysis (capability/stability diagnostics using the provided Excel data) with a leadership-ready presentation (PowerPoint). To make verification trivial and self-documenting, the rubric mandates two artifacts: a structured Excel analysis workbook and a structured PPTX deck. Stage 1 (LLM-only) enforces this exact shape. Stage 2 blends lightweight code checks (sheet presence, numeric sanity, variability metrics) with higher-weight LLM cross-artifact consistency and methodological correctness. Stage 3 assesses overall communication quality, visual clarity, and actionability for executives.", "max_total_score": 30.0, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate (PPTX + Excel)", "description": "Gate: Ensure the agent delivers BOTH a leadership-ready PowerPoint deck and a companion Excel analysis workbook with mandated structure that enables verification.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.0, "rules": [{"type": "llm_judge", "name": "PPTX + Excel Structured Output Requirement", "description": "Verify the candidate produced a PPTX deck for leadership and an Excel analysis workbook with the exact sections enabling verification.", "weight": 6.0, "judge_prompt": "You are checking if the submission satisfies the REQUIRED shape. Consider ALL provided outputs. This is a strict gate.\n\nRequired Deliverables:\nA) PowerPoint deck (.pptx) prepared for leadership review\nB) Excel analysis workbook (.xlsx) that documents calculations and diagnostics\n\nPowerPoint Deck \u2014 Required Structure (be flexible with similar section names):\n1) Title slide: includes site name \u201cBrightland Processing Center\u201d and task scope (process capability/stability)\n2) Executive Overview/Key Findings: summary of capability and stability for all three processes, and which process has greatest variability\n3) Methodology: capability study approach, stability/controls (e.g., I-MR or p/u charts), time-trend review; clear note not to invent thresholds if not provided\n4) Three process sections (Task Duration, Failure Rate, System Errors). For EACH process, 2\u20133 slides that collectively show:\n   - Capability/performance summary (table or clear bullets with metrics/observations)\n   - Stability assessment over time (control chart or clear diagnostic explanation)\n   - Time-trend visualization or narrative\n5) Consolidated comparison: identifies process with greatest variability and why (e.g., coefficient of variation or equivalent) and highlights risk\n6) Recommendations/Next Steps slide tailored to findings\n7) Appendix: definitions, data notes, any backup visuals or references\n\nExcel Analysis Workbook \u2014 Required Structure (be flexible with similar section names):\n- Sheet: \u201cSummary\u201d (or similar) with a comparison table across the three processes including at least one variability metric (e.g., CV, stdev) and a stability flag/indicator (e.g., rule violations count or yes/no)\n- Sheets (one per process): \u201cTask Duration\u201d, \u201cFailure Rate\u201d, \u201cSystem Errors\u201d (accept close variants such as \u201cDuration\u201d, \u201cFailures/Defect Rate\u201d, \u201cSystem Errors\u201d). Each process sheet must include clearly labeled sections:\n  a) Data Profile: a small table with n/count, mean/average, stdev, min/max, median\n  b) Stability Diagnostics: table with chart type used and count/description of rule violations or out-of-control signals\n  c) Capability/Performance Summary: a table of metrics and notes; if spec limits/targets are not provided in the data, this must be stated (e.g., \u201cSpec limits: Not provided\u201d)\n  d) Time Trend/Windowed Stats: brief table or notes summarizing trend over time (e.g., trend observed yes/no, notes)\n- Optional: a \u201cControl Limits\u201d or \u201cCalc Log\u201d sheet with intermediate calculations\n\nScoring for this Gate (0 to 6):\n- 6: Both files present (PPTX + XLSX). PPT has all required sections; Excel has Summary + 3 process sheets with the required subsections per process.\n- 5: Both files present; minor omissions (e.g., one process missing one subsection OR Appendix absent).\n- 4: Both files present but missing multiple non-core elements (e.g., Summary present, but 1\u20132 process sheets are missing one or more subsections) OR PPT is strong but Excel is missing one process sheet.\n- 3: Only PPT provided, but it contains most required sections; Excel missing entirely.\n- 2: Wrong formats or largely incomplete (e.g., PDF report instead of PPTX; or Excel exists without required structure; or PPT absent and Excel alone).\n- 0: No usable deliverables or clearly wrong artifact types.\n\nOnly assess presence/format/structure. Do NOT grade correctness or quality here.", "expectation": "A complete PPTX deck plus a structured Excel workbook with a Summary sheet and one sheet per process, each containing Data Profile, Stability Diagnostics, Capability/Performance Summary (with explicit note if no spec limits), and Time Trend/Windowed Stats."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Verification (Analysis + Methods)", "description": "Now verify that the analytics are plausible, consistent, and methodologically appropriate using code checks (lightweight, deterministic) and LLM judges (deeper reasoning, cross-artifact consistency).", "is_required": false, "max_points": 16.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Excel Workbook Presence and Sheet Fuzzy Match", "description": "Find an Excel output and confirm presence of a Summary sheet and per-process sheets via fuzzy matching.", "weight": 1.5, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    # Locate an Excel output among all outputs\n    excels = []\n    for r in context.get_all_outputs() or []:\n        try:\n            if getattr(r, 'is_spreadsheet', False):\n                excels.append(r)\n        except Exception:\n            continue\n    if not excels:\n        return 0.0, 'No Excel analysis workbook found.'\n\n    # Use the first Excel workbook\n    wb = excels[0]\n    try:\n        path = context.files.get_path(wb.id)\n        xls = pd.ExcelFile(path)\n        sheets = [s.lower() for s in xls.sheet_names]\n    except Exception as e:\n        return 0.0, f'Failed to read Excel workbook: {e}'\n\n    # Fuzzy expectations\n    has_summary = any('summary' in s for s in sheets)\n\n    def has_process_sheet(keywords):\n        for s in sheets:\n            for kw in keywords:\n                if kw in s:\n                    return True\n        return False\n\n    duration_ok = has_process_sheet(['task duration','duration','cycle','cycle time'])\n    failure_ok = has_process_sheet(['failure','defect','failure rate'])\n    errors_ok = has_process_sheet(['system error','errors','error rate'])\n\n    found = sum([has_summary, duration_ok, failure_ok, errors_ok])\n    score = found / 4.0\n    feedback = f'Sheets found \u2014 Summary:{has_summary}, Duration:{duration_ok}, Failure:{failure_ok}, Errors:{errors_ok}'\n    return max(0.0, min(1.0, score)), feedback"}, {"type": "code", "name": "Numeric Profile Sanity and Variability Extract", "description": "Within each process sheet, locate basic stats (n, mean, stdev) by fuzzy label scan and verify plausible CV (>=0 and finite) when mean>0.", "weight": 1.5, "code": "import re\nimport numpy as np\nimport pandas as pd\n\nMETRIC_KEYS = {\n    'n': ['n','count','samples'],\n    'mean': ['mean','avg','average'],\n    'stdev': ['stdev','std','sigma','standard deviation']\n}\n\n\ndef extract_metrics(df):\n    metrics = {}\n    # Scan all cells: if a cell matches a metric label, take right-adjacent as value\n    for i in range(min(200, df.shape[0])):\n        for j in range(min(10, df.shape[1])):\n            val = str(df.iat[i, j]).strip().lower()\n            for key, keys in METRIC_KEYS.items():\n                if any(k == val or k in val for k in keys):\n                    # Prefer the cell to the right if numeric\n                    try:\n                        right = df.iat[i, j+1]\n                    except Exception:\n                        right = None\n                    cand = None\n                    for c in [right]:\n                        try:\n                            num = float(str(c).replace(',',''))\n                            cand = num\n                            break\n                        except Exception:\n                            pass\n                    if cand is not None:\n                        metrics[key] = cand\n    return metrics\n\n\ndef evaluate(workflow, context):\n    # Find Excel\n    excels = [r for r in (context.get_all_outputs() or []) if getattr(r, 'is_spreadsheet', False)]\n    if not excels:\n        return 0.0, 'No Excel workbook to validate.'\n    wb = excels[0]\n\n    try:\n        path = context.files.get_path(wb.id)\n        xls = pd.ExcelFile(path)\n        sheet_names = xls.sheet_names\n    except Exception as e:\n        return 0.0, f'Failed reading workbook: {e}'\n\n    # Identify process sheets via fuzzy match\n    proc_map = {}\n    for s in sheet_names:\n        sl = s.lower()\n        if any(k in sl for k in ['task duration','duration','cycle']):\n            proc_map['duration'] = s\n        if any(k in sl for k in ['failure rate','failure','defect']):\n            proc_map['failure'] = s\n        if any(k in sl for k in ['system error','errors','error rate']):\n            proc_map['errors'] = s\n\n    if not proc_map:\n        return 0.0, 'No recognizable process sheets found.'\n\n    valid = 0\n    total = len(proc_map)\n    cv_values = {}\n\n    for key, sheet in proc_map.items():\n        try:\n            df = pd.read_excel(path, sheet_name=sheet, header=None)\n            m = extract_metrics(df)\n            mean = m.get('mean', None)\n            stdev = m.get('stdev', None)\n            # CV check when mean>0 and stdev>=0\n            if mean is not None and stdev is not None and mean != 0:\n                cv = abs(stdev) / abs(mean)\n                if np.isfinite(cv) and cv >= 0:\n                    valid += 1\n                    cv_values[key] = cv\n        except Exception:\n            continue\n\n    score = valid / max(1, total)\n    feedback = f'Computed CV for {len(cv_values)}/{total} processes: {cv_values}'\n    return max(0.0, min(1.0, score)), feedback"}, {"type": "llm_judge", "name": "Methodological Correctness (SPC + Capability without invented thresholds)", "description": "Check that methods are appropriate: I-MR (or equivalent) for continuous task duration; p- or u-chart (or equivalent) for failure/errors; capability indices only when limits are provided; otherwise focus on variation, trends, and risk without inventing thresholds.", "weight": 5.0, "judge_prompt": "Evaluate methodology across PPT and the Excel workbook:\n- Task Duration: Uses appropriate continuous-data control chart (e.g., I-MR, X-bar/R) or a clear stability diagnostic for time series.\n- Failure Rate and System Errors: Use attribute-data charts (e.g., p-chart, u-chart, c-chart) or equivalent logic consistent with data definitions.\n- Capability indices (Cp/Cpk, DPMO, etc.) are only used when spec limits/targets are explicitly provided (and cited). If not provided, the analysis explicitly states that and focuses on variation (e.g., CV, stdev), trend, and control signals.\n- The methodology avoids creating thresholds not present in the data.\n\nScoring (0\u20135):\n- 5: Correct chart/diagnostic choices for all processes; capability indices only when justified by provided limits; explicit statement when limits are absent; no invented thresholds.\n- 3\u20134: Mostly correct; one minor mismatch or minor ambiguity about limits; overall avoids inventing thresholds.\n- 1\u20132: Several mismatches (e.g., wrong chart types) or unclear treatment of limits; hints of assumed thresholds.\n- 0: Clearly incorrect methods or invented thresholds used as the basis of conclusions.", "expectation": "Appropriate SPC selection by data type, and explicit handling of spec limits (use when provided; otherwise refrain)."}, {"type": "llm_judge", "name": "Cross-Artifact Consistency: Greatest Variability Identification", "description": "Verify that the process identified as having the greatest variability in the PPT matches the Excel Summary metrics and supporting evidence (e.g., CV, stdev, or equivalent) and time-trend visuals.", "weight": 5.0, "judge_prompt": "Check alignment between the PPT and Excel workbook:\n- Does the PPT clearly identify which process has the greatest variability?\n- Does the Excel Summary (or process sheets) contain a variability metric (e.g., CV, stdev, variance) that supports the PPT claim?\n- Do time-trend or control-chart visuals/narrative align with that decision?\n\nScoring (0\u20135):\n- 5: Clear, explicit match across PPT and Excel; metric cited (e.g., CV) and visuals support the claim.\n- 3\u20134: Generally consistent but not all references are explicit; evidence is present though light.\n- 1\u20132: Weak or ambiguous match; claim made but minimal evidence.\n- 0: Contradiction between PPT claim and Excel metrics, or no claim made.", "expectation": "PPT\u2019s stated highest-variability process agrees with Excel metrics and is defensible with visuals/notes."}, {"type": "llm_judge", "name": "Use of Provided Data Only (No Invented Limits/Thresholds)", "description": "Ensure the analysis uses the provided data, avoids invented thresholds, and flags limits as not provided when absent.", "weight": 3.0, "judge_prompt": "Assess data usage and threshold handling:\n- Are data sources clearly tied to the attached Process Capability Data.xlsx (or equivalent naming) and time windows?\n- If limits/targets are used, do they have an explicit source (e.g., data file, defined business standard)?\n- If not provided, does the analysis avoid fabricating numeric thresholds and instead focus on variation, trend, and risks?\n\nScoring (0\u20133):\n- 3: Clearly sourced data; limits/targets used only with explicit source; otherwise, analysis avoids invented thresholds.\n- 2: Mostly compliant with minor ambiguity.\n- 1: Some reliance on implicit/assumed thresholds.\n- 0: Clear invention of thresholds with no source.", "expectation": "No invented thresholds; clear sourcing of any limits; emphasis on trends/variation when limits are missing."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Executive Readiness", "description": "Holistic quality assessment of the presentation: clarity, prioritization, visuals, and actionability for leadership.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Executive Communication and Structure", "description": "Is the deck structured for leadership? Clear storyline from problem to findings to actions; succinct executive summary.", "weight": 2.0, "judge_prompt": "Evaluate executive communication quality:\n- Clear narrative: context \u2192 analysis \u2192 findings \u2192 implications\n- Executive summary succinctly conveys stability/capability status and prioritization\n- Slide titles are meaningful (message titles) and call attention to key insights\n\nScoring (0\u20132): 2 = excellent executive flow and clarity; 1 = adequate but could be sharper; 0 = unclear or analyst-centric with no executive framing.", "expectation": "Crisp, leadership-ready narrative with message-driven slides and a strong executive summary."}, {"type": "llm_judge", "name": "Visual Quality and Accessibility", "description": "Charts/tables are legible, labeled (units, axes), annotated for out-of-control points, and use accessible, high-contrast colors.", "weight": 2.0, "judge_prompt": "Assess visualization quality:\n- Axes, units, and labels are present and correct\n- Out-of-control or anomalous points are highlighted/annotated\n- Readability on typical screens; high contrast and minimal clutter\n\nScoring (0\u20132): 2 = professional and accessible visuals; 1 = acceptable with minor issues; 0 = poor/unclear visuals.", "expectation": "Professional, readable visuals with proper labels and annotations."}, {"type": "llm_judge", "name": "Actionability and Prioritization of Recommendations", "description": "Recommendations are practical, prioritized, tied to findings (root causes, controls, error-proofing), and note owners/next steps at a high level.", "weight": 2.0, "judge_prompt": "Evaluate recommendations:\n- Directly tied to identified risks (e.g., high variability process)\n- Practical corrective actions (e.g., standard work, error-proofing, SPC monitoring cadence)\n- Prioritized and framed with next steps/ownership at a high level\n\nScoring (0\u20132): 2 = specific, prioritized, and actionable; 1 = somewhat actionable but generic; 0 = vague or disconnected from findings.", "expectation": "Clear, prioritized actions aligned with findings and process risks."}, {"type": "llm_judge", "name": "Traceability and Definitions", "description": "Content is traceable to data and defines terms so stakeholders can interpret results correctly.", "weight": 2.0, "judge_prompt": "Assess traceability and definitions:\n- Slides/appendix cite the data file name/time window and any key parameters used\n- Definitions or brief glossary for terms like CV, control limits, rule violations\n- Linkage between PPT assertions and Excel appendix (e.g., sheet/section references)\n\nScoring (0\u20132): 2 = clear traceability and definitions; 1 = partial; 0 = none.", "expectation": "Citations to data, defined terms, and linkages between PPT and Excel details."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "fe0d3941-e32c-4bf1-a643-b566d2b4cb3c", "rubric": {"category_name": "Wholesale Trade \u2013 Technical/Scientific Sales: Non-Invasive Blood Analyte Concept Materials", "rationale": "Mixed deliverables: a structured presentation of workflows and a two-page PDF survey. Stage 1 enforces exact shapes so later checks are trivial. Stage 2 mixes precise code validation (file presence, counts) with LLM verification of content intent enabled by the mandated shape. Stage 3 judges professionalism and strategic value for stakeholder decision-making.", "max_total_score": 15.0, "stages": [{"name": "Stage 1 \u2013 Shape Enforcement Gate", "description": "MANDATORY structure check. Verify deliverables are present in correct formats with required sections so later verification is possible. LLM judges only.", "is_required": true, "max_points": 2.0, "min_score_to_pass": 1.5, "rules": [{"type": "llm_judge", "name": "Presentation Shape: Workflows Deck", "description": "Check presence and structure of the workflows presentation.", "weight": 1.0, "judge_prompt": "You are verifying ONLY the structure/format of the workflows presentation deliverable. Multiple files may be present. Identify the presentation file (prefer .pptx; a PDF slide export is acceptable if it clearly contains slides). Check strictly for presence and section headers/slide titles, not quality.\n\nAcceptable file types: PPTX (preferred) or a PDF slide deck.\n\nRequired structure within the presentation:\n- A title slide clearly indicating the deck is about \"Workflows\" for a non-invasive blood analyte device concept. Optional: a title image/illustration is allowed but not required.\n- A slide for the CURRENT process workflow (lab-based blood draw \u2192 lab processing \u2192 physician review/diagnosis). Should be clearly labeled as current process or similar.\n- A slide for the NEW/FUTURE process workflow (non-invasive, light-based sensing integrated in consumer device \u2192 instant data to physician). Should be clearly labeled as new/future process or similar.\n- A brief legend slide explaining workflow notation (e.g., meaning of shapes/arrows/colors) \u2014 can be titled \"Legend\", \"How to read this workflow\", or similar.\n- A slide highlighting benefits in diagnosis (e.g., instant results, reduced scheduling/friction, non-invasive), with a clear header like \"Benefits\" or \"Diagnostic Benefits\".\n\nScoring:\n- 1.0: Presentation file found and all five required slide types are present with clear slide headers.\n- 0.7: Presentation file found and 4 of 5 slide types are present.\n- 0.4: Presentation file found and 3 of 5 slide types are present.\n- 0.0: No presentation file or fewer than 3 required slides present, or wrong format.\n\nOnly assess presence/structure (slide topics/headers), not content quality or correctness.", "expectation": "A PPTX (or PDF slide export) with title, current workflow, new workflow, legend, and benefits slides."}, {"type": "llm_judge", "name": "Survey Shape: PDF with Two Separate Pages and Yes/No Questions", "description": "Check presence and structure of the survey PDF.", "weight": 1.0, "judge_prompt": "You are verifying ONLY the structure/format of the survey deliverable. Multiple files may be present. Identify the PDF survey document.\n\nRequired format and structure:\n- File type: PDF document.\n- Title of the survey present: \"Instant non-invasive blood analysis\" (case-insensitive, minor punctuation/spacing variations acceptable) near the beginning.\n- At least two pages total.\n- Two separate pages (or clearly separated sections) titled:\n  \u2022 \"Questions for physicians\" (or a close variant like \"Physician Questions\")\n  \u2022 \"Questions for non-physicians\" (or a close variant like \"Consumer/General Public Questions\")\n- Question constraints:\n  \u2022 Physicians: 5 to 7 questions.\n  \u2022 Non-physicians: 3 to 5 questions.\n  \u2022 All questions answerable with Yes/No only (e.g., each question is binary in form, or the section clearly instructs answers are Yes/No).\n\nScoring:\n- 1.0: Valid PDF, correct title, two distinct pages/sections for physicians and non-physicians, and question counts in required ranges with Yes/No-only answers.\n- 0.7: Valid PDF, correct title, two sections present but one section slightly off in count (\u00b11) or Yes/No format is implied but not explicit.\n- 0.4: Valid PDF and title present, but sections are not clearly separated or counts are outside ranges; Yes/No format unclear.\n- 0.0: Not a PDF, missing title, or missing one of the two sections/pages.\n\nDo not judge content quality or intent coverage; only verify the structural requirements.", "expectation": "A PDF titled as specified with two separate pages/sections, physician 5\u20137 yes/no questions, non-physician 3\u20135 yes/no questions."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification (Correctness and Consistency)", "description": "Now that the shape is enforced, verify correctness and consistency of the deliverables. Mix precise code checks with LLM reasoning enabled by the mandated structure.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Deliverables Present: Presentation and Survey Files", "description": "Confirm both required deliverables exist: a presentation (PPTX or a distinct PDF slide deck) and a survey PDF. Be flexible if the deck is exported to PDF separate from the survey PDF.", "weight": 0.6, "code": "import re\nfrom pathlib import Path\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Returns a normalized score in [0,1].\n    Checks presence of:\n      - Survey: a PDF containing the survey title\n      - Presentation: either a PPTX file, or a separate PDF that is not the survey (ideally containing 'workflow' in text)\n    \"\"\"\n    outputs = context.get_all_outputs() or []\n    if not outputs:\n        return 0.0, \"No outputs provided.\"\n\n    # Identify files by extension\n    pdfs = []\n    ppts = []\n    for r in outputs:\n        try:\n            suffix = Path(context.files.get_path(r.id)).suffix.lower()\n        except Exception:\n            continue\n        if suffix == '.pdf':\n            pdfs.append(r)\n        elif suffix in ['.pptx', '.ppt']:\n            ppts.append(r)\n\n    # Find survey PDF by title text\n    survey_pdf = None\n    survey_title_pattern = re.compile(r\"instant\\s*non-\\s*invasive\\s*blood\\s*analysis\", re.I)\n    for r in pdfs:\n        try:\n            txt = context.files.read_pdf_text(r.id) or \"\"\n        except Exception:\n            txt = \"\"\n        if survey_title_pattern.search(txt):\n            survey_pdf = r\n            break\n\n    # Determine presentation: prefer PPTX; else a non-survey PDF (ideally with 'workflow' text)\n    presentation = None\n    if ppts:\n        presentation = ppts[0]\n    else:\n        # Try to find a PDF distinct from survey that looks like a deck (heuristic: contains 'workflow' or 'workflows')\n        for r in pdfs:\n            if survey_pdf and r.id == survey_pdf.id:\n                continue\n            try:\n                txt = (context.files.read_pdf_text(r.id) or \"\").lower()\n            except Exception:\n                txt = \"\"\n            if 'workflow' in txt or 'workflows' in txt or 'slide' in txt:\n                presentation = r\n                break\n        # Fallback: any other non-survey PDF\n        if not presentation:\n            for r in pdfs:\n                if survey_pdf and r.id == survey_pdf.id:\n                    continue\n                presentation = r\n                break\n\n    have_survey = survey_pdf is not None\n    have_presentation = presentation is not None\n\n    if have_survey and have_presentation:\n        return 1.0, \"Found survey PDF and a presentation file.\"\n    if have_survey or have_presentation:\n        return 0.5, \"Only one deliverable found (survey=%s, presentation=%s).\" % (have_survey, have_presentation)\n    return 0.0, \"Neither survey PDF nor presentation file detected.\""}, {"type": "code", "name": "Survey Counts and Yes/No Structure Check", "description": "Validate the survey PDF has correct title, headings for both audiences, required number of questions, and binary (Yes/No) answerability.", "weight": 0.7, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Returns a normalized score in [0,1].\n    Heuristically parses survey PDF text to check:\n      - Title present\n      - Headings for physicians and non-physicians\n      - Question counts within required ranges\n      - Yes/No answerability signaled\n    \"\"\"\n    outputs = context.get_all_outputs() or []\n    if not outputs:\n        return 0.0, \"No outputs to evaluate.\"\n\n    # Find survey PDF by title\n    survey_pdf = None\n    survey_text = \"\"\n    title_ok = False\n    title_pattern = re.compile(r\"instant\\s*non-\\s*invasive\\s*blood\\s*analysis\", re.I)\n\n    for r in outputs:\n        try:\n            text = context.files.read_pdf_text(r.id)\n        except Exception:\n            text = None\n        if not text:\n            continue\n        if title_pattern.search(text or \"\"):\n            survey_pdf = r\n            survey_text = text\n            title_ok = True\n            break\n\n    if not survey_pdf:\n        return 0.0, \"Survey PDF with required title not found.\"\n\n    txt = (survey_text or \"\").strip()\n    low = txt.lower()\n\n    # Identify headings (flexible variants)\n    phys_headings = [\"questions for physicians\", \"physician questions\", \"for physicians\", \"physicians\"]\n    nonphys_headings = [\"questions for non-physicians\", \"non-physician questions\", \"for non-physicians\", \"consumers\", \"general public\", \"nonphysicians\", \"non physicians\"]\n\n    def find_heading_span(s, candidates):\n        idx = -1\n        label = None\n        for c in candidates:\n            i = s.find(c)\n            if i != -1 and (idx == -1 or i < idx):\n                idx = i\n                label = c\n        return idx, label\n\n    p_idx, p_lab = find_heading_span(low, phys_headings)\n    n_idx, n_lab = find_heading_span(low, nonphys_headings)\n\n    headings_ok = (p_idx != -1 and n_idx != -1)\n\n    # Extract sections in order by index\n    sections = {}\n    if headings_ok:\n        if p_idx < n_idx:\n            phys_sec = low[p_idx:n_idx]\n            nonphys_sec = low[n_idx:]\n        else:\n            nonphys_sec = low[n_idx:p_idx]\n            phys_sec = low[p_idx:]\n        sections['phys'] = phys_sec\n        sections['nonphys'] = nonphys_sec\n    else:\n        sections['all'] = low\n\n    # Heuristic question counting: lines with '?' or numbered items\n    def count_questions(s):\n        lines = [l.strip() for l in s.splitlines() if l.strip()]\n        q = 0\n        for l in lines:\n            if '?' in l:\n                q += 1\n            else:\n                if re.match(r\"^(\\d+\\.|\\d+\\)|\\u2022|-)\\s+\", l):\n                    # consider bullet a question if it ends with yes/no cue\n                    if 'yes' in l or 'no' in l:\n                        q += 1\n        return q\n\n    phys_q = count_questions(sections.get('phys', ''))\n    nonphys_q = count_questions(sections.get('nonphys', ''))\n\n    phys_count_ok = 5 <= phys_q <= 7\n    nonphys_count_ok = 3 <= nonphys_q <= 5\n\n    # Yes/No presence in each section\n    def has_yes_no(s):\n        return ('yes' in s and 'no' in s) or ('yes/no' in s)\n\n    phys_yesno = has_yes_no(sections.get('phys', low))\n    nonphys_yesno = has_yes_no(sections.get('nonphys', low))\n\n    score = 0.0\n    fb = []\n\n    if title_ok:\n        score += 0.25; fb.append('Title ok')\n    if headings_ok:\n        score += 0.25; fb.append('Headings for both audiences found')\n    else:\n        # partial if at least one heading detected\n        if p_idx != -1 or n_idx != -1:\n            score += 0.15; fb.append('One audience heading found')\n        else:\n            fb.append('No clear audience headings')\n\n    if phys_count_ok:\n        score += 0.25; fb.append(f'Physician questions in range ({phys_q})')\n    else:\n        fb.append(f'Physician questions out of range ({phys_q})')\n\n    if nonphys_count_ok:\n        score += 0.15; fb.append(f'Non-physician questions in range ({nonphys_q})')\n    else:\n        fb.append(f'Non-physician questions out of range ({nonphys_q})')\n\n    if phys_yesno and nonphys_yesno:\n        score += 0.10; fb.append('Yes/No answerability indicated for both sections')\n    elif phys_yesno or nonphys_yesno:\n        score += 0.05; fb.append('Yes/No indicated for one section')\n    else:\n        fb.append('Yes/No answerability not clearly indicated')\n\n    score = max(0.0, min(1.0, score))\n    return score, \"; \".join(fb) if fb else \"Parsed survey PDF.\""}, {"type": "llm_judge", "name": "Workflows Content Verification (Current vs New)", "description": "Verify the presentation actually conveys two distinct workflows with stepwise logic and a legend that maps symbols to meanings.", "weight": 2.4, "judge_prompt": "Evaluate the workflows presentation identified earlier (PPTX or PDF slide deck). Do not re-check basic presence/format; instead verify content consistency enabled by the required shape:\n\nChecks:\n1) The CURRENT process workflow represents lab-based blood collection, lab analysis, result reporting, and physician diagnosis. Steps should be sequentially clear.\n2) The NEW/FUTURE workflow shows non-invasive, light-based sensing integrated into a consumer device (e.g., phone/watch), periodic/instant data capture, and data availability to physician without lab visit.\n3) A legend slide explains symbols/shapes/arrows so the workflows are interpretable.\n4) The workflows are clearly distinguished (labeled or visually separated as current vs new/future).\n\nScoring (0\u20132.4):\n- 2.4: All four checks clearly satisfied.\n- 1.8: Three checks satisfied.\n- 1.2: Two checks satisfied.\n- 0.6: One check satisfied.\n- 0.0: None satisfied or cannot verify.", "expectation": "Two clear, distinct workflows (current lab process vs new non-invasive device process) with a legend explaining notation."}, {"type": "llm_judge", "name": "Benefits Slide Accuracy and Diagnostic Relevance", "description": "Evaluate whether the benefits slide captures key diagnostic advantages credibly tied to the new workflow.", "weight": 2.4, "judge_prompt": "Assess only the benefits slide in the workflows presentation.\n\nLook for mention of at least three of the following, framed as diagnostic benefits:\n- Instant/near-instant results or continuous monitoring\n- Reduced scheduling friction and faster clinical decision-making\n- Non-invasive (needle-free), improved patient experience/adherence\n- Potential reduction in variability via AI/ML calibration/algorithms\n- Increased data frequency enabling trend detection/earlier intervention\n- Potential to integrate with physician workflows/EHRs\n\nScoring (0\u20132.4):\n- 2.4: Mentions 4+ relevant benefits with clear diagnostic linkage.\n- 1.8: Mentions 3 relevant benefits.\n- 1.2: Mentions 2 relevant benefits.\n- 0.6: Mentions 1 relevant benefit.\n- 0.0: No meaningful benefits or not a benefits slide.", "expectation": "A clinically relevant benefits summary tied to rapid diagnosis, non-invasiveness, data frequency, and workflow integration."}, {"type": "llm_judge", "name": "Survey Intent Coverage and Binary Format", "description": "Ensure physician questions cover usefulness/need/reliability/acceptance/instant results, and non-physician questions cover usefulness and willingness to pay, all in Yes/No form.", "weight": 1.9, "judge_prompt": "Evaluate the survey PDF content (not just structure). Confirm:\n- Physician section questions collectively address: usefulness, clinical need, reliability/accuracy, acceptance/adoption, and instant/real-time results.\n- Non-physician section questions collectively address: usefulness and willingness to pay (WTP) or purchase intent.\n- All questions are binary Yes/No (explicit or clearly implied by instructions) and unambiguous.\n\nScoring (0\u20131.9):\n- 1.9: Physician questions cover all five physician intents; non-physician questions cover both intents; all questions clearly Yes/No and unambiguous.\n- 1.4: Physician covers at least four intents; non-physician covers both; Yes/No format clear.\n- 0.9: Physician covers at least three intents; non-physician covers at least one; Yes/No format mostly clear.\n- 0.5: Minimal coverage and/or some ambiguity in Yes/No format.\n- 0.0: Lacks intent coverage or not Yes/No.", "expectation": "Physician: usefulness, need, reliability, acceptance, instant results. Non-physician: usefulness and willingness to pay. All binary Yes/No."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Quality and Professionalism", "description": "Holistic assessment of presentation quality, strategic value, clarity, and audience appropriateness.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Presentation Professionalism and Clarity", "description": "Assess visual clarity, typography, layout, and overall readability of the workflows deck.", "weight": 1.5, "judge_prompt": "Judge the workflows presentation\u2019s professionalism:\n- Clear, readable typography and contrast\n- Consistent styling, alignment, and spacing across slides\n- Diagrams not cluttered; steps logically arranged; labels legible\n- Minimal typos/grammar issues on slide text\n\nScoring (0\u20131.5):\n- 1.5: Highly professional and clear throughout\n- 1.1: Generally professional with minor issues\n- 0.7: Mixed clarity; noticeable inconsistencies\n- 0.3: Poor readability/formatting\n- 0.0: Unprofessional or unreadable", "expectation": "Clean, consistent slides with legible workflow diagrams and minimal errors."}, {"type": "llm_judge", "name": "Strategic Value and Persuasiveness", "description": "Evaluate how convincingly the materials support internal justification for investment.", "weight": 1.5, "judge_prompt": "Consider both deliverables:\n- Do workflows and benefits together articulate a compelling improvement over current process?\n- Are clinical and operational benefits connected to stakeholder needs (physicians, patients, organization)?\n- Are major feasibility considerations at least acknowledged (e.g., calibration/AI variability, regulatory/accuracy expectations) without derailing the case?\n\nScoring (0\u20131.5):\n- 1.5: Strong, persuasive narrative linking workflows to valuable outcomes\n- 1.1: Solid case with minor gaps\n- 0.7: Some promise but weak linkage or missing key points\n- 0.3: Barely persuasive\n- 0.0: Not persuasive", "expectation": "A coherent case that the new workflow materially improves diagnosis and experience, with brief nod to feasibility."}, {"type": "llm_judge", "name": "Audience Appropriateness and Neutrality", "description": "Assess whether the materials suit physicians and general consumers without bias and with clear binary survey phrasing.", "weight": 1.0, "judge_prompt": "Evaluate:\n- Tone suitable for physicians (clinical accuracy) and consumers (plain language)\n- Survey neutrality (avoid leading/loaded phrasing)\n- Binary Yes/No questions are concise and unambiguous\n\nScoring (0\u20131.0):\n- 1.0: Appropriate and neutral throughout\n- 0.7: Minor tone or neutrality issues\n- 0.4: Mixed appropriateness or some leading questions\n- 0.1: Significant issues\n- 0.0: Inappropriate or biased", "expectation": "Balanced tone for both audiences; neutral, unambiguous Yes/No survey items."}, {"type": "llm_judge", "name": "Clarity and Accessibility of Explanations", "description": "Judge the clarity of legends, labels, and any explanatory notes for non-expert understanding.", "weight": 1.0, "judge_prompt": "Focus on clarity aids:\n- Legend effectively explains symbols/flows so a new reader can interpret diagrams\n- Headers and labels are descriptive and self-explanatory\n- Minimal reliance on jargon, or brief definitions provided\n\nScoring (0\u20131.0):\n- 1.0: Very clear and accessible\n- 0.7: Mostly clear with minor gaps\n- 0.4: Some unclear elements\n- 0.1: Largely unclear\n- 0.0: Not understandable", "expectation": "A legend and labels that make the workflows easy to understand for first-time readers."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "e4f664ea-0e5c-4e4e-a0d3-a87a33da947a", "rubric": {"category_name": "Screenwriting: Short Film Script (SAINTLINESS)", "rationale": "This rubric enforces a self-documenting, production-ready screenplay in standard format. Stage 1 is an LLM-only gate that mandates the exact deliverable shape (PDF screenplay with title page, proper elements, page and scene targets). Stage 2 mixes light code checks (structure/regex on extracted text) with higher-weight LLM verification of formatting, consistency, and \u201cshow, don\u2019t tell\u201d compliance. Stage 3 assesses overall creative and professional quality appropriate for producers/directors.", "max_total_score": 40.0, "stages": [{"name": "Stage 1 \u2014 Format and Structural Gate (MANDATORY)", "description": "LLM-only validation that the candidate produced a properly formatted screenplay PDF with the required structural elements enabling further verification.", "is_required": true, "max_points": 10.0, "min_score_to_pass": 7.5, "rules": [{"type": "llm_judge", "name": "Screenplay Format and Structure Present", "description": "Check that the output is a production-ready screenplay PDF in industry-standard shape.", "weight": 10.0, "judge_prompt": "You are evaluating whether the submitted file is a properly structured screenplay PDF for a short film titled \u201cSAINTLINESS.\u201d Only validate shape/structure, not writing quality or correctness of content.\n\nCheck the following strictly as PRESENCE/FORMAT (be flexible with minor naming variations):\n\n1) File/Length/Pagination\n- File is a PDF (not Word/Excel/plain text).\n- Contains a title page and then a body of 8\u201312 script pages (title page does not count toward the 8\u201312).\n\n2) Title Page\n- Clearly shows the title \u201cSAINTLINESS\u201d centered and capitalized.\n- Includes writer credit (e.g., \u201cWritten by\u201d + name). Contact info is optional.\n\n3) Screenplay Elements in Body\n- Scene headings (sluglines) in ALL CAPS using INT./EXT./INT/EXT or I/E and a location, plus a time of day (DAY/NIGHT/DAWN/DUSK/SUNSET/SUNRISE or similar). Target 10\u201315 scenes total.\n- Action lines (description of visuals/sounds; present-tense, not prose paragraphs about thoughts).\n- Character cues (names in ALL CAPS above dialogue) with dialogue blocks beneath.\n- Parentheticals appear where appropriate.\n- Transitions present (e.g., \u201cFADE IN:\u201d at start and \u201cFADE OUT.\u201d or \u201cFADE TO BLACK.\u201d at end; other transitions like CUT TO:, DISSOLVE TO: acceptable).\n- When a character is first introduced in action, their name appears in ALL CAPS.\n- Script appears to use industry-standard screenplay formatting (Courier 12pt, proper spacing/margins/layout). You do not need to measure margins; check visually for typical screenplay appearance.\n\nScoring (0 to 10):\n- 10: PDF with a title page + 8\u201312 body pages; 10\u201315 scenes with proper sluglines; action, dialogue, character cues, parentheticals, and at least start/end transitions present; overall looks like properly formatted screenplay (Courier 12, standard layout) at a glance.\n- 8: PDF; has title page; body length within 7\u201313 pages or obviously within target; most elements present but one minor element missing (e.g., parentheticals not used) OR scene count slightly outside 10\u201315 but sluglines clearly used throughout.\n- 6: PDF; title page present; body pages exist; multiple key elements missing or inconsistent (e.g., few sluglines or missing time of day; weak evidence of standard formatting), but it\u2019s still clearly a screenplay.\n- 3: PDF but lacks title page or uses prose-like formatting with minimal screenplay elements.\n- 0: Not a PDF or not a screenplay (e.g., essay, outline, or grossly wrong format).", "expectation": "A properly structured screenplay PDF with title page, 8\u201312 body pages, 10\u201315 scenes, and standard screenplay elements visible."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification of Formatting Compliance and Structural Correctness", "description": "Now that the screenplay shape is confirmed, verify key correctness aspects. Code rules perform deterministic text/regex checks on the extracted PDF text. LLM judges handle nuanced format and \u201cshow, don\u2019t tell\u201d compliance. Code rules have lower weight than LLM rules.", "is_required": false, "max_points": 20.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Scene Headings Count (INT./EXT.)", "description": "Detect plausible scene headings and score for target scene count.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float score in [0, 1.5]\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        return 0.0\n    if not text:\n        return 0.0\n    # Detect scene headings like INT., EXT., INT/EXT., I/E.\n    # Allow optional spaces and mixed forms, and ensure starts at line-begin.\n    pattern = re.compile(r\"^(?:INT\\.?|EXT\\.?|INT/EXT\\.?|I/E\\.?)\\s+[^\\n]+\", re.MULTILINE)\n    scenes = pattern.findall(text)\n    count = len(scenes)\n    # Score based on proximity to 10\u201315 scenes\n    if 10 <= count <= 15:\n        return 1.5\n    elif 8 <= count <= 16:\n        return 1.2\n    elif 6 <= count <= 18:\n        return 0.75\n    elif count > 0:\n        return 0.3\n    else:\n        return 0.0"}, {"type": "code", "name": "Dialogue and Character Cue Structure", "description": "Check presence of ALL-CAPS character cues with following dialogue lines.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Returns:\n        float score in [0, 1.5]\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        return 0.0\n    if not text:\n        return 0.0\n    lines = [l.strip() for l in text.splitlines()]\n    # Candidate character cues: all-caps lines of reasonable length without punctuation-heavy content\n    # Exclude common non-cue caps like INT., EXT., FADE, CUT, DISSOLVE, etc.\n    exclude_prefixes = (\"INT\", \"EXT\", \"I/E\", \"INT/EXT\", \"FADE\", \"CUT\", \"DISSOLVE\", \"SMASH\", \"MATCH\", \"MONTAGE\", \"SUPER\")\n    cue_re = re.compile(r\"^[A-Z0-9][A-Z0-9 .\\-()']{1,24}$\")\n    cues = []\n    for i, l in enumerate(lines):\n        if not l or len(l) > 30:\n            continue\n        if not cue_re.match(l):\n            continue\n        if any(l.startswith(p) for p in exclude_prefixes):\n            continue\n        # Heuristic: next 1-2 lines should be non-all-caps dialogue (not empty, not a slugline)\n        following = \" \".join(lines[i+1:i+3]).strip()\n        if following and not following.isupper():\n            cues.append(i)\n    unique_cues = len(cues)\n    if unique_cues >= 10:\n        return 1.5\n    elif unique_cues >= 6:\n        return 1.1\n    elif unique_cues >= 3:\n        return 0.6\n    elif unique_cues >= 1:\n        return 0.3\n    else:\n        return 0.0"}, {"type": "code", "name": "Transitions Present (Start/End)", "description": "Check for common transitions; reward start and end transitions.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Returns:\n        float score in [0, 1.0]\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        return 0.0\n    if not text:\n        return 0.0\n    t = text.upper()\n    has_fade_in = \"FADE IN\" in t\n    # End transitions: FADE OUT., FADE TO BLACK., or THE END after fade\n    end_markers = [\"FADE OUT\", \"FADE TO BLACK\", \"CUT TO BLACK\"]\n    has_end = any(m in t for m in end_markers)\n    any_transition = any(x in t for x in [\"CUT TO:\", \"DISSOLVE TO:\", \"SMASH CUT:\", \"MATCH CUT:\"])\n    if has_fade_in and has_end:\n        return 1.0\n    elif has_fade_in or has_end:\n        return 0.7\n    elif any_transition:\n        return 0.3\n    else:\n        return 0.0"}, {"type": "llm_judge", "name": "Formatting Compliance (Courier 12, Sluglines, Time of Day, Extensions)", "description": "Evaluate nuanced formatting details that code cannot robustly verify.", "weight": 4.0, "judge_prompt": "Verify nuanced screenplay formatting compliance in the PDF:\n- Typeface/size: Looks like Courier 12pt (monospaced, standard screenplay spacing).\n- Sluglines: Consistently ALL CAPS with INT./EXT./I/E, specific location, and time of day each time.\n- Character first mentions in action are ALL CAPS; character cues above dialogue are ALL CAPS.\n- Extensions and parentheticals: Appropriate use of V.O., O.S., CONT'D, and brief parentheticals; not overused.\n- Layout: Dialogue blocks appear narrower than action, typical of screenplay margins.\n\nScoring (0\u20134):\n- 4: Strong evidence of Courier 12 with consistent standard formatting, sluglines with time of day, correct character cue usage, and proper extensions/parentheticals.\n- 3: Mostly compliant; a few minor inconsistencies.\n- 2: Mixed compliance; noticeable recurring format issues.\n- 1: Poor adherence; frequent formatting mistakes.\n- 0: Not in standard screenplay format.", "expectation": "Looks and reads like an industry-standard formatted screenplay throughout."}, {"type": "llm_judge", "name": "\u201cShow, Don\u2019t Tell\u201d Compliance", "description": "Assess whether action and dialogue restrict to what can be seen and heard; avoid prose/inner thoughts/exposition.", "weight": 4.0, "judge_prompt": "Evaluate adherence to cinema\u2019s \u201cshow, don\u2019t tell\u201d principle:\n- Action lines describe observable visuals/sounds in present tense; avoid inner thoughts and novelistic exposition.\n- Dialogue conveys subtext without on-the-nose exposition or explaining backstory the audience wouldn\u2019t naturally hear.\n- No script-only author asides, analysis, or instructions to audience; avoids describing unfilmable internal states unless externalized.\n\nScoring (0\u20134):\n- 4: Strong \u201cshow, don\u2019t tell\u201d discipline; visual, cinematic writing.\n- 3: Mostly shows; a few telling moments.\n- 2: Mixed; frequent telling.\n- 1: Predominantly telling.\n- 0: Largely prose/expository, not filmable.", "expectation": "Filmable, visual storytelling with minimal telling."}, {"type": "llm_judge", "name": "Consistency and Continuity", "description": "Check for consistent character naming, coherent locations/times, and reasonable transitions.", "weight": 4.0, "judge_prompt": "Assess consistency and continuity:\n- Character names are consistent across cues; no unexplained renames.\n- Locations and times in sluglines match the described action; time-of-day use is coherent across adjacent scenes.\n- Transitions and scene order are logical; no jarring, unmotivated jumps without cinematic intent.\n\nScoring (0\u20134):\n- 4: Fully consistent names/locations/times; smooth continuity.\n- 3: Minor inconsistencies that don\u2019t impede reading.\n- 2: Noticeable inconsistencies causing mild confusion.\n- 1: Significant confusion from inconsistent labels or ordering.\n- 0: Disorganized, confusing continuity.", "expectation": "A clean, consistent read with coherent scene/character continuity."}, {"type": "llm_judge", "name": "Length, Scene Economy, and Concision", "description": "Verify the script fits an 8\u201312 page target and uses 10\u201315 concise scenes effectively.", "weight": 4.0, "judge_prompt": "Evaluate length and scene economy:\n- Body length approximately 8\u201312 pages (title page excluded) and around 10\u201315 scenes.\n- Scenes feel concise and purposeful; minimal padding.\n- Dialogue is economical; no long monologues without cinematic justification.\n\nScoring (0\u20134):\n- 4: Within targets; concise, efficient scenes and dialogue.\n- 3: Slightly off targets or a few padded scenes.\n- 2: Noticeably off targets or frequent padding.\n- 1: Far from targets; meandering scenes.\n- 0: Grossly mis-sized or unfocused.", "expectation": "A tight short-film length with focused, purposeful scenes."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Holistic Quality and Production Readiness", "description": "LLM-only assessment of creative excellence, readability, and producibility for a short film.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Narrative Coherence and Arc", "description": "Does the story have a clear beginning\u2013middle\u2013end with stakes and resolution appropriate for a short?", "weight": 2.5, "judge_prompt": "Assess story structure:\n- Clear setup, development, and resolution within short-film scope.\n- Thematic cohesion befitting a film titled \u201cSAINTLINESS.\u201d\n- Emotional throughline and stakes that escalate and resolve.\n\nScoring (0\u20132.5):\n- 2.5: Compelling, cohesive arc with satisfying resolution.\n- 1.75: Mostly coherent with minor gaps.\n- 1.0: Mixed coherence; uneven arc.\n- 0.5: Weak or confusing arc.\n- 0: Little coherent structure.", "expectation": "A cohesive short-film arc that fits the concept."}, {"type": "llm_judge", "name": "Character and Dialogue Quality", "description": "Evaluate character distinctiveness and dialogue speakability/subtext.", "weight": 2.5, "judge_prompt": "Assess character and dialogue quality:\n- Characters feel distinct with observable behavior and goals.\n- Dialogue sounds natural, playable, and carries subtext; avoids on-the-nose exposition.\n- Moments of silence/visual beats used effectively where appropriate.\n\nScoring (0\u20132.5):\n- 2.5: Distinct characters, strong subtext, highly playable dialogue.\n- 1.75: Generally strong with some flat lines.\n- 1.0: Mixed, occasionally stilted.\n- 0.5: Frequently on-the-nose.\n- 0: Wooden or unnatural throughout.", "expectation": "Distinct, performable voices with effective subtext."}, {"type": "llm_judge", "name": "Cinematic Vividness and Pacing", "description": "Judge visual specificity, sound design cues, and pacing flow.", "weight": 2.5, "judge_prompt": "Evaluate cinematic craft:\n- Visual specificity and concise sensory detail enable clear staging.\n- Effective use of sound cues and action to tell story.\n- Pacing maintains momentum; scene transitions feel intentional.\n\nScoring (0\u20132.5):\n- 2.5: Highly cinematic and well-paced.\n- 1.75: Generally effective with minor lulls.\n- 1.0: Uneven pacing; vague visuals.\n- 0.5: Often flat or unclear.\n- 0: Not cinematic/pacing collapses.", "expectation": "A visually engaging, well-paced read."}, {"type": "llm_judge", "name": "Production Readiness and Polish", "description": "Assess professionalism: title page, clean formatting, typos, and producibility.", "weight": 2.5, "judge_prompt": "Evaluate production readiness:\n- Title page present and clean; consistent formatting and spacing.\n- Minimal typos/grammar issues that would distract production.\n- Action lines and dialogue are practical to produce (no impossible directions unless stylized and intentional).\n\nScoring (0\u20132.5):\n- 2.5: Production-ready, polished.\n- 1.75: Minor polish issues only.\n- 1.0: Some issues; needs cleanup.\n- 0.5: Many errors/formatting noise.\n- 0: Unpolished and not ready for sharing.", "expectation": "A professional PDF ready to circulate to cast/crew."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "4b894ae3-1f23-4560-b13d-07ed1132074e", "rubric": {"category_name": "Audio Engineering \u2014 Bass Edit and Final Mix Delivery", "rationale": "This rubric enforces a self-documenting workflow for audio edit/mix delivery. Stage 1 mandates a verifiable output package: a properly named stereo mix WAV, an Edit Manifest (tabular log of all bass fixes with timecodes), and Mix Notes (methodology and level-matching plan). Stage 2 verifies technical specs and structural correctness using code, while LLM judges confirm the edits align with the artist\u2019s intent (fix wrong notes by copying in-key material, remove offensive noises with silence, preserve song length and 70s vibe, and match bass level to the rough mix without changing other instruments). Stage 3 assesses overall professional quality and plausibility of the approach. This structure forces the agent to prove correctness via artifacts rather than unverifiable claims.", "max_total_score": 13.0, "stages": [{"name": "Stage 1 \u2014 Output Shape Enforcement (GATE)", "description": "Confirm the candidate delivered the exact structured package enabling verification: a correctly named stereo mix WAV, an Edit Manifest (table with required columns and timecodes), Mix Notes with required sections, and (recommended) a separate edited bass stem for verification.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.1, "rules": [{"type": "llm_judge", "name": "Structured Delivery Package Present", "description": "Check presence and structure of required deliverables enabling verification.", "weight": 3.0, "judge_prompt": "Evaluate the candidate\u2019s delivered files for REQUIRED structure. Only check presence/format and obvious structure \u2014 not audio quality or calculation correctness.\n\nREQUIRED DELIVERABLES (all must be present for full credit):\n1) Final Stereo Mix WAV\n   - File name must be exactly: \"State of Affairs_FULL_EDIT_MIX\" (with .wav extension). Be flexible on upper/lower case of extension.\n   - This is the final mix with the edited bass.\n\n2) Edit Manifest (spreadsheet preferred; PDF/DOCX acceptable if a clear table is present)\n   - A tabular log documenting bass edits with columns (flexible naming, but intent must be clear):\n     \u2022 Timecode (format mm:ss.mmm, e.g., 01:44.375)\n     \u2022 Issue Type (e.g., wrong note, out of key, string noise/click/pop)\n     \u2022 Action / Fix Description (e.g., replace with copied note from section X; mute to silence)\n     \u2022 Source Timecode for replacement material (if applicable)\n     \u2022 Before Note or Description\n     \u2022 After Note or Description\n     \u2022 Comments/Justification\n   - Should list multiple rows covering all provided problem spots and any additional found issues.\n\n3) Mix Notes (PDF or DOCX)\n   - Professionally formatted with clear headings, containing these sections (flexible titles allowed if meaning matches):\n     \u2022 Project Overview\n     \u2022 Editing Methodology (state that wrong notes are replaced by in-key copies from repeating sections; string noises replaced with silence; no time/length changes)\n     \u2022 Specific Corrections (summarize the major edits; may reference the Edit Manifest)\n     \u2022 Level Matching to Rough Mix (describe how bass level was matched to the rough mix WITHOUT changing other instruments\u2019 levels)\n     \u2022 Length and Sync Assurance (state all track lengths remain unchanged and alignment preserved)\n     \u2022 Deliverables Summary\n\nRECOMMENDED (not strictly required, but helps verification):\n4) Edited Bass Stem WAV (e.g., filename includes \"Bass\" and \"Edited\")\n   - Same song length as final mix for sync verification.\n\nSCORING:\n- 3.0: All 3 REQUIRED items present with correct structure; Edit Manifest clearly tabular with appropriate columns; Mix Notes include all listed sections. Edited Bass stem present earns stronger confidence but not required for full score if the first three are excellent.\n- 2.5: All REQUIRED present; one minor omission in columns or a missing Mix Notes subsection.\n- 2.1: Two REQUIRED present with solid structure; the third is partially structured but usable.\n- 0.0\u20132.0: Missing key deliverables or wrong format (e.g., no Edit Manifest table or no Mix Notes document). If the final WAV is missing or misnamed, score near 0.\nOnly assess presence/structure and basic naming; do not judge audio content or technical headers here.", "expectation": "A correctly named final WAV, a clearly structured Edit Manifest with timecodes and actions, and Mix Notes covering methodology, level matching, and length/sync assurance."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification of Correctness (Mixed: Code + LLM)", "description": "With the structure in place, verify technical specs and that the documented edits meet the requirements: technical WAV properties, manifest integrity and timecodes, sync/length assurances, and an edit/mix approach consistent with the artist\u2019s intent.", "is_required": true, "max_points": 7.5, "min_score_to_pass": 4.5, "rules": [{"type": "code", "name": "Final WAV Technical Specs and Naming", "description": "Verify primary output is a .wav named correctly with stereo, 48 kHz, 24-bit sample width.", "weight": 0.6, "code": "import wave\nfrom pathlib import Path\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No primary output.\"\n    try:\n        p = context.files.get_path(output.id)\n    except Exception as e:\n        return 0.0, f\"Cannot access primary output path: {e}\"\n\n    weight = 0.6\n    score = 0.0\n    notes = []\n\n    # Filename check (stem must match exactly, extension .wav case-insensitive)\n    target_stem = \"State of Affairs_FULL_EDIT_MIX\".lower()\n    stem_ok = p.stem.lower() == target_stem\n    if stem_ok:\n        score += weight * 0.25\n    else:\n        notes.append(f\"Filename stem mismatch: got '{p.stem}', expected 'State of Affairs_FULL_EDIT_MIX'.\")\n\n    if p.suffix.lower() == \".wav\":\n        score += weight * 0.25\n    else:\n        notes.append(\"File is not a .wav extension.\")\n        # If not wav, cannot open headers; return partial for name only\n        return max(0.0, min(score, weight)), \"; \".join(notes) if notes else \"\"\n\n    # Read WAV headers\n    try:\n        with wave.open(str(p), 'rb') as wf:\n            nch = wf.getnchannels()\n            rate = wf.getframerate()\n            sampwidth = wf.getsampwidth()\n        if nch == 2:\n            score += weight * 0.20\n        else:\n            notes.append(f\"Expected stereo (2 ch), found {nch} ch.\")\n        if rate == 48000:\n            score += weight * 0.15\n        else:\n            notes.append(f\"Expected 48kHz, found {rate} Hz.\")\n        if sampwidth == 3:\n            score += weight * 0.15\n        else:\n            notes.append(f\"Expected 24-bit (3-byte) sample width, found {sampwidth} bytes.\")\n    except Exception as e:\n        notes.append(f\"WAV header read error: {e}\")\n\n    score = max(0.0, min(score, weight))\n    return score, \"; \".join(notes) if notes else \"OK\""}, {"type": "code", "name": "Edit Manifest Structure and Timecode Validity", "description": "Locate Edit Manifest (.xlsx/.csv) and verify required columns and timecode formatting (mm:ss.mmm).", "weight": 0.5, "code": "import re\nimport pandas as pd\nfrom pathlib import Path\n\ndef _find_manifest(context):\n    cands = []\n    for res in context.get_all_outputs():\n        try:\n            p = context.files.get_path(res.id)\n        except Exception:\n            continue\n        name = p.name.lower()\n        if p.suffix.lower() in [\".xlsx\", \".csv\", \".xls\"] and (\"edit\" in name or \"manifest\" in name or \"log\" in name):\n            cands.append((res, p))\n    # Prefer files with 'manifest' or 'edit' in name, xlsx first\n    cands.sort(key=lambda rp: (0 if 'manifest' in rp[1].name.lower() else 1, 0 if rp[1].suffix.lower()=='.xlsx' else 1))\n    return cands[0] if cands else (None, None)\n\n\ndef _read_table(res, path, context):\n    if path.suffix.lower() == '.csv':\n        try:\n            return context.files.read_csv(res.id)\n        except Exception:\n            return pd.read_csv(path)\n    else:\n        try:\n            # Try to read first sheet via pandas directly for robustness\n            xl = pd.ExcelFile(path)\n            sheet = xl.sheet_names[0]\n            return pd.read_excel(path, sheet_name=sheet)\n        except Exception:\n            # Fallback to helper with default sheet name\n            try:\n                return context.files.read_excel(res.id)\n            except Exception:\n                return None\n\n\ndef evaluate(workflow, context):\n    weight = 0.5\n    res, p = _find_manifest(context)\n    if not res:\n        return 0.0, \"No Edit Manifest (.xlsx/.csv) found.\"\n\n    df = _read_table(res, p, context)\n    if df is None or df.empty:\n        return 0.0, \"Edit Manifest could not be read or is empty.\"\n\n    # Normalize columns\n    cols = [str(c).strip().lower() for c in df.columns]\n    def has_any(keys):\n        return any(any(k in c for k in keys) for c in cols)\n\n    req_map = {\n        'timecode': ['timecode', 'tc', 'spot'],\n        'issue': ['issue', 'problem', 'type'],\n        'action': ['action', 'fix', 'resolution', 'edit'],\n        'source_timecode': ['source time', 'source tc', 'from time', 'src time', 'source'],\n        'before': ['before', 'orig', 'original', 'pre'],\n        'after': ['after', 'post', 'new'],\n        'comments': ['comment', 'notes', 'justification']\n    }\n\n    present = {}\n    for key, alts in req_map.items():\n        present[key] = has_any(alts)\n    col_score = sum(1 for v in present.values() if v) / len(req_map)\n\n    # Timecode format check on plausible columns\n    tc_pattern = re.compile(r\"^\\d{1,2}:\\d{2}\\.\\d{3}$\")\n    tc_cols = [i for i,c in enumerate(cols) if any(k in c for k in ['time','tc','spot'])]\n    valid_rows = 0\n    total_rows = min(len(df), 50)  # sample up to 50 rows\n    for i in range(total_rows):\n        row = df.iloc[i]\n        ok = False\n        for idx in tc_cols:\n            try:\n                val = str(row.iloc[idx]).strip()\n            except Exception:\n                continue\n            if tc_pattern.match(val):\n                ok = True\n                break\n        if ok:\n            valid_rows += 1\n    tc_score = (valid_rows / total_rows) if total_rows > 0 else 0.0\n\n    # Weighted blend: columns 70%, timecode 30%\n    raw = 0.7 * col_score + 0.3 * tc_score\n    score = max(0.0, min(weight * raw, weight))\n\n    missing = [k for k,v in present.items() if not v]\n    fb = f\"Columns OK={sum(1 for v in present.values() if v)}/{len(req_map)}; Timecode OK rows={valid_rows}/{total_rows}. Missing: {', '.join(missing)}\" if missing else f\"Columns complete; Timecode OK rows={valid_rows}/{total_rows}.\"\n    return score, fb"}, {"type": "code", "name": "Length/Sync Alignment: Edited Bass vs Final Mix", "description": "If an edited bass stem WAV is provided, verify its duration closely matches the final mix, indicating preserved length/sync.", "weight": 0.4, "code": "import wave\nfrom pathlib import Path\n\ndef _wav_info(path):\n    with wave.open(str(path), 'rb') as wf:\n        return {\n            'nch': wf.getnchannels(),\n            'rate': wf.getframerate(),\n            'frames': wf.getnframes(),\n            'sampwidth': wf.getsampwidth()\n        }\n\n\ndef evaluate(workflow, context):\n    weight = 0.4\n    primary = context.get_primary_output()\n    if not primary:\n        return 0.0, \"No primary output.\"\n    try:\n        mix_path = context.files.get_path(primary.id)\n    except Exception as e:\n        return 0.0, f\"Cannot access primary output: {e}\"\n\n    # Find candidate edited bass stem\n    cand = None\n    for res in context.get_all_outputs():\n        try:\n            p = context.files.get_path(res.id)\n        except Exception:\n            continue\n        if p == mix_path:\n            continue\n        if p.suffix.lower() == '.wav':\n            name = p.name.lower()\n            if 'bass' in name and any(k in name for k in ['edit','edited','fix','clean']):\n                cand = p\n                break\n    if not cand:\n        return 0.0, \"No edited bass stem WAV found for length check (recommended deliverable).\"\n\n    try:\n        mix = _wav_info(mix_path)\n        bass = _wav_info(cand)\n    except Exception as e:\n        return 0.0, f\"Error reading WAV info: {e}\"\n\n    # Require same sample rate for meaningful frame comparison\n    if mix['rate'] != bass['rate']:\n        return 0.0, f\"Sample rate mismatch: mix {mix['rate']} vs bass {bass['rate']}.\"\n\n    # Allow small tolerance (<= 10 ms)\n    tol_frames = int(0.010 * mix['rate'])\n    diff = abs(mix['frames'] - bass['frames'])\n    if diff == 0:\n        return weight, \"Exact frame match: lengths aligned.\"\n    elif diff <= tol_frames:\n        return weight * 0.5, f\"Minor length diff within tolerance: {diff} frames.\"\n    else:\n        return 0.0, f\"Length mismatch exceeds tolerance: {diff} frames.\""}, {"type": "llm_judge", "name": "Edit Actions Match Requirements", "description": "Do the documented edits meet the intent: wrong notes replaced by in-key copies from other repeating sections; string noises replaced with silence; no time/length changes?", "weight": 2.0, "judge_prompt": "Review the Edit Manifest and Mix Notes. Judge whether the described edits MATCH the task requirements:\n- Wrong/out-of-key bass notes are replaced by copying correct notes from other repeating sections in the song (not pitch-shifting, not time-stretching unless clearly justified). The manifest should reference source timecodes for replacements.\n- Offensive string noise/clicks/pops are edited out and replaced with silence (brief mutes), not affecting overall length or timing.\n- No changes to arrangement or track lengths.\nProvide a score based on how clearly and consistently the documents demonstrate the above. If claims are vague, missing, or suggest alternate methods (e.g., pitch correction instead of copy/replace), deduct. Consider the plausibility and completeness across multiple spots.", "expectation": "Clear, specific entries showing copy-paste replacements with source timecodes and silence edits for noises; explicit assertion that timing/lengths are preserved."}, {"type": "llm_judge", "name": "Cross-Reference to Provided Spots", "description": "The package should show it addressed the provided timecode spots from the artist\u2019s reference. Judge whether the documents explicitly cross-reference those spots.", "weight": 2.0, "judge_prompt": "In the Edit Manifest and/or Mix Notes, look for explicit cross-references to the artist-provided timecode spots (e.g., a section like \"Provided Spots Addressed\" or a column noting \"From provided list\"), with mm:ss.mmm formatting. You do not need the original reference file; judge whether the candidate shows a credible, specific mapping to the referenced spots (multiple timecodes listed, descriptions aligned with issues). Deduct if there is no sign of cross-referencing or if it is generic and non-specific.", "expectation": "A clear list or column indicating each provided spot was addressed, with precise timecodes and actions."}, {"type": "llm_judge", "name": "Bass Level Matching Strategy", "description": "Check that Mix Notes describe how the bass was matched to the rough mix level without changing other instruments\u2019 levels.", "weight": 2.0, "judge_prompt": "Examine the Mix Notes for a concrete bass level-matching method relative to the provided rough mix, explicitly stating that other instrument levels were not altered. Look for details such as referencing peak/RMS/LUFS or simple comparative gain matching, meter references, or fader alignment to the rough mix. Deduct if this is missing or if it implies changing other instruments\u2019 levels.", "expectation": "A practical, credible method for matching bass loudness to the rough mix with no changes to other instrument levels."}], "on_failure_action": "skip_remaining", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism", "description": "Holistic assessment of the work\u2019s professionalism, clarity, and alignment with the 70s-era aesthetic requested by the artist.", "is_required": false, "max_points": 2.5, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Technical Credibility and Risk Control", "description": "Evaluate whether the described workflow minimizes risks (phase, clicks, timing shifts) and demonstrates sound editorial practice.", "weight": 0.8, "judge_prompt": "Assess if the Mix Notes and Edit Manifest demonstrate a technically credible workflow: clean clip boundaries and crossfades to avoid clicks, phase-aware editing when copying notes, no unintended timing shifts, and clear mute-to-silence for noises. Higher scores for explicit mention of fades, zero-crossing awareness, and verification steps (solo checks, null/phase checks).", "expectation": "Clear mention of fades/crossfades, zero-crossing awareness, and checks to avoid artifacts."}, {"type": "llm_judge", "name": "Aesthetic Alignment (70s-era feel)", "description": "Judge how well the plan preserves the semi-rough, natural 70s vibe while fixing obvious bass issues.", "weight": 0.8, "judge_prompt": "Based on the documentation, does the approach preserve the semi-rough, natural 70s-era character (e.g., minimal processing, avoiding heavy quantization/tuning, subtle level moves) while eliminating obvious bass mistakes? Deduct if the approach implies over-processing that would modernize the sound excessively.", "expectation": "Minimal, musical edits that keep the natural 70s character intact."}, {"type": "llm_judge", "name": "Deliverable Hygiene and Clarity", "description": "Evaluate naming, organization, and clarity of documentation for handoff to the artist.", "weight": 0.9, "judge_prompt": "Review the entire package for professional hygiene: correct final filename, organized folders or clear file naming, documents with headings and page numbers, legible tables, and a concise Deliverables Summary. Higher scores for clear, client-ready presentation.", "expectation": "Well-organized, clearly named files and clean, readable documentation suitable for client handoff."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "a95a5829-34bb-40f3-993b-558aed6dcdef", "rubric": {"category_name": "Government | First-Line Supervisors of Police and Detectives \u2014 General Order: Training Request Procedure", "rationale": "This rubric enforces a self-documenting, verifiable policy document. Stage 1 is a strict LLM-only shape gate mandating a DOCX/PDF with standard policy sections, explicit approval/signature routing including named roles, and an appendix with Excel logging instructions. Stage 2 mixes lightweight code checks (text extraction, regex) with LLM verification of procedural correctness and compliance. Stage 3 evaluates professional quality, clarity, and implementation readiness.", "max_total_score": 30.6, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (Document Structure and Required Elements)", "description": "LLM-only gate to verify the candidate produced a formal policy document (DOCX or PDF) with the exact structure enabling verification, including signature routing and Excel logging instructions.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 5.0, "rules": [{"type": "llm_judge", "name": "Document Format and Required Structure Presence", "description": "Check that the output is a formal General Order/Policy document (preferably DOCX) with all mandated sections, explicit signatory roles, and Excel logging instructions.", "weight": 8.0, "judge_prompt": "You are evaluating whether the submission satisfies the strict STRUCTURAL requirements for a Police Department General Order governing Training Requests. Only judge structure/presence, not content quality or correctness.\n\nAcceptable formats: DOCX (preferred) or PDF. Reject plain text, markdown, or spreadsheets. Minimum 2 pages equivalent. Be flexible with exact header names but require clear, visible section headers.\n\nRequired front matter (may be on first page):\n- Title indicating this is a General Order/Policy on Training Requests\n- Policy/General Order number (or placeholder), Effective Date, Review Cycle, Authority/Issuing Officer\n\nRequired sections with clear headers:\n1) Purpose\n2) Scope\n3) Definitions\n4) References (or Related Policies/Laws)\n5) Responsibilities (must enumerate departmental roles: Training Coordinator/Unit; Ethics Liaison Officer; Chief, Division of Parole; Chief, Fiscal Services Unit; Chairman; Unit Commanders; Individual Officers/Employees)\n6) Procedures (must include distinct, labeled subsections covering ALL of the following):\n   - Eligibility to submit requests\n   - Required information in a request\n   - Submission method and timelines (deadlines/lead times)\n   - Review/evaluation criteria and workflow (routing/order of review)\n   - Final approval authority\n   - Notifications of approval/denial and appeals or re-submission\n   - Tracking/documentation steps\n7) Signature/Approval page with explicit sign-off lines/blocks for ALL of the following named roles (with Signature and Date lines):\n   - Ethics Liaison Officer\n   - Chief, Division of Parole\n   - Chief, Fiscal Services Unit\n   - Chairman\n8) Records Management/Retention and Audit Readiness section\n9) Excel Logging Instructions section (may be a dedicated section or Appendix) that includes:\n   - Clear directions that approved trainings are logged in an Excel spreadsheet\n   - A visible list or table of expected columns for the log (flexible naming), e.g., Date, Employee Name/ID, Course Title, Provider, Hours, Cost/Funding, Approval Date, Approval Authority, Sign-offs for the listed roles, Completion Date, Pass/Score, CEU Credits, Record Retention Location\n\nScoring:\n- 8.0: Valid DOCX/PDF and ALL required sections/subsections present; signature blocks for all named roles; Excel logging instructions with a visible column list/table\n- 7.0\u20137.9: All core sections present but minor omissions (e.g., References or minor detail missing) OR Excel columns described but not tabulated\n- 5.0\u20136.9: Missing one core section/subsection OR one of the required signatory roles not clearly present OR Excel logging mentioned but columns not enumerated\n- 0.1\u20134.9: Multiple core sections missing, or signature/approval page absent, or no Excel logging instructions\n- 0.0: Not a DOCX/PDF, fewer than two pages, or grossly wrong format\n\nOnly check PRESENCE and STRUCTURE. Do not judge correctness of procedures or legality.", "expectation": "A DOCX/PDF General Order with all mandatory sections, explicit signatory blocks for the four named roles, and an Excel logging instructions section listing expected columns."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification of Correctness and Compliance", "description": "Mixed code and LLM checks to verify that the policy\u2019s procedures, approvals, timelines, logging, and records management are concretely and correctly specified.", "is_required": true, "max_points": 12.6, "min_score_to_pass": 6.3, "rules": [{"type": "code", "name": "Required Signatories Present with Signature/Approval Language", "description": "Verify the four named roles appear and are associated with signature/approval/date language.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float or (float, str)\n    \"\"\"\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output\"\n    if not output.is_document:\n        return 0.0, \"Output is not a document\"\n\n    text = \"\"\n    # Try DOCX, then PDF, then text\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    if not text or not text.strip():\n        return 0.0, \"Unable to read document text\"\n\n    low = text.lower()\n    role_patterns = [\n        r\"ethics\\s+liaison\\s+officer\",\n        r\"chief[,\\s]+division\\s+of\\s+parole\",\n        r\"chief[,\\s]+fiscal\\s+services\\s+unit\",\n        r\"chairman\"\n    ]\n\n    def has_sig_near(pattern):\n        found = False\n        for m in re.finditer(pattern, low):\n            s, e = m.start(), m.end()\n            window = low[max(0, s-120):min(len(low), e+120)]\n            if re.search(r\"\\b(sign|signature|approve|approval|authorized|date)\\b\", window):\n                found = True\n                break\n        return found\n\n    hits = [has_sig_near(p) for p in role_patterns]\n    score = sum(hits) / len(role_patterns) if role_patterns else 0.0\n    return score * 0.8, f\"Signatory proximity hits: {sum(hits)}/{len(role_patterns)}\""}, {"type": "code", "name": "Timelines Specified for Submission, Review, and Notification", "description": "Check for explicit day-based timelines near submission, review/approval, and notification/decision terms.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    if not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    if not text.strip():\n        return 0.0\n\n    low = text.lower()\n\n    def has_timeline(keywords):\n        for kw in keywords:\n            for m in re.finditer(re.escape(kw), low):\n                s, e = m.start(), m.end()\n                win = low[max(0, s-100):min(len(low), e+100)]\n                if re.search(r\"\\b\\d{1,3}\\s*(business|calendar)?\\s*day\", win):\n                    return True\n        return False\n\n    submission = has_timeline([\"submit\", \"submission\", \"request\"])\n    review = has_timeline([\"review\", \"evaluate\", \"approval\", \"approve\"]) \n    notice = has_timeline([\"notify\", \"notification\", \"decision\", \"communicat\"]) \n\n    count = sum([submission, review, notice])\n    if count == 0:\n        frac = 0.0\n    elif count == 1:\n        frac = 1/3\n    elif count == 2:\n        frac = 2/3\n    else:\n        frac = 1.0\n    return 0.5 * frac"}, {"type": "code", "name": "Excel Logging Instructions Name Key Fields", "description": "Validate presence of Excel logging instructions and core column names.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    low = text.lower()\n    if not low:\n        return 0.0\n\n    mentions_excel = any(k in low for k in [\"excel\", \"spreadsheet\"]) and any(k in low for k in [\"log\", \"logging\", \"tracker\", \"register\"])\n\n    # Core expected columns (flexible wording)\n    cols = [\n        (\"date\", [\"date\"]),\n        (\"employee\", [\"employee\", \"staff\", \"member\"]),\n        (\"course\", [\"course\", \"title\", \"training name\"]),\n        (\"provider\", [\"provider\", \"vendor\", \"academy\"]),\n        (\"hours\", [\"hours\", \"duration\"]),\n        (\"cost\", [\"cost\", \"tuition\", \"fees\", \"funding\"]),\n        (\"approval\", [\"approval\", \"approved\", \"approval date\", \"approver\"]),\n        (\"completion\", [\"completion\", \"completed\", \"finish date\"]),\n    ]\n\n    core_hits = 0\n    for _, synonyms in cols:\n        if any(s in low for s in synonyms):\n            core_hits += 1\n\n    # Sign-off related roles in logging\n    sign_roles = [\"ethics\", \"parole\", \"fiscal\", \"chairman\"]\n    sign_hits = sum(1 for r in sign_roles if r in low)\n\n    # Score components\n    if not mentions_excel:\n        return 0.0\n    core_frac = core_hits / len(cols)\n    sign_frac = min(sign_hits, 2) / 2  # require at least two sign-off references for full credit here\n    frac = 0.6 * core_frac + 0.4 * sign_frac\n    return 0.5 * frac"}, {"type": "code", "name": "Records Retention Period Specified (Years)", "description": "Check for an explicit records retention period stated in years.", "weight": 0.3, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    low = text.lower()\n    if not low:\n        return 0.0\n\n    # Look for retention language and years\n    has_retention = (\"retain\" in low or \"retention\" in low or \"record\" in low)\n    has_years = re.search(r\"\\b\\d{1,3}\\s*(year|years)\\b\", low) is not None\n    return 0.3 if (has_retention and has_years) else 0.0"}, {"type": "llm_judge", "name": "Procedural Completeness and Soundness", "description": "Verify that procedures concretely cover eligibility, required info, submission method and timelines, evaluation criteria, routing, notifications, appeals, tracking, and final approval authority.", "weight": 4.6, "judge_prompt": "Evaluate the policy\u2019s procedural completeness and soundness. Using the enforced structure from Stage 1, check whether the Procedures section provides clear, stepwise instructions that include ALL of the following:\n- Eligibility: who may submit training requests (roles/positions)\n- Required information to include in a request (fields/items list)\n- Submission method and timelines (deadlines/lead times)\n- Evaluation criteria (objective, documented criteria used in review)\n- Routing/Review workflow (order of review, responsibilities of each role)\n- Final approval authority identified explicitly\n- Notifications (approval/denial) and appeals or re-submission pathways\n- Tracking/documentation steps and handoffs\n\nScoring (0 to 4.6):\n- 4.2\u20134.6: Every element above is explicitly addressed with actionable, unambiguous steps\n- 3.2\u20134.1: One minor element is vague or partially missing\n- 2.0\u20133.1: Multiple elements are vague or missing\n- 0.1\u20131.9: Procedures are high-level only or largely incomplete\n- 0.0: No concrete procedures present", "expectation": "A full, step-by-step procedure covering all required elements with clear routing and responsible roles."}, {"type": "llm_judge", "name": "Approval Workflow and Authority Chain Clarity", "description": "Confirm that the signatory workflow is explicit, ordered, and includes all named roles with final approval authority clearly identified.", "weight": 2.4, "judge_prompt": "Assess the clarity and completeness of the approval workflow. Requirements:\n- The following roles are explicitly included with sign-off requirements: Ethics Liaison Officer; Chief, Division of Parole; Chief, Fiscal Services Unit; Chairman\n- The order of review/approval is stated or can be clearly inferred\n- Final approval authority is explicitly identified\n- What happens on denial (revision/appeal) is described\n\nScoring (0 to 2.4):\n- 2.1\u20132.4: All roles present; order and final authority explicit; denial handling defined\n- 1.3\u20132.0: Roles present but order or denial handling is vague\n- 0.1\u20131.2: One role missing or approval authority unclear\n- 0.0: Multiple roles missing or approval workflow absent", "expectation": "Explicit routing including all named roles, with a clearly identified final approver and denial/appeal handling."}, {"type": "llm_judge", "name": "Compliance, Documentation, and Audit Readiness", "description": "Judge whether the policy aligns with typical state training mandates, includes documentation controls, and supports audits.", "weight": 1.8, "judge_prompt": "Evaluate if the policy addresses compliance and documentation controls:\n- States compliance with state training mandates or cites relevant references\n- Defines documentation standards (forms/templates, version control, effective/review dates)\n- Specifies recordkeeping and retention steps to support audits\n- Addresses conflict-of-interest/ethics review touchpoints where relevant\n\nScoring (0 to 1.8):\n- 1.6\u20131.8: Strong compliance language, clear documentation controls, and explicit audit support\n- 1.0\u20131.5: Adequate compliance references with some gaps\n- 0.1\u20130.9: Minimal or generic compliance statements\n- 0.0: No compliance or documentation language", "expectation": "Clear compliance alignment, documentation control mechanisms, and audit-ready recordkeeping."}, {"type": "llm_judge", "name": "Excel Logging Instructions Specificity and Actionability", "description": "Determine if instructions for Excel logging are specific enough to enable immediate implementation.", "weight": 1.8, "judge_prompt": "Review the Excel logging instructions. Requirements:\n- Explicit direction to use an Excel spreadsheet to log approved trainings\n- A list or table of expected columns (flexible naming), including at least: Date, Employee Name/ID, Course Title, Provider, Hours, Cost/Funding, Approval Date, Approval Authority, Sign-offs for the named roles, Completion Date, and performance/CEU fields\n- Assignment of responsibility for maintaining the log and where it is stored\n\nScoring (0 to 1.8):\n- 1.6\u20131.8: Clear, implementable instructions with a comprehensive column list and ownership/location\n- 1.0\u20131.5: Mostly clear; minor details or a few columns missing\n- 0.1\u20130.9: Vague instructions; columns not sufficiently enumerated\n- 0.0: No actionable Excel logging instructions", "expectation": "Actionable Excel instructions with a concrete column list, owner, and storage location."}], "on_failure_action": "skip_remaining", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism Assessment", "description": "LLM-only holistic assessment of professionalism, clarity, usability, and readiness for adoption.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Tone, Formatting, and Readability", "description": "Assess whether the document reads as a professional General Order with clear formatting and consistent style.", "weight": 2.5, "judge_prompt": "Evaluate professional presentation: consistency of headers, numbering, typography; grammar; tone suitable for a police department general order; logical flow; and absence of contradictions.\n\nScoring (0 to 2.5):\n- 2.2\u20132.5: Highly professional, consistent formatting and excellent readability\n- 1.5\u20132.1: Generally professional with minor issues\n- 0.5\u20131.4: Noticeable formatting or clarity problems\n- 0.0\u20130.4: Poorly formatted or difficult to follow", "expectation": "A polished, professional policy with consistent formatting and clear writing."}, {"type": "llm_judge", "name": "Clarity and Accessibility for Intended Audience", "description": "Judge whether supervisors, officers, and administrative staff can easily understand and follow the policy.", "weight": 2.5, "judge_prompt": "Assess clarity for the intended audience: Are instructions plain-language with minimal jargon? Are steps easy to follow? Are responsibilities unambiguous for line supervisors, training staff, and command?\n\nScoring (0 to 2.5):\n- 2.2\u20132.5: Very clear and accessible; minimal ambiguity\n- 1.5\u20132.1: Mostly clear; a few ambiguous areas\n- 0.5\u20131.4: Several confusing sections\n- 0.0\u20130.4: Broadly unclear or inaccessible", "expectation": "Plain, direct instructions with unambiguous roles and steps."}, {"type": "llm_judge", "name": "Risk, Accountability, and Governance Coverage", "description": "Evaluate inclusion of safeguards, accountability controls, and governance elements.", "weight": 2.5, "judge_prompt": "Check for risk and governance elements: segregation of duties, conflict-of-interest/ethics checkpoints, fiscal oversight, appeal/escalation paths, version control (effective/review dates), and audit trails.\n\nScoring (0 to 2.5):\n- 2.2\u20132.5: Strong controls and governance throughout\n- 1.5\u20132.1: Adequate controls with minor gaps\n- 0.5\u20131.4: Weak or partial coverage\n- 0.0\u20130.4: Little to no governance coverage", "expectation": "Clear accountability mechanisms and governance controls embedded in the policy."}, {"type": "llm_judge", "name": "Implementation Readiness (Templates and Operational Details)", "description": "Determine whether the policy includes appendices/templates and operational details enabling immediate rollout.", "weight": 2.5, "judge_prompt": "Assess implementation readiness: presence of a request form template or checklist, sample routing/signature page, example or template for the Excel log (column list/table), and instructions for where documents are stored (shared drive or system), plus responsible roles.\n\nScoring (0 to 2.5):\n- 2.2\u20132.5: Comprehensive templates/checklists and clear operational details\n- 1.5\u20132.1: Mostly ready with minor omissions\n- 0.5\u20131.4: Partial materials; requires significant work to implement\n- 0.0\u20130.4: Not implementable without major additions", "expectation": "Includes usable templates, examples, and storage/ownership details for immediate adoption."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "a4a9195c-5ebe-4b8d-a0c2-4a6b7a49da8b", "rubric": {"category_name": "SOP: ESD Handling and Storage (Manufacturing \u2013 Warehouse)", "rationale": "Pattern B (Document). The rubric enforces a strict, self-documenting DOCX structure in Stage 1 using only LLM judges, then verifies correctness with mixed rules in Stage 2 (light code checks + heavier LLM checks aligned to IPC-A-610G and ESD best practices). Stage 3 evaluates overall professional quality, clarity, and training readiness. Code rules are lightweight and resilient; LLM rules carry most weight for nuanced validation.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2013 Shape Enforcement Gate (Structure and Format)", "description": "LLM-only gate to ensure the output is a 2\u20135 page Word document (DOCX) with all required SOP sections that enable verification.", "is_required": true, "max_points": 2.0, "min_score_to_pass": 1.5, "rules": [{"type": "llm_judge", "name": "Document Format and Length Gate (Word, 2\u20135 pages)", "description": "Check that the candidate output is a Word document (DOCX) and between 2 and 5 pages, professionally formatted.", "weight": 1.0, "judge_prompt": "You are verifying basic format and length. Inspect the submitted file as rendered.\nRequirements:\n- File format: Must be a Word document (DOCX). Not plain text, not Excel. PDF is not acceptable for full credit, but if everything else is perfect, award partial credit.\n- Length: Between 2 and 5 pages inclusive. More than 5 pages fails the gate. Fewer than 2 pages is insufficient.\n- Professional formatting: Clear title, headings, readable layout.\nScoring:\n- 1.0: DOCX format AND 2\u20135 pages AND professionally formatted.\n- 0.5: PDF format (not DOCX) but meets 2\u20135 pages and professional formatting.\n- 0.2: DOCX or PDF but outside 2\u20135 pages OR formatting is poor.\n- 0.0: Wrong format (not DOCX/PDF), or obviously not a document, or egregiously fails length.\nOnly judge presence/format/length; do not evaluate content quality.", "expectation": "A 2\u20135 page DOCX SOP with professional layout."}, {"type": "llm_judge", "name": "Required SOP Sections Present", "description": "Check that the SOP contains all required sections to enable verification.", "weight": 1.0, "judge_prompt": "Check if the document visibly contains the following sections (allow equivalent naming):\n1) Title with document control info (version/revision, date, author)\n2) Purpose and Scope\n3) References (explicitly mention IPC-A-610G; variants like IPC A 610G acceptable)\n4) Definitions/Glossary (ESD terms)\n5) Roles & Responsibilities (warehouse clerks, leads, QA, ESD coordinator)\n6) Required ESD Controls/Materials (e.g., wrist straps, heel grounders, ESD mats, ionizers, shielding bags, labels)\n7) Handling Procedures (numbered, step-by-step)\n8) Storage Procedures (numbered, step-by-step)\n9) Training & Records (training frequency, sign-offs, logs)\n10) Safety/Warnings/Precautions (ESD symbol, do/don'ts)\n11) Revision History/Document Control table\nScoring (be flexible on headers but require clear equivalents):\n- 1.0: 9\u201311 sections clearly present including References (with IPC-A-610G) and both Handling and Storage.\n- 0.7: 7\u20138 sections present, still includes References (with IPC-A-610G) and both Handling and Storage.\n- 0.4: 5\u20136 sections present, at least includes Handling OR Storage.\n- 0.0: Fewer than 5 sections or missing both Handling and Storage.\nDo not judge technical correctness, only presence/structure.", "expectation": "A complete SOP scaffold with the listed sections visible."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness Verification (Mixed)", "description": "Verify the SOP is technically sound and implementable. Light code checks for anchors; LLM checks for substantive alignment to ESD best practices and IPC-A-610G.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Doc Type & Length Proxy (technical check)", "description": "Confirm document type and approximate length via word count proxy to support the length requirement.", "weight": 0.5, "code": "import re\nfrom pathlib import Path\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output:\n            return 0.0\n        if not output.is_document:\n            return 0.0\n        path = context.files.get_path(output.id)\n        ext = path.suffix.lower()\n        text = ''\n        if ext == '.docx':\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                text = ''\n        elif ext == '.pdf':\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                text = ''\n        else:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = ''\n        words = re.findall(r\"\\w+\", text or '')\n        wc = len(words)\n        # Score combines type (40%) and length compliance (60%)\n        type_score = 0.4 if ext == '.docx' else (0.2 if ext == '.pdf' else 0.0)\n        length_score = 0.0\n        # 2\u20135 pages proxy: <=1600 words as full; <=2000 partial\n        if wc > 0 and wc <= 1600:\n            length_score = 0.6\n        elif wc > 1600 and wc <= 2000:\n            length_score = 0.3\n        else:\n            length_score = 0.0\n        raw = type_score + length_score\n        # Cap at weight\n        return min(raw, 0.5)\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Reference to IPC-A-610G Present", "description": "Verify the SOP text explicitly references IPC-A-610G (accept reasonable variants).", "weight": 0.7, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        text = ''\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                text = ''\n        t = (text or '').lower()\n        patterns = [r'ipc\\s*-?a\\s*-?610g', r'ipc\\s*-?a\\s*-?610', r'610g']\n        found = any(re.search(p, t) for p in patterns)\n        return 0.7 if found else 0.0\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Core Headings Present (Keyword Scan)", "description": "Check for presence of essential headings via keyword proxies: purpose/scope, references, responsibilities, handling, storage, training/records, document control/revision, safety/warnings.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        text = ''\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                text = ''\n        t = (text or '').lower()\n        keywords = [\n            'purpose', 'scope', 'reference', 'definitions', 'glossary',\n            'roles', 'responsibilities', 'handling', 'storage',\n            'training', 'records', 'document control', 'revision',\n            'safety', 'warning'\n        ]\n        hits = 0\n        for k in keywords:\n            if k in t:\n                hits += 1\n        # Expect at least 8 keyword hits for full credit\n        ratio = min(hits / 12.0, 1.0)  # normalize conservatively\n        return 0.5 * ratio\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Numbered Procedures Detected", "description": "Detect whether the SOP includes numbered, step-by-step procedures.", "weight": 0.3, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        text = ''\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                text = ''\n        t = text or ''\n        matches = re.findall(r'^\\s*\\d+\\.', t, flags=re.MULTILINE)\n        n = len(matches)\n        if n >= 5:\n            return 0.3\n        elif n >= 3:\n            return 0.18\n        elif n >= 1:\n            return 0.09\n        else:\n            return 0.0\n    except Exception:\n        return 0.0"}, {"type": "llm_judge", "name": "Handling Procedures \u2013 Technical Correctness", "description": "Evaluate whether the handling procedures reflect sound ESD controls aligned with industry best practices and IPC-A-610G context.", "weight": 3.0, "judge_prompt": "Assess the technical correctness of the HANDLING PROCEDURES for ESD-sensitive items. Look for:\n- Establishment of an ESD Protected Area (EPA), use of ESD-safe work surfaces/mats, and bonding/grounding.\n- Personal grounding (wrist straps, heel/toe grounders), with pre-use testing/verification frequency.\n- Use of ionization for insulators where applicable.\n- Controls for clothing, tools, carts, labels, and movement between EPA/non-EPA.\n- Proper unpacking/opening methods, handling only in EPA, and minimizing handling/time out of shielding.\n- Explicit do/don'ts and precautions for static-generating materials.\nScoring:\n- 3.0: Comprehensive, specific, and implementable steps covering most items above with checks/tests.\n- 2.0: Generally correct with minor gaps; still implementable.\n- 1.0: Partially correct but misses multiple critical controls.\n- 0.0: Vague/incorrect; lacks core ESD handling controls.", "expectation": "Clear, stepwise handling controls including EPA, grounding, verification, ionization, and do/don'ts."}, {"type": "llm_judge", "name": "Storage Controls \u2013 Technical Correctness", "description": "Evaluate the storage procedures for ESD protection and control.", "weight": 2.5, "judge_prompt": "Assess STORAGE PROCEDURES for ESD-sensitive items. Look for:\n- Use of proper ESD packaging: shielding bags/containers; avoidance of non-shielding bags for primary protection.\n- Labeling with ESD symbols and status (e.g., received, inspected, quarantined), lot/traceability information.\n- Storage furniture/containers that are static-dissipative/grounded as appropriate; separation from static-generating materials.\n- Environmental controls as applicable (e.g., humidity targets, housekeeping, no plastic films/foams near storage without mitigation).\n- Movement and transport in ESD-safe carts/totes; maintaining protection during kitting/issuance.\nScoring:\n- 2.5: Thorough, specific controls covering packaging, labeling, storage environment, and transport.\n- 1.5: Generally correct but with notable gaps.\n- 0.5: Minimal or vague controls.\n- 0.0: Largely incorrect or missing.", "expectation": "Storage specifies shielding containers, labeling, grounded storage, environment controls, and protected transport."}, {"type": "llm_judge", "name": "Training, Records, and Auditing", "description": "Assess whether the SOP defines training requirements, records, and audit/verification mechanisms.", "weight": 1.5, "judge_prompt": "Check for:\n- Defined training requirements (initial/refresher), roles responsible, and competency sign-offs.\n- Records/logs (e.g., wrist strap tester logs, mat resistance checks, audit checklists, issuance/receiving logs).\n- Audit/inspection frequency and responsibilities; corrective action loop.\nScoring:\n- 1.5: Clear training plan, explicit records, and recurring audits with responsibilities.\n- 1.0: Mostly present with minor gaps.\n- 0.5: Some references but unclear/incomplete.\n- 0.0: Absent.", "expectation": "Actionable training and records with clear ownership and cadence."}, {"type": "llm_judge", "name": "Alignment to IPC-A-610G", "description": "Evaluate whether the SOP meaningfully aligns to IPC-A-610G (beyond name-dropping).", "weight": 1.0, "judge_prompt": "Determine if the SOP meaningfully aligns with IPC-A-610G:\n- References IPC-A-610G in context (acceptability of electronic assemblies) and uses its intent to justify handling/storage controls that protect assembly acceptability.\n- Provides citations or section references when relevant (flexible), or clearly states how the SOP supports meeting acceptance criteria.\nScoring:\n- 1.0: Clear, correct alignment beyond citation only.\n- 0.5: Mentions standard and loosely aligns.\n- 0.0: No meaningful alignment.", "expectation": "Demonstrated linkage from SOP controls to IPC-A-610G acceptability intent."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Quality and Professionalism", "description": "Holistic assessment of clarity, usability, and professional readiness for warehouse rollout.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity and Actionability for Warehouse Staff", "description": "Evaluate whether the SOP is clear, concise, and immediately actionable for clerks.", "weight": 2.0, "judge_prompt": "Judge clarity and actionability for warehouse personnel:\n- Plain language; minimal jargon; concise steps.\n- Step-by-step checklists; numbered procedures; quick-reference tables.\n- Clear role assignments (who does what) and escalation paths.\nScoring: 2.0 excellent; 1.0 adequate with gaps; 0.0 poor.", "expectation": "Concise, role-oriented instructions and checklists usable on the floor."}, {"type": "llm_judge", "name": "Professional Presentation and Document Control", "description": "Assess overall presentation, consistency, and document control professionalism.", "weight": 2.0, "judge_prompt": "Check for:\n- Consistent headings, typography, and spacing; readable tables/figures if any.\n- Document control elements: version/revision, date, author/owner, approval, revision history table, page numbers.\n- Proper use of ESD symbols and warnings where appropriate.\nScoring: 2.0 strong; 1.0 mixed; 0.0 weak.", "expectation": "Looks and feels like a formal SOP with proper document control."}, {"type": "llm_judge", "name": "Conciseness within 5 Pages without Omissions", "description": "Ensure the SOP fits within 5 pages yet remains complete on essentials.", "weight": 2.0, "judge_prompt": "Evaluate balance of brevity and completeness:\n- Within 5 pages while covering essentials for handling, storage, training, and records.\n- Avoids unnecessary background text; prioritizes procedures and controls.\nScoring: 2.0 well-balanced; 1.0 borderline; 0.0 verbose or missing essentials.", "expectation": "Tight, complete, and within the page limit."}, {"type": "llm_judge", "name": "Operational Integration and Change Readiness", "description": "Assess readiness for integration into training and daily processes.", "weight": 2.0, "judge_prompt": "Evaluate how well the SOP supports rollout:\n- Clear references to integration with onboarding, refresher training, daily start-up checks (e.g., strap tester use), and audits.\n- Includes or references simple forms/logs or checklists for immediate use.\n- Alignment with warehouse flows: receiving, storage, kitting/issue, returns/quarantine.\nScoring: 2.0 fully ready; 1.0 partially ready; 0.0 not ready.", "expectation": "Ready to drop into training and daily operations with minimal rework."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "4520f882-715a-482d-8e87-1cb3cbdfe975", "rubric": {"category_name": "Finance & Insurance \u2014 Financial Managers: CBA-Compliant Musician Payroll Model (Excel)", "rationale": "This rubric enforces a self-documenting, verifiable Excel payroll model designed for a theatre\u2019s local music contractor. Stage 1 uses an LLM-only gate to require a strict, auditable workbook structure that makes verification trivial. Stage 2 blends lightweight code checks (sanity/bounds and reconciliation) with LLM consistency audits tied to the mandated shape, keeping code weights ~5x lower than LLM. Stage 3 evaluates professional quality, usability, maintainability, and scalability for real-world contractor use.", "max_total_score": 25.0, "stages": [{"name": "Stage 1 \u2014 Structural Format Gate (LLM-only)", "description": "Enforce exact workbook shape to make verification trivial. If shape fails, the entire category scores 0.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Required Workbook Structure Present", "description": "Validates the exact workbook structure and key tables needed for verification. LLM checks sheets, table structures, and required fields. Only structure, not correctness.", "weight": 8.0, "judge_prompt": "You are evaluating whether the candidate output is a valid Excel payroll model with the exact STRUCTURE required below. Only judge structure/presence and basic layout\u2014not calculation correctness. Be flexible on sheet names if clearly equivalent.\n\nFORMAT REQUIREMENTS\n- Must be an Excel spreadsheet (.xlsx). Not PDF/DOCX/CSV.\n- Professional layout: clear section headers, labeled tables, visible column headers.\n\nREQUIRED SHEETS (all required unless marked OPTIONAL)\n1) \u201cReadMe\u201d or \u201cInstructions\u201d (Required)\n   - Purpose of workbook\n   - How to use (step-by-step for the contractor)\n   - What/where to enter inputs\n   - How to update CBA rates annually\n   - Explanation of validations/compliance flags\n   Minimum: 5 bullet points or 3+ sentences across clear subsections.\n\n2) \u201cRates\u201d (Required)\n   The following logically separated tables (can be stacked vertically):\n   - Wage Rates table with columns like: [Category | Subtype/Role | Unit (week/service/hour) | Rate | Currency | Effective Start | Effective End | Source/Notes]\n   - Premiums/Multipliers table: [Premium Type (Doubling/Leader/Contractor/etc.) | Condition | Multiplier or Amount | Unit | Cap/Threshold | Notes]\n   - Contributions table: [Type (Pension | Health & Welfare | Vacation) | Basis (Gross/Pensionable) | Rate (% or amount) | Cap | Notes]\n   - Overtime Rules table: [Context (Daily/Weekly/Per Service) | Threshold | Multiplier | Applies To (Rehearsal/Performance/Both) | Notes]\n   - Cartage/Travel table: [Item | Rate/Amount | Unit | Conditions | Notes]\n   These tables may be named differently but must clearly cover wages, premiums, contributions, overtime, and cartage/travel with effective dates.\n\n3) \u201cRoster & Inputs\u201d (Required) \u2014 Contractor Inputs\n   - Roster table with columns like: [Musician Name | Union ID | Instrument(s) | Chair/Role | Leader/Contractor (Y/N) | Doubling (list/flags) | Engagement Start | Engagement End (or Weeks) | Weekly Guarantee or Services | Home City | Cartage Eligible (Y/N) | Per Diem Eligible (Y/N) | Notes]\n   - Weekly schedule/entry area capturing work details by day/service for each week: [Date | Type (Rehearsal/Performance/Travel) | Units (hours or service count) | Other Adjustments | Notes]. Inputs must be clearly highlighted.\n\n4) \u201cPayroll_Calc\u201d or \u201cCalculations\u201d (Required)\n   - Row-level calculation log with columns like: [Week Starting | Musician | Date/Service | Type (Reh/Perf) | Units | Base Rate Lookup | Multiplier/Premium Applied | Amount | Category | Reference (Rate ID) | Compliance Check (OK/Flag) | Notes]\n   - Must show traceable application of rates, multipliers, and contributions.\n\n5) \u201cSummary\u201d (Required)\n   - Totals by musician by payroll category with a matrix-like table. Expected columns include: [Musician | Base/Guarantee | Rehearsal | Performance | Overtime | Doubling Premiums | Leader/Contractor Premium | Cartage | Per Diem/Travel | Other Adjustments | Gross Wages | Pension | Health & Welfare | Vacation | Employer Contributions (if separate) | Total Employer Cost]. Names may vary but the structure should clearly separate wage categories, contributions, and a final total.\n   - A second table for Totals by Category (optional but preferred).\n\n6) \u201cCompliance Checks\u201d or \u201cValidations\u201d (Required)\n   - Table capturing rule checks: [Item/ID | Person | Week/Date | Rule Name | Detected Value | Threshold | Status (OK/Flag) | Message/Recommended Fix].\n\nOPTIONAL SHEETS (not required but beneficial)\n- \u201cSubmission Export\u201d or \u201cPayroll Export\u201d: flat table formatted for payroll system upload.\n- \u201cLookups\u201d: e.g., instrument-to-doubling mapping, role codes.\n\nSCORING\n- 8: All required sheets present with clearly labeled tables as described; instructions/readme is substantive; structure makes verification straightforward.\n- 6: All required sheets present but one area is underdeveloped (e.g., missing effective dates on Rates or minimal Compliance Checks table) OR one optional section missing where otherwise implied.\n- 4: Missing one required sheet OR multiple required tables in Rates are missing.\n- 0: Not an Excel file OR missing multiple required sheets/tables making verification impractical.\n\nOnly evaluate structure, not correctness or formatting polish beyond basic readability.", "expectation": "A multi-sheet Excel model matching the specified structure with inputs, rates, calculations, compliance checks, and summary totals."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness & Verification (Mixed: LLM heavy, light code)", "description": "Now that structure is enforced, verify calculation rigor, traceability, and CBA rule coverage using a mix of LLM checks and deterministic code checks. Code weights kept ~5x lower than LLM.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "CBA Coverage and Rate Logic Mapping", "description": "Do the Rates tables comprehensively capture typical theatre musician CBA elements and map to how payroll should be calculated?", "weight": 3.5, "judge_prompt": "Evaluate whether the \u2018Rates\u2019 sheet(s) provide comprehensive coverage and usable parameters for a theatre-musician CBA. Look for:\n- Wage categories for weekly/guarantee, per-service performance, rehearsal, and clear units.\n- Premiums: doubling (by instrument or count), leader/contractor premiums, any chair/section premiums.\n- Overtime rules: context (daily/weekly/per service), thresholds, multipliers, and scope (rehearsal/performance/both).\n- Employer contributions: Pension, Health & Welfare, Vacation with basis (gross/pensionable), rates, caps.\n- Cartage/Travel/Per Diem and any conditions.\n- Effective date ranges to support year-over-year updates.\nScoring:\n- 3.5: All areas covered with clear, parameterized fields (thresholds, multipliers, units, dates) and obviously linkable to calculations.\n- 2.5: Minor gaps (e.g., partial overtime detail or missing caps) but largely usable.\n- 1.0: Several gaps; unclear units or missing effective dates; mapping to calc is weak.\n- 0: Sparse or missing key CBA components.", "expectation": "Rates comprehensively encode CBA elements with clear parameters, enabling mechanical application in calculations."}, {"type": "llm_judge", "name": "Calculation Traceability & Reconciliation", "description": "Are per-line calculations in Payroll_Calc transparently derived from Rates and do they reconcile to Summary totals?", "weight": 3.5, "judge_prompt": "Inspect \u2018Payroll_Calc\u2019 and \u2018Summary\u2019 sheets.\n- Does each line in Payroll_Calc show: unit count, base rate, applied multiplier/premium, resulting amount, and category mapping? Are references to rate IDs or rate sections visible?\n- Is there a visible reconciliation: sums by person and category in Payroll_Calc matching totals by person in Summary? Evidence can include subtotals or cross-footing notes.\n- Are employer contributions computed on the correct basis (gross vs pensionable) and included in Summary appropriately?\nScoring:\n- 3.5: Transparent line-level math, clear references to Rates, and obvious reconciliation to Summary.\n- 2.5: Mostly traceable with minor ambiguities; reconciliation appears correct overall.\n- 1.0: Weak traceability and unclear ties to Rates; reconciliation uncertain.\n- 0: No visible connection between calc and summary.", "expectation": "A clear audit trail from inputs to line calcs to person/category totals, with visible cross-footing."}, {"type": "llm_judge", "name": "Compliance Validation Scope and Logic", "description": "Do validations flag likely CBA conflicts and provide actionable messages?", "weight": 3.0, "judge_prompt": "Evaluate \u2018Compliance Checks\u2019 (or \u2018Validations\u2019) and any inline flags in Payroll_Calc.\nLook for coverage of common issues:\n- Under minimum weekly/guarantee; insufficient service pay.\n- Overtime triggering beyond thresholds (daily, weekly, or per service) with correct multipliers.\n- Excess services per day or rest-period violations (if modeled), excessive rehearsal hours, or performance stacking.\n- Doubling premiums correctly required when playing multiple instruments; leader/contractor premiums applied appropriately.\n- Cartage or per diem conditions recognized; travel days counted properly.\n- Missing critical identifiers (e.g., Union ID) or out-of-range inputs.\nAlso check that messages indicate the rule and the remedy.\nScoring:\n- 3.0: Broad coverage with clear, actionable messages and visible pass/fail status.\n- 2.0: Decent coverage with some gaps or vague messages.\n- 1.0: Minimal checks and limited usefulness.\n- 0: No meaningful validation logic.", "expectation": "A validations log that catches common violations and guides the contractor to fixes."}, {"type": "code", "name": "Compliance Flags Ratio (Deterministic Check)", "description": "Checks the Compliance/Validation sheet for flagged items; fewer flags imply better conformance for the sample provided.", "weight": 1.0, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float score in [0, weight]\n    \"\"\"\n    weight = 1.0\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n        # Find a Compliance/Validation sheet\n        target = None\n        for sn in xls.sheet_names:\n            s = str(sn).strip().lower()\n            if any(k in s for k in [\"compliance\", \"validation\", \"check\"]):\n                target = sn\n                break\n        if target is None:\n            return 0.0\n        df = pd.read_excel(file_path, sheet_name=target)\n        if df.empty:\n            return weight  # No issues listed; treat as pass for provided sample\n        # Normalize columns\n        cols = {c: str(c).strip().lower() for c in df.columns}\n        df.columns = list(cols.values())\n        status_col = None\n        for c in df.columns:\n            if c in (\"status\",) or re.search(r\"status|flag|result\", c):\n                status_col = c\n                break\n        if status_col is None:\n            return 0.3 * weight  # Partial credit: sheet exists but no explicit status\n        series = df[status_col].astype(str).str.lower().str.strip()\n        if len(series) == 0:\n            return weight\n        total = len(series)\n        flagged = series.str.contains(r\"flag|fail|error|violation|warn\").sum()\n        if total == 0:\n            return weight\n        rate = flagged / total\n        if flagged == 0:\n            return weight\n        elif rate <= 0.10:\n            return 0.5 * weight\n        elif rate <= 0.30:\n            return 0.2 * weight\n        else:\n            return 0.0\n    except Exception:\n        return 0.0\n"}, {"type": "code", "name": "Summary Totals Sanity (Deterministic Check)", "description": "Checks that per-musician Total is at least Gross plus employer contributions where columns exist; non-strict reconciliation sanity check.", "weight": 1.0, "code": "import re\nimport numpy as np\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float score in [0, weight]\n    \"\"\"\n    weight = 1.0\n    tol = 1.0  # $1 tolerance\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n        # Find Summary sheet\n        summary_sn = None\n        for sn in xls.sheet_names:\n            s = str(sn).strip().lower()\n            if \"summary\" in s or \"totals\" in s:\n                summary_sn = sn\n                break\n        if summary_sn is None:\n            return 0.0\n        df = pd.read_excel(file_path, sheet_name=summary_sn)\n        if df.empty:\n            return 0.0\n        # Normalize columns\n        cols = [str(c).strip().lower() for c in df.columns]\n        df.columns = cols\n        # Identify key columns\n        total_cols = [c for c in cols if \"total\" in c and \"subtotal\" not in c]\n        gross_cols = [c for c in cols if \"gross\" in c and (\"wage\" in c or \"gross\" in c)]\n        contrib_cols = [c for c in cols if any(k in c for k in [\"pension\", \"health\", \"welfare\", \"vacation\"]) and \"total\" not in c]\n        contrib_total_cols = [c for c in cols if (\"contrib\" in c or \"contribution\" in c) and \"total\" in c]\n        if not total_cols:\n            return 0.0\n        total_col = total_cols[0]\n        gross_col = gross_cols[0] if gross_cols else None\n        contrib_total_col = contrib_total_cols[0] if contrib_total_cols else None\n        # Keep numeric rows\n        df_numeric = df.copy()\n        for c in df_numeric.columns:\n            df_numeric[c] = pd.to_numeric(df_numeric[c], errors='coerce')\n        valid_rows = df_numeric[total_col].notna()\n        if valid_rows.sum() == 0:\n            return 0.0\n        dfv = df_numeric[valid_rows]\n        passes = 0\n        total_rows = len(dfv)\n        for _, row in dfv.iterrows():\n            total_val = row.get(total_col, np.nan)\n            if pd.isna(total_val):\n                continue\n            checks_ok = True\n            if gross_col is not None and gross_col in row.index:\n                gross_val = row.get(gross_col, np.nan)\n                if pd.notna(gross_val):\n                    if total_val + tol < gross_val:\n                        checks_ok = False\n                # Contribution expectation\n                contrib_est = None\n                if contrib_total_col is not None:\n                    contrib_est = row.get(contrib_total_col, np.nan)\n                elif contrib_cols:\n                    contrib_est = 0.0\n                    for cc in contrib_cols:\n                        v = row.get(cc, np.nan)\n                        if pd.notna(v):\n                            contrib_est += float(v)\n                if contrib_est is not None and pd.notna(gross_val) and pd.notna(contrib_est):\n                    if total_val + tol < gross_val + contrib_est:\n                        checks_ok = False\n            if checks_ok:\n                passes += 1\n        if total_rows == 0:\n            return 0.0\n        ratio = passes / total_rows\n        return float(ratio * weight)\n    except Exception:\n        return 0.0\n"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality & Professionalism", "description": "Holistic assessment of usability, maintainability, scalability, and professional presentation for contractor operations.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Usability for Contractor (Clarity of Inputs & Flow)", "description": "Assess whether inputs are obvious, guarded with validation, and the overall workflow is intuitive for weekly submission.", "weight": 1.25, "judge_prompt": "Judge usability from a contractor\u2019s perspective:\n- Inputs clearly highlighted and separated from outputs; instructions give step-by-step flow.\n- Data validation (drop-downs, Y/N flags, date pickers) where appropriate; clear warnings.\n- Minimal manual calculations; clear indicators of what must be filled before totals.\n- Submission/export area easy to find and use.\nScore:\n- 1.25: Very clear, guarded inputs, and intuitive end-to-end flow.\n- 0.9: Generally clear with minor rough edges.\n- 0.5: Usable but confusing in places; weak guidance/validation.\n- 0: Hard to use; inputs ambiguous.", "expectation": "A contractor can confidently input roster and schedule, view flags, and submit totals without confusion."}, {"type": "llm_judge", "name": "Maintainability & Rate Update Readiness", "description": "Evaluate how easily annual CBA rate changes can be applied without breaking formulas.", "weight": 1.25, "judge_prompt": "Assess maintainability:\n- Rates centralized in one place with effective dates; calculations reference these lookups (not hard-coded).\n- Named ranges/tables used; minimal fragile cell references.\n- Clear instructions for updating rates; version/effective date fields present.\nScore:\n- 1.25: Fully centralized, robust referencing, and clear update instructions.\n- 0.9: Mostly centralized with some hard-coding or limited guidance.\n- 0.5: Mixed; risk of breakage when updating.\n- 0: Heavily hard-coded; unclear how to update.", "expectation": "Centralized rates with parameterized formulas and instructions to update per year."}, {"type": "llm_judge", "name": "Scalability & Robustness", "description": "Can the model handle any orchestra configuration or production run length without rework?", "weight": 1.25, "judge_prompt": "Consider:\n- Dynamic tables that scale to many musicians, multiple weeks, and diverse schedules.\n- Support for multiple instruments/doubling, leader/contractor flags, and variable services per day.\n- Avoids brittle ranges; uses structured references.\n- Reasonable performance (no excessive volatile formulas) and not overly complex.\nScore:\n- 1.25: Clearly scalable and robust with dynamic structures.\n- 0.9: Scales with minor manual steps.\n- 0.5: Limited scalability; manual copying likely.\n- 0: Not scalable; rigid design.", "expectation": "Dynamic, table-driven model that expands to more musicians/weeks with minimal effort."}, {"type": "llm_judge", "name": "Professional Presentation & Output Readiness", "description": "Formatting, readability, and readiness of export/summary for payroll submission.", "weight": 1.25, "judge_prompt": "Evaluate presentation:\n- Consistent formatting, currency and date formats, readable column widths.\n- Clear separation of wages vs employer contributions and a final total.\n- Optional export/submission sheet or a clean print area in Summary.\n- Minimal visual clutter; helpful labels and legends.\nScore:\n- 1.25: Polished, submission-ready outputs.\n- 0.9: Generally professional with small issues.\n- 0.5: Functional but unpolished/confusing.\n- 0: Poor presentation.", "expectation": "A clean, professional Summary/Export suitable for direct submission."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "84322284-5c2c-4873-b507-b147449d209d", "rubric": {"category_name": "Retail PI Weekly Report (Internal Review)", "rationale": "This is a document-style task (Pattern B). The deliverable is a professional investigative report in PDF that reconstructs a weekly timeline, analyzes observations, and concludes with a professional assessment and recommendations. Stage 1 uses an LLM gate to strictly enforce a verifiable, self-documenting structure (sections, tables, appendix). Stage 2 mixes code rules (deterministic text and structure checks from the PDF) with LLM judges (consistency, evidence alignment, and control mapping). Stage 3 provides a holistic quality assessment focused on professional standards and client usefulness.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Structure Gate (PDF Report Contract)", "description": "Enforce exact output shape for a professional investigative report to enable verification. LLM-only gate. If structure is not met, the entire category is zeroed.", "is_required": true, "max_points": 5.0, "min_score_to_pass": 3.5, "rules": [{"type": "llm_judge", "name": "PDF Structure and Required Sections", "description": "Verify the deliverable is a PDF investigative report with all required sections and the mandated timeline table structure to enable verification and cross-referencing.", "weight": 5.0, "judge_prompt": "You are evaluating whether the candidate\u2019s OUTPUT FILE is a properly structured investigative report for a retail internal review. Only check structure/format and presence of required elements, not the content quality or correctness.\n\nFormat requirements:\n- Must be a PDF report (not Excel/CSV/code). At least 2 pages.\n- Professional formatting with clear headings and subsections.\n\nRequired sections (flexible naming is acceptable if clearly equivalent):\n1) Executive Summary (or Overview) \u2014 must summarize the week\u2019s key findings at a high level.\n2) Background and Scope \u2014 case context, location, objectives, scope/limitations.\n3) Methodology and Sources \u2014 explicitly references the field investigator\u2019s weekly notes and the time log/general observations template provided by the supervisor/company.\n4) Reconstructed Week Timeline \u2014 presented as a table. The table must be clearly labeled and include columns similar to all of the following (flexible naming allowed):\n   - Date and Day\n   - Time (Start/End or specific time)\n   - Location/Area\n   - Staff Member(s) and/or Role\n   - Event/Observation (narrative)\n   - Source/Reference (e.g., Note ID, Time Log Ref, Template Ref, CCTV ref)\n   - Risk/Category (e.g., refund abuse, voids, sweethearting, receiving discrepancies)\n   - Follow-up/Action\n5) Observations and Areas of Concern \u2014 synthesizes suspicious behaviors/patterns.\n6) Analysis \u2014 links observations to plausible causes of discrepancies.\n7) Conclusions and Recommendations \u2014 explicit, actionable next steps for the client.\n8) Appendix \u2014 includes at minimum:\n   - Appendix A: Event Log (timeline table repeated or expanded for cross-reference)\n   - Appendix B: Evidence/Reference Map (how entries map back to the investigator\u2019s notes/time log/template; naming like Exhibits/References acceptable).\n\nScoring (structure only):\n- 5.0: PDF with professional formatting, all 8 sections present including a clearly labeled timeline table with the specified columns.\n- 4.0: PDF with professional formatting, missing only one non-core appendix element (e.g., missing Appendix B but Appendix A and all core sections exist).\n- 3.0: PDF with professional formatting, missing one core section OR timeline table present but missing multiple key columns.\n- 1.5: PDF present but only 3\u20134 sections or the timeline is not a table.\n- 0.0: Not a PDF OR fewer than 3 sections OR no timeline section.\n\nOnly evaluate presence/format and table structure; do not judge correctness of content.", "expectation": "A cleanly structured PDF report with the enumerated sections and a clearly labeled timeline table that includes the specified columns to enable downstream verification."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Verification", "description": "Now that the report is in a verifiable shape, check correctness, internal consistency, evidence mapping, and plausibility. Mix of deterministic code checks and LLM judgment. Code rules are lightweight; LLM rules carry most of the weight.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Valid Document and Sufficient Length", "description": "Confirm the output is a document with sufficient length to cover a full weekly investigation (word count threshold).", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output found.\"\n    if not output.is_document:\n        return 0.0, \"Primary output is not a document.\"\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0, \"Unable to extract text from document.\"\n    if not text or not text.strip():\n        return 0.0, \"Empty or unreadable document text.\"\n    words = re.findall(r\"\\b\\w+\\b\", text)\n    count = len(words)\n    # Expect a thorough report: aim for >= 800 words for full score.\n    score = min(1.0, count / 800.0)\n    return score, f\"Word count: {count}.\""}, {"type": "code", "name": "Timeline Signals and Ordering Cues", "description": "Detect presence of a timeline heading and sufficient time/day markers to reconstruct a week. Partial credit if some signals exist.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n    t = text.lower()\n    has_timeline_heading = 1.0 if (\"timeline\" in t or \"event log\" in t) else 0.0\n    # Time patterns: 24h and 12h forms\n    times_24 = re.findall(r\"\\b(?:[01]?\\d|2[0-3]):[0-5]\\d\\b\", t)\n    times_12 = re.findall(r\"\\b(?:[1-9]|1[0-2]):[0-5]\\d\\s?(?:am|pm)\\b\", t)\n    times_ampm_simple = re.findall(r\"\\b(?:[1-9]|1[0-2])\\s?(?:am|pm)\\b\", t)\n    time_set = set(times_24) | set(times_12) | set(times_ampm_simple)\n    time_score = min(1.0, len(time_set) / 5.0)\n    # Day/date signals\n    day_words = re.findall(r\"\\b(mon|tue|wed|thu|fri|sat|sun|monday|tuesday|wednesday|thursday|friday|saturday|sunday)\\b\", t)\n    day_nums = re.findall(r\"\\bday\\s*(?:one|two|three|four|five|six|seven|[1-7])\\b\", t)\n    dates_us = re.findall(r\"\\b(?:0?[1-9]|1[0-2])[/-](?:0?[1-9]|[12]\\d|3[01])[/-](?:\\d{2}|\\d{4})\\b\", t)\n    dates_iso = re.findall(r\"\\b\\d{4}-\\d{2}-\\d{2}\\b\", t)\n    day_set = set(day_words) | set(day_nums) | set(dates_us) | set(dates_iso)\n    day_score = min(1.0, len(day_set) / 3.0)\n    score = (has_timeline_heading + time_score + day_score) / 3.0\n    return float(score)"}, {"type": "code", "name": "Retail Forensic Keyword Coverage", "description": "Check coverage of retail loss-prevention and inventory discrepancy concepts to ensure plausible domain depth.", "weight": 0.5, "code": "def evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n    t = text.lower()\n    keywords = set([\n        'pos','register','till','cash drop','eod','end of day','refund','return','void','discount',\n        'override','no-sale','no sale','sweetheart','sweethearting','markdown','inventory','shrink',\n        'cycle count','receiving','backroom','cctv','camera','blind spot','bag check','loss prevention',\n        'exception report','comp','freebie','theft','policy','sop','tender','drawer','audit','variance'\n    ])\n    present = set()\n    for k in keywords:\n        if k in t:\n            present.add(k)\n    # Full credit around 8+ distinct domain terms\n    score = min(1.0, len(present) / 8.0)\n    return float(score)"}, {"type": "code", "name": "Actionable Recommendations Presence", "description": "Verify the report includes actionable recommendations (imperative verbs, at least three distinct actions).", "weight": 0.3, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n    t = text.lower()\n    action_verbs = [\n        'implement','audit','review','restrict','reconcile','monitor','install','relocate','configure',\n        'train','discipline','separate duties','segregate','whitelist','blacklist','flag','investigate',\n        'validate','count','calibrate','secure','lock','log','document','randomize','spot-check','escalate'\n    ]\n    # Count lines with recommendation-like structure\n    lines = [ln.strip().lower() for ln in t.splitlines()]\n    rec_lines = []\n    for ln in lines:\n        if 'recommend' in ln or ln.startswith('- ') or re.match(r'^\\d+\\.', ln):\n            for v in action_verbs:\n                if v in ln:\n                    rec_lines.append(ln)\n                    break\n    distinct = len(set(rec_lines))\n    score = min(1.0, distinct / 3.0)\n    return float(score)"}, {"type": "llm_judge", "name": "Timeline Coherence and Cross-Referencing", "description": "Assess whether the reconstructed timeline is logically ordered and entries are cross-referenced to sources (notes, time log, template, CCTV refs).", "weight": 3.0, "judge_prompt": "Evaluate the reconstructed timeline section:\n- Is the timeline clearly ordered (by date/day and time)?\n- Do entries include staff/role, area, and event/observation details?\n- Do entries reference sources (investigator\u2019s notes, time log, template, exhibits, or CCTV markers) so an auditor could cross-check?\n- Are gaps or uncertainties explicitly noted?\nScoring:\n- 1.0: Fully ordered, rich details, consistent cross-references on most entries.\n- 0.7: Mostly ordered with minor gaps; some entries have references.\n- 0.4: Partially ordered but sparse details; few references.\n- 0.0: Not reconstructable or lacks ordering and references.\nOnly judge internal coherence and presence of cross-references, not truthfulness.", "expectation": "A clearly ordered, cross-referenced timeline that enables straightforward audit against logs and CCTV."}, {"type": "llm_judge", "name": "Evidence-to-Conclusion Alignment", "description": "Check whether key findings and conclusions flow from the observations, avoiding unsupported assertions.", "weight": 2.5, "judge_prompt": "Review the analysis, observations, and conclusion/recommendations sections together.\n- Do conclusions clearly cite or paraphrase specific observations/events from the week?\n- Are plausible alternative explanations noted (e.g., process/training issues vs. intentional misconduct)?\n- Are speculative statements labeled as such, with suggested follow-up steps instead of definitive claims?\nScoring:\n- 1.0: Strong linkage from observations to conclusions, alternatives considered, speculation labeled.\n- 0.7: Generally linked but with minor unsupported leaps.\n- 0.4: Weak linkage; several assertions lack grounding.\n- 0.0: Conclusions largely unsupported by the described observations.", "expectation": "Conclusions grounded in the week\u2019s observations with transparent reasoning and acknowledgment of uncertainty."}, {"type": "llm_judge", "name": "Risk and Control Mapping", "description": "Assess whether identified risks map to concrete controls and investigative next steps relevant to retail loss prevention.", "weight": 2.5, "judge_prompt": "Evaluate how well the report translates observed risks into controls and next steps.\n- Are common retail discrepancy vectors addressed (e.g., refund/void abuse, sweethearting, receiving/backroom errors, cash handling violations, policy non-compliance)?\n- Are recommended controls specific and feasible (e.g., POS exception review, CCTV repositioning, bag check policy, cycle counts, drawer audits, segregation of duties)?\n- Are responsibilities and immediate vs. longer-term actions distinguished where appropriate?\nScoring:\n- 1.0: Clear mapping from risks to specific controls and prioritized actions.\n- 0.7: Mostly mapped with some generalities.\n- 0.4: Vague or generic controls.\n- 0.0: Risks not connected to actionable controls.", "expectation": "Practical, retail-relevant control recommendations tied to identified risks."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality Assessment", "description": "Holistic evaluation of presentation quality, client usefulness, and professionalism of the investigative report.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Tone and Readability", "description": "Assess clarity, structure, and polished tone appropriate for a client-facing investigative report.", "weight": 1.5, "judge_prompt": "Evaluate the report\u2019s writing quality and organization:\n- Clear, concise, and professional tone throughout?\n- Logical progression with informative headings and subheadings?\n- An effective Executive Summary that highlights key findings and recommended actions?\nScoring: 1.0 excellent; 0.7 good with minor issues; 0.4 uneven; 0.0 poor or hard to read.", "expectation": "A polished, clearly organized, and professional client-facing document."}, {"type": "llm_judge", "name": "Client Usefulness and Actionability", "description": "Judge whether the report equips the client to act: prioritization, next steps, and expected outcomes.", "weight": 1.5, "judge_prompt": "Assess practical usefulness:\n- Are recommendations prioritized or sequenced, with clear next steps?\n- Are expected benefits or risk reductions articulated?\n- Would a store/regional manager know what to do next after reading?\nScoring: 1.0 highly actionable; 0.7 mostly actionable; 0.4 generic; 0.0 unclear next steps.", "expectation": "Clear, prioritized actions that the client can implement immediately."}, {"type": "llm_judge", "name": "Ethical and Legal Prudence", "description": "Ensure the report avoids unfounded accusations, respects privacy/employment constraints, and uses appropriate caveats.", "weight": 1.0, "judge_prompt": "Evaluate professional restraint and compliance:\n- Does the report avoid definitive accusations without corroborating evidence?\n- Are privacy and employment-law sensitivities acknowledged where relevant (e.g., CCTV usage, searches, HR processes)?\n- Are recommended actions framed with proper approvals/chain of custody when applicable?\nScoring: 1.0 strong; 0.7 adequate; 0.4 weak; 0.0 problematic.", "expectation": "Responsible, compliant framing with appropriate caveats and approvals."}, {"type": "llm_judge", "name": "Visual Clarity of Tables and Appendices", "description": "Assess whether the timeline table and appendices are easy to scan and consistently formatted for cross-referencing.", "weight": 1.0, "judge_prompt": "Review tables/appendices:\n- Is the timeline table clearly labeled with consistent columns and formatting?\n- Are appendices (Event Log, Evidence/Reference Map) present and legible?\n- Are references/exhibits named consistently between the main text and appendices?\nScoring: 1.0 excellent clarity/consistency; 0.7 minor issues; 0.4 cluttered/inconsistent; 0.0 missing or unusable.", "expectation": "Clear, consistent tables and appendices that support quick audit and cross-reference."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "61e7b9c6-0051-429f-a341-fda9b6578a84", "rubric": {"category_name": "Healthcare: Menopause Service Formulary (Medical and Health Services Managers)", "rationale": "This rubric enforces a self-documenting, verifiable Excel-based formulary. Stage 1 (LLM-only) mandates a precise workbook structure to enable automated checks. Stage 2 mixes light-weight code rules (deterministic checks on prices and brand uniqueness) with deeper LLM verification (regulatory alignment, coverage breadth, pricing traceability). Stage 3 assesses professional quality and clinical usability for the intended audience (physicians and advanced practice nurses).", "max_total_score": 15.0, "stages": [{"name": "Stage 1 \u2014 Workbook Structure Gate (LLM-only)", "description": "MANDATORY shape enforcement for the Menopause Formulary workbook. If the structure is not present, downstream verification is impossible.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Formulary Workbook Structural Requirements", "description": "Check if the candidate output is a single Excel workbook with the required sheets and columns to enable verification, following the provided template intent.", "weight": 4.0, "judge_prompt": "You are validating ONLY the STRUCTURE (not correctness) of a Menopause Formulary workbook for a US online Women\u2019s Health clinic. The output must be an Excel file with the following structure. Be flexible with exact sheet and column names where synonyms are clearly equivalent (e.g., \"Formulary\" vs \"Medication List\"), but all core elements must be present.\n\nOverall Format Requirements:\n- Must be a single Excel workbook (.xlsx or .xls). Not PDF, not DOCX, not CSV alone.\n- Contains at least 15 formulary rows total.\n- Columns must be presented as visible headers, with tabular data in rows.\n\nRequired Sheet A: \"Formulary\" (or a clearly equivalent primary sheet)\nMust include these columns (exact or close synonyms):\n1) Category (e.g., MHT/Systemic, MHT/Local, Nonhormonal FDA-approved, Nonhormonal Off-label)\n2) Generic Name\n3) Brand Name (Chosen Brand)\n4) Dosage Form / Formulation (e.g., tablet, patch, gel, ring, capsule, cream, spray)\n5) Strength (include units)\n6) Route (e.g., oral, transdermal, vaginal)\n7) Dosing Schedule\n8) Indication(s)/Symptoms Targeted\n9) FDA Status (e.g., FDA-approved for menopause vs off-label)\n10) Contraindications/Key Warnings\n11) Monitoring/Clinical Notes\n12) 30-Day Price USD (Uninsured) \u2014 a monthly price field\n13) Price Source (URL)\n14) Date Checked (for pricing)\n\nRequired Sheet B: \"Pricing Sources\" (or clearly equivalent)\nMust include columns enabling traceability of prices, such as:\n- Generic Name and/or Brand Name\n- Source URL (or Price Source)\n- Pharmacy or Vendor\n- Quantity/Package basis and Unit/Extended price\n- Zip code or location basis (if used)\n- Coupon/Program (if applicable)\n- Date Checked\n- Notes\n\nRequired Sheet C: \"Methodology & Assumptions\" (or clearly equivalent, e.g., \"Notes & Assumptions\")\nMust include prose sections explaining:\n- Source sites used for prices (e.g., GoodRx) and how monthly price was derived\n- Rule that only FDA-approved medications will be prescribed; how off-label use is indicated\n- When and how the clinic selects a single brand for identical formulations\n- Any default assumptions (e.g., conversion to 30-day quantity, zip code used)\n\nOptional but Helpful Sheet D: \"Version Control\" (or \"Cover/ReadMe\")\n- Prepared by, Date, Version, Data last updated, and Change Log\n\nScoring:\n- 4.0: Valid Excel AND all three required sheets present with the listed columns/sections; >=15 formulary rows; columns are clearly mapped (minor naming variations OK). Optional sheet D may or may not be present.\n- 3.0: Valid Excel; core Formulary sheet present with most required columns (no more than 3 missing), AND at least one of the two supporting sheets (Pricing Sources or Methodology & Assumptions) present with most elements.\n- 1.5: Valid Excel; Formulary sheet present but missing 4\u20136 required columns or missing both supporting sheets.\n- 0.0: Not Excel, or missing the Formulary sheet, or <10 formulary rows.\n\nOnly check PRESENCE and STRUCTURE, not accuracy of content or clinical correctness.", "expectation": "A well-structured Excel workbook with Formulary, Pricing Sources, and Methodology sheets, enabling downstream verification."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness & Consistency)", "description": "Deterministic checks via code plus deeper clinical/regulatory and coverage checks via LLM.", "is_required": false, "max_points": 7.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Price Plausibility and Traceability (Deterministic)", "description": "Checks if monthly prices are numeric and plausible, and that price source URL and date are present for most rows on the Formulary sheet.", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        # Find a sheet that looks like the formulary\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheet_name = None\n        for s in xls.sheet_names:\n            if 'formulary' in s.lower() or 'medication' in s.lower() or 'drug' in s.lower():\n                sheet_name = s\n                break\n        if sheet_name is None:\n            sheet_name = xls.sheet_names[0]\n        df = pd.read_excel(path, sheet_name=sheet_name)\n        if df.empty or df.shape[0] < 5:\n            return 0.0, \"Formulary sheet too small to assess.\"\n        # Normalize columns\n        cols_map = {c: str(c).strip() for c in df.columns}\n        df.columns = [c.lower() for c in cols_map.values()]\n        # Identify key columns by fuzzy names\n        def find_col(cands):\n            for c in df.columns:\n                for cand in cands:\n                    if cand in c:\n                        return c\n            return None\n        price_col = find_col([\"30\", \"month\", \"monthly\", \"price\"]) or find_col([\"price\"]) \n        source_col = find_col([\"price source\", \"source url\", \"url\", \"source\"]) \n        date_col = find_col([\"date checked\", \"date\", \"priced on\"]) \n        # If we can't find a price column at all, fail softly\n        if price_col is None:\n            return 0.0, \"No recognizable monthly price column.\"\n        # Clean price to numeric\n        def to_num(x):\n            if pd.isna(x):\n                return np.nan\n            s = str(x)\n            # Take first numeric (allows ranges like \"$30\u2013$40\")\n            m = re.search(r\"[-+]?[0-9]*\\.?[0-9]+\", s.replace(\",\", \"\"))\n            try:\n                return float(m.group(0)) if m else np.nan\n            except:\n                return np.nan\n        prices = df[price_col].apply(to_num)\n        valid_price = prices.between(1, 3000)  # plausible monthly window\n        n = len(df)\n        frac_price = valid_price.mean() if n else 0.0\n        # Source URL validity (basic)\n        if source_col is not None:\n            src = df[source_col].astype(str).str.strip()\n            valid_src = src.str.startswith(\"http\").fillna(False)\n            frac_src = valid_src.mean() if n else 0.0\n        else:\n            frac_src = 0.0\n        # Date plausibility (parseable, not future, within 5 years)\n        if date_col is not None:\n            dates = pd.to_datetime(df[date_col], errors='coerce')\n            now = pd.Timestamp.utcnow().normalize()\n            within_5y = dates.notna() & (dates <= now) & (dates >= now - pd.DateOffset(years=5))\n            frac_date = within_5y.mean() if n else 0.0\n        else:\n            frac_date = 0.0\n        # Weighted completeness: price (0.6), source (0.2), date (0.2)\n        score_frac = 0.6*frac_price + 0.2*frac_src + 0.2*frac_date\n        # Require at least 8 rows with valid price to avoid trivial passes\n        sufficient = (valid_price.sum() >= 8)\n        final = (score_frac if sufficient else score_frac*0.5) * 0.5  # scale to weight=0.5\n        feedback = f\"Valid price rows: {valid_price.sum()}/{n}; price completeness={frac_price:.2f}, source={frac_src:.2f}, date={frac_date:.2f}.\"\n        return final, feedback\n    except Exception as e:\n        return 0.0, f\"Exception in price plausibility rule: {e}\""}, {"type": "code", "name": "Single Brand per Identical Formulation (Deterministic)", "description": "Verifies that for identical generic+strength+form+route, the chosen brand is unique (the clinic picks only one brand per exact formulation).", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheet_name = None\n        for s in xls.sheet_names:\n            if 'formulary' in s.lower() or 'medication' in s.lower() or 'drug' in s.lower():\n                sheet_name = s\n                break\n        if sheet_name is None:\n            sheet_name = xls.sheet_names[0]\n        df = pd.read_excel(path, sheet_name=sheet_name)\n        if df.empty or df.shape[0] < 5:\n            return 0.0, \"Formulary sheet too small to assess.\"\n        # Normalize columns\n        df_cols = [str(c).strip().lower() for c in df.columns]\n        df.columns = df_cols\n        def pick(col_keys):\n            for c in df.columns:\n                for k in col_keys:\n                    if k in c:\n                        return c\n            return None\n        generic_col = pick([\"generic\"]) \n        brand_col = pick([\"brand\"]) \n        form_col = pick([\"dosage form\", \"formulation\", \"form\"]) \n        strength_col = pick([\"strength\", \"dose\"]) \n        route_col = pick([\"route\"]) \n        needed = [generic_col, brand_col, form_col, strength_col, route_col]\n        if any(c is None for c in needed):\n            return 0.1, \"Missing one or more required columns (generic/brand/form/strength/route); partial credit.\"\n        def norm(s):\n            s = '' if pd.isna(s) else str(s).strip().lower()\n            return re.sub(r\"\\s+\", \" \", s)\n        g = df.groupby([\n            df[generic_col].apply(norm),\n            df[strength_col].apply(norm),\n            df[form_col].apply(norm),\n            df[route_col].apply(norm)\n        ], dropna=False)\n        uniq_brands = g[brand_col].apply(lambda x: len(set(norm(v) for v in x if str(v).strip()!='')))\n        total_groups = len(uniq_brands)\n        if total_groups == 0:\n            return 0.0, \"No groupable entries.\"\n        ok_groups = (uniq_brands <= 1).sum()\n        frac = ok_groups / total_groups\n        # Provide partial credit if at least most groups satisfy uniqueness\n        score = frac * 0.5\n        return score, f\"Groups with single brand: {ok_groups}/{total_groups}.\"\n    except Exception as e:\n        return 0.0, f\"Exception in brand uniqueness rule: {e}\""}, {"type": "llm_judge", "name": "Regulatory Alignment and Labeling Clarity", "description": "Assesses whether FDA-approved vs off-label status is clearly and correctly indicated; excludes compounded/bioidentical non-FDA products; appropriately notes uterine protection when systemic estrogen is used.", "weight": 2.0, "judge_prompt": "Review the workbook (especially the Formulary and Methodology/Notes sheets) for regulatory alignment and labeling clarity:\n\nCheck for:\n- Clear identification of which entries are FDA-approved for menopause indications vs off-label uses. Off-label items should be explicitly labeled as such.\n- Absence of non-FDA-approved compounded/bioidentical products (e.g., custom-compounded creams) on the prescription list.\n- For systemic estrogen therapy, presence of notes about the need for endometrial protection with a progestogen when the patient has an intact uterus.\n- Inclusion of relevant contraindications/warnings for estrogen therapy (e.g., VTE risk) in a designated column.\n\nScoring (0\u20132):\n- 2.0: All criteria met; FDA vs off-label labels are consistent; no compounded items; appropriate uterine protection notes and key warnings present.\n- 1.0: Minor gaps (e.g., a few items missing explicit label or sparse warnings), but overall consistent.\n- 0.0: Major issues (compounded products included, labeling absent/inaccurate, or systemic estrogen without any uterine protection guidance).", "expectation": "Clear, accurate FDA vs off-label labeling; no compounded products; proper uterine protection notes for systemic estrogen."}, {"type": "llm_judge", "name": "Coverage Breadth Across Key Therapy Classes", "description": "Evaluates whether the formulary covers the major therapy classes for perimenopause/menopause symptoms, including MHT and common nonhormonal options.", "weight": 2.0, "judge_prompt": "Assess coverage breadth of therapy classes on the Formulary:\nExpect to see a broad selection across:\n- Systemic estrogen options (oral and transdermal) and appropriate progestogens (e.g., micronized progesterone); combination products where appropriate.\n- Local (vaginal) estrogen therapies.\n- SERMs/TSECs (e.g., ospemifene; bazedoxifene/conjugated estrogens).\n- Vaginal prasterone (DHEA) for dyspareunia.\n- Nonhormonal FDA-approved option for vasomotor symptoms (e.g., low-dose paroxetine 7.5 mg), plus commonly used off-label agents (e.g., SSRIs/SNRIs, gabapentin, clonidine) clearly labeled as off-label.\n\nScoring (0\u20132):\n- 2.0: Strong breadth with multiple entries across most categories listed above.\n- 1.0: Partial breadth; at least one option in most categories but sparse.\n- 0.0: Narrow coverage; missing several major categories.", "expectation": "Representation of major MHT and nonhormonal classes relevant to menopause symptom management."}, {"type": "llm_judge", "name": "Pricing Traceability and Methodological Transparency", "description": "Reviews whether pricing can be traced back to sources with sufficient details and whether the methodology for price estimation and brand selection is documented.", "weight": 2.0, "judge_prompt": "Review the Pricing Sources and Methodology/Notes sheets for traceability and transparency:\n\nLook for:\n- Source URLs, pharmacy/vendor names, quantity/package basis, and date recorded to support the monthly price shown on the Formulary.\n- Explanation of how a 30-day price was derived (e.g., conversion from package size, assumptions), and any default zip code or location basis.\n- Stated rule on choosing only one brand per identical formulation.\n\nScoring (0\u20132):\n- 2.0: Clear URLs and details for most items; methodology explains conversion to monthly cost and brand-selection rule.\n- 1.0: Some traceability and partial methodology; a few gaps but generally understandable.\n- 0.0: Sparse or missing URLs/details and no clear methodology.", "expectation": "Most items have traceable pricing data; methodology explains monthly price derivation and brand selection."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Usability Assessment", "description": "Holistic LLM assessment of professional quality, clinical usefulness, and maintainability.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Data Hygiene", "description": "Checks formatting, readability, consistent units, and basic analytics affordances (filters, frozen headers).", "weight": 1.0, "judge_prompt": "Evaluate the workbook\u2019s professional presentation:\n- Clear headers, readable layout, consistent units (mcg, mg), and normalized routes/forms.\n- Reasonable formatting (e.g., filters enabled, frozen header row, no obvious typos or duplicated columns).\n- Avoids clutter; columns are consistently populated and not misaligned.\n\nScoring (0\u20131):\n- 1.0: Professional, clean, easy to read.\n- 0.5: Minor issues but usable.\n- 0.0: Messy or confusing presentation.", "expectation": "A clean, professional workbook with consistent formatting and units."}, {"type": "llm_judge", "name": "Clinical Usability for Prescribers", "description": "Assesses whether dosing schedules, indications, and key warnings support safe, efficient prescribing.", "weight": 1.0, "judge_prompt": "Assess the clinical usability of the Formulary for physicians/APNs:\n- Dosing schedules are specific (e.g., frequency, titration where applicable).\n- Indications/symptoms targeted are explicit.\n- Key warnings/contraindications and monitoring notes support safe prescribing (e.g., VTE risk, breast cancer considerations, BP monitoring for clonidine).\n\nScoring (0\u20131):\n- 1.0: Clear dosing and safety notes across entries.\n- 0.5: Partially specified; usable with some gaps.\n- 0.0: Sparse dosing/safety details, not prescriber-ready.", "expectation": "Actionable dosing and safety notes that clinicians can use immediately."}, {"type": "llm_judge", "name": "Patient-Centered Economic Clarity", "description": "Evaluates how well the sheet communicates price to aid shared decision-making.", "weight": 1.0, "judge_prompt": "Evaluate the economic clarity of the Formulary:\n- Monthly price is clearly labeled; units/quantity assumptions are understandable.\n- Notes acknowledge variability (e.g., coupons, pharmacy differences) or provide ranges when appropriate.\n- Where multiple strengths exist, pricing logic is clear (e.g., per-month basis is comparable).\n\nScoring (0\u20131):\n- 1.0: Clear and helpful cost framing.\n- 0.5: Some clarity but minor ambiguities.\n- 0.0: Confusing or poorly explained pricing.", "expectation": "Transparent monthly price communication to support shared decision-making."}, {"type": "llm_judge", "name": "Versioning and Maintainability", "description": "Assesses whether the workbook includes version/date, preparer, and change log for ongoing maintenance.", "weight": 1.0, "judge_prompt": "Check for versioning/maintenance elements (on a cover or version sheet):\n- Prepared by, Version, Date, Data last updated.\n- Brief change log or update notes.\n\nScoring (0\u20131):\n- 1.0: Clear versioning and change history present.\n- 0.5: Partial info present.\n- 0.0: No versioning indicators.", "expectation": "Basic governance metadata for ongoing maintenance."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "c657103b-b348-4496-a848-b2b7165d28b2", "rubric": {"category_name": "Finance/Insurance \u2014 Personal Financial Advisor: 8-Year Roth Conversion Plan (Slides + Excel Model)", "rationale": "Mixed deliverables: a client-facing 8-slide presentation and a detailed Excel model. Stage 1 is a strict LLM-only gate to enforce exact structure that makes verification trivial. Stage 2 mixes targeted code checks (bounds, structure, plausibility) with higher-weight LLM cross-checks (consistency of assumptions, timelines, and comparative outcomes). Stage 3 evaluates overall quality, clarity, and client-appropriateness.", "max_total_score": 30.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (LLM only)", "description": "Verify that BOTH required files exist with the exact structures needed for verification: (1) an 8-slide presentation; (2) a structured Excel workbook with specific sheets/sections enabling checking of conversion plan, RMDs, balances, taxes, and methodology.", "is_required": true, "max_points": 5.0, "min_score_to_pass": 3.5, "rules": [{"type": "llm_judge", "name": "Excel Model: Required Sheets and Structures", "description": "Check the candidate Excel model\u2019s file format and sheet-level structure. Only verify presence/shape, not correctness of calculations.", "weight": 2.5, "judge_prompt": "You are checking ONLY the presence and structure of an EXCEL workbook required for a Roth conversion analysis, not the correctness of numbers.\n\nConfirm all of the following are present (be flexible with exact names; small naming variations are OK if intent is clear). The file must be an Excel workbook (.xlsx) with:\n\n1) Sheet: \"Assumptions\"\n   - Contains a clearly labeled table of key inputs: at least the following fields are explicitly present and filled:\n     \u2022 Starting balance or starting 401(k)/Traditional IRA balance (2025) \u2248 $3.5M\n     \u2022 Client age turning 65 at end of 2025\n     \u2022 Filing status: Married Filing Jointly (MFJ)\n     \u2022 Annual non-IRA income: $200,000\n     \u2022 Annual return assumption \u2248 8%\n     \u2022 Marginal tax bracket reference: 32\u201335% range\n\n2) Sheet: \"Conversion Plan\" (or similarly named like \"Roth Conversion Plan\")\n   - A year-by-year layout including: Period (0..29) and/or Calendar Year (2025..2054), Age, Roth Conversion Amount, Estimated Tax from Conversion, and columns for account balances (end-of-year balances sufficient to trace effects).\n   - The sheet must show an 8-year Roth conversion schedule (8 distinct years with positive conversions), and reflect 2025 as Period 0 and 2054 as Period 29 (or the equivalent Year range).\n\n3) Sheet: \"RMDs\" (or similarly named: \"RMD Calculations\")\n   - Columns/tables for: Period/Year, Age, IRS Uniform Lifetime Table factor (labeled as Distribution Factor/Divisor), RMD Amounts for both scenarios (at least RMD for baseline RMD-only, and preferably RMD with conversion), and Taxes on RMDs or tax rate used.\n   - RMDs begin in the year the client turns 72.\n\n4) Sheet: \"Balances Comparison\" (or similarly named: \"Balances & Growth\", \"Scenario Comparison\")\n   - Side-by-side balances over time: Traditional IRA (No Conversion/RMD-only), Traditional IRA (With Conversion), Roth IRA (With Conversion), and preferably a total tax-advantaged balance or after-tax estate value column.\n\n5) Sheet: \"Tax Summary\" (or similarly named: \"Tax Impact\", \"Projected Tax Savings\")\n   - Summarizes lifetime/period total taxes by scenario and shows a projected tax savings metric attributable to the conversion strategy (even if preliminary).\n\n6) Sheet: \"Methodology & Sources\" (or similarly named) \n   - Contains at least 5 sentences explaining the approach, references the IRS 2025 Uniform Lifetime Table (a URL is fine), states any major assumptions, and notes limitations.\n\nScoring:\n- 2.5: All six sheets present with the specified content/columns. Period/Year coverage (2025..2054 or 0..29) is visible in the relevant sheets.\n- 2.0: One minor section/column missing but sheets are present overall; or small mislabeling yet structure is clearly there.\n- 1.2: One required sheet missing OR multiple sections/columns are unclear/incomplete.\n- 0.6: Two required sheets missing or time horizon not visible.\n- 0.0: Not an Excel file or missing multiple core sheets.\n\nOnly check structure and presence, not accuracy of numbers/calculations.", "expectation": "A well-structured .xlsx with Assumptions, Conversion Plan, RMDs, Balances Comparison, Tax Summary, Methodology & Sources; clear period/year coverage and 8-year conversion present."}, {"type": "llm_judge", "name": "Presentation: 8 Slides, Structure, and Theme", "description": "Check the candidate presentation\u2019s format and slide structure. Only verify presence/shape, not content quality.", "weight": 2.5, "judge_prompt": "You are checking ONLY the presence and structure of a presentation, not its numerical correctness.\n\nConfirm the following for the presentation (PPTX or PDF export of slides is acceptable):\n- It is a presentation file (PPTX preferred; PDF acceptable if it is clearly slide-formatted).\n- Exactly 8 slides are present.\n- Slide structure (flexible wording allowed):\n  1) Title slide: references Roth Conversion Strategy and client meeting context.\n  2) Purpose/Overview: why implement a Roth conversion (high-level benefits and goals).\n  3) Suitable Candidate: who benefits (tie to client profile like MFJ, income level, balance size, age, etc.).\n  4) Process/Steps: clear step-by-step overview of how an 8-year conversion would proceed.\n  5) Timeline: an 8-year timeline visual or clear depiction of the conversion cadence.\n  6) Tax Impact Overview: summarizes tax trade-offs, references or visually aligns with the Excel analysis.\n  7) Heirs & Estate Benefits: emphasizes tax-free distributions to heirs and estate tax exposure reduction.\n  8) Next Steps & Disclaimers: outlines decision points, implementation steps, and a general advisory/tax law disclaimer.\n- Use of a modern business theme; \u201cbusiness digital tunnel\u201d template or a similar professional, tunnel-like/gradient business visual is acceptable. Do not fail solely for template brand mismatch if the style is clearly professional and consistent.\n\nScoring:\n- 2.5: Valid presentation (PPTX or PDF) with exactly 8 slides and all 8 structural elements present.\n- 1.7: Exactly 8 slides but missing 1 structural element.\n- 1.0: 7\u20138 slides but missing 2+ required elements, or unclear theme/slide formatting.\n- 0.0: Not a slide deck, fewer than 7 slides, or grossly off-format.\n\nOnly check structure/presence, not the depth or accuracy of content.", "expectation": "An 8-slide, professionally themed deck covering purpose, suitability, steps, timeline, tax impact, heirs/estate benefits, and next steps/disclaimer."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Consistency Verification", "description": "Now that the files are in the correct shape, verify correctness and cross-document consistency of assumptions, timelines, RMD logic, and comparative outcomes. Mix code checks (deterministic structure/bounds) with higher-weight LLM checks (consistency and reasoning).", "is_required": false, "max_points": 15.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Assumptions Consistency (Profile \u2192 Model \u2192 Slides)", "description": "Cross-check the client profile and stated assumptions against the Excel Assumptions sheet and slide content.", "weight": 4.0, "judge_prompt": "Evaluate consistency of assumptions across materials:\n- Client profile: 64 now, turning 65 end of 2025; Married Filing Jointly; no other retirement assets; $200,000 annual non-IRA income; marginal tax brackets around 32%\u201335%; 8% annual return assumption; starting 401(k)/Traditional balance \u2248 $3.5M at end of 2025.\n- Excel Assumptions sheet should explicitly reflect these.\n- Slides should mention an 8-year Roth conversion plan and emphasize tax-free distributions to heirs and reduced estate tax exposure.\n\nScoring:\n- 4.0: All core assumptions clearly match in Excel and are referenced/echoed in slides.\n- 3.0: Minor omissions (e.g., one item missing) but broadly consistent.\n- 2.0: Multiple omissions or inconsistencies, but overall intent is visible.\n- 1.0: Sparse matching; several key profile elements missing or contradicted.\n- 0.0: Little to no alignment with the provided client profile/assumptions.", "expectation": "Assumptions and client facts are faithfully represented in the model and referenced in the deck."}, {"type": "llm_judge", "name": "RMD Start and Horizon Alignment", "description": "Check that RMDs begin at age 72 and the 30-period horizon is implemented and used consistently across sheets and slides.", "weight": 4.0, "judge_prompt": "Verify the following:\n- The model\u2019s RMDs begin in the year the client turns 72 (per the task\u2019s instruction), which should be 2032 given age 65 at end of 2025.\n- The modeling horizon covers Period 0..29 (30 periods) or Years 2025..2054.\n- RMDs are reflected in appropriate sheets with distribution factors posted (IRS 2025 Uniform Lifetime Table reference in the workbook\u2019s methodology).\n- The presentation references an 8-year conversion and long-term comparison that appears consistent with the model\u2019s time horizon.\n\nScoring:\n- 4.0: Clear and correct RMD start year (age 72) and horizon coverage across model and slides.\n- 3.0: Minor ambiguity, but correct start and coverage are apparent.\n- 2.0: Horizon present but unclear RMD start or slight mismatch.\n- 1.0: Significant mismatch in RMD timing or incomplete horizon.\n- 0.0: Missing or incorrect RMD start and no visible horizon alignment.", "expectation": "RMDs begin in 2032 (age 72); model spans 2025\u20132054; slides align."}, {"type": "llm_judge", "name": "Comparative Outcomes and Tax Savings Explanation", "description": "Ensure both scenarios are modeled and the projected tax savings are explained in a logically sound way that aligns to the spreadsheet.", "weight": 4.0, "judge_prompt": "Check that the analysis includes both scenarios and explains outcomes:\n- Scenario A: RMD-only (no conversions).\n- Scenario B: 8-year Roth conversion.\n- The workbook includes a tax summary comparing lifetime/period taxes and shows a projected tax savings (even if approximate) for Scenario B vs A.\n- The presentation explains short-term vs long-term tax tradeoffs, highlights benefits to heirs and estate planning, and references the Excel outputs (charts/tables).\n\nScoring:\n- 4.0: Both scenarios are clearly modeled; tax savings are quantified; the narrative explains tradeoffs and estate benefits, consistent with the model.\n- 3.0: Minor gaps in narrative or quantification, but core comparison is clear.\n- 2.0: Comparison exists but savings not well quantified or inconsistently referenced.\n- 1.0: Scenarios or savings unclear.\n- 0.0: Only one scenario or no coherent comparison.", "expectation": "Clear A/B scenario modeling with quantified projected savings and coherent explanation."}, {"type": "code", "name": "Period Coverage and 8-Year Conversion Count", "description": "Programmatically check that the Excel model covers 2025..2054 (or Period 0..29) and includes exactly 8 distinct years with positive Roth conversions.", "weight": 1.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float in [0,1] or (float, str)\n    \"\"\"\n    # Find the spreadsheet resource\n    outputs = context.get_all_outputs() or []\n    xls = None\n    for r in outputs:\n        try:\n            if getattr(r, 'is_spreadsheet', False):\n                xls = r\n                break\n        except Exception:\n            continue\n    if not xls:\n        return 0.0, \"No spreadsheet output found.\"\n\n    # Get sheet names\n    try:\n        xls_path = context.files.get_path(xls.id)\n        xfile = pd.ExcelFile(xls_path)\n        sheet_names = [s for s in xfile.sheet_names]\n    except Exception as e:\n        return 0.0, f\"Could not open Excel file: {e}\"\n\n    def find_sheet(candidates):\n        lname = [s.lower() for s in sheet_names]\n        for cand in candidates:\n            for i, s in enumerate(lname):\n                if cand in s:\n                    return sheet_names[i]\n        return None\n\n    # Identify likely sheets\n    plan_sheet = find_sheet(['conversion plan', 'roth conversion', 'conversion'])\n    if not plan_sheet:\n        # Fall back to first sheet\n        plan_sheet = sheet_names[0]\n\n    # Read plan sheet\n    try:\n        df = pd.read_excel(xls_path, sheet_name=plan_sheet)\n    except Exception as e:\n        return 0.0, f\"Cannot read conversion plan sheet: {e}\"\n\n    # Normalize columns\n    df_cols = {c: str(c).strip().lower() for c in df.columns}\n    df.columns = list(df_cols.values())\n\n    # Heuristics to find period/year columns\n    period_col = None\n    year_col = None\n    for c in df.columns:\n        if 'period' in c:\n            period_col = c\n        if 'year' == c or 'calendar year' in c or (('year' in c) and ('years' not in c)):\n            year_col = c\n    # Heuristic to find conversion amount column\n    conv_col = None\n    for c in df.columns:\n        if ('conversion' in c) and ('amount' in c or 'amt' in c or 'roth' in c):\n            conv_col = c\n            break\n    if conv_col is None:\n        # try any 'conversion' column numeric\n        cand = [c for c in df.columns if 'conversion' in c]\n        if cand:\n            conv_col = cand[0]\n\n    score = 0.0\n    notes = []\n\n    # Check horizon coverage\n    horizon_ok = False\n    try:\n        if period_col and pd.api.types.is_numeric_dtype(df[period_col]):\n            periods = df[period_col].dropna().astype(float)\n            if (periods.min() <= 0.0 + 1e-6) and (periods.max() >= 29.0 - 1e-6):\n                horizon_ok = True\n        elif year_col and pd.api.types.is_numeric_dtype(df[year_col]):\n            years = df[year_col].dropna().astype(float)\n            if (years.min() <= 2025 + 1e-6) and (years.max() >= 2054 - 1e-6):\n                horizon_ok = True\n        else:\n            # Try infer years from any column with 4-digit values\n            for c in df.columns:\n                s = df[c]\n                if pd.api.types.is_numeric_dtype(s):\n                    vals = s.dropna().astype(float)\n                    if (vals.min() <= 2025 + 1e-6) and (vals.max() >= 2054 - 1e-6):\n                        horizon_ok = True\n                        break\n    except Exception:\n        pass\n\n    if horizon_ok:\n        score += 0.5\n    else:\n        notes.append(\"Horizon 2025..2054 or Period 0..29 not clearly present in conversion plan.\")\n\n    # Check exactly 8 years with positive conversions\n    conv_ok = False\n    try:\n        if conv_col is not None and conv_col in df.columns:\n            conv_vals = pd.to_numeric(df[conv_col], errors='coerce').fillna(0.0)\n            positive_years = int((conv_vals > 0).sum())\n            if positive_years == 8:\n                conv_ok = True\n            elif 6 <= positive_years <= 10:\n                # partial credit if near target\n                score += 0.25\n                notes.append(f\"Found {positive_years} years of conversions (target 8).\")\n        else:\n            notes.append(\"No conversion amount column detected.\")\n    except Exception:\n        pass\n\n    if conv_ok:\n        score += 0.5\n\n    # Cap and return\n    score = max(0.0, min(1.0, score))\n    feedback = \", \".join(notes) if notes else \"OK\"\n    return score, feedback"}, {"type": "code", "name": "RMD Start Year and Factor Plausibility", "description": "Programmatically check that RMDs begin no earlier than the year the client turns 72 (\u22482032) and that at least one distribution factor near age 72 is plausible (\u224825\u201330).", "weight": 1.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Returns float in [0,1] or (float, str)\n    \"\"\"\n    outputs = context.get_all_outputs() or []\n    xls = None\n    for r in outputs:\n        try:\n            if getattr(r, 'is_spreadsheet', False):\n                xls = r\n                break\n        except Exception:\n            continue\n    if not xls:\n        return 0.0, \"No spreadsheet output found.\"\n\n    try:\n        xls_path = context.files.get_path(xls.id)\n        xfile = pd.ExcelFile(xls_path)\n        sheet_names = xfile.sheet_names\n    except Exception as e:\n        return 0.0, f\"Excel open error: {e}\"\n\n    def find_sheet(candidates):\n        lname = [s.lower() for s in sheet_names]\n        for cand in candidates:\n            for i, s in enumerate(lname):\n                if cand in s:\n                    return sheet_names[i]\n        return None\n\n    rmd_sheet = find_sheet(['rmd', 'required minimum', 'distribution'])\n    if not rmd_sheet:\n        return 0.0, \"No RMD sheet found.\"\n\n    try:\n        df = pd.read_excel(xls_path, sheet_name=rmd_sheet)\n    except Exception as e:\n        return 0.0, f\"Cannot read RMD sheet: {e}\"\n\n    # Normalize cols\n    df.columns = [str(c).strip().lower() for c in df.columns]\n\n    # Identify columns\n    year_col = None\n    period_col = None\n    age_col = None\n    rmd_cols = []\n    factor_col = None\n\n    for c in df.columns:\n        if c == 'year' or 'calendar year' in c or (('year' in c) and ('years' not in c)):\n            year_col = c\n        if 'period' in c:\n            period_col = c\n        if 'age' in c:\n            age_col = c\n        if 'rmd' in c or 'required' in c:\n            rmd_cols.append(c)\n        if 'factor' in c or 'divisor' in c or 'distribution period' in c:\n            factor_col = c\n\n    score = 0.0\n    notes = []\n\n    # Check RMD start timing: first non-zero RMD should be year>=2032 or period>=7\n    try:\n        rmd_series = None\n        if rmd_cols:\n            # pick the first numeric RMD column\n            for rc in rmd_cols:\n                s = pd.to_numeric(df[rc], errors='coerce')\n                if s.notna().sum() > 0:\n                    rmd_series = s.fillna(0)\n                    break\n        if rmd_series is None:\n            notes.append(\"No numeric RMD column detected.\")\n        else:\n            idx_first = None\n            nz = rmd_series[rmd_series > 1e-2]\n            if not nz.empty:\n                idx_first = nz.index[0]\n            if idx_first is not None:\n                # get corresponding year/period\n                yr_ok = False\n                if year_col is not None:\n                    try:\n                        yv = pd.to_numeric(df.loc[idx_first, year_col], errors='coerce')\n                        if pd.notna(yv) and yv >= 2032 - 1e-6:\n                            yr_ok = True\n                    except Exception:\n                        pass\n                pr_ok = False\n                if period_col is not None:\n                    try:\n                        pv = pd.to_numeric(df.loc[idx_first, period_col], errors='coerce')\n                        if pd.notna(pv) and pv >= 7 - 1e-6:\n                            pr_ok = True\n                    except Exception:\n                        pass\n                if yr_ok or pr_ok:\n                    score += 0.5\n                else:\n                    notes.append(\"RMDs appear to start before age 72 (before 2032/period 7).\")\n    except Exception:\n        pass\n\n    # Check factor plausibility around age 72\n    try:\n        factor_ok = False\n        if factor_col is not None:\n            factors = pd.to_numeric(df[factor_col], errors='coerce')\n            # Prefer matching age 72 if age column exists\n            if age_col is not None and (72 in set(pd.to_numeric(df[age_col], errors='coerce').dropna().astype(int))):\n                mask72 = pd.to_numeric(df[age_col], errors='coerce') == 72\n                fvals = pd.to_numeric(df.loc[mask72, factor_col], errors='coerce')\n                if fvals.notna().any():\n                    fv = fvals.dropna().iloc[0]\n                    if 25 <= fv <= 30:\n                        factor_ok = True\n            else:\n                # fallback: any factor in plausible 25-30 range\n                if factors.dropna().between(25, 30).any():\n                    factor_ok = True\n        if factor_ok:\n            score += 0.5\n        else:\n            notes.append(\"No plausible age-72 distribution factor (\u224825\u201330) found.\")\n    except Exception:\n        pass\n\n    score = max(0.0, min(1.0, score))\n    feedback = \", \".join(notes) if notes else \"OK\"\n    return score, feedback"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Client-Facing Excellence", "description": "Holistic quality assessment of communication, usability, and strategic value for a client meeting. LLM judges only.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Presentation Clarity and Visual Polish", "description": "Assess whether the 8-slide deck is clear, concise, visually consistent, and suitable for an executive client meeting.", "weight": 2.5, "judge_prompt": "Evaluate the deck for:\n- Clear headlines and logical flow from purpose \u2192 suitability \u2192 process \u2192 timeline \u2192 tax impact \u2192 heirs/estate \u2192 next steps.\n- Visual polish: consistent theme, readable typography, appropriate use of the business digital tunnel style or a comparable professional theme, diagrams/icons where helpful.\n- No clutter; slides are client-friendly and not overly technical.\n\nScoring:\n- 2.5: Professional, clear, and visually polished; excellent client readiness.\n- 1.7: Generally good with minor readability or layout issues.\n- 1.0: Adequate but wordy, visually inconsistent, or hard to follow.\n- 0.0: Poor clarity or formatting; not client-ready.", "expectation": "An executive-quality, visually consistent 8-slide deck that is easy to follow."}, {"type": "llm_judge", "name": "Client-Friendly Messaging and Compliance", "description": "Assesses tone, plain-language explanations, and presence of appropriate disclaimers/caveats.", "weight": 2.5, "judge_prompt": "Assess whether:\n- Explanations use plain language suitable for a high-net-worth client, without jargon overload.\n- Includes appropriate disclaimers (e.g., educational purposes, consult tax advisor, tax laws can change) and notes about modeling assumptions and limitations.\n- Actionable next steps are clear (e.g., tax coordination, conversion cadence, withholding planning, beneficiary updates).\n\nScoring:\n- 2.5: Clear, client-friendly tone; appropriate disclaimers; solid action steps.\n- 1.7: Minor gaps in tone or disclaimers; next steps somewhat light.\n- 1.0: Technical/jargony, or missing key disclaimers/next steps.\n- 0.0: Not client-appropriate; no disclaimers.", "expectation": "Plain-language with proper caveats and actionable next steps."}, {"type": "llm_judge", "name": "Spreadsheet Usability and Transparency", "description": "Evaluate if the Excel file is easy to audit: inputs are separated, labeled, and methodology is documented.", "weight": 2.5, "judge_prompt": "Evaluate the Excel workbook for:\n- Clear separation of Inputs/Assumptions vs. Outputs/Results.\n- Labels, units, and headers are consistent; period/year mapping is unambiguous.\n- Methodology & Sources sheet contains at least 5 sentences, cites the IRS 2025 Uniform Lifetime Table, and explains calculation logic.\n- Ease of tracing: can a reviewer find where conversion amounts, RMDs, taxes, and balances are computed without ambiguity?\n\nScoring:\n- 2.5: Highly transparent; inputs isolated; documentation thorough.\n- 1.7: Mostly clear with small gaps.\n- 1.0: Usable but hard to audit; documentation thin.\n- 0.0: Unclear model structure; little/no documentation.", "expectation": "Auditable model with clear inputs, labeled outputs, and a thorough methodology sheet."}, {"type": "llm_judge", "name": "Strategic Estate Planning Emphasis", "description": "Assesses whether the materials compellingly explain long-term, heir-focused benefits and estate exposure reduction.", "weight": 2.5, "judge_prompt": "Assess whether the deck and workbook:\n- Emphasize tax-free growth in Roth and benefits to heirs (e.g., tax-free distributions to beneficiaries).\n- Address estate planning angles (e.g., reducing taxable estate exposure, beneficiary designations, legacy goals) at a high level.\n- Connect the 8-year conversion strategy to long-term after-tax wealth outcomes supported by the workbook\u2019s comparisons.\n\nScoring:\n- 2.5: Strong, well-supported estate/heir emphasis integrated throughout.\n- 1.7: Present but not deeply integrated.\n- 1.0: Light mention without clear linkage to the analysis.\n- 0.0: Not addressed.\n", "expectation": "Clear articulation of heir-focused, tax-free distribution benefits and estate exposure considerations."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "76418a2c-a3c0-4894-b89d-2493369135d9", "rubric": {"category_name": "Manufacturing \u2022 Shipping, Receiving, and Inventory Clerks \u2014 Shipment Method Selection and Savings Manifest", "rationale": "This rubric enforces a self-documenting analytical workflow. Stage 1 (LLM-only) strictly mandates a verifiable Excel shape so later checks are trivial. Stage 2 blends light deterministic code checks (bounds and arithmetic) with heavier LLM verification (logic integrity and cross-sheet consistency). Stage 3 assesses professional quality and sales usefulness. Code rules are deliberately lower weight than LLM rules per guidance.", "max_total_score": 24.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (STRUCTURE ONLY)", "description": "Output must be a fully-structured Excel manifest enabling verification of shipment method selection by weight and savings vs industry average.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 2.8, "rules": [{"type": "llm_judge", "name": "Structured Excel Manifest with Required Sheets and Tables", "description": "Check the candidate produced a properly structured Excel workbook with the required sheets and table structures. Do not check calculation correctness\u2014only presence/format.", "weight": 4.0, "judge_prompt": "You are evaluating whether the submitted OUTPUT is a valid Excel workbook with the exact verifiable structure needed. Be flexible with sheet/section names that are clearly equivalent.\n\nFormat requirements:\n- File must be an Excel spreadsheet (.xlsx) with at least 3 distinct worksheets.\n\nRequired worksheets and structures (naming can be flexible but must be clearly equivalent):\n1) Main Detail: \"Daily Shipment Manifest\" (or similar: Manifest, Shipment Detail, Daily Shipments)\n   - A visible table with clear column headers. Must include columns (name variants acceptable):\n     \u2022 Pick Ticket / Order ID (e.g., Pick Ticket, Order, Order ID)\n     \u2022 Customer Name (e.g., Customer, Account)\n     \u2022 Destination (e.g., Ship-To City/State or City, State, ZIP)\n     \u2022 Weight (lbs)\n     \u2022 Shipping Method (e.g., Method, Mode, Service Level)\n     \u2022 Carrier (e.g., Carrier, SCAC)\n     \u2022 Actual Shipping Cost (USD) (e.g., Actual Cost, Actual)\n     \u2022 Industry Average Cost (USD) (e.g., Industry Avg, Benchmark, Average Cost)\n     \u2022 Savings (USD) (e.g., Savings, Delta)\n     \u2022 Shipment Date (e.g., Date Shipped)\n     \u2022 Tracking/PRO Number (e.g., Tracking, PRO)\n   - Rows populated for each order.\n\n2) Parameters: \"Shipping Parameters\" (or similar: Parameters, Rate Table, Method Rules)\n   - A table documenting the selection logic by weight including, at minimum:\n     \u2022 Weight range columns (e.g., Min lbs / Max lbs, or a Weight Range text column like \"0\u2013150\")\n     \u2022 Recommended Method (e.g., Parcel/Small Package, LTL, FTL/Truckload)\n     \u2022 Optional: Default/Preferred Carrier and any notes or rate basis\n\n3) Logic/Calculation Log: \"Method Selection Log\" (or similar: Calculation Log, Methodology, Selection Rationale)\n   - A per-order or per-scenario explanation showing how weight mapped to a bracket and why the Shipping Method was chosen. Can be a table or a clear step log; must reference weight brackets from the Parameters sheet.\n\n4) Summary: \"Summary\" (or similar: Totals, Executive Summary, Results)\n   - Clearly shows totals/aggregates including at least:\n     \u2022 Total number of shipments\n     \u2022 Total weight\n     \u2022 Total Actual Cost vs Total Industry Average Cost\n     \u2022 Total Savings\n     \u2022 Savings grouped by Shipping Method (e.g., Parcel vs LTL vs FTL)\n\nScoring (structure only):\n- 4.0: All 4 sheets present with the specified structures and required columns/sections visible.\n- 3.0: Exactly one required sheet missing OR the Main Detail sheet is present but missing up to two required columns.\n- 2.0: Two required sheets missing OR Main Detail missing three or more required columns.\n- 0.0: Not an Excel file or structure is largely missing/incomplete.\n\nDo NOT judge the correctness of calculations or whether numbers add up\u2014only the presence of the required structure that enables verification.", "expectation": "A well-structured Excel with Main Manifest, Shipping Parameters, Method Selection Log, and Summary sheets, each with clear tables and headers."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness and Consistency)", "description": "Now that the structure is present, verify correctness with light code checks and deeper LLM reasoning. Code rules carry lower weight; LLM rules carry higher weight.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Numeric Validity and Savings Arithmetic Check", "description": "Verify main manifest has numeric weight/costs, reasonable bounds, and Savings \u2248 (Industry Avg \u2013 Actual) within tolerance on most rows.", "weight": 0.75, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n\n        # Load workbook\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n\n        # Helper: find manifest-like sheet by matching header synonyms\n        def header_score(cols):\n            cols_l = [str(c).strip().lower() for c in cols]\n            patterns = {\n                'order': ['order', 'pick', 'ticket', 'order id', 'pick ticket'],\n                'customer': ['customer', 'account', 'buyer'],\n                'destination': ['destination', 'ship-to', 'ship to', 'city', 'state', 'zip'],\n                'weight': ['weight', 'lbs', 'weight (lbs)'],\n                'method': ['method', 'mode', 'service level', 'shipment method', 'ship method'],\n                'carrier': ['carrier', 'scac'],\n                'actual': ['actual shipping cost', 'actual cost', 'actual'],\n                'avg': ['industry average cost', 'industry avg', 'average cost', 'benchmark', 'avg'],\n                'savings': ['savings', 'saved', 'delta'],\n                'date': ['shipment date', 'date shipped', 'date'],\n                'tracking': ['tracking', 'pro', 'tracking/pro', 'tracking number', 'pro number'],\n            }\n            score = 0\n            for pats in patterns.values():\n                if any(any(p in c for p in pats) for c in cols_l):\n                    score += 1\n            return score\n\n        best_sheet = None\n        best_score = -1\n        dfs = {}\n        for s in xls.sheet_names:\n            try:\n                df = pd.read_excel(path, sheet_name=s)\n                dfs[s] = df\n                sc = header_score(df.columns)\n                if sc > best_score:\n                    best_sheet, best_score = s, sc\n            except Exception:\n                continue\n\n        if best_sheet is None or best_score < 5:\n            return 0.0, \"No manifest-like sheet with sufficient headers found.\"\n\n        dfm = dfs[best_sheet].copy()\n        dfm.columns = [str(c).strip().lower() for c in dfm.columns]\n\n        # Helper to find best-matching column by synonyms\n        def find_col(syns):\n            for s in syns:\n                for c in dfm.columns:\n                    if s in c:\n                        return c\n            return None\n\n        weight_col = find_col(['weight', 'lbs'])\n        actual_col = find_col(['actual shipping cost', 'actual cost', 'actual'])\n        avg_col = find_col(['industry average cost', 'industry avg', 'average cost', 'benchmark', 'avg'])\n        savings_col = find_col(['savings', 'saved', 'delta'])\n\n        # If key columns missing, cannot verify arithmetic\n        key_missing = [k for k,v in {\n            'weight': weight_col,\n            'actual': actual_col,\n            'avg': avg_col,\n            'savings': savings_col\n        }.items() if v is None]\n        if key_missing:\n            return 0.2, f\"Key columns missing: {', '.join(key_missing)}\"\n\n        # Coerce numerics\n        for c in [weight_col, actual_col, avg_col, savings_col]:\n            dfm[c] = pd.to_numeric(dfm[c], errors='coerce')\n\n        # Reasonable bounds and non-negativity for costs; weight in [0, 50000]\n        valid_weight = dfm[weight_col].between(0, 50000)\n        valid_actual = dfm[actual_col] >= 0\n        valid_avg = dfm[avg_col] >= 0\n\n        n = len(dfm)\n        if n == 0:\n            return 0.1, \"Empty manifest.\"\n\n        frac_bounds = np.mean(valid_weight.fillna(False) & valid_actual.fillna(False) & valid_avg.fillna(False))\n\n        # Savings arithmetic check: avg - actual \u2248 savings (tolerance 0.02)\n        tri_mask = dfm[[avg_col, actual_col, savings_col]].notna().all(axis=1)\n        if tri_mask.any():\n            diff = (dfm.loc[tri_mask, avg_col] - dfm.loc[tri_mask, actual_col]) - dfm.loc[tri_mask, savings_col]\n            tol = 0.02\n            ok = (diff.abs() <= tol)\n            frac_ok = ok.mean() if len(ok) else 0.0\n        else:\n            frac_ok = 0.0\n\n        # Combine: arithmetic accuracy (60%) and numeric/bounds validity (40%)\n        score = 0.6 * frac_ok + 0.4 * frac_bounds\n        feedback = f\"Manifest sheet '{best_sheet}': arithmetic OK on {frac_ok:.0%} rows; bounds valid on {frac_bounds:.0%} rows.\"\n        return float(score), feedback\n    except Exception as e:\n        return 0.0, f\"Error during check: {e}\""}, {"type": "code", "name": "Method Selection Compliance with Weight Parameters", "description": "Check that Shipping Method choices align with weight-based brackets in the Shipping Parameters sheet (flexible parsing of ranges and method names).", "weight": 0.75, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n\n        # Load all sheets\n        dfs = {s: pd.read_excel(path, sheet_name=s) for s in xls.sheet_names}\n\n        # Find manifest sheet\n        def header_score(cols):\n            cols_l = [str(c).strip().lower() for c in cols]\n            pats = ['weight', 'method', 'mode', 'service', 'actual', 'avg', 'savings', 'carrier']\n            return sum(any(p in c for p in pats) for c in cols_l)\n\n        manifest = None\n        for s, df in dfs.items():\n            try:\n                if header_score(df.columns) >= 4:\n                    manifest = (s, df)\n                    break\n            except Exception:\n                continue\n        if manifest is None:\n            return 0.0, \"No manifest-like sheet found.\"\n        m_name, m_df = manifest\n        m_df.columns = [str(c).strip().lower() for c in m_df.columns]\n\n        # Find parameters sheet by columns or name\n        params = None\n        for s, df in dfs.items():\n            cols_l = [str(c).strip().lower() for c in df.columns]\n            name_l = str(s).lower()\n            clues = sum(any(k in c for k in ['weight', 'lbs', 'min', 'max', 'range', 'method', 'mode', 'service']) for c in cols_l)\n            if ('param' in name_l or 'rate' in name_l or 'rule' in name_l or 'logic' in name_l) and clues >= 2:\n                params = (s, df)\n                break\n            if clues >= 3 and ('weight' in ' '.join(cols_l) and any(k in ' '.join(cols_l) for k in ['method','mode','service'])):\n                params = (s, df)\n                break\n        if params is None:\n            return 0.1, \"No parameters-like sheet found.\"\n        p_name, p_df = params\n        p_df = p_df.dropna(how='all').copy()\n        p_df.columns = [str(c).strip().lower() for c in p_df.columns]\n\n        # Identify columns for weight ranges and method\n        def find_col(df, syns):\n            for s in syns:\n                for c in df.columns:\n                    if s in c:\n                        return c\n            return None\n\n        method_col = find_col(p_df, ['method', 'mode', 'service'])\n        # Weight range handling: min/max or a single range column\n        min_col = find_col(p_df, ['min', 'from', 'start'])\n        max_col = find_col(p_df, ['max', 'to', 'end'])\n        range_col = find_col(p_df, ['range', 'weight', 'lbs']) if not (min_col and max_col) else None\n\n        brackets = []\n        if min_col and max_col and method_col:\n            p_df[min_col] = pd.to_numeric(p_df[min_col], errors='coerce')\n            p_df[max_col] = pd.to_numeric(p_df[max_col], errors='coerce')\n            for _, r in p_df.dropna(subset=[min_col, max_col, method_col]).iterrows():\n                brackets.append((float(r[min_col]), float(r[max_col]), str(r[method_col]).lower()))\n        elif range_col and method_col:\n            for _, r in p_df.dropna(subset=[range_col, method_col]).iterrows():\n                rng = str(r[range_col]).lower()\n                wmin, wmax = None, None\n                # Parse patterns like \"0-150\", \"<=150\", \">10000\", \"150 to 10000\"\n                m = re.findall(r\"[0-9,.]+\", rng.replace(',', ''))\n                if '-' in rng or 'to' in rng:\n                    if len(m) >= 2:\n                        wmin, wmax = float(m[0]), float(m[1])\n                elif '>=' in rng or '>' in rng:\n                    if len(m) >= 1:\n                        wmin, wmax = float(m[0]), float('inf')\n                elif '<=' in rng or '<' in rng:\n                    if len(m) >= 1:\n                        wmin, wmax = 0.0, float(m[0])\n                if wmin is not None and wmax is not None:\n                    brackets.append((wmin, wmax, str(r[method_col]).lower()))\n        else:\n            return 0.2, \"Parameters sheet found but weight range/method columns not clear.\"\n\n        if not brackets:\n            return 0.2, \"No parsable weight brackets discovered in parameters.\"\n\n        # Normalize method to categories\n        def norm_method(s):\n            s = (s or '').lower()\n            if any(k in s for k in ['ltl', 'less than truck']):\n                return 'ltl'\n            if any(k in s for k in ['ftl', 'truckload', 'full truck']):\n                return 'ftl'\n            if any(k in s for k in ['parcel', 'small', 'ups', 'fedex', 'ground', 'courier']):\n                return 'parcel'\n            return s.strip()\n\n        norm_brackets = [(a, b, norm_method(m)) for a, b, m in brackets]\n\n        # Identify columns in manifest\n        m_weight = None\n        for c in m_df.columns:\n            if 'weight' in c or 'lbs' in c:\n                m_weight = c\n                break\n        m_method = None\n        for c in m_df.columns:\n            if any(k in c for k in ['method', 'mode', 'service']):\n                m_method = c\n                break\n        if not m_weight or not m_method:\n            return 0.2, \"Manifest missing weight/method columns.\"\n\n        m_df[m_weight] = pd.to_numeric(m_df[m_weight], errors='coerce')\n\n        # Compare each row\n        eligible = 0\n        matches = 0\n        for _, r in m_df.iterrows():\n            w = r[m_weight]\n            meth = norm_method(str(r[m_method]))\n            if pd.isna(w) or meth == '':\n                continue\n            eligible += 1\n            # Find first bracket containing weight\n            br = None\n            for a,b,m in norm_brackets:\n                hi = b if np.isfinite(b) else 1e12\n                if w >= a and w <= hi:\n                    br = (a,b,m)\n                    break\n            if br is None:\n                continue\n            if meth == br[2]:\n                matches += 1\n        if eligible == 0:\n            return 0.2, \"No eligible rows to compare.\"\n\n        frac = matches / eligible\n        return float(frac), f\"Method matches parameters on {matches}/{eligible} rows ({frac:.0%}).\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "llm_judge", "name": "Method Selection Logic Coherence", "description": "Check that the Method Selection/Calculation Log clearly shows how each order\u2019s weight maps into a parameter bracket and justifies the chosen method.", "weight": 3.5, "judge_prompt": "Review the workbook\u2019s Method Selection Log (or Calculation Log/Methodology sheet). Assess whether it:\n- Explicitly references the weight brackets from the Shipping Parameters sheet.\n- Shows, for a sample of individual orders, the weight, matched bracket, and the resulting method choice.\n- Addresses edge cases (e.g., boundary weights between brackets) with clear rationale.\nScoring:\n- 3.5: Clear, consistent mapping for multiple orders; bracket references are explicit; edge cases addressed.\n- 2.5: Mostly clear with minor gaps; mapping visible but not always explicit; limited edge case treatment.\n- 1.0: Vague mapping; difficult to trace from parameters to method choices.\n- 0.0: No usable method selection rationale visible.", "expectation": "Traceable, per-order mapping from weight to bracket to method, with clear justifications."}, {"type": "llm_judge", "name": "Savings Computation Reasonableness", "description": "Spot-check whether Savings equal (Industry Average \u2212 Actual) on visible rows and are not obviously inconsistent.", "weight": 3.5, "judge_prompt": "Inspect the Main Manifest detail table. For a reasonable sample of rows, check whether Savings \u2248 (Industry Average Cost \u2212 Actual Cost). Minor rounding differences are acceptable; large discrepancies are not.\nScoring:\n- 3.5: Savings consistently match (Avg \u2212 Actual) with only rounding noise.\n- 2.5: Mostly correct with occasional small errors.\n- 1.0: Frequent mismatches or obvious arithmetic mistakes.\n- 0.0: Savings column appears unrelated to (Avg \u2212 Actual).", "expectation": "Savings values closely align to the difference between average and actual costs across rows."}, {"type": "llm_judge", "name": "Summary Integrity and Cross-Sheet Consistency", "description": "Verify that Summary totals agree with the detail and that savings by method are presented clearly.", "weight": 3.5, "judge_prompt": "Review the Summary sheet and compare to the detail in the Main Manifest:\n- Do total Actual, total Industry Average, and total Savings match the sum of the detail rows (allow minor rounding)?\n- Is there a breakdown of savings by Shipping Method (e.g., Parcel vs LTL vs FTL)?\n- Are shipment counts and total weight plausible and consistent with the detail?\nScoring:\n- 3.5: Summary totals align with detail; method-level savings breakdown present; counts/weight consistent.\n- 2.5: Mostly aligned with minor gaps (e.g., missing one breakdown or small inconsistencies).\n- 1.0: Noticeable inconsistencies or missing key summary elements.\n- 0.0: Summary does not reflect detail or is missing.", "expectation": "Summary reconciles to detail with a clear savings-by-method breakdown."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality Assessment (Professional Usefulness)", "description": "Holistic evaluation of presentation quality, clarity, and sales-facing usefulness. LLM-only qualitative checks.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Presentation and Usability", "description": "Professional formatting, clear labels, and ease of use for operations and sales.", "weight": 2.0, "judge_prompt": "Assess spreadsheet usability:\n- Clear headers, units (e.g., lbs, USD), and frozen header rows where appropriate.\n- Consistent number formats (currency for costs, integer/decimal for weights).\n- Legible layout with no clutter; tables properly bounded and labeled.\nScoring: 2=Excellent formatting and usability; 1=Acceptable with minor issues; 0=Poorly formatted/hard to use.", "expectation": "Clean, professional layout with appropriate number formats and labeled units."}, {"type": "llm_judge", "name": "Sales-Facing Value and Narrative", "description": "Does the workbook communicate the value to customers (choosing least expensive viable method and passing savings)?", "weight": 2.0, "judge_prompt": "Determine whether the deliverable clearly communicates customer value:\n- A concise narrative or notes indicating that the least expensive viable method was chosen.\n- Clear highlight of total savings and per-method savings helpful for Sales.\n- Optional: a brief note on how customers benefit (e.g., transparency, cost savings).\nScoring: 2=Strong, clear narrative and highlights; 1=Some value signals but limited clarity; 0=No evident sales-facing value messaging.", "expectation": "A succinct, sales-usable summary of savings and method choices."}, {"type": "llm_judge", "name": "Methodology Transparency and Reproducibility", "description": "Assumptions and sources are documented so another clerk could reproduce the process.", "weight": 2.0, "judge_prompt": "Evaluate whether assumptions and sources are documented:\n- Parameters sheet cites source (e.g., TMS screen, date/time captured).\n- Methodology/Log explains how to update parameters and re-run.\n- Any assumptions (e.g., parcel vs LTL thresholds) are stated.\nScoring: 2=Fully transparent with sources/assumptions; 1=Partially documented; 0=Opaque methodology.", "expectation": "Clear, reproducible methodology with sources and assumptions."}, {"type": "llm_judge", "name": "Risk/Exceptions and Practical Guidance", "description": "Notes on edge cases (oversize, hazmat, residential surcharges), and what to do in exceptions.", "weight": 2.0, "judge_prompt": "Check whether the workbook addresses practical exceptions:\n- Mentions common surcharges/constraints (oversize, hazmat, residential, liftgate) that may affect method selection.\n- Provides brief guidance for exceptions or when to escalate.\nScoring: 2=Thoughtful treatment of exceptions and guidance; 1=Mentions some but limited guidance; 0=No exceptions considered.", "expectation": "Acknowledges real-world exceptions with brief, actionable guidance."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "f841ddcf-2a28-4f6d-bac3-61b607219d3e", "rubric": {"category_name": "Wholesale Trade \u2014 Order Clerks: June 2025 Shipment Recap (PO Log)", "rationale": "Analytical Pattern A. The rubric forces an Excel workbook with verifiable, filterable summary tables and PO-level detail that enables deterministic checks. Stage 1 is a hard LLM gate mandating exact structure. Stage 2 mixes lightweight code checks (math/date/unit validation) with heavier LLM consistency verification. Stage 3 assesses professional quality and usability for account managers.", "max_total_score": 24.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement (GATE)", "description": "LLM-only verification that the output is a properly structured Excel workbook with mandated sheets, tables, and narrative required to enable automated verification.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Workbook and Sheet Structure Requirement", "description": "Verify the candidate output is a valid Excel workbook (.xlsx) with all required sheets present and appropriately named (flexible matching allowed).", "weight": 3.0, "judge_prompt": "You are validating STRUCTURE ONLY (not correctness of calculations). Check the candidate output (rendered Excel) for the following:\n\nFormat:\n- Must be an Excel workbook (.xlsx). Not PDF, not CSV, not DOCX.\n\nRequired sheets (names can vary slightly; accept close variants):\n1) Summary \u2014 June Shipments (e.g., \"Summary - June Shipments\", \"June Shipment Summary\")\n2) Detail \u2014 June Shipments (PO-level rows)\n3) Summary \u2014 June Window Shipped in July (or similar wording like \"June Window \u2192 Shipped in July\")\n4) Detail \u2014 June Window Shipped in July (PO-level rows)\n5) Notes & Methodology (text section)\n\nFilterability:\n- The two Summary sheets must be simple tables with an Account column and appear filterable (Excel Table with filter dropdowns). Flexibly judge based on visual cues (header row with filter icons or Excel Table styling).\n\nScoring:\n- 3.0: Excel workbook + all 5 sheets present and tables appear filterable by Account\n- 2.0: Excel workbook + 4 of 5 sheets present OR minor naming variances but clearly intended structure; tables appear filterable\n- 1.0: Excel workbook + only 3 of 5 sheets present OR filterability unclear\n- 0.0: Not an Excel workbook OR missing multiple required sheets\n\nOnly evaluate presence/structure, not the data correctness.", "expectation": "An .xlsx file with the 5-sheet structure enabling verification and filterable summary tables."}, {"type": "llm_judge", "name": "Required Columns in Detail Tabs", "description": "Verify each Detail sheet contains the required columns enabling verification of dates and amounts.", "weight": 3.0, "judge_prompt": "Check the two PO-level Detail sheets contain these columns (flexible header matching allowed; synonyms OK). Do not verify numbers, only presence/shape.\n\nDetail \u2014 June Shipments must include:\n- Account (e.g., Account, Account Name, Customer)\n- PO Number (e.g., PO, PO #, Purchase Order)\n- Start Ship Date\n- Cancel Date\n- Actual Ship Date\n- PO Value at Cost (ordered cost value)\n- Actual Shipped Value at Cost (shipped cost value)\n- Percent Shipped (can be % or decimal)\n- Short-Ship $ (dollar amount not shipped)\n\nDetail \u2014 June Window Shipped in July must include:\n- Account\n- PO Number\n- Start Ship Date\n- Cancel Date\n- Actual Ship Date\n- PO Value at Cost\n\nScoring:\n- 3.0: All required columns present in both Detail sheets\n- 2.0: Missing up to 1 required column across both sheets\n- 1.0: Missing 2\u20133 required columns across both sheets\n- 0.0: Missing more than 3 required columns or wrong format", "expectation": "Both detail tabs have all listed columns with clear headers enabling automated checks."}, {"type": "llm_judge", "name": "Summary Tabs and Notes Presence", "description": "Verify that both Summary sheets and the Notes sheet contain the required content and layout for later verification.", "weight": 2.0, "judge_prompt": "Check high-level content in summary and notes (structure only):\n\nSummary \u2014 June Shipments should have:\n- A per-account table including: Account, Total June Shipped $ (cost), Percent Shipped, Short-Ship $\n- A visible grand total row or field for June shipped $ (cost)\n\nSummary \u2014 June Window Shipped in July should have:\n- A per-account table quantifying the value (at cost) of POs with a June ship window that actually shipped in July\n- A visible grand total for that June-window-to-July value\n\nNotes & Methodology sheet should include:\n- A few sentences that explicitly state: (a) total June shipped value at cost; (b) impact from POs expected in June but shipped in July\n\nScoring:\n- 2.0: Both Summary sheets meet the above and Notes sheet includes the two requested statements\n- 1.0: Minor omissions in one Summary or Notes lacks one requested statement\n- 0.0: Missing core elements in summaries or Notes absent\n\nDo not judge calculation correctness here; only presence/clarity of needed fields.", "expectation": "Summaries show per-account metrics and grand totals; Notes narrates June total and impact from delayed POs."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness Verification", "description": "Mixed code + LLM checks for date logic, math accuracy, and cross-sheet consistency between detail, summaries, and notes.", "is_required": true, "max_points": 10.0, "min_score_to_pass": 5.0, "rules": [{"type": "code", "name": "June Detail: Date and Math Validations", "description": "Validate that June Detail rows have Actual Ship Dates within June 2025 and that Percent Shipped and Short-Ship $ align with PO Value and Actual Shipped Value.", "weight": 1.0, "code": "import pandas as pd\nimport numpy as np\nimport re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output\"\n\n    # Helpers\n    def sheet_names(path):\n        try:\n            x = pd.ExcelFile(path)\n            return x.sheet_names\n        except Exception:\n            return []\n\n    def find_sheet_by_keywords(names, must_include):\n        mi = [m.lower() for m in must_include]\n        for n in names:\n            nl = n.lower()\n            if all(k in nl for k in mi):\n                return n\n        return None\n\n    def find_col(df, candidates):\n        cols = [str(c).strip().lower() for c in df.columns]\n        for i, c in enumerate(cols):\n            for cand in candidates:\n                if cand in c:\n                    return df.columns[i]\n        return None\n\n    path = context.files.get_path(output.id)\n    names = sheet_names(path)\n    if not names:\n        return 0.0, \"Cannot read sheet names\"\n\n    # Try to locate the June Detail sheet\n    june_detail = (\n        find_sheet_by_keywords(names, [\"detail\", \"june\", \"ship\"]) or\n        find_sheet_by_keywords(names, [\"detail\", \"june\", \"shipment\"]) or\n        find_sheet_by_keywords(names, [\"detail\", \"june\"])  # fallback\n    )\n    if not june_detail:\n        return 0.0, \"June detail sheet not found\"\n\n    try:\n        df = pd.read_excel(path, sheet_name=june_detail)\n    except Exception:\n        return 0.0, \"Failed to read June detail sheet\"\n\n    # Required columns (flexible matching)\n    col_account = find_col(df, [\"account\", \"account name\", \"customer\"])\n    col_po = find_col(df, [\"po #\", \"po#\", \"po \", \"purchase order\", \"po number\", \"po\"])  # order matters\n    col_start = find_col(df, [\"start ship\", \"start date\", \"ship start\"])\n    col_cancel = find_col(df, [\"cancel date\", \"cancel\"])\n    col_actual = find_col(df, [\"actual ship\", \"actual shipped\", \"ship date\"])\n    col_po_val = find_col(df, [\"po value at cost\", \"po cost\", \"order value at cost\", \"ordered cost\", \"po value\"])\n    col_ship_val = find_col(df, [\"actual shipped value at cost\", \"shipped value at cost\", \"actual shipped cost\", \"shipped cost\", \"shipped value\"])\n    col_pct = find_col(df, [\"percent shipped\", \"% shipped\", \"pct shipped\"])    \n    col_short = find_col(df, [\"short-ship\", \"short ship\", \"short$\", \"short $\", \"short-shipped\", \"short shipped\"])\n\n    required = [col_account, col_po, col_start, col_cancel, col_actual, col_po_val, col_ship_val, col_pct, col_short]\n    if any(c is None for c in required):\n        return 0.0, \"Missing required columns in June detail\"\n\n    # Coerce types\n    d_actual = pd.to_datetime(df[col_actual], errors='coerce')\n    d_start = pd.to_datetime(df[col_start], errors='coerce')\n    d_cancel = pd.to_datetime(df[col_cancel], errors='coerce')\n\n    po_val = pd.to_numeric(df[col_po_val], errors='coerce')\n    ship_val = pd.to_numeric(df[col_ship_val], errors='coerce')\n\n    pct = pd.to_numeric(df[col_pct].astype(str).str.replace('%','', regex=False), errors='coerce')\n    # If values look like percentages (>1), convert to decimals\n    if pct.dropna().gt(1).mean() > 0.5:\n        pct = pct / 100.0\n\n    short = pd.to_numeric(df[col_short], errors='coerce')\n\n    n = len(df)\n    if n == 0:\n        return 1.0, \"Empty June detail (treated as passing)\"\n\n    # Check actual ship dates within June 2025\n    june_start = pd.Timestamp(2025,6,1)\n    june_end = pd.Timestamp(2025,6,30,23,59,59)\n    in_june = (d_actual >= june_start) & (d_actual <= june_end)\n    # Rows with actual ship date must be in June (tolerate blanks if PO not yet shipped)\n    has_actual = d_actual.notna()\n    date_ok = ((~has_actual) | in_june).mean()\n\n    # Math checks where values are present\n    valid_rows = (po_val.notna() & ship_val.notna() & (po_val >= 0) & (ship_val >= 0))\n    pct_calc = np.where(po_val.fillna(0) != 0, ship_val/po_val, np.nan)\n    pct_diff = np.abs(pct - pct_calc)\n    pct_ok = pd.Series(np.where(valid_rows, pct_diff <= 0.01, True)).mean()\n\n    # Short ship should be PO Value - Shipped (>=0). Allow tolerance: max($1, 0.5% of PO)\n    tol = np.maximum(1.0, 0.005 * po_val.fillna(0))\n    short_expected = (po_val - ship_val).clip(lower=0)\n    short_diff = np.abs(short - short_expected)\n    short_ok = pd.Series(np.where(valid_rows, short_diff <= tol, True)).mean()\n\n    score = (date_ok + pct_ok + short_ok) / 3.0\n    feedback = f\"DateOK={date_ok:.2f}, PctOK={pct_ok:.2f}, ShortOK={short_ok:.2f}\"\n    return float(score), feedback"}, {"type": "code", "name": "Delayed Detail: Window and Ship Timing Validations", "description": "Validate that Detail \u2014 June Window Shipped in July rows have a June 2025 ship window (Start and Cancel in June) and Actual Ship Dates in July 2025.", "weight": 1.0, "code": "import pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output\"\n\n    def sheet_names(path):\n        try:\n            x = pd.ExcelFile(path)\n            return x.sheet_names\n        except Exception:\n            return []\n\n    def find_sheet_by_keywords(names, must_include):\n        mi = [m.lower() for m in must_include]\n        for n in names:\n            nl = n.lower()\n            if all(k in nl for k in mi):\n                return n\n        return None\n\n    def find_col(df, candidates):\n        cols = [str(c).strip().lower() for c in df.columns]\n        for i, c in enumerate(cols):\n            for cand in candidates:\n                if cand in c:\n                    return df.columns[i]\n        return None\n\n    path = context.files.get_path(output.id)\n    names = sheet_names(path)\n    if not names:\n        return 0.0, \"Cannot read sheet names\"\n\n    delayed_detail = (\n        find_sheet_by_keywords(names, [\"detail\", \"june\", \"july\"]) or\n        find_sheet_by_keywords(names, [\"detail\", \"june\", \"window\"]) or\n        find_sheet_by_keywords(names, [\"detail\", \"june\", \"shipped in july\"]) or\n        find_sheet_by_keywords(names, [\"detail\", \"july\"])  # fallback\n    )\n    if not delayed_detail:\n        return 0.0, \"Delayed detail sheet not found\"\n\n    try:\n        df = pd.read_excel(path, sheet_name=delayed_detail)\n    except Exception:\n        return 0.0, \"Failed to read delayed detail sheet\"\n\n    col_start = find_col(df, [\"start ship\", \"start date\", \"ship start\"])\n    col_cancel = find_col(df, [\"cancel date\", \"cancel\"])\n    col_actual = find_col(df, [\"actual ship\", \"actual shipped\", \"ship date\"])\n\n    if any(c is None for c in [col_start, col_cancel, col_actual]):\n        return 0.0, \"Missing required date columns in delayed detail\"\n\n    d_start = pd.to_datetime(df[col_start], errors='coerce')\n    d_cancel = pd.to_datetime(df[col_cancel], errors='coerce')\n    d_actual = pd.to_datetime(df[col_actual], errors='coerce')\n\n    n = len(df)\n    if n == 0:\n        return 1.0, \"No delayed rows (treated as passing)\"\n\n    june_start = pd.Timestamp(2025,6,1)\n    june_end = pd.Timestamp(2025,6,30,23,59,59)\n    july_start = pd.Timestamp(2025,7,1)\n    july_end = pd.Timestamp(2025,7,31,23,59,59)\n\n    window_ok = (d_start >= june_start) & (d_start <= june_end) & (d_cancel >= june_start) & (d_cancel <= june_end)\n    july_ship = (d_actual >= july_start) & (d_actual <= july_end)\n\n    row_ok = (window_ok & july_ship)\n    score = row_ok.mean()\n    feedback = f\"WindowJulyOK={score:.2f} (rows meeting June window + July actual ship)\"\n    return float(score), feedback"}, {"type": "llm_judge", "name": "Reconciliation and Definitions Adherence", "description": "Check that the June Summary total matches the June Detail aggregation and that June-window-to-July totals reconcile to the delayed Detail, adhering to definitions.", "weight": 4.0, "judge_prompt": "Using the rendered workbook, verify correctness at a high level:\n- June Summary total shipped $ (cost) equals (or is within ~1% rounding tolerance of) the sum of Actual Shipped Value at Cost in the June Detail. Also check a couple of accounts (if visible) to see if per-account totals reconcile.\n- June-window-to-July Summary total equals (or is within ~1%) the aggregated PO Value at Cost for POs listed in the delayed Detail tab (or whatever amount the Summary claims to quantify).\n- Confirm the June Detail appears to include only POs with Actual Ship Dates in June 2025, and the delayed Detail includes POs with Start/Cancel in June 2025 and Actual Ship Dates in July 2025.\n\nScoring:\n- 4.0: All reconciliations look consistent; definitions adhered to; variances \u22641%\n- 2.5: Mostly consistent with minor variances (\u22643%) or small omissions\n- 1.0: Noticeable inconsistencies (>3%) or definition drift, but some parts correct\n- 0.0: Totals clearly do not reconcile or definitions ignored\n\nPrefer observable numeric evidence from the visible tables; ignore formatting.", "expectation": "Totals between summaries and details match within rounding, and date definitions are followed."}, {"type": "llm_judge", "name": "Narrative Consistency with Tables", "description": "Check that the Notes & Methodology statements cite the same June total and delayed impact values visible in the Summary tables and reasonably describe the situation.", "weight": 4.0, "judge_prompt": "Review the Notes & Methodology text:\n- Does it explicitly state the June total shipped $ at cost and the impact (dollar value) of POs expected in June but shipped in July?\n- Do these numbers match the corresponding totals in the two Summary sheets (within rounding)?\n- Is the description consistent with the definitions used (e.g., June = 6/1/25\u20136/30/25; delayed = Start & Cancel in June, Actual Ship in July)?\n\nScoring:\n- 4.0: Clear statements with numbers that match the summaries; definitions restated or clearly implied\n- 2.5: Statements present but partially vague or small mismatches (rounding only)\n- 1.0: Statements present but inconsistent with tables or missing one of the two numbers\n- 0.0: Notes missing or contradict the tables materially", "expectation": "Notes give clear, matching numbers and correctly interpret the June and delayed impacts."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Usability", "description": "LLM assessment of professional polish, usability for account managers, and analytical clarity.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Usability and Filterability", "description": "Assess whether tables are easy to filter by Account, with appropriate header freezing and number/date formatting.", "weight": 1.5, "judge_prompt": "Evaluate practical usability:\n- Are the summary tables formatted as Excel Tables (or otherwise clearly filterable) with visible filter dropdowns on headers?\n- Are header rows frozen or otherwise easy to navigate? (Be flexible if not visible.)\n- Are currency columns formatted as $ with consistent decimals, and dates formatted clearly (MM/DD/YYYY)?\n\nScoring: 1.5 = strong usability; 0.8 = acceptable with minor issues; 0.0 = poor usability", "expectation": "Filterable tables with clean currency/date formats and usable headers."}, {"type": "llm_judge", "name": "Clarity and Labeling", "description": "Assess clarity of sheet names, column headers, totals, and explicit date range labeling for June fiscal month.", "weight": 1.5, "judge_prompt": "Check clarity and labeling:\n- Sheet names are clear and self-explanatory\n- Column headers are unambiguous (e.g., explicitly say \"at cost\", \"% shipped\")\n- Grand totals are clearly labeled and easy to find\n- June fiscal range (6/1/25\u20136/30/25) is stated on Summary and/or Notes\n\nScoring: 1.5 = very clear; 0.8 = mostly clear; 0.0 = unclear", "expectation": "Clear labels, visible totals, and explicit date range references."}, {"type": "llm_judge", "name": "Traceability and Auditability", "description": "Evaluate whether a reviewer can trace summary numbers back to PO-level rows and understand the methodology.", "weight": 1.5, "judge_prompt": "Assess traceability:\n- PO-level Detail tabs allow tracing from account totals to specific POs\n- Formulas/methods for Percent Shipped and Short-Ship $ are evident (e.g., brief note or column formula display)\n- Any assumptions or exclusions are briefly documented in Notes\n\nScoring: 1.5 = highly traceable and documented; 0.8 = partially traceable; 0.0 = opaque", "expectation": "Clear path from summaries to detailed POs, with simple, documented methods."}, {"type": "llm_judge", "name": "Professional Presentation", "description": "Assess overall presentation quality: layout, consistency, and absence of obvious errors.", "weight": 1.5, "judge_prompt": "Evaluate presentation:\n- Consistent fonts and alignment; reasonable column widths\n- Minimal clutter; key figures highlighted appropriately\n- No obvious formula errors (#VALUE!, #DIV/0!) or stray artifacts\n\nScoring: 1.5 = polished; 0.8 = acceptable; 0.0 = unprofessional", "expectation": "Clean, consistent, and error-free Excel presentation."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "476db143-163a-4537-9e21-fe46adad703b", "rubric": {"category_name": "Move-Out Inspection Communication Pack (Qyrevia Property Management)", "rationale": "This rubric enforces a self-documenting, mixed-output deliverable: a resident notification email PDF and an inspection schedule PDF. Stage 1 (LLM-only) mandates precise, checkable structure so later verification is trivial. Stage 2 mixes light code checks (file presence/types and date plausibility) with LLM cross-checks (content compliance and alignment between email and schedule). Stage 3 assesses overall professionalism, clarity, and operational usefulness for a property management context. Code rules in Stage 2 carry substantially less weight than LLM rules, reflecting their narrower scope.", "max_total_score": 25.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate", "description": "MANDATORY STRUCTURE: Candidate must provide exactly the required document shapes enabling verification. LLM-only checks. If this gate fails, the entire category is 0.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Required Files and Structure Present", "description": "Verify the candidate produced TWO distinct PDFs: (1) a resident notification email PDF; (2) an inspection schedule PDF with a tabular layout. Only check presence/structure, not correctness.", "weight": 4.0, "judge_prompt": "You are evaluating whether the submission satisfies strict structural requirements. Only assess presence and format, not content correctness.\n\nRequired deliverables (two separate PDFs):\n1) Resident Notification Email (PDF):\n   - Must be a PDF (not DOCX/Excel/text).\n   - Should look like a professional email rendered as a document.\n   - Must clearly include the following visible, labeled parts:\n     a) A subject line (e.g., begins with \"Subject:\" or an obvious subject header)\n     b) A greeting addressing residents\n     c) A body that references a tentative move-out inspection date of 9/23/25\n     d) Instructions for requesting an alternate inspection date\n     e) Contact information for Qyrevia Property Management (company name, phone/email or both)\n     f) Closing/sign-off (e.g., \"Sincerely,\" with name/title/company)\n\n2) Inspection Schedule (PDF):\n   - Must be a PDF (not DOCX/Excel/text).\n   - Must contain a clearly structured table with headers for ALL of the following columns (exact or very close synonyms acceptable):\n     - \"Unit #\"\n     - \"Resident Name\" (or \"Resident\u2019s Name\")\n     - \"Move-Out Date\"\n     - \"Scheduled Inspection Date\"\n   - Table should have one row per moving-out resident.\n   - Include a visible title/header indicating what the document is (e.g., \"September Move-Out Inspection Schedule\") and preferably the community/property name.\n\nScoring (be flexible with minor naming differences, but strict about core elements):\n- 4.0: Both PDFs present. Email PDF includes subject, greeting, body referencing 9/23/25, rescheduling instructions, contact info, and closing. Schedule PDF contains a table with all four required columns and a document title/header.\n- 3.0: Both PDFs present with minor omissions (e.g., one email element missing like closing OR schedule title/header missing) but core elements exist (email references 9/23/25, schedule has all four columns).\n- 1.6: Only one of the two required PDFs is present OR both present but the schedule is missing 1 of the required columns.\n- 0.0: Missing PDFs, wrong formats (not PDFs), or the schedule table structure is largely absent (2+ required columns missing).\n\nOnly judge presence/format. Do not evaluate calculation accuracy, counts, or content quality.", "expectation": "Two PDFs: a properly structured email notification and a tabular inspection schedule with the specified columns."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification and Correctness", "description": "Now that the shape is correct, verify content consistency and plausibility. Mix light code checks with LLM judgment. Code rules carry significantly less weight than LLM rules.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Detect Two Distinct PDFs and Classify", "description": "Confirm at least two document outputs exist and heuristically classify one as the email and one as the schedule by keyword patterns.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        primary = context.get_primary_output()\n        if not primary:\n            return 0.0, \"No primary output.\"\n        # Gather all outputs\n        try:\n            outputs = context.get_all_outputs()\n        except Exception:\n            outputs = [primary]\n        # Filter documents (PDF/DOCX considered, but prefer PDFs)\n        docs = [r for r in outputs if getattr(r, 'is_document', False)]\n        if len(docs) < 2:\n            return 0.0, f\"Found {len(docs)} document(s), need 2 PDFs.\"\n        email_candidates = []\n        schedule_candidates = []\n        for r in docs:\n            text = \"\"\n            try:\n                # Try PDF first\n                text = context.files.read_pdf_text(r.id)\n                if not text:\n                    # Fallback DOCX\n                    text = context.files.read_docx_text(r.id)\n            except Exception:\n                pass\n            low = (text or \"\").lower()\n            # Heuristic: email should mention subject/greeting and inspection\n            is_email = (\"subject:\" in low or \"dear\" in low) and (\"inspection\" in low or \"move-out\" in low) and (\"qyrevia\" in low)\n            # Heuristic: schedule should have table headers keywords\n            header_hits = sum(k in low for k in [\"unit\", \"resident\", \"move-out\", \"scheduled inspection\"])\n            is_schedule = header_hits >= 3\n            if is_email:\n                email_candidates.append(r)\n            if is_schedule:\n                schedule_candidates.append(r)\n        if email_candidates and schedule_candidates:\n            return 1.0, \"Detected both email and schedule documents.\"\n        if email_candidates or schedule_candidates:\n            return 0.5, \"Detected only one of the two (email or schedule).\"\n        return 0.0, \"Could not classify any document as email or schedule.\"\n    except Exception as e:\n        return 0.0, f\"Error during classification: {e}\""}, {"type": "code", "name": "Default Date Presence/Plausibility in Schedule", "description": "Check that the schedule mentions the default inspection date (9/23/25 or 09/23/2025 or \"September 23, 2025\").", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        primary = context.get_primary_output()\n        if not primary:\n            return 0.0, \"No primary output.\"\n        try:\n            outputs = context.get_all_outputs()\n        except Exception:\n            outputs = [primary]\n        # Identify likely schedule doc by header keywords\n        schedule_texts = []\n        for r in outputs:\n            if not getattr(r, 'is_document', False):\n                continue\n            text = \"\"\n            try:\n                text = context.files.read_pdf_text(r.id)\n                if not text:\n                    text = context.files.read_docx_text(r.id)\n            except Exception:\n                text = \"\"\n            low = (text or \"\").lower()\n            header_hits = sum(k in low for k in [\"unit\", \"resident\", \"move-out\", \"scheduled inspection\"])\n            if header_hits >= 3:\n                schedule_texts.append(low)\n        if not schedule_texts:\n            return 0.0, \"No schedule-like document text found.\"\n        patterns = [r\"\\b9/23/25\\b\", r\"\\b09/23/2025\\b\", r\"september\\s+23,\\s*2025\"]\n        contains_default = any(re.search(p, txt) for p in patterns for txt in schedule_texts)\n        if contains_default:\n            return 1.0, \"Default date found in schedule.\"\n        # Partial credit: any September 2025 dates present\n        sept_2025 = any(re.search(r\"(september|09)\\s*/?\\s*\\d{1,2}\\s*(,\\s*2025|/\\s*2025|/\\s*25)\\b\", txt) for txt in schedule_texts)\n        if sept_2025:\n            return 0.5, \"September 2025 dates present, but default 9/23/25 not detected.\"\n        return 0.0, \"No plausible default or September 2025 inspection dates detected.\"\n    except Exception as e:\n        return 0.0, f\"Error during date plausibility check: {e}\""}, {"type": "llm_judge", "name": "Email Content Compliance", "description": "Check that the email PDF communicates the required info clearly and correctly.", "weight": 4.0, "judge_prompt": "Evaluate the Resident Notification Email PDF only. Confirm these required content elements are explicitly present and coherent:\n- Subject line present and relevant to move-out inspection.\n- Greeting that addresses residents.\n- Clear statement that a final move-out inspection is scheduled/tentatively set for 9/23/25.\n- Instructions for requesting an alternate inspection date (how to contact, by when).\n- Contact information for Qyrevia Property Management (company name plus phone and/or email).\n- Professional closing/signature with name/title/company.\n\nScoring:\n- 4.0: All elements present, clear, and unambiguous.\n- 3.0: One minor element missing or unclear (e.g., missing exact deadline but rescheduling method present).\n- 2.0: Multiple elements missing/unclear but intent still understandable.\n- 1.0: Barely meets requirements; key elements missing or ambiguous.\n- 0.0: Not an email-like document or fails to mention inspection scheduling and 9/23/25.\n\nJudge only the email PDF content; ignore schedule here.", "expectation": "A professional email that explicitly includes the default inspection date, rescheduling instructions, contact info, and a proper sign-off."}, {"type": "llm_judge", "name": "Schedule Table Integrity and Completeness", "description": "Verify the schedule PDF shows the required table with complete headers and usable rows.", "weight": 3.0, "judge_prompt": "Evaluate the Inspection Schedule PDF for structural integrity and completeness:\n- Table with these headers (or close synonyms): Unit #, Resident Name, Move-Out Date, Scheduled Inspection Date.\n- One row per moving-out resident (no obvious missing fields). Rows should be legible and aligned.\n- Dates appear in recognizable formats and are plausible for September 2025.\n- The document includes a title/header (e.g., community name and a report title).\n\nScoring:\n- 3.0: All four headers present, rows appear complete/consistent, dates plausible, and document titled.\n- 2.0: Minor issues (one header phrased differently or slight inconsistencies) but overall complete and usable.\n- 1.0: Significant omissions (missing header or multiple incomplete rows), yet partially usable.\n- 0.0: Lacks the required tabular structure or multiple headers missing.\n\nFocus on the schedule PDF only.", "expectation": "A clean, complete table with the four required columns and usable rows."}, {"type": "llm_judge", "name": "Cross-Document Alignment", "description": "Check consistency between the email and the schedule (default date and property context).", "weight": 3.0, "judge_prompt": "Consider BOTH PDFs together and assess alignment:\n- The email\u2019s default inspection date (9/23/25) matches the predominant scheduled date(s) shown in the schedule (allowing exceptions where residents asked for different dates if noted).\n- The property/community name and company (Qyrevia Property Management) are used consistently across both documents.\n- The call-to-action in the email (how to request a different date) is consistent with what appears feasible/represented in the schedule.\n\nScoring:\n- 3.0: Strong consistency on default date and context; minor acceptable differences only.\n- 2.0: Mostly consistent but with noticeable mismatches or unclear references.\n- 1.0: Multiple inconsistencies (e.g., dates differ significantly; property/company names misaligned).\n- 0.0: Little to no alignment between the documents.\n\nIf external reference files (MOVE_OUT RPT, NOTES) are not present, judge based on internal consistency only.", "expectation": "Email and schedule tell the same story: default 9/23/25, consistent property/company naming, and feasible rescheduling mechanics."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism", "description": "Holistic assessment of communication quality, presentation, and operational usefulness for property management workflows.", "is_required": false, "max_points": 9.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Email Professionalism and Clarity", "description": "Tone, clarity, and suitability of the email for residents.", "weight": 3.0, "judge_prompt": "Assess the email PDF for professional tone, clarity, and appropriateness for residents:\n- Courteous, concise, and free of typos/formatting issues.\n- Clear instructions with action steps and reasonable timelines.\n- Accessible language avoiding jargon; ADA-friendly considerations (readability, logical structure).\n\nScoring:\n- 3.0: Highly professional, clear, and resident-friendly.\n- 2.0: Generally professional with minor clarity or formatting issues.\n- 1.0: Noticeable tone/clarity problems that could confuse residents.\n- 0.0: Unprofessional or confusing communication.", "expectation": "A polished, courteous, and clear resident communication."}, {"type": "llm_judge", "name": "Schedule Readability and Usability", "description": "Evaluate the schedule\u2019s formatting for manager use during end-of-month operations.", "weight": 3.0, "judge_prompt": "Assess the schedule PDF for operational readability:\n- Clean layout with clear headers, consistent date formats, and legible rows.\n- Logical ordering (e.g., by Unit # or by Scheduled Inspection Date) and/or inclusion of helpful meta (report date, page numbers).\n- Minimal visual clutter; easy to scan and use on-site.\n\nScoring:\n- 3.0: Excellent readability and operationally ready.\n- 2.0: Minor formatting issues but still easy to use.\n- 1.0: Several formatting issues hinder quick use.\n- 0.0: Poorly formatted; difficult to use in practice.", "expectation": "A clear, scannable schedule that a manager can use at a glance."}, {"type": "llm_judge", "name": "Operational Completeness and Data Hygiene", "description": "Check for completeness, consistency, and absence of obvious data hygiene issues.", "weight": 3.0, "judge_prompt": "Evaluate overall operational completeness across both PDFs:\n- No obvious omissions (e.g., missing unit numbers or names); duplicates minimized.\n- Consistent naming (property, company, dates) and no sensitive data leaked unnecessarily.\n- For a 98-unit community, the count of scheduled move-outs appears plausible (does not exceed community size; number is explainable by context).\n\nScoring:\n- 3.0: Complete, consistent, and clean.\n- 2.0: Minor issues but overall acceptable.\n- 1.0: Multiple issues that could cause errors or confusion.\n- 0.0: Serious data hygiene problems.", "expectation": "Documents are clean, consistent, and operationally complete without exposing unnecessary sensitive data."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "24d1e93f-9018-45d4-b522-ad89dfd78079", "rubric": {"category_name": "Manufacturing \u2014 Buyers & Purchasing Agents: NPV Vendor Selection (Headlamps)", "rationale": "This rubric enforces a self-documenting, verifiable Excel model for vendor NPV comparison. Stage 1 (LLM-only gate) mandates a precise workbook shape (vendor sheets + summary sheet, explicit assumptions, calculation logs). Stage 2 mixes lightweight code checks (plausibility and structural consistency) with LLM judges for financial logic and cross-sheet consistency. Stage 3 uses LLM judges to assess professional quality, clarity, and decision usefulness.", "max_total_score": 15.0, "stages": [{"name": "Stage 1 \u2014 Structured Output Format Gate", "description": "LLM-only gate: verifies the Excel workbook exists and follows the exact structure required to enable deterministic verification in Stage 2.", "is_required": true, "max_points": 2.0, "min_score_to_pass": 1.4, "rules": [{"type": "llm_judge", "name": "Workbook Structure and Required Sections", "description": "Check the candidate produced an Excel workbook with strictly defined, verification-friendly structure for NPV evaluation.", "weight": 2.0, "judge_prompt": "You are evaluating whether the submitted OUTPUT is an Excel workbook with the exact, verification-friendly structure below. Be flexible on casing and minor naming variations, but the functional sections must be present and readable.\n\nFile format:\n- Must be a spreadsheet (Excel .xlsx preferred). Not PDF, not DOCX, not plain text.\n\nRequired sheets:\n- Three vendor calculation sheets, one per vendor (names can vary slightly, but must clearly map to each vendor):\n  \u2022 Autolantic (e.g., \"Autolantic NPV\", \"Autolantic \u2013 Model I\")\n  \u2022 Vendocrat (e.g., \"Vendocrat NPV\")\n  \u2022 Solimoto (e.g., \"Solimoto NPV\")\n- One final comparison sheet named clearly (e.g., \"Summary\", \"NPV Summary\", \"Vendor Comparison\")\n\nEach vendor sheet must include clearly labeled sections (headers visible) with the following tables/blocks:\n1) Input Assumptions (table) \u2014 with columns similar to [Parameter | Value | Source/Justification]. Must include rows that explicitly cover:\n   - Discounting approach across product years (explicitly showing 10% for years 2\u20134)\n   - 4-year product life (years 1\u20134 clearly indicated somewhere)\n   - 70:30 variant split (Base:Top or equivalent wording)\n   - Tooling amortization over first 100,000 sets (1 set = 2 headlamps) irrespective of variant\n   - R&D paid in Year 1 (upfront) and split equally across variants\n2) Per-Unit Pricing by Variant (table) \u2014 clearly lists Base vs Premium (Top) unit prices used in calculations\n3) Volume Plan (table) \u2014 shows Year, Total Vehicle Sales, Sets (or implied), and split into Base Units and Top Units according to 70:30\n4) NPV Calculation (table) \u2014 shows step-by-step cash outflow logic, with at minimum the columns: [Year | Net Cash Outflow (or similar) | Discount Factor | Present Value]. The final row or cell must show the vendor\u2019s NPV total.\n5) Methodology & Assumptions (text block) \u2014 at least 3 sentences explaining how the model treats unit pricing, tooling amortization, R&D timing/split, discounting years, and any other assumptions.\n\nFinal Summary sheet must include:\n- A comparison table of [Vendor | NPV | Ranking]\n- A recommendation statement naming the nominated vendor and supporting comments (1\u20133 short paragraphs or bullet points) referencing trade-offs (e.g., cost vs. innovation/technology, localization, timeline/risks).\n\nOptional (nice-to-have, not required): Sensitivity analysis varying discount rate.\n\nScoring guidance (evaluate only structure/presence, not correctness):\n- 2.0: Spreadsheet format with all four sheets present (3 vendor sheets + summary). Each vendor sheet includes all 5 required sections/tables. Summary shows comparison table + recommendation text. Sections are recognizable and reasonably complete.\n- 1.6: All sheets present; each vendor sheet has at least Input Assumptions, Per-Unit Pricing, Volume Plan, and NPV Calculation; Methodology text is brief or thin; Summary includes comparison table but recommendation text is minimal.\n- 1.2: One missing required element across vendor sheets (e.g., no Per-Unit Pricing table or no Volume Plan) OR Summary missing either comparison table or recommendation text.\n- 0.8: Multiple missing elements, but it is still clearly an Excel workbook attempting the required structure.\n- 0.0: Not a spreadsheet, or missing multiple vendor sheets, or lacks the core NPV Calculation tables.\n\nOnly check presence/format/structure. Do NOT judge the correctness of numbers here.", "expectation": "A clean Excel workbook having three vendor NPV sheets with assumptions, pricing, volumes, and NPV tables, plus a summary comparison and recommendation."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Consistency Verification", "description": "Verifies key financial logic and consistency now that the structure is present. Mix of lightweight code checks and LLM reasoning. Code rules are bounded/plausibility checks; LLM judges assess nuanced financial correctness and cross-sheet consistency.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 5.5, "rules": [{"type": "code", "name": "Discounting Plausibility (10% applied to Years 2\u20134)", "description": "Detect presence/use of discount factors at approx [Year1=1.00, Year2\u22480.909, Year3\u22480.826, Year4\u22480.751], or at least explicit 10% discount rate mention tied to discounting. Fuzzy across vendor sheets.", "weight": 0.4, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n        sheets = xls.sheet_names\n        vendor_keys = ['autolantic', 'vendocrat', 'solimoto']\n        vendor_sheets = [s for s in sheets if any(k in s.lower() for k in vendor_keys)]\n        if not vendor_sheets:\n            # fallback: consider all sheets except typical summary names\n            vendor_sheets = [s for s in sheets if 'summary' not in s.lower() and 'compare' not in s.lower()]\n            if not vendor_sheets:\n                return 0.0, \"No vendor sheets detected.\"\n\n        def check_sheet(sheet):\n            try:\n                df = pd.read_excel(file_path, sheet_name=sheet, dtype=object)\n            except Exception:\n                return 0.0\n            cols = [str(c).lower() for c in df.columns]\n            # Find year and discount factor columns\n            year_col_idx = None\n            df_col_idx = None\n            for i, c in enumerate(cols):\n                if 'year' in c:\n                    year_col_idx = i\n                if ('discount' in c and ('factor' in c or 'df' in c)) or c.strip() in ['df', 'discount factor']:\n                    df_col_idx = i\n            score = 0.0\n            if year_col_idx is not None and df_col_idx is not None:\n                try:\n                    years = pd.to_numeric(df.iloc[:, year_col_idx], errors='coerce')\n                    factors_raw = pd.to_numeric(df.iloc[:, df_col_idx], errors='coerce')\n                    good = 0\n                    checks = 0\n                    for y in [1,2,3,4]:\n                        idxs = np.where(years == y)[0]\n                        if len(idxs) == 0:\n                            continue\n                        fvals = factors_raw.iloc[idxs]\n                        fvals = fvals[~pd.isna(fvals)]\n                        if len(fvals) == 0:\n                            continue\n                        f = float(fvals.iloc[0])\n                        # handle percentages mistakenly stored as 100x\n                        candidates = [f, f/100.0]\n                        expected = 1.0 if y == 1 else (1.0 / (1.0 + 0.10) ** (y-1))\n                        tol = 0.03\n                        if any(abs(val - expected) <= tol for val in candidates):\n                            good += 1\n                        checks += 1\n                    if checks >= 3 and good >= 2:  # at least 2 matches among 3+ checks\n                        score = 1.0\n                    elif checks >= 1 and good >= 1:\n                        score = 0.6\n                except Exception:\n                    pass\n            if score == 0.0:\n                # Fallback: textual detection of 10% discount mention near discounting\n                try:\n                    text_hits = 0\n                    sample = df.astype(str).apply(lambda s: s.str.lower(), axis=1).values.flatten().tolist()\n                    blob = ' '.join(sample)\n                    if ('discount' in blob or 'disc.' in blob or 'df' in blob) and (('10%' in blob) or ('0.10' in blob) or ('10 %' in blob)):\n                        text_hits = 1\n                    if text_hits:\n                        score = 0.5\n                except Exception:\n                    pass\n            return score\n        sheet_scores = [check_sheet(s) for s in vendor_sheets]\n        if not sheet_scores:\n            return 0.0, \"No sheets evaluated.\"\n        avg = float(np.mean(sheet_scores))\n        return max(0.0, min(1.0, avg)), f\"Avg discounting plausibility across {len(vendor_sheets)} sheets: {avg:.2f}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Tooling Amortization \u2014 100,000 Sets Mention", "description": "Verify workbook explicitly states tooling amortization over first 100,000 sets and references sets (not per lamp), indicating variant-agnostic application.", "weight": 0.4, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n        sheets = xls.sheet_names\n        patterns = {\n            'tool': re.compile(r'tool', re.I),\n            'amort': re.compile(r'amort', re.I),\n            'sets': re.compile(r'\\bset(s)?\\b', re.I),\n            'hundredk': re.compile(r'100\\s*,?\\s*000|100k', re.I)\n        }\n        def sheet_text(sheet):\n            try:\n                df = pd.read_excel(file_path, sheet_name=sheet, dtype=object)\n                text = ' '.join(df.astype(str).apply(lambda s: s.str.lower(), axis=1).values.flatten().tolist())\n                return text\n            except Exception:\n                return ''\n        scores = []\n        for s in sheets:\n            text = sheet_text(s)\n            if not text:\n                continue\n            has_tool = bool(patterns['tool'].search(text))\n            has_amort = bool(patterns['amort'].search(text))\n            has_sets = bool(patterns['sets'].search(text))\n            has_100k = bool(patterns['hundredk'].search(text))\n            if has_tool and has_amort and has_sets and has_100k:\n                scores.append(1.0)\n            elif has_tool and has_100k and (has_amort or has_sets):\n                scores.append(0.6)\n            elif has_tool and has_100k:\n                scores.append(0.4)\n            else:\n                scores.append(0.0)\n        if not scores:\n            return 0.0, \"No readable sheets.\"\n        best = max(scores)  # credit if clearly stated anywhere in the workbook\n        return best, f\"Best tooling amortization mention score: {best:.2f}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "R&D Upfront in Year 1 and Split Equally", "description": "Detect that R&D is paid upfront in Year 1 and split equally across variants (textual mention plus, if possible, Year 1-only cost).", "weight": 0.4, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n        sheets = xls.sheet_names\n        def score_sheet(sheet):\n            try:\n                df = pd.read_excel(file_path, sheet_name=sheet, dtype=object)\n            except Exception:\n                return 0.0\n            text = ' '.join(df.astype(str).apply(lambda s: s.str.lower(), axis=1).values.flatten().tolist())\n            # textual cues\n            has_rd = any(k in text for k in ['r&d', 'r and d', 'research & development', 'research and development'])\n            has_y1 = any(k in text for k in ['year 1', 'yr 1', 'y1'])\n            has_split = any(k in text for k in ['split equally', 'equally split', '50/50', '50 / 50', 'equal split', 'each variant'])\n            base_score = 0.0\n            if has_rd and has_y1 and has_split:\n                base_score = 0.7\n            elif has_rd and has_y1:\n                base_score = 0.5\n            elif has_rd:\n                base_score = 0.3\n            # numeric/year check (optional boost)\n            boost = 0.0\n            try:\n                cols = [str(c).lower() for c in df.columns]\n                year_idx = None\n                for i,c in enumerate(cols):\n                    if 'year' in c:\n                        year_idx = i\n                        break\n                if year_idx is not None:\n                    years = pd.to_numeric(df.iloc[:, year_idx], errors='coerce')\n                    # find any row mentioning r&d\n                    rd_rows = df.apply(lambda row: any(isinstance(v, str) and ('r&d' in v.lower() or 'research' in v.lower()) for v in row), axis=1)\n                    if rd_rows.any():\n                        yvals = years[rd_rows]\n                        yvals = yvals[~pd.isna(yvals)]\n                        if not yvals.empty:\n                            if (yvals == 1).all():\n                                boost = 0.3\n                            elif (yvals == 1).any():\n                                boost = 0.2\n            except Exception:\n                pass\n            return min(1.0, base_score + boost)\n        scores = [score_sheet(s) for s in sheets]\n        if not scores:\n            return 0.0, \"No sheets parsed.\"\n        best = max(scores)\n        return best, f\"Best R&D upfront/split score: {best:.2f}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "llm_judge", "name": "Variant Split and Volume Derivation", "description": "Checks that the 70:30 Base:Top split is applied to the Model I volume plan per year, units per variant sum to total sets, and volumes flow into cash outflow calculations for each vendor.", "weight": 1.6, "judge_prompt": "Evaluate the vendor sheets for correct use of the 70:30 volume split between Base and Top variants:\n- On each vendor sheet, the Volume Plan should show yearly total vehicle sales (or sets) converted into Base and Top units using 70% and 30% respectively.\n- Base + Top units per year should equal total sets per year (1 set per vehicle).\n- These split volumes should feed into annual cash outflow calculations (using per-unit prices by variant) for that vendor.\n\nScore guidance:\n- 1.6: Clear 70:30 split each year; Base+Top = Total; the split volumes clearly drive annual spend calculations on all vendor sheets.\n- 1.2: Split is shown and sums correctly, but link to spend is implicit (not visibly referenced) on one vendor sheet.\n- 0.8: Split appears but has inconsistencies in at least one year/vendor OR totals do not sum exactly but are close.\n- 0.4: Split mentioned in assumptions but not actually applied in the volume table/calculations.\n- 0.0: No evidence of 70:30 application in volumes.\nOnly judge correctness relative to the workbook; do not create new assumptions.", "expectation": "Volumes are consistently split 70:30 and reconcile to total sets; spend lines use these volumes."}, {"type": "llm_judge", "name": "Cash Flow Logic: Unit Pricing, R&D, and Tooling", "description": "Assesses whether each vendor\u2019s annual cash outflows correctly include variant-specific unit pricing, R&D upfront in Year 1 (split equally), and tooling amortized across the first 100,000 sets (variant-agnostic).", "weight": 1.6, "judge_prompt": "Assess each vendor sheet for correct cash flow construction:\n- Variant unit prices (Base vs Top) used to compute annual spend from split volumes.\n- R&D is paid upfront in Year 1 only and split equally between the two variants (or otherwise clearly stated but equivalent treatment) with no inflation.\n- Tooling is amortized over the first 100,000 sets irrespective of variant; this amortized amount should be included in cash outflows until 100,000 sets are consumed.\n\nScore guidance:\n- 1.6: All three components (unit pricing by variant, R&D upfront split, tooling amortization across first 100k sets) are clearly included and correctly timed on all vendors.\n- 1.2: Minor issues on one vendor (e.g., tooling schedule presentation unclear but amount consistent).\n- 0.8: One component is missing or misapplied on at least one vendor.\n- 0.4: Multiple components missing/misapplied.\n- 0.0: Cash flow lacks key components or timing is fundamentally wrong.\nIgnore discounting in this rule (evaluated elsewhere).", "expectation": "Per-vendor cash outflows reflect unit prices by variant, R&D upfront in Y1, and tooling amortization across earliest 100k sets."}, {"type": "llm_judge", "name": "Discounting Correctness and NPV Computation", "description": "Checks that discount factors reflect 10% applied to Years 2\u20134 (Year 1 undiscounted), present values are computed correctly, and each vendor\u2019s NPV total reflects the sum of PVs.", "weight": 1.6, "judge_prompt": "Check the discounting and NPV math:\n- Discounting uses 10% for Years 2\u20134; Year 1 cash flow is undiscounted (DF=1.00). If a Year 0 row exists for immediate outflows, treat it separately but ensure years 2\u20134 use 10%.\n- Present Value = Net Cash Outflow \u00d7 Discount Factor on each row.\n- NPV total equals the sum of present values across the product life for each vendor.\n\nScore guidance:\n- 1.6: All vendors show correct discount factors (1.00, ~0.909, ~0.826, ~0.751) and PV math; NPV totals match PV sums.\n- 1.2: Minor rounding/formatting differences, but logic is correct for all vendors.\n- 0.8: One vendor has a discount or PV arithmetic mistake.\n- 0.4: Multiple vendors have mistakes in discounting or PV math.\n- 0.0: Discounting or NPV is fundamentally incorrect.", "expectation": "NPV mechanics are correct and consistent across vendors."}, {"type": "llm_judge", "name": "Summary Consistency and Recommendation Justification", "description": "Validates that Summary NPVs match per-sheet totals, ranking is correct, and the recommendation is explicitly justified with relevant considerations.", "weight": 1.6, "judge_prompt": "Review the Summary sheet:\n- Do the NPV values for Autolantic, Vendocrat, and Solimoto match the totals shown on their respective vendor sheets?\n- Is the ranking consistent with the NPV values (lowest NPV outflow typically preferred in procurement context)? If a non-lowest NPV vendor is recommended, is the rationale explicitly justified (e.g., innovation, tech features, risk, localization, timeline)?\n- Does the recommendation clearly name the nominated vendor and provide brief supporting comments?\n\nScore guidance:\n- 1.6: NPVs match across sheets; ranking correct; recommendation is explicit and well-justified.\n- 1.2: Small cross-sheet variance or a minor ranking oversight, but recommendation remains logically justified.\n- 0.8: Noticeable mismatch in one vendor OR weak justification.\n- 0.4: Multiple mismatches; recommendation unclear.\n- 0.0: No usable summary or recommendation.", "expectation": "Summary aligns with vendor sheets and presents a clear, justified recommendation."}], "on_failure_action": "skip_remaining", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Decision Usefulness", "description": "Holistic assessment of presentation quality, transparency, and decision usefulness for a Finance Controller/audience.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Model Organization and Readability", "description": "Assesses clarity of sheet naming, section headers, formatting, and ease of navigation for a finance audience.", "weight": 1.5, "judge_prompt": "Evaluate organization and readability:\n- Clear, consistent sheet names (vendor sheets and a single summary sheet).\n- Section headers are visible; tables are neatly laid out with labels, units, and consistent formatting.\n- Easy to follow from assumptions \u2192 volumes \u2192 pricing \u2192 cash flows \u2192 NPV.\n\nScore 0.0\u20131.5 based on professional clarity and ease of audit.", "expectation": "Well-labeled, consistent formatting; straightforward to navigate and review."}, {"type": "llm_judge", "name": "Assumptions Transparency and Traceability", "description": "Checks that key assumptions are explicitly stated with sources and can be traced to calculations.", "weight": 1.5, "judge_prompt": "Assess assumptions transparency:\n- Input Assumptions table cites parameters, values, and sources/justifications (e.g., vendor quotes, program volumes).\n- Explicitly lists discounting treatment (10% Y2\u2013Y4), 70:30 split, tooling amortization over first 100k sets, and R&D Year 1 split equally.\n- Clear linkage from assumptions to calculations.\n\nScore 0.0\u20131.5 based on completeness and traceability.", "expectation": "All key assumptions are explicitly listed with sources and are traceable to the model."}, {"type": "llm_judge", "name": "Auditability and Calculation Transparency", "description": "Evaluates whether a reviewer can quickly audit and validate logic without hidden leaps or opaque hardcoding.", "weight": 1.0, "judge_prompt": "Evaluate auditability:\n- Formulas/steps are visible and labeled (no unexplained hardcoding of repeated values).\n- A short methodology note explains how cash flows are built and how tooling/R&D are treated.\n- Totals and cross-references are easy to verify.\n\nScore 0.0\u20131.0 based on transparency and audit readiness.", "expectation": "Calculation steps are explicit with minimal hardcoding and clear labels."}, {"type": "llm_judge", "name": "Decision Support Quality (Executive Recommendation)", "description": "Assesses whether the summary provides an executive-ready recommendation with relevant risk/benefit context and optional sensitivity.", "weight": 1.0, "judge_prompt": "Assess the decision support:\n- The Summary includes a concise recommendation naming the nominated vendor and explains why (cost/NPV plus qualitative factors like technology, localization, supply risk, timing).\n- Notes key risks/assumptions and, if included, comments on sensitivity (e.g., discount rate or volume changes).\n\nScore 0.0\u20131.0 based on clarity, relevance, and usefulness for executives.", "expectation": "Clear, concise recommendation with relevant context and optional sensitivity commentary."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "b78fd844-db76-448e-a783-5e9877cb74c2", "rubric": {"category_name": "Finance & Insurance \u00b7 Financial Managers \u00b7 Investment Recommendation Dossier", "rationale": "Pattern C (Mixed): The deliverable is a professionally formatted PDF/DOCX report addressed to the Board that embeds structured, directional financial analysis (NPV/IRR), a comparative summary, a clear recommendation, a risk register with mitigations, and an allocation plan for $100M. Stage 1 is a strict LLM-only shape gate that mandates a verifiable document structure enabling downstream checks. Stage 2 mixes small, robust code checks (text extraction for key constraints and directional-language evidence) with higher-weight LLM verification of financial-logical consistency, mitigation specificity, and allocation validity. Stage 3 holistically evaluates executive-level quality, strategic insight, readability, and actionability. Code rules carry substantially less weight than LLM rules to reflect their narrower, deterministic scope.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Shape & Format Gate (LLM-only)", "description": "Gate that enforces exact deliverable shape so verification is possible. Checks PDF/DOCX format (\u226415 pages), audience-appropriate address, and presence of required sections and tables.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.5, "rules": [{"type": "llm_judge", "name": "Document Format and Structural Compliance (Gate)", "description": "Verify that the output is a properly structured PDF/DOCX report addressed to the Board with all required sections and tables.", "weight": 6.0, "judge_prompt": "You are checking ONLY structure and format (not correctness of analysis). Evaluate the candidate output as a rendered document (PDF or DOCX). Be flexible with section/header names that are close in meaning.\n\nRequired format:\n- Must be a PDF or DOCX (not Excel, not plain text)\n- Length must be 15 pages or fewer\n- Addressed to the Board of Directors of Tiny-Rod Hit Inc. (or clear equivalent)\n- Dated January 2025 (or clearly indicates timing as January 2025)\n\nRequired sections and elements (allow equivalent names):\n1) Executive Summary (on or near first page) stating a preliminary recommendation.\n2) Two Project Overviews (one per project) each including:\n   - Assumptions (bullets or brief text)\n   - Directional Financials table with columns similar to: [Metric | Project | Directional Estimate | Rationale]. Metrics must include NPV (directional sign or range) and IRR (range) and initial investment.\n3) Comparative Analysis section including a side-by-side table (e.g., \"Side-by-Side Comparison\") with columns similar to: [Criterion | Project 1 | Project 2 | Notes]. Criteria should include items like strategic fit, risk level, and payback timing (or close equivalents).\n4) Recommendation section clearly choosing ONE project initially and justifying at a high level.\n5) Risk Register for the recommended project with a table (e.g., \"Top Risks\") having columns similar to: [Risk | Type (Financial/Operational) | Probability (Low/Med/High) | Impact (Low/Med/High) | Mitigation | Contingency]. At least 3 distinct risks.\n6) Capital Allocation Plan (if both projects pursued) including a table (e.g., \"Allocation of $100M\") with columns similar to: [Project | Allocation ($M) | Rationale | Expected Timing], with a visible total row (e.g., Total \u2248 100).\n7) Appendix: Methodology & Assumptions referencing the 9% WACC and clarifying that analysis is directional (not exact).\n\nScoring:\n- 6.0: Valid PDF/DOCX, \u226415 pages, addressed to Board and dated January 2025; all 7 required elements present with identifiable tables/sections.\n- 5.0: All core sections present but one minor element missing (e.g., total row in allocation table not explicit) OR small formatting lapses.\n- 3.5\u20134.0: Missing 1\u20132 required sections/tables but still a valid PDF/DOCX addressed appropriately and within page limit.\n- 1.0\u20132.5: Valid file but missing multiple core sections or clearly incomplete structure.\n- 0.0: Not PDF/DOCX, or >15 pages, or not addressed to Board, or grossly missing structure.\n\nJudge only presence/structure, not analytical correctness or prose quality.", "expectation": "A \u226415-page PDF/DOCX addressed to the Board (Jan 2025) containing clearly labeled sections and tables enabling verification: executive summary, two project overviews with directional NPV/IRR table, comparative analysis table, recommendation, a 3+ risk register with mitigations/contingencies, a $100M allocation table with total, and an appendix citing 9% WACC and directional methodology."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness & Consistency Verification", "description": "Verify key analytical correctness and internal consistency using a mix of lightweight code checks and LLM judgment. Code rules focus on deterministic text signals; LLM rules assess financial logic, consistency, and feasibility.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Core Inputs and Constraints Mentioned", "description": "Checks presence of core context signals: WACC 9%, $100 million availability, company/Board references.", "weight": 0.5, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_document:\\n            return 0.0, 'No document output.'\\n        text = ''\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_docx_text(output.id)\\n            except Exception:\\n                return 0.0, 'Unable to read PDF/DOCX text.'\\n        t = text.lower()\\n\\n        checks = []\\n        # WACC and 9%\\n        checks.append(('wacc' in t))\\n        checks.append(('9%' in t) or ('9 %' in t))\\n        # $100 million availability (flexible)\\n        hundred_m_patterns = [r'\\b100\\s*(million|mm|m)\\b', r'\\$\\s*100\\s*million', r'\\$100m', r'\\$100\\s*mm']\\n        checks.append(any(re.search(p, t) for p in hundred_m_patterns))\\n        # Company and Board\\n        checks.append(('tiny-rod hit' in t) or ('tiny rod hit' in t))\\n        checks.append(('board of directors' in t) or ('board' in t))\\n\\n        score_ratio = sum(1 for c in checks if c) / max(1, len(checks))\\n        score = score_ratio * 0.5\\n        feedback = f\"Signals found: {sum(1 for c in checks if c)}/{len(checks)}\"\\n        return score, feedback\\n    except Exception as e:\\n        return 0.0, f'Error: {e}'"}, {"type": "code", "name": "NPV and IRR Coverage (Both Projects)", "description": "Checks that NPV and IRR are each discussed with sufficient frequency to plausibly cover both projects.", "weight": 0.5, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_document:\\n            return 0.0, 'No document output.'\\n        text = ''\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_docx_text(output.id)\\n            except Exception:\\n                return 0.0, 'Unable to read PDF/DOCX text.'\\n        t = text.lower()\\n        npv_count = len(re.findall(r'\\bnpv\\b', t))\\n        irr_count = len(re.findall(r'\\birr\\b', t))\\n\\n        if npv_count >= 2 and irr_count >= 2:\\n            score = 0.5\\n            fb = 'NPV and IRR discussed for both projects (likely).'\n        elif npv_count >= 1 and irr_count >= 1:\n            score = 0.375\n            fb = 'NPV and IRR present but frequency is low.'\n        elif (npv_count >= 1) or (irr_count >= 1):\n            score = 0.2\n            fb = 'Only one metric sufficiently present.'\n        else:\n            score = 0.0\n            fb = 'NPV/IRR not detected.'\n        return score, f\"NPV:{npv_count} IRR:{irr_count} - {fb}\"\\n    except Exception as e:\\n        return 0.0, f'Error: {e}'"}, {"type": "code", "name": "Directional Approach Evidence", "description": "Checks for language indicating directional/approximate analysis rather than exact calculations.", "weight": 0.5, "code": "import re\\n\\nDIRECTIONAL_KEYWORDS = [\\n    'directional', 'approximate', 'approx.', 'range', 'order of magnitude',\\n    'indicative', 'estimate', 'estimates', 'not exact', 'high-level'\\n]\\n\\ndef evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_document:\\n            return 0.0, 'No document output.'\\n        text = ''\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_docx_text(output.id)\\n            except Exception:\\n                return 0.0, 'Unable to read PDF/DOCX text.'\\n        t = text.lower()\\n        found = []\\n        for kw in DIRECTIONAL_KEYWORDS:\\n            if kw in t:\\n                found.append(kw)\n        unique_found = set(found)\n        n = len(unique_found)\n        if n >= 3:\n            score = 0.5\n        elif n == 2:\n            score = 0.35\n        elif n == 1:\n            score = 0.2\n        else:\n            score = 0.0\n        return score, f\"Directional keywords found: {sorted(list(unique_found))}\"\\n    except Exception as e:\\n        return 0.0, f'Error: {e}'"}, {"type": "llm_judge", "name": "Discount-Rate Coherence and Investment Rule", "description": "Assesses whether the report consistently applies the 9% WACC and uses correct value-creation logic (IRR > WACC, positive NPV) to frame directional conclusions.", "weight": 2.0, "judge_prompt": "Evaluate whether the report applies discount-rate logic consistently and correctly:\n- Explicitly references 9% WACC (or equivalent) in methodology/assumptions\n- Uses correct investment rule logic: IRR above WACC suggests value creation; NPV directional signs match the narrative\n- Links the chosen recommendation to this logic (e.g., preferred project shows IRR above WACC or higher directional NPV)\nScoring:\n- 2.0: Clear, explicit, and consistent use of 9% WACC; recommendation logically follows IRR/NPV logic.\n- 1.0: Partial or implicit treatment; some logic present but not tightly linked to the recommendation.\n- 0.0: Misapplied or absent discount-rate logic; contradictions between IRR/NPV and recommendation.", "expectation": "Sound directional application of 9% WACC with coherent IRR/NPV-based reasoning that underpins the recommendation."}, {"type": "llm_judge", "name": "Recommendation-Analysis Consistency", "description": "Checks that the initial single-project recommendation is directly supported by the comparative analysis and assumptions; no major contradictions.", "weight": 2.0, "judge_prompt": "Assess whether the stated recommendation is consistent with the comparative analysis and project assumptions:\n- The pros/cons in comparative tables and narrative align with the selected project\n- No glaring contradictions (e.g., choosing the riskier project while claiming lower risk without rationale)\n- Justification blends quantitative directionality (NPV/IRR/payback) and qualitative factors (strategic fit, diversification, capability)\nScoring:\n- 2.0: Strong alignment between analysis and recommendation; coherent quantitative and qualitative rationale.\n- 1.0: Mostly aligned but with gaps or weak links.\n- 0.0: Poor alignment or contradictions.", "expectation": "Recommendation clearly follows from comparative evidence and articulated assumptions."}, {"type": "llm_judge", "name": "Risk Mitigation Specificity and Feasibility", "description": "Evaluates whether the top three risks for the recommended project include concrete mitigations and clear contingency plans.", "weight": 1.5, "judge_prompt": "Review the risk register for the recommended project:\n- Are at least three distinct financial/operational risks identified?\n- For each risk, are mitigation strategies specific and actionable (owners, levers, timelines where applicable)?\n- Are contingency plans credible and appropriately triggered by defined conditions?\nScoring:\n- 1.5: Three or more well-specified risks with actionable mitigations and credible contingencies.\n- 0.8: Risks identified but mitigations/contingencies are generic or partially specified.\n- 0.0: Risks missing or non-actionable.", "expectation": "Three+ concrete risks with targeted mitigations and clear contingency paths."}, {"type": "llm_judge", "name": "Capital Allocation Plan Validity and Rationale", "description": "Checks whether the optional plan to pursue both projects credibly allocates the $100M total with rationale and timing/sequencing considerations.", "weight": 1.0, "judge_prompt": "Evaluate the capital allocation plan for pursuing both projects:\n- Does the table clearly allocate $100M in total with a visible total row?\n- Is the rationale for split tied to risk, strategic diversification, and capacity/timing?\n- Are phasing/deployment timing considerations included or implied sensibly?\nScoring:\n- 1.0: Total visibly \u2248 $100M and rationale/timing are coherent.\n- 0.5: Total implied but unclear or rationale thin.\n- 0.0: No viable allocation plan or incoherent totals/rationale.", "expectation": "A coherent $100M allocation with total and rationale that reflects strategy, risk, and timing."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Executive Quality & Communication", "description": "Holistic assessment of professional quality, strategic insight, readability, and actionability for a Board audience.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Executive-Level Clarity, Tone, and Structure", "description": "Assesses whether the report is concise, well-structured, and written for Board-level readers.", "weight": 1.5, "judge_prompt": "Judge the executive quality of the writing and structure:\n- Clear executive summary with key points up front\n- Logical flow with informative headings and brevity\n- Professional tone suitable for a Board packet\nScoring:\n- 1.5: Clear, concise, and well-structured; highly executive-friendly.\n- 0.8: Generally clear but with minor verbosity or structure issues.\n- 0.0: Unclear, verbose, or poorly structured.", "expectation": "Concise, structured Board-ready prose with strong front-loaded messaging."}, {"type": "llm_judge", "name": "Strategic Insight and Long-Term Value Orientation", "description": "Evaluates whether the analysis considers strategic fit, capabilities, diversification, and long-term value creation beyond raw returns.", "weight": 2.0, "judge_prompt": "Assess strategic depth beyond project-specific returns:\n- Addresses long-term value creation, capability building, optionality\n- Considers diversification, portfolio balance, and strategic alignment\n- Discusses organizational/operational readiness where relevant\nScoring:\n- 2.0: Strong, nuanced strategic insight integrated into recommendation.\n- 1.0: Some strategic points mentioned but shallow or disconnected.\n- 0.0: Little to no strategic consideration.", "expectation": "Recommendation integrates strategy, diversification, and capability considerations."}, {"type": "llm_judge", "name": "Use of Tables/Visuals and Readability", "description": "Assesses whether tables/visuals are used effectively to enhance comprehension and scan-ability.", "weight": 1.0, "judge_prompt": "Evaluate the use of tables/visuals:\n- Key comparisons and risk/allocation details are easy to scan\n- Tables have clear labels/columns\n- Formatting supports quick Board digestion\nScoring:\n- 1.0: Tables/visuals materially improve readability.\n- 0.5: Present but moderately helpful.\n- 0.0: Absent or confusing.", "expectation": "Clear, labeled tables that improve executive readability."}, {"type": "llm_judge", "name": "Actionability and Next Steps", "description": "Evaluates whether the report ends with concrete next steps, decision asks, and caveats/monitoring guidance.", "weight": 1.5, "judge_prompt": "Assess actionability:\n- Clear recommendation and specific next steps (e.g., diligence items, stage gates, approvals, timing)\n- Decision asks for the Board are explicit\n- Caveats and monitoring metrics are identified\nScoring:\n- 1.5: Highly actionable with clear next steps and metrics.\n- 0.8: Partially actionable or missing metrics.\n- 0.0: Not actionable.", "expectation": "Crisp decision ask with next steps and monitoring approach."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "3a4c347c-4aec-43c7-9a54-eb1f816ab1f9", "rubric": {"category_name": "Enterprise Tech Asia Season Proposal (Editor)", "rationale": "This rubric enforces a self-documenting, verifiable proposal for an Asia-focused editorial season in enterprise technology. Stage 1 is a strict LLM-only shape gate that mandates a DOCX deliverable with precisely defined sections, a 4-week M/W/F schedule including a Friday TV broadcast and VT/radio/podcast re-versioning, KPIs, and budget. Stage 2 mixes lightweight code checks (bounds/coverage/keyword consistency) with higher-weight LLM judges for cross-referencing and plausibility. Stage 3 evaluates overall editorial quality, feasibility, international audience fit, and sponsor appeal. Code rules are deliberately lower-weight than LLM rules to reflect nuanced editorial judgment while still verifying objective constraints.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Format and Structural Gate (LLM-only)", "description": "Gate: Enforce exact output shape to enable verification. Must be a DOCX proposal \u22646 pages with all required sections, a structured 4-week M/W/F schedule including Friday TV broadcast and one VT re-versioned to radio/podcast, explicit KPIs list, budget, story ideas with contributors and VT/radio suitability, CTO interviewees, and reference to boilerplate.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "DOCX Format, Length, and Professional Layout", "description": "Check the file is a Word document (DOCX), professionally formatted, and no more than six pages.", "weight": 2.0, "judge_prompt": "You are validating FORMAT and LENGTH only. Review the candidate output.\nCriteria (flexible with wording, strict on intent):\n- Must be a Word document (DOCX), not PDF/Markdown/Excel.\n- Length: no more than 6 pages (allow small variance if obviously ~6 pages or fewer).\n- Professional layout with clear section headers and readable structure.\nScoring:\n- 2.0: DOCX; layout is clearly professional; \u22646 pages.\n- 1.0: DOCX and professional layout but likely slightly over length by minor margin (e.g., ~7 pages) OR ambiguous page estimation.\n- 0.5: DOCX but poor layout (unclear headings) OR substantially exceeds length.\n- 0.0: Not a DOCX file.\nOnly score based on format/length/layout presence, not content quality.", "expectation": "A cleanly formatted DOCX proposal that appears to be 6 pages or fewer."}, {"type": "llm_judge", "name": "Required Sections and Structured Schedule Presence", "description": "Verify presence of all required sections and the structured 4-week schedule with modalities.", "weight": 2.0, "judge_prompt": "You are validating PRESENCE and STRUCTURE (not quality). Review the DOCX content and check for these sections/elements (flexible on exact headings, strict on inclusion):\nRequired content:\n1) Suggested season title\n2) Introduction\n3) Aims of the season\n4) Potential news hooks (for scheduling)\n5) Suggested budget (with at least travel and freelancer costs lines)\n6) Story ideas including: proposed contributors AND each story\u2019s suitability for VT/radio/podcast clearly indicated where applicable\n7) Proposed CTO interviewees (at least one per week)\n8) Draft broadcast and publication schedule over a 4-week period that shows Monday, Wednesday, Friday publishing, and marks Friday TV broadcast. One story in the season must be designated as a VT and explicitly noted as re-versioned as radio and podcast.\n9) KPIs section listing: page views, time on page, bounce rate, click-through rate (CTR), likes/shares/comments on social media, and sponsorship success by sales for the season.\n10) Reference or alignment note acknowledging \u201cEnterprise Technology BOILERPLATE.docx\u201d (by name) for tone/audience/positioning.\nScoring:\n- 2.0: All 10 items present with clear section headers; schedule spans 4 weeks (M/W/F visible), Friday TV broadcast noted, and one item designated VT with radio/podcast re-versioning.\n- 1.5: Missing exactly one minor item (e.g., boilerplate mention) but all core items (title, intro, aims, hooks, budget, story ideas+contributors+modality, CTO list, 4-week schedule, KPIs) are present.\n- 1.0: Missing any one core item OR schedule incomplete (e.g., missing one weekday or unclear weeks), but majority present.\n- 0.5: Multiple core items missing but clear attempt at structure.\n- 0.0: Lacks core structure (e.g., no schedule or no budget or no KPIs).\nOnly check presence and structure, not correctness.", "expectation": "A clearly structured proposal with all required sections and a visible, structured 4-week M/W/F schedule including a Friday TV broadcast and VT/radio/podcast designation."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Consistency Verification", "description": "Now that structure is enforced, verify correctness and consistency via mixed code and LLM checks. Code focuses on deterministic checks (bounds, counts, keywords). LLM checks assess cross-references and plausibility.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Schedule Completeness and Modality Flags (M/W/F x 4 weeks)", "description": "Detect 4-week cadence and required modalities in text: M/W/F pattern, \u22654 CTO mentions, and VT + radio + podcast presence.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document output.\"\n        # Read text\n        text = \"\"\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_text(output.id)\n                except Exception:\n                    return 0.0, \"Unable to read document text.\"\n        if not text:\n            return 0.0, \"Empty document text.\"\n        t = text.lower()\n        # Weeks detection (Week 1..4, or words one..four)\n        week_hits = set()\n        for pat in [r\"week\\s*(?:1|2|3|4)\", r\"week\\s*(?:one|two|three|four)\"]:\n            for m in re.findall(pat, t):\n                week_hits.add(m)\n        weeks_ok = 1.0 if len(week_hits) >= 4 else 0.0\n        # Days presence\n        days = [\"monday\", \"wednesday\", \"friday\"]\n        days_ok = 1.0 if all(d in t for d in days) else 0.0\n        # CTO interview count\n        cto_count = len(re.findall(r\"\\bcto\\b|chief technology officer\", t))\n        cto_ok = 1.0 if cto_count >= 4 else (0.5 if cto_count >= 2 else 0.0)\n        # VT presence and re-versioning\n        vt_ok = 1.0 if (\" vt \" in f\" {t} \" or \"video package\" in t or \"video tape\" in t or \"vt)\" in t or \"(vt\" in t) else 0.0\n        radio_podcast_ok = 1.0 if (\"radio\" in t and \"podcast\" in t) else (0.5 if (\"radio\" in t or \"podcast\" in t) else 0.0)\n        # Aggregate equally\n        components = [weeks_ok, days_ok, cto_ok, vt_ok, radio_podcast_ok]\n        score = sum(components) / len(components)\n        return max(0.0, min(1.0, score)), f\"weeks_ok={weeks_ok}, days_ok={days_ok}, cto_ok={cto_ok}, vt={vt_ok}, radio/podcast={radio_podcast_ok}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Asian Country Coverage Breadth", "description": "Check for a good number of distinct Asian countries mentioned across the plan (aiming for broad regional coverage).", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document output.\"\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_text(output.id)\n                except Exception:\n                    return 0.0, \"Unable to read document text.\"\n        if not text:\n            return 0.0, \"Empty document text.\"\n        t = text\n        countries = {\n            \"china\":\"china\",\"india\":\"india\",\"japan\":\"japan\",\"south korea\":\"south korea\",\"korea\":\"korea\",\"singapore\":\"singapore\",\"indonesia\":\"indonesia\",\"malaysia\":\"malaysia\",\"thailand\":\"thailand\",\"vietnam\":\"vietnam\",\"philippines\":\"philippines\",\"taiwan\":\"taiwan\",\"hong kong\":\"hong kong\",\"pakistan\":\"pakistan\",\"bangladesh\":\"bangladesh\",\"sri lanka\":\"sri lanka\",\"nepal\":\"nepal\",\"myanmar\":\"myanmar\",\"cambodia\":\"cambodia\",\"laos\":\"laos\",\"brunei\":\"brunei\",\"mongolia\":\"mongolia\",\"kazakhstan\":\"kazakhstan\",\"uzbekistan\":\"uzbekistan\",\"uae\":\"uae\",\"united arab emirates\":\"united arab emirates\",\"saudi arabia\":\"saudi arabia\",\"qatar\":\"qatar\",\"kuwait\":\"kuwait\",\"bahrain\":\"bahrain\",\"oman\":\"oman\",\"israel\":\"israel\",\"turkey\":\"turkey\",\"iran\":\"iran\",\"iraq\":\"iraq\",\"jordan\":\"jordan\",\"lebanon\":\"lebanon\"\n        }\n        found = set()\n        tl = t.lower()\n        for k in countries.keys():\n            if re.search(r\"\\\\b\" + re.escape(k) + r\"\\\\b\", tl):\n                # normalize korea to south korea only if specified; keep generic 'korea'\n                found.add(countries[k])\n        # Consolidate: if both 'korea' and 'south korea' present, keep 'south korea'\n        if 'south korea' in found and 'korea' in found:\n            found.discard('korea')\n        n = len(found)\n        # Scoring tiers\n        if n >= 6:\n            score = 1.0\n        elif n == 5:\n            score = 0.8\n        elif n == 4:\n            score = 0.6\n        elif n == 3:\n            score = 0.4\n        elif n == 2:\n            score = 0.2\n        else:\n            score = 0.0\n        return score, f\"Distinct Asian countries found: {n} ({', '.join(sorted(found))})\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Budget Plausibility: Travel Range and Freelancer Costs", "description": "Verify travel budget mentions ~\u00a320k\u2013\u00a325k and freelancer feature costs ~\u00a31k\u2013\u00a31.5k for two features.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document output.\"\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_text(output.id)\n                except Exception:\n                    return 0.0, \"Unable to read document text.\"\n        if not text:\n            return 0.0, \"Empty document text.\"\n        t = text.lower()\n        # Extract money mentions\n        amounts = []\n        for m in re.finditer(r\"\u00a3\\s?([0-9][0-9,]*)(?:\\.[0-9]{2})?\", t):\n            try:\n                val = int(m.group(1).replace(\",\", \"\"))\n            except:\n                continue\n            start = m.start()\n            context_snip = t[max(0, start-80):start+80]\n            amounts.append((val, context_snip))\n        # Travel budget check ~20k-25k near travel context\n        travel_ok = False\n        for val, ctx_snip in amounts:\n            if 20000 <= val <= 25000 and any(w in ctx_snip for w in [\"travel\", \"flights\", \"accommodation\", \"transport\", \"crew\", \"on-the-ground\"]):\n                travel_ok = True\n                break\n        # Also allow textual numbers without \u00a3 if clearly stated range\n        if not travel_ok:\n            if re.search(r\"20,?000\\s*(?:to|\u2013|-|\u2014|and|~|approx\\.?|approximately)\\s*25,?000\", t):\n                travel_ok = True\n        # Freelancer costs ~1k-1.5k and two features\n        freelancer_ok = False\n        has_freelancer_word = (\"freelancer\" in t or \"freelance\" in t)\n        small_amount = False\n        for val, _ in amounts:\n            if 1000 <= val <= 1500:\n                small_amount = True\n                break\n        two_features = bool(re.search(r\"\\b(two|2)\\b\\s*(features|stories)\", t))\n        if has_freelancer_word and small_amount and two_features:\n            freelancer_ok = True\n        # Score half for each condition\n        score = (0.5 if travel_ok else 0.0) + (0.5 if freelancer_ok else 0.0)\n        return score, f\"travel_ok={travel_ok}, freelancer_ok={freelancer_ok}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "KPI Presence Completeness", "description": "Confirm presence of all required KPIs and sponsorship metric.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document output.\"\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_text(output.id)\n                except Exception:\n                    return 0.0, \"Unable to read document text.\"\n        if not text:\n            return 0.0, \"Empty document text.\"\n        t = text.lower()\n        checks = {\n            'page views': any(s in t for s in ['page views', 'pageviews']),\n            'time on page': 'time on page' in t,\n            'bounce rate': 'bounce rate' in t,\n            'ctr': ('click through rate' in t) or ('click-through rate' in t) or re.search(r'\\bctr\\b', t) is not None,\n            'likes': 'likes' in t,\n            'shares': 'shares' in t,\n            'comments': 'comments' in t,\n            'sponsorship': ('sponsorship' in t or 'sponsor' in t) and ('sales' in t)\n        }\n        present = sum(1 for v in checks.values() if v)\n        total = len(checks)\n        score = present / total\n        return score, f\"KPI items present {present}/{total}: {checks}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "llm_judge", "name": "Schedule and Modality Compliance (LLM cross-check)", "description": "Verify each week has 2 online features and 1 CTO interview; Friday TV broadcast is included; one item is a VT re-versioned for radio/podcast; schedule is coherent and non-contradictory.", "weight": 2.0, "judge_prompt": "Check the schedule content for compliance and internal consistency:\n- Exactly or at least: 2 online features + 1 CTO interview per week across 4 weeks.\n- Monday/Wednesday/Friday cadence is visible.\n- Friday includes the short TV programme broadcast on the rolling international news service.\n- At least one story is explicitly designated as a VT (video package) and clearly marked as re-versioned for radio and podcast.\n- Cross-check that the items listed in the schedule align with the story ideas and the CTO interviewees sections (names/themes should be consistent or obviously mapped).\nScoring:\n- 2.0: All conditions met; cross-references are consistent and unambiguous.\n- 1.5: Minor ambiguity (e.g., one unclear mapping) but requirements satisfied.\n- 1.0: One requirement partially unmet (e.g., cadence unclear or VT re-versioning not explicit).\n- 0.5: Multiple issues but a recognizable schedule exists.\n- 0.0: Schedule missing or fundamentally non-compliant.", "expectation": "A clear 4-week M/W/F plan with 2 features + 1 CTO each week, explicit Friday TV broadcast, and one VT re-versioned to radio/podcast, consistent with story and interview sections."}, {"type": "llm_judge", "name": "Budget Coherence and Justification", "description": "Evaluate whether the budget covers required components and aligns with stated assumptions and constraints.", "weight": 2.0, "judge_prompt": "Review the budget section for coherence and justification:\n- Travel budget is approximately \u00a320,000\u2013\u00a325,000 for a small crew (reporter + camera op/producer) for 3\u20134 days per location, covering flights, accommodation, local transport, and on-the-ground support. This should be explicitly addressed.\n- In-house team produces the CTO interviews and two features; two additional features use a freelancer at ~\u00a31,000\u2013\u00a31,500 each; these costs are included and labeled correctly.\n- Optional but strong: Any additional production/post costs and contingency are noted.\n- Numbers appear consistent across the document (budget totals match line items, no contradictions with schedule scope).\nScoring:\n- 2.0: All key components present, amounts and assumptions align, and totals/logic are consistent.\n- 1.5: Mostly coherent with small omissions or slightly vague justifications.\n- 1.0: Key components present but weak linkage to assumptions (e.g., days per location) or missing one cost element.\n- 0.5: Some numbers present but inconsistent or poorly justified.\n- 0.0: No workable budget or clearly inconsistent with stated constraints.", "expectation": "A plausible, itemized budget aligned to the stated travel scope and freelancer costs, with internally consistent totals."}, {"type": "llm_judge", "name": "Story Ideas and Contributor Suitability", "description": "Assess whether story ideas are robust, include proposed contributors (analysts/experts), and suitability for VT/radio is indicated with justification.", "weight": 2.0, "judge_prompt": "Evaluate the story ideas section:\n- Each proposed story includes 1) a clear angle relevant to enterprise tech innovation in Asia and 2) proposed contributors (e.g., industry analysts, CTOs, academics, experts) with rationale.\n- Suitability for VT and/or radio/podcast is indicated for each relevant story, with brief justification (visual/audio elements, access to locations/interviewees, timeliness).\n- Coverage spans a good range of Asian countries (not concentrating all stories in a single market) and avoids stereotypes.\n- Ideas are aligned to an international audience; jargon is controlled or explained.\nScoring:\n- 2.0: Strong, well-justified ideas with contributors and modality suitability across a diverse set of Asian markets.\n- 1.5: Generally strong with minor gaps (e.g., one story missing contributors or weak VT rationale).\n- 1.0: Mixed quality; contributors listed but limited justification or narrow geography.\n- 0.5: Sparse ideas and/or poor contributor fit.\n- 0.0: Missing or not actionable.", "expectation": "Well-scoped, internationally relevant story ideas with named contributors and modality suitability across multiple Asian markets."}, {"type": "llm_judge", "name": "KPIs and Measurement Plan", "description": "Confirm KPIs include targets/baselines and a measurement approach, including sponsorship success metric.", "weight": 2.0, "judge_prompt": "Inspect KPIs and success measures:\n- The section lists standard KPIs: page views, time on page, bounce rate, CTR, likes/shares/comments, plus sponsorship success for the season.\n- Includes targets and/or benchmarks (e.g., numeric targets, percentage improvements, baselines) and a measurement plan (how/when KPIs will be tracked and reported).\n- Targets are realistic given schedule and channels, and sponsorship success criterion is clearly defined (e.g., sponsorship secured for full season, target value/partner count, or conversion timeline).\nScoring:\n- 2.0: Complete KPI list with clear targets/baselines and practical measurement plan; sponsorship success well-specified.\n- 1.5: KPI list complete with partial/indicative targets or brief measurement plan.\n- 1.0: KPIs present but no targets or vague measurement approach.\n- 0.5: Incomplete KPI list and minimal measurement detail.\n- 0.0: Missing KPIs/sponsorship metric.", "expectation": "Complete KPIs with realistic targets and a simple, credible tracking/reporting plan including sponsorship success."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Editorial Quality and Strategic Value", "description": "Holistic assessment of professionalism, audience fit, feasibility, and sponsor appeal of the proposal.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professionalism, Clarity, and Coherence", "description": "Assess overall editorial polish, clarity, flow, and coherence as a professional proposal.", "weight": 1.5, "judge_prompt": "Evaluate writing quality and coherence:\n- Professional tone and clear organization suitable for a respected international news publisher.\n- Concise, readable, and logically ordered (no more than 6 pages); headings/subheadings support navigation.\n- Minimal jargon; acronyms are defined at first use.\nScoring: 1.5 excellent; 1.0 good with minor issues; 0.5 adequate but uneven; 0.0 poor.", "expectation": "A polished, concise, and well-structured professional proposal."}, {"type": "llm_judge", "name": "International Audience Fit and Geographic Diversity", "description": "Judge how well the proposal serves an international audience and distributes coverage across Asia.", "weight": 1.5, "judge_prompt": "Assess audience and geographic strategy:\n- Content is designed for an international audience (not UK-centric) while acknowledging UK base.\n- Coverage is diverse across Asian regions (East, Southeast, South, possibly West/Central), avoiding stereotypes.\n- Context and explanations are provided for global readers (e.g., regulatory context, market scale, terminology).\nScoring: 1.5 strong international framing and diverse coverage; 1.0 reasonable; 0.5 limited; 0.0 weak.", "expectation": "Internationally accessible framing with varied Asian country coverage."}, {"type": "llm_judge", "name": "Feasibility, Logistics, and Risk Awareness", "description": "Evaluate operational realism of the plan: logistics, timing, permissions, risks, and contingencies.", "weight": 1.5, "judge_prompt": "Consider feasibility:\n- Realistic travel and production logistics for 3\u20134 days per location with a small crew; timeline fits M/W/F cadence.\n- Mentions key risks/constraints (e.g., visas, permissions, access, time zones, language/support) and basic mitigations.\n- Scheduling dependencies (interview availability, news hooks timing) are acknowledged.\nScoring: 1.5 comprehensive; 1.0 adequate; 0.5 limited; 0.0 absent.", "expectation": "A feasible plan with basic risk/contingency considerations."}, {"type": "llm_judge", "name": "Sponsor Appeal and Commercial Strategy (without compromising editorial)", "description": "Assess how well the proposal positions the season for sponsorship while maintaining editorial standards.", "weight": 1.5, "judge_prompt": "Evaluate sponsor strategy:\n- Clear value proposition for sponsors of international-facing coverage; suitable categories/segments suggested without conflicts.\n- Integration points (branding, pre/post/mid-roll for VT/radio/podcast, site placements) are sensible and disclosed.\n- Editorial integrity preserved (no undue influence); transparent separation of sales/editorial roles.\nScoring: 1.5 strong and ethical; 1.0 decent with gaps; 0.5 minimal; 0.0 absent.", "expectation": "An ethical, compelling sponsorship approach aligned to the season\u2019s audience and channels."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "bd72994f-5659-4084-9fab-fc547d1efe3b", "rubric": {"category_name": "Retail Client Outreach: Resort 2025 Lookbook + Appointment Template", "rationale": "Pattern B (Document task). The deliverables are two files: (1) a 4\u20136 slide PDF presentation of styled looks from a single luxury brand's Resort 2025 collection, and (2) a reusable outreach template (email + SMS) as a DOCX or PDF. Stage 1 forces a strict, verifiable structure to make subsequent verification trivial. Stage 2 mixes lightweight code checks (file presence, token/regex consistency, cross-file alignment) with LLM judges for nuanced content and consistency. Stage 3 evaluates professional quality, brand alignment, and sales effectiveness for practical deployment by retail staff.", "max_total_score": 16.0, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate (Documents Only)", "description": "LLM-only gate ensuring deliverables exist with exact, verifiable structure. If this gate fails, the entire category scores 0.", "is_required": true, "max_points": 2.0, "min_score_to_pass": 1.5, "rules": [{"type": "llm_judge", "name": "Presentation PDF Structure (4\u20136 Look Slides)", "description": "Verify the PDF presentation exists and has exactly the required structure for 4\u20136 Look slides from a single Resort 2025 collection.", "weight": 1.0, "judge_prompt": "You are validating the SHAPE of the candidate outputs, not their content quality. Examine all provided files.\n\nRequirement A: Presentation PDF of 4\u20136 slides (each slide is a Look) from a single luxury brand's Resort 2025 collection.\n\nCheck the following STRICT structure on the PDF:\n- File format: PDF (not DOCX, not PPTX, not images alone)\n- Number of slides/pages: between 4 and 6 inclusive (each page counts as a slide)\n- Each slide must be a Look slide and include ALL of:\n  1) A header that begins with the word \"Look\" (e.g., \"Look 1: [Name]\")\n  2) The brand and the text \"Resort 2025\" (or \"2025 Resort\") on the slide\n  3) At least one product image on the slide (visual check)\n  4) A bullet list labeled \"Key Pieces\" (3\u20136 items expected; only check presence of a bulleted list under that label)\n  5) A short \"Styling Notes\" or \"How to Wear\" note (a brief sentence or two)\n  6) A brief \"Occasion\" or \"Use Case\" note\n  7) A visible source/citation or URL referencing the brand\u2019s official site/lookbook\n- All slides must reference the SAME single brand and the same Resort 2025 collection.\n\nScoring (structure only):\n- 1.0: PDF exists; 4\u20136 pages; EVERY slide meets all 7 structural elements, and single-brand/single-collection consistency is visible.\n- 0.7: PDF exists; 4\u20136 pages; 1 slide is missing 1 structural element OR brand/collection label missing on 1 slide but otherwise consistent.\n- 0.4: PDF exists; 4\u20136 pages; 2+ slides missing structural elements, but slides are clearly Looks; still appears to be one brand resort collection.\n- 0.0: Not a PDF, or page count outside 4\u20136, or slides are not look slides, or multiple brands/collections are mixed.", "expectation": "A single-brand Resort 2025 PDF with 4\u20136 look slides, each including header, brand+collection label, image, Key Pieces list, Styling Notes, Occasion, and source link."}, {"type": "llm_judge", "name": "Outreach Template Document Structure (Email + SMS)", "description": "Verify a separate outreach template document exists with both Email and SMS variants and required placeholders/sections.", "weight": 1.0, "judge_prompt": "You are validating the SHAPE of the candidate outputs, not their content quality. Examine all provided files.\n\nRequirement B: Outreach template as DOCX or PDF (a separate document from the presentation PDF).\n\nCheck the following STRICT structure:\n- File format: DOCX or PDF (not plain text/markdown)\n- Contains TWO clearly labeled variants: \"Email\" and \"SMS\" (or \"Text Message\")\n- Email variant MUST include, in order:\n  1) Section label \"Email\"\n  2) At least TWO subject line options labeled (e.g., \"Subject Options\")\n  3) A greeting line with a client name placeholder (e.g., [Client Name] or {Client Name})\n  4) A personalized hook referencing the brand and \"Resort 2025\"\n  5) An appointment booking CTA with placeholders for at least two date/time options and a link or phone number\n  6) An associate signature block with placeholders (e.g., [Associate Name], [Store Name], [Phone], [Address])\n- SMS variant MUST include:\n  1) Section label \"SMS\" or \"Text Message\"\n  2) A concise message referencing the brand and \"Resort 2025\"\n  3) A clear booking CTA with a link or phone\n  4) An opt-out line (e.g., \"Reply STOP to opt out\")\n- The brand and \"Resort 2025\" should appear in BOTH variants.\n\nScoring (structure only):\n- 1.0: Valid DOCX/PDF exists separate from presentation; both Email and SMS sections present with all listed items.\n- 0.7: Valid DOCX/PDF exists; one minor item missing (e.g., only one subject line) but both variants present.\n- 0.4: Valid DOCX/PDF exists; only one variant (Email or SMS) present, or multiple required items missing.\n- 0.0: No valid outreach template document, wrong format, or no recognizable Email/SMS sections.", "expectation": "A separate DOCX/PDF file with both Email and SMS templates, complete with placeholders, CTA, and opt-out."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness and Consistency)", "description": "Now that structure is enforced, verify consistency, plausibility, and cross-file alignment using code and LLM rules.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Deliverable Presence and Typing Check", "description": "Confirm at least two document resources exist: one PDF presentation and one separate template (PDF or DOCX).", "weight": 0.5, "code": "import re\nfrom pathlib import Path\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs() or []\n        if not outputs:\n            return 0.0, \"No outputs found.\"\n        docs = [r for r in outputs if getattr(r, 'is_document', False)]\n        if len(docs) < 2:\n            return 0.0, \"Need at least two documents: presentation PDF and outreach template.\"\n        # Identify PDFs and DOCX by extension\n        pdfs = []\n        others = []\n        for r in docs:\n            p = context.files.get_path(r.id)\n            ext = p.suffix.lower()\n            if ext == '.pdf':\n                pdfs.append(r)\n            elif ext in ('.docx', '.pdf'):\n                others.append(r)\n            else:\n                others.append(r)\n        # Presentation must be a PDF\n        if not pdfs:\n            return 0.0, \"No PDF presentation detected.\"\n        # Require another document besides the presentation (could also be a PDF or DOCX)\n        if len(docs) < 2:\n            return 0.0, \"Only one document found; outreach template missing.\"\n        return 0.5, \"Found multiple documents including at least one PDF (likely presentation) and another document (likely template).\"\n    except Exception as e:\n        return 0.0, f\"Error during presence/type check: {e}\""}, {"type": "code", "name": "Slide Count Heuristic via 'Look' Headers", "description": "Heuristic check that the presentation PDF contains 4\u20136 look slides by counting 'Look' headers.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs() or []\n        # Choose the PDF with the most occurrences of 'Look'\n        best_pdf = None\n        best_count = -1\n        for r in outputs:\n            try:\n                p = context.files.get_path(r.id)\n                if p.suffix.lower() != '.pdf':\n                    continue\n                text = context.files.read_pdf_text(r.id) or ''\n                # Count headers like \"Look 1:\" or lines beginning with Look\n                count = len(re.findall(r'\\bLook\\s*\\d*\\s*[:\\-]?', text, flags=re.IGNORECASE))\n                if count > best_count:\n                    best_count = count\n                    best_pdf = r\n            except Exception:\n                continue\n        if not best_pdf:\n            return 0.0, \"No PDF found for slide count heuristic.\"\n        # Heuristic: expect between 4 and 6 occurrences\n        if 4 <= best_count <= 6:\n            return 0.5, f\"Detected {best_count} look headers in PDF.\"\n        elif best_count > 0:\n            return 0.25, f\"Detected {best_count} look headers; outside 4\u20136 range.\"\n        else:\n            return 0.0, \"No 'Look' headers detected in PDF.\"\n    except Exception as e:\n        return 0.0, f\"Error during slide count heuristic: {e}\""}, {"type": "code", "name": "Single-Collection Consistency Across Files", "description": "Verify both files reference the same brand and the Resort 2025 collection.", "weight": 0.5, "code": "import re\n\ndef extract_brand_and_collection(text: str):\n    if not text:\n        return None, None\n    # Look for patterns like \"Brand Resort 2025\" or \"Brand Cruise 2025\"\n    m = re.search(r'([A-Z][A-Za-z&\\-\\s]{2,50})\\s*(Resort|Cruise)\\s*2025', text, flags=re.IGNORECASE)\n    if m:\n        brand = m.group(1).strip()\n        coll = f\"{m.group(2).title()} 2025\"\n        return brand, coll\n    # Fallback: look for Resort 2025 anywhere\n    if re.search(r'Resort\\s*2025', text, flags=re.IGNORECASE):\n        return None, 'Resort 2025'\n    return None, None\n\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs() or []\n        if not outputs:\n            return 0.0, \"No outputs.\"\n        # Gather texts\n        pdf_texts = []\n        other_texts = []\n        for r in outputs:\n            p = context.files.get_path(r.id)\n            if p.suffix.lower() == '.pdf':\n                try:\n                    pdf_texts.append(context.files.read_pdf_text(r.id) or '')\n                except Exception:\n                    pdf_texts.append('')\n            elif p.suffix.lower() == '.docx':\n                try:\n                    other_texts.append(context.files.read_docx_text(r.id) or '')\n                except Exception:\n                    other_texts.append('')\n        all_pdf_text = \"\\n\".join(pdf_texts)\n        all_other_text = \"\\n\".join(other_texts)\n        b1, c1 = extract_brand_and_collection(all_pdf_text)\n        b2, c2 = extract_brand_and_collection(all_other_text)\n        score = 0.0\n        notes = []\n        if c1 in ('Resort 2025', 'Cruise 2025') or (c1 and '2025' in c1):\n            score += 0.2\n            notes.append(f\"Presentation collection: {c1}\")\n        if c2 in ('Resort 2025', 'Cruise 2025') or (c2 and '2025' in c2):\n            score += 0.2\n            notes.append(f\"Template collection: {c2}\")\n        if b1 and b2:\n            if b1.lower().strip() == b2.lower().strip():\n                score += 0.1\n                notes.append(f\"Brand match: {b1}\")\n            else:\n                notes.append(f\"Brand mismatch: {b1} vs {b2}\")\n        elif b1 or b2:\n            # Partial brand detection\n            score += 0.05\n            notes.append(\"Brand detected in one file only.\")\n        return min(score, 0.5), \"; \".join(notes) if notes else \"No brand/collection text found.\"\n    except Exception as e:\n        return 0.0, f\"Error during brand/collection consistency: {e}\""}, {"type": "code", "name": "Outreach Template Token and CTA Check", "description": "Validate template contains placeholders, booking language, and both Email and SMS markers.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs() or []\n        if not outputs:\n            return 0.0, \"No outputs.\"\n        # Find non-presentation document likely to be the template\n        best_text = ''\n        for r in outputs:\n            p = context.files.get_path(r.id)\n            text = ''\n            try:\n                if p.suffix.lower() == '.docx':\n                    text = context.files.read_docx_text(r.id) or ''\n                elif p.suffix.lower() == '.pdf':\n                    text = context.files.read_pdf_text(r.id) or ''\n            except Exception:\n                continue\n            if not text:\n                continue\n            # Heuristic: presence of Email/SMS and booking words suggests template\n            markers = sum([bool(re.search(r'\\bEmail\\b', text, re.I)), bool(re.search(r'\\bSMS\\b|Text Message', text, re.I)), bool(re.search(r'\\bSubject\\b', text, re.I))])\n            if markers >= 2 and len(text) > len(best_text):\n                best_text = text\n        if not best_text:\n            return 0.0, \"No recognizable outreach template content found.\"\n        score = 0.0\n        # Placeholders\n        if re.search(r'\\[(?:Client|Associate|Store|Phone|Address)[^\\]]*\\]', best_text, re.I) or re.search(r'\\{(?:Client|Associate|Store)[^\\}]*\\}', best_text, re.I):\n            score += 0.15\n        # Email and SMS sections\n        if re.search(r'\\bEmail\\b', best_text, re.I):\n            score += 0.1\n        if re.search(r'\\bSMS\\b|Text Message', best_text, re.I):\n            score += 0.1\n        # Subject options (plural or multiple lines)\n        subj_hits = re.findall(r'(?i)subject', best_text)\n        if len(subj_hits) >= 1:\n            score += 0.05\n        # Booking/CTA words and link/phone\n        if re.search(r'\\bbook|appointment|visit|rsvp\\b', best_text, re.I):\n            score += 0.05\n        if re.search(r'https?://', best_text, re.I) or re.search(r'\\b\\+?\\d[\\d\\s\\-()]{6,}\\b', best_text):\n            score += 0.05\n        # Opt-out line for SMS\n        if re.search(r'opt\\s*out|reply\\s*stop', best_text, re.I):\n            score += 0.1\n        return min(score, 0.5), \"Template markers and CTA evaluated.\"\n    except Exception as e:\n        return 0.0, f\"Error during template token/CTA check: {e}\""}, {"type": "llm_judge", "name": "Collection Authenticity and Single-Brand Consistency", "description": "Confirm all look slides depict a single brand\u2019s Resort 2025 collection; sources plausibly point to official materials.", "weight": 1.5, "judge_prompt": "Review the presentation PDF. Judge whether ALL look slides plausibly belong to ONE luxury brand and the SAME Resort 2025 collection. Signs of correctness: consistent brand marks/naming across slides, images that appear from the same lookbook/editorial, and a source/URL that plausibly references the brand\u2019s official site. Also verify that the outreach template references the same brand and the term \"Resort 2025\".\n\nScoring:\n- 1.5: Strong evidence of single-brand, one Resort 2025 collection; sources/URLs look official and consistent; template references match the brand/collection.\n- 1.0: Likely single-brand and Resort 2025; minor ambiguity in one slide or source but overall consistent; template references align.\n- 0.5: Mixed indicators; some slides/wording suggest inconsistencies or unofficial sources; template partially aligns.\n- 0.0: Multiple brands/collections mixed or sources clearly unofficial; template doesn\u2019t match.", "expectation": "A coherent single-brand Resort 2025 story with plausible official sourcing across slides and matching references in the template."}, {"type": "llm_judge", "name": "Look Slide Content Validity", "description": "Ensure each look slide includes image(s), Key Pieces (3\u20136 items), Styling Notes, and Occasion/Use Case, with items that are plausible fashion pieces.", "weight": 1.5, "judge_prompt": "Inspect each look slide in the PDF. For each slide, verify: (1) there is at least one image; (2) there is a Key Pieces list with 3\u20136 plausible fashion items (e.g., dress, blazer, sandals, tote; no nonsense terms); (3) a short Styling Notes section; (4) an Occasion/Use Case note. Consider whether the items on a slide make thematic sense together.\n\nScoring:\n- 1.5: All slides satisfy all four content elements with plausible items and coherent themes.\n- 1.0: One slide has a minor shortcoming (e.g., only 2 key items or unclear styling note) but others are solid.\n- 0.5: Multiple slides have missing elements or implausible items.\n- 0.0: Most slides lack required elements or items are not credible fashion pieces.", "expectation": "Each look slide is visually present, lists 3\u20136 realistic items, has styling notes, and an occasion."}, {"type": "llm_judge", "name": "Outreach Template Completeness and Clarity", "description": "Evaluate that the Email and SMS templates include subject options, personalization, clear CTA with availability, and compliance (opt-out).", "weight": 1.5, "judge_prompt": "Review the outreach template document. Judge whether it includes: (a) two or more Email subject options; (b) a greeting with a client name placeholder; (c) a personalized hook referencing the brand and \"Resort 2025\"; (d) a clear booking CTA offering at least two date/time options or an easy link; (e) associate signature with placeholders; (f) an SMS variant that is concise, references the brand and Resort 2025, includes a booking link/phone, and an opt-out line.\n\nScoring:\n- 1.5: All elements present and clearly labeled; easy to customize.\n- 1.0: One element weak or missing but overall usable.\n- 0.5: Multiple elements missing; customization unclear.\n- 0.0: Lacks core elements (no subject options, no CTA, no SMS or no opt-out).", "expectation": "A ready-to-use Email + SMS template with clear placeholders, CTA, and opt-out."}, {"type": "llm_judge", "name": "Cross-Reference: Brand and Collection Alignment", "description": "Check that brand name and Resort 2025 are consistent between the presentation and the outreach template.", "weight": 1.5, "judge_prompt": "Compare the presentation and the outreach template. Do they reference the same brand and \"Resort 2025\" throughout? Minor variations in punctuation/capitalization are acceptable.\n\nScoring:\n- 1.5: Clear, consistent brand and Resort 2025 references in both deliverables.\n- 1.0: Mostly consistent with minor omissions in one place.\n- 0.5: Intermittent or ambiguous references.\n- 0.0: Brand/collection differ between files or missing in one file.", "expectation": "Same luxury brand and Resort 2025 consistently referenced across both files."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism", "description": "Holistic LLM assessment of merchandising quality, luxury tone, and sales effectiveness for team deployment.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Visual Merchandising Quality", "description": "Assess whether looks feel cohesive, on-season for Resort 2025, and curated to luxury standards.", "weight": 1.5, "judge_prompt": "Evaluate the overall curation across the look slides: color stories, fabric/texture harmony, occasion suitability for Resort season (resort, cruise, warm-weather travel), and luxury presentation standards. Consider if the sequence of looks feels intentional and balanced.\n\nScoring:\n- 1.5: Highly cohesive, seasonally spot-on, and premium in presentation.\n- 1.0: Generally cohesive with minor inconsistencies.\n- 0.5: Mixed curation; some looks feel off-season or mismatched.\n- 0.0: Poor curation; not suitable for a luxury boutique clientele.", "expectation": "Polished, coherent curation suitable for luxury clients."}, {"type": "llm_judge", "name": "Luxury Tone and Brand Alignment in Outreach", "description": "Judge whether writing matches a refined luxury tone without discounting language, correctly reflecting the brand.", "weight": 1.5, "judge_prompt": "Read the outreach templates (Email + SMS). Assess tone (refined, warm, invitation-led), avoidance of discount/promo language, correct use of brand voice, and appropriateness for high-net-worth clients. Minor localization differences are fine.\n\nScoring:\n- 1.5: Exemplary luxury tone and brand alignment throughout; no discounting language.\n- 1.0: Mostly luxury-appropriate; minor awkwardness.\n- 0.5: Noticeable mismatches or occasional promotional tone.\n- 0.0: Inappropriate tone for luxury clients (pushy, discount-heavy, generic).", "expectation": "Refined, brand-true tone across Email and SMS."}, {"type": "llm_judge", "name": "Sales Effectiveness (CTA, Friction, Clarity)", "description": "Evaluate how effectively the templates drive booking: clarity of CTA, low friction, and next steps.", "weight": 1.5, "judge_prompt": "Assess the clarity and strength of the appointment CTA (e.g., offering time windows, direct link/phone), reduction of friction (simple reply options), and clarity of next steps (confirmation, concierge help). Consider whether SMS is concise yet effective.\n\nScoring:\n- 1.5: Clear, compelling CTA with minimal friction and obvious next steps.\n- 1.0: Usable CTA but with minor friction or ambiguity.\n- 0.5: Weak CTA; unclear steps.\n- 0.0: No clear CTA or confusing call to action.", "expectation": "Clear, low-friction booking path for clients."}, {"type": "llm_judge", "name": "Team Reusability and Clarity of Placeholders", "description": "Determine if templates are easy for new staff to personalize and reuse.", "weight": 1.5, "judge_prompt": "Judge whether placeholders are clearly indicated (e.g., [Client Name], [Associate Name], [Date Option 1/2]), any brief usage notes are provided, and the structure is straightforward to customize. Overly complex or ambiguous placeholders reduce reusability.\n\nScoring:\n- 1.5: Highly reusable with clear placeholders and simple customization.\n- 1.0: Generally reusable with minor ambiguities.\n- 0.5: Difficult to personalize; placeholder scheme unclear.\n- 0.0: Not practically reusable by new staff.", "expectation": "Simple, clear placeholder scheme and structure that new staff can use immediately."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "cecac8f9-8203-4ebd-ad49-54436a8c4171", "rubric": {"category_name": "Retail Black Friday Preparation and Team Launch (UK Store, Sept\u2013Nov 2024)", "rationale": "This rubric enforces a self-documenting, two-deliverable output: a PDF 8-week preparation plan and a PDF team launch deck. Stage 1 strictly gates on document shape and section presence. Stage 2 verifies correctness and consistency (mix of code and LLM rules), emphasizing that plan objectives align with the deck and that the plan truly spans eight weeks leading into Black Friday 2024 (UK context). Stage 3 assesses professional quality and usefulness for store leadership and frontline teams.", "max_total_score": 22.0, "stages": [{"name": "Stage 1 \u2013 Shape Enforcement Gate (Documents Only)", "description": "Gate: Verify BOTH deliverables exist as PDFs and follow the mandated section/slide structure. LLM-only checks; failure zeros the category.", "is_required": true, "max_points": 2.0, "min_score_to_pass": 1.6, "rules": [{"type": "llm_judge", "name": "Preparation Plan PDF: Format and Structure", "description": "Verify the 8-week preparation plan is delivered as a PDF with required sections and week-by-week bullets.", "weight": 1.0, "judge_prompt": "You are checking ONLY structure/format, not content quality. Review the candidate outputs and confirm the presence of a dedicated PDF for the 8-week preparation plan with the following structure.\n\nFormat requirements:\n- Must be a PDF (not DOCX/Excel/plain text)\n- Professional document formatting with clear headings\n- Preferably 2+ pages (not required to score; do not fail solely on page count)\n\nRequired sections and elements (be flexible with exact header wording):\n1) Title and context on first page referencing Black Friday 2024 and UK store context\n2) A top-level section labeled \u201cStrategic Objectives\u201d (or similar like \u201cObjectives,\u201d \u201cSuccess Metrics,\u201d \u201cTargets\u201d), placed before the weekly plan\n3) A clear \u201c8-Week Preparation Plan\u201d section with sub-headers and bullet lists for each of: Week 1, Week 2, Week 3, Week 4, Week 5, Week 6, Week 7, Week 8\n   - Each week must have at least 2 bullet action items (flexible phrasing)\n4) A brief \u201cBlack Friday Weekend Operations\u201d or \u201cEvent Weekend\u201d section (can be short) outlining day-of focus areas (e.g., staffing, merchandising, POS readiness)\n\nOptional but do NOT penalize if missing (only use for tie-breaks):\n- A simple appendix/checklist or references section\n\nScoring (structure only):\n- 1.0: All required elements present (items 1\u20134) and in a coherent order (Objectives before weekly plan)\n- 0.7: Missing exactly one required element OR weeks section present but fewer than 2 bullets for up to 2 weeks\n- 0.4: Weeks section present but missing 2+ week sub-headers OR objectives missing\n- 0.0: Not a PDF OR core structure missing (no Strategic Objectives OR fewer than 6 week sub-headers)\n\nOutput a score only. Do not judge content correctness.", "expectation": "A standalone PDF with Strategic Objectives up front and a clearly labeled 8-week plan (Week 1\u2013Week 8) using bullets, plus a short event-weekend section."}, {"type": "llm_judge", "name": "Team Launch Deck PDF: Slide Structure", "description": "Verify the Black Friday Team Launch deck is delivered as a PDF with key slides present for day-of instruction.", "weight": 1.0, "judge_prompt": "You are checking ONLY structure/format, not content quality. Confirm there is a separate PDF that is clearly a launch deck (slide-like pages) including the following key slides/sections (flexible names acceptable):\n\nFormat requirements:\n- Must be a PDF deck (slide-like pages; landscape orientation preferred but not required)\n\nRequired slides/sections:\n1) Title slide referencing Black Friday 2024\n2) \u201cPerformance Goals\u201d slide(s) summarizing measurable targets (aligning with the plan\u2019s Strategic Objectives)\n3) \u201cPromotional Offers Overview\u201d slide(s) describing key promos for the weekend\n4) \u201cExecution Priorities\u201d slide(s) for floor operations (e.g., queue management, recovery, stock, POS readiness)\n5) \u201cBriefing cadence\u201d for i) Black Friday morning huddle, ii) later arrivals, iii) throughout the weekend (this can be one combined slide if clearly stated)\n6) \u201cEscalation & Safety\u201d or \u201cIssue Handling\u201d slide (e.g., manager on duty, refunds/exchanges, safety)\n7) \u201cReal-time Metrics/Scoreboard\u201d or similar slide indicating what to track during the day\n\nOptional (do not penalize if missing):\n- FAQ, visual planograms, images/graphics\n\nScoring (structure only):\n- 1.0: All 7 required slide types clearly present\n- 0.7: Missing exactly one of the required slide types\n- 0.4: Missing two required slide types\n- 0.0: Not a PDF OR missing three or more required slide types\n\nOutput a score only. Do not judge content correctness.", "expectation": "A PDF deck with clear slides covering goals, promotions, execution priorities, briefing cadence, escalation/safety, and live metrics tracking."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness and Cross-Verification", "description": "Verify the plan truly spans 8 weeks into Black Friday 2024, that deck and plan align on goals and offers, and that UK retail context is reflected. Uses a mix of code and LLM rules.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Both Deliverables Present and Classified", "description": "Programmatically confirm two document outputs exist and can be classified as the plan and the deck based on textual signals.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    # Return a normalized score in [0,1]\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0, \"No outputs found.\"\n\n    docs = [r for r in outputs if getattr(r, 'is_document', False)]\n    if len(docs) < 2:\n        return 0.0, f\"Found {len(docs)} document(s); expected at least 2 PDFs.\"\n\n    def read_text(res):\n        try:\n            if hasattr(res, 'file_extension') and str(res.file_extension).lower().endswith('.pdf'):\n                return context.files.read_pdf_text(res.id)\n        except Exception:\n            pass\n        try:\n            return context.files.read_docx_text(res.id)\n        except Exception:\n            pass\n        try:\n            return context.files.read_text(res.id)\n        except Exception:\n            return \"\"\n\n    plan_score = 0\n    deck_score = 0\n    plan_candidate = None\n    deck_candidate = None\n\n    for d in docs:\n        text = (read_text(d) or \"\").lower()\n        if not text:\n            continue\n        # Plan markers\n        has_objectives = any(k in text for k in [\"strategic objectives\", \"objectives\", \"success metrics\", \"targets\", \"kpis\"]) \n        weeks_hits = sum(1 for i in range(1,9) if re.search(rf\"\\bweek\\s*[-:]?\\s*0?{i}\\b\", text))\n        has_weeks = weeks_hits >= 6\n        has_event_ops = any(k in text for k in [\"event weekend\", \"black friday weekend\", \"operations\", \"ops checklist\", \"day-of\", \"day of\"])\n        if has_objectives and has_weeks:\n            plan_score = max(plan_score, 1)\n            plan_candidate = d\n        # Deck markers\n        deck_signals = 0\n        deck_signals += 1 if (\"title\" in text or \"black friday 2024\" in text or \"team launch\" in text) else 0\n        deck_signals += 1 if any(k in text for k in [\"performance goals\", \"goals\", \"targets\"]) else 0\n        deck_signals += 1 if any(k in text for k in [\"promotional offers\", \"promotions\", \"promo\", \"offer overview\"]) else 0\n        deck_signals += 1 if any(k in text for k in [\"execution priorities\", \"priorities\", \"floor execution\"]) else 0\n        deck_signals += 1 if any(k in text for k in [\"morning huddle\", \"later arrivals\", \"throughout the day\", \"weekend briefing\", \"briefing cadence\"]) else 0\n        deck_signals += 1 if any(k in text for k in [\"escalation\", \"safety\", \"issue handling\", \"manager on duty\"]) else 0\n        deck_signals += 1 if any(k in text for k in [\"real-time\", \"scoreboard\", \"live metrics\", \"track during the day\"]) else 0\n        if deck_signals >= 5:\n            deck_score = max(deck_score, 1)\n            deck_candidate = d\n\n    if plan_candidate and deck_candidate:\n        return 1.0, \"Plan and deck detected.\"\n    if len(docs) >= 2 and (plan_candidate or deck_candidate):\n        return 0.6, \"Two documents present; only one classified confidently.\"\n    if len(docs) >= 2:\n        return 0.4, \"Two documents present but classification inconclusive.\"\n    return 0.0, \"Fewer than two documents or unreadable text.\""}, {"type": "code", "name": "Eight-Week Coverage in Plan", "description": "Confirm that all Week 1\u2013Week 8 labels are present in the plan text.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0\n    def read_text(res):\n        try:\n            return context.files.read_pdf_text(res.id)\n        except Exception:\n            try:\n                return context.files.read_docx_text(res.id)\n            except Exception:\n                try:\n                    return context.files.read_text(res.id)\n                except Exception:\n                    return \"\"\n    best = 0.0\n    for r in outputs:\n        if not getattr(r, 'is_document', False):\n            continue\n        text = (read_text(r) or \"\").lower()\n        if not text:\n            continue\n        # Count week labels 1..8\n        hits = 0\n        for i in range(1, 9):\n            if re.search(rf\"\\bweek\\s*[-:]?\\s*0?{i}\\b\", text):\n                hits += 1\n        best = max(best, hits / 8.0)\n    return best"}, {"type": "code", "name": "Goal Number Consistency (Plan vs Deck)", "description": "Extract percentages (\u00a3, %) tied to goals/targets/KPIs in both documents and check overlap.", "weight": 0.8, "code": "import re\n\nGOAL_KEYS = re.compile(r\"\\b(goal|goals|target|targets|kpi|kpis|objective|objectives|performance)\\b\", re.I)\nMONEY = re.compile(r\"\u00a3\\s*([0-9]{1,3}(?:,[0-9]{3})*(?:\\.[0-9]{1,2})?|[0-9]+(?:\\.[0-9]{1,2})?)\")\nPCT = re.compile(r\"([0-9]{1,3}(?:\\.[0-9]+)?)\\s*%\")\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0, \"No outputs.\"\n\n    def read_text(res):\n        try:\n            return context.files.read_pdf_text(res.id)\n        except Exception:\n            try:\n                return context.files.read_docx_text(res.id)\n            except Exception:\n                try:\n                    return context.files.read_text(res.id)\n                except Exception:\n                    return \"\"\n\n    texts = []\n    for r in outputs:\n        if getattr(r, 'is_document', False):\n            t = read_text(r) or \"\"\n            if t:\n                texts.append(t)\n    if len(texts) < 2:\n        return 0.0, \"Fewer than two readable documents.\"\n\n    # Heuristic: extract numbers from lines containing goal-related words\n    def extract_numbers(t):\n        lines = t.splitlines()\n        money_vals = []\n        pct_vals = []\n        for ln in lines:\n            if GOAL_KEYS.search(ln):\n                for m in MONEY.findall(ln):\n                    try:\n                        v = float(str(m).replace(\",\", \"\"))\n                        money_vals.append(v)\n                    except Exception:\n                        pass\n                for p in PCT.findall(ln):\n                    try:\n                        pct_vals.append(float(p))\n                    except Exception:\n                        pass\n        return money_vals, pct_vals\n\n    m_sets = []\n    p_sets = []\n    for t in texts:\n        m, p = extract_numbers(t)\n        m_sets.append(set(round(v, 2) for v in m))\n        p_sets.append(set(round(v, 2) for v in p))\n\n    # Choose two largest sets as plan/deck proxies\n    m_sets.sort(key=lambda s: -len(s))\n    p_sets.sort(key=lambda s: -len(s))\n    mA = m_sets[0] if m_sets else set()\n    mB = m_sets[1] if len(m_sets) > 1 else set()\n    pA = p_sets[0] if p_sets else set()\n    pB = p_sets[1] if len(p_sets) > 1 else set()\n\n    # Compute overlap ratios (handle empty cases)\n    def overlap(a, b):\n        if not a or not b:\n            return 0.0\n        inter = len(a & b)\n        denom = max(1, min(len(a), len(b)))\n        return inter / denom\n\n    money_overlap = overlap(mA, mB)\n    pct_overlap = overlap(pA, pB)\n\n    # Combine with light weighting\n    if money_overlap == 0 and pct_overlap == 0:\n        # If both docs have no goal lines at all, return low partial credit\n        any_nums = (len(mA)|len(mB)|len(pA)|len(pB)) > 0\n        return (0.2 if not any_nums else 0.0), f\"Money overlap={money_overlap:.2f}, Percent overlap={pct_overlap:.2f}\"\n\n    score = 0.6 * pct_overlap + 0.4 * money_overlap\n    return max(0.0, min(1.0, score)), f\"Money overlap={money_overlap:.2f}, Percent overlap={pct_overlap:.2f}\""}, {"type": "code", "name": "UK Context Signals and Date Proximity", "description": "Check for UK-specific context and Black Friday 2024 timing signals across documents.", "weight": 0.8, "code": "import re\n\nUK_TOKENS = [\"\u00a3\", \"vat\", \"queue\", \"queuing\", \"till\", \"click & collect\", \"click and collect\", \"gmt\", \"bst\", \"uk\"]\nBF_TOKENS = [\"black friday 2024\", \"29 november\", \"29/11\", \"29 nov\", \"nov 29\"]\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0, \"No outputs.\"\n\n    def read_text(res):\n        try:\n            return (context.files.read_pdf_text(res.id) or \"\").lower()\n        except Exception:\n            try:\n                return (context.files.read_docx_text(res.id) or \"\").lower()\n            except Exception:\n                try:\n                    return (context.files.read_text(res.id) or \"\").lower()\n                except Exception:\n                    return \"\"\n\n    texts = []\n    for r in outputs:\n        if getattr(r, 'is_document', False):\n            t = read_text(r)\n            if t:\n                texts.append(t)\n    if not texts:\n        return 0.0, \"No readable documents.\"\n\n    joined = \"\\n\".join(texts)\n    uk_hits = sum(1 for tok in UK_TOKENS if tok in joined)\n    bf_hits = sum(1 for tok in BF_TOKENS if tok in joined)\n\n    # Score: require at least one BF timing signal and at least one UK token for non-zero\n    if uk_hits == 0 and bf_hits == 0:\n        return 0.0, \"No UK or Black Friday 2024 timing signals found.\"\n    # Normalize with diminishing returns\n    score = min(1.0, (uk_hits/4.0) * 0.6 + (bf_hits/2.0) * 0.4)\n    return score, f\"UK hits={uk_hits}, BF hits={bf_hits}\""}, {"type": "llm_judge", "name": "Strategic Objectives: Specific and Measurable", "description": "Plan\u2019s Strategic Objectives are measurable (KPIs/targets) and define success for Black Friday.", "weight": 3.0, "judge_prompt": "Evaluate the preparation plan PDF\u2019s \u201cStrategic Objectives\u201d section (or equivalent). Check:\n- Are objectives clearly measurable (e.g., sales \u00a3, conversion %, AOV \u00a3, UPT, attach rate, NPS, queue time targets)?\n- Do they explicitly pertain to Black Friday 2024 performance at this store (UK context acceptable)?\n- Are baselines or target deltas implied or stated (e.g., +X% vs 2023)?\n\nScoring:\n- 3.0: Objectives are specific, measurable, time-bound to Black Friday 2024; at least 3 relevant KPIs with numeric targets\n- 2.0: Mostly measurable with 2 numeric KPIs; timing/context clear\n- 1.0: Vague or only 1 numeric KPI\n- 0.0: No clear objectives/metrics\n\nScore only; do not evaluate design aesthetics.", "expectation": "A concise objectives section with numeric KPIs aligned to Black Friday 2024."}, {"type": "llm_judge", "name": "Operational Coverage and Sequencing (8 Weeks)", "description": "Plan\u2019s weekly bullets cover critical operational domains in a logical sequence.", "weight": 3.0, "judge_prompt": "Review the 8-week plan bullets. Check coverage of these domains and that sequencing is realistic leading into Black Friday:\n- Staffing & training (recruitment, rota, shift coverage, briefings)\n- Inventory & receiving (allocations, top-ups, backroom prep)\n- Merchandising & signage (planograms, price labels, promo signage)\n- Pricing & promotions readiness (POS configs, price checks)\n- POS/Payments readiness (till testing, receipt messaging, refunds policy)\n- Omnichannel/Click & Collect/Ship-from-store (staging, SLA)\n- Customer experience & queue management (line control, fitting rooms)\n- Risk/safety/compliance (crowd control, slip/trip, theft, cash handling)\n- Communications & escalation (manager on duty, radios, handovers)\n\nScoring:\n- 3.0: Covers 7\u20139 domains with clear sequencing week by week\n- 2.0: Covers 5\u20136 domains adequately\n- 1.0: Covers 3\u20134 domains or sequencing unclear\n- 0.0: Covers 0\u20132 domains or largely off-topic\n\nScore only; do not judge visual design.", "expectation": "A realistic, end-to-end operational ramp that touches the majority of critical domains."}, {"type": "llm_judge", "name": "Plan\u2013Deck Alignment on Goals and Offers", "description": "Deck reiterates the plan\u2019s goals and promotional offers without contradictions.", "weight": 2.0, "judge_prompt": "Compare the plan PDF and the deck PDF:\n- Do the deck\u2019s stated performance goals match the plan\u2019s Strategic Objectives (same KPIs and numeric targets, allowing minor rounding)?\n- Are the promotional offers described in the deck consistent with any promo references in the plan (names, discounts, exclusions)?\n- No material contradictions between documents.\n\nScoring:\n- 2.0: Clear alignment on KPIs and targets; offers consistent; no contradictions\n- 1.0: Mostly aligned; minor omissions or small inconsistencies\n- 0.0: Significant misalignment or contradictions\n\nScore only.", "expectation": "Deck faithfully echoes the plan\u2019s goals and offers."}, {"type": "llm_judge", "name": "Deck Day-Of Usability", "description": "Deck provides actionable, time-of-day guidance for i) morning huddle, ii) later arrivals, iii) weekend.", "weight": 1.8, "judge_prompt": "Examine the deck PDF for clear guidance to be used:\n- At opening/morning huddle (who/what/where, immediate priorities)\n- For team members arriving later (catch-up instructions)\n- Throughout the weekend (cadence, checkpoints, scoreboard updates)\n\nScoring:\n- 1.8: Explicit, actionable guidance for all three situations\n- 1.0: Two situations covered well; one is weak/missing\n- 0.4: Only one situation covered\n- 0.0: No practical day-of guidance\n\nScore only.", "expectation": "Concrete, practical guidance for each arrival/time scenario."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Quality and Professionalism", "description": "Holistic quality: clarity, professionalism, strategic value, and team readiness.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Readability", "description": "Assess formatting, clarity, and readability of both documents for a retail audience.", "weight": 2.5, "judge_prompt": "Evaluate overall presentation of both PDFs:\n- Clear headings, logical flow, and scannable bullets\n- Readable typography/contrast; slides not overcrowded\n- Appropriate tone for UK retail store teams\n\nScoring:\n- 2.5: Highly professional and easy to consume\n- 1.5: Generally clear with minor issues\n- 0.7: Mixed clarity; several formatting issues\n- 0.0: Hard to read or unprofessional", "expectation": "Clean, concise, and readable documents suitable for frontline teams and leaders."}, {"type": "llm_judge", "name": "Actionability and Ownership", "description": "Evaluate whether action items are actionable and, where appropriate, indicate ownership/timeframes.", "weight": 2.0, "judge_prompt": "Review whether the weekly bullets and deck instructions:\n- Use action-oriented language with clear next steps\n- Indicate owners/roles or timeframes where appropriate (e.g., MOD, Dept Leads)\n- Help leaders manage execution without ambiguity\n\nScoring:\n- 2.0: Highly actionable; roles/timeframes mostly clear\n- 1.0: Moderately actionable; some ambiguity\n- 0.0: Vague or non-actionable", "expectation": "Bullets that drive execution with minimal interpretation required."}, {"type": "llm_judge", "name": "Customer Experience and Safety Emphasis", "description": "Weight on CX, safety, and accessibility during the event.", "weight": 1.8, "judge_prompt": "Assess how well the plan and deck emphasize:\n- Customer experience (queue, fitting room flow, recovery)\n- Safety/compliance (crowd control, incident handling, cash/security)\n- Accessibility and inclusivity considerations\n\nScoring:\n- 1.8: Strong, integrated emphasis with clear practices\n- 1.0: Some emphasis but uneven\n- 0.0: Little to no emphasis", "expectation": "Clear practices to protect CX and safety under peak traffic."}, {"type": "llm_judge", "name": "Team Readiness and Communications", "description": "Evaluate training, briefings, escalation paths, and morale/wellbeing considerations.", "weight": 1.7, "judge_prompt": "Check for elements that ensure the team is truly ready:\n- Pre-event training/refreshers and job aids\n- Briefing cadence, handovers, and escalation paths\n- Break planning and wellbeing/morale notes\n\nScoring:\n- 1.7: Comprehensive and practical\n- 1.0: Adequate but some gaps\n- 0.0: Sparse or missing", "expectation": "A people-first plan with clear comms and support structures."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "b7a5912e-0e63-41f5-8c22-9cdb8f46ab01", "rubric": {"category_name": "Daily Closed Operational Report (Car Rental)", "rationale": "This rubric enforces a self-documenting Excel deliverable for a daily closed operational report in a car rental airport location. Stage 1 is a strict, LLM-only format gate ensuring the workbook has the exact sheets and structures required to make verification trivial. Stage 2 mixes lightweight code checks (deterministic arithmetic and cross-sheet totals) with higher-weight LLM checks for cross-referenced correctness and reasonableness. Stage 3 assesses overall professional quality, insightfulness for management/sales, and usability. Code rules are intentionally low-weight relative to LLM rules to reflect their narrower scope, while still validating key computations.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Structure/Format Gate (LLM only)", "description": "Gate that requires the output to be a well-structured Excel workbook with exact sheets and clearly labeled sections/tables so verification is possible.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.5, "rules": [{"type": "llm_judge", "name": "Structured Workbook Format Requirement", "description": "Checks that the output is a properly structured Excel workbook with required sheets and sections for the daily closed operational report.", "weight": 6.0, "judge_prompt": "You are evaluating whether the candidate output is a properly structured Excel workbook for a Daily Closed Operational Report for June 27, 2025 (airport car rental location). Only check structure/presence\u2014not the correctness of numbers.\n\nFormat requirements:\n- The output MUST be an Excel workbook (.xlsx). File name should clearly indicate the date (acceptable variations like: \"Daily Closed Operational Report June 27, 2025.xlsx\" or close variants).\n\nRequired sheets and sections (be flexible with exact wording but not with intent):\n\n1) Sheet: \"Daily Activity & Key Trends\" (or very close, e.g., \"Daily Activity and Key Trends\", \"Key Trends & Daily Activity\")\n   Must include clearly labeled metrics (as a two-column table or a labeled section) for ALL of the following:\n   - Total number of closed rentals (or \"Total Closed Rentals\")\n   - Total number of rental days (or \"Total Rental Days\")\n   - Average Length Of Rental (LOR)\n   - Total revenue (currency)\n   - Average revenue per rental (currency)\n   - Average daily rate (ADR) (currency per day)\n   - Category Utilization rate (% of rentals per vehicle category) \u2014 may be a small table or list in this sheet or referenced to a dedicated utilization section; ensure presence somewhere in the workbook.\n\n2) Sheet: \"Category Breakdown\" (or close variant like \"Breakdown by Category\", \"Vehicle Category Breakdown\") with a table containing columns that cover:\n   - Vehicle Category\n   - Total number of rentals\n   - Total rental days\n   - Total revenue\n   - Average revenue per rental\n   - Average length of rental (days)\n   - Average revenue per day\n\n3) Sheet: \"Booking Source Summary\" (or variants like \"Booking Sources\", \"Channel Summary\") with a table that includes:\n   - Booking Source (e.g., Website, Expedia, Call Center, etc.)\n   - Total revenue per source\n   - Preferably includes total rentals per source (optional but encouraged)\n\n4) Sheet: \"Payment Method Summary\" with a table that includes:\n   - Payment Method (e.g., Credit Card, Debit Card, etc.)\n   - Total revenue collected by payment method\n\n5) Sheet: \"Observations\" (or \"Insights\", \"Management Notes\") containing brief, insightful observations relevant to management and sales. Must include at least 3 bullet points or short paragraphs focusing on rental trends, payment methods, booking sources, or category utilization.\n\nScoring:\n- 6.0: Excel format and all 5 sheets present with clearly labeled sections/tables meeting the above requirements. Category utilization is explicitly present somewhere (either in Key Trends or a dedicated table), and all required metrics are visible.\n- 5.0: Excel format and all sheets present, but one minor labeling/placement deviation (e.g., utilization shown only within Category Breakdown but clearly marked) or one minor metric label ambiguity.\n- 4.5: Excel format with all sheets present; one required metric missing OR one required column missing in a breakdown/summary table, but the intent is still verifiable.\n- 3.0: Excel format but missing 1 required sheet OR multiple key metrics/columns missing, making verification harder.\n- 1.0: Excel format but only 1\u20132 relevant sheets with sparse content.\n- 0.0: Not an Excel workbook OR totally wrong structure.\n\nDo not evaluate numerical correctness or style quality\u2014only presence/structure and whether the workbook is organized to enable verification.", "expectation": "A single .xlsx file named for the date, with five sheets: Daily Activity & Key Trends (with all listed metrics), Category Breakdown (complete metric columns), Booking Source Summary (with at least revenue), Payment Method Summary (with revenue), and Observations (>=3 insights). Category utilization is present as a percentage by category."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Cross-Checks", "description": "Verifies numerical consistency, arithmetic identities, and cross-referenced claims between sheets; combines deterministic code checks with higher-level LLM checks.", "is_required": false, "max_points": 9.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Cross-Sheet Totals and Utilization Coherence", "description": "Check that sums from Category Breakdown match the overall totals in Key Trends; validate utilization shares if present.", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheet_names = [s for s in xls.sheet_names]\n        def find_sheet(candidates):\n            cand_l = [c.lower() for c in candidates]\n            for s in sheet_names:\n                sl = s.lower()\n                if any(all(token in sl for token in name.split()) for name in cand_l):\n                    return s\n            return None\n        # Locate sheets (flexible matching)\n        key_trends_sheet = find_sheet([\"daily activity & key trends\", \"daily activity and key trends\", \"key trends\", \"daily activity\"])\n        cat_sheet = find_sheet([\"category breakdown\", \"breakdown by category\", \"vehicle category breakdown\", \"category by\"])\n        if not key_trends_sheet or not cat_sheet:\n            return 0.0, \"Missing required sheets for this rule.\"\n        df_key = pd.read_excel(path, sheet_name=key_trends_sheet, header=None).replace({np.nan: None})\n        df_cat = pd.read_excel(path, sheet_name=cat_sheet).replace({np.nan: None})\n        # Helpers\n        def parse_number(x):\n            if x is None:\n                return None\n            if isinstance(x, (int, float, np.integer, np.floating)) and not pd.isna(x):\n                return float(x)\n            if isinstance(x, str):\n                s = x.strip()\n                if s == \"\":\n                    return None\n                s = s.replace(\",\", \"\")\n                s = s.replace(\"$\", \"\")\n                neg = False\n                if s.startswith(\"(\") and s.endswith(\")\"):\n                    s = s[1:-1]\n                    neg = True\n                s = s.replace(\"%\", \"\")\n                try:\n                    val = float(s)\n                    return -val if neg else val\n                except:\n                    # Try to extract first numeric pattern\n                    m = re.search(r\"-?\\d+(?:\\.\\d+)?\", s)\n                    if m:\n                        try:\n                            return float(m.group(0))\n                        except:\n                            return None\n                    return None\n            return None\n        def find_metric_value(df, keywords):\n            # Search for a row where any cell contains the metric keywords (all must match in that cell)\n            kws = [k.lower() for k in keywords]\n            for r in range(df.shape[0]):\n                for c in range(df.shape[1]):\n                    cell = df.iat[r, c]\n                    if isinstance(cell, str):\n                        cl = cell.lower()\n                        if all(k in cl for k in kws):\n                            # Try numeric in same row (prefer adjacent cells)\n                            row_vals = [df.iat[r, cc] for cc in range(df.shape[1]) if cc != c]\n                            nums = [parse_number(v) for v in row_vals]\n                            nums = [v for v in nums if v is not None]\n                            if nums:\n                                # Return the first plausible numeric\n                                return nums[0]\n            return None\n        total_rentals = find_metric_value(df_key, [\"total\", \"rental\"])  # may pick total closed rentals\n        total_days = find_metric_value(df_key, [\"total\", \"rental\", \"day\"]) or find_metric_value(df_key, [\"total\", \"days\"]) \n        total_revenue = find_metric_value(df_key, [\"total\", \"revenue\"]) \n        # Map Category Breakdown columns\n        cols = {c.lower(): c for c in df_cat.columns}\n        def pick(col_options):\n            for k in df_cat.columns:\n                kl = str(k).lower()\n                if any(all(tok in kl for tok in opt.split()) for opt in col_options):\n                    return k\n            return None\n        col_cat = pick([\"vehicle category\", \"category\", \"car class\", \"vehicle class\"]) \n        col_rentals = pick([\"total rentals\", \"rentals\", \"rental count\", \"# rentals\"]) \n        col_days = pick([\"total rental days\", \"rental days\", \"days\"]) \n        col_rev = pick([\"total revenue\", \"revenue\"]) \n        # Basic availability check\n        needed = [col_cat, col_rentals, col_days, col_rev]\n        if any(v is None for v in needed):\n            return 0.0, \"Missing key columns in Category Breakdown.\"\n        # Clean numeric columns\n        rentals_series = df_cat[col_rentals].apply(parse_number)\n        days_series = df_cat[col_days].apply(parse_number)\n        rev_series = df_cat[col_rev].apply(parse_number)\n        # Compute sums\n        sum_rentals = float(pd.to_numeric(rentals_series, errors='coerce').fillna(0).sum())\n        sum_days = float(pd.to_numeric(days_series, errors='coerce').fillna(0).sum())\n        sum_rev = float(pd.to_numeric(rev_series, errors='coerce').fillna(0).sum())\n        checks = 0\n        passed = 0\n        # Check totals vs key trends (if available)\n        tol_ratio = 0.02  # 2%\n        if total_rentals is not None:\n            checks += 1\n            if total_rentals == 0:\n                match = (sum_rentals == 0)\n            else:\n                match = abs(sum_rentals - total_rentals) <= max(1, abs(total_rentals) * tol_ratio)\n            passed += 1 if match else 0\n        if total_days is not None:\n            checks += 1\n            if total_days == 0:\n                match = (sum_days == 0)\n            else:\n                match = abs(sum_days - total_days) <= max(1, abs(total_days) * tol_ratio)\n            passed += 1 if match else 0\n        if total_revenue is not None:\n            checks += 1\n            if total_revenue == 0:\n                match = (sum_rev == 0)\n            else:\n                match = abs(sum_rev - total_revenue) <= max(1.0, abs(total_revenue) * tol_ratio)\n            passed += 1 if match else 0\n        # Utilization check: if a utilization/share column exists OR we can compute shares\n        util_col = pick([\"utilization\", \"% of rentals\", \"share\", \"% share\", \"category utilization\"]) \n        if util_col is not None and total_rentals is not None and total_rentals > 0:\n            checks += 1\n            util_vals = pd.to_numeric(df_cat[util_col].apply(parse_number), errors='coerce')\n            util_sum = float(util_vals.fillna(0).sum())\n            # If values look like percentages (0-100 or 0-1), normalize to percent scale\n            # Heuristic: if max <= 1.5, treat as fraction; else as percent values\n            mx = util_vals.max()\n            if pd.notna(mx) and mx is not None and mx <= 1.5:\n                util_sum *= 100.0\n                util_vals = util_vals * 100.0\n            # Check sum ~100\n            if abs(util_sum - 100.0) <= 2.0:  # within 2 percentage points\n                # Optionally check a few rows for consistency with rentals/total\n                idxs = util_vals.dropna().index[:3]\n                ok = True\n                for i in idxs:\n                    r = parse_number(df_cat.loc[i, col_rentals]) or 0\n                    expected = (r / total_rentals) * 100.0 if total_rentals else 0\n                    ui = util_vals.loc[i]\n                    if abs(ui - expected) > 3.0:  # within 3pp for a few samples\n                        ok = False\n                        break\n                passed += 1 if ok else 0\n            else:\n                # Sum not near 100 \u2013 fail this subcheck\n                # still counted as a check\n                passed += 0\n        score = (passed / checks) if checks > 0 else 0.0\n        return float(max(0.0, min(1.0, score))), f\"Passed {passed}/{checks} checks.\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Averages Arithmetic Identity Checks", "description": "Validate that average metrics in Key Trends are consistent with totals: Avg Revenue per Rental \u2248 Total Revenue/Total Rentals; ADR \u2248 Total Revenue/Total Rental Days; LOR \u2248 Total Rental Days/Total Rentals.", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet.\"\n        path = context.files.get_path(output.id)\n        df = context.files.read_excel(output.id, sheet_name=0)  # not used; will load specific sheet below\n        # find key trends sheet\n        xls = pd.ExcelFile(path)\n        def find_sheet(cands):\n            for s in xls.sheet_names:\n                sl = s.lower()\n                if any(all(tok in sl for tok in c.lower().split()) for c in cands):\n                    return s\n            return None\n        kt = find_sheet([\"daily activity & key trends\", \"key trends\", \"daily activity\"])\n        if not kt:\n            return 0.0, \"Key Trends sheet not found.\"\n        dfk = pd.read_excel(path, sheet_name=kt, header=None).replace({np.nan: None})\n        def parse_number(x):\n            if x is None:\n                return None\n            if isinstance(x, (int, float, np.integer, np.floating)) and not pd.isna(x):\n                return float(x)\n            if isinstance(x, str):\n                s = x.strip().replace(\",\", \"\").replace(\"$\", \"\")\n                neg = False\n                if s.startswith(\"(\") and s.endswith(\")\"):\n                    s = s[1:-1]\n                    neg = True\n                s = s.replace(\"%\", \"\")\n                try:\n                    v = float(s)\n                    return -v if neg else v\n                except:\n                    m = re.search(r\"-?\\d+(?:\\.\\d+)?\", s)\n                    if m:\n                        try:\n                            return float(m.group(0))\n                        except:\n                            return None\n            return None\n        def find_metric(df, labels):\n            labs = [l.lower() for l in labels]\n            for r in range(df.shape[0]):\n                for c in range(df.shape[1]):\n                    v = df.iat[r, c]\n                    if isinstance(v, str):\n                        vl = v.lower()\n                        if all(l in vl for l in labs):\n                            # find numeric on same row\n                            nums = []\n                            for cc in range(df.shape[1]):\n                                if cc == c: continue\n                                p = parse_number(df.iat[r, cc])\n                                if p is not None:\n                                    nums.append(p)\n                            if nums:\n                                return nums[0]\n            return None\n        total_rentals = find_metric(dfk, [\"total\", \"rental\"]) \n        total_days = find_metric(dfk, [\"total\", \"rental\", \"day\"]) or find_metric(dfk, [\"total\", \"days\"]) \n        total_rev = find_metric(dfk, [\"total\", \"revenue\"]) \n        avg_rev_per_rental = find_metric(dfk, [\"average\", \"revenue\", \"rental\"]) or find_metric(dfk, [\"avg\", \"rev\", \"rental\"]) \n        adr = find_metric(dfk, [\"average\", \"daily\", \"rate\"]) or find_metric(dfk, [\"adr\"]) \n        lor = find_metric(dfk, [\"average\", \"length\", \"rental\"]) or find_metric(dfk, [\"lor\"]) \n        checks = 0\n        passed = 0\n        tol = 0.03  # 3%\n        # Avg revenue per rental\n        if total_rev is not None and total_rentals is not None and total_rentals != 0 and avg_rev_per_rental is not None:\n            checks += 1\n            expected = total_rev / total_rentals\n            if expected == 0:\n                ok = abs(avg_rev_per_rental) <= 0.5\n            else:\n                ok = abs(avg_rev_per_rental - expected) <= abs(expected) * tol + 1.0\n            passed += 1 if ok else 0\n        # ADR\n        if total_rev is not None and total_days is not None and total_days != 0 and adr is not None:\n            checks += 1\n            expected = total_rev / total_days\n            if expected == 0:\n                ok = abs(adr) <= 0.5\n            else:\n                ok = abs(adr - expected) <= abs(expected) * tol + 1.0\n            passed += 1 if ok else 0\n        # LOR\n        if total_days is not None and total_rentals is not None and total_rentals != 0 and lor is not None:\n            checks += 1\n            expected = total_days / total_rentals\n            if expected == 0:\n                ok = abs(lor) <= 0.1\n            else:\n                ok = abs(lor - expected) <= abs(expected) * tol + 0.05\n            passed += 1 if ok else 0\n        score = (passed / checks) if checks > 0 else 0.0\n        return float(max(0.0, min(1.0, score))), f\"Passed {passed}/{checks} identity checks.\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Channel and Payment Revenue Reconciliation", "description": "Sum of Booking Source Summary revenue and sum of Payment Method Summary revenue should each \u2248 Total Revenue in Key Trends.", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet.\"\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheets = xls.sheet_names\n        def find_sheet(cands):\n            for s in sheets:\n                sl = s.lower()\n                if any(all(tok in sl for tok in c.lower().split()) for c in cands):\n                    return s\n            return None\n        kt = find_sheet([\"daily activity & key trends\", \"key trends\", \"daily activity\"])\n        bs = find_sheet([\"booking source summary\", \"booking sources\", \"channel summary\", \"booking source\"]) \n        pm = find_sheet([\"payment method summary\", \"payment methods\", \"payments summary\"]) \n        if not kt or not bs or not pm:\n            return 0.0, \"Missing one or more required sheets.\"\n        dfk = pd.read_excel(path, sheet_name=kt, header=None).replace({np.nan: None})\n        dfb = pd.read_excel(path, sheet_name=bs).replace({np.nan: None})\n        dfp = pd.read_excel(path, sheet_name=pm).replace({np.nan: None})\n        def parse_number(x):\n            if x is None:\n                return None\n            if isinstance(x, (int, float, np.integer, np.floating)) and not pd.isna(x):\n                return float(x)\n            if isinstance(x, str):\n                s = x.strip().replace(\",\", \"\").replace(\"$\", \"\")\n                neg = False\n                if s.startswith(\"(\") and s.endswith(\")\"):\n                    s = s[1:-1]\n                    neg = True\n                s = s.replace(\"%\", \"\")\n                try:\n                    v = float(s)\n                    return -v if neg else v\n                except:\n                    m = re.search(r\"-?\\d+(?:\\.\\d+)?\", s)\n                    if m:\n                        try:\n                            return float(m.group(0))\n                        except:\n                            return None\n            return None\n        def find_metric(df, labels):\n            labs = [l.lower() for l in labels]\n            for r in range(df.shape[0]):\n                for c in range(df.shape[1]):\n                    v = df.iat[r, c]\n                    if isinstance(v, str):\n                        vl = v.lower()\n                        if all(l in vl for l in labs):\n                            nums = []\n                            for cc in range(df.shape[1]):\n                                if cc == c: continue\n                                p = parse_number(df.iat[r, cc])\n                                if p is not None:\n                                    nums.append(p)\n                            if nums:\n                                return nums[0]\n            return None\n        total_rev = find_metric(dfk, [\"total\", \"revenue\"]) \n        if total_rev is None:\n            return 0.0, \"Total revenue not found in Key Trends.\"\n        # Find revenue columns in summaries\n        def pick_rev_col(df):\n            for col in df.columns:\n                cl = str(col).lower()\n                if any(tok in cl for tok in [\"revenue\", \"amount\", \"total\", \"sales\"]):\n                    return col\n            return None\n        col_rb = pick_rev_col(dfb)\n        col_rp = pick_rev_col(dfp)\n        if col_rb is None or col_rp is None:\n            return 0.0, \"Revenue column not found in summaries.\"\n        sum_rb = float(pd.to_numeric(dfb[col_rb].apply(parse_number), errors='coerce').fillna(0).sum())\n        sum_rp = float(pd.to_numeric(dfp[col_rp].apply(parse_number), errors='coerce').fillna(0).sum())\n        tol = max(1.0, abs(total_rev) * 0.02)\n        checks = 2\n        passed = 0\n        if abs(sum_rb - total_rev) <= tol:\n            passed += 1\n        if abs(sum_rp - total_rev) <= tol:\n            passed += 1\n        score = passed / checks\n        return float(max(0.0, min(1.0, score))), f\"Passed {passed}/2 reconciliations.\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "llm_judge", "name": "Observations Align with Tables", "description": "Do the written observations reflect and not contradict the visible tables (categories, booking sources, payment methods, key metrics)?", "weight": 2.5, "judge_prompt": "Review the workbook. Focus on the Observations/Insights sheet. Check whether the claims made (e.g., which category led in revenue, notable booking source trends, which payment method dominates, spikes/dips in LOR/ADR) are consistent with the data shown in Key Trends, Category Breakdown, Booking Source Summary, and Payment Method Summary.\n\nScoring:\n- 2.5: Observations are consistent with the tables; any specific claims (top category, top channel, dominant payment method, notable LOR/ADR shifts) match the data. No contradictions detected.\n- 1.5: Mostly consistent; one minor ambiguity or slight mismatch in phrasing but no material contradiction.\n- 0.5: Some inconsistencies; at least one claim appears doubtful or not supported by the data.\n- 0.0: Observations contradict the tables in major ways or appear unrelated to the data.", "expectation": "Observations cite or reflect the top-performing category/channel/payment method and any clear trend signals without contradicting the tables."}, {"type": "llm_judge", "name": "Top Drivers Correctness", "description": "If the report highlights top category/channel/payment method or notable underperformance, verify these identifications are correct per the tables.", "weight": 2.5, "judge_prompt": "Scan the tables to identify the highest values by (a) Vehicle Category \u2014 revenue (and/or rentals), (b) Booking Source \u2014 total revenue, (c) Payment Method \u2014 total revenue. Then check any explicit mentions in the Observations (or headings/notes elsewhere) that call out these top drivers. Are the highlighted top items actually top according to the tables?\n\nScoring:\n- 2.5: All highlighted top items match the actual table tops (or the report explicitly avoids such claims). No incorrect \"top\" assertions.\n- 1.5: One item is ambiguous or very close but still reasonable; otherwise correct.\n- 0.5: One clearly incorrect top claim.\n- 0.0: Multiple incorrect top claims or misleading highlights.", "expectation": "Either correct identification of top category/channel/payment method or no explicit top claims. When present, claims should match the tables."}, {"type": "llm_judge", "name": "Date and Scope Consistency", "description": "Confirm the report is for June 27, 2025 and reflects closed rentals (not forecasts or open rentals).", "weight": 2.5, "judge_prompt": "Verify that the workbook title/file name and relevant sheet headers indicate the correct date (June 27, 2025). Also confirm the scope pertains to closed rentals for that date/location (not forecasts, open agreements, or unrelated periods). Minor wording variations are acceptable.\n\nScoring:\n- 2.5: Date is clearly June 27, 2025, and scope explicitly/clearly about closed rentals for that date.\n- 1.5: Date is present but slightly ambiguous (e.g., in a subtitle) or scope implied rather than explicit, yet nothing contradicts closed rentals for the day.\n- 0.5: Date or scope is confusing; could be interpreted as a different period or dataset.\n- 0.0: Wrong date or clearly wrong scope.", "expectation": "Workbook correctly names and frames June 27, 2025, closed rental activity for the location."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Presentation and Insight Quality", "description": "Assesses professional presentation, clarity, usefulness for management/sales, and completeness of insights.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Formatting and Readability", "description": "Evaluate formatting quality: clear titles, labeled columns, readable fonts, appropriate number formats (currency, percent), and logical layout.", "weight": 1.5, "judge_prompt": "Assess the workbook\u2019s professional presentation:\n- Are sheet titles and section headers clear?\n- Are numeric formats appropriate (currency for revenue/ADR, percent for utilization, reasonable decimals for LOR)?\n- Are tables easy to read (consistent headers, alignment, spacing)?\n- Is the layout logical for a manager (Key Trends first, then breakdowns, then summaries, then observations)?\n\nScoring:\n- 1.5: Clean, professional formatting across sheets; appropriate number formats and clear readability.\n- 1.0: Generally good; minor formatting inconsistencies.\n- 0.5: Several readability/formatting issues.\n- 0.0: Poorly formatted, hard to read.", "expectation": "Manager-ready workbook with proper currency/percent formats and clear labeling."}, {"type": "llm_judge", "name": "Insightfulness and Actionability", "description": "Quality of insights for management/sales: concise, relevant, and suggesting actions or implications.", "weight": 1.5, "judge_prompt": "Review the Observations/Insights. Judge the depth and usefulness for management and sales:\n- Do they identify meaningful trends (e.g., category mix shifts, booking channel performance, payment behavior)?\n- Are they concise and relevant?\n- Do they suggest actionable implications (e.g., adjust pricing, allocate inventory, partner promotions)?\n\nScoring:\n- 1.5: At least 3 concise, insightful, and action-oriented observations tied to the data.\n- 1.0: Useful observations but light on actionability or specificity.\n- 0.5: Generic or obvious points with little value.\n- 0.0: No real insights.", "expectation": ">=3 succinct insights with clear implications for pricing, inventory, or channel/sales strategy."}, {"type": "llm_judge", "name": "Audience Appropriateness", "description": "Tone and level tailored to management/sales teams; avoids unnecessary jargon; highlights what matters.", "weight": 1.0, "judge_prompt": "Evaluate whether the language and focus are appropriate for management/sales:\n- Avoids technical/data jargon\n- Emphasizes business impact (revenue, utilization, LOR, channel mix)\n- Clear, concise tone\n\nScoring:\n- 1.0: Well-tailored to management/sales.\n- 0.5: Mostly appropriate with minor tone issues.\n- 0.0: Not suited to the audience.", "expectation": "Business-focused, concise language targeting management/sales priorities."}, {"type": "llm_judge", "name": "Completeness and Usability", "description": "All requested components present and easy to navigate; labels help users find answers quickly.", "weight": 1.0, "judge_prompt": "Check overall completeness and usability beyond Stage 1 structure:\n- Do the sheets interrelate logically (e.g., totals footers, clear notes)?\n- Are categories, channels, and payment methods labeled consistently across sheets?\n- Is navigation intuitive for someone scanning for key answers?\n\nScoring:\n- 1.0: Fully complete with coherent navigation and consistent labels.\n- 0.5: Minor gaps or inconsistencies but overall usable.\n- 0.0: Hard to navigate or noticeably incomplete.", "expectation": "Consistent labeling across sheets with clear navigation cues and coherent totals/notes."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "45c6237b-f9c9-4526-9a8d-6a5c404624ec", "rubric": {"category_name": "Retail Trade \u2013 First-Line Supervisors of Retail Sales Workers \u2013 Assortment Presentation (PDF)", "rationale": "This is a Mixed task (Pattern C): a document-style presentation with embedded merchandising/data elements. Stage 1 strictly enforces the PDF slide structure to make verification trivial. Stage 2 mixes LLM rules (for cross-referencing slides, images, sizing logic) with light code rules (keyword/bounds checks) at ~5x lower weight than LLM. Stage 3 assesses professional quality and executive appropriateness.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2013 Structure & Format Gate (LLM ONLY)", "description": "Gate that enforces exact output shape: a PDF presentation (<10 slides) with specific slide titles/sections, an image section, and a final summary table slide. Failure zeros the category.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.5, "rules": [{"type": "llm_judge", "name": "Format and Slide Count", "description": "Checks output is a PDF presentation with an appropriate number of slides/pages (<10).", "weight": 2.0, "judge_prompt": "You are validating the STRUCTURE ONLY (not content quality). Examine the candidate output.\nRequirements:\n- Must be a PDF presentation (not PPTX/Word/Excel/plain text).\n- Slide/page count should be 5\u20139 pages inclusive (target <10 slides as requested; allow concise decks with minimum coverage for all required sections).\nScoring:\n- 2.0: PDF detected and 5\u20139 pages total.\n- 1.0: PDF detected but page count is 3\u20134 or 10\u201311 (minor deviation but usable) OR format is acceptable presentation-like PDF with sufficient pagination ambiguity.\n- 0.5: PDF detected but page count is <3 or >11.\n- 0.0: Not a PDF.\nOnly assess presence/format and approximate slide count from the PDF pages. Do not assess content quality.", "expectation": "A PDF presentation containing 5\u20139 pages."}, {"type": "llm_judge", "name": "Required Titles and Core Sections", "description": "Checks title slide and category slides with correct titles and store mapping headers.", "weight": 3.0, "judge_prompt": "Check the presentation STRUCTURE ONLY.\nConfirm the following are present:\n1) First slide titled exactly or very close to: \"Crescent Pines Lodge & Spa\" with subtitle \u201cPurchase Assortment Spring 2022\u201d.\n2) Subsequent slides use title \u201cCrescent Pines Lodge & Spa\u201d (or very close) and include separate sections/headers for:\n   - Custom Hats (to purchase for Gift Shop)\n   - Custom Shirts (to purchase for Apparel Store)\n3) A clearly labeled slide/section for \u201cNext Season Assortment\u201d.\nScoring:\n- 3.0: All above present; titles closely match; hats and shirts clearly separated with intended store mapping indicated.\n- 2.0: Title slide correct; one section slightly mislabeled but unambiguous; store mapping implied but not explicitly stated.\n- 1.0: Title slide present; sections present but ambiguous (e.g., combined hats/shirts or missing store mapping) OR minor title deviations.\n- 0.0: Title slide missing or core sections missing.\nOnly check structure and labeling, not correctness of any numbers.", "expectation": "Clear first slide title/subtitle and distinct sections for Custom Hats (Gift Shop) and Custom Shirts (Apparel Store)."}, {"type": "llm_judge", "name": "Image Section and Final Summary Table Presence", "description": "Checks for the presence of a visual assortment section and a final slide with a summary table including SKU, price, and quantity.", "weight": 1.0, "judge_prompt": "Verify STRUCTURAL elements:\n- There is a dedicated section/slide titled (or subtitled) \u201cNext Season Assortment\u201d that includes embedded images/photos of products.\n- The final slide presents a summary table of purchase order details with columns or clearly labeled fields for: Item/SKU, Wholesale Price, and Proposed Purchase Quantity.\nScoring:\n- 1.0: Both the image section and the final summary table are present and clearly labeled.\n- 0.5: Only one of the two is present or both are present but labeling is unclear.\n- 0.0: Neither is present.\nDo not assess data correctness, only the presence and basic labeling.", "expectation": "A visible image-based assortment section and a final tabular slide summarizing SKU, price, and quantity."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness & Consistency Verification", "description": "Verify key merchandising logic and cross-references now that the structure is valid. Uses light code checks plus LLM judgment. Code rules have ~5x less weight than LLM rules.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Title and Subtitle Keywords", "description": "Detects presence of the required title and subtitle strings in the PDF text.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n    text = \"\"\n    try:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            return 0.0, \"Unable to read document text\"\n    lt = text.lower()\n    has_title = \"crescent pines lodge & spa\" in lt\n    has_sub = \"purchase assortment spring 2022\" in lt\n    score = 0.0\n    if has_title and has_sub:\n        score = 1.0\n    elif has_title or has_sub:\n        score = 0.5\n    else:\n        score = 0.0\n    return score, f\"title={has_title}, subtitle={has_sub}\""}, {"type": "code", "name": "Size Allocation Keywords (72/28 and Sizes)", "description": "Checks presence of size allocation keywords and percentages for shirts.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0, \"Unable to read document text\"\n    lt = text.lower()\n    pct72 = \"72%\" in lt or re.search(r\"\\b72\\s*percent\", lt)\n    pct28 = \"28%\" in lt or re.search(r\"\\b28\\s*percent\", lt)\n    has_popular = any(s in lt for s in [\" m \", \" m,\", \" m/\", \" l \", \" xl \", \"m/l/xl\", \"m / l / xl\"])  # heuristic\n    has_less = (\" s \" in lt or \" s,\" in lt or \" s/\" in lt) and (\" xxl\" in lt or \"xxl \" in lt)\n    hits = sum([pct72, pct28, has_popular, has_less])\n    score = hits / 4.0\n    return score, f\"72={pct72}, 28={pct28}, popular={has_popular}, less={has_less}\""}, {"type": "code", "name": "Summary Table Signals (SKU, Price, Quantity)", "description": "Heuristic check that the final slide includes a tabular summary with SKU, price, and quantity signals.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0, \"Unable to read document text\"\n    lt = text.lower()\n    has_sku = \"sku\" in lt or \"item\" in lt\n    has_price = (\"wholesale\" in lt or \"price\" in lt or \"cost\" in lt) and bool(re.search(r\"\\$\\s?\\d\", text))\n    has_qty = (\"qty\" in lt or \"quantity\" in lt)\n    hits = sum([has_sku, has_price, has_qty])\n    score = hits / 3.0\n    return score, f\"sku={has_sku}, price={has_price}, qty={has_qty}\""}, {"type": "code", "name": "Hats One-Size Assertion", "description": "Checks that Custom Hats are indicated as OS/One-Size only.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0, \"Unable to read document text\"\n    lt = text.lower()\n    mentions_hats = (\"hat\" in lt or \"hats\" in lt)\n    mentions_os = any(x in lt for x in [\"one-size\", \"one size\", \"os \", \" os\", \"os-only\", \"os only\"])\n    score = 1.0 if (mentions_hats and mentions_os) else (0.5 if mentions_os else 0.0)\n    return score, f\"hats={mentions_hats}, os={mentions_os}\""}, {"type": "llm_judge", "name": "Shirt Size Allocation Logic (72/28 even split)", "description": "LLM verifies that shirt size quantities reflect ~72% across M/L/XL (evenly) and ~28% across S/XXL (evenly), matching proposed totals.", "weight": 1.5, "judge_prompt": "Review the shirts section(s). Determine whether proposed shirt order quantities by size reflect:\n- Approximately 72% of the total per SKU allocated evenly across M, L, XL; and\n- Approximately 28% allocated evenly across S and XXL.\nIf explicit counts are provided per SKU, check that the size breakdown sums to the proposed total for that SKU. Tolerance: \u00b15 percentage points per group is acceptable.\nScoring:\n- 1.5: Clear, correct application of the 72/28 rule with even splits; sums match proposed totals when shown.\n- 1.0: Mostly correct with minor rounding deviations or partial evenness; totals generally consistent.\n- 0.5: Mentions the rule but size breakdown or totals are inconsistent/unclear.\n- 0.0: No evidence of 72/28 logic or clearly incorrect allocation.", "expectation": "Sizes follow ~72% even across M/L/XL and ~28% even across S/XXL with sums matching totals."}, {"type": "llm_judge", "name": "Store Mapping Correctness (Hats\u2192Gift Shop, Shirts\u2192Apparel Store)", "description": "LLM checks that Custom Hats are mapped to Gift Shop and Custom Shirts to Apparel Store within the presentation slides.", "weight": 1.5, "judge_prompt": "Check the slides for explicit mapping:\n- Custom Hats: to be purchased for Gift Shop.\n- Custom Shirts: to be purchased for Apparel Store.\nScoring:\n- 1.5: Both mappings are explicit and unambiguous on the relevant slides/sections.\n- 1.0: Both mappings are present but one is implied rather than explicitly stated.\n- 0.5: Only one correct mapping present.\n- 0.0: Neither mapping is present or mappings are incorrect.", "expectation": "Hats clearly labeled for Gift Shop; shirts clearly labeled for Apparel Store."}, {"type": "llm_judge", "name": "Next Season Assortment \u2013 Images Included and Labeled", "description": "LLM confirms images are embedded on the \"Next Season Assortment\" slide(s) and are visibly product shots (not placeholders).", "weight": 1.5, "judge_prompt": "Inspect the section titled (or subtitled) \u201cNext Season Assortment\u201d. Confirm:\n- Embedded product images/photos are present (not just empty placeholders or text listings).\n- The section title (or subtitle) is visible and images appear relevant to vendor assortment.\nScoring:\n- 1.5: Clear assortment slide(s) with multiple relevant product images and visible labeling.\n- 1.0: Images present but limited or labeling somewhat unclear.\n- 0.5: Minimal imagery (e.g., one image) or ambiguous relevance.\n- 0.0: No images present in the assortment section.", "expectation": "A labeled assortment section with multiple product images."}, {"type": "llm_judge", "name": "Summary Cross-Reference (Shown Items \u2194 Summary Table)", "description": "LLM checks that items shown in slides (hats/shirts) appear in the final summary table with SKU, price, and quantity.", "weight": 1.5, "judge_prompt": "Cross-check the final summary table against the products shown on earlier slides:\n- Items/SKUs depicted (e.g., specific styles/colors) should appear in the final summary table with both wholesale price and proposed purchase quantity.\n- Do not require all images to map one-to-one to SKUs if clearly grouped, but there should be a consistent mapping between the showcased assortment and the summarized purchase list.\nScoring:\n- 1.5: Strong correspondence\u2014depicted items/styles are represented in the summary table with SKU, price, and quantity.\n- 1.0: Mostly consistent with minor omissions or unclear mapping for a few items.\n- 0.5: Weak correspondence\u2014several showcased items missing or table lacks price/quantity for multiple items.\n- 0.0: Little to no correspondence between showcased items and the summary table.", "expectation": "Depicted items are represented with SKU, price, and quantity in the final summary."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Presentation Quality & Executive Readiness", "description": "Holistic evaluation of professionalism, clarity, and merchandising value for the Director of Retail audience.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Design and Consistency", "description": "Assesses slide aesthetics, consistent titling/branding, clean layouts, legible typography, and concise structure <10 slides.", "weight": 1.5, "judge_prompt": "Evaluate presentation polish:\n- Consistent use of the title \u201cCrescent Pines Lodge & Spa\u201d across slides (except the title slide which includes the subtitle).\n- Clean, readable layout; appropriate fonts, spacing, and alignment.\n- Visual coherence (colors/images/tables well integrated) and kept under 10 slides without being sparse.\nScoring:\n- 1.5: Highly professional, consistent, and polished.\n- 1.0: Generally professional with minor inconsistencies.\n- 0.5: Adequate but cluttered or inconsistent styling.\n- 0.0: Unprofessional or hard to read.", "expectation": "A polished, consistent deck suitable for senior review."}, {"type": "llm_judge", "name": "Clarity and Executive Readiness", "description": "Evaluates whether a Director-level audience can quickly grasp selections, quantities, and pricing to approve purchases.", "weight": 1.5, "judge_prompt": "Assess clarity for an executive audience:\n- Key decisions (what to buy, for which store, how much, at what price) are obvious.\n- Minimal extraneous text; clear headlines and summaries.\n- Final slide is approval-ready (concise summary table, totals/notes if included).\nScoring:\n- 1.5: Crystal clear and approval-ready.\n- 1.0: Mostly clear with minor gaps.\n- 0.5: Understandable but requires effort or missing key summaries.\n- 0.0: Unclear or confusing.", "expectation": "Concise, decision-ready content with obvious selections and pricing."}, {"type": "llm_judge", "name": "Merchandising Coherence and Assortment Rationale", "description": "Judges whether the assortment feels well-chosen for stores, covers styles/colors sensibly, and references rationale or trends.", "weight": 1.5, "judge_prompt": "Evaluate merchandising coherence:\n- Assortment appears balanced and appropriate for Gift Shop (hats) and Apparel Store (shirts).\n- Sensible variety of styles/colors; any rationale (e.g., seasonal trends) is articulated or implied by curation.\n- Sizing logic for shirts is framed as data-driven.\nScoring:\n- 1.5: Strong, well-argued assortment rationale and balance.\n- 1.0: Reasonable assortment with limited rationale.\n- 0.5: Thin or uneven assortment logic.\n- 0.0: Incoherent or arbitrary assortment.", "expectation": "Balanced, on-brand assortment with clear rationale."}, {"type": "llm_judge", "name": "Data Presentation Quality (Tables, Labels, Currency)", "description": "Assesses whether tables are readable, units/currency are labeled, and data appears internally consistent.", "weight": 1.5, "judge_prompt": "Assess data presentation quality:\n- Tables have clear headers (SKU, price/cost, quantity) and readable formatting.\n- Currency formatting is consistent (e.g., $ and two decimals or consistent style), and quantities make sense.\n- No obvious internal contradictions (e.g., totals not matching, mismatched labels).\nScoring:\n- 1.5: Clear, consistent tables with appropriate labels and formatting.\n- 1.0: Minor formatting issues but overall clear.\n- 0.5: Messy tables or unclear labeling.\n- 0.0: Confusing or contradictory data presentation.", "expectation": "Clean, labeled tables with consistent currency and quantities."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "81db15ff-ceea-4f63-a1cd-06dc88114709", "rubric": {"category_name": "Telehealth Provider Type Evaluation (NP vs PA) \u2013 Structure, Verification, and Quality", "rationale": "Pattern C (Mixed). The deliverable is an Excel workbook with a structured regulatory matrix (analytical) and a narrative recommendation (document-style) embedded in sheets. Stage 1 uses LLM-only to enforce exact shape so downstream checks are trivial. Stage 2 mixes lightweight code checks for coverage/consistency with heavier LLM plausibility and alignment checks. Stage 3 assesses professional quality, clarity, and strategic usefulness for leadership decisions.", "max_total_score": 13.0, "stages": [{"name": "Stage 1 \u2013 Shape Enforcement Gate", "description": "Require a specifically structured Excel workbook enabling verification: a regulatory matrix for five states and both provider types, a recommendation summary, and sources.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.0, "rules": [{"type": "llm_judge", "name": "Workbook Structure and Sections Present (Excel Only)", "description": "Check that the output is a well-structured Excel file with mandated sheets/sections and tables. Only verify presence/shape, not correctness.", "weight": 3.0, "judge_prompt": "You are verifying STRUCTURE ONLY (not correctness) of the candidate output. Inspect the provided output (rendered Excel). Confirm it is an Excel spreadsheet with the following structure. Be flexible with sheet names and header wording, but ensure the required elements exist.\n\nFORMAT REQUIREMENTS:\n- Must be an Excel workbook (.xlsx). Not PDF, not DOCX, not CSV.\n- Contains clearly labeled sections and tables (headers visible, grid/tabular layout).\n\nREQUIRED CONTENT (Sheets/Sections):\n1) Regulatory Matrix sheet (name may vary: e.g., \"State Regulatory Matrix\", \"Scope & Supervision\", \"Regulatory Overview\"): \n   - A table with at least these columns (synonyms acceptable):\n     \u2022 State (full name or two-letter abbreviation)\n     \u2022 Provider Type (values: Nurse Practitioner/NP and Physician Assistant/PA)\n     \u2022 Independent Practice (Yes/No/Conditional)\n     \u2022 Physician Cosign Required (Yes/No/Conditional)\n     \u2022 Max Supervised per Physician (a number or N/A)\n     \u2022 Telehealth Notes/Conditions (short notes acceptable)\n   - Must include both NP and PA entries for EACH of these states: Arizona, Pennsylvania, Washington, West Virginia, Virginia.\n   - One row per (State, Provider Type) is acceptable.\n\n2) Summary & Recommendation sheet (name may vary: \"Summary & Recommendation\", \"Executive Summary\", \"Decision Summary\"): \n   - An Overall Recommendation statement that clearly chooses either Nurse Practitioners or Physician Assistants as the stronger strategic choice overall ACROSS THE FIVE STATES.\n   - Must explicitly acknowledge that cost is equal between NPs and PAs (e.g., \"hourly rate is the same\").\n   - A Per-State Recommendation table with columns similar to: [State | Recommended Provider | Rationale].\n\n3) Sources sheet (name may vary: \"Sources\", \"References\", \"Citations\"): \n   - A tabular list of sources used, with columns similar to: [State | Provider Type | Source Title/URL | Accessed Date]. At least one source per state-provider combination OR per state with clarity is acceptable.\n\nSCORING (STRUCTURE ONLY):\n- 3.0: Excel file present with all three sheets (or clearly named equivalents), the Regulatory Matrix table with all required columns, NP and PA rows for each of the five states, the Summary & Recommendation with explicit overall choice and cost parity mention AND per-state recommendation table, and a Sources table.\n- 2.0: Excel file with Regulatory Matrix complete (all five states x both provider types) and either the Summary & Recommendation OR Sources sheet present but not both, OR both present but one is missing a required element (e.g., missing cost parity mention or missing per-state table).\n- 1.0: Excel file exists but Regulatory Matrix is incomplete (missing some states/provider rows) OR key required columns are missing; OR missing both Summary & Recommendation and Sources.\n- 0.0: Not an Excel file OR lacks a recognizable Regulatory Matrix.\n\nOnly evaluate presence/structure, not the accuracy of content.", "expectation": "A single .xlsx with: a complete Regulatory Matrix table for AZ, PA, WA, WV, VA (both NP and PA), a Summary & Recommendation with explicit overall pick and cost parity mention plus per-state picks, and a Sources list."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness and Consistency Verification", "description": "Now that structure exists, verify coverage, internal consistency, plausibility, and alignment of the recommendation with the matrix.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Coverage and Unique Key Completeness (5 States x 2 Providers)", "description": "Detect the Regulatory Matrix table; verify that each of the five required states has both NP and PA rows and that key columns exist. Flexible matching of sheet/column names.", "weight": 0.6, "code": "import re\nimport json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output\"\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n        sheet_candidates = [s for s in xls.sheet_names]\n        # Heuristic: pick the sheet that looks like the regulatory matrix\n        key_words = [\"state\", \"scope\", \"matrix\", \"regulatory\", \"supervision\", \"practice\"]\n        def is_matrix_sheet(name):\n            ln = name.lower()\n            return any(k in ln for k in key_words)\n        matrix_sheets = [s for s in sheet_candidates if is_matrix_sheet(s)]\n        sheet_name = matrix_sheets[0] if matrix_sheets else sheet_candidates[0]\n        df = pd.read_excel(file_path, sheet_name=sheet_name)\n        if df.shape[0] == 0:\n            return 0.0, f\"Matrix sheet '{sheet_name}' is empty\"\n        cols = [str(c).strip() for c in df.columns]\n        lcols = [c.lower() for c in cols]\n        def find_col(syns):\n            for i,c in enumerate(lcols):\n                for s in syns:\n                    if s in c:\n                        return cols[i]\n            return None\n        col_state = find_col([\"state\", \"jurisdiction\"])\n        col_provider = find_col([\"provider type\", \"provider\", \"role\", \"type\"])\n        col_ind = find_col([\"independent practice\", \"full practice\", \"autonomy\", \"practice authority\", \"independence\"])\n        col_cosign = find_col([\"cosign\", \"co-sign\", \"chart sign\", \"physician signature\", \"signature requirement\", \"cosignature\"])\n        col_max = find_col([\"max supervised\", \"supervision limit\", \"supervision ratio\", \"ratio\", \"max supervisees\", \"max number supervised\", \"supervision cap\", \"max per physician\"])\n        # Minimal required columns: state, provider, independent practice, cosign, max supervised\n        needed = [col_state, col_provider, col_ind, col_cosign, col_max]\n        if any(c is None for c in needed):\n            return 0.2, \"One or more required columns not found (state/provider/independent/cosign/max)\"\n        # Normalize states and providers\n        state_map = {\n            'arizona':'AZ','az':'AZ',\n            'pennsylvania':'PA','pa':'PA',\n            'washington':'WA','wa':'WA',\n            'west virginia':'WV','wv':'WV',\n            'virginia':'VA','va':'VA'\n        }\n        def norm_state(x):\n            if pd.isna(x):\n                return None\n            s = str(x).strip().lower()\n            return state_map.get(s, state_map.get(re.sub(r\"[^a-z]\",\"\", s), None))\n        def norm_provider(x):\n            if pd.isna(x):\n                return None\n            s = str(x).strip().lower()\n            if (\"nurse\" in s and \"pract\" in s) or re.search(r\"\\bnp\\b\", s):\n                return 'NP'\n            if (\"physician\" in s and \"assistant\" in s) or re.search(r\"\\bpa\\b\", s):\n                return 'PA'\n            return None\n        required_states = {\"AZ\",\"PA\",\"WA\",\"WV\",\"VA\"}\n        present = set()\n        for _,row in df.iterrows():\n            st = norm_state(row.get(col_state))\n            pr = norm_provider(row.get(col_provider))\n            if st in required_states and pr in {\"NP\",\"PA\"}:\n                present.add((st, pr))\n        total_needed = len(required_states) * 2  # NP + PA per state\n        coverage = len(present) / total_needed if total_needed else 0.0\n        # Score: full if all 10 present; partial linearly otherwise. Small bonus if all required columns present.\n        score = coverage * 0.6\n        feedback = f\"Found {len(present)}/10 (state,provider) combos on sheet '{sheet_name}'.\"\n        if coverage == 1.0:\n            feedback += \" All required combinations present.\"\n        return score, feedback\n    except Exception as e:\n        return 0.0, f\"Error reading spreadsheet: {e}\""}, {"type": "code", "name": "Internal Consistency of Independence, Cosign, and Supervision", "description": "Check logical consistency per row: if Independent Practice=Yes, cosign should generally be No and Max Supervised per Physician should be N/A/blank; if Independent Practice=No, cosign should generally be Yes/Depends and supervision count should be provided (not N/A).", "weight": 0.6, "code": "import re\nimport json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output\"\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n        # Find likely matrix sheet\n        key_words = [\"state\", \"scope\", \"matrix\", \"regulatory\", \"supervision\", \"practice\"]\n        def is_matrix_sheet(name):\n            ln = name.lower()\n            return any(k in ln for k in key_words)\n        sheet_name = next((s for s in xls.sheet_names if is_matrix_sheet(s)), xls.sheet_names[0])\n        df = pd.read_excel(file_path, sheet_name=sheet_name)\n        if df.shape[0] == 0:\n            return 0.0, f\"Matrix sheet '{sheet_name}' is empty\"\n        cols = [str(c).strip() for c in df.columns]\n        lcols = [c.lower() for c in cols]\n        def find_col(syns):\n            for i,c in enumerate(lcols):\n                for s in syns:\n                    if s in c:\n                        return cols[i]\n            return None\n        col_ind = find_col([\"independent practice\", \"full practice\", \"autonomy\", \"practice authority\", \"independence\"])\n        col_cosign = find_col([\"cosign\", \"co-sign\", \"chart sign\", \"physician signature\", \"signature requirement\", \"cosignature\"])\n        col_max = find_col([\"max supervised\", \"supervision limit\", \"supervision ratio\", \"ratio\", \"max supervisees\", \"max number supervised\", \"supervision cap\", \"max per physician\"])\n        if any(c is None for c in [col_ind, col_cosign, col_max]):\n            return 0.1, \"Missing one or more of independent/cosign/max columns\"\n        def to_text(v):\n            return \"\" if pd.isna(v) else str(v).strip().lower()\n        def is_yes(t):\n            return any(k in t for k in [\"yes\", \"true\", \"full\", \"independent\", \"unrestricted\"])\n        def is_no(t):\n            return any(k in t for k in [\"no\", \"false\", \"not required\", \"none\", \"n/a\"]) and not is_yes(t)\n        def is_depends(t):\n            return any(k in t for k in [\"depend\", \"conditional\", \"transition\", \"collab\", \"protocol\", \"requires\", \"limited\", \"supervis\"])\n        def is_na(t):\n            return t in [\"\", \"n/a\", \"na\", \"not applicable\", \"none\", \"-\", \"\u2014\", \"\u2013\"]\n        def has_numeric(t):\n            return bool(re.search(r\"\\d\", t))\n        n_rows = 0\n        points = 0.0\n        for _,row in df.iterrows():\n            t_ind = to_text(row.get(col_ind))\n            t_cos = to_text(row.get(col_cosign))\n            t_max = to_text(row.get(col_max))\n            if not (t_ind or t_cos or t_max):\n                continue\n            n_rows += 1\n            row_pts = 0.0\n            if is_yes(t_ind):\n                # Expect no cosign and N/A max\n                cond1 = is_no(t_cos) or is_na(t_cos)\n                cond2 = is_na(t_max) or (not has_numeric(t_max))\n                row_pts = (1.0 if cond1 else 0.0) * 0.5 + (1.0 if cond2 else 0.0) * 0.5\n            elif is_no(t_ind) or is_depends(t_ind):\n                # Expect cosign yes/depends and a numeric or not N/A supervision value\n                cond1 = (not is_no(t_cos))  # yes or depends acceptable\n                cond2 = (not is_na(t_max))  # some value provided; numeric preferred\n                row_pts = (1.0 if cond1 else 0.0) * 0.5 + (1.0 if cond2 else 0.0) * 0.5\n            else:\n                # Unknown flag; partial if at least something present\n                row_pts = 0.25 if (t_cos or t_max) else 0.0\n            points += row_pts\n        if n_rows == 0:\n            return 0.0, \"No usable rows to assess consistency\"\n        frac = points / n_rows\n        return frac * 0.6, f\"Internal consistency fraction={frac:.2f} across {n_rows} rows on '{sheet_name}'\"\n    except Exception as e:\n        return 0.0, f\"Error during consistency checks: {e}\""}, {"type": "llm_judge", "name": "Regulatory Plausibility and Cross-State Reasonableness", "description": "Assess whether the entries for independence, cosignature, and supervision caps look legally plausible for AZ, PA, WA, WV, and VA by general domain knowledge (no web search). Flag blatant contradictions or obviously reversed trends (e.g., WA NP not independent).", "weight": 2.3, "judge_prompt": "Evaluate whether the Regulatory Matrix entries are broadly plausible for the five states (Arizona, Pennsylvania, Washington, West Virginia, Virginia) for both NPs and PAs. Use general domain knowledge about NP full practice authority and PA supervision norms (no web browsing). You are not grading exact legal precision but checking for red flags:\n- Do NP entries generally reflect known trends (e.g., states like AZ and WA tending toward broader NP independence)?\n- Do PA entries generally require supervision and reflect realistic supervision caps (if listed)?\n- Are any states obviously mislabeled (e.g., Washington NP not independent; or wildly implausible supervision caps)?\n- Are telehealth-specific notes reasonable (e.g., no claims that telehealth uniquely removes statutory supervision where it normally applies)?\n\nScoring:\n- 2.3: All five states\u2019 entries look plausible and internally coherent. No glaring contradictions.\n- 1.5: Mostly plausible; 1 minor issue or unclear entry.\n- 0.8: Several questionable entries or inconsistencies.\n- 0.0: Largely implausible or contradictory across multiple states.\n\nOnly assess plausibility, not formatting or completeness.", "expectation": "Matrix reflects common regulatory realities: NPs often have or approach independence in AZ/WA; PAs generally supervised with caps; nuances for PA/NP in VA/WV/PA are not wildly wrong."}, {"type": "llm_judge", "name": "Recommendation Alignment and Justification", "description": "Check that the overall recommendation clearly picks NP or PA across the five states, references the matrix, acknowledges equal cost, and provides per-state picks aligned with the matrix entries.", "weight": 2.5, "judge_prompt": "Review the Summary & Recommendation sheet. Verify:\n1) There is a single, explicit overall choice (either Nurse Practitioners or Physician Assistants) as the stronger strategic choice across the five states combined.\n2) The text explicitly notes that NP and PA cost is equal (same hourly rate) and thus the decision is based on regulatory considerations.\n3) A Per-State Recommendation table is present and the picks (NP vs PA) are reasonably consistent with the Regulatory Matrix entries for each state (e.g., choosing NP where independence is broader, or PA where supervision constraints are lighter, etc.).\n4) The reasoning ties back to the Regulatory Matrix (independence, cosignature, supervision caps) and is logically argued.\n\nScoring:\n- 2.5: All four criteria are met with clear linkage to the matrix.\n- 1.7: Three criteria met; minor gaps.\n- 1.0: Only one or two criteria met; weak linkage.\n- 0.0: No clear overall recommendation or misaligned with the matrix.", "expectation": "An explicit overall choice (NP or PA), cost parity acknowledged, per-state picks, and reasoning grounded in the matrix."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Quality and Strategic Value", "description": "Evaluate professional presentation, clarity, and strategic usefulness to leadership making telehealth hiring decisions.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Usability", "description": "Assess formatting, readability, and ease of use for leadership. Check clear headers, consistent terms, and scannability.", "weight": 1.2, "judge_prompt": "Assess the workbook\u2019s professional polish and usability:\n- Are headers clear and consistent? Are NP/PA labels unambiguous? \n- Is the Regulatory Matrix easy to scan (sorting, consistent Yes/No/Conditional values, readable notes)?\n- Are sheets logically named and navigable for executives?\n\nScoring:\n- 1.2: Clean, consistent, executive-ready formatting; very easy to use.\n- 0.8: Generally good; minor inconsistencies.\n- 0.4: Mixed formatting; some confusion.\n- 0.0: Poorly formatted; hard to read.", "expectation": "Clear, consistent, executive-friendly presentation."}, {"type": "llm_judge", "name": "Clarity of Definitions and Notes", "description": "Check that key terms (independent practice, physician cosign, supervision cap) are understandable and that notes clarify state-specific conditions relevant to telehealth.", "weight": 1.2, "judge_prompt": "Evaluate clarity of definitions and notes:\n- Are terms like Independent Practice, Physician Cosign Required, and Max Supervised per Physician defined or self-evident from context?\n- Do Telehealth Notes explain conditions or nuances (e.g., transition periods, collaboration requirements) without legal jargon?\n\nScoring:\n- 1.2: Very clear definitions/notes; unambiguous.\n- 0.8: Mostly clear, minor ambiguities.\n- 0.4: Vague or missing clarifications.\n- 0.0: Confusing or misleading terminology.", "expectation": "Concise, comprehensible definitions and notes tailored to telehealth context."}, {"type": "llm_judge", "name": "Strategic Insight for Telehealth Operations", "description": "Assess whether the narrative links regulations to operational impacts (coverage, scheduling, cosign delays, physician bandwidth) and telehealth-specific realities.", "weight": 1.2, "judge_prompt": "Assess strategic insight:\n- Does the recommendation explain operational implications for telehealth (e.g., impact of cosign on turnaround time, supervision caps on staffing models, coverage hours across time zones, physician bandwidth)?\n- Does it consider risk/compliance implications relevant to telehealth?\n\nScoring:\n- 1.2: Strong, actionable operational insight.\n- 0.8: Some strategic linkage; could be deeper.\n- 0.4: Minimal linkage.\n- 0.0: No meaningful operational insight.", "expectation": "Insightful linkage between regulatory details and telehealth staffing efficiency and risk."}, {"type": "llm_judge", "name": "Actionability and Next Steps", "description": "Check whether the deliverable outlines assumptions, risks, and concrete next steps (e.g., legal validation, policy setup, supervising physician model).", "weight": 0.4, "judge_prompt": "Evaluate actionability:\n- Are assumptions and risks acknowledged (e.g., laws change, telehealth-specific rules vary by payer)?\n- Are next steps provided (e.g., legal counsel review, confirm supervision ratios in statutes, credentialing/privileging plan, supervising physician recruitment where needed)?\n\nScoring:\n- 0.4: Clear, concrete next steps and assumptions/risks.\n- 0.3: Mostly present; minor gaps.\n- 0.1: Weak actionability.\n- 0.0: No next steps or assumptions noted.", "expectation": "A short, practical next-steps list and key assumptions/risks."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "a69be28f-9a84-47c9-992e-b90446cdca9d", "rubric": {"category_name": "Wholesale Trade \u2014 Sales Managers: Regional Fit Performance Presentation (Best Jeans)", "rationale": "Pattern C (Mixed): The deliverable is a document (presentation PDF) that embeds results of data analysis from an Excel source. Stage 1 uses an LLM gate to enforce an exact, verifiable slide structure so downstream checks are trivial. Stage 2 mixes lightweight code checks (text extraction from PDF for presence/coverage and sales metric patterns) with higher-weight LLM consistency checks across slides and summary. Stage 3 evaluates overall presentation quality, clarity, and strategic usefulness for merchandising/planning stakeholders.", "max_total_score": 25.0, "stages": [{"name": "Stage 1 \u2014 Format & Structure Gate (LLM only)", "description": "Gate: Ensure the output is a properly structured presentation PDF with required slides and verifiable tables/charts enabling automated/LLM verification. Only structure/presence is checked here, not correctness.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Structured Presentation Requirements (PDF)", "description": "Check that the candidate produced a PDF presentation with the exact slide structure required for verification.", "weight": 8.0, "judge_prompt": "You are verifying the STRUCTURE ONLY of a presentation for Best Jeans (a premium denim brand). The candidate should have produced a PDF presentation (exported from PowerPoint or similar). Do NOT judge correctness or quality here\u2014only whether the structure and elements exist to enable verification.\n\nRequired Format:\n- File format: PDF document (a presentation exported to PDF). Not Excel/Word/Markdown.\n- Professional slide formatting with clear titles on each slide.\n\nRequired Slide Structure (be flexible with similar titles, but all elements must be present):\n1) Title Slide\n   - Title contains phrases like \u201cRegional Performance Recap\u201d or \u201cRegional Fit Performance.\u201d\n   - Brand name \u201cBest Jeans\u201d is visible.\n   - Date: July 9, 2025 (exact date) or an obviously equivalent current date notation.\n\n2) Executive Summary (1\u20132 slides)\n   - Clearly labeled \u201cExecutive Summary,\u201d \u201cSummary,\u201d or similar.\n   - Aggregated U.S. sales overview by fit, with gender split (Men\u2019s and Women\u2019s shown separately or side-by-side).\n   - At least one chart or table showing sales by fit (units and/or revenue).\n\n3) Men\u2019s Regional Performance (4 slides total, one per region)\n   - One slide each for: Midwest, South, Northeast, West Coast.\n   - Each slide must present top-selling fits for MEN based on total units and total revenue.\n   - Each slide must include either:\n     a) A table with columns analogous to: [Rank | Fit | Units | Revenue], or\n     b) A clearly labeled chart (e.g., bar/column) where categories are fits and values are units and/or revenue.\n   - The slide should explicitly highlight or identify the top-selling fit(s).\n\n4) Women\u2019s Regional Performance (4 slides total, one per region)\n   - One slide each for: Midwest, South, Northeast, West Coast.\n   - Same requirements as Men\u2019s slides, but for WOMEN.\n\n5) Methodology / Data Notes (1 slide)\n   - Mentions the data source as an Excel file with sell-in by fit name, gender, and account location.\n   - States the measures used (Units and Revenue) and that regions included are: Midwest, South, Northeast, West Coast.\n   - Briefly describes aggregation approach (e.g., grouped by fit within each region and gender; ranked by units and revenue).\n\n6) Optional Appendix\n   - Optional: additional detailed tables by region/fit.\n\nScoring (STRUCTURE only):\n- 8.0: PDF format and all required elements present: Title, Executive Summary, all 8 regional slides (4 Men + 4 Women), Methodology/Data Notes; each regional slide shows a table or chart as specified.\n- 7.0: All required elements except the optional Appendix (if referenced elsewhere) \u2014 still full credit; or minor labeling variations but unambiguous coverage of all required slides.\n- 6.0: Missing exactly one required component (e.g., one regional slide or the Methodology slide) but the rest is correctly structured.\n- 3.0: Missing multiple required components (e.g., more than one regional slide missing), or charts/tables absent on most regional slides.\n- 0.0: Not a PDF, or structure is largely missing/undetectable.\n\nImportant: Only check presence/structure and clear labeling. Do NOT assess numerical correctness or design quality here.", "expectation": "A well-structured PDF deck with: Title (with brand and date), Executive Summary, 4 Men\u2019s region slides, 4 Women\u2019s region slides, and a Methodology/Data Notes slide, with tables/charts on each regional slide."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness & Consistency)", "description": "Now that the structure is enforced, verify internal consistency and plausibility. Mix light code checks with higher-weight LLM judges that compare executive summary vs. regional details and confirm methodology clarity. Do not grade aesthetics here.", "is_required": true, "max_points": 12.0, "min_score_to_pass": 6.0, "rules": [{"type": "code", "name": "Coverage Check: Regions and Genders Mentioned", "description": "Verify the PDF text includes all four regions and references to both men and women, indicating actual coverage.", "weight": 1.0, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    \\\"\\\"\\n    Args:\\n        workflow: Workflow object\\n        context: ValidationContext with .files accessor\\n\\n    Returns:\\n        float in [0,1] or (float, str)\\n    \\\"\\\"\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0, \\\"No document output found.\\\"\\n\\n    try:\\n        text = context.files.read_pdf_text(output.id)\\n    except Exception as e:\\n        return 0.0, f\\\"Failed to read PDF text: {e}\\\"\\n\\n    if not text or not text.strip():\\n        return 0.0, \\\"Empty or unreadable PDF text.\\\"\\n\\n    t = text.lower()\\n\\n    # Regions (flexible matching)\\n    regions = {\\n        'midwest': [r'\\bmidwest\\b', r'\\bmid-west\\b'],\\n        'south': [r'\\bsouth\\b', r'\\bsouthern\\b'],\\n        'northeast': [r'\\bnorth\\s?east\\b', r'\\bnortheast\\b', r'\\bnew england\\b'],\\n        'west coast': [r'\\bwest\\s?coast\\b', r'\\bwestern\\b', r'\\bwest\\b']\\n    }\\n    matched_regions = 0\\n    for key, pats in regions.items():\\n        if any(re.search(p, t) for p in pats):\\n            matched_regions += 1\\n\\n    # Gender mentions (flexible)\\n    men_patterns = [r\"\\bmen\\b\", r\"\\bmen's\\b\", r\"\\bmens\\b\", r\"\\bmen\u2019s\\b\"]\\n    women_patterns = [r\"\\bwomen\\b\", r\"\\bwomen's\\b\", r\"\\bwomens\\b\", r\"\\bwomen\u2019s\\b\"]\\n    has_men = any(re.search(p, t) for p in men_patterns)\\n    has_women = any(re.search(p, t) for p in women_patterns)\\n\\n    region_score = matched_regions / 4.0\\n    gender_score = (1.0 if has_men else 0.0) + (1.0 if has_women else 0.0)\\n    gender_score /= 2.0\\n\\n    score = 0.6 * region_score + 0.4 * gender_score\\n    feedback = f\\\"Regions matched: {matched_regions}/4; Men: {has_men}; Women: {has_women}.\\\"\\n    return max(0.0, min(1.0, score)), feedback\\n"}, {"type": "code", "name": "Sales Metrics Present (Units/Revenue/Currency)", "description": "Check that the PDF text includes units, revenue, and currency/number patterns, indicating that numeric sales details are presented.", "weight": 1.0, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    \\\"\\\"\\n    Args:\\n        workflow: Workflow object\\n        context: ValidationContext with .files accessor\\n\\n    Returns:\\n        float in [0,1] or (float, str)\\n    \\\"\\\"\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0, \\\"No document output found.\\\"\\n\\n    try:\\n        text = context.files.read_pdf_text(output.id)\\n    except Exception as e:\\n        return 0.0, f\\\"Failed to read PDF text: {e}\\\"\\n\\n    if not text or not text.strip():\\n        return 0.0, \\\"Empty or unreadable PDF text.\\\"\\n\\n    t = text.lower()\\n    has_units_word = 'units' in t or 'unit' in t\\n    has_revenue_word = 'revenue' in t or 'sales' in t\\n\\n    currency_matches = re.findall(r\"\\$\\s?\\d[\\d,]*(?:\\.\\d{2})?\", text)\\n    big_number_matches = re.findall(r\"\\b\\d{1,3}(?:,\\d{3})+\\b\", text)  # e.g., 12,345\\n\\n    # Score components\\n    comp = 0.0\\n    comp += 0.25 if has_units_word else 0.0\\n    comp += 0.25 if has_revenue_word else 0.0\\n    comp += 0.25 if len(currency_matches) >= 3 else (0.10 if len(currency_matches) >= 1 else 0.0)\\n    comp += 0.25 if len(big_number_matches) >= 3 else (0.10 if len(big_number_matches) >= 1 else 0.0)\\n\\n    feedback = (f\\\"units:{has_units_word}, revenue:{has_revenue_word}, \\\"\\n                f\\\"$-patterns:{len(currency_matches)}, big-numbers:{len(big_number_matches)}\\\")\\n    return max(0.0, min(1.0, comp)), feedback\\n"}, {"type": "llm_judge", "name": "Per-Slide Ranking and Label Consistency", "description": "Within each region/gender slide, verify that the identified top fit(s) align with the shown numbers. If units and revenue rankings differ, the labeling should specify which metric defines \u201ctop.\u201d", "weight": 2.5, "judge_prompt": "Evaluate INTERNAL CONSISTENCY on each regional slide. For each region (Midwest, South, Northeast, West Coast) and for both Men\u2019s and Women\u2019s slides:\n- If a table is shown (Rank, Fit, Units, Revenue), check that Rank 1 corresponds to the highest Units (and/or highest Revenue) per the slide\u2019s stated basis. If the slide claims Units-based ranking, Rank 1 should have the highest Units, etc.\n- If a chart is shown, confirm that the bar/column labeled as the top fit visually corresponds to the largest value per the chosen metric.\n- If units and revenue produce different leaders, the slide should state which metric defines \u201ctop,\u201d and the labeling should be consistent with that choice.\n\nScoring:\n- 2.5: All region/gender slides are internally consistent (no contradictions between labels and values).\n- 1.75: Minor inconsistencies on up to 2 slides, but the metric basis is mostly clear.\n- 1.0: Multiple inconsistencies or unclear basis across several slides, yet most data still roughly aligns.\n- 0.0: Widespread contradictions (labels don\u2019t match values) or impossible to verify.", "expectation": "Every regional slide cleanly aligns top labels with the numeric or visual leader per the stated metric."}, {"type": "llm_judge", "name": "Executive Summary vs. Regional Detail Coherence", "description": "Executive summary aggregates and claims should be consistent with the detailed regional slides for Men and Women.", "weight": 2.5, "judge_prompt": "Compare the Executive Summary slide(s) to the four regional slides per gender:\n- Do the Men\u2019s and Women\u2019s total units and revenue in the summary plausibly correspond to the sum across the four regions? (Exact arithmetic isn\u2019t required; check for obvious contradictions.)\n- If the summary states top fits nationally (by unit or revenue), do these align with the leading appearances across regional slides?\n- Check that any stated insights (e.g., \u201cSkinny is top in the West Coast\u201d) are not contradicted by the regional slides.\n\nScoring:\n- 2.5: Executive summary is coherent with regional details; no contradictions.\n- 1.75: Minor discrepancies or unclear statements, but mostly consistent.\n- 1.0: Several inconsistencies; still partially aligned.\n- 0.0: Major contradictions or the summary is detached from regional details.", "expectation": "Summary totals and leadership claims should line up with a reasonable roll-up of regional slides."}, {"type": "llm_judge", "name": "Methodology Sufficiency and Clarity", "description": "The Methodology/Data Notes must explain metrics, source, and grouping logic clearly enough to reproduce the results.", "weight": 2.5, "judge_prompt": "Inspect the Methodology/Data Notes slide:\n- Does it state the data source as an Excel file with sell-in by fit, gender, and account location?\n- Are the measures defined (Units and Revenue), and are the included regions explicitly listed?\n- Is the aggregation method described (e.g., grouped by fit by region and gender; ranked by total units/revenue for the period)?\n- Are any key assumptions mentioned (e.g., handling of duplicates/returns if relevant, region mapping, timeframe)?\n\nScoring:\n- 2.5: Clear, reproducible methodology covering source, metrics, regions, and aggregation approach.\n- 1.75: Minor omissions but fundamentally reproducible.\n- 1.0: Vague with notable gaps; only partially reproducible.\n- 0.0: Missing or too ambiguous to understand.", "expectation": "A concise but complete methodology that would let someone replicate the aggregation and ranking."}, {"type": "llm_judge", "name": "Data Labeling and Axis/Legend Accuracy", "description": "Charts/tables must have accurate labels, units, currency formatting, and legends so the metric basis is unambiguous.", "weight": 2.5, "judge_prompt": "Review charts/tables across the deck:\n- Are axes labeled (e.g., Units, Revenue $) and do legends correctly identify Men vs Women or fit categories when used?\n- Are currency values formatted as dollars and are units clearly numerical counts?\n- Are slide titles and labels aligned with the data shown (e.g., a slide titled \u201cTop Women\u2019s Fits \u2014 Midwest\u201d actually shows women\u2019s fits for Midwest)?\n\nScoring:\n- 2.5: Labeling and formatting are consistently accurate and unambiguous.\n- 1.75: Minor issues that don\u2019t impede understanding.\n- 1.0: Several labeling/formatting issues that create confusion.\n- 0.0: Frequent or fundamental mislabeling that obscures meaning.", "expectation": "All visual elements correctly communicate metric basis and segmentation."}], "on_failure_action": "skip_remaining", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality & Communication", "description": "Holistic evaluation of presentation quality, clarity, and strategic usefulness for merchandising/planning decisions. This is not about numerical correctness but about how effectively insights are communicated.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Executive Summary Clarity and Actionability", "description": "Assesses whether the summary communicates the key takeaways (by region and gender) succinctly, with clear next steps.", "weight": 1.25, "judge_prompt": "Evaluate the Executive Summary for clarity and actionability:\n- Are the key takeaways about top fits and notable regional/gender differences clearly stated?\n- Are implications for assortment or planning called out (e.g., which fits to prioritize by region/gender)?\n- Is the language concise and executive-friendly?\n\nScore 1.25 = very clear/actionable; 0.9 = mostly clear; 0.5 = partially clear; 0.0 = unclear or absent.", "expectation": "Concise, decision-oriented summary that highlights where demand is strongest by fit and segment."}, {"type": "llm_judge", "name": "Visual Design and Readability", "description": "Evaluates layout consistency, typography, chart/table readability, and overall professional polish.", "weight": 1.25, "judge_prompt": "Judge presentation quality:\n- Consistent slide layouts, fonts, and color usage.\n- Charts/tables easy to read; adequate contrast; not overcrowded.\n- Slide titles are informative and consistent; slide numbers/helpful headers present.\n\nScore 1.25 = highly professional; 0.9 = minor issues; 0.5 = mixed readability; 0.0 = poor/unprofessional.", "expectation": "Clean, consistent slides with easily readable visuals."}, {"type": "llm_judge", "name": "Audience Appropriateness (Merchandising/Planning)", "description": "Checks that depth, terminology, and structure are suited for merchandising and planning stakeholders.", "weight": 1.25, "judge_prompt": "Assess audience fit:\n- Uses appropriate merchandising/planning language (fits, units, revenue, assortment, allocation).\n- Right level of detail (not overly technical; focuses on actionable insights by region/gender/fit).\n- Logical flow from summary to region details to methodology.\n\nScore 1.25 = well-targeted; 0.9 = mostly appropriate; 0.5 = uneven targeting; 0.0 = poorly targeted.", "expectation": "Content and structure suit decision-making by merchandising and planning teams."}, {"type": "llm_judge", "name": "Insightfulness and Strategic Recommendations", "description": "Rates the usefulness of insights and the presence of practical recommendations tied to findings.", "weight": 1.25, "judge_prompt": "Evaluate the insights and recommendations:\n- Highlights meaningful patterns (e.g., certain fits overperform in specific regions or differ by gender).\n- Provides specific recommendations for future assortment or regional allocation, grounded in the data shown.\n- Avoids generic or unsupported claims.\n\nScore 1.25 = insightful with concrete recommendations; 0.9 = some useful insights; 0.5 = limited insight; 0.0 = no meaningful recommendations.", "expectation": "Clear, data-grounded recommendations for assortment and planning decisions."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "bb499d9c-0263-4684-9238-75e8e86077b1", "rubric": {"category_name": "Sales Operations L1 Process (Fintech Marketplace)", "rationale": "Task Type: Pattern B (Document). Output: A DOCX Word document (<=25 pages) with specific sections and two flowcharts. Stage 1 uses LLM-only structure enforcement to make downstream verification trivial. Stage 2 mixes lightweight code checks (presence of key tokens, cadence, metrics) with LLM semantic checks (coherence, compliance, two-sided alignment). Stage 3 assesses overall professional quality, usability, and senior management readiness. Code rules are weighted ~5x less than LLM rules on average within Stage 2.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (LLM-only)", "description": "Gate: The candidate must deliver a DOCX with the exact structural elements required so verification is possible. No quality or correctness checks here\u2014only format and presence of sections/diagrams.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Format and Core Sections Present", "description": "Verify the document is a DOCX Word file (not PDF), within 25 pages, professionally formatted, and contains all required sections and subsections as headers so they are visually identifiable.", "weight": 5.0, "judge_prompt": "You are evaluating whether the primary output is a DOCX Word document with the required structure. Only check PRESENCE/FORMAT, not content quality. Use these criteria:\n\nFormat requirements:\n- Must be a DOCX Word file (not PDF, not plain text, not Excel).\n- No longer than 25 pages.\n- Professional formatting with clear, visible section headers.\n\nRequired sections (flexible on wording but headers must be visible):\n1) Overview (purpose, scope, audience)\n2) Stakeholders (internal teams + external parties)\n3) Process Definition (with the following clearly labeled subsections):\n   - Process Goal\n   - Trigger Event\n   - Preconditions\n   - Inputs\n   - Output(s)/Deliverables\n   - Success end condition (for issuers and for investors)\n   - Failure end condition\n   - Compliance (key regulations and internal policies)\n4) Key Roles (internal stakeholder roles and responsibilities)\n5) Key Forms (e.g., NDA, KYC/AML, issuer onboarding, investor suitability)\n6) Key Metrics (e.g., AUM, ARR, volume, margins, retention)\n7) Key Reports (cadence/frequency)\n8) Potential Risks and Mitigation Controls\n9) Asset Issuers Process model (flowchart section) + textual breakdown\n10) Retail Investors Process model (flowchart section) + textual breakdown\n\nScoring guide (only structure/format):\n- 1.0: DOCX format, <=25 pages, all 10 sections present with headers and required Process Definition subsections visible.\n- 0.8: DOCX, <=25 pages, missing up to 2 minor subsections but all 10 major sections present.\n- 0.6: DOCX, <=25 pages, missing 1\u20132 major sections OR Process Definition subsections mostly incomplete.\n- 0.3: DOCX but >25 pages OR only 5\u20137 major sections present.\n- 0.0: Not DOCX OR <5 major sections present.\n\nReturn a single numeric score in [0.0, 1.0] reflecting the above. Do not evaluate content quality or correctness.", "expectation": "A <=25-page DOCX with all 10 sections and the required Process Definition subsections as clearly labeled headers."}, {"type": "llm_judge", "name": "Presence of Both Flowcharts + Textual Breakdowns", "description": "Verify the document visibly contains two distinct flowchart visuals (images/diagrams) and accompanying text breakdown sections for: (a) Asset Issuers and (b) Retail Investors.", "weight": 3.0, "judge_prompt": "Check that the document contains BOTH of the following, focusing only on presence/structure:\n- A distinct flowchart (visual image/diagram) for the Asset Issuers sales process, AND a textual breakdown of each stage referenced in that chart.\n- A distinct flowchart (visual image/diagram) for the Retail Investors sales process, AND a textual breakdown of each stage referenced in that chart.\n- The Asset Issuers flowchart section should be clearly labeled and separate from the Retail Investors flowchart section.\n- The Issuers model section should indicate customization by issuer groups (e.g., private companies, private funds, public listings, banks/originators)\u2014the presence of group labels or references is enough here.\n\nScoring guide (presence only):\n- 1.0: Both flowcharts are visually present AND each has a corresponding textual breakdown; Issuers model references issuer groups.\n- 0.7: Both flowcharts present but one lacks a clear textual breakdown OR issuer groups are not referenced.\n- 0.4: Only one flowchart present (with or without textual breakdowns).\n- 0.0: No flowcharts present.\n\nReturn a numeric score in [0.0, 1.0] based solely on presence and labeling, not quality.", "expectation": "Two distinct flowcharts present (Issuers and Investors), each with a textual breakdown; issuer model references group customization."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Internal Consistency", "description": "Now verify the document\u2019s internal logic, coverage, and basic completeness. Mix deterministic code checks with LLM reasoning. Code rules are light-touch token/breadth checks; LLM rules assess coherence and appropriateness.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Header Coverage (Flexible Match)", "description": "Checks for presence of key headers/terms across the document text to confirm the Process Definition and other required sections are materially included.", "weight": 0.5, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Returns fraction of matched required headers. Flexible substring matching.\n    \"\"\"\n    import re\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0\n    if not text:\n        return 0.0\n    tl = text.lower()\n    required = [\n        'overview', 'stakeholders', 'process definition', 'process goal', 'trigger event',\n        'preconditions', 'inputs', 'output', 'outputs', 'deliverables',\n        'success end condition', 'failure end condition', 'compliance',\n        'key roles', 'roles and responsibilities', 'key forms', 'key metrics',\n        'key reports', 'potential risks', 'mitigation controls',\n        'asset issuers process model', 'retail investors process model'\n    ]\n    # Deduplicate logical pairs by set containment rather than strict counts\n    found = set()\n    for term in required:\n        if term in tl:\n            found.add(term)\n    # Consider an item present if any of its synonyms present\n    logical_groups = [\n        {'overview'},\n        {'stakeholders'},\n        {'process definition'},\n        {'process goal'},\n        {'trigger event'},\n        {'preconditions'},\n        {'inputs'},\n        {'output','outputs','deliverables'},\n        {'success end condition'},\n        {'failure end condition'},\n        {'compliance'},\n        {'key roles','roles and responsibilities'},\n        {'key forms'},\n        {'key metrics'},\n        {'key reports'},\n        {'potential risks','mitigation controls'},\n        {'asset issuers process model'},\n        {'retail investors process model'}\n    ]\n    present = 0\n    for group in logical_groups:\n        if any(g in found for g in group):\n            present += 1\n    ratio = present / len(logical_groups)\n    return max(0.0, min(1.0, ratio))"}, {"type": "code", "name": "Compliance References Breadth", "description": "Checks that the Compliance section references multiple relevant regulations/policies (e.g., KYC/AML, SEC/FINRA, MiFID, GDPR/CCPA, suitability/BI).", "weight": 0.5, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Count distinct compliance-related tokens. Full credit at >=5 present.\n    \"\"\"\n    import re\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0\n    tl = text.lower()\n    tokens = [\n        'kyc', 'know your customer', 'aml', 'anti-money laundering', 'sanctions', 'ofac', 'pep',\n        'suitability', 'best interest', 'reg bi', 'fiduciary', 'mifid', 'gdpr', 'ccpa',\n        'sec', 'finra', 'reg d', 'regulation d', 'reg s', 'regulation s', 'blue sky', 'sipc',\n        'privacy', 'data protection', 'recordkeeping', 'books and records', 'broker-dealer', 'investment adviser', 'outsourcing', 'complaints'\n    ]\n    present = set()\n    for t in tokens:\n        if t in tl:\n            present.add(t)\n    # normalize related families so we don't overcount variants\n    families = [\n        {'kyc','know your customer'},\n        {'aml','anti-money laundering'},\n        {'sanctions','ofac','pep'},\n        {'suitability','best interest','reg bi','fiduciary'},\n        {'mifid'},\n        {'gdpr','ccpa','privacy','data protection'},\n        {'sec','finra','broker-dealer','investment adviser'},\n        {'reg d','regulation d'},\n        {'reg s','regulation s','blue sky'},\n        {'sipc'},\n        {'recordkeeping','books and records'},\n        {'outsourcing'},\n        {'complaints'}\n    ]\n    count = 0\n    for fam in families:\n        if any(f in present for f in fam):\n            count += 1\n    ratio = min(1.0, count / 5.0)  # full credit at 5+ families\n    return max(0.0, ratio)"}, {"type": "code", "name": "Key Metrics Richness", "description": "Checks for breadth of common sales/marketplace metrics (AUM, ARR, CAC, LTV, conversion, retention, churn, win rate, pipeline, take rate, GMV, ARPU, etc.). Full credit at >=8 unique metrics terms.", "weight": 0.5, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Count distinct metrics-related tokens. Full credit at 8+.\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0\n    tl = text.lower()\n    tokens = [\n        'aum', 'arr', 'mrr', 'cac', 'ltv', 'payback', 'conversion', 'win rate', 'pipeline', 'forecast accuracy',\n        'retention', 'churn', 'nps', 'csat', 'arpu', 'arpa', 'gmv', 'take rate', 'margin', 'gross margin',\n        'deal velocity', 'sales cycle', 'time to close', 'average deal size', 'activation rate', 'cohort', 'funnel',\n        'sql', 'mql', 'opportunity', 'bookings', 'revenue', 'net revenue retention', 'grr', 'nrr'\n    ]\n    present = set()\n    for t in tokens:\n        if t in tl:\n            present.add(t)\n    count = len(present)\n    ratio = min(1.0, count / 8.0)\n    return max(0.0, ratio)"}, {"type": "code", "name": "Reports Cadence and Content Signals", "description": "Checks for reporting cadence variety (weekly/monthly/quarterly/annual) and presence of report/dashboard concepts (pipeline, forecast, board, cohort, funnel).", "weight": 0.5, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Half credit for >=2 cadences; half credit for >=2 report-related terms.\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0\n    tl = text.lower()\n    cadences = ['weekly', 'monthly', 'quarterly', 'annual', 'annually']\n    report_terms = ['dashboard', 'pipeline', 'forecast', 'board report', 'board pack', 'cohort', 'funnel', 'kpi report', 'executive summary']\n    c_count = sum(1 for c in cadences if c in tl)\n    r_count = sum(1 for r in report_terms if r in tl)\n    score = 0.0\n    if c_count >= 2:\n        score += 0.5\n    if r_count >= 2:\n        score += 0.5\n    return max(0.0, min(1.0, score))"}, {"type": "llm_judge", "name": "Operational Coherence and Handoffs", "description": "Assess whether the process steps, roles, and handoffs across teams (Sales, Marketing, Product, Compliance, Legal, RevOps/Finance, Customer Success) are coherent and clearly connected through the cycle.", "weight": 2.0, "judge_prompt": "Evaluate the operational coherence of the process (not just presence). Look for: clear step-by-step flow; explicit handoffs between teams; alignment between roles and the steps they own; coordination with Compliance/Legal and RevOps/Finance; and linkage to onboarding/CS for both issuers and investors.\n\nScoring:\n- 1.0: Steps are clear and logically sequenced with explicit handoffs and owners across all key functions.\n- 0.7: Mostly coherent; minor gaps in role ownership or a few unclear handoffs.\n- 0.4: Significant ambiguities about who owns steps or how teams coordinate.\n- 0.0: Disorganized flow with unclear or missing handoffs/owners.", "expectation": "End-to-end flow is logically sequenced with explicit owners and cross-functional handoffs."}, {"type": "llm_judge", "name": "End-State Definitions and IO Alignment", "description": "Check that Trigger, Preconditions, Inputs, Outputs, Success/Failure end conditions are specific and internally consistent for BOTH issuers and investors.", "weight": 1.5, "judge_prompt": "Review the Process Definition subsections to verify: (1) trigger events are concrete; (2) preconditions are sufficient; (3) inputs map to outputs; (4) success/failure end conditions are specific and measurable for both asset issuers and retail investors; and (5) outputs logically support the success conditions.\n\nScoring:\n- 1.0: Clear, specific, and internally consistent across all elements (triggers, preconditions, inputs/outputs, success/failure) for both sides.\n- 0.6: Generally sound but some elements are generic or misaligned.\n- 0.3: Multiple vague or inconsistent elements.\n- 0.0: Largely missing or incoherent.", "expectation": "Concrete triggers, preconditions, inputs/outputs, and measurable success/failure conditions for issuers and investors."}, {"type": "llm_judge", "name": "Risk and Control Coverage", "description": "Evaluate whether the risks and mitigation controls cover the typical fintech two-sided marketplace threats (regulatory, operational, data/privacy, fraud, suitability, reputational).", "weight": 2.0, "judge_prompt": "Assess the 'Potential Risks and Mitigation Controls' section for breadth and specificity. Look for coverage of: regulatory compliance (KYC/AML/sanctions, broker-dealer/adviser obligations), data security/privacy, fraud/abuse (issuer misrep, investor fraud), operational continuity/scalability, suitability/Reg BI (or analogous), reputational risk, and reporting/recordkeeping controls.\n\nScoring:\n- 1.0: Broad, specific controls mapped to major risks with clear owners/checkpoints.\n- 0.7: Covers most risks but with limited specificity or missing owners/checkpoints.\n- 0.4: Several major risk categories missing or thin.\n- 0.0: Superficial or missing.", "expectation": "Comprehensive mapping of major risks to actionable controls and owners."}, {"type": "llm_judge", "name": "Two-Sided Marketplace Adaptation", "description": "Evaluate whether the process meaningfully differentiates flows and requirements for asset issuers vs retail investors and ties them together where needed (e.g., product listing readiness, disclosures, suitability).", "weight": 1.5, "judge_prompt": "Check that the document: (a) distinguishes the sales process steps for asset issuers and for retail investors; (b) customizes issuer steps by issuer groups (private companies, private funds, public listings, banks/originators); (c) addresses how issuer onboarding/product readiness (docs, disclosures, KYC/KYB) feeds investor-side listing and go-to-market steps; and (d) shows alignment points between sides (e.g., timing, approvals, compliance gates).\n\nScoring:\n- 1.0: Clear differentiation and integration across both sides, with issuer-group customization.\n- 0.6: Some differentiation but limited customization or weak linkage between sides.\n- 0.3: Mostly generic single-sided view.\n- 0.0: Does not address the two-sided nature.", "expectation": "Distinct but integrated issuer and investor processes, with issuer-group customization and linkage to investor GTM."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Management Readiness", "description": "Holistic assessment of presentation, strategic usefulness, and readiness for senior management approval.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Executive Readiness and Actionability", "description": "Is the document immediately usable by senior management to approve and launch L1 operations (e.g., includes decision gates, checklists, timelines/SLAs, owners)?", "weight": 1.5, "judge_prompt": "Judge how actionable and approval-ready the document is for senior leadership. Consider: presence of decision gates/approvals, clear ownership, SLAs/timelines where relevant, concise summaries, and checklists/runbooks that can be operationalized.\n\nScoring:\n- 1.0: Highly actionable and approval-ready; minimal modifications required.\n- 0.6: Generally usable but missing a few operational details.\n- 0.3: Requires substantial rework before use.\n- 0.0: Not suitable for decision-making.", "expectation": "A concise, approval-ready package with clear owners, gates, and operational details."}, {"type": "llm_judge", "name": "Clarity, Structure, and Concision", "description": "Assess clarity of writing, organization, and adherence to the <=25-page constraint without fluff.", "weight": 1.0, "judge_prompt": "Evaluate clarity and organization: logical flow, scannable headers, minimal jargon or well-defined terms, and adherence to concision (no obvious fluff). Confirm it respects the intended length limit.\n\nScoring:\n- 1.0: Clear, well-structured, concise.\n- 0.6: Minor clarity/structure issues.\n- 0.3: Several confusing or bloated sections.\n- 0.0: Poorly structured or verbose.", "expectation": "Clear, logically structured, and concise writing that supports quick executive review."}, {"type": "llm_judge", "name": "Visuals Quality and Alignment", "description": "Evaluate whether the two flowcharts are readable, correctly sequenced, and aligned with the textual breakdowns.", "weight": 1.0, "judge_prompt": "Assess the two flowcharts for readability (labels, stages), logical sequencing, and consistency with the text breakdowns. Minor stylistic differences are acceptable; focus on alignment and clarity.\n\nScoring:\n- 1.0: Clear, logically sequenced, and consistent with text.\n- 0.6: Minor misalignments or readability issues.\n- 0.3: Significant inconsistencies or confusion.\n- 0.0: Not readable or not aligned.", "expectation": "Readable flowcharts that accurately reflect the described steps."}, {"type": "llm_judge", "name": "Cross-Functional Feasibility and Best-Practice Grounding", "description": "Does the plan reflect industry best practices and realistic cross-functional coordination for a regulated fintech marketplace?", "weight": 1.5, "judge_prompt": "Assess whether the plan appears grounded in best practices (e.g., B2B/B2C sales ops, regulated fintech, marketplace dynamics): realistic role definitions, stakeholder coordination, compliance checkpoints, data/reporting hygiene, and change management. Consider feasibility and practicality.\n\nScoring:\n- 1.0: Strong best-practice alignment and practical feasibility.\n- 0.6: Generally aligned with some gaps.\n- 0.3: Superficial or impractical in parts.\n- 0.0: Misaligned or implausible.", "expectation": "Practical, best-practice-aligned plan with credible cross-functional execution."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "36d567ba-e205-4313-9756-931c6e4691fe", "rubric": {"category_name": "Government \u2014 Compliance Officers \u2014 Pre-Award Risk Assessment Tool (Uniform Guidance)", "rationale": "Document task (Pattern B). The rubric enforces a strict, self-documenting structure for a 1\u20132 page DOCX/PDF question-set titled \u201cFederal Applicant - Risk Assessment Tool.\u201d Stage 1 (LLM-only) mandates the exact document shape so verification is trivial. Stage 2 mixes lightweight code checks (format, coverage, citations presence) with higher-weight LLM checks (two-part structure by topic, citation relevance, cross-topic applicability). Stage 3 provides a holistic quality review focused on clarity, professionalism, and usefulness for pre-award risk assessment across all applicant types.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Format and Structural Gate", "description": "Mandatory structural requirements for a self-verifiable question set in DOCX/PDF. Only checks presence and structure; not content correctness.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Document Format, Title, and Length", "description": "Verify the candidate output is a DOCX or PDF with the correct title and 1\u20132 pages in length.", "weight": 1.5, "judge_prompt": "You are evaluating whether the submission meets strict structural requirements for format, title, and length. Use only what you can see in the rendered DOCX/PDF.\n\nPass criteria to award full credit:\n- File format is DOCX or PDF (not plain text, not spreadsheet).\n- The document title \u201cFederal Applicant - Risk Assessment Tool\u201d appears prominently on the first page (case-insensitive match acceptable, minor punctuation/capitalization variations acceptable).\n- Total length is approximately 1\u20132 pages.\n\nScoring (out of 1.5):\n- 1.5: All three conditions clearly met.\n- 1.0: One minor deviation (e.g., slightly over 2 pages but still concise, or title has slight variation but clearly intended).\n- 0.5: Only one of the three conditions is clearly met.\n- 0.0: Wrong format OR no recognizable title OR far outside 1\u20132 pages.\n\nOnly check presence/format. Do not assess content quality or correctness.", "expectation": "A professional-looking DOCX/PDF titled \u201cFederal Applicant - Risk Assessment Tool,\u201d roughly 1\u20132 pages."}, {"type": "llm_judge", "name": "Topic Coverage, Per-Topic Structure, and Citations Presence", "description": "Verify that all 11 topics are present as numbered items, with per-topic two-part questions (except conflicts-of-interest) and that topics #6\u2013#10 include 2 CFR Part 200 citations (presence only).", "weight": 2.5, "judge_prompt": "Check the document\u2019s structure against these requirements. Be flexible with wording, but strict on coverage and structure:\n\nRequired numbered topics (1\u201311):\n1. Tracking multiple sources of revenue/funding separately\n2. Written accounting policies and procedures\n3. Financial Management System - tracking expenditures\n4. Timing of federal payments and disbursement of funds\n5. Internal controls\n6. Records retention\n7. Conflicts of interest\n8. Applicant point person\u2019s knowledge of federal requirements\n9. Subaward management and monitoring\n10. Timekeeping\n11. High-risk status with federal agencies\n\nPer-topic question structure:\n- For all topics EXCEPT #7 (conflicts of interest): include a two-part question presented in one or more sentences where:\n  \u2022 Part 1: phrased so the applicant can begin with a simple Yes/No.\n  \u2022 Part 2: requests additional detail as an open-ended response.\n- Topic #7 may be single- or two-part; do not penalize if single-part here.\n\nCitations presence requirement:\n- For topics #6 through #10, the question(s) should include an explicit reference to the Uniform Guidance (2 CFR Part 200), ideally with a section number (e.g., \u201c2 CFR 200.334\u201d), but for this gate, only check that some 2 CFR Part 200 reference is present near each of these topics.\n\nScoring (out of 2.5):\n- 2.5: All 11 topics present as clearly delineated items; topics 1\u20135 and 8\u201311 have two-part questions; topic 7 acceptable even if single-part; topics 6\u201310 each visibly include a 2 CFR Part 200 reference.\n- 2.0: One minor miss (e.g., one topic missing a two-part structure or one of topics #6\u2013#10 missing a visible citation) but overall structure intact.\n- 1.5: Two structural/citation misses total.\n- 1.0: Three misses OR one missing topic but most structure present.\n- 0.5: Four misses OR multiple topics missing.\n- 0.0: Not in a numbered 11-topic format OR major structural deviations.\n\nDo not judge correctness of citation mapping here\u2014only presence and structure.", "expectation": "A numbered list of 11 topics, two-part questions (except #7 allowed single), and visible 2 CFR Part 200 references for topics #6\u2013#10."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Structure-Enabled Correctness)", "description": "Now that the document is in the correct shape, verify content correctness and compliance details via code and LLM. Code rules are lightweight; LLM rules evaluate nuanced aspects.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "File is Document and Has Title (Light Check)", "description": "Confirm output is a document and contains the intended title text.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float or (float, str)\n    \"\"\"\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    if not output.is_document:\n        return 0.0\n    # Read text from DOCX/PDF with fallbacks\n    text = ''\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = ''\n    t = (text or '').lower()\n    if 'federal applicant - risk assessment tool' in t:\n        return 0.5\n    # Allow minor punctuation/case variation: strip punctuation and spaces\n    norm = re.sub(r\"[^a-z]+\", \" \", t).strip()\n    if 'federal applicant risk assessment tool' in norm:\n        return 0.3\n    return 0.0"}, {"type": "code", "name": "Approximate Length Check (1\u20132 Pages by Word Count)", "description": "Approximate 1\u20132 pages (roughly 250\u2013900 words).", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = ''\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = ''\n    words = re.findall(r\"\\b\\w+\\b\", text or '')\n    n = len(words)\n    # Full credit in a broad 1\u20132 page band; partial if slightly off\n    if 250 <= n <= 900:\n        return 0.5\n    if 150 <= n < 250 or 900 < n <= 1100:\n        return 0.25\n    return 0.0"}, {"type": "code", "name": "Topic Coverage by Keyword Match", "description": "Fuzzy keyword coverage check across the 11 required topics. Scores proportionally.", "weight": 0.7, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = ''\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = ''\n    t = (text or '').lower()\n    topics = {\n        1: [\"multiple sources\", \"revenue\", \"funding\", \"track separately\", \"segregat\"],\n        2: [\"written\", \"accounting polic\", \"procedure\"],\n        3: [\"financial management system\", \"track expenditure\", \"expenditure\", \"general ledger\", \"g/l\"],\n        4: [\"timing\", \"federal payment\", \"disbursement\", \"drawdown\", \"reimburse\"],\n        5: [\"internal control\", \"control activit\", \"segregation of dut\"],\n        6: [\"records retention\", \"retain record\", \"recordkeeping\", \"documentation retention\"],\n        7: [\"conflict of interest\", \"conflicts of interest\"],\n        8: [\"federal requirement\", \"uniform guidance\", \"2 cfr part 200\", \"point person\", \"knowledge\"],\n        9: [\"subaward\", \"subrecipient\", \"monitoring\", \"pass-through\"],\n        10: [\"timekeeping\", \"timesheet\", \"personnel\", \"effort reporting\"],\n        11: [\"high-risk\", \"high risk\", \"special condition\", \"high-risk status\"]\n    }\n    covered = 0\n    for i, kws in topics.items():\n        if any(kw in t for kw in kws):\n            covered += 1\n    # Proportional score\n    return 0.7 * (covered / 11.0)"}, {"type": "code", "name": "Presence of Uniform Guidance Citations (Topics 6\u201310)", "description": "Detect presence of 2 CFR Part 200 section references; proportional credit with target \u22655 occurrences.", "weight": 0.3, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = ''\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = ''\n    t = (text or '')\n    # Match variants like \"2 CFR 200.334\", \"2 CFR Part 200\", \"\u00a7 200.318\"\n    pat_sections = re.findall(r\"2\\s*CFR\\s*200\\s*\\.\\s*\\d+\", t, flags=re.I)\n    pat_part = re.findall(r\"2\\s*CFR\\s*Part\\s*200\", t, flags=re.I)\n    pat_section_symbol = re.findall(r\"\\u00A7\\s*200\\s*\\.\\s*\\d+|\u00a7\\s*200\\s*\\.\\s*\\d+\", t)\n    count = len(set([s.lower() for s in pat_sections])) + len(pat_part) + len(pat_section_symbol)\n    # Expect at least ~5 references (for topics 6\u201310). Proportional up to 5.\n    score = min(count / 5.0, 1.0) * 0.3\n    return score"}, {"type": "llm_judge", "name": "Two-Part Question Structure by Topic", "description": "Check that each topic uses a Yes/No lead-in plus an open-ended detail request (except topic #7 may be single-part).", "weight": 3.5, "judge_prompt": "Evaluate per-topic question structure:\n- For topics 1\u20135 and 8\u201311: Each should be a two-part question (in one or more sentences) where Part 1 can be answered Yes/No, and Part 2 solicits additional detail as an open-ended response.\n- Topic 6\u201310: These also should be two-part in the same way, while also including citations (citation correctness is graded elsewhere).\n- Topic 7 (conflicts of interest): May be single- or two-part; do not penalize if single-part.\n\nEvidence to look for:\n- Phrasing like \u201cYes/No\u201d prompts, or constructions that naturally allow a Yes/No start.\n- Follow-on prompts such as \u201cIf yes/no, please describe\u2026\u201d, \u201cProvide details\u2026\u201d, or equivalent open-ended detail requests.\n\nScoring (out of 3.5):\n- 3.5: 10/11 topics clearly meet the two-part structure (topic #7 exemption honored); minor wording variations OK.\n- 3.0: 8\u20139 topics clearly meet structure.\n- 2.0: 6\u20137 topics clearly meet structure.\n- 1.0: 4\u20135 topics clearly meet structure.\n- 0.0: Fewer than 4 topics show the two-part structure.\n\nFocus on structure, not writing quality.", "expectation": "Nearly all topics show Yes/No lead-ins and open-ended detail prompts (except topic 7 allowed single)."}, {"type": "llm_judge", "name": "Uniform Guidance Citations Relevance for Topics 6\u201310", "description": "Check that each of topics #6\u2013#10 includes a relevant Uniform Guidance (2 CFR Part 200) citation aligned with the topic.", "weight": 2.5, "judge_prompt": "Assess whether the questions for topics 6\u201310 each include a relevant citation to the Uniform Guidance, 2 CFR Part 200, and that the citation reasonably corresponds to the topic. Examples (not exhaustive):\n- 6 Records retention: 2 CFR 200.334 (or closely related)\n- 7 Conflicts of interest: 2 CFR 200.112 and/or 200.318(c)(1) (or closely related)\n- 8 Knowledge of federal requirements: references to 2 CFR Part 200 general compliance sections (e.g., Subpart D) are acceptable\n- 9 Subaward management/monitoring: 2 CFR 200.331\u2013.332 (or closely related)\n- 10 Timekeeping: 2 CFR 200.430 or 200.431, or current personnel compensation sections (or closely related under latest UG numbering)\n\nBe flexible to reasonable section numbering under current revisions; judge plausibility and alignment.\n\nScoring (out of 2.5):\n- 2.5: All five topics (6\u201310) include citations that are plausible and aligned to the topic.\n- 2.0: One topic is missing or has a weak/mismatched citation.\n- 1.0: Two topics missing or mismatched.\n- 0.5: Three topics missing or mismatched.\n- 0.0: Four or more missing/mismatched, or no UG citations at all for 6\u201310.", "expectation": "Each of topics 6\u201310 cites a plausible and suitably aligned 2 CFR Part 200 section."}, {"type": "llm_judge", "name": "Applicability and Pre-Award Risk Focus", "description": "Evaluate whether the tool is applicable to all applicant types and clearly supports pre-award risk assessment and potential conditions.", "weight": 2.0, "judge_prompt": "Judge the document on two dimensions:\n1) Applicability: Are the questions written in neutral, inclusive terms suitable for all applicant types (IHEs, nonprofits, local governments, etc.)? Avoids assumptions unique to a single entity type.\n2) Pre-award risk focus and usefulness: Do the questions elicit information that will help a federal agency assess risk prior to award and determine if special conditions are warranted (e.g., about systems, policies, monitoring, controls, timing, capacity)?\n\nScoring (out of 2.0):\n- 2.0: Clearly applicable to all applicant types and strongly oriented to pre-award risk assessment and potential conditions.\n- 1.5: Generally applicable and useful, with minor narrow phrasing or slight drift from pre-award focus.\n- 1.0: Some applicability issues or limited usefulness for risk assessment.\n- 0.5: Significant applicability issues or weak tie to pre-award risk.\n- 0.0: Not applicable to broad applicant types and not useful for pre-award risk.", "expectation": "Neutral language and strong pre-award risk alignment to support determining special conditions."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism", "description": "Holistic assessment of clarity, professionalism, and practical utility for compliance officers.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity and Plain Language", "description": "Questions are concise, clear, and understandable to a broad set of applicants.", "weight": 1.5, "judge_prompt": "Assess clarity and plain language:\n- Are questions concise and easily understood without legalese?\n- Do they avoid ambiguity and double-barreled constructions?\n- Would typical applicants (IHEs, nonprofits, local governments) understand what is being asked?\n\nScoring (out of 1.5):\n- 1.5: Very clear and accessible questions.\n- 1.0: Mostly clear; a few minor ambiguities.\n- 0.5: Noticeable ambiguity or complexity in multiple questions.\n- 0.0: Frequently unclear or confusing.", "expectation": "Concise, unambiguous, plain-language questions."}, {"type": "llm_judge", "name": "Professional Formatting and Organization", "description": "Professional presentation: title, consistent numbering, spacing, and logical flow across topics.", "weight": 1.5, "judge_prompt": "Evaluate presentation:\n- Clear title and professional formatting.\n- Consistent numbering 1\u201311 and logical ordering.\n- Adequate spacing, indentation, and typographic consistency conducive to completing the form.\n\nScoring (out of 1.5):\n- 1.5: Highly professional and consistent formatting.\n- 1.0: Generally professional; minor inconsistencies.\n- 0.5: Multiple formatting issues.\n- 0.0: Poorly formatted or disorganized.", "expectation": "A clean, professional 11-item format with consistent layout."}, {"type": "llm_judge", "name": "Actionability and Evidence Orientation", "description": "Prompts elicit actionable details (policies, systems, records, documentation, examples) to inform conditions/monitoring.", "weight": 1.5, "judge_prompt": "Judge whether the follow-up prompts request actionable detail:\n- Do they seek references to policies/procedures, system capabilities, records, samples, or controls?\n- Would answers meaningfully inform risk determination and potential special conditions or monitoring plans?\n\nScoring (out of 1.5):\n- 1.5: Follow-ups consistently ask for concrete, verifiable details.\n- 1.0: Generally actionable; some prompts are generic.\n- 0.5: Limited actionability.\n- 0.0: Vague prompts that won\u2019t inform risk decisions.", "expectation": "Follow-ups that drive concrete, verifiable information."}, {"type": "llm_judge", "name": "Regulatory Tone and Accuracy (Non-Overreaching)", "description": "Tone reflects stewardship and compliance expectations without misstating or overreaching beyond UG requirements.", "weight": 1.5, "judge_prompt": "Evaluate whether the document maintains an appropriate regulatory tone and avoids inaccuracies:\n- Tone: professional stewardship without intimidation.\n- Accuracy: does not misstate Uniform Guidance requirements.\n- Avoids overreach inconsistent with 2 CFR Part 200.\n\nScoring (out of 1.5):\n- 1.5: Accurate, balanced tone fully aligned with UG.\n- 1.0: Minor issues but generally aligned.\n- 0.5: Some questionable statements or tone.\n- 0.0: Misstatements or overreaching requirements.", "expectation": "Balanced, accurate compliance tone aligned with Uniform Guidance."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "105f8ad0-8dd2-422f-9e88-2be5fbd2b215", "rubric": {"category_name": "Wholesale Fragrance MSRP Benchmarking & Recommendation Model", "rationale": "Pattern A (Analytical). The deliverable is an Excel pricing model that structures competitive research and outputs SKU-level MSRP recommendations. Stage 1 uses LLM-only gating to enforce a strict workbook shape that makes verification trivial. Stage 2 mixes code rules (deterministic checks like bounds, math, and mapping) with LLM rules (criteria adherence, source rigor, and reasoning consistency). Code rules have lower weight relative to LLM rules, reflecting the nuanced judgment required. Stage 3 provides a holistic LLM quality assessment of modeling professionalism and strategic fit for a premium rebrand.", "max_total_score": 36.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate", "description": "LLM-only gate verifying the workbook\u2019s exact structure so downstream checks are trivial. If this fails, the entire category is zeroed.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Structured Excel Model Presence and Required Sheets/Sections", "description": "Checks that the output is an Excel workbook with the exact, verifiable structure and labeled sections required for automated validation and analysis.", "weight": 8.0, "judge_prompt": "You are verifying ONLY the presence and structure of an Excel pricing model. Do not judge correctness of calculations. Be flexible with sheet/section names if clearly equivalent. If the file is not an Excel spreadsheet or if core structure is missing, score 0.\n\nRequired format: Excel (.xlsx) with at least 5 sheets, professionally organized with headers and clearly labeled columns.\n\nRequired sheets and structures (accept near-equivalents in naming):\n1) \"SKU List\" (or \"Input SKUs\")\n   - A tabular list of the company\u2019s SKUs with columns including (flexible naming ok):\n     \u2022 SKU or Item Code\n     \u2022 Product Name\n     \u2022 Concentration (EDT, EDP, Elixir)\n     \u2022 Size (oz)\n     \u2022 Current MSRP (USD)\n     \u2022 COGS (USD)\n     \u2022 Size Range/Bucket (may be auto-generated here or elsewhere)\n\n2) \"Competitive Set\" (or \"Competitors\", \"Benchmark Data\")\n   - A research table of competitor items with columns including:\n     \u2022 Brand\n     \u2022 Fragrance Name/Product\n     \u2022 Concentration (EDT/EDP/Elixir)\n     \u2022 Size (oz)\n     \u2022 Distribution Channel (Macy\u2019s/Ulta/Sephora/Brand site)\n     \u2022 U.S. MSRP (USD, non-sale)\n     \u2022 Price per oz (USD/oz)\n     \u2022 Source URL (retailer or brand)\n     \u2022 As-of Date (target: September 2025)\n     \u2022 Eligibility/Include flag (TRUE/FALSE) and Exclusion Reason\n\n3) \"Competitor Averages\" (or \"Benchmark Averages\", \"Averages\")\n   - Aggregated table by Size Range/Bucket and Concentration including:\n     \u2022 Size Range (buckets per brief)\n     \u2022 Concentration\n     \u2022 Count of items\n     \u2022 Average MSRP per oz\n     \u2022 Min/Max (optional but helpful)\n\n4) \"Recommendations\" (or \"Pricing Recommendations\", \"Outputs\")\n   - A SKU-level output table with:\n     \u2022 SKU\n     \u2022 Concentration\n     \u2022 Size (oz)\n     \u2022 Current MSRP\n     \u2022 COGS\n     \u2022 Competitor Avg $/oz (matched bucket & concentration)\n     \u2022 New/Recommended MSRP (USD)\n     \u2022 New $/oz\n     \u2022 Deviation vs Competitor Avg (%)\n     \u2022 Gross Margin (%) and/or MSRP/COGS multiple\n     \u2022 Brief Rationale per SKU (1\u20133 sentences) OR a single rationale column with concise notes\n\n5) \"Methodology & Rationale\" (or \"Assumptions\", \"Notes & Methodology\")\n   - Clearly written sections covering:\n     \u2022 Data sources and date (target: September 2025, non-sale prices)\n     \u2022 Inclusion/exclusion criteria (channels, concentrations, size ranges, exclusions like gift sets/refills/LE/multi-packs)\n     \u2022 Size bucket mapping logic per the brief\n     \u2022 Concentration pricing logic (EDP vs EDT vs Elixir)\n     \u2022 MSRP rounding policy and how \u00b16% envelope vs competitor average per-oz is enforced\n\nScoring guidance (0 to 8 points):\n- 8: Excel file with all 5 sheets present, each with the required columns/sections listed above (minor naming variations ok). Tables are clearly labeled and coherent.\n- 6\u20137: Excel file with 4 of 5 required sheets present OR minor column omissions that don\u2019t block verification.\n- 3\u20135: Excel file present but only 2\u20133 required sheets or major omissions in required columns that hinder verification.\n- 1\u20132: Excel file present but skeletal; most required elements missing.\n- 0: Not an Excel file OR structure missing such that verification is not possible.\n\nOnly check presence/structure, not correctness of data or math.", "expectation": "An .xlsx workbook with the five required sheets, tables, and columns so automated checks are possible."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Compliance Verification", "description": "Deterministic checks (code) and nuanced reviews (LLM) to confirm the model follows task rules, math checks out, and recommendations comply with the \u00b16% envelope and logical concentration/COGS relationships.", "is_required": false, "max_points": 20.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Price-per-oz Math Consistency (Competitors and Recommendations)", "description": "Verifies that price-per-oz fields equal MSRP/Size within a small tolerance for both the Competitive Set and Recommendations sheets.", "weight": 1.0, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n    try:\n        x_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(x_path)\n        sheets = {s.lower(): s for s in xls.sheet_names}\n        def find_sheet(candidates):\n            for key, val in sheets.items():\n                if any(c in key for c in candidates):\n                    return val\n            return None\n        comp_sheet = find_sheet([\"competitive\", \"competitor\", \"benchmark\", \"market\", \"research\", \"comp \"])\n        rec_sheet = find_sheet([\"recommend\", \"pricing\", \"output\", \"msrp\", \"proposed\", \"new price\"])\n        if not comp_sheet or not rec_sheet:\n            return 0.0, \"Missing Competitive Set and/or Recommendations sheet.\"\n        comp = pd.read_excel(x_path, sheet_name=comp_sheet)\n        rec = pd.read_excel(x_path, sheet_name=rec_sheet)\n        def fuzzy_col(cols, keywords):\n            cols_l = [c for c in cols]\n            for c in cols_l:\n                cl = str(c).lower()\n                if any(k in cl for k in keywords):\n                    return c\n            return None\n        # Competitive set checks\n        c_msrp = fuzzy_col(comp.columns, [\"msrp\", \"price\"])\n        c_size = fuzzy_col(comp.columns, [\"oz\", \"size\"])\n        c_ppo  = fuzzy_col(comp.columns, [\"per oz\", \"per_oz\", \"oz price\", \"$/oz\", \"price/oz\"])\n        comp_score = 0.0\n        comp_n = 0\n        if c_msrp is not None and c_size is not None:\n            temp = comp[[c_msrp, c_size]].copy()\n            temp = temp.replace({np.nan: None})\n            def to_float(x):\n                if x is None: return None\n                try:\n                    if isinstance(x, str):\n                        x2 = re.sub(r\"[^0-9.]+\", \"\", x)\n                        return float(x2) if x2 != '' else None\n                    return float(x)\n                except:\n                    return None\n            temp['msrp'] = temp[c_msrp].apply(to_float)\n            temp['oz'] = temp[c_size].apply(to_float)\n            temp = temp[(temp['msrp']>0) & (temp['oz']>0)]\n            if not temp.empty:\n                comp_n = len(temp)\n                calc = (temp['msrp'] / temp['oz']).values\n                if c_ppo is not None and c_ppo in comp.columns:\n                    provided = comp.loc[temp.index, c_ppo].apply(to_float).values\n                    diffs = []\n                    for a,b in zip(calc, provided):\n                        if b is None or b==0:\n                            diffs.append(0.0)\n                        else:\n                            diffs.append(1.0 if abs(a-b)/b <= 0.02 else 0.0)\n                    comp_score = np.mean(diffs) if len(diffs)>0 else 0.0\n                else:\n                    # If no provided ppo, partial credit for being able to compute\n                    comp_score = 0.5\n        # Recommendations checks\n        r_msrp = fuzzy_col(rec.columns, [\"new msrp\", \"recommended\", \"proposed\", \"msrp\"])\n        r_size = fuzzy_col(rec.columns, [\"oz\", \"size\"])\n        r_ppo  = fuzzy_col(rec.columns, [\"new $/oz\", \"per oz\", \"per_oz\", \"oz price\", \"$/oz\", \"price/oz\"])\n        rec_score = 0.0\n        rec_n = 0\n        if r_msrp is not None and r_size is not None:\n            tmp = rec[[r_msrp, r_size]].copy().replace({np.nan: None})\n            def to_float2(x):\n                try:\n                    if isinstance(x, str):\n                        x2 = re.sub(r\"[^0-9.]+\", \"\", x)\n                        return float(x2) if x2!='' else None\n                    return float(x)\n                except:\n                    return None\n            tmp['msrp'] = tmp[r_msrp].apply(to_float2)\n            tmp['oz'] = tmp[r_size].apply(to_float2)\n            tmp = tmp[(tmp['msrp']>0) & (tmp['oz']>0)]\n            if not tmp.empty:\n                rec_n = len(tmp)\n                calc = (tmp['msrp'] / tmp['oz']).values\n                if r_ppo is not None and r_ppo in rec.columns:\n                    provided = rec.loc[tmp.index, r_ppo].apply(to_float2).values\n                    diffs = []\n                    for a,b in zip(calc, provided):\n                        if b is None or b==0:\n                            diffs.append(0.0)\n                        else:\n                            diffs.append(1.0 if abs(a-b)/b <= 0.02 else 0.0)\n                    rec_score = np.mean(diffs) if len(diffs)>0 else 0.0\n                else:\n                    rec_score = 0.5\n        # Aggregate\n        parts = [s for s in [comp_score, rec_score] if s>0]\n        if len(parts)==0:\n            return 0.0, \"Could not validate price-per-oz on either sheet.\"\n        score = float(np.mean(parts))\n        fb = f\"Comp ppo score={comp_score:.2f} (n={comp_n}), Rec ppo score={rec_score:.2f} (n={rec_n}).\"\n        return max(0.0, min(1.0, score)), fb\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Size Bucket Mapping Validity", "description": "Checks that sizes fall into required buckets (0.30\u20131.4, 1.5\u20132.9, 3.0\u20134.2, 4.3\u20136.8 oz) and, if a size-range column exists, that it matches the computed bucket for both SKU List and Competitive Set.", "weight": 1.0, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheets = {s.lower(): s for s in xls.sheet_names}\n        def find_sheet(cands):\n            for key,val in sheets.items():\n                if any(c in key for c in cands):\n                    return val\n            return None\n        sku_sheet = find_sheet([\"sku list\", \"input sku\", \"skus\", \"inputs\"])\n        comp_sheet = find_sheet([\"competitive\", \"competitor\", \"benchmark\", \"market\", \"research\"])\n        if not sku_sheet or not comp_sheet:\n            return 0.0\n        sku = pd.read_excel(path, sheet_name=sku_sheet)\n        comp = pd.read_excel(path, sheet_name=comp_sheet)\n        def fcol(cols, ks):\n            for c in cols:\n                cl = str(c).lower()\n                if any(k in cl for k in ks):\n                    return c\n            return None\n        def to_float(x):\n            try:\n                if isinstance(x, str):\n                    x2 = re.sub(r\"[^0-9.]+\", \"\", x)\n                    return float(x2) if x2!='' else None\n                return float(x)\n            except:\n                return None\n        def bucket(oz):\n            if oz is None or np.isnan(oz):\n                return None\n            if 0.30 <= oz <= 1.40: return \"0.30\u20131.4\"\n            if 1.50 <= oz <= 2.90: return \"1.5\u20132.9\"\n            if 3.00 <= oz <= 4.20: return \"3.0\u20134.2\"\n            if 4.30 <= oz <= 6.80: return \"4.3\u20136.8\"\n            return None\n        # SKU sheet\n        s_size = fcol(sku.columns, [\"oz\", \"size\"]) \n        s_bucket = fcol(sku.columns, [\"bucket\", \"size range\", \"range\"])\n        sc = 0.0\n        if s_size is not None:\n            sz = sku[s_size].apply(to_float)\n            calc_b = sz.apply(bucket)\n            valid = calc_b.notna().mean() if len(calc_b)>0 else 0\n            sc = 0.5*valid\n            if s_bucket is not None:\n                declared = sku[s_bucket].astype(str).str.lower().str.replace(\" \", \"\")\n                calc_b2 = calc_b.fillna(\"\").astype(str).str.lower().str.replace(\" \", \"\")\n                match = (declared==calc_b2).mean() if len(declared)>0 else 0\n                sc += 0.5*match\n        # Competitive sheet\n        c_size = fcol(comp.columns, [\"oz\", \"size\"]) \n        c_bucket = fcol(comp.columns, [\"bucket\", \"size range\", \"range\"])\n        cc = 0.0\n        if c_size is not None:\n            sz = comp[c_size].apply(to_float)\n            calc_b = sz.apply(bucket)\n            valid = calc_b.notna().mean() if len(calc_b)>0 else 0\n            cc = 0.5*valid\n            if c_bucket is not None:\n                declared = comp[c_bucket].astype(str).str.lower().str.replace(\" \", \"\")\n                calc_b2 = calc_b.fillna(\"\").astype(str).str.lower().str.replace(\" \", \"\")\n                match = (declared==calc_b2).mean() if len(declared)>0 else 0\n                cc += 0.5*match\n        score = np.mean([sc, cc]) if (sc>0 or cc>0) else 0.0\n        return float(max(0.0, min(1.0, score)))\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "\u00b16% Envelope vs Competitor Average (by Bucket and Concentration)", "description": "Checks that new MSRP price-per-oz is within \u00b16% of the competitor average for its size bucket and concentration for the majority of SKUs.", "weight": 1.0, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet.\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheets = {s.lower(): s for s in xls.sheet_names}\n        def find_sheet(cands):\n            for k,v in sheets.items():\n                if any(c in k for c in cands):\n                    return v\n            return None\n        avg_sheet = find_sheet([\"average\", \"averages\", \"benchmark avg\", \"competitor avg\", \"comp avg\"]) \n        rec_sheet = find_sheet([\"recommend\", \"pricing\", \"output\", \"msrp\", \"proposed\", \"new price\"]) \n        if not avg_sheet or not rec_sheet:\n            return 0.0, \"Missing averages or recommendations sheet.\"\n        avg = pd.read_excel(path, sheet_name=avg_sheet)\n        rec = pd.read_excel(path, sheet_name=rec_sheet)\n        def fcol(cols, ks):\n            for c in cols:\n                cl = str(c).lower()\n                if any(k in cl for k in ks):\n                    return c\n            return None\n        def to_float(x):\n            try:\n                if isinstance(x, str):\n                    x2 = re.sub(r\"[^0-9.]+\", \"\", x)\n                    return float(x2) if x2!='' else None\n                return float(x)\n            except:\n                return None\n        # Map for averages\n        a_bucket = fcol(avg.columns, [\"bucket\", \"size range\", \"range\"]) \n        a_conc   = fcol(avg.columns, [\"concentration\", \"format\", \"type\"]) \n        a_avg    = fcol(avg.columns, [\"avg\", \"average\", \"mean\"]) \n        if not a_bucket or not a_conc or not a_avg:\n            return 0.0, \"Averages missing required columns.\"\n        avg_clean = avg[[a_bucket, a_conc, a_avg]].copy()\n        avg_clean.columns = ['bucket','conc','avgppo']\n        avg_clean['bucket'] = avg_clean['bucket'].astype(str).str.lower().str.strip()\n        avg_clean['conc'] = avg_clean['conc'].astype(str).str.lower().str.strip()\n        # Recs\n        r_size = fcol(rec.columns, [\"oz\", \"size\"]) \n        r_conc = fcol(rec.columns, [\"concentration\", \"format\", \"type\"]) \n        r_msrp = fcol(rec.columns, [\"new msrp\", \"recommended\", \"proposed\", \"msrp\"]) \n        if not r_size or not r_conc or not r_msrp:\n            return 0.0, \"Recommendations missing size/concentration/msrp.\"\n        def bucket(oz):\n            if oz is None or (isinstance(oz,float) and np.isnan(oz)): return None\n            if 0.30 <= oz <= 1.40: return \"0.30\u20131.4\"\n            if 1.50 <= oz <= 2.90: return \"1.5\u20132.9\"\n            if 3.00 <= oz <= 4.20: return \"3.0\u20134.2\"\n            if 4.30 <= oz <= 6.80: return \"4.3\u20136.8\"\n            return None\n        df = rec[[r_size, r_conc, r_msrp]].copy()\n        df['oz'] = df[r_size].apply(to_float)\n        df['conc'] = df[r_conc].astype(str).str.lower().str.strip()\n        df['msrp'] = df[r_msrp].apply(to_float)\n        df = df[(df['oz']>0) & (df['msrp']>0)]\n        if df.empty:\n            return 0.0, \"No valid rows.\"\n        df['bucket'] = df['oz'].apply(bucket)\n        df = df[df['bucket'].notna()]\n        if df.empty:\n            return 0.0, \"No bucketable rows.\"\n        # Join to averages by bucket+conc (flex: normalize labels)\n        av = avg_clean.copy()\n        av['bucket_n'] = av['bucket'].str.replace(\" \", \"\")\n        av['conc_n'] = av['conc'].str.replace(\".\", \"\").str.replace(\" \", \"\")\n        d2 = df.copy()\n        d2['bucket_n'] = d2['bucket'].str.lower().str.replace(\" \", \"\")\n        d2['conc_n'] = d2['conc'].str.replace(\".\", \"\").str.replace(\" \", \"\")\n        merged = pd.merge(d2, av[['bucket_n','conc_n','avgppo']], on=['bucket_n','conc_n'], how='left')\n        merged['ppo'] = merged['msrp']/merged['oz']\n        merged = merged.dropna(subset=['avgppo'])\n        if merged.empty:\n            return 0.0, \"No matches to averages.\"\n        merged['dev'] = (merged['ppo'] - merged['avgppo']).abs() / merged['avgppo']\n        within = (merged['dev'] <= 0.06).mean()\n        return float(max(0.0, min(1.0, within))), f\"Within \u00b16%: {within:.2%} of {len(merged)} matched rows.\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "COGS-to-MSRP Relationship & Concentration Premium Alignment", "description": "Checks markup plausibility (MSRP > COGS, non-extreme multiples), positive relationship between COGS/oz and MSRP/oz, and directional alignment of concentration premiums (EDP > EDT; Elixir > EDP where applicable).", "weight": 1.0, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheets = {s.lower(): s for s in xls.sheet_names}\n        def find_sheet(cands):\n            for k,v in sheets.items():\n                if any(c in k for c in cands):\n                    return v\n            return None\n        rec_sheet = find_sheet([\"recommend\", \"pricing\", \"output\", \"msrp\", \"proposed\", \"new price\"]) \n        if not rec_sheet:\n            return 0.0\n        rec = pd.read_excel(path, sheet_name=rec_sheet)\n        def fcol(cols, ks):\n            for c in cols:\n                cl = str(c).lower()\n                if any(k in cl for k in ks):\n                    return c\n            return None\n        def to_float(x):\n            try:\n                if isinstance(x, str):\n                    x2 = re.sub(r\"[^0-9.]+\", \"\", x)\n                    return float(x2) if x2!='' else None\n                return float(x)\n            except:\n                return None\n        cogs_c = fcol(rec.columns, [\"cogs\", \"cost\"])\n        msrp_c = fcol(rec.columns, [\"new msrp\", \"recommended\", \"proposed\", \"msrp\"]) \n        size_c = fcol(rec.columns, [\"oz\", \"size\"]) \n        conc_c = fcol(rec.columns, [\"concentration\", \"format\", \"type\"]) \n        if not cogs_c or not msrp_c or not size_c or not conc_c:\n            return 0.0\n        df = rec[[cogs_c, msrp_c, size_c, conc_c]].copy()\n        df['cogs'] = df[cogs_c].apply(to_float)\n        df['msrp'] = df[msrp_c].apply(to_float)\n        df['oz'] = df[size_c].apply(to_float)\n        df['conc'] = df[conc_c].astype(str)\n        df = df[(df['cogs']>0) & (df['msrp']>0) & (df['oz']>0)]\n        if df.empty:\n            return 0.0\n        # 1) Markup plausibility\n        df['markup'] = df['msrp']/df['cogs']\n        plaus = ((df['markup']>1.1) & (df['markup']<20)).mean()\n        # 2) COGS/oz vs MSRP/oz correlation\n        df['cogs_per_oz'] = df['cogs']/df['oz']\n        df['msrp_per_oz'] = df['msrp']/df['oz']\n        corr = df[['cogs_per_oz','msrp_per_oz']].corr().iloc[0,1]\n        corr_score = 1.0 if pd.notnull(corr) and corr>=0.3 else (0.5 if pd.notnull(corr) and corr>=0.1 else 0.0)\n        # 3) Concentration premium alignment (directional)\n        def norm_conc(s):\n            s = str(s).lower()\n            if 'elixir' in s: return 'elixir'\n            if 'parfum' in s or 'edp' in s: return 'edp'\n            if 'toilet' in s or 'edt' in s: return 'edt'\n            return s\n        df['conc_n'] = df['conc'].apply(norm_conc)\n        g = df.groupby('conc_n').median(numeric_only=True)[['cogs_per_oz','msrp_per_oz']]\n        premium_score = 0.0\n        checks = 0\n        # EDP vs EDT\n        if {'edp','edt'}.issubset(g.index):\n            checks += 1\n            dir_ok = np.sign(g.loc['edp','cogs_per_oz'] - g.loc['edt','cogs_per_oz']) == np.sign(g.loc['edp','msrp_per_oz'] - g.loc['edt','msrp_per_oz'])\n            premium_score += 1.0 if dir_ok else 0.0\n        # Elixir vs EDP\n        if {'elixir','edp'}.issubset(g.index):\n            checks += 1\n            dir_ok = np.sign(g.loc['elixir','cogs_per_oz'] - g.loc['edp','cogs_per_oz']) == np.sign(g.loc['elixir','msrp_per_oz'] - g.loc['edp','msrp_per_oz'])\n            premium_score += 1.0 if dir_ok else 0.0\n        prem = (premium_score/checks) if checks>0 else 0.0\n        # Aggregate: 0.5 markup plausibility + 0.25 corr + 0.25 premium direction\n        score = 0.5*plaus + 0.25*corr_score + 0.25*prem\n        return float(max(0.0, min(1.0, score)))\n    except Exception:\n        return 0.0"}, {"type": "llm_judge", "name": "Competitive Set Compliance with Criteria", "description": "Verifies that the competitive set used for averages respects distribution (Macy\u2019s/Ulta/Sephora/Brand site), concentration (EDT/EDP/Elixir), size buckets, and exclusions (gift sets/refills/LE/multi-packs).", "weight": 4.0, "judge_prompt": "Review the Excel workbook, focusing on the Competitive Set and Competitor Averages sheets (and any notes). Judge whether included items used for averages comply with ALL criteria:\n- Distribution: only Macy\u2019s, Ulta, Sephora, or Brand site prices are used.\n- Concentration: only EDT, EDP, or Elixir.\n- Sizes: use the specified comparison ranges/buckets per the brief.\n- Exclusions: gift sets, refills, limited editions, and multi-packs are excluded from averages.\n- The Competitor Averages summary reflects only eligible items.\n\nScoring (0\u20134):\n- 4: Clear, consistent adherence on all points; eligibility flags and exclusion reasons are evident; averages obviously exclude ineligible items.\n- 3: Minor lapses or a few ambiguous entries but unlikely to change averages materially.\n- 2: Noticeable gaps (e.g., some excluded types or off-channel prices included) that could bias averages.\n- 1: Widespread non-compliance or unclear filtering.\n- 0: Competitive set fundamentally does not follow criteria.", "expectation": "Only eligible items from specified channels, concentrations, and sizes are included in averages; excluded items clearly flagged."}, {"type": "llm_judge", "name": "Source Rigor and Dating (September 2025, non-sale)", "description": "Checks that each competitor price cites a source URL, is dated as of September 2025, and represents non-sale list pricing with brand-site preference when available.", "weight": 4.0, "judge_prompt": "Inspect the Competitive Set sheet for source rigor:\n- Each entry has a source URL (brand or retailer) and an As-of date.\n- Dates target September 2025. If a mix of dates exists, they should cluster near Sep 2025 with notes.\n- Prices appear to be regular (non-sale) list prices; brand site preferred when available.\n\nScoring (0\u20134):\n- 4: Nearly all entries have URLs and Sep 2025 dates; non-sale list prices are clear; brand site predominates when available.\n- 3: Minor gaps (a few missing dates/URLs) but overall acceptable and clearly non-sale.\n- 2: Multiple missing dates/URLs or unclear sale/list status; partial compliance.\n- 1: Sparse sourcing and dating; unclear price status.\n- 0: Little to no sourcing and dating present.", "expectation": "Well-documented sources with URLs and Sep 2025 dates; non-sale list prices prioritized from brand sites."}, {"type": "llm_judge", "name": "Concentration Logic Across Sizes", "description": "Evaluates whether the recommended pricing maintains logical relationships across concentrations (EDT < EDP < Elixir), reasonably consistent premiums, and no inversions for like sizes.", "weight": 4.0, "judge_prompt": "Review the Recommendations and Methodology sheets. Assess whether recommended price-per-oz generally increases with concentration for comparable sizes (EDT < EDP < Elixir), with reasonably consistent premiums across sizes. Minor exceptions are acceptable if justified.\n\nScoring (0\u20134):\n- 4: No inversions for comparable sizes; premiums are consistent and explained.\n- 3: Mostly consistent with a few small anomalies, adequately justified.\n- 2: Several inconsistencies or weak justification.\n- 1: Frequent inversions or erratic premiums.\n- 0: No discernible concentration logic.", "expectation": "Clear, consistent pricing hierarchy by concentration with rationale."}, {"type": "llm_judge", "name": "Envelope Exceptions Visibility and Justification", "description": "Assesses whether any deviations outside the \u00b16% envelope are clearly flagged and well-justified in the Recommendations or Methodology.", "weight": 4.0, "judge_prompt": "Check whether the Recommendations sheet clearly flags deviation vs competitor average (%) and whether any items outside the \u00b16% per-oz envelope are explicitly annotated with concise justifications (e.g., brand equity, rounding to target price points, pack uniqueness). Rationale should be credible and consistent with the premium rebranding goal.\n\nScoring (0\u20134):\n- 4: All deviations are clearly flagged and convincingly justified; rounding policy is explicit; envelope rules are respected or exceptions are rare and documented.\n- 3: Minor missing flags or thin justifications but overall transparency.\n- 2: Several unflagged deviations or weak justifications.\n- 1: Many unflagged deviations; little justification.\n- 0: No deviation tracking or justification.", "expectation": "Deviations, if any, are transparent and well-justified in context of premium positioning."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Modeling Quality and Strategic Fit", "description": "Holistic quality assessment of modeling professionalism, clarity, and strategic alignment with a premium rebrand.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Modeling and Readability", "description": "Evaluates formatting, labeling, and usability (clean headers, units, consistent currency formats, freeze panes, filters, and no broken references).", "weight": 2.0, "judge_prompt": "Assess the workbook\u2019s professionalism: clear sheet names, readable tables, consistent units and currency formats, sensible sorting/filters, helpful notes, and no obvious broken references. Presentation should facilitate stakeholder review without confusion.\n\nScoring (0\u20132):\n- 2: Highly professional, clean, and easy to read; thoughtful layout.\n- 1: Adequate but with formatting inconsistencies or clutter.\n- 0: Messy or confusing; hinders review.", "expectation": "A clean, readable workbook with professional formatting and labels."}, {"type": "llm_judge", "name": "Strategic Alignment with Premium Rebrand", "description": "Judges whether the pricing narrative and recommendations support a premium, competitive positioning while maintaining credible value vs. COGS.", "weight": 2.0, "judge_prompt": "Evaluate whether the methodology and SKU-level rationales articulate a coherent strategy that supports premium rebranding while remaining competitive: explains relationship to COGS, competitive positioning, and brand equity. Pricing should be aspirational yet credible.\n\nScoring (0\u20132):\n- 2: Strong, coherent narrative with clear strategic alignment.\n- 1: Partial alignment; strategy is present but underdeveloped.\n- 0: Weak or no strategic framing.", "expectation": "A succinct, convincing narrative that ties data to premium positioning and COGS realities."}, {"type": "llm_judge", "name": "Usability and Maintainability", "description": "Assesses whether the model is straightforward to update (e.g., clearly marked inputs, minimal hardcoding, documented assumptions).", "weight": 2.0, "judge_prompt": "Review whether inputs are clearly marked, assumptions are documented, and the model avoids unnecessary hardcoding. It should be easy to refresh competitor data and have formulas that adapt to new rows without breaking.\n\nScoring (0\u20132):\n- 2: Easy to update; assumptions and inputs are obvious; formulas robust.\n- 1: Some update friction or undocumented assumptions.\n- 0: Fragile or opaque; difficult to maintain.", "expectation": "Clearly marked inputs and documented assumptions; formulas generalize without brittle hardcoding."}, {"type": "llm_judge", "name": "Risk, Sensitivity, and Rounding Policy Clarity", "description": "Checks for articulation of key risks/limitations, simple sensitivity (e.g., \u00b1$ per-oz shifts), and clear rounding policy for final MSRPs.", "weight": 2.0, "judge_prompt": "Inspect the Methodology/Notes for acknowledgment of risks (e.g., data freshness, retailer disparities), any simple sensitivity view (e.g., impact of \u00b1$0.25/oz on MSRPs), and a clear rounding policy (e.g., $XX.00, $XX.50). \n\nScoring (0\u20132):\n- 2: Risks and sensitivities documented; rounding policy clearly stated and applied consistently.\n- 1: Partial coverage (mentions but lacks clarity or consistency).\n- 0: Not addressed.", "expectation": "Transparent risks, at least a simple sensitivity, and a coherent rounding policy applied consistently."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "1e5a1d7f-12c1-48c6-afd9-82257b3f2409", "rubric": {"category_name": "Weekly PM Schedule (.docx) - Real Estate Property Management", "rationale": "This rubric enforces a self-documenting, verifiable deliverable: a DOCX schedule table with specific columns and cyclical PM tasks. Stage 1 uses an LLM gate to mandate exact structure so later checks are trivial. Stage 2 mixes light code checks (headers, days, times, week labels) with heavier LLM verification (specificity, cyclical coverage, feasibility). Stage 3 holistically assesses professional quality and usefulness for property managers.", "max_total_score": 14.3, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate", "description": "Gate: Output must be a DOCX with a single primary schedule table having the four required columns and clear weekly/cyclical structure.", "is_required": true, "max_points": 2.0, "min_score_to_pass": 1.4, "rules": [{"type": "llm_judge", "name": "Structured DOCX Schedule Table Present", "description": "Check that the output is a DOCX document containing one main schedule table with the four required columns and rows covering time-of-day entries and week-of-month designations.", "weight": 2.0, "judge_prompt": "You are evaluating whether the candidate output satisfies STRICT structure requirements for a weekly schedule document. Only check PRESENCE and STRUCTURE, not content quality.\n\nRequirements:\n- File format: Prefer DOCX (Word). If clearly not a document (e.g., plain text or spreadsheet), score 0. If it appears as a PDF with the correct table structure, allow partial credit.\n- One primary schedule table is clearly present (can be on the first page). If multiple tables exist, one must be clearly the main schedule.\n- The main table must have exactly or effectively four columns labeled (flexible naming allowed if clearly equivalent):\n  1) \"Time\" (e.g., \"Time\", \"Time of Day\", \"Time Slot\")\n  2) \"Activity\" (e.g., \"Activity\", \"Focus\", \"Task\")\n  3) \"Details/Tracker\" (variations allowed like \"Details & Tracker\", \"Details - Tracker\", or two adjacent headers \"Details\" and \"Tracker\" merged visually)\n  4) \"Week of the Month\" (e.g., \"Week of Month\", \"Week #\", \"Wk 1-4/5\", or similar)\n- Rows include concrete time-of-day entries (e.g., 8:00 AM, 09:30, AM/PM blocks) and week-of-month designations (e.g., Week 1, Week 2, Week 3, Week 4, and optionally Week 5).\n- Days-of-week coverage is visible (labels or implicit grouping such as Monday\u2013Friday or Monday\u2013Sunday). Exact day labels may be in a separate column, within cells, or as grouped sections above the table.\n\nScoring:\n- 2.0: DOCX with one clear main table; all four required columns labeled (or obvious equivalents); rows show time-of-day values and week-of-month designations; days-of-week coverage is evident.\n- 1.6: Document (DOCX or PDF) has a main schedule table; three of the four columns clearly labeled; minor naming variance or week/day labeling embedded in cells but still clearly usable.\n- 1.0: Document (DOCX or PDF) has a table resembling a schedule but missing multiple required column labels or lacking visible time-of-day or week-of-month values.\n- 0.0: Not a document; no table; or structure too ambiguous to recognize as the required schedule.\n\nDo not evaluate content correctness or professionalism in this stage\u2014only format and presence of the required structural elements.", "expectation": "A DOCX file with a main schedule table containing four correctly labeled columns and rows covering time-of-day and week-of-month across days."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification of Content Correctness and Completeness", "description": "Verify that the schedule actually encodes a realistic, cyclical PM workflow: correct headers present, days and times specified, and week-of-month logic appears. Use code for simple text signals and LLM for nuanced checks.", "is_required": false, "max_points": 6.3, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Header Labels Presence (Flexible)", "description": "Detects whether the four target headers (or close variants) appear in the document text. Partial credit based on how many are detected.", "weight": 0.3, "code": "import re\\n\\n\\ndef _read_any_text(context, output):\\n    text = None\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_text(output.id)\\n            except Exception:\\n                text = None\\n    return text or \"\"\\n\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output:\\n        return 0.0, \"No output resource provided.\"\\n\\n    text = _read_any_text(context, output).lower()\\n    if not text.strip():\\n        return 0.0, \"Unable to read document text.\"\\n\\n    # Flexible header detection\\n    has_time = bool(re.search(r\"\\\\btime( of day| slot)?\\\\b\", text))\\n    has_activity = bool(re.search(r\"\\\\b(activity|focus|task)s?\\\\b\", text))\\n    has_details_tracker = bool(re.search(r\"details\\\\s*([/&-]|and)\\\\s*tracker\", text)) or (\"details\" in text and \"tracker\" in text)\\n    has_week_of_month = bool(re.search(r\"week\\\\s+of\\\\s+(the\\\\s+)?month\", text)) or bool(re.search(r\"\\\\b(wk|week)\\\\s*[1-5]\\\\b\", text)) or bool(re.search(r\"(1st|2nd|3rd|fourth|fifth)\\\\s+week\", text))\\n\\n    count = sum([has_time, has_activity, has_details_tracker, has_week_of_month])\\n    score = count / 4.0\\n    feedback = f\"Headers detected: time={has_time}, activity={has_activity}, details_tracker={has_details_tracker}, week_of_month={has_week_of_month}.\"\\n    return max(0.0, min(1.0, score)), feedback"}, {"type": "code", "name": "Days of Week Coverage", "description": "Checks for presence of distinct weekday names or abbreviations to ensure coverage across the week.", "weight": 0.3, "code": "import re\\n\\n\\ndef _read_any_text(context, output):\\n    text = None\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_text(output.id)\\n            except Exception:\\n                text = None\\n    return text or \"\"\\n\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output:\\n        return 0.0, \"No output resource provided.\"\\n\\n    text = _read_any_text(context, output).lower()\\n    days = [\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \"sunday\",\\n            \"mon\", \"tue\", \"tues\", \"wed\", \"thu\", \"thur\", \"thurs\", \"fri\", \"sat\", \"sun\"]\\n    found = set()\\n    for d in days:\\n        if re.search(r\"\\\\b\" + re.escape(d) + r\"\\\\b\", text):\\n            # Map abbreviations to full names for uniqueness\\n            if d.startswith(\"mon\"): found.add(\"monday\")\\n            elif d.startswith(\"tue\"): found.add(\"tuesday\")\\n            elif d.startswith(\"wed\"): found.add(\"wednesday\")\\n            elif d.startswith(\"thu\"): found.add(\"thursday\")\\n            elif d.startswith(\"fri\"): found.add(\"friday\")\\n            elif d.startswith(\"sat\"): found.add(\"saturday\")\\n            elif d.startswith(\"sun\"): found.add(\"sunday\")\\n            else: found.add(d)\n\\n    # Expect at least Mon-Fri coverage for full credit\\n    target = 5\\n    covered = len(found.intersection({\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"sunday\"}))\\n    score = min(1.0, covered / target)\\n    feedback = f\"Days detected: {sorted(list(found))}.\"\\n    return max(0.0, min(1.0, score)), feedback"}, {"type": "code", "name": "Time-of-Day Present", "description": "Verifies presence of explicit time-of-day markers (HH:MM or AM/PM).", "weight": 0.3, "code": "import re\\n\\n\\ndef _read_any_text(context, output):\\n    text = None\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_text(output.id)\\n            except Exception:\\n                text = None\\n    return text or \"\"\\n\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output:\\n        return 0.0, \"No output resource provided.\"\\n\\n    text = _read_any_text(context, output).lower()\\n    # Times like 8:00, 14:30, 9am, 9 am, 09:00 AM\\n    patterns = [r\"\\\\b([01]?\\\\d|2[0-3]):[0-5]\\\\d\\\\b\", r\"\\\\b[0-1]?\\\\d\\\\s?(am|pm)\\\\b\", r\"\\\\b([01]?\\\\d|2[0-3]):[0-5]\\\\d\\\\s?(am|pm)\\\\b\"]\\n    matches = set()\\n    for p in patterns:\\n        for m in re.findall(p, text):\\n            if isinstance(m, tuple):\\n                m = \":\".join([str(x) for x in m if x]) if any(m) else \"\"\\n            if m:\\n                matches.add(str(m))\\n    count = len(matches)\\n    # Expect at least 3 distinct time markers for full credit\\n    score = 1.0 if count >= 3 else max(0.0, min(1.0, count / 3.0))\\n    feedback = f\"Distinct time markers found: {min(count,10)} (showing up to 10).\"\\n    return score, feedback"}, {"type": "code", "name": "Week-of-Month Coverage", "description": "Checks that week-of-month designations (Week 1-4/5 or equivalents) appear.", "weight": 0.3, "code": "import re\\n\\n\\ndef _read_any_text(context, output):\\n    text = None\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_text(output.id)\\n            except Exception:\\n                text = None\\n    return text or \"\"\\n\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output:\\n        return 0.0, \"No output resource provided.\"\\n\\n    text = _read_any_text(context, output).lower()\\n    weeks_found = set()\\n    # Patterns: Week 1, Wk 2, 1st week, first week, etc.\\n    for i, ordw in enumerate([\"first\",\"second\",\"third\",\"fourth\",\"fifth\"], start=1):\\n        if re.search(rf\"\\\\bweek\\\\s*{i}\\\\b|\\\\bwk\\\\s*{i}\\\\b|\\\\b{ordw}\\\\s+week\\\\b|\\\\b{i}(st|nd|rd|th)\\\\s+week\\\\b\", text):\\n            weeks_found.add(i)\\n    # Expect at least Weeks 1-4 for full credit\\n    score = 1.0 if len(weeks_found) >= 4 else max(0.0, len(weeks_found) / 4.0)\\n    feedback = f\"Weeks detected: {sorted(list(weeks_found))}.\"\\n    return score, feedback"}, {"type": "llm_judge", "name": "Actionability and Tracker Specificity", "description": "Activities should be concrete and paired with Details/Tracker guidance (systems, reports, sources: e.g., Yardi/AppFolio, work order queues, AR aging, inspection checklists, vendor portals).", "weight": 1.7, "judge_prompt": "Evaluate whether the schedule rows are actionable and include clear tracker/source guidance. For most rows, the Activity should be a specific action (e.g., \"Process work orders\", \"Conduct unit inspections\"), and the Details/Tracker cell should identify where to look or what to update (e.g., PMS system like Yardi/AppFolio/Buildium, AR aging, delinquency list, move-in/move-out calendar, vendor portal, shared drive template).\\n\\nScoring:\\n- 1.7: Clear, actionable activities paired with specific trackers/sources for a strong majority (\u226580%) of rows.\\n- 1.1: Activities are mostly actionable but tracker/source specificity is inconsistent (\u224850\u201379% of rows).\\n- 0.6: Many generic activities; few rows point to specific trackers/sources (\u224820\u201349%).\\n- 0.0: Vague activities with no usable tracker/source guidance.", "expectation": "Each activity row includes where to pull data from or what to update (PMS modules, reports, calendars, checklists)."}, {"type": "llm_judge", "name": "Cyclical PM Coverage by Week", "description": "Checks that tasks align with the monthly property management cycle across weeks (rent cycle, delinquencies, reporting, inspections, vendor/turnover coordination, renewals).", "weight": 1.7, "judge_prompt": "Assess whether tasks are distributed across weeks in a way that reflects property management cycles: examples include Week 1 rent collection/posting and bank reconciliations; Week 2 delinquency follow-up/late notices; Week 3 ongoing work orders, inspections, vendor management; Week 4 month-end activities (owner reports, accruals/approvals), renewals outreach (60\u201390 days out), turns/unit readiness. Flexibility allowed, but the weekly structure should make cyclical sense and cover core duties.\\n\\nScoring:\\n- 1.7: Clear, well-distributed cycle coverage across at least Weeks 1\u20134, touching rent, AR/delinquency, maintenance/vendor coordination, inspections, and owner/portfolio reporting.\\n- 1.1: Covers most core areas with minor gaps or clustering.\\n- 0.6: Partial cycle with significant omissions (e.g., no owner reporting or no delinquency follow-up).\\n- 0.0: Lacks recognizable monthly PM cycle mapping.", "expectation": "Weeks map to common PM cycles: rent/AR, delinquencies, inspections/maintenance, reporting/approvals, renewals/turns."}, {"type": "llm_judge", "name": "Feasibility and Load Balancing", "description": "Evaluates whether the schedule avoids time conflicts and balances workload across days and weeks.", "weight": 1.7, "judge_prompt": "Check for practical feasibility: time blocks should not require the same manager to be in two places at once; tasks should fit typical workday constraints; there should be a sensible mix of office time (email/AR/approvals) and field time (inspections/vendor meetings). Some buffer time or catch-up periods are ideal.\\n\\nScoring:\\n- 1.7: No obvious conflicts; balanced workload with realistic timing and buffers.\\n- 1.1: Minor crowding but mostly feasible.\\n- 0.6: Frequent tight stacking or conflicts likely.\\n- 0.0: Unrealistic or conflicting schedule.", "expectation": "A realistic, non-conflicting weekly cadence with balanced office/field tasks."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism Assessment", "description": "Holistic LLM assessment of presentation quality, clarity, usefulness, and domain appropriateness.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation", "description": "Checks for professional formatting: clear title, consistent table styling, legible layout, and basic document hygiene.", "weight": 1.5, "judge_prompt": "Evaluate presentation quality: Is there a professional title (e.g., \"Weekly PM Schedule\"), optional date or portfolio label, consistent fonts, readable table (header row distinguished, adequate column widths), and overall clean layout? Typos should be minimal.\\n\\nScoring:\\n- 1.5: Polished, professional, and easy to read.\\n- 1.0: Generally good with minor formatting issues.\\n- 0.5: Messy or inconsistent formatting but usable.\\n- 0.0: Unprofessional or hard to read.", "expectation": "A clean, professional DOCX with a clear title and readable table."}, {"type": "llm_judge", "name": "Clarity and Usability for PM Team", "description": "Assesses whether language is concise, consistent, and easy for property managers to follow.", "weight": 1.5, "judge_prompt": "Is the language clear and concise? Do column entries use consistent, action-oriented phrasing (verbs first), standardized time formats, and consistent week labels? Would a property manager quickly understand what to do and where to look?\\n\\nScoring:\\n- 1.5: Very clear and easy to use; consistent terminology and formatting.\\n- 1.0: Mostly clear with occasional inconsistencies.\\n- 0.5: Some confusion or mixed terminology.\\n- 0.0: Unclear or confusing for intended users.", "expectation": "Consistent verbs and formats that enable quick comprehension and action."}, {"type": "llm_judge", "name": "Practical Utility and Adaptability", "description": "Judges whether the schedule is practically useful and adaptable across properties/portfolios.", "weight": 1.5, "judge_prompt": "Assess practical utility: Does the schedule feel directly usable week-to-week? Are there cues that make it adaptable (e.g., placeholders for property names, adjustable time blocks, references to configurable PMS reports)?\\n\\nScoring:\\n- 1.5: Highly practical and adaptable.\\n- 1.0: Useful but limited adaptability.\\n- 0.5: Barely usable without substantial edits.\\n- 0.0: Not practically useful.", "expectation": "A schedule that can be applied broadly with minor tweaks."}, {"type": "llm_judge", "name": "Risk/Compliance and Domain Awareness", "description": "Checks inclusion of common PM risk/compliance elements and domain understanding.", "weight": 1.5, "judge_prompt": "Look for domain-aware elements: COI tracking, safety/fire-life-safety inspections, notice timelines, fair housing awareness, vendor W-9/insurance checks, SLA follow-ups, lease renewals (60\u201390 days), move-in/move-out coordination, month-end reporting. Not all must be present, but some evidence of risk/compliance awareness should exist.\\n\\nScoring:\\n- 1.5: Strong coverage of risk/compliance tasks integrated into the cycle.\\n- 1.0: Some coverage present.\\n- 0.5: Minimal coverage.\\n- 0.0: None evident.", "expectation": "Schedule reflects real-world PM risks/compliance and key recurring duties."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "327fbc21-7d26-4964-bf7c-f4f41e55c54d", "rubric": {"category_name": "Wholesale Trade \u2013 First-Line Supervisors (Non-Retail) | By-Door May Sales Plan (Candy/Merch Planning)", "rationale": "Analytical Pattern A with an Excel workbook as the primary deliverable. Stage 1 is an LLM-only structural gate mandating a precise, self-documenting workbook shape that enables deterministic checks. Stage 2 mixes lightweight code rules (bounds/consistency) with higher-weight LLM judgments (cross-references, reasonableness, methodology). Stage 3 evaluates professional quality and communication for stakeholder readiness.", "max_total_score": 10.5, "stages": [{"name": "Stage 1 \u2013 Shape Enforcement Gate (Workbook Structure)", "description": "LLM-only gate verifying exact, self-documenting Excel structure enabling verification. If this fails, the entire category scores zero.", "is_required": true, "max_points": 2.0, "min_score_to_pass": 1.4, "rules": [{"type": "llm_judge", "name": "Workbook Structure: By-Door Plan + Summary", "description": "Output must be an Excel workbook with mandated sheets, tables, and summary elements.", "weight": 2.0, "judge_prompt": "You are checking ONLY the STRUCTURE/FORMAT (not calculation correctness).\nConfirm the output is a valid Excel workbook (.xlsx or .xls) with the following structure. Be flexible with column names but ensure each functional group exists clearly.\n\nRequired Sheets:\n1) Main by-door planning sheet named like one of: \"By Door Plan\", \"May By Door Plan\", \"May Sales Plan\", or similar. It must contain a single main table with headers and these column groups:\n   A. Store identification: includes a \"Store ID\" column (name may vary like \"Store ID#\", \"ID\"). \"Store Name\" is optional.\n   B. Status/Active indicator: a column that indicates whether a store is Active vs Closed (examples: a column named \"Status\" or \"Active\" where active marked with \"x\" and closed marked with \"Closed\").\n   C. LY May weekly sales: four columns for Last Year (LY) sales by week for May: W1, W2, W3, W4 (e.g., \"LY W1\", \"LY Week 1\").\n   D. LY May Total: a column summing LY W1\u2013W4 (e.g., \"LY May Total\").\n   E. STD Sales and LY STD Sales: two columns (e.g., \"STD Sales\" and \"LY STD Sales\").\n   F. STD Trend %: a column representing percent change TY/LY (e.g., \"STD Trend %\").\n   G. Planned May weekly sales: four columns for plan by week: W1, W2, W3, W4 (e.g., \"Plan W1\", \"W1 Plan\").\n   H. Plan May Total: a column summing planned W1\u2013W4.\n   I. Plan % to LY: a column showing percent change over LY (e.g., \"% to LY\", \"Plan % vs LY\").\n   J. Notes column is optional but acceptable if present.\n   The table should include both Active and Closed doors, where only Active doors are expected to receive planned values (Closed doors should not be planned).\n\n2) A separate \"Summary\" sheet (name like \"Summary\", \"Rollup\", or similar) containing:\n   - A rollup table with rows: Total Stores, Closed Stores, Comp Stores (Comp = Total \u2013 Closed). Columns must include (names may vary): LY May Total $, Plan May Total $, and Plan % to LY.\n   - A short narrative (1\u20132 sentences) that states: the May Sales Plan dollars and percent change vs LY for Total Stores and for Comparable Stores, and the LY volume from stores that are now closed.\n\nFormatting expectations:\n- Values should be visible in table cells (not just screenshots or empty shells). Number formatting can vary; do not judge correctness.\n\nScoring:\n- 2.0: Correct Excel format with both required sheets present. By-door table includes all column groups A\u2013I (J optional). Summary sheet includes rollup table with the 3 rows and required columns, plus the 1\u20132 sentence narrative.\n- 1.4: Minor deviations in sheet naming or one minor column group is missing (e.g., explicit STD Trend % absent but both STD Sales and LY STD Sales present). Summary has rollup table and narrative present.\n- 0.8: Only one of the required sheets is present OR multiple column groups in the by-door sheet are missing, but it is still clearly an Excel sales planning workbook.\n- 0.0: Not an Excel file OR structure missing to the point that verification is not possible.\n\nOnly evaluate structure and presence. Do NOT check if numbers or logic are correct.", "expectation": "A two-sheet Excel workbook: an itemized By-Door planning table with LY weekly, LY total, STD TY/LY and trend, plan weekly, plan total, and plan % to LY; plus a Summary sheet with Total/Closed/Comp rollups and a 1\u20132 sentence narrative."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification (Correctness and Compliance)", "description": "Deterministic checks and LLM verification now that the shape is enforced. Code rules focus on numeric consistency; LLM judges assess cross-sheet alignment, methodology, and reasonableness vs guidance.", "is_required": false, "max_points": 5.5, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Row Totals: LY and Plan Weekly Sum Consistency", "description": "Checks that LY May Total equals the sum of LY W1\u2013W4 and Plan May Total equals the sum of Plan W1\u2013W4 (for active stores). Partial credit based on fraction of compliant rows.", "weight": 0.25, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        out = context.get_primary_output()\n        if not out or not out.is_spreadsheet:\n            return 0.0, \"No spreadsheet output\"\n        path = context.files.get_path(out.id)\n        xls = pd.ExcelFile(path)\n        # Heuristic: choose sheet likely to be the by-door plan\n        plan_sheet = None\n        for s in xls.sheet_names:\n            ns = re.sub(r'[^a-z0-9]', '', s.lower())\n            if any(k in ns for k in [\"bydoor\",\"plan\",\"maysales\",\"mayplan\",\"doorplan\"]):\n                plan_sheet = s\n                break\n        if plan_sheet is None:\n            plan_sheet = xls.sheet_names[0]\n        df = context.files.read_excel(out.id, sheet_name=plan_sheet)\n        df.columns = [str(c) for c in df.columns]\n        def norm(s):\n            return re.sub(r'[^a-z0-9]', '', str(s).lower())\n        cols = list(df.columns)\n        # Find weekly LY columns\n        def find_week_col(label, i):\n            keys = [f\"w{i}\", f\"wk{i}\", f\"week{i}\"]\n            for c in cols:\n                nc = norm(c)\n                if any(k in nc for k in keys):\n                    if label == 'ly' and 'ly' in nc and 'plan' not in nc:\n                        return c\n                    if label == 'plan' and 'plan' in nc:\n                        return c\n            return None\n        ly_w = [find_week_col('ly', i) for i in range(1,5)]\n        plan_w = [find_week_col('plan', i) for i in range(1,5)]\n        # Totals\n        def find_col(includes, excludes=None):\n            for c in cols:\n                nc = norm(c)\n                if all(k in nc for k in includes) and (not excludes or all(k not in nc for k in excludes)):\n                    return c\n            return None\n        ly_total_col = find_col(['ly','total']) or find_col(['ly','may','total'])\n        plan_total_col = find_col(['plan','total']) or find_col(['may','plan','total'])\n        status_col = (find_col(['status']) or find_col(['active']) or find_col(['store','status']) or find_col(['door','status']))\n        # Convert numerics\n        def clean_num(s):\n            return pd.to_numeric(s.replace({'\\\\$':'', ',':''}, regex=True), errors='coerce')\n        # Prepare masks\n        is_active = pd.Series(True, index=df.index)\n        if status_col is not None:\n            sc = df[status_col].astype(str).str.lower()\n            is_closed = sc.str.contains('closed')\n            active_flag = sc.str.contains('x') | sc.str.contains('open') | sc.str.contains('active')\n            # If any explicit closed flags, set active accordingly\n            if is_closed.any():\n                is_active = ~is_closed\n            elif active_flag.any():\n                is_active = active_flag\n        # Check LY totals\n        checks = []\n        if all(c is not None for c in ly_w) and ly_total_col is not None:\n            ly_vals = [clean_num(df[c]) for c in ly_w]\n            ly_sum = sum(ly_vals)\n            ly_total = clean_num(df[ly_total_col])\n            ly_ok = (ly_sum - ly_total).abs() <= 0.51  # allow rounding\n            # consider rows with at least one LY week value\n            ly_mask = pd.concat(ly_vals, axis=1).notna().any(axis=1) & ly_total.notna()\n            if ly_mask.sum() > 0:\n                checks.append(ly_ok[ly_mask].mean())\n        # Check Plan totals for active rows only\n        if all(c is not None for c in plan_w) and plan_total_col is not None:\n            plan_vals = [clean_num(df[c]) for c in plan_w]\n            plan_sum = sum(plan_vals)\n            plan_total = clean_num(df[plan_total_col])\n            plan_ok = (plan_sum - plan_total).abs() <= 0.51\n            plan_mask = pd.concat(plan_vals, axis=1).notna().any(axis=1) & plan_total.notna() & is_active\n            if plan_mask.sum() > 0:\n                checks.append(plan_ok[plan_mask].mean())\n        if not checks:\n            return 0.0, \"Could not locate required LY/Plan weekly and total columns\"\n        score = float(np.mean(checks))\n        return max(0.0, min(1.0, score)), f\"Mean consistency across checks: {np.mean(checks):.2f}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Rounding to $50 and Minimum Floor", "description": "Verifies that all positive weekly plan values are multiples of $50 and are at least $50. Partial credit based on compliant fraction across active stores.", "weight": 0.25, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        out = context.get_primary_output()\n        if not out or not out.is_spreadsheet:\n            return 0.0\n        path = context.files.get_path(out.id)\n        xls = pd.ExcelFile(path)\n        # Choose plan sheet heuristically\n        plan_sheet = None\n        for s in xls.sheet_names:\n            ns = re.sub(r'[^a-z0-9]', '', s.lower())\n            if any(k in ns for k in [\"bydoor\",\"plan\",\"maysales\",\"mayplan\",\"doorplan\"]):\n                plan_sheet = s\n                break\n        if plan_sheet is None:\n            plan_sheet = xls.sheet_names[0]\n        df = context.files.read_excel(out.id, sheet_name=plan_sheet)\n        df.columns = [str(c) for c in df.columns]\n        def norm(s):\n            return re.sub(r'[^a-z0-9]', '', str(s).lower())\n        cols = list(df.columns)\n        def find_week_col(label, i):\n            keys = [f\"w{i}\", f\"wk{i}\", f\"week{i}\"]\n            for c in cols:\n                nc = norm(c)\n                if any(k in nc for k in keys) and 'plan' in nc:\n                    return c\n            return None\n        plan_w = [find_week_col('plan', i) for i in range(1,5)]\n        if not all(plan_w):\n            return 0.0, \"Weekly plan columns not found\"\n        def find_col(includes, excludes=None):\n            for c in cols:\n                nc = norm(c)\n                if all(k in nc for k in includes) and (not excludes or all(k not in nc for k in excludes)):\n                    return c\n            return None\n        status_col = (find_col(['status']) or find_col(['active']) or find_col(['store','status']) or find_col(['door','status']))\n        is_active = pd.Series(True, index=df.index)\n        if status_col is not None:\n            sc = df[status_col].astype(str).str.lower()\n            is_closed = sc.str.contains('closed')\n            active_flag = sc.str.contains('x') | sc.str.contains('open') | sc.str.contains('active')\n            if is_closed.any():\n                is_active = ~is_closed\n            elif active_flag.any():\n                is_active = active_flag\n        def clean_num(s):\n            return pd.to_numeric(s.replace({'\\\\$':'', ',':''}, regex=True), errors='coerce')\n        plans = [clean_num(df[c]) for c in plan_w]\n        P = pd.concat(plans, axis=1)\n        P_active = P[is_active]\n        vals = P_active.stack().dropna()\n        # Only positive values are subject to rounding/floor\n        pos = vals[vals > 0]\n        if pos.empty:\n            return 0.0, \"No positive plan values found\"\n        # Check multiple of 50 (within small tolerance) and >= 50\n        mod_ok = ((pos % 50).abs().round(2) <= 0.01) | (((50 - (pos % 50)) % 50).abs().round(2) <= 0.01)\n        floor_ok = pos >= 50 - 0.01\n        compliant = (mod_ok & floor_ok).mean()\n        return float(compliant), f\"Rounding/floor compliance: {compliant:.2f}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "STD Trend Calculation Consistency", "description": "Checks that STD Trend % equals (STD Sales / LY STD Sales) - 1 within a small tolerance for rows where values exist.", "weight": 0.25, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        out = context.get_primary_output()\n        if not out or not out.is_spreadsheet:\n            return 0.0\n        path = context.files.get_path(out.id)\n        xls = pd.ExcelFile(path)\n        # Choose plan sheet heuristically\n        plan_sheet = None\n        for s in xls.sheet_names:\n            ns = re.sub(r'[^a-z0-9]', '', s.lower())\n            if any(k in ns for k in [\"bydoor\",\"plan\",\"maysales\",\"mayplan\",\"doorplan\"]):\n                plan_sheet = s\n                break\n        if plan_sheet is None:\n            plan_sheet = xls.sheet_names[0]\n        df = context.files.read_excel(out.id, sheet_name=plan_sheet)\n        df.columns = [str(c) for c in df.columns]\n        def norm(s):\n            return re.sub(r'[^a-z0-9]', '', str(s).lower())\n        cols = list(df.columns)\n        def find_col(includes, excludes=None):\n            for c in cols:\n                nc = norm(c)\n                if all(k in nc for k in includes) and (not excludes or all(k not in nc for k in excludes)):\n                    return c\n            return None\n        std_ty = find_col(['std','sales'], excludes=['ly'])\n        std_ly = find_col(['ly','std','sales']) or find_col(['std','ly'])\n        std_trend = find_col(['std','trend']) or find_col(['trend','pct']) or find_col(['trend','percent'])\n        if std_ty is None or std_ly is None or std_trend is None:\n            return 0.0, \"STD Sales / LY STD Sales / STD Trend columns not all found\"\n        def clean_num(s):\n            return pd.to_numeric(s.replace({'\\\\%':'', '\\\\$':'', ',':''}, regex=True), errors='coerce')\n        ty = clean_num(df[std_ty])\n        ly = clean_num(df[std_ly])\n        tr = clean_num(df[std_trend]) / 100.0 if df[std_trend].astype(str).str.contains('%').any() else clean_num(df[std_trend])\n        mask = (ty.notna() & ly.notna() & (ly != 0) & tr.notna())\n        if mask.sum() == 0:\n            return 0.0, \"No rows with complete STD values\"\n        calc = (ty[mask] / ly[mask]) - 1.0\n        ok = (calc - tr[mask]).abs() <= 0.005  # within 0.5%\n        score = ok.mean()\n        return float(score), f\"STD Trend match rate: {score:.2f}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Aggregate Week Weighting Reasonableness", "description": "Checks if aggregate plan distribution is W1-heavy and approximately within guidance bands. Full credit if W1 61\u201363%, W2 22\u201324%, W3 7\u20138%, W4 7\u20138%; partial for looser bands or at least correct ordering.", "weight": 0.25, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        out = context.get_primary_output()\n        if not out or not out.is_spreadsheet:\n            return 0.0\n        path = context.files.get_path(out.id)\n        xls = pd.ExcelFile(path)\n        # Choose plan sheet heuristically\n        plan_sheet = None\n        for s in xls.sheet_names:\n            ns = re.sub(r'[^a-z0-9]', '', s.lower())\n            if any(k in ns for k in [\"bydoor\",\"plan\",\"maysales\",\"mayplan\",\"doorplan\"]):\n                plan_sheet = s\n                break\n        if plan_sheet is None:\n            plan_sheet = xls.sheet_names[0]\n        df = context.files.read_excel(out.id, sheet_name=plan_sheet)\n        df.columns = [str(c) for c in df.columns]\n        def norm(s):\n            return re.sub(r'[^a-z0-9]', '', str(s).lower())\n        cols = list(df.columns)\n        def find_week_col(label, i):\n            keys = [f\"w{i}\", f\"wk{i}\", f\"week{i}\"]\n            for c in cols:\n                nc = norm(c)\n                if any(k in nc for k in keys) and 'plan' in nc:\n                    return c\n            return None\n        plan_w = [find_week_col('plan', i) for i in range(1,5)]\n        if not all(plan_w):\n            return 0.0, \"Plan weekly columns not found\"\n        def find_col(includes, excludes=None):\n            for c in cols:\n                nc = norm(c)\n                if all(k in nc for k in includes) and (not excludes or all(k not in nc for k in excludes)):\n                    return c\n            return None\n        status_col = (find_col(['status']) or find_col(['active']) or find_col(['store','status']) or find_col(['door','status']))\n        is_active = pd.Series(True, index=df.index)\n        if status_col is not None:\n            sc = df[status_col].astype(str).str.lower()\n            is_closed = sc.str.contains('closed')\n            active_flag = sc.str.contains('x') | sc.str.contains('open') | sc.str.contains('active')\n            if is_closed.any():\n                is_active = ~is_closed\n            elif active_flag.any():\n                is_active = active_flag\n        def clean_num(s):\n            return pd.to_numeric(s.replace({'\\\\$':'', ',':''}, regex=True), errors='coerce')\n        P = pd.concat([clean_num(df[c]) for c in plan_w], axis=1)\n        P.columns = ['W1','W2','W3','W4']\n        P = P[is_active]\n        totals = P.sum(skipna=True)\n        total = totals.sum()\n        if total <= 0 or totals.isna().all():\n            return 0.0, \"No aggregate plan found\"\n        shares = (totals / total).fillna(0.0)\n        w1,w2,w3,w4 = shares['W1'], shares['W2'], shares['W3'], shares['W4']\n        # Scoring tiers\n        exact = (0.61 <= w1 <= 0.63) and (0.22 <= w2 <= 0.24) and (0.07 <= w3 <= 0.08) and (0.07 <= w4 <= 0.08)\n        loose = (0.58 <= w1 <= 0.66) and (0.19 <= w2 <= 0.27) and (0.05 <= w3 <= 0.10) and (0.05 <= w4 <= 0.10)\n        ordered = (w1 > w2) and (w2 > max(w3,w4)) and (w1 >= 0.55)\n        score = 1.0 if exact else (0.6 if loose else (0.3 if ordered else 0.0))\n        fb = f\"Shares W1={w1:.2%}, W2={w2:.2%}, W3={w3:.2%}, W4={w4:.2%}\"\n        return score, fb\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "llm_judge", "name": "Narrative and Summary Alignment", "description": "Checks the 1\u20132 sentence narrative includes: May Sales Plan $ and % to LY for Total Stores and Comparable Stores, plus LY volume from now-closed stores, and that these values align with the Summary rollup table.", "weight": 1.5, "judge_prompt": "Evaluate the Summary sheet. Does the 1\u20132 sentence narrative explicitly state: (a) May Sales Plan dollars and percent change vs LY for Total Stores; (b) the same for Comparable Stores; and (c) the LY volume from stores that are now closed? Do these values match the adjacent rollup table entries on the Summary sheet (rows: Total Stores, Closed Stores, Comp Stores; columns include LY May Total $, Plan May Total $, Plan % to LY)?\n\nScoring:\n- 1.5: All required items are present in the narrative and match the rollup table figures.\n- 1.0: Narrative includes all items but one figure appears slightly off or rounding-only discrepancy.\n- 0.6: Narrative is missing one required item OR values are inconsistently labeled but mostly align.\n- 0.3: Narrative present but missing multiple required items or largely inconsistent with the table.\n- 0.0: No narrative, or narrative does not relate to the rollup at all.", "expectation": "Clear, concise 1\u20132 sentence summary that matches the rollup table exactly or within rounding."}, {"type": "llm_judge", "name": "Active vs Closed Planning Compliance", "description": "Verifies that closed stores do not receive weekly plan values while active stores do, and that the active/closed status is clearly indicated.", "weight": 0.75, "judge_prompt": "Inspect the By-Door planning sheet. Confirm that:\n- A status/active indicator column clearly identifies Active vs Closed stores (e.g., \"x\" for active or explicit \"Closed\").\n- Weekly plan values (for W1\u2013W4) appear only for Active stores, not for Closed stores (closed rows should be blank or zero for plan columns).\n\nScoring:\n- 0.75: Status is clear and there are no plan values allocated to closed stores.\n- 0.5: Status is clear and at most one minor exception is observed.\n- 0.25: Multiple exceptions or ambiguous status labeling.\n- 0.0: No status labeling, or closed stores are planned broadly.", "expectation": "Only active doors receive plan values; closed doors show no plans."}, {"type": "llm_judge", "name": "Use of STD Trend and Topside Guidance", "description": "Assesses whether methodology uses the store\u2019s STD trend with LY volume and aligns topside to approximately -15% vs LY for comparable stores.", "weight": 1.25, "judge_prompt": "Review the workbook (especially any methodology/notes on the Summary sheet). Determine whether:\n- The approach explicitly references using each store\u2019s STD trend (TY vs LY) together with LY volume to inform the plan.\n- The overall Comparable Stores plan is approximately -15% vs LY (allow reasonable tolerance, e.g., -10% to -20%, with an explanation if outside that).\n- Spot-check a few rows: does Plan % to LY sensibly relate to the STD Trend (e.g., stronger positive trends not planned worse than sharply negative trends without explanation)?\n\nScoring:\n- 1.25: Methodology clearly states STD trend + LY usage and Comp Stores % vs LY is near -15% with sensible alignment at the store level or explanations.\n- 0.9: Methodology stated; comp % near target but minor inconsistencies without notes.\n- 0.5: Vague methodology or comp % notably off target without justification.\n- 0.0: No sign of using STD trend or topside guidance.", "expectation": "Method cites STD trend and LY, and comp plan approximately -15% vs LY unless justified."}, {"type": "llm_judge", "name": "Week Weighting Implementation", "description": "Checks that the plan reflects W1-heavy weighting (approx. 61\u201363%) with W2 ~22\u201324% and W3/W4 ~7\u20138% overall or with justified deviations.", "weight": 1.0, "judge_prompt": "Evaluate the aggregate weekly distribution of the plan (Total or Comp totals). Is Week 1 the clear majority (~61\u201363%), Week 2 ~22\u201324%, Weeks 3 and 4 ~7\u20138% each? If there is a deliberate deviation (e.g., due to store notes/anomalies), is this rationale mentioned in the workbook notes/methodology?\n\nScoring:\n- 1.0: Distribution is within the target bands or deviations are explicitly and reasonably justified.\n- 0.6: Close to targets with small unexplained drift.\n- 0.3: W1 is heavier than other weeks, but distribution is materially off without solid rationale.\n- 0.0: Distribution does not reflect guidance and no explanation is provided.", "expectation": "W1 ~61\u201363%, W2 ~22\u201324%, W3/W4 ~7\u20138% or reasoned justification for variance."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Quality Assessment (Professionalism and Communication)", "description": "Holistic quality and stakeholder readiness of the workbook and summary.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Formatting", "description": "Assesses workbook polish: headers, freeze panes, currency/percent formats, readable tab names, and overall cleanliness.", "weight": 1.0, "judge_prompt": "Assess the workbook\u2019s professionalism:\n- Clear column headers and readable tab names.\n- Freeze panes and/or filters on the By-Door table for usability.\n- Currency formatting for dollars and percent formatting for rates (e.g., STD Trend %, Plan % to LY).\n- Consistent number formatting (no random decimals for dollars unless necessary).\n\nScoring:\n- 1.0: Professionally formatted across these aspects.\n- 0.7: Generally good with minor issues.\n- 0.4: Several formatting issues reduce readability.\n- 0.0: Poorly formatted and hard to read.", "expectation": "Clean, formatted Excel with professional readability."}, {"type": "llm_judge", "name": "Clarity and Usability for Operators/Leaders", "description": "Evaluates whether end users can quickly understand and use the plan: filters, sorting, simple structure, and clear labels.", "weight": 0.8, "judge_prompt": "Evaluate clarity/usability:\n- Is the By-Door table easy to filter/sort by store, status, or key metrics?\n- Are labels unambiguous and consistent (e.g., LY vs Plan, W1\u2013W4)?\n- Is there a simple, obvious structure for totals and a separate Summary for leadership?\n\nScoring:\n- 0.8: Highly usable with clear labels and intuitive layout.\n- 0.5: Usable with some friction.\n- 0.2: Confusing structure or inconsistent labels.\n- 0.0: Hard to navigate or understand.", "expectation": "Practical, easy-to-use workbook for planning and review."}, {"type": "llm_judge", "name": "Strategic Insight and Anomaly Handling", "description": "Assesses thoughtful handling of store-level notes/anomalies and any deviations from guidance with rationale.", "weight": 0.7, "judge_prompt": "Review the Notes/Methodology and any annotations. Do they:\n- Identify and handle anomalies (e.g., closures, remodels, local events) noted in the store list?\n- Explain deviations from week weighting or topside target where applicable?\n- Highlight risks/opportunities succinctly?\n\nScoring:\n- 0.7: Clear, relevant insights and justifications tied to anomalies.\n- 0.5: Some insight but limited linkage to plans.\n- 0.2: Minimal or generic commentary.\n- 0.0: No insight or rationale provided.", "expectation": "Brief but cogent rationale tied to actual plan differences."}, {"type": "llm_judge", "name": "Executive Communication Quality", "description": "Checks that the 1\u20132 sentence summary is concise, accurate, and uses appropriate business language and calendar terms.", "weight": 0.5, "judge_prompt": "Evaluate the 1\u20132 sentence summary for:\n- Conciseness and clarity (no fluff, correct figures).\n- Appropriate business language and correct retail calendar references (e.g., May P4, W1\u2013W4, comps).\n- Direct answer to what leadership needs: plan $, % vs LY (Total and Comps), and LY from closed stores.\n\nScoring:\n- 0.5: Clear, concise, and accurate.\n- 0.3: Mostly clear with minor issues.\n- 0.1: Wordy/unclear or minor inaccuracies.\n- 0.0: Missing or misleading.", "expectation": "Tight, leadership-ready 1\u20132 sentence statement."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "75401f7c-396d-406d-b08e-938874ad1045", "rubric": {"category_name": "Showreel Editing \u2013 Motion Graphics/CGI (Goodsin Studios)", "rationale": "This rubric enforces a self-documenting deliverables package for a video showreel. Stage 1 (LLM-only) mandates a precise set of artifacts that make verification trivial (MP4 + structured CSV/JSON/MD). Stage 2 mixes lightweight code checks (container/codec/duration, shot order, SFX mapping) with higher-weight LLM judgments (coverage of physics simulations, pacing, audio usage and licensing, ordering of most advanced shots). Stage 3 provides a holistic, professional quality assessment. Code rules intentionally have significantly lower weight than LLM rules, reflecting their narrow, deterministic validations.", "max_total_score": 24.0, "stages": [{"name": "Stage 1 \u2013 Deliverables Shape Enforcement (Gate)", "description": "LLM-only gate verifying the presence and structure of a complete, self-documenting showreel package enabling automated checks in later stages. Only checks structure/shape, not correctness.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Deliverables Package Present and Structured", "description": "Verify the candidate provided a complete deliverables package with proper file types and clearly labeled documents enabling verification.", "weight": 2.5, "judge_prompt": "You are validating the structure/shape of the deliverables for a 2025 showreel editing task. Examine ALL provided outputs. Do NOT judge quality or correctness here\u2014only check presence and structural completeness.\n\nRequired Deliverables (be flexible with exact filenames but the purpose must be clear):\n1) Final Showreel Video\n   - One .mp4 file intended as the final deliverable.\n   - Must be clearly identifiable as the final reel (e.g., filename contains \"showreel\" or \"reel\").\n\n2) Technical Specs JSON (Media Info)\n   - A .json file containing media metadata sufficient to verify: container, video codec, width, height, duration (seconds), audio presence. Accept names like \"media_info.json\", \"technical_spec.json\", or similar. Keys can be flexibly named but must include those fields.\n\n3) Shot List / EDL CSV\n   - A .csv enumerating every used shot in sequence order with at least: an order/index, source filename (of the provided footage), and timecodes or durations. Column names can vary (e.g., order/index, file/source/clip, in/out/start/end). A brief shot description column is encouraged.\n\n4) Audio Cue Sheet CSV\n   - A .csv listing music and SFX placements with at least: cue type (music/SFX), cue name/file, in/out/position times, and which video shot/filename it belongs to. Column names can vary.\n\n5) Music Licensing Note\n   - A short document (Markdown .md, PDF .pdf, or Word .docx) indicating the track used is royalty-free and where it came from (URL or source). If a different track from the provided one is used, note that and include source and license terms. If the provided track is used, state that explicitly.\n\n6) Edit Notes (Brief)\n   - A short document (Markdown .md, PDF .pdf, or Word .docx) summarizing key edit choices (e.g., where SFX are placed, any beat-sync approach, most advanced shots placement) to help verification. 3\u20138 bullet points or a short paragraph is enough.\n\nScoring:\n- 2.5: All 6 deliverables present with correct file types and clearly labeled.\n- 2.0: Missing exactly one of items (2)\u2013(6) OR clearly combined two items into one doc but still fully covers both purposes.\n- 1.0: Only the MP4 plus 1\u20132 supporting docs present (insufficient structure for verification).\n- 0.0: Missing the MP4 OR missing most supporting artifacts.\n\nImportant: Only evaluate structural presence and reasonable labeling. Do not validate contents/correctness at this stage.", "expectation": "A clearly labeled MP4 plus JSON tech sheet, CSV shot list, CSV cue sheet, licensing note, and brief edit notes."}, {"type": "llm_judge", "name": "Structured Fields Within CSV/JSON", "description": "Verify that the CSV/JSON artifacts appear to contain the fields needed for later checks (without validating values).", "weight": 1.5, "judge_prompt": "Check the provided CSV/JSON documents for structural completeness. Do NOT verify correctness of values; only assess whether the needed fields appear present and readable.\n\nExpectations:\n- Technical Specs JSON: Contains fields representing container, video codec, width, height, duration in seconds (or a parsable time), and an indication of audio presence (e.g., audio codec or channels). Flexible key names are acceptable if unambiguous.\n- Shot List / EDL CSV: Has columns sufficient to reconstruct sequence: an order/index, a source filename (original footage name), and timing (start/in and end/out or duration). A description column is helpful but optional.\n- Audio Cue Sheet CSV: Has columns indicating cue type (music/SFX), cue name/file, timing (in/out/position), and mapping to a video shot or source filename.\n\nScoring:\n- 1.5: All three artifacts are clearly structured with appropriate columns/fields visible.\n- 1.0: Exactly one artifact is marginal (e.g., missing one minor field) but still mostly usable.\n- 0.5: Two artifacts are marginal or unclear.\n- 0.0: CSV/JSON artifacts are missing or too ambiguous to use.\n\nOnly structural visibility is judged here.", "expectation": "All required CSV/JSON show the expected fields/columns so automated checks can run."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness Verification (Mixed Code + LLM)", "description": "Verify compliance with specs and task constraints using deterministic code checks where feasible, plus higher-weight LLM judgments for nuanced requirements.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Technical Specs Compliance (MP4 + H.264 + 1920x1080 + <=80s + audio)", "description": "Parse the Technical Specs JSON to verify: container MP4, H.264/AVC video codec, 1920x1080 resolution, duration <= 80s, and presence of audio.", "weight": 1.0, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef _find_json_resource(context):\n    for r in context.get_all_outputs():\n        name = (r.name or '').lower()\n        if name.endswith('.json') and any(k in name for k in ['media', 'tech', 'spec']):\n            return r\n    # fallback: any json\n    for r in context.get_all_outputs():\n        name = (r.name or '').lower()\n        if name.endswith('.json'):\n            return r\n    return None\n\ndef _get_val(d, keys, default=None):\n    for k in keys:\n        # support nested paths like 'video.codec'\n        parts = k.split('.')\n        cur = d\n        ok = True\n        for p in parts:\n            if isinstance(cur, dict) and p in cur:\n                cur = cur[p]\n            else:\n                ok = False\n                break\n        if ok:\n            return cur\n    return default\n\ndef _to_float_seconds(x):\n    if x is None:\n        return None\n    if isinstance(x, (int, float)):\n        return float(x)\n    s = str(x).strip()\n    # try HH:MM:SS.mmm\n    m = re.match(r'^(\\d{1,2}):(\\d{2}):(\\d{2})(?:[\\.,](\\d{1,3}))?$', s)\n    if m:\n        h, mi, se, ms = m.groups()\n        secs = int(h)*3600 + int(mi)*60 + int(se)\n        if ms:\n            secs += float('0.' + ms)\n        return float(secs)\n    # try seconds as string\n    try:\n        return float(s)\n    except:\n        return None\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, 'No primary output.'\n\n    jr = _find_json_resource(context)\n    if not jr:\n        return 0.0, 'No technical JSON found.'\n\n    try:\n        raw = context.files.read_text(jr.id)\n        data = json.loads(raw)\n    except Exception as e:\n        return 0.0, f'Failed to parse JSON: {e}'\n\n    # Attempt to locate fields with flexible keys\n    container = _get_val(data, ['container', 'format', 'format_name', 'container.format'])\n    vcodec = _get_val(data, ['video_codec', 'vcodec', 'codec', 'codec_name', 'video.codec'])\n    width = _get_val(data, ['width', 'video.width'])\n    height = _get_val(data, ['height', 'video.height'])\n    duration = _to_float_seconds(_get_val(data, ['duration_sec', 'duration', 'format.duration']))\n    acodec = _get_val(data, ['audio_codec', 'acodec', 'audio.codec'])\n    channels = _get_val(data, ['audio_channels', 'channels', 'audio.channels'])\n\n    score = 0.0\n    msgs = []\n\n    # Container MP4\n    cont_ok = False\n    if container:\n        cont_ok = 'mp4' in str(container).lower() or 'isom' in str(container).lower()\n    msgs.append(f'container={container}')\n    if cont_ok:\n        score += 0.2\n\n    # Video codec H.264/AVC\n    vc_ok = False\n    if vcodec:\n        s = str(vcodec).lower()\n        vc_ok = any(k in s for k in ['h264', 'h.264', 'avc', 'avc1'])\n    msgs.append(f'video_codec={vcodec}')\n    if vc_ok:\n        score += 0.25\n\n    # Resolution 1920x1080\n    w_ok = (str(width).isdigit() and int(width) == 1920)\n    h_ok = (str(height).isdigit() and int(height) == 1080)\n    msgs.append(f'resolution={width}x{height}')\n    if w_ok and h_ok:\n        score += 0.25\n\n    # Duration <= 80s\n    dur_ok = duration is not None and duration <= 80.0\n    msgs.append(f'duration_sec={duration}')\n    if dur_ok:\n        score += 0.2\n\n    # Audio present (codec or channels)\n    aud_ok = (acodec is not None) or (channels is not None and int(str(channels)) >= 1)\n    msgs.append(f'audio_codec={acodec}, channels={channels}')\n    if aud_ok:\n        score += 0.1\n\n    return min(score, 1.0), '; '.join(msgs)"}, {"type": "code", "name": "Required Clips Order in Shot List (logos first, logo_2 last)", "description": "Check Shot List/EDL CSV: first shot from logos.mp4 and last shot from logo_2.mp4; ensure sequence is non-empty and time increases.", "weight": 0.8, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\nSHOT_FILE_COLUMNS = ['source', 'source_file', 'file', 'filename', 'clip', 'shot_file']\nORDER_COLUMNS = ['order', 'index', 'position', 'seq', 'sequence', 'row']\nSTART_COLUMNS = ['in', 'in_time', 'start', 'start_time', 'time_in']\nEND_COLUMNS = ['out', 'out_time', 'end', 'end_time', 'time_out']\n\n\ndef _find_csv(context, keywords):\n    # Find a CSV whose name includes any of the keywords\n    for r in context.get_all_outputs():\n        name = (r.name or '').lower()\n        if name.endswith('.csv') and any(k in name for k in keywords):\n            return r\n    # fallback to any csv\n    for r in context.get_all_outputs():\n        name = (r.name or '').lower()\n        if name.endswith('.csv'):\n            return r\n    return None\n\n\ndef _choose_col(cols, candidates):\n    cols_l = [c.lower() for c in cols]\n    for cand in candidates:\n        if cand in cols_l:\n            return cols[cols_l.index(cand)]\n    return None\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, 'No primary output.'\n\n    r = _find_csv(context, ['shot', 'edl'])\n    if not r:\n        return 0.0, 'No Shot List/EDL CSV found.'\n\n    try:\n        df = context.files.read_csv(r.id)\n    except Exception as e:\n        return 0.0, f'Failed to read CSV: {e}'\n\n    if df.empty:\n        return 0.0, 'Shot list is empty.'\n\n    # Determine columns\n    file_col = _choose_col(df.columns.tolist(), SHOT_FILE_COLUMNS)\n    if not file_col:\n        # try to infer by looking for .mp4 in any column\n        for c in df.columns:\n            if df[c].astype(str).str.contains('\\.mp4', case=False, regex=True).any():\n                file_col = c\n                break\n    if not file_col:\n        return 0.0, 'No source filename column found.'\n\n    # Order or time to determine first/last\n    ord_col = _choose_col(df.columns.tolist(), ORDER_COLUMNS)\n    if ord_col:\n        df_sorted = df.sort_values(by=ord_col, kind='mergesort')\n    else:\n        start_col = _choose_col(df.columns.tolist(), START_COLUMNS)\n        if start_col:\n            # attempt to parse as float seconds; coerce errors\n            def _to_sec(x):\n                try:\n                    s = str(x)\n                    if re.match(r'^\\d+$', s):\n                        return float(s)\n                    m = re.match(r'^(\\d{1,2}):(\\d{2}):(\\d{2})(?:[\\.,](\\d{1,3}))?$', s)\n                    if m:\n                        h, mi, se, ms = m.groups()\n                        secs = int(h)*3600 + int(mi)*60 + int(se)\n                        if ms:\n                            secs += float('0.' + ms)\n                        return float(secs)\n                    return float(s)\n                except:\n                    return np.nan\n            ss = df[start_col].apply(_to_sec)\n            df_sorted = df.assign(_start=ss).sort_values(by='_start', kind='mergesort')\n        else:\n            df_sorted = df.copy()\n\n    first_file = str(df_sorted[file_col].iloc[0]).lower()\n    last_file = str(df_sorted[file_col].iloc[-1]).lower()\n\n    score = 0.0\n    msgs = []\n\n    if 'logos.mp4' in first_file:\n        score += 0.4\n    else:\n        msgs.append(f'First is {first_file}')\n\n    if 'logo_2.mp4' in last_file:\n        score += 0.4\n    else:\n        msgs.append(f'Last is {last_file}')\n\n    # Non-empty and some increasing measure\n    if len(df_sorted) >= 2:\n        score += 0.0  # already implied; keep at 0 for weight balance\n\n    return min(score, 0.8), '; '.join(msgs)"}, {"type": "code", "name": "SFX Placement Compliance (3 specific cues mapped to correct shots)", "description": "Verify in the Audio Cue Sheet CSV that required SFX are present and associated with the expected shots: Electricity with logos.mp4; ExplosionFire PS01_92 with CastleExplosion(TyFlow+Phoenix).mp4; LargeMultiImpactsW PE280701 with Shores_Comp_04222020.mp4.", "weight": 0.7, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\nCUE_COLUMNS = ['cue', 'cue_name', 'name', 'sfx_name']\nTYPE_COLUMNS = ['type', 'category', 'cue_type']\nFILE_COLUMNS = ['file', 'filename', 'audio_file']\nVIDEO_MAP_COLUMNS = ['video_shot', 'video_file', 'shot_file', 'source', 'source_file']\n\nREQS = [\n    {\n        'sfx_kw': ['mountain', 'electricity'],\n        'video_kw': ['logos.mp4']\n    },\n    {\n        'sfx_kw': ['explosionfire', 'ps01_92'],\n        'video_kw': ['castle']\n    },\n    {\n        'sfx_kw': ['largemultiimpactsw', 'pe280701'],\n        'video_kw': ['shores', '04222020']\n    },\n]\n\n\ndef _find_csv(context, keywords):\n    for r in context.get_all_outputs():\n        name = (r.name or '').lower()\n        if name.endswith('.csv') and any(k in name for k in keywords):\n            return r\n    for r in context.get_all_outputs():\n        name = (r.name or '').lower()\n        if name.endswith('.csv'):\n            return r\n    return None\n\n\ndef _choose_col(cols, candidates):\n    cols_l = [c.lower() for c in cols]\n    for cand in candidates:\n        if cand in cols_l:\n            return cols[cols_l.index(cand)]\n    return None\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, 'No primary output.'\n\n    r = _find_csv(context, ['cue', 'audio', 'sfx'])\n    if not r:\n        return 0.0, 'No Audio Cue Sheet CSV found.'\n\n    try:\n        df = context.files.read_csv(r.id)\n    except Exception as e:\n        return 0.0, f'Failed to read CSV: {e}'\n\n    if df.empty:\n        return 0.0, 'Cue sheet empty.'\n\n    cue_col = _choose_col(df.columns.tolist(), CUE_COLUMNS) or _choose_col(df.columns.tolist(), FILE_COLUMNS) or df.columns[0]\n    type_col = _choose_col(df.columns.tolist(), TYPE_COLUMNS)\n    map_col = _choose_col(df.columns.tolist(), VIDEO_MAP_COLUMNS)\n\n    if not map_col:\n        # Try to infer any column mentioning .mp4\n        for c in df.columns:\n            if df[c].astype(str).str.contains('mp4', case=False, regex=True).any():\n                map_col = c\n                break\n    if not map_col:\n        return 0.0, 'No mapping to video shot/file found.'\n\n    def _norm(s):\n        return re.sub(r'[^a-z0-9]+', '', str(s).lower())\n\n    found = [False, False, False]\n\n    for i, row in df.iterrows():\n        cue = _norm(row.get(cue_col, ''))\n        mapped = _norm(row.get(map_col, ''))\n        # Check type if available; allow both empty or SFX\n        if type_col:\n            t = _norm(row.get(type_col, ''))\n            # not strictly required to be 'sfx' as names vary\n        for idx, req in enumerate(REQS):\n            sfx_ok = all(k in cue for k in req['sfx_kw'])\n            vid_ok = all(k in mapped for k in req['video_kw'])\n            if sfx_ok and vid_ok:\n                found[idx] = True\n\n    score = sum(1 for f in found if f)\n    # 3 matches -> 0.7, 2 -> ~0.47, 1 -> ~0.23\n    return 0.7 * (score/3.0), f\"Matches: {found}\""}, {"type": "llm_judge", "name": "Coverage and Pacing vs Brief", "description": "Judge whether the reel, as described in the shot list and notes, covers the requested categories (water, fire, smoke, explosions, destruction, compositing/roto) and maintains a fast, energetic pace within 1:20.", "weight": 3.5, "judge_prompt": "Use the Shot List/EDL, Edit Notes, and any provided visuals to assess brief alignment. Focus on correctness versus the brief, not artistic quality.\n\nRequirements to verify:\n- Categories Coverage: The sequence includes several of the requested physics/CG shots: water, fire, smoke, explosions, destructions; plus advanced techniques like compositing or rotoscoping, as indicated by shot descriptions or filenames. You can be flexible if synonyms are used.\n- Pacing: The cut density and timing described (and visible if you can view the reel) are high-energy; average shot length is relatively short; no long lulls.\n- Duration Compliance: The declared duration in the Technical Specs JSON and/or packaging is at or below 1 minute 20 seconds (<= 80 seconds). If duration seems longer from visible content, consider that a deduction.\n\nScoring:\n- 3.5: Clear coverage of most categories (\u22654 categories including explosions/destruction) and energetic pacing; duration within limit.\n- 2.5: Some categories covered (\u22653), decent pacing; duration within limit.\n- 1.5: Minimal category coverage (1\u20132) or pacing feels slow; duration within limit.\n- 0.5: Poor coverage and slow pacing but duration within limit.\n- 0.0: Duration likely exceeds the limit OR content does not reflect brief at all.", "expectation": "Fast-paced reel within 80s showcasing multiple physics simulation categories and compositing/roto where applicable."}, {"type": "llm_judge", "name": "Audio Usage and Licensing Compliance", "description": "Confirm that only permitted embedded audio is used for specified shots, required SFX are included as documented, and music is royalty-free with source/license noted.", "weight": 3.0, "judge_prompt": "Cross-check the Audio Cue Sheet, Shot List, and the Music Licensing Note.\n- Embedded/Diegetic audio allowed ONLY for: building explosions and helicopter landing shots. Any other embedded production audio should not be present.\n- Required SFX mappings: \n  1) Mountain Audio - Electricity with the opening neon sign (logos.mp4)\n  2) ExplosionFire PS01_92 with the castle explosion (CastleExplosion(TyFlow+Phoenix).mp4)\n  3) LargeMultiImpactsW PE280701 for collapsing building (Shores_Comp_04222020.mp4), can be cut up\n- Music licensing: Either the provided track is used OR a different royalty-free track; in both cases the licensing note should indicate source and license terms or royalty-free status.\n\nScoring:\n- 3.0: All constraints met and clearly documented; licensing/source clearly stated.\n- 2.0: Minor documentation gaps but usage appears compliant; licensing reasonably indicated.\n- 1.0: Noticeable ambiguities in SFX placement or embedded audio compliance; licensing unclear.\n- 0.0: Violates embedded audio constraint or no evidence of royalty-free licensing.", "expectation": "Clean documentation showing correct SFX placement, permitted embedded audio only on specified shots, and a clear royalty-free music statement with source."}, {"type": "llm_judge", "name": "Most Advanced Shots Front-Loaded", "description": "Assess whether the opening third of the reel emphasizes the studio\u2019s most technically advanced work.", "weight": 3.0, "judge_prompt": "Using the Shot List/EDL and Edit Notes, judge whether the first ~1/3 of the reel (by time or shot count) contains the most technically advanced and impressive shots (e.g., complex fluid/pyro sims, destruction with detailed debris/smoke, sophisticated compositing/roto). Be flexible: rely on shot descriptions, filenames, and any visible content you can see. \n\nScoring:\n- 3.0: Clear concentration of most advanced shots in the opening segment.\n- 2.0: Mixed but still front-loaded with several advanced shots.\n- 1.0: Little evidence of front-loading; advanced shots scattered.\n- 0.0: Advanced shots mostly later or absent.", "expectation": "A strong, technically complex opening that quickly signals capability."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Holistic Quality Assessment (LLM)", "description": "Professional polish and audience suitability assessment.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Visual Cohesion and Flow", "description": "Evaluate how well the reel flows visually: transitions, shot variety without whiplash, color/style consistency.", "weight": 2.0, "judge_prompt": "Judge the reel\u2019s visual cohesion and flow based on the Shot List/EDL, Edit Notes, and visible content if available.\n- Are transitions appropriate and not distracting?\n- Is there a coherent arc or at least a sensible progression of intensity and subjects?\n- Does color grading/finishing feel consistent enough for a showreel?\nScoring: 2.0 excellent cohesion; 1.0 mixed; 0.0 poor/chaotic.", "expectation": "Purposeful sequencing with consistent finishing and smooth flow."}, {"type": "llm_judge", "name": "Sound Design Quality and Sync Perception", "description": "Assess the perceived quality of the mix: balance of music vs SFX, tasteful use of embedded audio, and sense of sync to beats/impacts.", "weight": 2.0, "judge_prompt": "Evaluate the overall sound design quality. Consider:\n- Music level vs SFX clarity.\n- Impact hits and key edit points feel synced to beats or musical accents.\n- Embedded audio (where allowed) enhances rather than distracts.\nScoring: 2.0 strong, impactful mix with evident sync; 1.0 adequate; 0.0 weak or distracting.", "expectation": "Tight, energetic audio with clear beats/impacts supporting picture."}, {"type": "llm_judge", "name": "Branding Presence and Ending Strength", "description": "Evaluate the effectiveness of the opening and closing identity shots and the final impression for prospective clients.", "weight": 2.0, "judge_prompt": "Check that the reel starts with the neon sign logo (logos.mp4) and ends with the closing logo (logo_2.mp4), and that branding feels strong.\n- Opening creates immediate identity and excitement.\n- Ending resolves cleanly with a memorable brand impression.\nScoring: 2.0 strong open/close; 1.0 adequate; 0.0 weak or missing.", "expectation": "Crisp open with energy and a clean, memorable brand close."}, {"type": "llm_judge", "name": "Professional Polish (Titles, Typos, Artifacts)", "description": "Check for professional finishing: title cards (if any) clean, no typos, no rendering glitches or obvious technical artifacts.", "weight": 2.0, "judge_prompt": "Assess polish and finish quality. Look for:\n- Clean titles/lower-thirds if present; no typos.\n- No obvious render artifacts (banding, flicker) distracting from work.\n- Audio free of clipping/pops.\nScoring: 2.0 highly professional; 1.0 minor issues; 0.0 noticeable defects.", "expectation": "Studio-grade finishing without distracting errors."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "fd3ad420-6f7d-43b1-a990-c0c5c047d071", "rubric": {"category_name": "Real Estate Broker Compensation Plan (Qualifying Broker)", "rationale": "This rubric enforces a self-documenting, verifiable one-page PDF that clearly structures a compensation model for Qualifying Brokers and commission splits for Agents/Associate Brokers, tailored for a new multi-state brokerage (FL, GA, NC). Stage 1 gates on format and section structure. Stage 2 verifies correctness via mixed code (deterministic text checks) and LLM (cross-references, numeric splits, and role coverage). Stage 3 assesses professional quality, clarity, and suitability for a non-licensed founder audience.", "max_total_score": 11.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate", "description": "Document format and section structure requirements for a one-page broker compensation plan.", "is_required": true, "max_points": 1.0, "min_score_to_pass": 0.7, "rules": [{"type": "llm_judge", "name": "Format and Structure Requirements (One-Page PDF)", "description": "Check that the output is a one-page PDF with required sections and basic structural elements.", "weight": 1.0, "judge_prompt": "You are evaluating whether the candidate output satisfies strict SHAPE requirements for a one-page broker compensation plan. Use the rendered file preview (PDF pages) to judge format and visible structure. Be flexible with minor header name variations but enforce presence and layout.\n\nREQUIREMENTS:\n1) File Format and Length\n- Must be a PDF. Not a Word, not plain text, not Excel.\n- Exactly one page (single-page document). No more than one page.\n\n2) Title and Branding Context\n- A clear title visible near the top indicating this is a broker compensation structure (e.g., \"Broker Compensation Structure\", \"Qualifying Broker Compensation Plan\", or similar).\n- Should reference the firm name \"Sample Realty\" somewhere on the page.\n\n3) Required Sections (top-level headers; flexible naming allowed):\n- \"Purpose\" (or similar like \"Objective\"/\"Overview\")\n- \"Commission Split Structure\" (or similar like \"Commission Structure\"/\"Compensation Structure\")\n- \"Summary\" (or similar like \"Conclusion\"/\"Recap\")\n\n4) Commission Split Structure Subcontent (shape only, not correctness):\n- Under the Commission Split Structure section, there must be clearly delineated bullet points or subheadings that distinguish:\n  a) Qualifying Broker compensation model\n  b) Commission splits for Agents and/or Associate Brokers\n- At least one numeric indicator should be present in this section (e.g., a percentage or $ amount) as visible text (not embedded images of text that cannot be read).\n\n5) State Applicability Mention\n- Somewhere on the page, it should explicitly indicate applicability to the states where the Qualifying Broker holds a license: FL, GA, and NC (abbreviations or full names acceptable).\n\nSCORING:\n- 1.0: All requirements met (PDF, one page, clear title with Sample Realty, all three required sections present, Commission Split Structure includes subcontent distinguishing roles with at least one numeric, and FL/GA/NC mentioned).\n- 0.7: PDF one-page with title and Sample Realty mention; all three sections present; Commission Split Structure present but subcontent is only partially delineated OR numeric indicator missing; states mentioned inexactly but reasonably inferable.\n- 0.4: PDF one-page with title but missing one required section OR multiple structural gaps (e.g., no role distinction under Commission Split Structure, or no state applicability mentioned).\n- 0.0: Not a PDF, more than one page, or missing multiple required sections such that structure cannot be verified.\n\nImportant: Do NOT judge content quality or correctness here\u2014only presence and structure.", "expectation": "A single-page PDF with a clear title referencing Sample Realty and three sections (Purpose, Commission Split Structure with role-differentiated bullets/subheadings and at least one numeric, and Summary), plus visible state applicability for FL, GA, and NC."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness Verification", "description": "Verify that the compensation content is complete and internally coherent given the mandated structure.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "State Coverage Present (FL, GA, NC)", "description": "Checks that the document text explicitly mentions Florida/Georgia/North Carolina or their abbreviations (FL, GA, NC).", "weight": 0.4, "code": "import re\\n\\n\ndef evaluate(workflow, context):\\n    \\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n\\n    text = \"\"\\n    try:\\n        # Prefer PDF text; fallback to DOCX if provided\\n        if str(output.name).lower().endswith('.pdf'):\\n            text = context.files.read_pdf_text(output.id) or \"\"\\n        else:\\n            text = context.files.read_docx_text(output.id) or \"\"\\n    except Exception:\\n        text = \"\"\\n\\n    if not text:\\n        return 0.0\\n\\n    lt = text.lower()\\n    def present(patterns):\\n        return any(re.search(p, lt) for p in patterns)\\n\\n    fl_ok = present([r\"\\\\bflorida\\\\b\", r\"\\\\bfl\\\\b\"])\\n    ga_ok = present([r\"\\\\bgeorgia\\\\b\", r\"\\\\bga\\\\b\"])\\n    nc_ok = present([r\"\\\\bnorth\\\\s+carolina\\\\b\", r\"\\\\bnc\\\\b\"])\\n\\n    count = sum([fl_ok, ga_ok, nc_ok])\\n    return count / 3.0"}, {"type": "code", "name": "Compensation Terms Presence (Percents/Fees/Caps)", "description": "Verifies presence of at least one percentage and at least one monetary/fee term relevant to commission structures.", "weight": 0.4, "code": "import re\\n\\n\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n\\n    try:\\n        text = context.files.read_pdf_text(output.id) if str(output.name).lower().endswith('.pdf') else context.files.read_docx_text(output.id)\\n    except Exception:\\n        text = None\\n\\n    if not text:\\n        return 0.0\\n\\n    lt = text.lower()\\n\\n    # Percentage like 70%, 85 %, 100%\\n    has_pct = re.search(r\"\\\\b(100|[1-9]?\\\\d)\\\\s*%\", lt) is not None\\n\\n    # Monetary terms and fee/cap indicators\\n    has_money = (re.search(r\"\\\\$\\\\s?\\\\d{1,3}(,\\\\d{3})*(\\\\.\\\\d{2})?\", lt) is not None)\\n    fee_terms = any(k in lt for k in [\\n        'cap', 'monthly', 'per-transaction', 'transaction fee', 'desk fee',\\n        'retainer', 'override', 'royalty', 'split', 'company dollar'\\n    ])\\n\\n    # Commission context mention\\n    has_commission_context = ('commission' in lt or 'split' in lt)\\n\\n    checks = [has_pct, (has_money or fee_terms), has_commission_context]\\n    return sum(checks) / len(checks)"}, {"type": "code", "name": "Role Mention Coverage (Qualifying Broker and Agents)", "description": "Checks that both 'Qualifying Broker' and at least one of 'Agent' or 'Associate Broker' are explicitly mentioned.", "weight": 0.4, "code": "def evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n\\n    try:\\n        text = context.files.read_pdf_text(output.id) if str(output.name).lower().endswith('.pdf') else context.files.read_docx_text(output.id)\\n    except Exception:\\n        text = None\\n\\n    if not text:\\n        return 0.0\\n\\n    lt = text.lower()\\n    has_qb = ('qualifying broker' in lt) or ('qb' in lt and 'broker' in lt)\\n    has_agent = ('agent' in lt)\\n    has_assoc_broker = ('associate broker' in lt) or ('assoc broker' in lt)\\n\\n    return 1.0 if has_qb and (has_agent or has_assoc_broker) else 0.0"}, {"type": "llm_judge", "name": "Split Structure Completeness (Agents and Associate Brokers)", "description": "Checks that the Commission Split Structure clearly provides splits for Agents and Associate Brokers, with explicit numeric examples and any caps/fees described.", "weight": 2.4, "judge_prompt": "Evaluate the Commission Split Structure section for completeness and specificity. Focus on whether it clearly distinguishes and specifies splits for Agents and Associate Brokers, and includes numeric examples (e.g., % splits) and any cap/fee framework. Ignore tone/formatting quality; judge content presence and clarity.\n\nLook for:\n- Distinct coverage of Agent and Associate Broker splits (they may be combined with clear distinctions, or listed separately).\n- At least one explicit numeric split example (e.g., 70/30, 80%, etc.).\n- Mention of cap/fee framework (e.g., monthly fee, transaction fee, annual cap) if relevant to the model (simple flat-split is acceptable but note if no cap/fee is mentioned).\n- Internal coherence (e.g., splits that imply broker/agent shares sum logically; if a single percentage is shown, confirm it indicates which party receives it).\n\nScoring:\n- 2.4: Clear, distinct splits for Agents and Associate Brokers; includes numeric examples and describes cap/fee handling or explicitly indicates no cap/fees; internally coherent.\n- 1.6: Covers both roles with at least one numeric example but cap/fee handling is vague or omitted; mostly coherent.\n- 0.8: Mentions both roles but lacks numeric specificity OR only one role has numeric detail.\n- 0.0: Does not clearly cover Agent and Associate Broker splits.", "expectation": "A clear Commission Split Structure with numeric split examples for Agents and Associate Brokers and intelligible cap/fee handling."}, {"type": "llm_judge", "name": "Qualifying Broker Compensation Model Specified", "description": "Verifies the document explicitly states a compensation model for the Qualifying Broker (e.g., monthly retainer, per-transaction oversight fee, override on company split, hybrid), with numeric details and applicability to FL/GA/NC.", "weight": 2.4, "judge_prompt": "Evaluate whether the document explicitly defines a compensation model for the Qualifying Broker (QB) contracted with Sample Realty and ties it to the states of operation (FL, GA, NC). Focus on presence and specificity, not prose quality.\n\nLook for:\n- A clearly stated model for QB compensation (e.g., retainer, per-transaction oversight fee, override percentage of company split, hybrid), ideally with numeric amounts or ranges.\n- Indication that the model applies in FL, GA, and NC, or a note that state-specific variations may apply.\n- Coherence with the Commission Split Structure (e.g., an override references which portion it applies to).\n\nScoring:\n- 2.4: QB compensation model clearly defined with numeric details; explicit applicability to FL/GA/NC or a clear state-variation note; coherent with the split structure.\n- 1.6: QB model present but only partially numeric or vague on state applicability; generally coherent.\n- 0.8: QB model mentioned but non-specific (no numbers) and no state context.\n- 0.0: No explicit QB compensation model present.", "expectation": "An explicit, numeric Qualifying Broker compensation model with state applicability context and coherence with splits."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality Assessment", "description": "Holistic professional quality, strategic fit, and clarity for a non-licensed founder audience.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and One-Page Fit", "description": "Assesses professional polish, clear headings, scannability, and true one-page fit without crowding.", "weight": 1.0, "judge_prompt": "Assess the document's professional presentation for a one-page business-ready artifact: clear hierarchy (title/headers), readable spacing, scannable bullets/tables, and tidy layout that comfortably fits a single page. Do not re-check required structure beyond presentation quality.\n\nScoring:\n- 1.0: Polished, well-structured, easy to scan; excellent one-page fit.\n- 0.6: Generally professional but mildly crowded or with minor layout issues.\n- 0.3: Noticeably cluttered or weak hierarchy but still usable.\n- 0.0: Poorly formatted, hard to read, or effectively not a usable one-pager.", "expectation": "A clean, professional one-pager with clear hierarchy and scannability."}, {"type": "llm_judge", "name": "Clarity and Actionability of Terms", "description": "Evaluates whether a non-licensed founder can understand and implement the plan.", "weight": 1.0, "judge_prompt": "Judge the clarity and actionability of the compensation plan for a non-licensed founder. Are terms defined or intuitive? Are numbers and examples sufficiently concrete to implement? Is ambiguity minimized (e.g., who pays what, when, and how)?\n\nScoring:\n- 1.0: Clear and actionable; roles, numbers, and processes are easy to follow; minimal ambiguity.\n- 0.6: Mostly clear but some steps/terms would require clarification.\n- 0.3: Several ambiguous or undefined terms; implementation would be risky without further guidance.\n- 0.0: Confusing or non-actionable.", "expectation": "Plain-language, unambiguous terms with concrete examples and implementation cues."}, {"type": "llm_judge", "name": "Strategic Suitability for a New Multi-State Brokerage", "description": "Assesses whether the model fits Sample Realty\u2019s early-stage, multi-state launch context.", "weight": 1.0, "judge_prompt": "Evaluate strategic fit for a new, multi-state brokerage (FL/GA/NC). Consider whether the model is lean and scalable, addresses recruiting incentives (e.g., competitive splits, caps), and notes where state-by-state variation may be needed without overcomplication.\n\nScoring:\n- 1.0: Strong fit; scalable, competitive, and cognizant of multi-state nuances.\n- 0.6: Generally suitable but lacking in either competitiveness or multi-state considerations.\n- 0.3: Weak fit; likely problematic for scaling or recruiting.\n- 0.0: Not suitable for a new multi-state brokerage.", "expectation": "A lean, scalable model with competitive positioning and acknowledgment of state variation."}, {"type": "llm_judge", "name": "Compliance Awareness and Risk Framing", "description": "Checks for appropriate disclaimers and compliance framing without delivering legal advice.", "weight": 1.0, "judge_prompt": "Assess whether the document appropriately frames compliance and legal risk: e.g., notes that laws vary by state, that policies must comply with state real estate commissions, and recommends legal review. It should avoid implying legal advice.\n\nScoring:\n- 1.0: Clear compliance framing and prudent disclaimers without overreaching into legal advice.\n- 0.6: Mentions compliance but lacks a clear disclaimer or next steps (e.g., legal review).\n- 0.3: Minimal/implicit compliance references.\n- 0.0: No compliance awareness or misleading legal assertions.", "expectation": "A concise compliance note with a recommendation for legal review and state-commission alignment."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "15ddd28d-8445-4baa-ac7f-f41372e1344e", "rubric": {"category_name": "Automotive Supplier Negotiation Strategy (Manufacturing - Buyers & Purchasing Agents)", "rationale": "Pattern B (Document). We require a 2\u20133-page executive-ready negotiation strategy in PDF/DOCX with a strict section structure that makes verification trivial. Stage 1 is an LLM-only gate enforcing format and section presence. Stage 2 mixes light code checks (keyword/constraint/bounds/length) with heavier LLM verification on correctness and feasibility tied to concrete constraints (volumes, timelines, tooling). Stage 3 provides holistic quality assessment for executive usability, clarity, and strategic value.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate (MANDATORY)", "description": "LLM-only gate to ensure the output is a 2\u20133-page strategy document in PDF/DOCX with the exact structure needed for verification. No calculation/quality checks here\u2014only presence and structure.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Document Format and Structural Completeness", "description": "Verify the candidate produced a properly structured 2\u20133-page strategy document in PDF/DOCX with all required sections present so later verification is possible.", "weight": 4.0, "judge_prompt": "You are evaluating whether the submitted file is a valid, well-structured strategy document with the REQUIRED sections present. Only check presence/format, not quality or correctness of ideas.\n\nRequirements to check:\n1) Format and length\n- File type is PDF or DOCX (not plain text, not Excel)\n- Length is approximately 2\u20133 pages (allow 2\u20134 pages if layout makes content dense)\n- Professionally formatted with clear headings and subsections\n\n2) REQUIRED sections with visible headers (be flexible with exact wording):\n- Title block and Executive Summary (Executive Summary should begin on the first page)\n- Situation Overview and Objectives (context, urgency, production risk)\n- Preferred Path with LPI (empathy-driven engagement to understand supplier issues; sincere attempt despite low probability)\n- Negotiation Framework explicitly naming both: BATNA and ZOPA\n- Continuity Options and Bridge Plan (near-term plan to avoid production halt; may reference LPI capacity and delivery flexibility)\n- Transition Plan to Local Supplier (timeline/action plan; milestones; note that plastics and electronics can proceed in parallel)\n- Tooling Ownership and Exit Management (tooling transfer, clean exit clauses)\n- Commercial Levers (e.g., flexible delivery, advance payments, residual low-volume business)\n- Risks and Mitigations (risk register or equivalent)\n\n3) OPTIONAL supporting elements (do not penalize if missing unless required above is missing):\n- RACI/Governance cadence (e.g., roles for Engineering, Quality, Purchase)\n- Timeline table or Gantt-style visualization and/or KPIs\n\nScoring:\n- 4.0: PDF/DOCX; ~2\u20133 pages; all REQUIRED sections present with clear headers\n- 3.0: PDF/DOCX; ~2\u20133 pages; missing exactly one REQUIRED section\n- 2.0: PDF/DOCX; length OK; missing two REQUIRED sections or headers unclear\n- 1.0: PDF/DOCX but length far off and/or only 2\u20133 required sections present\n- 0.0: Not PDF/DOCX OR fewer than 2 pages OR missing multiple core sections\n\nOnly assess structural presence/format. Do not judge content quality or correctness yet.", "expectation": "A 2\u20133-page PDF/DOCX with all required sections clearly labeled, enabling straightforward verification later."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Feasibility Verification", "description": "Now that structure exists, check factual alignment with constraints, presence of key concepts, and feasibility of plans. Mix light code checks with LLM reasoning. Code rules carry lower weight than LLM rules.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Constraint References Present", "description": "Checks the document references key scenario constraints: demand 800/month, LPI capacity 1,500 (ramp 2,500), 25-day tooling transfer, 3\u20134 months plastics, 4\u20135 months electronics, and parallel development/transfer.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float score in [0, 1] representing fraction of constraints referenced\n    \"\"\"\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n\n    text = ''\n    try:\n        if output.is_document:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    text = ''\n        if not text and output.is_text_format:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = ''\n    except Exception:\n        text = ''\n\n    if not text:\n        return 0.0\n\n    t = text.lower()\n\n    patterns = [\n        r\"\\b800\\b\",                                  # monthly demand 800\n        r\"\\b1[\\,\\s]?500\\b\",                         # 1,500 capacity (flexible formatting)\n        r\"\\b2[\\,\\s]?500\\b\",                         # 2,500 ramp\n        r\"25\\s*(day|days)\",                           # 25-day tooling transfer\n        r\"(3\\s*-\\s*4|3\u20134|3 to 4)\\s*month\",           # plastics 3\u20134 months\n        r\"(4\\s*-\\s*5|4\u20135|4 to 5)\\s*month\",           # electronics 4\u20135 months\n        r\"parallel\",                                   # parallel development\n        r\"tool(ing)?\\s+transfer\"                       # explicit mention of tooling transfer\n    ]\n\n    matches = 0\n    for p in patterns:\n        try:\n            if re.search(p, t):\n                matches += 1\n        except re.error:\n            continue\n\n    ratio = matches / len(patterns)\n    # Return as 0..1; platform will weight externally\n    return float(ratio)\n"}, {"type": "code", "name": "Length Suitability (Words \u2248 2\u20133 pages)", "description": "Estimates word count to verify the document is likely 2\u20133 pages. Full credit if ~800\u20131200 words; partial if ~600\u20131400.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n\n    text = ''\n    try:\n        if output.is_document:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    text = ''\n        if not text and output.is_text_format:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = ''\n    except Exception:\n        text = ''\n\n    if not text:\n        return 0.0\n\n    words = re.findall(r\"\\w+\", text)\n    wc = len(words)\n\n    # Scoring: 1.0 if 800-1200; 0.7 if 600-1400; else 0.3 if 450-1600; else 0\n    if 800 <= wc <= 1200:\n        return 1.0\n    elif 600 <= wc <= 1400:\n        return 0.7\n    elif 450 <= wc <= 1600:\n        return 0.3\n    else:\n        return 0.0\n"}, {"type": "code", "name": "Key Negotiation Concepts Present", "description": "Checks for explicit presence of BATNA and ZOPA, plus operational planning cues (timeline/Gantt/milestones) and risk mitigation mention.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n\n    text = ''\n    try:\n        if output.is_document:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    text = ''\n        if not text and output.is_text_format:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = ''\n    except Exception:\n        text = ''\n\n    if not text:\n        return 0.0\n\n    t = text.lower()\n    checks = {\n        'batna': r\"\\bbatna\\b\",\n        'zopa': r\"\\bzopa\\b\",\n        'timeline': r\"\\b(timeline|gantt|milestone|roadmap)\\b\",\n        'risk': r\"\\brisk\\b.*\\b(mitigate|mitigation|owner|action)\\b\"\n    }\n\n    hits = 0\n    for k, p in checks.items():\n        try:\n            if re.search(p, t, flags=re.DOTALL):\n                hits += 1\n        except re.error:\n            continue\n\n    return hits / len(checks)\n"}, {"type": "llm_judge", "name": "Negotiation Framework Soundness (BATNA, ZOPA)", "description": "Assesses if BATNA and ZOPA are realistic and tailored to LiIon/LPI context (domestic transition timing, volumes, and the supplier\u2019s capacity).", "weight": 1.7, "judge_prompt": "Evaluate the BATNA and ZOPA sections for realism, internal consistency, and tailoring to the scenario.\n\nLook for:\n- BATNA ties to transitioning to domestic suppliers with durations consistent with constraints (plastics 3\u20134 months; electronics 4\u20135 months; can proceed in parallel; tooling transfer ~25 days)\n- ZOPA is articulated with plausible ranges/conditions (e.g., price/volume commitment, delivery flexibility, partial/bridge supply, residual low-volume business)\n- Use of actual volumes and capacities (800/month demand; LPI 1,500 capacity; ramp to 2,500)\n- Recognition that a negotiated bridge may be short-term while transition proceeds\n\nScoring guidance:\n- 1.7: Clear, specific, plausible BATNA and ZOPA tightly linked to constraints and volumes\n- 1.1: Reasonable but somewhat generic; minor gaps or soft linkage to constraints\n- 0.6: Superficial/hand-wavy; missing concrete linkage to constraints/volumes\n- 0.0: No meaningful BATNA/ZOPA or factually inconsistent with scenario", "expectation": "BATNA and ZOPA are explicitly stated and numerically/operationally grounded in the provided constraints."}, {"type": "llm_judge", "name": "Continuity and Bridge Plan Feasibility", "description": "Assesses whether the plan can realistically avoid production stoppage in the next weeks using concrete levers with LPI while transition proceeds.", "weight": 1.7, "judge_prompt": "Judge the near-term continuity plan to prevent production halt given LPI\u2019s 3-week stoppage threat.\nCheck for:\n- Concrete bridging tactics: flexible delivery schedules, expedited shipments, overtime/ramp at LPI (to 1,500\u20132,500), buffer stock, safety stock, air/sea freight combos, and/or advance/early payments\n- Diplomacy and supplier empathy consistent with LiIon\u2019s culture to re-open supply lines\n- Volume math alignment (current 800/month vs. LPI capacity); credible ability to cover the next 3\u20138 weeks\n- Clear triggers and monitoring (what success looks like; how quickly to pivot if bridge fails)\n\nScoring guidance:\n- 1.7: Specific, actionable, and numerically grounded plan covering the next 3\u20138 weeks\n- 1.1: Reasonable plan but lacks numerical rigor or specific operational levers\n- 0.6: Vague intent with minimal operational detail\n- 0.0: No continuity plan or ignores near-term risk", "expectation": "A credible bridge plan that could keep production running while the transition is prepared."}, {"type": "llm_judge", "name": "Transition Plan Feasibility (Local Supplier)", "description": "Checks that the transition plan is operationally feasible and respects development, certification, and tooling transfer realities.", "weight": 1.7, "judge_prompt": "Evaluate the transition plan for moving to a domestic supplier.\nLook for:\n- Parallel workstreams: plastics (3\u20134 months) and electronics (4\u20135 months, includes safety certification) proceeding in parallel\n- Tooling ownership leveraged and transfer lead time (~25 days) addressed, including logistics and readiness at new supplier\n- Milestones and gating: design freeze, pilot builds, validation, PPAP, SOP, quality readiness\n- Realistic risk/mitigation (e.g., certification delays, supplier readiness, component availability)\n\nScoring guidance:\n- 1.7: Detailed, milestone-based plan reflecting parallel tracks and realistic durations\n- 1.1: Mostly sound but missing some milestones or clarity on certification/tooling\n- 0.6: High-level only; weak timelines or ignores certification/tooling realities\n- 0.0: No workable transition plan", "expectation": "A milestone-based, parallel transition plan aligned with the stated timeframes and tooling constraints."}, {"type": "llm_judge", "name": "Tooling Ownership and Exit Management Rigor", "description": "Assesses whether the plan leverages LiIon\u2019s ownership of plastic tooling and manages LPI\u2019s exit diplomatically and legally.", "weight": 1.7, "judge_prompt": "Assess how the document addresses tooling ownership and LPI exit management.\nCheck for:\n- Explicit recognition that LiIon owns plastic-part tooling; handling for retrieval/transfer and readiness at new supplier\n- Clean exit mechanisms: termination assistance, knowledge transfer, IP/licensing separation for electronics, indemnities, and non-disruption clauses\n- Diplomatic tone balancing relationship preservation with assertive protection of LiIon\u2019s production continuity\n\nScoring guidance:\n- 1.7: Clear, actionable, and balanced plan with legal/operational steps\n- 1.1: Reasonable but missing some critical elements\n- 0.6: Vague statements with little actionable detail\n- 0.0: Tooling ownership/exit path not addressed", "expectation": "A concrete, diplomatic exit/transfer plan that protects continuity and leverages tooling ownership."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Holistic Quality and Executive Readiness", "description": "LLM-only qualitative assessment of clarity, strategic value, professionalism, and usability for executive negotiations.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Executive Readiness and Clarity", "description": "Is the document concise, logically structured, and immediately usable in an executive setting?", "weight": 2.0, "judge_prompt": "Assess whether the document is executive-ready: crisp, logically structured, and easy to navigate with clear headers, bullets, and summaries. Consider whether the Executive Summary captures the crux, the flow is coherent, and the language is concise and professional. Score higher if a busy CPO/CEO could use it directly in a meeting.", "expectation": "Concise, well-structured, professional tone with an effective executive summary and navigable sections."}, {"type": "llm_judge", "name": "Strategic Depth and Negotiation Sophistication", "description": "Evaluates creativity and depth of levers, trade-offs, and scenario thinking tailored to LPI/LiIon context.", "weight": 2.0, "judge_prompt": "Evaluate the strategic depth: Are negotiation levers (pricing, volume commitments, delivery flexibility, advance payments, residual low-volume business, clean exit clauses) deployed creatively with trade-offs and contingencies? Is advice tailored to the LPI context rather than generic? Score higher for nuanced, scenario-based thinking.", "expectation": "Thoughtful, tailored strategy with clear trade-offs and multiple levers aligned to the context."}, {"type": "llm_judge", "name": "Actionability and Prioritization", "description": "Assesses the presence of concrete steps, owners, timelines, and metrics so the plan can be executed.", "weight": 2.0, "judge_prompt": "Judge the actionability: Are next steps prioritized with clear owners (e.g., Engineering, Quality, Purchase), timelines, checkpoints, and KPIs? Are assumptions explicit and risks linked to mitigations? Score higher for clear sequencing and decision gates.", "expectation": "A prioritized, owner-assigned action plan with timelines, checkpoints, and measurable outcomes."}, {"type": "llm_judge", "name": "Presentation Quality and Professional Polish", "description": "Checks formatting, readability, and use of visuals/tables that aid comprehension within 2\u20133 pages.", "weight": 2.0, "judge_prompt": "Evaluate presentation quality: professional formatting; readable typography; effective use of bullets, tables, or a compact timeline visual; minimal clutter; consistent terminology. Score higher if visuals or tables make the content easier to digest within the page limit.", "expectation": "Professional, polished presentation that enhances comprehension without exceeding the page budget."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "d025a41c-c439-4ee1-bc79-dd5c94b27a2d", "rubric": {"category_name": "Finance & Insurance \u2013 CSR Chat Coaching Deliverable", "rationale": "This rubric enforces a self-documenting, verifiable Word/PDF deliverable that lists problematic CSR chat statements for three cases, explains why each is problematic, and offers improved alternatives. Stage 1 is a strict LLM-only gate requiring a DOCX/PDF with exact structure (bold case headers, labeled triplets, 1.5 spacing, <5 pages). Stage 2 mixes light code checks (structure, counts, sentence bounds, length proxy) with higher-weight LLM correctness checks (issue validity, rewrite quality, coverage, guide alignment). Stage 3 uses LLM judgment for holistic quality (professional presentation, actionability, tone, concision). Code rules are weighted ~5x less than LLM rules within Stage 2.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2013 Shape Enforcement Gate", "description": "LLM-only gate to ensure the output is a properly structured document enabling verification: DOCX/PDF titled \u201cCase Feedback\u201d, bold headers for Case One/Two/Three, labeled list items with Original/Why/Alternative for each, 1.5 spacing, under 5 pages.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Document Format and Structural Requirements", "description": "Check presence and structure of the required document and sections. Only verify shape, not content quality or correctness.", "weight": 4.0, "judge_prompt": "You are evaluating ONLY the structure/format of the candidate\u2019s deliverable. Do not judge content quality.\n\nRequirements to check (be flexible with minor wording but strict on presence and shape):\n- File format: Prefer DOCX (Word). PDF acceptable if clearly a formatted document. Not plain text/markdown.\n- Title: The document should be titled \u201cCase Feedback\u201d. This can be the file name or a prominent title on page 1.\n- Global formatting: 1.5 line spacing across the document (minor isolated deviations acceptable). Total length < 5 pages.\n- Section headers: Three bold, top-level headings labeled (or extremely close variants): \u201cCase One\u201d, \u201cCase Two\u201d, \u201cCase Three\u201d.\n- Under each case heading: a list of items. Each item must include clearly labeled subparts:\n  1) \u201cOriginal statement:\u201d (or close variant, e.g., \u201cProblematic statement:\u201d), showing the rep\u2019s original words.\n  2) \u201cWhy it\u2019s problematic:\u201d (or close variant), with an explanation intended to be 1\u20133 sentences.\n  3) \u201cImproved alternative:\u201d (or close variant), providing a better phrasing.\n- Minimum quantity: Aim for \u22653 items per case. If a case has only 2 items but everything else is perfect, allow small deduction rather than failing the gate.\n\nScoring (0\u20134):\n- 4.0: DOCX or well-formatted PDF; clear title \u201cCase Feedback\u201d; all three bold case headers; each case has a list with \u22653 items; every item has the three labeled subparts; spacing is 1.5 throughout; total <5 pages.\n- 3.5: All core elements present, but one minor deviation (e.g., one case has only 2 items, or minor spacing inconsistency, or title placement is not prominent) \u2013 still fully verifiable.\n- 2.5: Several structural issues but still recognizable and verifiable (e.g., headers present but not bold; some items missing one label; one case missing the third label consistently; or length borderline but likely <5 pages).\n- 1.0: Document present but largely noncompliant with required structure (e.g., cases not separated, labels largely missing, lists absent).\n- 0.0: Not a DOCX/PDF, cases missing, no labeled items, or clearly >5 pages.\n\nOnly evaluate structure/format and presence of required elements. Do not score content quality or correctness.", "expectation": "A DOCX (or well-formatted PDF) titled \u201cCase Feedback\u201d with bold headings for Case One/Two/Three, each containing \u22653 list items with the labeled triplet: Original, Why, Improved Alternative; 1.5 spacing; <5 pages."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification (Correctness & Compliance)", "description": "Now that the shape is correct, verify correctness and compliance using a mix of deterministic code checks and LLM judgment. Code rules do light structural and bounds checks; LLM rules assess substance.", "is_required": false, "max_points": 10.6, "min_score_to_pass": 5.3, "rules": [{"type": "code", "name": "Headers Present for All Three Cases", "description": "Verify that the document text contains headings for Case One, Case Two, and Case Three.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 0.4\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = ''\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0\n    t = text.lower()\n    needed = [\"case one\", \"case two\", \"case three\"]\n    present = sum(1 for k in needed if k in t)\n    return weight * (present / len(needed))"}, {"type": "code", "name": "Labeled Triplets Presence (Original/Why/Alternative)", "description": "Check that the labeled subparts appear repeatedly, indicating multiple items across cases.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 0.4\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0\n    t = text.lower()\n    cnt_orig = len(re.findall(r\"\\boriginal\\s+statement\\s*:\\s*\", t)) + len(re.findall(r\"\\bproblematic\\s+statement\\s*:\\s*\", t))\n    cnt_alt = len(re.findall(r\"\\b(improved\\s+alternative|better\\s+alternative|suggested\\s+alternative|alternative)\\s*:\\s*\", t))\n    cnt_why = len(re.findall(r\"\\bwhy\\s+(it\\s*'?s|it\\s+is|this\\s+is).*problematic\\s*:\\s*\", t)) + len(re.findall(r\"\\bwhy\\s*this\\s+is\\s+an?\\s+issue\\s*:\\s*\", t))\n    # Scale by thresholds reflecting ~3 items per case\n    score = 0.0\n    # Full if strong evidence of ~9+ items\n    if min(cnt_orig, cnt_alt, cnt_why) >= 9:\n        score = weight\n    # Partial tiers\n    elif min(cnt_orig, cnt_alt, cnt_why) >= 6:\n        score = weight * 0.75\n    elif min(cnt_orig, cnt_alt, cnt_why) >= 3:\n        score = weight * 0.5\n    else:\n        score = weight * 0.0\n    return score"}, {"type": "code", "name": "Explanation Length Bounds (1\u20133 sentences)", "description": "Estimate if explanations between the Why and Alternative labels are 1\u20133 sentences.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 0.4\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0\n    t = text\n    # Capture text between Why...Problematic: and Alternative: labels (case-insensitive, dotall)\n    pattern = re.compile(r\"(why\\s+(?:it\\s*'?s|it\\s+is|this\\s+is).*?problematic)\\s*:\\s*(.+?)\\s*(?:improved\\s+alternative|better\\s+alternative|suggested\\s+alternative|alternative)\\s*:\\s*\", re.IGNORECASE | re.DOTALL)\n    segments = [m.group(2).strip() for m in pattern.finditer(t)]\n    if not segments:\n        return 0.0\n    def sentence_count(s):\n        # crude sentence split on . ! ? ; with length filters\n        parts = re.split(r\"[.!?]+\\s+|[;]\\s+\", s.strip())\n        parts = [p for p in parts if len(p.strip()) > 0]\n        return len(parts)\n    counts = [sentence_count(s) for s in segments]\n    valid = [c for c in counts if 1 <= c <= 3]\n    ratio = (len(valid) / len(counts)) if counts else 0.0\n    return weight * max(0.0, min(1.0, ratio))"}, {"type": "code", "name": "Length Proxy (<5 pages) via Word Count", "description": "Approximate the <5 pages rule using a word-count proxy (<= 2000 words = full, <= 2400 = partial).", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 0.4\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = ''\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0\n    words = re.findall(r\"\\b\\w+\\b\", text)\n    n = len(words)\n    if n <= 2000:\n        return weight\n    elif n <= 2400:\n        return weight * 0.5\n    else:\n        return 0.0"}, {"type": "llm_judge", "name": "Issue Identification Validity", "description": "Do the selected statements represent genuinely problematic customer service phrasing/errors per live chat etiquette?", "weight": 3.5, "judge_prompt": "Assess whether the listed \u201cOriginal/Problematic\u201d statements are truly problematic per live chat etiquette (use general best practices if you cannot open the linked guide). Look for issues like: lack of empathy, robotic/canned tone, blaming language, policy-dumps without guidance, unclear timeframes, vague commitments, deflection/no ownership, jargon, capitalization/punctuation that reads brusque, overuse of negatives, or missing personalization.\n\nScoring (0\u20133.5):\n- 3.5: The majority of selected statements are clearly problematic and representative of common pitfalls. Very few (or none) are nitpicks.\n- 2.5: Mostly valid issues with a few marginal or debatable picks.\n- 1.0: Many selected statements are not clearly problematic or are trivial.\n- 0.0: Selections are largely inappropriate or unrelated.", "expectation": "Items reflect common, material chat issues that plausibly led to low CSAT (tone, clarity, ownership, empathy)."}, {"type": "llm_judge", "name": "Alternative Rewrites Quality", "description": "Evaluate whether the \u201cImproved alternative\u201d phrasings are empathetic, specific, and policy-compliant, and likely to improve customer experience.", "weight": 3.0, "judge_prompt": "Review the \u201cImproved alternative\u201d lines. Judge if they: show empathy and ownership; give clear next steps/timeframes; reduce friction; avoid blame; use customer-friendly language; remain compliant with bank policy. Prefer concise, natural language that feels human and helpful.\n\nScoring (0\u20133.0):\n- 3.0: Rewrites consistently improve tone and clarity, add ownership/timeframes where needed, and feel compliant and helpful.\n- 2.0: Mostly strong, with some opportunities for clearer specifics or tone.\n- 1.0: Improvements are minimal, generic, or sometimes introduce new issues.\n- 0.0: Rewrites are worse, inaccurate, or non-compliant.", "expectation": "Alternatives should be empathetic, action-oriented, and practical without overpromising."}, {"type": "llm_judge", "name": "Coverage and Prioritization", "description": "Check if feedback covers the main pain points in each case (not just minor wording).", "weight": 1.5, "judge_prompt": "For each case, consider whether the feedback addresses the most impactful issues that likely drove low satisfaction (e.g., tone lapses, ownership gaps, unclear resolutions) rather than focusing mainly on minor stylistic nitpicks.\n\nScoring (0\u20131.5):\n- 1.5: Feedback captures key drivers across all cases and prioritizes the most consequential issues.\n- 1.0: Reasonable coverage with some imbalance or missed major opportunities.\n- 0.5: Surface-level coverage or emphasis on low-impact edits.\n- 0.0: Misses main issues entirely.", "expectation": "Each case\u2019s core problems are identified alongside illustrative examples."}, {"type": "llm_judge", "name": "Alignment with Live Chat Etiquette Guide", "description": "Judge whether the reasoning explicitly or implicitly aligns with recognized live chat etiquette best practices (referenced link or general norms).", "weight": 1.0, "judge_prompt": "Evaluate if the explanations (Why it\u2019s problematic) reflect recognized live chat etiquette principles (e.g., empathy, personalization, clarity, positive language, ownership, proactive guidance, avoiding jargon). Explicit citations are optional; alignment in reasoning is sufficient.\n\nScoring (0\u20131.0):\n- 1.0: Clear alignment across most items.\n- 0.5: Partial/implicit alignment.\n- 0.0: Little to no alignment with accepted etiquette principles.", "expectation": "Reasoning maps to established live chat etiquette norms, whether cited or not."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Holistic Quality Assessment", "description": "Professionalism and usefulness of the deliverable as coaching material for a CSR peer in banking.", "is_required": false, "max_points": 5.4, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Readability", "description": "Visual clarity, consistent labeling, easy-to-skim lists, coherent structure beyond the minimum shape.", "weight": 1.5, "judge_prompt": "Assess presentation: Are headings clear and consistent? Are lists easy to scan? Are labels consistent across items? Does the document feel professional and suitable for internal banking use? Ignore strict shape (already checked); judge overall readability and polish.\n\nScoring (0\u20131.5):\n- 1.5: Highly professional, consistent, and easy to navigate.\n- 1.0: Generally clear with minor inconsistencies.\n- 0.5: Readable but somewhat messy or inconsistent.\n- 0.0: Hard to follow or unprofessional.", "expectation": "Clean, consistent formatting that supports quick peer review."}, {"type": "llm_judge", "name": "Actionability of Feedback", "description": "How actionable and coach-like the feedback is for improving future chats.", "weight": 1.5, "judge_prompt": "Evaluate whether each explanation + alternative provides clear, actionable guidance a CSR can apply (what to do differently, how to phrase it, why it matters). Consider whether advice avoids generic platitudes and focuses on specific behaviors.\n\nScoring (0\u20131.5):\n- 1.5: Highly actionable, behavior-focused, and practical.\n- 1.0: Mostly actionable with some generalities.\n- 0.5: Vague or generic, limited coaching value.\n- 0.0: Not actionable.", "expectation": "Concrete, behavior-level coaching a CSR can adopt immediately."}, {"type": "llm_judge", "name": "Tone and Coaching Empathy", "description": "Peer-support tone that is respectful, constructive, and motivating.", "weight": 1.5, "judge_prompt": "Judge the tone of the document as peer-to-peer coaching. It should be respectful, supportive, and constructive without shaming. Look for balanced framing (what worked + what to improve) and positive, growth-oriented language.\n\nScoring (0\u20131.5):\n- 1.5: Consistently empathetic and constructive.\n- 1.0: Generally positive with occasional harshness or missed balance.\n- 0.5: Noticeably critical or dry.\n- 0.0: Demeaning or unprofessional.", "expectation": "Supportive coaching tone suitable for internal team development."}, {"type": "llm_judge", "name": "Concision and Adherence to Constraints", "description": "Overall concision, adherence to 1\u20133 sentence explanations, and keeping length under limit.", "weight": 0.9, "judge_prompt": "Evaluate concision and adherence to constraints (1\u20133 sentence explanations, overall brevity). Minor deviations are acceptable, but the document should avoid unnecessary verbosity and remain within the spirit of <5 pages.\n\nScoring (0\u20130.9):\n- 0.9: Concise and well within the constraints.\n- 0.6: Generally concise with some sections a bit long.\n- 0.3: Often verbose or meandering.\n- 0.0: Clearly overlong or ignores constraints.", "expectation": "Tight, focused writing that respects the brief."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "57b2cdf2-ad62-4591-aa91-aad489740320", "rubric": {"category_name": "Retail Trade \u2014 Private Detectives and Investigators: Surveillance Report Finalization", "rationale": "Pattern B (Document). We enforce a strict, self-documenting PDF report shape first (gate), then verify timeline correctness, chronology, and photo-text alignment using a mix of lightweight code checks and deeper LLM judgments. Finally, we assess professional quality. Stage 1 uses only LLM judging per the philosophy. Stage 2 leans on LLMs for nuanced verification, with small code checks to validate times, headings, and concise structure. Stage 3 evaluates client-ready quality and objectivity.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate", "description": "Mandatory shape enforcement for a verifiable PDF surveillance report.", "is_required": true, "max_points": 2.0, "min_score_to_pass": 1.2, "rules": [{"type": "llm_judge", "name": "Required PDF Structure Present", "description": "Check that the candidate output is a properly structured surveillance report PDF with the exact required sections and basic content layout to enable verification.", "weight": 2.0, "judge_prompt": "You are evaluating the SHAPE ONLY of the candidate\u2019s output. Do not judge content quality or correctness. You may view the rendered PDF and its pages/images.\n\nPass criteria: The output must be a PDF not exceeding 2 pages and must include the exact report sections: Summary, Surveillance, and Assessment. The Surveillance section should present a clear, timestamped timeline of events. The report should reference or include photos so that alignment can be verified later.\n\nCheck the following:\n1) File format and length:\n   - Is the output a PDF? \n   - Does it have 1\u20132 pages (not more than 2)?\n2) Required sections (case-insensitive headers acceptable):\n   - A visible section header \u201cSummary\u201d\n   - A visible section header \u201cSurveillance\u201d\n   - A visible section header \u201cAssessment\u201d\n3) Surveillance timeline structure:\n   - At least 3 discrete observation entries with explicit times (e.g., 7:30 p.m., 21:15, 12:05 a.m.)\n   - Times are clearly identifiable (e.g., leading each entry, bolded, or otherwise easy to spot)\n4) Photographs:\n   - At least one embedded photo in the PDF OR explicit references in-text (e.g., \u201cPhoto 1\u201d, \u201cFigure 2\u201d) to photos included within the PDF pages.\n\nScoring:\n- 2.0: All checks satisfied (PDF, \u22642 pages, all 3 headers present, \u22653 timestamped observations, and at least one photo embedded or clearly referenced within the PDF).\n- 1.5: PDF and \u22642 pages with all 3 headers, but only 2 timestamped entries OR photos referenced but not obviously embedded (or vice versa).\n- 1.0: PDF and \u22642 pages with all 3 headers, but fewer than 2 timestamped entries AND no clear photo presence/reference.\n- 0.5: PDF but missing one required header OR more than 2 pages.\n- 0.0: Not a PDF or structure too far from requirements.\n\nOnly evaluate presence/format, not correctness of observations or identity in photos.", "expectation": "A 1\u20132 page PDF with visible headers: Summary, Surveillance, Assessment; a timestamped timeline under Surveillance; and at least one embedded or clearly referenced photo."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Timeline, Consistency, Alignment)", "description": "Verify timeline window, chronology, references to photos, and alignment between text and images. Mixed code + LLM rules with heavier LLM weighting.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Timeline Window and Chronology Check", "description": "Parse times in the PDF text; reward if multiple timestamps are present, mostly chronological, and within 7:30 p.m. (Jul 3) to 1:00 a.m. (Jul 4) window.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float or (float, str) within [0, weight]\n    \"\"\"\n    weight = 0.8\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = ''\n    try:\n        if output.filename.lower().endswith('.pdf'):\n            text = context.files.read_pdf_text(output.id)\n        elif output.filename.lower().endswith('.docx'):\n            text = context.files.read_docx_text(output.id)\n        else:\n            # Fallback for other texty docs (should not happen per gate)\n            text = context.files.read_text(output.id)\n    except Exception:\n        return 0.0\n\n    if not text or not isinstance(text, str):\n        return 0.0\n\n    s = text\n    # Combined time patterns: HH:MM am/pm, H am/pm, 24h HH:MM\n    ampm_min = r'\\b((?:1[0-2]|0?[1-9]):[0-5]\\d)\\s*(a\\.m\\.|am|p\\.m\\.|pm)\\b'\n    ampm_hr  = r'\\b((?:1[0-2]|0?[1-9]))\\s*(a\\.m\\.|am|p\\.m\\.|pm)\\b'\n    h24      = r'\\b((?:[01]?\\d|2[0-3]):[0-5]\\d)\\b'\n\n    pattern = re.compile(f'({ampm_min})|({ampm_hr})|({h24})', re.IGNORECASE)\n\n    def parse_time(tok, meridian=None):\n        # Returns minutes since Jul 3 00:00; if after midnight (AM <=05:59) -> add 24h\n        t = tok.strip()\n        h, m = 0, 0\n        if ':' in t:\n            parts = t.split(':')\n            try:\n                h = int(parts[0])\n                m = int(re.sub(r'[^0-9]', '', parts[1]) or 0)\n            except Exception:\n                return None\n        else:\n            try:\n                h = int(re.sub(r'[^0-9]', '', t))\n                m = 0\n            except Exception:\n                return None\n        mer = (meridian or '').lower().replace(' ', '')\n        minutes = None\n        if mer:\n            # Normalize am/pm\n            is_pm = 'p' in mer\n            is_am = 'a' in mer\n            base_h = h % 12\n            if is_pm:\n                base_h += 12\n            minutes = base_h * 60 + m\n            # After-midnight AM hours up to 05:59 considered next day relative to evening surveillance\n            if is_am and (base_h < 6):\n                minutes += 24 * 60\n        else:\n            # 24-hour clock\n            minutes = h * 60 + m\n            # Treat 00:00-05:59 as after midnight next day for the surveillance context\n            if 0 <= minutes < 6 * 60:\n                minutes += 24 * 60\n        return minutes\n\n    times = []\n    for m in pattern.finditer(s):\n        if m.group(1):\n            # ampm with minutes\n            tm = re.search(r'((?:1[0-2]|0?[1-9]):[0-5]\\\\d)', m.group(1), re.IGNORECASE)\n            mer = re.search(r'(a\\.m\\.|am|p\\.m\\.|pm)', m.group(1), re.IGNORECASE)\n            if tm and mer:\n                val = parse_time(tm.group(1), mer.group(1))\n                if val is not None:\n                    times.append(val)\n        elif m.group(3):\n            # ampm hour only\n            tm = re.search(r'((?:1[0-2]|0?[1-9]))', m.group(3), re.IGNORECASE)\n            mer = re.search(r'(a\\.m\\.|am|p\\.m\\.|pm)', m.group(3), re.IGNORECASE)\n            if tm and mer:\n                val = parse_time(tm.group(1), mer.group(1))\n                if val is not None:\n                    times.append(val)\n        elif m.group(5):\n            # 24h\n            val = parse_time(m.group(5))\n            if val is not None:\n                times.append(val)\n\n    if not times:\n        return (0.0, 'No parsable timestamps found')\n\n    # Window: Jul 3 19:30 (1170) to Jul 4 01:00 (1500)\n    start_win = 19 * 60 + 30\n    end_win = 24 * 60 + 60\n\n    within = [t for t in times if (start_win <= t <= end_win)]\n    prop_within = len(within) / max(len(times), 1)\n\n    nondec_pairs = 0\n    for i in range(1, len(times)):\n        if times[i] >= times[i-1]:\n            nondec_pairs += 1\n    chron_score = nondec_pairs / max(len(times)-1, 1)\n\n    # Presence of at least 2 timestamps\n    presence = 1.0 if len(times) >= 2 else 0.5\n\n    # Aggregate: 40% window, 40% chronology, 20% presence\n    raw = 0.4 * prop_within + 0.4 * chron_score + 0.2 * presence\n    raw = max(0.0, min(1.0, raw))\n    return (raw * weight, f\"Found {len(times)} timestamps; {len(within)} in window; chronology {chron_score:.2f}\")"}, {"type": "code", "name": "Headers and Concision Check", "description": "Verify presence of required headers in text and check concise length (word count).", "weight": 0.7, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 0.7\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        if output.filename.lower().endswith('.pdf'):\n            text = context.files.read_pdf_text(output.id)\n        elif output.filename.lower().endswith('.docx'):\n            text = context.files.read_docx_text(output.id)\n        else:\n            text = context.files.read_text(output.id)\n    except Exception:\n        return 0.0\n    if not text:\n        return 0.0\n    t = text\n    # Header presence: count exact section names as standalone headers (case-insensitive)\n    present = 0\n    for hdr in [r'^\\s*summary\\s*$', r'^\\s*surveillance\\s*$', r'^\\s*assessment\\s*$']:\n        if re.search(hdr, t, flags=re.IGNORECASE | re.MULTILINE):\n            present += 1\n        else:\n            # Fallback: at least appears somewhere\n            if re.search(hdr.strip('^$'), t, flags=re.IGNORECASE):\n                present += 0.5\n    headers_score = present / 3.0\n\n    # Concision via word count heuristic\n    words = len(re.findall(r\"\\b\\w+\\b\", t))\n    # Full credit 200-900 words; partial 120-199 or 901-1200; otherwise 0\n    if 200 <= words <= 900:\n        concision = 1.0\n    elif (120 <= words < 200) or (900 < words <= 1200):\n        concision = 0.5\n    else:\n        concision = 0.0\n\n    raw = 0.6 * headers_score + 0.4 * concision\n    raw = max(0.0, min(1.0, raw))\n    return raw * weight"}, {"type": "code", "name": "Photo Reference Presence", "description": "Check that the text references photos/figures/images, enabling alignment checks.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 0.5\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        if output.filename.lower().endswith('.pdf'):\n            text = context.files.read_pdf_text(output.id)\n        elif output.filename.lower().endswith('.docx'):\n            text = context.files.read_docx_text(output.id)\n        else:\n            text = context.files.read_text(output.id)\n    except Exception:\n        return 0.0\n    if not text:\n        return 0.0\n\n    t = text.lower()\n    # Look for references like \"photo\", \"figure\", \"image\", \"fig.\" and numbered labels\n    has_kw = any(k in t for k in [\"photo\", \"figure\", \"image\", \"fig.\"])\n    score = 0.0\n    if has_kw:\n        score = 0.6\n        # Bonus if we find numbered references and a nearby time\n        # e.g., \"Photo 1\" or \"Figure 2\" with a time within 50 chars\n        numbered = list(re.finditer(r'(photo|figure|fig\\.)\\s*\\d+', t))\n        if numbered:\n            # simple time pattern near reference\n            time_pat = re.compile(r'((?:[01]?\\d|2[0-3]):[0-5]\\d|(?:1[0-2]|0?[1-9])\\s*(?:a\\.m\\.|am|p\\.m\\.|pm))')\n            near_with_time = 0\n            for m in numbered:\n                start = max(0, m.start()-50)\n                end = min(len(t), m.end()+50)\n                if time_pat.search(t[start:end]):\n                    near_with_time += 1\n            if near_with_time > 0:\n                score = 1.0\n    return score * weight"}, {"type": "llm_judge", "name": "Chronological Narrative and Time Emphasis", "description": "Events should be in chronological order and times clearly emphasized/scan-friendly.", "weight": 2.5, "judge_prompt": "Evaluate whether the Surveillance section presents a clear, chronological timeline with prominent times (e.g., starting each entry with a time, bolding times, or otherwise making times easy to scan).\n\nCheck:\n- Are events ordered chronologically from the start of surveillance through end?\n- Are times consistently formatted and easy to spot?\n- Does the timeline reflect a start around 7:30 p.m. and coverage through the client\u2019s requested 9:00 p.m. to 1:00 a.m. window?\n\nScoring:\n- 2.5: Fully chronological, consistent, and clearly emphasized times; explicitly indicates coverage of the requested 9 p.m.\u20131 a.m. window.\n- 1.5: Mostly chronological with minor inconsistencies; times present but not consistently emphasized or explicit window coverage unclear.\n- 0.5: Limited chronology; times scattered or inconsistent; window coverage ambiguous.\n- 0.0: Not chronological or times unclear/non-emphasized.", "expectation": "A clean, chronological log with prominent, consistent timestamps clearly spanning the requested surveillance window."}, {"type": "llm_judge", "name": "Photo\u2013Text Alignment", "description": "Do embedded photos (or clearly referenced ones) align with the observations and identifications in the text?", "weight": 3.5, "judge_prompt": "Compare the report\u2019s narrative with the photos visible in the PDF (or clearly referenced/embedded images on the pages).\n\nCheck:\n- Do the referenced photos actually appear and correspond to the described moments (e.g., time, location, activity)?\n- Are captions or in-text references consistent with what the image shows (e.g., vehicle, attire, apparent subject)?\n- Are identity references cautious and evidence-based (e.g., \u201cappears consistent with provided photo\u201d)?\n\nScoring:\n- 3.5: Photos are present and consistently match described observations with sensible captions/references.\n- 2.5: Photos mostly align; minor mismatches/ambiguity.\n- 1.0: Photos present but weak linkage to text or unclear relevance.\n- 0.0: No usable photo alignment (missing, mismatched, or not referenced).", "expectation": "Photos are embedded/referenced and clearly tied to specific observations without overclaiming identity."}, {"type": "llm_judge", "name": "Accuracy and Specificity of Observations", "description": "Assess whether observations are factual, specific, and free from speculation; times and actions are precise.", "weight": 2.0, "judge_prompt": "Assess the factual nature and specificity of the observations.\n\nCheck:\n- Are actions described concretely (e.g., entering/exiting locations, interactions) with times and relevant descriptors (vehicle, locations)?\n- Avoids speculation or defamatory assertions; uses objective language.\n- Any inferences are labeled as such and grounded in observable facts.\n\nScoring:\n- 2.0: Precise, factual, and objective throughout; any inferences minimal and clearly framed.\n- 1.0: Mostly factual with occasional vague or speculative phrasing.\n- 0.0: Frequent speculation, unsubstantiated claims, or unclear descriptions.", "expectation": "Objective, timestamped, specific observations that avoid speculation."}, {"type": "llm_judge", "name": "Compliance with Instructions and Scope", "description": "Verify the report follows client/timeframe instructions and removes extraneous information.", "weight": 2.0, "judge_prompt": "Check compliance with the assignment brief.\n\nConfirm:\n- Start time around 7:30 p.m. as planned by the office, and coverage through the client\u2019s requested 9:00 p.m. to 1:00 a.m. window.\n- The Summary, Surveillance, and Assessment sections are used as specified.\n- Unnecessary details have been removed; content focuses on relevant observations and client needs.\n\nScoring:\n- 2.0: Fully complies on timing, structure, and focus.\n- 1.0: Minor deviations (e.g., slight timing ambiguity or minor extraneous info).\n- 0.0: Significant non-compliance (wrong sections, off-scope content, or missing required time coverage).", "expectation": "Report adheres to the instructed timeline and structure and stays focused on relevant facts."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality", "description": "Holistic quality of writing, presentation, and client-readiness.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity, Grammar, and Readability", "description": "Evaluate overall clarity and correctness after edits.", "weight": 2.0, "judge_prompt": "Judge the clarity and correctness of the final report\u2019s prose.\n\nConsider:\n- Grammar, punctuation, and sentence structure improvements.\n- Clear, concise phrasing suitable for a client-facing surveillance report.\n\nScoring:\n- 2.0: Clean, professional prose; no notable errors.\n- 1.0: Generally clear with minor issues.\n- 0.0: Noticeable errors that impede professionalism.", "expectation": "Clean, concise, error-free writing suitable for a client."}, {"type": "llm_judge", "name": "Professional Tone and Ethical Compliance", "description": "Objective, non-judgmental tone; respects privacy and legal/ethical boundaries.", "weight": 1.5, "judge_prompt": "Evaluate tone and ethical posture.\n\nCheck for:\n- Objective, non-inflammatory language.\n- Avoidance of defamatory or speculative claims.\n- No unnecessary personally identifying information beyond investigative need.\n\nScoring:\n- 1.5: Fully professional and ethically cautious.\n- 0.8: Generally appropriate with minor tone/ethics lapses.\n- 0.0: Inappropriate tone or ethical issues.", "expectation": "Objective, respectful, and ethically compliant tone."}, {"type": "llm_judge", "name": "Structure and Scanability", "description": "Visual organization, times emphasized, and easy scanning for a client.", "weight": 1.5, "judge_prompt": "Assess structural quality and scanability.\n\nConsider:\n- Clear sectioning and consistent formatting.\n- Times visually emphasized (e.g., lead each entry, bold, or columns) to facilitate quick review.\n- Logical flow from Summary to Surveillance to Assessment.\n\nScoring:\n- 1.5: Highly scannable and well-structured.\n- 0.8: Reasonably organized with minor issues.\n- 0.0: Disorganized or hard to scan.", "expectation": "A well-structured, easy-to-scan report with obvious timestamps."}, {"type": "llm_judge", "name": "Actionable Assessment and Concise Summary", "description": "Summary and Assessment provide actionable insights without excess length.", "weight": 1.0, "judge_prompt": "Evaluate whether the Summary and Assessment sections succinctly convey key findings and recommended next steps (if any) without exceeding the two-page constraint.\n\nScoring:\n- 1.0: Concise, actionable, and aligned to client needs.\n- 0.5: Generally useful but somewhat wordy or light on actionability.\n- 0.0: Vague, non-actionable, or bloated.", "expectation": "Tight, client-oriented Summary and Assessment with clear takeaways."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "d4525420-a427-4ef2-b4e9-2dcc2d31b3b6", "rubric": {"category_name": "Retail Supervisor Hiring Decision (Overnight Manager Selection)", "rationale": "This rubric enforces a self-documenting, verifiable decision memo for selecting an overnight hourly manager. Stage 1 (LLM-only) strictly mandates a document shape that makes verification trivial, including a named candidate, a 5\u20137 sentence rationale paragraph, and an evidence checklist that encodes priority weights (leadership/adaptability highest; productivity lowest). Stage 2 mixes lightweight code checks (sentence count, factor mentions, explicit de-prioritization of productivity, clear candidate naming) with LLM verifications for priority alignment, evidence grounding, and overnight-role suitability. Stage 3 provides a holistic quality assessment of tone, clarity, concision, and managerial usefulness. Code-rule weights are kept ~5x lower than LLM rules on average as required.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate", "description": "Verify the output is a properly structured decision memo document that enables trivial verification later. LLM-only gate.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.5, "rules": [{"type": "llm_judge", "name": "Structured Decision Memo Requirements", "description": "Output must be a single decision memo file (PDF, DOCX, or Markdown) with required sections and elements.", "weight": 6.0, "judge_prompt": "You are checking ONLY structure and format (not content quality or correctness). Examine the candidate's primary output.\n\nAcceptable formats: PDF, DOCX, or Markdown (.md). The document should be professionally formatted and readable.\n\nRequired structural elements (be flexible with exact phrasing of headers):\n1) Title/Header near the top indicating the purpose, e.g., \"Overnight Manager Selection\" or \"Overnight Manager Recommendation\".\n2) A clear, explicit line that names the selected candidate (e.g., \"Candidate Selected:\" or \"Selected Candidate:\" or \"Recommendation:\" followed by a person\u2019s name).\n3) A single rationale paragraph that is 5\u20137 sentences long.\n4) An \"Evidence Checklist\" presented as a visible table (Markdown table or embedded document table) with columns that correspond to the following intent: [Criterion | Evidence Source | Evidence Snippet | Priority]. You may accept close variants of column names, but the structure must clearly map to those four concepts.\n5) The Evidence Checklist must include at least one row each for these four decision factors (allow flexible wording/synonyms):\n   - Leadership/Management Potential (Priority should be High)\n   - Adaptability/Ability to Step into New Roles (Priority should be High)\n   - Attendance/Reliability (Priority should be at least Medium or High)\n   - Productivity (cases/hour) (Priority should be Low)\n6) An \"Overnight Considerations\" mini-section (a short bullet list or short sub-section) with at least two points related to overnight shift realities (e.g., reliability with low supervision, issue escalation, safety/security, team direction at night).\n\nScoring guidance:\n- 6.0: All 6 structural requirements present (title, named candidate, 5\u20137 sentence rationale paragraph, 4-column Evidence Checklist, the 4 required factor rows with correct priority indications, and an Overnight Considerations mini-section).\n- 5.0: Exactly one structural element missing or clearly incomplete.\n- 4.0: Exactly two elements missing or clearly incomplete.\n- 2.5: Three elements missing or incomplete.\n- 1.0: Four elements missing or incomplete, but still a readable document in an acceptable format.\n- 0.0: Not a PDF/DOCX/MD file, OR grossly wrong shape (no named candidate and no visible evidence table), OR unreadable.\n\nOnly judge structure/format and presence. Do not judge correctness, persuasiveness, or data accuracy.", "expectation": "A single PDF/DOCX/MD decision memo with a title, a named candidate, a 5\u20137 sentence rationale paragraph, a 4-column evidence checklist with the specified four factors and priorities, and a short overnight considerations section."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness & Compliance)", "description": "Now that the output is in verifiable shape, check priority alignment, evidence grounding, and explicit deprioritization of productivity. Mix of code checks and LLM judgment.", "is_required": false, "max_points": 9.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Deliverable is text document with a 5\u20137 sentence rationale", "description": "Confirm we can extract text and the rationale paragraph is 5\u20137 sentences (flexibly detected).", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output resource.\"\n\n    # Attempt to extract text from supported formats\n    text = \"\"\n    try:\n        if getattr(output, 'is_document', False):\n            # Try PDF first, then DOCX\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    text = \"\"\n        elif getattr(output, 'is_text_format', False):\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n        else:\n            # As a last resort, try reading as text\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    except Exception:\n        text = \"\"\n\n    if not text or len(text.strip()) < 30:\n        return 0.0, \"Could not extract sufficient text from document.\"\n\n    # Try to locate a rationale paragraph\n    lowered = text.lower()\n    # Heuristic: look for a heading cue\n    pattern_heads = r\"(rationale|justification|decision summary|recommendation)\"  # flexible headings\n    idx = None\n    m = re.search(pattern_heads, lowered)\n    if m:\n        idx = m.end()\n    # Extract a candidate paragraph from the vicinity\n    if idx is not None:\n        snippet = text[idx: idx + 2000]\n    else:\n        # fallback: first ~2000 chars\n        snippet = text[:2000]\n\n    # Split into paragraphs\n    paras = [p.strip() for p in re.split(r\"\\n\\s*\\n\", snippet) if p.strip()]\n    # Choose the first paragraph with at least 40 words as the rationale\n    rationale = \"\"\n    for p in paras:\n        if len(re.findall(r\"\\w+\", p)) >= 40:\n            rationale = p\n            break\n    if not rationale:\n        # fallback to first paragraph-like chunk\n        rationale = paras[0] if paras else snippet.strip()\n\n    # Count sentences (simple heuristic)\n    sentences = [s.strip() for s in re.split(r\"(?<=[\\.!?])\\s+\", rationale) if len(s.strip()) > 0]\n    n = len(sentences)\n\n    if 5 <= n <= 7:\n        return 0.4, f\"Rationale sentence count OK: {n}.\"\n    elif 4 <= n <= 9:\n        return 0.2, f\"Rationale sentence count slightly off: {n}.\"\n    else:\n        return 0.0, f\"Rationale sentence count not within 4\u20139: {n}.\""}, {"type": "code", "name": "Mentions required decision factors", "description": "Check the document mentions leadership/management potential, adaptability/new-role readiness, attendance/reliability, and productivity (cases/hour).", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output resource.\"\n\n    # Extract text\n    text = \"\"\n    try:\n        if getattr(output, 'is_document', False):\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    text = \"\"\n        elif getattr(output, 'is_text_format', False):\n            text = context.files.read_text(output.id)\n    except Exception:\n        text = \"\"\n\n    if not text:\n        return 0.0, \"No readable text.\"\n\n    t = text.lower()\n\n    leadership_syn = [\"leadership\", \"management potential\", \"managerial\", \"supervisor potential\", \"lead\", \"team lead\"]\n    adaptability_syn = [\"adaptability\", \"adaptable\", \"new role\", \"new roles\", \"learn quickly\", \"learning agility\", \"versatile\", \"cross-trained\", \"flexible\"]\n    attendance_syn = [\"attendance\", \"reliable\", \"reliability\", \"punctual\", \"punctuality\", \"absence\", \"absences\", \"tardy\", \"tardiness\", \"dependable\"]\n    productivity_syn = [\"productivity\", \"cases per hour\", \"cph\", \"units per hour\"]\n\n    def present(syns):\n        return any(s in t for s in syns)\n\n    hits = [present(leadership_syn), present(adaptability_syn), present(attendance_syn), present(productivity_syn)]\n    count = sum(1 for h in hits if h)\n\n    if count == 4:\n        return 0.4, \"All four factor groups mentioned.\"\n    elif count == 3:\n        return 0.3, \"Three factor groups mentioned.\"\n    elif count == 2:\n        return 0.2, \"Two factor groups mentioned.\"\n    elif count == 1:\n        return 0.1, \"Only one factor group mentioned.\"\n    else:\n        return 0.0, \"No required factor groups detected.\""}, {"type": "code", "name": "Explicit de-prioritization of productivity", "description": "Verify that productivity is explicitly framed as lowest priority (e.g., \"lowest priority\", \"not the primary factor\", etc.).", "weight": 0.35, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output resource.\"\n\n    # Extract text\n    text = \"\"\n    try:\n        if getattr(output, 'is_document', False):\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    text = \"\"\n        elif getattr(output, 'is_text_format', False):\n            text = context.files.read_text(output.id)\n    except Exception:\n        text = \"\"\n\n    if not text:\n        return 0.0, \"No readable text.\"\n\n    t = text.lower()\n\n    # Look for mentions of productivity and low priority phrasing in proximity\n    prod_words = [\"productivity\", \"cases per hour\", \"cph\"]\n    low_phrases = [\"lowest priority\", \"lower priority\", \"low priority\", \"not the primary\", \"secondary to\", \"not the main factor\", \"less important\", \"de-priorit\", \"depriorit\"]\n\n    found_prod = any(w in t for w in prod_words)\n    found_low = any(p in t for p in low_phrases)\n\n    # Also try proximity within 120 chars\n    prox_hit = False\n    for w in prod_words:\n        for m in re.finditer(re.escape(w), t):\n            start = max(0, m.start()-120)\n            end = m.end()+120\n            window = t[start:end]\n            if any(p in window for p in low_phrases):\n                prox_hit = True\n                break\n        if prox_hit:\n            break\n\n    if found_prod and (found_low or prox_hit):\n        return 0.35, \"Productivity explicitly de-prioritized.\"\n    elif found_prod:\n        return 0.15, \"Productivity mentioned but not clearly de-prioritized.\"\n    else:\n        return 0.0, \"No productivity mention detected.\""}, {"type": "code", "name": "Candidate named clearly", "description": "Check for an explicit candidate selection statement with a plausible person name.", "weight": 0.35, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output resource.\"\n\n    # Extract text\n    text = \"\"\n    try:\n        if getattr(output, 'is_document', False):\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    text = \"\"\n        elif getattr(output, 'is_text_format', False):\n            text = context.files.read_text(output.id)\n    except Exception:\n        text = \"\"\n\n    if not text:\n        return 0.0, \"No readable text.\"\n\n    # Look for explicit selection cues and a capitalized name\n    cues = r\"(candidate selected|selected candidate|selected|recommendation|recommend|we recommend|i recommend|i select|we select|our choice|we choose|i choose)\"\n    # Name pattern: Two or three capitalized words (first/last or first/middle/last)\n    name_pat = r\"([A-Z][a-z]+\\s+[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)?)\"\n\n    text_no_newlines = re.sub(r\"\\s+\", \" \", text)\n    m = re.search(cues + r\"\\s*[:\\-]?\\s*\" + name_pat, text_no_newlines)\n    if m:\n        return 0.35, f\"Found explicit selection: {m.group(0)[:60]}...\"\n\n    # Fallback: look for a labeled line like 'Candidate Selected: Name'\n    m2 = re.search(r\"candidate\\s*selected\\s*[:\\-]\\s*\" + name_pat, text_no_newlines, flags=re.IGNORECASE)\n    if m2:\n        return 0.35, f\"Found 'Candidate Selected' line: {m2.group(0)[:60]}...\"\n\n    # Partial: any plausible two-word name present anywhere\n    m3 = re.search(r\"\\b[A-Z][a-z]+\\s+[A-Z][a-z]+\\b\", text)\n    if m3:\n        return 0.15, \"Found a plausible name but not clearly marked as selected.\"\n\n    return 0.0, \"No clear selected-candidate statement detected.\""}, {"type": "llm_judge", "name": "Priority Alignment and Compliance", "description": "Evaluate whether the rationale prioritizes leadership/management potential and adaptability over productivity, and explicitly treats productivity as the lowest priority.", "weight": 2.5, "judge_prompt": "Assess the decision memo\u2019s rationale for PRIORITY ALIGNMENT (not style). Focus on whether the selection clearly prioritizes leadership/management potential and adaptability/new-role readiness above productivity. Look for explicit language that productivity (cases/hour) is the lowest priority. Consider whether the reasoning shows this order in practice, not just by listing it.\n\nScoring guidance:\n- 2.5: Strong, explicit prioritization of leadership and adaptability; productivity clearly de-prioritized; reasoning consistently reflects this order.\n- 1.7: Generally follows priorities but somewhat mixed emphasis or only partially explicit.\n- 0.8: Mentions priorities but emphasis is muddled; could be interpreted as over-weighting productivity.\n- 0.0: Clearly contradicts the stated priorities or ignores them entirely.", "expectation": "Leadership/adaptability clearly outrank productivity in the justification; productivity explicitly framed as lowest priority."}, {"type": "llm_judge", "name": "Evidence Grounding Across Sources", "description": "Check whether the explanation draws on the provided categories of evidence: attendance data, employee evaluations, and interview notes (and acknowledges productivity).", "weight": 2.5, "judge_prompt": "Judge how well the selection rationale anchors claims in plausible evidence types: attendance reliability, performance evaluations (including leadership indicators), interview notes (behaviors/examples), and at least an acknowledgement of productivity. You are not checking factual correctness\u2014only whether the memo cites or references these evidence types in a coherent way.\n\nScoring guidance:\n- 2.5: References all three core sources (attendance, evaluations, interview notes) and mentions productivity context.\n- 1.7: Covers two core sources adequately and mentions productivity.\n- 0.8: Only one core source or very vague references.\n- 0.0: No meaningful evidence grounding or generic claims without any reference to source types.", "expectation": "References attendance, evaluations, and interview notes; acknowledges productivity without overemphasizing it."}, {"type": "llm_judge", "name": "Overnight Role Suitability", "description": "Evaluate whether the memo addresses overnight-specific demands and explains why the candidate is suited for low-supervision, nighttime operations.", "weight": 2.5, "judge_prompt": "Assess how well the memo discusses overnight-shift realities: reliability with minimal supervision, ability to lead small teams, problem-solving/escalation, safety/security, communication across shifts, and maintaining standards. Judge whether the candidate is argued to be a good fit for these constraints.\n\nScoring guidance:\n- 2.5: Clear, concrete overnight-fit reasoning with multiple relevant points.\n- 1.7: Some overnight considerations mentioned, but limited depth.\n- 0.8: Vague or generic statements not specific to overnight context.\n- 0.0: No overnight-specific considerations.", "expectation": "Cites specific overnight considerations (reliability, low supervision, issue handling) and links them to the candidate."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism", "description": "Holistic assessment of writing quality, persuasiveness, and managerial usefulness.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity and Professional Tone", "description": "Writing is clear, professional, and free of distracting errors; tone is appropriate for a store-manager audience.", "weight": 1.25, "judge_prompt": "Rate clarity and professionalism of the memo\u2019s writing. Consider readability, grammar, and tone appropriateness for a store manager. Do not re-check structure\u2014focus on communication quality.\n\n- 1.25: Clear, professional, polished.\n- 0.8: Generally clear with minor issues.\n- 0.4: Noticeable issues that reduce clarity/professionalism.\n- 0.0: Poorly written or unprofessional.", "expectation": "Concise, professional tone suitable for management."}, {"type": "llm_judge", "name": "Concision and Structure", "description": "The memo adheres to 5\u20137 sentence rationale and presents information in an easily scannable structure (title, candidate name, evidence checklist).", "weight": 1.25, "judge_prompt": "Judge whether the memo is concise and well-structured for quick managerial review: a 5\u20137 sentence rationale and clearly surfaced sections, with minimal verbosity.\n\n- 1.25: Tight, well-structured, easy to scan.\n- 0.8: Mostly concise and structured, minor excess or clutter.\n- 0.4: Somewhat rambling or poorly organized.\n- 0.0: Rambling or difficult to follow.", "expectation": "Tight 5\u20137 sentence rationale and easy-to-scan sections."}, {"type": "llm_judge", "name": "Persuasiveness and Decision Defensibility", "description": "Argument feels credible, balanced, and defensible to stakeholders.", "weight": 1.25, "judge_prompt": "Rate how persuasive and defensible the decision feels given the stated priorities and evidence references. Does the argument anticipate reasonable questions and inspire confidence?\n\n- 1.25: Strongly persuasive and defensible.\n- 0.8: Moderately persuasive; some gaps.\n- 0.4: Weakly persuasive; notable gaps.\n- 0.0: Not persuasive or incoherent.", "expectation": "A balanced, defensible rationale that would satisfy a store manager."}, {"type": "llm_judge", "name": "Risk Awareness and Mitigation", "description": "Notes any risks or development needs for the selected candidate and suggests mitigation for overnight transition.", "weight": 1.25, "judge_prompt": "Assess whether the memo briefly acknowledges risks or development needs (e.g., limited tenure, learning curve) and suggests practical mitigation for the overnight transition (coaching, shadowing, checklists, early check-ins).\n\n- 1.25: Clear, practical risk awareness and mitigation.\n- 0.8: Mentions risks or mitigation but not both, or lacks specificity.\n- 0.4: Vague nod to risk without useful mitigation.\n- 0.0: No risk/mitigation consideration.", "expectation": "Explicit brief risks with concrete mitigation steps appropriate to overnight operations."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "58ac1cc5-5754-4580-8c9c-8c67e1a9d619", "rubric": {"category_name": "GMP Change Control and QA Escalation Package (Project Management Specialist)", "rationale": "This rubric enforces a self-documenting, multi-file deliverable that mirrors real GMP practice: a filled change control request (PDF), a QA escalation email draft, an internal summary note for team comms, and a standalone risk assessment (Word). Stage 1 is a strict LLM-only gate that mandates the exact file set and structural sections to enable verification. Stage 2 blends lightweight code checks (text extraction, keyword/structure detection, cross-file presence) with higher-weight LLM judgments on correctness and cross-referencing. Stage 3 assesses professional quality, compliance alignment, and actionability.", "max_total_score": 25.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (Multi-File Package)", "description": "LLM-only gate verifying the submission includes four distinct files with required formats and visible section structure to enable verification.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Deliverables Present with Required Sections", "description": "Check that the candidate delivered a multi-file package with the exact artifacts and structural sections needed for verification.", "weight": 4.0, "judge_prompt": "You are evaluating whether the submission has the required deliverables and visible structure. Review ALL submitted files. This is a structure-only check; do not judge correctness.\n\nRequired deliverables (be flexible with file names, but enforce formats and sections):\n\nA) Change Control Request (PDF only)\n- Must be a PDF with a filled form or structured document showing the following SECTIONS or clearly labeled fields:\n  1) Header info: Change Control ID (or placeholder), Title/Short Description, Requestor, Date\n  2) Discrepancy Description (mismatch between RMS and vendor COA)\n  3) Affected Documentation and Workflows (must reference RMS-3333 and QMS or equivalent)\n  4) Proposed Resolution (e.g., accept under deviation or requalify; RMS update)\n  5) Basic Risk Assessment\n  6) Temporary Controls (e.g., quarantine/hold)\n  7) Follow-up Actions (e.g., RMS update, vendor comm tracking)\n  8) Approvals/Signatures placeholders\n\nB) QA Escalation Email (PDF or DOCX)\n- A document that looks like an email with:\n  - To/CC/Subject fields (or clearly labeled equivalents)\n  - Body referencing the discrepancy, RMS-3333, vendor CompCello, and material QY-GEL Antifoam\n  - An explicit question asking whether to accept under a deviation or require requalification\n  - Reference to the attached Change Control Request\n\nC) Internal Summary Note for Team (MD, PDF, or DOCX)\n- A short internal note with clear headings including:\n  - Issue Summary\n  - Actions Taken So Far (e.g., material hold/quarantine, change control initiated)\n  - Current Status / Next Steps (owners and timing if available)\n\nD) Risk Assessment (DOCX only)\n- A Word document with sections:\n  - Background/Context (vendor change notification was sent; missed due to ex-employee)\n  - Breakdown of how the communication failure occurred\n  - Risks Introduced (operational/documentation)\n  - Recommended Mitigations (e.g., centralized vendor comm tracking, SOP updates)\n  - Owners and Timeline (or placeholders)\n\nScoring (out of 4.0):\n- 4.0: All four deliverables present; formats correct (Change Control is PDF; Risk Assessment is DOCX); each shows the specified sections.\n- 3.5: All four present but one has minor structural omissions OR one of A/D has slight format mismatch but sections are clearly present.\n- 2.0: Missing one core deliverable OR major structural gaps in one of A or D.\n- 0.0: Missing multiple deliverables OR wrong overall format (e.g., only one file, or no PDF/DOCX where required).\n\nOnly evaluate presence/structure and basic formatting. Do not judge correctness or writing quality.", "expectation": "A properly structured, multi-file package ready for verification: PDF change control form with required sections, DOCX risk assessment with mitigation plan sections, and separate QA email and internal summary note documents."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Cross-Verification", "description": "Verify that content accurately describes the discrepancy, references correct identifiers, proposes appropriate paths (deviation vs requalification), and includes actionable mitigation aligned to GMP expectations.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "File Set and Format Validation (Lightweight)", "description": "Programmatically confirm the presence of the four deliverables and expected formats using text heuristics.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float or (float, str)\n    \"\"\"\n    outputs = context.get_all_outputs() or []\n    if not outputs:\n        return 0.0, \"No files submitted.\"\n\n    def read_any(res):\n        text = \"\"\n        # Try PDF\n        try:\n            t = context.files.read_pdf_text(res.id)\n            if isinstance(t, str) and len(t.strip()) > 20:\n                return t, 'pdf'\n        except Exception:\n            pass\n        # Try DOCX\n        try:\n            t = context.files.read_docx_text(res.id)\n            if isinstance(t, str) and len(t.strip()) > 20:\n                return t, 'docx'\n        except Exception:\n            pass\n        # Try text/markdown\n        try:\n            t = context.files.read_text(res.id)\n            if isinstance(t, str) and len(t.strip()) > 20:\n                return t, 'text'\n        except Exception:\n            pass\n        return \"\", \"\"\n\n    candidates = []\n    for r in outputs:\n        txt, ftype = read_any(r)\n        if txt:\n            candidates.append((r, txt, ftype))\n\n    if not candidates:\n        return 0.0, \"Unable to extract text from any file.\"\n\n    found_change_control = False\n    found_change_control_pdf = False\n    found_risk_assessment = False\n    found_risk_assessment_docx = False\n    found_qa_email = False\n    found_internal_note = False\n\n    feedback_bits = []\n\n    for r, txt, ftype in candidates:\n        ltxt = txt.lower()\n        # Change Control heuristics\n        if (('change control' in ltxt) and (('discrepancy' in ltxt) or ('affected' in ltxt) or ('temporary control' in ltxt) or ('follow-up' in ltxt) or ('approvals' in ltxt) or ('risk' in ltxt))):\n            found_change_control = True\n            if ftype == 'pdf':\n                found_change_control_pdf = True\n        # Risk Assessment heuristics\n        if (('risk assessment' in ltxt) or ('risk mitigation' in ltxt)) and (('compecello' in ltxt) or ('compcello' in ltxt) or ('qy-gel' in ltxt) or ('antifoam' in ltxt) or ('rms-3333' in ltxt)):\n            found_risk_assessment = True\n            if ftype == 'docx':\n                found_risk_assessment_docx = True\n        # QA Email heuristics\n        if (('subject:' in ltxt) or ('to:' in ltxt)) and (('qa' in ltxt) or ('quality' in ltxt)) and (('deviation' in ltxt) or ('requalification' in ltxt)):\n            if ('rms-3333' in ltxt) and (('compcello' in ltxt) or ('qy-gel' in ltxt) or ('antifoam' in ltxt)):\n                found_qa_email = True\n        # Internal Summary Note heuristics\n        if (('internal summary' in ltxt) or ('summary' in ltxt)) and (('status' in ltxt) or ('actions taken' in ltxt) or ('next steps' in ltxt)):\n            found_internal_note = True\n\n    score = 0.0\n    if found_change_control:\n        score += 0.2\n        if found_change_control_pdf:\n            score += 0.1\n    if found_risk_assessment:\n        score += 0.2\n        if found_risk_assessment_docx:\n            score += 0.2\n    if found_qa_email:\n        score += 0.15\n    if found_internal_note:\n        score += 0.15\n\n    score = min(score, 0.8)\n\n    feedback_bits.append(f\"Change Control: {'OK (PDF)' if found_change_control_pdf else ('OK (not PDF)' if found_change_control else 'MISSING')}\")\n    feedback_bits.append(f\"Risk Assessment: {'OK (DOCX)' if found_risk_assessment_docx else ('OK (not DOCX)' if found_risk_assessment else 'MISSING')}\")\n    feedback_bits.append(f\"QA Email: {'OK' if found_qa_email else 'MISSING'}\")\n    feedback_bits.append(f\"Internal Note: {'OK' if found_internal_note else 'MISSING'}\")\n\n    return score, \"; \".join(feedback_bits)"}, {"type": "code", "name": "Discrepancy Details Present Across Docs", "description": "Verify essential identifiers and mismatch details appear across the package.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs() or []\n    if not outputs:\n        return 0.0\n\n    def read_any(res):\n        for reader in (context.files.read_pdf_text, context.files.read_docx_text, context.files.read_text):\n            try:\n                t = reader(res.id)\n                if isinstance(t, str) and len(t.strip()) > 0:\n                    return t\n            except Exception:\n                continue\n        return \"\"\n\n    corpus = (\" \".join([read_any(r) for r in outputs])).lower()\n    if not corpus.strip():\n        return 0.0, \"No readable text.\"\n\n    checks = {\n        'RMS-3333': 'rms-3333' in corpus,\n        'Vendor (CompCello)': ('compcello' in corpus),\n        'Material (QY-GEL Antifoam)': (('qy-gel' in corpus) or ('qy gel' in corpus)) and ('antifoam' in corpus),\n        'Analyte (Endotoxin)': ('endotoxin' in corpus),\n        'Spec (< 1 EU/ml)': bool(re.search(r\"(<\\s*1\\s*eu\\s*/\\s*m[l|L])|(less than 1\\s*eu\\s*/\\s*ml)\", corpus)),\n        'COA phrasing (Report Result/only)': ('report result' in corpus) or ('report only' in corpus)\n    }\n\n    # Score proportionally\n    true_count = sum(1 for v in checks.values() if v)\n    score = (true_count / len(checks)) * 0.8\n\n    missing = [k for k, v in checks.items() if not v]\n    fb = f\"Found {true_count}/{len(checks)} key discrepancy elements. Missing: {', '.join(missing) if missing else 'None'}.\"\n    return score, fb"}, {"type": "code", "name": "Risk Mitigation Specificity (Centralized comms, SOP updates)", "description": "Check the Risk Assessment document mentions core mitigation levers: centralized vendor communication tracking and SOP/process updates; plus acknowledgment of change notification and disposition path (deviation vs requalification).", "weight": 0.9, "code": "import re\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs() or []\n    if not outputs:\n        return 0.0\n\n    def read_docx_only(res):\n        try:\n            t = context.files.read_docx_text(res.id)\n            if isinstance(t, str) and len(t.strip()) > 0:\n                return t\n        except Exception:\n            return \"\"\n        return \"\"\n\n    def read_any(res):\n        for reader in (context.files.read_pdf_text, context.files.read_docx_text, context.files.read_text):\n            try:\n                t = reader(res.id)\n                if isinstance(t, str) and len(t.strip()) > 0:\n                    return t\n            except Exception:\n                continue\n        return \"\"\n\n    # Prefer the Risk Assessment docx if available; else search all\n    docx_texts = [read_docx_only(r) for r in outputs]\n    docx_texts = [t for t in docx_texts if t]\n    search_space = (\" \".join(docx_texts) if docx_texts else \" \".join([read_any(r) for r in outputs])).lower()\n    if not search_space.strip():\n        return 0.0\n\n    flags = {\n        'Centralized vendor comms': any(k in search_space for k in ['centralized vendor', 'centralised vendor', 'centralized communication', 'centralised communication', 'shared mailbox', 'central mailbox', 'ticketing system', 'vendor portal', 'central registry']),\n        'SOP/process updates': ('sop' in search_space) or ('standard operating procedure' in search_space) or ('procedure update' in search_space) or ('workflow update' in search_space),\n        'Change notification acknowledgment': ('change notification' in search_space) or ('vendor notification' in search_space),\n        'Disposition path (deviation/requalification)': ('deviation' in search_space) or ('requalification' in search_space) or ('re-qualification' in search_space)\n    }\n\n    true_count = sum(1 for v in flags.values() if v)\n    score = (true_count / len(flags)) * 0.9\n    missing = [k for k, v in flags.items() if not v]\n    return score, f\"Mitigation elements present {true_count}/{len(flags)}. Missing: {', '.join(missing) if missing else 'None'}.\""}, {"type": "llm_judge", "name": "Discrepancy Accuracy and GMP Pathway", "description": "Confirm the materials accurately describe the exact mismatch (RMS <1 EU/ml vs COA report-only) and that the non-conformance/change control pathway is correctly stated.", "weight": 2.8, "judge_prompt": "Review all files. Judge whether the submission correctly captures the specific discrepancy and immediate GMP pathway:\n- Accurately states that RMS-3333 specifies Endotoxin < 1 EU/ml while the vendor COA states Report Result (report-only, no pass/fail spec)\n- Identifies that this causes a non-conformance requiring hold/quarantine and formal change control\n- References vendor (CompCello) and material (QY-GEL Antifoam) correctly\n\nScoring (2.8 max):\n- 2.8: All elements clearly and accurately described across the package\n- 1.6: Mostly correct but one element unclear/missing\n- 0.8: Vague or partially incorrect; key detail missing\n- 0.0: Incorrect mismatch or no clear GMP pathway described", "expectation": "Clear articulation of the RMS vs COA mismatch, immediate hold, and initiation of change control with correct identifiers."}, {"type": "llm_judge", "name": "Change Control Completeness (Controls, Impact, Resolution)", "description": "Assess whether the change control request includes the essential fields and proposes an appropriate path forward.", "weight": 2.8, "judge_prompt": "Focus on the Change Control Request PDF. Evaluate whether it:\n- Describes the discrepancy and impact on documentation/workflows (explicit RMS-3333 and QMS references)\n- Identifies temporary controls (e.g., quarantine of material lot) and traceability of the lot if mentioned\n- Proposes a resolution path (seek deviation for this lot vs requalification) and follow-up actions (e.g., RMS update to align with supplier change or add acceptance criteria)\n- Includes a basic risk assessment statement (likelihood/severity or narrative) and approvals section placeholders\n\nScoring (2.8 max):\n- 2.8: All elements present and coherent\n- 1.6: One significant element missing or weak\n- 0.8: Two+ elements missing or very thin\n- 0.0: Not addressed", "expectation": "A filled change control form with controls, impact, and a proposed resolution plus follow-ups."}, {"type": "llm_judge", "name": "QA Escalation Email \u2014 Ask and References", "description": "Evaluate whether the email is professionally structured and asks QA the correct decision question while referencing the right artifacts.", "weight": 2.0, "judge_prompt": "Review the QA escalation email document. Check:\n- Presence of To/CC/Subject and a clear, professional body\n- Correct references: RMS-3333, CompCello, QY-GEL Antifoam, the COA report-only change\n- Explicit question: Can QA accept this lot under a deviation, or is full requalification required?\n- Mentions the attached Change Control Request\n\nScoring (2.0 max):\n- 2.0: All present; clear and direct\n- 1.2: Minor omissions or unclear phrasing\n- 0.6: Significant omissions\n- 0.0: Not a proper escalation email", "expectation": "A clear escalation that helps QA decide deviation vs requalification and references the change control."}, {"type": "llm_judge", "name": "Internal Summary Note \u2014 Status and Actions", "description": "Assess if the internal note succinctly informs the team and supports tracking.", "weight": 1.9, "judge_prompt": "Check the internal team summary note. It should include:\n- Brief issue summary\n- Actions taken so far (e.g., hold/quarantine, change control initiated, QA notified)\n- Current status and next steps with owners/timelines if available\n\nScoring (1.9 max):\n- 1.9: Clear and complete\n- 1.1: Minor gaps\n- 0.6: Important elements missing\n- 0.0: Not an actionable summary", "expectation": "A concise update suitable for MS Teams or project tracking entries."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Compliance Alignment", "description": "Holistic LLM evaluation of tone, actionability, compliance alignment, and clarity across artifacts.", "is_required": false, "max_points": 9.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Tone and Compliance Alignment", "description": "Evaluate whether all artifacts are professional, GMP-aware, and align with quality system expectations.", "weight": 2.5, "judge_prompt": "Across all documents, assess:\n- Professional, respectful tone appropriate for QA leadership and internal teams\n- Proper GMP/quality vocabulary (e.g., deviation, change control, qualification, RMS, QMS)\n- Avoids overcommitting; defers final acceptance to QA/Quality where appropriate\n\nScoring (2.5 max):\n- 2.5: Strong professional tone and compliance-aware language\n- 1.5: Generally professional; minor slips\n- 0.8: Mixed tone or weak compliance awareness\n- 0.0: Unprofessional or non-compliant tone", "expectation": "Consistent professional tone that respects Quality decision-making authority."}, {"type": "llm_judge", "name": "Actionability and Specificity of Mitigation Plan", "description": "Judge whether risk mitigations are concrete, owned, and time-bound.", "weight": 2.5, "judge_prompt": "In the Risk Assessment (DOCX) and change control, evaluate:\n- Specific mitigations: centralized vendor comm tracking (e.g., shared mailbox/ticketing), SOP updates, training, periodic vendor change log review\n- Ownership and timelines/milestones (even if tentative)\n- Clear linkage to the identified root cause (missed vendor change notification)\n\nScoring (2.5 max):\n- 2.5: Specific actions, owners, and timing clearly stated\n- 1.5: Actions listed but vague on owners/timing\n- 0.8: High-level only; minimal specifics\n- 0.0: No meaningful mitigation", "expectation": "Concrete, implementable mitigations tied to the communication failure."}, {"type": "llm_judge", "name": "Clarity, Structure, and Readability", "description": "Assess clarity and document hygiene for all artifacts.", "weight": 2.0, "judge_prompt": "Evaluate whether the documents are easy to read and well-structured:\n- Clear headings, bullets, concise sentences\n- Minimal redundancy and no contradictions across files\n- Correct spelling/grammar sufficient for professional use\n\nScoring (2.0 max):\n- 2.0: Clear, well-structured, minimal edits needed\n- 1.2: Generally clear with minor edits needed\n- 0.6: Several confusing areas or errors\n- 0.0: Poorly structured or confusing", "expectation": "Submission requires minimal cleanup to be sent to stakeholders."}, {"type": "llm_judge", "name": "Traceability and Cross-Referencing", "description": "Check that artifacts reference each other and key identifiers for traceability.", "weight": 2.0, "judge_prompt": "Across the package, verify:\n- References to RMS-3333, vendor CompCello, material QY-GEL Antifoam\n- The email references the attached Change Control Request; the internal note references both the hold and the change control\n- If a change control ID is included, it is reused consistently across documents\n\nScoring (2.0 max):\n- 2.0: Strong, consistent cross-references and traceability\n- 1.2: Minor inconsistencies\n- 0.6: Weak linkage\n- 0.0: No cross-referencing", "expectation": "A cohesive set of documents that can be filed and audited with clear traceability."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "6dcae3f5-bf1c-48e0-8b4b-23e6486a934c", "rubric": {"category_name": "Residency Key Indicators Benchmarking and Early-Flag System", "rationale": "This is a mixed analytical + document task. The rubric enforces a self-documenting Excel workbook shape that makes verification trivial, then verifies correctness of calculations/flags and cross-references the email summary. Stage 1 is an LLM-only gate defining the exact workbook/document structure. Stage 2 uses a mix of lightweight code checks (bounds/structure consistency) and heavier LLM judges for nuanced validation. Stage 3 assesses professional quality, clarity, and actionability for the Program Director.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (STRUCTURE ONLY)", "description": "LLM-only gate that checks the candidate produced BOTH: (1) an Excel workbook with the required sheets/sections/tables and (2) a professional email in DOCX/PDF. No calculation correctness here\u2014only the presence and structure that enables verification.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Excel Workbook Structure Requirement", "description": "Check the Excel is present and has all required sheets/sections to enable verification, with flexible matching for names.", "weight": 2.0, "judge_prompt": "You are validating the STRUCTURE ONLY of an Excel workbook intended to benchmark key indicators for otolaryngology residents and flag residents below 2 SD.\n\nCheck the candidate outputs and locate a single Excel file (XLSX). If there are multiple, use the most relevant one (look for a name like \"Chief Key Indicator 5-Year\"). Then check for the following structure (be flexible with sheet names; small variations are acceptable):\n\nREQUIRED SHEETS (flexible naming allowed):\n1) \"Compilation (PGY5 2021-2025)\" or similar name containing \"Compilation\" or \"PGY5\" or \"2021-2025\":\n   - Contains TWO clearly labeled tables:\n     a) \"Key Indicator Yearly Counts\" table (long format) with column headers equivalent to: [Resident | Year | Key Indicator | Count]\n     b) \"Totals Yearly\" table with columns equivalent to: [Resident | Year | Total Key Indicators | Total Cases]\n\n2) \"Benchmarks by PGY\" or similar containing \"Benchmark\":\n   - Two sections/tables clearly labeled:\n     a) \"Benchmark Averages\" with columns like: [PGY | Key Indicator | Mean/Avg]\n     b) \"Benchmark Standard Deviations\" with columns like: [PGY | Key Indicator | SD/StdDev]\n   - The tables must include rows for: PGY-1 through PGY-5 (flexible: PGY1/PGY-1) and must include entries for both \"Total Key Indicators\" and \"Total Cases\" in addition to individual key indicators.\n\n3) \"ACGME Requirements Met\" or similar containing \"Requirements\":\n   - A table with columns equivalent to: [Resident | Key Indicator | Requirement Number | PGY Met]\n\n4) \"Flags (Below -2 SD)\" or similar containing \"Flags\" or \"At-Risk\":\n   - A table with columns roughly: [Resident | PGY | Key Indicator | Value/Observed | Mean | SD | Z-score | Flag]\n   - The sheet should also contain either: (i) pasted rows from the source dataset for each flagged resident OR (ii) a clear listing of all flagged metrics by resident and PGY. Cells corresponding to below-2SD metrics should be highlighted in red (conditional formatting or manual highlighting). If no residents are flagged, the sheet must explicitly indicate that none are below -2 SD and still show the table headers.\n\n5) \"Methodology\" (OPTIONAL but recommended):\n   - Brief text explaining data source (Key Indicators.xlsx), how averages/SDs were computed, and the -2 SD flagging logic.\n\nSTRUCTURE SCORING (structure only):\n- 2.0: Excel present with all 4 required sheets (Compilation, Benchmarks, Requirements, Flags) structured as above; optional Methodology present or not.\n- 1.5: Excel present; 1 of the required sheets/sections is missing or not clearly labeled, but the rest are present.\n- 1.0: Excel present; 2 required sheets/sections missing.\n- 0.5: Excel present; only 1 required sheet found with partial tables.\n- 0.0: No Excel file or completely wrong format/structure.\n\nDo NOT judge correctness of numbers\u2014only whether the structure exists to enable verification.", "expectation": "A single .xlsx file containing the four required sheets with the described tables/sections. Optional Methodology is a plus."}, {"type": "llm_judge", "name": "Email Document Structure Requirement", "description": "Check a DOCX or PDF email draft exists with required elements and cross-reference intent.", "weight": 2.0, "judge_prompt": "Locate a Word (DOCX) or PDF file that is clearly a drafted email to the Program Director, Dr. Smith. STRUCTURE check only:\n\nREQUIREMENTS:\n- File type is DOCX or PDF (not plain text).\n- Addressed to Dr. Smith (Program Director) in opening.\n- States the task is completed and references the workbook (e.g., \"Chief Key Indicator 5-Year\").\n- Summarizes the number of residents, their PGY(s), names, and the specific metrics (key indicators, total key indicators, total cases) that are 2 or more SD below the mean; OR explicitly states that no residents were flagged if none.\n- Professional sign-off from \"Residency Program Coordinator\".\n- At least one short paragraph (3+ sentences) and readable formatting.\n\nSCORING:\n- 2.0: Meets all bullets clearly.\n- 1.5: Minor omission (e.g., missing PGY detail or workbook reference) but overall complete.\n- 1.0: Multiple omissions; still a recognizable email with addressee, completion statement, and sign-off.\n- 0.0: No DOCX/PDF email or grossly wrong format.\n\nDo NOT judge content accuracy against the Excel in this stage, only presence/structure.", "expectation": "A professional DOCX or PDF email to Dr. Smith with completion notice, flagged summary or explicit none, and proper sign-off."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness and Consistency)", "description": "Now that the structure exists, verify that calculations, thresholds, and cross-references are correct and consistent. Includes both code-based validations and LLM spot checks. Code rules have lower weight; LLM rules carry most of the verification weight.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Compilation Sheet Year Range Validity", "description": "Verify the Excel has a compilation/PGY5 sheet including a Year column with values in 2021\u20132025, and resident/key indicator columns present in some form.", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    # Find a spreadsheet among outputs\n    outputs = context.get_all_outputs()\n    xls = None\n    for r in outputs:\n        if getattr(r, 'is_spreadsheet', False):\n            xls = r\n            break\n    if not xls:\n        return 0.0, 'No spreadsheet output found'\n\n    fp = context.files.get_path(xls.id)\n    try:\n        x = pd.ExcelFile(fp)\n    except Exception as e:\n        return 0.0, f'Could not open Excel: {e}'\n\n    # Find a sheet likely to be the compilation sheet\n    target_sheet = None\n    for s in x.sheet_names:\n        sl = s.lower()\n        if ('compil' in sl) or ('pgy5' in sl) or ('2021' in sl) or ('2025' in sl):\n            target_sheet = s\n            break\n    if target_sheet is None and x.sheet_names:\n        target_sheet = x.sheet_names[0]\n\n    # Try headered read first\n    try:\n        df = pd.read_excel(fp, sheet_name=target_sheet)\n    except Exception:\n        return 0.0, 'Failed to read compilation-like sheet'\n\n    score_parts = []\n\n    # Check Year column values within 2021-2025\n    year_ok = 0\n    year_cols = [c for c in df.columns if isinstance(c, str) and 'year' in c.lower()]\n    if year_cols:\n        years = pd.to_numeric(df[year_cols[0]], errors='coerce')\n        valid = years.dropna().astype(int)\n        if not valid.empty:\n            # Consider success if at least half of non-NA years are within 2021-2025\n            in_range = ((valid >= 2021) & (valid <= 2025)).mean()\n            if in_range >= 0.5:\n                year_ok = 1\n    score_parts.append(year_ok)\n\n    # Check presence of resident/name and key indicator columns (fuzzy)\n    cols_lower = [str(c).lower() for c in df.columns]\n    name_present = int(any(('resident' in c) or ('name' in c) for c in cols_lower))\n    ki_present = int(any(('key indicator' in c) or ('indicator' in c) for c in cols_lower))\n    score_parts.append(name_present)\n    score_parts.append(ki_present)\n\n    return sum(score_parts)/3.0, f\"Checks passed: {sum(score_parts)}/3\""}, {"type": "code", "name": "Benchmarks by PGY Present and Numeric", "description": "Verify a Benchmarks sheet exists with PGY 1\u20135 and numeric mean/SD columns, including rows for Total Key Indicators and Total Cases.", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    xls = None\n    for r in outputs:\n        if getattr(r, 'is_spreadsheet', False):\n            xls = r\n            break\n    if not xls:\n        return 0.0, 'No spreadsheet found'\n\n    fp = context.files.get_path(xls.id)\n    try:\n        x = pd.ExcelFile(fp)\n    except Exception as e:\n        return 0.0, f'Cannot open Excel: {e}'\n\n    bench_sheet = None\n    for s in x.sheet_names:\n        if 'bench' in s.lower():\n            bench_sheet = s\n            break\n    if bench_sheet is None:\n        return 0.0, 'No Benchmarks sheet'\n\n    df = pd.read_excel(fp, sheet_name=bench_sheet)\n    cols_lower = [str(c).lower() for c in df.columns]\n\n    # Identify PGY column\n    pgy_cols = [c for c in df.columns if isinstance(c, str) and 'pgy' in c.lower()]\n    pgy_ok = 0\n    if pgy_cols:\n        pgy_vals = df[pgy_cols[0]].astype(str).str.extract(r'(\\d)').dropna()[0].astype(int)\n        if not pgy_vals.empty:\n            # Require presence of values 1..5 (not necessarily all in same rows)\n            present = set(pgy_vals.unique().tolist())\n            if {1,2,3,4,5}.issubset(present):\n                pgy_ok = 1\n\n    # Mean and SD columns numeric\n    mean_ok = int(any(('mean' in c) or ('avg' in c) or ('average' in c) for c in cols_lower))\n    sd_ok = int(any(('sd' in c) or ('std' in c) or ('stdev' in c) for c in cols_lower))\n\n    # Check presence of rows for total metrics (fuzzy match in any text col)\n    total_ok = 0\n    text_df = df.applymap(lambda v: str(v).lower() if pd.notna(v) else '')\n    if ((text_df.apply(lambda col: col.str.contains('total key')).any(axis=0)).any() or\n        (text_df.apply(lambda col: col.str.contains('total cases')).any(axis=0)).any()):\n        total_ok = 1\n\n    parts = [pgy_ok, mean_ok, sd_ok, total_ok]\n    return sum(parts)/4.0, f\"Bench checks passed: {sum(parts)}/4\""}, {"type": "code", "name": "Flags Z-Score Consistency", "description": "If any flagged rows exist, verify that any flagged row has value <= mean - 2*sd and/or z <= -2. If no flagged rows, award full credit.", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    xls = None\n    for r in outputs:\n        if getattr(r, 'is_spreadsheet', False):\n            xls = r\n            break\n    if not xls:\n        return 0.0, 'No spreadsheet found'\n\n    fp = context.files.get_path(xls.id)\n    try:\n        x = pd.ExcelFile(fp)\n    except Exception as e:\n        return 0.0, f'Cannot open Excel: {e}'\n\n    flag_sheet = None\n    for s in x.sheet_names:\n        sl = s.lower()\n        if ('flag' in sl) or ('risk' in sl):\n            flag_sheet = s\n            break\n    if flag_sheet is None:\n        return 0.0, 'Flags sheet missing'\n\n    df = pd.read_excel(fp, sheet_name=flag_sheet)\n    if df.empty:\n        # If sheet exists but empty, treat as no flags and headers missing\n        return 0.5, 'Flags sheet empty; partial credit'\n\n    cols = [str(c).lower() for c in df.columns]\n    # Identify columns\n    val_col = None\n    for c in df.columns:\n        cl = str(c).lower()\n        if any(k in cl for k in ['value','observed','count','actual']):\n            val_col = c; break\n    mean_col = None\n    for c in df.columns:\n        if 'mean' in str(c).lower() or 'avg' in str(c).lower():\n            mean_col = c; break\n    sd_col = None\n    for c in df.columns:\n        if any(k in str(c).lower() for k in ['sd','std','stdev']):\n            sd_col = c; break\n    z_col = None\n    for c in df.columns:\n        if 'z' in str(c).lower():\n            z_col = c; break\n    flag_col = None\n    for c in df.columns:\n        if 'flag' in str(c).lower():\n            flag_col = c; break\n\n    # If critical cols missing, partial at best\n    if mean_col is None or sd_col is None:\n        return 0.3, 'Missing mean/sd columns'\n\n    # Determine flagged rows by either z<=-2 or explicit flag\n    flagged_idx = []\n    if z_col is not None:\n        z = pd.to_numeric(df[z_col], errors='coerce')\n        flagged_idx = list(df.index[(z <= -2).fillna(False)])\n    if not flagged_idx and flag_col is not None:\n        flags = df[flag_col].astype(str).str.lower().isin(['true','yes','1','y','flag','below','below 2 sd','at risk'])\n        flagged_idx = list(df.index[flags])\n\n    if not flagged_idx:\n        # No flags is acceptable\n        return 1.0, 'No flagged rows; full credit'\n\n    # Consistency check\n    passed = 0; total = 0\n    vals = pd.to_numeric(df[val_col], errors='coerce') if val_col is not None else None\n    means = pd.to_numeric(df[mean_col], errors='coerce')\n    sds = pd.to_numeric(df[sd_col], errors='coerce')\n\n    for i in flagged_idx:\n        m = means.iloc[i]\n        s = sds.iloc[i]\n        v = vals.iloc[i] if vals is not None else np.nan\n        z_ok = False\n        if z_col is not None:\n            z = pd.to_numeric(df[z_col], errors='coerce').iloc[i]\n            if pd.notna(z) and z <= -2:\n                z_ok = True\n        thresh_ok = False\n        if pd.notna(m) and pd.notna(s) and pd.notna(v):\n            thresh_ok = v <= (m - 2*s + 1e-9)\n        if z_ok or thresh_ok:\n            passed += 1\n        total += 1\n\n    if total == 0:\n        return 0.7, 'Flags present but insufficient numeric data'\n    return passed/total, f'Flag consistency {passed}/{total}'"}, {"type": "code", "name": "Requirements Sheet Validity", "description": "Verify the ACGME Requirements Met sheet has expected columns and sane values for requirement numbers and PGY Met (1\u20135).", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    xls = None\n    for r in outputs:\n        if getattr(r, 'is_spreadsheet', False):\n            xls = r\n            break\n    if not xls:\n        return 0.0, 'No spreadsheet found'\n\n    fp = context.files.get_path(xls.id)\n    try:\n        x = pd.ExcelFile(fp)\n    except Exception as e:\n        return 0.0, f'Cannot open Excel: {e}'\n\n    req_sheet = None\n    for s in x.sheet_names:\n        if 'require' in s.lower():\n            req_sheet = s\n            break\n    if req_sheet is None:\n        return 0.0, 'Requirements sheet missing'\n\n    df = pd.read_excel(fp, sheet_name=req_sheet)\n    if df.empty:\n        return 0.2, 'Requirements sheet empty'\n\n    cols = [str(c).lower() for c in df.columns]\n    name_ok = int(any(('resident' in c) or ('name' in c) for c in cols))\n    ki_ok = int(any(('key indicator' in c) or ('indicator' in c) for c in cols))\n\n    # Requirement number column\n    req_col = None\n    for c in df.columns:\n        if 'require' in str(c).lower():\n            req_col = c; break\n    pgy_col = None\n    for c in df.columns:\n        if 'pgy' in str(c).lower() and ('met' in str(c).lower() or 'year' in str(c).lower()):\n            pgy_col = c; break\n\n    valid_req = 0\n    valid_pgy = 0\n    if req_col is not None:\n        reqs = pd.to_numeric(df[req_col], errors='coerce')\n        if reqs.notna().any():\n            # At least 80% positive\n            valid_req = int((reqs.dropna() > 0).mean() >= 0.8)\n    if pgy_col is not None:\n        pgy_vals = pd.to_numeric(df[pgy_col].astype(str).str.extract(r'(\\d+)')[0], errors='coerce')\n        if pgy_vals.notna().any():\n            valid_pgy = int(((pgy_vals >= 1) & (pgy_vals <= 5)).mean() >= 0.8)\n\n    parts = [name_ok, ki_ok, int(req_col is not None), int(pgy_col is not None), valid_req, valid_pgy]\n    return sum(parts)/6.0, f\"Requirements checks passed: {sum(parts)}/6\""}, {"type": "llm_judge", "name": "Benchmark Logic and Coverage (LLM)", "description": "LLM verifies that the benchmark means and SDs are computed year-over-year from the PGY-5 cohort across 2021\u20132025, mapped to PGY1\u2013PGY5, and include totals.", "weight": 2.5, "judge_prompt": "Open the Excel and examine the Benchmarks sheet and Compilation sheet. Verify:\n- Means and SDs appear to be computed from the PGY-5 residents' 5-year histories (2021\u20132025) and mapped to PGY-1 through PGY-5.\n- Benchmarks include both individual key indicators AND the two aggregate metrics: Total Key Indicators and Total Cases.\n- Coverage is complete: for each PGY (1\u20135), there are benchmark averages and SDs for multiple indicators and both totals.\nScoring:\n- 2.5: Clear, complete, and correctly scoped benchmarks as described.\n- 1.5: Mostly correct with minor gaps (e.g., one PGY missing an SD or a missing total row).\n- 0.8: Partial coverage; structure present but multiple gaps.\n- 0.0: Benchmarks not actually computed as described or obviously mismapped.", "expectation": "A coherent benchmark table set with PGY1\u20135 rows and both averages and SDs for all metrics, derived from the PGY-5 cohort years."}, {"type": "llm_judge", "name": "Flags Correctness Spot-Check (LLM)", "description": "LLM spot-checks a few flagged rows to verify the -2 SD rule is applied correctly using provided Mean/SD/Value/Z.", "weight": 2.0, "judge_prompt": "Review the Flags sheet. For up to 2 flagged entries (if any), recompute in your head whether Value <= Mean - 2*SD (or Z <= -2). If the sheet provides Z-scores, check them too. Also check that the flagged cells in the pasted row are highlighted in red. Scoring:\n- 2.0: Sampled entries comply with the -2 SD criterion; highlights present; no obvious misflags.\n- 1.2: Minor arithmetic discrepancy or highlight inconsistency but overall rule applied.\n- 0.6: Multiple discrepancies; application of -2 SD rule questionable.\n- 0.0: Flags do not reflect the -2 SD rule at all.\nIf no residents are flagged, score 2.0 if the sheet explicitly states none and the logic is still visible (headers, formulas/notes).", "expectation": "Flags sheet correctly implements -2 SD rule and uses clear visual highlighting for below-threshold metrics."}, {"type": "llm_judge", "name": "Requirements Completion Year Coherence (LLM)", "description": "LLM checks that the PGY Met year per resident/indicator is consistent with cumulative counts from Compilation and stated requirement numbers.", "weight": 1.5, "judge_prompt": "Open the Requirements sheet and Compilation sheet. Choose one or two residents and indicators. Using the yearly counts from the Compilation sheet, cumulatively sum by year (PGY1..PGY5) and identify the first PGY where the cumulative count meets/exceeds the Requirement Number. Compare with the \"PGY Met\" provided. Scoring:\n- 1.5: Consistent for sampled cases; no obvious contradictions.\n- 0.9: Mostly consistent with a minor discrepancy.\n- 0.4: Multiple inconsistencies or unclear mapping.\n- 0.0: \"PGY Met\" appears arbitrary or contradicted by the data.", "expectation": "The PGY Met year is the first year where cumulative counts reach or exceed the requirement for that indicator."}, {"type": "llm_judge", "name": "Email Cross-Reference with Flags (LLM)", "description": "LLM verifies that the email summary accurately reflects the Flags sheet (names, PGY, metrics, count of residents) or explicitly states no flags.", "weight": 3.0, "judge_prompt": "Compare the email document to the Excel Flags sheet:\n- If there are flagged residents, the email should list the correct number of residents, their names, PGY(s), and which metric(s) are below 2 SD. Spot-check a couple of items for exact match.\n- If there are no flagged residents, the email should clearly state that none were below 2 SD.\nScoring:\n- 3.0: Accurate, complete cross-reference.\n- 2.0: Minor omissions (e.g., missing PGY for one resident) but overall correct.\n- 1.0: Multiple inaccuracies or vague summary.\n- 0.0: Email contradicts the Excel or lacks required summary.", "expectation": "The email faithfully summarizes the Flags sheet or clearly reports no flags."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism", "description": "Holistic assessment of presentation quality, clarity, and usefulness to the Program Director. LLM-only rules.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Spreadsheet Professionalism and Usability", "description": "Assesses clarity of sheet names, headers, formatting, conditional highlighting, and general usability.", "weight": 1.5, "judge_prompt": "Evaluate the Excel workbook for professional presentation:\n- Clear, descriptive sheet names and table headers\n- Readable formatting (aligned columns, number formats, filters, freeze panes if visible)\n- Red highlighting clearly calls attention to below-2 SD metrics\n- Minimal clutter and easy navigation\nScoring: 1.5 excellent; 1.0 good with minor issues; 0.5 rough but usable; 0.0 poor/unusable.", "expectation": "A clean, well-formatted workbook with clear highlights and easy navigation."}, {"type": "llm_judge", "name": "Documentation and Reproducibility", "description": "Checks presence and clarity of methodology, data source notes, and assumptions to allow future updates.", "weight": 1.5, "judge_prompt": "Check for a Methodology/Notes section or equivalent:\n- Does it explain the data source (Key Indicators.xlsx) and the ACGME PDF link?\n- Are calculation methods (average, SD, -2 SD rule) described?\n- Could another coordinator update the file next year based on these notes?\nScoring: 1.5 strong; 1.0 adequate; 0.5 minimal; 0.0 absent.", "expectation": "Concise notes enabling another person to replicate/update the analysis."}, {"type": "llm_judge", "name": "Actionability for PD", "description": "Assesses whether the outputs provide clear, actionable insight for early intervention.", "weight": 1.5, "judge_prompt": "From the workbook and email, assess actionability:\n- Are at-risk residents easy to identify and understand why they are at risk?\n- Are PGY-by-PGY benchmarks clear?\n- Is there a concise summary suitable for PD decision-making?\nScoring: 1.5 highly actionable; 1.0 somewhat; 0.5 limited; 0.0 not actionable.", "expectation": "Clear identification of at-risk residents and benchmarks that inform interventions."}, {"type": "llm_judge", "name": "Email Tone and Professional Completeness", "description": "Assesses the professionalism, clarity, and completeness of the drafted email to Dr. Smith.", "weight": 1.5, "judge_prompt": "Evaluate the drafted email:\n- Professional tone and concise explanation\n- Includes all required elements (completion notice, summary of flags or none, sign-off)\n- Free of major grammatical issues\nScoring: 1.5 excellent; 1.0 good; 0.5 fair; 0.0 poor.", "expectation": "A professional, concise email appropriate for a Program Director."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "664a42e5-3240-413a-9a57-ea93c6303269", "rubric": {"category_name": "ILIT Strategy Presentation (Finance & Insurance \u2014 Personal Financial Advisors)", "rationale": "Pattern B (Document task). The rubric enforces a presentation-style PDF/DOCX deck with a precise, verifiable slide structure (Stage 1 gate). Stage 2 mixes lightweight code checks (text parsing for key terms, numeric plausibility) with richer LLM verification of correctness and cross-linking. Stage 3 LLM judges assess professional quality, audience fit, and actionability. Code rules are limited in weight relative to LLM rules to reflect nuanced judgment needs.", "max_total_score": 22.0, "stages": [{"name": "Stage 1 \u2014 Deck Shape Enforcement (GATE)", "description": "Validates the output is a presentation-style PDF/DOCX deck with the exact structure needed for verification.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.0, "rules": [{"type": "llm_judge", "name": "Deck Structure and Required Slides Gate", "description": "Output must be a PDF/DOCX presentation deck with the specific slide structure enabling verification.", "weight": 6.0, "judge_prompt": "You are checking ONLY format and structural completeness (not content quality or correctness).\n\nAcceptable formats: PDF or DOCX (presentation-style deck). Reject any other format.\n\nRequired overall:\n- 8\u201312 slides recommended (7 minimum). Clear slide titles and bullet formatting.\n- Professional, client-facing deck style (titles + bullets per slide).\n\nRequired slides/sections (flexible naming, but each topic must be clearly identifiable):\n1) Title slide: mentions ILIT or irrevocable life insurance trust and indicates purpose/overview.\n2) Agenda/Overview slide.\n3) ILIT Basics & Parties: defines ILIT and identifies key roles (grantor, trustee, beneficiaries; Crummey powerholders may be beneficiaries).\n4) Step-by-Step Implementation: a numbered or clearly sequenced process from decision through drafting, funding, Crummey notices, premium payment, and recordkeeping.\n5) Funding & Premiums: how trust is funded, annual gift tax exclusion, how premiums get paid by trustee (gifts \u2192 notices \u2192 premium).\n6) Crummey Powers & Timeline: shows time cycle of Crummey notices/withdrawal window and explicitly references the 2025 annual gift tax exclusion amount.\n7) Policy Types in the Trust: e.g., term, whole life, UL, survivorship/second-to-die; ownership by ILIT.\n8) Death Proceeds Flow: what happens at grantor\u2019s death; proceeds to trust, liquidity, distribution plan.\n9) Key Factors/Considerations: risks, administration, costs, control, tax and legal coordination.\n10) Side-by-Side Comparison: with-ILIT vs without-ILIT (preferably a 2-column table) covering key considerations.\n11) Next Steps & Disclaimers (optional but recommended): action items and compliance/legal disclaimers.\n\nScoring (STRUCTURE ONLY):\n- 6.0: PDF/DOCX; 8\u201312 slides (\u22657); all 10 required topics present; comparison as a table; clear titles/bullets.\n- 5.0: PDF/DOCX; \u22657 slides; all topics covered but one topic is merged/brief OR comparison not in table but clearly side-by-side.\n- 4.0: PDF/DOCX; \u22657 slides; missing up to two required topics OR agenda/step-by-step/timeline weakly shown.\n- 2.0: PDF/DOCX but <7 slides OR missing 3\u20134 required topics OR structure not slide-like (e.g., long prose page).\n- 0.0: Not PDF/DOCX OR clearly not a presentation deck.\n\nOnly evaluate PRESENCE and STRUCTURE, not correctness or quality.", "expectation": "A PDF/DOCX deck with clearly titled slides covering each required topic, including a table-based comparison and a Crummey timeline referencing the 2025 exclusion."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness Verification (Mixed)", "description": "Verifies substantive correctness, coherence of process, and presence of key technical elements.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Key Parties Coverage (Text Parse)", "description": "Checks that the deck text mentions core ILIT parties and Crummey powers.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float or (float, str)\n    \"\"\"\n    weight = 0.6\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = ''\n    try:\n        if str(output.extension).lower() == '.pdf':\n            text = context.files.read_pdf_text(output.id)\n        else:\n            # assume DOCX or other document\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                # fallback for markdown or text if misclassified\n                text = context.files.read_text(output.id)\n    except Exception:\n        return 0.0\n    if not text:\n        return 0.0\n    t = text.lower()\n    terms = {\n        'grantor': any(w in t for w in ['grantor', 'settlor']),\n        'trustee': 'trustee' in t,\n        'beneficiary': any(w in t for w in ['beneficiary', 'beneficiaries']),\n        'crummey': 'crummey' in t,\n    }\n    count = sum(1 for v in terms.values() if v)\n    score = (count/4.0) * weight\n    feedback = f\"Parties mentioned: {', '.join([k for k,v in terms.items() if v])}\"\n    return score, feedback"}, {"type": "code", "name": "Annual Exclusion Amount Presence & Plausibility (2025)", "description": "Checks for an annual gift tax exclusion dollar amount near relevant terms and basic plausibility.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 0.6\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    # Extract text\n    try:\n        if str(output.extension).lower() == '.pdf':\n            text = context.files.read_pdf_text(output.id)\n        else:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                text = context.files.read_text(output.id)\n    except Exception:\n        return 0.0\n    if not text:\n        return 0.0\n    t = text\n    tl = t.lower()\n    base = 0.0\n    # Must mention annual exclusion concept\n    has_concept = ('annual' in tl and 'exclusion' in tl) or ('gift tax exclusion' in tl)\n    if has_concept:\n        base += 0.3\n    # Look for a dollar amount nearby\n    score_amt = 0.0\n    for m in re.finditer(r\\$\\s*([0-9]{2,3}(?:,[0-9]{3})?)', t):\n        amt_str = m.group(1).replace(',', '')\n        try:\n            amt = float(amt_str)\n        except:\n            continue\n        if 10000 <= amt <= 30000:\n            score_amt = 0.3\n            break\n    base += score_amt\n    # Give extra if \"2025\" appears within 100 chars of 'annual'/'exclusion'\n    extra = 0.0\n    for m in re.finditer(r'2025', t):\n        idx = m.start()\n        window = t[max(0, idx-120): idx+120].lower()\n        if 'annual' in window and 'exclusion' in window:\n            extra = 0.0  # keep neutral to avoid overweighting\n            break\n    # Cap at weight\n    score = min(weight, base)\n    feedback = f\"Annual exclusion concept: {has_concept}; plausible $ amount found: {score_amt>0}\"\n    return score, feedback"}, {"type": "code", "name": "Crummey Notice Timeline Signals", "description": "Looks for timeline mechanics: notices, days window, withdrawal/lapse concepts near 'Crummey'.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 0.4\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    # Extract text\n    try:\n        if str(output.extension).lower() == '.pdf':\n            text = context.files.read_pdf_text(output.id)\n        else:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                text = context.files.read_text(output.id)\n    except Exception:\n        return 0.0\n    if not text:\n        return 0.0\n    t = text.lower()\n    if 'crummey' not in t:\n        return 0.0\n    score = 0.0\n    # Evidence of notice window in days\n    window_hits = re.findall(r'(\\b30\\b|\\b45\\b|\\b60\\b)\\s*day', t)\n    if window_hits:\n        score += 0.2\n    # Mention of notice / withdrawal / lapse\n    if any(w in t for w in ['notice', 'withdrawal right', 'withdraw right', 'lapse']):\n        score += 0.2\n    return min(weight, score)"}, {"type": "code", "name": "Policy Types Mentioned", "description": "Checks for presence of at least two insurance policy types relevant to ILITs.", "weight": 0.4, "code": "def evaluate(workflow, context):\n    weight = 0.4\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    # Extract text\n    try:\n        if str(output.extension).lower() == '.pdf':\n            text = context.files.read_pdf_text(output.id)\n        else:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                text = context.files.read_text(output.id)\n    except Exception:\n        return 0.0\n    if not text:\n        return 0.0\n    t = text.lower()\n    types = ['term', 'whole life', 'universal life', 'indexed universal', 'variable universal', 'survivorship', 'second-to-die']\n    found = [typ for typ in types if typ in t]\n    unique = set(found)\n    n = len(unique)\n    if n == 0:\n        return 0.0, 'No policy types found'\n    elif n == 1:\n        return weight * 0.5, f\"Found 1 policy type: {', '.join(unique)}\"\n    else:\n        return weight, f\"Found {n} policy types: {', '.join(sorted(unique))}\""}, {"type": "llm_judge", "name": "Step-by-Step Implementation Flow Integrity", "description": "Process is coherent and correctly ordered from planning through trust creation, funding, notices, premium payment, and administration.", "weight": 3.0, "judge_prompt": "Evaluate whether the deck presents a coherent, correctly ordered ILIT implementation process. Look for: engaging counsel, draft & execute trust, obtain EIN and open trust account, select policy, fund gifts to trust, send Crummey notices with withdrawal window, trustee pays premiums from trust, maintain records/receipts, ongoing administration. Award more if steps are clearly numbered and dependencies are correct (e.g., trustee pays premiums only after gifts and notice). Penalize if grantor retains incidents of ownership or trustee role is misstated.", "expectation": "A numbered, logically ordered checklist/flow that matches standard ILIT administration."}, {"type": "llm_judge", "name": "Proceeds Distribution and Estate Tax Treatment", "description": "Explains what happens at death, including estate exclusion and liquidity benefits.", "weight": 2.0, "judge_prompt": "Check if the deck clearly explains the post-death flow: policy proceeds paid to ILIT, not to estate; used to provide liquidity (e.g., loans or purchases from estate), beneficiary distribution per trust terms; avoidance of estate inclusion if incidents of ownership are avoided. Deduct if the deck implies proceeds are paid into the taxable estate or omits liquidity mechanics.", "expectation": "Clear explanation that ILIT owns policy, proceeds pass to trust, commonly used for estate liquidity; not included in grantor\u2019s estate if structured correctly."}, {"type": "llm_judge", "name": "Crummey Timeline Accuracy (with 2025 Exclusion)", "description": "Validates that the timeline accurately reflects notice timing and ties to the 2025 annual exclusion.", "weight": 1.0, "judge_prompt": "Assess whether the deck shows a correct Crummey timeline: contributions to trust; notices to beneficiaries; a withdrawal window (often 30\u201360 days); after window lapses, trustee pays premium; ties the 2025 annual exclusion dollar amount to the number of beneficiaries and total annual gifts. Partial credit if timeline is present but the 2025 amount is not explicitly tied to the example.", "expectation": "A timeline graphic or bullet sequence with days window and numeric illustration using the 2025 exclusion."}, {"type": "llm_judge", "name": "Side-by-Side Comparison Completeness", "description": "Compares with-ILIT vs without-ILIT across multiple dimensions.", "weight": 1.0, "judge_prompt": "Review the comparison slide. Award more if it contrasts at least four dimensions such as: estate tax inclusion, liquidity for taxes, control/flexibility, administrative burden/costs, creditor protection, complexity/risks. Prefer a 2-column table. Deduct if it is vague, one-sided, or misses key trade-offs.", "expectation": "Balanced, multi-dimension comparison in a 2-column format."}, {"type": "llm_judge", "name": "Funding and Premium Payment Mechanics", "description": "Explains how gifts qualify for annual exclusion and how the trustee pays premiums.", "weight": 1.0, "judge_prompt": "Evaluate whether the deck correctly describes: gifts to ILIT, Crummey notices to qualify for annual exclusion, waiting period, then trustee pays premiums from trust account. Deduct if gifts/premiums flow are confused or imply donor pays insurer directly instead of trustee after notices.", "expectation": "Clear mechanics: donor gifts \u2192 notices \u2192 window \u2192 trustee pays premium from trust."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality Assessment", "description": "Assesses presentation quality, client appropriateness, strategic value, and compliance tone.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Audience-Appropriate Clarity and Tone", "description": "Clarity for HNW clients ($5\u2013$10M), plain language, minimal jargon with brief definitions.", "weight": 2.0, "judge_prompt": "Rate clarity and tone for a sophisticated but non-technical client. Look for concise bullets, defined jargon (e.g., Crummey powers), and logical flow. Deduct for dense text blocks, unexplained acronyms, or overly technical language.", "expectation": "Clear, concise bullets; brief definitions; logical flow tailored to HNW clients."}, {"type": "llm_judge", "name": "Professional Presentation Structure & Design", "description": "Structure, visual hierarchy, and slide craftsmanship.", "weight": 1.5, "judge_prompt": "Assess slide craftsmanship: consistent titles, readable bullets, white space, optional visuals (timeline/flow/table), and overall cohesion (8\u201312 slides). Deduct for clutter, inconsistent formatting, or missing key visuals like the comparison table or timeline.", "expectation": "Consistent, professional slide design with helpful visuals and readable layout."}, {"type": "llm_judge", "name": "Actionability and Next Steps", "description": "Clear next steps checklist and meeting readiness.", "weight": 1.0, "judge_prompt": "Check for actionable next steps (e.g., confirm goals, coordinate with estate attorney/CPA, draft trust, obtain EIN, open trust account, select policy, gifting schedule, administration calendar). Prefer a short checklist and a closing slide with contact/disclaimer.", "expectation": "Concrete next steps a client can follow after the meeting."}, {"type": "llm_judge", "name": "Risk, Compliance, and Disclaimers", "description": "Addresses key risks and includes appropriate disclaimers.", "weight": 1.5, "judge_prompt": "Look for balanced risk discussion (irrevocability, administration burden, gift-splitting, GST considerations for skip persons, trustee duties, state law variability) and compliance/legal/tax disclaimers advising consultation with attorney/CPA. Deduct if risks or disclaimers are missing.", "expectation": "Balanced treatment of risks and clear legal/tax disclaimers."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "575f8679-b4c1-47a2-8e96-d570d4ed9269", "rubric": {"category_name": "Govt | Child, Family, and School Social Workers \u2014 Program Evaluation Plan (Immigration and Family Stress)", "rationale": "This rubric enforces a self-documenting, file-based deliverable: a Word document structured to enable verification. Stage 1 (LLM-only) is a strict gate that checks format and required sections. Stage 2 mixes precise code checks (presence of key methods/instruments) with LLM-based correctness/consistency assessments aligned to the prompt. Stage 3 applies holistic quality criteria for clarity, feasibility, ethics, and local relevance to Northwest Kansas immigrant families.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Format and Structural Gate (DOCUMENT ONLY)", "description": "Gate: The output must be a DOCX (preferred) or PDF with the exact structural components needed for verification.", "is_required": true, "max_points": 2.0, "min_score_to_pass": 1.0, "rules": [{"type": "llm_judge", "name": "Document Format and Required Sections Present", "description": "Verify the candidate produced a Word document (DOCX preferred) with all required sections and an Appendix containing instruments/tools references or samples.", "weight": 2.0, "judge_prompt": "You are evaluating whether the submission satisfies the STRUCTURE requirements for this assignment. Only assess PRESENCE/FORMAT, not content quality.\n\nACCEPTABLE FORMATS:\n- DOCX (preferred) or PDF. If plain text/markdown/spreadsheet, score 0.\n\nMINIMUM LENGTH:\n- At least 2 pages (for PDF) or clearly more than ~600 words (for DOCX text). Be lenient but enforce that it\u2019s more than a brief memo.\n\nREQUIRED SECTIONS (flexible with naming but headers must be clearly visible):\n1) Program Overview or Introduction \u2014 describes program goals and target population (immigrant families in Northwest Kansas).\n2) Evaluation Framework \u2014 explicitly distinguishes Formative evaluation and Summative evaluation (sub-headings or clearly separated paragraphs).\n3) Data Collection and Analysis Methods \u2014 covers all of the following subparts:\n   - Data collection tools (e.g., surveys, assessments, interview guides, observation forms)\n   - Data sources (e.g., participants, staff, community partners)\n   - Quantitative and qualitative measures\n   - Description of how data will be analyzed to track progress and measure impact\n4) Appendix: Instruments and Tools \u2014 includes summaries, sample items, or citations/links to validated tools. Must explicitly reference PHQ-9 and GAD-7 by name in the Appendix.\n\nSCORING (return a score from 0.0 to 2.0):\n- 2.0: DOCX or PDF; meets length; ALL four sections present; Appendix references PHQ-9 and GAD-7 (names visible) with samples or citations/links.\n- 1.5: DOCX or PDF; meets length; all sections present but Appendix missing samples/links OR only one of PHQ-9/GAD-7 named.\n- 1.0: DOCX or PDF; meets length; exactly one major section missing OR Appendix present but does not clearly contain instruments/tools content.\n- 0.5: DOCX or PDF but too short OR multiple required sections missing.\n- 0.0: Not a DOCX/PDF, or virtually no recognizable structure.\n\nOnly check format and section presence. Do not judge correctness of methodology here.", "expectation": "A DOCX with clear headers for all required sections and an Appendix mentioning PHQ-9 and GAD-7."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification of Methods and Evidence", "description": "Checks that the plan includes the specified methods, instruments, and feasible evaluation logic. Mix of precise code checks and LLM judgment for methodological coherence.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Formative and Summative Mention Check", "description": "Verifies both 'formative' and 'summative' appear in the document (case-insensitive), indicating both evaluation types are addressed.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float (score) in [0, weight]\n    \"\"\"\n    weight = 0.4\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = \"\"\n    t = text.lower()\n    has_formative = 'formative' in t\n    has_summative = 'summative' in t\n    score = 0.0\n    if has_formative and has_summative:\n        score = weight\n    elif has_formative or has_summative:\n        score = weight * 0.5\n    else:\n        score = 0.0\n    return score"}, {"type": "code", "name": "Instrument Citations and References Presence", "description": "Checks for presence of PHQ-9 and GAD-7 plus at least one credible reference token (e.g., Kroenke, DOI, CORC, NCBI/PMC, AAP 2008). Partial credit based on matches.", "weight": 0.3, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 0.3\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = \"\"\n    t = text.lower()\n    # Required instruments\n    has_phq9 = ('phq-9' in t) or ('phq9' in t)\n    has_gad7 = ('gad-7' in t) or ('gad7' in t) or ('generalized anxiety disorder assessment' in t)\n    # Reference tokens: any of these indicate credible sourcing\n    ref_tokens = [\n        'kroenke',\n        'spitzer',\n        '10.1046/j.1525-1497.2001.016009606.x',\n        'corc',\n        'ncbi',\n        'pmc',\n        'aap 2008',\n        'arbourhospital',\n        'daiseysolutions',\n        'daisey'\n    ]\n    ref_hits = sum(1 for r in ref_tokens if r in t)\n    # Scoring: instruments count double, refs contribute partial\n    instr_score = (1 if has_phq9 else 0) + (1 if has_gad7 else 0)  # 0..2\n    ref_score = min(ref_hits, 2)  # cap at 2\n    total_units = instr_score * 2 + ref_score  # max 2*2 + 2 = 6\n    max_units = 6\n    return weight * (total_units / max_units) if max_units > 0 else 0.0"}, {"type": "code", "name": "Quant + Qual Methods and Tools Presence", "description": "Checks that both quantitative and qualitative methods are referenced alongside concrete tools (e.g., surveys, interviews, focus groups, observations).", "weight": 0.3, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 0.3\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = \"\"\n    t = text.lower()\n    has_quant = 'quantitative' in t\n    has_qual = 'qualitative' in t\n    quant_tools = any(w in t for w in ['survey', 'questionnaire', 'assessment'])\n    qual_tools = any(w in t for w in ['interview', 'focus group', 'observation'])\n    # scoring: 25% each component\n    score = 0.0\n    score += 0.25 if has_quant else 0.0\n    score += 0.25 if quant_tools else 0.0\n    score += 0.25 if has_qual else 0.0\n    score += 0.25 if qual_tools else 0.0\n    return weight * score"}, {"type": "llm_judge", "name": "Methodological Alignment and Analysis Plan", "description": "Assesses whether data collection tools, data sources, and quantitative/qualitative measures are coherently aligned with goals, and whether the analysis plan explains how progress and impact will be measured over time.", "weight": 1.4, "judge_prompt": "Evaluate the METHODS AND ANALYSIS content for correctness and completeness. Look in the section labeled like \"Data Collection and Analysis Methods\" and related areas.\n\nCheck for:\n- Clear linkage from program goals (improving immigrant families\u2019 mental health in Northwest Kansas) to measures (e.g., PHQ-9, GAD-7) and to outcomes/indicators.\n- Specifics on data sources (participants, staff, community partners) and how each contributes to answering the evaluation questions.\n- An analysis plan that describes how quantitative outcomes (e.g., change scores, pre/post, response rates) and qualitative data (e.g., thematic analysis) will be used to track progress and measure impact.\n- Reasonable approach to handling missing data, attrition, or response bias (at least a brief mention counts).\n\nScoring (0 to 1.4):\n- 1.4: Strong, coherent alignment; concrete measures; clear analysis steps for both quant and qual; addresses missing data/attrition.\n- 0.9: Adequate alignment; mostly clear analysis steps; minor gaps.\n- 0.5: Partial alignment; vague analysis steps or ignores either quant or qual.\n- 0.0: Lacks a credible analysis plan or alignment to goals.", "expectation": "A coherent analysis plan linking goals, measures, data sources, and methods for both quantitative and qualitative data."}, {"type": "llm_judge", "name": "Sampling, Timeline, and Performance Tracking", "description": "Assesses whether the plan includes a feasible sampling/recruitment approach, timing/frequency for data collection, and a way to monitor performance over time.", "weight": 1.3, "judge_prompt": "Assess whether the plan specifies: (1) who will be sampled/recruited and how (inclusion/exclusion, outreach methods), (2) when and how often data will be collected (e.g., intake, 6-week, discharge, 3-month follow-up), and (3) how performance will be tracked over time (e.g., dashboards, run charts, defined indicators/KPIs).\n\nScoring (0 to 1.3):\n- 1.3: Clear sampling/recruitment; explicit schedule or cadence; specific indicators and monitoring approach.\n- 0.8: Mostly present but one element is vague (e.g., timeline or indicators not fully specified).\n- 0.4: Mentions sampling or timing but lacks specifics and no tracking approach.\n- 0.0: None of these elements are clear.", "expectation": "Concrete sampling plan, timeline, and ongoing performance tracking approach."}, {"type": "llm_judge", "name": "Formative Fidelity and Feedback Loops", "description": "Assesses the formative evaluation elements for monitoring implementation fidelity and using feedback to improve the program during rollout.", "weight": 1.3, "judge_prompt": "Evaluate the FORMATIVE evaluation section for:\n- Implementation fidelity measures (e.g., adherence checklists, dosage, reach, staff training/competency).\n- Structured feedback loops (e.g., PDSA cycles, monthly review meetings, rapid-cycle learning) to improve delivery during implementation.\n- Roles and responsibilities for collecting/acting on fidelity data.\n\nScoring (0 to 1.3):\n- 1.3: Clear fidelity metrics, documented feedback cycle, and defined roles.\n- 0.8: Includes fidelity or feedback concepts but lacks detail in one area.\n- 0.4: Very general mention without operational details.\n- 0.0: No meaningful formative/fidelity content.", "expectation": "Operational formative evaluation plan with fidelity metrics and feedback loops."}, {"type": "llm_judge", "name": "Appendix Instruments Suitability for Immigrant Families", "description": "Assesses whether the Appendix instruments are appropriate for immigrant families in Northwest Kansas with attention to language access and cultural responsiveness.", "weight": 1.0, "judge_prompt": "Review the Appendix: Instruments and Tools. Judge whether the listed tools (e.g., PHQ-9, GAD-7) are appropriate for the target population and whether the plan addresses language access and cultural responsiveness (e.g., translated versions, interpreter support, literacy level, cultural adaptation guidance, validated translations).\n\nScoring (0 to 1.0):\n- 1.0: Instruments are appropriate and adaptations for language/culture are explicitly addressed (e.g., Spanish versions, validated translations, interpreter protocol).\n- 0.6: Tools appropriate; language/culture considerations are mentioned but not fully planned.\n- 0.3: Tools listed with little/no consideration of cultural or language needs.\n- 0.0: Instruments inappropriate or missing for the target population.", "expectation": "Appendix includes PHQ-9, GAD-7 with notes on translated versions and culturally responsive use."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Holistic Quality and Professionalism", "description": "LLM-only quality review focused on clarity, practicality, ethics, and local relevance to Northwest Kansas immigrant families.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity, Organization, and Professional Tone", "description": "Judges readability, coherent structure, and professional presentation suitable for an Executive Director.", "weight": 0.7, "judge_prompt": "Assess overall clarity, organization, and professional tone. The document should read as a practical plan with clear headings, concise prose, and minimal jargon. Look for coherence of sections, logical flow, and scan-ability (lists/tables where helpful).\n\nScoring (0 to 0.7):\n- 0.7: Exceptionally clear, well-organized, professional.\n- 0.5: Clear and professional with minor issues.\n- 0.3: Understandable but uneven organization or tone.\n- 0.0: Disorganized or unprofessional.", "expectation": "Clear, well-structured, and professionally written plan."}, {"type": "llm_judge", "name": "Local Relevance and Stakeholder Alignment", "description": "Evaluates tailoring to Northwest Kansas context and alignment with stakeholders (participants, staff, community partners).", "weight": 0.7, "judge_prompt": "Judge whether the plan is tailored to the Northwest Kansas context (rural access, local partnerships, community resources) and aligns with stakeholder roles (participants, staff, community partners). Look for mention of local barriers/facilitators (e.g., transportation, language access, trust), and stakeholder engagement in evaluation.\n\nScoring (0 to 0.7):\n- 0.7: Strong local tailoring and stakeholder alignment.\n- 0.5: Some local/contextual details and stakeholder roles present.\n- 0.3: Generic with minimal local relevance.\n- 0.0: No evidence of local tailoring or stakeholder alignment.", "expectation": "Concrete tailoring to Northwest Kansas and explicit stakeholder roles."}, {"type": "llm_judge", "name": "Feasibility, Ethics, and Data Privacy", "description": "Assesses practicality of the plan (resources, roles, timelines) and attention to ethics, informed consent, and data privacy/security.", "weight": 0.6, "judge_prompt": "Evaluate whether the plan appears feasible (realistic roles, resources, and timelines) and addresses ethics and data privacy/security (consent, confidentiality, secure storage, de-identification; references to HIPAA/FERPA or equivalent where appropriate). Consider whether an IRB or local review is discussed if applicable.\n\nScoring (0 to 0.6):\n- 0.6: Clearly feasible with explicit ethical/privacy safeguards.\n- 0.4: Generally feasible; mentions ethics/privacy but lacks detail.\n- 0.2: Feasibility or ethics/privacy addressed superficially.\n- 0.0: Not feasible and/or ignores ethics/privacy.", "expectation": "Realistic plan with explicit consent, confidentiality, and secure data handling considerations."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "0fad6023-767b-42c1-a1b3-027cd4f583cb", "rubric": {"category_name": "Retail Trade \u2013 General and Operations Managers | Meat & Seafood FSC Planogram (POG) Template", "rationale": "This rubric enforces a self-documenting, Excel-based planogram (POG) tool for a 24-foot full-service case (FSC). Stage 1 is an LLM-only gate that mandates a very specific workbook structure so verification is trivial. Stage 2 mixes small code checks (bounds/structure) with heavier LLM checks to confirm that calculations, instructions, and functional requirements are actually implemented. Stage 3 assesses professional quality, usability for beginner Excel users, print-readiness, and extensibility for real store operations. Code rules use the required API and are robust with flexible matching and try/except handling.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2013 Shape Enforcement Gate (LLM ONLY)", "description": "Gate that ensures the output is an Excel POG template with the exact required structure enabling verification. If this fails, evaluation ends and score is zero.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Excel POG Template Requirements", "description": "Verify the candidate produced an Excel workbook with the specific sheets and structural elements required to enable verification.", "weight": 4.0, "judge_prompt": "You are evaluating whether the candidate delivered a properly structured Excel (.xlsx) POG template for a 24-foot full-service case (FSC). Only check presence/structure, not content quality or correctness.\n\nRequired format:\n- File must be an Excel workbook (XLSX). Not PDF/Word.\n- Title visible on the main planning sheet header or top area: \u201cMeat Seafood FSC POG Template\u201d. Minor variations in spacing/case are acceptable.\n\nRequired sheets (names can vary slightly; be flexible with synonyms):\n1) Main planning sheet (e.g., \u201cPOG Planner\u201d, \u201cPlanogram\u201d, or \u201cFSC Layout\u201d)\n   Must visibly include ALL of the following elements:\n   - A tabular list of pans with columns that clearly map to:\n     \u2022 Pan number/position (e.g., \u201cPan #\u201d, \u201cPosition\u201d, or similar)\n     \u2022 Pan width (inches) as an editable input (commonly 6 or 8 inches)\n     \u2022 Item/description as an editable text field\n     \u2022 A visible total/summary area that shows total feet used vs. 24 ft available (e.g., Total Feet Used, Remaining Feet, or % of 24 ft). Only presence required; do not verify math.\n   - A visual representation of the case layout that shows each pan in order and proportion (e.g., merged cells, shapes, or scaled grid) so a viewer can see every pan across the full case.\n   - Printer-friendly layout: landscape orientation and obviously arranged to fit on a printed page (e.g., fit-to-width, reasonable margins). You only need to confirm that it appears intentionally formatted for printing.\n\n2) Instructions sheet (e.g., \u201cInstructions\u201d, \u201cHow to Use\u201d)\n   Must visibly include step-by-step guidance on how to use the tool, written for beginner Excel users (at least a few steps/sentences explaining editing pan widths and descriptions and how to print).\n\nOptional but beneficial (do not penalize if missing):\n- A separate \u201cPrint View\u201d or \u201cSummary\u201d sheet optimized explicitly for printing, containing the title and a clean, full-layout view of the pans.\n\nScoring:\n- 4.0: Excel workbook present AND main planning sheet + instructions sheet present AND all required structural elements identified above are clearly visible (pan table, editable width & description fields, total/summary vs 24 ft, visual layout of pans, printer-friendly formatting).\n- 3.0: Excel workbook present and both sheets present; only one minor structural element missing (e.g., no obvious landscape/fit-to-width setting OR visual layout less explicit but still clearly present as a case-wide representation).\n- 2.0: Excel workbook present but missing one required sheet OR major structural element (e.g., no instructions sheet or no visible total vs 24 ft).\n- 1.0: Excel workbook present but structure largely incorrect (e.g., just a simple list with no visible visual layout and no totals).\n- 0.0: Not an Excel file or missing multiple required components.\n\nOnly check presence/format. Do not judge correctness of calculations or instruction quality.", "expectation": "A clean XLSX with a main POG sheet (pan table + visual layout + totals vs 24 ft + printer-friendly) and a clear Instructions sheet for beginners. Optional Print View welcomed."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness Verification (Mixed)", "description": "Now that the shape is enforced, verify calculation logic sufficiency, editable fields, and instruction completeness at a functional level.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Total Pan Width Does Not Exceed 24 Feet", "description": "Checks that the sum of pan widths (interpreted in inches) does not significantly exceed 24 ft. Uses flexible matching to find the planner sheet and width column.", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output found.\"\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheet_names = [s for s in xls.sheet_names]\n        # Pick a likely planner sheet by keyword\n        candidates = []\n        for s in sheet_names:\n            sl = s.lower()\n            if any(k in sl for k in [\"pog\", \"plan\", \"layout\", \"case\", \"fsc\"]):\n                candidates.append(s)\n        sheet = candidates[0] if candidates else sheet_names[0]\n\n        # Try to locate a table with a width column\n        def load_with_header_guess(sheet):\n            for hdr in [0,1,2,3,4,5,6,7,8,9]:\n                try:\n                    df = pd.read_excel(path, sheet_name=sheet, header=hdr)\n                    if df is not None and df.shape[1] >= 2:\n                        cols = [str(c).strip().lower() for c in df.columns]\n                        if any(re.search(r\"width|inch|inches\", c) for c in cols):\n                            return df\n                except Exception:\n                    continue\n            # Fallback\n            return pd.read_excel(path, sheet_name=sheet)\n\n        df = load_with_header_guess(sheet)\n        cols = [str(c).strip().lower() for c in df.columns]\n        width_idx = None\n        for i, c in enumerate(cols):\n            if re.search(r\"width|inch|inches|pan width|pan\\s*in\", c):\n                width_idx = i\n                break\n        if width_idx is None:\n            return 0.0, \"No width column detected.\"\n        widths = pd.to_numeric(df.iloc[:, width_idx], errors='coerce').dropna()\n        if len(widths) == 0:\n            return 0.0, \"Width column has no numeric values.\"\n        total_inches = float(widths.sum())\n        total_feet = total_inches / 12.0\n        # Score with tolerance\n        if total_feet <= 24.01:\n            return 0.5, f\"Total feet used \u2248 {total_feet:.2f} (within 24 ft).\"\n        elif total_feet <= 25.0:\n            return 0.25, f\"Total feet used \u2248 {total_feet:.2f} (slightly over 24 ft).\"\n        else:\n            return 0.0, f\"Total feet used \u2248 {total_feet:.2f} (exceeds 24 ft).\"\n    except Exception as e:\n        return 0.0, f\"Error evaluating: {e}\""}, {"type": "code", "name": "Editable Inputs Present (Widths and Descriptions)", "description": "Verifies there are editable-looking columns for pan width and item/description with sufficient non-empty entries.", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output found.\"\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheet_names = [s for s in xls.sheet_names]\n        candidates = []\n        for s in sheet_names:\n            sl = s.lower()\n            if any(k in sl for k in [\"pog\", \"plan\", \"layout\", \"case\", \"fsc\"]):\n                candidates.append(s)\n        sheet = candidates[0] if candidates else sheet_names[0]\n\n        def load_with_header_guess(sheet):\n            for hdr in [0,1,2,3,4,5,6,7,8,9]:\n                try:\n                    df = pd.read_excel(path, sheet_name=sheet, header=hdr)\n                    if df is not None and df.shape[1] >= 2:\n                        return df\n                except Exception:\n                    continue\n            return pd.read_excel(path, sheet_name=sheet)\n\n        df = load_with_header_guess(sheet)\n        cols = [str(c).strip().lower() for c in df.columns]\n        # Width\n        w_idx = None\n        for i, c in enumerate(cols):\n            if re.search(r\"width|inch|inches|pan width|pan\\s*in\", c):\n                w_idx = i\n                break\n        # Description\n        d_idx = None\n        for i, c in enumerate(cols):\n            if re.search(r\"desc|item|product|label|what's in|what is in|content\", c):\n                d_idx = i\n                break\n        score = 0.0\n        feedbacks = []\n        if w_idx is not None:\n            widths = pd.to_numeric(df.iloc[:, w_idx], errors='coerce').dropna()\n            if len(widths) >= 4:\n                score += 0.25\n                feedbacks.append(\"Width column with sufficient numeric entries found.\")\n            else:\n                feedbacks.append(\"Width column found but too few numeric entries.\")\n        else:\n            feedbacks.append(\"No width column detected.\")\n        if d_idx is not None:\n            descs = df.iloc[:, d_idx].astype(str).replace({\"nan\":\"\"}).str.strip()\n            non_empty = (descs != \"\").sum()\n            if non_empty >= 4:\n                score += 0.25\n                feedbacks.append(\"Description column with multiple non-empty entries found.\")\n            else:\n                feedbacks.append(\"Description column found but too few non-empty entries.\")\n        else:\n            feedbacks.append(\"No description column detected.\")\n        return score, \"; \".join(feedbacks)\n    except Exception as e:\n        return 0.0, f\"Error evaluating: {e}\""}, {"type": "llm_judge", "name": "Functional Space Tracker and Overfill Handling", "description": "Check that the workbook clearly calculates total feet used against a 24 ft capacity, shows remaining or percent used, and visibly indicates when over 24 ft.", "weight": 4.0, "judge_prompt": "Evaluate the functional calculation presentation (not exact math):\n- Is there a clear, visible total space used against a 24 ft capacity (e.g., \u201cTotal Feet Used\u201d vs 24, Remaining Feet, or % Used)?\n- Is 24 ft capacity explicitly referenced somewhere in the calculation area?\n- Is there an obvious indicator when the total exceeds 24 ft (e.g., conditional formatting, warning text, or negative remaining value)?\n- Do headings/labels make it clear that widths are interpreted in inches and converted to feet for totals?\n\nScoring:\n- 4.0: Clear total vs 24 ft, remaining and/or percent used displayed, and an obvious over-capacity indicator is present.\n- 2.5: Clear total vs 24 ft and remaining/percent used, but no explicit over-capacity indicator.\n- 1.5: Some total is shown but the 24 ft capacity reference is ambiguous or missing.\n- 0.0: No clear total vs 24 ft presentation.\n", "expectation": "A visible summary block with Total Feet Used, Remaining, and percent of 24 ft, with an over-capacity warning."}, {"type": "llm_judge", "name": "Beginner-Focused Instructions Sufficient for Use", "description": "Check that the Instructions tab adequately teaches a beginner how to use the tool to plan an FSC.", "weight": 3.0, "judge_prompt": "Inspect the Instructions sheet. Determine if it provides beginner-friendly, step-by-step guidance to use the POG tool:\nMust include (flexibly worded):\n- How to edit pan widths (e.g., typical 6\"/8\" pans, data validation if present)\n- How to edit/add item descriptions and reorder or insert/delete pans safely\n- How to confirm the 24 ft space utilization (what to look at in the total/remaining area)\n- How to print (landscape, fit to page, which sheet to print)\n- Basic do/don\u2019t tips to avoid breaking formulas (e.g., only edit yellow cells)\n\nScoring:\n- 3.0: Clear, plain-language steps covering all points above with numbering or bullets.\n- 2.0: Covers most points (4 of 5) with reasonable clarity.\n- 1.0: Minimal or vague instructions that miss multiple points.\n- 0.0: No meaningful instructions.\n", "expectation": "A short, numbered/bulleted how-to with concrete steps for editing widths/descriptions, reading totals, and printing."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Quality and Professionalism (LLM)", "description": "Holistic assessment of usability, print-ready presentation, accessibility, and extensibility for real store operations.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Print Readiness", "description": "Assess if the main planning/print sheets look professional and are ready to print without tweaks.", "weight": 2.0, "judge_prompt": "Judge professional presentation and print readiness:\n- Clean layout with consistent fonts, alignment, and spacing\n- Landscape orientation, fit-to-width (grid not cut off), appropriate margins\n- Header/footer includes the template title and optional page/date/store info\n- No clipped visuals or orphaned elements upon print\n\nScoring:\n- 2.0: Professional and clearly print-ready.\n- 1.0: Generally acceptable with minor issues.\n- 0.0: Messy or unfit for printing without significant fixes.", "expectation": "A polished, clearly printable sheet with consistent formatting and a proper header."}, {"type": "llm_judge", "name": "Visual Planogram Clarity", "description": "Evaluate how effectively the tool visually communicates the pan layout across the 24 ft FSC.", "weight": 2.0, "judge_prompt": "Evaluate visual clarity of the planogram:\n- Each pan is represented in position/order across the 24 ft case\n- Visual proportion or clear labeling indicates relative pan widths\n- Helpful labels and/or foot markers (e.g., 0ft, 6ft, 12ft, 18ft, 24ft)\n- Color-coding or borders enhance readability without clutter\n\nScoring:\n- 2.0: Clear, proportional, and easy to interpret at a glance.\n- 1.0: Understandable but could be clearer.\n- 0.0: Confusing or no real visual planogram beyond a table.", "expectation": "A layout where pans read left-to-right across 24 ft with obvious widths and labels."}, {"type": "llm_judge", "name": "Accessibility and Safe Editing", "description": "Check for beginner-friendly safeguards and accessibility considerations.", "weight": 2.0, "judge_prompt": "Assess:\n- Input cells (widths/descriptions) are visually distinguished (e.g., shaded) and formula cells appear protected or at least visually marked as non-editable\n- Data validation for pan width (e.g., dropdown 6\"/8\") while allowing overrides if needed\n- Adequate contrast and non-color cues so it\u2019s usable when printed in grayscale\n\nScoring:\n- 2.0: Strong safeguards and accessibility cues present.\n- 1.0: Some safeguards/cues but incomplete.\n- 0.0: Little to no attention to safe editing or accessibility.", "expectation": "Clear input vs formula areas, sensible validation, and high-contrast design."}, {"type": "llm_judge", "name": "Operational Extensibility and Practicality", "description": "Evaluate whether the template is practical for real store use across Meat and Seafood and adaptable to variations.", "weight": 2.0, "judge_prompt": "Consider practicality and extensibility:\n- Supports both Meat and Seafood (e.g., a department selector/field or guidance on using the same template for both)\n- Ability to adjust case length if needed (e.g., a parameter) or at least clearly labeled as 24 ft constant\n- Space for notes/annotations (e.g., specials, vendor notes)\n- Version/date information for control and updates\n\nScoring:\n- 2.0: Clearly practical and adaptable with multiple supports.\n- 1.0: Some practical elements present but limited adaptability.\n- 0.0: Narrow, rigid design with little operational utility beyond the bare minimum.", "expectation": "Useful in day-to-day store ops with light configurability and notes/versioning."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "c2e8f271-7858-412f-b460-472463ad81d9", "rubric": {"category_name": "Coding Standards Draft (CIS Managers - Product Agency)", "rationale": "Document task (Pattern B). Stage 1 uses LLM-only gate to enforce a precise, reviewable DOCX/PDF structure with required sections and explicit examples. Stage 2 mixes a few lightweight code checks (presence of stack references and external links) with higher-weight LLM verifications for correctness and cross-reference consistency. Stage 3 provides a holistic quality assessment focused on clarity, actionability, and maintainability. Code rules are low-weight relative to LLM rules, leveraging the shape enforced in Stage 1.", "max_total_score": 30.0, "stages": [{"name": "Stage 1 \u2014 Format and Structural Gate (LLM-only)", "description": "Gate: Verify the candidate produced a properly structured coding standards document that is easy to review and verify. Only checks presence/format, not content quality.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Document Format and Required Structure", "description": "Check that the output is a Word or PDF document with all required sections, explicit examples, and bounded length.", "weight": 8.0, "judge_prompt": "You are the Stage 1 gate. Inspect the candidate's latest primary output file.\n\nFormat requirements:\n- Must be a DOCX (Word) or PDF document (DOCX preferred). Not plain text, not Excel.\n- Maximum length: 6 pages.\n- Professionally formatted with clear, scannable headings and subheadings.\n\nRequired sections and coverage (be flexible with exact titles, but all topics must be present):\n1) Purpose and Scope \u2014 why this standard exists; who it applies to (all teams/monorepo).\n2) Baseline Style and References \u2014 adopt a community baseline (e.g., Google TypeScript, TS Dev Style) and list URLs to at least two external guides.\n3) Project Structure and Monorepo Conventions \u2014 package boundaries, naming, shared libs, app vs. libs layout.\n4) TypeScript/Node Backend Standards \u2014 typing strategy, error handling, async patterns, environment config, module resolution; call out Prettier as formatting source-of-truth.\n5) React/Next.js Frontend and API Standards \u2014 component structure, hooks, file naming, API routes, server/client components guidance.\n6) Database & Data Access \u2014 Drizzle ORM usage, migrations, schema naming, Neon/Postgres practices.\n7) Testing Standards \u2014 React Testing Library usage patterns, unit/integration distinctions, test naming and structure, what to mock, sample tests.\n8) Tooling & Automation \u2014 Prettier and linting expectations, CI required checks.\n9) Git Workflow \u2014 PR titles, PR descriptions/checklist, branch naming convention, and commit-message guidelines (with patterns and examples).\n10) Documentation \u2014 README/ADR expectations, inline docs, how to document decisions.\n11) Governance & Rollout \u2014 staged rollout plan, ownership, change/versioning of the standard.\n\nExplicit example requirements (must be visible as examples, lists, or code-style blocks):\n- A PR title format example (e.g., \"feat(api): short description [JIRA-123]\").\n- A branch naming example (e.g., \"feature/JIRA-123-short-desc\").\n- A commit message guideline with at least one example (e.g., Conventional Commits style or similar), including subject line rules and body guidance.\n- A lightweight PR checklist (e.g., tests added, docs updated, breaking changes noted).\n- An explicit statement that Prettier governs formatting to avoid style debates.\n\nScoring (return a score from 0.0 to 1.0, which will be weighted):\n- 1.0: Valid DOCX/PDF, <=6 pages, all 11 topic areas present with clear headings; includes all explicit examples and mentions Prettier as formatting authority; includes at least two external reference links.\n- 0.8: Valid DOCX/PDF, <=6 pages; all core topics present; missing exactly one explicit example or one supporting topic; still includes Prettier statement and at least two references.\n- 0.6: Valid DOCX/PDF; minor structural gaps (missing 1\u20132 topic areas) or length risk (slightly crowded but appears within 6 pages); examples may be incomplete (missing up to two items) but PR/branch/commit guidance exists.\n- 0.3: Valid DOCX/PDF but multiple missing core topics (e.g., Testing or Git Workflow), or missing most explicit examples; unclear structure.\n- 0.0: Not a DOCX/PDF, or obviously exceeds 6 pages, or lacks most required sections.\n\nOnly evaluate presence/structure and length bound, not content correctness or quality.", "expectation": "A 4\u20136 page DOCX/PDF with all required sections, explicit PR/branch/commit examples, a PR checklist, and Prettier as formatting authority, plus at least two external references."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification of Coverage and Correctness", "description": "Confirm the document correctly covers the specified tech stack and practices with workable, specific guidance and consistency across sections.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Tech Stack Coverage Presence (Code)", "description": "Verify the document text explicitly mentions and provides guidance for key stack elements: TypeScript, Node, React, Next.js, Neon/Postgres, Drizzle, React Testing Library, Prettier, Monorepo.", "weight": 0.7, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Checks presence of key stack keywords. Returns fraction of items found.\n    \"\"\"\n    import re\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        if output.extension and output.extension.lower().endswith('.docx'):\n            text = context.files.read_docx_text(output.id) or \"\"\n        elif output.extension and output.extension.lower().endswith('.pdf'):\n            text = context.files.read_pdf_text(output.id) or \"\"\n        else:\n            # Fallback if document but extension unknown\n            try:\n                text = context.files.read_docx_text(output.id) or \"\"\n            except Exception:\n                try:\n                    text = context.files.read_pdf_text(output.id) or \"\"\n                except Exception:\n                    text = \"\"\n    except Exception:\n        text = \"\"\n    t = text.lower()\n    # Each item may have multiple aliases\n    checks = [\n        any(k in t for k in [\"typescript\", \"type script\", \"ts \"]),\n        any(k in t for k in [\"node.js\", \"nodejs\", \"node \"]),\n        \"react\" in t,\n        any(k in t for k in [\"next.js\", \"nextjs\", \" next \"]),\n        any(k in t for k in [\"neon\", \"neon.tech\"]) and (\"postgres\" in t or \" postgresql\" in t or \" postgres \" in t),\n        \"drizzle\" in t,\n        any(k in t for k in [\"react testing library\", \"rtl (react testing library)\", \"@testing-library/react\", \" testing library \"]),\n        \"prettier\" in t,\n        \"monorepo\" in t or \"mono-repo\" in t or \"mono repo\" in t,\n    ]\n    score = sum(1 for c in checks if c) / float(len(checks)) if checks else 0.0\n    return score"}, {"type": "code", "name": "External References Present (Code)", "description": "Check that at least two of the referenced external guides/links are present.", "weight": 0.5, "code": "def evaluate(workflow, context):\n    \"\"\"Looks for at least two external guideline references among the provided list.\"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        if output.extension and output.extension.lower().endswith('.docx'):\n            text = context.files.read_docx_text(output.id) or \"\"\n        elif output.extension and output.extension.lower().endswith('.pdf'):\n            text = context.files.read_pdf_text(output.id) or \"\"\n    except Exception:\n        text = text or \"\"\n    t = (text or \"\").lower()\n    refs = [\n        \"google.github.io/styleguide/tsguide\",\n        \"ts.dev/style\",\n        \"typescriptlang.org/docs/handbook\",\n        \"docs.aws.amazon.com/prescriptive-guidance\",\n        \"aws\",\n    ]\n    found = 0\n    for r in refs:\n        if r in t:\n            found += 1\n    # Count at most once per reference base. If only 'aws' is present without the specific URL, count as 0.5 credit.\n    # Compute normalized: 1.0 if >=2 strong URLs; 0.5 if exactly 1 strong URL or only generic 'aws'; else 0.\n    strong_urls = 0\n    for r in refs[:4]:\n        if r in t:\n            strong_urls += 1\n    if strong_urls >= 2:\n        return 1.0\n    if strong_urls == 1 or (strong_urls == 0 and \"aws\" in t):\n        return 0.5\n    return 0.0"}, {"type": "llm_judge", "name": "Git Workflow Specificity and Examples", "description": "Verify the Git guidance is concrete and usable: PR title pattern, branch naming convention, commit-message rules with examples, and a PR checklist that reduces reviewer friction.", "weight": 3.6, "judge_prompt": "Evaluate the document's Git workflow guidance for concreteness and correctness.\nLook for:\n- A PR title naming pattern with a worked example (e.g., Conventional Commits style or similar) and mention of ticket/issue IDs if applicable.\n- A branch naming convention with a worked example (e.g., feature/JIRA-123-short-desc, hotfix/..., chore/...).\n- Commit-message guidelines, including subject-line rules (imperative mood, length), body details (why, what, breaking changes), and at least one valid example.\n- A concise PR checklist (tests added/updated, docs updated, breaking changes noted, migration steps).\n- Coherence with Prettier/lint automation (avoid nitpicks).\n\nScoring (0.0\u20131.0):\n- 1.0: All four areas present with clear examples/patterns and rationale; guidance helps avoid bikeshedding and speeds reviews.\n- 0.7: Three areas strong, one weaker or missing examples; still broadly usable.\n- 0.4: Only one\u2013two areas concrete; others vague.\n- 0.0: Missing or too vague to be actionable.", "expectation": "Actionable Git workflow with explicit patterns and examples for PR titles, branches, commits, plus a practical PR checklist."}, {"type": "llm_judge", "name": "Testing Guidance Completeness (React Testing Library)", "description": "Check that testing guidance is technically sound for the stated stack and prevents common pitfalls.", "weight": 3.6, "judge_prompt": "Evaluate the testing standards, with emphasis on React Testing Library (RTL) and the stack specifics.\nLook for:\n- Clear guidance on unit vs. integration tests; what to test vs. avoid (implementation details vs. behavior).\n- RTL best practices (queries by role/label, user events, async waits, accessibility-first selectors).\n- Test organization: naming, colocating tests, example structure, fixtures/mocks, when to mock network/DB.\n- Coverage targets or quality bars; CI enforcement (fail builds on missing tests when applicable).\n- Backend testing notes (Node/TypeScript), data access tests with Drizzle/Postgres (e.g., using test containers or an isolated DB) where appropriate.\n\nScoring (0.0\u20131.0):\n- 1.0: Comprehensive, stack-aware guidance with examples; addresses common pitfalls.\n- 0.7: Mostly complete with minor gaps; at least one good example.\n- 0.4: Partial and generic; lacks RTL specifics.\n- 0.0: Missing or incorrect.", "expectation": "Behavior-focused RTL guidance with examples, clear organization, minimal flakiness, and CI integration."}, {"type": "llm_judge", "name": "Rollout, Governance, and Maintainability", "description": "Verify the document defines a realistic rollout plan, ownership, and change control so the standard remains living and authoritative.", "weight": 3.6, "judge_prompt": "Assess whether the document defines how this standard will be adopted and maintained.\nLook for:\n- Staged rollout plan (e.g., pilot team, feedback window, org-wide enablement) with timelines or triggers.\n- Ownership (e.g., VP Eng or standards working group), review cadence, and versioning (changelog or version number in doc).\n- Contribution process for teams (how to propose edits via PRs/ADRs) and conflict resolution (tie-breakers, when to defer to Prettier/community baselines).\n- Backwards-compatibility and migration guidance (how to handle legacy code, exceptions).\n\nScoring (0.0\u20131.0):\n- 1.0: Clear, pragmatic rollout and governance with mechanisms to evolve safely.\n- 0.7: Mostly clear but missing one element (e.g., versioning or migration policy).\n- 0.4: High-level only; lacks process details.\n- 0.0: Absent.", "expectation": "A maintainable governance model with staged rollout, ownership, versioning, and contribution workflow."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Holistic Quality Assessment", "description": "Assess professionalism, clarity, actionability, and strategic fit for the organization.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity, Brevity, and Navigability", "description": "Is the document concise (<=6 pages), well-structured, easy to scan and reference over time?", "weight": 2.5, "judge_prompt": "Judge the document on clarity and navigability:\n- Concise (aims for <=6 pages) with clear headings, numbered sections, and consistent formatting.\n- Uses bullets, tables, or callouts for quick reference; avoids walls of text.\n- Logical flow from principles to specifics; minimal duplication.\nReturn 0.0\u20131.0.", "expectation": "A crisp, scannable standard suitable for day-to-day reference."}, {"type": "llm_judge", "name": "Actionability and Example Quality", "description": "Do the standards translate into clear developer actions with examples, checklists, and conventions that reduce ambiguity?", "weight": 2.5, "judge_prompt": "Evaluate how actionable the guidance is:\n- Specific do/don't lists, examples (naming patterns, code snippets or pseudo-code), and checklists.\n- Explicit defaults for ambiguous areas; minimal room for preference debates (delegate formatting to Prettier).\n- Clear acceptance criteria for reviews (what must be present for approval).\nScore 0.0\u20131.0.", "expectation": "Actionable standards with concrete examples, defaults, and acceptance criteria."}, {"type": "llm_judge", "name": "Alignment with Stack and Community Baselines", "description": "Does the doc align with the stated stack and appropriately leverage community guidelines with sensible deviations?", "weight": 2.5, "judge_prompt": "Evaluate alignment and rationale:\n- Leverages community baselines (Google TS, TS Dev Style, Handbook do's/don'ts, AWS) with links.\n- Any deviations are minimal, well-justified, and consistent across backend/frontend/DB/testing.\n- Ties decisions back to speed and reliability goals; reduces bikeshedding.\nReturn 0.0\u20131.0.", "expectation": "Well-aligned with community standards; deviations rare and justified."}, {"type": "llm_judge", "name": "Professional Tone and Cross-Team Applicability", "description": "Assess tone, inclusivity, and applicability across four teams in a client-facing product agency context.", "weight": 2.5, "judge_prompt": "Assess professionalism and applicability:\n- Tone is clear, respectful, and directive without being dogmatic.\n- Guidance anticipates cross-team variations in a monorepo and client constraints.\n- Mentions handling exceptions/legacy code and communication norms to reduce friction between authors/reviewers.\nScore 0.0\u20131.0.", "expectation": "Professional, inclusive tone with guidance applicable across teams and clients."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "47ef842d-8eac-4b90-bda8-dd934c228c96", "rubric": {"category_name": "Wholesale Trade \u2014 Order Clerks: Weekly Inventory Health and WOS Summary (Top 5 UPCs)", "rationale": "This rubric enforces a self-documenting Excel deliverable that makes verification trivial. Stage 1 (LLM-only gate) mandates an exact workbook structure: a Summary sheet with required columns and the five specified UPCs, an Intermediate Calculations sheet that shows the math (including Avg Daily Sold over the last 4 weeks and derivations of Weekly Unit Rate of Sale and WOS), a Store-Level OOS sheet to support active store logic, and a chart visualizing percent of stores OOS. Stage 2 mixes lightweight code rules (math and consistency checks) with LLM rules (method adherence, cross-sheet coherence, and chart correctness). Stage 3 evaluates professional quality and stakeholder usefulness.", "max_total_score": 15.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate", "description": "Confirm the candidate produced an Excel workbook with the exact structure needed for verification, including required sheets, columns, rows for specified UPCs, and a chart.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.1, "rules": [{"type": "llm_judge", "name": "Structured Excel Format and Required Sections", "description": "Validate the deliverable is an Excel file with the mandated sheets, tables, and chart to enable verification.", "weight": 3.0, "judge_prompt": "You are evaluating whether the submitted output is a properly structured Excel workbook for inventory analysis. Use the rendered view of the workbook to check structure only (not correctness). Be flexible with naming, but verify the presence of the required components.\n\nREQUIRED FILE FORMAT:\n- Must be an Excel workbook (.xlsx). Not CSV, PDF, DOCX, or images.\n\nREQUIRED SHEETS AND CONTENT:\n1) Sheet named 'Summary' (or close variant like 'Executive Summary', 'Overview', 'Top 5 Summary') containing a single clearly labeled table with one row for each of these EXACT 5 UPCs:\n   - 901153373247\n   - 567219040266\n   - 217313054556\n   - 875218534223\n   - 375301052429\n   Required columns in the Summary table (flexible on exact header names; must be visibly present):\n   - UPC\n   - Active Stores (count of active stores for the UPC)\n   - Stores Out of Stock (count)\n   - Percent Stores Out of Stock (% of active stores)\n   - Weekly Unit Rate of Sale (units/week) \u2014 defined as Avg Daily Sold (last 4 weeks) \u00d7 7\n   - Weeks of Supply (weeks)\n   - Inventory Units On Hand (or similar; needed to compute WOS)\n   Optional but nice to have: Product Name/Description.\n\n2) Sheet named 'Calculations' (or 'Methodology', 'Working', 'Intermediate Calculations') that shows the math per UPC, including at minimum:\n   - UPC\n   - Avg Daily Sold (last 4 weeks) per UPC\n   - Weekly Unit Rate of Sale (calculated as above)\n   - Inventory Units On Hand (total units for UPC at the retailer)\n   - Weeks of Supply (Inventory Units / Weekly Unit Rate of Sale)\n   Plus a short methodology note (\u22653 sentences) that states:\n   - How Avg Daily Sold (last 4 weeks) was derived\n   - How Weekly Unit Rate of Sale was calculated (daily \u00d7 7)\n   - How Active Stores are defined (store appears for item AND has OOS%)\n\n3) Sheet named 'Store-Level OOS' (or 'By Store', 'Store Detail', 'Store OOS') with a clearly labeled table including:\n   - UPC\n   - Store Number/ID\n   - OOS % (or OOS flag) per store\n   This sheet should make it possible to count Active Stores (stores that appear for the item and have OOS%).\n\n4) A chart (preferably bar chart) that shows Percent of Stores Out of Stock by UPC for the five UPCs. The chart may be on the Summary sheet or a 'Charts' sheet. It should have a clear title, axis labels, and include all five UPCs. The bars should make it easy to see which UPCs have the highest OOS rate.\n\nSCORING (Structure only):\n- 3.0: Excel workbook present with all required sheets and the chart; Summary table includes all required columns; all 5 specified UPCs included as distinct rows.\n- 2.5: Excel with Summary and Calculations present with all required Summary columns; Store-Level OOS present; chart present but missing minor elements (e.g., title) or on a different sheet.\n- 2.1: Excel with Summary and Calculations present, but missing Store-Level OOS OR the chart. Summary still has all required columns and 5 UPCs.\n- 1.0: Excel present but missing multiple required sections/columns OR fewer than 5 specified UPCs present.\n- 0.0: Not an Excel file or structure too incomplete to verify.\n\nOnly assess presence/structure/format. Do not verify numeric correctness.", "expectation": "A well-structured .xlsx with Summary, Calculations, Store-Level OOS, and a Percent OOS chart, containing exactly the five specified UPCs and the required Summary columns."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Consistency Verification", "description": "Verify calculations and cross-sheet consistency given the mandated structure. Code rules perform deterministic checks; LLM judges verify methodology adherence, cross-references, and chart correctness.", "is_required": false, "max_points": 7.5, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Summary Columns and UPC Coverage", "description": "Check that required Summary columns are present (fuzzy match) and the five specified UPCs are included as rows.", "weight": 0.35, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        # Helper to find sheet by fuzzy name\n        def pick_sheet(sheets, patterns):\n            low = {s.lower(): s for s in sheets}\n            for s in sheets:\n                sl = s.lower()\n                if any(p in sl for p in patterns):\n                    return low[sl]\n            return None\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheets = xls.sheet_names\n        sum_sheet = pick_sheet(sheets, [\"summary\", \"overview\"])\n        if not sum_sheet:\n            # fallback to first sheet\n            sum_sheet = sheets[0]\n        df = pd.read_excel(path, sheet_name=sum_sheet)\n        # Normalize columns\n        cols = {c: c.lower().strip() for c in df.columns}\n        inv = {v: k for k,v in cols.items()}\n        def find_col(cands):\n            for c in df.columns:\n                cl = c.lower()\n                for pat in cands:\n                    if pat in cl:\n                        return c\n            return None\n        required_map = {\n            'upc': ['upc', 'sku', 'item'],\n            'active': ['active store', 'store count', 'stores active', 'active count'],\n            'oos_ct': ['stores out of stock', 'stores oos', 'oos count', 'oos stores'],\n            'oos_pct': ['percent stores out of stock', '% stores out of stock', 'oos %', 'oos pct', 'percent oos', 'pct oos'],\n            'weekly_ros': ['weekly unit rate of sale', 'weekly ros', 'weekly units', 'weekly sales rate'],\n            'wos': ['weeks of supply', 'wos'],\n            'inventory': ['inventory units', 'on hand', 'on-hand', 'units on hand', 'total inventory', 'oh units']\n        }\n        found = {}\n        for k, pats in required_map.items():\n            col = find_col(pats)\n            found[k] = col\n        # Compute column presence score (7 required fields)\n        col_present = sum(1 for v in found.values() if v is not None)\n        col_score = col_present / 7.0\n        # UPC coverage\n        upc_col = found['upc'] if found['upc'] else df.columns[0]\n        upcs_raw = df[upc_col].astype(str).str.replace(r'[^0-9]', '', regex=True).str.lstrip('0')\n        # Normalize to preserve leading zeros for 12-digit, but compare numerically as strings without leading zeros\n        target_upcs = [\n            '901153373247',\n            '567219040266',\n            '217313054556',\n            '875218534223',\n            '375301052429'\n        ]\n        norm_upcs = set(upcs_raw.tolist())\n        coverage_hits = sum(1 for t in target_upcs if t in norm_upcs)\n        # UPC score: all 5 present => 1, else partial\n        upc_score = coverage_hits / 5.0\n        # Final combines columns (50%) and UPC coverage (50%)\n        final = 0.5 * col_score + 0.5 * upc_score\n        return max(0.0, min(1.0, final)) * 0.35, f\"Columns found {col_present}/7; UPCs covered {coverage_hits}/5.\"\n    except Exception as e:\n        return 0.0, f\"Error in rule: {e}\""}, {"type": "code", "name": "Percent OOS Math Consistency", "description": "Verify Percent Stores OOS \u2248 Stores OOS / Active Stores for each UPC in the Summary table (within tolerance).", "weight": 0.4, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet.\"\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        def pick_sheet(sheets, patterns):\n            for s in sheets:\n                sl = s.lower()\n                if any(p in sl for p in patterns):\n                    return s\n            return sheets[0]\n        sum_sheet = pick_sheet(xls.sheet_names, [\"summary\", \"overview\"])\n        df = pd.read_excel(path, sheet_name=sum_sheet)\n        def find_col(cands):\n            for c in df.columns:\n                cl = c.lower()\n                for pat in cands:\n                    if pat in cl:\n                        return c\n            return None\n        c_active = find_col([\"active store\", \"store count\", \"stores active\", \"active count\"])\n        c_oosct = find_col([\"stores out of stock\", \"stores oos\", \"oos count\", \"oos stores\"])\n        c_oosp = find_col([\"percent stores out of stock\", \"% stores out of stock\", \"oos %\", \"oos pct\", \"percent oos\", \"pct oos\"])\n        if not (c_active and c_oosct and c_oosp):\n            return 0.0, \"Missing columns needed for OOS% check.\"\n        act = pd.to_numeric(df[c_active], errors='coerce')\n        oos = pd.to_numeric(df[c_oosct], errors='coerce')\n        pct = pd.to_numeric(df[c_oosp].astype(str).str.replace('%','', regex=False), errors='coerce')\n        # Normalize pct to 0-1 scale if values look like percentages > 1\n        pct_norm = pct.copy()\n        if pct_norm.dropna().gt(1.5).any():\n            pct_norm = pct_norm / 100.0\n        with np.errstate(divide='ignore', invalid='ignore'):\n            calc = np.where(act>0, oos/act, np.nan)\n        # Tolerance 2 percentage points (0.02 absolute)\n        diff = np.abs(calc - pct_norm)\n        mask = (~np.isnan(diff)) & (~np.isinf(diff))\n        if mask.sum() == 0:\n            return 0.0, \"No comparable rows for OOS% check.\"\n        ok = (diff[mask] <= 0.02).sum()\n        frac = ok / mask.sum()\n        return frac * 0.4, f\"OOS%% consistency on {ok}/{int(mask.sum())} rows.\"\n    except Exception as e:\n        return 0.0, f\"Error in rule: {e}\""}, {"type": "code", "name": "WURoS and WOS Formula Consistency", "description": "Check Weekly Unit Rate of Sale \u2248 Avg Daily Sold (last 4 weeks) \u00d7 7, and WOS \u2248 Inventory Units / Weekly Unit Rate of Sale.", "weight": 0.4, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet.\"\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheets = xls.sheet_names\n        def pick_sheet(patterns):\n            for s in sheets:\n                sl = s.lower()\n                if any(p in sl for p in patterns):\n                    return s\n            return None\n        calc_sheet = pick_sheet([\"calculation\", \"method\", \"work\", \"intermediate\"])\n        sum_sheet = pick_sheet([\"summary\", \"overview\"]) or sheets[0]\n        if not calc_sheet:\n            return 0.0, \"No Calculations/Working sheet found.\"\n        dcalc = pd.read_excel(path, sheet_name=calc_sheet)\n        dsum = pd.read_excel(path, sheet_name=sum_sheet)\n        def fcol(df, cands):\n            for c in df.columns:\n                cl = c.lower()\n                for p in cands:\n                    if p in cl:\n                        return c\n            return None\n        c_upc_c = fcol(dcalc, [\"upc\", \"sku\", \"item\"])\n        c_upc_s = fcol(dsum, [\"upc\", \"sku\", \"item\"])\n        c_daily = fcol(dcalc, [\"avg daily sold\", \"daily inventory sold\", \"daily units\", \"daily sales last 4 weeks\", \"avg daily units\"])\n        c_weekly = fcol(dcalc, [\"weekly unit rate of sale\", \"weekly ros\", \"weekly units\", \"weekly sales rate\"])\n        c_inv_c = fcol(dcalc, [\"inventory units\", \"on hand\", \"on-hand\", \"units on hand\", \"total inventory\", \"oh units\"])\n        c_wos_c = fcol(dcalc, [\"weeks of supply\", \"wos\"])\n        if not all([c_upc_c, c_upc_s, c_daily, c_weekly, c_inv_c, c_wos_c]):\n            return 0.0, \"Missing required columns for formula checks.\"\n        # Normalize UPCs to string digits\n        def norm_upc(s):\n            return str(s) if pd.isna(s) else re.sub(r\"[^0-9]\", \"\", str(s))\n        dcalc['_upc'] = dcalc[c_upc_c].map(norm_upc)\n        dsum['_upc'] = dsum[c_upc_s].map(norm_upc)\n        # Merge on UPC\n        m = pd.merge(dsum[['_upc']], dcalc[['_upc', c_daily, c_weekly, c_inv_c, c_wos_c]], on='_upc', how='left')\n        m_num = m.copy()\n        daily = pd.to_numeric(m_num[c_daily], errors='coerce')\n        weekly = pd.to_numeric(m_num[c_weekly], errors='coerce')\n        inv = pd.to_numeric(m_num[c_inv_c], errors='coerce')\n        wos = pd.to_numeric(m_num[c_wos_c], errors='coerce')\n        # Check WURoS ~ daily*7 (within 10%) and WOS ~ inv/weekly (within 10%, with handling weekly ~0)\n        with np.errstate(divide='ignore', invalid='ignore'):\n            weekly_calc = daily * 7.0\n            wos_calc = np.where(weekly>0, inv/weekly, np.nan)\n        # Tolerance 10% relative or absolute small numbers tolerance\n        def rel_ok(a, b):\n            diff = np.abs(a - b)\n            with np.errstate(divide='ignore', invalid='ignore'):\n                rel = np.where(np.abs(b)>1e-6, diff/np.abs(b), diff)\n            return (diff <= 0.5) | (rel <= 0.10)\n        ok_weekly = rel_ok(weekly, weekly_calc)\n        ok_wos = rel_ok(wos, wos_calc)\n        mask_weekly = ~np.isnan(weekly) & ~np.isnan(weekly_calc)\n        mask_wos = ~np.isnan(wos) & ~np.isnan(wos_calc)\n        n1 = int(mask_weekly.sum())\n        n2 = int(mask_wos.sum())\n        s1 = int((ok_weekly & mask_weekly).sum()) if n1>0 else 0\n        s2 = int((ok_wos & mask_wos).sum()) if n2>0 else 0\n        if n1 + n2 == 0:\n            return 0.0, \"No comparable rows for WURoS/WOS checks.\"\n        frac = ( (s1/(n1 if n1 else 1)) + (s2/(n2 if n2 else 1)) ) / 2.0\n        return frac * 0.4, f\"WURoS ok {s1}/{n1}; WOS ok {s2}/{n2}.\"\n    except Exception as e:\n        return 0.0, f\"Error in rule: {e}\""}, {"type": "code", "name": "Active Stores Alignment with Store-Level OOS", "description": "Validate Summary Active Stores counts align with Store-Level OOS sheet using the specified active-store definition (store appears for the item AND has OOS%).", "weight": 0.35, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet.\"\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheets = xls.sheet_names\n        def pick_sheet(patterns):\n            for s in sheets:\n                if any(p in s.lower() for p in patterns):\n                    return s\n            return None\n        sum_sheet = pick_sheet([\"summary\", \"overview\"]) or sheets[0]\n        store_sheet = pick_sheet([\"store\", \"oos\", \"detail\"])\n        if not store_sheet:\n            return 0.0, \"No Store-Level OOS sheet found.\"\n        dsum = pd.read_excel(path, sheet_name=sum_sheet)\n        dstore = pd.read_excel(path, sheet_name=store_sheet)\n        def fcol(df, cands):\n            for c in df.columns:\n                cl = c.lower()\n                for p in cands:\n                    if p in cl:\n                        return c\n            return None\n        c_upc_s = fcol(dsum, [\"upc\", \"sku\", \"item\"])\n        c_act = fcol(dsum, [\"active store\", \"store count\", \"stores active\", \"active count\"])\n        c_upc_t = fcol(dstore, [\"upc\", \"sku\", \"item\"])\n        c_store = fcol(dstore, [\"store\", \"location\", \"store id\", \"store number\"])\n        c_oosp = fcol(dstore, [\"oos %\", \"oos pct\", \"percent oos\", \"% oos\", \"out of stock %\", \"oos\"])\n        if not all([c_upc_s, c_act, c_upc_t, c_store, c_oosp]):\n            return 0.0, \"Missing required columns for active-store alignment.\"\n        def norm_upc(v):\n            return re.sub(r\"[^0-9]\", \"\", str(v))\n        dsum['_upc'] = dsum[c_upc_s].map(norm_upc)\n        dstore['_upc'] = dstore[c_upc_t].map(norm_upc)\n        # Active store: store row exists AND OOS% is present (not null)\n        oosp = pd.to_numeric(dstore[c_oosp].astype(str).str.replace('%','', regex=False), errors='coerce')\n        active_store_rows = dstore[~oosp.isna()].copy()\n        # Count active stores per UPC (unique stores)\n        counts = active_store_rows.groupby('_upc')[c_store].nunique()\n        # Compare to Summary\n        dsum['_act'] = pd.to_numeric(dsum[c_act], errors='coerce')\n        comp = dsum.merge(counts.rename('calc_act'), left_on='_upc', right_index=True, how='left')\n        comp['calc_act'] = comp['calc_act'].fillna(0)\n        # Tolerance: allow +/- 1 store difference\n        diff = (comp['_act'] - comp['calc_act']).abs()\n        valid = (~comp['_act'].isna()) & (~comp['calc_act'].isna())\n        if valid.sum() == 0:\n            return 0.0, \"No comparable UPCs for alignment.\"\n        ok = (diff[valid] <= 1).sum()\n        frac = ok / valid.sum()\n        return frac * 0.35, f\"Active store alignment ok {ok}/{int(valid.sum())}.\"\n    except Exception as e:\n        return 0.0, f\"Error in rule: {e}\""}, {"type": "llm_judge", "name": "Methodology Adherence and Definitions", "description": "Confirm the Calculations/Methodology sheet explains the approach and matches required definitions: Avg Daily Sold from last 4 weeks, Weekly Unit ROS = daily \u00d7 7, active store definition (store exists for item AND has OOS%).", "weight": 2.0, "judge_prompt": "Review the Calculations/Methodology sheet. Check that it clearly states and applies these specific requirements:\n- Weekly Unit Rate of Sale is derived from Avg Daily Sold over the last 4 weeks multiplied by 7.\n- The analysis uses the last 4 weeks for the daily sold calculation (not another period).\n- The active store definition is correctly stated as: a store is active if it appears in the dataset for that item AND it has an out-of-stock percentage.\n- There is a brief methodology description (at least 3 sentences) covering how each metric was derived and any assumptions.\n\nScoring:\n- 2.0: All elements present and correct.\n- 1.0: Mostly correct, minor omissions or ambiguous phrasing.\n- 0.0: Missing or incorrect methodology/definitions.", "expectation": "A clear, explicit methodology with correct definitions and the last-4-weeks basis for Avg Daily Sold."}, {"type": "llm_judge", "name": "Cross-Sheet Consistency (Spot Check)", "description": "Spot-check that Summary values for at least a couple of UPCs match the Calculations sheet values (within rounding).", "weight": 1.0, "judge_prompt": "Pick at least two of the five UPCs and compare the values on the Summary vs the Calculations sheet for Weekly Unit Rate of Sale, Weeks of Supply, and Inventory Units. They should match within normal rounding differences.\n\nScoring:\n- 1.0: Clear match for the checked UPCs.\n- 0.5: Minor mismatches likely due to rounding/formatting.\n- 0.0: Significant inconsistencies.", "expectation": "Summary metrics mirror Calculations values closely."}, {"type": "llm_judge", "name": "Chart Correctness and Focus", "description": "Validate the chart visualizes Percent of Stores OOS for all five UPCs with clear labeling and emphasis on highest OOS.", "weight": 1.5, "judge_prompt": "Inspect the chart (on Summary or Charts sheet). Confirm:\n- It is a bar or column chart showing Percent of Stores Out of Stock by UPC.\n- All five specified UPCs are included and labeled.\n- The title and axes are clear. The highest OOS items are visually apparent (e.g., sorted, highlighted, or distinct color).\n\nScoring:\n- 1.5: All criteria met.\n- 1.0: Minor labeling or ordering issues but still clearly communicates highest OOS.\n- 0.0: Wrong metric or missing UPCs/labels.", "expectation": "A clean bar chart of % Stores OOS by UPC, easy to identify the riskiest items."}, {"type": "llm_judge", "name": "Show-Your-Work Evidence", "description": "Ensure the workbook visibly shows the calculations and references (not just final numbers).", "weight": 1.5, "judge_prompt": "Check that the workbook demonstrates the calculation steps (not just results). Look for:\n- Intermediate columns and values (Avg Daily Sold last 4 weeks, Weekly ROS, Inventory Units, WOS) per UPC.\n- Clear linkage between Calculations and Summary (same UPCs, consistent figures).\n\nScoring:\n- 1.5: Strong evidence of intermediate steps and linkages.\n- 0.8: Some steps shown but partial.\n- 0.0: No visible calculation trail.", "expectation": "Transparent intermediate calculations supporting the Summary."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Presentation and Strategic Quality", "description": "Evaluate professional polish, clarity, and strategic value for account management stakeholders.", "is_required": false, "max_points": 4.5, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Readability", "description": "Assess formatting, clarity of headers, number formats (percentages, units), sorting, and basic usability.", "weight": 1.5, "judge_prompt": "Evaluate the workbook\u2019s professional polish:\n- Clear, consistent headers and units (e.g., % formatted as percent, units as integers/decimals as appropriate).\n- Logical sorting (e.g., by % Stores OOS or by risk).\n- Clean layout, readable fonts, adequate spacing.\n\nScoring:\n- 1.5: Professional and easy to read; clear number formats and organization.\n- 0.8: Generally readable with minor issues.\n- 0.0: Poor formatting; hard to read.", "expectation": "A professional, easy-to-read Excel with sensible formatting and sorting."}, {"type": "llm_judge", "name": "Insight and Prioritization", "description": "Judge whether the deliverable surfaces which UPCs are most at risk and provides actionable direction.", "weight": 1.5, "judge_prompt": "Look for brief insights or notes indicating which UPCs have the highest OOS and potential stock risk (e.g., low WOS in high-volume contexts). Prefer callouts or a small notes/insights area.\n\nScoring:\n- 1.5: Clear, actionable insights and prioritization of at-risk UPCs.\n- 0.8: Some insights but limited prioritization.\n- 0.0: No insights beyond raw numbers.", "expectation": "Concise prioritization guidance highlighting riskiest UPCs."}, {"type": "llm_judge", "name": "Audience Appropriateness", "description": "Evaluate whether content is tailored to an Account Management audience (concise metrics, definitions, minimal jargon).", "weight": 0.8, "judge_prompt": "Assess if the workbook communicates appropriately for account managers:\n- Concise metrics with brief definitions.\n- Minimal jargon; clear takeaways.\n\nScoring:\n- 0.8: Well-tailored for account management.\n- 0.4: Mixed suitability.\n- 0.0: Not oriented to the intended audience.", "expectation": "Concise, business-facing content suitable for account managers."}, {"type": "llm_judge", "name": "Chart Storytelling Quality", "description": "Beyond correctness, judge whether the chart helps tell the risk story at a glance.", "weight": 0.7, "judge_prompt": "Evaluate if the chart\u2019s design supports quick understanding:\n- Clear title that mentions % Stores OOS.\n- Axis labels and legible data labels or values where appropriate.\n- Visual emphasis on highest OOS items (order, color, annotation).\n\nScoring:\n- 0.7: Strong storytelling and clarity.\n- 0.4: Adequate but could be clearer.\n- 0.0: Confusing or unhelpful visualization.", "expectation": "A chart that immediately highlights which UPCs are most out of stock."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "61b0946a-5c1c-4bf6-8607-84d7c7e0dfe0", "rubric": {"category_name": "Collaborative Cadaver Program Proposal (Health Care and Social Assistance > Medical and Health Services Managers)", "rationale": "Mixed-output task: a professional proposal document (Word/PDF) that embeds a cost-savings analysis and chart derived from an external budget workbook. The rubric follows the self-documenting, three-stage approach: Stage 1 enforces strict document structure so that verification is possible; Stage 2 mixes lightweight code checks (compliance phrases, key assumptions) with heavier LLM judges for analytical and domain-consistency verification; Stage 3 evaluates overall professional quality and actionability.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate", "description": "Validate that the candidate produced a properly structured proposal document enabling verification. LLM-only per guidance.", "is_required": true, "max_points": 2.0, "min_score_to_pass": 1.2, "rules": [{"type": "llm_judge", "name": "Document Format and Structural Requirements", "description": "Check presence of the required document format, title, sections, and the cost-savings chart required for verification.", "weight": 2.0, "judge_prompt": "You are evaluating whether the submitted output is a valid proposal document with the exact required structure to enable verification. Be flexible on exact section titles, but ensure all core elements are present.\n\nFormat Requirements:\n- File must be a PDF or DOCX (not spreadsheet, not plain text).\n- Minimum length: 3 pages equivalent.\n- Professional formatting with clear section headers.\n- Title includes the phrase: \"Collaborative Cadaver Program Proposal\" (or very close variation) on the first page.\n\nRequired Sections (accept synonymous headers):\n1) Introduction / Overview:\n   - States what the Collaborative Cadaver Program is and what it hopes to accomplish.\n2) Cost Savings Analysis:\n   - Explicitly highlights cost savings.\n   - States that costs were calculated using the department's \u201cCadaver Budget.xlsx\u201d (or a close name like \"cadaver budget\"), and that the lab fee is included while supplies and education expenses are excluded.\n   - Includes a chart/graph/figure that shows cost or savings as a function of number of participating departments (1, 2, 3, 4). Graph should have labels or legend so it is interpretable.\n3) Cadaver Use by Department (Body Regions):\n   - For each of the four departments (General Surgery, Thoracic Surgery, Otolaryngology, Orthopedic Surgery), lists which body regions they would generally use (e.g., abdomen, thorax/chest, head and neck, limbs/skeleton). This may be a list, table, or annotated image/picture.\n4) Procedure Capacity by Participation and Complexity:\n   - Presents approximate ranges of number of procedures per cadaver based on: number of participating departments (1\u20134), freeze/thaw cycles (10\u201312), and surgery complexity (simple, standard, complex).\n   - Explicitly states that the proposal does not account for mixing complexities.\n   - References the operational constraints: thaw window (~3 hours), durations for simple (30\u201345 min), standard (60\u201390 min), complex (2\u20133 hours, single procedure per cycle).\n5) Methods/Appendix/Notes (short):\n   - Brief calculation notes, key assumptions, and any references.\n\nScoring Guide (structure only; do not judge correctness yet):\n- 2.0: Valid PDF/DOCX with all required sections present; cost-savings chart present and interpretable; explicit cost assumptions (include lab fee, exclude supplies/education) are stated; departments and body regions covered; procedure capacity section includes ranges and non-mixing note.\n- 1.6: Valid PDF/DOCX with all core sections, but missing only the Methods/Appendix/Notes.\n- 1.2: Valid PDF/DOCX with one core requirement missing (e.g., chart absent OR body regions by department missing OR capacity section missing explicit non-mixing statement).\n- 0.6: Valid PDF/DOCX but multiple core elements missing or unclear (e.g., no chart and no department body-region mapping).\n- 0.0: Not a PDF/DOCX, or less than 3 pages, or lacks most required sections.\n\nOnly evaluate presence/structure and whether a graph is visibly included. Do NOT evaluate correctness of numbers or depth of analysis at this stage.", "expectation": "A professional PDF/DOCX titled \u201cCollaborative Cadaver Program Proposal\u201d with Introduction, Cost Savings (with interpretable chart and explicit include/exclude cost assumptions), Cadaver Use by Department (body regions), Procedure Capacity (ranges by participation and complexity, non-mixing noted), and brief Methods/Appendix."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Verification", "description": "Verify correctness and internal consistency of assumptions, methodology, and proposed allocations. Mix code checks for key phrases/assumptions with deeper LLM consistency checks. Code rules have lower total weight than LLM rules.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Cost Assumptions: Include Lab Fee; Exclude Supplies and Education", "description": "Checks if the document text explicitly includes the lab fee and excludes supplies and education expenses from the analysis.", "weight": 0.35, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0, \"No document output to validate.\"\\n    # Read text from PDF or DOCX\\n    text = \"\"\\n    try:\\n        if output.extension and output.extension.lower().endswith('.pdf'):\\n            text = context.files.read_pdf_text(output.id) or \"\"\\n        else:\\n            text = context.files.read_docx_text(output.id) or \"\"\\n    except Exception:\\n        try:\\n            text = context.files.read_docx_text(output.id) or \"\"\\n        except Exception:\\n            text = \"\"\\n    t = text.lower()\\n\\n    # Look for inclusion of lab fee\\n    has_lab_fee_term = ('lab fee' in t) or ('anatomy lab fee' in t) or ('lab charges' in t)\\n    include_lab_fee = False\\n    if has_lab_fee_term:\\n        # inclusion phrasing near lab fee\\n        patterns = [\\n            r\"include(?:d|s|ing)?[^\\n\\r]{0,60}lab fee\",\\n            r\"lab fee[^\\n\\r]{0,60}include(?:d|s|ing)?\",\\n            r\"including[^\\n\\r]{0,60}lab fee\"\\n        ]\\n        include_lab_fee = any(re.search(p, t) for p in patterns) or ('lab fee is included' in t)\\n\\n    # Look for explicit exclusions of supplies and education expenses\\n    def excluded(term):\\n        return bool(re.search(rf\"(exclude(?:d|s|ing)?|excluding|excludes)[^\\n\\r]{{0,60}}{term}\", t)) or \\\n               bool(re.search(rf\"{term}[^\\n\\r]{{0,60}}(exclude(?:d|s|ing)?|excluding|excludes)\", t))\\n\\n    supplies_excluded = excluded('supplies')\\n    education_excluded = excluded('education') or excluded('education expenses')\\n\\n    score = 0.0\\n    # Scoring: partial credit for partial compliance\\n    if has_lab_fee_term:\\n        score += 0.1\\n    if include_lab_fee:\\n        score += 0.15\\n    if supplies_excluded:\\n        score += 0.05\\n    if education_excluded:\\n        score += 0.05\\n\\n    # Cap at weight\\n    score = min(score, 0.35)\\n    feedback = []\\n    if not has_lab_fee_term:\\n        feedback.append(\"No mention of 'lab fee' found.\")\\n    if has_lab_fee_term and not include_lab_fee:\\n        feedback.append(\"'Lab fee' mentioned but not explicitly stated as included.\")\\n    if not supplies_excluded:\\n        feedback.append(\"'Supplies' not explicitly stated as excluded.\")\\n    if not education_excluded:\\n        feedback.append(\"'Education expenses' not explicitly stated as excluded.\")\\n    return score, \"; \".join(feedback) if feedback else \"OK\""}, {"type": "code", "name": "Complexity Classes and Non-Mixing Statement", "description": "Checks that simple, standard, and complex complexities are mentioned and that non-mixing of complexities is explicitly stated.", "weight": 0.35, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0, \"No document output to validate.\"\\n    # Read text\\n    text = \"\"\\n    try:\\n        if output.extension and output.extension.lower().endswith('.pdf'):\\n            text = context.files.read_pdf_text(output.id) or \"\"\\n        else:\\n            text = context.files.read_docx_text(output.id) or \"\"\\n    except Exception:\\n        try:\\n            text = context.files.read_docx_text(output.id) or \"\"\\n        except Exception:\\n            text = \"\"\\n    t = text.lower()\\n\\n    has_simple = 'simple' in t\\n    has_standard = 'standard' in t\\n    has_complex = 'complex' in t\\n\\n    # Look for a statement that mixing complexities is not accounted for\\n    non_mixing = False\\n    # Phrases like: \"does not account for mixing\", \"no mixing of complexities\", etc.\\n    patterns = [\\n        r\"does not account for mixing\",\\n        r\"doesn't account for mixing\",\\n        r\"no mixing of complexit\",\\n        r\"not mixing complexit\",\\n        r\"assumes no mixing\",\\n        r\"without mixing complexit\"\\n    ]\\n    non_mixing = any(re.search(p, t) for p in patterns)\\n\\n    score = 0.0\\n    score += 0.1 if has_simple else 0.0\\n    score += 0.1 if has_standard else 0.0\\n    score += 0.1 if has_complex else 0.0\\n    score += 0.15 if non_mixing else 0.0\\n\\n    score = min(score, 0.35)\\n    fb = []\\n    if not has_simple: fb.append(\"Missing 'simple' complexity mention.\")\\n    if not has_standard: fb.append(\"Missing 'standard' complexity mention.\")\\n    if not has_complex: fb.append(\"Missing 'complex' complexity mention.\")\\n    if not non_mixing: fb.append(\"Missing explicit 'no mixing of complexities' assumption.\")\\n    return score, \", \".join(fb) if fb else \"OK\""}, {"type": "code", "name": "Participation Levels and Savings Progression Mention", "description": "Checks that the document references participation levels (1\u20134 departments) in the context of savings and implies increasing savings with more participation.", "weight": 0.3, "code": "import re\\n\\nWINDOW = 80\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0, \"No document output to validate.\"\\n    # Read text\\n    text = \"\"\\n    try:\\n        if output.extension and output.extension.lower().endswith('.pdf'):\\n            text = context.files.read_pdf_text(output.id) or \"\"\\n        else:\\n            text = context.files.read_docx_text(output.id) or \"\"\\n    except Exception:\\n        try:\\n            text = context.files.read_docx_text(output.id) or \"\"\\n        except Exception:\\n            text = \"\"\\n    t = text.lower()\\n\\n    # Find references to 1, 2, 3, 4 departments\\n    nums_found = set()\\n    # Look for patterns like \"1 department\", \"2 departments\", or phrases like \"all four departments\"\\n    patterns = [\\n        r\"\\b1\\b[^\\n\\r]{0,15}department\",\\n        r\"\\b2\\b[^\\n\\r]{0,15}department\",\\n        r\"\\b3\\b[^\\n\\r]{0,15}department\",\\n        r\"\\b4\\b[^\\n\\r]{0,15}department\",\\n        r\"all four department\"\\n    ]\\n    for i, p in enumerate(patterns[:4], start=1):\\n        if re.search(p, t):\\n            nums_found.add(i)\\n    if re.search(patterns[4], t):\\n        nums_found.add(4)\\n\\n    # Look for phrasing that implies monotonic savings with more participation\\n    monotonic_patterns = [\\n        r\"more departments[^\\n\\r]{0,60}more savings\",\\n        r\"as participation increase[^\\n\\r]{0,60}savings increase\",\\n        r\"increasing participation[^\\n\\r]{0,60}increase[^\\n\\r]{0,10}savings\",\\n        r\"each additional department[^\\n\\r]{0,60}(save|savings|cost per department decrease)\"\\n    ]\\n    has_monotonic_claim = any(re.search(p, t) for p in monotonic_patterns)\\n\\n    score = 0.0\\n    # Award up to 0.2 for participation mentions (0.05 per level found)\\n    score += 0.05 * len(nums_found)\\n    # Award 0.1 if monotonic savings claim present\\n    if has_monotonic_claim:\\n        score += 0.1\\n\\n    score = min(score, 0.3)\\n    fb = []\\n    if len(nums_found) < 4:\\n        fb.append(f\"Only referenced participation levels: {sorted(nums_found)}\")\\n    if not has_monotonic_claim:\\n        fb.append(\"No explicit statement that savings increase with more departments.\")\\n    return score, \"; \".join(fb) if fb else \"OK\""}, {"type": "llm_judge", "name": "Cost-Savings Logic and Chart Interpretability", "description": "Judge whether the cost-savings analysis is logically consistent with shared cadaver utilization and whether the chart is interpretable, labeled, and aligns with the narrative assumptions (includes lab fee; excludes supplies/education).", "weight": 1.4, "judge_prompt": "Evaluate the cost-savings analysis for logical consistency and clarity. Consider the following:\n- Does the narrative explain how sharing cadavers across 1\u20134 departments reduces per-department cadaver costs? Does it explicitly include lab fee and exclude supplies/education costs in the calculations?\n- Is the chart/graph interpretable (axes labeled, legend or captions as needed)? Does it correctly represent savings (or total cost) across 1, 2, 3, and 4 participating departments?\n- Do the stated assumptions and the described calculation steps plausibly match the plotted data (at a high level)? You do not need exact numbers; check coherence.\nScoring:\n- 1.4: Clear, coherent logic; chart well-labeled and matches the explained assumptions; include/exclude cost scope stated.\n- 1.0: Mostly coherent; minor labeling or clarity issues but scope and general trend are correct.\n- 0.6: Some inconsistencies or unclear chart labeling; unclear include/exclude scope.\n- 0.0: Illogical analysis or chart missing/uninterpretable.", "expectation": "A coherent explanation of savings mechanics with a clearly labeled chart reflecting increasing savings with more departments and correct include/exclude cost scope."}, {"type": "llm_judge", "name": "Cadaver Use Mapping by Department (Domain Plausibility)", "description": "Judge whether proposed body-region allocations per department align with typical residency surgical domains and original constraints, minimizing destructive overlap.", "weight": 1.3, "judge_prompt": "Evaluate the section assigning cadaver body regions to each department:\n- General Surgery should focus primarily on abdomen (as per the task context). Thoracic Surgery: thorax/chest; Otolaryngology (ENT): head and neck; Orthopedic Surgery: limbs/skeleton (upper/lower extremities, joints). Variations acceptable if justified.\n- Is the mapping plausible and respectful of sequencing to minimize destructive overlap (e.g., plan order of use or distinct sides/regions)?\n- Does it reflect the goal of maximizing use across 10\u201312 freeze/thaw cycles while respecting the 3-hour thaw window?\nScoring:\n- 1.3: Accurate, well-justified mapping with consideration of sequencing and preservation.\n- 0.9: Generally correct mapping, limited discussion of sequencing or preservation.\n- 0.5: Superficial mapping; potential conflicts/overlaps not addressed.\n- 0.0: Mismatched domains or disregard of constraints.", "expectation": "A realistic mapping of departments to body regions aligned with their surgical scope, considering preservation and sequence of use."}, {"type": "llm_judge", "name": "Procedure Capacity Estimates (Freeze/Thaw and Complexity Constraints)", "description": "Judge whether procedure ranges by participation count and complexity class are realistic and derived from the provided constraints.", "weight": 1.3, "judge_prompt": "Assess the procedure capacity section for realism and internal consistency:\n- Are ranges provided for number of procedures per cadaver as a function of: number of participating departments (1\u20134), freeze/thaw cycles (~10\u201312), and complexity (simple 30\u201345 min; standard 60\u201390 min; complex 2\u20133 hours)?\n- Do ranges reflect that complex procedures typically allow only 1 procedure per thaw window, while simple may allow multiple, and standard allows 2\u20133 within the 3-hour window?\n- Are assumptions and any calculation notes coherent and consistent with earlier sections? Is it explicitly stated that mixing complexities is not accounted for?\nScoring:\n- 1.3: Ranges are well-reasoned and consistent with all constraints and notes.\n- 0.9: Mostly consistent; minor gaps/omissions.\n- 0.5: Superficial or partially inconsistent treatment of constraints.\n- 0.0: Incoherent or missing.", "expectation": "Clear, defensible ranges for procedure counts driven by freeze/thaw limits and time-per-procedure constraints, with a non-mixing assumption explicitly stated."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Presentation and Professional Quality", "description": "Holistic assessment of presentation quality, clarity, and actionability for a hospital leadership audience.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professionalism and Document Design", "description": "Assess overall professionalism: formatting, typography, headings, figure captions, and document flow.", "weight": 1.0, "judge_prompt": "Assess professionalism and design suitability for a hospital leadership audience:\n- Clear title page/heading, consistent headings and styles, page numbers, and readable typography.\n- Figures/tables (especially the cost-savings chart) are captioned, labeled, and referenced in the text.\n- Logical flow from introduction to recommendations and appendices.\nScoring:\n- 1.0: Highly professional and publication-ready.\n- 0.7: Generally professional with minor inconsistencies.\n- 0.4: Adequate but with noticeable formatting and flow issues.\n- 0.0: Poorly formatted or difficult to navigate.", "expectation": "A clean, consistent, and executive-ready document with labeled visuals and coherent flow."}, {"type": "llm_judge", "name": "Strategic Value and Feasibility", "description": "Evaluate whether the proposal presents a feasible plan with clear benefits, ethical considerations, and operational feasibility.", "weight": 1.0, "judge_prompt": "Evaluate strategic value and feasibility:\n- Are benefits quantified or described credibly (cost savings, utilization, respect for donors, training value)?\n- Are ethics/respect-for-donors and compliance with cadaver lab certification addressed?\n- Does it outline feasible implementation steps: scheduling/coordination across departments, chain-of-custody, storage/thaw logistics, and governance/ownership?\nScoring:\n- 1.0: Strong benefits case with practical and ethical implementation plan.\n- 0.7: Good benefits and some implementation detail; minor gaps.\n- 0.4: Light on feasibility or ethics; mostly conceptual.\n- 0.0: Lacks credible feasibility or ethical considerations.", "expectation": "A persuasive, ethical, and feasible plan with concrete operational steps and safeguards."}, {"type": "llm_judge", "name": "Clarity and Accessibility", "description": "Assess clarity for a non-technical administrative audience and the use of plain language, definitions, and summaries.", "weight": 0.7, "judge_prompt": "Assess clarity and accessibility:\n- Uses plain language with defined terms (freeze/thaw cycles, complexity levels) and summaries.\n- Clear executive summary or key takeaways for quick understanding.\n- Visuals and tables are legible and described in text.\nScoring:\n- 0.7: Clear, concise, and accessible.\n- 0.5: Mostly clear; minor jargon or density.\n- 0.3: Somewhat hard to follow.\n- 0.0: Confusing or overly technical.", "expectation": "Clear, concise communication with defined terms and legible, well-explained visuals."}, {"type": "llm_judge", "name": "Actionability and Next Steps", "description": "Evaluate whether the document provides concrete next steps and ownership for moving forward.", "weight": 0.3, "judge_prompt": "Evaluate actionability:\n- Are next steps, pilot timeline, and responsible parties or points of contact clearly stated?\n- Are decision points and required approvals identified (e.g., department chairs, Anatomy Lab leadership, IRB/ethics if applicable)?\nScoring:\n- 0.3: Clear, actionable next steps with owners.\n- 0.2: Some next steps noted; partial ownership/timeline.\n- 0.1: Vague suggestions only.\n- 0.0: No actionable next steps.", "expectation": "A brief, concrete action plan with owners and a suggested timeline/pilot path."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "f9a1c16c-53fd-4c8f-88cc-5c325ec2f0bb", "rubric": {"category_name": "Touring Band Stage Plot (A/V and IEM Tech)", "rationale": "This rubric enforces a self-documenting, verifiable one-page stage plot PDF. Stage 1 (LLM-only) mandates the exact visual and structural shape needed for verification. Stage 2 mixes lightweight code checks (for numbered Input/Output lists, key channel presence, wedge assignments) with LLM verification of spatial placement and routing correctness. Code rules are kept simple and worth ~5x less than LLM rules to reflect their limited scope compared to nuanced diagram checks. Stage 3 assesses professional quality and venue-advance utility.", "max_total_score": 25.0, "stages": [{"name": "Stage 1: Shape Enforcement Gate", "description": "Gate that verifies the output is a one-page landscape PDF stage plot with required labeled structure and elements to enable verification.", "is_required": true, "max_points": 5.0, "min_score_to_pass": 3.5, "rules": [{"type": "llm_judge", "name": "Stage Plot Structure and Format Requirements", "description": "Verify the candidate produced a one-page, landscape PDF with the mandated structural elements and labeled components.", "weight": 5.0, "judge_prompt": "You are evaluating whether the submission is in the correct STRUCTURAL SHAPE for a touring band stage plot. Do NOT assess correctness of routing or mix content, only presence/format/structure. Be flexible with exact wording of labels, but the elements must be clearly present and visually identifiable.\n\nRequired Format:\n- Must be a single-page PDF (not DOCX/PNG/etc.).\n- Page orientation must be landscape.\n- The front of the stage must be at the bottom of the page and clearly indicated (text like \"Front of Stage\" or an arrow/label).\n\nRequired Visual Elements (icons/symbols acceptable):\n- Five performers shown on stage: Bass (stage right), Vox1 (stage right), Vox2 (stage left), Guitar (stage left), Drums (upstage/center). Positions don\u2019t have to be exact yet, but each role must be depicted.\n- Icons for: drum kit, wedges/monitors, microphones, DI boxes (for bass\u2019s accordion and acoustic guitar), IEM splits (for Vox1 and Vox2), and backline amps (bass amp on Stage Right, guitar amp on Stage Left).\n- Each band member\u2019s mic and wedge labeled with the corresponding title next to those items: \"Bass\", \"Vox1\", \"Vox2\", \"Guitar\", \"Drums\".\n\nRequired Lists at Top (side-by-side):\n- An \"Input List\" with numbered inputs (e.g., \"Input 1 - Vox1 Vocal\").\n- An \"Output List\" with numbered outputs (e.g., wedges and IEM splits with intended sends). This must include wedge numbering and IEM split lines. Wedges should be numbered counterclockwise from Stage Right (you only need to check that wedge numbers are present and the label implies Stage Right origin; spatial correctness is Stage 2).\n\nScoring (map to weight):\n- 5.0: Single-page PDF, landscape, front-of-stage indicated at bottom, both Input and Output lists at top side-by-side and numbered, all five roles depicted with labeled mic and wedge, and icons for amps, DI boxes, IEM splits, mics, drum kit, and wedges present.\n- 4.0: Minor omission (e.g., one icon type missing or lists not perfectly side-by-side) but overall structure complete and clearly a landscape one-page PDF with labels.\n- 3.5: Missing one major structural element (e.g., IEM split icons not shown, or only one of the two lists present) but still one-page landscape PDF with performers, labeled mics and wedges, and basic icons.\n- 2.0: PDF present but multiple structural requirements missing (e.g., not landscape, missing lists, unlabeled wedges/mics, or missing several required icons).\n- 0.0: Not a PDF OR clearly not a stage plot (no performers/gear), OR multiple pages.\n\nOnly evaluate presence/format/structure\u2014do not judge routing correctness, mix content, or exact placement details here.", "expectation": "A one-page landscape PDF stage plot with top-of-page side-by-side numbered Input/Output lists, labeled mics and wedges for each role, icons for amps/DI/IEM/mics/drums/wedges, and front-of-stage at the bottom."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Correctness Verification", "description": "Verify routing, spatial logic, and consistency using code checks for lists/labels and LLM for nuanced spatial/routing validation.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Input List Completeness (Key Onstage Inputs Present)", "description": "Check that the PDF Input List includes the required onstage inputs with flexible matching: Vox1 Vocal, Vox2 Vocal, Drums Vocal, Bass Speech Mic, Accordion DI, Acoustic Guitar DI.", "weight": 0.7, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns: float in [0, weight] or (score, feedback)\n    \"\"\"\n    weight = 0.7\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    # Require PDF per task spec\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        return 0.0\n    if not text:\n        return 0.0\n    t = text.lower()\n\n    # Extract candidate input lines to focus matching\n    lines = [ln.strip() for ln in t.splitlines() if 'input' in ln]\n\n    def has_line(pred):\n        for ln in lines:\n            if pred(ln):\n                return True\n        return False\n\n    checks = []\n    # Vox1 Vocal\n    checks.append(has_line(lambda s: ('input' in s) and ('vox1' in s or 'vox 1' in s or 'v1' in s) and ('vocal' in s or 'mic' in s)))\n    # Vox2 Vocal\n    checks.append(has_line(lambda s: ('input' in s) and ('vox2' in s or 'vox 2' in s or 'v2' in s) and ('vocal' in s or 'mic' in s)))\n    # Drums Vocal (drummer)\n    checks.append(has_line(lambda s: ('input' in s) and (('drum' in s) or ('drummer' in s)) and ('vocal' in s or 'mic' in s)))\n    # Bass Speech mic\n    checks.append(has_line(lambda s: ('input' in s) and ('bass' in s) and (('speech' in s) or ('talk' in s) or ('banter' in s)) and ('mic' in s)))\n    # Accordion DI\n    checks.append(has_line(lambda s: ('input' in s) and (('accordion' in s) or ('accord' in s)) and ('di' in s)))\n    # Acoustic Guitar DI\n    checks.append(has_line(lambda s: ('input' in s) and (('acoustic' in s) and ('guitar' in s)) and ('di' in s)))\n\n    score = (sum(checks) / 6.0) * weight\n    feedback = f\"Inputs found: {sum(checks)}/6\"\n    return score, feedback"}, {"type": "code", "name": "Output List Sequencing and IEM Split Presence", "description": "Ensure Output list numbering is contiguous starting at 1 and includes IEM split outputs for Vox1 and Vox2, plus wedge references.", "weight": 0.7, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 0.7\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        return 0.0\n    if not text:\n        return 0.0\n    t = text.lower()\n    lines = [ln.strip() for ln in t.splitlines() if 'output' in ln]\n\n    # Extract output numbers\n    nums = []\n    for ln in lines:\n        for m in re.finditer(r'output\\s*(\\d+)', ln):\n            try:\n                nums.append(int(m.group(1)))\n            except:\n                pass\n    unique_nums = sorted(set(nums))\n    contiguous = (len(unique_nums) > 0 and unique_nums == list(range(1, len(unique_nums)+1)))\n\n    # Presence checks\n    has_iem_vox1 = any(('iem' in ln and ('vox1' in ln or 'vox 1' in ln or 'v1' in ln)) for ln in lines)\n    has_iem_vox2 = any(('iem' in ln and ('vox2' in ln or 'vox 2' in ln or 'v2' in ln)) for ln in lines)\n    has_wedge_any = any(('wedge' in ln) for ln in lines)\n\n    # Scoring allocation: contiguity 0.4, IEM1 0.15, IEM2 0.15, wedge mention 0.0-0.1\n    score = 0.0\n    if contiguous:\n        score += 0.4\n    if has_iem_vox1:\n        score += 0.15\n    if has_iem_vox2:\n        score += 0.15\n    if has_wedge_any:\n        score += 0.1\n\n    return min(score, weight)"}, {"type": "code", "name": "Wedge Assignments Cover All Five Roles", "description": "Check that Output list assigns wedges for Bass, Vox1, Vox2, Guitar, and Drums with wedge numbers referenced.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 0.6\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        return 0.0\n    if not text:\n        return 0.0\n    t = text.lower()\n    lines = [ln.strip() for ln in t.splitlines() if 'output' in ln or 'wedge' in ln]\n\n    role_patterns = {\n        'bass': lambda s: 'bass' in s,\n        'vox1': lambda s: ('vox1' in s or 'vox 1' in s or 'v1' in s),\n        'vox2': lambda s: ('vox2' in s or 'vox 2' in s or 'v2' in s),\n        'guitar': lambda s: 'guitar' in s and 'acoustic' not in s,\n        'drums': lambda s: ('drum' in s or 'drummer' in s)\n    }\n\n    covered = set()\n    wedge_nums = set()\n    for ln in lines:\n        if 'wedge' in ln:\n            m = re.search(r'wedge\\s*(\\d+)', ln)\n            if m:\n                try:\n                    wedge_nums.add(int(m.group(1)))\n                except:\n                    pass\n        for role, pred in role_patterns.items():\n            if pred(ln):\n                if 'wedge' in ln:\n                    covered.add(role)\n\n    coverage_score = (len(covered) / 5.0) * 0.5  # up to 0.5\n    # Encourage ~5 distinct numbered wedges (flexible if >5 due to extras)\n    wedge_count = len([n for n in wedge_nums if isinstance(n, int)])\n    wedge_score = 0.1 if wedge_count >= 5 else (0.02 * wedge_count)\n\n    return min(coverage_score + wedge_score, weight)"}, {"type": "llm_judge", "name": "Routing Correctness and Role-Specific Needs", "description": "Validate that the diagram and Output List reflect correct routing: IEM XLR splits for Vox1/Vox2 to IEM and FOH, wedges for Vox1/Vox2, drummer wedge at ~10 o\u2019clock with both vocalists in send, guitar wedge includes guitar, bass wedge for bass fill, and bass speech mic present.", "weight": 3.5, "judge_prompt": "Evaluate routing correctness against the task requirements. Check the visual diagram and the written Output List together. Look for:\n- Two IEM XLR splits: one per vocalist (Vox1, Vox2), each clearly splitting to IEM and FOH.\n- Vocalists also have wedges in front of them (even though they use IEMs).\n- Drummer has a wedge placed diagonally in front (~10 o\u2019clock orientation noted or implied) and receives both vocalists in their monitor send.\n- Guitar position has a wedge with guitar in the mix.\n- Bass has a wedge primarily for bass fill.\n- A Bass speech mic is present in inputs (not necessarily routed to wedges unless specified) and shown on-stage.\nScore generously if these are clearly indicated via labels/legend and Output List sends.\nScoring (map to weight 3.5):\n- 3.5: All items correctly shown and unambiguous.\n- 2.5: One minor routing omission/ambiguity (e.g., not explicit about drummer receiving both vocals).\n- 1.5: Multiple omissions but IEM splits and most wedges are correct.\n- 0.5: Only a few elements correct (e.g., wedges present but no IEM split depiction).\n- 0.0: Routing largely incorrect or missing.", "expectation": "Clear, correctly labeled routing for IEM splits and wedges aligned to each role\u2019s needs."}, {"type": "llm_judge", "name": "Spatial Placement and Stage Orientation Accuracy", "description": "Verify front-of-stage at bottom, left/right orientation, and plausible placement of roles and backline as described.", "weight": 3.5, "judge_prompt": "Evaluate spatial/orientation accuracy:\n- Front of stage indicated at bottom; landscape orientation.\n- Vox1 located Stage Right, Vox2 Stage Left; they are flanked by Bass (SR) and Guitar (SL).\n- Drums are upstage/center (ride mentioned center downstage is okay as long as the drum kit is centered upstage).\n- Bass amp shown behind Bass on Stage Right; Guitar amp shown behind Guitar on Stage Left.\n- Drummer\u2019s wedge is placed diagonally in front (~10 o\u2019clock), not straight ahead.\n- DI boxes for accordion and acoustic are placed near the Bass position on SR.\nScoring (map to 3.5):\n- 3.5: All spatial elements correct and clearly labeled.\n- 2.5: One minor placement issue (e.g., wedges not angled but location plausible).\n- 1.5: Multiple spatial inaccuracies but overall orientation (front at bottom, SR/SL) is correct.\n- 0.5: Serious misplacements or unclear orientation.\n- 0.0: Orientation wrong or roles/amps in incorrect sides.", "expectation": "Diagram reflects correct SR/SL orientation, roles, wedges, DI positions, and backline placement."}, {"type": "llm_judge", "name": "Iconography and Labeling Completeness", "description": "Confirm that all required items appear with clear labels: 5 roles, 4 mics (2 vocalists + drums vocal + bass speech), 2 DI boxes, 2 amps, IEM split icons, wedges, drum kit; and that titles are adjacent to each role\u2019s mic and wedge.", "weight": 3.0, "judge_prompt": "Check the diagram for complete iconography and labels:\n- Roles: Bass, Vox1, Vox2, Guitar, Drums (all shown and labeled).\n- Microphones: Vox1 mic, Vox2 mic, Drums vocal mic, Bass speech mic (4 total) visibly indicated.\n- DI boxes: Accordion DI and Acoustic Guitar DI near the Bass position.\n- Backline: Bass amp on SR and Guitar amp on SL.\n- IEM split icons or explicit split indicators for both vocal mics.\n- Wedges for all five roles, each labeled with the role.\n- Top-of-page Input/Output lists exist and items correspond to depicted gear.\nScoring (map to 3.0):\n- 3.0: All items present and labeled; lists match the diagram.\n- 2.0: One to two items missing or mislabeled.\n- 1.0: Several items missing but basic set present.\n- 0.0: Many items missing or unlabeled.", "expectation": "All essential icons and labels present and consistent with Input/Output lists."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3: Presentation and Professional Quality", "description": "Holistic assessment of clarity, professionalism, and venue-advance usefulness.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Visual Clarity and Readability", "description": "Assess whether labels, numbering, and symbols are readable at typical print size; good contrast and uncluttered layout.", "weight": 2.0, "judge_prompt": "Evaluate visual clarity:\n- Labels and numbers are legible at normal print size (e.g., 8.5x11 or A4 landscape).\n- Adequate contrast and spacing; lines and icons do not overlap excessively.\n- Clear legends/keys if custom symbols are used.\nScoring (2.0 max): 2.0 excellent clarity; 1.0 acceptable with minor clutter; 0.5 borderline; 0.0 poor/illegible.", "expectation": "Clean, legible plot with readable labels and minimal clutter."}, {"type": "llm_judge", "name": "Professional Layout and Conventions", "description": "Check for professional polish: front-of-stage arrow/label, Stage Left/Right callouts, consistent numbering/labeling, neat alignment.", "weight": 2.0, "judge_prompt": "Assess professional layout:\n- Front-of-stage clearly labeled or arrowed; Stage Left/Right indicated.\n- Consistent numbering for Inputs/Outputs and wedge IDs.\n- Alignment and spacing look intentional and tidy.\n- File appears print-ready without needing edits.\nScoring (2.0 max): 2.0 fully professional; 1.5 minor issues; 1.0 noticeable roughness; 0.0 unprofessional.", "expectation": "Polished, consistent, print-ready layout using standard stage plot conventions."}, {"type": "llm_judge", "name": "Venue-Advance Usefulness", "description": "Judge whether the plot conveys enough practical information for venue crews to set positions, wedges, and signal splits without confusion.", "weight": 2.0, "judge_prompt": "Evaluate utility for advancing:\n- Clear positions for performers and wedges; obvious where DI boxes and amps go.\n- Input/Output lists clearly map to onstage items (minimal ambiguity).\n- Any essential notes (e.g., split to FOH, mix contents) are succinct and actionable.\nScoring (2.0 max): 2.0 highly actionable; 1.5 mostly clear; 1.0 some ambiguity; 0.0 confusing/insufficient.", "expectation": "Actionable diagram that a venue crew can set from without back-and-forth."}, {"type": "llm_judge", "name": "Accessibility and Labeling Consistency", "description": "Evaluate use of consistent labels, minimal jargon, and not relying on color alone to convey meaning.", "weight": 2.0, "judge_prompt": "Assess accessibility and consistency:\n- Consistent role names (Bass, Vox1, Vox2, Guitar, Drums) across diagram and lists.\n- Does not rely solely on color; uses shapes/labels/legends.\n- Minimal unexplained jargon.\nScoring (2.0 max): 2.0 strong; 1.5 minor inconsistencies; 1.0 several inconsistencies; 0.0 poor.", "expectation": "Consistent, accessible labeling that doesn\u2019t assume color printing or insider jargon."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "b39a5aa7-cd1b-47ad-b249-90afd22f8f21", "rubric": {"category_name": "Finance & Insurance \u2014 Financial Managers \u2014 CBA Compensation Cost & Projections Model", "rationale": "This rubric enforces a self-documenting, auditable Excel-based financial model for CBA-driven musician compensation. Stage 1 (LLM-only) mandates an exact workbook shape that enables verification. Stage 2 mixes code and LLM checks to verify correctness, cross-sheet consistency, and modeling logic, with code rules weighted ~5x less than LLM rules. Stage 3 evaluates professional quality, usability, and strategic value.", "max_total_score": 28.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (Structure-Only, LLM)", "description": "Mandate an Excel workbook with specific sheets, section names, and table structures enabling downstream verification. This is a hard gate; if structure is missing, evaluation stops.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Workbook Structure & Required Sections Present", "description": "Verify the output is an Excel workbook with the exact structure enabling verification. Do NOT check calculation correctness; only structure and presence.", "weight": 8.0, "judge_prompt": "You are validating the SHAPE ONLY of the candidate's output. Confirm it is an Excel workbook (.xlsx) with the following structure. Be flexible with closely-related sheet names (e.g., 'Inputs', 'Assumptions', 'Inputs & Assumptions' treated as the same), but the intended content must be clearly present and visible as tables/sections in the workbook.\n\nFORMAT REQUIREMENTS:\n- File type: Excel (.xlsx)\n- Workbook includes at least 4 sheets with the following structure:\n\n1) Sheet: 'Inputs & Assumptions' (or similar: 'Inputs', 'Assumptions')\n   Must contain clearly labeled sections:\n   A. CBA Parameters / Drivers:\n      - Table columns: [Parameter or Driver | Current Year | Year+1 | Year+2 | Unit/Notes]\n      - Contains compensation drivers and negotiated terms (e.g., base rate, overtime rate, per-service fee, rehearsal multipliers, premium differentials, minimum guarantees, benefits rate, payroll taxes rate, etc.)\n   B. Compensation Types Mapping:\n      - Table columns: [Compensation Type | Rate/Basis | Included? | Notes]\n      - Compensation types must be explicitly listed and align with the summary/projections (e.g., Base, Overtime, Rehearsal, Performance/Service Fees, Premiums/Differentials, Benefits/Payroll Taxes, Other).\n\n2) Sheet: 'Current Year Summary' (or similar: 'CY Summary', 'Current-Year Summary')\n   - Table of compensation expense by type and quarter for the current calendar year.\n   - Columns: [Compensation Type | Q1 | Q2 | Q3 | Q4 | Total]\n   - Must present numeric values by quarter and a total column.\n\n3) Sheet: 'Projections' (or similar: 'Forecast', 'Projection Summary')\n   - Quarter-by-quarter projection tables for the next two years (Year+1 and Year+2) using the same compensation types as the current year.\n   - Must include Year-over-Year growth (YoY) vs the prior year, shown by quarter and/or total, clearly labeled as 'YoY' or 'Year-over-Year'.\n   - Acceptable structures:\n     a) Separate blocks per year with an adjacent YoY block, or\n     b) Columns for Q1\u2013Q4 and Total for each year plus a YoY Growth column/section.\n\n4) Sheet: 'Calculations' (or similar: 'Calc', 'Detail Calcs', 'Model Logic')\n   - Shows the calculation logic (line-by-line) that drives the summary and projections, with references to inputs and (if included) roster.\n   - Must present intermediate computations (e.g., headcount/service counts \u00d7 rates, premiums, benefits, payroll taxes) that reconcile to the summarized tables.\n\nOptional but beneficial:\n- Sheet: 'Roster' (or similar: 'Headcount', 'Musicians Roster'): a table with roster-level details (e.g., Employee/ID, Role/Instrument, Status, Base Rate, etc.)\n- Sheet: 'Read Me' or 'Instructions' with usage notes and color-coding conventions for inputs vs outputs.\n\nSCORING (Structure only):\n- 1.0: Valid .xlsx AND all 4 required sheets present with the specified structures and tables (flexible naming allowed), including: Inputs with 3-year drivers, Current Year Summary by quarter, Projections for two future years with YoY, and a Calculations sheet with visible intermediate steps.\n- 0.8: Valid .xlsx; all required sheets present but one required table/section is incomplete or merged ambiguously (e.g., YoY present but not clearly labeled, or Inputs missing Year+2 column).\n- 0.6: Valid .xlsx; 3 of 4 required sheets/sections clearly present and properly structured.\n- 0.3: Valid .xlsx; only 1\u20132 required sheets/sections present or tables are not clearly structured.\n- 0.0: Not an Excel file OR workbook missing multiple required sheets/sections such that verification is not possible.\n\nOnly evaluate presence/structure. Do not assess correctness or quality.", "expectation": "A verifiable Excel model with Inputs, Current Year Summary, Projections with YoY, and a Calculations tab that exposes the logic."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness & Verification (Mixed: Code-light + LLM)", "description": "Now that the workbook shape is enforced, verify quantitative consistency, YoY presence and plausibility, cross-sheet alignment, and calculation traceability. Code rules perform deterministic checks; LLM judges assess nuanced consistency and logic. Code rules total weight is ~5x less than LLM rules.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Quarterly Summary Structure & Totals Consistency", "description": "Check that the Current Year Summary has Q1\u2013Q4 columns, and if a Total column exists, it roughly equals the sum of quarters for most rows.", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef _find_sheet_name(sheet_names, keywords):\n    keys = [k.lower() for k in keywords]\n    for s in sheet_names:\n        sl = s.lower()\n        if any(k in sl for k in keys):\n            return s\n    return None\n\ndef _read_with_header_guess(path, sheet_name):\n    # Try default header=0 first\n    try:\n        df = pd.read_excel(path, sheet_name=sheet_name, header=0)\n        return df\n    except Exception:\n        pass\n    # Fallback: read without header then detect header row\n    df = pd.read_excel(path, sheet_name=sheet_name, header=None)\n    # Look for a row containing at least 3 of q1..q4\n    for r in range(min(10, len(df))):\n        row_vals = [str(v).strip().lower() for v in list(df.iloc[r, :].values)]\n        hits = sum(v in [\"q1\",\"q2\",\"q3\",\"q4\"] or re.search(r\"q[1-4]\", v or \"\") for v in row_vals)\n        if hits >= 3:\n            df2 = pd.read_excel(path, sheet_name=sheet_name, header=r)\n            return df2\n    # If still not found, return original\n    return df\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output\"\n    path = context.files.get_path(output.id)\n    try:\n        xls = pd.ExcelFile(path)\n    except Exception as e:\n        return 0.0, f\"Cannot open Excel: {e}\"\n    # Find a summary sheet\n    summ_sheet = _find_sheet_name(xls.sheet_names, [\"current year\", \"summary\", \"cy summary\", \"current-year\"]) or \\\n                 _find_sheet_name(xls.sheet_names, [\"summary\"]) or \\\n                 (xls.sheet_names[0] if xls.sheet_names else None)\n    if not summ_sheet:\n        return 0.0, \"Summary sheet not found\"\n    try:\n        df = _read_with_header_guess(path, summ_sheet)\n    except Exception as e:\n        return 0.0, f\"Failed to read summary: {e}\"\n    # Normalize headers\n    cols = [str(c).strip().lower() for c in df.columns]\n    # Find quarter columns\n    q_cols_idx = [i for i,c in enumerate(cols) if re.fullmatch(r\"q[1-4]\", c) or c in [\"q1\",\"q2\",\"q3\",\"q4\"]]\n    q_cols = [df.columns[i] for i in q_cols_idx]\n    has_quarters = len(q_cols) >= 3  # allow missing one but expect 4 ideally\n    # Find total column if present\n    total_idx = None\n    for i,c in enumerate(cols):\n        if \"total\" in c:\n            total_idx = i\n            break\n    # Compute consistency\n    score = 0.0\n    feedback = []\n    if not has_quarters:\n        feedback.append(\"Quarter columns Q1-Q4 not clearly found\")\n        return 0.0, \"; \".join(feedback)\n    feedback.append(f\"Found quarter columns: {q_cols}\")\n    if total_idx is None:\n        # Give partial credit for quarter structure\n        score = 0.5\n        feedback.append(\"No Total column detected; partial credit for quarter structure\")\n        return score, \"; \".join(feedback)\n    total_col = df.columns[total_idx]\n    # Compare sums vs total\n    valid_rows = 0\n    match_rows = 0\n    for _, row in df.iterrows():\n        try:\n            vals = [pd.to_numeric(row[c], errors='coerce') for c in q_cols]\n            if sum(v is not None and not pd.isna(v) for v in vals) >= 3:\n                s = np.nansum(vals)\n                t = pd.to_numeric(row[total_col], errors='coerce')\n                if pd.isna(t):\n                    continue\n                valid_rows += 1\n                tol = max(1.0, 0.01 * (abs(s) + 1.0))\n                if abs((t or 0) - s) <= tol:\n                    match_rows += 1\n        except Exception:\n            continue\n    if valid_rows == 0:\n        feedback.append(\"No numeric rows to compare totals\")\n        return 0.5, \"; \".join(feedback)\n    match_ratio = match_rows / valid_rows\n    # Base 0.5 for having quarters + up to 0.5 for consistency\n    score = 0.5 + 0.5 * match_ratio\n    feedback.append(f\"Total check rows={valid_rows}, match={match_rows} ({match_ratio:.0%})\")\n    return float(score), \"; \".join(feedback)\n"}, {"type": "code", "name": "Projections Include YoY Growth with Plausible Ranges", "description": "Detect a Projections/Forecast sheet with YoY values and check that a reasonable number of YoY entries fall within plausible percent ranges (-100% to +300%).", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef _find_sheet_name(sheet_names, keywords):\n    keys = [k.lower() for k in keywords]\n    for s in sheet_names:\n        sl = s.lower()\n        if any(k in sl for k in keys):\n            return s\n    return None\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output\"\n    path = context.files.get_path(output.id)\n    try:\n        xls = pd.ExcelFile(path)\n    except Exception as e:\n        return 0.0, f\"Cannot open Excel: {e}\"\n    proj_sheet = _find_sheet_name(xls.sheet_names, [\"projection\", \"forecast\", \"projections\", \"projection summary\", \"pro forma\"]) \n    if not proj_sheet:\n        return 0.0, \"Projections sheet not found\"\n    try:\n        df = pd.read_excel(path, sheet_name=proj_sheet, header=0)\n    except Exception as e:\n        return 0.0, f\"Cannot read projections: {e}\"\n    # Find columns that appear to be YoY\n    cols = [str(c).lower() for c in df.columns]\n    yoy_cols_idx = [i for i,c in enumerate(cols) if (\"yoy\" in c) or (\"year-over\" in c) or (\"y/y\" in c)]\n    yoy_vals = []\n    if yoy_cols_idx:\n        for i in yoy_cols_idx:\n            col = df.columns[i]\n            vals = pd.to_numeric(df[col], errors='coerce')\n            yoy_vals.extend(list(vals.dropna().values))\n    else:\n        # Try to detect rows labeled YoY (e.g., a left-most label column)\n        label_col = df.columns[0]\n        label_series = df[label_col].astype(str).str.lower()\n        yoy_rows = df[label_series.str.contains(\"yoy|year-over|y/y\", regex=True, na=False)]\n        if not yoy_rows.empty:\n            for c in df.columns[1:]:\n                vals = pd.to_numeric(yoy_rows[c], errors='coerce')\n                yoy_vals.extend(list(vals.dropna().values))\n    if len(yoy_vals) == 0:\n        return 0.0, \"No YoY values detected\"\n    yoy_vals = np.array(yoy_vals, dtype=float)\n    # Treat percentages formatted as 15% => 0.15 as valid\n    plausible = (yoy_vals >= -1.0) & (yoy_vals <= 3.0)\n    plausible_ratio = float(np.mean(plausible)) if len(yoy_vals) > 0 else 0.0\n    # Heuristic for having at least 4 quarterly YoY entries\n    coverage = 1.0 if len(yoy_vals) >= 4 else len(yoy_vals)/4.0\n    score = 0.5 * plausible_ratio + 0.5 * coverage\n    return float(score), f\"YoY count={len(yoy_vals)}, plausible={plausible_ratio:.0%}, coverage={coverage:.0%}\"\n"}, {"type": "code", "name": "Non-negative and Realistic Magnitudes", "description": "Check most numeric compensation values are non-negative and within a realistic magnitude across Summary and Projections.", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef _find_sheet_name(sheet_names, keywords):\n    keys = [k.lower() for k in keywords]\n    for s in sheet_names:\n        sl = s.lower()\n        if any(k in sl for k in keys):\n            return s\n    return None\n\ndef _collect_numeric(df):\n    vals = []\n    for c in df.columns:\n        s = pd.to_numeric(df[c], errors='coerce')\n        vals.extend(list(s.dropna().values))\n    if len(vals) == 0:\n        return np.array([])\n    return np.array(vals, dtype=float)\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output\"\n    path = context.files.get_path(output.id)\n    try:\n        xls = pd.ExcelFile(path)\n    except Exception as e:\n        return 0.0, f\"Cannot open Excel: {e}\"\n    # Try to read summary and projections\n    sheets = xls.sheet_names\n    summ = _find_sheet_name(sheets, [\"current year\", \"summary\", \"cy summary\"]) or _find_sheet_name(sheets, [\"summary\"]) \n    proj = _find_sheet_name(sheets, [\"projection\", \"forecast\", \"projections\"]) \n    vals = []\n    try:\n        if summ:\n            df1 = pd.read_excel(path, sheet_name=summ, header=0)\n            vals.extend(list(_collect_numeric(df1)))\n        if proj:\n            df2 = pd.read_excel(path, sheet_name=proj, header=0)\n            vals.extend(list(_collect_numeric(df2)))\n    except Exception:\n        pass\n    vals = np.array(vals, dtype=float) if len(vals)>0 else np.array([])\n    if vals.size == 0:\n        return 0.0, \"No numeric values found\"\n    nonneg_ratio = float(np.mean(vals >= -1e-6))\n    realistic_ratio = float(np.mean(np.abs(vals) <= 1e8))  # <= $100M\n    score = 0.5 * nonneg_ratio + 0.5 * realistic_ratio\n    return float(score), f\"Non-negative={nonneg_ratio:.0%}, Realistic={realistic_ratio:.0%}, N={vals.size}\"\n"}, {"type": "llm_judge", "name": "Compensation Types Align Across Inputs, Summary, and Projections", "description": "Check whether compensation type categories are consistent across Inputs & Assumptions, Current Year Summary, and Projections. Names can be flexible but should clearly align and reconcile.", "weight": 3.5, "judge_prompt": "Evaluate if compensation types/categories are consistent across the model:\n- The Inputs & Assumptions sheet lists compensation types and rates/bases.\n- The Current Year Summary table uses the same (or clearly mapped) types by quarter.\n- The Projections use the same categories for Year+1 and Year+2.\n\nEvidence you should look for:\n- Matching or clearly mapped type names (e.g., 'Base Pay' vs 'Base').\n- No unexplained extra or missing categories between sheets.\n- Totals per category appear to be derived from those types referenced in Inputs.\n\nScoring:\n- 1.0: Clear one-to-one alignment/mapping across all three areas with no unexplained gaps.\n- 0.7: Minor naming variances but mapping is still clear; no material gaps.\n- 0.4: Some categories do not map cleanly or appear missing/inconsistent.\n- 0.0: Categories are largely inconsistent or untraceable across sheets.", "expectation": "Consistent category taxonomy throughout the workbook enabling reconciliation."}, {"type": "llm_judge", "name": "Calculation Traceability and Transparency", "description": "Assess whether the Calculations sheet shows step-by-step logic that clearly ties Inputs/roster drivers to summary/projection outputs.", "weight": 3.5, "judge_prompt": "Examine the Calculations sheet for traceability:\n- Are intermediate steps present (e.g., headcount \u00d7 services \u00d7 rates, premiums/differentials, benefits and payroll tax applications)?\n- Do calculations reference or clearly depend on Inputs & Assumptions (and Roster, if present)?\n- Can a reviewer follow the flow from drivers to summarized totals and projections?\n\nScoring:\n- 1.0: Transparent line-by-line logic with clear references/labels that reconcile to summary and projections.\n- 0.7: Mostly clear with minor gaps; reconciliation is apparent but not fully explicit for all lines.\n- 0.4: Some logic is opaque; difficult to connect to outputs.\n- 0.0: No meaningful calculation trail.", "expectation": "A clear audit trail showing how inputs drive outputs, with intermediate computations visible."}, {"type": "llm_judge", "name": "Scenario Inputs and YoY Presentation", "description": "Evaluate if scenario inputs allow ad hoc analysis for next two years and whether YoY growth by quarter (and/or total) is clearly shown.", "weight": 3.5, "judge_prompt": "Review Inputs & Assumptions and Projections for scenario analysis readiness:\n- Inputs include adjustable drivers for Current Year, Year+1, Year+2 (e.g., rates, multipliers, benefits/tax rates, service counts, COLA).\n- Projections show by-quarter results for the next two years.\n- YoY growth vs prior year is clearly labeled and visible (quarterly and/or total).\n- Any user guidance (e.g., color coding for input cells, notes) is present.\n\nScoring:\n- 1.0: Inputs comprehensively cover drivers for 3 years; projections by quarter for two future years with clear YoY; user can perform ad hoc changes.\n- 0.7: Inputs mostly sufficient; projections present; YoY shown but not uniformly; minor usability gaps.\n- 0.4: Limited inputs or unclear YoY; scenario changes would be hard to test.\n- 0.0: No practical scenario analysis capability.", "expectation": "Well-structured inputs enabling easy scenario testing and clear YoY visibility."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality & Professionalism (LLM)", "description": "Holistic assessment of modeling craft, presentation, usability, and decision-support value for a finance leader.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Modeling Practices & Formatting", "description": "Evaluate layout, formatting, readability, and standard modeling conventions.", "weight": 2.0, "judge_prompt": "Assess professionalism:\n- Consistent formatting, clear headers, frozen panes, readable fonts.\n- Input cells visually distinct (e.g., color-coded) from outputs.\n- Units/currency clearly indicated; accounting formats used for dollar values.\n- Minimal hardcoding in outputs; calculations centralized.\n\nScore 1.0 = excellent professional finish; 0.7 = good with minor issues; 0.4 = uneven/rough; 0.0 = poor formatting impairs use.", "expectation": "Clean, professional workbook with clear separation of inputs, calcs, outputs."}, {"type": "llm_judge", "name": "Clarity, Documentation, and Reviewer Guidance", "description": "Check for instructions, notes, and assumptions documentation that make the model self-explanatory.", "weight": 2.0, "judge_prompt": "Look for a 'Read Me' or notes/instructions on how to use the model:\n- Purpose, how to update inputs, definitions of drivers/assumptions.\n- Change log or version/date (optional but good practice).\n- Any warnings/limitations documented.\n\nScore 1.0 = clear instructions and documentation; 0.7 = some guidance present; 0.4 = minimal hints; 0.0 = none.", "expectation": "A reviewer can understand and operate the model without hand-holding."}, {"type": "llm_judge", "name": "Decision-Support Value and Insights", "description": "Evaluate whether the model surfaces useful insights for management (e.g., key drivers, totals, growth).", "weight": 2.0, "judge_prompt": "Consider if the workbook helps a finance leader quickly grasp the cost impact and drivers:\n- Clear total compensation by quarter and year; highlights key categories.\n- Visible growth metrics and drivers.\n- Optional summary/dashboard elements (charts, highlights) if present.\n\nScore 1.0 = high decision-support value; 0.7 = helpful; 0.4 = limited; 0.0 = minimal.", "expectation": "Concise, useful summaries that aid decisions."}, {"type": "llm_judge", "name": "Robustness and Auditability", "description": "Assess whether the model seems robust to changes and is easy to audit.", "weight": 2.0, "judge_prompt": "Judge robustness and auditability based on presentation and structure:\n- Inputs clearly drive outputs; minimal circularity or fragile links.\n- Calculations avoid opaque steps; rows/sections are labeled and traceable.\n- Cross-references (e.g., to Inputs/Roster) are understandable.\n\nScore 1.0 = robust and auditable; 0.7 = generally sound; 0.4 = some fragility; 0.0 = fragile/opaque.", "expectation": "A resilient, auditable model suitable for recurring use."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "19403010-3e5c-494e-a6d3-13594e99f6af", "rubric": {"category_name": "Wholesale Sales Analysis \u2013 XR Retailer Makeup 2023 Recap", "rationale": "Analytical task (Pattern A) producing a single-sheet Excel recap with structured sections and calculable fields. Stage 1 strictly enforces the required spreadsheet shape so verification is possible. Stage 2 mixes lightweight code checks (numeric/structural consistency) with LLM cross-checks (sorting logic, definitions, totals). Stage 3 holistically assesses professional quality and executive usefulness. Code checks are weighted substantially less than LLM judgments, per guidance.", "max_total_score": 21.0, "stages": [{"name": "Stage 1 \u2013 Shape Enforcement Gate", "description": "Verify the candidate produced a single-sheet Excel recap with the exact structural elements required to enable verification. LLM-only gate, flexible with synonyms but strict on presence/structure.", "is_required": true, "max_points": 5.0, "min_score_to_pass": 3.5, "rules": [{"type": "llm_judge", "name": "Structured One-Page Excel Recap Present (Exact Sections and Tables)", "description": "Confirms the output is an Excel file with one visible sheet containing all five sections with specified tables/columns and totals rows where required.", "weight": 5.0, "judge_prompt": "You are evaluating whether the candidate delivered the required Excel recap in the exact verifiable shape. You can see the rendered spreadsheet.\n\nFORMAT REQUIREMENTS:\n- Must be a spreadsheet (Excel). Ideally one visible sheet only, designed to fit on one printed page.\n- The sheet/tab title or prominent on-sheet header should read close to: \"XR Retailer 2023 Sales Performance Analysis Makeup Category Final\" (flexible on minor wording/case; must include XR Retailer, 2023, Makeup Category, and clear analysis title).\n\nREQUIRED SECTIONS (use flexible matching on headers, but all content must be visible as tables/metrics):\n1) Section 1: \"OVERALL BUSINESS\" (or similar). A compact metrics area showing all four items:\n   - Sales Dollars TY (2023)\n   - Sales Dollars LY (2022)\n   - % Change Sales Dollars (2023 vs 2022)\n   - $ Change Sales Dollars (2023 vs 2022)\n\n2) Section 2: \"Discontinued SKUs \u2013 Risk to 2024 Business\" (or similar). Must show:\n   - Total Sales $ of Ongoing SKUs (Material Status codes 05 or 06)\n   - Total Sales $ of Discontinued SKUs (Material Status codes 07 or 08)\n   - % of Sales (discos) = Discontinued Sales $ / Total Sales Dollars TY (2023)\n   The definition of ongoing vs discontinued using the specified material status codes must be visible via labels/notes.\n\n3) Section 3: \"Top Volume Drivers\" table with exactly three function rows plus a Total row. Columns must align to these nine logical columns (allow close synonyms):\n   - Function\n   - XR Sales Dollars 2023 (TY)\n   - XR Sales Dollars 2022 (LY)\n   - Sales Dollars $ Change (TY vs LY)\n   - Sales Dollars % Change (TY vs LY)\n   - % to Total Business 2023\n   - % to Total Business 2022\n   - $ DISCO (2023 sales for material status 07/08)\n   - % DISCO ($ DISCO / 2023 sales for that function)\n\n4) Section 4: \"Largest Volume Increases\" table with the same nine columns, exactly three function rows plus a Total row.\n\n5) Section 5: \"Largest Volume Detractors\" table with the same nine columns, exactly three function rows plus a Total row.\n\nSCORING (assess only presence/structure, not correctness):\n- 5.0: Spreadsheet present; single-sheet one-page layout; clear overall title; all 5 sections present; sections 3\u20135 each have the nine-column tables with exactly 3 rows + Total; Section 1 has all four metrics; Section 2 has the three disco metrics and the status-code definition.\n- 4.0: Spreadsheet present; minor title deviations; all 5 sections present; sections 3\u20135 tables present with correct column set and 3 rows + Total, OR one table has a minor column name deviation; Section 2\u2019s definition is implied but clear.\n- 3.0: Spreadsheet present; missing exactly one required element (e.g., one of the five sections, or a totals row in one table, or one of the four metrics in Section 1).\n- 2.0: Spreadsheet present; missing two required elements.\n- 1.0: Spreadsheet present but multiple major elements missing (e.g., fewer than three of the five sections present) OR more than one sheet without clear indication of the single-page recap.\n- 0.0: Not a spreadsheet or no recognizable required sections.\n\nOnly evaluate structure and visibility, not the numerical correctness.", "expectation": "A single-sheet Excel recap with the five required sections, the specified metrics and tables, and totals rows in Sections 3\u20135."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification (Calculations and Logic)", "description": "Now that the shape is enforced, verify correctness and internal consistency. Mix of deterministic code checks and LLM judgment. Code rules are light and fuzzy to avoid brittleness; LLM checks do deeper reasoning (sorting logic, totals, status-code interpretation).", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Functions Table Present (Fuzzy Header Detection)", "description": "Detect at least one of the Sections 3\u20135 nine-column tables by scanning for a header row containing 'Function' and multiple expected column signals (2023/2022, change, % to total, disco).", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output\"\n    try:\n        raw = context.files.read_excel(output.id, sheet_name=0, header=None)\n    except Exception as e:\n        return 0.0, f\"Read error: {e}\"\n    def norm(s):\n        s = str(s).strip().lower()\n        s = re.sub(r\"\\s+\", \" \", s)\n        return s\n    header_row_idx = None\n    # Look for a row that looks like the 9-col functions header\n    for i in range(min(len(raw), 300)):\n        row = [norm(v) for v in raw.iloc[i].tolist()]\n        joined = \" | \".join(row)\n        signals = 0\n        if any(\"function\" in c for c in row):\n            signals += 1\n        if (\"2023\" in joined) and (\"2022\" in joined):\n            signals += 1\n        if \"disco\" in joined:\n            signals += 1\n        if \"% to total\" in joined or (\"%\" in joined and \"total\" in joined):\n            signals += 1\n        if \"change\" in joined or \"chg\" in joined:\n            signals += 1\n        if \"% change\" in joined or \"% chg\" in joined:\n            signals += 1\n        if signals >= 3:\n            header_row_idx = i\n            break\n    if header_row_idx is None:\n        return 0.0, \"No functions header detected\"\n    return 1.0, f\"Header detected at row {header_row_idx}\""}, {"type": "code", "name": "Top-3 + Total Row Structure", "description": "From the detected functions header, verify there are at least 3 data rows and a Total row within the block.", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        raw = context.files.read_excel(output.id, sheet_name=0, header=None)\n    except Exception:\n        return 0.0\n    def norm(s):\n        s = str(s).strip().lower()\n        s = re.sub(r\"\\s+\", \" \", s)\n        return s\n    header_row = None\n    for i in range(min(len(raw), 300)):\n        row = [norm(v) for v in raw.iloc[i].tolist()]\n        joined = \"|\".join(row)\n        signals = 0\n        if any(\"function\" in c for c in row):\n            signals += 1\n        if (\"2023\" in joined) and (\"2022\" in joined):\n            signals += 1\n        if \"disco\" in joined:\n            signals += 1\n        if \"% to total\" in joined or (\"%\" in joined and \"total\" in joined):\n            signals += 1\n        if signals >= 3:\n            header_row = i\n            break\n    if header_row is None:\n        return 0.0\n    # Count data rows until a fully blank row\n    data_rows = 0\n    has_total = False\n    for r in range(header_row+1, min(len(raw), header_row+1+50)):\n        row_vals = raw.iloc[r].tolist()\n        if all((pd.isna(v) or str(v).strip()==\"\") for v in row_vals):\n            break\n        first_nonnull = None\n        for v in row_vals:\n            if not (pd.isna(v) or str(v).strip()==\"\"):\n                first_nonnull = str(v)\n                break\n        if first_nonnull is None:\n            break\n        if re.search(r\"total\", str(first_nonnull), flags=re.I):\n            has_total = True\n        else:\n            data_rows += 1\n    score = 0.0\n    if data_rows >= 3:\n        score += 0.5\n    if has_total:\n        score += 0.5\n    return score/1.0"}, {"type": "code", "name": "Row-Level Math Consistency (TY, LY, $ Change, % Change, Disco %)", "description": "Spot-check up to 3 function rows for internal math: TY - LY \u2248 $ Change; % Change \u2248 $ Change / LY; % DISCO \u2248 $ DISCO / TY (with tolerance and flexible percent scaling).", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        raw = context.files.read_excel(output.id, sheet_name=0, header=None)\n    except Exception:\n        return 0.0\n    def norm(s):\n        s = str(s).strip().lower()\n        s = re.sub(r\"\\s+\", \" \", s)\n        return s\n    def parse_num(x):\n        if x is None or (isinstance(x, float) and pd.isna(x)):\n            return None\n        if isinstance(x, (int, float)):\n            return float(x)\n        s = str(x)\n        if s.strip()==\"\":\n            return None\n        try:\n            s2 = re.sub(r\"[,$]\", \"\", s)\n            s2 = s2.replace(\"(\", \"-\").replace(\")\", \"\")\n            return float(s2)\n        except Exception:\n            m = re.search(r\"-?\\d+[\\d,]*(?:\\.\\d+)?\", s)\n            if m:\n                try:\n                    return float(m.group(0).replace(\",\",\"\"))\n                except:\n                    return None\n            return None\n    def parse_pct(x):\n        v = parse_num(x)\n        if v is None:\n            return None\n        # detect if already 0-1 vs 0-100\n        s = str(x)\n        if \"%\" in s:\n            return v/100.0\n        # heuristic scaling\n        if v>1.5:\n            return v/100.0\n        return v\n    # Find header row for functions table\n    header_row = None\n    header_map = {}\n    for i in range(min(len(raw), 300)):\n        row = [norm(v) for v in raw.iloc[i].tolist()]\n        if any(\"function\" in c for c in row) and (\"2023\" in \"|\".join(row)) and (\"2022\" in \"|\".join(row)):\n            header_row = i\n            break\n    if header_row is None:\n        return 0.0\n    # Build column index mapping via fuzzy matching\n    headers = [norm(v) for v in raw.iloc[header_row].tolist()]\n    col_idx = {k: None for k in [\"function\",\"ty\",\"ly\",\"chg_d\",\"chg_p\",\"disco_d\",\"disco_p\"]}\n    for j, h in enumerate(headers):\n        if h==\"\" or h==\"nan\":\n            continue\n        if \"function\" in h:\n            col_idx[\"function\"] = col_idx[\"function\"] or j\n        if (\"sales\" in h or \"xr\" in h) and \"2023\" in h:\n            col_idx[\"ty\"] = col_idx[\"ty\"] or j\n        if (\"sales\" in h or \"xr\" in h) and (\"2022\" in h or \"ly\" in h):\n            col_idx[\"ly\"] = col_idx[\"ly\"] or j\n        if (\"change\" in h or \"chg\" in h) and (\"$\" in h or \"dollar\" in h):\n            col_idx[\"chg_d\"] = col_idx[\"chg_d\"] or j\n        if (\"change\" in h or \"chg\" in h) and (\"%\" in h or \"percent\" in h):\n            col_idx[\"chg_p\"] = col_idx[\"chg_p\"] or j\n        if (\"disco\" in h) and (\"$\" in h or \"dollar\" in h):\n            col_idx[\"disco_d\"] = col_idx[\"disco_d\"] or j\n        if (\"disco\" in h) and (\"%\" in h or \"percent\" in h):\n            col_idx[\"disco_p\"] = col_idx[\"disco_p\"] or j\n    needed = [\"ty\",\"ly\",\"chg_d\",\"chg_p\",\"disco_d\",\"disco_p\"]\n    if sum(col_idx[k] is not None for k in needed) < 4:\n        return 0.0\n    # Iterate up to 3 rows\n    max_rows = 3\n    checked = 0\n    passes = 0\n    for r in range(header_row+1, min(len(raw), header_row+1+20)):\n        row = raw.iloc[r]\n        # stop at blank line or total row\n        if all((pd.isna(v) or str(v).strip()==\"\") for v in row.tolist()):\n            break\n        first_cell = str(row.iloc[col_idx[\"function\"]]) if col_idx[\"function\"] is not None else str(row.iloc[0])\n        if re.search(r\"total\", str(first_cell), flags=re.I):\n            continue\n        ty = parse_num(row.iloc[col_idx[\"ty\"]]) if col_idx[\"ty\"] is not None else None\n        ly = parse_num(row.iloc[col_idx[\"ly\"]]) if col_idx[\"ly\"] is not None else None\n        chg_d = parse_num(row.iloc[col_idx[\"chg_d\"]]) if col_idx[\"chg_d\"] is not None else None\n        chg_p = parse_pct(row.iloc[col_idx[\"chg_p\"]]) if col_idx[\"chg_p\"] is not None else None\n        dd = parse_num(row.iloc[col_idx[\"disco_d\"]]) if col_idx[\"disco_d\"] is not None else None\n        dp = parse_pct(row.iloc[col_idx[\"disco_p\"]]) if col_idx[\"disco_p\"] is not None else None\n        ok = 0\n        tot = 0\n        # TY - LY \u2248 $ Change\n        if ty is not None and ly is not None and chg_d is not None:\n            tot += 1\n            if abs((ty-ly) - chg_d) <= max(0.01*max(abs(ty),1.0), 1e-6):\n                ok += 1\n        # % Change \u2248 $ Change / LY\n        if chg_d is not None and ly not in (None, 0) and chg_p is not None:\n            tot += 1\n            calc = chg_d/ly if ly else None\n            if calc is not None and abs(calc - chg_p) <= max(0.02, 0.02*abs(calc)):\n                ok += 1\n        # % DISCO \u2248 $ DISCO / TY\n        if dd is not None and ty not in (None, 0) and dp is not None:\n            tot += 1\n            calc2 = dd/ty if ty else None\n            if calc2 is not None and abs(calc2 - dp) <= max(0.02, 0.02*abs(calc2)):\n                ok += 1\n        if tot>0:\n            checked += 1\n            passes += ok/float(tot)\n        if checked >= max_rows:\n            break\n    if checked==0:\n        return 0.0\n    return max(0.0, min(1.0, passes/checked))"}, {"type": "code", "name": "% to Total Plausibility", "description": "Within a detected functions table, check that the sum of the '% to total business 2023' across the three rows (excluding Total) is plausible (<=100% after flexible scaling).", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        raw = context.files.read_excel(output.id, sheet_name=0, header=None)\n    except Exception:\n        return 0.0\n    def norm(s):\n        s = str(s).strip().lower()\n        s = re.sub(r\"\\s+\", \" \", s)\n        return s\n    def parse_num(x):\n        if x is None or (isinstance(x, float) and pd.isna(x)):\n            return None\n        if isinstance(x, (int, float)):\n            return float(x)\n        s = str(x)\n        if s.strip()==\"\":\n            return None\n        s2 = re.sub(r\"[,$%]\", \"\", s)\n        s2 = s2.replace(\"(\", \"-\").replace(\")\", \"\")\n        try:\n            return float(s2)\n        except:\n            return None\n    # Find header row\n    header_row = None\n    for i in range(min(len(raw), 300)):\n        row = [norm(v) for v in raw.iloc[i].tolist()]\n        if any(\"function\" in c for c in row) and (\"% to total\" in \"|\".join(row) or (\"%\" in \"|\".join(row) and \"total\" in \"|\".join(row))):\n            header_row = i\n            break\n    if header_row is None:\n        return 0.0\n    headers = [norm(v) for v in raw.iloc[header_row].tolist()]\n    share_col = None\n    func_col = None\n    for j, h in enumerate(headers):\n        if share_col is None and (\"% to total\" in h and \"2023\" in h) or (\"2023\" in h and \"%\" in h and \"total\" in h):\n            share_col = j\n        if func_col is None and \"function\" in h:\n            func_col = j\n    if share_col is None:\n        return 0.0\n    values = []\n    for r in range(header_row+1, min(len(raw), header_row+1+20)):\n        row = raw.iloc[r]\n        # stop at blank line\n        if all((pd.isna(v) or str(v).strip()==\"\") for v in row.tolist()):\n            break\n        fcell = str(row.iloc[func_col]) if func_col is not None else str(row.iloc[0])\n        if re.search(r\"total\", fcell or \"\", flags=re.I):\n            continue\n        v = parse_num(row.iloc[share_col])\n        if v is not None:\n            values.append(v)\n        if len(values) >= 3:\n            break\n    if len(values) == 0:\n        return 0.0\n    s = sum(values)\n    # Detect scale: if any value > 1, assume 0-100 scale\n    if any(v>1.0 for v in values):\n        plausible = s <= 105.0\n    else:\n        plausible = s <= 1.05\n    return 1.0 if plausible else 0.0"}, {"type": "llm_judge", "name": "Section 1 \u2013 Overall Business Calculations Correct", "description": "Check that Section 1 shows TY (2023), LY (2022), $ change, and % change. Verify % and $ change are computed correctly from TY and LY (allow rounding).", "weight": 3.0, "judge_prompt": "Inspect Section 1 (Overall Business). Confirm it includes all four metrics: TY 2023 sales, LY 2022 sales, $ Change (TY-LY), and % Change (TY vs LY). Now mentally verify math consistency:\n- Does $ Change = TY - LY (allow small rounding)?\n- Does % Change \u2248 (TY - LY) / LY (allow rounding; treat % as 0\u2013100 or 0\u20131 as appropriate)?\n\nScoring:\n- 3.0: All four metrics present and both calculations correct within reasonable rounding.\n- 2.0: All four present; one calculation slightly off or ambiguous due to formatting but mostly correct.\n- 1.0: Missing one metric or math clearly incorrect.\n- 0.0: Missing multiple metrics or cannot locate Section 1.", "expectation": "A compact block with four figures where $ and % changes are consistent with TY and LY."}, {"type": "llm_judge", "name": "Section 2 \u2013 Disco Logic and Ratio Correct", "description": "Verify Section 2 correctly defines ongoing (05/06) vs discontinued (07/08), reports totals for each, and shows Disco % of total 2023 sales. Check math consistency and plausibility.", "weight": 3.0, "judge_prompt": "Inspect Section 2 (Discontinued SKUs \u2013 Risk to 2024). Confirm:\n- The definition aligns with Material Status codes: ongoing = 05/06; discontinued = 07/08.\n- The block shows: Total $ ongoing, Total $ discontinued, and % of Sales (discos) = discontinued $ / total TY 2023 sales.\n- Check that the displayed % is consistent with the shown totals (allow rounding), and result is between 0% and 100%.\n\nScoring:\n- 3.0: Clear correct definitions, all three figures present, and % calculation matches the numbers.\n- 2.0: Definitions present; minor rounding ambiguity but plausibly correct.\n- 1.0: Definition missing/unclear or one figure missing; math questionable.\n- 0.0: Section absent or clearly incorrect use of status codes.", "expectation": "Disco exposure is clearly quantified and linked to the proper status codes with correct ratio math."}, {"type": "llm_judge", "name": "Sections 3\u20135 \u2013 Selection, Sorting, and Totals", "description": "Confirm the top/increase/detractor tables use the nine columns, show exactly 3 rows + a Total row each, are correctly sorted per their definition, and totals equal the sum of the three rows.", "weight": 2.0, "judge_prompt": "Review Sections 3\u20135 tables:\n- Each table should have the nine specified columns and exactly 3 function rows plus a Total row.\n- Section 3 (Top Volume Drivers): rows sorted by 2023 sales descending.\n- Section 4 (Largest Volume Increases): rows sorted by $ change (TY-LY) descending.\n- Section 5 (Largest Volume Detractors): rows sorted by $ change ascending (most negative first).\n- Verify each table\u2019s Total row equals the sum of the three rows for $ metrics (TY, LY, $ change, $ DISCO) and % metrics are presented appropriately (do not necessarily sum, but formatting is consistent).\n\nScoring:\n- 2.0: All three tables meet row count, columns, sorting, and totals (allow rounding).\n- 1.0: Minor deviation in sorting or a small totals discrepancy in one table, but overall correct.\n- 0.0: Major issues (wrong row counts, not sorted as specified, missing totals).", "expectation": "Three concise, correctly sorted tables with accurate totals rows."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Quality and Executive Usefulness", "description": "Holistic, professional-quality assessment: clarity, formatting, and strategic usefulness for a national account director.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Executive Readability on One Page", "description": "Assess whether the sheet reads clearly as a one-page recap with distinct sections, readable fonts, spacing, and labels suitable for an executive meeting.", "weight": 1.5, "judge_prompt": "Evaluate overall readability as a single-page executive recap:\n- Clear section headers and logical layout\n- Appropriate sizing/spacing so all content is legible on one page\n- Minimal clutter; uses whitespace and alignment well\n\nScoring:\n- 1.5: Clean, well-structured, comfortably legible one-pager.\n- 1.0: Generally readable with minor clutter/sizing issues.\n- 0.5: Crowded or awkward layout but still decipherable.\n- 0.0: Hard to read or evidently spans more than one page.", "expectation": "A crisp one-page layout with clearly delineated sections and legible content."}, {"type": "llm_judge", "name": "Professional Number Formatting and Labeling", "description": "Currency shown with $ and thousands separators; percents with %; negative changes clear; years labeled consistently; columns aligned.", "weight": 1.5, "judge_prompt": "Check professional formatting:\n- Currency numbers use $ and thousands separators consistently\n- Percents use % with sensible precision\n- Negative changes clearly indicated (minus sign or parentheses; optional red color)\n- Year labels (2023 vs 2022) consistent across sections\n- Column headers aligned and unambiguous\n\nScoring:\n- 1.5: Consistently professional formatting and labeling.\n- 1.0: Mostly professional; minor inconsistencies.\n- 0.5: Several inconsistencies but understandable.\n- 0.0: Poor formatting that hinders interpretation.", "expectation": "Consistent, executive-grade numeric and label formatting."}, {"type": "llm_judge", "name": "Strategic Insight and Callouts for 2024", "description": "Does the recap highlight key insights, risks, or opportunities and suggest where to dig deeper for 2024 planning?", "weight": 1.5, "judge_prompt": "Evaluate whether the sheet adds strategic value beyond raw numbers:\n- Brief insights/callouts near sections (e.g., drivers, detractors, disco exposure)\n- Highlights potential 2024 risks/opportunities inferred from 2023 data\n- Points to where to dig deeper (e.g., specific functions/SKUs for follow-up)\n\nScoring:\n- 1.5: Clear, concise insights with actionable callouts for 2024.\n- 1.0: Some insights; partly actionable.\n- 0.5: Minimal/implicit insights.\n- 0.0: No strategic guidance present.", "expectation": "A few crisp callouts that guide 2024 actions (risk mitigation, growth bets)."}, {"type": "llm_judge", "name": "Audience Fit and Naming Accuracy", "description": "Ensure the artifact is anchored to XR Retailer and Makeup category, dated appropriately (2023), and suitable for a national account director audience.", "weight": 1.5, "judge_prompt": "Assess audience fit and naming:\n- Title references XR Retailer and Makeup, covering 2023 performance\n- Tone and prioritization fit a national account director (high-level KPIs, top movers, risks)\n- Any footnotes/assumptions are concise and helpful\n\nScoring:\n- 1.5: Spot-on audience fit and accurate naming/date context.\n- 1.0: Minor deviations but acceptable.\n- 0.5: Noticeable issues (e.g., vague titling or mislabeling).\n- 0.0: Misaligned audience or incorrect retailer/category/timeframe.", "expectation": "A polished, director-level artifact clearly labeled for XR Retailer Makeup 2023."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "68d8d901-dd0b-4a7e-bf9a-1074fddf1a96", "rubric": {"category_name": "Manufacturing Trial Scheduling \u2014 Freeze-Dried Beef (Crispivore)", "rationale": "This rubric enforces a self-documenting, verifiable Excel deliverable for a manufacturing trial schedule. Stage 1 (LLM-only) strictly mandates a specific three-sheet Excel structure to make downstream verification trivial. Stage 2 mixes light-weight code checks (bounds and cross-sheet consistency) with higher-weight LLM checks for operational correctness, leveraging the mandated structure. Stage 3 assesses professional quality, usability for supervisors, and readiness for real operations.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (Structure-Only)", "description": "Must submit a single Excel workbook with exactly the required sheets and structural elements so verification is possible. LLM-only per system design.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Excel with 3 Required Sheets and Specified Tables", "description": "Check that the output is an editable Excel workbook with the three required tabs and the mandated table structures/fields. Do not judge correctness, only presence/format.", "weight": 4.0, "judge_prompt": "You are evaluating whether the candidate output is a properly structured and editable Excel workbook (not PDF/CSV/screenshot) with three specific worksheets that enable verification. Only judge structure and presence, not content correctness.\n\nFormat requirements:\n- Must be an Excel workbook with multiple worksheets (e.g., .xlsx). CSV or a single-sheet file is not acceptable.\n- Workbook must be editable (cells, not images).\n- Must contain three worksheets named clearly and distinctly (flexible on naming variants):\n  1) \"Work Schedule\" (or close variant: Work Plan, Schedule, Trial Schedule)\n  2) \"Production Assignments\" (or close variant: Staffing, Roles & Assignments)\n  3) \"Production Sequences\" (or close variant: Process Sequence, Dryer Sequences)\n\nRequired structural elements inside each sheet (be flexible with exact column wording but ensure the intent is present):\n\n1) Work Schedule sheet \u2014 must present a top-level summary block with key metrics as labeled fields and associated values (as a two-column list or a Parameter/Value table). The following items must be present and clearly labeled:\n   - Production Target (lbs) for the 4-week trial\n   - Shift Length (hours)\n   - Shifts per Day\n   - Labor Availability / Employee Count (should show 20 as total team size)\n   - Equipment/Dryer Count (should indicate two dryers)\n   - Equipment Capacity per Dryer per Cycle (lbs per cycle or equivalent capacity measure)\n   - Cycle Time (hours)\n   - Trial Duration (weeks) or Total Run Hours\n   - Assumptions/Notes section (a text block or a labeled area)\n\n2) Production Assignments sheet \u2014 must contain a table of the 20 personnel with at least the following columns:\n   - Employee/Name or ID (unique identifier per person)\n   - Role/Position/Station\n   - Brief Role Description / Primary Tasks\n   - Shift (or coverage pattern)\n   - Dryer Assignment or Area (e.g., Dryer 1 / Dryer 2 / Prep / Packaging)\n   - Optional but desirable: Cross-Training/Backup and Notes\n   The table should clearly list the 20 individuals (20 rows excluding headers; flexible if grouped by shift as long as 20 unique personnel are present).\n\n3) Production Sequences sheet \u2014 must detail sub-steps for each dryer (two dryers). Use either a single combined table with a column identifying the dryer (e.g., Dryer 1 / Dryer 2) or separate sections. The sequence must include columns for:\n   - Step # (or explicit sequence order)\n   - Stage/Sub-step name (e.g., Tray Prep, Load, Freeze Cycle, Unload, Package)\n   - Responsible Role(s) and Headcount\n   - Duration (minutes or hours)\n   - Start Offset and End Offset (or an equivalent timeline indicator that enables verifying staggering/overlap)\n   - Predecessor/Dependency and Parallelizable (Yes/No) or a clear way to see which steps can overlap\n   The two dryers must be explicitly represented as distinct entities in the sequences.\n\nScoring (out of 4.0):\n- 4.0: Valid Excel workbook with all 3 sheets present and each sheet contains the required structural elements (as described). Both dryers explicitly represented in sequences with order, responsibilities, and durations/timings.\n- 3.0: Excel workbook with all 3 sheets present and mostly complete structures; only minor omissions (e.g., missing one non-critical timing column or a secondary field like Notes/Parallelizable) but clearly verifiable.\n- 2.0: Excel workbook present but missing one major required sheet element (e.g., no clear role descriptions, no cycle time/capacity on Work Schedule, or sequences lack step ordering/timings) \u2014 structure insufficient for full verification.\n- 1.0: Wrong or minimal structure (e.g., missing an entire required sheet or not an Excel workbook with multiple worksheets).\n- 0.0: Not an Excel workbook or structure is not meaningfully compliant.\n\nImportant: Be flexible on exact wording of headers but strict about the presence of the intended sections/columns. Only check structure/presence, not the correctness of values.", "expectation": "A clean .xlsx with three named sheets, each containing the specified tables/fields enabling verification."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness and Cross-Checks)", "description": "Now that the structure exists, verify correctness and consistency using a mix of code rules (deterministic checks) and LLM rules (operational reasonableness). Code rules have lighter weight than LLM rules.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Assignments Count and Unique Personnel = ~20", "description": "Verify the Production Assignments sheet lists approximately 20 unique personnel (ideal: exactly 20).", "weight": 0.7, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        file_path = context.files.get_path(output.id)\n        xl = pd.ExcelFile(file_path)\n        # Fuzzy find assignments sheet\n        def pick_sheet(names, keys):\n            keys = [k.lower() for k in keys]\n            best = None\n            for s in names:\n                sl = s.lower()\n                if any(k in sl for k in keys):\n                    best = s\n                    break\n            return best\n        sheet = pick_sheet(xl.sheet_names, [\"assign\", \"staff\", \"role\"])\n        if not sheet:\n            return 0.0, \"Assignments sheet not found.\"\n        df = xl.parse(sheet)\n        if df.empty:\n            return 0.0, \"Assignments sheet is empty.\"\n        # Find a column that likely contains employee names/IDs\n        cols = [c for c in df.columns if isinstance(c, str)]\n        emp_col = None\n        for k in [\"employee\", \"employee name\", \"name\", \"personnel\", \"id\"]:\n            for c in cols:\n                if k in c.lower():\n                    emp_col = c\n                    break\n            if emp_col:\n                break\n        if not emp_col:\n            # fallback: use first column if it looks like identifier-ish\n            emp_col = cols[0] if cols else None\n        if not emp_col:\n            return 0.0, \"No identifiable employee column.\"\n        series = df[emp_col].astype(str).str.strip()\n        series = series[series.str.len() > 0]\n        # Exclude obvious header repeats\n        uniques = set([s for s in series if s.lower() not in [\"name\", \"employee\", \"id\", \"employee name\"]])\n        n = len(uniques)\n        # Score mapping\n        if n == 20:\n            score = 0.7\n        elif 18 <= n <= 22:\n            score = 0.5\n        elif 15 <= n <= 25:\n            score = 0.3\n        else:\n            score = 0.0\n        return score, f\"Detected {n} unique personnel.\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Throughput Feasibility vs Target", "description": "Using Work Schedule values, estimate max feasible output = dryers * capacity_per_cycle * floor(total_run_hours / cycle_time). Score if >= target; partial if close. Defaults: 2 dryers, 4 weeks if missing.", "weight": 0.8, "code": "import re\nimport math\nimport pandas as pd\n\nKEYS = {\n    'target': ['production target', 'target (lbs)', 'target lbs', 'lbs target', 'goal (lbs)', 'production goal'],\n    'cycle_time': ['cycle time', 'freeze cycle', 'dry cycle', 'cycle (hours)', 'cycle hours'],\n    'capacity': ['capacity per cycle', 'equipment capacity', 'lbs per cycle', 'dryer capacity'],\n    'total_hours': ['total run hours', 'total hours', 'available hours'],\n    'shift_len': ['shift length', 'shift hrs', 'shift (hours)'],\n    'shifts_per_day': ['shifts per day', 'shift/day', '# shifts/day'],\n    'weeks': ['trial duration (weeks)', 'duration (weeks)', 'weeks'],\n    'dryers': ['dryer count', 'dryers', 'equipment count']\n}\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        file_path = context.files.get_path(output.id)\n        xl = pd.ExcelFile(file_path)\n        # Fuzzy find work schedule sheet\n        def pick_sheet(names, keys):\n            keys = [k.lower() for k in keys]\n            for s in names:\n                sl = s.lower()\n                if any(k in sl for k in keys):\n                    return s\n            return None\n        ws = pick_sheet(xl.sheet_names, [\"work\", \"schedule\", \"plan\", \"summary\"])\n        if not ws:\n            return 0.0, \"Work Schedule sheet not found.\"\n        df = xl.parse(ws)\n        if df.empty:\n            return 0.0, \"Work Schedule is empty.\"\n\n        # Helper: try to build key->value mapping from common table patterns\n        def to_kv(dfin):\n            mapping = {}\n            d = dfin.copy()\n            d.columns = [str(c) for c in d.columns]\n            # Case A: Parameter/Value style\n            cand_param = None\n            cand_value = None\n            for c in d.columns:\n                cl = c.lower()\n                if any(k in cl for k in ['parameter','metric','item','field','label']):\n                    cand_param = c\n                if any(k in cl for k in ['value','val','amount']):\n                    cand_value = c\n            if cand_param is not None and cand_value is not None:\n                for _, row in d.iterrows():\n                    k = str(row.get(cand_param, '')).strip().lower()\n                    v = row.get(cand_value, None)\n                    if k:\n                        mapping[k] = v\n            # Case B: two-column pairs\n            if not mapping and d.shape[1] >= 2:\n                left = d.columns[0]\n                right = d.columns[1]\n                for _, row in d.iterrows():\n                    k = str(row.get(left, '')).strip().lower()\n                    v = row.get(right, None)\n                    if k:\n                        mapping[k] = v\n            return mapping\n\n        kv = to_kv(df)\n\n        def find_num(keys):\n            # Search in kv first\n            for k in list(kv.keys()):\n                for needle in keys:\n                    if needle in k:\n                        try:\n                            val = kv[k]\n                            if pd.isna(val):\n                                continue\n                            # Extract number from strings like \"250,000 lbs\"\n                            s = str(val)\n                            m = re.findall(r\"[-+]?[0-9]*\\.?[0-9]+\", s.replace(',', ''))\n                            if m:\n                                return float(m[0])\n                        except Exception:\n                            pass\n            # Fallback: scan entire frame\n            for i in range(df.shape[0]):\n                for j in range(df.shape[1]):\n                    cell = df.iat[i,j]\n                    if not isinstance(cell, str):\n                        try:\n                            cell = str(cell)\n                        except Exception:\n                            continue\n                    cell_l = cell.lower()\n                    if any(needle in cell_l for needle in keys):\n                        # check right then below for numeric\n                        # right\n                        for jj in range(j+1, df.shape[1]):\n                            c = df.iat[i,jj]\n                            s = str(c)\n                            m = re.findall(r\"[-+]?[0-9]*\\.?[0-9]+\", s.replace(',', ''))\n                            if m:\n                                return float(m[0])\n                        # below\n                        for ii in range(i+1, df.shape[0]):\n                            c = df.iat[ii,j]\n                            s = str(c)\n                            m = re.findall(r\"[-+]?[0-9]*\\.?[0-9]+\", s.replace(',', ''))\n                            if m:\n                                return float(m[0])\n            return None\n\n        target = find_num(KEYS['target'])\n        cycle_time = find_num(KEYS['cycle_time'])\n        capacity = find_num(KEYS['capacity'])\n        total_hours = find_num(KEYS['total_hours'])\n        if total_hours is None:\n            shift_len = find_num(KEYS['shift_len'])\n            spd = find_num(KEYS['shifts_per_day'])\n            weeks = find_num(KEYS['weeks'])\n            if weeks is None:\n                weeks = 4.0  # Default per task\n            if shift_len is not None and spd is not None:\n                total_hours = shift_len * spd * 7.0 * weeks\n        dryers = find_num(KEYS['dryers'])\n        if dryers is None:\n            dryers = 2.0\n\n        missing = [k for k,v in [('target',target),('cycle_time',cycle_time),('capacity',capacity),('total_hours',total_hours)] if v is None]\n        if missing:\n            return 0.3, f\"Missing keys for feasibility calc: {', '.join(missing)}. Partial credit if some values present.\"\n        if any(v <= 0 for v in [cycle_time, capacity, dryers, total_hours]) or target is None or target <= 0:\n            return 0.0, \"Non-positive values detected; cannot assess throughput feasibility.\"\n\n        cycles = math.floor(total_hours / cycle_time)\n        feasible = dryers * capacity * cycles\n        if feasible >= target:\n            return 0.8, f\"Feasible output {feasible:.0f} >= target {target:.0f}.\"\n        elif feasible >= 0.85 * target:\n            return 0.5, f\"Feasible output {feasible:.0f} is within 85% of target {target:.0f}.\"\n        else:\n            return 0.0, f\"Feasible output {feasible:.0f} falls short of target {target:.0f}.\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Roles in Sequences Match Assigned Roles", "description": "Check that Production Sequences reference role names that exist in the Production Assignments table. Score by coverage ratio.", "weight": 0.5, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        file_path = context.files.get_path(output.id)\n        xl = pd.ExcelFile(file_path)\n        def pick_sheet(names, keys):\n            keys = [k.lower() for k in keys]\n            for s in names:\n                sl = s.lower()\n                if any(k in sl for k in keys):\n                    return s\n            return None\n        assign_sheet = pick_sheet(xl.sheet_names, [\"assign\", \"staff\", \"role\"])\n        seq_sheet = pick_sheet(xl.sheet_names, [\"sequence\", \"process\", \"dryer\"])\n        if not assign_sheet or not seq_sheet:\n            return 0.0, \"Assignments or Sequences sheet not found.\"\n        da = xl.parse(assign_sheet)\n        ds = xl.parse(seq_sheet)\n        if da.empty or ds.empty:\n            return 0.0, \"Assignments or Sequences is empty.\"\n        # Role column in assignments\n        role_cols = [c for c in da.columns if isinstance(c,str) and any(k in c.lower() for k in [\"role\",\"position\",\"station\"]) ]\n        if role_cols:\n            role_col = role_cols[0]\n        else:\n            return 0.0, \"No role/position column found in assignments.\"\n        roles = set()\n        for x in da[role_col].dropna().astype(str).tolist():\n            parts = re.split(r\"[,/&;+]| and | \\/ \", x.lower())\n            for p in parts:\n                p = p.strip()\n                if p:\n                    roles.add(p)\n        if not roles:\n            return 0.0, \"No roles detected in assignments.\"\n        # Responsible roles in sequences\n        resp_cols = [c for c in ds.columns if isinstance(c,str) and any(k in c.lower() for k in [\"responsible\",\"role\",\"operator\",\"crew\"]) ]\n        if not resp_cols:\n            return 0.0, \"No responsible/role column found in sequences.\"\n        resp_col = resp_cols[0]\n        total = 0\n        matched = 0\n        for x in ds[resp_col].dropna().astype(str).tolist():\n            total += 1\n            parts = re.split(r\"[,/&;+]| and | \\/ \", x.lower())\n            ok = False\n            for p in parts:\n                p = p.strip()\n                if not p:\n                    continue\n                # match by inclusion either way for fuzziness\n                if any(p in r or r in p for r in roles):\n                    ok = True\n                    break\n            if ok:\n                matched += 1\n        if total == 0:\n            return 0.0, \"No sequence steps with responsible roles to evaluate.\"\n        ratio = matched / total\n        return 0.5 * ratio, f\"Matched roles for {matched}/{total} steps (coverage {ratio:.0%}).\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "llm_judge", "name": "Staggered Dryer Operation and Overlap Logic", "description": "Sequence must demonstrate two dryers running with staggered cycles to minimize idle time, clear unloading/loading overlaps, and no obvious long idle gaps.", "weight": 3.0, "judge_prompt": "Evaluate the Production Sequences sheet(s) only for operational staggering and overlap logic. Do not assess formatting aesthetics.\n\nCheck:\n- Two distinct dryers are represented (e.g., Dryer 1 and Dryer 2 or equivalent) with clearly ordered steps.\n- Cycles are staggered: the end of one dryer\u2019s cycle should not routinely coincide with the other; offsets or start/end times indicate stagger.\n- Unload->Load transitions are timed to minimize idle dryer time.\n- Parallelizable steps are identified or obviously implied (e.g., tray prep and packaging overlapping with freeze cycle).\n- No obvious long idle gaps for dryers due to sequencing mistakes.\n\nScoring (0\u20133):\n- 3.0: Clear, consistent staggering with well-timed overlaps, minimal idle time.\n- 2.0: Generally staggered with minor inefficiencies or occasional overlaps/gaps.\n- 1.0: Staggering is attempted but unclear or often ineffective; notable idle periods.\n- 0.0: No meaningful staggering or only a single dryer\u2019s sequence is effectively shown.", "expectation": "Two clearly staggered dryer timelines with coordinated load/unload and support activities."}, {"type": "llm_judge", "name": "Completeness of End-to-End Sub-steps and Durations", "description": "Verify that the sequences cover the full process (prep, load, freeze, unload, package, QA/label) with realistic durations defined for each sub-step.", "weight": 3.0, "judge_prompt": "Inspect Production Sequences for process completeness and duration coverage. Focus on whether the full end-to-end flow is represented for each dryer with realistic step durations and resource indications.\n\nConsider:\n- Presence of major sub-steps: raw material/tray prep, loading, freeze cycle, unloading, packaging (weigh, fill, seal, label/lot), boxing/palletizing, and any necessary sanitation/reset steps.\n- Each sub-step includes a duration (minutes or hours) sufficient to build a timeline.\n- Headcount for labor-intensive steps is indicated.\n- Durations are broadly reasonable for a freeze-dry beef trial context (e.g., multi-hour freeze cycles; prep/unload/packaging in shorter time blocks; exact values need not be perfect).\n\nScoring (0\u20133):\n- 3.0: All key sub-steps present with durations and headcount where needed; looks operationally complete.\n- 2.0: Minor omissions or a few steps lack durations/headcount but overall flow is coherent.\n- 1.0: Significant gaps in sub-steps or many missing durations.\n- 0.0: Fragmentary sequence; cannot trace end-to-end process.", "expectation": "Complete, timed sub-steps for both dryers covering prep through packaging."}, {"type": "llm_judge", "name": "Cross-Tab Consistency (Headcount, Shifts, Capacity)", "description": "Assess whether Work Schedule assumptions align with Assignments and Sequences: headcount ~20, shifts/shift length feasible, and capacity/cycle times consistent.", "weight": 2.0, "judge_prompt": "Assess consistency across all three sheets:\n- Work Schedule: target, shift length, shifts/day, cycle time, dryer count (2), and capacity per cycle.\n- Production Assignments: total personnel \u2248 20, roles align with process needs (prep, dryers, packaging), shift coverage matches the stated shifts.\n- Production Sequences: durations and staffing per step appear consistent with the cycle time and staffing availability.\n\nScoring (0\u20132):\n- 2.0: Consistent across tabs; assumptions and numbers align (e.g., headcount, shifts, capacity/cycle times) with no major contradictions.\n- 1.0: Minor mismatches but largely consistent.\n- 0.0: Major contradictions (e.g., headcount/shift plan impossible relative to sequences, or capacity assumptions inconsistent with schedules).", "expectation": "No major cross-tab contradictions; headcount/shifts/capacity hang together."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professional Usability", "description": "Holistic assessment of presentation quality, usability for supervisors, and strategic value for managing the trial.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Editability", "description": "Evaluate formatting, readability, and ease of editing (clear headers, filters, units, consistent time/weight formatting, not locked).", "weight": 1.5, "judge_prompt": "Evaluate the overall presentation quality of the Excel workbook:\n- Clear, consistent headers and units (hours, minutes, lbs).\n- Sort/filter enabled on tables where appropriate.\n- Consistent formatting (number/date/time formats).\n- Obvious input/assumption cells are easy to edit.\n- Workbook appears editable (not images) and not locked.\n\nScoring (0\u20131.5):\n- 1.5: Clean, professional, and easy to edit.\n- 1.0: Generally good with minor inconsistencies.\n- 0.5: Usable but cluttered/inconsistent.\n- 0.0: Hard to read/edit or poorly formatted.", "expectation": "A professional, readable, and editable workbook suitable for shop-floor and management use."}, {"type": "llm_judge", "name": "Clarity and Usability for Supervisors", "description": "Assess whether supervisors can quickly understand staffing decisions, shift coverage, and sequencing to make decisions.", "weight": 2.0, "judge_prompt": "Judge how easily a first-line supervisor can use this workbook to plan and run the trial:\n- Can they quickly see staffing by role/shift and who covers each dryer?\n- Is it obvious how many people are needed at each step and when?\n- Are key KPIs (target, hours, capacity) clearly summarized?\n- Does the layout support day-to-day adjustments?\n\nScoring (0\u20132):\n- 2.0: Very clear and directly useful for decisions.\n- 1.0: Mostly usable but some ambiguity requires interpretation.\n- 0.0: Confusing or missing decision-critical information.", "expectation": "Managers can rapidly determine staffing and sequencing without extra interpretation."}, {"type": "llm_judge", "name": "Scheduling Strategy and Rationale", "description": "Assess whether the Work Schedule includes assumptions/rationale and shows a coherent strategy for meeting the 250,000 lb goal.", "weight": 1.5, "judge_prompt": "Review the Work Schedule for strategy and rationale:\n- Are key assumptions stated (e.g., cycle time assumptions, capacity per cycle, staffing constraints)?\n- Is there a clear logic connecting shifts, hours, and expected throughput to the target?\n- Are constraints/trade-offs briefly documented?\n\nScoring (0\u20131.5):\n- 1.5: Clear, coherent strategy with assumptions and trade-offs noted.\n- 1.0: Mostly coherent with limited rationale.\n- 0.5: Minimal rationale; strategy implicit.\n- 0.0: No visible rationale/strategy.", "expectation": "A brief but coherent rationale tying schedule/assumptions to the production target."}, {"type": "llm_judge", "name": "Scalability and What-If Readiness", "description": "Evaluate whether inputs are set up for quick adjustments (e.g., change shift length or cycle time and see effects).", "weight": 1.0, "judge_prompt": "Assess whether the workbook supports quick what-if adjustments:\n- Clearly identified input cells for key drivers (shift length, shifts/day, cycle time, capacity per cycle, headcount).\n- Calculations that appear to update downstream metrics (e.g., total run hours, feasible throughput).\n\nScoring (0\u20131):\n- 1.0: Obvious, well-structured input cells with formula-driven outputs.\n- 0.5: Some inputs adjustable but limited propagation.\n- 0.0: Hard-coded values everywhere; no evident what-if capability.", "expectation": "Basic input cells and formulas support quick scenario updates."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "211d0093-2c64-4bd0-828c-0201f18924e7", "rubric": {"category_name": "Retail Electronics Department \u2014 Daily Task List (DTL) PDF Template", "rationale": "This rubric enforces a self-documenting, verifiable PDF template for a Daily Task List used by first-line supervisors in a retail electronics department. Stage 1 (LLM-only) strictly mandates the document\u2019s shape so verification is trivial. Stage 2 mixes light code checks (text-based structure cues) with LLM reasoning about process alignment and completeness. Stage 3 evaluates overall professional quality and usability in a real store context. Code rules are intentionally lightweight and worth ~5x less than LLM rules on average within Stage 2.", "max_total_score": 24.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (LLM Only)", "description": "Gate: The output must be a properly structured PDF DTL template with specific sections and tables enabling downstream verification. Only structure and format are checked here, not content quality or calculation correctness.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Format and Placement Gate", "description": "Verify the candidate is a PDF document with professional layout appropriate for a printable DTL kept at the department\u2019s main desk. Confirm presence of a clear title and header block information placeholders.", "weight": 3.0, "judge_prompt": "You are checking the SHAPE ONLY of the candidate output. Do not judge writing quality. Do not judge correctness of content. Only verify structure and format.\n\nRequirements (all must be visible in the PDF):\n- File format is PDF (not DOCX/Excel/plain text)\n- Has a clear title near the top indicating this is a Daily Task List (accept variations like: Daily Task List, DTL, Daily Checklist, Department Daily Task List)\n- Has a header block with placeholders to record at least: Date and Department/Store (accept variants like Dept, Section, Area)\n- Looks professionally formatted and printable (legible fonts, reasonable margins, page orientation portrait or landscape acceptable)\n\nScoring:\n- 3.0: PDF format + clear DTL title + header block with Date and Department/Store present + professional printable layout\n- 2.0: PDF + title + at least one of the header placeholders (Date or Department/Store)\n- 1.0: PDF + recognizable title but header placeholders missing\n- 0.0: Not a PDF OR lacks a recognizable DTL title entirely", "expectation": "A printable PDF with a clear DTL title and a header block capturing Date and Department/Store."}, {"type": "llm_judge", "name": "Required Sections and Tables Gate", "description": "Verify presence of all required DTL sections enabling assignment, tracking, and end-of-day sign-off.", "weight": 5.0, "judge_prompt": "You are checking the SHAPE ONLY of the candidate PDF. Be flexible with section names but strict about the presence of the following elements. Do not assess quality\u2014only whether they exist visibly.\n\nRequired structural elements:\n1) Assignment/Task Table: A prominent table for the day\u2019s tasks with columns that cover at least:\n   - Task (task name/description)\n   - Assigned Employee(s) (names of who is assigned)\n   - Employee Acknowledgment (e.g., Initials/Signature field)\n   - Notes (free-text notes/comments)\n   - Status/Timing (any of: checkbox, status, start/due/target time)\n   The exact column names may vary; accept close equivalents.\n\n2) Instructions/Usage Cues: Brief visible guidance that the first-on-shift assigns tasks and that employees initial upon completion (can be a short instruction line or legend on the page).\n\n3) End-of-Day Closing Verification: A distinct area for the closing employee to verify completion (accept labels like: Closing Verification, End-of-Day Check, Final Check by Closer) with a place for the closer\u2019s name/initials and time/date.\n\n4) Final Manager Sign-Off Block AT THE END: A clearly separated sign-off section located at the end of the document with fields for Manager\u2019s Name and Date (accept variants like Manager Signature/Printed Name and Date). This must be distinct from any per-task manager initials.\n\nOptional but beneficial (do not penalize if missing): Staff/Shift roster area for listing scheduled team members.\n\nScoring:\n- 5.0: All four required elements present, with the Manager Sign-Off block clearly at the end\n- 4.0: Missing exactly one required element (but the Assignment/Task Table must be present)\n- 2.5: Assignment/Task Table present but two required elements missing\n- 1.0: Minimal table-like list but clearly lacks most required elements\n- 0.0: No recognizable task assignment table", "expectation": "A PDF showing a task table with columns for Task, Assigned Employee(s), Employee Initials/Signature, Notes, plus Status/Timing; concise instructions; an end-of-day closer verification area; and a final manager sign-off block at the end with Manager Name and Date."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness Verification (Mixed)", "description": "Now that the structure is enforced, verify process alignment and textual cues supporting correct use. Code rules perform robust text checks; LLM judges assess nuanced alignment with the described workflow.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "PDF Text \u2014 Key Fields Presence", "description": "Checks PDF text for essential fields/labels enabling assignment, acknowledgment, and authorization.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext\n    Returns: float in [0,1]\n    \"\"\"\n    import re\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        return 0.0\n    if not text:\n        return 0.0\n    t = text.lower()\n    # Flexible keyword families\n    must_haves = [\n        # Assignment/acknowledgment basics\n        [\"assigned employee\", \"assigned to\", \"assignee\", \"employee(s)\"],\n        [\"initials\", \"employee initials\", \"employee signature\", \"acknowledge\"],\n        [\"notes\", \"comments\", \"remarks\"],\n        # Manager and date fields\n        [\"manager\", \"manager's name\", \"manager signature\"],\n        [\"date\", \"dated\"],\n    ]\n    found_groups = 0\n    for group in must_haves:\n        if any(kw in t for kw in group):\n            found_groups += 1\n    return found_groups / len(must_haves)\n"}, {"type": "code", "name": "Final Sign-Off Position and Closing Check", "description": "Confirms that the final manager sign-off appears near the end and that closing verification language exists.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext\n    Returns: float in [0,1]\n    \"\"\"\n    import re\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        return 0.0\n    if not text:\n        return 0.0\n    t = text.lower()\n    L = len(t)\n    # Closing verification cues\n    closing_cues = [\"closing\", \"end-of-day\", \"closer\", \"final check\", \"closing verification\"]\n    has_closing = any(c in t for c in closing_cues)\n    # Manager sign-off cues\n    mgr_cues = [\"manager\", \"manager's name\", \"manager signature\", \"manager sign-off\", \"manager sign off\"]\n    date_cues = [\"date\", \"dated\"]\n    # Position heuristic: look for last occurrence of manager-related + date near the end 20%\n    last_mgr_pos = max((t.rfind(c) for c in mgr_cues), default=-1)\n    last_date_pos = max((t.rfind(c) for c in date_cues), default=-1)\n    near_end_thresh = int(L * 0.8) if L > 0 else 0\n    near_end = (last_mgr_pos >= near_end_thresh) or (last_date_pos >= near_end_thresh)\n    score = 0.0\n    if has_closing:\n        score += 0.5\n    if near_end and last_mgr_pos != -1 and last_date_pos != -1:\n        score += 0.5\n    return score\n"}, {"type": "llm_judge", "name": "Workflow Alignment \u2014 Begin, During, End", "description": "Assesses whether the document clearly supports the described workflow: opening assignment, employees initial upon completion, manager verification, and end-of-day close-out with filing note.", "weight": 3.0, "judge_prompt": "Evaluate whether the PDF template matches the operational workflow. Look for explicit cues/instructions that:\n- At the beginning of day: First-on-shift reviews schedule and evenly assigns tasks to scheduled team members (instructions can be brief, but should be present).\n- During the day: Employees initial or sign upon completion in the task table.\n- Manager verification: Either per-task verification fields OR clear instruction to have a manager verify completed tasks, PLUS a final manager sign-off block at the end with name and date.\n- End of day: A closing employee verifies all tasks are complete and the document includes a note/instruction to file the DTL in the Manager\u2019s Office filing cabinet (accept close phrasing like file in manager office cabinet).\n\nScoring:\n- 3.0: All four workflow points are clearly supported in the PDF\n- 2.0: Three points clearly supported (must include employee initials and final manager sign-off)\n- 1.0: Two points clearly supported\n- 0.0: One or zero points supported", "expectation": "The PDF explicitly guides opening assignment, in-day initials, manager verification, and end-of-day close with filing location note."}, {"type": "llm_judge", "name": "Task Table Adequacy and Fillability", "description": "Checks that the main task table provides enough rows/space and has fillable-looking fields for names, initials, notes, and timing/status.", "weight": 2.5, "judge_prompt": "Check the task table for practical use in a busy retail department. Look for:\n- Sufficient rows/space to capture the full day\u2019s tasks (at least ~10 rows on one or more pages or a clear indication more pages can be used)\n- Clear columns for Task, Assigned Employee(s), Employee Initials/Signature, Notes, and a Status/Timing field\n- Fields/lines/checkboxes that appear writable/fillable when printed\n\nScoring:\n- 2.5: All listed aspects are present\n- 1.5: Columns exist but space is clearly limited (e.g., very few rows) or fields look cramped\n- 0.5: Minimal table; difficult to fill in practice\n- 0.0: No usable task table", "expectation": "A practical, writable table with enough rows and clear columns for task execution tracking."}, {"type": "llm_judge", "name": "Clarity of Final Manager Sign-Off Block", "description": "Ensures the final sign-off area is distinctly separated and includes manager name and date fields.", "weight": 1.5, "judge_prompt": "Inspect the final manager sign-off section at the end of the PDF. It should be visually distinct and include fields for Manager\u2019s Name (or signature/printed name) and Date. Ensure it is separate from per-task fields.\n\nScoring:\n- 1.5: Clearly distinct final block with Manager Name and Date\n- 0.8: Final block present but ambiguous or not clearly separate\n- 0.0: No clear final manager sign-off block", "expectation": "A clearly separated final manager sign-off block with Name and Date fields."}, {"type": "llm_judge", "name": "Attachment Readiness (Tasks Source)", "description": "Confirms the template anticipates importing/entering tasks from the referenced list (even if that list is not attached here).", "weight": 1.0, "judge_prompt": "Determine if the template makes it obvious where to place the daily tasks from an external list (e.g., an attached Word doc referenced in the instructions). This can be implied by the Task column header, placeholder text, or a note like Enter tasks per today\u2019s task list.\n\nScoring:\n- 1.0: Clear that task names from an external list should go into the Task column/area\n- 0.5: Implied but not explicit\n- 0.0: No clear indication where task names would be entered", "expectation": "Obvious place for task names taken from a separate daily list."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Usability Assessment (LLM)", "description": "Holistic evaluation of professional quality, clarity, and real-world usability at a retail electronics department\u2019s main desk.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Readability", "description": "Visual clarity, hierarchy, spacing, and print-readiness suitable for a store environment.", "weight": 2.0, "judge_prompt": "Judge the professional polish of the PDF: visual hierarchy (titles, headers), clean spacing, alignment, readable fonts, and overall print-readiness. Is it suitable for posting/placing at the department\u2019s main desk?\n\nScoring:\n- 2.0: Professional, clean, highly readable\n- 1.0: Adequate but could be cleaner\n- 0.0: Cluttered or hard to read", "expectation": "Professional, clean, readable layout."}, {"type": "llm_judge", "name": "Operational Usability at the Desk", "description": "Ease of use through the day by multiple employees across shifts.", "weight": 1.5, "judge_prompt": "Evaluate how easy the template is to use during live operations: logical flow from header to assignments to verification; room for handwriting; intuitive columns; minimal ambiguity; works for multiple shifts/employees.\n\nScoring:\n- 1.5: Highly usable throughout the day\n- 0.8: Usable with minor friction\n- 0.0: Confusing or impractical", "expectation": "Intuitive flow and adequate space for manual entries across shifts."}, {"type": "llm_judge", "name": "Completeness for Retail Department Operations", "description": "Covers essential metadata and notes needed for a retail electronics floor.", "weight": 1.5, "judge_prompt": "Assess completeness for daily retail operations: presence of Date, Department/Store, optional Shift/Time, space for multiple employees per task, and a general notes/incidents area.\n\nScoring:\n- 1.5: All essentials present, including a general notes/incidents area\n- 0.8: Most essentials present\n- 0.0: Several essentials missing", "expectation": "Includes operational metadata and a general notes/incidents area."}, {"type": "llm_judge", "name": "Reusability and Adaptability", "description": "Template can be reused daily with minimal edits; neutral wording suitable for various product areas (TVs, computers, appliances).", "weight": 1.0, "judge_prompt": "Is the template day-agnostic and easily reusable? Check for generic, adaptable language (not tied to a single product type) and optional conveniences like version/date in footer or room for additional pages.\n\nScoring:\n- 1.0: Clearly reusable and adaptable\n- 0.5: Mostly reusable with minor issues\n- 0.0: Narrow or one-off design", "expectation": "Day-agnostic, adaptable template suitable across electronics product categories."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "76d10872-9ffa-4ede-83ee-e0f1ec5e2b8d", "rubric": {"category_name": "Government \u2014 Child, Family, and School Social Workers \u2014 New Case Creation Report (Child Support Enforcement)", "rationale": "This rubric enforces a self-documenting, audit-ready PDF report for a new child support case. Stage 1 is a strict LLM-only shape gate mandating a precise document structure aligned to a Case Creation Guide, enabling trivial verification. Stage 2 blends light, deterministic code checks (file parsability, PII safety, dates/amounts plausibility, fuzzy section anchors) with higher-weight LLM judges for correctness and internal consistency. Stage 3 evaluates professional quality, usability for DCS data entry, and compliance. Code rules contribute far less than LLM rules, reflecting their role as precise but narrow checks, while LLM judges assess nuanced cross-references and completeness.", "max_total_score": 18.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (LLM only)", "description": "Gate: Verify the output is a professionally structured New Case Creation Report as a PDF with the exact sections and tables required to enable automated verification. Only checks structure/format, not correctness.", "is_required": true, "max_points": 2.0, "min_score_to_pass": 1.4, "rules": [{"type": "llm_judge", "name": "Structured PDF Report Format Requirement", "description": "Check that the candidate output is a properly structured New Case Creation Report PDF matching the Case Creation Guide layout with all required sections, headers, and tables present.", "weight": 2.0, "judge_prompt": "You are evaluating whether the submitted output is a single report file with the REQUIRED structure for a New Case Creation Report in a child support enforcement context.\n\nFormat requirements:\n- Must be a PDF file (preferred). A DOCX file may receive partial credit but PDF is the target format for submission.\n- At least 2 pages in length (preferably 3+ pages for full coverage).\n- Professional document layout with clear section headers, tables where indicated, page numbers, and a footer or header.\n\nRequired sections and fields (be flexible with exact header wording, but all content must be present and clearly delineated with visible headers):\n1) Cover/Header\n   - Document title clearly indicating \u201cNew Case Creation Report\u201d\n   - Case name referencing Michael Reynolds\n   - Case number (or clearly indicated TBD), Prepared by, Date prepared\n   - Confidentiality notice\n\n2) Section A: Case Information\n   - Case ID/Number (or TBD), Jurisdiction/County, DCS office\n   - Intake/Referral date, Referral source\n\n3) Section B: Parties\n   - Custodial Parent (CP): Name, DOB, SSN Last4 (masked), Address, Phone\n   - Noncustodial Parent (NCP): Michael Reynolds (Name), DOB, SSN Last4 (masked), Address, Phone/Email\n   - Relationship to child(ren)\n\n4) Section C: Child(ren) Details (as a table)\n   - Columns labeled similarly to: Child Name | DOB | Sex | SSN Last4 | CP | NCP\n\n5) Section D: Paternity\n   - Status (Established/Not), Establishment method (Acknowledgment/Genetic Test/Court)\n   - Test/Order date, Lab/Order reference, Probability if genetic test\n\n6) Section E: Employment and Income Verification\n   - Employer name, Address, Phone, FEIN (or unknown), Hire date, Pay frequency\n   - Income source/type; Health insurance availability\n   - Verification method and date\n\n7) Section F: Support Order Summary\n   - Order type (Administrative/Judicial), Order date, Effective date\n   - Amounts: Basic monthly support, Medical support, Child care, Arrears/Retroactive if any\n   - Payment frequency, Payee, Remittance details\n   - Order identifiers (cause/case no.)\n\n8) Section G: Data Entry Checklist (for DCS system fields)\n   - A checklist or table with items like: Parties, Children, Paternity, Order terms, Employer, Health Insurance, Service of Process, Notes \u2014 each marked Entered/Verified/Unknown\n\n9) Section H: Enforcement Readiness and Next Actions\n   - Service of process plan/status; IWO (income withholding) status; Locate efforts; Next review date\n\n10) Section I: Attachments/Reference Index\n   - List of included/relied-upon reference documents (case detail summary, paternity results, support order, guide)\n\n11) Footer/Sign-off\n   - Prepared by/title/signature line/date; Supervisor review line; Page numbers\n\nScoring guidance:\n- 2.0: PDF format AND all 11 sections present with clearly labeled headers; child details are in a table; checklist is a checklist/table; professional pagination.\n- 1.6\u20131.9: All sections structurally present but minor formatting gaps (e.g., DOCX instead of PDF, or slight deviation in tables/pagination).\n- 1.0\u20131.5: Missing 1\u20132 non-core subsections OR unclear delineation of 2\u20133 sections, but overall structure recognizable.\n- 0.1\u20130.9: Major structural gaps (missing 3\u20135 sections) OR wrong format (plain text) while still showing some attempt at the structure.\n- 0.0: Not a document, or severely off-format (no recognizable sections), or under 2 pages.\n\nOnly evaluate presence/structure/format \u2014 do NOT judge correctness of details yet.", "expectation": "A multi-page PDF with the specified sections, clear headers, tables for children and checklist, and a professional layout enabling verification."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Compliance Verification", "description": "Now verify internal consistency, completeness for DCS data entry, and compliance. Mix of light code checks and higher-weight LLM judges. Code checks are deterministic and limited; LLM judges handle nuanced cross-references.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Parsable document and basic metadata presence", "description": "Confirm the primary output is a document (preferably PDF), text is extractable (not scanned-only), and basic header elements exist.", "weight": 0.5, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_document:\\n            return 0.0, 'No document output'\\n\\n        text = ''\\n        is_pdf = False\\n        is_docx = False\\n        # Try PDF first\\n        try:\\n            text = context.files.read_pdf_text(output.id) or ''\\n            if text:\\n                is_pdf = True\\n        except Exception:\\n            pass\\n        if not text:\\n            try:\\n                text = context.files.read_docx_text(output.id) or ''\\n                if text:\\n                    is_docx = True\\n            except Exception:\\n                pass\\n        if not text:\\n            return 0.0, 'Unparsable document or empty text'\\n\\n        score = 0.0\\n        # Format component\\n        if is_pdf:\\n            score += 0.3\\n        elif is_docx:\\n            score += 0.2\\n        # Length heuristic\\n        tlen = len(text)\\n        if tlen >= 1500:\\n            score += 0.2\\n        elif tlen >= 800:\\n            score += 0.1\\n        # Header tokens\\n        header_hits = 0\\n        for tok in ['new case creation report','michael reynolds','prepared by','date']:\\n            if tok in text.lower():\\n                header_hits += 1\\n        score += min(0.0 + header_hits * 0.025, 0.1)\\n\\n        score = min(score, 0.5)\\n        return score, f'pdf={is_pdf}, docx={is_docx}, len={tlen}, header_hits={header_hits}'\\n    except Exception as e:\\n        return 0.0, f'error: {e}'"}, {"type": "code", "name": "Key identities and section anchors coverage", "description": "Check that Michael Reynolds is referenced and that several required section anchors appear (fuzzy match).", "weight": 0.5, "code": "import re\\n\\nREQUIRED_ANCHORS = [\\n    'case information','parties','children','child','paternity','employment',\\n    'support order','data entry checklist','enforcement','next actions','attachments','sign-off','prepared by'\\n]\\n\\ndef evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_document:\\n            return 0.0, 'No document output'\\n        text = ''\\n        try:\\n            text = context.files.read_pdf_text(output.id) or ''\\n        except Exception:\\n            try:\\n                text = context.files.read_docx_text(output.id) or ''\\n            except Exception:\\n                return 0.0, 'Cannot extract text'\\n        tl = text.lower()\\n        name_present = 'michael reynolds' in tl\\n        anchors_found = sum(1 for a in REQUIRED_ANCHORS if a in tl)\\n        # Score 50% for name, 50% for anchors coverage\\n        score = 0.0\\n        if name_present:\\n            score += 0.25\\n        score += min(anchors_found / max(len(REQUIRED_ANCHORS),1) * 0.25, 0.25)\\n        return score, f'name={name_present}, anchors_found={anchors_found}/{len(REQUIRED_ANCHORS)}'\\n    except Exception as e:\\n        return 0.0, f'error: {e}'"}, {"type": "code", "name": "PII safety \u2014 SSN masking check", "description": "Penalize unredacted SSNs. Reward presence of masked SSN patterns and absence of full SSNs.", "weight": 0.5, "code": "import re\\n\\nFULL_SSN_PATTERNS = [\\n    re.compile(r'\\\\b\\\\d{3}-\\\\d{2}-\\\\d{4}\\\\b'),\\n    re.compile(r'(?<![\\\\d])\\\\d{9}(?![\\\\d])')\\n]\\nMASKED_SSN_PATTERNS = [\\n    re.compile(r'(?:XXX|xxx|\\*\\*\\*|###)[-\\\\s]?(?:XX|xx|\\*\\*|##)[-\\\\s]?\\\\d{3,4}')\\n]\\n\\ndef evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_document:\\n            return 0.0, 'No document output'\\n        text = ''\\n        try:\\n            text = context.files.read_pdf_text(output.id) or ''\\n        except Exception:\\n            try:\\n                text = context.files.read_docx_text(output.id) or ''\\n            except Exception:\\n                return 0.0, 'Cannot extract text'\\n        # Count full SSNs\\n        full_hits = 0\\n        for pat in FULL_SSN_PATTERNS:\\n            full_hits += len(pat.findall(text))\\n        masked = any(pat.search(text) for pat in MASKED_SSN_PATTERNS)\\n        if full_hits > 0:\\n            return 0.0, f'Unredacted SSNs found: {full_hits}'\\n        # If no full SSNs: partial credit without masked, full with masked\\n        score = 0.25 if not masked else 0.5\\n        return score, f'masked_present={masked}, full_hits={full_hits}'\\n    except Exception as e:\\n        return 0.0, f'error: {e}'"}, {"type": "code", "name": "Dates and monetary amounts plausibility", "description": "Check for plausible date formats and reasonable child support-related dollar amounts.", "weight": 0.5, "code": "import re\\n\\nDATE_PATS = [\\n    re.compile(r'\\\\b(0?[1-9]|1[0-2])[\\\\/-](0?[1-9]|[12]\\\\d|3[01])[\\\\/-](19|20)\\\\d{2}\\\\b'),\\n    re.compile(r'\\\\b(19|20)\\\\d{2}\\\\b')\\n]\\nMONEY_PAT = re.compile(r'\\\\$\\\\s*([0-9]{1,3}(?:,[0-9]{3})*|[0-9]+)(?:\\\\.[0-9]{2})?')\\n\\ndef _to_float(s):\\n    try:\\n        return float(s.replace(',', ''))\\n    except Exception:\\n        return None\\n\\ndef evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_document:\\n            return 0.0, 'No document output'\\n        text = ''\\n        try:\\n            text = context.files.read_pdf_text(output.id) or ''\\n        except Exception:\\n            try:\\n                text = context.files.read_docx_text(output.id) or ''\\n            except Exception:\\n                return 0.0, 'Cannot extract text'\\n\\n        # Dates\\n        date_hits = 0\\n        for pat in DATE_PATS:\\n            date_hits += len(pat.findall(text))\\n        date_score = 0.25 if date_hits >= 3 else (0.15 if date_hits >= 1 else 0.0)\\n\\n        # Money amounts\\n        amounts = [m.group(1) for m in MONEY_PAT.finditer(text)]\\n        vals = [v for v in (_to_float(x) for x in amounts) if v is not None]\\n        plausible = [v for v in vals if 25 <= v <= 5000]\\n        money_score = 0.25 if len(plausible) >= 2 else (0.15 if len(plausible) == 1 else 0.0)\\n\\n        score = min(0.5, date_score + money_score)\\n        return score, f'dates={date_hits}, money_vals={len(vals)}, plausible_money={len(plausible)}'\\n    except Exception as e:\\n        return 0.0, f'error: {e}'"}, {"type": "llm_judge", "name": "Internal consistency across sections", "description": "Names, identifiers, and key fields must be consistent across the report (e.g., Michael Reynolds, case/jurisdiction identifiers, dates, and amounts).", "weight": 2.0, "judge_prompt": "Evaluate internal consistency of the New Case Creation Report. Check the following:\n- The name Michael Reynolds (NCP) is consistently spelled and referenced across Cover, Parties, Children, and Order sections.\n- Case/jurisdiction identifiers (case ID/cause number, county) are consistent wherever repeated.\n- Dates align logically (e.g., paternity test date precedes order effective date; intake date is not after order date; next review date is after the current date in context of the document).\n- Monetary amounts match across sections (e.g., Support Order Summary vs. any references in Enforcement/Checklist).\nScoring:\n- 2.0: All consistent; no contradictions found.\n- 1.0\u20131.5: Minor inconsistencies (spelling variance or one mismatched date/amount) with otherwise coherent record.\n- 0.1\u20130.9: Multiple inconsistencies that risk incorrect DCS data entry.\n- 0.0: Pervasive contradictions or cannot determine due to missing cross-references.", "expectation": "A report with consistent names, IDs, dates, and amounts throughout."}, {"type": "llm_judge", "name": "Paternity and order details correctness", "description": "Verify paternity status and method align with children listed and the order terms; order details are specific and actionable.", "weight": 2.0, "judge_prompt": "Assess the Paternity and Support Order sections for substantive correctness and actionability:\n- Paternity status is clearly stated per child (if multiple children) with method (Acknowledgment/Genetic Test/Court) and relevant dates/probability if applicable.\n- The Support Order Summary specifies order type, order/effective date, payment frequency, amounts (basic, medical, child care, arrears/retroactive), and payee/remittance details.\n- No contradictions (e.g., paternity not established while a final order requires payment without interim/provisional status explained).\nScoring:\n- 2.0: Clear, complete, non-contradictory, child-specific as needed.\n- 1.0\u20131.5: Minor omissions (e.g., missing lab reference or payee details) but overall actionable.\n- 0.1\u20130.9: Significant gaps that impede entry (e.g., missing order date or amounts).\n- 0.0: Paternity/order content largely missing or contradictory.", "expectation": "Paternity data aligns with children and the order terms are complete and precise."}, {"type": "llm_judge", "name": "Employment verification sufficiency", "description": "Employment section includes enough detail to support IWO issuance and health insurance determination, or documents why unavailable.", "weight": 2.0, "judge_prompt": "Review Employment and Income Verification for sufficiency:\n- Employer name, address, phone, FEIN (or noted unknown), hire date, pay frequency, income type present.\n- Health insurance availability indicated (Yes/No/Unknown) with notes.\n- Verification method and date documented (e.g., wage match, employer contact, new hire report).\n- If not available, clear explanation of locate/verification steps taken and next attempt date.\nScoring:\n- 2.0: Complete and ready for IWO/medical support processing.\n- 1.0\u20131.5: Minor gaps but broadly sufficient.\n- 0.1\u20130.9: Important details missing (e.g., FEIN and no explanation, or no pay frequency).\n- 0.0: Section absent or unusable.", "expectation": "Details sufficient to issue income withholding and assess medical support."}, {"type": "llm_judge", "name": "DCS data entry checklist completeness", "description": "Checklist covers all DCS-required fields; unknowns are explicitly flagged with notes; next actions documented.", "weight": 2.0, "judge_prompt": "Evaluate the Data Entry Checklist and Next Actions:\n- Each required item (Parties, Children, Paternity, Order terms, Employer, Health Insurance, Service of Process, Notes) is marked Entered/Verified/Unknown.\n- Unknown/Not available fields are explicitly labeled and accompanied by a note and next step.\n- Enforcement readiness is clear (e.g., service plan, IWO status) with a concrete next review date.\nScoring:\n- 2.0: All items covered with clear status and next steps.\n- 1.0\u20131.5: Minor omissions in status or notes.\n- 0.1\u20130.9: Several checklist items unaddressed or no next steps.\n- 0.0: Checklist/next actions missing.", "expectation": "Complete, statused checklist and actionable next steps for case initiation."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Readiness", "description": "Holistic assessment of presentation quality, adherence to the guide, clarity, and compliance posture. LLM judges only.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professionalism, tone, and readability", "description": "Assess professional tone, grammar, layout consistency, and readability for internal record-keeping and review.", "weight": 1.5, "judge_prompt": "Evaluate professionalism and readability:\n- Clear, concise writing; professional tone suited to government case documentation.\n- Minimal grammar/spelling errors; consistent fonts, spacing, and headings.\n- Logical flow and scannability with tables and bullets where appropriate.\nScore 1.5 = exemplary; 0.8\u20131.2 = minor issues; 0.1\u20130.7 = noticeable issues; 0.0 = poor/unclear.", "expectation": "A clean, well-structured, professional report that is easy to read and scan."}, {"type": "llm_judge", "name": "Adherence to Case Creation Guide layout and labeling", "description": "How closely the document follows the guide\u2019s layout, labeling, and table/checklist conventions.", "weight": 1.5, "judge_prompt": "Compare the document structure to the Case Creation Guide conventions:\n- Section order and labeling generally match.\n- Children details are presented in a proper table; Checklist appears as a checklist/table.\n- Cover elements, sign-off, pagination present.\nScore 1.5 = strong adherence; 0.8\u20131.2 = minor deviations; 0.1\u20130.7 = several deviations; 0.0 = weak/no adherence.", "expectation": "Strong alignment with the guide\u2019s layout and labeling to streamline DCS data entry."}, {"type": "llm_judge", "name": "Compliance and privacy posture", "description": "Evaluate whether the document handles PII appropriately and includes necessary notices and redactions.", "weight": 1.5, "judge_prompt": "Assess compliance/privacy posture:\n- Confidentiality notice visibly present.\n- SSNs are masked (Last4 only) and minors\u2019 PII is handled appropriately (no unnecessary exposure).\n- Sensitive info limited to what\u2019s required for entry; addresses/phones included only where needed.\nScore 1.5 = fully compliant; 0.8\u20131.2 = minor lapses; 0.1\u20130.7 = notable concerns; 0.0 = noncompliant.", "expectation": "Clear confidentiality notice and appropriate PII redaction/masking."}, {"type": "llm_judge", "name": "Clarity and actionability of next steps", "description": "Are next actions clear, time-bound, and sufficient to initiate enforcement/service?", "weight": 1.5, "judge_prompt": "Review Enforcement Readiness/Next Actions for clarity and actionability:\n- Specific, time-bound tasks (e.g., issue IWO within X days, attempt employer contact by DATE, initiate service via method, set review date).\n- Responsibilities assigned (who will act) if indicated by the guide.\n- No ambiguity in what happens next to start enforcement.\nScore 1.5 = very clear and actionable; 0.8\u20131.2 = mostly clear; 0.1\u20130.7 = vague; 0.0 = missing.", "expectation": "Concrete, time-bound steps suitable for immediate case initiation."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "02314fc6-a24e-42f4-a8cd-362cae0f0ec1", "rubric": {"category_name": "Retail Store Safety Checklist (GM/DM/LP Monthly Review)", "rationale": "Task Type: Pattern B (Document). Deliverable: a completed, professional PDF checklist document with structured sections, tables, scoring summary, and corrective action plan. Stage 1 uses LLM-only gating to enforce exact structure that makes verification trivial. Stage 2 mixes light code checks (numeric and CAP consistency) with stronger LLM verification of coverage and logic. Stage 3 applies holistic quality assessment for usability, professionalism, and operational fit. Code rules are limited and weighted ~5x less than LLM rules in Stage 2, per guidance.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (PDF Structure and Sections)", "description": "LLM-only gate to enforce exact document format and structure so later verification is trivial. If this stage fails, the entire category is zeroed.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "PDF Format + Mandatory Sections and Tables", "description": "Verify the output is a multi-page PDF checklist with specific sections, tables, metadata, scoring summary, and CAP template.", "weight": 4.0, "judge_prompt": "You are evaluating whether the candidate output satisfies strict STRUCTURE requirements for a monthly Retail Store Safety Checklist. Only check PRESENCE and STRUCTURE (not content quality). The output MUST be a PDF and include all listed elements. Be flexible with similar section names and column variants, but insist on the presence of each required area and key structures.\n\nRequired FORMAT:\n- Must be a PDF (not Word/Excel/plain text)\n- At least 3 pages\n- Professional, clearly sectioned, with tables for checklist items\n\nRequired FRONT-MATTER METADATA (can be on page 1):\n- Title indicating Monthly Safety Checklist or equivalent\n- Store metadata: Store Name/ID, Address/Location, Month/Year of audit\n- Prepared by (Safety Coordinator or Store Management) with date\n- Reviewers with signature/date lines: General Manager (GM), District Manager (DM), Loss Prevention (LP)\n\nRequired SECTIONS (accept close variants):\n1) Parking Lot, Sidewalks & Ramps\n2) General Store Conditions\n3) First Aid & Emergency Procedures\n4) Safety and Compliance\n5) Food Safety\n6) Fire Prevention and Protection\n7) Record Keeping & Posters\n8) Scoring and Follow-up\n\nFor EACH checklist area (1\u20137), verify a TABLE exists with columns similar to:\n- Item #/ID\n- Inspection Item / Criterion\n- Status (Yes/No/NA OR Pass/Fail/NA \u2014 checkboxes acceptable)\n- Notes/Evidence\n- Severity/Priority/Risk (or equivalent)\n- Responsible Owner\n\nScoring and Follow-up section must include:\n- A summary table/box showing: Total items, # Compliant/Pass, # Missed/Non-compliant, # N/A\n- A clear threshold statement: \u201cStores can miss up to 10 items; more than 10 requires a corrective action plan (CAP)\u201d (close wording ok)\n- Overall status/result (e.g., Pass / Needs Targeted Improvement)\n- A Corrective Action Plan (CAP) template/section with columns similar to: Issue/Gap, Corrective Action, Owner, Due Date, Completion Date, Verification/Reviewed by (DM/LP)\n\nScoring for this rule (return a score from 0 to 4):\n- 4.0: PDF format, 3+ pages, all metadata present, all 7 checklist areas each with a table, AND complete Scoring & Follow-up (summary + threshold statement + CAP template)\n- 3.0: PDF format, 3+ pages, all 7 areas with tables, but minor metadata omission OR Scoring & Follow-up missing one sub-element (e.g., missing overall status or one CAP column)\n- 2.0: PDF format, 3+ pages, but missing one required checklist area OR Scoring & Follow-up largely incomplete\n- 0.0: Not a PDF, fewer than 3 pages, or missing multiple required areas/tables\n\nOnly evaluate structure/presence, not correctness of calculations or prose quality.", "expectation": "A multi-page PDF with all specified sections, tables per area, full metadata, and a Scoring & Follow-up section including a CAP template."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Consistency, Coverage, and Logic)", "description": "Now that the structure is enforced, verify numeric consistency, threshold logic, and adequacy of coverage. Mix of light code checks and LLM judgment. Code rules carry ~5x less weight than LLM rules.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Scoring Numbers and Threshold Language Consistency", "description": "Parse PDF text to check basic numeric consistency in the scoring summary and presence of the 10-miss threshold statement.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No primary document output.\"\n    # Try to extract text from PDF, then DOCX as fallback\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0, \"Unable to read document text.\"\n    if not text:\n        return 0.0, \"Empty document text.\"\n\n    t = text.lower()\n\n    def find_int(patterns):\n        for p in patterns:\n            m = re.search(p, t)\n            if m:\n                try:\n                    return int(m.group(1))\n                except Exception:\n                    pass\n        return None\n\n    total = find_int([\n        r\"total\\s+(?:checklist\\s+)?items\\s*[:\\-]?\\s*(\\d+)\",\n        r\"items\\s+total\\s*[:\\-]?\\s*(\\d+)\"\n    ])\n    compliant = find_int([\n        r\"compliant\\s*[:\\-]?\\s*(\\d+)\",\n        r\"pass\\s*[:\\-]?\\s*(\\d+)\",\n        r\"yes\\s*[:\\-]?\\s*(\\d+)\"\n    ])\n    missed = find_int([\n        r\"non[\\-\\s]?compliant\\s*[:\\-]?\\s*(\\d+)\",\n        r\"missed\\s*[:\\-]?\\s*(\\d+)\",\n        r\"fail\\s*[:\\-]?\\s*(\\d+)\",\n        r\"no\\s*[:\\-]?\\s*(\\d+)\"\n    ])\n    na = find_int([\n        r\"n/?a\\s*[:\\-]?\\s*(\\d+)\",\n        r\"not\\s+applicable\\s*[:\\-]?\\s*(\\d+)\"\n    ])\n\n    score = 0.0\n    notes = []\n\n    # Basic plausibility\n    if total is not None and missed is not None:\n        if 0 <= missed <= total:\n            score += 0.4\n            notes.append(\"Missed count not exceeding total: OK\")\n        else:\n            notes.append(\"Missed count exceeds total or negative.\")\n    else:\n        notes.append(\"Could not parse total and missed values.\")\n\n    # Sum check if we have all\n    if all(v is not None for v in [total, compliant, missed, na]):\n        if compliant + missed + na == total:\n            score += 0.4\n            notes.append(\"Compliant + Missed + N/A equals Total: OK\")\n        else:\n            notes.append(\"Counts do not sum to Total.\")\n    else:\n        notes.append(\"Incomplete counts for sum check.\")\n\n    # Threshold language presence\n    threshold_present = bool(re.search(r\"(up to|no more than|<=)\\s*10\\s*(miss|misses|non[\\-\\s]?compliant|fail)s?\", t)) or \\\n                         bool(re.search(r\"more\\s+than\\s+10\\s+.*(requires|must|needs).*(corrective\\s+action|cap)\", t))\n    if threshold_present:\n        score += 0.2\n        notes.append(\"Threshold language (10 misses) found.\")\n    else:\n        notes.append(\"Threshold language not found.\")\n\n    score = max(0.0, min(1.0, score))\n    return score, \"; \".join(notes)\n"}, {"type": "code", "name": "Corrective Action Plan (CAP) Elements Present and Dated", "description": "Verify CAP section exists with key columns and at least one due/completion date. If the text indicates >10 misses, CAP presence becomes critical.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No primary document output.\"\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0, \"Unable to read document text.\"\n    if not text:\n        return 0.0, \"Empty document text.\"\n\n    t = text.lower()\n    score = 0.0\n    notes = []\n\n    cap_present = any(k in t for k in [\"corrective action plan\", \"action plan\", \"cap \", \" cap\\n\", \"cap:\"])\n    if cap_present:\n        score += 0.3\n        notes.append(\"CAP section referenced.\")\n    else:\n        notes.append(\"No explicit CAP reference found.\")\n\n    headers = [\"issue\", \"gap\", \"corrective action\", \"owner\", \"responsible\", \"due date\", \"completion date\", \"status\", \"verification\", \"reviewed by\", \"dm\", \"lp\"]\n    header_hits = sum(1 for h in headers if h in t)\n    if header_hits >= 4:\n        score += 0.5\n        notes.append(\"CAP-like headers present (>=4).\")\n    else:\n        notes.append(\"Insufficient CAP headers.\")\n\n    date_hits = re.findall(r\"\\b(\\d{4}-\\d{2}-\\d{2}|\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4})\\b\", t)\n    if len(date_hits) >= 1:\n        score += 0.2\n        notes.append(\"At least one date found (due/completion).\")\n    else:\n        notes.append(\"No due/completion dates found.\")\n\n    # If text suggests >10 misses, CAP becomes critical\n    more_than_10 = False\n    m = re.search(r\"miss(ed|es)?\\s*[:\\-]?\\s*(\\d+)\", t)\n    if m:\n        try:\n            more_than_10 = int(m.group(2)) > 10\n        except Exception:\n            more_than_10 = False\n    if (\"more than 10\" in t) or more_than_10:\n        if not cap_present or header_hits < 4:\n            # Cap the score if CAP is missing when required\n            score = min(score, 0.2)\n            notes.append(\">10 misses indicated but CAP structure insufficient.\")\n\n    score = max(0.0, min(1.0, score))\n    return score, \"; \".join(notes)\n"}, {"type": "llm_judge", "name": "Checklist Depth and Specificity Across All Areas", "description": "Confirm each required area has sufficient, concrete, observable items (measurable criteria).", "weight": 2.5, "judge_prompt": "Evaluate the CHECKLIST CONTENT DEPTH for each required area:\n- Parking Lot, Sidewalks & Ramps\n- General Store Conditions\n- First Aid & Emergency Procedures\n- Safety and Compliance\n- Food Safety\n- Fire Prevention and Protection\n- Record Keeping & Posters\n\nCriteria:\n- Each area should include multiple (ideally 5+) specific, observable items (e.g., \u201cLot lighting functional; no burnt bulbs,\u201d \u201cNo trip hazards; sidewalks even,\u201d \u201cThermometer calibration within X\u00b0F,\u201d \u201cFire exits unobstructed; exit signage illuminated\u201d).\n- Items should be written as objective checks (yes/no, pass/fail) with measurable criteria, not vague statements.\n- Tables should allow Notes/Evidence to support findings.\n\nScoring (0\u20132.5):\n- 2.5: All 7 areas contain adequate, concrete items (5+ each) with clear observables and notes/evidence support.\n- 1.7: Most areas are well-detailed; 1\u20132 areas thin (e.g., <5 items or vague wording).\n- 0.8: Several areas are sparse or mostly generic language.\n- 0.0: Many areas missing or items are largely non-specific.\n\nOnly assess adequacy and specificity, not stylistic polish.", "expectation": "Each area lists numerous, concrete, measurable checklist items with observable criteria and space for notes/evidence."}, {"type": "llm_judge", "name": "Threshold Logic, Roles, and Follow-up Accountability", "description": "Verify the 10-miss threshold is clearly applied and responsibilities are explicit for GM/DM/LP with timelines and verification steps.", "weight": 2.5, "judge_prompt": "Evaluate the SCORING AND FOLLOW-UP LOGIC:\n- Does the summary clearly state: up to 10 misses allowed; more than 10 requires a Corrective Action Plan (CAP)?\n- Is the overall status/result clear (e.g., Pass / Targeted Improvement Needed)?\n- Are roles and responsibilities explicit for GM, DM, and LP (who submits, who reviews, who verifies completion)?\n- Are follow-up actions time-bound with due dates and verification steps (e.g., DM checks by date, LP validation, evidence required)?\n\nScoring (0\u20132.5):\n- 2.5: Clear threshold logic, explicit responsibilities (GM/DM/LP), time-bound CAP with verification/closure steps.\n- 1.7: Threshold present and some roles/timeframes defined but partial clarity.\n- 0.8: Threshold mentioned but accountability or timelines unclear.\n- 0.0: Threshold not stated or follow-up process unclear.", "expectation": "Clear application of the 10-miss rule with defined roles (GM/DM/LP), timelines, and verification requirements."}, {"type": "llm_judge", "name": "Regulatory Alignment: Food Safety and Fire Protection Essentials", "description": "Check presence of critical elements aligned to food safety and fire protection best practices.", "weight": 2.0, "judge_prompt": "Focus ONLY on whether critical compliance elements are present within the appropriate sections:\n- Food Safety: cold/hot holding temps, thermometer calibration, date marking/FIFO, allergen segregation, sanitation (sanitizer ppm), handwashing facilities, pest activity/logs, product condition (swollen/dented cans), employee illness reporting.\n- Fire Prevention/Protection: clear egress, exit signage illumination, fire doors not propped, extinguishers present with in-service tags and monthly checks, sprinkler/ansul hood unobstructed, electrical panels clear, no daisy-chained cords.\n\nScoring (0\u20132.0):\n- 2.0: Most of the elements above are explicitly included in the relevant sections.\n- 1.2: Several elements present but notable gaps.\n- 0.5: Few critical elements present.\n- 0.0: Largely missing or only generic statements.", "expectation": "Food Safety and Fire Protection sections include explicit critical checks reflecting standard regulatory expectations."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism", "description": "Holistic assessment of presentation quality, usability, and operational fit for monthly enterprise use.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Usability", "description": "Layout, legibility, and ease-of-use in a busy retail environment.", "weight": 2.0, "judge_prompt": "Assess the document\u2019s professional polish and usability:\n- Clean, consistent layout with readable fonts, clear tables, page headers/footers, and logical flow\n- Obvious checkboxes or status fields (Yes/No/NA or Pass/Fail/NA)\n- Clear instructions for how to complete the checklist and attach evidence\n- Minimal clutter; easy to use during a walk-through\n\nScoring (0\u20132.0):\n- 2.0: Highly professional, easy to read and use; clear instructions.\n- 1.2: Generally professional with minor usability issues.\n- 0.5: Noticeable layout or clarity issues that hinder use.\n- 0.0: Poorly formatted or difficult to use.", "expectation": "A clean, professional, easy-to-use checklist with clear instructions and status fields."}, {"type": "llm_judge", "name": "Clarity and Accessibility", "description": "Plain language, clarity of items, and accessibility for store teams.", "weight": 2.0, "judge_prompt": "Evaluate clarity and accessibility:\n- Plain, unambiguous language; avoids jargon\n- Items are concise and action-focused\n- If present, support for accessibility (e.g., clear icons, high contrast) or multilingual notes\n- Guidance for evidence capture (photos, work order IDs)\n\nScoring (0\u20132.0):\n- 2.0: Very clear, accessible, and action-oriented; evidence capture guidance present.\n- 1.2: Mostly clear with minor ambiguity; partial guidance.\n- 0.5: Several unclear items or lack of guidance.\n- 0.0: Confusing or inaccessible.", "expectation": "Concise, unambiguous, action-oriented language with guidance for capturing evidence."}, {"type": "llm_judge", "name": "Operational Fit and Scalability", "description": "Suitability for monthly use across multiple stores and departments.", "weight": 2.0, "judge_prompt": "Assess operational suitability:\n- Includes month/year, version control, and store identifiers\n- Time-efficient structure (grouped by area, logical sequence)\n- Scales to multiple locations (generic enough yet comprehensive)\n- Digital readiness (fillable fields, signatures, or clear process for routing to GM/DM/LP)\n\nScoring (0\u20132.0):\n- 2.0: Strong operational fit, versioned, fillable or clearly routable, scalable across stores.\n- 1.2: Generally suitable with minor scalability/process gaps.\n- 0.5: Weak scalability or lacks clear routing/versioning.\n- 0.0: Not practical for monthly enterprise use.", "expectation": "Versioned, store-identified, time-efficient, and scalable format with clear routing for GM/DM/LP."}, {"type": "llm_judge", "name": "Completeness and Actionability", "description": "Whether the checklist drives action and continuous improvement.", "weight": 2.0, "judge_prompt": "Evaluate whether the checklist drives action:\n- Items map to actionable behaviors/conditions; include risk/severity to prioritize\n- CAP enables tracking to closure (owner, due date, verification)\n- Space for photos/attachments or references (e.g., ticket/work order IDs)\n- Contact/escalation info for incidents or urgent hazards\n\nScoring (0\u20132.0):\n- 2.0: Highly actionable; clear prioritization and closure mechanisms.\n- 1.2: Actionable with minor gaps.\n- 0.5: Limited actionability; weak prioritization/closure mechanisms.\n- 0.0: Lacks mechanisms to drive corrective action.", "expectation": "Action-focused checklist with prioritization, closure tracking, and escalation pathways."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "15d37511-75c5-4c7f-81f1-16e00c0d95f3", "rubric": {"category_name": "Year-1 Revenue and Gross Margin Projection (VerteCleanUV \u2192 GloNGroRealEstate)", "rationale": "Pattern A (Analytical). The output must be a self-documenting Excel model with explicit assumptions and clearly structured summary tables. Stage 1 (LLM-only) enforces a strict, verifiable shape. Stage 2 mixes lightweight code checks (arithmetic/bounds) with heavier LLM verification (tier logic, consumables inclusion, internal consistency). Stage 3 assesses executive-ready quality and strategic clarity.", "max_total_score": 16.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate", "description": "MANDATORY structure for a self-documenting financial model. If the shape is wrong, verification is impossible.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.0, "rules": [{"type": "llm_judge", "name": "Required Excel Structure Present", "description": "Check that the candidate output is a single Excel file with the mandated sheets/sections/tables that make verification trivial.", "weight": 3.0, "judge_prompt": "You are verifying ONLY the presence/structure of the delivered file, not correctness of any numbers.\n\nRequirement: The output MUST be an Excel workbook (.xlsx is preferred) that includes the following structure. Be flexible with exact section names but look for the elements listed below.\n\nSheet 1: \"Year 1 Projection\" (or similar like \"Y1 Projection\", \"Year One Projection\")\nRequired visible sections and elements:\n1) Assumptions & Volume\n   - Clearly shows client-stated Year 1 device volume \u2248 2,000 total across two products\n   - States or shows tiered pricing policy with two tiers: <1,000 units and >1,000 units (or \"1,000+\"), including the rule that a discount applies in the >1,000 tier\n   - Notes that two products are included: BrightzoneUV Duct and BrightzoneUV Ceiling\n\n2) Pricing Tiers Reference\n   - A small table or clearly labeled block that documents tier thresholds and the retail and cost (or discount) applied for each tier. Per-product pricing tiers are acceptable if the model shows both products. Flexible naming (e.g., \"Tiered Pricing\", \"Price Tiers\", \"Pricing Levels\").\n\n3) Product Margin Summary (Devices)\n   - One clearly labeled table for devices containing columns equivalent to:\n     [Product Name | Quantity | Marketplace Retail Price (per unit) | GloNGroRealEstate Product Cost (per unit) | Margin $ per unit | Margin % | Total Gross Margin $]\n   - Must include at least two rows: BrightzoneUV Duct and BrightzoneUV Ceiling\n\n4) Consumables Margin Summary\n   - A second table (can be on the same sheet) for consumables required annually\n   - Columns equivalent to the device table (OK if labeled similarly)\n\n5) Year 1 Totals / Summary\n   - A clearly labeled subtotal/total area that rolls up Year 1: Total Units, Total Revenue, and Total Gross Margin (devices + consumables). Flexible naming (e.g., \"Year 1 Totals\", \"Y1 Summary\", \"Annual Total\").\n\nOptional but helpful (do not penalize if missing):\n- Sheet 2: \"Notes & Sources\" (or similar), listing assumptions from the pricing email, definitions, and how tier selection was applied.\n\nScoring:\n- 3.0: Excel format AND all 5 required elements present and clearly labeled.\n- 2.0: Excel format AND 4 of 5 required elements present (minor naming differences OK).\n- 1.0: Excel format AND only 3 of 5 required elements present.\n- 0.0: Not an Excel file OR fewer than 3 required elements present.\n\nOnly check presence/format/structure, not any numerical correctness.", "expectation": "A clean, well-structured Excel workbook with Year 1 assumptions, tier tables, two device line items, consumables table, and a clear Year 1 totals roll-up."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness Verification", "description": "Verify internal arithmetic, plausibility, and consistency with tiered pricing and consumables inclusion.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Device Table Arithmetic (Margin/unit and Line Total)", "description": "Recalculate margin $/unit = retail - cost and Total Gross Margin = margin/unit * quantity for device rows; award credit if most rows match within tolerance.", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n    except Exception as e:\n        return 0.0, f\"Failed to open Excel: {e}\"\n\n    def normalize(s):\n        return re.sub(r\"\\s+\", \" \", str(s).strip().lower())\n\n    # Try to locate a table with expected headers on any sheet\n    expected_keys = {\n        'product': ['product', 'product name'],\n        'qty': ['qty', 'quantity', 'units'],\n        'retail': ['retail', 'price', 'marketplace retail'],\n        'cost': ['cost', 'product cost', 'glo'],\n        'margin_unit': ['margin $ per unit', 'margin per unit', 'unit margin'],\n        'margin_pct': ['margin %', 'margin percent', 'margin%'],\n        'total_gm': ['total gross margin', 'gross margin $', 'line gross margin']\n    }\n\n    def header_match(row_vals):\n        headers = [normalize(v) for v in row_vals]\n        def find_any(options):\n            return any(any(k in h for k in [normalize(o) for o in options]) for h in headers)\n        found = {k: find_any(v) for k, v in expected_keys.items()}\n        return found\n\n    def extract_table(df):\n        # scan first 30 rows for a header row containing most key columns\n        for r in range(min(30, len(df))):\n            found = header_match(df.iloc[r, :].tolist())\n            # Require at least product, qty, retail, cost, margin_unit, total_gm\n            needed = ['product','qty','retail','cost','margin_unit','total_gm']\n            if all(found.get(k, False) for k in needed):\n                header = df.iloc[r, :]\n                body = df.iloc[r+1:, :].copy()\n                body.columns = header\n                # Drop fully empty rows\n                body = body.dropna(how='all')\n                return body\n        return None\n\n    tables = []\n    for sn in xls.sheet_names:\n        try:\n            df = pd.read_excel(path, sheet_name=sn, header=None)\n        except Exception:\n            continue\n        tbl = extract_table(df)\n        if tbl is not None:\n            tables.append(tbl)\n\n    if not tables:\n        return 0.0, \"Could not locate a device table with expected headers\"\n\n    # Use the first detected table\n    tbl = tables[0].copy()\n    # Normalize column names for access\n    cols_map = {}\n    for c in tbl.columns:\n        cl = normalize(c)\n        cols_map[cl] = c\n\n    def pick_col(options):\n        for k in options:\n            for col_norm, orig in cols_map.items():\n                if k in col_norm:\n                    return orig\n        return None\n\n    c_prod = pick_col(['product'])\n    c_qty = pick_col(['quantity','qty','units'])\n    c_retail = pick_col(['marketplace retail','retail','price'])\n    c_cost = pick_col(['product cost','cost'])\n    c_mu = pick_col(['margin $ per unit','margin per unit','unit margin'])\n    c_tgm = pick_col(['total gross margin','gross margin $','line gross margin'])\n\n    if not all([c_prod, c_qty, c_retail, c_cost, c_mu, c_tgm]):\n        return 0.2, \"Partial headers found; limited checks applied\"\n\n    # Coerce numerics\n    q = pd.to_numeric(tbl[c_qty], errors='coerce')\n    p = pd.to_numeric(tbl[c_retail], errors='coerce')\n    c = pd.to_numeric(tbl[c_cost], errors='coerce')\n    mu = pd.to_numeric(tbl[c_mu], errors='coerce')\n    tgm = pd.to_numeric(tbl[c_tgm], errors='coerce')\n\n    # Focus on rows that look like device rows (non-null product and numeric fields)\n    mask = (~tbl[c_prod].isna()) & q.notna() & p.notna() & c.notna()\n    if mask.sum() == 0:\n        return 0.0, \"No valid rows to check\"\n\n    calc_mu = (p - c).round(2)\n    calc_tgm = (calc_mu * q).round(2)\n\n    mu_ok = (mu.round(2) - calc_mu.abs()).abs() <= 0.02\n    tgm_ok = (tgm.round(2) - calc_tgm.abs()).abs() <= 0.02\n\n    ok_rows = (mu_ok & tgm_ok & mask).sum()\n    total_rows = mask.sum()\n    if total_rows == 0:\n        return 0.0, \"No rows matched for arithmetic checks\"\n\n    score = ok_rows / total_rows\n    return float(score), f\"Arithmetic checks passed on {ok_rows}/{total_rows} rows\""}, {"type": "code", "name": "Margin % Formula Coherence", "description": "Check margin % \u2248 (margin per unit)/(retail price) within tolerance for rows with valid numerics.", "weight": 0.4, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n    except Exception:\n        return 0.0\n\n    def normalize(s):\n        return re.sub(r\"\\s+\", \" \", str(s).strip().lower())\n\n    def find_table(df):\n        # look for a row with margin % and margin per unit and retail\n        for r in range(min(30, len(df))):\n            row = [normalize(v) for v in df.iloc[r,:].tolist()]\n            if any('margin %' in v or 'margin%' in v or 'margin percent' in v for v in row):\n                if any('margin per unit' in v or 'unit margin' in v for v in row) and any('retail' in v or 'price' in v for v in row):\n                    header = df.iloc[r,:]\n                    body = df.iloc[r+1:,:].copy()\n                    body.columns = header\n                    return body\n        return None\n\n    table = None\n    for sn in xls.sheet_names:\n        try:\n            df = pd.read_excel(path, sheet_name=sn, header=None)\n        except Exception:\n            continue\n        t = find_table(df)\n        if t is not None:\n            table = t\n            break\n    if table is None:\n        return 0.0\n\n    cols = {normalize(c): c for c in table.columns}\n    def pick(*keys):\n        for k in keys:\n            for cn, orig in cols.items():\n                if k in cn:\n                    return orig\n        return None\n\n    c_mu = pick('margin $ per unit','margin per unit','unit margin')\n    c_pct = pick('margin %','margin percent','margin%')\n    c_retail = pick('marketplace retail','retail','price')\n    if not all([c_mu, c_pct, c_retail]):\n        return 0.0\n\n    mu = pd.to_numeric(table[c_mu], errors='coerce')\n    pct = pd.to_numeric(table[c_pct].astype(str).str.replace('%','', regex=False), errors='coerce')\n    retail = pd.to_numeric(table[c_retail], errors='coerce')\n\n    mask = mu.notna() & pct.notna() & retail.notna() & (retail > 0)\n    if mask.sum() == 0:\n        return 0.0\n\n    calc_pct = (mu / retail) * 100.0\n    ok = (calc_pct - pct).abs() <= 1.0  # within 1 percentage point\n    score = ok[mask].mean() if mask.sum() else 0.0\n    return float(score)"}, {"type": "code", "name": "Year-1 Device Volume Near 2,000 and Both Products Present", "description": "Sum device quantities; check total near 2,000 and presence of both BrightzoneUV Duct and BrightzoneUV Ceiling.", "weight": 0.4, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"Not a spreadsheet\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n    except Exception as e:\n        return 0.0, f\"Open error: {e}\"\n\n    def normalize(s):\n        return re.sub(r\"\\s+\", \" \", str(s).strip().lower())\n\n    total_qty = 0.0\n    found_duct = False\n    found_ceiling = False\n\n    for sn in xls.sheet_names:\n        try:\n            df = pd.read_excel(path, sheet_name=sn)\n        except Exception:\n            continue\n        # try to find product and quantity columns\n        cols_norm = {normalize(c): c for c in df.columns}\n        prod_col = None\n        qty_col = None\n        for cn, orig in cols_norm.items():\n            if 'product' in cn:\n                prod_col = orig\n            if 'qty' in cn or 'quantity' in cn or 'units' in cn:\n                qty_col = orig\n        if prod_col is None or qty_col is None:\n            continue\n        prods = df[prod_col].astype(str).str.lower()\n        qty = pd.to_numeric(df[qty_col], errors='coerce')\n        total_qty += qty.fillna(0).sum()\n        if prods.str.contains('duct').any():\n            found_duct = True\n        if prods.str.contains('ceiling').any():\n            found_ceiling = True\n\n    score = 0.0\n    # presence of both products\n    if found_duct and found_ceiling:\n        score += 0.4\n    # volume near 2,000 (allow 1,500\u20132,500)\n    if 1500 <= total_qty <= 2500:\n        score += 0.6\n    return score"}, {"type": "code", "name": "Numeric Sanity and Non-Negative Checks", "description": "Ensure key numeric fields (retail, cost, quantity, totals) are non-negative and within plausible ranges; reward partial if most pass.", "weight": 0.3, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n    except Exception:\n        return 0.0\n\n    def normalize(s):\n        return re.sub(r\"\\s+\", \" \", str(s).strip().lower())\n\n    checks = []\n    for sn in xls.sheet_names:\n        try:\n            df = pd.read_excel(path, sheet_name=sn)\n        except Exception:\n            continue\n        for c in df.columns:\n            cn = normalize(c)\n            if any(k in cn for k in ['retail','price','cost','qty','quantity','units','gross margin']):\n                s = pd.to_numeric(df[c], errors='coerce')\n                if s.notna().sum() == 0:\n                    continue\n                nonneg_ratio = (s.dropna() >= 0).mean()\n                checks.append(nonneg_ratio)\n                # basic plausibility: margin% if present should be <= 100\n                if 'margin %' in cn or 'margin percent' in cn or 'margin%' in cn:\n                    pct = pd.to_numeric(df[c].astype(str).str.replace('%','', regex=False), errors='coerce')\n                    if pct.notna().sum() > 0:\n                        checks.append((pct.dropna() <= 100).mean())\n    if not checks:\n        return 0.0\n    return float(np.mean(checks))"}, {"type": "llm_judge", "name": "Tiered Pricing Selection and Consistency", "description": "Verify that the model explicitly selects the >1,000 units tier for Year 1 (given ~2,000 devices) and applies that tier consistently for both products and their consumables per the documented tier table within the workbook.", "weight": 2.2, "judge_prompt": "Check the workbook for the documented tiered pricing logic. Considering Year 1 total volume is approximately 2,000 devices, verify:\n- The model clearly selects the >1,000 units tier (or equivalent) for Year 1 pricing.\n- The same tier logic is applied consistently across both BrightzoneUV Duct and BrightzoneUV Ceiling.\n- Consumables pricing follows the documented tier logic when applicable (or clearly states if consumables have separate pricing).\nScoring:\n- 2.2: Tier documentation present AND >1,000 tier is selected and applied consistently across both device products (and consumables, if applicable).\n- 1.3: Tier documentation present AND mostly consistent application with only minor issues.\n- 0.7: Tier documentation present but unclear or inconsistently applied.\n- 0.0: No clear tier documentation or wrong tier used for ~2,000 volume.", "expectation": "A visible tier table and a clear selection of the >1,000 tier with consistent use across products."}, {"type": "llm_judge", "name": "Consumables Included and Annualized Correctly", "description": "Confirm consumables are included for both products with Year 1 annual quantities and margins, and included in the Year 1 totals.", "weight": 2.1, "judge_prompt": "Evaluate whether a separate consumables section/table exists and is included in Year 1 totals:\n- Both BrightzoneUV Duct and BrightzoneUV Ceiling consumables are listed.\n- Quantities reflect Year 1 installed base (e.g., per-device annual requirement) and are clearly annualized.\n- Margin columns (per unit, %, and total gross margin) are filled and appear arithmetically coherent.\n- The Year 1 total section/roll-up explicitly includes consumables.\nScoring:\n- 2.1: All criteria met with clear inclusion in Year 1 totals.\n- 1.2: Consumables included but minor clarity or inclusion issues.\n- 0.6: Consumables partially included or unclear annualization.\n- 0.0: Consumables missing or not included in totals.", "expectation": "Clear consumables table tied to Year 1 base and correctly rolled up into totals."}, {"type": "llm_judge", "name": "Roll-up Totals Internal Consistency", "description": "Cross-check that the Year 1 totals (units, revenue, and gross margin) correctly roll up the device and consumables tables with no visible double counting or omissions.", "weight": 2.1, "judge_prompt": "Visually verify the integrity of the Year 1 roll-up totals:\n- The Year 1 totals section includes both devices and consumables.\n- Totals appear to be the sum of the lines in the device and consumables tables (spot-check reasonable sums; do not over-penalize minor rounding).\n- No obvious double counting (e.g., a product appearing in both device table and consumables treated as device again).\nScoring:\n- 2.1: Roll-up is clear and consistent with line items.\n- 1.2: Mostly consistent with small discrepancies or ambiguous labeling.\n- 0.6: Significant ambiguity in what is included in totals.\n- 0.0: Roll-up totals appear incorrect or exclude major components.", "expectation": "A clean Year 1 total that transparently sums devices + consumables, free from double counting."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Executive Readiness", "description": "Assess presentation quality, clarity for executives, and strategic usefulness.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Executive-Ready Presentation", "description": "Is the workbook clear, professional, and easy for an executive to digest in minutes?", "weight": 1.4, "judge_prompt": "Evaluate the presentation quality:\n- Clear section headers and table labels.\n- Currency/percentage formatting applied appropriately.\n- Minimal clutter; logical flow from assumptions to summaries.\n- Print/PDF-ready layout (readable without deep digging).\nScoring: 1.4 excellent, 0.9 good, 0.5 fair, 0.0 poor.", "expectation": "Clean formatting, proper number formats, and clear labeling suitable for an executive audience."}, {"type": "llm_judge", "name": "Audience Appropriateness and Clarity of Assumptions", "description": "Assumptions are clear and succinct for a VP audience; key drivers are surfaced without unnecessary detail.", "weight": 1.3, "judge_prompt": "Assess whether assumptions and key drivers are well-communicated:\n- Year 1 volume (~2,000 devices) highlighted.\n- Tier thresholds and discount logic plainly stated.\n- Any product-specific notes (e.g., ceiling vs. duct differences) succinctly explained.\n- If reference email is cited, it\u2019s paraphrased into clear assumptions.\nScoring: 1.3 excellent, 0.8 good, 0.4 fair, 0.0 unclear.", "expectation": "Concise assumptions that a VP can quickly grasp."}, {"type": "llm_judge", "name": "Strategic Usefulness", "description": "Does the model support decision-making (e.g., easy to adjust quantities/prices, shows contributions by product and consumables)?", "weight": 1.3, "judge_prompt": "Judge strategic usefulness:\n- Visibility of per-product margin contributions and consumables add-on value.\n- Simple levers (e.g., quantities, tier selection) are easy to adjust.\n- Clear Year 1 roll-up and, if present, an optional sensitivity or scenario hint earns credit.\nScoring: 1.3 excellent, 0.8 good, 0.4 fair, 0.0 poor.", "expectation": "A practical, decision-ready view of margin contribution by product and consumables."}, {"type": "llm_judge", "name": "Self-Documentation and Traceability", "description": "Are assumptions and sources documented so another person can audit or update later?", "weight": 1.0, "judge_prompt": "Evaluate self-documentation:\n- Notes/Sources sheet or section exists.\n- Explains where prices/costs/tier rules came from (pricing email).\n- Brief method note: how margin % and totals are calculated.\nScoring: 1.0 strong, 0.6 adequate, 0.3 minimal, 0.0 none.", "expectation": "A short notes/sources area with provenance and method makes the workbook maintainable."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "5349dd7b-bf0a-4544-9a17-75b7013767e6", "rubric": {"category_name": "Outbound Flat-Rate Carrier Selection (Manufacturing \u2013 Shipping/Receiving/Inventory)", "rationale": "Analytical task (Pattern A) with an Excel deliverable. Stage 1 mandates a strict spreadsheet structure to make verification trivial. Stage 2 mixes precise code checks (bounds, arithmetic consistency, cross-sheet coherence) with LLM verification for nuanced elements (source quality, rate/business options correctness, exclusions, recommendation consistency). Stage 3 evaluates overall professional quality and decision usefulness for the shipping team.", "max_total_score": 17.0, "stages": [{"name": "Stage 1 \u2013 Structured Output Gate (Excel Shape Enforcement)", "description": "LLM-only check that the output is a single Excel file with the exact sheets and section layouts required to enable verification.", "is_required": true, "max_points": 2.0, "min_score_to_pass": 1.0, "rules": [{"type": "llm_judge", "name": "Excel Structure and Required Sections", "description": "Verify the deliverable is an Excel file with the specified sheets and in-sheet tables/sections. Only check structure and presence, not correctness.", "weight": 2.0, "judge_prompt": "You are evaluating the STRUCTURE ONLY of the candidate\u2019s output. The expected deliverable is a single Excel workbook with the following sheets and sections. Be flexible with minor naming variations (e.g., \"FedEx\" vs \"FEDEX\"), but the structure must be clearly present. Do not judge correctness of numbers\u2014only the presence and organization.\n\nRequired Workbook Format (Excel only):\n- Sheet 1: \"Rate Increase Analysis\"\n  - Table A: Historical Annual Rate Increases (2020\u20132025)\n    - Columns: [Carrier | 2020 | 2021 | 2022 | 2023 | 2024 | 2025]\n    - Rows must include USPS, UPS, and FedEx\n  - Table B: Average and Estimate\n    - Columns: [Carrier | Average 2020\u20132025 | 2026 Estimated Increase]\n  - Short methodology note (2\u20133 sentences) describing how the average was computed and applied to 2026.\n\n- Sheet 2: \"Current Flat Rates\"\n  - Table: [Carrier | Package Size | Flat Rate (USD) | Service/Speed Used | Business/Account Rate? (Yes/No) | Source URL | Retrieved Date]\n  - Package sizes expected: Pak (or Pack/Pak), Small Box, Medium Box, Large Box, Extra Large Box. If a carrier does not offer a size, it should be marked N/A or excluded for that size.\n\n- Sheet 3: \"Volumes 2026\"\n  - Table: [Package Size | Projected Units]\n  - Must include exactly these projections:\n    - Pak: 1000\n    - Small Box: 2300\n    - Medium Box: 2100\n    - Large Box: 540\n    - Extra Large Box: 120\n\n- Sheet 4: \"2026 Cost Calculations\"\n  - Table: [Carrier | Package Size | 2025 Unit Price | Est. 2026 Unit Price | Projected Units | Total Cost 2026]\n  - Est. 2026 Unit Price should be based on each carrier\u2019s Average 2020\u20132025 increase.\n\n- Sheet 5: \"Recommendations\"\n  - Table: [Package Size | Recommended Carrier | Total Cost 2026 | Runner-up Carrier | Delta vs Runner-up]\n  - Explicit note on exclusions for sizes a carrier does not offer.\n\n- Sheet 6: \"Assumptions & Sources\"\n  - Assumptions text block (bulleted or paragraph) listing: standard delivery only, business rates when available, exclusions handling, and any other modeling assumptions.\n  - Sources table: [Source Title | URL | Carrier | Content Type (Rates/Policy/News) | Publication/Effective Date | Accessed Date]\n\nScoring:\n- 2.0: Excel format with all 6 sheets present and each required table/section clearly visible with appropriate columns and labels. Minor naming differences allowed.\n- 1.5: Excel format with all sheets present but one sheet is missing a minor subsection (e.g., the methodology note or the runner-up/delta column) OR one required sheet is slightly merged with another but all required tables are still present.\n- 1.0: Excel format but missing exactly one required sheet or one core table.\n- 0.0: Not an Excel file OR missing multiple required sheets/tables.\n\nOnly assess presence/format, not correctness of values.", "expectation": "A single well-structured Excel workbook enabling straightforward verification."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness and Consistency Verification", "description": "Mixed code and LLM checks. Code rules do deterministic validation of volumes, bounds, arithmetic, and cross-sheet consistency. LLM judges nuanced elements like source quality, exclusions, business/standard rate usage, and recommendation alignment with totals.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Projected Volumes Match Specification", "description": "Verify that the Volumes 2026 table includes the required package sizes and exact unit counts.", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output\"\n        path = context.files.get_path(output.id)\n        xl = pd.ExcelFile(path)\n        # Find volumes sheet by name first\n        vol_sheet = None\n        for sn in xl.sheet_names:\n            if 'volume' in sn.lower():\n                vol_sheet = sn\n                break\n        # Fallback: choose a sheet that contains a column likely named like volume\n        if vol_sheet is None:\n            for sn in xl.sheet_names:\n                df_try = pd.read_excel(path, sheet_name=sn)\n                cols = [str(c).lower() for c in df_try.columns]\n                if any('volume' in c or 'projected' in c for c in cols):\n                    vol_sheet = sn\n                    break\n        if vol_sheet is None:\n            return 0.0, \"No Volumes sheet found\"\n        df = pd.read_excel(path, sheet_name=vol_sheet)\n        # Normalize\n        df_columns_lower = [str(c).strip().lower() for c in df.columns]\n        # Attempt to identify size and volume columns\n        size_col_idx = None\n        vol_col_idx = None\n        for i, c in enumerate(df_columns_lower):\n            if ('package' in c and 'size' in c) or c.strip() in ['package size','size']:\n                size_col_idx = i\n            if ('volume' in c) or ('unit' in c) or ('projected' in c):\n                vol_col_idx = i\n        if size_col_idx is None or vol_col_idx is None:\n            # Try to infer from first two columns if unnamed\n            if df.shape[1] >= 2:\n                size_col_idx = 0\n                vol_col_idx = 1\n            else:\n                return 0.0, \"Could not identify size/volume columns\"\n        sizes = df.iloc[:, size_col_idx].astype(str).str.strip().str.lower()\n        vols = pd.to_numeric(df.iloc[:, vol_col_idx], errors='coerce')\n        mapping = dict(zip(sizes, vols))\n        # Synonyms\n        synonyms = {\n            'pak': ['pak','pack','pack/pak','pak/pak','envelope pak','pak envelope','pak (envelope)'],\n            'small box': ['small box','small flat rate box','small'],\n            'medium box': ['medium box','med box','medium'],\n            'large box': ['large box','lg box','large'],\n            'extra large box': ['extra large box','extra-large box','x-large box','xl box','extra large']\n        }\n        expected = {\n            'pak': 1000,\n            'small box': 2300,\n            'medium box': 2100,\n            'large box': 540,\n            'extra large box': 120\n        }\n        def lookup_vol(key, alts):\n            for k,v in mapping.items():\n                for a in alts:\n                    if a == k or (a in k and 'box' in a and 'box' in k):\n                        if pd.notnull(v):\n                            return float(v)\n            return None\n        score = 0\n        total = len(expected)\n        missing = []\n        for label, exp in expected.items():\n            got = lookup_vol(label, synonyms[label])\n            if got is not None and abs(got - exp) < 1e-6:\n                score += 1\n            else:\n                missing.append(label)\n        return (score/total) * 0.5, f\"Matched {score}/{total}; missing or mismatched: {', '.join(missing)}\"\n    except Exception as e:\n        return 0.0, f\"Exception: {e}\""}, {"type": "code", "name": "Historical Increase Bounds & Average Presence", "description": "Check that 2020\u20132025 increases are present for USPS/UPS/FedEx and within plausible bounds; verify presence of a 2026 estimate approximately equal to the average.", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output\"\n        path = context.files.get_path(output.id)\n        xl = pd.ExcelFile(path)\n        # Find rate increase sheet\n        target_sheet = None\n        for sn in xl.sheet_names:\n            low = sn.lower()\n            if 'increase' in low or ('rate' in low and 'analysis' in low):\n                target_sheet = sn\n                break\n        if target_sheet is None:\n            return 0.0, \"No Rate Increase Analysis sheet found\"\n        df = pd.read_excel(path, sheet_name=target_sheet)\n        df_cols = [str(c).lower() for c in df.columns]\n        # Identify year columns\n        year_cols = []\n        for i,c in enumerate(df.columns):\n            s = str(c)\n            if re.fullmatch(r\"20(2[0-5]|1[0-9]|0[0-9])\", s):\n                if int(s) >= 2020 and int(s) <= 2025:\n                    year_cols.append(c)\n        if len(year_cols) < 3:  # need multiple years\n            # Try alternative: columns containing year in text\n            for c in df.columns:\n                if any(y in str(c) for y in ['2020','2021','2022','2023','2024','2025']):\n                    year_cols.append(c)\n            year_cols = list(dict.fromkeys(year_cols))\n        carriers = ['usps','ups','fedex']\n        # Find carrier column (first col assumed if not named)\n        carrier_col_idx = 0\n        for i,c in enumerate(df_cols):\n            if 'carrier' in c:\n                carrier_col_idx = i\n                break\n        score_parts = 0\n        checks = 0\n        avg_close_checks = 0\n        avg_close_hits = 0\n        for ridx, row in df.iterrows():\n            name = str(row.iloc[carrier_col_idx]).strip().lower()\n            if any(c in name for c in carriers):\n                vals = []\n                for yc in year_cols:\n                    try:\n                        v = float(row[yc])\n                        # Accept 0-100 where >1; 0-1 where <=1\n                        v_pct = v if v <= 1.0 else v/100.0\n                        if 0 <= v_pct <= 0.15:\n                            vals.append(v_pct)\n                    except Exception:\n                        pass\n                if len(vals) >= 3:\n                    checks += 1\n                    score_parts += 1\n                    avg = float(np.mean(vals))\n                    # Find a 2026 estimate column\n                    est_col = None\n                    for c in df.columns:\n                        cl = str(c).lower()\n                        if '2026' in cl or ('estimate' in cl and ('2026' in cl or 'next' in cl)):\n                            est_col = c\n                            break\n                    if est_col is not None:\n                        try:\n                            est = float(row[est_col])\n                            est_pct = est if est <= 1.0 else est/100.0\n                            # within +/- 1 percentage point\n                            if abs(est_pct - avg) <= 0.01:\n                                avg_close_hits += 1\n                            avg_close_checks += 1\n                        except Exception:\n                            pass\n        base_score = 0.3 * (score_parts / max(checks, 1)) if checks>0 else 0.0\n        avg_score = 0.2 * (avg_close_hits / max(avg_close_checks, 1)) if avg_close_checks>0 else 0.0\n        return base_score + avg_score, f\"Rows checked: {checks}, avg-within-1pp: {avg_close_hits}/{max(avg_close_checks,1)}\"\n    except Exception as e:\n        return 0.0, f\"Exception: {e}\""}, {"type": "code", "name": "2026 Unit Prices Reflect Carrier Averages", "description": "Check that 2026 unit prices implied by totals align with 2025 base prices grown by each carrier\u2019s average increase, within a small tolerance.", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef _sheet_by_keywords(xl, path, keywords):\n    for sn in xl.sheet_names:\n        low = sn.lower()\n        if all(k in low for k in keywords):\n            return sn\n    return None\n\ndef _load_table(path, sheet):\n    df = pd.read_excel(path, sheet_name=sheet)\n    # drop fully empty columns/rows\n    df = df.dropna(axis=0, how='all').dropna(axis=1, how='all')\n    return df\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output\"\n        path = context.files.get_path(output.id)\n        xl = pd.ExcelFile(path)\n        # Sheets\n        inc_sheet = _sheet_by_keywords(xl, path, ['increase']) or _sheet_by_keywords(xl, path, ['rate','analysis'])\n        price_sheet = _sheet_by_keywords(xl, path, ['flat','rate']) or _sheet_by_keywords(xl, path, ['current','rate'])\n        cost_sheet = _sheet_by_keywords(xl, path, ['2026']) or _sheet_by_keywords(xl, path, ['cost'])\n        vol_sheet = _sheet_by_keywords(xl, path, ['volume'])\n        if not all([inc_sheet, price_sheet, cost_sheet, vol_sheet]):\n            return 0.0, \"Missing one or more key sheets for this check\"\n        df_inc = _load_table(path, inc_sheet)\n        df_price = _load_table(path, price_sheet)\n        df_cost = _load_table(path, cost_sheet)\n        df_vol = _load_table(path, vol_sheet)\n        # Build carrier avg increase map\n        inc_cols = [c for c in df_inc.columns if any(str(c).startswith(str(y)) for y in range(2020,2026))]\n        if len(inc_cols) < 3:\n            inc_cols = [c for c in df_inc.columns if any(str(y) in str(c) for y in range(2020,2026))]\n        # identify carrier col\n        carrier_col_inc = None\n        for c in df_inc.columns:\n            if 'carrier' in str(c).lower():\n                carrier_col_inc = c\n                break\n        if carrier_col_inc is None:\n            carrier_col_inc = df_inc.columns[0]\n        avg_map = {}\n        for _,r in df_inc.iterrows():\n            name = str(r[carrier_col_inc]).strip().lower()\n            if any(k in name for k in ['usps','ups','fedex']):\n                vals = []\n                for c in inc_cols:\n                    try:\n                        v = float(r[c])\n                        v = v if v <= 1.0 else v/100.0\n                        if 0 <= v <= 0.5:\n                            vals.append(v)\n                    except:\n                        pass\n                if vals:\n                    avg_map['usps' if 'usps' in name else ('ups' if 'ups' in name else 'fedex')] = float(np.mean(vals))\n        if len(avg_map) < 2:\n            return 0.0, \"Could not compute average increases for carriers\"\n        # Build base price map from Current Flat Rates: key (carrier,size)\n        # Identify columns\n        def col(df, opts):\n            for c in df.columns:\n                cl = str(c).lower()\n                if any(o in cl for o in opts):\n                    return c\n            return None\n        c_car = col(df_price, ['carrier']) or df_price.columns[0]\n        c_size = col(df_price, ['size']) or df_price.columns[1]\n        c_price = col(df_price, ['flat rate','price','usd']) or df_price.columns[-1]\n        base = {}\n        for _,r in df_price.iterrows():\n            try:\n                car = str(r[c_car]).strip().lower()\n                if not any(k in car for k in ['usps','ups','fedex']):\n                    continue\n                size = str(r[c_size]).strip().lower()\n                p = float(str(r[c_price]).replace('$','').replace(',',''))\n                key = (('usps' if 'usps' in car else ('ups' if 'ups' in car else 'fedex')), size)\n                base[key] = p\n            except:\n                continue\n        if not base:\n            return 0.0, \"No base prices parsed\"\n        # Volumes map by normalized size keywords\n        v_size_col = col(df_vol, ['size']) or df_vol.columns[0]\n        v_vol_col = col(df_vol, ['volume','unit','projected']) or df_vol.columns[1]\n        vol_map = {}\n        for _,r in df_vol.iterrows():\n            try:\n                s = str(r[v_size_col]).strip().lower()\n                v = float(r[v_vol_col])\n                vol_map[s] = v\n            except:\n                pass\n        # From 2026 costs, try to get unit price 2026 or infer from total/volume\n        car_col = col(df_cost, ['carrier']) or df_cost.columns[0]\n        size_col = col(df_cost, ['size']) or df_cost.columns[1]\n        unit26_col = col(df_cost, ['2026 unit','est. 2026','unit price'])\n        total_col = col(df_cost, ['total'])\n        units_col = col(df_cost, ['projected','units','volume'])\n        matches = 0\n        attempts = 0\n        for _,r in df_cost.iterrows():\n            try:\n                car = str(r[car_col]).strip().lower()\n                if not any(k in car for k in ['usps','ups','fedex']):\n                    continue\n                carrier_key = 'usps' if 'usps' in car else ('ups' if 'ups' in car else 'fedex')\n                size = str(r[size_col]).strip().lower()\n                # expected 2026 unit = base*(1+avg)\n                # find matching base by fuzzy size match\n                base_key = None\n                for (c,s),p in base.items():\n                    if c==carrier_key and (s==size or (s in size) or (size in s)):\n                        base_key = (c,s)\n                        break\n                if base_key is None:\n                    continue\n                base_p = base[base_key]\n                avg = avg_map.get(carrier_key)\n                if avg is None:\n                    continue\n                expected_unit_26 = base_p * (1.0 + avg)\n                # get actual unit 26\n                if unit26_col is not None and unit26_col in df_cost.columns:\n                    actual_unit = float(str(r[unit26_col]).replace('$','').replace(',',''))\n                else:\n                    if total_col is None:\n                        continue\n                    total = float(str(r[total_col]).replace('$','').replace(',',''))\n                    # derive volume\n                    if units_col is not None:\n                        vol = float(r[units_col])\n                    else:\n                        # fallback to Volumes sheet\n                        vol = None\n                        for vs, vv in vol_map.items():\n                            if vs==size or (vs in size) or (size in vs):\n                                vol = vv\n                                break\n                        if vol is None or vol==0:\n                            continue\n                    actual_unit = total/vol\n                attempts += 1\n                # tolerance: absolute $0.03 or 2% whichever larger\n                tol = max(0.03, 0.02*expected_unit_26)\n                if abs(actual_unit - expected_unit_26) <= tol:\n                    matches += 1\n            except:\n                continue\n        if attempts == 0:\n            return 0.0, \"No comparable rows for 2026 unit price check\"\n        return 0.5 * (matches/attempts), f\"Matched {matches}/{attempts} rows within tolerance\"\n    except Exception as e:\n        return 0.0, f\"Exception: {e}\""}, {"type": "code", "name": "Totals Equal Unit Price \u00d7 Volume", "description": "Verify row-level arithmetic: Total Cost 2026 approximately equals Est. 2026 Unit Price \u00d7 Projected Units.", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output\"\n        path = context.files.get_path(output.id)\n        xl = pd.ExcelFile(path)\n        target_sheet = None\n        for sn in xl.sheet_names:\n            low = sn.lower()\n            if '2026' in low or 'cost' in low:\n                target_sheet = sn\n                break\n        if target_sheet is None:\n            return 0.0, \"No 2026 cost calculations sheet found\"\n        df = pd.read_excel(path, sheet_name=target_sheet)\n        # Identify columns\n        def col(opts):\n            for c in df.columns:\n                cl = str(c).lower()\n                if any(o in cl for o in opts):\n                    return c\n            return None\n        unit_col = col(['2026 unit','est. 2026','unit price'])\n        total_col = col(['total'])\n        units_col = col(['projected','units','volume'])\n        if total_col is None or units_col is None:\n            return 0.0, \"Missing total or units column\"\n        # If unit_col missing, cannot directly test, return low\n        if unit_col is None:\n            # try to see if total/units is stable across rows (>0 variability minimal) \u2013 weak check\n            return 0.1, \"No explicit unit price column; weak verification only\"\n        ok = 0\n        n = 0\n        for _,r in df.iterrows():\n            try:\n                unit = float(str(r[unit_col]).replace('$','').replace(',',''))\n                total = float(str(r[total_col]).replace('$','').replace(',',''))\n                units = float(r[units_col])\n                # tolerate rounding\n                tol = max(0.03, 0.005*max(total,1.0))\n                if abs(total - (unit*units)) <= tol:\n                    ok += 1\n                n += 1\n            except:\n                pass\n        if n == 0:\n            return 0.0, \"No rows verified\"\n        return 0.5 * (ok/n), f\"Totals correct for {ok}/{n} rows\"\n    except Exception as e:\n        return 0.0, f\"Exception: {e}\""}, {"type": "llm_judge", "name": "Recommendations Match Lowest 2026 Total Cost", "description": "Check that for each package size, the recommended carrier corresponds to the lowest total 2026 cost (ties allowed if documented).", "weight": 2.5, "judge_prompt": "Review the workbook tables. For each package size, compare the \"Recommendations\" sheet against the computed totals in \"2026 Cost Calculations\". Confirm the recommended carrier has the lowest Total Cost 2026 for that size. If there is a tie, confirm the sheet documents the tie and either selects one with a rationale or marks as tie. Score based on consistency across sizes.\n\nScoring:\n- 2.5: All sizes have recommendations matching the lowest computed total (ties handled and documented correctly).\n- 1.5: One size has a mismatch or tie not documented; others correct.\n- 0.5: Multiple mismatches but some alignment remains.\n- 0.0: Recommendations largely do not match the computed totals.", "expectation": "A perfect one-to-one match between recommended carrier and the lowest cost per size, with proper tie handling."}, {"type": "llm_judge", "name": "Source Quality and Coverage", "description": "Evaluate whether sources are credible, relevant, and sufficiently cover both historical increases and current flat-rate prices.", "weight": 2.0, "judge_prompt": "Inspect the \"Assumptions & Sources\" sheet and any source columns in other sheets. Judge whether the sources are:\n- Credible (e.g., official USPS/UPS/FedEx pages, official announcements, reputable trade publications)\n- Relevant (specifically about historical general rate increases 2020\u20132025 and current flat-rate pricing for business/standard services)\n- Complete (each carrier covered for both historical increases and current flat-rate prices; URLs and dates present)\n\nScoring:\n- 2.0: Credible, relevant, and complete sources with URLs and dates for all carriers and both data types.\n- 1.0: Minor gaps (e.g., one missing date or secondary source used for one carrier) but overall solid.\n- 0.0: Major credibility or coverage issues; missing URLs/dates or off-topic sources.", "expectation": "Official or reputable sources with URLs and dates for all carriers and both historical increases and current prices."}, {"type": "llm_judge", "name": "Business Rates and Standard Delivery Verified", "description": "Confirm that the selected prices reflect standard delivery speeds and business/commercial rates when available (no premium options).", "weight": 1.7, "judge_prompt": "Check the \"Current Flat Rates\" sheet for columns indicating Service/Speed Used and Business/Account Rate. Confirm that:\n- Only standard delivery speeds are used (no overnight/expedited options)\n- When business/commercial rates exist, those rates are chosen over retail\n\nScoring:\n- 1.7: Clearly uses standard service and business/commercial rates for all carriers/sizes where applicable.\n- 0.9: Mostly correct with minor exceptions or minor ambiguity.\n- 0.0: Uses expedited/retail rates or is ambiguous for multiple entries.", "expectation": "All rates use standard delivery and business/commercial pricing when available."}, {"type": "llm_judge", "name": "Exclusions for Non-offered Sizes", "description": "Verify that if a carrier does not offer a specific flat-rate size, they are excluded or marked N/A in analyses and not recommended for that size.", "weight": 1.8, "judge_prompt": "Scan the pricing, calculations, and recommendations. If a carrier does not offer a given flat-rate size, ensure:\n- That size is marked N/A or excluded for that carrier in pricing and calculations\n- The recommendations do not select that carrier for that size\n\nScoring:\n- 1.8: All exclusions correctly handled across sheets.\n- 1.0: One minor inconsistency (e.g., a stray N/A omission) but recommendation still correct.\n- 0.0: Carrier is recommended or priced for a size they do not offer.", "expectation": "Clean, consistent exclusion of non-offered sizes in all tables and the final recommendation."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Presentation and Decision Usefulness", "description": "LLM-only qualitative assessment of professional quality, clarity, and actionable value for the shipping team.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Decision Usefulness for 2026 Operations", "description": "Assess how actionable the recommendations are, including clarity per size and simple prioritization insights (e.g., savings vs runner-up).", "weight": 2.0, "judge_prompt": "Evaluate whether the Excel deliverable provides clear, actionable guidance to the shipping team for 2026:\n- Each package size has a clear recommended carrier\n- Runner-up and delta vs runner-up clarify the magnitude of savings\n- Any practical notes (e.g., tie handling, exclusions) are visible\n\nScoring:\n- 2.0: Highly actionable, easy to implement, clear per-size direction with savings context.\n- 1.0: Generally actionable but missing some helpful context.\n- 0.0: Not readily actionable or too ambiguous for operational use.", "expectation": "Concise, per-size guidance with savings context to drive carrier selection."}, {"type": "llm_judge", "name": "Clarity and Formatting", "description": "Evaluate readability, labeling, consistent currency/percent formatting, and clean layout across sheets.", "weight": 1.5, "judge_prompt": "Assess the workbook\u2019s clarity and formatting:\n- Clear headers, consistent naming of sizes and carriers\n- Currency and percentages properly formatted\n- Tables well-separated, no clutter, minimal merged cells that hurt readability\n\nScoring:\n- 1.5: Professional, clean, and easy to read; consistent formats\n- 0.8: Mostly clear with minor inconsistencies\n- 0.0: Poorly formatted or confusing layout that impedes understanding", "expectation": "Professional, consistent formatting with clear labels and number formats."}, {"type": "llm_judge", "name": "Transparency and Reproducibility", "description": "Look for clear assumptions, visible formulas/methodology notes, and enough detail to reproduce the math.", "weight": 1.0, "judge_prompt": "Evaluate transparency:\n- Assumptions are explicitly listed (standard speed, business rates, exclusions, methodology)\n- Calculation logic is documented (how 2026 unit prices derived from averages)\n- If possible, formulas or step-by-step notes are visible\n\nScoring:\n- 1.0: Clear assumptions and methodology enabling easy reproduction\n- 0.5: Some assumptions present but methodology light\n- 0.0: Opaque; difficult to understand how numbers were derived", "expectation": "Sufficient assumptions and method detail to reproduce results."}, {"type": "llm_judge", "name": "Risks and Limitations", "description": "Check whether reasonable caveats are noted (e.g., future rate changes, dimensional restrictions, surcharges).", "weight": 0.5, "judge_prompt": "Look for a short section noting limitations or risks:\n- Potential changes to 2026 rates vs estimates\n- Size definitions and dimensional restrictions\n- Possible surcharges (rural, residential) not included\n\nScoring:\n- 0.5: Clear, concise limitations and risks listed\n- 0.2: Minimal or generic caveats\n- 0.0: No limitations noted", "expectation": "Concise, relevant limitations for realistic operational planning."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
