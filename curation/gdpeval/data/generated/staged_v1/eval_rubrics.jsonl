{"task_id": "efca245f-c24f-4f75-a9d5-59201330ab7a", "rubric": {"category_name": "Manufacturing \u2014 Running Board Recovery Plan (Production Manager)", "rationale": "This rubric enforces a self-documenting, verifiable submission for a mixed analytical + document task. Stage 1 (LLM-only) mandates exact spreadsheet and summary-document structures to make verification trivial. Stage 2 uses code rules to deterministically check capacity constraints, schedule windows, prioritization logic, and deadline attainment using the Excel artifacts that Stage 1 required. Stage 3 assesses professional quality and managerial usefulness of the plan and summary for an operations meeting audience.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Structured Output Gate", "description": "MANDATORY structure and format check. Ensures an Excel workbook with three scenario plans plus an assumptions/open-POs foundation exists, and a separate written summary document with required sections is present. Only checks presence/structure, not correctness.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Spreadsheet + Summary Structure Present", "description": "Check for correct file formats and required structural elements across the submission: one Excel workbook with required sheets and three fully delineated scenario daily plans, plus a PDF/DOCX written summary with required scenario sections.", "weight": 4.0, "judge_prompt": "You are the Stage 1 gate. Examine the candidate submission outputs. Pass this gate ONLY if the deliverables exist in the correct formats and contain the mandated structure. Do not judge calculation correctness or quality\u2014only presence and shape.\n\nREQUIRED FILES:\nA) One Excel workbook (.xlsx) named appropriately (any name acceptable) containing:\n  1) An 'Assumptions' sheet (or similar: 'Inputs', 'Assumptions & Commitments') with clearly labeled tables that include at minimum:\n     - Materials Available Date (must reflect Jan 22, 2018)\n     - Baseline capacity 120 sets/day (running boards) before upgrade\n     - Upgrade date (Feb 5, 2018) and upgraded capacity 135 sets/day\n     - Grill Guard weekly requirement 100 units/week if produced in the cell\n     - Grill Guard move date (for scenarios where Grill Guard is moved)\n     - 10-hour shift start and end dates (Scenario 3), and a note about 30-day notification\n     - Work pattern: Mon\u2013Fri only; note that stat holidays are observed\n     - Shipping commitments: April PO latest transit/ship date = Apr 13, 2018; May PO latest ship date = May 1, 2018\n  2) An 'Open POs' sheet listing POs with a tabular structure with columns similar to:\n     [PO ID (or Ref), SKU (Crew Cab Running Board / Extended Cab Running Board / Grill Guard), PO Month or Requested Ship Date, Quantity]. Rows should at least cover Nov 2017 through May 2018 where applicable.\n  3) Three scenario daily plan sheets (sheet names can vary but must be clearly distinguishable):\n     - Scenario 1: Current Capacity and Cells (includes Grill Guard in the cell)\n     - Scenario 2: Current Capacity without Truck Grill Guard (Grill Guard moved off the cell)\n     - Scenario 3: Expanded Capacity with 10-Hour Shift and no Grill Guard in the cell\n     Each of the three scenario sheets must include:\n       a) A daily plan table spanning at least Jan 22, 2018 through May 1, 2018 (inclusive), weekdays only.\n       b) Columns that clearly correspond to: Date, Product/SKU (Crew/Extended/Grill Guard), Planned Quantity for the day, and at least one cumulative/backlog tracking column (e.g., 'Cumulative vs Open POs' or 'Remaining Backlog by SKU/Month'). Column names may vary but must be clearly labeled.\n       c) A short scenario summary box on the sheet (top or bottom) indicating: total running boards by SKU through May 1, whether April commitment is met by Apr 13, whether May commitment is met by May 1, and Grill Guard plan status relevant to the scenario.\n\nB) One written summary document (.pdf or .docx), professionally formatted, minimum ~1 page, containing THREE sections titled or clearly labeled as:\n   - Scenario 1 (Current Capacity and Cells)\n   - Scenario 2 (Current Capacity without Truck Grill Guard)\n   - Scenario 3 (10-Hour Shift, no Grill Guard in cell)\n   For each scenario section, include:\n     - Actions taken/assumptions\n     - Implications for Crew Cab Running Boards, Extended Cab Running Boards, and Truck Grill Guard\n     - An explicit statement whether May\u2019s PO is shipped on time by May 1 (Yes/No) and whether April\u2019s PO meets the Apr 13 in-transit requirement (Yes/No)\n\nSCORING (structure only):\n- 4.0: Excel workbook present with all required sheets (Assumptions, Open POs, and three scenario sheets each with daily plan table and scenario summary box) AND a PDF/DOCX summary with all three scenario sections and required statements.\n- 3.0: Excel workbook present with all three scenario sheets and Open POs, minor omissions in Assumptions OR summary document present but missing a few required scenario details.\n- 2.0: Excel workbook present but missing one required sheet (Assumptions or Open POs) OR only 2 scenario sheets are complete; summary doc present.\n- 1.0: Only partial Excel content (\u22641 scenario sheet) or summary doc missing; some structure visible but inadequate.\n- 0.0: Not an Excel workbook or no scenario sheets; no summary doc.\n\nBe flexible with sheet and section naming but strict about the presence of the required elements and tables. Do not evaluate numerical correctness.\n", "expectation": "An .xlsx with Assumptions, Open POs, and three fully delineated daily plan sheets following the specified columns, plus a PDF/DOCX scenario summary with required sections and explicit Yes/No statements on deadlines."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification and Compliance Checks", "description": "Deterministic checks leveraging the structured Excel to verify schedule windows, capacity limits, prioritization, and deadline attainment. Also validates weekday-only production and basic sheet/column presence programmatically.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 2.0, "rules": [{"type": "code", "name": "Excel Found and Key Sheets Detected", "description": "Locate the Excel workbook among outputs and check for expected sheets: Assumptions/Inputs, Open POs, and at least three scenario sheets.", "weight": 0.8, "code": "import re\nimport pandas as pd\nfrom datetime import datetime\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    excel_res = None\n    for r in outputs:\n        if getattr(r, 'is_spreadsheet', False):\n            excel_res = r\n            break\n    if not excel_res:\n        return 0.0, 'No Excel workbook found.'\n\n    try:\n        x_path = context.files.get_path(excel_res.id)\n        xls = pd.ExcelFile(x_path)\n        sheets = [s.strip() for s in xls.sheet_names]\n        sheets_lower = [s.lower() for s in sheets]\n    except Exception as e:\n        return 0.0, f'Failed to open Excel: {e}'\n\n    def has_sheet_like(names):\n        for name in names:\n            for s in sheets_lower:\n                if name in s:\n                    return True\n        return False\n\n    has_assumptions = has_sheet_like(['assumption', 'input'])\n    has_open_pos = has_sheet_like(['open po', 'open_po', 'pos'])\n    # Scenario sheets: look for at least three sheets that look like scenarios\n    scenario_candidates = [s for s in sheets_lower if 'scenario' in s or 'current' in s or '10' in s or 'grill' in s]\n    scenario_count = 0\n    # Heuristic: read candidates and see if they have a 'date' column-like and a 'plan'/'qty' column-like\n    def has_plan_columns(df):\n        cols = [str(c).lower() for c in df.columns]\n        has_date = any('date' in c for c in cols)\n        has_qty = any(('qty' in c or 'quantity' in c or 'units' in c or ('plan' in c and ('qty' in c or 'output' in c or 'units' in c))) for c in cols)\n        has_sku = any(('product' in c or 'sku' in c or 'item' in c or 'type' in c))\n        return has_date and has_qty and has_sku\n\n    for s in sheets:\n        sl = s.lower()\n        if ('scenario' in sl) or ('current' in sl) or ('10' in sl) or ('grill' in sl and 'scenario' in sl):\n            try:\n                df = context.files.read_excel(excel_res.id, sheet_name=s)\n                if df is not None and df.shape[1] >= 3 and has_plan_columns(df):\n                    scenario_count += 1\n            except Exception:\n                pass\n\n    score = 0.0\n    feedback = []\n    if has_assumptions: score += 0.25\n    else: feedback.append('Missing Assumptions/Inputs sheet')\n    if has_open_pos: score += 0.25\n    else: feedback.append('Missing Open POs sheet')\n    if scenario_count >= 3:\n        score += 0.3\n    elif scenario_count == 2:\n        score += 0.2; feedback.append('Only two scenario sheets with valid tables detected')\n    elif scenario_count == 1:\n        score += 0.1; feedback.append('Only one scenario sheet with valid table detected')\n    else:\n        feedback.append('No valid scenario sheets detected')\n\n    return min(score, 0.8), '; '.join(feedback) if feedback else 'OK'"}, {"type": "code", "name": "Scenario Columns and Date Range + Weekday-only", "description": "Verify each scenario sheet has Date and Planned Qty columns, spans at least Jan 22, 2018 through May 1, 2018, and schedules production only on weekdays (Mon\u2013Fri).", "weight": 0.8, "code": "import re\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    excel_res = None\n    for r in outputs:\n        if getattr(r, 'is_spreadsheet', False):\n            excel_res = r\n            break\n    if not excel_res:\n        return 0.0, 'No Excel workbook found.'\n\n    x_path = context.files.get_path(excel_res.id)\n    xls = pd.ExcelFile(x_path)\n    sheets = [s.strip() for s in xls.sheet_names]\n\n    target_start = pd.Timestamp('2018-01-22')\n    target_end = pd.Timestamp('2018-05-01')\n\n    def find_cols(df):\n        cols = [str(c) for c in df.columns]\n        low = [c.lower() for c in cols]\n        date_col = None\n        qty_col = None\n        for i, c in enumerate(low):\n            if 'date' in c: date_col = cols[i]\n        for i, c in enumerate(low):\n            if ('qty' in c) or ('quantity' in c) or (('plan' in c) and ('qty' in c or 'units' in c or 'output' in c)) or ('units' in c):\n                qty_col = cols[i]\n        return date_col, qty_col\n\n    # Find likely scenario sheets\n    scenario_sheets = []\n    for s in sheets:\n        sl = s.lower()\n        if ('scenario' in sl) or ('current' in sl) or ('10' in sl):\n            scenario_sheets.append(s)\n\n    if len(scenario_sheets) == 0:\n        return 0.0, 'No scenario sheets recognized.'\n\n    total = len(scenario_sheets)\n    ok_span = 0\n    ok_weekdays = 0\n\n    fb_details = []\n\n    for s in scenario_sheets:\n        try:\n            df = context.files.read_excel(excel_res.id, sheet_name=s)\n        except Exception:\n            continue\n        date_col, qty_col = find_cols(df)\n        if date_col is None or qty_col is None:\n            fb_details.append(f'{s}: Missing Date or Planned Qty column')\n            continue\n        # Parse dates\n        dates = pd.to_datetime(df[date_col], errors='coerce')\n        qty = pd.to_numeric(df[qty_col], errors='coerce').fillna(0)\n        # Filter valid rows\n        mask = dates.notna()\n        dates = dates[mask]\n        qty = qty[mask]\n        if dates.empty:\n            fb_details.append(f'{s}: No valid dates')\n            continue\n        if dates.min() <= target_start and dates.max() >= target_end:\n            ok_span += 1\n        else:\n            fb_details.append(f'{s}: Date span insufficient (needs {target_start.date()} to {target_end.date()})')\n        # Weekday-only check: production (>0) only Mon-Fri\n        if not dates.empty:\n            df2 = pd.DataFrame({ 'd': dates.dt.normalize(), 'q': qty })\n            by_day = df2.groupby('d')['q'].sum()\n            weekends = by_day[by_day.index.weekday >= 5]\n            if (weekends > 0).sum() == 0:\n                ok_weekdays += 1\n            else:\n                fb_details.append(f'{s}: Weekend production detected')\n\n    # Scoring: half for span, half for weekday-only\n    span_score = 0.4 * (ok_span / total)\n    weekday_score = 0.4 * (ok_weekdays / total)\n    score = span_score + weekday_score\n    return score, '; '.join(fb_details) if fb_details else 'OK'"}, {"type": "code", "name": "Capacity Compliance by Day (per Scenario)", "description": "Check daily planned totals do not exceed allowed capacity windows: 120/day baseline, 135/day from Feb 5, and for Scenario 3 a 170/day window only within the stated 10-hour shift dates. Validates Scenario 1 includes Grill Guard weekly output \u2265100/week if Grill Guard remains in the cell.", "weight": 1.0, "code": "import re\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    excel_res = None\n    for r in outputs:\n        if getattr(r, 'is_spreadsheet', False):\n            excel_res = r\n            break\n    if not excel_res:\n        return 0.0, 'No Excel workbook found.'\n\n    x_path = context.files.get_path(excel_res.id)\n    xls = pd.ExcelFile(x_path)\n    sheets = [s.strip() for s in xls.sheet_names]\n\n    # Read assumptions if present\n    assumptions_sheet = None\n    for s in sheets:\n        sl = s.lower()\n        if 'assumption' in sl or 'input' in sl:\n            assumptions_sheet = s\n            break\n    params = {\n        'materials_date': pd.Timestamp('2018-01-22'),\n        'upgrade_date': pd.Timestamp('2018-02-05'),\n        'base_cap': 120,\n        'upg_cap': 135,\n        'gg_weekly': 100,\n        'gg_move_date': None,\n        's3_10h_start': None,\n        's3_10h_end': None,\n    }\n    try:\n        if assumptions_sheet:\n            dfA = context.files.read_excel(excel_res.id, sheet_name=assumptions_sheet)\n            # Try to parse key/value style tables\n            for i,row in dfA.iterrows():\n                for c in dfA.columns:\n                    val = str(row[c]).strip().lower()\n                    if 'material' in val and 'date' in val:\n                        # Next cell or another col may hold value\n                        pass\n            # Simpler: try get dict by two-column name/value if exists\n            cols_low = [str(c).lower() for c in dfA.columns]\n            try:\n                if len(dfA.columns) >= 2:\n                    name_col = dfA.columns[0]\n                    val_col = dfA.columns[1]\n                    for _, r in dfA[[name_col, val_col]].dropna().iterrows():\n                        name = str(r[name_col]).strip().lower()\n                        val = str(r[val_col]).strip()\n                        if 'material' in name and 'date' in name:\n                            params['materials_date'] = pd.to_datetime(val, errors='coerce') or params['materials_date']\n                        if ('upgrade' in name and 'date' in name) or ('capacity' in name and '135' in name):\n                            d = pd.to_datetime(val, errors='coerce')\n                            if pd.notna(d): params['upgrade_date'] = d\n                        if ('base' in name and 'cap' in name) or ('120' in name and 'cap' in name):\n                            try: params['base_cap'] = int(float(val))\n                            except: pass\n                        if ('upgrade' in name and 'cap' in name) or ('135' in name and 'cap' in name):\n                            try: params['upg_cap'] = int(float(val))\n                            except: pass\n                        if 'grill' in name and ('week' in name or '100' in name):\n                            try: params['gg_weekly'] = int(float(val))\n                            except: pass\n                        if 'grill' in name and ('move' in name or 'transfer' in name):\n                            d = pd.to_datetime(val, errors='coerce')\n                            if pd.notna(d): params['gg_move_date'] = d\n                        if ('10' in name and 'start' in name) or ('10-hour' in name and 'start' in name):\n                            d = pd.to_datetime(val, errors='coerce')\n                            if pd.notna(d): params['s3_10h_start'] = d\n                        if ('10' in name and 'end' in name) or ('10-hour' in name and 'end' in name):\n                            d = pd.to_datetime(val, errors='coerce')\n                            if pd.notna(d): params['s3_10h_end'] = d\n            except Exception:\n                pass\n    except Exception:\n        pass\n\n    def find_cols(df):\n        cols = [str(c) for c in df.columns]\n        low = [c.lower() for c in cols]\n        date_col = None\n        qty_col = None\n        prod_col = None\n        for i, c in enumerate(low):\n            if 'date' in c: date_col = cols[i]\n            if ('qty' in c) or ('quantity' in c) or (('plan' in c) and ('qty' in c or 'units' in c or 'output' in c)) or ('units' in c):\n                if qty_col is None: qty_col = cols[i]\n            if ('product' in c) or ('sku' in c) or ('item' in c) or ('type' in c):\n                prod_col = cols[i]\n        return date_col, qty_col, prod_col\n\n    # Identify scenario sheets\n    scenario_sheets = []\n    for s in sheets:\n        sl = s.lower()\n        if ('scenario' in sl) or ('current' in sl) or ('10' in sl):\n            scenario_sheets.append(s)\n\n    if not scenario_sheets:\n        return 0.0, 'No scenario sheets found.'\n\n    def day_cap(d, is_s3=False):\n        cap = params['base_cap']\n        if pd.to_datetime(d) >= params['upgrade_date']:\n            cap = params['upg_cap']\n        if is_s3 and params['s3_10h_start'] is not None and params['s3_10h_end'] is not None:\n            if (pd.to_datetime(d) >= params['s3_10h_start']) and (pd.to_datetime(d) <= params['s3_10h_end']):\n                cap = max(cap, 170)\n        return cap\n\n    def scenario_kind(name):\n        n = name.lower()\n        s3 = ('10' in n) or ('10-hour' in n)\n        s1 = ('scenario 1' in n) or ('current' in n)\n        s2 = ('scenario 2' in n) or ('without' in n) or ('no grill' in n)\n        # Fallback classification by best guess\n        if s3: return 3\n        if s2 and not s3: return 2\n        return 1\n\n    capacity_ok_days = 0\n    capacity_total_days = 0\n    gg_weeks_ok = 0\n    gg_weeks_total = 0\n    fb = []\n\n    for s in scenario_sheets:\n        try:\n            df = context.files.read_excel(excel_res.id, sheet_name=s)\n        except Exception:\n            continue\n        date_col, qty_col, prod_col = find_cols(df)\n        if date_col is None or qty_col is None:\n            fb.append(f'{s}: Missing Date or Qty column')\n            continue\n        dates = pd.to_datetime(df[date_col], errors='coerce')\n        qty = pd.to_numeric(df[qty_col], errors='coerce').fillna(0)\n        mask = dates.notna()\n        dates = dates[mask]\n        qty = qty[mask]\n        if dates.empty:\n            fb.append(f'{s}: No valid dates')\n            continue\n        is_s3 = (scenario_kind(s) == 3)\n        # sum by day across products\n        by_day = pd.DataFrame({'d': dates.dt.normalize(), 'q': qty}).groupby('d')['q'].sum()\n        for d, qsum in by_day.items():\n            capacity_total_days += 1\n            cap = day_cap(d, is_s3=is_s3)\n            if qsum <= cap + 1e-6:\n                capacity_ok_days += 1\n            else:\n                fb.append(f'{s}: {d.date()} planned {qsum} > cap {cap}')\n        # For Scenario 1, check Grill Guard weekly 100 units/week if product appears in this sheet (implies in-cell)\n        if scenario_kind(s) == 1 and prod_col is not None:\n            prod = df.loc[mask, prod_col].astype(str).str.lower()\n            is_gg = prod.str.contains('grill')\n            if is_gg.any():\n                gg_df = pd.DataFrame({'d': dates.dt.to_period('W').apply(lambda p: p.start_time), 'q': qty.where(is_gg, 0)})\n                week_sums = gg_df.groupby('d')['q'].sum()\n                for wd, wq in week_sums.items():\n                    gg_weeks_total += 1\n                    if wq >= params['gg_weekly'] - 5:  # small tolerance\n                        gg_weeks_ok += 1\n                    else:\n                        fb.append(f'{s}: Grill Guard week starting {wd.date()} has {wq} < {params['gg_weekly']}')\n        # For Scenario 2/3, we do not require in-cell Grill Guard; skip weekly check.\n\n    if capacity_total_days == 0:\n        return 0.0, 'No daily plans to evaluate.'\n\n    cap_score = 0.8 * (capacity_ok_days / capacity_total_days)\n    gg_score = 0.2\n    if gg_weeks_total > 0:\n        gg_score = 0.2 * (gg_weeks_ok / gg_weeks_total)\n    # If no GG weeks to check in Scenario 1, leave gg_score at 0.2 (neutral) to avoid over-penalizing ambiguous labeling\n\n    score = cap_score + gg_score\n    return min(max(score, 0.0), 1.0), '; '.join(fb) if fb else 'OK'"}, {"type": "code", "name": "Crew-First Prioritization vs Extended", "description": "Verify daily plans do not schedule Extended Cab Running Boards while there remains Crew Cab backlog for Nov\u2013Feb POs, and after that, do not schedule Extended for Mar\u2013Apr while Crew Mar\u2013Apr backlog remains. Uses the Open POs sheet and the scenario daily plans.", "weight": 0.8, "code": "import re\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    excel_res = None\n    for r in outputs:\n        if getattr(r, 'is_spreadsheet', False):\n            excel_res = r\n            break\n    if not excel_res:\n        return 0.0, 'No Excel workbook found.'\n\n    x_path = context.files.get_path(excel_res.id)\n    xls = pd.ExcelFile(x_path)\n    sheets = [s.strip() for s in xls.sheet_names]\n\n    # Load Open POs\n    open_pos_sheet = None\n    for s in sheets:\n        sl = s.lower()\n        if 'open po' in sl or 'open_po' in sl or sl == 'pos' or 'purchase' in sl:\n            open_pos_sheet = s\n            break\n    if not open_pos_sheet:\n        return 0.0, 'Open POs sheet not found.'\n\n    dfP = context.files.read_excel(excel_res.id, sheet_name=open_pos_sheet)\n    if dfP is None or dfP.empty:\n        return 0.0, 'Open POs is empty.'\n\n    cols = [str(c) for c in dfP.columns]\n    low = [c.lower() for c in cols]\n    sku_col = None\n    month_col = None\n    qty_col = None\n    for i,c in enumerate(low):\n        if ('sku' in c) or ('product' in c) or ('item' in c): sku_col = cols[i]\n        if ('month' in c) or ('requested' in c and 'date' in c) or ('ship' in c and 'date' in c): month_col = cols[i]\n        if ('qty' in c) or ('quantity' in c) or ('units' in c): qty_col = cols[i]\n    if sku_col is None or month_col is None or qty_col is None:\n        return 0.0, 'Open POs missing SKU/Month/Quantity columns.'\n\n    dfP = dfP[[sku_col, month_col, qty_col]].dropna()\n    dfP[qty_col] = pd.to_numeric(dfP[qty_col], errors='coerce').fillna(0)\n    # Normalize month to period\n    m = pd.to_datetime(dfP[month_col], errors='coerce')\n    if m.isna().all():\n        # Try parsing Month strings like 'Nov 2017'\n        m = pd.to_datetime(dfP[month_col].astype(str), errors='coerce')\n    dfP['month'] = m.dt.to_period('M')\n    dfP['sku_norm'] = dfP[sku_col].astype(str).str.lower()\n\n    def is_crew(s): return 'crew' in s\n    def is_ext(s): return ('extended' in s) or ('ext' in s)\n\n    # Aggregate backlog buckets\n    crew_novfeb = dfP[(dfP['sku_norm'].apply(is_crew)) & (dfP['month'] >= pd.Period('2017-11')) & (dfP['month'] <= pd.Period('2018-02'))][qty_col].sum()\n    ext_novfeb = dfP[(dfP['sku_norm'].apply(is_ext)) & (dfP['month'] >= pd.Period('2017-11')) & (dfP['month'] <= pd.Period('2018-02'))][qty_col].sum()\n    crew_marapr = dfP[(dfP['sku_norm'].apply(is_crew)) & (dfP['month'] >= pd.Period('2018-03')) & (dfP['month'] <= pd.Period('2018-04'))][qty_col].sum()\n    ext_marapr = dfP[(dfP['sku_norm'].apply(is_ext)) & (dfP['month'] >= pd.Period('2018-03')) & (dfP['month'] <= pd.Period('2018-04'))][qty_col].sum()\n\n    # Identify scenario sheets\n    scenario_sheets = []\n    for s in sheets:\n        sl = s.lower()\n        if ('scenario' in sl) or ('current' in sl) or ('10' in sl):\n            scenario_sheets.append(s)\n\n    if not scenario_sheets:\n        return 0.0, 'No scenario sheets found.'\n\n    violations = 0\n    total_ext_units = 0\n\n    for s in scenario_sheets:\n        df = context.files.read_excel(excel_res.id, sheet_name=s)\n        cols = [str(c) for c in df.columns]\n        low = [c.lower() for c in cols]\n        date_col = None\n        qty_col2 = None\n        prod_col = None\n        for i,c in enumerate(low):\n            if 'date' in c: date_col = cols[i]\n            if ('qty' in c) or ('quantity' in c) or (('plan' in c) and ('qty' in c or 'units' in c or 'output' in c)) or ('units' in c):\n                if qty_col2 is None: qty_col2 = cols[i]\n            if ('product' in c) or ('sku' in c) or ('item' in c) or ('type' in c):\n                prod_col = cols[i]\n        if date_col is None or qty_col2 is None or prod_col is None:\n            continue\n        dates = pd.to_datetime(df[date_col], errors='coerce')\n        qty = pd.to_numeric(df[qty_col2], errors='coerce').fillna(0)\n        prod = df[prod_col].astype(str).str.lower()\n        mask = dates.notna()\n        data = pd.DataFrame({'date': dates[mask], 'qty': qty[mask], 'prod': prod[mask]})\n        data = data.sort_values('date')\n\n        c_nf = crew_novfeb\n        e_nf = ext_novfeb\n        c_ma = crew_marapr\n        e_ma = ext_marapr\n\n        for _, row in data.iterrows():\n            p = row['prod']\n            q = row['qty']\n            if 'grill' in p:\n                continue\n            if 'crew' in p:\n                if c_nf > 0:\n                    take = min(q, c_nf)\n                    c_nf -= take\n                    q -= take\n                if q > 0 and c_ma > 0:\n                    take = min(q, c_ma)\n                    c_ma -= take\n                    q -= take\n            elif ('extended' in p) or ('ext' in p):\n                total_ext_units += q\n                # Violation if any extended produced while Crew Nov-Feb backlog remains\n                if c_nf > 0 and q > 0:\n                    violations += q\n                # After Crew Nov-Feb cleared, during Mar-Apr, extended produced while Crew Mar-Apr remains is also a violation\n                if c_nf <= 0 and c_ma > 0 and q > 0:\n                    violations += q\n            else:\n                # Unknown product label; ignore\n                pass\n\n    if total_ext_units == 0:\n        # If no extended scheduled at all, pass this check fully\n        return 0.8, 'No Extended production scheduled before Crew backlog; OK'\n\n    # Score based on share of Extended that violated priority\n    violation_rate = min(1.0, violations / max(1.0, total_ext_units))\n    score = 0.8 * (1.0 - violation_rate)\n    fb = f'Priority violations counted as {violations} units of Extended before Crew backlog. '\n    return score, fb"}, {"type": "code", "name": "Deadline Compliance \u2014 April and May POs", "description": "Check that April POs are completed by Apr 13 and May POs by May 1 using cumulative allocation of running board output to monthly PO buckets (oldest-first allocation across Crew and Extended).", "weight": 0.6, "code": "import re\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    excel_res = None\n    for r in outputs:\n        if getattr(r, 'is_spreadsheet', False):\n            excel_res = r\n            break\n    if not excel_res:\n        return 0.0, 'No Excel workbook found.'\n\n    x_path = context.files.get_path(excel_res.id)\n    xls = pd.ExcelFile(x_path)\n    sheets = [s.strip() for s in xls.sheet_names]\n\n    # Load Open POs\n    open_pos_sheet = None\n    for s in sheets:\n        sl = s.lower()\n        if 'open po' in sl or 'open_po' in sl or sl == 'pos' or 'purchase' in sl:\n            open_pos_sheet = s\n            break\n    if not open_pos_sheet:\n        return 0.0, 'Open POs sheet not found.'\n\n    dfP = context.files.read_excel(excel_res.id, sheet_name=open_pos_sheet)\n    if dfP is None or dfP.empty:\n        return 0.0, 'Open POs is empty.'\n\n    cols = [str(c) for c in dfP.columns]\n    low = [c.lower() for c in cols]\n    sku_col = None\n    month_col = None\n    qty_col = None\n    for i,c in enumerate(low):\n        if ('sku' in c) or ('product' in c) or ('item' in c): sku_col = cols[i]\n        if ('month' in c) or ('requested' in c and 'date' in c) or ('ship' in c and 'date' in c): month_col = cols[i]\n        if ('qty' in c) or ('quantity' in c) or ('units' in c): qty_col = cols[i]\n    if sku_col is None or month_col is None or qty_col is None:\n        return 0.0, 'Open POs missing SKU/Month/Quantity columns.'\n\n    dfP = dfP[[sku_col, month_col, qty_col]].dropna()\n    dfP[qty_col] = pd.to_numeric(dfP[qty_col], errors='coerce').fillna(0)\n    dfP['month'] = pd.to_datetime(dfP[month_col], errors='coerce').dt.to_period('M')\n    # Totals needed by deadline\n    april_need = dfP[dfP['month'] == pd.Period('2018-04')][qty_col].sum()\n    may_need = dfP[dfP['month'] == pd.Period('2018-05')][qty_col].sum()\n\n    # Identify scenario sheets\n    scenario_sheets = []\n    for s in sheets:\n        sl = s.lower()\n        if ('scenario' in sl) or ('current' in sl) or ('10' in sl):\n            scenario_sheets.append(s)\n\n    if not scenario_sheets:\n        return 0.0, 'No scenario sheets found.'\n\n    def alloc_completion_date(df, deadline_month):\n        # Allocate RB output oldest-first across months up to the target month\n        # Build RB backlog by month from Open POs\n        months_order = pd.period_range(pd.Period('2017-11'), pd.Period(deadline_month))\n        backlog = {m: float(dfP[dfP['month']==m][qty_col].sum()) for m in months_order}\n        # Parse scenario plan\n        cols = [str(c) for c in df.columns]\n        low = [c.lower() for c in cols]\n        date_col = next((cols[i] for i,c in enumerate(low) if 'date' in c), None)\n        qty_col2 = next((cols[i] for i,c in enumerate(low) if ('qty' in c or 'quantity' in c or ('plan' in c and ('qty' in c or 'units' in c or 'output' in c)) or 'units' in c)), None)\n        prod_col = next((cols[i] for i,c in enumerate(low) if ('product' in c or 'sku' in c or 'item' in c or 'type' in c)), None)\n        if date_col is None or qty_col2 is None or prod_col is None:\n            return None\n        dates = pd.to_datetime(df[date_col], errors='coerce')\n        qty = pd.to_numeric(df[qty_col2], errors='coerce').fillna(0)\n        prod = df[prod_col].astype(str).str.lower()\n        mask = dates.notna()\n        data = pd.DataFrame({'date': dates[mask], 'qty': qty[mask], 'prod': prod[mask]}).sort_values('date')\n        # Only running boards (exclude grill)\n        data = data[~data['prod'].str.contains('grill')]\n        if data.empty:\n            return None\n        completion_date = None\n        # Iterate and allocate\n        for _, row in data.iterrows():\n            d = row['date']\n            q = float(row['qty'])\n            # allocate to oldest month with positive backlog\n            for m in months_order:\n                if q <= 0: break\n                need = backlog.get(m, 0.0)\n                if need > 0:\n                    take = min(need, q)\n                    backlog[m] = need - take\n                    q -= take\n            # after allocation on this day, check if target month backlog is zero\n            if backlog.get(pd.Period(deadline_month), 0.0) <= 1e-6:\n                completion_date = d\n                break\n        return completion_date\n\n    apr_deadline = pd.Timestamp('2018-04-13')\n    may_deadline = pd.Timestamp('2018-05-01')\n\n    apr_ok = 0\n    may_ok = 0\n    checked = 0\n\n    for s in scenario_sheets:\n        df = context.files.read_excel(excel_res.id, sheet_name=s)\n        apr_done = alloc_completion_date(df, '2018-04')\n        may_done = alloc_completion_date(df, '2018-05')\n        if apr_done is not None:\n            checked += 1\n            if apr_done <= apr_deadline: apr_ok += 1\n        if may_done is not None:\n            # Only count May if April also computed; otherwise the allocation logic may be incomplete\n            if may_done <= may_deadline: may_ok += 1\n\n    if checked == 0:\n        return 0.0, 'Could not evaluate deadlines (missing or malformed scenario tables).'\n\n    # Score: half for April, half for May across scenarios\n    apr_score = 0.3 * (apr_ok / max(1, checked))\n    may_score = 0.3 * (may_ok / max(1, checked))\n    score = apr_score + may_score\n\n    fb = f'April met in {apr_ok}/{checked} scenarios; May met in {may_ok}/{checked} scenarios.'\n    return score, fb"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Managerial Usefulness", "description": "Holistic assessment of presentation quality, clarity of scenarios, and usefulness for an operations managers meeting. Evaluates both spreadsheet readability and the written summary\u2019s clarity and insight.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity, Presentation, and Managerial Insight", "description": "Assess whether the spreadsheet and summary are professionally presented, easy to follow, and provide actionable insight and clear decisions for the meeting audience.", "weight": 2.0, "judge_prompt": "Evaluate the overall professional quality and managerial usefulness of the submission (Excel + PDF/DOCX summary). Consider:\n\nSpreadsheet:\n- Clear sheet/tab names; consistent column naming across scenarios\n- Readability: frozen headers, clear units/labels, visible totals and scenario summary boxes\n- Traceability: Assumptions/Inputs and Open POs are easy to link to daily plans; cumulative/backlog tracking visible\n- Scenario comparability: the three scenario sheets share the same structure\n\nWritten Summary:\n- Each scenario has a concise explanation of actions/assumptions and implications for Crew Cab, Extended Cab, and Grill Guard\n- Explicit Yes/No on meeting Apr 13 and May 1 commitments\n- Highlights trade-offs, risks (e.g., labor constraints, notice period for 10-hour shift), and rationale for chosen plan\n- Appropriate tone and organization for an operations managers meeting\n\nScoring:\n- 2.0: Highly professional and clear; directly supports decision-making; scenario comparisons and risks are explicit\n- 1.0: Generally clear but with minor issues (e.g., some labels unclear or limited risk discussion)\n- 0.5: Understandable but rough; hard to compare scenarios; limited insight\n- 0.0: Disorganized or lacking clarity; not suitable for managerial review\n", "expectation": "A clean, consistent workbook with obvious traceability and a concise, decision-focused summary that clearly states trade-offs and whether deadlines are met."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "b57efde3-26d6-4742-bbff-2b63c43b4baa", "rubric": {"category_name": "Prospecting List for Aqua Nor 2025 (AUV/ROV/Underwater Cameras)", "rationale": "Pattern A (Analytical/spreadsheet curation). The rubric forces a self-documenting Excel with a clearly defined Prospect List table plus supporting documentation sheets to enable deterministic verification. Stage 1 (LLM-only) enforces the exact workbook shape. Stage 2 uses code rules for structural/correctness checks (columns present, values valid, URLs, duplicates, row counts, freshness) and one small LLM consistency check. Stage 3 assesses professional quality and strategic fit.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (LLM ONLY)", "description": "Hard gate. Output must be an Excel workbook with required sheets and table structures to enable verification.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Workbook Structure and Required Sections", "description": "Check that the candidate output is an Excel workbook with the required sheets and table structures.", "weight": 4.0, "judge_prompt": "You are validating that the submission is an Excel workbook with very specific structure. Only check presence/format/structure, not content quality.\n\nACCEPTABLE FORMATS:\n- Excel (.xlsx or .xls). CSV is acceptable ONLY if a single CSV corresponds to the 'Prospect List' table and a second file for documentation is NOT expected (but prefer .xlsx). If multiple CSV files are provided, treat as not meeting the consolidated workbook requirement.\n\nREQUIRED SHEETS (names can be flexible but must be clearly equivalent):\n1) Core table: Sheet named 'Prospect List' (flexible equivalents like 'Prospects', 'Leads - Aqua Nor', 'AquaNor Prospects') containing a single, headered, filterable table with these REQUIRED columns (column names can be close variants/synonyms, but the meaning must be clear):\n   - Company Name\n   - Product Category (must denote AUV, ROV, or Underwater Camera; allow variants like 'Vehicle Type', 'Category', 'AUV/ROV/UC')\n   - Product Names/Models\n   - Relevance to DO Sensor (why LakeHealth DO Sensor is useful)\n   - Integration Notes / Use Cases\n   - Aqua Nor Exhibitor Page URL\n   - Company Website URL\n   - Booth Number (or 'Stand')\n   - Country\n   - Priority (High/Medium/Low or equivalent scale)\n   - Next Action\n   - Owner (or Contact Owner/Rep)\n   - Last Updated (a date field)\n   - Data Source (link or note)\n   Optional but appreciated columns (do not penalize if missing): LinkedIn URL, Email, Contact Type (OEM/Partner/Distributor), Decision Rationale/Source Snippet.\n\n2) Documentation: Sheet named 'Data Dictionary' describing each field in two columns [Field Name | Definition]. It must include definitions for the required fields above AND list allowed values for Product Category (AUV, ROV, UC) and Priority (e.g., High/Medium/Low or equivalent mapping).\n\n3) Method: Sheet named 'Methodology & Coverage' (flexible like 'Methodology', 'Approach & Coverage') with clearly labeled sections:\n   - Approach (how the list was built)\n   - Inclusion Criteria\n   - Exclusion Criteria\n   - How Data Was Verified / Sources\n   - Coverage Summary (counts: total exhibitors reviewed, total prospects identified, breakdown by Product Category)\n   - Last Updated date (a visible date)\n\nOPTIONAL (nice-to-have; do not block if missing):\n4) Shortlist: Sheet 'Shortlist' (e.g., 'Top 10') listing top prospects with columns [Company Name | Product Category | Why High-Priority | Booth Number | Next Action].\n5) Pivot & Stats: Any summary/pivot sheet showing counts by Category and Country.\n\nSTRUCTURE EXPECTATIONS:\n- The 'Prospect List' is a single, clean tabular range with a header row. Columns should be scannable and not merged. Data should be row-wise (one company per row or one product per row tied to a company).\n- Sheet names can vary slightly but must be clearly equivalent in purpose. The required fields must be present in the Prospect List sheet under reasonably named headers.\n\nSCORING:\n- 4.0: Excel workbook present and contains all three required sheets with their required structures. Prospect List has all required columns; Data Dictionary defines fields and allowed values; Methodology & Coverage includes the required sections and a Last Updated date. Optional sheets (Shortlist, Pivot & Stats) may be present but are not required.\n- 3.2: Excel workbook present; Prospect List and Data Dictionary are correct; Methodology & Coverage present but missing one subsection OR Prospect List missing 1-2 non-critical required columns (e.g., Owner or Data Source) while core columns exist.\n- 2.4: Excel workbook present; Prospect List present but missing 3-4 required columns OR Data Dictionary missing; Methodology & Coverage missing.\n- 0.0: Not an Excel workbook; Prospect List sheet missing; or structure is not a proper table (e.g., unstructured notes only).", "expectation": "A well-structured .xlsx with Prospect List, Data Dictionary, and Methodology & Coverage sheets, enabling straightforward verification."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Deterministic + Light LLM)", "description": "Now that the workbook shape is enforced, verify correctness and consistency with code checks and a small LLM consistency check.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Required Fields Present (Fuzzy Column Match)", "description": "Verify that key logical fields exist in the Prospect List sheet using flexible synonym matching.", "weight": 0.9, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef _load_table(context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return None, 'No spreadsheet output.'\n    path = context.files.get_path(output.id)\n    suffix = path.suffix.lower()\n    df = None\n    chosen_sheet = None\n    try:\n        if suffix == '.csv':\n            df = context.files.read_csv(output.id)\n            chosen_sheet = 'csv'\n        else:\n            # Try to find a sheet with 'prospect' or 'lead' in name\n            xls = pd.ExcelFile(path)\n            sheet_names = xls.sheet_names\n            candidates = [s for s in sheet_names if re.search(r\"prospect|lead|aqua\", s, re.I)]\n            chosen_sheet = candidates[0] if candidates else sheet_names[0]\n            df = pd.read_excel(path, sheet_name=chosen_sheet)\n    except Exception as e:\n        return None, f'Failed reading spreadsheet: {e}'\n    if df is None or df.shape[0] == 0:\n        return None, 'Empty or unreadable table.'\n    return (df, chosen_sheet), ''\n\n# Logical fields and synonyms\nREQ_FIELDS = {\n    'company': [\"company name\",\"company\",\"name\",\"organization\",\"org\"],\n    'product_category': [\"product category\",\"category\",\"vehicle type\",\"auv/rov/uc\",\"platform\",\"type\"],\n    'product_models': [\"product names\",\"products\",\"models\",\"product\",\"model\"],\n    'relevance': [\"relevance\",\"fit\",\"do sensor fit\",\"benefit\",\"value proposition\",\"why\"],\n    'integration': [\"integration notes\",\"integration\",\"use cases\",\"application\",\"notes\"],\n    'exhibitor_url': [\"aqua nor url\",\"exhibitor url\",\"exhibitor page\",\"aqua nor page\",\"exhibitor link\"],\n    'company_url': [\"company url\",\"website\",\"url\",\"site\",\"homepage\"],\n    'booth': [\"booth\",\"stand\",\"booth number\",\"stand number\"],\n    'country': [\"country\",\"location\",\"hq country\",\"nation\"],\n    'priority': [\"priority\",\"score\",\"rank\",\"tier\"],\n    'next_action': [\"next action\",\"next step\",\"follow up\",\"follow-up\"],\n    'owner': [\"owner\",\"rep\",\"contact owner\",\"assignee\"],\n    'last_updated': [\"last updated\",\"updated\",\"date\",\"last update\",\"last refreshed\"],\n    'data_source': [\"data source\",\"source\",\"source link\",\"reference\"]\n}\n\nCORE_MIN = ['company','product_category','relevance','integration','exhibitor_url','company_url','priority','next_action']\n\n\ndef evaluate(workflow, context):\n    loaded, err = _load_table(context)\n    if not loaded:\n        return 0.0, err\n    (df, sheet_name) = loaded\n    cols = [str(c).strip().lower() for c in df.columns]\n\n    found = {}\n    missing_core = []\n    for key, syns in REQ_FIELDS.items():\n        found[key] = None\n        for s in syns:\n            if any(s == c or s in c for c in cols):\n                found[key] = s\n                break\n    for k in CORE_MIN:\n        if not found.get(k):\n            missing_core.append(k)\n\n    # Score: all core present => full. Missing 1-2 => partial. Missing >=3 => 0\n    weight = 0.9\n    if len(missing_core) == 0:\n        score = weight\n    elif len(missing_core) <= 2:\n        score = weight * 0.6\n    else:\n        score = 0.0\n    feedback = f\"Found fields: { [k for k,v in found.items() if v] }. Missing core: {missing_core}. Sheet: {sheet_name}\"\n    return score, feedback"}, {"type": "code", "name": "Row Count Sufficiency", "description": "Check that there are enough identified prospects to be useful for a trade show (uniques by Company Name).", "weight": 0.7, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef _load_table(context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return None\n    path = context.files.get_path(output.id)\n    try:\n        if str(path).lower().endswith('.csv'):\n            df = context.files.read_csv(output.id)\n            return df\n        else:\n            import pandas as pd\n            xls = pd.ExcelFile(path)\n            sheet_names = xls.sheet_names\n            candidates = [s for s in sheet_names if re.search(r\"prospect|lead|aqua\", s, re.I)]\n            sheet = candidates[0] if candidates else sheet_names[0]\n            df = pd.read_excel(path, sheet_name=sheet)\n            return df\n    except Exception:\n        return None\n\ndef _find_company_col(cols):\n    cols = [str(c).lower() for c in cols]\n    for key in [\"company name\",\"company\",\"name\",\"organization\",\"org\"]:\n        for c in cols:\n            if key in c:\n                return c\n    return None\n\n\ndef evaluate(workflow, context):\n    df = _load_table(context)\n    weight = 0.7\n    if df is None or df.shape[0] == 0:\n        return 0.0, 'No data rows found.'\n    cols_map = {str(c).lower(): c for c in df.columns}\n    comp_col_l = _find_company_col(df.columns)\n    if not comp_col_l:\n        # fallback to first column\n        comp_col_l = str(df.columns[0]).lower()\n    comp_col = cols_map.get(comp_col_l, df.columns[0])\n\n    series = df[comp_col].astype(str).str.strip()\n    nonempty = series[series.str.len() > 0]\n    unique_count = nonempty.str.lower().nunique()\n\n    if unique_count >= 15:\n        score = weight\n    elif unique_count >= 10:\n        score = weight * 0.8\n    elif unique_count >= 5:\n        score = weight * 0.5\n    else:\n        score = 0.0\n    return score, f'Unique companies: {unique_count}'"}, {"type": "code", "name": "Valid Product Category Values", "description": "Ensure Product Category values include AUV/ROV/Underwater Camera (or close synonyms) for most rows.", "weight": 0.6, "code": "import re, pandas as pd\n\nCATS = [\"auv\",\"rov\",\"uc\",\"underwater camera\",\"camera\",\"uuv\",\"underwater vehicle\"]\n\ndef _load_df(context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return None\n    path = context.files.get_path(output.id)\n    try:\n        if str(path).lower().endswith('.csv'):\n            return context.files.read_csv(output.id)\n        else:\n            import pandas as pd\n            xls = pd.ExcelFile(path)\n            names = xls.sheet_names\n            target = None\n            for s in names:\n                if re.search(r\"prospect|lead|aqua\", s, re.I):\n                    target = s; break\n            if target is None:\n                target = names[0]\n            return pd.read_excel(path, sheet_name=target)\n    except Exception:\n        return None\n\ndef _find_cat_col(cols):\n    cols = [str(c).lower() for c in cols]\n    for key in [\"product category\",\"category\",\"vehicle type\",\"auv/rov/uc\",\"platform\",\"type\"]:\n        for c in cols:\n            if key in c:\n                return c\n    return None\n\n\ndef evaluate(workflow, context):\n    weight = 0.6\n    df = _load_df(context)\n    if df is None or df.shape[0] == 0:\n        return 0.0, 'No data.'\n    cols_map = {str(c).lower(): c for c in df.columns}\n    cat_l = _find_cat_col(df.columns)\n    if not cat_l:\n        return 0.0, 'Product Category column not found.'\n    cat_col = cols_map.get(cat_l)\n\n    vals = df[cat_col].astype(str).str.lower()\n    valid = vals.apply(lambda x: any(tok in x for tok in CATS))\n    if valid.size == 0:\n        return 0.0, 'No category values.'\n    prop = valid.mean()\n    score = weight * prop\n    return score, f'Valid category coverage: {prop:.2%}'"}, {"type": "code", "name": "URL Validity Coverage (Company/Exhibitor)", "description": "Check that URLs in Company Website and/or Aqua Nor Exhibitor URL look valid (http(s) and domain present).", "weight": 0.6, "code": "import re, pandas as pd\n\nURL_RE = re.compile(r\"^https?://[\\w\\.-]+(?:/[\\w\\-\\./%?#=&]*)?$\")\n\ndef _load_df(context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return None\n    path = context.files.get_path(output.id)\n    try:\n        if str(path).lower().endswith('.csv'):\n            return context.files.read_csv(output.id)\n        else:\n            import pandas as pd\n            xls = pd.ExcelFile(path)\n            names = xls.sheet_names\n            pick = None\n            for s in names:\n                if re.search(r\"prospect|lead|aqua\", s, re.I):\n                    pick = s; break\n            if pick is None:\n                pick = names[0]\n            return pd.read_excel(path, sheet_name=pick)\n    except Exception:\n        return None\n\nSYN_MAP = {\n    'company_url': [\"company url\",\"website\",\"url\",\"site\",\"homepage\"],\n    'exhibitor_url': [\"aqua nor url\",\"exhibitor url\",\"exhibitor page\",\"aqua nor page\",\"exhibitor link\"]\n}\n\n\ndef evaluate(workflow, context):\n    weight = 0.6\n    df = _load_df(context)\n    if df is None or df.shape[0] == 0:\n        return 0.0, 'No data.'\n    cols_l = [str(c).lower() for c in df.columns]\n    cols_map = {str(c).lower(): c for c in df.columns}\n    def find_col(keys):\n        for k in keys:\n            for c in cols_l:\n                if k in c:\n                    return cols_map[c]\n        return None\n    cu = find_col(SYN_MAP['company_url'])\n    eu = find_col(SYN_MAP['exhibitor_url'])\n    if cu is None and eu is None:\n        return 0.0, 'No URL columns found.'\n    vals = []\n    if cu is not None:\n        vals.append(df[cu].astype(str))\n    if eu is not None:\n        vals.append(df[eu].astype(str))\n    import pandas as pd\n    combined = pd.concat(vals, axis=1)\n    any_valid = combined.apply(lambda row: any(bool(URL_RE.match(str(v).strip())) for v in row), axis=1)\n    prop = any_valid.mean() if any_valid.size else 0.0\n    score = weight * prop\n    return score, f'Rows with at least one valid URL: {prop:.2%}'"}, {"type": "code", "name": "Duplicate Company Check", "description": "Reward uniqueness: fewer duplicate company rows (case-insensitive) = higher score.", "weight": 0.3, "code": "import re, pandas as pd\n\ndef _load_df(context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return None\n    path = context.files.get_path(output.id)\n    try:\n        if str(path).lower().endswith('.csv'):\n            return context.files.read_csv(output.id)\n        else:\n            import pandas as pd\n            xls = pd.ExcelFile(path)\n            names = xls.sheet_names\n            target = None\n            for s in names:\n                if re.search(r\"prospect|lead|aqua\", s, re.I):\n                    target = s; break\n            if target is None:\n                target = names[0]\n            return pd.read_excel(path, sheet_name=target)\n    except Exception:\n        return None\n\n\ndef evaluate(workflow, context):\n    weight = 0.3\n    df = _load_df(context)\n    if df is None or df.shape[0] == 0:\n        return 0.0, 'No data.'\n    cols_l = [str(c).lower() for c in df.columns]\n    comp_col = None\n    for k in [\"company name\",\"company\",\"name\",\"organization\",\"org\"]:\n        for c in cols_l:\n            if k in c:\n                comp_col = c; break\n        if comp_col: break\n    if comp_col is None:\n        comp_col = cols_l[0]\n    orig_col = {str(c).lower(): c for c in df.columns}[comp_col]\n    s = df[orig_col].astype(str).str.strip().str.lower()\n    if s.size == 0:\n        return 0.0, 'No company names.'\n    total = len(s)\n    unique = s.nunique()\n    dup_ratio = 0.0 if total == 0 else max(0.0, (total - unique) / max(1, total))\n    score = weight * (1 - dup_ratio)\n    return score, f'Duplicate ratio: {dup_ratio:.2%}'"}, {"type": "code", "name": "Date Parsability and Freshness", "description": "Check Last Updated dates are parseable and reasonably recent (year >= 2024).", "weight": 0.3, "code": "import re, pandas as pd\nfrom datetime import datetime\n\ndef _load_df_and_method(context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return None, None\n    path = context.files.get_path(output.id)\n    try:\n        if str(path).lower().endswith('.csv'):\n            return context.files.read_csv(output.id), None\n        else:\n            import pandas as pd\n            xls = pd.ExcelFile(path)\n            names = xls.sheet_names\n            target = None\n            for s in names:\n                if re.search(r\"prospect|lead|aqua\", s, re.I):\n                    target = s; break\n            if target is None:\n                target = names[0]\n            df = pd.read_excel(path, sheet_name=target)\n            # Try to read a methodology sheet too if exists\n            method_name = None\n            for s in names:\n                if re.search(r\"method|coverage|approach\", s, re.I):\n                    method_name = s; break\n            mdf = None\n            if method_name:\n                try:\n                    mdf = pd.read_excel(path, sheet_name=method_name, header=None)\n                except Exception:\n                    mdf = None\n            return df, mdf\n    except Exception:\n        return None, None\n\n\ndef evaluate(workflow, context):\n    weight = 0.3\n    df, mdf = _load_df_and_method(context)\n    if df is None or df.shape[0] == 0:\n        return 0.0, 'No data.'\n    # Find last updated column\n    cols_map = {str(c).lower(): c for c in df.columns}\n    last_col_l = None\n    for k in [\"last updated\",\"updated\",\"date\",\"last update\",\"last refreshed\"]:\n        for c in cols_map:\n            if k in c:\n                last_col_l = c; break\n        if last_col_l: break\n    parseable = 0\n    recent = 0\n    total = 0\n    if last_col_l:\n        col = cols_map[last_col_l]\n        s = df[col]\n        total = len(s)\n        for v in s:\n            try:\n                if pd.isna(v):\n                    continue\n                d = pd.to_datetime(v, errors='raise')\n                parseable += 1\n                if d.year >= 2024:\n                    recent += 1\n            except Exception:\n                continue\n    # Also search methodology sheet for a date token\n    meth_recent_bonus = 0\n    if mdf is not None:\n        try:\n            text = ' '.join(mdf.fillna('').astype(str).values.ravel())\n            # Find yyyy or month year patterns\n            m = re.search(r'(20\\d{2})', text)\n            if m and int(m.group(1)) >= 2024:\n                meth_recent_bonus = 1\n        except Exception:\n            pass\n\n    if parseable == 0 and meth_recent_bonus == 0:\n        return 0.0, 'No parseable dates found.'\n    # Scoring: 50% for having parseable dates; 50% for being recent OR methodology shows recent year\n    parse_score = 0.5 if parseable > 0 else 0.0\n    recent_score = 0.5 if (recent > 0 or meth_recent_bonus) else 0.0\n    score = weight * (parse_score + recent_score)\n    return score, f'Rows with parseable dates: {parseable}, recent rows: {recent}. Methodology recent bonus: {meth_recent_bonus}'"}, {"type": "llm_judge", "name": "Internal Consistency: Categories vs Notes", "description": "Spot-check consistency between Product Category and Relevance/Integration notes. Do notes reference DO/water quality context and plausible integration for AUV/ROV/UC?", "weight": 0.6, "judge_prompt": "Examine the Prospect List sheet. Sample at least 10 rows (or all if fewer). For each sampled row, check:\n- Product Category includes AUV/ROV/Underwater Camera (or clear equivalent).\n- Relevance/Integration notes mention dissolved oxygen (DO), oxygen, water quality, sensor/probe integration, or equivalent benefits relevant to HiTech H20's LakeHealth DO sensor.\n- Notes are specific (e.g., cite product model, underwater ops, aquaculture context) rather than generic.\n\nScore guidance:\n- 0.6: Majority (>=75%) of sampled rows have consistent categories and specific, DO/water-quality-relevant notes.\n- 0.4: About half (50\u201374%) consistent and relevant.\n- 0.2: Some (25\u201349%) consistent and relevant.\n- 0.0: Few (<25%) consistent/relevant or notes are mostly generic/empty.\n\nOnly judge internal consistency based on the sheet content. Do not browse the web.", "expectation": "Notes substantively tie AUV/ROV/UC products to DO-sensor use cases in aquaculture/water monitoring."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality Assessment (LLM)", "description": "Professionalism, usability at the trade show, and strategic value.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Prospecting Quality and Target Fit", "description": "Evaluate whether the list targets true AUV/ROV/Underwater Camera manufacturers relevant to aquaculture and provides enough context to engage at Aqua Nor.", "weight": 1.3, "judge_prompt": "Assess the Prospect List for strategic fit and usefulness for an OEM sales rep at Aqua Nor:\n- Are companies clearly within AUV/ROV/Underwater Camera space and plausibly relevant to aquaculture?\n- Are entries actionable for event outreach (booth/stand numbers, exhibitor links, next actions, priority, owner)?\n- Do notes differentiate value propositions and suggest concrete integration angles for a dissolved oxygen sensor?\n\nScoring:\n- 1.3: Strong targeting and actionable detail across most rows; obviously useful for show-floor engagement.\n- 0.9: Generally good targeting with minor gaps; usable with light fixes.\n- 0.5: Mixed quality; several marginal/non-target leads or thin notes.\n- 0.0: Poor targeting; not useful for Aqua Nor outreach.", "expectation": "A focused, actionable list of high-potential AUV/ROV/UC prospects for DO-sensor integration at Aqua Nor."}, {"type": "llm_judge", "name": "Presentation and Usability", "description": "Evaluate professional presentation, readability, and usability of the workbook on the show floor.", "weight": 0.7, "judge_prompt": "Evaluate the workbook\u2019s presentation:\n- Is the Prospect List a clean, filterable table with clear headers, reasonable column widths, and readable text (wrap text for notes is fine)?\n- Are sheet names clear and professional? No merged cells that break filtering/sorting.\n- Are optional artifacts (Shortlist, Pivot & Stats) present and useful? (Optional \u2013 presence helps but is not required.)\n\nScoring:\n- 0.7: Professional, easy to read, filter, and use; sheet naming and layout are clean.\n- 0.4: Adequate but has minor formatting issues; still usable.\n- 0.2: Messy layout impedes use.\n- 0.0: Hard to read/use (e.g., no headers, merged cells, broken filters).", "expectation": "A professional, readable workbook suitable for rapid filtering and conversations at the event."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "0e4fe8cd-16d0-4f41-8247-6385b4762582", "rubric": {"category_name": "Ultra-High-End Travel Itinerary (Excel) \u2014 Concierge Deliverable", "rationale": "This rubric enforces a self-documenting, verifiable Excel itinerary for a four-day Istanbul trip for an ultra-high net worth principal. Stage 1 (LLM-only) strictly gates the workbook\u2019s shape so downstream checks are trivial. Stage 2 uses deterministic code to verify presence of key events, times, links, logistics, and notes the brief allows (including alternatives when a named vendor cannot be verified). Stage 3 judges overall polish and white-glove quality appropriate to the audience.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Structured Workbook Gate", "description": "LLM-only gate to ensure the output is an Excel workbook with four day-specific tabs and a uniform, verifiable table structure per day, enabling deterministic validation later.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Excel Format and Four-Day Tab Structure", "description": "Check the candidate produced a multi-tab Excel itinerary with mandated structure and day content, enabling verification.", "weight": 4.0, "judge_prompt": "You are validating SHAPE ONLY (not correctness of calculations or factual accuracy). Evaluate the submitted output against these requirements:\n\nFORMAT REQUIREMENTS:\n- The output must be an Excel workbook (.xlsx). Not PDF, not DOCX, not plain text.\n- Exactly four day-oriented tabs representing the four-day journey (naming can be flexible but must clearly correspond to Day 1 (June 1), Day 2 (June 2), Day 3 (June 3), Day 4 (June 4)). Accept flexible names like \u201cDay 1 \u2013 June 1\u201d, \u201cJune 1 / Day 1\u201d, etc.\n- Each day must present an itinerary table that is easy to read with one row per action.\n\nTABLE STRUCTURE PER DAY (flexible column names, but must map clearly to these):\n- Time (start or end time)\n- Action/Item (e.g., Pickup, Wheels Up, Dinner, etc.)\n- Location/Venue\n- Transport/Vehicle\n- Provider/Contact\n- Link/URL (clickable links in cells)\n- Confirmation/Reservation\n- Notes/Details (room identifiers, staging instructions, buffer notes, etc.)\n\nPRESENCE OF KEY ROWS PER DAY (STRUCTURE ONLY \u2014 do not judge correctness, just that rows exist to cover these events):\n- Day 1 (June 1): 08:00 pickup at main house; 09:00 wheels up from JVY Airport; long-haul flight row(s).\n- Day 2 (June 2): 03:00 wheels down at Istanbul Airport (ISL/IST acceptable); 04:30 drop-off at Four Seasons Bosphorus; 09:00 breakfast at Yali (reservation noted); 11:00 private tour with guide (Oguz or comparable alternative flagged); 14:00 lunch at Hidden Garden; 17:00 hotel return + General Manager meeting; 19:00 tuxedo drop-off (Maserto or comparable alternative flagged); 20:30 SUV pickup to dinner; 21:00 dinner at Garden 1897; 22:30 depart/return.\n- Day 3 (June 3): 09:00 breakfast at Yali; 11:00 hair & makeup in guest room (Samira Lowell or alternative flagged); 13:00 Bespoke Tuxedo fitting (master bedroom); 14:00 in-room dining lunch; 16:00 SUV to wedding at Adile Sultan Palace; 22:30 return to hotel.\n- Day 4 (June 4): 08:00 hotel pickup; 09:00 wheels up ISL/IST to JVY; 11:00 landing JVY; 11:00 plane-side pickup; 11:30 drop-off at main house.\n\nSCORING (STRUCTURE ONLY):\n- 4.0: Valid .xlsx with four day tabs; each tab has a clear, row-based itinerary table; columns cover all required fields (flexible names ok); all key rows are present per day; links shown in a dedicated Link/URL column and appear clickable.\n- 3.0: Valid .xlsx with four day tabs; minor deviations (e.g., one missing support column like Provider/Contact or Confirmation, or a couple key rows grouped/combined but still clearly present; links present but mixed into Notes rather than a Link column).\n- 2.0: Valid .xlsx but missing one day tab or multiple required columns; or table format is unclear (e.g., merged text blocks instead of rows) though day content is mostly present.\n- 1.0: Workbook present but wrong format (not .xlsx table per day) or only 1\u20132 days tabulated.\n- 0.0: Not an Excel file or no usable itinerary structure.\n\nOnly evaluate format and structural presence so that verification is possible. Do not assess factual accuracy, timing correctness, or quality of choices here.", "expectation": "A clean .xlsx with four day tabs, each with a single table with columns for Time, Action, Location, Transport, Provider/Contact, Link, Confirmation, and Notes; and rows covering the specified events per day, with clickable links."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Deterministic Correctness & Coverage", "description": "Code-based checks leveraging the enforced shape to verify presence of key events and times, link coverage, logistics notes, and separation of actions as specified. Flexible, fuzzy matching is used where appropriate.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Key Events Presence and Timing Consistency", "description": "Verify that each day contains the specified key events at the specified times with reasonable fuzzy matching (\u00b110 minutes), and that event text references the expected venues/providers.", "weight": 1.4, "code": "import re\\nimport pandas as pd\\nimport numpy as np\\n\\ndef evaluate(workflow, context):\\n    \\n    def norm(s):\\n        if s is None:\\n            return ''\\n        return str(s).strip().lower()\\n\\n    def time_to_minutes(t):\\n        if t is None:\\n            return None\\n        s = str(t).strip().lower()\\n        s = s.replace('a.m.', 'am').replace('p.m.', 'pm')\\n        s = re.sub(r'\\s+', '', s)\\n        # Extract HH:MM with optional am/pm\\n        m = re.match(r'^(\\d{1,2})(?::?(\\d{2}))?(am|pm)?$', s)\\n        if not m:\\n            # try common patterns like 08:00, 8:30am, 20:30\\n            m2 = re.search(r'(\\d{1,2}):(\\d{2})(am|pm)?', s)\\n            if not m2:\\n                return None\\n            hh = int(m2.group(1)); mm = int(m2.group(2)); ap = m2.group(3)\\n        else:\\n            hh = int(m.group(1)); mm = int(m.group(2)) if m.group(2) else 0; ap = m.group(3)\\n        if ap:\\n            if ap == 'am':\\n                if hh == 12:\\n                    hh = 0\\n            else:  # pm\\n                if hh != 12:\\n                    hh += 12\\n        return hh * 60 + mm\\n\\n    def find_columns(df):\\n        cols = {c: norm(c) for c in df.columns}\\n        keymap = {\\n            'time': ['time','start','starttime','when'],\\n            'action': ['action','item','activity','event','agenda','task'],\\n            'location': ['location','venue','place','address','hotel'],\\n            'transport': ['transport','vehicle','car','suv','transfer'],\\n            'provider': ['provider','contact','vendor','host','company','name'],\\n            'link': ['link','url','website','href'],\\n            'confirm': ['confirmation','reservation','res','booking','ref'],\\n            'notes': ['notes','details','remarks','info','instructions']\\n        }\\n        out = {k: None for k in keymap}\\n        for k, cands in keymap.items():\\n            for orig, lower in cols.items():\\n                if any(c in lower for c in cands):\\n                    out[k] = orig\\n                    break\\n        return out\\n\\n    def get_day_sheets(xls):\\n        sheet_names = xls.sheet_names\\n        days = {}\\n        for s in sheet_names:\\n            m = re.search(r'day\\s*(\\d+)', s, re.I)\\n            if m:\\n                try:\\n                    d = int(m.group(1))\\n                    if 1 <= d <= 4:\\n                        days[d] = s\\n                except:\\n                    pass\\n            else:\\n                # try dates: June 1-4 or Jun 1-4\\n                m2 = re.search(r'jun(e)?\\s*(\\d{1,2})', s, re.I)\\n                if m2:\\n                    try:\\n                        dnum = int(m2.group(2))\\n                        if dnum in [1,2,3,4]:\\n                            days[dnum] = s\\n                    except:\\n                        pass\\n        # Fallback to first four sheets in order if mapping incomplete\\n        if len(days) < 4:\\n            for idx, s in enumerate(sheet_names[:4], start=1):\\n                if idx not in days:\\n                    days[idx] = s\\n        return [days.get(1), days.get(2), days.get(3), days.get(4)]\\n\\n    def df_text_rows(df):\\n        # Combine text across relevant columns per row for keyword search\\n        text_cols = [c for c in df.columns if df[c].dtype == object]\\n        joined = []\\n        for _, r in df.iterrows():\\n            vals = []\\n            for c in text_cols:\\n                vals.append(norm(r.get(c)))\\n            joined.append(' '.join(vals))\\n        return joined\\n\\n    def has_event(df, cols, keywords, target_time=None, tol_min=10):\\n        # keywords: list of strings required to be present in combined text\\n        texts = df_text_rows(df)\\n        time_col = cols.get('time')\\n        # Preparse times once\\n        times = []\\n        if time_col and time_col in df.columns:\\n            for v in df[time_col].values:\\n                times.append(time_to_minutes(v))\\n        else:\\n            times = [None]*len(texts)\\n        for i, t in enumerate(texts):\\n            if all(k.lower() in t for k in keywords):\\n                if target_time is None:\\n                    return True\\n                tm = times[i]\\n                if tm is None:\\n                    # If time not parseable, allow match without time penalty\\n                    return True\\n                if abs(tm - target_time) <= tol_min:\\n                    return True\\n        return False\\n\\n    output = context.get_primary_output()\\n    if not output or not output.is_spreadsheet:\\n        return 0.0, 'No spreadsheet output found.'\\n\\n    try:\\n        xfile = context.files.get_path(output.id)\\n        xls = pd.ExcelFile(xfile)\\n    except Exception as e:\\n        return 0.0, f'Failed to open Excel: {e}'\\n\\n    sheets = get_day_sheets(xls)\\n    # Safety: ensure we have four sheets\\n    if any(s is None for s in sheets):\\n        return 0.0, 'Could not resolve four day sheets.'\\n\\n    checks = []\\n    passed = 0\\n    feedback = []\\n\\n    # Day 1 expected events\\n    d1 = pd.read_excel(xls, sheet_name=sheets[0])\\n    c1 = find_columns(d1)\\n    checks.append(('Day1 08:00 pickup main house', has_event(d1, c1, ['pickup','main','house'], 8*60)))\\n    checks.append(('Day1 09:00 wheels up JVY', has_event(d1, c1, ['wheels','up','jvy'], 9*60) or has_event(d1, c1, ['wheels','up','airport'], 9*60)))\\n\\n    # Day 2 expected events\\n    d2 = pd.read_excel(xls, sheet_name=sheets[1])\\n    c2 = find_columns(d2)\\n    checks.append(('Day2 03:00 wheels down ISL/IST', has_event(d2, c2, ['wheels','down','istanbul'], 3*60) or has_event(d2, c2, ['wheels','down','isl'], 3*60) or has_event(d2, c2, ['wheels','down','ist'], 3*60)))\\n    checks.append(('Day2 04:30 drop-off Four Seasons Bosphorus', has_event(d2, c2, ['drop','four','seasons','bosphorus'], 4*60+30)))\\n    checks.append(('Day2 09:00 breakfast Yali', has_event(d2, c2, ['breakfast','yali'], 9*60)))\\n    checks.append(('Day2 11:00 tour with guide (Oguz or alt)', has_event(d2, c2, ['tour','guide'], 11*60)))\\n    checks.append(('Day2 14:00 lunch Hidden Garden', has_event(d2, c2, ['lunch','hidden','garden'], 14*60)))\\n    checks.append(('Day2 17:00 GM meeting at Four Seasons', has_event(d2, c2, ['general','manager','four','seasons'], 17*60)))\\n    checks.append(('Day2 19:00 tux drop-off (Maserto or alt)', has_event(d2, c2, ['tux'], 19*60)))\\n    checks.append(('Day2 20:30 SUV pickup to dinner', has_event(d2, c2, ['pickup','suv'], 20*60+30)))\\n    checks.append(('Day2 21:00 dinner Garden 1897', has_event(d2, c2, ['dinner','garden','1897'], 21*60)))\\n    checks.append(('Day2 22:30 depart/return to hotel', has_event(d2, c2, ['depart','return'], 22*60+30) or has_event(d2, c2, ['pickup','hotel'], 22*60+30)))\\n\\n    # Day 3 expected events\\n    d3 = pd.read_excel(xls, sheet_name=sheets[2])\\n    c3 = find_columns(d3)\\n    checks.append(('Day3 09:00 breakfast Yali', has_event(d3, c3, ['breakfast','yali'], 9*60)))\\n    checks.append(('Day3 11:00 hair & makeup (Samira or alt)', has_event(d3, c3, ['hair','makeup'], 11*60)))\\n    checks.append(('Day3 13:00 Bespoke Tuxedo fitting', has_event(d3, c3, ['tuxedo','fitting'], 13*60)))\\n    checks.append(('Day3 14:00 in-room dining', has_event(d3, c3, ['in-room','dining'], 14*60) or has_event(d3, c3, ['room','service'], 14*60)))\\n    checks.append(('Day3 16:00 SUV to Adile Sultan Palace', has_event(d3, c3, ['suv','adile','sultan','palace'], 16*60)))\\n    checks.append(('Day3 22:30 return to hotel', has_event(d3, c3, ['return','hotel'], 22*60+30) or has_event(d3, c3, ['pickup','hotel'], 22*60+30)))\\n\\n    # Day 4 expected events\\n    d4 = pd.read_excel(xls, sheet_name=sheets[3])\\n    c4 = find_columns(d4)\\n    checks.append(('Day4 08:00 hotel pickup', has_event(d4, c4, ['pickup','hotel'], 8*60)))\\n    checks.append(('Day4 09:00 wheels up ISL/IST to JVY', has_event(d4, c4, ['wheels','up'], 9*60)))\\n    checks.append(('Day4 11:00 landing JVY', has_event(d4, c4, ['landing','jvy'], 11*60) or has_event(d4, c4, ['land','jvy'], 11*60)))\\n    checks.append(('Day4 11:00 plane-side pickup', has_event(d4, c4, ['plane-side','pickup'], 11*60) or has_event(d4, c4, ['planeside','pickup'], 11*60)))\\n    checks.append(('Day4 11:30 drop-off main house', has_event(d4, c4, ['drop','main','house'], 11*60+30)))\\n\\n    for label, ok in checks:\\n        if ok:\\n            passed += 1\\n        else:\\n            feedback.append(f'Missing or mismatched: {label}')\\n\\n    total = len(checks)\\n    # Convert to score by proportion of checks passed\\n    score = (passed / total) * 1.4 if total > 0 else 0.0\\n    return score, '; '.join(feedback) if feedback else 'All key events and times found with acceptable tolerance.'"}, {"type": "code", "name": "Link Coverage for Critical Entities", "description": "Check that rows referencing critical venues/providers include clickable links (http/https). Accept high-quality alternatives if the named entity is unverifiable and the sheet notes an alternative.", "weight": 0.9, "code": "import re\\nimport pandas as pd\\n\\ndef evaluate(workflow, context):\\n    def norm(s):\\n        return '' if s is None else str(s).strip().lower()\\n\\n    def find_columns(df):\\n        cols = {c: norm(c) for c in df.columns}\\n        link_cols = [orig for orig, low in cols.items() if any(k in low for k in ['link','url','website','href'])]\\n        text_cols = [orig for orig, low in cols.items() if df[orig].dtype == object]\\n        notes_col = None\\n        for orig, low in cols.items():\\n            if any(k in low for k in ['note','detail','remark','instruction']):\\n                notes_col = orig\\n                break\\n        return link_cols, text_cols, notes_col\\n\\n    def has_http(s):\\n        if not s: return False\\n        return 'http://' in s or 'https://' in s\\n\\n    output = context.get_primary_output()\\n    if not output or not output.is_spreadsheet:\\n        return 0.0, 'No spreadsheet to check links.'\\n\\n    xfile = context.files.get_path(output.id)\\n    xls = pd.ExcelFile(xfile)\\n\\n    entities = [\\n        ('four seasons bosphorus', ['four','seasons','bosphorus']),\\n        ('yali', ['yali']),\\n        ('oguz (tour guide) or alternative', ['tour','guide']),\\n        ('hidden garden', ['hidden','garden']),\\n        ('general manager (four seasons bosphorus)', ['general','manager','four','seasons']),\\n        ('maserto or alternative tux provider', ['tux']),\\n        ('garden 1897', ['garden','1897']),\\n        ('samira lowell or alternative hair/makeup', ['hair','makeup']),\\n        ('bespoke tuxedo', ['tuxedo']),\\n        ('adile sultan palace', ['adile','sultan','palace'])\\n    ]\\n\\n    found_with_links = []\\n    missing = []\\n\\n    for name, keys in entities:\\n        found = False\\n        linked = False\\n        alt_noted = False\\n        for s in xls.sheet_names:\\n            df = pd.read_excel(xls, sheet_name=s)\\n            link_cols, text_cols, notes_col = find_columns(df)\\n            for _, r in df.iterrows():\\n                text_blob = ' '.join([norm(r.get(c)) for c in text_cols])\\n                if all(k in text_blob for k in keys):\\n                    found = True\\n                    # check links anywhere in row\\n                    row_has_link = False\\n                    for lc in link_cols:\\n                        if has_http(str(r.get(lc))):\\n                            row_has_link = True\\n                            break\\n                    if not row_has_link:\\n                        # fallback: links embedded in notes/details or other text cols\\n                        if 'http://' in text_blob or 'https://' in text_blob:\\n                            row_has_link = True\\n                    if notes_col:\\n                        ntext = norm(r.get(notes_col))\\n                        if any(k in ntext for k in ['alternative','comparable','unverified','cannot verify']):\\n                            alt_noted = True\\n                    if row_has_link:\\n                        linked = True\\n                        break\\n            if linked:\\n                break\\n        if linked or (found and alt_noted):\\n            found_with_links.append(name)\\n        else:\\n            missing.append(name)\\n\\n    total = len(entities)\\n    got = len(found_with_links)\\n    weight = 0.9\\n    score = (got/total)*weight if total>0 else 0.0\\n    fb = ''\\n    if missing:\\n        fb = 'Missing links or alternative note for: ' + ', '.join(missing)\\n    else:\\n        fb = 'All critical entities have links or documented alternatives.'\\n    return score, fb"}, {"type": "code", "name": "Car Service Logistics and Staging", "description": "Verify car/SUV logistics are explicitly documented: SUV specified for key pickups, staging instructions at dinner (Day 2) and wedding (Day 3), and plane-side handling on Day 4.", "weight": 0.6, "code": "import re\\nimport pandas as pd\\n\\ndef evaluate(workflow, context):\\n    def norm(s):\\n        return '' if s is None else str(s).strip().lower()\\n\\n    def find_cols(df):\\n        cols = {c: norm(c) for c in df.columns}\\n        transport = None\\n        notes = None\\n        for orig, low in cols.items():\\n            if any(k in low for k in ['transport','vehicle','car','suv','transfer']):\\n                transport = orig\\n            if any(k in low for k in ['note','detail','instruction','remark']):\\n                notes = orig\\n        text_cols = [c for c in df.columns if df[c].dtype == object]\\n        return transport, notes, text_cols\\n\\n    def sheet_by_day(xls, day_index):\\n        # fallback: day_index is 0-based\\n        names = xls.sheet_names\\n        if day_index < len(names):\\n            return names[day_index]\\n        return names[-1]\\n\\n    output = context.get_primary_output()\\n    if not output or not output.is_spreadsheet:\\n        return 0.0, 'No spreadsheet found.'\\n\\n    xfile = context.files.get_path(output.id)\\n    xls = pd.ExcelFile(xfile)\\n\\n    # Day 2 dinner staging\\n    s2 = sheet_by_day(xls, 1)\\n    d2 = pd.read_excel(xls, sheet_name=s2)\\n    tcol, ncol, tcols = find_cols(d2)\\n    dinner_stage_ok = False\\n    for _, r in d2.iterrows():\\n        row_text = ' '.join([norm(r.get(c)) for c in tcols])\\n        if 'garden' in row_text and '1897' in row_text:\\n            if (tcol and 'suv' in norm(r.get(tcol))) or 'suv' in row_text:\\n                if 'stage' in row_text or 'waiting' in row_text or 'standby' in row_text:\\n                    dinner_stage_ok = True\\n                    break\\n    # Day 3 wedding staging\\n    s3 = sheet_by_day(xls, 2)\\n    d3 = pd.read_excel(xls, sheet_name=s3)\\n    tcol3, ncol3, tcols3 = find_cols(d3)\\n    wedding_stage_ok = False\\n    for _, r in d3.iterrows():\\n        row_text = ' '.join([norm(r.get(c)) for c in tcols3])\\n        if 'adile' in row_text and 'sultan' in row_text and 'palace' in row_text:\\n            if (tcol3 and 'suv' in norm(r.get(tcol3))) or 'suv' in row_text:\\n                if 'stage' in row_text or 'waiting' in row_text or 'standby' in row_text:\\n                    wedding_stage_ok = True\\n                    break\\n    # Day 4 plane-side handling\\n    s4 = sheet_by_day(xls, 3)\\n    d4 = pd.read_excel(xls, sheet_name=s4)\\n    tcol4, ncol4, tcols4 = find_cols(d4)\\n    planeside_ok = any(('plane-side' in ' '.join([norm(r.get(c)) for c in tcols4])) or ('planeside' in ' '.join([norm(r.get(c)) for c in tcols4])) for _, r in d4.iterrows())\\n\\n    # Day 2 SUV explicitly specified at pickup\\n    suv_pickup_ok = False\\n    for _, r in d2.iterrows():\\n        row_text = ' '.join([norm(r.get(c)) for c in tcols])\\n        if 'pickup' in row_text and ('suv' in row_text or (tcol and 'suv' in norm(r.get(tcol)))):\\n            suv_pickup_ok = True\\n            break\\n\\n    checks = [dinner_stage_ok, wedding_stage_ok, planeside_ok, suv_pickup_ok]\\n    score = (sum(1 for x in checks if x) / 4.0) * 0.6\\n    missing = []\\n    if not dinner_stage_ok: missing.append('Day 2 dinner staging with SUV not explicit')\\n    if not wedding_stage_ok: missing.append('Day 3 wedding staging with SUV not explicit')\\n    if not planeside_ok: missing.append('Day 4 plane-side handling not explicit')\\n    if not suv_pickup_ok: missing.append('Day 2 pickup does not explicitly specify SUV')\\n    fb = 'All car logistics clear.' if not missing else '; '.join(missing)\\n    return score, fb"}, {"type": "code", "name": "Action Separation and Reservations", "description": "Ensure separate rows for Day 2 pickup vs dinner, and that reservations under Smith are documented for Yali (breakfast) and Garden 1897 (dinner).", "weight": 0.7, "code": "import re\\nimport pandas as pd\\n\\ndef evaluate(workflow, context):\\n    def norm(s):\\n        return '' if s is None else str(s).strip().lower()\\n\\n    output = context.get_primary_output()\\n    if not output or not output.is_spreadsheet:\\n        return 0.0, 'No spreadsheet.'\\n    xls = pd.ExcelFile(context.files.get_path(output.id))\\n    if len(xls.sheet_names) < 2:\\n        return 0.0, 'Insufficient sheets.'\\n    d2 = pd.read_excel(xls, sheet_name=xls.sheet_names[1])\\n    # Identify text columns\\n    tcols = [c for c in d2.columns if d2[c].dtype == object]\\n\\n    def row_matches(r, must):\\n        txt = ' '.join([norm(r.get(c)) for c in tcols])\\n        return all(m in txt for m in must)\\n\\n    # Separate rows: pickup vs dinner\\n    pickup_rows = [i for i, r in d2.iterrows() if row_matches(r, ['pickup','hotel'])]\\n    dinner_rows = [i for i, r in d2.iterrows() if row_matches(r, ['dinner','garden','1897'])]\\n    separate_ok = bool(pickup_rows) and bool(dinner_rows) and set(pickup_rows).isdisjoint(dinner_rows)\\n\\n    # Reservations under Smith\\n    res_ok_yali = any(row_matches(r, ['yali','smith']) and ('breakfast' in ' '.join([norm(r.get(c)) for c in tcols])) for _, r in d2.iterrows())\\n    res_ok_garden = any(row_matches(r, ['garden','1897','smith']) and ('dinner' in ' '.join([norm(r.get(c)) for c in tcols])) for _, r in d2.iterrows())\\n\\n    checks = [separate_ok, res_ok_yali, res_ok_garden]\\n    score = (sum(1 for x in checks if x) / 3.0) * 0.7\\n    missing = []\\n    if not separate_ok: missing.append('Pickup and dinner not on separate rows (Day 2).')\\n    if not res_ok_yali: missing.append('Reservation under Smith not noted for Yali (Day 2 breakfast).')\\n    if not res_ok_garden: missing.append('Reservation under Smith not noted for Garden 1897 (Day 2 dinner).')\\n    fb = 'Action separation and reservations documented.' if not missing else ' '.join(missing)\\n    return score, fb"}, {"type": "code", "name": "Travel Math and Timezone Notes", "description": "Check for explicit notes about 10-hour flight + 8-hour push on outbound, and regaining 8 hours on return.", "weight": 0.4, "code": "import re\\nimport pandas as pd\\n\\ndef evaluate(workflow, context):\\n    def norm(s):\\n        return '' if s is None else str(s).strip().lower()\\n\\n    output = context.get_primary_output()\\n    if not output or not output.is_spreadsheet:\\n        return 0.0, 'No spreadsheet.'\\n    xls = pd.ExcelFile(context.files.get_path(output.id))\\n\\n    def sheet(idx):\\n        names = xls.sheet_names\\n        return names[idx] if idx < len(names) else names[-1]\\n\\n    d1 = pd.read_excel(xls, sheet_name=sheet(0))\\n    d4 = pd.read_excel(xls, sheet_name=sheet(3))\\n    tcols1 = [c for c in d1.columns if d1[c].dtype == object]\\n    tcols4 = [c for c in d4.columns if d4[c].dtype == object]\\n\\n    text1 = ' '.join([norm(v) for c in tcols1 for v in d1[c].astype(str)])\\n    text4 = ' '.join([norm(v) for c in tcols4 for v in d4[c].astype(str)])\\n\\n    out_ok = (('10' in text1 and 'hour' in text1) and ('8' in text1 and ('push' in text1 or 'ahead' in text1 or 'forward' in text1)))\\n    rtn_ok = (('8' in text4) and ('gain' in text4 or 'gaining' in text4 or 'back' in text4))\\n\\n    score = 0.0\\n    if out_ok: score += 0.2\\n    if rtn_ok: score += 0.2\\n\\n    fb_bits = []\\n    if not out_ok: fb_bits.append('Outbound flight details do not explicitly note 10 hours + 8-hour push.')\\n    if not rtn_ok: fb_bits.append('Return day does not explicitly note gaining 8 hours.')\\n    fb = 'Both outbound and return timezone notes present.' if not fb_bits else ' '.join(fb_bits)\\n    return score, fb"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 White-Glove Quality and Presentation", "description": "LLM assessment of professional polish, readability, and concierge-level foresight appropriate for an ultra-high net worth principal.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professionalism, Readability, and White-Glove Touches", "description": "Evaluate the clarity, usability, and strategic concierge touches of the itinerary.", "weight": 2.0, "judge_prompt": "Judge the overall quality of the Excel itinerary now that structure exists. Consider:\\n- Readability and layout: clear time column, consistent formatting, row-per-action clarity, sensible grouping, and scannability (e.g., freeze top row, bold headers).\\n- Clickable links: links are actually clickable, not just pasted as plaintext, and are placed in a dedicated Link/URL column or clearly indicated.\\n- White-glove touches: proactive buffer times, staging notes, room identifiers (guest room vs. master), plane-side handling, contingency notes (e.g., backup tour guide/vendor), cultural/tipping considerations, dress code cues, and VIP networking notes (e.g., GM profile context).\\n- Appropriateness for principal: high-end tone, succinct but complete details, no clutter, and clear separation of pickups/transfers vs. reservations.\\n- Strategic value: thoughtful sequencing around jet lag, local traffic patterns, and not over-scheduling immediately after long flights.\\n\\nScoring:\\n- 2.0: Highly professional, exceptionally readable, thoughtful white-glove enhancements and contingencies evident.\\n- 1.0: Generally professional and readable with some concierge touches; minor gaps.\\n- 0.5: Adequate but plain; limited foresight or polish.\\n- 0.0: Disorganized or not suitable for UHNW principal use.\\n\\nDo not re-check shape; focus on presentation quality and concierge caliber.", "expectation": "A polished, scannable, VIP-ready itinerary with proactive, thoughtful touches and contingencies, suitable for immediate use by an UHNW principal."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "91060ff0-3eb5-4ddf-9edb-f6758b95499e", "rubric": {"category_name": "Retail Pharmacist: Educational Poster on Warts (Health Fair)", "rationale": "This rubric uses a self-documenting, staged approach. Stage 1 is a strict LLM-only shape gate that mandates a poster-style PDF with specific sections, visuals, and a product comparison table so later verification is trivial. Stage 2 mixes code rules (text extraction from PDF) with LLM checks to validate factual coverage, safety/red-flag guidance, OTC specifics, follow-up and references. Stage 3 uses LLM judges to assess professional presentation quality, readability, and audience appropriateness.", "max_total_score": 28.5, "stages": [{"name": "Stage 1 Gate: Poster Format and Structural Completeness (LLM-only)", "description": "Verify the candidate produced a single-page landscape PDF poster (approximately 36 x 24 inches, ~3:2 aspect ratio) with all required sections, visuals, and a product comparison table. Only check structure and presence, not correctness.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.0, "rules": [{"type": "llm_judge", "name": "Structured Poster Requirement (PDF, sections, visuals, references)", "description": "Check the poster is a one-page landscape PDF, sized approximately 36x24 inches (3:2 aspect), with required sections and visuals, including an OTC product comparison table and references.", "weight": 6.0, "judge_prompt": "You are validating the SHAPE and STRUCTURE only. Inspect the candidate output (PDF is rendered for you). Determine if it is a single-page, landscape poster approximately 36 x 24 inches (aspect ratio ~3:2) with a clear, section-based layout and the required components.\n\nFormat requirements:\n- File type: PDF only (not Word, not image-only assets without text). One page, landscape orientation. Approximate poster aspect ratio ~3:2 consistent with 36x24 inches.\n- Large, prominent title (e.g., \u201cWarts: What to Know\u201d or similar) and readable headings.\n- Multi-column layout (2\u20133 columns) with visible section headers.\n\nRequired sections (exact wording can vary; judge by intent and headers):\n1) What warts are and how they develop (overview/definition/pathophysiology)\n2) Causes, including viral origin (HPV) and contributing/risk factors\n3) Common signs and symptoms\n4) Goals of treatment\n5) When to refer to a physician/specialist (red flags)\n6) Pharmacological treatments with emphasis on OTC options in community pharmacy\n7) Non-pharmacological and preventative strategies\n8) When to follow up\n9) References (at least 3 reputable sources listed) \n10) Pharmacist\u2019s role or a \u201cTalk to your pharmacist\u201d callout box\n\nVisuals/structural elements:\n- At least one comparison table of OTC wart products (or an equivalent matrix) showing, at minimum: Product/Brand, Active Ingredient, Strength, Form, Basic Directions/Use, Key Warnings/Contraindications, and Age/Population considerations. At least 3 products.\n- At least 3 supportive visuals (icons, simple illustrations, or charts). They can be small, but they must be discernible as visuals, not just text.\n\nScoring:\n- 6.0: PDF poster (one landscape page ~3:2) with ALL required sections, a clear comparison table (>=3 products with the listed informational columns), >=3 visuals, and a references section (>=3 sources).\n- 5.0: Meets format and most structure, but minor omissions (e.g., one section thin or 1 missing visual) while the table and references are still present.\n- 3.5: Poster-like PDF but missing 1\u20132 required sections OR missing the comparison table OR missing references.\n- 2.0: Poster-like structure but multiple major omissions (several sections missing) or wrong format cues (multi-page, portrait) though still a PDF.\n- 0.0: Not a PDF; or not a poster (e.g., plain page without poster structure); or severely incomplete.", "expectation": "A one-page landscape PDF poster (~36x24 in aspect) with all specified sections, a >=3-product OTC comparison table, >=3 visuals, and references (>=3)."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Correctness and Safety Verification (Code + LLM)", "description": "Now that the poster shape is verified, check correctness and completeness of content using code (PDF text extraction) and LLM judges for visual/table verification and medical safety consistency.", "is_required": false, "max_points": 16.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Core Topic Coverage (8 required areas)", "description": "Text-based check that required topics are covered: definition/development, causes (HPV), signs/symptoms, goals of treatment, when to refer, OTC pharmacologic options, non-pharmacologic/prevention, and follow-up.", "weight": 3.0, "code": "def evaluate(workflow, context):\n    import re\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output.\"\n    if not output.is_document:\n        return 0.0, \"Primary output is not a document.\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n        if not text:\n            return 0.0, \"No extractable text from PDF.\"\n        t = text.lower()\n        groups = {\n            'what_are_warts': ['what is a wart', 'what are warts', 'overview', 'definition', 'develop', 'pathophysiology'],\n            'causes_hpv': ['hpv', 'human papillomavirus', 'viral', 'cause', 'contagious'],\n            'signs_symptoms': ['signs', 'symptoms', 'appearance', 'clinical feature'],\n            'goals_treatment': ['goal of treatment', 'treatment goals', 'objectives'],\n            'when_to_refer': ['refer', 'seek medical', 'physician', 'doctor', 'specialist'],\n            'pharm_otc': ['over-the-counter', 'otc', 'salicylic', 'cryotherapy', 'freeze', 'product'],\n            'nonpharm_prevent': ['non-pharmacologic', 'nonpharmacologic', 'prevention', 'prevent', 'hygiene', 'home care'],\n            'follow_up': ['follow up', 'follow-up', 'reassess', 'monitor', 'return']\n        }\n        found = 0\n        for k, kws in groups.items():\n            if any(kw in t for kw in kws):\n                found += 1\n        score = (found / len(groups)) * 3.0\n        return score, f\"Covered {found}/{len(groups)} core topics.\"\n    except Exception as e:\n        return 0.0, f\"Exception during coverage check: {e}\""}, {"type": "code", "name": "OTC Treatment Specificity (salicylic strengths, freeze, instructions)", "description": "Checks for specific OTC details: salicylic acid mention with typical strengths (e.g., 17% or 40%), freeze therapy (DMEP), and basic use instructions (soak/file/apply) and duration (weeks).", "weight": 2.0, "code": "def evaluate(workflow, context):\n    import re\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        t = (context.files.read_pdf_text(output.id) or '').lower()\n        parts = 0\n        # Salicylic acid + strengths\n        has_salic = ('salicylic acid' in t) or ('salicylic' in t)\n        has_strength = ('17%' in t) or ('40%' in t)\n        if has_salic and has_strength:\n            parts += 1\n        # Freeze/DMEP mention\n        if any(k in t for k in ['freeze', 'cryotherapy', 'dimethyl ether', 'propane', 'dmep']):\n            parts += 1\n        # Instructions: soak/file/apply daily concepts\n        if any(k in t for k in ['soak', 'file', 'pumice', 'emery', 'daily', 'once daily', 'apply daily']):\n            parts += 1\n        # Duration in weeks\n        if re.search(r\"\\b(\\d{1,2})\\s*(week|weeks|wk|wks)\\b\", t):\n            parts += 1\n        score = (parts / 4) * 2.0\n        return score, f\"OTC specificity elements present: {parts}/4.\"\n    except Exception as e:\n        return 0.0, f\"Exception in OTC specificity check: {e}\""}, {"type": "code", "name": "Referral Red Flags Mentioned", "description": "Checks for appropriate referral criteria: face/genitals, immunocompromised, diabetes/poor circulation, nail involvement, bleeding/infected/painful lesions, uncertain diagnosis, pregnancy/young children.", "weight": 1.5, "code": "def evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        t = (context.files.read_pdf_text(output.id) or '').lower()\n        flags = [\n            'face', 'facial', 'genital', 'mucous',\n            'immunocompromised', 'immunosuppressed',\n            'diabetes', 'poor circulation', 'peripheral neuropathy',\n            'under the nail', 'subungual', 'nail involvement',\n            'bleeding', 'infected', 'infection', 'painful',\n            'uncertain diagnosis', 'not sure', 'suspicious lesion',\n            'pregnancy', 'pregnant', 'child', 'children', 'pediatric'\n        ]\n        count = sum(1 for f in flags if f in t)\n        # Heuristic: require at least 6 distinct red-flag mentions for full credit\n        ratio = min(count / 6.0, 1.0)\n        return ratio * 1.5, f\"Red-flag mentions found: {count}.\"\n    except Exception as e:\n        return 0.0, f\"Exception in red-flag check: {e}\""}, {"type": "code", "name": "Follow-up Guidance Presence", "description": "Checks that follow-up advice is provided, ideally with a timeframe (weeks) and conditional guidance if not improved.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    import re\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        t = (context.files.read_pdf_text(output.id) or '').lower()\n        has_follow = ('follow up' in t) or ('follow-up' in t) or ('reassess' in t) or ('monitor' in t)\n        has_timeframe = bool(re.search(r\"\\b(\\d{1,2})\\s*(week|weeks|wk|wks|day|days)\\b\", t))\n        has_if_not_improved = any(k in t for k in ['if not improved', 'no improvement', 'persistent', 'worsen', 'worsening'])\n        parts = sum([has_follow, has_timeframe, has_if_not_improved])\n        return (parts/3)*1.0, f\"Follow-up components present: {parts}/3.\"\n    except Exception as e:\n        return 0.0, f\"Exception in follow-up check: {e}\""}, {"type": "code", "name": "References Section and Count", "description": "Verifies that a references/sources section exists with at least 3 citations (year/URL/DOI/PMID).", "weight": 1.5, "code": "def evaluate(workflow, context):\n    import re\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        txt = context.files.read_pdf_text(output.id) or ''\n        t = txt.lower()\n        # Locate references section\n        start_idx = -1\n        for key in ['\\nreferences', '\\nsources', '\\ncitations']:\n            i = t.find(key)\n            if i != -1:\n                start_idx = i\n                break\n        if start_idx == -1:\n            return 0.0, \"No references section detected.\"\n        ref_text = txt[start_idx:]\n        lines = [ln.strip() for ln in ref_text.splitlines() if ln.strip()]\n        patt = re.compile(r\"((19|20)\\d{2})|doi\\.org|https?://|pmid|isbn\", re.I)\n        hits = [ln for ln in lines if patt.search(ln)]\n        n = len(hits)\n        ratio = min(n/3.0, 1.0)\n        return ratio*1.5, f\"Detected ~{n} reference-like entries.\"\n    except Exception as e:\n        return 0.0, f\"Exception in references check: {e}\""}, {"type": "code", "name": "Minimum Informational Density (Word Count)", "description": "Ensures sufficient self-study content. Checks for at least ~300 words in the poster text.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        txt = context.files.read_pdf_text(output.id) or ''\n        words = [w for w in txt.split() if w.strip()]\n        n = len(words)\n        if n >= 300:\n            return 1.0, f\"Word count OK: {n}.\"\n        elif n >= 200:\n            return 0.6, f\"Moderate word count: {n}.\"\n        else:\n            return 0.0, f\"Low word count: {n}.\"\n    except Exception as e:\n        return 0.0, f\"Exception in word count check: {e}\""}, {"type": "code", "name": "Non-pharmacologic and Prevention Strategies", "description": "Checks mention of common non-drug strategies and prevention tips (e.g., duct tape/occlusion, soak/file, avoid picking, hygiene, do not share, shower shoes).", "weight": 1.0, "code": "def evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        t = (context.files.read_pdf_text(output.id) or '').lower()\n        terms = [\n            'duct tape', 'occlusion', 'soak', 'file', 'pumice', 'emery',\n            'avoid picking', 'do not pick', 'hand hygiene', 'wash hands', 'do not share',\n            'shower shoes', 'flip-flops', 'avoid walking barefoot', 'keep feet dry', 'cover the wart'\n        ]\n        count = sum(1 for k in terms if k in t)\n        # Aim for at least 3 items mentioned\n        ratio = min(count/3.0, 1.0)\n        return ratio*1.0, f\"Non-pharm/prevention items found: {count}.\"\n    except Exception as e:\n        return 0.0, f\"Exception in non-pharm check: {e}\""}, {"type": "code", "name": "HPV Etiology Stated", "description": "Confirms viral origin: HPV/human papillomavirus.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        t = (context.files.read_pdf_text(output.id) or '').lower()\n        has_hpv = ('hpv' in t) or ('human papillomavirus' in t)\n        has_virus = 'virus' in t or 'viral' in t\n        parts = int(has_hpv) + int(has_virus)\n        return (parts/2)*1.0, f\"HPV/viral mentions: {parts}/2.\"\n    except Exception as e:\n        return 0.0, f\"Exception in HPV etiology check: {e}\""}, {"type": "llm_judge", "name": "Visuals and OTC Comparison Table Presence (Visual Verification)", "description": "Visually verify that the poster includes a clear OTC product comparison table with multiple products and multiple key fields, and at least 3 supportive visuals (icons/illustrations/charts).", "weight": 2.0, "judge_prompt": "Visually inspect the poster PDF for:\n1) An OTC product comparison table (or equivalent matrix) showing multiple products (>=3). It should clearly identify at least: Product/Brand, Active Ingredient, Strength, Form, Basic Directions/Use, Key Warnings/Contraindications, Age/Population.\n2) At least 3 supportive visuals (icons, illustrations, or charts), distinct from plain text.\n\nScoring (0\u20132.0):\n- 2.0: Clear table with >=3 products and >=5 of the listed fields; AND >=3 visuals present.\n- 1.0: Table present but limited (e.g., only 2\u20134 fields or only 2 products) OR visuals count only 1\u20132.\n- 0.0: No comparison table or no meaningful visuals.", "expectation": "A visible multi-product OTC table plus >=3 distinct visuals."}, {"type": "llm_judge", "name": "Medical Safety and Scope Consistency", "description": "Check for safety consistency: poster should not encourage unsafe self-treatment (e.g., facial/genital warts, uncertain lesions) and should include red-flag referral cues and age/pregnancy/condition cautions.", "weight": 2.0, "judge_prompt": "Assess whether safety and scope guidance appears medically appropriate:\n- Includes cautions NOT to self-treat suspicious, facial, or genital lesions.\n- Mentions key referral/red flags (e.g., immunocompromised, diabetes/poor circulation/peripheral neuropathy, severe pain/bleeding/infection, subungual involvement, failure after adequate OTC trial).\n- Provides age/pregnancy/condition warnings where relevant.\n- Does not make unsafe or misleading claims.\n\nScoring (0\u20132.0):\n- 2.0: Comprehensive and accurate safety guidance with appropriate do-not-self-treat statements and referral criteria clearly stated.\n- 1.0: Partially present but some important cautions missing or vague.\n- 0.0: Lacks safety guidance or contains unsafe recommendations.", "expectation": "Clear, conservative, pharmacist-appropriate safety and referral guidance."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3: Presentation Quality and Audience Fit (LLM)", "description": "Holistic quality checks: visual hierarchy, readability at poster distance, tone for mixed audience, and pharmacist-forward messaging.", "is_required": false, "max_points": 6.5, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Visual Design and Readability", "description": "Evaluate layout, visual hierarchy, spacing, color/contrast, and readability at typical poster-viewing distance (3\u20136 feet).", "weight": 2.0, "judge_prompt": "Evaluate visual design quality:\n- Clear hierarchy: prominent title, section headers, logical flow (2\u20133 columns), consistent typography.\n- Readability: legible body text and headers at poster distance (3\u20136 ft); avoids dense wall-of-text.\n- Visual clarity: adequate white space, alignment, and color/contrast sufficient for accessibility.\n\nScoring (0\u20132.0): 2.0 excellent, 1.0 adequate with minor issues, 0.0 poor/overcrowded/low contrast.", "expectation": "Professional, readable poster with strong hierarchy and adequate contrast/spacing."}, {"type": "llm_judge", "name": "Clarity for Mixed Audience", "description": "Tone and content balance for laypersons and healthcare professionals; avoids jargon or explains it; actionable takeaways.", "weight": 1.5, "judge_prompt": "Judge suitability for a mixed audience:\n- Avoids unexplained jargon (or defines it briefly).\n- Balances lay explanations with concise professional cues.\n- Provides actionable takeaways for the public and practical notes for clinicians.\n\nScoring (0\u20131.5): 1.5 strong balance, 0.75 partial, 0.0 poor.", "expectation": "Approachable yet professional tone with clear, actionable points."}, {"type": "llm_judge", "name": "Pharmacist Role Emphasis and Calls to Action", "description": "Assesses whether the poster highlights pharmacist accessibility, triage, product selection guidance, and follow-up support.", "weight": 1.5, "judge_prompt": "Check for pharmacist-forward elements:\n- A visible callout like \u201cTalk to your pharmacist\u201d or a section on the pharmacist\u2019s role.\n- Concrete ways a pharmacist helps (product selection, technique coaching, monitoring, when to refer).\n- A clear call to action (e.g., ask us about treatment options; follow-up timing).\n\nScoring (0\u20131.5): 1.5 present and clear; 0.75 partial; 0.0 minimal/absent.", "expectation": "Pharmacist\u2019s role is visible with clear calls to action."}, {"type": "llm_judge", "name": "Reference Quality and Attribution", "description": "Assess whether cited sources appear credible (textbooks, peer-reviewed articles, OTC product websites) and are properly attributed.", "weight": 1.0, "judge_prompt": "Evaluate references for quality and attribution:\n- Mix of credible sources (textbooks, peer-reviewed articles, reputable OTC product sites).\n- Sufficient bibliographic detail (authors/titles/years/URLs/DOIs) to allow lookup.\n\nScoring (0\u20131.0): 1.0 credible and properly attributed; 0.5 somewhat credible but sparse; 0.0 poor/unclear.", "expectation": "Credible, traceable sources with adequate detail."}, {"type": "llm_judge", "name": "Editing and Professional Polish", "description": "Grammar, consistency, and professional presentation polish.", "weight": 0.5, "judge_prompt": "Assess professionalism: grammar, spelling, consistent terminology/capitalization, tidy alignment and labeling.\n\nScoring (0\u20130.5): 0.5 excellent polish; 0.25 minor issues; 0.0 noticeable errors.", "expectation": "Well-edited, professional presentation."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "9e39df84-ac57-4c9b-a2e3-12b8abf2c797", "rubric": {"category_name": "Manufacturing Dashboard Output (Supervisor)", "rationale": "Pattern A (Analytical/data model in Excel). Output is an Excel workbook with two worksheets: a structured fact table and a dashboard. Stage 1 uses an LLM judge as a strict gate to enforce the exact, verifiable shape so later checks are trivial. Stage 2 mixes code (deterministic integrity/calculation checks on the data sheet) with an LLM spot-check for dashboard interactivity. Stage 3 uses an LLM judge for professional quality and usability. The rubric forces the agent to prove correctness via structure, formulas, and visible artifacts.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Structured Output Gate (Excel Shape Enforcement)", "description": "LLM-only gate verifying the workbook is an Excel file with EXACT sheets and required structure/components so later code checks can target known places.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Workbook, Sheets, and Required Structures Present", "description": "Check the candidate output is an Excel workbook named as specified and contains the two mandated sheets with the exact structural elements needed for verification.", "weight": 8.0, "judge_prompt": "You are the Stage 1 gate. Inspect the candidate output. Only evaluate structure/presence, not calculation correctness.\n\nConfirm ALL of the following shape requirements:\n\nA) File Format\n- It must be an Excel workbook (.xlsx). Not PDF/CSV/DOCX.\n\nB) Sheet Names (exact)\n- Must contain two worksheets named exactly:\n  1) \"Operator Output Data\"\n  2) \"Dashboard\"\n\nC) Sheet: \"Operator Output Data\" (single, structured fact table)\n- Visible tabular structure with these columns (flexible headers allowed if obviously equivalent):\n  \u2022 Week # (Week 1 to Week 48 present in the table)\n  \u2022 Operator (Operators 1..9; e.g., \"Operator 1\" \u2026 \"Operator 9\")\n  \u2022 Machine Line (Machine 1, Machine 2, Machine 3)\n  \u2022 Shift (Day or Night)\n  \u2022 Daily Output columns for Monday, Tuesday, Wednesday, Thursday, Friday (five separate columns)\n  \u2022 Average Output (weekly average; a dedicated column)\n  \u2022 Total Output (weekly total; a dedicated column)\n- Conditional formatting is applied to Total Output and Average Output columns to visually highlight top and bottom performers (e.g., color scales, icon sets, or top/bottom rules).\n- Data for Week 1 is populated (non-empty daily outputs for all nine operators in Week 1 rows).\n- Each operator is assigned to the SAME machine and SAME shift across all 48 weeks (you only need to visually verify consistency is evident or stated).\n\nD) Sheet: \"Dashboard\"\n- Includes PivotTables (or equivalent pivoted summaries) providing insights for a selected week or range of weeks for:\n  (a) operator performance/output,\n  (b) total machine output,\n  (c) average day vs. night shift output,\n  (d) a YTD \"leaderboard\" of total output per operator.\n- Includes a data validation drop-down (or equivalent validated lists/selectors) to choose a specific week or a range of weeks; clearly visible selector controls near the pivots.\n- Includes FOUR charts arranged in a 2x2 quadrant layout, based on Week 1 data:\n  \u2022 Bar chart: individual operator total output for the week\n  \u2022 Pie chart: each machine's total output for the week\n  \u2022 Pie chart: average output by shift (day vs. night) for the week\n  \u2022 Bar chart: YTD total output per operator\n- Includes a clearly labeled KPI summary table for Week 1 with:\n  (a) total units produced,\n  (b) top performing operator and top machine with their totals,\n  (c) average output per operator (units),\n  (d) day shift contribution % of total,\n  (e) night shift contribution % of total.\n\nScoring (return a single numeric score out of 8 based ONLY on structure):\n- 8.0: Excel workbook with BOTH exact sheets and ALL required components in C and D present and clearly visible.\n- 6.0\u20137.5: Minor omissions (e.g., missing one chart OR one pivot OR week selection handles single week only) but both sheets exist, the fact table has all required columns, conditional formatting is present, and most dashboard components are present.\n- 4.0\u20135.5: Major omissions (e.g., missing 2\u20133 required dashboard components, incomplete fact table columns, or no visible week selector) but it is an Excel file and both required sheets exist with a visible data table on \"Operator Output Data\".\n- 2.0\u20133.5: Excel file present but one required sheet is missing OR the data table lacks multiple required columns (e.g., missing daily columns or totals/averages) OR no dashboard structures.\n- 0.0: Not an Excel workbook OR neither required sheet present.\n\nDo not check numeric correctness or pivot interactivity\u2014only presence and structure.", "expectation": "A valid .xlsx with the two exact sheets, a fully structured data table (Week/Operator/Machine/Shift/Mon\u2013Fri/Average/Total with conditional formatting), and a dashboard sheet with the specified pivots, selector(s), four charts, and KPI table."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Data Integrity", "description": "Deterministic checks on the structured data and light LLM consistency checks on dashboard interactivity. Focus on correctness, not presentation.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 4.0, "rules": [{"type": "code", "name": "Operator Data Structure and Integrity", "description": "Verify the fact table exists with flexible column matching, complete week coverage (1..48), 9 operators (Operator 1..9), and consistent machine/shift assignment per operator across all weeks.", "weight": 3.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        # Locate sheet names\n        xls_path = context.files.get_path(output.id)\n        try:\n            xfile = pd.ExcelFile(xls_path)\n            sheets = xfile.sheet_names\n        except Exception as e:\n            return 0.0, f\"Unable to open Excel file: {e}\"\n        # Find required sheets (case-insensitive)\n        def find_sheet(target):\n            for s in sheets:\n                if s.strip().lower() == target.strip().lower():\n                    return s\n            return None\n        op_sheet = find_sheet(\"Operator Output Data\")\n        if not op_sheet:\n            # Be slightly flexible in Stage 2, though Stage 1 should enforce exact name\n            candidates = [s for s in sheets if all(k in s.lower() for k in [\"operator\",\"output\"]) and \"dashboard\" not in s.lower()]\n            op_sheet = candidates[0] if candidates else None\n        if not op_sheet:\n            return 0.0, \"Operator Output Data sheet not found.\"\n        # Read operator sheet\n        try:\n            df = pd.read_excel(xls_path, sheet_name=op_sheet)\n        except Exception as e:\n            return 0.0, f\"Failed reading sheet '{op_sheet}': {e}\"\n        if df.empty:\n            return 0.0, \"Operator data sheet is empty.\"\n        # Normalize column names\n        df.columns = [str(c).strip() for c in df.columns]\n        low_cols = [c.lower() for c in df.columns]\n        # Helper to find a column by keywords\n        def find_col(keywords):\n            for i, lc in enumerate(low_cols):\n                for k in keywords:\n                    if k in lc:\n                        return df.columns[i]\n            return None\n        week_col = find_col([\"week\"])\n        op_col = find_col([\"operator\"])\n        mach_col = find_col([\"machine\", \"line\"])\n        shift_col = find_col([\"shift\"])\n        mon_col = find_col([\"mon\"]) \n        tue_col = find_col([\"tue\"]) \n        wed_col = find_col([\"wed\"]) \n        thu_col = find_col([\"thu\",\"thur\"]) \n        fri_col = find_col([\"fri\"]) \n        avg_col = find_col([\"average\",\"avg\"])\n        tot_col = find_col([\"total\"]) \n        required = [week_col, op_col, mach_col, shift_col, mon_col, tue_col, wed_col, thu_col, fri_col, avg_col, tot_col]\n        score = 0.0\n        details = []\n        # 1) Required columns present\n        if all(required):\n            score += 1.2\n        else:\n            missing = []\n            names = {\n                'week': week_col,'operator': op_col,'machine/line': mach_col,'shift': shift_col,\n                'mon': mon_col,'tue': tue_col,'wed': wed_col,'thu': thu_col,'fri': fri_col,\n                'average': avg_col,'total': tot_col\n            }\n            for k,v in names.items():\n                if v is None:\n                    missing.append(k)\n            details.append(f\"Missing columns: {', '.join(missing)}\")\n        if not all(required):\n            return max(0.0, score), \"; \".join(details) if details else \"Missing required columns\"\n        # Coerce types\n        df_use = df[[week_col, op_col, mach_col, shift_col, mon_col, tue_col, wed_col, thu_col, fri_col, avg_col, tot_col]].copy()\n        # Drop obvious header or footer rows\n        df_use = df_use.dropna(how='all')\n        # Standardize operator strings\n        df_use[op_col] = df_use[op_col].astype(str).str.strip()\n        # 2) Weeks coverage 1..48\n        weeks = pd.to_numeric(df_use[week_col], errors='coerce').dropna().astype(int)\n        unique_weeks = set(weeks.unique().tolist())\n        full_weeks = set(range(1,49))\n        if full_weeks.issubset(unique_weeks):\n            score += 0.8\n        else:\n            missing_weeks = sorted(list(full_weeks - unique_weeks))\n            details.append(f\"Missing weeks: {missing_weeks[:10]}{'...' if len(missing_weeks)>10 else ''}\")\n        # 3) Exactly 9 operators (Operator 1..9) present\n        ops = set(df_use[op_col].dropna().astype(str).str.strip())\n        expected_ops = {f\"Operator {i}\" for i in range(1,10)}\n        if expected_ops.issubset({o.title() for o in ops}):\n            score += 0.5\n        else:\n            details.append(\"Operator set does not include all 'Operator 1'..'Operator 9'.\")\n        # 4) Consistent assignment per operator across all weeks\n        # Keep only rows with valid week numbers\n        valid = df_use[weeks.index]\n        # Normalize machine and shift values\n        valid[mach_col] = valid[mach_col].astype(str).str.strip()\n        valid[shift_col] = valid[shift_col].astype(str).str.strip().str.title()\n        consistent_ok = True\n        inconsistent_ops = []\n        for op_name, g in valid.groupby(op_col):\n            m_unique = set(g[mach_col].dropna().astype(str).str.strip())\n            s_unique = set(g[shift_col].dropna().astype(str).str.title())\n            if len(m_unique) > 1 or len(s_unique) > 1:\n                consistent_ok = False\n                inconsistent_ops.append(op_name)\n        if consistent_ok:\n            score += 0.5\n        else:\n            details.append(f\"Inconsistent machine/shift assignment for: {inconsistent_ops[:5]}{'...' if len(inconsistent_ops)>5 else ''}\")\n        # Cap score at weight 3.0\n        score = min(score, 3.0)\n        return score, \"; \".join(details) if details else \"OK\"\n    except Exception as e:\n        return 0.0, f\"Rule error: {e}\""}, {"type": "code", "name": "Week 1 Calculations Correct (Totals and Averages)", "description": "For Week 1 rows, check Total Output equals sum(Mon..Fri) and Average Output equals Total/5 within a small tolerance. Also verify day+night totals match overall Week 1 total.", "weight": 3.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        xls_path = context.files.get_path(output.id)\n        try:\n            xfile = pd.ExcelFile(xls_path)\n            sheets = xfile.sheet_names\n        except Exception as e:\n            return 0.0, f\"Unable to open Excel file: {e}\"\n        # Find operator sheet\n        def find_sheet(target):\n            for s in sheets:\n                if s.strip().lower() == target.strip().lower():\n                    return s\n            return None\n        op_sheet = find_sheet(\"Operator Output Data\")\n        if not op_sheet:\n            candidates = [s for s in sheets if all(k in s.lower() for k in [\"operator\",\"output\"]) and \"dashboard\" not in s.lower()]\n            op_sheet = candidates[0] if candidates else None\n        if not op_sheet:\n            return 0.0, \"Operator Output Data sheet not found.\"\n        df = pd.read_excel(xls_path, sheet_name=op_sheet)\n        if df.empty:\n            return 0.0, \"Operator data sheet is empty.\"\n        df.columns = [str(c).strip() for c in df.columns]\n        low_cols = [c.lower() for c in df.columns]\n        def find_col(keywords):\n            for i, lc in enumerate(low_cols):\n                for k in keywords:\n                    if k in lc:\n                        return df.columns[i]\n            return None\n        week_col = find_col([\"week\"])\n        mon_col = find_col([\"mon\"]) \n        tue_col = find_col([\"tue\"]) \n        wed_col = find_col([\"wed\"]) \n        thu_col = find_col([\"thu\",\"thur\"]) \n        fri_col = find_col([\"fri\"]) \n        avg_col = find_col([\"average\",\"avg\"])\n        tot_col = find_col([\"total\"]) \n        shift_col = find_col([\"shift\"]) \n        required = [week_col, mon_col, tue_col, wed_col, thu_col, fri_col, avg_col, tot_col]\n        if not all(required):\n            return 0.0, \"Missing required columns for calculation checks.\"\n        # Filter Week 1\n        dfw = df.copy()\n        dfw[week_col] = pd.to_numeric(dfw[week_col], errors='coerce')\n        w1 = dfw[dfw[week_col] == 1].copy()\n        if w1.empty:\n            return 0.0, \"No Week 1 rows found.\"\n        # Coerce numerics\n        for c in [mon_col, tue_col, wed_col, thu_col, fri_col, avg_col, tot_col]:\n            w1[c] = pd.to_numeric(w1[c], errors='coerce')\n        # Drop rows without required numeric data\n        req_cols = [mon_col, tue_col, wed_col, thu_col, fri_col, avg_col, tot_col]\n        w1n = w1.dropna(subset=req_cols)\n        if w1n.empty:\n            return 0.0, \"Week 1 rows lack numeric data.\"\n        # Compute\n        tol = 1e-6\n        calc_total = w1n[[mon_col, tue_col, wed_col, thu_col, fri_col]].sum(axis=1)\n        calc_avg = calc_total / 5.0\n        total_match = (np.abs(calc_total.values - w1n[tot_col].values) <= tol)\n        avg_match = (np.abs(calc_avg.values - w1n[avg_col].values) <= tol)\n        both_match = total_match & avg_match\n        if len(both_match) == 0:\n            return 0.0, \"No valid rows to compare.\"\n        match_rate = both_match.mean()\n        # Shift reconciliation (optional +0.5)\n        shift_score = 0.0\n        if shift_col is not None:\n            w1n[shift_col] = w1n[shift_col].astype(str).str.strip().str.title()\n            sum_by_shift = w1n.groupby(shift_col)[[mon_col, tue_col, wed_col, thu_col, fri_col]].sum().sum(axis=1)\n            total_all = calc_total.sum()\n            if total_all == 0:\n                shift_score = 0.0\n            else:\n                if np.isclose(sum_by_shift.sum(), total_all, rtol=0, atol=1e-6):\n                    shift_score = 0.5\n        # Score: up to 2.5 for row checks + 0.5 for shift reconciliation\n        score = min(2.5, 2.5 * match_rate) + shift_score\n        score = min(score, 3.0)\n        feedback = f\"Week1 match rate: {match_rate:.2%}; Shift check: +{shift_score:.1f}\"\n        return score, feedback\n    except Exception as e:\n        return 0.0, f\"Rule error: {e}\""}, {"type": "code", "name": "Output Bounds Sanity (Week 1)", "description": "Check Week 1 daily outputs and totals are non-negative and within generous plausible bounds to catch obvious errors.", "weight": 1.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        xls_path = context.files.get_path(output.id)\n        try:\n            xfile = pd.ExcelFile(xls_path)\n            sheets = xfile.sheet_names\n        except Exception as e:\n            return 0.0, f\"Unable to open Excel file: {e}\"\n        # Find operator sheet\n        def find_sheet(target):\n            for s in sheets:\n                if s.strip().lower() == target.strip().lower():\n                    return s\n            return None\n        op_sheet = find_sheet(\"Operator Output Data\")\n        if not op_sheet:\n            candidates = [s for s in sheets if all(k in s.lower() for k in [\"operator\",\"output\"]) and \"dashboard\" not in s.lower()]\n            op_sheet = candidates[0] if candidates else None\n        if not op_sheet:\n            return 0.0, \"Operator Output Data sheet not found.\"\n        df = pd.read_excel(xls_path, sheet_name=op_sheet)\n        if df.empty:\n            return 0.0, \"Operator data sheet is empty.\"\n        df.columns = [str(c).strip() for c in df.columns]\n        low_cols = [c.lower() for c in df.columns]\n        def find_col(keywords):\n            for i, lc in enumerate(low_cols):\n                for k in keywords:\n                    if k in lc:\n                        return df.columns[i]\n            return None\n        week_col = find_col([\"week\"])\n        mon_col = find_col([\"mon\"]) \n        tue_col = find_col([\"tue\"]) \n        wed_col = find_col([\"wed\"]) \n        thu_col = find_col([\"thu\",\"thur\"]) \n        fri_col = find_col([\"fri\"]) \n        tot_col = find_col([\"total\"]) \n        required = [week_col, mon_col, tue_col, wed_col, thu_col, fri_col, tot_col]\n        if not all(required):\n            return 0.0, \"Missing required columns for bounds checks.\"\n        df[week_col] = pd.to_numeric(df[week_col], errors='coerce')\n        w1 = df[df[week_col] == 1].copy()\n        if w1.empty:\n            return 0.0, \"No Week 1 rows found.\"\n        for c in [mon_col, tue_col, wed_col, thu_col, fri_col, tot_col]:\n            w1[c] = pd.to_numeric(w1[c], errors='coerce')\n        w1 = w1.dropna(subset=[mon_col, tue_col, wed_col, thu_col, fri_col, tot_col])\n        if w1.empty:\n            return 0.0, \"No numeric Week 1 rows for bounds checks.\"\n        # Generous plausible bounds for outputs\n        daily_ok = ((w1[[mon_col, tue_col, wed_col, thu_col, fri_col]] >= 0) & (w1[[mon_col, tue_col, wed_col, thu_col, fri_col]] <= 100000)).all(axis=None)\n        total_ok = ((w1[tot_col] >= 0) & (w1[tot_col] <= 500000)).all()\n        # Partial credit: proportion of valid rows\n        mat = w1[[mon_col, tue_col, wed_col, thu_col, fri_col]].values\n        daily_valid_rows = ((mat >= 0) & (mat <= 100000)).all(axis=1)\n        total_valid_rows = ((w1[tot_col] >= 0) & (w1[tot_col] <= 500000)).values\n        combined_valid = daily_valid_rows & total_valid_rows\n        if len(combined_valid) == 0:\n            return 0.0, \"No valid rows to test.\"\n        frac = combined_valid.mean()\n        score = min(1.0, frac * 1.0)\n        return score, f\"Week1 rows within bounds: {frac:.2%}\"\n    except Exception as e:\n        return 0.0, f\"Rule error: {e}\""}, {"type": "llm_judge", "name": "Dashboard Functionality and Interactivity (LLM Spot Check)", "description": "Check that dashboard pivots/charts reference the Operator Output Data table and respond to the week selector(s), and that Week 1 visuals/KPIs align in a self-consistent way.", "weight": 1.0, "judge_prompt": "Review the Excel dashboard sheet for functional coherence and interactivity. You are not recalculating exact numbers; instead, verify visible linkages and self-consistency.\n\nPass criteria (score up to 1.0):\n- PivotTables and the four charts appear to be sourced from the same underlying dataset as the \"Operator Output Data\" sheet (e.g., field names align, values match general magnitudes, labels correspond to operators/machines/shifts).\n- The week selector(s) (data validation drop-downs or similar) clearly control the pivots/charts (e.g., instructions or linked slicers/filters; changing selection would logically update all pivots).\n- The Week 1 KPI summary values are self-consistent with the Week 1 charts/pivots (totals match between the KPI table and the operator/machine summaries; the top operator in the KPI matches the bar chart leader; day/night shares roughly match the shift chart).\n\nScoring guidance:\n- 1.0: Clear linkage to the data table AND week selector controls all pivots/charts AND KPIs/charts are mutually consistent for Week 1.\n- 0.5: Evident linkage to data and mostly consistent visuals, but selector control is unclear or only some charts/pivots seem connected.\n- 0.0: Charts/pivots appear disconnected from the data, selector absent/unclear, or KPIs conflict with charts.\n", "expectation": "Dashboard pivots/charts visibly tied to the Operator Output Data table, week selector(s) affecting all, and Week 1 KPIs consistent with visual summaries."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Presentation and Usability Quality", "description": "Professionalism, clarity, and meeting-room readiness of the dashboard.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Usability", "description": "Assess visual clarity, labeling, and usability for weekly meetings.", "weight": 4.0, "judge_prompt": "Evaluate the workbook\u2019s professional quality and usability for a weekly production meeting. Do not re-check structure beyond how it affects presentation.\n\nConsider:\n- Clarity: sensible sheet names, section headers, legends, axis titles, number formats (thousand separators, no excessive decimals), readable fonts, adequate contrast.\n- Layout: four charts arranged cleanly in quadrants with consistent sizing; KPI table is prominent and labeled; pivot areas are tidy and aligned; gridlines/freeze panes where helpful.\n- Guidance: brief on-sheet instructions for using the week selector(s) and refreshing pivots; descriptive titles on charts and tables.\n- Consistency: operators/machines/shift labels match across sheets; conditional formatting helps interpretation without clutter.\n- Print/meeting readiness: fits on screen/print area; no clipped elements; avoids unnecessary scroll; optional sheet protection to prevent accidental edits.\n\nScoring (0\u20134):\n- 4: Highly professional, polished, and easy to present; excellent labeling, layout, and usability.\n- 3: Solid and professional with minor issues (spacing, small labels, minor formatting inconsistencies).\n- 2: Adequate but several distracting issues (crowded charts, unclear labels, inconsistent formats).\n- 1: Hard to follow; poor labeling/layout or confusing styles.\n- 0: Unprofessional or very difficult to use.\n", "expectation": "Clean, labeled, and readable dashboard with clear instructions, consistent formatting, and meeting-ready layout."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "6a900a40-8d2b-4064-a5b1-13a60bc173d8", "rubric": {"category_name": "Updated Quotation with Transport Options (Wholesale Medical Kits)", "rationale": "This rubric enforces a self-documenting, verifiable Excel quotation for a confirmed 400-kit order with three transport options. Stage 1 (LLM) mandates a strict, reviewable structure in the spreadsheet so that Stage 2 code checks can reliably verify arithmetic consistency and required statements. Stage 3 (LLM) assesses professional quality, appropriateness for an NGO client, and risk communication.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Structured Quotation Format Gate (LLM-only)", "description": "Gate: Verify the candidate produced a properly structured Excel quotation with mandated sections enabling verification.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Structured Excel Quotation Requirements", "description": "Check that the output is a single Excel spreadsheet with the required structure and elements for this NGO quotation update.", "weight": 8.0, "judge_prompt": "You are evaluating whether the candidate produced a single, well-structured Excel quotation for an NGO client. Only check the PRESENCE/STRUCTURE of elements, not the correctness of numbers. Be flexible with exact phrasing but strict about required sections being present and visibly labeled. If the file is not an Excel spreadsheet, score 0.\n\nFile/Format Requirements:\n- Must be a valid Excel workbook (.xlsx). One primary file only.\n- Preferably saved as: Q9749821-revised_including_transport.xlsx (naming is a bonus; don\u2019t fail solely for naming).\n\nWorkbook Structure (flexible sheet names like \u201cQuotation\u201d, \u201cOffer\u201d, \u201cProposal\u201d are acceptable):\n1) Header/Identity block on the main sheet:\n   - Supplier: Danish Wholesale & Co.\n   - Client: Health NGO\n   - Quote reference: Q9749821 (or clearly linked to original reference)\n   - Date/validity area\n2) Itemized quotation section:\n   - Includes the sterilization kit line (Sterilization C kits or equivalent description)\n   - Quantity: 400 kits\n   - Unit price column visible\n   - Line total and a clearly labeled Total EXW (e.g., \u201cTotal EXW\u201d, \u201cEXW Total\u201d) for the whole order\n   - Delivery/lead time shown (from internal reference)\n   - Weight/volume for shipment: 5,500 kg and 7.1 cbm present somewhere on the sheet\n3) Transport Options block placed directly below or immediately after the Total EXW area:\n   - A labeled section header such as \u201cTransport Options\u201d / \u201cFreight Options\u201d\n   - Exactly three options present: Airfreight, Seafreight/Ocean, and Road Freight\n   - Each option indicates the forwarder:\n       \u2022 Air: Euro Air Cargo\n       \u2022 Sea: Red Ocean Shipping or Red Water Shipping (accept either name)\n       \u2022 Road: Euro Road Logistics Co.\n   - Table-like structure including columns/cells for: Mode, Forwarder, Transit Time, Notes/Reason (or Item Remarks), Freight Cost, and Grand Total (EXW + Freight)\n   - For each option, there is a visible Grand Total (EXW + freight) amount\n   - For each option, the Item Remarks/Notes include a transit time and a short reasoning about suitability\n   - The Road option is clearly flagged for potential delays/disruptions related to border zones\n4) General Remarks / Terms section:\n   - Contains a statement in red font that freight rates are subject to change, have limited validity (14\u201330 days), and are subject to reconfirmation at time of final order\n   - A note that cold chain packaging is not required (acceptable anywhere in the quote if clearly stated)\n\nScoring Guide:\n- 8.0: All four structural blocks (Header, Itemized quotation with Total EXW, Transport Options with all fields, General Remarks with required red-font statement) are present, clearly labeled, and placed as specified (Transport Options immediately below Total EXW). Forwarders and three modes are correctly listed. Freight and Grand Total columns exist for each option. Remarks per option include transit time and brief suitability note. Road risk is explicitly flagged.\n- 6.5\u20137.5: All major blocks present but with minor issues (e.g., small naming deviations, slight placement variance, or one missing minor field like forwarder name on one option). Red font statement present with all three clauses.\n- 5.0\u20136.0: Most core blocks present, but 1\u20132 important elements missing (e.g., Transport Options present but not immediately below Total EXW, or one mode missing its Grand Total cell, or remarks missing on one option). Red-font statement present but missing one clause.\n- 2.0\u20134.5: Valid Excel with partial structure; multiple key elements missing (e.g., only two transport modes, no Grand Total cells, or no clear Total EXW). Red-font statement missing or not clearly visible.\n- 0.0\u20131.5: Not an Excel file OR structure largely missing (no Transport Options, no Total EXW, etc.).\n\nOnly assess structure and presence/placement. Do NOT check math or content quality.", "expectation": "A clean, single Excel workbook with a main quotation sheet that includes identity, itemized 400-kit EXW total, a clearly positioned Transport Options table with three modes and forwarders, per-option remarks, and a red-font general remark about freight rate change, limited validity (14\u201330 days), and reconfirmation."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Consistency Verification (Code + LLM-amenable)", "description": "Now verify key correctness aspects enabled by the structured shape: arithmetic consistency, presence of required statements, and critical risk flags.", "is_required": false, "max_points": 7.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Spreadsheet Type + Key Labels Presence", "description": "Verify primary output is an Excel spreadsheet and includes key transport mode labels and EXW terminology somewhere in the workbook.", "weight": 1.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output file.\"\n    if not output.is_spreadsheet:\n        return 0.0, \"Primary output is not a spreadsheet.\"\n\n    try:\n        path = context.files.get_path(output.id)\n        xl = pd.ExcelFile(path)\n    except Exception as e:\n        return 0.0, f\"Failed to open Excel: {e}\"\n\n    texts = []\n    try:\n        for sheet in xl.sheet_names:\n            try:\n                df = pd.read_excel(path, sheet_name=sheet, header=None, dtype=object)\n            except Exception:\n                df = pd.read_excel(path, sheet_name=sheet, header=None)\n            for col in df.columns:\n                for val in df[col].astype(str).fillna(\"\"):\n                    texts.append(str(val))\n    except Exception as e:\n        return 0.3, f\"Opened Excel but failed to read all cells: {e}\"\n\n    text_all = \"\\n\".join(texts).lower()\n\n    score = 0.0\n    feedback = []\n\n    # Check EXW label presence\n    if (\"total exw\" in text_all) or (\"exw total\" in text_all) or (\"exw\" in text_all):\n        score += 0.3\n    else:\n        feedback.append(\"EXW terminology not found.\")\n\n    # Transport mode presence (flexible synonyms)\n    air_ok = any(k in text_all for k in [\"airfreight\",\"air freight\",\"air freight\",\"air\"]) \n    sea_ok = any(k in text_all for k in [\"seafreight\",\"sea freight\",\"ocean freight\",\"ocean\",\"sea\"]) \n    road_ok = any(k in text_all for k in [\"road freight\",\"road\",\"truck\"]) \n\n    mode_hits = sum([air_ok, sea_ok, road_ok])\n    if mode_hits >= 3:\n        score += 0.7\n    else:\n        feedback.append(f\"Transport modes missing/unclear: air={air_ok}, sea={sea_ok}, road={road_ok}\")\n        score += 0.2 * mode_hits\n\n    score = min(score, 1.0)\n    return score, \"; \".join(feedback) if feedback else \"OK\""}, {"type": "code", "name": "Shipment Metrics and Quantity Presence", "description": "Detect presence of core metrics: 400 kits, 5,500 kg, 7.1 cbm (flexible formats).", "weight": 1.0, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    path = context.files.get_path(output.id)\n    try:\n        xl = pd.ExcelFile(path)\n    except Exception:\n        return 0.0\n\n    texts = []\n    for sheet in xl.sheet_names:\n        df = pd.read_excel(path, sheet_name=sheet, header=None, dtype=object)\n        for col in df.columns:\n            texts.extend([str(v) for v in df[col].fillna(\"\")])\n    text = \"\\n\".join(texts).lower()\n\n    score = 0.0\n    # Quantity 400 kits\n    if re.search(r\"\\b400\\b\", text):\n        score += 0.35\n    # Weight ~ 5500 kg (accept 5 500 / 5,500 / 5500)\n    if re.search(r\"5[\\s,\\.]?500\\s*(kg|kilogram|kilos)?\", text):\n        score += 0.325\n    # Volume ~ 7.1 cbm (accept 7,1)\n    if re.search(r\"7[\\.,]1\\s*(cbm|m3|cubic)\", text):\n        score += 0.325\n\n    return min(score, 1.0)"}, {"type": "code", "name": "General Remark Clause Presence", "description": "Check the presence of the required clauses in the General Remark: subject to change, limited validity (14\u201330 days), reconfirmation at final order.", "weight": 1.5, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    path = context.files.get_path(output.id)\n    try:\n        xl = pd.ExcelFile(path)\n    except Exception:\n        return 0.0\n\n    texts = []\n    for sheet in xl.sheet_names:\n        df = pd.read_excel(path, sheet_name=sheet, header=None, dtype=object)\n        for col in df.columns:\n            texts.extend([str(v) for v in df[col].fillna(\"\")])\n    t = \"\\n\".join(texts).lower()\n\n    score = 0.0\n    # Subject to change\n    if (\"subject to change\" in t) and (\"freight\" in t or \"rate\" in t or \"freight rates\" in t):\n        score += 0.5\n    # Limited validity window mention (14\u201330 days)\n    validity_hit = False\n    if (\"valid\" in t or \"validity\" in t) and (\"day\" in t):\n        if re.search(r\"\\b14\\b\", t) or re.search(r\"\\b30\\b\", t) or re.search(r\"14\\s*[-\u2013to]{1,3}\\s*30\", t):\n            validity_hit = True\n    if validity_hit:\n        score += 0.5\n    # Reconfirmation at time of final order\n    if (\"reconfirm\" in t or \"reconfirmation\" in t) and (\"final order\" in t or \"time of final order\" in t or \"at final order\" in t):\n        score += 0.5\n\n    return min(score, 1.5)"}, {"type": "code", "name": "Transit Time in Option Remarks", "description": "Verify that each transport option mentions a transit time (e.g., 7 days, 2 weeks) near the option row.", "weight": 1.0, "code": "import re\nimport pandas as pd\n\ndef _collect_text_rows(path):\n    xl = pd.ExcelFile(path)\n    rows = []\n    for sheet in xl.sheet_names:\n        df = pd.read_excel(path, sheet_name=sheet, header=None, dtype=object)\n        for i in range(len(df)):\n            row_vals = [str(v).lower() for v in list(df.iloc[i].fillna(\"\"))]\n            rows.append(\" \".join(row_vals))\n    return rows\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    path = context.files.get_path(output.id)\n\n    try:\n        rows = _collect_text_rows(path)\n    except Exception:\n        return 0.0\n\n    def has_time(s):\n        return re.search(r\"\\b\\d+\\s*(day|days|week|weeks)\\b\", s) is not None\n\n    air_hit = False\n    sea_hit = False\n    road_hit = False\n\n    for idx, r in enumerate(rows):\n        if any(k in r for k in [\"airfreight\",\"air freight\",\"air\"]):\n            window = \" \".join(rows[idx: idx+2])\n            if has_time(window):\n                air_hit = True\n        if any(k in r for k in [\"seafreight\",\"sea freight\",\"ocean\",\"sea\"]):\n            window = \" \".join(rows[idx: idx+2])\n            if has_time(window):\n                sea_hit = True\n        if any(k in r for k in [\"road freight\",\"road\",\"truck\"]):\n            window = \" \".join(rows[idx: idx+2])\n            if has_time(window):\n                road_hit = True\n\n    hits = sum([air_hit, sea_hit, road_hit])\n    if hits == 3:\n        return 1.0\n    elif hits == 2:\n        return 0.7\n    elif hits == 1:\n        return 0.35\n    return 0.0"}, {"type": "code", "name": "Road Option Risk Flag", "description": "Check that road freight option is flagged for border delays/disruptions/risks.", "weight": 1.0, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    path = context.files.get_path(output.id)\n    xl = pd.ExcelFile(path)\n    rows = []\n    for sheet in xl.sheet_names:\n        df = pd.read_excel(path, sheet_name=sheet, header=None, dtype=object)\n        for i in range(len(df)):\n            row = \" \".join([str(v).lower() for v in list(df.iloc[i].fillna(\"\"))])\n            rows.append(row)\n\n    flagged = False\n    for idx, r in enumerate(rows):\n        if any(k in r for k in [\"road freight\",\"road\",\"truck\"]):\n            window = \" \".join(rows[max(0, idx-1): idx+2])\n            if any(v in window for v in [\"border\",\"delay\",\"delays\",\"disruption\",\"disruptions\",\"risk\",\"risks\"]):\n                flagged = True\n                break\n\n    return 1.0 if flagged else 0.0"}, {"type": "code", "name": "Arithmetic Consistency: Grand Total \u2248 EXW + Freight (per option)", "description": "Heuristically verify for each transport option that a plausible EXW total exists and per-option Grand Total is approximately EXW + Freight.", "weight": 1.5, "code": "import re\nimport math\nimport pandas as pd\nimport numpy as np\n\n# Helpers\ndef _to_float(x):\n    if isinstance(x, (int, float, np.number)):\n        return float(x)\n    if isinstance(x, str):\n        s = x.strip().lower()\n        # remove currency symbols and thousand separators\n        s = re.sub(r\"[a-z$\u20ac\u00a3\u00a5]\", \"\", s)\n        s = s.replace(\",\", \"\").replace(\" \", \"\")\n        s = s.replace(\"--\", \"-\")\n        try:\n            return float(s)\n        except Exception:\n            return None\n    return None\n\ndef _get_all_sheets(path):\n    xl = pd.ExcelFile(path)\n    sheets = {}\n    for sn in xl.sheet_names:\n        df = pd.read_excel(path, sheet_name=sn, header=None, dtype=object)\n        sheets[sn] = df\n    return sheets\n\ndef _find_exw_value(sheets):\n    # find row containing 'total exw' or similar; return largest numeric on that row\n    for sn, df in sheets.items():\n        for i in range(len(df)):\n            row_vals = [str(v).lower() for v in list(df.iloc[i].fillna(\"\"))]\n            row_text = \" \".join(row_vals)\n            if (\"total exw\" in row_text) or (\"exw total\" in row_text) or (re.search(r\"\\bexw\\b\", row_text)):\n                nums = []\n                for v in list(df.iloc[i]):\n                    fv = _to_float(v)\n                    if fv is not None and fv > 0:\n                        nums.append(fv)\n                if nums:\n                    return max(nums)\n    return None\n\ndef _find_option_rows(sheets):\n    # return dict mode -> list of (sn, idx)\n    keys = {\n        'air': [\"airfreight\",\"air freight\",\"air\"],\n        'sea': [\"seafreight\",\"sea freight\",\"ocean freight\",\"ocean\",\"sea\"],\n        'road': [\"road freight\",\"road\",\"truck\"],\n    }\n    out = {\"air\": [], \"sea\": [], \"road\": []}\n    for sn, df in sheets.items():\n        for i in range(len(df)):\n            row_text = \" \".join([str(v).lower() for v in list(df.iloc[i].fillna(\"\"))])\n            for mode, kws in keys.items():\n                if any(k in row_text for k in kws):\n                    out[mode].append((sn, i))\n    return out\n\ndef _row_numbers(df, i):\n    vals = []\n    for v in list(df.iloc[i]):\n        fv = _to_float(v)\n        if fv is not None:\n            vals.append(fv)\n    return vals\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"Not a spreadsheet.\"\n    path = context.files.get_path(output.id)\n\n    try:\n        sheets = _get_all_sheets(path)\n    except Exception as e:\n        return 0.0, f\"Failed to open sheets: {e}\"\n\n    exw = _find_exw_value(sheets)\n    if exw is None:\n        # Without EXW we can't verify arithmetic; partial at best\n        return 0.3, \"EXW not found heuristically.\"\n\n    option_rows = _find_option_rows(sheets)\n\n    def verify_mode(mode):\n        # Try first matching row for simplicity\n        rows = option_rows.get(mode, [])\n        for sn, i in rows:\n            df = sheets[sn]\n            nums = _row_numbers(df, i)\n            # Filter plausible currency numbers (exclude small times like 7 days)\n            money = [n for n in nums if abs(n) >= 100]\n            if len(money) < 1:\n                # try next row as numbers might be in adjacent row\n                if i+1 < len(df):\n                    money = [n for n in _row_numbers(df, i+1) if abs(n) >= 100]\n            if len(money) >= 1:\n                # Heuristic: assume grand total is the max, freight is the min < grand total\n                gt = max(money)\n                freight_candidates = [n for n in money if n < gt]\n                if not freight_candidates:\n                    continue\n                freight = min(freight_candidates)\n                # Check GT approx equals EXW + freight (1% tolerance or 1 currency unit)\n                expected = exw + freight\n                tol = max(1.0, 0.01 * gt)\n                if abs(gt - expected) <= tol:\n                    return True\n        return False\n\n    hits = sum(verify_mode(m) for m in [\"air\",\"sea\",\"road\"])\n\n    if hits == 3:\n        return 1.5, \"All 3 options arithmetically consistent.\"\n    elif hits == 2:\n        return 1.0, \"2 options consistent.\"\n    elif hits == 1:\n        return 0.5, \"1 option consistent.\"\n    return 0.0, \"No consistent options found.\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Client Fit (LLM)", "description": "Holistic quality review for presentation, clarity, NGO-appropriateness, and practical usefulness.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Client Fit", "description": "Assess overall quality, clarity, and appropriateness for an NGO client. Consider discount rationale, lead times vs. target delivery, and clarity of options.", "weight": 5.0, "judge_prompt": "Evaluate the overall quality of the Excel quotation for an NGO context. The structure should already exist (Stage 1). Now judge the professionalism and client usefulness of the content. Consider:\n\n- Clarity and readability: clean layout, consistent currency/units, clear sections, and professional tone\n- NGO-appropriateness: includes validity window, Incoterms (EXW) with labeled transport options, and concise risk explanations\n- Transport options: notes/remarks are practical and comparative (why each option is more/less suitable: cost, speed, risk), Road option risks clearly articulated\n- Lead time coherence: delivery/lead times align with ~2 months target (including transit) and stated as per internal reference\n- Volume-based pricing: evident unit price reflecting larger volume (a discount note or lower unit price vs. smaller quantity is acceptable if indicated; if prior quote is not visible, assess whether a discount rationale is stated)\n- Completeness: contact/signature block, page footers, quote reference, date; inclusion of shipment metrics (400 kits, 5,500 kg, 7.1 cbm); cold-chain not required noted\n\nScoring Guidance (5 max):\n- 4.5\u20135.0: Highly professional, easy to act on, compares options clearly with quantified trade-offs, risks clearly flagged (esp. road), realistic timelines for ~2 months, discount rationale explicit, polished presentation\n- 3.5\u20134.4: Solid and professional with minor issues (slight clarity gaps or minor omissions), but fully usable\n- 2.5\u20133.4: Adequate but with notable omissions (e.g., weak comparison notes or unclear timelines)\n- 1.0\u20132.4: Limited usability; confusing or missing key explanations\n- 0.0\u20130.9: Poor quality or not suitable for client use", "expectation": "A polished, NGO-ready quotation with clear comparison of air/sea/road trade-offs, realistic lead times, explicit risk flag for road, visible validity and reconfirmation, shipment metrics included, and professional presentation."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "a73fbc98-90d4-4134-a54f-2b1d0c838791", "rubric": {"category_name": "Spring Bazaar Layout and Vendor Assignment Plan", "rationale": "Mixed-output task: the agent must produce a verifiable Excel workbook (assignments and layout maps) and optionally a brief document explaining rationale. Stage 1 is a strict shape gate using an LLM judge to mandate a specific Excel structure that enables deterministic checks. Stage 2 uses code rules to verify assignments, preferences, power constraints, adjacency variety, and counts. Stage 3 assesses professional quality and usability for city staff, vendors, and shoppers.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Structured Output Gate (Excel Shape Enforcement)", "description": "Gate: Verify the candidate produced a structurally correct Excel workbook that enables verification. Do not judge correctness of the plan here\u2014only that the required sheets and sections exist with visible tables/columns.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Excel Workbook Requirement", "description": "Check that the output includes an Excel file with specific sheets and columns that enable verification of table assignments, power needs, location preferences, and adjacency variety.", "weight": 4.0, "judge_prompt": "You are checking ONLY the presence and structure of the deliverable, not correctness of assignments. Review the candidate outputs. Confirm there is an Excel (.xlsx) workbook with the following structure. Be flexible with sheet/column names (e.g., 'Vendor Assignments' vs 'Vendors', 'Table #' vs 'Table Number'), but the content/sections must be clearly present and readable (not hidden).\n\nRequired Workbook (Excel) with these sheets:\n1) Vendors (aka 'Vendor Assignments' or similar)\n   - Must be a tabular sheet listing vendors with columns that clearly correspond to:\n     \u2022 Business Name (or Vendor Name)\n     \u2022 Product Category (or Product/Description)\n     \u2022 Tables Purchased (quantity)\n     \u2022 Location Preference (Arena or Meeting Room)\n     \u2022 Electricity Needed (Yes/No)\n     \u2022 Additional Requests/Notes\n     \u2022 Assigned Table(s) (newly added column listing specific table IDs/numbers; multiple allowed)\n\n2) Arena Map (aka 'Arena Layout' or similar)\n   - A tabular listing of all arena tables with columns:\n     \u2022 Table Number/ID\n     \u2022 Electricity Available (Yes/No)\n     \u2022 Assigned Vendor (or left blank if unassigned)\n     \u2022 Adjacent Tables (a cell listing neighboring table IDs, e.g., comma-separated)\n\n3) Meeting Room Map (aka 'Meeting Room Layout' or similar)\n   - A tabular listing of all meeting room tables with columns:\n     \u2022 Table Number/ID\n     \u2022 Electricity Available (Yes/No)\n     \u2022 Assigned Vendor\n     \u2022 Adjacent Tables\n\n4) Constraints & Notes (can be named 'Methodology & Rules', 'Constraints Log', or similar)\n   - Must include BOTH of the following elements (can be in text blocks and/or small tables on the sheet):\n     \u2022 Power Outlets Summary (how many powered tables/outlets exist by area)\n     \u2022 Exceptions/Conflict Log (any intentional deviations, e.g., adjacency conflicts or unmet preferences)\n\nScoring (out of 4.0):\n- 4.0: Excel present AND all four sheets present with the listed column types/sections, including the Assigned Table(s) column on Vendors and the Adjacent Tables columns on both layout sheets.\n- 3.0: Excel present with Vendors + Arena Map + Meeting Room Map sheets correctly structured, but missing the Constraints & Notes sheet OR missing one of its two required elements.\n- 2.0: Excel present but missing one required layout sheet OR Vendors sheet lacks the Assigned Table(s) column.\n- 1.0: Excel present but only one of the required sheets is properly structured.\n- 0.0: No Excel workbook, or structure is not recognizable for this task.\n\nOnly check presence/format/structure. Do NOT evaluate if assignments, counts, or logic are correct.", "expectation": "A clearly structured Excel workbook enabling deterministic checks of assignments, power, locations, and adjacency."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification and Consistency Checks", "description": "Now that the shape is correct, verify correctness using deterministic rules. These checks validate assignments exist, tables are valid/unique, preferences honored, power needs met, adjacency variety observed, and counts reconcile.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Assignments Column Added and Populated", "description": "Verify Vendors sheet has an Assigned Table(s) column and that each vendor has the correct count of assigned tables matching Tables Purchased (flexible parsing).", "weight": 1.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    def find_spreadsheet():\n        # Prefer primary output if spreadsheet; else search all outputs\n        out = context.get_primary_output()\n        if out and getattr(out, 'is_spreadsheet', False):\n            return out\n        for r in context.get_all_outputs():\n            if getattr(r, 'is_spreadsheet', False):\n                return r\n        return None\n\n    def sheet_by_like(xls, likes):\n        names = [str(s) for s in xls.sheet_names]\n        lnames = [s.lower() for s in names]\n        for like in likes:\n            for i, ln in enumerate(lnames):\n                if like in ln:\n                    return names[i]\n        return None\n\n    def find_col(cols, *keywords_any):\n        lcols = [str(c) for c in cols]\n        for c in lcols:\n            lc = c.lower()\n            for kws in keywords_any:\n                # kws can be tuple/list of tokens that all must appear\n                if isinstance(kws, (list, tuple)):\n                    if all(k in lc for k in kws):\n                        return c\n                else:\n                    if kws in lc:\n                        return c\n        return None\n\n    def tokenize_tables(val):\n        if pd.isna(val):\n            return []\n        s = str(val)\n        # Split on common separators, keep alphanum/-/_ tokens\n        parts = re.split(r\"[\\s,;/]+\", s)\n        return [p.strip() for p in parts if p.strip()]\n\n    ss = find_spreadsheet()\n    if not ss:\n        return 0.0, \"No spreadsheet output found.\"\n    try:\n        xfile = pd.ExcelFile(context.files.get_path(ss.id))\n    except Exception as e:\n        return 0.0, f\"Failed to open Excel: {e}\"\n\n    vend_sheet = sheet_by_like(xfile, [\"vendor\", \"assignment\"])\n    if not vend_sheet:\n        return 0.0, \"Vendors sheet not found.\"\n\n    try:\n        vf = context.files.read_excel(ss.id, sheet_name=vend_sheet)\n    except Exception as e:\n        return 0.0, f\"Failed to read Vendors sheet: {e}\"\n\n    if vf.empty:\n        return 0.0, \"Vendors sheet is empty.\"\n\n    # Identify columns\n    col_business = find_col(vf.columns, (\"business\",), (\"vendor\",), (\"name\",))\n    col_tables_purch = find_col(vf.columns, (\"table\", \"purch\"), (\"tables\",), (\"qty\",), (\"number\", \"tables\"))\n    col_assigned = find_col(vf.columns, (\"assigned\", \"table\"), (\"assigned\",), (\"table\",))\n\n    if not col_assigned or not col_tables_purch:\n        return 0.0, \"Missing Assigned Table(s) or Tables Purchased column.\"\n\n    # Compute fraction correct\n    rows = vf.copy()\n    # Clean purchased count\n    def to_int(x):\n        try:\n            if pd.isna(x):\n                return np.nan\n            s = str(x).strip()\n            m = re.search(r\"-?\\d+\", s)\n            return int(m.group(0)) if m else np.nan\n        except Exception:\n            return np.nan\n\n    purchased = rows[col_tables_purch].apply(to_int)\n    assigned_lists = rows[col_assigned].apply(tokenize_tables)\n\n    valid_mask = purchased.notna() & (purchased >= 1)\n    if valid_mask.sum() == 0:\n        return 0.0, \"No valid purchased counts found.\"\n\n    exact_match = (assigned_lists.str.len() == purchased).fillna(False)\n    at_least_one = (assigned_lists.str.len() >= 1).fillna(False)\n\n    # Score: 70% weight on exact count match, 30% on non-empty assignment presence\n    frac_exact = (exact_match & valid_mask).sum() / valid_mask.sum()\n    frac_nonempty = (at_least_one & valid_mask).sum() / valid_mask.sum()\n    score = (0.7 * frac_exact + 0.3 * frac_nonempty) * 1.0\n\n    feedback = f\"Exact count match: {frac_exact:.2%}; Non-empty assigned: {frac_nonempty:.2%}.\"\n    return max(0.0, min(1.0, score)), feedback"}, {"type": "code", "name": "Tables Valid and Unique", "description": "Validate that all assigned tables exist on the Arena or Meeting Room sheets and that no table is double-assigned across vendors.", "weight": 0.9, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    def find_spreadsheet():\n        out = context.get_primary_output()\n        if out and getattr(out, 'is_spreadsheet', False):\n            return out\n        for r in context.get_all_outputs():\n            if getattr(r, 'is_spreadsheet', False):\n                return r\n        return None\n\n    def sheet_by_like(xls, likes):\n        names = [str(s) for s in xls.sheet_names]\n        lnames = [s.lower() for s in names]\n        for like in likes:\n            for i, ln in enumerate(lnames):\n                if like in ln:\n                    return names[i]\n        return None\n\n    def find_col(cols, *keywords_any):\n        for c in cols:\n            lc = str(c).lower()\n            for kws in keywords_any:\n                if isinstance(kws, (list, tuple)):\n                    if all(k in lc for k in kws):\n                        return c\n                else:\n                    if kws in lc:\n                        return c\n        return None\n\n    def tokenize_tables(val):\n        if pd.isna(val):\n            return []\n        parts = re.split(r\"[\\s,;/]+\", str(val))\n        return [p.strip() for p in parts if p.strip()]\n\n    ss = find_spreadsheet()\n    if not ss:\n        return 0.0, \"No spreadsheet found.\"\n    xfile = pd.ExcelFile(context.files.get_path(ss.id))\n\n    vend_sheet = sheet_by_like(xfile, [\"vendor\", \"assignment\"]) or sheet_by_like(xfile, [\"vendor\"]) or sheet_by_like(xfile, [\"assign\"])\n    arena_sheet = sheet_by_like(xfile, [\"arena\"])\n    meet_sheet = sheet_by_like(xfile, [\"meeting\"]) or sheet_by_like(xfile, [\"meeting room\"]) or sheet_by_like(xfile, [\"room\"]) \n\n    if not vend_sheet or not (arena_sheet and meet_sheet):\n        return 0.0, \"Missing Vendors or layout sheets.\"\n\n    vf = context.files.read_excel(ss.id, sheet_name=vend_sheet)\n    af = context.files.read_excel(ss.id, sheet_name=arena_sheet)\n    mf = context.files.read_excel(ss.id, sheet_name=meet_sheet)\n\n    # Table number columns\n    tcol_a = find_col(af.columns, (\"table\",), (\"table\", \"number\")) or af.columns[0]\n    tcol_m = find_col(mf.columns, (\"table\",), (\"table\", \"number\")) or mf.columns[0]\n\n    tables_arena = set(str(x).strip().lower() for x in af[tcol_a].dropna().astype(str))\n    tables_meet = set(str(x).strip().lower() for x in mf[tcol_m].dropna().astype(str))\n    all_tables = tables_arena | tables_meet\n\n    acol = find_col(vf.columns, (\"assigned\", \"table\"), (\"assigned\",))\n    if not acol:\n        return 0.0, \"Assigned Table(s) column not found.\"\n\n    assigned_pairs = []  # (table_id_norm, vendor_row_index)\n    total = 0\n    valid = 0\n\n    for i, val in vf[acol].items():\n        for tok in tokenize_tables(val):\n            total += 1\n            norm = str(tok).strip().lower()\n            assigned_pairs.append((norm, i))\n            if norm in all_tables:\n                valid += 1\n\n    if total == 0:\n        return 0.0, \"No assigned tables to validate.\"\n\n    valid_frac = valid / total\n\n    # Uniqueness: table assigned to more than one vendor row\n    table_to_rows = {}\n    for t, i in assigned_pairs:\n        table_to_rows.setdefault(t, set()).add(i)\n    dup_count = sum(1 for t, rows in table_to_rows.items() if len(rows) > 1)\n    uniq_frac = 1.0 - (dup_count / max(1, len(table_to_rows)))\n\n    score = 0.6 * valid_frac + 0.4 * uniq_frac\n    feedback = f\"Valid tables: {valid_frac:.2%}; Unique assignments: {uniq_frac:.2%} (duplicate tables: {dup_count}).\"\n    return max(0.0, min(0.9, score * 0.9)), feedback"}, {"type": "code", "name": "Location Preference Honored", "description": "Check that vendors preferring Arena or Meeting Room are assigned to tables in their preferred location.", "weight": 0.7, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    def find_spreadsheet():\n        out = context.get_primary_output()\n        if out and getattr(out, 'is_spreadsheet', False):\n            return out\n        for r in context.get_all_outputs():\n            if getattr(r, 'is_spreadsheet', False):\n                return r\n        return None\n\n    def sheet_by_like(xls, likes):\n        names = [str(s) for s in xls.sheet_names]\n        lnames = [s.lower() for s in names]\n        for like in likes:\n            for i, ln in enumerate(lnames):\n                if like in ln:\n                    return names[i]\n        return None\n\n    def find_col(cols, *keywords_any):\n        for c in cols:\n            lc = str(c).lower()\n            for kws in keywords_any:\n                if isinstance(kws, (list, tuple)):\n                    if all(k in lc for k in kws):\n                        return c\n                else:\n                    if kws in lc:\n                        return c\n        return None\n\n    def tokenize_tables(val):\n        if pd.isna(val):\n            return []\n        parts = re.split(r\"[\\s,;/]+\", str(val))\n        return [p.strip() for p in parts if p.strip()]\n\n    ss = find_spreadsheet()\n    if not ss:\n        return 0.0, \"No spreadsheet found.\"\n    xfile = pd.ExcelFile(context.files.get_path(ss.id))\n\n    vend_sheet = sheet_by_like(xfile, [\"vendor\", \"assignment\"]) or sheet_by_like(xfile, [\"vendor\"]) \n    arena_sheet = sheet_by_like(xfile, [\"arena\"]) \n    meet_sheet = sheet_by_like(xfile, [\"meeting\"]) or sheet_by_like(xfile, [\"meeting room\"]) or sheet_by_like(xfile, [\"room\"]) \n\n    if not vend_sheet or not (arena_sheet and meet_sheet):\n        return 0.0, \"Missing sheets.\"\n\n    vf = context.files.read_excel(ss.id, sheet_name=vend_sheet)\n    af = context.files.read_excel(ss.id, sheet_name=arena_sheet)\n    mf = context.files.read_excel(ss.id, sheet_name=meet_sheet)\n\n    tcol_a = find_col(af.columns, (\"table\",), (\"table\", \"number\")) or af.columns[0]\n    tcol_m = find_col(mf.columns, (\"table\",), (\"table\", \"number\")) or mf.columns[0]\n\n    tables_arena = set(str(x).strip().lower() for x in af[tcol_a].dropna().astype(str))\n    tables_meet = set(str(x).strip().lower() for x in mf[tcol_m].dropna().astype(str))\n\n    acol = find_col(vf.columns, (\"assigned\", \"table\"), (\"assigned\",))\n    lcol = find_col(vf.columns, (\"location\",), (\"preference\",), (\"preferred\",))\n    if not acol or not lcol:\n        return 0.0, \"Missing Assigned or Location Preference column.\"\n\n    checks = 0\n    ok = 0\n    for _, row in vf.iterrows():\n        pref = str(row[lcol]).strip().lower() if not pd.isna(row[lcol]) else \"\"\n        if not pref:\n            continue\n        if 'arena' in pref:\n            tgt = tables_arena\n        elif 'meeting' in pref or 'room' in pref:\n            tgt = tables_meet\n        else:\n            continue\n        tabs = [t.strip().lower() for t in re.split(r\"[\\s,;/]+\", str(row[acol])) if t.strip()] if not pd.isna(row[acol]) else []\n        if not tabs:\n            continue\n        checks += 1\n        if all(t in tgt for t in tabs):\n            ok += 1\n\n    if checks == 0:\n        return 0.0, \"No rows with clear preferences and assignments to check.\"\n\n    frac = ok / checks\n    score = frac * 0.7\n    return max(0.0, min(0.7, score)), f\"Preferences honored for {ok}/{checks} applicable vendors ({frac:.2%}).\""}, {"type": "code", "name": "Electricity Needs Met", "description": "Ensure vendors marked as needing electricity have at least one assigned powered table (based on Electricity Available columns in layout sheets).", "weight": 0.7, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    def find_spreadsheet():\n        out = context.get_primary_output()\n        if out and getattr(out, 'is_spreadsheet', False):\n            return out\n        for r in context.get_all_outputs():\n            if getattr(r, 'is_spreadsheet', False):\n                return r\n        return None\n\n    def sheet_by_like(xls, likes):\n        names = [str(s) for s in xls.sheet_names]\n        lnames = [s.lower() for s in names]\n        for like in likes:\n            for i, ln in enumerate(lnames):\n                if like in ln:\n                    return names[i]\n        return None\n\n    def find_col(cols, *keywords_any):\n        for c in cols:\n            lc = str(c).lower()\n            for kws in keywords_any:\n                if isinstance(kws, (list, tuple)):\n                    if all(k in lc for k in kws):\n                        return c\n                else:\n                    if kws in lc:\n                        return c\n        return None\n\n    def to_bool(x):\n        if pd.isna(x):\n            return False\n        s = str(x).strip().lower()\n        return s in {\"yes\",\"y\",\"true\",\"1\",\"t\",\"required\",\"need\",\"needs\"}\n\n    def tokenize_tables(val):\n        if pd.isna(val):\n            return []\n        parts = re.split(r\"[\\s,;/]+\", str(val))\n        return [p.strip().lower() for p in parts if p.strip()]\n\n    ss = find_spreadsheet()\n    if not ss:\n        return 0.0, \"No spreadsheet found.\"\n    xfile = pd.ExcelFile(context.files.get_path(ss.id))\n\n    vend_sheet = sheet_by_like(xfile, [\"vendor\", \"assignment\"]) or sheet_by_like(xfile, [\"vendor\"]) \n    arena_sheet = sheet_by_like(xfile, [\"arena\"]) \n    meet_sheet = sheet_by_like(xfile, [\"meeting\"]) or sheet_by_like(xfile, [\"meeting room\"]) or sheet_by_like(xfile, [\"room\"]) \n\n    if not vend_sheet or not (arena_sheet and meet_sheet):\n        return 0.0, \"Missing sheets.\"\n\n    vf = context.files.read_excel(ss.id, sheet_name=vend_sheet)\n    af = context.files.read_excel(ss.id, sheet_name=arena_sheet)\n    mf = context.files.read_excel(ss.id, sheet_name=meet_sheet)\n\n    tcol_a = find_col(af.columns, (\"table\",), (\"table\", \"number\")) or af.columns[0]\n    tcol_m = find_col(mf.columns, (\"table\",), (\"table\", \"number\")) or mf.columns[0]\n    ecol_a = find_col(af.columns, (\"electric\",), (\"power\",))\n    ecol_m = find_col(mf.columns, (\"electric\",), (\"power\",))\n\n    if not ecol_a and not ecol_m:\n        return 0.0, \"No electricity availability columns on layout sheets.\"\n\n    powered = set()\n    for df, tcol, ecol in [(af, tcol_a, ecol_a), (mf, tcol_m, ecol_m)]:\n        if ecol is None:\n            continue\n        for _, r in df[[tcol, ecol]].dropna(subset=[tcol]).iterrows():\n            if to_bool(r[ecol]):\n                powered.add(str(r[tcol]).strip().lower())\n\n    acol = find_col(vf.columns, (\"assigned\", \"table\"), (\"assigned\",))\n    pcol = find_col(vf.columns, (\"electric\",), (\"power\",), (\"needs\",))\n    if not acol or not pcol:\n        return 0.0, \"Missing Assigned or Electricity Needed column.\"\n\n    checks = 0\n    ok = 0\n    for _, row in vf.iterrows():\n        needs = to_bool(row[pcol])\n        if not needs:\n            continue\n        tabs = tokenize_tables(row[acol])\n        if not tabs:\n            continue\n        checks += 1\n        if any(t in powered for t in tabs):\n            ok += 1\n\n    if checks == 0:\n        return 0.0, \"No electricity-required vendors with assignments to check.\"\n\n    frac = ok / checks\n    score = frac * 0.7\n    return max(0.0, min(0.7, score)), f\"Electricity satisfied for {ok}/{checks} vendors ({frac:.2%}).\""}, {"type": "code", "name": "Adjacency Product Variety", "description": "Use Adjacent Tables mapping to penalize adjacent placements with the same product category. Requires Adjacent Tables column on both layout sheets and a product/category column on Vendors.", "weight": 0.4, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    def find_spreadsheet():\n        out = context.get_primary_output()\n        if out and getattr(out, 'is_spreadsheet', False):\n            return out\n        for r in context.get_all_outputs():\n            if getattr(r, 'is_spreadsheet', False):\n                return r\n        return None\n\n    def sheet_by_like(xls, likes):\n        names = [str(s) for s in xls.sheet_names]\n        lnames = [s.lower() for s in names]\n        for like in likes:\n            for i, ln in enumerate(lnames):\n                if like in ln:\n                    return names[i]\n        return None\n\n    def find_col(cols, *keywords_any):\n        for c in cols:\n            lc = str(c).lower()\n            for kws in keywords_any:\n                if isinstance(kws, (list, tuple)):\n                    if all(k in lc for k in kws):\n                        return c\n                else:\n                    if kws in lc:\n                        return c\n        return None\n\n    def tokenize_tables(val):\n        if pd.isna(val):\n            return []\n        parts = re.split(r\"[\\s,;/]+\", str(val))\n        return [p.strip().lower() for p in parts if p.strip()]\n\n    def norm_cat(x):\n        s = str(x).strip().lower()\n        # crude normalization: singularize common plurals\n        s = re.sub(r\"candles?\", \"candle\", s)\n        s = re.sub(r\"soaps?\", \"soap\", s)\n        s = re.sub(r\"jewelry|jewellery\", \"jewelry\", s)\n        # take first keyword token\n        m = re.search(r\"[a-z]+\", s)\n        return m.group(0) if m else s\n\n    ss = find_spreadsheet()\n    if not ss:\n        return 0.0, \"No spreadsheet found.\"\n    xfile = pd.ExcelFile(context.files.get_path(ss.id))\n\n    vend_sheet = sheet_by_like(xfile, [\"vendor\", \"assignment\"]) or sheet_by_like(xfile, [\"vendor\"]) \n    arena_sheet = sheet_by_like(xfile, [\"arena\"]) \n    meet_sheet = sheet_by_like(xfile, [\"meeting\"]) or sheet_by_like(xfile, [\"meeting room\"]) or sheet_by_like(xfile, [\"room\"]) \n\n    if not vend_sheet or not (arena_sheet and meet_sheet):\n        return 0.0, \"Missing sheets.\"\n\n    vf = context.files.read_excel(ss.id, sheet_name=vend_sheet)\n    af = context.files.read_excel(ss.id, sheet_name=arena_sheet)\n    mf = context.files.read_excel(ss.id, sheet_name=meet_sheet)\n\n    # Required columns\n    tcol_a = find_col(af.columns, (\"table\",), (\"table\", \"number\")) or af.columns[0]\n    tcol_m = find_col(mf.columns, (\"table\",), (\"table\", \"number\")) or mf.columns[0]\n    adj_a = find_col(af.columns, (\"adjacent\",), (\"neighbor\",), (\"near\",))\n    adj_m = find_col(mf.columns, (\"adjacent\",), (\"neighbor\",), (\"near\",))\n    acol = find_col(vf.columns, (\"assigned\", \"table\"), (\"assigned\",))\n    pcol = find_col(vf.columns, (\"category\",), (\"product\",), (\"description\",))\n\n    if not acol or not pcol or not adj_a or not adj_m:\n        return 0.0, \"Missing Adjacent Tables or product/category/assigned columns.\"\n\n    # Map table -> vendor category\n    table_to_cat = {}\n    for _, row in vf.iterrows():\n        cat = norm_cat(row[pcol]) if pcol in vf.columns else None\n        for t in tokenize_tables(row[acol]):\n            table_to_cat[t] = cat\n\n    # Build adjacency edges (undirected)\n    edges = set()\n    def add_edges(df, tcol, adjcol):\n        for _, r in df.dropna(subset=[tcol]).iterrows():\n            t = str(r[tcol]).strip().lower()\n            neighs = tokenize_tables(r.get(adjcol, \"\"))\n            for n in neighs:\n                a, b = sorted([t, n])\n                if a and b and a != b:\n                    edges.add((a, b))\n\n    add_edges(af, tcol_a, adj_a)\n    add_edges(mf, tcol_m, adj_m)\n\n    if not edges:\n        return 0.0, \"No adjacency relationships found.\"\n\n    total_comp = 0\n    violations = 0\n    for a, b in edges:\n        ca = table_to_cat.get(a)\n        cb = table_to_cat.get(b)\n        if ca is None or cb is None:\n            continue\n        total_comp += 1\n        if ca == cb and ca != \"\":\n            violations += 1\n\n    if total_comp == 0:\n        return 0.0, \"Adjacency edges found but insufficient assigned neighbors to compare.\"\n\n    viol_frac = violations / total_comp\n    score = (1.0 - viol_frac) * 0.4\n    return max(0.0, min(0.4, score)), f\"Adjacency comparisons: {total_comp}, same-category adjacencies: {violations}.\""}, {"type": "code", "name": "Counts Reconciled", "description": "Sum of assigned tables equals the sum of Tables Purchased across vendors (within strict equality).", "weight": 0.3, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    def find_spreadsheet():\n        out = context.get_primary_output()\n        if out and getattr(out, 'is_spreadsheet', False):\n            return out\n        for r in context.get_all_outputs():\n            if getattr(r, 'is_spreadsheet', False):\n                return r\n        return None\n\n    def sheet_by_like(xls, likes):\n        names = [str(s) for s in xls.sheet_names]\n        lnames = [s.lower() for s in names]\n        for like in likes:\n            for i, ln in enumerate(lnames):\n                if like in ln:\n                    return names[i]\n        return None\n\n    def find_col(cols, *keywords_any):\n        for c in cols:\n            lc = str(c).lower()\n            for kws in keywords_any:\n                if isinstance(kws, (list, tuple)):\n                    if all(k in lc for k in kws):\n                        return c\n                else:\n                    if kws in lc:\n                        return c\n        return None\n\n    def tokenize_tables(val):\n        if pd.isna(val):\n            return []\n        parts = re.split(r\"[\\s,;/]+\", str(val))\n        return [p.strip() for p in parts if p.strip()]\n\n    ss = find_spreadsheet()\n    if not ss:\n        return 0.0, \"No spreadsheet found.\"\n    xfile = pd.ExcelFile(context.files.get_path(ss.id))\n\n    vend_sheet = sheet_by_like(xfile, [\"vendor\", \"assignment\"]) or sheet_by_like(xfile, [\"vendor\"]) \n    if not vend_sheet:\n        return 0.0, \"Missing Vendors sheet.\"\n\n    vf = context.files.read_excel(ss.id, sheet_name=vend_sheet)\n\n    acol = find_col(vf.columns, (\"assigned\", \"table\"), (\"assigned\",))\n    pcol = find_col(vf.columns, (\"table\", \"purch\"), (\"tables\",), (\"qty\",), (\"number\", \"tables\"))\n    if not acol or not pcol:\n        return 0.0, \"Missing Assigned or Tables Purchased column.\"\n\n    def to_int(x):\n        try:\n            if pd.isna(x):\n                return 0\n            m = re.search(r\"-?\\d+\", str(x))\n            return int(m.group(0)) if m else 0\n        except Exception:\n            return 0\n\n    total_purchased = int(vf[pcol].apply(to_int).sum())\n    total_assigned = int(vf[acol].apply(lambda v: len(tokenize_tables(v))).sum())\n\n    if max(total_purchased, total_assigned) == 0:\n        return 0.0, \"No purchased or assigned tables counted.\"\n\n    if total_purchased == total_assigned:\n        return 0.3, f\"Totals match: {total_assigned}.\"\n    else:\n        ratio = min(total_purchased, total_assigned) / max(total_purchased, total_assigned)\n        return 0.3 * ratio, f\"Totals mismatch \u2014 purchased: {total_purchased}, assigned: {total_assigned}.\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Usability", "description": "Holistic assessment of clarity, usability, and professional presentation for a city-run event. Evaluate the workbook and any companion document (PDF/DOCX) that explains rationale and provides guidance for operations.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Plan Clarity, Rationale, and Operational Usability", "description": "Judge the professional quality of the deliverables for staff, vendors, and shoppers.", "weight": 2.0, "judge_prompt": "Review the Excel workbook and any companion PDF/DOCX, if provided. Assess the overall professional quality and operational usability. Consider:\n- Clarity: Are sheets, columns, and table numbers clearly labeled and consistent between Vendors and layout maps? Is it easy to find a vendor\u2019s assigned table(s)?\n- Rationale and guidance: Is there a concise explanation of placement logic (preferences, electricity, adjacency variety) and any exceptions? Are there notes that help day-of staff (e.g., check-in, power distribution, move-in)?\n- Shopper and vendor experience: Does the plan appear to distribute similar product types to avoid clustering? Are powered tables sensibly placed without tangling traffic? Does the layout suggest good flow and visibility?\n- Professionalism: Formatting, readability, and absence of obvious errors/omissions.\n\nScoring (0\u20132):\n- 2.0: Excellent clarity and organization; rationale and exception handling documented; highly usable for operations.\n- 1.5: Good clarity with minor gaps; generally usable.\n- 1.0: Adequate but some confusing elements or missing rationale.\n- 0.5: Poorly organized; difficult to use operationally.\n- 0.0: Unprofessional/unclear; unusable without rework.\n", "expectation": "A clear, professional plan that staff can run with and vendors can follow, reflecting preferences, power constraints, and product variety."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "a0ef404e-82a6-4507-bff1-633d7c8e0004", "rubric": {"category_name": "Real Estate and Rental and Leasing \u2014 Counter and Rental Clerks: Rental Agreement Instructional Guide", "rationale": "This rubric enforces a self-documenting, verifiable document shape (Stage 1 LLM gate), then validates correctness with code and LLM cross-checks (Stage 2), and finally assesses overall professional quality and usability for new clerks working peak shifts (Stage 3). The output must be a Word-style guide (DOCX preferred; PDF acceptable) that is generic (non\u2013software-specific), step-by-step, and contains explanations, tips, and troubleshooting to enable independent execution.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Document Shape Gate (LLM only)", "description": "Mandate exact document structure so verification is trivial. GATE: If the output is not a valid DOCX/PDF with the required sections and step-by-step structure, scoring stops and the category is zeroed.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Training Guide Format Requirement", "description": "Check that the candidate output is a DOCX or PDF training guide with all required sections and a numbered, step-by-step procedure covering the core rental agreement workflow.", "weight": 4.0, "judge_prompt": "You are the Stage 1 gatekeeper. Evaluate ONLY structure and presence, not the quality of writing or correctness of details.\n\nPass criteria: The output must be a DOCX or PDF training guide with the exact structural elements below. Be flexible on header names that are clearly equivalent.\n\nFormat requirements:\n- Must be a DOCX or PDF document (Word format preferred; PDF acceptable).\n- Professionally formatted with clear section headers.\n- At least 2 pages; 3+ pages recommended for completeness.\n- Step-by-step, numbered procedure visible (e.g., \u201cStep 1 \u2026 Step 2 \u2026\u201d or a numbered list 1., 2., 3., etc.).\n\nRequired sections (flexible naming allowed if clearly equivalent):\n1) Title and Document Metadata\n   - Includes a clear title (e.g., \u201cRental Agreement Creation Guide\u201d).\n   - Shows author/owner or version/date somewhere near the beginning.\n2) Purpose and Scope (or Overview)\n   - States who this is for (new rental clerks) and when to use it (busy shifts/airport counter).\n3) Prerequisites and Required Materials\n   - Lists required documents/items (e.g., driver\u2019s license/ID, payment card, reservation number) and any eligibility notes.\n4) Step-by-Step Procedure (numbered)\n   - A numbered workflow that covers the core steps: welcoming/greeting, locating or creating reservation, verifying ID/eligibility/required documents, recording contact details, obtaining valid payment method/authorization, assigning a vehicle (including inspection basics), reviewing rental terms and options (fuel policy, mileage, insurance/extras), collecting signatures/finalizing the agreement, and handover/closing steps.\n5) Practical Tips (efficiency or best practices)\n6) Common Mistakes and Troubleshooting\n7) Quick-Reference Checklist (end-of-doc or clearly labeled \u201cChecklist\u201d/\u201cQuick Reference\u201d)\n\nOptional but nice-to-have (do not penalize if missing):\n- Appendix with sample phrases/scripts\n- Glossary of terms\n\nScoring:\n- 1.0: DOCX or PDF AND all required sections (1\u20137) present AND step-by-step numbering is clearly visible AND at least 2 pages.\n- 0.8: DOCX or PDF with the step-by-step procedure present but missing exactly one supporting section among: Practical Tips, Common Mistakes and Troubleshooting, or Quick-Reference Checklist; or PDF instead of DOCX with everything else present.\n- 0.5: DOCX or PDF but missing two required items OR the step-by-step is not clearly numbered.\n- 0.0: Not DOCX/PDF OR single page only OR missing the Step-by-Step Procedure.\n\nReturn a score in [0.0, 1.0] for this rule based solely on the above structural criteria.", "expectation": "A Word-like guide (DOCX preferred) that is at least 2 pages, with clear sections, a numbered step-by-step process covering all core steps, plus dedicated Tips, Common Mistakes/Troubleshooting, and a Quick-Reference Checklist."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Verifiability (Code + LLM)", "description": "Now that the document has the mandated shape, verify correctness and completeness with deterministic checks and an LLM cross-check. These focus on step coverage, structure, presence of tips/mistakes, checklist, sufficient depth, and generic (non\u2013software-specific) guidance.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Core Step Coverage", "description": "Verifies the presence of all core steps using flexible keyword matching: welcome, reservation lookup/creation, ID and required docs, contact details, payment/authorization and deposit, vehicle assignment and inspection basics, review terms (fuel policy, mileage, insurance/extras), signatures/finalization, and handover/closing.", "weight": 1.6, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Checks for coverage of core steps via keyword groups.\n    Returns a normalized score [0,1] plus feedback listing missing steps.\n    \"\"\"\n    import re\n    \n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No primary output.\"\n    if not output.is_document:\n        return 0.0, \"Primary output is not a document (DOCX/PDF).\"\n\n    # Try to extract text robustly\n    text = \"\"\n    try:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            return 0.0, \"Could not read document text.\"\n\n    if not text or not isinstance(text, str):\n        return 0.0, \"Empty or unreadable document text.\"\n\n    t = text.lower()\n\n    core_steps = {\n        \"welcome_greet\": [\"welcome\", \"greet\", \"introduce\", \"opening line\"],\n        \"reservation_lookup\": [\"reservation\", \"confirmation\", \"lookup\", \"retrieve reservation\", \"walk-up\", \"create reservation\"],\n        \"id_and_docs\": [\"driver's license\", \"drivers license\", \"id\", \"identification\", \"passport\", \"eligibility\", \"age requirement\", \"insurance proof\"],\n        \"contact_details\": [\"contact\", \"phone\", \"email\", \"address\"],\n        \"payment_authorization\": [\"payment\", \"credit card\", \"debit\", \"authorization\", \"pre-authorization\", \"preauthorization\", \"deposit\", \"hold\"],\n        \"vehicle_assignment_inspection\": [\"assign vehicle\", \"assign car\", \"vehicle class\", \"upgrade\", \"vin\", \"plate\", \"odometer\", \"fuel level\", \"inspection\", \"damage\", \"walk-around\"],\n        \"review_terms\": [\"rental terms\", \"terms and conditions\", \"mileage\", \"fuel policy\", \"cdw\", \"ldw\", \"collision damage waiver\", \"insurance\", \"toll\", \"fees\", \"return time\", \"drop-off\"],\n        \"extras_addons\": [\"additional driver\", \"child seat\", \"gps\", \"toll pass\", \"roadside\", \"extras\", \"add-ons\", \"upsell\"],\n        \"sign_finalize\": [\"signature\", \"sign\", \"agreement\", \"contract\", \"receipt\", \"print\", \"email copy\", \"e-sign\", \"finalize\"],\n        \"handover_close\": [\"keys\", \"hand over\", \"handover\", \"parking bay\", \"stall\", \"pickup location\", \"exit instructions\", \"gate\", \"final check\"]\n    }\n\n    present = []\n    missing = []\n\n    for step, kws in core_steps.items():\n        if any(kw in t for kw in kws):\n            present.append(step)\n        else:\n            missing.append(step)\n\n    coverage = len(present) / max(1, len(core_steps))\n    feedback = f\"Core steps present: {len(present)}/{len(core_steps)}. Missing: {', '.join(missing) if missing else 'None'}.\"\n    return coverage, feedback"}, {"type": "code", "name": "Numbered Procedure Present", "description": "Checks for a clearly numbered step sequence (e.g., \u201cStep 1\u201d or lines starting with 1., 2., etc.). Rewards higher counts.", "weight": 0.6, "code": "def evaluate(workflow, context):\n    import re\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No readable document for numbered steps check.\"\n    # Read text\n    text = \"\"\n    try:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            return 0.0, \"Failed to extract text.\"\n    if not text:\n        return 0.0, \"Empty text.\"\n\n    # Count occurrences of typical numbering patterns\n    patterns = [r\"\\bstep\\s*\\d+\\b\", r\"(?m)^\\s*\\d+[\\.)]\\s\"]\n    count = 0\n    for p in patterns:\n        count += len(re.findall(p, text.lower()))\n\n    # Heuristic scoring\n    if count >= 12:\n        score = 1.0\n    elif count >= 8:\n        score = 0.8\n    elif count >= 5:\n        score = 0.6\n    elif count >= 3:\n        score = 0.3\n    else:\n        score = 0.0\n    return score, f\"Detected numbered step markers: {count}.\""}, {"type": "code", "name": "Tips and Troubleshooting Presence", "description": "Verifies presence of Practical Tips and Common Mistakes/Troubleshooting content using flexible keyword matching.", "weight": 0.6, "code": "def evaluate(workflow, context):\n    import re\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document to evaluate tips/troubleshooting.\"\n    try:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            return 0.0, \"Failed to read text.\"\n    t = (text or \"\").lower()\n\n    tips_hits = sum(1 for kw in [\"tips\", \"pro tip\", \"best practice\", \"efficiency\", \"time-saving\"] if kw in t)\n    trouble_hits = sum(1 for kw in [\"troubleshooting\", \"common mistakes\", \"pitfall\", \"avoid\", \"error\", \"issue\", \"workaround\"] if kw in t)\n\n    tips_score = 1.0 if tips_hits > 0 else 0.0\n    trouble_score = 1.0 if trouble_hits > 0 else 0.0\n\n    score = 0.5 * tips_score + 0.5 * trouble_score\n    return score, f\"Tips present: {tips_hits>0}, Troubleshooting/mistakes present: {trouble_hits>0}.\""}, {"type": "code", "name": "Quick-Reference Checklist Present", "description": "Checks for a labeled Checklist/Quick Reference section.", "weight": 0.4, "code": "def evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document for checklist check.\"\n    try:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            return 0.0, \"Failed to extract text.\"\n    t = (text or \"\").lower()\n    has_checklist = any(kw in t for kw in [\"checklist\", \"quick reference\", \"at-a-glance\", \"cheat sheet\", \"ready reckoner\"])\n    return (1.0 if has_checklist else 0.0), (\"Checklist found\" if has_checklist else \"Checklist/Quick Reference not found\")"}, {"type": "code", "name": "Length Sufficiency (Word Count)", "description": "Checks if the guide is sufficiently detailed. Full credit at >= 800 words; partial below that; very short docs get little/no credit.", "weight": 0.4, "code": "def evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document to evaluate length.\"\n    # Extract text\n    text = None\n    try:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            return 0.0, \"Failed to extract text.\"\n\n    if not text:\n        return 0.0, \"Empty text.\"\n\n    words = len([w for w in text.split() if w.strip()])\n    if words >= 800:\n        score = 1.0\n    elif words >= 600:\n        score = 0.8\n    elif words >= 450:\n        score = 0.6\n    elif words >= 300:\n        score = 0.3\n    else:\n        score = 0.0\n    return score, f\"Word count: {words}.\""}, {"type": "llm_judge", "name": "Process Logic and Genericness (Non\u2013Software-Specific)", "description": "Checks that the steps are logically ordered, include brief explanations of why each step is necessary, and avoid software- or UI-specific instructions. Focus on correctness and generic applicability, not prose quality.", "weight": 0.4, "judge_prompt": "Evaluate the document for process correctness and genericness (non\u2013software-specific). Do not assess overall writing quality here.\n\nCriteria:\n- Logical flow: Steps proceed in a sensible order from greeting through handover/close.\n- Explanatory notes: Most steps (\u224880% or more) include a short explanation of why the step is necessary (can be inline or as sub-bullets like \u201cWhy this step\u201d).\n- Generic guidance: Avoids UI/system-specific instructions (e.g., \u201cclick File > New,\u201d screenshots of a specific product, proprietary system names). Generic terms like \u201creservation system\u201d or \u201crental platform\u201d are fine.\n\nScoring (0.0\u20131.0):\n- 1.0: Logical flow, clear explanations for most steps, and fully system-agnostic.\n- 0.7: Mostly logical, explanations present for a majority of steps, and minor specific references that do not impede generic use.\n- 0.4: Flow is acceptable but explanations are sparse OR several software-specific instructions appear.\n- 0.2: Disorganized OR heavily software-specific; few explanations.\n- 0.0: Predominantly software-specific OR missing explanations and logic.", "expectation": "A logically ordered, generic (system-agnostic) process with brief rationale for each step."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Usability (LLM)", "description": "Holistic assessment of professional presentation and usability for new clerks working independently during peak hours.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity, Formatting, and Professional Tone", "description": "Assesses whether the guide is clearly written, well formatted with readable headings/lists, and maintains a professional, customer-service\u2013oriented tone appropriate for a busy airport rental counter.", "weight": 1.0, "judge_prompt": "Evaluate clarity, formatting, and professional tone.\n- Clarity: Instructions are concise and unambiguous; jargon is explained; acronyms (e.g., CDW/LDW) defined or contextualized.\n- Formatting: Consistent headings, numbered steps, bullets, callouts for warnings/notes; easy to scan on the job.\n- Tone: Professional, supportive, customer-service\u2013oriented, suitable for a training context.\n\nScoring (0.0\u20131.0):\n- 1.0: Very clear, well-structured, and professional throughout.\n- 0.7: Generally clear with minor formatting or tone issues.\n- 0.4: Mixed clarity/formatting; could confuse new clerks.\n- 0.2: Hard to follow or unprofessional tone.\n- 0.0: Disorganized and confusing.", "expectation": "A clean, readable, professional guide with strong headings, numbered steps, and scannable lists."}, {"type": "llm_judge", "name": "Usability During Peak Hours", "description": "Judges how practical and self-serve the guide is for new clerks working independently under time pressure.", "weight": 1.0, "judge_prompt": "Assess whether a new clerk could use this guide independently during peak hours.\nConsider:\n- Presence and usefulness of the Quick-Reference Checklist.\n- Inclusion of specific tips for speed/accuracy (e.g., batching ID checks, phrasing for upsells without delays).\n- Callouts for common pitfalls (e.g., payment holds, additional driver rules, fuel policy misunderstandings) and succinct troubleshooting guidance.\n- Scannability: Sectioning, summaries, and navigability.\n\nScoring (0.0\u20131.0):\n- 1.0: Highly usable under time pressure; quick-reference elements and targeted tips make tasks faster and safer.\n- 0.7: Usable with minor gaps.\n- 0.4: Some helpful content but hard to use rapidly.\n- 0.2: Minimal practical utility during peak hours.\n- 0.0: Not practically usable for self-service during busy shifts.", "expectation": "A scannable, practical guide that enables accurate, efficient execution with minimal supervision."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "7b08cd4d-df60-41ae-9102-8aaa49306ba2", "rubric": {"category_name": "Excel Profit and Loss Report", "rationale": "This rubric evaluates a financial summary for a music tour, focusing on structure, correctness, and presentation quality. The goal is to ensure the report is accurate, well-organized, and provides strategic insights for executives.", "max_total_score": 10.0, "stages": [{"name": "Stage 1: Format and Structure Validation", "description": "Checks if the Excel file has the required structure to enable verification.", "is_required": true, "min_score_to_pass": 0.7, "rules": [{"type": "llm_judge", "name": "Structured Excel Format", "description": "Ensure the Excel output has the required structural components for verification.", "weight": 1.0, "judge_prompt": "Evaluate if the output structured Excel file includes the following required elements:\n\n**Sheet Name: '2024 Fall Music Tour Summary'**\n\n1. Income and Cost Breakdown:\n   - Two tables with columns: [Source, Income, Costs, Total Combined]\n   - Separate income and costs by 'Tour Manager' and 'Production Company'\n\n2. Revenue Details:\n   - A detailed table of [City, Country, Gross Revenue, Withholding Tax, Net Revenue] for each tour stop\n   - Correct application of withholding rates for UK (20%), France (15%), Spain (24%), Germany (15.825%)\n   - All revenue figures reported in USD\n\n3. Expense Summary:\n   - Organized by categories: [Band and Crew, Other Tour Costs, Hotel & Restaurants, Other Travel Costs, Total Expenses]\n\n4. Net Income:\n   - Calculation of Total Net Revenue minus Total Expenses\n\n5. Header:\n   - Must include 'As of 12/31/2024' clearly in the document header.\n\n**Scoring:**\n- 1.0: All sections and elements are present with correct structure.\n- 0.7: Missing one section or minor structural errors.\n- 0.4: Missing multiple sections or major structural errors.\n- 0.0: Entirely incorrect format or structure.\n\nOnly validate the presence and structuring, not the data correctness.", "expectation": "The Excel file should exhibit a precise structure that clearly delineates income, costs, and calculations with all required sections."}], "max_points": 3.0, "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Verification of Correctness", "description": "Verify the accuracy of calculations and data entry based on the enforced structure from Stage 1.", "is_required": false, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Revenue Conversion and Withholding Accuracy", "description": "Validate that all revenue figures are converted to USD correctly and withholding taxes are applied accurately.", "weight": 1.5, "code": "def evaluate(workflow, context):\n    import pandas as pd\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        df = context.files.read_excel(output.id, sheet_name='2024 Fall Music Tour Summary')\n        # Check revenue calculation\n        gross_revenue = df['Gross Revenue'].sum()\n        withholding = df['Withholding Tax'].sum()\n        net_revenue = df['Net Revenue'].sum()\n        if abs(gross_revenue - (net_revenue + withholding)) < 1.0:\n            return 1.5\n        return (gross_revenue - (net_revenue + withholding)) / gross_revenue\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Expense Calculation Verification", "description": "Ensure that the total expenses are calculated correctly across all categories.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    import pandas as pd\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        df = context.files.read_excel(output.id, sheet_name='2024 Fall Music Tour Summary')\n        # Verify expenses\n        expenses_columns = ['Band and Crew', 'Other Tour Costs', 'Hotel & Restaurants', 'Other Travel Costs']\n        total_expenses = df[expenses_columns].sum().sum()\n        if abs(df['Total Expenses'].sum() - total_expenses) < 1.0:\n            return 1.0\n        return 0.0\n    except Exception:\n        return 0.0"}, {"type": "llm_judge", "name": "Net Income Accuracy", "description": "Check if Net Income is calculated correctly based on provided revenue and expenses data.", "weight": 1.0, "judge_prompt": "Please confirm whether the Net Income is correctly calculated as Total Net Revenue minus Total Expenses in the provided Excel sheet. Use the following criteria:\n\n- Verify 'Total Net Revenue' calculation accurately aggregates to consider all revenue and withholdings.\n- Check 'Total Expenses' is correctly summed from all expense categories.\n- Net Income should equal Total Net Revenue minus Total Expenses.\n\n**Scoring:**\n- 1.0: All calculations and entries are correct\n- 0.5: Minor calculation errors\n- 0.0: Major discrepancies or multiple errors", "expectation": "The net income should accurately reflect the subtraction of total expenses from total net revenue, indicating the financial performance of the tour."}], "max_points": 3.5, "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3: Quality and Presentation Assessment", "description": "Evaluate the quality, insights, and presentation professionalism of the report.", "is_required": false, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Formatting and Presentation", "description": "Assess the visual presentation and professional formatting of the Excel report.", "weight": 1.0, "judge_prompt": "Examine the Excel document for professional formatting and clear layout. Look for:\n- Consistent currency formatting in USD\n- Clear labeling of columns and rows\n- Easy-to-read and understand structure\n- Overall professionalism in design\n\n**Scoring:**\n- 1.0: Exemplary professional presentation\n- 0.7: Good but with minor formatting issues\n- 0.4: Functional but lacks professional polish\n- 0.0: Unorganized or unprofessional presentation", "expectation": "The report should be polished, clear, and professionally formatted, demonstrating attention to detail and effective visual communication."}, {"type": "llm_judge", "name": "Strategic Insights and Usefulness", "description": "Evaluate the strategic value and insight provided by the financial report.", "weight": 2.5, "judge_prompt": "Evaluate the financial report's ability to provide strategic insights for decision making:\n- Does it guide future planning by clearly identifying financial strengths and weaknesses?\n- Are there useful insights based on tour performance data?\n- Is the information presented in a way that executives find clear and actionable?\n\n**Scoring:**\n- 2.5: Highly insightful and actionable for strategic decision making\n- 1.5: Provides some useful insights with minor limitations\n- 0.5: Limited strategic value or lacks actionable insights\n- 0.0: Fails to provide any strategic value for planning", "expectation": "The summary should not only present data but also offer strategic insights that can be used to guide future financial planning decisions."}], "max_points": 3.5, "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "62f04c2f-e0f7-4710-876c-54ee9c2e8256", "rubric": {"category_name": "Wholesale Trade \u2013 First-Line Sales Supervisors: Exchange Program Overview + Authorization Form", "rationale": "This is a mixed deliverable: a one-page professional program overview (DOCX/PDF) and an operational Excel form. Stage 1 (LLM-only) enforces strict shape and structural completeness for both files. Stage 2 uses code rules to verify policy elements and form fields textually and structurally, leveraging flexible matching across all outputs. Stage 3 applies LLM quality review for professionalism and usability.", "max_total_score": 11.0, "stages": [{"name": "Stage 1 \u2013 Structured Output Gate (LLM-only)", "description": "Gate: Verify presence and structure of two separate deliverables: (1) a one-page program overview (DOCX/PDF) with mandated sections and key policy elements; (2) an Excel Exchange Authorization form with specified fields, columns, and signature blocks.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Program Overview Document \u2013 Format and Structural Requirements", "description": "Check the output contains a one-page DOCX/PDF overview for the Gravon Shoes Exchange Program with required headers/sections and visible key policy elements.", "weight": 2.0, "judge_prompt": "You are checking ONLY the STRUCTURE and PRESENCE of required elements (not the writing quality or correctness of details). Look through all candidate outputs.\n\nRequirements for this rule:\n1) There must be a professional document in DOCX or PDF format. It should be one page or clearly intended as a single-page overview (reasonable white space, concise layout). Title can be flexible (e.g., \"Gravon Shoes Exchange Program Overview\").\n2) The document must present clearly labeled sections or headings (exact names may vary) covering the following areas:\n   - Program Summary/Overview explicitly stating this is an exchange program (not a return program) and that credits may only be used for replacement merchandise.\n   - Eligibility/Participation indicating only credit-worthy customers can participate and limit of one exchange per season.\n   - Process/How to Initiate & Steps including: (a) Retailer submits inventory on a Gravon Shoes Exchange Authorization form; (b) Form is emailed to the assigned sales rep; (c) If approved, rep returns the form with an Exchange Authorization number; (d) A printed copy of the authorized form goes in the return shipment; (e) The Exchange Authorization number must be written on the outside of the box; (f) Orders for replacement merchandise must be placed when the Exchange Authorization number is granted.\n   - Fees/Charges specifying customer pays freight and a $5 per pair restocking fee; restocking fees charged to the customer\u2019s account.\n   - Warehouse Contact including address and phone number: 555 Waters Avenue, Austin, TX 78726; 455-864-3867.\n   - Effective Date stated as July 1, 2025.\n3) The document should use a professional layout (headers, bullets, or short paragraphs) suitable for management review and for field distribution.\n\nScoring (STRUCTURE ONLY):\n- 2.0: DOCX/PDF present; one-page; all sections above are clearly present with visible headers/bullets and all listed policy/process elements mentioned.\n- 1.5: DOCX/PDF present; one-page; missing 1-2 minor elements but overall structure and the required sections exist.\n- 1.0: DOCX/PDF present but missing 3-4 required elements/sections or appears longer than a concise single page.\n- 0.5: A document exists but is not DOCX/PDF, or is missing multiple core sections, or is not formatted as a professional overview.\n- 0.0: No valid DOCX/PDF overview found.\n\nOnly check presence/structure. Do not judge content quality or correctness.", "expectation": "A one-page professional DOCX/PDF with clear sections covering summary, eligibility, process steps, fees, contact, and effective date."}, {"type": "llm_judge", "name": "Excel Exchange Authorization Form \u2013 Structural Requirements", "description": "Check that an Excel spreadsheet exists and is structured as an Exchange Authorization form with top header fields, a line-item table, a footer note, and signature blocks.", "weight": 2.0, "judge_prompt": "You are checking ONLY STRUCTURE (not calculation correctness). Inspect all outputs for an Excel spreadsheet representing the Exchange Authorization form.\n\nRequired structure:\n- File type: Excel (.xlsx or .xls acceptable).\n- Sheet name: Prefer \"Exchange Authorization\" (minor variants acceptable such as \"Exchange Auth\", \"Exchange Authorization Form\").\n- Top header block (fields/labels; exact wording flexible): Customer Name, Address, Phone Number, Customer Number, Exchange Authorization Number, Date of Return Request.\n- Line-item table with visible column headers: Style/Color, Style Name, Pairs Shipped, Pairs to be Returned.\n- Footer note stating customers must prepay freight and pay the restocking fee.\n- Signature/Approval block with spaces/lines for: Sales Representative, GM, Sales Manager, each with Signature and Date fields.\n\nScoring (STRUCTURE ONLY):\n- 2.0: Valid Excel with all structural elements present (top header fields, table columns, footer note, signature block with all three roles).\n- 1.5: Valid Excel but missing 1-2 minor elements (e.g., one header field or a single signature role label).\n- 1.0: Valid Excel but missing multiple required elements (e.g., 3-4 missing fields/columns/labels).\n- 0.5: Excel present but not structured as a usable form (no clear headers/table/signature areas).\n- 0.0: No Excel form found.\n\nBe flexible with exact phrasing but ensure the user could realistically fill and print this as an Exchange Authorization form.", "expectation": "An Excel form with a clear header section, a detail table, a footer note about prepaying freight/restocking fees, and a three-role signature block."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification (Code + Lightweight LLM via shape from Stage 1)", "description": "Deterministic checks for presence of mandated policy details in the document and structural fields/labels in the form. Uses flexible text matching across outputs.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Program Policy Elements Present in Document", "description": "Verify the DOCX/PDF includes all required policy statements and procedural steps.", "weight": 2.0, "code": "import re\nfrom typing import List, Tuple\n\ndef evaluate(workflow, context):\n    try:\n        # Gather all documents (PDF/DOCX)\n        docs = [r for r in context.get_all_outputs() if getattr(r, 'is_document', False)]\n        if not docs:\n            return 0.0, 'No document (PDF/DOCX) outputs found.'\n\n        def read_doc(res):\n            text = ''\n            try:\n                # Try DOCX first\n                text = context.files.read_docx_text(res.id)\n            except Exception:\n                try:\n                    text = context.files.read_pdf_text(res.id)\n                except Exception:\n                    text = ''\n            return text or ''\n\n        full_text = ' '.join([read_doc(d) for d in docs])\n        t = full_text.lower()\n\n        checks = []\n        def add_check(name, passed):\n            checks.append((name, bool(passed)))\n\n        # 1) Exchange, not return; credits only for replacement\n        add_check('Exchange program (not returns)', (('exchange' in t) and (('not a return' in t) or ('not returns' in t) or ('exchange only' in t) or ('this is not a return' in t))))\n        add_check('Credits only for replacement merchandise', (('credit' in t) and ('replacement' in t) and ('only' in t)))\n\n        # 2) Orders must be placed when EA number is granted\n        ea_patterns = [\n            r'orders?\\s+.*(must|should)\\s+.*placed\\s+.*(when|once|at the time).*exchange authorization',\n            r'orders?\\s+.*(must|should)\\s+.*placed\\s+.*(when|once|at the time).*ea number'\n        ]\n        add_check('Order placed when EA number granted', any(re.search(p, t) for p in ea_patterns))\n\n        # 3) Only credit-worthy customers\n        add_check('Only credit-worthy customers', ('credit-worthy' in t) or ('creditworthy' in t) or ('credit worthiness' in t) or ('credit approval' in t))\n\n        # 4) One time per season\n        add_check('Limit: one time per season', ('one time per season' in t) or ('once per season' in t) or ('one-time per season' in t))\n\n        # 5) Process steps\n        add_check('Process: submit inventory on Exchange Authorization form', (('submit' in t) and ('inventory' in t) and (('exchange authorization form' in t) or ('authorization form' in t))))\n        add_check('Process: email to assigned sales rep', (('email' in t) and (('sales rep' in t) or ('sales representative' in t) or ('assigned rep' in t))))\n        add_check('Process: rep returns form with EA number', (('rep' in t or 'sales representative' in t) and (('return the form' in t) or ('returns the form' in t) or ('provide' in t) or ('issue' in t)) and (('exchange authorization number' in t) or ('ea number' in t))))\n        add_check('Process: print copy in return shipment', ((('print' in t) or ('printed' in t)) and (('return shipment' in t) or ('include a copy' in t) or ('place a copy' in t))))\n        add_check('Process: write EA number on outside of box', ((('outside of the box' in t) or ('outside of box' in t)) and (('exchange authorization number' in t) or ('ea number' in t))))\n\n        # 6) Fees\n        add_check('Customer pays freight', (('freight' in t) and (('customer' in t) and (('pay' in t) or ('prepay' in t) or ('paid by customer' in t)))))\n        add_check('$5 per pair restocking fee', (('$5' in t) and ('per pair' in t) and ('restocking' in t)))\n        add_check('Restocking fees charged to customer account', (('restocking' in t) and ('charged' in t) and ('account' in t) and (\"customer's\" in t or 'customer account' in t or 'customers account' in t)))\n\n        # 7) Warehouse contact\n        add_check('Warehouse address present', ('555 waters avenue' in t) and ('austin' in t) and ('78726' in t))\n        add_check('Warehouse phone present', ('455-864-3867' in t))\n\n        # 8) Effective date\n        add_check('Effective date July 1, 2025', ('july 1, 2025' in t))\n\n        total = len(checks)\n        passed = sum(1 for _, p in checks if p)\n        score = (passed / total) * 2.0\n        missing = [name for name, p in checks if not p]\n        feedback = f\"Document checks passed {passed}/{total}. Missing: {', '.join(missing) if missing else 'None'}\"\n        return score, feedback\n    except Exception as e:\n        return 0.0, f'Error evaluating document policy elements: {e}'\n"}, {"type": "code", "name": "Excel Form \u2013 Header/Table/Footer/Signature Elements", "description": "Verify the Excel form includes required header fields, table columns, footer notes, and signature roles.", "weight": 2.0, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    try:\n        # Gather all spreadsheets\n        sheets = [r for r in context.get_all_outputs() if getattr(r, 'is_spreadsheet', False)]\n        if not sheets:\n            return 0.0, 'No Excel spreadsheet outputs found.'\n\n        def extract_all_text_from_excel(res):\n            try:\n                path = context.files.get_path(res.id)\n                xls = pd.ExcelFile(path)\n                text_parts = []\n                for sn in xls.sheet_names:\n                    try:\n                        df = pd.read_excel(path, sheet_name=sn, dtype=str, header=None)\n                        df = df.fillna('')\n                        text_parts.append(' '.join(df.astype(str).values.flatten().tolist()))\n                    except Exception:\n                        continue\n                return (' '.join(text_parts)).lower()\n            except Exception:\n                # Fallback to single sheet read via helper if available\n                try:\n                    df = context.files.read_excel(res.id)\n                    df = df.fillna('')\n                    return ' '.join(df.astype(str).values.flatten().tolist()).lower()\n                except Exception:\n                    return ''\n\n        full = ' '.join([extract_all_text_from_excel(r) for r in sheets])\n        t = full.lower()\n\n        checks = []\n        def add(name, cond):\n            checks.append((name, bool(cond)))\n\n        # Header fields\n        add('Header: Customer Name', ('customer name' in t))\n        add('Header: Address', ('address' in t))\n        add('Header: Phone Number', ('phone' in t or 'phone number' in t or 'tel' in t))\n        add('Header: Customer Number', ('customer number' in t or 'cust #' in t or 'cust no' in t or 'customer #' in t))\n        add('Header: Exchange Authorization Number field', ('exchange authorization' in t or 'ea number' in t))\n        add('Header: Date of Return Request', ('date' in t and ('return request' in t or 'request date' in t or 'date of return' in t)))\n\n        # Table columns\n        add('Table: Style/Color', ('style/color' in t or 'style color' in t or ('style' in t and 'color' in t)))\n        add('Table: Style Name', ('style name' in t))\n        add('Table: Pairs Shipped', ('pairs shipped' in t))\n        add('Table: Pairs to be Returned', ('pairs to be returned' in t or 'pairs returned' in t or 'pairs to return' in t))\n\n        # Footer notes\n        add('Footer: Prepay freight note', (('prepay freight' in t) or ('pre-paid freight' in t) or ('prepaid freight' in t)))\n        add('Footer: Restocking fee note', ('restocking fee' in t))\n\n        # Signature block\n        add('Signature: Sales Representative', (('sales representative' in t) or ('sales rep' in t)))\n        add('Signature: GM', (('gm' in t) or ('general manager' in t)))\n        add('Signature: Sales Manager', ('sales manager' in t))\n        add('Signature fields: Signature + Date labels', (('signature' in t) and ('date' in t)))\n\n        total = len(checks)\n        passed = sum(1 for _, p in checks if p)\n        score = (passed / total) * 2.0\n        missing = [name for name, p in checks if not p]\n        feedback = f\"Excel form checks passed {passed}/{total}. Missing: {', '.join(missing) if missing else 'None'}\"\n        return score, feedback\n    except Exception as e:\n        return 0.0, f'Error evaluating Excel form: {e}'\n"}, {"type": "code", "name": "Cross-Consistency: EA + Fees across Doc and Form", "description": "Verify both the document and the form consistently reference Exchange Authorization and fee concepts.", "weight": 1.0, "code": "import pandas as pd\n\ndef evaluate(workflow, context):\n    try:\n        docs = [r for r in context.get_all_outputs() if getattr(r, 'is_document', False)]\n        xlxs = [r for r in context.get_all_outputs() if getattr(r, 'is_spreadsheet', False)]\n        if not docs or not xlxs:\n            return 0.0, 'Need both document and spreadsheet to check consistency.'\n\n        def read_doc_text(res):\n            try:\n                return (context.files.read_docx_text(res.id) or '')\n            except Exception:\n                try:\n                    return (context.files.read_pdf_text(res.id) or '')\n                except Exception:\n                    return ''\n        doc_text = ' '.join([read_doc_text(d) for d in docs]).lower()\n\n        def read_excel_text(res):\n            try:\n                path = context.files.get_path(res.id)\n                xls = pd.ExcelFile(path)\n                parts = []\n                for sn in xls.sheet_names:\n                    try:\n                        df = pd.read_excel(path, sheet_name=sn, header=None, dtype=str).fillna('')\n                        parts.append(' '.join(df.astype(str).values.flatten().tolist()))\n                    except Exception:\n                        continue\n                return (' '.join(parts)).lower()\n            except Exception:\n                try:\n                    df = context.files.read_excel(res.id)\n                    df = df.fillna('')\n                    return ' '.join(df.astype(str).values.flatten().tolist()).lower()\n                except Exception:\n                    return ''\n        xl_text = ' '.join([read_excel_text(x) for x in xlxs]).lower()\n\n        checks = []\n        def add(name, cond):\n            checks.append((name, bool(cond)))\n\n        # Both mention Exchange Authorization/EA number\n        add('Both mention Exchange Authorization', (('exchange authorization' in doc_text or 'ea number' in doc_text) and ('exchange authorization' in xl_text or 'ea number' in xl_text)))\n        # Both mention restocking fee concept\n        add('Both mention restocking fee', (('restocking fee' in doc_text or 'restocking' in doc_text) and ('restocking fee' in xl_text or 'restocking' in xl_text)))\n        # Both mention freight cost borne by customer/prepay (doc must; form footer should)\n        add('Both mention freight/prepay', (('freight' in doc_text) and ('freight' in xl_text)))\n\n        total = len(checks)\n        passed = sum(1 for _, p in checks if p)\n        score = (passed / total) * 1.0\n        missing = [name for name, p in checks if not p]\n        feedback = f\"Cross-consistency checks passed {passed}/{total}. Missing: {', '.join(missing) if missing else 'None'}\"\n        return score, feedback\n    except Exception as e:\n        return 0.0, f'Error evaluating cross-consistency: {e}'\n"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Quality and Professionalism (LLM)", "description": "Holistic assessment of clarity, professionalism, and usability for the target audience (management and field sales reps/customers).", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Overview Document \u2013 Communication Quality", "description": "Assess clarity, tone, concision, and professional presentation of the one-page overview for management approval and field use.", "weight": 1.0, "judge_prompt": "Evaluate the overall quality of the one-page Gravon Shoes Exchange Program overview (DOCX/PDF). Consider:\n- Clarity and concision (easy to scan; avoids unnecessary detail)\n- Professional tone and formatting (clear headings, bullets, whitespace)\n- Audience appropriateness (management approval + field distribution)\n- Actionability (process steps are understandable at a glance)\n- Risk/ambiguity reduction (policy statements are unambiguous)\nScoring:\n- 1.0 Excellent: Clear, concise, professional, immediately actionable.\n- 0.7 Good: Minor issues but overall professional and clear.\n- 0.4 Fair: Readable but cluttered, ambiguous, or poorly structured.\n- 0.1 Poor: Hard to follow, unprofessional layout.\n- 0.0 Unusable.", "expectation": "A concise, professionally formatted one-page overview with clear headings and bullet points that a manager can approve quickly."}, {"type": "llm_judge", "name": "Excel Form \u2013 Usability and Field Readiness", "description": "Assess whether the form is easy to fill, print, and use by retailers and sales reps.", "weight": 1.0, "judge_prompt": "Evaluate the Excel Exchange Authorization form for usability:\n- Logical grouping of fields (header info, line items, footer notes, signatures)\n- Clear labels and sufficient space for entries\n- Print readiness (clean layout, page fit, readable fonts)\n- Minimizes confusion or rework (unambiguous column headers and signatures)\nScoring:\n- 1.0 Excellent: Intuitive, print-ready, clearly labeled.\n- 0.7 Good: Minor layout/label issues but very usable.\n- 0.4 Fair: Several ambiguities or cramped layout.\n- 0.1 Poor: Confusing or impractical.\n- 0.0 Unusable.", "expectation": "A clean, intuitive, print-ready Excel form that a retailer can complete without guidance."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "e14e32ba-d310-4d45-9b8a-6d73d0ece1ae", "rubric": {"category_name": "NYC Jewish Deli Research One Sheet", "rationale": "This rubric enforces a self-documenting, producer-ready research deliverable for sourcing 4\u20136 iconic NYC Jewish delicatessens. Stage 1 is an LLM-only gate that mandates an exact, verification-friendly document structure (DOCX/PDF with consistent field labels and embedded photos). Stage 2 combines code checks (counts, URL and address patterns, list structure) with one LLM cross-check for deli relevance and prior-media linkage. Stage 3 assesses professional presentation and producer-minded usefulness (clarity, link usability, production notes). The shape-first approach makes verification trivial and avoids brittle parsing.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate (LLM Only)", "description": "MANDATORY gate. Ensures the output is a properly structured DOCX/PDF one-sheet with 4\u20136 entries, consistent field labels, and embedded photos so later verification is trivial.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Structured One-Sheet Requirements", "description": "Check that the output is a DOCX or PDF titled exactly as requested and contains 4\u20136 consistently structured entries with required fields and embedded photos.", "weight": 8.0, "judge_prompt": "You are evaluating whether the candidate output satisfies the STRICT structure requirements below. Only assess presence/format, not content quality.\n\nFormat and Structure Requirements:\n- File must be a DOCX or PDF document (not plain text, not Excel).\n- The main title on the first page must read exactly: \"NYC Jewish Deli Research One Sheet\".\n- The document must contain 4\u20136 distinct restaurant entries. Each entry must use these exact field labels (each label visibly present):\n  1) \"Restaurant Name:\"\n  2) \"Location:\"\n  3) \"Business Hours:\"\n  4) \"Website:\"\n  5) \"Notable Dishes:\"\n  6) \"Notes:\"\n  7) \"Media Links:\"\n- Each entry must include an embedded photo of the establishment (an actual image, not just a URL).\n- For \"Website:\" the entry should show a URL.\n- For \"Media Links:\" the entry should list one or more URLs OR explicitly say something like \"None found\" with a brief note.\n- Entries should be visually separated and consistently formatted so the labels are easy to identify.\n\nScoring (0 to 8 points):\n- 8.0: Valid DOCX/PDF, exact title present, 4\u20136 entries, and EVERY entry includes ALL seven labels AND an embedded photo; website URLs shown; media links listed or clearly marked \"None found\" with a brief note.\n- 7.0\u20137.5: One minor miss across the entire doc (e.g., one entry missing one label OR one entry missing its photo) but overall structure is correct.\n- 5.5\u20136.5: Structure largely followed but with several small misses (e.g., 2\u20133 fields missing across entries or a couple of entries with weak labeling); still has 4\u20136 entries.\n- 3.0\u20135.0: Wrong count of entries (e.g., 3 or 7), or many labels missing/merged; inconsistent structure that makes verification difficult.\n- 0.0: Not a DOCX/PDF, missing the exact title, or no recognizable labeled structure.\n\nImportant: Only check structure and presence. Do NOT judge the correctness of facts, the quality of photos, or whether the venues are truly Jewish delis.", "expectation": "A professional DOCX/PDF one-sheet titled exactly as specified, with 4\u20136 entries using the seven exact labels and embedded photos, enabling easy automated verification later."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Consistency Verification (Code + LLM)", "description": "Now that the one-sheet has a fixed structure, verify with code and a light LLM cross-check: entry counts, URLs, NYC location patterns, dish lists, and basic deli/media plausibility.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Entry Count Between 4 and 6", "description": "Verify the document includes 4\u20136 entries by counting occurrences of the label \"Restaurant Name:\" (or fallback \"Name:\"). Returns a ratio in [0,1].", "weight": 1.6, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    # Get text from DOCX or PDF\n    text = \"\"\n    try:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = context.files.read_pdf_text(output.id)\n    except Exception:\n        text = \"\"\n    if not text:\n        return 0.0\n    # Count entries via label occurrences\n    pattern = re.compile(r'^\\s*(restaurant\\s+name|name)\\s*:\\s*(.+)$', re.I | re.M)\n    names = pattern.findall(text)\n    n = len(names)\n    if 4 <= n <= 6:\n        return 1.0\n    elif n in (3, 7):\n        return 0.5\n    else:\n        return 0.0\n"}, {"type": "code", "name": "Exact Title Present", "description": "Check the document text includes the exact title string: \"NYC Jewish Deli Research One Sheet\".", "weight": 0.4, "code": "def evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = context.files.read_pdf_text(output.id)\n    except Exception:\n        text = \"\"\n    if not text:\n        return 0.0\n    return 1.0 if 'nyc jewish deli research one sheet' in text.lower() else 0.0\n"}, {"type": "code", "name": "Website URL per Entry", "description": "For each entry, the \"Website:\" field should include a URL. Score = proportion of entries with a detected URL on the Website line.", "weight": 1.6, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = context.files.read_pdf_text(output.id)\n    except Exception:\n        text = \"\"\n    if not text:\n        return 0.0\n    # Count entries\n    name_pat = re.compile(r'^\\s*(restaurant\\s+name|name)\\s*:\\s*(.+)$', re.I | re.M)\n    n = len(name_pat.findall(text))\n    # Find Website lines\n    web_pat = re.compile(r'^\\s*website\\s*:\\s*(.+)$', re.I | re.M)\n    web_lines = web_pat.findall(text)\n    url_pat = re.compile(r'https?://[^\\s)]+', re.I)\n    urls_per_line = [1 if url_pat.search(line) else 0 for line in web_lines]\n    total_with_url = sum(urls_per_line)\n    denom = n if n > 0 else max(1, len(web_lines))\n    ratio = min(1.0, total_with_url / denom) if denom else 0.0\n    return ratio\n"}, {"type": "code", "name": "Media Links Coverage (Video Platforms or Documented None)", "description": "Score based on how many entries have a Media Links field that includes at least one recognizable video/social video URL (YouTube, Facebook, Vimeo, Instagram, TikTok) OR explicitly says none found. Partial credit for partial coverage.", "weight": 1.4, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = context.files.read_pdf_text(output.id)\n    except Exception:\n        text = \"\"\n    if not text:\n        return 0.0\n    # Entries count\n    name_pat = re.compile(r'^\\s*(restaurant\\s+name|name)\\s*:\\s*(.+)$', re.I | re.M)\n    n = len(name_pat.findall(text))\n    if n == 0:\n        return 0.0\n    # Extract Media Links lines/blocks (single-line fallback)\n    media_line_pat = re.compile(r'^\\s*media\\s*links\\s*:\\s*(.*)$', re.I | re.M)\n    media_lines = media_line_pat.findall(text)\n    # Known video domains\n    video_domains = (\n        'youtube.com', 'youtu.be', 'facebook.com', 'fb.watch', 'vimeo.com',\n        'instagram.com', 'tiktok.com'\n    )\n    url_pat = re.compile(r'https?://[^\\s)]+', re.I)\n    covered = 0\n    for line in media_lines[:n]:  # assume one line per entry in order\n        line_l = line.lower()\n        if 'none' in line_l and 'found' in line_l:\n            covered += 0.5  # documented none\n            continue\n        urls = url_pat.findall(line)\n        has_video = any(any(dom in u.lower() for dom in video_domains) for u in urls)\n        if has_video:\n            covered += 1\n    ratio = min(1.0, covered / n)\n    return ratio\n"}, {"type": "code", "name": "NYC Location Pattern Match", "description": "For each entry's Location line, check for NYC evidence (borough names with NY, \"New York, NY\", or ZIP codes 10000\u201311699). Score is proportion of entries matching.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = context.files.read_pdf_text(output.id)\n    except Exception:\n        text = \"\"\n    if not text:\n        return 0.0\n    # Count entries and extract locations\n    name_pat = re.compile(r'^\\s*(restaurant\\s+name|name)\\s*:\\s*(.+)$', re.I | re.M)\n    n = len(name_pat.findall(text))\n    loc_pat = re.compile(r'^\\s*location\\s*:\\s*(.+)$', re.I | re.M)\n    loc_lines = loc_pat.findall(text)\n    if n == 0 or not loc_lines:\n        return 0.0\n    boroughs = ['manhattan', 'brooklyn', 'queens', 'bronx', 'staten island']\n    matches = 0\n    for i, loc in enumerate(loc_lines[:max(n, len(loc_lines))]):\n        l = loc.lower()\n        has_borough = any(b in l for b in boroughs) and (' ny' in l or ', ny' in l)\n        has_ny = 'new york, ny' in l or ', ny ' in l or l.strip().endswith(', ny')\n        has_zip = re.search(r'\\b1(0\\d{3}|1[0-6]\\d{2})\\b', l) is not None  # 10000\u201311699 approx\n        if has_borough or has_ny or has_zip:\n            matches += 1\n    denom = n\n    ratio = min(1.0, matches / denom) if denom else 0.0\n    return ratio\n"}, {"type": "code", "name": "Notable Dishes Are Listed (2+ items)", "description": "For each entry, the Notable Dishes block should list at least two items (bullets or comma/semicolon separated). Score by the proportion of entries meeting this threshold.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = context.files.read_pdf_text(output.id)\n    except Exception:\n        text = \"\"\n    if not text:\n        return 0.0\n    # Count entries\n    name_pat = re.compile(r'^\\s*(restaurant\\s+name|name)\\s*:\\s*(.+)$', re.I | re.M)\n    n = len(name_pat.findall(text))\n    if n == 0:\n        return 0.0\n    # Capture Notable Dishes blocks (best-effort single-line fallback)\n    block_pat = re.compile(r'(?im)^\\s*notable\\s*dishes\\s*:\\s*(.*?)(?=^\\s*(restaurant\\s+name|name|location|business\\s*hours|website|notes|media\\s*links)\\s*:|\\Z)')\n    blocks = [b[0] if isinstance(b, tuple) else b for b in block_pat.findall(text)]\n    good = 0\n    for blk in blocks[:n]:\n        # Normalize bullets to commas\n        b = blk.replace('\\u2022', ',').replace('\u2022', ',')\n        lines = [ln.strip('-\u2022* \\t') for ln in b.splitlines() if ln.strip()]\n        if len(lines) <= 1:\n            # If single line, split on commas/semicolons\n            items = [x.strip() for x in re.split(r'[;,]', b) if x.strip()]\n        else:\n            items = [x for x in lines if x]\n        items = [x for x in items if len(x) >= 3]\n        if len(items) >= 2:\n            good += 1\n    ratio = min(1.0, good / n)\n    return ratio\n"}, {"type": "llm_judge", "name": "Deli + Media Plausibility Check", "description": "LLM cross-check: Do the entries appear to be NYC Jewish delicatessens, and do Media Links seem to be relevant prior interviews/segments or similar digital coverage?", "weight": 1.0, "judge_prompt": "Evaluate plausibility and consistency ONLY (not aesthetics):\n- Based on names, locations, and notes, do the 4\u20136 entries appear to be Jewish delicatessens in NYC (Manhattan, Brooklyn, Queens, Bronx, Staten Island)?\n- Do Media Links look relevant to prior video/interview/digital coverage (e.g., YouTube, Facebook, Vimeo, Instagram/TikTok clips, or similar)?\n- If an entry says \"None found\" for media, is that stated clearly and plausibly in context?\n\nScoring (0\u20131):\n- 1.0: All entries plausibly match NYC Jewish delis; media links appear relevant for most entries; any \"None found\" is used sparingly and explained.\n- 0.5: Mixed: 1 entry seems questionable as a Jewish deli or media links look off-topic for a couple entries.\n- 0.0: Several entries are not Jewish delis in NYC OR media links are largely irrelevant.\n", "expectation": "All entries are clearly NYC Jewish delis; media links are relevant prior coverage or noted as not found with explanation."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Presentation and Producer Utility (LLM)", "description": "Professional polish and producer-minded usefulness: clear layout, consistent labeling, clickable links, and production-relevant notes.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Presentation and Readability", "description": "Assess formatting clarity, section consistency, and link usability for a producer-ready one-sheet.", "weight": 2.0, "judge_prompt": "Assess presentation quality (not content correctness):\n- Is the document professionally formatted and easy to scan (consistent labels, spacing, alignment)?\n- Are photos placed cleanly near their corresponding entries? Are entries visually distinct?\n- Are URLs clearly visible and likely clickable? Is the title prominent?\n\nScoring (0\u20132):\n- 2.0: Clean, consistent, highly readable one-sheet; links obvious; photos well placed.\n- 1.0: Generally readable with minor inconsistencies (spacing, label alignment, or a few messy link placements).\n- 0.0: Cluttered or inconsistent; hard to scan; links or labels are difficult to identify.", "expectation": "A clean, scannable one-sheet with consistent sections, obvious links, and well-placed photos."}, {"type": "llm_judge", "name": "Producer Utility and Insight", "description": "Evaluate whether notes and curation are helpful for a shoot producer (e.g., permissions, ambiance, constraints).", "weight": 2.0, "judge_prompt": "Judge the producer utility of the content:\n- Do the Notes mention useful production considerations (e.g., filming friendliness/permissions, space/ambiance, signature dishes for B-roll, peak hours, neighborhood context, cash-only policies, noise/lighting constraints)?\n- Does the selection show useful variety (e.g., neighborhoods/boroughs, styles of Jewish deli) to give options?\n\nScoring (0\u20132):\n- 2.0: Notes provide clear producer-relevant insights and the curation offers useful variety.\n- 1.0: Some helpful notes but limited depth or variety.\n- 0.0: Minimal or generic notes; little practical value to a producer.", "expectation": "Concise, producer-relevant notes with practical shoot considerations and a varied, thoughtful selection."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "5a2d70da-0a42-4a6b-a3ca-763e03f070a5", "rubric": {"category_name": "Manufacturing CNC Integration \u2013 Cover Plate Launch (Mechanical Engineering)", "rationale": "This rubric enforces a self-documenting, verifiable workflow for two Excel workbooks: a Master Tool List (budgeted, with links and tax) and a Cover Plate Manufacturing Steps plan. Stage 1 (LLM-only) is a strict shape gate to guarantee evaluable structure. Stage 2 uses code rules to verify arithmetic, budget compliance, tax rate consistency, cross-references between the two files, and sequence integrity. Stage 3 evaluates professional quality and plausibility for a manufacturing audience.", "max_total_score": 11.0, "stages": [{"name": "Stage 1 \u2013 Structure and Format Gate (LLM-only)", "description": "Gate: Verify there are exactly two Excel workbooks with the mandated structure so later checks are trivial.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Workbook Presence and Structural Completeness", "description": "Confirm both required Excel files exist and contain the specified sheets/sections and columns.", "weight": 4.0, "judge_prompt": "You are the Stage 1 gate. Only check structure and format, not correctness of numbers.\n\nTask requires TWO Excel workbooks:\n1) Master Tool List\n2) Cover Plate Manufacturing Steps\n\nCheck the candidate outputs for the following. Be flexible with exact sheet names and minor header wording, but the STRUCTURE must be present.\n\nA) Master Tool List workbook (Excel):\n- Contains a main items sheet (e.g., \"Items\", \"Master Tool List\", or similar) with a tabular list including columns:\n  \u2022 Type of Equipment (values like work holding, tool holder, cutting tool)\n  \u2022 Short Description (or Description)\n  \u2022 Manufacturer\n  \u2022 Manufacturer Part Number (MPN)\n  \u2022 Quantity\n  \u2022 Cost Each\n  \u2022 Cost Total\n  \u2022 Page Link (URL for purchase)\n- Includes a visible Sub-Total (pre-sales tax) value.\n- Includes a visible Grand Total (post-sales tax) value.\n- Conditional requirement: If the pre-tax Sub-Total exceeds $7,500, there must be a drafted budget increase email either:\n  \u2022 As an additional sheet in the same workbook (e.g., \"Budget Request Email\"), OR\n  \u2022 As a separate PDF/DOCX/MD file in the outputs.\n  The email should read like an email (greeting, body with justification, requested amount).\n\nB) Cover Plate Manufacturing Steps workbook (Excel):\n- Has a clearly labeled header area at the top with fields:\n  \u2022 Part Name\n  \u2022 Material Type\n  \u2022 Stock Size (in)\n  \u2022 Number of Operations\n  \u2022 Part Manufacturing Volume (one month quantity)\n- Contains a steps table with columns:\n  \u2022 Step Order Number\n  \u2022 Operation Number (per orientation)\n  \u2022 Cutting Tool (referencing items from the Master Tool List by name/MPN)\n  \u2022 Tool Holder(s) (may list multiple holders)\n\nFORMAT REQUIREMENTS:\n- Both deliverables must be Excel files (XLSX preferred). Do not accept CSVs or PDFs in place of these two.\n- The tables must be visible (not fully hidden/protected), with headers legible.\n\nSCORING (return a single score 0.0\u20134.0):\n- 4.0: Both Excel workbooks present with all required structural elements above.\n- 3.0: Both Excel files present; one minor structural element missing (e.g., a slightly renamed/misaligned column) but tables and required fields are clearly present.\n- 2.0: One Excel file is structurally complete; the other is missing multiple required elements (e.g., missing key columns or header block) but is clearly attempting the right format.\n- 1.0: Only one of the two required Excel files present and structured; the other missing or wrong format.\n- 0.0: Wrong format (not Excel) or severe structural omissions preventing verification.\n\nIMPORTANT: Do not judge numerical correctness or content quality; only verify presence/structure. If the Sub-Total appears to exceed budget, only check there is an email as described; do not calculate exact amounts.", "expectation": "Two well-structured Excel workbooks: a Master Tool List with specified columns and subtotal/grand total, and a Manufacturing Steps workbook with the required header and steps table; conditional budget email if over $7,500 pre-tax."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification: Budget, Arithmetic, Consistency, Cross-Reference", "description": "Deterministic checks using code for arithmetic integrity, budget/tax consistency, steps sequencing, and cross-referencing tools/holders to the Master Tool List.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Master Tool List \u2013 Table and Arithmetic Integrity", "description": "Verify required columns exist on an items sheet and per-row Cost Total \u2248 Quantity \u00d7 Cost Each.", "weight": 1.4, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    # Helper: find the Master Tool List workbook and items sheet\n    def normalize(s):\n        return re.sub(r\"[^a-z0-9]+\", \"\", str(s).strip().lower())\n\n    def find_items_table(resource):\n        try:\n            path = context.files.get_path(resource.id)\n            xls = pd.ExcelFile(path)\n        except Exception:\n            return None, None\n        required_map = {\n            'type': {'type','equipmenttype','category','equipment'},\n            'description': {'description','shortdescription','item','itemdescription','desc'},\n            'manufacturer': {'manufacturer','brand','maker'},\n            'mpn': {'manufacturerpartnumber','mpn','partnumber','sku'},\n            'qty': {'quantity','qty','qtytoorder','orderqty'},\n            'cost_each': {'costeach','unitcost','priceeach','unitprice','eachcost'},\n            'cost_total': {'costtotal','linetotal','extendedcost','totalcost','extprice','line total','total'},\n            'link': {'pagelink','link','urllink','purchaseurl','url','weblink'}\n        }\n        best = None\n        best_map = None\n        for sheet in xls.sheet_names:\n            try:\n                df = pd.read_excel(path, sheet_name=sheet)\n            except Exception:\n                continue\n            cols_norm = [normalize(c) for c in df.columns]\n            col_map = {}\n            for std, alts in required_map.items():\n                hit = None\n                for i, cn in enumerate(cols_norm):\n                    if cn in alts:\n                        hit = df.columns[i]\n                        break\n                col_map[std] = hit\n            # Require at least critical columns present for arithmetic\n            critical = ['qty','cost_each','cost_total']\n            if all(col_map.get(k) is not None for k in critical):\n                # Count how many of the full set we matched\n                score_cols = sum(1 for k,v in col_map.items() if v is not None)\n                if best is None or score_cols > sum(1 for k,v in (best_map or {}).items() if v is not None):\n                    best = df\n                    best_map = col_map\n        return best, best_map\n\n    # Locate a spreadsheet that looks like the Master Tool List\n    master = None\n    for r in context.get_all_outputs():\n        try:\n            if getattr(r, 'is_spreadsheet', False):\n                items_df, items_map = find_items_table(r)\n                if items_df is not None:\n                    master = (r, items_df, items_map)\n                    break\n        except Exception:\n            continue\n\n    if master is None:\n        return 0.0, \"Master Tool List items table not found.\"\n\n    r, df, m = master\n    # Clean rows (drop rows where all critical are NaN)\n    crit_cols = [m['qty'], m['cost_each'], m['cost_total']]\n    df2 = df.copy()\n    df2 = df2.dropna(how='all')\n    # Coerce numerics\n    def to_num(s):\n        try:\n            if isinstance(s, str):\n                s = s.replace(',', '').replace('$', '')\n            return float(s)\n        except Exception:\n            return np.nan\n    qty = df2[m['qty']].apply(to_num)\n    each = df2[m['cost_each']].apply(to_num)\n    total = df2[m['cost_total']].apply(to_num)\n    mask_valid = ~(qty.isna() | each.isna() | total.isna())\n    dfv = df2[mask_valid]\n    if len(dfv) == 0:\n        return 0.2, \"Columns present but no valid numeric rows for arithmetic.\"\n\n    calc = (qty[mask_valid] * each[mask_valid]).astype(float)\n    diff = (calc - total[mask_valid]).abs().fillna(np.inf)\n    ok_rows = (diff <= 0.02).sum()  # within 2 cents\n    frac_ok = ok_rows / max(1, len(dfv))\n\n    # Column coverage score (encourage presence of full set)\n    present_cols = sum(1 for k in ['type','description','manufacturer','mpn','qty','cost_each','cost_total','link'] if m.get(k) is not None)\n    coverage = present_cols / 8.0\n\n    # Weighted: 30% columns coverage, 70% arithmetic correctness across rows\n    score = 0.3 * coverage + 0.7 * frac_ok\n    return max(0.0, min(1.0, score))"}, {"type": "code", "name": "Budget and Tax Consistency (Suffolk County)", "description": "Validate subtotal vs. grand total and infer applied tax rate (~8.625% in Suffolk County, NY). Also check budget compliance (\u2264 $7,500 pre-tax or post-tax).", "weight": 1.2, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    def normalize(s):\n        return re.sub(r\"[^a-z0-9]+\", \"\", str(s).strip().lower())\n\n    def to_num(s):\n        try:\n            if isinstance(s, str):\n                s = s.replace(',', '').replace('$', '')\n            return float(s)\n        except Exception:\n            return np.nan\n\n    def find_master_and_items():\n        def find_items_table(resource):\n            try:\n                path = context.files.get_path(resource.id)\n                xls = pd.ExcelFile(path)\n            except Exception:\n                return None, None\n            required_map = {\n                'qty': {'quantity','qty','qtytoorder','orderqty'},\n                'cost_each': {'costeach','unitcost','priceeach','unitprice','eachcost'},\n                'cost_total': {'costtotal','linetotal','extendedcost','totalcost','extprice','line total','total'}\n            }\n            best = None\n            best_map = None\n            for sheet in xls.sheet_names:\n                try:\n                    df = pd.read_excel(path, sheet_name=sheet)\n                except Exception:\n                    continue\n                cols_norm = [normalize(c) for c in df.columns]\n                col_map = {}\n                for std, alts in required_map.items():\n                    hit = None\n                    for i, cn in enumerate(cols_norm):\n                        if cn in alts:\n                            hit = df.columns[i]\n                            break\n                    col_map[std] = hit\n                if all(col_map.get(k) is not None for k in required_map):\n                    best = (df, col_map)\n                    break\n            return best\n\n        # Pick first spreadsheet with items table\n        for r in context.get_all_outputs():\n            if getattr(r, 'is_spreadsheet', False):\n                res = find_items_table(r)\n                if res is not None:\n                    return r, res[0], res[1]\n        return None, None, None\n\n    def scan_named_amounts(resource, name_patterns):\n        # returns first matching numeric\n        try:\n            path = context.files.get_path(resource.id)\n            xls = pd.ExcelFile(path)\n        except Exception:\n            return None\n        for sheet in xls.sheet_names:\n            try:\n                df = pd.read_excel(path, sheet_name=sheet, header=None)\n            except Exception:\n                continue\n            arr = df.values\n            rows, cols = arr.shape if hasattr(arr, 'shape') else (0,0)\n            for r in range(rows):\n                for c in range(cols):\n                    val = arr[r, c]\n                    if isinstance(val, str):\n                        nv = normalize(val)\n                        for pat in name_patterns:\n                            npat = normalize(pat)\n                            # require all words of pattern appear in any order\n                            if all(w in nv for w in re.findall(r\"[a-z0-9]+\", npat)):\n                                # check right, below, or same cell numeric\n                                neigh = []\n                                if c+1 < cols: neigh.append(arr[r, c+1])\n                                if r+1 < rows: neigh.append(arr[r+1, c])\n                                neigh.append(arr[r, c])\n                                for n in neigh:\n                                    num = to_num(n)\n                                    if pd.notna(num) and np.isfinite(num):\n                                        return float(num)\n        return None\n\n    master_res, items_df, m = find_master_and_items()\n    if master_res is None:\n        return 0.0, \"Master Tool List not found.\"\n\n    # Compute subtotal from items\n    qty = items_df[m['qty']].apply(to_num)\n    each = items_df[m['cost_each']].apply(to_num)\n    total = items_df[m['cost_total']].apply(to_num)\n    mask_valid = ~(qty.isna() | each.isna() | total.isna())\n    computed_subtotal = float((total[mask_valid]).sum())\n\n    # Try to find displayed Sub-Total and Grand Total\n    sub_patterns = ['sub-total','subtotal','pre-tax total','pre sales tax','pre tax total']\n    grand_patterns = ['grand total','post-tax total','post sales tax','final total','grandtotal','post tax total']\n\n    displayed_sub = scan_named_amounts(master_res, sub_patterns)\n    displayed_grand = scan_named_amounts(master_res, grand_patterns)\n\n    # Budget compliance: pass if either computed_subtotal <= 7500 OR displayed_grand <= 7500 (if available)\n    budget_ok = computed_subtotal <= 7500.0 or (displayed_grand is not None and displayed_grand <= 7500.0)\n\n    # Tax rate evaluation (only if both values present and reasonable)\n    tax_score = 0.0\n    target_tax = 0.08625  # Suffolk County, NY\n    if displayed_sub is None:\n        displayed_sub = computed_subtotal\n    if displayed_sub > 0 and displayed_grand is not None and displayed_grand >= displayed_sub:\n        implied = (displayed_grand - displayed_sub) / displayed_sub\n        diff = abs(implied - target_tax)\n        if diff <= 0.015:  # within 1.5 percentage points\n            tax_score = 1.0\n        elif diff <= 0.03:\n            tax_score = 0.6\n        elif diff <= 0.05:\n            tax_score = 0.3\n        else:\n            tax_score = 0.0\n    else:\n        # If cannot infer, give partial credit if grand total >= subtotal\n        if displayed_grand is not None and displayed_sub is not None and displayed_grand >= displayed_sub:\n            tax_score = 0.3\n\n    # Combine: 60% tax consistency, 40% budget compliance\n    score = 0.6 * tax_score + 0.4 * (1.0 if budget_ok else 0.0)\n    return max(0.0, min(1.0, score))"}, {"type": "code", "name": "Conditional Budget Overage Email Presence", "description": "If budget is exceeded (> $7,500 pre- and post-tax), verify a drafted email exists (sheet or separate PDF/DOCX/MD) requesting budget increase with justification.", "weight": 0.4, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    def normalize(s):\n        return re.sub(r\"[^a-z0-9]+\", \"\", str(s).strip().lower())\n\n    def to_num(s):\n        try:\n            if isinstance(s, str):\n                s = s.replace(',', '').replace('$', '')\n            return float(s)\n        except Exception:\n            return np.nan\n\n    def find_master_and_totals():\n        def find_items_table(resource):\n            try:\n                path = context.files.get_path(resource.id)\n                xls = pd.ExcelFile(path)\n            except Exception:\n                return None, None\n            best = None\n            best_map = None\n            required_map = {\n                'qty': {'quantity','qty','qtytoorder','orderqty'},\n                'cost_each': {'costeach','unitcost','priceeach','unitprice','eachcost'},\n                'cost_total': {'costtotal','linetotal','extendedcost','totalcost','extprice','line total','total'}\n            }\n            for sheet in xls.sheet_names:\n                try:\n                    df = pd.read_excel(path, sheet_name=sheet)\n                except Exception:\n                    continue\n                cols_norm = [normalize(c) for c in df.columns]\n                col_map = {}\n                for std, alts in required_map.items():\n                    hit = None\n                    for i, cn in enumerate(cols_norm):\n                        if cn in alts:\n                            hit = df.columns[i]\n                            break\n                    col_map[std] = hit\n                if all(col_map.get(k) is not None for k in required_map):\n                    return resource, df, col_map\n            return None, None, None\n\n        for r in context.get_all_outputs():\n            if getattr(r, 'is_spreadsheet', False):\n                res, df, m = find_items_table(r)\n                if res is not None:\n                    qty = df[m['qty']].apply(to_num)\n                    total = df[m['cost_total']].apply(to_num)\n                    mask = ~(qty.isna() | total.isna())\n                    subtotal = float(total[mask].sum())\n                    # Try find displayed grand total\n                    try:\n                        path = context.files.get_path(r.id)\n                        xls = pd.ExcelFile(path)\n                        grand = None\n                        for sheet in xls.sheet_names:\n                            raw = pd.read_excel(path, sheet_name=sheet, header=None).values\n                            for rr in range(raw.shape[0]):\n                                for cc in range(raw.shape[1]):\n                                    v = raw[rr, cc]\n                                    if isinstance(v, str) and ('grand' in v.lower() and 'total' in v.lower() or 'post' in v.lower() and 'tax' in v.lower() and 'total' in v.lower()):\n                                        # neighbor right or below\n                                        nb = []\n                                        if cc+1 < raw.shape[1]: nb.append(raw[rr, cc+1])\n                                        if rr+1 < raw.shape[0]: nb.append(raw[rr+1, cc])\n                                        nb.append(v)\n                                        for n in nb:\n                                            try:\n                                                n2 = str(n).replace(',', '').replace('$', '')\n                                                val = float(n2)\n                                                grand = val\n                                                break\n                                            except Exception:\n                                                pass\n                                        if grand is not None:\n                                            break\n                                if grand is not None:\n                                    break\n                        return r, subtotal, grand\n                    except Exception:\n                        return r, subtotal, None\n        return None, None, None\n\n    res, subtotal, grand = find_master_and_totals()\n    if res is None:\n        return 0.0\n\n    exceeds = (subtotal is not None and subtotal > 7500.0) and (grand is None or grand > 7500.0)\n    if not exceeds:\n        return 1.0  # Not required; give full credit\n\n    # Search for email sheet or separate document\n    def text_from_excel_sheet(resource):\n        try:\n            path = context.files.get_path(resource.id)\n            xls = pd.ExcelFile(path)\n            big = []\n            for sn in xls.sheet_names:\n                if 'email' in sn.lower() or 'budget' in sn.lower():\n                    df = pd.read_excel(path, sheet_name=sn, header=None)\n                    big.append(' '.join(map(str, df.fillna('').astype(str).values.flatten().tolist())))\n            return ' '.join(big)\n        except Exception:\n            return ''\n\n    email_text = ''\n    # Check all outputs for a budget/email doc or sheet\n    for r in context.get_all_outputs():\n        try:\n            if getattr(r, 'is_document', False):\n                if str(getattr(r, 'extension', '')).lower() == '.pdf':\n                    email_text += ' ' + (context.files.read_pdf_text(r.id) or '')\n                else:\n                    email_text += ' ' + (context.files.read_docx_text(r.id) or '')\n            elif getattr(r, 'is_text_format', False):\n                email_text += ' ' + (context.files.read_text(r.id) or '')\n            elif getattr(r, 'is_spreadsheet', False):\n                email_text += ' ' + text_from_excel_sheet(r)\n        except Exception:\n            continue\n\n    text_norm = email_text.lower()\n    ok = (len(text_norm) >= 150) and ('budget' in text_norm) and ('increase' in text_norm or 'additional' in text_norm) and ('request' in text_norm or 'approval' in text_norm)\n    return 1.0 if ok else 0.0"}, {"type": "code", "name": "Manufacturing Steps \u2013 Header and Sequence Validation", "description": "Verify header fields exist, parse Number of Operations, and check step ordering and operation counts.", "weight": 1.0, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    def normalize(s):\n        return re.sub(r\"[^a-z0-9]+\", \"\", str(s).strip().lower())\n\n    def find_steps_sheet(resource):\n        try:\n            path = context.files.get_path(resource.id)\n            xls = pd.ExcelFile(path)\n        except Exception:\n            return None, None, None\n        # Try to find steps table by headers\n        steps_cols_map = {\n            'step': {'stepordernumber','step','steporder','step #','stepno'},\n            'op': {'operationnumber','operation','op','opnumber','op #'},\n            'tool': {'cuttingtool','tool','toolname'},\n            'holder': {'toolholder','toolholders','holder','holders'}\n        }\n        best = None\n        best_cols = None\n        best_sheet = None\n        for sn in xls.sheet_names:\n            try:\n                df = pd.read_excel(path, sheet_name=sn)\n            except Exception:\n                continue\n            cols_norm = [normalize(c) for c in df.columns]\n            col_map = {}\n            for std, alts in steps_cols_map.items():\n                hit = None\n                for i, cn in enumerate(cols_norm):\n                    if cn in alts:\n                        hit = df.columns[i]\n                        break\n                col_map[std] = hit\n            if all(col_map.values()):\n                best = df\n                best_cols = col_map\n                best_sheet = sn\n                break\n        # Find header block (top section) with key fields\n        header_vals = {\n            'part name': None,\n            'material type': None,\n            'stock size': None,\n            'number of operations': None,\n            'part manufacturing volume': None\n        }\n        if best_sheet is not None:\n            try:\n                hdrdf = pd.read_excel(path, sheet_name=best_sheet, header=None, nrows=20)\n                for r in range(min(20, hdrdf.shape[0])):\n                    for c in range(min(6, hdrdf.shape[1])):\n                        v = hdrdf.iat[r, c]\n                        if isinstance(v, str):\n                            vn = v.strip().lower()\n                            if vn in header_vals and c+1 < hdrdf.shape[1]:\n                                header_vals[vn] = str(hdrdf.iat[r, c+1])\n            except Exception:\n                pass\n        return best, best_cols, header_vals\n\n    # Find a spreadsheet that looks like the Manufacturing Steps workbook\n    steps_df = None\n    header = None\n    colmap = None\n    for r in context.get_all_outputs():\n        if getattr(r, 'is_spreadsheet', False):\n            df, cm, hv = find_steps_sheet(r)\n            if df is not None:\n                steps_df, colmap, header = df, cm, hv\n                break\n    if steps_df is None:\n        return 0.0, \"Manufacturing Steps table not found.\"\n\n    # Header presence score\n    needed_hdr = ['part name','material type','stock size','number of operations','part manufacturing volume']\n    hdr_present = sum(1 for k in needed_hdr if header.get(k) not in (None, '', 'nan'))\n    hdr_score = hdr_present / len(needed_hdr)\n\n    # Sequence and operations checks\n    s_col = colmap['step']\n    o_col = colmap['op']\n    try:\n        steps_df2 = steps_df.dropna(how='all')\n        steps_vals = pd.to_numeric(steps_df2[s_col], errors='coerce').dropna().astype(int).tolist()\n        op_vals = pd.to_numeric(steps_df2[o_col], errors='coerce').dropna().astype(int).tolist()\n        # Step sequence roughly increasing by 1 from 1..N\n        if len(steps_vals) >= 1:\n            expected = list(range(1, len(steps_vals)+1))\n            seq_ok = sum(1 for a,b in zip(steps_vals, expected) if a==b) / max(1, len(expected))\n        else:\n            seq_ok = 0.0\n        # Number of distinct operations matches header (if provided)\n        ops_distinct = len(set(op_vals))\n        try:\n            ops_hdr = int(float(str(header.get('number of operations', 'nan')).strip()))\n            ops_match = 1.0 if ops_hdr == ops_distinct else (0.5 if ops_hdr>0 else 0.0)\n        except Exception:\n            ops_match = 0.5 if ops_distinct>0 else 0.0\n    except Exception:\n        seq_ok = 0.0\n        ops_match = 0.0\n\n    score = 0.4 * hdr_score + 0.3 * seq_ok + 0.3 * ops_match\n    return max(0.0, min(1.0, score))"}, {"type": "code", "name": "Cross-Reference: Steps tools/holders exist in Master Tool List", "description": "Check that tools and holders referenced in steps can be matched (by MPN or description tokens) to Master Tool List entries.", "weight": 0.8, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    def normalize(s):\n        return re.sub(r\"\\s+\", \" \", str(s).strip())\n    def normlow(s):\n        return normalize(s).lower()\n\n    # Find Master items table\n    def find_master_items():\n        for r in context.get_all_outputs():\n            if getattr(r, 'is_spreadsheet', False):\n                try:\n                    path = context.files.get_path(r.id)\n                    xls = pd.ExcelFile(path)\n                except Exception:\n                    continue\n                for sn in xls.sheet_names:\n                    try:\n                        df = pd.read_excel(path, sheet_name=sn)\n                    except Exception:\n                        continue\n                    cols = {c.lower(): c for c in df.columns}\n                    # Basic presence\n                    need = ['manufacturer', 'manufacturer part number', 'description']\n                    # allow flexible matching\n                    def get_col(name_options):\n                        for k in cols:\n                            nk = re.sub(r'[^a-z0-9]+','', k)\n                            for opt in name_options:\n                                if nk == re.sub(r'[^a-z0-9]+','', opt):\n                                    return cols[k]\n                        return None\n                    mpn_col = get_col(['manufacturer part number','mpn','part number','sku'])\n                    desc_col = get_col(['description','short description','item description','desc'])\n                    man_col = get_col(['manufacturer','brand','maker'])\n                    if mpn_col is not None and (desc_col is not None or man_col is not None):\n                        return r, df, mpn_col, desc_col, man_col\n        return None, None, None, None, None\n\n    # Find Steps table\n    def find_steps_table():\n        for r in context.get_all_outputs():\n            if getattr(r, 'is_spreadsheet', False):\n                try:\n                    path = context.files.get_path(r.id)\n                    xls = pd.ExcelFile(path)\n                except Exception:\n                    continue\n                for sn in xls.sheet_names:\n                    try:\n                        df = pd.read_excel(path, sheet_name=sn)\n                    except Exception:\n                        continue\n                    cols = {c.lower(): c for c in df.columns}\n                    def pick(options):\n                        for k in cols:\n                            nk = re.sub(r'[^a-z0-9]+','', k)\n                            for opt in options:\n                                if nk == re.sub(r'[^a-z0-9]+','', opt):\n                                    return cols[k]\n                        return None\n                    tool_col = pick(['cutting tool','tool','tool name'])\n                    holder_col = pick(['tool holder(s)','tool holder','holders','tool holders'])\n                    if tool_col is not None and holder_col is not None:\n                        return r, df, tool_col, holder_col\n        return None, None, None, None\n\n    mr, mdf, mpn_col, desc_col, man_col = find_master_items()\n    sr, sdf, tool_col, holder_col = find_steps_table()\n    if mdf is None or sdf is None:\n        return 0.0, \"Missing master or steps tables.\"\n\n    # Build searchable corpus from master\n    master_strings = []\n    for _, row in mdf.iterrows():\n        parts = []\n        if mpn_col in row and pd.notna(row[mpn_col]):\n            parts.append(str(row[mpn_col]))\n        if desc_col and desc_col in row and pd.notna(row[desc_col]):\n            parts.append(str(row[desc_col]))\n        if man_col and man_col in row and pd.notna(row[man_col]):\n            parts.append(str(row[man_col]))\n        s = normlow(' '.join(parts))\n        if s:\n            master_strings.append(s)\n\n    def is_matched(text):\n        if not isinstance(text, str) or not text.strip():\n            return False\n        t = normlow(text)\n        # Direct MPN/substring match\n        for ms in master_strings:\n            if len(ms) >= 4 and (ms in t or t in ms):\n                return True\n        # Token overlap heuristic\n        tokens = [w for w in re.findall(r\"[a-z0-9\\-\\.]{4,}\", t) if not w.isdigit()]\n        if not tokens:\n            return False\n        hits = 0\n        for ms in master_strings:\n            for tok in tokens:\n                if tok in ms:\n                    hits += 1\n                    break\n            if hits:\n                return True\n        return False\n\n    # Compute fractions\n    sdf2 = sdf.dropna(how='all')\n    tools_ok = 0\n    holders_ok = 0\n    total_rows = 0\n    for _, row in sdf2.iterrows():\n        if tool_col in row or holder_col in row:\n            total_rows += 1\n            if tool_col in row and is_matched(row[tool_col]):\n                tools_ok += 1\n            if holder_col in row and is_matched(row[holder_col]):\n                holders_ok += 1\n    if total_rows == 0:\n        return 0.0\n    frac_tools = tools_ok / total_rows\n    frac_holders = holders_ok / total_rows\n    score = 0.5 * frac_tools + 0.5 * frac_holders\n    return max(0.0, min(1.0, score))"}, {"type": "code", "name": "Purchase Links Validity", "description": "Check that most items in the Master Tool List have valid HTTP/HTTPS purchase links.", "weight": 0.2, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    def normalize(s):\n        return re.sub(r\"[^a-z0-9]+\", \"\", str(s).strip().lower())\n\n    def find_items_with_link(resource):\n        try:\n            path = context.files.get_path(resource.id)\n            xls = pd.ExcelFile(path)\n        except Exception:\n            return None, None\n        for sn in xls.sheet_names:\n            try:\n                df = pd.read_excel(path, sheet_name=sn)\n            except Exception:\n                continue\n            cols_norm = [normalize(c) for c in df.columns]\n            link_col = None\n            for i, cn in enumerate(cols_norm):\n                if cn in {'pagelink','link','urllink','purchaseurl','url','weblink'}:\n                    link_col = df.columns[i]\n                    break\n            if link_col is not None:\n                return df, link_col\n        return None, None\n\n    items_df = None\n    link_col = None\n    for r in context.get_all_outputs():\n        if getattr(r, 'is_spreadsheet', False):\n            df, lc = find_items_with_link(r)\n            if df is not None:\n                items_df, link_col = df, lc\n                break\n    if items_df is None:\n        return 0.0\n\n    links = items_df[link_col].astype(str).fillna('')\n    def valid(u):\n        u = u.strip()\n        return bool(re.match(r\"^https?://[^\\s/$.?#].[^\\s]*$\", u))\n    vals = [valid(u) for u in links if u and u.lower() != 'nan']\n    if not vals:\n        return 0.0\n    frac = sum(vals)/len(vals)\n    # Expect majority valid\n    score = max(0.0, min(1.0, (frac - 0.5) / 0.5))  # 0 at 50%, 1 at 100%\n    return score"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Quality and Professionalism (LLM)", "description": "Holistic LLM assessment of technical plausibility, completeness, clarity, and professional presentation for a manufacturing audience.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Technical Plausibility and Manufacturability", "description": "Assess if the chosen workholding, tooling, holders, and the manufacturing steps are plausible and sufficient to produce a typical CNC cover plate given constraints.", "weight": 1.0, "judge_prompt": "Evaluate the overall technical plausibility and sufficiency to manufacture a typical CNC-machined cover plate:\n- Do tools and holders listed appear coherent for milling operations (e.g., end mills, drills, chamfer tools, taps if applicable, ER collets or end mill holders, drill chuck, vice/modular fixturing)?\n- Are steps logically sequenced by orientation/operation, referencing appropriate tools and holders?\n- Does the stock size in inches look reasonable for a plate part with allowables for clamping/finish? (Do not require exact geometry; judge reasonableness.)\n- Is allowance made for multiple operations and potential tool breakage (e.g., extra inserts or duplicate tools/holders)?\n- Does the plan seem executable on a standard VMC or the suggested machine type from an integration proposal?\n\nScore 0.0\u20131.0:\n- 1.0: Clear, coherent plan with realistic tools/holders and steps that would likely produce the part.\n- 0.5: Generally plausible but with notable gaps (e.g., missing a likely workholding element or a key tool) that are still fixable.\n- 0.0: Incoherent or missing key elements that would block manufacturing.", "expectation": "A coherent, executable plan with appropriate fixturing, holders, and tools for milling a plate, with logical orientation-based operations."}, {"type": "llm_judge", "name": "Presentation, Clarity, and Professionalism", "description": "Judge formatting quality, clarity of headers/tables, and professionalism (including purchase links readily usable).", "weight": 1.0, "judge_prompt": "Assess the professionalism and clarity of the two Excel workbooks:\n- Are headers clearly labeled and easy to find? Are tables legible and consistently formatted?\n- Is the Master Tool List easy to interpret (clear descriptions, manufacturers/MPNs, quantities, costs, and accessible purchase links)?\n- Are the Manufacturing Steps concise, unambiguous, and scannable by operators/programmers?\n- If a budget increase email is present, does it read professionally (tone, justification, clear requested amount)?\n\nScore 0.0\u20131.0 based on overall presentation quality.", "expectation": "Clean, operator-friendly tables; professional tone; clear, labeled headers; purchase links usable; optionally professional email if over budget."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "e222075d-5d62-4757-ae3c-e34b0846583b", "rubric": {"category_name": "Video Editing: 30s Broadcast Commercial \u2014 \u201cSupport Green Energy\u201d", "rationale": "This rubric enforces a self-documenting deliverable set for a video edit: the editor must supply a 30s MP4 plus verification artifacts (stock log, timing sheet, graphics proofs, cue sheet, and technical specs). Stage 1 (LLM-only) gates on shape, ensuring the verifiable package exists. Stage 2 mixes code and LLM checks to verify correctness against the requested structure (duration/resolution, card lines present as graphics, stock coverage, links, etc.). Stage 3 assesses overall professional quality and fitness-for-purpose.", "max_total_score": 30.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate", "description": "LLM-only gate: Verify the output package has the exact verifiable structure required to enable automated checks.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 5.6, "rules": [{"type": "llm_judge", "name": "Deliverable Package Structure Present", "description": "Output must include: (1) a 30s 1920x1080 H.264 MP4, (2) Stock Asset Log (CSV/XLSX), (3) Script Timing Sheet (CSV/XLSX) that includes the two specified lines, (4) Graphics proof stills for the two text cards (PNG/JPG), (5) Music Cue Sheet (PDF/DOCX), (6) Technical Specs sheet (MD/PDF/DOCX). This is a structural check only.", "weight": 8.0, "judge_prompt": "You are evaluating whether the candidate provided a complete, verifiable package for a 30-second broadcast spot.\n\nAcceptable files and structure (be flexible with file names, but the items must be clearly identifiable):\n1) Final Video Export: H.264 .mp4 at 1920x1080, exactly ~30 seconds. Watermarks in footage/music previews are acceptable.\n2) Stock Asset Log: CSV or XLSX containing a table with columns similar to: [Asset Type, Source Platform, URL, Clip ID, Title, Creator, License Status (Preview/Watermarked), Used Timecode(s), Shot Description, California Location Tag, Notes]. Column names can vary but overall meaning must be clear.\n3) Script Timing Sheet: CSV or XLSX mapping the script to timeline with columns similar to: [Start (s or timecode), End (s or timecode), Element Type (Video/VO/Music/Graphic), Text, Source/Asset ID, Notes]. It must include entries for these two exact lines marked as graphic/text cards (flexible on capitalization):\n   \u2022 \u201cRenewable, reliable, green energy projects will create jobs\u201d\n   \u2022 \u201cUrge your legislator to support green energy in California\u201d\n4) Graphics Proof Stills: PNG or JPG exports of the two title cards, showing black background with white type (Arial or similar clean sans-serif acceptable).\n5) Music Cue Sheet: PDF or DOCX listing at minimum [Track Title, Composer, Publisher, PRO (if any), Source/Library, URL, In/Out or Duration used (~30s), Notes].\n6) Technical Specs: DOCX/PDF/MD stating export settings (codec=H.264/AVC, resolution=1920x1080, frame rate, duration=30s, bitrate, audio settings).\n\nScoring (STRUCTURE ONLY \u2014 do not judge content quality):\n- 1.0: All 6 items are present and identifiable with reasonable structure. Timing sheet includes the two required lines and they are clearly marked as graphics/cards.\n- 0.8: Missing exactly one supporting artifact among items (2), (4), (5), or (6); video and timing sheet still present and correctly structured with the two lines.\n- 0.6: Missing one core artifact (video OR timing sheet) OR timing sheet present but does not include the two specified lines marked as graphics.\n- 0.3: Only the video is present without the supporting artifacts.\n- 0.0: No video provided or video is not an MP4.\n\nOnly verify presence/format/structure. Do not assess pacing, color, typography quality, or calculation correctness.", "expectation": "A complete package: MP4 + structured logs/sheets + graphics proof stills + cue sheet + tech specs, enabling straightforward verification."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness Verification", "description": "Now that the verifiable structure exists, check correctness and compliance with the brief using code and LLM rules.", "is_required": true, "max_points": 12.0, "min_score_to_pass": 6.0, "rules": [{"type": "code", "name": "Technical Specs Consistency (Self-Reported)", "description": "Check for MP4 presence and that the Technical Specs text indicates H.264/AVC codec, 1920x1080 resolution, and ~30s duration. Falls back to Timing Sheet duration if specs are missing.", "weight": 2.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs() or []\n        # Helper: get path and name safely\n        def get_path(res):\n            try:\n                return context.files.get_path(res.id)\n            except Exception:\n                return None\n        def is_mp4(res):\n            p = get_path(res)\n            return (p is not None) and (str(p).lower().endswith('.mp4'))\n        def read_text_from_res(res):\n            # Try text, then docx, then pdf\n            try:\n                if getattr(res, 'is_text_format', False):\n                    return context.files.read_text(res.id)\n            except Exception:\n                pass\n            try:\n                if getattr(res, 'is_document', False):\n                    p = get_path(res)\n                    if p and str(p).lower().endswith('.docx'):\n                        return context.files.read_docx_text(res.id)\n                    if p and str(p).lower().endswith('.pdf'):\n                        return context.files.read_pdf_text(res.id)\n            except Exception:\n                pass\n            return ''\n        def parse_duration_from_text(t):\n            t = (t or '').lower()\n            # Patterns: 30s, 30 sec, 00:30, 0:30, 30.0 seconds\n            if re.search(r'\\b30(\\.0+)?\\s*s(ec(onds)?)?\\b', t):\n                return 30.0\n            m = re.search(r'\\b(\\d{1,2}):(\\d{2})(?::(\\d{2}))?\\b', t)\n            if m:\n                hh = 0 if m.group(3) is None and m.lastindex < 3 else int(m.group(1)) if m.lastindex and m.lastindex>=3 else 0\n                if m.lastindex and m.lastindex>=3 and m.group(3):\n                    # hh:mm:ss\n                    hh = int(m.group(1))\n                    mm = int(m.group(2))\n                    ss = int(m.group(3))\n                    return hh*3600+mm*60+ss\n                else:\n                    # mm:ss\n                    mm = int(m.group(1))\n                    ss = int(m.group(2))\n                    return mm*60+ss\n            return None\n        def find_timing_sheet_duration():\n            # Try to infer duration from any sheet with start/end columns\n            for res in outputs:\n                p = get_path(res)\n                if not p: continue\n                s = str(p).lower()\n                try:\n                    df = None\n                    if s.endswith('.csv'):\n                        df = context.files.read_csv(res.id)\n                    elif s.endswith('.xlsx'):\n                        # read first sheet\n                        df = pd.read_excel(p, sheet_name=0)\n                    else:\n                        continue\n                    cols = {str(c).strip().lower(): c for c in df.columns}\n                    if any(k in cols for k in ['end','out','end (s)','end (sec)']):\n                        end_col = None\n                        for cand in ['end','out','end (s)','end (sec)']:\n                            if cand in cols:\n                                end_col = cols[cand]\n                                break\n                        vals = df[end_col].dropna().tolist()\n                        def to_secs(v):\n                            if isinstance(v, (int,float,np.floating)):\n                                return float(v)\n                            if isinstance(v, str):\n                                m = re.search(r'^(\\d+):(\\d{2})(?::(\\d{2}))?$', v.strip())\n                                if m:\n                                    if m.group(3):\n                                        return int(m.group(1))*3600 + int(m.group(2))*60 + int(m.group(3))\n                                    return int(m.group(1))*60 + int(m.group(2))\n                                m2 = re.search(r'^(\\d+(?:\\.\\d+)?)\\s*s', v.strip().lower())\n                                if m2:\n                                    return float(m2.group(1))\n                                try:\n                                    return float(v)\n                                except Exception:\n                                    return None\n                            return None\n                        secs = [to_secs(v) for v in vals]\n                        secs = [x for x in secs if x is not None]\n                        if secs:\n                            return max(secs)\n                except Exception:\n                    continue\n            return None\n        # Components\n        has_mp4 = any(is_mp4(r) for r in outputs)\n        # Gather technical specs text\n        tech_texts = []\n        for res in outputs:\n            p = get_path(res)\n            if not p: \n                continue\n            name = str(p.name).lower()\n            if getattr(res, 'is_document', False) or getattr(res, 'is_text_format', False):\n                if any(k in name for k in ['spec','export','tech','settings']):\n                    txt = read_text_from_res(res)\n                    if txt:\n                        tech_texts.append(txt)\n        tech_blob = '\\n'.join(tech_texts)\n        # Heuristics for resolution/codec/duration\n        has_resolution = bool(re.search(r'1920\\s*[x\u00d7]\\s*1080', tech_blob, re.I))\n        has_codec = bool(re.search(r'h\\.?264|avc', tech_blob, re.I))\n        dur = parse_duration_from_text(tech_blob)\n        if dur is None:\n            dur = find_timing_sheet_duration()\n        has_duration_30 = (dur is not None) and (29.5 <= dur <= 30.5)\n        parts = [has_mp4, has_resolution, has_codec, has_duration_30]\n        score = sum(1.0 for p in parts if p) / 4.0\n        fb = []\n        if not has_mp4: fb.append('Missing MP4 export in outputs.')\n        if not has_resolution: fb.append('Technical specs do not clearly state 1920x1080.')\n        if not has_codec: fb.append('Technical specs do not clearly state H.264/AVC codec.')\n        if not has_duration_30: fb.append('Duration not clearly ~30s in specs/timing.')\n        return (score, '; '.join(fb) if fb else 'Specs consistent.')\n    except Exception as e:\n        return (0.0, f'Error in Technical Specs Consistency check: {e}')\n"}, {"type": "code", "name": "Timing Sheet + Required Graphic Lines", "description": "Verify timing sheet exists with Start/End/Text/Element columns (flexible names). Confirm both required sentences are present and tagged as graphics/cards, with sane timing and ~30s end time.", "weight": 4.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs() or []\n        def get_path(res):\n            try:\n                return context.files.get_path(res.id)\n            except Exception:\n                return None\n        # Load candidate timing sheet\n        candidates = []\n        for res in outputs:\n            p = get_path(res)\n            if not p:\n                continue\n            s = str(p).lower()\n            if s.endswith('.csv') or s.endswith('.xlsx'):\n                # Prefer filenames hinting timing/script/cue\n                priority = 0\n                if any(k in s for k in ['timing','script','alignment','cue','edit log','timeline']):\n                    priority = 2\n                elif any(k in s for k in ['log','sheet','table']):\n                    priority = 1\n                candidates.append((priority, res, p))\n        candidates.sort(key=lambda x: -x[0])\n        df = None\n        chosen = None\n        for _, res, p in candidates:\n            try:\n                if str(p).lower().endswith('.csv'):\n                    tdf = context.files.read_csv(res.id)\n                else:\n                    tdf = pd.read_excel(p, sheet_name=0)\n                if tdf is None or tdf.empty:\n                    continue\n                cols = [str(c).strip() for c in tdf.columns]\n                lower = [c.lower() for c in cols]\n                if any(x in lower for x in ['start','in','start (s)','start (sec)']) and any(x in lower for x in ['end','out','end (s)','end (sec)']) and any(x in lower for x in ['text','line','dialog','content']):\n                    df = tdf\n                    chosen = p\n                    break\n            except Exception:\n                continue\n        if df is None:\n            return (0.0, 'No timing sheet with Start/End/Text columns found.')\n        name_map = {str(c).strip().lower(): c for c in df.columns}\n        def pick(*opts):\n            for o in opts:\n                if o in name_map:\n                    return name_map[o]\n            return None\n        c_start = pick('start','in','start (s)','start (sec)')\n        c_end = pick('end','out','end (s)','end (sec)')\n        c_text = pick('text','line','dialog','content')\n        c_type = pick('element type','type','element','track','category')\n        if c_start is None or c_end is None or c_text is None:\n            return (0.0, 'Timing sheet missing required columns (Start/End/Text).')\n        def to_secs(v):\n            if isinstance(v, (int,float,np.floating)):\n                return float(v)\n            if isinstance(v, str):\n                s = v.strip()\n                m = re.match(r'^(\\d+):(\\d{2})(?::(\\d{2}))?$', s)\n                if m:\n                    if m.group(3):\n                        return int(m.group(1))*3600 + int(m.group(2))*60 + int(m.group(3))\n                    return int(m.group(1))*60 + int(m.group(2))\n                m2 = re.match(r'^(\\d+(?:\\.\\d+)?)\\s*s', s.lower())\n                if m2:\n                    return float(m2.group(1))\n                try:\n                    return float(s)\n                except Exception:\n                    return None\n            return None\n        df2 = df[[c_start, c_end, c_text] + ([c_type] if c_type else [])].copy()\n        df2['__start_s'] = df2[c_start].apply(to_secs)\n        df2['__end_s'] = df2[c_end].apply(to_secs)\n        # Valid timing rows\n        valid = df2.dropna(subset=['__start_s','__end_s'])\n        valid = valid[valid['__end_s'] > valid['__start_s']]\n        max_end = valid['__end_s'].max() if not valid.empty else None\n        # Required phrases\n        p1 = 'renewable, reliable, green energy projects will create jobs'\n        p2 = 'urge your legislator to support green energy in california'\n        def norm(s):\n            return (str(s) if s is not None else '').strip().lower()\n        # Find rows containing phrases\n        idx1 = valid[c_text].apply(lambda x: p1 in norm(x))\n        idx2 = valid[c_text].apply(lambda x: p2 in norm(x))\n        has_p1 = bool(idx1.any())\n        has_p2 = bool(idx2.any())\n        # If type column exists, ensure tagged as graphics/cards\n        graphic_ok = True\n        graphic_tokens = ['graphic','card','title','text','super','slate']\n        fb = []\n        if c_type is not None:\n            sub_ok = True\n            if has_p1:\n                row = valid[idx1].iloc[0]\n                t = norm(row[c_type])\n                if not any(g in t for g in graphic_tokens):\n                    sub_ok = False\n            if has_p2:\n                row = valid[idx2].iloc[0]\n                t = norm(row[c_type])\n                if not any(g in t for g in graphic_tokens):\n                    sub_ok = False\n            graphic_ok = sub_ok\n        # Scoring components\n        comp_presence = (1.0 if has_p1 else 0.0) + (1.0 if has_p2 else 0.0)\n        comp_presence /= 2.0\n        comp_graphics = (1.0 if graphic_ok else 0.0) if (has_p1 and has_p2 and c_type is not None) else (0.5 if (has_p1 and has_p2) else 0.0)\n        comp_timing = 0.0\n        if max_end is not None:\n            # Reward if ends near 30s\n            if 29.5 <= max_end <= 30.5:\n                comp_timing = 1.0\n            elif 28.0 <= max_end <= 32.0:\n                comp_timing = 0.5\n            else:\n                comp_timing = 0.0\n        score = 0.5*comp_presence + 0.3*comp_graphics + 0.2*comp_timing\n        if score < 0: score = 0.0\n        if score > 1: score = 1.0\n        if not has_p1: fb.append('Missing required line: jobs card.')\n        if not has_p2: fb.append('Missing required line: call-to-action card.')\n        if (has_p1 and has_p2) and c_type is None:\n            fb.append('No Element Type column; cannot confirm cards; partial credit given.')\n        if comp_timing == 0.0:\n            fb.append('End time not near 30s in timing sheet.')\n        return (score, '; '.join(fb) if fb else 'Timing and required graphic lines verified.')\n    except Exception as e:\n        return (0.0, f'Error verifying timing sheet: {e}')\n"}, {"type": "code", "name": "Stock Asset Log Completeness and Coverage", "description": "Verify stock log exists, URLs are valid, and coverage includes renewable energy shots, California icons, people-at-work, and at least one classical-style music track.", "weight": 3.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs() or []\n        def get_path(res):\n            try:\n                return context.files.get_path(res.id)\n            except Exception:\n                return None\n        # Load candidate stock/asset log\n        best = None\n        df = None\n        for res in outputs:\n            p = get_path(res)\n            if not p:\n                continue\n            s = str(p).lower()\n            if s.endswith('.csv') or s.endswith('.xlsx'):\n                try:\n                    if any(k in s for k in ['stock','asset','shot','footage','log','sources']):\n                        tdf = context.files.read_csv(res.id) if s.endswith('.csv') else pd.read_excel(p, sheet_name=0)\n                        if tdf is None or tdf.empty:\n                            continue\n                        cols = [str(c).strip().lower() for c in tdf.columns]\n                        if any('url'==c or 'link'==c for c in cols):\n                            best = p\n                            df = tdf\n                            break\n                except Exception:\n                    continue\n        if df is None:\n            return (0.0, 'No stock/asset log with URL column found.')\n        cols = {str(c).strip().lower(): c for c in df.columns}\n        c_url = cols.get('url') or cols.get('link')\n        # Textual fields to mine\n        text_cols = [c for c in df.columns if str(c).strip().lower() in ['title','description','shot description','notes','tags','location','california location tag','asset type','type']]\n        def row_text(row):\n            parts = []\n            for c in text_cols:\n                try:\n                    v = row[c]\n                    if pd.isna(v):\n                        continue\n                    parts.append(str(v))\n                except Exception:\n                    continue\n            return ' '.join(parts).lower()\n        urls = df[c_url].astype(str).fillna('') if c_url in df.columns else pd.Series([], dtype=str)\n        def is_valid_url(u):\n            u = u.strip().lower()\n            if not re.match(r'^https?://', u):\n                return False\n            # Must be plausible stock/music platform (flexible)\n            return any(dom in u for dom in ['adobe','stock','shutterstock','pond5','istock','getty','storyblocks','artgrid','artlist','premiumbeat','audiojungle','epidemic','pixabay','pexels','unsplash','videvo'])\n        url_valid_ratio = 0.0\n        if len(urls) > 0:\n            url_valid_ratio = sum(1 for u in urls if is_valid_url(str(u))) / float(len(urls))\n        # Coverage categories\n        has_renewable = False\n        has_ca_icon = False\n        has_people_work = False\n        has_music_classical = False\n        for _, row in df.iterrows():\n            t = row_text(row)\n            # Renewable indicators\n            if any(k in t for k in ['solar','photovoltaic','pv','wind','turbine','renewable','geothermal']):\n                has_renewable = True\n            # California indicators\n            if any(k in t for k in ['california','golden gate','san francisco','los angeles','la skyline','hollywood','silicon valley','beach','coast','pacific','agriculture','farmland','vineyard']):\n                has_ca_icon = True\n            # People at work\n            if any(k in t for k in ['worker','at work','office','restaurant','engineer','technician','installer','construction','factory','crew']):\n                has_people_work = True\n            # Music classical style\n            if any(k in t for k in ['music','track','score','orchestral','classical','strings','symphony','ensemble']):\n                # Also check URL domain suggests music lib if possible\n                u = ''\n                try:\n                    u = str(row[c_url]).lower()\n                except Exception:\n                    pass\n                if any(dom in u for dom in ['artlist','premiumbeat','audiojungle','epidemic','pond5','shutterstock','adobe']):\n                    if any(k in t for k in ['classical','orchestral','strings','symphony']):\n                        has_music_classical = True\n        # Heuristic counts\n        n_rows = int(df.shape[0])\n        enough_assets = n_rows >= 7  # ~6+ video and 1 music\n        # Score composition\n        s_url = url_valid_ratio  # 0..1\n        s_cov = (1.0 if has_renewable else 0.0) * 0.34 + (1.0 if has_ca_icon else 0.0) * 0.33 + (1.0 if has_people_work else 0.0) * 0.33\n        s_music = 1.0 if has_music_classical else 0.0\n        s_count = 1.0 if enough_assets else (0.5 if n_rows >= 4 else 0.0)\n        # Combine (weighted within rule)\n        score = 0.35*s_url + 0.35*s_cov + 0.2*s_music + 0.1*s_count\n        fb = []\n        if url_valid_ratio < 1.0: fb.append(f'URL validity ratio {url_valid_ratio:.0%}.')\n        if not has_renewable: fb.append('No clear renewable energy asset.')\n        if not has_ca_icon: fb.append('No clear California icon/landscape asset.')\n        if not has_people_work: fb.append('No clear people-at-work asset.')\n        if not has_music_classical: fb.append('No classical/orchestral music indicated.')\n        if not enough_assets: fb.append(f'Low asset count: {n_rows}.')\n        return (min(max(score,0.0),1.0), '; '.join(fb) if fb else 'Stock log complete with required coverage.')\n    except Exception as e:\n        return (0.0, f'Error in Stock Asset Log check: {e}')\n"}, {"type": "code", "name": "Graphics Proof Stills Present", "description": "Check presence of two still images for graphic cards (filenames containing 'card' or 'graphic').", "weight": 1.5, "code": "def evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs() or []\n        def get_path(res):\n            try:\n                return context.files.get_path(res.id)\n            except Exception:\n                return None\n        imgs = []\n        for res in outputs:\n            p = get_path(res)\n            if not p: continue\n            s = str(p).lower()\n            if s.endswith('.png') or s.endswith('.jpg') or s.endswith('.jpeg'):\n                if ('card' in s) or ('graphic' in s) or ('title' in s):\n                    imgs.append(p)\n        n = len(imgs)\n        score = 1.0 if n >= 2 else (0.5 if n == 1 else 0.0)\n        fb = f'Found {n} graphics still(s).' if n>0 else 'No graphics proof stills found.'\n        return (score, fb)\n    except Exception as e:\n        return (0.0, f'Error checking graphics stills: {e}')\n"}, {"type": "llm_judge", "name": "Content Compliance Cross-Check (Video)", "description": "Visually/audibly verify the video includes California/renewable imagery, two graphic cards with the specified lines, scratch VO aligned to the script, and classical energetic music with strong start/end. Structural presence only, not quality.", "weight": 1.0, "judge_prompt": "Inspect the MP4 and verify the following STRUCTURAL items (not quality):\n- Two text cards on black background with white, clean sans-serif type (Arial or similar), containing exactly:\n  1) \u201cRenewable, reliable, green energy projects will create jobs\u201d\n  2) \u201cUrge your legislator to support green energy in California\u201d\n- Presence of California visuals (e.g., Golden Gate Bridge, LA skyline, beaches, agriculture; flexible).\n- Presence of renewable energy visuals (e.g., solar fields, wind turbines; flexible).\n- Scratch voiceover present and generally aligned with the script timing (does not need to be professional quality).\n- Classical/Orchestral-feel music that is energetic and edited to a strong opening and ending within ~30s.\n\nScoring:\n- 1.0: All items present. Cards clearly legible as specified, and both categories of visuals appear.\n- 0.7: Missing exactly one minor element (e.g., visuals present but music ending not clearly strong), cards correct.\n- 0.4: Missing two required elements OR one of the cards is incorrect/absent.\n- 0.0: Lacks major required elements (no cards, no CA/renewable imagery, or no VO/music).", "expectation": "Video includes both specified cards, CA + renewable visuals, scratch VO, and classical energetic music with strong start/end."}], "on_failure_action": "skip_remaining", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality Assessment", "description": "Holistic assessment of editorial craft, pacing, polish, and persuasive effectiveness for the intended audience and brief.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Editorial Quality, Tone, and Craft", "description": "Judge the professional quality of the edit: pacing, storytelling, clarity, typography, mix, and persuasive impact, aligned to brief (optimistic, proud, elegant, medium-high energy).", "weight": 10.0, "judge_prompt": "Assess overall professional quality for a 30s broadcast commercial titled \u201cSupport Green Energy.\u201d Use only the video (and any provided proof assets) as reference. Consider:\n- Pacing and rhythm: medium-high energy while maintaining elegance and clarity.\n- Story arc and visual cohesion: California pride + environmental and jobs opportunity message.\n- Typography craft on cards: readability, balance, alignment, clean sans-serif (or similar) and compositional polish.\n- Music edit: energetic classical feel; strong opening/downbeat and confident ending; cohesion with VO.\n- Audio mix: VO intelligibility against music; no abrupt or distracting level changes.\n- Color, exposure, and transitions: coherent and tasteful; no jarring jump cuts unless motivated.\n- Call-to-action clarity and impact.\n\nScoring guide:\n- 9\u201310: Broadcast-ready polish; compelling, elegant, confidently paced; excellent mix and typography.\n- 7\u20138.5: Strong professional work with minor issues; meets brief.\n- 5\u20136.5: Adequate student/junior level; some pacing/mix/typography inconsistencies.\n- 3\u20134.5: Noticeable flaws disrupt persuasion; uneven pacing or mix; weak typography.\n- 0\u20132.5: Unusable: lacks clarity, wrong tone, or technically distracting.\nProvide a concise justification referencing specific observations (no need to restate requirements).", "expectation": "A polished, persuasive, on-brief 30s spot with elegant, energetic pacing, clean cards, clear VO, and confident musical shape."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "3f625cb2-f40e-4ead-8a97-6924356d5989", "rubric": {"category_name": "Legal Memorandum: Children's Privacy (YouTube)", "rationale": "This rubric enforces a self-documenting, verifiable legal memo. Stage 1 (LLM-only) gates strict format/structure in a PDF of \u22643 pages so later checks are trivial. Stage 2 uses code rules to verify statutory coverage, enforcement/rights, case references, and actionable options via robust text searches. Stage 3 uses LLM judges to assess clarity, organization, and practical value for a client audience.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2013 Format and Structure Gate (LLM ONLY)", "description": "Mandate exact memo shape so verification is possible. File must be a PDF (not DOCX), 1\u20133 pages, with specific sections/headers enabling deterministic checks later. Do NOT judge correctness or quality here\u2014only presence/structure.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "PDF Memo Structure Requirement", "description": "Check the candidate produces a client-facing legal memo PDF (\u22643 pages) with required sections and layout.", "weight": 4.0, "judge_prompt": "You are evaluating whether the provided output meets STRUCTURAL requirements for a client-facing legal memorandum. Only check presence/format, not legal correctness.\n\nFormat Requirements (must have):\n- File type: PDF (not DOCX, not plain text, not Excel)\n- Length: 1 to 3 pages inclusive\n- Professional memo header on page 1 containing at least two of: To, From, Date, Re/Subject (flexible naming OK). Prefer inclusion of client/matter (e.g., ABC Father / YouTube issue).\n\nRequired Sections (flexible naming OK; check visible headers):\n1) Executive Summary or Overview (should be on page 1)\n2) Facts or Background (context of the 10-year-old and alleged data collection)\n3) Applicable Law or Legal Framework (must include both COPPA and California privacy law headings or clear subsections)\n4) Analysis or Discussion (with identifiable subsections for: a) COPPA; b) California privacy law)\n5) Legal Options and/or Remedies (what the client can do)\n6) Recommendations and/or Next Steps (clear action guidance)\nOptional but rewarded: Authorities/Citations/References (inline footnotes or a short list is fine)\n\nScoring (0 to 4 total):\n- +1.5 if: PDF AND 1\u20133 pages (both satisfied). 0 if either fails.\n- +0.7 if: Memo header on page 1 with at least two of To/From/Date/Re (flexible), preferably with client/matter.\n- +0.6 if: Executive Summary/Overview present (preferably on page 1).\n- +0.8 if: Facts/Background, Applicable Law, and Analysis/Discussion sections are all present.\n- +0.6 if: Analysis/Discussion explicitly has clearly delineated subsections for COPPA and California privacy law.\n- +0.4 if: Both Legal Options and Recommendations/Next Steps are present.\n- +0.4 if: Any Authorities/Citations/References or footnotes section is present.\nCap the score at 4.0. Be flexible with exact header names but strict about section presence. Do NOT assess legal accuracy or writing quality here\u2014structure only.\n\nOutput only a numeric score from 0 to 4 based on these criteria.", "expectation": "A PDF memo of \u22643 pages with all required headers/sections present so later rules can verify content."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Substantive Verification (Code + lightweight heuristics)", "description": "Deterministic checks for coverage of COPPA and California laws, consent/applicability concepts, enforcement/remedies accuracy, presence of notable case law, and actionable client options. Flexible keyword/regex matching is used; partial credit awarded.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Statutory Coverage (COPPA + California)", "description": "Verify mention of COPPA and California privacy laws with flexible patterns; bonus for ancillary CA youth/privacy laws.", "weight": 1.2, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        text = \"\"\n    if not text:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text:\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text:\n        return 0.0\n    t = text.lower()\n\n    def any_match(patterns):\n        return any(re.search(p, t) for p in patterns)\n\n    coppa_patterns = [r\"\\bcoppa\\b\", r\"children[\u2019']s online privacy protection act\", r\"15\\s*u\\.?s\\.?c\\.?\\s*\u00a7?\\s*650\", r\"16\\s*c\\.?f\\.?r\\.?\\s*part\\s*312\", r\"\\bftc\\b.*coppa\"]\n    ca_core_patterns = [r\"\\bccpa\\b\", r\"\\bcpra\\b\", r\"california consumer privacy act\", r\"cal\\.\\s*civ\\.\\s*code\\s*\u00a7?\\s*1798\", r\"privacy rights act\"]\n    ca_ancillary_patterns = [r\"caloppa\", r\"online eraser\", r\"bus\\.\\s*&\\s*prof\\.\\s*code\\s*\u00a7?\\s*2258\", r\"sopipa\", r\"bus\\.\\s*&\\s*prof\\.\\s*code\\s*\u00a7?\\s*17200\", r\"unfair competition law\"]\n\n    score = 0.0\n    if any_match(coppa_patterns):\n        score += 0.6\n    if any_match(ca_core_patterns):\n        score += 0.6\n    if any_match(ca_ancillary_patterns):\n        score += 0.2\n    if score > 1.2:\n        score = 1.2\n    return score"}, {"type": "code", "name": "Consent and Applicability Concepts", "description": "Check for COPPA core concepts: verifiable parental consent, under-13 threshold, child-directed/mixed-audience/actual knowledge standards.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        text = \"\"\n    if not text:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text:\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text:\n        return 0.0\n    t = text.lower()\n\n    def hit(p):\n        return re.search(p, t) is not None\n\n    consent = hit(r\"verifiable\\s+parental\\s+consent\") or (hit(r\"parental\\s+consent\") and hit(r\"verif\"))\n    under_13 = hit(r\"under\\s*13\") or hit(r\"younger\\s*than\\s*13\") or hit(r\"under\\s*thirteen\") or hit(r\"age\\s*13\")\n    audience = hit(r\"child\\s*-?directed\") or hit(r\"mixed\\s*audience\") or hit(r\"general\\s*audience\") or hit(r\"actual\\s*knowledge\")\n\n    score = 0.0\n    if consent:\n        score += 0.3\n    if under_13:\n        score += 0.3\n    if audience:\n        score += 0.2\n    return min(score, 0.8)"}, {"type": "code", "name": "Relevant Case Law / Enforcement Actions", "description": "Detect reference to 2019 YouTube/Google COPPA settlement and/or other relevant cases involving children\u2019s data tracking (e.g., Jones v. Google).", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        text = \"\"\n    if not text:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text:\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text:\n        return 0.0\n    t = text.lower()\n\n    def any_match(ps):\n        return any(re.search(p, t) for p in ps)\n\n    yt_settlement = [r\"(2019|\\$\\s*170\\s*million).*(youtube|google).*coppa\", r\"united\\s*states\\s*v\\.?\\s*google\", r\"ftc\\s*v\\.?\\s*youtube\", r\"youtube.*coppa.*settlement\"]\n    other_cases = [r\"jones\\s*v\\.?\\s*google\", r\"doe\\s*v\\.?\\s*google\", r\"doe\\s*v\\.?\\s*youtube\", r\"hubbard\\s*v\\.?\\s*google\", r\"in\\s*re\\s*google.*(tracking|privacy)\"]\n\n    score = 0.0\n    if any_match(yt_settlement):\n        score += 0.5\n    if any_match(other_cases):\n        score += 0.3\n    return min(score, 0.8)"}, {"type": "code", "name": "Enforcement and Remedies Accuracy", "description": "Verify mention of FTC and state enforcement, plus private right of action limitations (COPPA none; CCPA limited to data breaches).", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        text = \"\"\n    if not text:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text:\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text:\n        return 0.0\n    t = text.lower()\n\n    def hit(p):\n        return re.search(p, t) is not None\n\n    # Enforcement actors\n    ftc = hit(r\"\\bftc\\b|federal\\s*trade\\s*commission\")\n    ag = hit(r\"attorney\\s*general\") or hit(r\"california\\s*attorney\\s*general\")\n    cpra_agency = hit(r\"california\\s*privacy\\s*rights\\s*agency\") or hit(r\"\\bcpra\\b\\s*agency\") or hit(r\"\\bcpra\\b.*agency\")\n\n    # Private right of action limitations\n    coppa_no_pra = hit(r\"coppa.*no\\s*private\\s*right\\s*of\\s*action\") or (hit(r\"coppa\") and hit(r\"no\\s*private\\s*right\"))\n    ccpa_limited_pra = hit(r\"ccpa.*private\\s*right.*(limited|data\\s*breach)\") or hit(r\"private\\s*right.*(limited|data\\s*breach).*ccpa\") or hit(r\"ccpa.*(data\\s*breach).*private\\s*right\")\n\n    score = 0.0\n    if ftc or ag or cpra_agency:\n        score += 0.4\n    if coppa_no_pra:\n        score += 0.2\n    if ccpa_limited_pra:\n        score += 0.2\n    return min(score, 0.8)"}, {"type": "code", "name": "Actionable Options and Complaint Paths", "description": "Confirm presence of concrete options: demand/deletion requests, complaints (FTC, CA AG, CPRA agency), and litigation/ADR possibilities.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        text = \"\"\n    if not text:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text:\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text:\n        return 0.0\n    t = text.lower()\n\n    def any_match(ps):\n        return any(re.search(p, t) for p in ps)\n\n    steps = 0\n    # Company-facing actions\n    if any_match([r\"demand\\s*letter\", r\"notice\\s*letter\", r\"preservation\\s*letter\", r\"request\\s*to\\s*delete\", r\"deletion\\s*request\", r\"data\\s*deletion\", r\"data\\s*access\\s*request\", r\"data\\s*correction\\s*request\"]):\n        steps += 1\n    # Regulator complaints\n    if any_match([r\"ftc\\s*complaint\", r\"federal\\s*trade\\s*commission\\s*complaint\", r\"attorney\\s*general\\s*complaint\", r\"california\\s*attorney\\s*general\", r\"california\\s*privacy\\s*rights\\s*agency\", r\"cpra\\s*agency\\s*complaint\"]):\n        steps += 1\n    # Litigation/ADR\n    if any_match([r\"arbitration\", r\"class\\s*action\", r\"small\\s*claims\", r\"ucla?\\b|unfair\\s*competition\\s*law|bus\\.\\s*&\\s*prof\\.\\s*code\\s*\u00a7?\\s*17200\"]):\n        steps += 1\n\n    # Score mapping: 0, 1, 2, 3 distinct categories found -> 0.0, 0.15, 0.3, 0.4\n    mapping = {0: 0.0, 1: 0.15, 2: 0.3, 3: 0.4}\n    return mapping.get(steps, 0.4)"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Quality and Client Value (LLM)", "description": "Holistic assessment of clarity, organization, professionalism, and actionable value for a California client. Do not re-check structure (Stage 1) or basic coverage (Stage 2); focus on readability, prioritization, and practical guidance.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Client-Friendly Clarity and Organization", "description": "Evaluate readability, flow, and whether a non-lawyer client can understand the memo without heavy legalese.", "weight": 1.0, "judge_prompt": "Evaluate the memo\u2019s clarity and organization for a non-lawyer client in California. Ignore structural presence (already checked). Focus on plain language, coherent flow, and whether the Executive Summary conveys the bottom line plainly.\nScoring (0\u20131):\n- 1.0: Plain language, minimal jargon with explanations, coherent headings and transitions; Executive Summary clearly states the bottom line and key factors.\n- 0.7: Generally clear with occasional jargon and minor organizational issues; main conclusions understandable.\n- 0.4: Noticeable legalese and choppy flow; conclusions are hard to identify.\n- 0.1: Dense legal jargon, difficult to follow; client value limited.\n- 0.0: Unreadable or no meaningful client-facing explanation.", "expectation": "Clear, concise, client-appropriate explanations with a strong Executive Summary."}, {"type": "llm_judge", "name": "Professional Judgment and Actionability", "description": "Assess whether recommendations are prioritized, realistic for California, and note key risks/limits (e.g., COPPA/CCPA private right issues) with next steps tailored to the client.", "weight": 1.0, "judge_prompt": "Assess the practical value of the recommendations for a California parent of a 10-year-old regarding alleged YouTube data collection. Ignore section presence; judge usefulness and prioritization.\nScoring (0\u20131):\n- 1.0: Prioritized, realistic plan (e.g., immediate data deletion requests, evidence preservation, targeted complaints to FTC/CA regulators), accurate notes on limits (no private right under COPPA; CCPA private action mostly for breaches), and clear timelines/expectations.\n- 0.7: Mostly actionable but missing some prioritization or minor accuracy nuances.\n- 0.4: Vague or generic options without clear sequencing; limited alignment to California context.\n- 0.1: Recommendations are impractical or misleading.\n- 0.0: No usable recommendations.", "expectation": "Clear, prioritized next steps calibrated to California, with realistic remedies and limitations."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "4de6a529-4f61-41a1-b2dc-64951ba03457", "rubric": {"category_name": "Stanton Capital CIO \u2013 Active Allocation Table (Q1 2025)", "rationale": "This rubric enforces a self-documenting, verifiable PDF deliverable for Stanton Capital\u2019s quarterly active allocation table. Stage 1 (LLM-only) strictly mandates the document\u2019s structure, including a machine-readable appendix to enable deterministic checks. Stage 2 (code + LLM) verifies correctness and internal consistency using the appendix and core textual references. Stage 3 (LLM) evaluates professional quality and strategic clarity.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2013 Format & Structure Gate (LLM-only)", "description": "MANDATORY structure in a PDF so verification is trivial. The document must present two sections (Main Asset Classes; Preference by Asset Class subdivided into Equities, Fixed Income, Currency) with specific table columns and a machine-readable appendix for code-based checks.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Output Format Requirement (PDF Gate)", "description": "Check that the candidate output is a properly structured PDF containing all required sections, tables, columns, and the machine-readable appendix.", "weight": 4.0, "judge_prompt": "You are verifying the SHAPE and STRUCTURE ONLY (not content accuracy) of the candidate\u2019s output. The expected deliverable is a professionally formatted PDF with the EXACT elements below. Be flexible with minor naming variations (e.g., \u201cEquity\u201d vs \u201cEquities\u201d), but the structure and machine-readable appendix MUST exist.\n\nFORMAT REQUIREMENTS\n- File type: PDF only (not DOCX, not Excel, not plain text).\n- Length: At least 2 pages.\n- Title block on page 1 includes all of the following (exact or close variants):\n  - \u201cStanton Capital\u201d\n  - \u201cCIO Active Allocation\u201d or \u201cActive Allocation Table\u201d\n  - \u201cQ1 2025\u201d and \u201cQuarter Ended March 31, 2025\u201d\n  - \u201cIndependent Views\u201d (or a close variant making it clear they are Stanton-only independent views)\n\nMANDATORY SECTIONS AND TABLES\n1) Top section labeled \u201cMain Asset Classes\u201d (or \u201cCross-Asset Overview\u201d / \u201cCross-Asset Opportunity Sets\u201d).\n   - Must include a table with columns (exact or close variants):\n     [Asset/Sub-Asset | UW | N | OW | Change (vs prior qtr) | Conviction | One-Sentence Justification]\n   - For UW/N/OW selection, the chosen signal must be text-extractable (e.g., \u201cX\u201d or \u201c\u2713\u201d placed in that column). Do not rely solely on color or icons.\n   - Change column must use one of: \u2191, \u2193, NC (no change). Do not rely on color only.\n   - Conviction must be \u201cLow\u201d or \u201cModerate\u201d.\n\n2) Bottom section labeled \u201cPreference by Asset Class\u201d. It must be clearly divided into three sub-sections:\n   - Equities\n   - Fixed Income\n   - Currency\n   Each sub-section must have its own table with the same columns as above:\n   [Asset/Sub-Asset | UW | N | OW | Change (vs prior qtr) | Conviction | One-Sentence Justification]\n   - Each sub-section should list multiple sub-asset classes (at least 5 rows per sub-section as a structural expectation, though exact items are flexible).\n   - UW/N/OW must be marked using text-extractable markers (e.g., \u201cX\u201d or \u201c\u2713\u201d). Change must use \u2191, \u2193, or NC. Conviction must be Low or Moderate. Each row must include a one-sentence justification.\n\nSUPPORTING TEXT SECTIONS\n3) \u201cMethodology and Macro Context\u201d section (or similar label) with a brief paragraph that explicitly references:\n   - global growth showing a slight improvement\n   - the Fed being in a rate-cutting cycle\n   - the overall economy showing healthy signs\n   - that minimal macro changes occurred from last quarter\n   - that change markers refer to the comparison vs the prior quarter (Q4 2024 \u2192 Q1 2025)\n\n4) \u201cDefinitions\u201d section (or a clearly labeled legend) that summarizes the meaning of Underweight, Neutral, Overweight, and Conviction levels (Low, Moderate), consistent with the task\u2019s definitions.\n\nMACHINE-READABLE APPENDIX (CRITICAL FOR VERIFICATION)\n5) An appendix titled something like \u201cAppendix A \u2014 Machine-Readable Table Export\u201d. It MUST contain a plain-text, pipe-delimited (|) block between literal markers:\n   DATA_EXPORT_START\n   section|group|sub_asset|signal|change|conviction|justification\n   ... one line per item, pipe-delimited ...\n   DATA_EXPORT_END\n\n   - section: \u201cMain Asset Classes\u201d or \u201cPreference by Asset Class\u201d\n   - group: For Main Asset Classes use \u201cMain\u201d; for Preference by Asset Class use \u201cEquities\u201d, \u201cFixed Income\u201d, or \u201cCurrency\u201d\n   - sub_asset: the row label (e.g., \u201cUS Large Cap\u201d)\n   - signal: one of UW, N, OW (exact, uppercase)\n   - change: one of UP, DOWN, NC (exact, uppercase)\n   - conviction: one of Low, Moderate (case-sensitive spelling)\n   - justification: a single sentence with no pipe characters (|)\n   - Example line (illustrative only):\n     Preference by Asset Class|Equities|US Large Cap|OW|NC|Moderate|Easing financial conditions and resilient earnings support a modest preference.\n\nSCORING (Structure ONLY)\n- 4.0: PDF with all required sections present; both tables sets exist with correct columns; clear subdivision (Equities, Fixed Income, Currency); Methodology/Macro and Definitions present; machine-readable appendix present with correct markers and header.\n- 3.5: Minor omission or naming variance but core tables, columns, subdivisions, and appendix are present.\n- 3.0: Missing exactly one core element (e.g., missing Methodology/Macro OR Definitions OR one sub-section table). Appendix still present.\n- 2.0: Multiple structural omissions (e.g., missing a major section or missing the appendix); still a PDF with some relevant tables.\n- 0.0: Not a PDF, or missing both major sections, or no allocation tables at all.\n\nOnly evaluate presence/structure, not correctness of content.", "expectation": "A 2+ page PDF with a title block, a Main Asset Classes table, a Preference by Asset Class section subdivided into Equities/Fixed Income/Currency each with the specified columns, supporting Methodology/Macro Context and Definitions sections, and a pipe-delimited Machine-Readable Table Export between DATA_EXPORT_START and DATA_EXPORT_END."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification (Code + LLM)", "description": "Deterministic checks using the machine-readable appendix and PDF text to verify internal consistency, field validity, and cross-references to macro context and quarter labeling.", "is_required": false, "max_points": 4.5, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Parse Machine-Readable Export", "description": "Locate and parse the DATA_EXPORT_* block; validate header and at least one data row.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No primary document output.\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        return 0.0, \"Unable to read PDF text.\"\n    if not text:\n        return 0.0, \"Empty PDF text.\"\n\n    m = re.search(r'DATA_EXPORT_START(.*?)DATA_EXPORT_END', text, re.S)\n    if not m:\n        return 0.0, \"Machine-readable export block not found.\"\n    block = m.group(1).strip()\n    lines = [ln.strip() for ln in block.splitlines() if ln.strip()]\n    if not lines:\n        return 0.0, \"Export block is empty.\"\n    header = lines[0]\n    expected_header = 'section|group|sub_asset|signal|change|conviction|justification'\n    if header != expected_header:\n        return 0.0, f\"Header mismatch. Found: {header}\"\n    if len(lines) < 2:\n        return 0.0, \"No data rows in export.\"\n    # Quick sanity: ensure at least one line splits into 7 fields\n    sample = lines[1].split('|')\n    if len(sample) != 7:\n        return 0.0, \"Data row does not have 7 fields.\"\n    return 1.0, f\"Parsed {len(lines)-1} data rows from export.\""}, {"type": "code", "name": "Row Count Minimums by Group", "description": "Check minimum row counts: Main >=5; Equities >=6; Fixed Income >=6; Currency >=5.", "weight": 0.8, "code": "import re\nfrom collections import Counter\n\ndef evaluate(workflow, context):\n    def parse_export(text):\n        m = re.search(r'DATA_EXPORT_START(.*?)DATA_EXPORT_END', text, re.S)\n        if not m:\n            return []\n        block = m.group(1).strip()\n        lines = [ln.strip() for ln in block.splitlines() if ln.strip()]\n        if not lines:\n            return []\n        header = lines[0]\n        if header != 'section|group|sub_asset|signal|change|conviction|justification':\n            return []\n        rows = []\n        for ln in lines[1:]:\n            parts = ln.split('|')\n            if len(parts) != 7:\n                continue\n            rows.append({\n                'section': parts[0], 'group': parts[1], 'sub_asset': parts[2],\n                'signal': parts[3], 'change': parts[4], 'conviction': parts[5], 'justification': parts[6]\n            })\n        return rows\n\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No primary document.\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        return 0.0, \"Unable to read PDF.\"\n    rows = parse_export(text)\n    if not rows:\n        return 0.0, \"Export missing or unparsable.\"\n\n    counts = Counter([r['group'] for r in rows])\n    reqs = {'Main': 5, 'Equities': 6, 'Fixed Income': 6, 'Currency': 5}\n    ok = 0\n    details = []\n    for g, req in reqs.items():\n        c = counts.get(g, 0)\n        details.append(f\"{g}={c}/{req}\")\n        if c >= req:\n            ok += 1\n    score = ok / len(reqs)\n    return score, \"; \".join(details)"}, {"type": "code", "name": "Field Validity & Uniqueness", "description": "Validate allowed values and duplicate pairs. signal in {UW,N,OW}; change in {UP,DOWN,NC}; conviction in {Low,Moderate}. No duplicate (group, sub_asset).", "weight": 1.2, "code": "import re\n\ndef evaluate(workflow, context):\n    def parse_export(text):\n        m = re.search(r'DATA_EXPORT_START(.*?)DATA_EXPORT_END', text, re.S)\n        if not m:\n            return []\n        block = m.group(1).strip()\n        lines = [ln.strip() for ln in block.splitlines() if ln.strip()]\n        if not lines or lines[0] != 'section|group|sub_asset|signal|change|conviction|justification':\n            return []\n        rows = []\n        for ln in lines[1:]:\n            parts = ln.split('|')\n            if len(parts) != 7:\n                continue\n            rows.append(tuple(parts))\n        return rows\n\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No primary document.\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        return 0.0, \"Unable to read PDF.\"\n    rows = parse_export(text)\n    if not rows:\n        return 0.0, \"Export missing or unparsable.\"\n\n    allowed_signal = {'UW','N','OW'}\n    allowed_change = {'UP','DOWN','NC'}\n    allowed_conv = {'Low','Moderate'}\n\n    seen = set()\n    valid = 0\n    total = 0\n    dup_count = 0\n    for (section, group, sub_asset, signal, change, conv, just) in rows:\n        total += 1\n        key = (group.strip(), sub_asset.strip().lower())\n        if key in seen:\n            dup_count += 1\n        seen.add(key)\n        if (signal in allowed_signal) and (change in allowed_change) and (conv in allowed_conv):\n            valid += 1\n    if total == 0:\n        return 0.0, \"No rows.\"\n    base = valid/total\n    # Penalize duplicates: if any duplicates, reduce score proportionally\n    pen = max(0.0, 1.0 - min(1.0, dup_count/ max(1,total)))\n    score = base * pen\n    fb = f\"valid={valid}/{total}; duplicates={dup_count}\"\n    return max(0.0, min(1.0, score)), fb"}, {"type": "code", "name": "Justification One-Sentence & Length", "description": "Each justification should be a single sentence (ends with a period) and 8\u201335 words.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    def parse_export(text):\n        m = re.search(r'DATA_EXPORT_START(.*?)DATA_EXPORT_END', text, re.S)\n        if not m:\n            return []\n        block = m.group(1).strip()\n        lines = [ln.strip() for ln in block.splitlines() if ln.strip()]\n        if not lines or lines[0] != 'section|group|sub_asset|signal|change|conviction|justification':\n            return []\n        rows = []\n        for ln in lines[1:]:\n            parts = ln.split('|')\n            if len(parts) != 7:\n                continue\n            rows.append(parts)\n        return rows\n\n    def word_count(s):\n        return len([w for w in re.findall(r\"\\b\\w[\\w'\\-]*\\b\", s)])\n\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No primary document.\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        return 0.0, \"Unable to read PDF.\"\n    rows = parse_export(text)\n    if not rows:\n        return 0.0, \"Export missing or unparsable.\"\n\n    total = len(rows)\n    good = 0\n    for section, group, sub_asset, signal, change, conv, just in rows:\n        wc = word_count(just)\n        is_sentence = just.strip().endswith('.') and ('?' not in just and '!' not in just)\n        if 8 <= wc <= 35 and is_sentence:\n            good += 1\n    score = good/total if total else 0.0\n    return score, f\"one-sentence+length OK for {good}/{total} rows\""}, {"type": "code", "name": "Signal/Change Diversity & NC Share", "description": "Require at least one UW and one OW overall, and at least 50% of rows marked NC for change.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    def parse_export(text):\n        m = re.search(r'DATA_EXPORT_START(.*?)DATA_EXPORT_END', text, re.S)\n        if not m:\n            return []\n        block = m.group(1).strip()\n        lines = [ln.strip() for ln in block.splitlines() if ln.strip()]\n        if not lines or lines[0] != 'section|group|sub_asset|signal|change|conviction|justification':\n            return []\n        rows = []\n        for ln in lines[1:]:\n            parts = ln.split('|')\n            if len(parts) != 7:\n                continue\n            rows.append(parts)\n        return rows\n\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No primary document.\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        return 0.0, \"Unable to read PDF.\"\n    rows = parse_export(text)\n    if not rows:\n        return 0.0, \"Export missing or unparsable.\"\n\n    total = len(rows)\n    sigs = [r[3] for r in rows]\n    chg = [r[4] for r in rows]\n    have_uw = any(s == 'UW' for s in sigs)\n    have_ow = any(s == 'OW' for s in sigs)\n    nc_share = (sum(1 for c in chg if c == 'NC')/total) if total else 0.0\n\n    score_parts = 0\n    if have_uw:\n        score_parts += 1\n    if have_ow:\n        score_parts += 1\n    if nc_share >= 0.5:\n        score_parts += 1\n    score = score_parts/3\n    return score, f\"UW={have_uw}, OW={have_ow}, NC_share={nc_share:.2f}\""}, {"type": "code", "name": "Macro Context & Quarter References Present", "description": "Verify PDF text references: \u201cQ1 2025\u201d, \u201cMarch 31, 2025\u201d, and mentions of global growth improvement, Fed rate cuts, and healthy economy.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No primary document.\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        return 0.0, \"Unable to read PDF.\"\n    if not text:\n        return 0.0, \"Empty PDF text.\"\n\n    tl = text.lower()\n    checks = {\n        'q1_2025': ('q1 2025' in tl),\n        'march_31_2025': ('march 31, 2025' in tl),\n        'global_growth_improving': ('global growth' in tl and any(w in tl for w in ['improving','improvement','slight improvement'])),\n        'fed_rate_cuts': ('fed' in tl and any(w in tl for w in ['rate-cut', 'rate cutting', 'cutting cycle', 'rate cuts', 'cut rates'])),\n        'healthy_economy': ('healthy' in tl and 'econom' in tl)\n    }\n    score = sum(1 for v in checks.values() if v)/len(checks)\n    fb = ', '.join([k for k, v in checks.items() if v])\n    return score, f\"Found: {fb}\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Professional Quality & Strategic Coherence (LLM)", "description": "Holistic assessment of presentation quality, clarity, and strategic coherence given the minimal macro changes and stated context.", "is_required": false, "max_points": 1.5, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Quality, Clarity, and Strategic Value", "description": "Evaluate the professional quality and usefulness of the document for an institutional audience.", "weight": 1.5, "judge_prompt": "Assess the QUALITY (not the structure) of the PDF as an institutional CIO deliverable. Consider:\n- Professional presentation: clean layout, readable tables, consistent typography, clear labeling of sections and sub-sections.\n- Clarity and usefulness: one-sentence justifications are concise, specific, and decision-useful; signals are easy to scan.\n- Strategic coherence: top (Main Asset Classes) and bottom (Equities/Fixed Income/Currency) views are aligned; rationale reflects slight global growth improvement, Fed cutting cycle, and healthy economy; minimal quarter-over-quarter changes are sensible.\n- Appropriateness for investors: tone, precision, and risk framing suitable for a capital markets expectations report.\n\nScoring guidance (be decisive):\n- 1.5: Highly professional and coherent; justifications are crisp and valuable; clear alignment across sections.\n- 1.0: Generally strong but with minor issues in clarity or alignment.\n- 0.5: Mixed quality; noticeable clarity gaps or inconsistent signals vs narrative.\n- 0.0: Unprofessional or confusing; poor investor usability.", "expectation": "A clear, professional CIO-style PDF with crisp, aligned justifications and investor-ready presentation."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "0ec25916-1b5c-4bfe-93d3-4e103d860f3a", "rubric": {"category_name": "ED SBAR Template - Registered Nurse Handover (Health Care and Social Assistance)", "rationale": "Self-documenting, staged evaluation: Stage 1 enforces an exact 1-page SBAR PDF template structure so verification is possible. Stage 2 performs mixed code/LLM checks for clinical prompt coverage and safety-critical details (allergies, arrival date/time, investigations) and documentation fields. Stage 3 assesses professional presentation and ED usability.", "max_total_score": 18.0, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate (LLM-only)", "description": "Gate: Output must be a 1-page PDF SBAR template formatted as a 2-column by 4-row table, titled exactly as specified, with required sections and documentation space. Only structure/presence is judged here (not content quality).", "is_required": true, "max_points": 5.0, "min_score_to_pass": 3.5, "rules": [{"type": "llm_judge", "name": "Structured 1-page SBAR PDF Template Requirement", "description": "Validates that the candidate output is a single-page PDF with the required SBAR table and documentation space as specified.", "weight": 5.0, "judge_prompt": "You are checking ONLY the format and structural completeness of the candidate output. Do not judge content quality or correctness. Use flexible matching for headers and minor wording differences, but enforce the shape strictly.\n\nRequirements to PASS (structure only):\n1) File format and length\n   - Must be a PDF (not Word, not image-only without text). If DOCX is provided, treat as insufficient for this gate unless explicitly exported PDF is present.\n   - Must be exactly 1 page.\n\n2) Title\n   - The document must have the exact title text (case-insensitive match allowed): \"SBAR Template Emergency Department\". Title should be clearly visible near the top.\n\n3) SBAR Table Layout\n   - A clearly rendered table that is two columns by four rows (2x4) for the SBAR blocks.\n     \u2022 Allow small formatting variations such as a separate header row or merged title cell, but the main SBAR content must be in 4 distinct rows corresponding to S, B, A, R.\n   - First column lists the four SBAR building blocks as row headers: Situation, Background, Assessment, Recommendations (singular or plural acceptable; minor variants like \"Recommendation\" are acceptable).\n   - For each of the four SBAR rows, the first column must contain at least two brief guiding points describing what that block should include (e.g., bullet points or short sub-points).\n\n4) Prompts and writable space in second column\n   - For each SBAR row, the second column must contain clinical prompts plus blank, lined spaces (e.g., underscores or dotted lines) to write in details. The writable spaces must be visually apparent under the prompts.\n\n5) Top documentation space (above the table or clearly at the start)\n   - A small section prompting the nurse to state their name and department at the beginning of the call (e.g., \"Your Name:\" and \"Department:\").\n   - Also include documentation prompts to record handover metadata useful for notes: receiving professional\u2019s name/role or receiving department, and date/time of handover.\n\n6) Inclusion of key safety-critical prompts (presence only, not placement):\n   - Somewhere in the prompts (likely under Situation/Background/Assessment), the following must appear as prompts (exact wording flexible):\n     \u2022 Allergies\n     \u2022 Patient arrival date/time (e.g., \"Arrival time\" or \"Presented at\" with date/time)\n     \u2022 Nursing investigations/interventions performed (e.g., ECG, IV cannula, bloods, interventions)\n\nScoring (0.0\u20135.0):\n- 5.0: All structural requirements 1\u20136 satisfied.\n- 4.0: All structural requirements except one minor item (e.g., one SBAR row has only one guiding point; or top documentation is missing one of the metadata prompts but still includes nurse name and department).\n- 3.0: Generally correct format (PDF, 1 page, 2-column SBAR table with S/B/A/R rows) but missing multiple supporting elements (e.g., guiding points in more than one row, or writable lines not clearly present in one or two rows, or only two of the three safety-critical prompts).\n- 1.0: Attempted SBAR-like document but fails key shape elements (not 1 page, table not in 2x4 structure, missing multiple SBAR rows, or missing top documentation AND missing most safety-critical prompts).\n- 0.0: Not a PDF or not a recognizable SBAR table structure.\n\nOnly evaluate structure/format and presence. Do not verify medical correctness or design quality here.", "expectation": "A single-page PDF titled \"SBAR Template Emergency Department\", containing a 2-column by 4-row SBAR table with guiding points in the first column and writable prompts in the second column, plus a top documentation section for nurse identity and handover metadata, and inclusion of the three safety-critical prompts."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification: Clinical Prompt Coverage and Documentation Consistency", "description": "Now that structure is present, verify key clinical prompts and documentation elements are included and reasonably mapped to SBAR blocks. Uses mixed code + LLM checks.", "is_required": false, "max_points": 9.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Key Safety Prompts Present (Allergies, Arrival Date/Time, Nursing Investigations)", "description": "Detects presence of safety-critical prompts: allergies; arrival date/time; and nursing investigations/interventions (e.g., ECG, IV cannula, bloods). Partial credit for partial coverage.", "weight": 2.0, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_document:\\n            return 0.0, 'No document output'\\n\\n        text = ''\\n        try:\\n            if output.file_extension.lower() == '.pdf':\\n                text = context.files.read_pdf_text(output.id) or ''\\n            elif output.file_extension.lower() == '.docx':\\n                text = context.files.read_docx_text(output.id) or ''\\n            else:\\n                text = context.files.read_text(output.id) or ''\\n        except Exception:\\n            return 0.0, 'Could not extract text'\\n\\n        t = text.lower()\\n\\n        # Allergies\\n        has_allergies = 'allerg' in t\\n\\n        # Arrival date/time: look for arrival/presenting + time/date cues\\n        arrival_terms = any(x in t for x in ['arrival', 'arrived', 'presented', 'presentation'])\\n        time_cues = any(x in t for x in ['time', 'date', 'dd/mm', 'mm/dd', 'hh:mm']) or \\\n                    bool(re.search(r'\\b\\d{1,2}[:\\.]\\d{2}\\b', t)) or \\\n                    bool(re.search(r'\\b\\d{1,2}/\\d{1,2}/\\d{2,4}\\b', t))\\n        has_arrival_dt = arrival_terms and time_cues\\n\\n        # Nursing investigations/interventions\\n        inv_keywords = ['investigat', 'intervention', 'iv', 'cannula', 'ecg', 'bloods', 'labs', 'urinalysis', 'wound', 'catheter']\\n        has_investigations = any(k in t for k in inv_keywords)\\n\\n        count = sum([has_allergies, has_arrival_dt, has_investigations])\\n        score = count / 3.0\\n        feedback = f\"Allergies={has_allergies}, ArrivalDT={has_arrival_dt}, Investigations={has_investigations}\"\\n        return score, feedback\\n    except Exception as e:\\n        return 0.0, f'Error: {e}'"}, {"type": "code", "name": "SBAR Block Prompt Coverage (at least 2 cues per block)", "description": "Checks that each SBAR block vicinity contains at least 2 relevant prompt keywords. Fuzzy, windowed text search after each header.", "weight": 2.0, "code": "import re\\n\\ndef _extract_text(context, output):\\n    if output.file_extension.lower() == '.pdf':\\n        return context.files.read_pdf_text(output.id) or ''\\n    elif output.file_extension.lower() == '.docx':\\n        return context.files.read_docx_text(output.id) or ''\\n    else:\\n        return context.files.read_text(output.id) or ''\\n\\ndef evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_document:\\n            return 0.0, 'No document output'\\n        try:\\n            text = _extract_text(context, output)\\n        except Exception:\\n            return 0.0, 'Could not extract text'\\n\\n        t = text.lower()\\n\\n        blocks = {\\n            'situation': ['patient', 'identifier', 'mrn', 'dob', 'age', 'reason', 'presenting', 'transfer', 'from', 'to', 'isolation', 'code status', 'dnr', 'alert', 'falls'],\\n            'background': ['history', 'allerg', 'medication', 'arrival', 'date', 'time', 'diagnosis', 'past', 'surgery', 'baseline', 'language', 'interpreter', 'risk'],\\n            'assessment': ['vital', 'bp', 'hr', 'rr', 'temp', 'spo2', 'ews', 'news', 'mews', 'glucose', 'pain', 'finding', 'labs', 'result', 'imaging', 'ecg', 'iv', 'line'],\\n            'recommendation': ['recommend', 'request', 'need', 'plan', 'monitor', 'observation', 'frequency', 'escalation', 'review', 'consult', 'admit', 'transfer', 'pending', 'task', 'order', 'follow']\\n        }\\n\\n        score_count = 0\\n        feedback_parts = []\\n        for name, keywords in blocks.items():\\n            # Find header position\\n            m = re.search(name, t)\\n            if not m:\\n                # Try plural/singular variants for recommendation(s)\\n                if name == 'recommendation':\\n                    m = re.search('recommendations?', t)\\n            if not m:\\n                feedback_parts.append(f\"{name}: header not found\")\\n                continue\\n            start = m.end()\\n            window = t[start:start+500]  # look ahead window\\n            hits = set([k for k in keywords if k in window])\\n            ok = len(hits) >= 2\\n            score_count += 1 if ok else 0\\n            feedback_parts.append(f\"{name}: {len(hits)} cues ({', '.join(list(hits)[:5])})\")\\n\\n        ratio = score_count / 4.0\\n        return ratio, '; '.join(feedback_parts)\\n    except Exception as e:\\n        return 0.0, f'Error: {e}'"}, {"type": "code", "name": "Writable Lines Present (underscores/dotted lines)", "description": "Detects presence of blank, lined spaces for handwriting (e.g., ________ or ........).", "weight": 1.0, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_document:\\n            return 0.0, 'No document output'\\n        try:\\n            if output.file_extension.lower() == '.pdf':\\n                text = context.files.read_pdf_text(output.id) or ''\\n            elif output.file_extension.lower() == '.docx':\\n                text = context.files.read_docx_text(output.id) or ''\\n            else:\\n                text = context.files.read_text(output.id) or ''\\n        except Exception:\\n            return 0.0, 'Could not extract text'\\n        t = text\\n        has_lines = bool(re.search(r'[_\\.]{5,}', t))\\n        return 1.0 if has_lines else 0.0, f\"writable_lines={has_lines}\"\\n    except Exception as e:\\n        return 0.0, f'Error: {e}'"}, {"type": "code", "name": "Top Call Documentation Fields (Name, Department, Receiver, Date/Time)", "description": "Checks for prompts to document nurse identity and handover metadata. Partial credit if some fields present.", "weight": 1.0, "code": "def evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_document:\\n            return 0.0, 'No document output'\\n        try:\\n            if output.file_extension.lower() == '.pdf':\\n                text = context.files.read_pdf_text(output.id) or ''\\n            elif output.file_extension.lower() == '.docx':\\n                text = context.files.read_docx_text(output.id) or ''\\n            else:\\n                text = context.files.read_text(output.id) or ''\\n        except Exception:\\n            return 0.0, 'Could not extract text'\\n        t = text.lower()\\n        # Nurse identity\\n        has_name = 'name' in t\\n        has_dept = 'department' in t or 'unit' in t or 'ward' in t\\n        # Receiver metadata\\n        has_receiver = any(x in t for x in ['received by', 'receiver', 'recipient', 'receiving', 'clinician'])\\n        # Date/Time\\n        has_dt = 'date' in t and 'time' in t\\n        count = 0\\n        if has_name and has_dept:\\n            count += 1\\n        if has_receiver:\\n            count += 1\\n        if has_dt:\\n            count += 1\\n        ratio = count / 3.0\\n        return ratio, f\"identity={has_name and has_dept}, receiver={has_receiver}, date_time={has_dt}\"\\n    except Exception as e:\\n        return 0.0, f'Error: {e}'"}, {"type": "code", "name": "Title Presence: \"SBAR Template Emergency Department\"", "description": "Checks exact title presence (case-insensitive).", "weight": 0.5, "code": "def evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_document:\\n            return 0.0, 'No document output'\\n        try:\\n            if output.file_extension.lower() == '.pdf':\\n                text = context.files.read_pdf_text(output.id) or ''\\n            elif output.file_extension.lower() == '.docx':\\n                text = context.files.read_docx_text(output.id) or ''\\n            else:\\n                text = context.files.read_text(output.id) or ''\\n        except Exception:\\n            return 0.0, 'Could not extract text'\\n        t = text.lower()\\n        ok = 'sbar template emergency department' in t\\n        return 1.0 if ok else 0.0, f\"title_ok={ok}\"\\n    except Exception as e:\\n        return 0.0, f'Error: {e}'"}, {"type": "code", "name": "Evidence of Referencing (optional footer)", "description": "Optional credit if at least 2 of the provided credible sources are cited anywhere (e.g., in a small footer).", "weight": 0.5, "code": "def evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_document:\\n            return 0.0, 'No document output'\\n        try:\\n            if output.file_extension.lower() == '.pdf':\\n                text = context.files.read_pdf_text(output.id) or ''\\n            elif output.file_extension.lower() == '.docx':\\n                text = context.files.read_docx_text(output.id) or ''\\n            else:\\n                text = context.files.read_text(output.id) or ''\\n        except Exception:\\n            return 0.0, 'Could not extract text'\\n        t = text.lower()\\n        hosts = ['nes.nhs.scot', 'ncbi.nlm.nih.gov', 'wiley.com', 'nursingprocess.org', 'gemr.org', 'england.nhs.uk']\\n        count = sum(1 for h in hosts if h in t)\\n        ratio = min(1.0, count / 2.0) if count > 0 else 0.0\\n        return ratio, f\"refs_found={count}\"\\n    except Exception as e:\\n        return 0.0, f'Error: {e}'"}, {"type": "llm_judge", "name": "Clinical Mapping and Prompt Appropriateness (LLM)", "description": "LLM checks that prompts are sensibly placed under the correct SBAR blocks and cover typical ED transfer needs.", "weight": 2.0, "judge_prompt": "Now that the structural gate is passed, check the mapping and coverage of clinical prompts to SBAR blocks. Use clinical reasoning but only for appropriateness (not advanced medical advice). Consider:\n\n- Situation: Should prompt for patient identifiers and immediate situation (e.g., reason for transfer, location from/to, urgency/alerts). These belong in Situation, not Background.\n- Background: Should include history/diagnoses/medications/allergies and relevant timing such as arrival date/time. Allergies should appear here or clearly within Background-oriented content.\n- Assessment: Should include current assessment: vital signs (BP/HR/RR/Temp/SpO2), NEWS/MEWS if present, pain, relevant results (labs, ECG, imaging), IV/lines/drains, nursing investigations/interventions performed.\n- Recommendation(s): Should capture the ask/plan: what is needed from receiving team, monitoring frequency, pending tasks, escalation plan, contact details.\n\nScoring (0.0\u20132.0):\n- 2.0: Clear, appropriate placement for most prompts with comprehensive coverage across all four blocks (each block has at least 2 sensible prompts specific to that block). Safety-critical items (allergies, arrival date/time, investigations) are placed in plausible blocks (allergies in Background; arrival date/time in Situation/Background; investigations in Assessment).\n- 1.0: Generally good but with minor misplacements or thin coverage in one block.\n- 0.5: Several prompts misplaced or one SBAR block largely missing clinically relevant prompts.\n- 0.0: Prompts are largely mismatched to blocks or major omissions across blocks.\n\nOnly judge mapping and coverage coherence; do not penalize style or formatting here.", "expectation": "Prompts align with standard SBAR usage for ED transfers; critical items are included under appropriate sections."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Usability (LLM)", "description": "Holistic assessment of professional presentation, readability, and ED usability as a printable 1-page guide.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Print Usability", "description": "Assess visual clarity, readability, and print readiness of the 1-page template for ED phone-side use.", "weight": 3.0, "judge_prompt": "Evaluate the overall presentation quality for a 1-page printable guide placed beside ED phones:\n- Readability: clear typography, adequate font size, good contrast, concise bullets.\n- Table clarity: visible 2-column structure, clear row headings, consistent alignment.\n- Writable spaces: lines are sufficiently long and spaced for handwriting; not cramped.\n- Brevity and scannability: prompts are short, action-oriented, and easy to follow during a call.\n- Safety and professionalism: no misleading or unsafe clinical statements; neutral, professional tone.\n\nScoring (0.0\u20133.0):\n- 3.0: Highly readable, professional, and immediately usable at the point of care.\n- 2.0: Generally good with minor issues (slightly dense text or marginal spacing).\n- 1.0: Usable but cluttered or with noticeable formatting shortcomings.\n- 0.0: Poorly formatted or hard to use in real time.", "expectation": "Clean, professional, print-ready SBAR template with clear prompts and adequate handwriting space."}, {"type": "llm_judge", "name": "Clarity and ED Context Appropriateness", "description": "Assesses whether language and examples fit ED handover context and are understandable to ED nursing staff.", "weight": 1.0, "judge_prompt": "Assess clarity and ED appropriateness:\n- Language: clear, jargon-light, unambiguous prompts suited to ED nurses.\n- ED relevance: prompts reflect ED transfers (e.g., acuity, isolation status, immediate risks, pending tasks).\n- Inclusivity and brevity: short, directive phrases suitable for time-pressured handovers.\n\nScoring (0.0\u20131.0):\n- 1.0: Clear and well-tailored to ED handovers.\n- 0.5: Mostly clear with minor ED-context gaps.\n- 0.0: Vague, generic, or ill-suited to ED context.", "expectation": "Concise, clear prompts that align with ED handover needs and workflows."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "fd3ad420-6f7d-43b1-a990-c0c5c047d071", "rubric": {"category_name": "Real Estate Broker Compensation PDF Rubric", "rationale": "This is a document-style task (reports/proposals) with a required one-page PDF that outlines a compensation model. Following the self-documenting approach: Stage 1 uses an LLM judge to strictly enforce the required output shape (PDF/DOCX, one page, and specific section headers), enabling straightforward verification. Stage 2 uses code rules to validate textual correctness and internal consistency (presence of required entities/states, numeric terms, and plausibility checks) based on the structure established in Stage 1. Stage 3 uses an LLM judge to assess professional quality, clarity, and appropriateness for a new multi-state brokerage.", "max_total_score": 10.0, "stages": [{"name": "Stage 1: Shape and Format Gate", "description": "LLM-only gate enforcing required document format, page length, and section structure so verification is possible.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "One-Page Broker Compensation PDF Structure", "description": "Check that the output is a one-page PDF (or DOCX as fallback) with required sections and key structural elements.", "weight": 4.0, "judge_prompt": "You are evaluating whether the candidate output satisfies strict STRUCTURE requirements for a one-page Broker Compensation document.\n\nCheck ONLY structure/format/presence (not quality or correctness). Be flexible with exact header wording but ensure meaning matches.\n\nRequirements:\n1) Format\n- Prefer PDF. DOCX acceptable but scores lower.\n- Must be a professional document (not a spreadsheet, not plain text/markdown).\n- Exactly one page (single page). If >1 page or <1 page, score lower.\n\n2) Required sections (visible headers):\n- \"Purpose\" (or equivalent: \"Objective\", \"Intent\", \"Purpose of this document\")\n- \"Commission Split Structure\" (or equivalent: \"Commission Structure\", \"Compensation Structure\", \"Commission Splits\")\n- \"Summary\" (or equivalent: \"Conclusion\", \"Recap\")\n\n3) Key inclusions (structural presence, not correctness):\n- The firm name \"Sample Realty\" appears.\n- The relevant states are mentioned: FL/Florida, GA/Georgia, NC/North Carolina (any reasonable variant).\n- The Commission Split Structure section contains role-level subsections or bullets for: Qualifying Brokers, Agents, and Associate Brokers (labels can vary slightly, e.g., \"Qualifying Broker\", \"Agent/Sales Agent\", \"Associate Broker\").\n- The Commission Split Structure includes numeric terms (e.g., percentages like 70%, split forms like 70/30, or fees). Do not validate the math\u2014just visible presence of numeric terms in that section.\n\nScoring (out of 4.0):\n- 4.0: PDF, exactly 1 page, all 3 required sections clearly labeled, firm name shown, states mentioned (FL, GA, NC), commission section includes role breakdown (Qualifying Broker, Agent, Associate Broker) AND includes numeric terms.\n- 3.0: PDF or DOCX, exactly 1 page, all 3 sections present, and at least two of: firm name present; states mentioned; commission section has role breakdown; numeric terms present.\n- 2.0: Document is PDF or DOCX but missing one required section OR not exactly 1 page; includes at least one of: firm name, states, role breakdown, or numeric terms in commission section.\n- 0.0: Wrong format (not PDF/DOCX) OR missing multiple required sections OR clearly not a one-page professional document.\n\nOnly evaluate structure and presence. Do not judge content quality or correctness.", "expectation": "A one-page professional PDF with Purpose, Commission Split Structure, and Summary sections; mentions Sample Realty and FL/GA/NC; and contains role-level commission elements for Qualifying Brokers, Agents, and Associate Brokers with numeric terms."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Verification and Consistency Checks", "description": "Code-based checks for presence of required entities and reasonable numeric patterns enabled by the structured document.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Mentions Firm and States", "description": "Verify the document text mentions 'Sample Realty' and the target states (FL/Florida, GA/Georgia, NC/North Carolina).", "weight": 1.2, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 1.2\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n\n    # Accept document or text format for robustness\n    text = None\n    try:\n        if output.is_document:\n            # Try PDF then DOCX\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    text = None\n        if text is None and output.is_text_format:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = None\n    except Exception:\n        text = None\n\n    if not text:\n        return 0.0\n\n    t = text.lower()\n    score = 0.0\n    feedback_parts = []\n\n    # Firm name\n    if 'sample realty' in t:\n        score += 0.4\n        feedback_parts.append('Firm name found')\n    else:\n        feedback_parts.append('Firm name missing')\n\n    # States: FL/Florida, GA/Georgia, NC/North Carolina\n    def present(patterns):\n        return any(re.search(p, t) for p in patterns)\n\n    fl = present([r'\\bflorida\\b', r'\\bfl\\b'])\n    ga = present([r'\\bgeorgia\\b', r'\\bga\\b'])\n    nc = present([r'\\bnorth\\s+carolina\\b', r'\\bnc\\b'])\n\n    state_count = sum([fl, ga, nc])\n    score += (0.4 * state_count / 3.0)\n    feedback_parts.append(f\"States mentioned: {state_count}/3\")\n\n    # Qualifying Broker mention check (structural correctness)\n    if 'qualifying broker' in t:\n        score += 0.4\n        feedback_parts.append('Qualifying Broker mentioned')\n    else:\n        feedback_parts.append('Qualifying Broker not found')\n\n    # Bound score\n    score = max(0.0, min(score, weight))\n    return score, '; '.join(feedback_parts)\n"}, {"type": "code", "name": "Commission Roles and Numeric Terms Present", "description": "Check that the Commission Split Structure section mentions the key roles and includes numeric expressions (percentages or split ratios).", "weight": 1.4, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 1.4\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n\n    # Read document text\n    text = None\n    try:\n        if output.is_document:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    text = None\n        if text is None and output.is_text_format:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = None\n    except Exception:\n        text = None\n\n    if not text:\n        return 0.0\n\n    t = text.lower()\n\n    # Isolate Commission Split Structure section heuristically\n    start_match = re.search(r'(commission\\s+(split\\s+)?structure|compensation\\s+structure|commission\\s+structure)', t)\n    if start_match:\n        start = start_match.start()\n        # Look for next major header keyword after start\n        next_match = re.search(r'\\n\\s*(summary|conclusion|recap|purpose|objective|overview)\\b', t[start+1:])\n        if next_match:\n            end = start + 1 + next_match.start()\n            section = t[start:end]\n        else:\n            section = t[start:]\n    else:\n        section = t\n\n    score = 0.0\n    feedback = []\n\n    # Role presence in section\n    roles_found = 0\n    # Qualifying Broker\n    if re.search(r'qualifying\\s+broker', section):\n        roles_found += 1\n    # Agent synonyms\n    if re.search(r'\\b(agent|sales\\s*agent|salesperson)\\b', section):\n        roles_found += 1\n    # Associate Broker synonyms\n    if re.search(r'associate\\s+broker', section):\n        roles_found += 1\n\n    score += (0.7 * (roles_found / 3.0))\n    feedback.append(f\"Roles found: {roles_found}/3\")\n\n    # Numeric terms: percentages like 70%, or split ratios like 70/30, 70:30, 70-30\n    percents = re.findall(r'\\b(\\d{1,3})\\s*%\\b', section)\n    splits = re.findall(r'\\b(\\d{1,3})\\s*[/:\\-]\\s*(\\d{1,3})\\b', section)\n\n    numeric_count = len(percents) + len(splits)\n    # Award up to 0.7 for numeric presence (full if >=2 numeric items)\n    num_score = 0.0\n    if numeric_count >= 2:\n        num_score = 0.7\n    elif numeric_count == 1:\n        num_score = 0.35\n    else:\n        num_score = 0.0\n    score += num_score\n    feedback.append(f\"Numeric items found: {numeric_count}\")\n\n    score = max(0.0, min(score, weight))\n    return score, '; '.join(feedback)\n"}, {"type": "code", "name": "Numeric Plausibility (Bounds and Split Sums)", "description": "Validate percentages are within [0,100] and that at least one split pair sums to ~100 (+/- 2).", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 1.0\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n\n    # Read text\n    text = None\n    try:\n        if output.is_document:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    text = None\n        if text is None and output.is_text_format:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = None\n    except Exception:\n        text = None\n\n    if not text:\n        return 0.0\n\n    t = text.lower()\n    # Try to focus on commission section\n    import re\n    start_match = re.search(r'(commission\\s+(split\\s+)?structure|compensation\\s+structure|commission\\s+structure)', t)\n    if start_match:\n        start = start_match.start()\n        next_match = re.search(r'\\n\\s*(summary|conclusion|recap|purpose|objective|overview)\\b', t[start+1:])\n        if next_match:\n            end = start + 1 + next_match.start()\n            section = t[start:end]\n        else:\n            section = t[start:]\n    else:\n        section = t\n\n    score = 0.0\n    feedback = []\n\n    # Percentages bounds\n    percents = [int(p) for p in re.findall(r'\\b(\\d{1,3})\\s*%\\b', section)]\n    if percents:\n        in_bounds = [0 <= p <= 100 for p in percents]\n        if all(in_bounds):\n            score += 0.5\n            feedback.append('All percentages within [0,100]')\n        else:\n            bad = [p for p in percents if not (0 <= p <= 100)]\n            feedback.append(f'Out-of-range percents: {bad}')\n    else:\n        feedback.append('No percentage values found')\n\n    # Split pairs sum check (70/30, 70:30, 70-30)\n    splits = re.findall(r'\\b(\\d{1,3})\\s*[/:\\-]\\s*(\\d{1,3})\\b', section)\n    split_ok = False\n    for a, b in splits:\n        try:\n            s = int(a) + int(b)\n            if abs(s - 100) <= 2:\n                split_ok = True\n                break\n        except Exception:\n            pass\n    if split_ok:\n        score += 0.5\n        feedback.append('Found a split pair summing ~100')\n    else:\n        if splits:\n            feedback.append('No split pairs summing ~100')\n        else:\n            feedback.append('No split pairs found')\n\n    score = max(0.0, min(score, weight))\n    return score, '; '.join(feedback)\n"}, {"type": "code", "name": "Fee/Term Elements Presence", "description": "Check for common compensation elements like caps, fees, or overrides to ensure structural completeness of the model.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 0.4\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n\n    text = None\n    try:\n        if output.is_document:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    text = None\n        if text is None and output.is_text_format:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = None\n    except Exception:\n        text = None\n\n    if not text:\n        return 0.0\n\n    t = text.lower()\n    keywords = [\n        'cap', 'monthly fee', 'desk fee', 'transaction fee', 'e&o', 'errors and omissions',\n        'technology fee', 'compliance fee', 'onboarding', 'signing bonus', 'referral',\n        'override', 'mentor', 'training', 'profit share', 'revenue share', 'minimum',\n        'maximum', 'draw', 'retainer', 'base salary', 'stipend'\n    ]\n\n    found = any(k in t for k in keywords)\n    score = weight if found else 0.0\n    feedback = 'Found fee/term element' if found else 'No fee/term elements found'\n    return score, feedback\n"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3: Quality and Professionalism", "description": "LLM judge assesses clarity, professionalism, and usefulness for a new multi-state brokerage.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Practicality", "description": "Assess the overall tone, clarity, readability, and practical usefulness of the compensation structure for Sample Realty.", "weight": 2.0, "judge_prompt": "Evaluate the final document for professional quality and practical usefulness. The audience is a new brokerage (Sample Realty) expanding into FL, GA, and NC with a non-licensed founder.\n\nConsider:\n- Clarity and conciseness suitable for a one-page policy (headers, bullets, readable layout)\n- Professional tone and consistent terminology (e.g., \"Qualifying Broker\", \"Agent\", \"Associate Broker\")\n- Practical guidance: easily actionable structure (e.g., clearly described splits, when they apply, and any caps/fees)\n- Alignment with a multi-state startup brokerage context (not state-specific legal advice, but generally applicable structure)\n\nScoring (out of 2.0):\n- 2.0: Highly professional, clear, concise, and actionable; well-structured and suitable for implementation.\n- 1.0: Generally professional and understandable but could be clearer or more actionable.\n- 0.0: Unclear, unprofessional, or not useful for implementation.\n\nDo not re-check structure already scored in Stage 1. Focus on overall quality and appropriateness.", "expectation": "A concise, professional, and actionable one-page compensation structure tailored to a new multi-state brokerage."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "90edba97-74f0-425a-8ff6-8b93182eb7cb", "rubric": {"category_name": "Dialysis Monthly Lab Tracker and Standing-Order Application (Registered Nurse)", "rationale": "This rubric enforces a self-documenting, verifiable Excel-based workflow. Stage 1 (LLM-only) mandates a precise workbook structure so downstream checks are trivial. Stage 2 uses code rules to verify correctness against CMS-aligned standing orders (Aranesp, iron/venofer not explicitly enforced due to absent protocol details, TUMS, phosphate binders) and operational rules (repeat labs for low Kt/V), plus data integrity and bounds. Stage 3 evaluates professional presentation and auditability.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 Gate \u2014 Structured Workbook Shape (LLM-only)", "description": "Enforce exact workbook structure enabling verification. Must be an Excel workbook with the required sheets and columns, patient/provider mapping, and monthly documentation of orders/changes.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Workbook Structure and Required Sections Present", "description": "Check that the candidate output is a single Excel workbook with mandated sheets, columns, and content presence to enable verification of monthly standing-order decisions.", "weight": 4.0, "judge_prompt": "You are verifying the SHAPE ONLY of the submitted workbook (not correctness of calculations). Inspect the output and score based on the following structural requirements. Be flexible with minor naming variations but strict about required presence.\n\nFORMAT REQUIREMENTS\n- Must be an Excel file (.xlsx or .xls). Not a PDF, DOCX, CSV, or images.\n- Contains a unified tracker for all 7 named patients across 12 months.\n\nREQUIRED SHEETS (names may vary slightly; matching examples provided)\n1) Patient Directory (acceptable names: \"Patient Directory\", \"Patients\", \"Roster\")\n   - Table columns must include: Patient Name, MRN (Medical Record Number), Provider (Doctor/Physician/MD)\n   - Must list exactly these 7 patients with providers:\n     \u2022 Dr. Joe: Cash Stonewater; Fred Fintmore\n     \u2022 Dr. Johnson: Betty Brite; Tina Lee Bell\n     \u2022 Dr. Lee: Eric Bird; Homer Sandson\n     \u2022 Dr. Michael: Jessica Rashmore\n   - MRN must be present (not blank) for each of the 7 patients.\n\n2) Monthly Tracker (acceptable names: \"Monthly Tracker- Patient Lab Results\", \"Monthly Tracker\", \"Lab Tracker\")\n   - Single, consistent table (wide or long) with at least these columns:\n     \u2022 Patient Name\n     \u2022 MRN\n     \u2022 Provider\n     \u2022 Month (as month name or a date or YYYY-MM)\n     \u2022 HGB (Hemoglobin)\n     \u2022 Kt/V (Kt/V or KtV)\n     \u2022 Serum Calcium (mg/dL) (labeled \"Calcium\" or \"Ca\")\n     \u2022 Serum Phosphorus (mg/dL) (labeled \"Phosphorus\", \"Phos\", or \"Phosphate\")\n   - For each of the 7 patients, there must be 12 monthly entries (one per month in the year).\n\n3) Orders & Changes (acceptable names: \"Orders & Changes\", \"Treatment Plan\", \"Medication Changes\", \"Plan of Care\")\n   - A per-patient, per-month documentation table with columns that capture:\n     \u2022 Patient Name, MRN, Month\n     \u2022 Triggering Condition(s) or Rationale referencing labs (e.g., HGB<10, Kt/V<1.2, Ca 7.9\u20138.4, Phos range)\n     \u2022 Order Type (Start/Continue/Decrease/Repeat)\n     \u2022 Medication/Order Name (e.g., Aranesp, Venofer, Renvela, Phoslo, TUMS, Repeat Labs)\n     \u2022 Dose and Frequency/Route (e.g., \"10 mcg IVP each treatment\", \"2 tabs PO 3x/week\", or meal-based dosing)\n     \u2022 Order Source (Standing Order vs Provider)\n   - There must be an explicit row each month per patient either documenting changes or \u201cNo change\u201d.\n\n4) Protocol Log (acceptable names: \"Standing Orders Applied\", \"Protocol Log\", \"Protocol Mapping\")\n   - A brief table or bullet list mapping each standing-order rule to its trigger and the corresponding order that should be applied (e.g., \"HGB < 10.0 \u2192 Aranesp 10 mcg IVP each treatment\"). This is a reference within the workbook.\n\n5) Optional: Summary/Overview (acceptable names: \"Summary\", \"Dashboard\")\n   - Aggregated counts of orders by type/provider, or KPIs. This is optional and should not reduce score if absent.\n\nSCORING\n- 4.0: Excel file with all 4 required sheets present and correctly structured; directory lists exactly the 7 patients with correct providers and non-blank MRNs; Monthly Tracker has all required columns and 12 months for each patient; Orders & Changes captures monthly entries with rationale/trigger and order details; Protocol Log exists.\n- 3.0: Excel with most required elements; exactly 7 patients with correct provider mapping; Monthly Tracker table + Orders & Changes exist and are usable, but minor omissions (e.g., a column label slightly ambiguous or 1\u20132 missing monthly entries total across all patients) OR Protocol Log is minimal but present.\n- 2.0: Excel format is correct but missing a key required sheet or several required columns (e.g., missing Month or Kt/V), or patients/provider mapping incomplete (e.g., only 5\u20136 patients), or >2 monthly entries missing across the cohort.\n- 1.0: Excel exists but structure is largely noncompliant (missing multiple required sheets/columns; cannot identify Orders & Changes; <12 months widely missing).\n- 0.0: Not Excel OR completely wrong structure.\n\nOnly evaluate structure and presence. Do not judge correctness of medical decisions or calculations.", "expectation": "A single Excel workbook containing the specified sheets, columns, 7 patient roster with providers, 12 months per patient, monthly orders/changes entries, and a protocol log mapping triggers to orders."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification of Correctness (Code checks)", "description": "Deterministic checks on data integrity and conformance with standing orders using the enforced structure from Stage 1.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 1.5, "rules": [{"type": "code", "name": "Data Completeness and Provider Mapping", "description": "Verify 7 required patients exist, provider mapping is correct, MRNs present, and 12 months per patient recorded in the tracker.", "weight": 0.8, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n        sheet_names = [str(s) for s in xls.sheet_names]\n\n        def pick_sheet(names, inkl, excl=None):\n            excl = excl or []\n            ln = [n for n in names if any(k in n.lower() for k in inkl) and not any(e in n.lower() for e in excl)]\n            return ln[0] if ln else None\n\n        tracker_sheet = pick_sheet(sheet_names, [\"monthly\", \"tracker\", \"lab\"])\n        if tracker_sheet is None:\n            # fallback: first sheet\n            tracker_sheet = sheet_names[0]\n        df = pd.read_excel(file_path, sheet_name=tracker_sheet)\n        # Normalize columns\n        cols_lc = {c: str(c).strip().lower() for c in df.columns}\n        df.columns = list(cols_lc.values())\n\n        def get_col(cands):\n            for c in df.columns:\n                for k in cands:\n                    if k in c:\n                        return c\n            return None\n\n        c_patient = get_col([\"patient\"])\n        c_mrn = get_col([\"mrn\", \"medical record\"])\n        c_provider = get_col([\"provider\", \"doctor\", \"physician\", \"md\"])\n        c_month = get_col([\"month\", \"date\", \"collection\"])  # flexible month/date\n\n        # Scoring parts\n        score = 0.0\n        max_score = 0.8\n        part_patients = 0.3\n        part_mapping = 0.3\n        part_mrn = 0.1\n        part_months = 0.1\n\n        # Expected mapping\n        expected = {\n            \"cash stonewater\": \"dr. joe\",\n            \"fred fintmore\": \"dr. joe\",\n            \"betty brite\": \"dr. johnson\",\n            \"tina lee bell\": \"dr. johnson\",\n            \"eric bird\": \"dr. lee\",\n            \"homer sandson\": \"dr. lee\",\n            \"jessica rashmore\": \"dr. michael\",\n        }\n\n        # Patient presence\n        if c_patient is None:\n            return 0.0, \"Missing patient name column in tracker.\"\n        names = df[c_patient].astype(str).str.strip().str.lower().dropna()\n        present = set([n for n in names if n in expected])\n        score += part_patients * (len(present) / len(expected))\n\n        # Provider mapping correctness (if provider column exists)\n        if c_provider is not None:\n            sub = df[[c_patient, c_provider]].dropna()\n            sub[c_patient] = sub[c_patient].astype(str).str.strip().str.lower()\n            sub[c_provider] = sub[c_provider].astype(str).str.strip().str.lower()\n            correct = 0\n            counted = set()\n            for p, exp in expected.items():\n                rows = sub[sub[c_patient] == p]\n                if len(rows) > 0:\n                    # if any row has correct provider, count as correct\n                    if any(exp in v for v in rows[c_provider].tolist()):\n                        correct += 1\n                    counted.add(p)\n            score += part_mapping * (correct / len(expected))\n        else:\n            # no provider column -> no credit for mapping\n            pass\n\n        # MRN presence per expected patient (if MRN column exists)\n        if c_mrn is not None:\n            sub = df[[c_patient, c_mrn]].dropna()\n            sub[c_patient] = sub[c_patient].astype(str).str.strip().str.lower()\n            nonblank = 0\n            for p in expected.keys():\n                vals = sub.loc[sub[c_patient] == p, c_mrn].astype(str).str.strip()\n                if len(vals) > 0 and any(v not in [\"\", \"nan\", \"none\"] for v in vals):\n                    nonblank += 1\n            score += part_mrn * (nonblank / len(expected))\n\n        # 12 months per patient (if month/date column exists)\n        if c_month is not None:\n            df[c_month + \"__norm\"] = pd.to_datetime(df[c_month], errors=\"coerce\").dt.to_period(\"M\").astype(str)\n            counts = df.groupby(df[c_patient].astype(str).str.strip().str.lower())[c_month + \"__norm\"].nunique()\n            have = 0\n            for p in expected.keys():\n                if p in counts.index and counts.loc[p] >= 12:\n                    have += 1\n            score += part_months * (have / len(expected))\n\n        return max(0.0, min(max_score, score)), f\"Computed data completeness score: {score:.3f} / {max_score}\"\n    except Exception as e:\n        return 0.0, f\"Error in completeness check: {e}\""}, {"type": "code", "name": "Plausible Lab Bounds", "description": "Check that key lab values lie within plausible physiological ranges to catch data-entry errors.", "weight": 0.6, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n        sheet_names = [str(s) for s in xls.sheet_names]\n\n        def pick_sheet(names, inkl):\n            ln = [n for n in names if any(k in n.lower() for k in inkl)]\n            return ln[0] if ln else names[0]\n\n        tracker_sheet = pick_sheet(sheet_names, [\"monthly\", \"tracker\", \"lab\"])\n        df = pd.read_excel(file_path, sheet_name=tracker_sheet)\n        df.columns = [str(c).strip().lower() for c in df.columns]\n\n        def col_like(keys):\n            for c in df.columns:\n                for k in keys:\n                    if k in c:\n                        return c\n            return None\n\n        c_hgb = col_like([\"hgb\", \"hemoglobin\"]) \n        c_ktv = col_like([\"kt/v\", \"ktv\"]) \n        c_ca = col_like([\"calcium\", \" ca\"]) \n        c_phos = col_like([\"phosph\", \"phos\", \"phosphate\"]) \n\n        checks = []\n        if c_hgb is not None:\n            s = pd.to_numeric(df[c_hgb], errors='coerce')\n            ok = s.between(5, 20) | s.isna()\n            checks.append(ok.mean())\n        if c_ktv is not None:\n            s = pd.to_numeric(df[c_ktv], errors='coerce')\n            ok = s.between(0, 3) | s.isna()\n            checks.append(ok.mean())\n        if c_ca is not None:\n            s = pd.to_numeric(df[c_ca], errors='coerce')\n            ok = s.between(6, 12) | s.isna()\n            checks.append(ok.mean())\n        if c_phos is not None:\n            s = pd.to_numeric(df[c_phos], errors='coerce')\n            ok = s.between(1, 10) | s.isna()\n            checks.append(ok.mean())\n\n        if not checks:\n            return 0.0, \"No lab columns found for bounds check.\"\n        score = float(np.mean(checks)) * 0.6\n        return score, f\"Bounds pass rate: {np.mean(checks):.2%}\"\n    except Exception as e:\n        return 0.0, f\"Error in bounds check: {e}\""}, {"type": "code", "name": "Aranesp for HGB < 10.0", "description": "If monthly HGB < 10.0, there should be an order mentioning Aranesp 10 mcg IVP each treatment (at minimum: contains 'Aranesp').", "weight": 0.8, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n        sheet_names = [str(s) for s in xls.sheet_names]\n\n        def pick(names, inkl):\n            cands = [n for n in names if any(k in n.lower() for k in inkl)]\n            return cands[0] if cands else names[0]\n\n        tracker_sheet = pick(sheet_names, [\"monthly\", \"tracker\", \"lab\"])\n        orders_sheet = pick(sheet_names, [\"order\", \"change\", \"treatment\", \"plan\", \"medication\"])\n\n        df = pd.read_excel(file_path, sheet_name=tracker_sheet)\n        df.columns = [str(c).strip().lower() for c in df.columns]\n        do = pd.read_excel(file_path, sheet_name=orders_sheet)\n        do.columns = [str(c).strip().lower() for c in do.columns]\n\n        def col_like(dataf, keys):\n            for c in dataf.columns:\n                for k in keys:\n                    if k in c:\n                        return c\n            return None\n\n        pcol = col_like(df, [\"patient\"]) \n        mcol = col_like(df, [\"month\", \"date\", \"collection\"]) \n        hgb = col_like(df, [\"hgb\", \"hemoglobin\"]) \n        if pcol is None or hgb is None:\n            return 0.0, \"Missing patient or HGB column.\"\n\n        # Normalize month\n        if mcol is not None:\n            df[mcol+\"__m\"] = pd.to_datetime(df[mcol], errors='coerce').dt.to_period('M').astype(str)\n        else:\n            df[mcol+\"__m\"] = \"\"\n        df[pcol] = df[pcol].astype(str).str.strip().str.lower()\n\n        # Build orders text index by (patient, month)\n        op = col_like(do, [\"patient\"]) or pcol\n        om = col_like(do, [\"month\", \"date\", \"collection\"]) \n        do_text_cols = [c for c in do.columns if do[c].dtype == object]\n        do = do.copy()\n        do[\"__row_text\"] = do.apply(lambda r: \" \\n \".join([str(r[c]) for c in do_text_cols if pd.notnull(r[c])]).lower(), axis=1)\n        do_keyed = {}\n        # Normalize key columns\n        if op is not None:\n            do[op] = do[op].astype(str).str.strip().str.lower()\n        if om is not None:\n            do[om+\"__m\"] = pd.to_datetime(do[om], errors='coerce').dt.to_period('M').astype(str)\n        for _, r in do.iterrows():\n            key = (r.get(op, \"\"), r.get((om+\"__m\") if om is not None else None, \"\"))\n            do_keyed.setdefault(key, []).append(r[\"__row_text\"])\n\n        # Evaluate triggers\n        d = df.copy()\n        d[hgb] = pd.to_numeric(d[hgb], errors='coerce')\n        needs = d[d[hgb] < 10.0]\n        total = len(needs)\n        if total == 0:\n            return 0.8, \"No HGB<10 cases; full credit.\"\n        hits = 0\n        for _, r in needs.iterrows():\n            key = (r[pcol], r[mcol+\"__m\"] if mcol is not None else \"\")\n            texts = \" \\n \".join(do_keyed.get(key, []))\n            if (\"aranesp\" in texts):\n                hits += 1\n        score = 0.8 * (hits / total)\n        return score, f\"Aranesp matches: {hits}/{total}\"\n    except Exception as e:\n        return 0.0, f\"Error in Aranesp rule: {e}\""}, {"type": "code", "name": "Repeat Labs for Kt/V < 1.2", "description": "If Kt/V < 1.2, ensure an order to repeat labs next month exists (e.g., contains 'repeat' and 'lab' or 'recheck').", "weight": 0.4, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n        sheet_names = [str(s) for s in xls.sheet_names]\n        def pick(names, inkl):\n            cands = [n for n in names if any(k in n.lower() for k in inkl)]\n            return cands[0] if cands else names[0]\n        tracker_sheet = pick(sheet_names, [\"monthly\", \"tracker\", \"lab\"])\n        orders_sheet = pick(sheet_names, [\"order\", \"change\", \"treatment\", \"plan\", \"medication\"])\n        df = pd.read_excel(file_path, sheet_name=tracker_sheet)\n        do = pd.read_excel(file_path, sheet_name=orders_sheet)\n        df.columns = [str(c).strip().lower() for c in df.columns]\n        do.columns = [str(c).strip().lower() for c in do.columns]\n        def col_like(dataf, keys):\n            for c in dataf.columns:\n                for k in keys:\n                    if k in c:\n                        return c\n            return None\n        pcol = col_like(df, [\"patient\"]) \n        mcol = col_like(df, [\"month\", \"date\", \"collection\"]) \n        ktv = col_like(df, [\"kt/v\", \"ktv\"]) \n        if pcol is None or ktv is None:\n            return 0.0, \"Missing patient or Kt/V column.\"\n        if mcol is not None:\n            df[mcol+\"__m\"] = pd.to_datetime(df[mcol], errors='coerce').dt.to_period('M').astype(str)\n        else:\n            df[mcol+\"__m\"] = \"\"\n        df[pcol] = df[pcol].astype(str).str.strip().str.lower()\n        op = col_like(do, [\"patient\"]) or pcol\n        om = col_like(do, [\"month\", \"date\", \"collection\"]) \n        do_text_cols = [c for c in do.columns if do[c].dtype == object]\n        do = do.copy()\n        do[\"__row_text\"] = do.apply(lambda r: \" \".join([str(r[c]) for c in do_text_cols if pd.notnull(r[c])]).lower(), axis=1)\n        do_keyed = {}\n        if op is not None:\n            do[op] = do[op].astype(str).str.strip().str.lower()\n        if om is not None:\n            do[om+\"__m\"] = pd.to_datetime(do[om], errors='coerce').dt.to_period('M').astype(str)\n        for _, r in do.iterrows():\n            key = (r.get(op, \"\"), r.get((om+\"__m\") if om is not None else None, \"\"))\n            do_keyed.setdefault(key, []).append(r[\"__row_text\"])\n        d = df.copy()\n        d[ktv] = pd.to_numeric(d[ktv], errors='coerce')\n        needs = d[d[ktv] < 1.2]\n        total = len(needs)\n        if total == 0:\n            return 0.4, \"No Kt/V<1.2 cases; full credit.\"\n        def has_repeat(txt):\n            txt = txt.lower()\n            return (\"repeat\" in txt and \"lab\" in txt) or (\"recheck\" in txt) or (\"re-test\" in txt or \"retest\" in txt)\n        hits = 0\n        for _, r in needs.iterrows():\n            key = (r[pcol], r[mcol+\"__m\"] if mcol is not None else \"\")\n            texts = \" \\n \".join(do_keyed.get(key, []))\n            if has_repeat(texts):\n                hits += 1\n        score = 0.4 * (hits / total)\n        return score, f\"Repeat-labs matches: {hits}/{total}\"\n    except Exception as e:\n        return 0.0, f\"Error in Kt/V repeat rule: {e}\""}, {"type": "code", "name": "TUMS for low-normal Calcium (7.9\u20138.4 mg/dL)", "description": "If Calcium is between 7.9 and 8.4 mg/dL inclusive, ensure an order mentions TUMS (calcium carbonate) 2 tabs PO three times a week (min check: mention of TUMS/calcium carbonate).", "weight": 0.6, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n        sheet_names = [str(s) for s in xls.sheet_names]\n        def pick(names, inkl):\n            cands = [n for n in names if any(k in n.lower() for k in inkl)]\n            return cands[0] if cands else names[0]\n        tracker_sheet = pick(sheet_names, [\"monthly\", \"tracker\", \"lab\"])\n        orders_sheet = pick(sheet_names, [\"order\", \"change\", \"treatment\", \"plan\", \"medication\"])\n        df = pd.read_excel(file_path, sheet_name=tracker_sheet)\n        do = pd.read_excel(file_path, sheet_name=orders_sheet)\n        df.columns = [str(c).strip().lower() for c in df.columns]\n        do.columns = [str(c).strip().lower() for c in do.columns]\n        def col_like(dataf, keys):\n            for c in dataf.columns:\n                for k in keys:\n                    if k in c:\n                        return c\n            return None\n        pcol = col_like(df, [\"patient\"]) \n        mcol = col_like(df, [\"month\", \"date\", \"collection\"]) \n        cal = col_like(df, [\"calcium\", \" ca\"]) \n        if pcol is None or cal is None:\n            return 0.0, \"Missing patient or Calcium column.\"\n        if mcol is not None:\n            df[mcol+\"__m\"] = pd.to_datetime(df[mcol], errors='coerce').dt.to_period('M').astype(str)\n        else:\n            df[mcol+\"__m\"] = \"\"\n        df[pcol] = df[pcol].astype(str).str.strip().str.lower()\n        op = col_like(do, [\"patient\"]) or pcol\n        om = col_like(do, [\"month\", \"date\", \"collection\"]) \n        do_text_cols = [c for c in do.columns if do[c].dtype == object]\n        do = do.copy()\n        do[\"__row_text\"] = do.apply(lambda r: \" \\n \".join([str(r[c]) for c in do_text_cols if pd.notnull(r[c])]).lower(), axis=1)\n        do_keyed = {}\n        if op is not None:\n            do[op] = do[op].astype(str).str.strip().str.lower()\n        if om is not None:\n            do[om+\"__m\"] = pd.to_datetime(do[om], errors='coerce').dt.to_period('M').astype(str)\n        for _, r in do.iterrows():\n            key = (r.get(op, \"\"), r.get((om+\"__m\") if om is not None else None, \"\"))\n            do_keyed.setdefault(key, []).append(r[\"__row_text\"])\n        d = df.copy()\n        d[cal] = pd.to_numeric(d[cal], errors='coerce')\n        needs = d[(d[cal] >= 7.9) & (d[cal] <= 8.4)]\n        total = len(needs)\n        if total == 0:\n            return 0.6, \"No Ca 7.9\u20138.4 cases; full credit.\"\n        hits = 0\n        for _, r in needs.iterrows():\n            key = (r[pcol], r[mcol+\"__m\"] if mcol is not None else \"\")\n            texts = \" \\n \".join(do_keyed.get(key, []))\n            if (\"tums\" in texts) or (\"calcium carbonate\" in texts):\n                hits += 1\n        score = 0.6 * (hits / total)\n        return score, f\"TUMS matches: {hits}/{total}\"\n    except Exception as e:\n        return 0.0, f\"Error in TUMS rule: {e}\""}, {"type": "code", "name": "Phosphate Binder Selection by Provider", "description": "For Dr. Joe/Dr. Johnson: if Phos 5.6\u20137.4 mg/dL, initiate Renvela 800 mg with meals (min check: 'Renvela' or 'sevelamer' mention). For Dr. Michael/Dr. Lee: if Phos 5.5\u20137.4 mg/dL, initiate Phoslo 667 mg, 2 tabs each meal (min check: 'Phoslo' or 'calcium acetate' mention).", "weight": 0.8, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n        sheet_names = [str(s) for s in xls.sheet_names]\n        def pick(names, inkl):\n            cands = [n for n in names if any(k in n.lower() for k in inkl)]\n            return cands[0] if cands else names[0]\n        tracker_sheet = pick(sheet_names, [\"monthly\", \"tracker\", \"lab\"])\n        orders_sheet = pick(sheet_names, [\"order\", \"change\", \"treatment\", \"plan\", \"medication\"])\n        df = pd.read_excel(file_path, sheet_name=tracker_sheet)\n        do = pd.read_excel(file_path, sheet_name=orders_sheet)\n        df.columns = [str(c).strip().lower() for c in df.columns]\n        do.columns = [str(c).strip().lower() for c in do.columns]\n        def col_like(dataf, keys):\n            for c in dataf.columns:\n                for k in keys:\n                    if k in c:\n                        return c\n            return None\n        pcol = col_like(df, [\"patient\"]) \n        mcol = col_like(df, [\"month\", \"date\", \"collection\"]) \n        prov = col_like(df, [\"provider\", \"doctor\", \"physician\", \"md\"]) \n        phos = col_like(df, [\"phosph\", \"phos\", \"phosphate\"]) \n        if pcol is None or prov is None or phos is None:\n            return 0.0, \"Missing patient, provider, or phosphorus column.\"\n        if mcol is not None:\n            df[mcol+\"__m\"] = pd.to_datetime(df[mcol], errors='coerce').dt.to_period('M').astype(str)\n        else:\n            df[mcol+\"__m\"] = \"\"\n        df[pcol] = df[pcol].astype(str).str.strip().str.lower()\n        df[prov] = df[prov].astype(str).str.strip().str.lower()\n        op = col_like(do, [\"patient\"]) or pcol\n        om = col_like(do, [\"month\", \"date\", \"collection\"]) \n        do_text_cols = [c for c in do.columns if do[c].dtype == object]\n        do = do.copy()\n        do[\"__row_text\"] = do.apply(lambda r: \" \\n \".join([str(r[c]) for c in do_text_cols if pd.notnull(r[c])]).lower(), axis=1)\n        do_keyed = {}\n        if op is not None:\n            do[op] = do[op].astype(str).str.strip().str.lower()\n        if om is not None:\n            do[om+\"__m\"] = pd.to_datetime(do[om], errors='coerce').dt.to_period('M').astype(str)\n        for _, r in do.iterrows():\n            key = (r.get(op, \"\"), r.get((om+\"__m\") if om is not None else None, \"\"))\n            do_keyed.setdefault(key, []).append(r[\"__row_text\"])\n        d = df.copy()\n        d[phos] = pd.to_numeric(d[phos], errors='coerce')\n        # Providers groups\n        group_renvela = [\"dr. joe\", \"joe\"] + [\"dr. johnson\", \"johnson\"]\n        group_phoslo = [\"dr. michael\", \"michael\"] + [\"dr. lee\", \"lee\"]\n        # Trigger subsets\n        needs_renvela = d[(d[prov].apply(lambda x: any(g in x for g in group_renvela))) & (d[phos] >= 5.6) & (d[phos] <= 7.4)]\n        needs_phoslo = d[(d[prov].apply(lambda x: any(g in x for g in group_phoslo))) & (d[phos] >= 5.5) & (d[phos] <= 7.4)]\n        total = len(needs_renvela) + len(needs_phoslo)\n        if total == 0:\n            return 0.8, \"No binder-trigger cases; full credit.\"\n        hits = 0\n        for _, r in needs_renvela.iterrows():\n            key = (r[pcol], r[mcol+\"__m\"] if mcol is not None else \"\")\n            texts = \" \\n \".join(do_keyed.get(key, []))\n            if (\"renvela\" in texts) or (\"sevelamer\" in texts):\n                hits += 1\n        for _, r in needs_phoslo.iterrows():\n            key = (r[pcol], r[mcol+\"__m\"] if mcol is not None else \"\")\n            texts = \" \\n \".join(do_keyed.get(key, []))\n            if (\"phoslo\" in texts) or (\"calcium acetate\" in texts):\n                hits += 1\n        score = 0.8 * (hits / total)\n        return score, f\"Binder matches: {hits}/{total}\"\n    except Exception as e:\n        return 0.0, f\"Error in binder rule: {e}\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Auditability", "description": "Holistic evaluation of presentation quality, clarity, and audit trail for clinical use.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Presentation, Traceability, and Professionalism", "description": "Assess whether the workbook is professionally organized and easy to audit: clear labeling, readable tables, consistent units, and traceable rationale from labs to orders.", "weight": 2.0, "judge_prompt": "Evaluate the overall professional quality and auditability of the Excel workbook. Consider:\n- Readability and organization (clear headers, consistent units for labs, freeze panes/filters helpful but optional).\n- Traceability: Can a reviewer easily follow from monthly lab values to the corresponding orders/changes? Do Orders & Changes rows cite the triggering lab values and rule names (standing orders vs provider orders)?\n- Completeness of monthly documentation: Is \"No change\" documented when appropriate? Are rationales specific (e.g., \u201cHGB 9.6 \u2192 Aranesp per standing order\u201d)?\n- Consistency in naming (meds, doses, frequencies) and appropriate clinical language.\n- Presence of a brief Protocol Log mapping triggers to actions enhances auditability.\n\nScoring:\n- 2.0: Highly professional, easy to audit; consistent terminology; clear lab-to-order trace; strong protocol mapping.\n- 1.0: Generally clear but with minor inconsistencies or occasional missing rationales; still reasonably auditable.\n- 0.5: Barely sufficient; difficult to trace changes across months; inconsistent labels.\n- 0.0: Poorly organized; cannot trace labs to orders; unclear or unprofessional presentation.", "expectation": "A clean, well-structured workbook where each monthly change is clearly justified, labeled, and traceable; terminology and units are consistent; audit trail is obvious."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "401a07f1-d57e-4bb0-889b-22de8c900f0e", "rubric": {"category_name": "Science Editorial (Editors - Information)", "rationale": "This rubric enforces a self-documenting workflow for a 500-word science editorial delivered as a DOCX or PDF. Stage 1 (LLM-only) mandates an exact, verifiable document structure that enables downstream checks. Stage 2 mixes code and LLM rules to verify measurable properties (length, source domains, linkage) and basic evidentiary correctness. Stage 3 assesses overall professional quality and audience fit.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Format and Structural Gate (LLM-only)", "description": "Gate that enforces required document format and structural elements so verification is possible.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Document Structure and Presence of Required Elements", "description": "Output must be a professionally formatted editorial document (PDF or DOCX) with headline, standfirst, body, links to reputable sources, and a call to action.", "weight": 4.0, "judge_prompt": "You are the Stage 1 gatekeeper. Evaluate ONLY structure/format and presence of required elements (not content quality or factual correctness).\n\nCheck the candidate output against these requirements:\n\nFormat Requirements:\n- The output must be a single editorial delivered as a PDF or DOCX file (not plain text, not Excel).\n- Professionally formatted: readable fonts, paragraphs laid out as an article (no bullet-only drafts).\n- Target length: approximately 500 words. For structure checking, accept 450\u2013600 words as compliant.\n\nRequired Structural Elements (presence-only check):\n1) Headline: A clear headline at the top.\n2) Standfirst (deck/subheading): A short summary paragraph or 1\u20133 sentences placed immediately under the headline.\n3) Body copy: A coherent prose article (not notes), with clear narrative flow across multiple paragraphs.\n4) Hyperlinks: At least two in-text links to reputable outlets drawn from this list: Nature (nature.com), Science (science.org), Scientific American (scientificamerican.com), The Guardian (theguardian.com). The links must appear embedded in the copy (not just a bare references list at the end). If URLs are printed visibly or are clickable, both are acceptable.\n5) Call to action: A clear call to action in the concluding section (e.g., urging researchers, policymakers, funders, or institutions to do something).\n6) Guardian style guideline followed at a basic structural level: presence of headline + standfirst; do NOT assess deeper style quality here.\n\nScoring (be flexible on naming like \u201cdeck\u201d for standfirst; judge by visible structure):\n- 4.0: PDF/DOCX format; headline and standfirst present near top; 450\u2013600 words; at least two in-text links from the approved outlets; body is coherent prose; clear call to action near the end.\n- 3.0: PDF/DOCX format with headline and standfirst; body ~400\u2013650 words; has at least two links from approved outlets; either minor structural miss (e.g., call to action is weak but present, or links are present but one appears in a references block) but overall clearly usable.\n- 2.0: PDF/DOCX with headline and body but missing one core element (e.g., no standfirst or only one qualifying link or call to action not evident). Still clearly an editorial article.\n- 0.0: Wrong format (not PDF/DOCX) OR missing multiple core elements (e.g., no headline, or <300 words, or zero qualifying links) such that verification is not feasible.\n\nReturn a score only based on STRUCTURE and PRESENCE, not correctness or writing quality.", "expectation": "A DOCX or PDF editorial with headline, standfirst, 450\u2013600 words of body text, at least two in-text links from approved outlets, and a concluding call to action."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Compliance Verification", "description": "Verify measurable properties and basic evidentiary linkage using code and LLM checks.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Word Count Compliance (Overall)", "description": "Checks total word count of the document text for 500-word target (tolerances allowed).", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float or (float, str)\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n\n    text = \"\"\n    try:\n        # Try DOCX first\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = \"\"\n\n    if not text or len(text.strip()) == 0:\n        return 0.0, \"Unable to extract text from the document.\"\n\n    # Word count across the entire document (robust to punctuation/apostrophes)\n    words = re.findall(r\"\\b[\\w'\u2019\\-]+\\b\", text)\n    wc = len(words)\n\n    # Scoring bands relative to 500-word target\n    weight = 1.5\n    if 450 <= wc <= 600:\n        score = weight\n    elif (400 <= wc < 450) or (600 < wc <= 650):\n        score = weight * 0.75\n    elif (350 <= wc < 400) or (650 < wc <= 700):\n        score = weight * 0.5\n    else:\n        score = 0.0\n\n    return score, f\"Word count detected: {wc}.\""}, {"type": "code", "name": "Approved Source Links Present (Domain Check)", "description": "Finds URLs in extracted text and verifies at least two are from approved domains: nature.com, science.org, scientificamerican.com, theguardian.com.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float or (float, str)\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = \"\"\n\n    if not text or len(text.strip()) == 0:\n        return 0.0, \"Unable to extract text for link check.\"\n\n    urls = re.findall(r\"https?://[^\\s)\\]]+\", text)\n    allowed = [\"nature.com\", \"science.org\", \"scientificamerican.com\", \"theguardian.com\"]\n\n    def is_allowed(u):\n        m = re.match(r\"https?://([^/]+)\", u)\n        if not m:\n            return False\n        host = m.group(1).lower()\n        return any(host == d or host.endswith(\".\" + d) for d in allowed)\n\n    allowed_count = sum(1 for u in urls if is_allowed(u))\n\n    weight = 1.0\n    if allowed_count >= 2:\n        return weight, f\"Found {allowed_count} approved-domain link(s).\"\n    elif allowed_count == 1:\n        return weight * 0.6, \"Only one approved-domain link found.\"\n    else:\n        return 0.0, \"No approved-domain links detected in extracted text. Note: if links are embedded without visible URLs, this may undercount.\""}, {"type": "llm_judge", "name": "Evidence and Linking Correctness", "description": "Assesses whether factual claims are supported by in-text links to approved outlets and whether a clear call to action is present.", "weight": 1.5, "judge_prompt": "Evaluate whether the editorial meets these correctness criteria (not writing quality):\n\n1) Linking and Source Compliance:\n   - At least two in-text hyperlinks to stories from approved outlets: Nature (nature.com), Science (science.org), Scientific American (scientificamerican.com), The Guardian (theguardian.com).\n   - Links are used substantively in the copy (supporting or contextualizing claims), not merely listed at the end.\n2) Evidentiary Support:\n   - Major claims, facts, or statistics cited in the piece are attributed or linked so a sub-editor can verify them.\n3) Call to Action:\n   - A clear call to action is present and tied to the editorial\u2019s argument.\n\nScoring:\n- 1.5: Meets all three conditions clearly (>=2 approved links, claims supported via links/attribution, clear call to action).\n- 1.0: Minor gaps (e.g., only one approved link, or one major claim lacks attribution) but overall adequately supported and includes a call to action.\n- 0.5: Evidence/linking is weak (e.g., links not from approved outlets or mostly listed at the end) OR call to action is vague.\n- 0.0: No meaningful linking/attribution to support claims and no clear call to action.", "expectation": "Clear in-text use of at least two approved hyperlinks; claims supported; explicit call to action tied to the evidence."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Editorial Quality and Audience Fit", "description": "Holistic assessment of professional quality, clarity, and suitability for an international research and policy audience, following Guardian style.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Quality, Narrative, and Style", "description": "Evaluates clarity of argument, narrative structure, tone, and suitability for an international audience; checks for basic Guardian-style sensibilities.", "weight": 2.0, "judge_prompt": "Assess the editorial\u2019s overall professional quality for an international science magazine with researchers and policymakers as readers. Consider:\n- Narrative arc: clear headline, engaging standfirst, coherent development, and purposeful conclusion.\n- Argument: clear opinion, logically developed, and balanced with accurate context.\n- Audience fit: international relevance, appropriate level of technicality, and policy awareness.\n- Tone and style: succinct, active, Guardian-style sensibilities (British English spelling, concise headline, minimal jargon). Do not penalize minor deviations if overall professional.\n\nScoring:\n- 2.0: Strong, polished editorial with clear narrative, persuasive argument grounded in accurate context, internationally relevant, and largely in Guardian-style tone.\n- 1.0: Solid but with noticeable issues (clarity, flow, tone, or audience targeting) that reduce impact.\n- 0.0: Unclear, poorly structured, or unsuitable for the audience.", "expectation": "A polished, persuasive editorial with a clear narrative arc, appropriate tone, and relevance to international research and policy readers."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "c657103b-b348-4496-a848-b2b7165d28b2", "rubric": {"category_name": "Finance & Insurance \u2014 Personal Financial Advisor: 8-Year Roth Conversion Strategy (Client Deck + Excel Model)", "rationale": "Self-documenting, staged rubric that forces verifiable artifacts: Stage 1 LLM gate mandates exact structure for both the 8-slide client presentation and the Excel model. Stage 2 mixes code checks for timeline, 8-year conversion window, RMD timing/factors, and tax-comparison presence; plus an LLM cross-reference check. Stage 3 assesses professional quality and client suitability.", "max_total_score": 12.0, "stages": [{"name": "Stage 1 \u2014 Shape/Format Gate (LLM-only)", "description": "Gate: Verify both required deliverables exist with the mandated structure so downstream verification is trivial. Only check structure/presence, not correctness.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "PowerPoint Structure: 8-Slide Client Deck", "description": "Presentation must exist as PPTX or exported PDF with exactly 8 slides covering required topics.", "weight": 2.0, "judge_prompt": "You are checking ONLY structure/format, not content quality.\n\nDeliverable A (Presentation) \u2014 Pass if the submission includes a PowerPoint (.pptx) or a PDF export of slides with the following:\n\nFormat requirements:\n- File type: .pptx or .pdf only for the deck\n- Exactly 8 slides\n- Professional visual template. Preferably uses a tunnel/step/linear process graphic (\"business digital tunnel\" style) where appropriate. Be flexible on naming; look for consistent, modern, corporate template use.\n\nRequired slide-level coverage (order can vary, names can be close variants):\n1) Title slide: Roth Conversion Strategy overview (client-facing)\n2) Purpose: Why implement an 8-year Roth conversion (tax planning goals)\n3) Suitable Candidate Profile: who benefits (e.g., MFJ, high-income retirees)\n4) Process Overview: steps and timing (8-year period) and how conversions work\n5) Timeline: explicitly signal 8-year schedule (2026\u20132033) aligned to \"Period 1\u20138\" and reference 2025 as setup/year 0\n6) Tax & RMD Impact: marginal bracket management; mention RMDs begin at age 72 per brief\n7) Estate/Heirs Benefits: emphasize tax-free distributions to heirs, estate exposure reduction\n8) Next Steps & Disclaimers: action items, general educational disclaimer (not tax/legal advice)\n\nScoring:\n- 2.0: Valid PPTX/PDF with exactly 8 slides AND all 8 topic areas covered (headers or clear content signals)\n- 1.5: Valid format with 8 slides but missing 1 required topic OR has 7\u20139 slides with all topics covered\n- 1.0: Valid format with 7\u20139 slides; \u22655 topics clearly covered\n- 0.5: Valid format but <7 slides OR \u22644 topics covered\n- 0.0: No valid PPTX/PDF deck\n\nOnly verify PRESENCE/STRUCTURE and slide count/topics coverage. Do not assess correctness or quality.", "expectation": "A clean 8-slide client-facing deck, using a professional template, covering the specified eight topics."}, {"type": "llm_judge", "name": "Excel Model Structure: Inputs \u2022 Plan \u2022 RMD Factors \u2022 Summary", "description": "Workbook must exist as Excel and contain the mandated sheets and tables with identifiable columns enabling verification.", "weight": 2.0, "judge_prompt": "You are checking ONLY structure/format, not correctness.\n\nDeliverable B (Excel) \u2014 Pass if the submission includes an Excel workbook (.xlsx) with the following structure (be flexible with similar sheet names):\n\nRequired sheets and sections:\n1) Inputs & Assumptions (sheet name like: \"Inputs\", \"Assumptions\", or combined):\n   - A visible table with columns: [Parameter | Value | Source/Notes]\n   - Must include rows for: Starting balance YE 2025 = $3.5M; Filing status MFJ; Ages (64/65 in 2025, retiring end-2025); Annual non-IRA income $200k; Marginal tax brackets 32\u201335%; Return assumption 8%; 8-year conversion window 2026\u20132033; RMD start age 72; Source for RMD factors = IRS 2025 Uniform Lifetime Table.\n\n2) Conversion Plan / Projections (sheet name like: \"Conversion Plan\", \"Plan\", \"Projections\", \"Schedule\", or similar):\n   - Year-by-year table covering 2025 (Period 0) through 2054 (Period 29)\n   - Minimum identifiable columns (names can vary but meaning must be clear):\n     \u2022 Year\n     \u2022 Period (0\u201329 mapping with 2025=0)\n     \u2022 Age\n     \u2022 Traditional IRA balance (start/end)\n     \u2022 Roth balance (start/end) and a column for \"Conversion\" amount per year\n     \u2022 RMD amount(s) and associated taxes (at least for a baseline scenario)\n     \u2022 Marginal tax rate / tax impact columns\n     \u2022 Clear differentiation of baseline (RMD-only) vs with-conversion scenario (can be on same or separate columns)\n\n3) RMD Factors (sheet name like: \"RMD Factors\", \"Uniform Lifetime Table\", etc.):\n   - Table with columns: [Age | Distribution Period/Factor | Source]\n   - Reference to IRS 2025 Uniform Lifetime Table visible in sheet or Inputs\n\n4) Comparison Summary (sheet name like: \"Summary\", \"Comparison\", or \"Overview\"):\n   - Clear summary metrics comparing baseline vs with-conversion including: total/ cumulative taxes, after-tax distributions and/or ending balances, and an explicit metric or label indicating projected \"Tax Savings\" or comparable delta\n   - At least one chart or visual anywhere in the workbook (can be here or in another sheet). LLMs can see chart objects in the rendered workbook image; if present in any sheet, count it.\n\nScoring:\n- 2.0: Valid Excel + all 4 sheets present with the specified components; model spans 2025\u20132054 with Period 0\u201329 and conversion years identifiable\n- 1.5: Valid Excel + 3/4 core sheets present with required table structures\n- 1.0: Valid Excel + 2/4 core sheets present OR missing period mapping but year range present\n- 0.5: Valid Excel but only 1 core sheet or skeletal content\n- 0.0: No valid Excel workbook\n\nOnly check presence/structure enabling verification, not calculation correctness.", "expectation": "An Excel model with Inputs, Conversion Plan/Projections (2025\u20132054, Period 0\u201329), RMD Factors (IRS 2025 ULT), and a Comparison Summary with an explicit tax-savings metric and at least one chart."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness & Consistency Verification", "description": "Now that the shape is enforced, verify key mechanics deterministically and with a light LLM cross-check. Focus on timeline fidelity, 8-year conversion window, RMD timing and factors, and presence of tax comparison metrics.", "is_required": true, "max_points": 5.0, "min_score_to_pass": 2.5, "rules": [{"type": "code", "name": "Timeline Integrity: 2025\u20132054 with Period 0\u201329", "description": "Checks the plan/projection sheet for required year range and period mapping (2025=0 ... 2054=29).", "weight": 1.5, "code": "import pandas as pd\\nimport numpy as np\\nimport re\\n\\ndef evaluate(workflow, context):\\n    weight = 1.5\\n    def get_excel_res():\\n        prim = context.get_primary_output()\\n        if prim and getattr(prim, 'is_spreadsheet', False):\\n            return prim\\n        for r in context.get_all_outputs() or []:\\n            if getattr(r, 'is_spreadsheet', False):\\n                return r\\n        return None\\n    try:\\n        res = get_excel_res()\\n        if not res:\\n            return 0.0\\n        path = context.files.get_path(res.id)\\n        xlf = pd.ExcelFile(path)\\n        sheets = [s for s in xlf.sheet_names]\\n        # Pick likely plan/projection sheet\\n        cand_keys = ['plan','projection','convert','schedule','model','analysis']\\n        plan_sheet = None\\n        for s in sheets:\\n            sl = s.lower()\\n            if any(k in sl for k in cand_keys):\\n                plan_sheet = s\\n                break\\n        if plan_sheet is None:\\n            plan_sheet = sheets[0]\\n        df = pd.read_excel(path, sheet_name=plan_sheet)\\n        cols = [str(c) for c in df.columns]\\n        lcols = [c.lower() for c in cols]\\n        score_parts = []\\n        # Find Year and Period columns\\n        year_idx = next((i for i,c in enumerate(lcols) if 'year' in c), None)\\n        period_idx = next((i for i,c in enumerate(lcols) if 'period' in c), None)\\n        if year_idx is None:\\n            return 0.0\\n        years_raw = pd.to_numeric(df.iloc[:, year_idx], errors='coerce').dropna().astype(int)\\n        # Check presence of full range 2025..2054 (30 years)\\n        needed_years = set(range(2025, 2055))\\n        present_years = set(int(y) for y in years_raw.tolist())\\n        has_range = needed_years.issubset(present_years)\\n        score_parts.append(1 if has_range else 0)\\n        # Period mapping\\n        if period_idx is not None:\\n            periods = pd.to_numeric(df.iloc[:, period_idx], errors='coerce')\\n            ok_period_vals = set(range(0,30)).issubset(set(int(p) for p in periods.dropna().astype(int).tolist()))\\n            score_parts.append(1 if ok_period_vals else 0)\\n            # Check mapping consistency for at least 30 matched rows\\n            merged = df.copy()\\n            merged['__year__'] = pd.to_numeric(df.iloc[:, year_idx], errors='coerce')\\n            merged['__period__'] = pd.to_numeric(df.iloc[:, period_idx], errors='coerce')\\n            matched = merged.dropna(subset=['__year__','__period__'])\\n            # Count rows where period == year - 2025\\n            matched['__exp__'] = matched['__year__'] - 2025\\n            correct_map = (matched['__period__'].astype(int) == matched['__exp__'].astype(int))\\n            # Require at least 25 correctly mapped rows for full sub-credit\\n            score_parts.append(1 if correct_map.sum() >= 25 else 0)\\n        else:\\n            # No period column\\n            score_parts.append(0)\\n            score_parts.append(0)\\n        # Age monotonicity (sanity): if age column exists, check mostly increases by 1 each year consecutively\\n        age_idx = next((i for i,c in enumerate(lcols) if 'age' in c), None)\\n        if age_idx is not None:\\n            df2 = df.copy()\\n            df2['__year__'] = pd.to_numeric(df.iloc[:, year_idx], errors='coerce')\\n            df2 = df2.dropna(subset=['__year__']).sort_values('__year__')\\n            ages = pd.to_numeric(df2.iloc[:, age_idx], errors='coerce')\\n            if ages.notna().sum() >= 10:\\n                diffs = ages.diff().dropna()\\n                ok = (diffs.abs() <= 1.1).mean() >= 0.8\\n                score_parts.append(1 if ok else 0)\\n            else:\\n                score_parts.append(0)\\n        else:\\n            score_parts.append(0)\\n        # Aggregate score (5 checks)\n        total_checks = 5\n        achieved = sum(score_parts)\n        return float(weight * (achieved / total_checks))\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "8-Year Conversion Window (2026\u20132033)", "description": "Verifies that exactly eight years have positive Roth conversion amounts and that they occur within 2026\u20132033 inclusive.", "weight": 1.5, "code": "import pandas as pd\\nimport numpy as np\\n\\ndef evaluate(workflow, context):\\n    weight = 1.5\\n    def get_excel_res():\\n        prim = context.get_primary_output()\\n        if prim and getattr(prim, 'is_spreadsheet', False):\\n            return prim\\n        for r in context.get_all_outputs() or []:\\n            if getattr(r, 'is_spreadsheet', False):\\n                return r\\n        return None\\n    try:\\n        res = get_excel_res()\\n        if not res:\\n            return 0.0\\n        path = context.files.get_path(res.id)\\n        xlf = pd.ExcelFile(path)\\n        # Find likely plan sheet\\n        plan_sheet = None\\n        for s in xlf.sheet_names:\\n            sl = s.lower()\\n            if any(k in sl for k in ['plan','projection','convert','schedule','model']):\\n                plan_sheet = s\\n                break\\n        if plan_sheet is None:\\n            plan_sheet = xlf.sheet_names[0]\\n        df = pd.read_excel(path, sheet_name=plan_sheet)\\n        cols = [str(c) for c in df.columns]\\n        lcols = [c.lower() for c in cols]\\n        # Year column\\n        y_idx = next((i for i,c in enumerate(lcols) if 'year' in c), None)\\n        if y_idx is None:\\n            return 0.0\\n        years = pd.to_numeric(df.iloc[:, y_idx], errors='coerce')\\n        # Conversion columns (prefer ones with 'convert' or 'roth' & 'conversion')\\n        conv_cols = [i for i,c in enumerate(lcols) if ('convert' in c) or ('roth' in c and ('conv' in c or 'conversion' in c or 'xfer' in c))]\\n        if not conv_cols:\\n            # also consider columns with 'conversion amount' wording\n            conv_cols = [i for i,c in enumerate(lcols) if 'conversion' in c]\n        if not conv_cols:\\n            return 0.0\\n        conv_series = None\\n        # choose the first plausible conversion amount column with numeric data\\n        best = None\\n        max_nonzero = -1\\n        for idx in conv_cols:\\n            ser = pd.to_numeric(df.iloc[:, idx], errors='coerce').fillna(0)\\n            npos = (ser.abs() > 1e-6).sum()\\n            if npos > max_nonzero:\\n                max_nonzero = npos\\n                best = ser\\n        conv_series = best if best is not None else pd.Series(dtype=float)\\n        if conv_series.empty:\\n            return 0.0\\n        good = pd.DataFrame({'year': years, 'conv': conv_series})\\n        good = good.dropna(subset=['year']).copy()\\n        good['year'] = good['year'].astype(int)\\n        # Count years with positive (or non-zero) conversions\n        by_year = good.groupby('year')['conv'].apply(lambda s: (s.abs() > 1e-6).any()).astype(int)\n        active_years = [y for y,v in by_year.items() if v == 1]\n        count_ok = (len(active_years) == 8)\n        # Check range constraint (all between 2026 and 2033 inclusive)\n        in_range_ok = all(2026 <= y <= 2033 for y in active_years) and len(active_years) > 0\n        # Partial scoring: 60% for count, 40% for range\n        score = 0.0\n        score += 0.6 * weight if count_ok else 0.0\n        score += 0.4 * weight if in_range_ok else 0.0\n        return float(score)\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "RMD Timing (Age 72) + Factor Sanity", "description": "Validates that RMDs start in the year age reaches 72, are near-zero before, and that the RMD factor table exists and is plausible (monotone decreasing with age; age 72 factor in a reasonable range).", "weight": 1.0, "code": "import pandas as pd\\nimport numpy as np\\n\\ndef evaluate(workflow, context):\\n    weight = 1.0\\n    def get_excel_res():\\n        prim = context.get_primary_output()\\n        if prim and getattr(prim, 'is_spreadsheet', False):\\n            return prim\\n        for r in context.get_all_outputs() or []:\\n            if getattr(r, 'is_spreadsheet', False):\\n                return r\\n        return None\\n    try:\\n        res = get_excel_res()\\n        if not res:\\n            return 0.0\\n        path = context.files.get_path(res.id)\\n        xlf = pd.ExcelFile(path)\\n        sheets = xlf.sheet_names\\n        # Identify RMD factor sheet\n        rmd_sheet = None\n        for s in sheets:\n            sl = s.lower()\n            if 'rmd' in sl or 'uniform' in sl or 'lifetime' in sl or 'factor' in sl:\n                rmd_sheet = s\n                break\n        if rmd_sheet is None:\n            return 0.0\n        fdf = pd.read_excel(path, sheet_name=rmd_sheet)\n        # Guess columns for age and factor\n        fcols = [str(c) for c in fdf.columns]\n        fl = [c.lower() for c in fcols]\n        age_idx = next((i for i,c in enumerate(fl) if 'age' in c), None)\n        fac_idx = next((i for i,c in enumerate(fl) if 'factor' in c or 'distribution' in c), None)\n        if age_idx is None or fac_idx is None:\n            return 0.0\n        ages = pd.to_numeric(fdf.iloc[:, age_idx], errors='coerce').dropna().astype(int)\n        facs = pd.to_numeric(fdf.iloc[:, fac_idx], errors='coerce').dropna()\n        if ages.empty or facs.empty or len(ages) != len(facs):\n            return 0.0\n        # Sort by age and check monotone decreasing factors\n        ff = pd.DataFrame({'age': ages.values, 'f': facs.values}).dropna().drop_duplicates()\n        ff = ff.sort_values('age')\n        mono_ok = (ff['f'].diff().dropna() <= 0).mean() >= 0.9\n        # Check age 72 present and factor in plausible range [20, 30]\n        f72 = ff.loc[ff['age'] == 72, 'f']\n        f72_ok = False\n        if not f72.empty:\n            v = float(f72.iloc[0])\n            f72_ok = (20.0 <= v <= 30.0)\n        # Now check plan sheet RMD start timing\n        plan_sheet = None\n        for s in sheets:\n            sl = s.lower()\n            if any(k in sl for k in ['plan','projection','convert','schedule','model']):\n                plan_sheet = s\n                break\n        if plan_sheet is None:\n            plan_sheet = sheets[0]\n        df = pd.read_excel(path, sheet_name=plan_sheet)\n        cols = [str(c) for c in df.columns]\n        lcols = [c.lower() for c in cols]\n        # Year and Age columns\n        y_idx = next((i for i,c in enumerate(lcols) if 'year' in c), None)\n        age_idx2 = next((i for i,c in enumerate(lcols) if 'age' in c), None)\n        if y_idx is None:\n            return 0.0\n        years = pd.to_numeric(df.iloc[:, y_idx], errors='coerce')\n        # Compute first RMD year per brief if no age col: 2025 age 65 => age 72 at 2032\n        first_rmd_year_est = 2032\n        if age_idx2 is not None:\n            age_vals = pd.to_numeric(df.iloc[:, age_idx2], errors='coerce')\n            tmp = pd.DataFrame({'year': years, 'age': age_vals}).dropna()\n            cand = tmp.loc[tmp['age'] >= 72, 'year']\n            if not cand.empty:\n                first_rmd_year_est = int(cand.iloc[0])\n        # Identify an RMD amount column\n        rmd_col_idx = next((i for i,c in enumerate(lcols) if 'rmd' in c and ('tax' not in c and 'factor' not in c)), None)\n        if rmd_col_idx is None:\n            # fallback: any column with 'required minimum' or similar\n            rmd_col_idx = next((i for i,c in enumerate(lcols) if 'required' in c and 'minimum' in c), None)\n        if rmd_col_idx is None:\n            return 0.0\n        rmd_vals = pd.to_numeric(df.iloc[:, rmd_col_idx], errors='coerce').fillna(0)\n        dd = pd.DataFrame({'year': years, 'rmd': rmd_vals}).dropna()\n        before = dd[dd['year'] < first_rmd_year_est]['rmd']\n        after = dd[dd['year'] >= first_rmd_year_est]['rmd']\n        near_zero_before = (before.abs() <= 1.0).all() if len(before) > 0 else True\n        positive_after = (after > 0).any() if len(after) > 0 else False\n        subscores = [mono_ok, f72_ok, near_zero_before, positive_after]\n        return float(weight * (sum(1 for x in subscores if x) / 4.0))\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Tax Comparison Presence (Baseline vs With-Conversion)", "description": "Detects baseline vs with-conversion tax columns or an explicit tax-savings metric across sheets. Gives partial credit for weaker signals.", "weight": 0.5, "code": "import pandas as pd\\nimport numpy as np\\nimport re\\n\\ndef evaluate(workflow, context):\\n    weight = 0.5\\n    def get_excel_res():\\n        prim = context.get_primary_output()\\n        if prim and getattr(prim, 'is_spreadsheet', False):\\n            return prim\\n        for r in context.get_all_outputs() or []:\\n            if getattr(r, 'is_spreadsheet', False):\\n                return r\\n        return None\\n    try:\\n        res = get_excel_res()\\n        if not res:\\n            return 0.0\\n        path = context.files.get_path(res.id)\\n        xlf = pd.ExcelFile(path)\\n        sheets = xlf.sheet_names\\n        found_tax_cols = False\\n        found_dual_scenarios = False\\n        found_savings_metric = False\\n        # Look for summary-like sheet first\n        sum_sheet = None\n        for s in sheets:\n            sl = s.lower()\n            if any(k in sl for k in ['summary','comparison','overview']):\n                sum_sheet = s\n                break\n        if sum_sheet is not None:\n            sdf = pd.read_excel(path, sheet_name=sum_sheet, header=None)\n            # scan text for 'tax saving'\n            txt = ' '.join(str(x) for x in sdf.astype(str).values.ravel()).lower()\n            if 'tax saving' in txt or 'tax savings' in txt or 'savings' in txt:\n                # look for a number near it\n                nums = re.findall(r\"[-+]?\\d*[\\,]?\\d+\\.?\\d*\", txt)\n                if len(nums) > 0:\n                    found_savings_metric = True\n        # Inspect plan sheet columns\n        plan_sheet = None\n        for s in sheets:\n            sl = s.lower()\n            if any(k in sl for k in ['plan','projection','convert','schedule','model']):\n                plan_sheet = s\n                break\n        if plan_sheet is None:\n            plan_sheet = sheets[0]\n        df = pd.read_excel(path, sheet_name=plan_sheet)\n        lcols = [str(c).lower() for c in df.columns]\n        tax_cols = [c for c in lcols if 'tax' in c]\n        if tax_cols:\n            found_tax_cols = True\n            base_cols = [c for c in tax_cols if any(k in c for k in ['base','without','no conv','rmd only','baseline'])]\n            conv_cols = [c for c in tax_cols if any(k in c for k in ['with conv','conversion','roth'])]\n            found_dual_scenarios = bool(base_cols) and bool(conv_cols)\n        # Scoring: 0.2 for any tax columns; +0.2 for dual scenarios; +0.1 for explicit savings metric\n        score = 0.0\n        if found_tax_cols:\n            score += 0.2\n        if found_dual_scenarios:\n            score += 0.2\n        if found_savings_metric:\n            score += 0.1\n        return float(min(weight, score))\n    except Exception:\n        return 0.0"}, {"type": "llm_judge", "name": "Cross-Reference: Deck aligns with Model Mechanics", "description": "LLM checks consistency between the deck and the Excel model: 8-year window shown as 2026\u20132033, RMD at age 72, and heirs/tax-free benefits tied to model\u2019s tax savings.", "weight": 0.5, "judge_prompt": "Cross-reference the presentation and the Excel workbook for consistency. You may rely on visible text, headers, and obvious labels; do not recompute math.\n\nCheck for ALL of the following:\n1) The deck states or clearly shows an 8-year Roth conversion window and it matches the model\u2019s conversion years (preferably 2026\u20132033 with 2025 as period 0).\n2) The deck references RMD timing at age 72 (per the brief) and the model shows RMDs commencing at that age/year.\n3) The deck emphasizes estate/heirs tax-free benefits AND the model\u2019s summary references \u201cTax Savings\u201d or comparable delta supporting that narrative.\n\nScoring:\n- 0.5: All three consistency points are satisfied\n- 0.3: Any two are satisfied\n- 0.2: Any one is satisfied\n- 0.0: None are satisfied", "expectation": "The deck mirrors the model\u2019s 8-year schedule (2026\u20132033), acknowledges RMDs at age 72, and ties heirs\u2019 benefits to a visible tax-savings comparison in the model."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality & Client Suitability", "description": "Holistic evaluation of clarity, professionalism, and suitability for a client meeting. Not about correctness\u2014about communication impact and usability.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Presentation Quality & Client-Facing Readability", "description": "Assesses the 8-slide deck\u2019s clarity, visuals, and meeting readiness.", "weight": 1.5, "judge_prompt": "Assess the 8-slide presentation for professional client use. Consider:\n- Visual quality: consistent, professional template; appropriate charts/graphics/icons; clean layout; legible fonts and contrast\n- Clarity: concise headers, plain-language explanations; logical flow from purpose \u2192 process \u2192 impacts \u2192 estate benefits \u2192 next steps\n- Client readiness: includes agenda/flow cues; avoids dense jargon; includes a general educational disclaimer (not tax/legal advice)\n- Actionability: clear next steps (e.g., tax pro coordination, timing mechanics)\n\nScoring:\n- 1.5: Highly professional, clear, polished, and ready for client use\n- 1.0: Generally good; minor issues but acceptable for client meeting\n- 0.5: Understandable but weak visuals/structure; needs revision\n- 0.0: Unprofessional or confusing", "expectation": "A polished, client-ready deck with clear flow, visuals, and an appropriate disclaimer."}, {"type": "llm_judge", "name": "Model Explainability & Documentation Quality", "description": "Assesses how well the Excel file documents assumptions, sources, and methodology for client and advisor understanding.", "weight": 1.5, "judge_prompt": "Evaluate the Excel workbook\u2019s explainability and documentation (not math correctness):\n- Inputs & Assumptions clarity: parameters labeled, units (%, $) clear; key values from brief present; edit-friendly\n- Source transparency: IRS 2025 Uniform Lifetime Table cited; any other sources noted\n- Methodology notes: brief description of how RMDs and conversions are calculated; any simplifying assumptions; how marginal brackets are handled\n- Communication aids: color-coding, freeze panes, defined sections, charts; a summary page that a client can understand\n\nScoring:\n- 1.5: Excellent documentation and clarity; easy to follow for both advisor and client\n- 1.0: Adequate documentation; understandable with minor effort\n- 0.5: Sparse documentation; a practitioner could infer but a client could not\n- 0.0: Little to no documentation or labeling", "expectation": "A workbook with clear inputs, labeled methodology, cited RMD source, and a readable summary with at least one chart."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "854f3814-681c-4950-91ac-55b0db0e3781", "rubric": {"category_name": "OverpassQL I-40 Route Dataset + Usage Instructions", "rationale": "This rubric enforces a self-documenting, verifiable deliverable for a mixed task: a Markdown guide containing a runnable OverpassQL query and clear usage instructions. Stage 1 (LLM-only) strictly gates on document shape and presence of a properly structured OverpassQL code block. Stage 2 uses code rules to verify key technical elements of the query and instructions without judging prose quality. Stage 3 assesses overall professional quality and fitness for use in speed and lane availability analysis for autonomous freight routing.", "max_total_score": 20.0, "stages": [{"name": "Stage 1: Markdown Structure and OverpassQL Presence Gate", "description": "Gate that enforces the exact structural shape of the deliverable. Must be a Markdown document with required sections and a fenced OverpassQL code block.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Structured Markdown + OverpassQL Block Requirement", "description": "Verify the output is a Markdown (.md) document with required sections and a fenced OverpassQL query block suitable for generating an I-40 ABQ\u2192OKC dataset.", "weight": 8.0, "judge_prompt": "You are evaluating whether the candidate output file is a valid Markdown document with the exact structural elements required for verification. Do NOT judge technical correctness of the query or the writing quality\u2014only the presence and structure.\n\nFormat requirements:\n- Must be a Markdown file (.md) with clear section headers.\n- Must contain at least one fenced code block (``` ... ```) containing an OverpassQL query (label may be ```overpassql```, ```overpass```, ```ql```, or unlabeled but clearly Overpass syntax).\n\nRequired sections (flexible naming but meaning must match):\n1) Title indicating scope (should mention I-40 and Albuquerque to Oklahoma City or ABQ\u2192OKC).\n2) Overview/Purpose of the dataset and what it supports (speed and lane availability analysis).\n3) Prerequisites/Requirements (e.g., Overpass Turbo or Overpass API endpoint, tooling).\n4) OverpassQL Query section with a single, copyable fenced code block.\n5) How to Run / Step-by-step instructions (e.g., using Overpass Turbo or API) to generate the dataset.\n6) Exporting/Downloading Data instructions (e.g., export to GeoJSON/JSON/CSV and note ways/nodes/relations).\n7) Data Tags/Schema focus for analysis (explicitly list relevant tags like maxspeed, lanes, access/hgv/weight/height/width, oneway; exact list flexible).\n8) Geographic Scope/I-40 Segment definition between ABQ and OKC (mention states NM, TX panhandle, and OK and/or coordinates/bounds).\n\nOptional but helpful (do not penalize if missing):\n- Validation/QA checks for the dataset.\n- Performance considerations (timeout, bounding, relation vs bbox approach).\n- Alternate query variants (relation-based and bbox/area-based).\n\nCode block shape (structure only, not correctness):\n- Overpass header like [out:json] (or xml) and optional timeout.\n- A relation/route or motorway/way filter targeting I-40.\n- Recursion to include member ways and nodes (e.g., (r;>;); or way(r); node(w)).\n- out statements that return tags/geometry/meta.\n\nScoring (structure only):\n- 1.0: Markdown format + all 8 required sections present + at least one OverpassQL fenced code block with the structural elements above.\n- 0.8: Markdown + 7/8 required sections present + valid OverpassQL code block present.\n- 0.6: Markdown + 6/8 required sections present + valid OverpassQL code block present.\n- 0.3: Markdown present but only 3\u20135 required sections and/or code block missing structural elements.\n- 0.0: Not Markdown, or missing OverpassQL code block, or fewer than 3 required sections.\n\nOutput a score according to this scale. Do not evaluate technical correctness\u2014only presence and structure.", "expectation": "A single Markdown file with clear sections and one fenced OverpassQL block that structurally targets I-40 ABQ\u2192OKC, plus run/export instructions and tags focus for speed/lanes."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Technical Verification (Query + Instructions)", "description": "Code-based checks on the Markdown content to verify key technical elements of the OverpassQL query and the usage instructions.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Overpass Code Block + Output Directive", "description": "Detect an OverpassQL fenced code block and the presence of an output directive ([out:json] or [out:xml]).", "weight": 2.0, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output:\\n        return 0.0, 'No primary output.'\\n    if not output.is_text_format:\\n        return 0.0, 'Primary output is not a text/Markdown file.'\\n    try:\\n        text = context.files.read_text(output.id) or ''\\n    except Exception as e:\\n        return 0.0, f'Failed to read text: {e}'\\n\\n    # Find fenced code blocks\\n    code_blocks = re.findall(r\"```[a-zA-Z0-9_-]*\\n([\\s\\S]*?)```\", text)\\n    # Heuristic: Pick the first block that looks like Overpass (contains [out: or rel/way)\\n    overpass_query = ''\\n    for blk in code_blocks:\\n        if '[out:' in blk or 'rel' in blk or 'way' in blk:\\n            overpass_query = blk\\n            break\\n\\n    has_block = bool(overpass_query)\\n    has_out = bool(re.search(r\"\\\\[out\\\\s*:\\\\s*(json|xml)\\\\s*\\\\]\", overpass_query, re.I))\\n\\n    score = 0.0\\n    if has_block:\\n        score += 1.0\\n    if has_out:\\n        score += 1.0\\n\\n    feedback = []\\n    feedback.append('Overpass code block detected' if has_block else 'No Overpass-like code block found')\\n    feedback.append('Has output directive' if has_out else 'Missing [out:json] or [out:xml]')\\n\\n    # Scale to weight 2.0 (each subcheck worth 1.0)\\n    return score, '; '.join(feedback)\\n"}, {"type": "code", "name": "I-40 Targeting (Relation or Motorway Ref)", "description": "Verify the query targets Interstate 40 either via route relation (network=US:I, ref=40) or via motorway ways with matching ref (e.g., I-40/I 40).", "weight": 2.0, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_text_format:\\n        return 0.0\\n    text = context.files.read_text(output.id) or ''\\n\\n    # Extract likely Overpass query\\n    blocks = re.findall(r\"```[a-zA-Z0-9_-]*\\n([\\s\\S]*?)```\", text)\\n    q = ''\\n    for b in blocks:\\n        if '[out:' in b or 'rel' in b or 'way' in b:\\n            q = b.lower()\\n            break\\n\\n    if not q:\\n        return 0.0, 'No query found.'\\n\\n    # Relation-based targeting patterns\\n    rel_pattern = r\"rel[^;]*\\\\[[^]]*route\\\\s*=\\\\s*('|\")?road('|\")?[^]]*ref\\\\s*=\\\\s*('|\")?40('|\")?[^]]*(network\\\\s*=\\\\s*('|\")?us:i('|\")?)?[^]]*\\\\]\"\\n    rel_ok = re.search(rel_pattern, q, re.I) is not None\\n\\n    # Motorway + ref targeting patterns (accept I-40 or I 40 variants)\\n    mw_pattern = r\"way[^;]*\\\\[[^]]*highway\\\\s*=\\\\s*('|\")?motorway('|\")?[^]]*\\\\]\"\\n    ref40_pattern = r\"\\\\[[^]]*ref\\\\s*=\\\\s*('|\")?(i[- ]?40|40)('|\")?[^]]*\\\\]\"\\n    mw_ok = re.search(mw_pattern, q, re.I) is not None and re.search(ref40_pattern, q, re.I) is not None\\n\\n    if rel_ok and mw_ok:\\n        score = 2.0\\n        fb = 'Targets I-40 via relation and motorway ref.'\\n    elif rel_ok:\\n        score = 2.0\\n        fb = 'Targets I-40 via relation filter.'\\n    elif mw_ok:\\n        score = 1.2\\n        fb = 'Targets I-40 via motorway ref only (accepted, but relation preferred).'\n    else:\\n        score = 0.0\\n        fb = 'No clear I-40 targeting via relation or motorway ref.'\\n    return score, fb\\n"}, {"type": "code", "name": "Recursion to Collect Ways and Nodes", "description": "Check that the query includes recursion to fetch member ways and their nodes (e.g., (r;>;), way(r), node(w)).", "weight": 1.5, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_text_format:\\n        return 0.0\\n    text = context.files.read_text(output.id) or ''\\n    blocks = re.findall(r\"```[a-zA-Z0-9_-]*\\n([\\s\\S]*?)```\", text)\\n    q = ''\\n    for b in blocks:\\n        if '[out:' in b or 'rel' in b or 'way' in b:\\n            q = b.lower()\\n            break\\n    if not q:\\n        return 0.0, 'No query found.'\\n\\n    has_way_from_rel = ('way(r)' in q) or re.search(r\"\\(r;\\s*>;\\s*\\)\", q) is not None or ('>>' in q)\\n    has_node_from_way = ('node(w)' in q) or re.search(r\"node\\s*\\(w\\)\", q) is not None or re.search(r\"\\(._;\\s*>;\\s*\\)\", q) is not None\\n\\n    score = 0.0\\n    if has_way_from_rel:\\n        score += 0.9\\n    if has_node_from_way:\\n        score += 0.6\\n    fb = []\\n    fb.append('Has way-from-relation recursion' if has_way_from_rel else 'Missing way-from-relation recursion')\\n    fb.append('Has node-from-way recursion' if has_node_from_way else 'Missing node-from-way recursion')\\n    return min(score, 1.5), '; '.join(fb)\\n"}, {"type": "code", "name": "Geographic Scope Mention (ABQ \u2194 OKC / States)", "description": "Ensure the document references the ABQ to OKC segment and/or the states (NM, TX, OK) or bounding concepts to locate the corridor.", "weight": 1.5, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_text_format:\\n        return 0.0\\n    text = (context.files.read_text(output.id) or '').lower()\\n\\n    has_abq = 'albuquerque' in text or 'abq' in text\\n    has_okc = 'oklahoma city' in text or 'okc' in text\\n    has_states = ('new mexico' in text) and ('oklahoma' in text)\\n    has_tx = 'texas' in text or 'tx' in text\\n    has_bbox_terms = any(k in text for k in ['bbox', 'bounding box', 'south', 'west', 'north', 'east'])\\n\\n    # Scoring: endpoints strongest, then states, then generic bbox terms\\n    if has_abq and has_okc:\\n        base = 1.2\\n    elif has_states and (has_tx or True):\\n        base = 1.0\\n    elif has_bbox_terms:\\n        base = 0.6\\n    else:\\n        base = 0.0\\n\\n    # Cap at weight 1.5 by scaling\\n    score = min(1.5, base * 1.25)\\n    fb = []\\n    fb.append('Mentions ABQ' if has_abq else 'No ABQ mention')\\n    fb.append('Mentions OKC' if has_okc else 'No OKC mention')\\n    fb.append('Mentions NM/OK states' if has_states else 'No NM/OK states mention')\\n    fb.append('Mentions TX' if has_tx else 'No TX mention')\\n    fb.append('Mentions bbox concepts' if has_bbox_terms else 'No bbox concepts')\\n    return score, '; '.join(fb)\\n"}, {"type": "code", "name": "Run + Export Instructions (Overpass Turbo, Formats)", "description": "Verify instructions reference Overpass Turbo or API usage and describe exporting to common formats (GeoJSON/JSON/CSV).", "weight": 1.0, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_text_format:\\n        return 0.0\\n    text = (context.files.read_text(output.id) or '').lower()\\n\\n    mentions_turbo = ('overpass turbo' in text) or ('overpass-turbo.eu' in text)\\n    mentions_api = ('overpass api' in text) or ('overpass-api.de' in text)\\n    mentions_export = 'export' in text or 'download' in text or 'save' in text\\n    mentions_format = any(fmt in text for fmt in ['geojson', 'json', 'csv'])\\n\\n    score = 0.0\\n    if mentions_turbo or mentions_api:\\n        score += 0.5\\n    if mentions_export and mentions_format:\\n        score += 0.5\\n\\n    fb = []\\n    fb.append('Overpass Turbo/API mentioned' if (mentions_turbo or mentions_api) else 'No Overpass Turbo/API mention')\\n    fb.append('Export with format mentioned' if (mentions_export and mentions_format) else 'Export/format details missing')\\n    return score, '; '.join(fb)\\n"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3: Professional Quality and Fitness for Purpose", "description": "LLM assessment of clarity, professionalism, and practical utility for speed and lane availability analysis for autonomous freight routing.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity, Practicality, and Analytical Readiness", "description": "Assess whether the document is professional, clear, and directly useful for generating and using the filtered dataset for speed and lane availability analysis.", "weight": 4.0, "judge_prompt": "Evaluate the document holistically for professional quality and usefulness. Consider:\n- Clarity and completeness of step-by-step instructions to run the OverpassQL query (Overpass Turbo/API) and export data.\n- Practical readiness for speed and lane availability analysis: explicitly mentions relevant tags (e.g., maxspeed, lanes, oneway, access/hgv/maxweight/height/width, change:lanes/turn:lanes) and notes how to use them.\n- Geographic specificity: clearly constrains to I-40 between Albuquerque and Oklahoma City and acknowledges the segment crosses NM, TX panhandle, and OK or provides equivalent bounding/area guidance.\n- Reproducibility and performance: includes timeout/bounding/area hints or relation-vs-bbox tradeoffs; any caveats about OSM data quality.\n- Professional formatting and tone.\n\nScoring:\n- 1.0: Excellent\u2014clear, professional, actionable; covers tags, export, and geography with performance/QA notes.\n- 0.8: Good\u2014minor gaps but overall very usable.\n- 0.5: Adequate\u2014usable but missing multiple helpful details (e.g., tag guidance or performance notes).\n- 0.2: Weak\u2014hard to follow or missing key elements for actual use.\n- 0.0: Not useful for running or applying the query.\n\nReturn a score according to this scale.", "expectation": "A polished, step-by-step guide with a runnable OverpassQL query and explicit guidance on tags and exports, tailored to I-40 ABQ\u2192OKC for speed/lane analyses."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "02aa1805-c658-4069-8a6a-02dec146063a", "rubric": {"category_name": "Hydrogen Project: Source Water Wells Evaluation", "rationale": "Mixed-output task: an analytical spreadsheet plus a managerial email recommendation. The rubric enforces a verifiable Excel structure (Stage 1 LLM gate), then validates correctness with code (Stage 2), and finally scores presentation quality with LLM (Stage 3). The gate requires two sheets with specific columns and an email that references the attachment and selection criteria. Code rules verify criteria logic, sheet consistency, system coverage, and email cross-references.", "max_total_score": 30.0, "stages": [{"name": "Stage 1 Gate: Structured Deliverables Present", "description": "LLM-only gate to ensure the candidate produced the exact verifiable shapes: a two-sheet Excel workbook with defined columns and a separate email document referencing the attachment and highlighting top options.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Structured Workbook + Email Deliverable Requirement", "description": "Check the presence and structure of the Excel workbook and the accompanying email document per the task requirements.", "weight": 8.0, "judge_prompt": "You are the Stage 1 shape-enforcement judge. Review ALL outputs. This gate ONLY checks presence and structure, not correctness of data.\n\nPASS CRITERIA (Excel + Email):\n\nA) Excel Workbook (must be .xlsx or .csv opened as a workbook; Excel preferred):\nRequired overall:\n- Two sheets/tabs minimum, with the following intent:\n  1) Sheet containing ALL extracted wells from the specified systems with filters and a boolean column to flag wells meeting ALL criteria.\n  2) Sheet containing ONLY the potential wells that meet the criteria.\n- Professionally readable table(s) with a clear header row on both sheets.\n\nRequired Sheet 1 (name can be flexible, e.g., \"All Wells\", \"All_Wells\", \"Full Extract\", \"Raw Wells\"): must include columns (flexible naming but clearly the same meaning):\n- Water system\n- Well ID\n- Well Description\n- Status\n- Depth\n- Minimum Setback\n- Pumpage\n- Aquifer Code\n- Aquifer Description\n- Max Zone\n- A boolean filter/flag column that clearly indicates wells that meet ALL required criteria (e.g., \"Meets All\", \"Eligible\", \"Meets_All_Criteria\").\n- Sheet should have visible filters/auto-filters enabled across the header (if visible in render).\n\nRequired Sheet 2 (name flexible, e.g., \"Potential Wells\", \"Shortlist\", \"Candidates\"): \n- Contains ONLY those wells that meet ALL criteria.\n- Includes at least the same core columns as Sheet 1 (the ten required above). The explicit boolean flag column may be included or not, but content must align with the criteria.\n\nOptional but recommended (do not penalize if missing):\n- A separate sheet or section listing source URLs (e.g., Illinois EPA factsheet links) used.\n\nB) Email Deliverable (must be a separate DOCX, PDF, or Markdown .md):\n- Structured like a professional email (To/Subject or greeting + purpose).\n- Explicitly references the attached Excel/workbook.\n- Summarizes the selection criteria (depth 160\u2013200; aquifer sand & gravel; active wells not marked abandoned/inactive/disconnected/emergency/sealed).\n- Highlights top options (at least 2 specific wells by Water system and Well ID) from the potential set.\n- Mentions Illinois EPA or links to the Illinois EPA Source Water Assessment factsheets.\n\nSCORING (0\u20138):\n- 8: Excel present with two properly structured sheets as described, including all required columns and a boolean flag column on Sheet 1; Sheet 2 contains only potential wells; Email present with all listed elements (attachment reference, criteria summary, top options with IDs, and IL EPA mention/link).\n- 6\u20137: Minor omissions only (e.g., boolean flag column present but filters not obviously visible; or email missing 1 minor element like the exact filename but still references attachment and criteria).\n- 3\u20135: Partially structured workbook (e.g., missing 1\u20132 required columns or second sheet lacks proper restriction) and/or email missing multiple required elements.\n- 1\u20132: Wrong format or severely incomplete workbook (not two sheets or missing many columns) and/or no meaningful email.\n- 0: No Excel workbook or no email document; or workbook not in a readable spreadsheet format.\n\nOnly judge the presence and structure. Do not verify data correctness.", "expectation": "A clean two-sheet Excel with the specified columns and a boolean Meets-All flag, plus a professional email that references the workbook, summarizes criteria, and highlights top options."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Verification of Data and Logic", "description": "Code checks leveraging the enforced shape to verify criteria logic, sheet consistency, system coverage, and cross-references between the email and the spreadsheet.", "is_required": true, "max_points": 14.0, "min_score_to_pass": 7.0, "rules": [{"type": "code", "name": "Workbook Structure and Columns (Flexible Match)", "description": "Verify that both sheets exist and contain the required core columns (flexible synonyms), and that Sheet 1 has a boolean flag column indicating wells meeting all criteria.", "weight": 4.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    # Helper: find the first spreadsheet output\n    def find_excel_and_doc():\n        excel_res = None\n        doc_res = None\n        for r in context.get_all_outputs():\n            if (excel_res is None) and getattr(r, 'is_spreadsheet', False):\n                excel_res = r\n            if (doc_res is None) and (getattr(r, 'is_document', False) or getattr(r, 'is_text_format', False)):\n                doc_res = r\n        return excel_res, doc_res\n\n    excel_res, _ = find_excel_and_doc()\n    if not excel_res:\n        return 0.0, \"No spreadsheet found\"\n\n    # Load sheet names\n    try:\n        x_path = context.files.get_path(excel_res.id)\n        xfile = pd.ExcelFile(x_path)\n        sheet_names = [s.strip() for s in xfile.sheet_names]\n    except Exception as e:\n        return 0.0, f\"Failed to open spreadsheet: {e}\"\n\n    # Heuristic to locate sheets\n    def pick_sheet(names, keywords_any):\n        lname = [n.lower() for n in names]\n        for i, n in enumerate(lname):\n            if any(k in n for k in keywords_any):\n                return names[i]\n        return names[0] if names else None\n\n    sheet_all = pick_sheet(sheet_names, [\"all\", \"full\", \"raw\", \"master\", \"extract\"])  # Sheet 1\n    sheet_pot = pick_sheet(sheet_names, [\"potential\", \"short\", \"candidate\", \"viable\", \"screen\"])  # Sheet 2\n\n    # Required columns (canonical -> alias tokens)\n    req = {\n        'water_system': [\"water system\", \"system\", \"water_system\", \"utility\", \"community\"],\n        'well_id': [\"well id\", \"well_id\", \"well number\", \"well no\", \"well#\", \"well\", \"well code\"],\n        'well_description': [\"well description\", \"description\", \"well desc\"],\n        'status': [\"status\", \"operational status\", \"current status\"],\n        'depth': [\"depth\", \"total depth\", \"well depth\"],\n        'minimum_setback': [\"minimum setback\", \"min setback\", \"setback\", \"min. setback\"],\n        'pumpage': [\"pumpage\", \"capacity\", \"pumping rate\", \"avg pump\", \"gpm\"],\n        'aquifer_code': [\"aquifer code\", \"aq code\", \"aquifer\"],\n        'aquifer_description': [\"aquifer description\", \"aquifer desc\"],\n        'max_zone': [\"max zone\", \"maxzone\", \"zone\"]\n    }\n\n    def norm(s):\n        return re.sub(r\"[^a-z0-9]+\", \" \", str(s).lower()).strip()\n\n    def map_cols(df):\n        cols = list(df.columns)\n        nmap = {c: norm(c) for c in cols}\n        out = {}\n        for key, aliases in req.items():\n            found = None\n            for c in cols:\n                nc = nmap[c]\n                for a in aliases:\n                    if a in nc:\n                        found = c\n                        break\n                if found:\n                    break\n            out[key] = found\n        # find meets_all-like column\n        meets_col = None\n        for c in cols:\n            nc = nmap[c]\n            if (\"meet\" in nc or \"eligible\" in nc or \"select\" in nc) and (\"all\" in nc or \"criteria\" in nc or \"flag\" in nc):\n                meets_col = c\n                break\n        out['meets_all'] = meets_col\n        return out\n\n    score = 0.0\n    feedback = []\n\n    try:\n        df_all = pd.read_excel(x_path, sheet_name=sheet_all)\n        colmap_all = map_cols(df_all)\n        have_all = sum(1 for k in req.keys() if colmap_all.get(k))\n        ratio_all = have_all / len(req)\n        # Require at least 8/10 on Sheet 1 to get most credit\n        score += min(2.0, 2.0 * ratio_all)\n        feedback.append(f\"Sheet '{sheet_all}': {have_all}/10 required columns matched.\")\n        # meets flag present boosts\n        if colmap_all.get('meets_all'):\n            score += 0.5\n            feedback.append(\"Sheet 1 has a Meets-All flag column.\")\n        else:\n            feedback.append(\"Sheet 1 missing an explicit Meets-All flag column.\")\n    except Exception as e:\n        feedback.append(f\"Failed to read/all map Sheet 1: {e}\")\n\n    try:\n        df_pot = pd.read_excel(x_path, sheet_name=sheet_pot)\n        colmap_pot = map_cols(df_pot)\n        have_pot = sum(1 for k in req.keys() if colmap_pot.get(k))\n        ratio_pot = have_pot / len(req)\n        score += min(1.5, 1.5 * ratio_pot)\n        feedback.append(f\"Sheet '{sheet_pot}': {have_pot}/10 required columns matched.\")\n    except Exception as e:\n        feedback.append(f\"Failed to read/map Sheet 2: {e}\")\n\n    # Presence of two distinct sheets\n    if sheet_all and sheet_pot and sheet_all != sheet_pot:\n        score += 0.5\n    else:\n        feedback.append(\"Did not confidently identify two distinct sheets for All vs Potential.\")\n\n    # Cap at weight\n    score = min(score, 4.0)\n    return score, \"; \".join(feedback)"}, {"type": "code", "name": "Criteria Logic and Potential Sheet Consistency", "description": "Check that the 160\u2013200 depth range, sand & gravel aquifer, and active-well logic are applied; verify that the Potential sheet equals the set of wells meeting all criteria and that any Meets-All flag in Sheet 1 matches computed logic.", "weight": 6.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    def find_excel_and_doc():\n        excel_res = None\n        for r in context.get_all_outputs():\n            if getattr(r, 'is_spreadsheet', False):\n                excel_res = r\n                break\n        return excel_res\n\n    excel_res = find_excel_and_doc()\n    if not excel_res:\n        return 0.0, \"No spreadsheet found\"\n\n    # Load\n    try:\n        x_path = context.files.get_path(excel_res.id)\n        xfile = pd.ExcelFile(x_path)\n        sheet_names = [s.strip() for s in xfile.sheet_names]\n    except Exception as e:\n        return 0.0, f\"Failed to open spreadsheet: {e}\"\n\n    def pick_sheet(names, keywords_any):\n        lname = [n.lower() for n in names]\n        for i, n in enumerate(lname):\n            if any(k in n for k in keywords_any):\n                return names[i]\n        return names[0] if names else None\n\n    sheet_all = pick_sheet(sheet_names, [\"all\", \"full\", \"raw\", \"master\", \"extract\"])  # Sheet 1\n    sheet_pot = pick_sheet(sheet_names, [\"potential\", \"short\", \"candidate\", \"viable\", \"screen\"])  # Sheet 2\n\n    # Column mapping\n    req = {\n        'water_system': [\"water system\", \"system\", \"water_system\", \"utility\", \"community\"],\n        'well_id': [\"well id\", \"well_id\", \"well number\", \"well no\", \"well#\", \"well\", \"well code\"],\n        'well_description': [\"well description\", \"description\", \"well desc\"],\n        'status': [\"status\", \"operational status\", \"current status\"],\n        'depth': [\"depth\", \"total depth\", \"well depth\"],\n        'aquifer_description': [\"aquifer description\", \"aquifer desc\"]\n    }\n\n    def norm(s):\n        return re.sub(r\"[^a-z0-9]+\", \" \", str(s).lower()).strip()\n\n    def map_cols(df):\n        cols = list(df.columns)\n        nmap = {c: norm(c) for c in cols}\n        out = {}\n        for key, aliases in req.items():\n            found = None\n            for c in cols:\n                nc = nmap[c]\n                for a in aliases:\n                    if a in nc:\n                        found = c\n                        break\n                if found:\n                    break\n            out[key] = found\n        # detect meets_all\n        meets_col = None\n        for c in cols:\n            nc = nmap[c]\n            if (\"meet\" in nc or \"eligible\" in nc or \"select\" in nc) and (\"all\" in nc or \"criteria\" in nc or \"flag\" in nc):\n                meets_col = c\n                break\n        out['meets_all'] = meets_col\n        return out\n\n    def coerce_num(v):\n        if pd.isna(v):\n            return np.nan\n        s = str(v)\n        m = re.search(r\"-?\\d+(?:\\.\\d+)?\", s)\n        if not m:\n            return np.nan\n        try:\n            return float(m.group(0))\n        except:\n            return np.nan\n\n    try:\n        df_all = pd.read_excel(x_path, sheet_name=sheet_all)\n        df_pot = pd.read_excel(x_path, sheet_name=sheet_pot)\n    except Exception as e:\n        return 0.0, f\"Failed reading sheets: {e}\"\n\n    cm_all = map_cols(df_all)\n    cm_pot = map_cols(df_pot)\n\n    # If key columns missing, cannot meaningfully verify\n    needed = ['water_system','well_id','well_description','status','depth','aquifer_description']\n    if any(cm_all.get(k) is None for k in needed):\n        return 0.5, \"Missing key columns on Sheet 1; partial credit only\"\n\n    # Compute criteria on Sheet 1\n    desc_col = cm_all['well_description']\n    status_col = cm_all['status']\n    depth_col = cm_all['depth']\n    aqu_desc_col = cm_all['aquifer_description']\n\n    banned = [\"abandoned\",\"inactive\",\"disconnected\",\"emergency\",\"sealed\"]\n\n    depths = df_all[depth_col].apply(coerce_num)\n    depth_ok = (depths >= 160) & (depths <= 200)\n\n    aqu = df_all[aqu_desc_col].astype(str).str.lower()\n    aqu_ok = aqu.str.contains(\"sand\") & aqu.str.contains(\"gravel\")\n\n    wd = df_all[desc_col].astype(str).str.lower()\n    is_active = ~wd.apply(lambda s: any(b in s for b in banned))\n\n    meets_calc = depth_ok.fillna(False) & aqu_ok.fillna(False) & is_active.fillna(False)\n\n    # Compare meets_all column if present\n    score = 0.0\n    fb = []\n    if cm_all.get('meets_all') and cm_all['meets_all'] in df_all.columns:\n        mcol = cm_all['meets_all']\n        mvals = df_all[mcol].astype(str).str.strip().str.lower()\n        mbool = mvals.isin([\"true\",\"1\",\"yes\",\"y\",\"t\"]) | (df_all[mcol]==1) | (df_all[mcol]==True)\n        agree = (mbool.fillna(False) == meets_calc.fillna(False))\n        acc = agree.mean() if len(agree) else 0.0\n        score += 2.5 * acc\n        fb.append(f\"Meets-All flag agreement: {acc:.2f}\")\n    else:\n        fb.append(\"No explicit Meets-All flag; skipping agreement check.\")\n\n    # Compare Potential sheet content\n    # Map key columns for pot sheet\n    needed_p = ['water_system','well_id','well_description','depth','aquifer_description']\n    if any(cm_pot.get(k) is None for k in needed_p):\n        fb.append(\"Potential sheet missing key columns; limited consistency checks.\")\n    else:\n        ws_all = df_all[cm_all['water_system']].astype(str).str.strip().str.lower()\n        id_all = df_all[cm_all['well_id']].astype(str).str.strip().str.lower()\n        key_all = ws_all + \"||\" + id_all\n        key_meet = set(key_all[meets_calc.fillna(False)].tolist())\n\n        ws_p = df_pot[cm_pot['water_system']].astype(str).str.strip().str.lower()\n        id_p = df_pot[cm_pot['well_id']].astype(str).str.strip().str.lower()\n        key_pot = set((ws_p + \"||\" + id_p).tolist())\n\n        if len(key_meet) == 0 and len(key_pot) == 0:\n            jacc = 1.0\n        else:\n            inter = len(key_meet & key_pot)\n            union = len(key_meet | key_pot) if (key_meet | key_pot) else 1\n            jacc = inter / union\n        score += 2.5 * jacc\n        fb.append(f\"Potential sheet Jaccard match vs computed eligible: {jacc:.2f}\")\n\n        # Verify all rows in Potential actually meet criteria\n        # Recompute criteria for pot rows using thresholds\n        depths_p = df_pot[cm_pot['depth']].apply(coerce_num)\n        aqu_p = df_pot[cm_pot['aquifer_description']].astype(str).str.lower()\n        wd_p = df_pot[cm_pot['well_description']].astype(str).str.lower()\n        depth_ok_p = (depths_p >= 160) & (depths_p <= 200)\n        aqu_ok_p = aqu_p.str.contains(\"sand\") & aqu_p.str.contains(\"gravel\")\n        is_active_p = ~wd_p.apply(lambda s: any(b in s for b in banned))\n        all_ok = (depth_ok_p.fillna(False) & aqu_ok_p.fillna(False) & is_active_p.fillna(False))\n        acc_p = all_ok.mean() if len(all_ok) else 0.0\n        score += 1.0 * acc_p\n        fb.append(f\"Potential rows meeting criteria: {acc_p:.2f}\")\n\n    # Cap score\n    score = min(score, 6.0)\n    return score, \"; \".join(fb)"}, {"type": "code", "name": "Coverage of Required Water Systems", "description": "Check for presence of the specified Illinois systems and reward coverage; soft penalty for extraneous systems is not applied (only reward coverage).", "weight": 2.0, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    target_systems = [\n        'farmer city','springerton','bartlett','enfield','crossville','weldon','norris city','waynesville'\n    ]\n\n    # Locate spreadsheet\n    excel_res = None\n    for r in context.get_all_outputs():\n        if getattr(r, 'is_spreadsheet', False):\n            excel_res = r\n            break\n    if not excel_res:\n        return 0.0, \"No spreadsheet found\"\n\n    try:\n        x_path = context.files.get_path(excel_res.id)\n        xfile = pd.ExcelFile(x_path)\n        sheet = xfile.sheet_names[0]\n        df = pd.read_excel(x_path, sheet_name=sheet)\n    except Exception as e:\n        return 0.0, f\"Failed to open/read spreadsheet: {e}\"\n\n    # Find water system column flexibly\n    def norm(s):\n        import re\n        return re.sub(r\"[^a-z0-9]+\", \" \", str(s).lower()).strip()\n\n    ws_col = None\n    for c in df.columns:\n        nc = norm(c)\n        if (\"water\" in nc and \"system\" in nc) or nc == \"system\" or \"community\" in nc or \"utility\" in nc:\n            ws_col = c\n            break\n    if ws_col is None:\n        return 0.5, \"Water system column not found; partial credit\"\n\n    vals = df[ws_col].dropna().astype(str).str.lower().str.strip()\n    present = set()\n    for t in target_systems:\n        if any(t in v for v in vals):\n            present.add(t)\n    coverage = len(present) / len(target_systems)\n    score = 2.0 * coverage\n    fb = f\"Covered {len(present)}/{len(target_systems)} target systems.\"\n    return score, fb"}, {"type": "code", "name": "Email Cross-Reference to Attachment and Wells", "description": "Verify the email references the attachment and IL EPA, and mentions at least one Well ID from the Potential sheet.", "weight": 2.0, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    excel_res = None\n    email_res = None\n    for r in context.get_all_outputs():\n        if getattr(r, 'is_spreadsheet', False) and excel_res is None:\n            excel_res = r\n        if (getattr(r, 'is_document', False) or getattr(r, 'is_text_format', False)) and email_res is None:\n            email_res = r\n    if not excel_res or not email_res:\n        return 0.0, \"Missing spreadsheet or email\"\n\n    # Read email text\n    text = \"\"\n    try:\n        if getattr(email_res, 'is_document', False):\n            # Try PDF then DOCX\n            try:\n                text = context.files.read_pdf_text(email_res.id)\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(email_res.id)\n                except Exception:\n                    text = \"\"\n        if (not text) and getattr(email_res, 'is_text_format', False):\n            text = context.files.read_text(email_res.id)\n    except Exception:\n        text = \"\"\n    text_l = text.lower()\n\n    # Load potential sheet and extract some well IDs\n    try:\n        x_path = context.files.get_path(excel_res.id)\n        xfile = pd.ExcelFile(x_path)\n        # Pick potential-like sheet\n        sheet_pot = None\n        for s in xfile.sheet_names:\n            ls = s.lower()\n            if any(k in ls for k in [\"potential\",\"short\",\"candidate\",\"viable\",\"screen\"]):\n                sheet_pot = s\n                break\n        if sheet_pot is None:\n            sheet_pot = xfile.sheet_names[min(1, len(xfile.sheet_names)-1)]\n        dfp = pd.read_excel(x_path, sheet_name=sheet_pot)\n    except Exception:\n        dfp = pd.DataFrame()\n\n    # Find well id col\n    def norm(s):\n        return re.sub(r\"[^a-z0-9]+\", \" \", str(s).lower()).strip()\n    well_col = None\n    for c in dfp.columns if len(dfp.columns)>0 else []:\n        nc = norm(c)\n        if (\"well\" in nc and (\"id\" in nc or \"number\" in nc or \"no\" in nc)) or nc == \"well\":\n            well_col = c\n            break\n    ids = []\n    if well_col is not None and len(dfp) > 0:\n        ids = [str(v) for v in dfp[well_col].dropna().astype(str).head(10).tolist()]\n\n    score = 0.0\n    fb = []\n\n    # Attachment mention\n    if any(k in text_l for k in [\"attached\",\"attachment\",\"enclosed\"]) and any(k in text_l for k in [\"excel\",\"workbook\",\"spreadsheet\",\".xlsx\"]):\n        score += 0.7\n    else:\n        fb.append(\"Email does not clearly reference attached Excel/workbook.\")\n\n    # IL EPA mention/link\n    if (\"illinois epa\" in text_l) or (\"dataservices.epa.illinois.gov/swap\" in text_l) or (\"source water assessment\" in text_l):\n        score += 0.6\n    else:\n        fb.append(\"Email missing explicit Illinois EPA / SWAP reference.\")\n\n    # Mentions at least one potential well ID\n    if ids:\n        found = any(str(i).lower() in text_l for i in ids)\n        if found:\n            score += 0.7\n        else:\n            fb.append(\"Email does not mention any listed potential Well IDs.\")\n    else:\n        fb.append(\"Could not parse potential Well IDs from spreadsheet.\")\n\n    score = min(score, 2.0)\n    return score, \"; \".join(fb)"}], "on_failure_action": "skip_remaining", "on_failure_score": 0.0}, {"name": "Stage 3: Quality and Professionalism", "description": "Holistic assessment of presentation quality and managerial usefulness.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Spreadsheet Presentation and Usability", "description": "Evaluate professional formatting and usability of the workbook.", "weight": 4.0, "judge_prompt": "Evaluate the Excel workbook\u2019s presentation and usability (not data correctness):\n\nLook for:\n- Clear, readable header row; consistent formatting; no obvious typos in headers.\n- Filters/auto-filters visible and sensible. Freeze panes or otherwise easy navigation is a plus.\n- Units are clear (e.g., depth/pumpage) and columns are appropriately typed (numbers look like numbers when rendered).\n- Second sheet appears to only contain the filtered/eligible wells and mirrors the core columns from Sheet 1.\n- A helpful boolean \"Meets All\" (or similar) flag exists in Sheet 1 and can be used to filter quickly.\n\nScoring (0\u20134):\n- 4: Highly professional, clean formatting, filters apparent, very easy to use.\n- 3: Generally clean with minor issues (some formatting inconsistencies but clearly usable).\n- 2: Usable but somewhat messy or confusing.\n- 1: Barely usable; poor formatting.\n- 0: Not an Excel workbook or presentation is unusable.", "expectation": "A clean, filterable workbook with consistent headers and an obvious flag to filter eligible wells."}, {"type": "llm_judge", "name": "Email Quality and Manager Readiness", "description": "Assess if the email is concise, professional, and provides an actionable recommendation.", "weight": 4.0, "judge_prompt": "Evaluate the email document:\n- Professional tone and clarity with a proper greeting and purpose.\n- Summarizes the three screening criteria accurately (depth 160\u2013200 ft; sand & gravel aquifer; active wells).\n- Highlights 2\u20135 top options by Water system and Well ID, stating why they\u2019re strong candidates.\n- References the attached Excel and guides the manager on next steps (e.g., permitting checks, site proximity, capacity validation).\n- Writing quality: concise, free of major grammar/spelling errors.\n\nScoring (0\u20134):\n- 4: Clear, concise, professional; strong prioritization and next steps.\n- 3: Professional and useful; minor omissions.\n- 2: Some usefulness but lacks clarity or specifics.\n- 1: Minimal value; vague recommendations.\n- 0: Not an email or not present.", "expectation": "A manager-ready email that succinctly explains criteria, names specific top options, references the workbook, and suggests next steps."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "1a78e076-445e-4c5d-b8ce-387d2fe5e715", "rubric": {"category_name": "Evidence-Based Literature Review: Hypertension Treatment Adherence in Older Adults", "rationale": "This rubric enforces a self-documenting workflow: Stage 1 (LLM-only) mandates a strict, verifiable document structure in PDF/DOCX with all required sections and subthemes. Stage 2 uses code rules to verify structural elements programmatically and check the presence of required topical coverage, search strategy details, and reference constraints. Stage 3 applies LLM judgment to evaluate synthesis quality, clinical utility, and professional writing. The shape-first gate ensures we can reliably verify correctness and quality thereafter.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Format and Structural Gate (LLM Only)", "description": "Gate that enforces exact document format and section structure to enable downstream verification. Only checks presence/shape, not correctness or quality.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.1, "rules": [{"type": "llm_judge", "name": "Document Format and Required Sections Present", "description": "Verify the candidate produced a properly structured literature review document matching the task\u2019s required format and sections.", "weight": 3.0, "judge_prompt": "You are the Stage 1 gate. Only evaluate STRUCTURE and FORMAT, not content quality or correctness.\n\nCheck the candidate output (you can see rendered PDFs/DOCX) against these requirements:\n\nFormat requirements:\n- File type: PDF or DOCX (not plain text, not spreadsheet).\n- Length: 10\u201315 pages (be flexible for minor variations due to formatting; typical essay layout with paragraph text; no requirement for tables).\n- Paragraph form (not bullet-only lists).\n\nSection and heading requirements (flexible with close synonyms, but each must be clearly present as a section):\n1) Factors Affecting Adherence in Hypertension Management (allow variations like \u201cFactors Affecting Adherence,\u201d must clearly reference hypertension/high blood pressure context).\n2) Search Strategy (synonyms like \u201cMethods \u2013 Literature Search\u201d acceptable if it clearly describes the database search approach).\n3) Results with the following clearly labeled subheadings inside Results:\n   - Multidisciplinary\n   - Psychosocial\n   - Patient-Centered (accept \u201cpatient centred\u201d, \u201cpatient-centered care\u201d)\n   - Technological Intervention (accept \u201ctechnology\u201d, \u201cdigital\u201d, \u201cmHealth/telehealth interventions\u201d as explicit subheading labels)\n4) Strengths and Limitations (accept as combined header or two separate sections).\n5) Conclusion (accept \u201cConclusions\u201d).\n6) Future Research (accept \u201cFuture Directions\u201d).\n7) References (bibliography/works cited). The list must not exceed 30 items.\n\nTopical presence requirements (just check that the document visibly addresses each topic somewhere; do not judge accuracy):\n- Prevalence data related to hypertension/adherence.\n- How adherence varies across older age groups.\n- Morbidity and mortality associated with poor adherence.\n- Financial impact of hypertension management.\n\nCitation presence:\n- In-text citations present (e.g., author-year or numbered) AND a references list at the end.\n\nScoring (return a score from 0 to 3.0):\n- 3.0: PDF/DOCX, 10\u201315 pages, all required sections and all four Results subheadings present, references section present with \u226430 entries, in-text citations present, all four topical presence items apparent.\n- 2.4: Valid format, minor deviations (e.g., 9 or 16 pages), or one missing/merged Results subheading, or references slightly unclear but still clearly < = 30, and topical presence items mostly present (3/4).\n- 1.5: Valid format but missing 1\u20132 required sections OR multiple Results subheadings missing; or length outside range and several structural issues; topical presence items only partially present (2/4).\n- 0.5: Valid file type but major structural noncompliance (e.g., only a couple sections, no Results subheadings, or no references) OR substantially too short/long.\n- 0.0: Not PDF/DOCX OR structure grossly noncompliant.\n\nImportant: Do not assess correctness of data or quality of analysis\u2014only whether the structure exists to enable verification later.", "expectation": "A professionally formatted PDF/DOCX, 10\u201315 pages, with all required sections and Results subheadings, in-text citations, and a references list with \u226430 items; addresses the four topical presence items somewhere in the document."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification: Structural and Content Presence (Code + LLM-Optional)", "description": "Programmatic checks leveraging the enforced shape to verify presence of required sections, subthemes, topical coverage, search strategy details, and reference count within bounds. Not assessing writing quality; focuses on verifiable elements.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Programmatic Section Headers Verification", "description": "Verify that key section headers appear in text: Factors Affecting Adherence..., Search Strategy, Results, Strengths and Limitations (or both terms), Conclusion(s), Future Research/Directions, and References.", "weight": 1.2, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document output\"\n        text = None\n        try:\n            if hasattr(context.files, 'read_pdf_text') and output.extension and output.extension.lower().endswith('pdf'):\n                text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = None\n        if not text:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                text = None\n        if not text:\n            return 0.0, \"Unable to extract text from document\"\n        t = re.sub(r\"\\s+\", \" \", text).lower()\n\n        checks = []\n        # Factors Affecting Adherence in Hypertension Management\n        factors = (\"factors\" in t and \"adherence\" in t and (\"hypertension\" in t or \"high blood pressure\" in t))\n        checks.append(factors)\n        # Search Strategy\n        search = (\"search strategy\" in t) or (\"literature search\" in t) or (\"methods\" in t and \"search\" in t)\n        checks.append(search)\n        # Results\n        checks.append(\"results\" in t)\n        # Strengths and Limitations (either combined or both words present)\n        strengths_limitations = (\"strengths and limitations\" in t) or (\"strengths\" in t and \"limitations\" in t)\n        checks.append(strengths_limitations)\n        # Conclusion\n        checks.append((\"conclusion\" in t) or (\"conclusions\" in t))\n        # Future Research/Directions\n        checks.append((\"future research\" in t) or (\"future directions\" in t))\n        # References/Bibliography\n        checks.append((\"references\" in t) or (\"bibliography\" in t) or (\"works cited\" in t))\n\n        score_ratio = sum(1 for c in checks if c) / len(checks)\n        score = 1.2 * score_ratio\n        missing = []\n        names = [\n            \"Factors Affecting Adherence (+hypertension)\",\n            \"Search Strategy\",\n            \"Results\",\n            \"Strengths and Limitations\",\n            \"Conclusion(s)\",\n            \"Future Research/Directions\",\n            \"References/Bibliography\"\n        ]\n        for ok, name in zip(checks, names):\n            if not ok:\n                missing.append(name)\n        fb = \"All headers present\" if not missing else (\"Missing/unclear: \" + \", \".join(missing))\n        return score, fb\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Results Subthemes Present", "description": "Verify presence of the four Results subthemes: Multidisciplinary, Psychosocial, Patient-Centered, Technological Intervention (flexible synonyms).", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document output\"\n        text = None\n        try:\n            if hasattr(context.files, 'read_pdf_text') and output.extension and output.extension.lower().endswith('pdf'):\n                text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = None\n        if not text:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                text = None\n        if not text:\n            return 0.0, \"Unable to extract text\"\n        t = text.lower()\n        flags = []\n        # Multidisciplinary / Interdisciplinary\n        flags.append((\"multidisciplinary\" in t) or (\"interdisciplinary\" in t))\n        # Psychosocial\n        flags.append(\"psychosocial\" in t)\n        # Patient-Centered (allow centred/centered care)\n        flags.append((\"patient-centered\" in t) or (\"patient centred\" in t) or (\"patient-centered care\" in t))\n        # Technological Intervention (allow technology/digital/mhealth/telehealth)\n        flags.append((\"technological intervention\" in t) or (\"technology\" in t) or (\"digital\" in t) or (\"mhealth\" in t) or (\"telehealth\" in t) or (\"ehealth\" in t))\n        ratio = sum(flags) / 4\n        score = 1.0 * ratio\n        missing = []\n        names = [\"Multidisciplinary/Interdisciplinary\",\"Psychosocial\",\"Patient-Centered\",\"Technological/Digital/Telehealth\"]\n        for ok, name in zip(flags, names):\n            if not ok:\n                missing.append(name)\n        fb = \"All subthemes present\" if not missing else (\"Missing/unclear subthemes: \" + \", \".join(missing))\n        return score, fb\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Key Topics Coverage (Presence)", "description": "Verify presence (not correctness) of the four topical elements: prevalence, adherence variation across older age groups, morbidity/mortality due to poor adherence, and financial impact.", "weight": 1.2, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document output\"\n        text = None\n        try:\n            if hasattr(context.files, 'read_pdf_text') and output.extension and output.extension.lower().endswith('pdf'):\n                text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = None\n        if not text:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                text = None\n        if not text:\n            return 0.0, \"Unable to extract text\"\n        t = text.lower()\n        checks = []\n        # Prevalence\n        checks.append((\"prevalence\" in t) or (\"epidemiolog\" in t))\n        # Adherence varies across older age groups\n        age_variation = (\"age\" in t and (\"older\" in t or \"elderly\" in t or \"65\" in t or \"75\" in t or \"80\" in t or \"85\" in t or \"age group\" in t))\n        checks.append(age_variation)\n        # Morbidity/Mortality associated with poor adherence\n        morbidity_mortality = (\"mortality\" in t) or (\"morbidity\" in t) or (\"stroke\" in t) or (\"cardiovascular\" in t) or (\"cvd\" in t) or (\"hospitalization\" in t)\n        checks.append(morbidity_mortality)\n        # Financial impact\n        financial = (\"cost\" in t) or (\"economic\" in t) or (\"expenditure\" in t) or (\"financial\" in t) or (\"cost-effect\" in t)\n        checks.append(financial)\n        ratio = sum(1 for c in checks if c) / 4\n        score = 1.2 * ratio\n        missing = []\n        names = [\"Prevalence\",\"Age-group variation in adherence\",\"Morbidity/Mortality\",\"Financial impact\"]\n        for ok, name in zip(checks, names):\n            if not ok:\n                missing.append(name)\n        fb = \"All topics present\" if not missing else (\"Missing/unclear topics: \" + \", \".join(missing))\n        return score, fb\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Search Strategy Specificity", "description": "Verify the Search Strategy describes methodological elements (databases/keywords/Boolean/inclusion-exclusion/date range).", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document output\"\n        text = None\n        try:\n            if hasattr(context.files, 'read_pdf_text') and output.extension and output.extension.lower().endswith('pdf'):\n                text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = None\n        if not text:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                text = None\n        if not text:\n            return 0.0, \"Unable to extract text\"\n        t = text.lower()\n        # Look for a search section and methodological details\n        has_search_section = (\"search strategy\" in t) or (\"literature search\" in t) or (\"search terms\" in t) or (\"methods\" in t and \"search\" in t)\n        features = 0\n        for kw in [\"database\", \"databases\", \"keywords\", \"boolean\", \"mesh\", \"inclusion\", \"exclusion\", \"date range\", \"years\", \"from 20\", \"screening\", \"eligib\", \"criteria\"]:\n            if kw in t:\n                features += 1\n        # Consider presence of at least two named databases as stronger evidence\n        named_sources = 0\n        for src in [\"pubmed\", \"cinahl\", \"google scholar\", \"american heart association\", \"aha\", \"cdc\", \"centers for disease control\"]:\n            if src in t:\n                named_sources += 1\n        # Score: require a search section and blend features + named sources\n        if not has_search_section:\n            return 0.0, \"No identifiable search strategy section\"\n        # Normalize features: cap at 6 for scaling\n        feat_score = min(features, 6) / 6\n        src_score = min(named_sources, 2) / 2\n        ratio = 0.7 * feat_score + 0.3 * src_score\n        score = 0.8 * ratio\n        fb = f\"Search features={features}, named sources={named_sources}\"\n        return score, fb\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "References Count Within Limit", "description": "Estimate reference count from References section; reward if 1\u201330, partial if 31\u201340, else low.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document output\"\n        text = None\n        try:\n            if hasattr(context.files, 'read_pdf_text') and output.extension and output.extension.lower().endswith('pdf'):\n                text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = None\n        if not text:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                text = None\n        if not text:\n            return 0.0, \"Unable to extract text\"\n        t = text\n        tl = t.lower()\n        # Locate references section\n        start_idx = -1\n        for hdr in [\"\\nreferences\", \"\\nbibliography\", \"\\nworks cited\"]:\n            idx = tl.find(hdr)\n            if idx != -1:\n                start_idx = idx\n                break\n        if start_idx == -1:\n            return 0.0, \"No References section detected\"\n        ref_text = t[start_idx:]\n        # Count likely entries: numbered lines, bracketed numbers, bullets, APA starts, DOI/URL occurrences\n        lines = ref_text.splitlines()\n        count = 0\n        apa_pat = re.compile(r\"^[A-Z][A-Za-z\\-']+,\\s*[A-Z](?:\\.|\\s|,)\")\n        num_pat = re.compile(r\"^\\s*(?:\\[\\d+\\]|\\d+[\\).])\\s+\")\n        bullet_pat = re.compile(r\"^\\s*[\\u2022\\-\\*]\\s+\")\n        year_pat = re.compile(r\"\\(20\\d{2}\\)|\\(19\\d{2}\\)\")\n        for line in lines:\n            if num_pat.search(line) or bullet_pat.search(line) or apa_pat.search(line) or year_pat.search(line):\n                count += 1\n        # Fallback: DOIs/URLs\n        count = max(count, len(re.findall(r\"doi:\\s*10\\.|https?://\", ref_text, flags=re.I)))\n        # Heuristic cleanup for very long sections\n        if count > 200:\n            count = 0\n        if count == 0:\n            score = 0.0\n            fb = \"Unable to estimate references count\"\n        elif 1 <= count <= 30:\n            score = 0.6\n            fb = f\"Estimated references: {count} (within limit)\"\n        elif 31 <= count <= 40:\n            score = 0.3\n            fb = f\"Estimated references: {count} (slightly above limit)\"\n        else:\n            score = 0.1\n            fb = f\"Estimated references: {count} (above limit)\"\n        return score, fb\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Authoritative Sources Mentioned", "description": "Check that at least some authoritative sources are mentioned (CDC/AHA/PubMed/CINAHL/Google Scholar).", "weight": 0.2, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document output\"\n        text = None\n        try:\n            if hasattr(context.files, 'read_pdf_text') and output.extension and output.extension.lower().endswith('pdf'):\n                text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = None\n        if not text:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                text = None\n        if not text:\n            return 0.0, \"Unable to extract text\"\n        t = text.lower()\n        sources = {\n            \"cdc\": (\"cdc\" in t) or (\"centers for disease control\" in t),\n            \"aha\": (\"american heart association\" in t) or (\"aha\" in t),\n            \"pubmed\": (\"pubmed\" in t),\n            \"cinahl\": (\"cinahl\" in t),\n            \"google scholar\": (\"google scholar\" in t)\n        }\n        hits = sum(1 for v in sources.values() if v)\n        ratio = min(hits / 3, 1.0)  # require at least 3 mentions for full credit\n        score = 0.2 * ratio\n        missing = [k for k, v in sources.items() if not v]\n        fb = f\"Sources mentioned: {hits}; missing candidates: {', '.join(missing)}\"\n        return score, fb\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Clinical Utility (LLM)", "description": "Holistic assessment of synthesis quality, clinical applicability for nurse practitioners, and professional writing and referencing.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Synthesis Depth and Clinical Applicability", "description": "Evaluate how well the review synthesizes the literature across subthemes and translates findings into clinically actionable insights for nurse practitioners managing older adults with hypertension.", "weight": 1.2, "judge_prompt": "Evaluate QUALITY (not shape) of the document. Consider:\n- Integration and synthesis: Does it weave findings across the Results subthemes (multidisciplinary, psychosocial, patient-centered, technological) into a coherent narrative rather than listing studies?\n- Evidence use: Are claims supported by appropriately cited peer-reviewed studies and reputable data sources (e.g., CDC, AHA)?\n- Clinical applicability: Are implications and strategies clearly articulated for nurse practitioners in real clinical settings serving older adults (e.g., workflow, patient education, adherence strategies)?\n- Alignment with scope: Focus on older adults and adherence; connects prevalence, age-group variation, morbidity/mortality, and financial impact to practical management.\n\nScoring (0 to 1.2):\n- 1.2: Strong synthesis across subthemes with clear, actionable clinical implications, well-supported by citations.\n- 0.8: Good synthesis with some actionable guidance; minor gaps or uneven coverage.\n- 0.4: Mostly descriptive or fragmented; limited clinical takeaways.\n- 0.0: Minimal synthesis; lacks clinical relevance or evidence support.", "expectation": "A cohesive, evidence-backed synthesis that translates findings into practical, well-justified strategies for nurse practitioners."}, {"type": "llm_judge", "name": "Writing Quality and Professionalism", "description": "Assess clarity, flow, academic tone, and reference integration/consistency.", "weight": 0.8, "judge_prompt": "Assess writing quality and professionalism (not structure):\n- Clarity and organization: Logical flow, coherent paragraphs, appropriate transitions.\n- Academic tone: Objective, precise, consistent terminology; avoids unsupported claims.\n- Referencing: Consistent in-text citations; references list matches citations and appears professionally formatted (style can vary).\n- Readability and formatting: Few errors; consistent headers; appropriate figure/table references if any (not required).\n\nScoring (0 to 0.8):\n- 0.8: Clear, polished academic writing; consistent and appropriate citations.\n- 0.5: Generally clear with minor issues; mostly consistent citations.\n- 0.2: Noticeable clarity/grammar issues or inconsistent citations.\n- 0.0: Poorly written or unprofessional; citations largely missing/inconsistent.", "expectation": "Professional, readable academic prose with consistent citation practice suitable for a clinical audience."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "a941b6d8-4289-4500-b45a-f8e4fc94a724", "rubric": {"category_name": "VFX Teleportation Shot - Film and Video Editor Compositing Task", "rationale": "This rubric enforces a self-documenting delivery package for a VFX teleportation shot. Stage 1 mandates an Excel \"VFX Verification Workbook\" with precisely structured sheets and tables so that automated checks in Stage 2 can reliably verify technical correctness (timings, tracking, masking, FX events, and deliverable presence). Stage 3 qualitatively assesses professional clarity and plausibility of the workflow documentation. The final video deliverable is verified for presence via the manifest within the workbook and the actual outputs list.", "max_total_score": 12.0, "stages": [{"name": "Stage 1 \u2014 Delivery Workbook Shape Gate", "description": "LLM gate verifies the primary output is an Excel VFX Verification Workbook with exact sheets and tables needed for verification. If this shape is missing, automated verification is impossible.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured VFX Verification Workbook Requirement", "description": "Check that the primary output is an Excel file with the exact sheets and tables below. Only verify presence/structure, not correctness of values or calculations.", "weight": 4.0, "judge_prompt": "You are verifying the SHAPE ONLY of the primary output. Do not judge correctness or quality. Confirm the primary output is an Excel spreadsheet (.xlsx) with ALL of the following sheets and tables, with headers as specified. Be flexible on capitalization and minor naming variations, but the structural intent must be clear. If multiple tables exist per sheet, they may be stacked with header rows clearly visible.\n\nRequired Sheets and Tables:\n\n1) Sheet: \"Delivery Manifest\"\n   - Table A: \"Files\" with columns: [Role | Filename]\n     Required Roles include at least: Base Clip, Overlay Clip, Output Composite, Smoke Asset(s) (one or more rows acceptable).\n   - Table B: \"Specs\" with columns: [Asset | Parameter | Value]\n     Required Assets: Base Clip, Overlay Clip, Output Composite\n     Required Parameters per asset: Resolution, Frame Rate, Codec (Duration optional)\n\n2) Sheet: \"Edit & Timing\"\n   - Single table \"Segments\" with columns: [Segment | Source Clip | Start Timecode | End Timecode | Duration (s) | Notes]\n     Required Segment values: \"Selected Performance\" and \"Post-Duck\"\n\n3) Sheet: \"Tracking & Matchmove\"\n   - Single table with columns including at least: [Frame | Timecode | tx | ty | Scale | Rotation] AND EITHER corner pin coordinates [x1,y1,x2,y2,x3,y3,x4,y4] OR a homography/warp representation (e.g., h11..h33 or similarly named). Residual error column (e.g., Residual_px) is encouraged but optional.\n   - A brief \"Stabilization Details\" note or subtable on the same sheet indicating method and reference frame (at least 1-2 cells of text).\n\n4) Sheet: \"Masking & Keying\"\n   - Summary table with columns: [Property | Value]\n     Required Properties include at least: Window Mask Type, Feather (px), Edge Blur (px), Keyframes (count), Alpha Channel (Yes/No)\n\n5) Sheet: \"Color Grade\"\n   - Table with columns: [Parameter | Value | Tool]\n     Must include at least three of: Lift, Gamma, Gain, Exposure, Temperature, Tint, LUT\n\n6) Sheet: \"FX Events\"\n   - Single table with columns: [Event | Timecode | Duration (frames) | Color | Blend Mode | Intensity | Asset Name | Source/URL | License | Start TC | End TC | Opacity (%)].\n     Required events include at least one row for \"Teleport Flash\" and at least one row for a \"Smoke\" element. For smoke rows, Source/URL and License should be populated.\n\nScoring:\n- 4.0: All six sheets present with the specified tables and required columns/rows as described.\n- 3.0: One minor structural deviation (e.g., one optional field missing, or small header naming differences) but tables and required rows are clearly present.\n- 1.5: Multiple deviations (e.g., missing one required sheet or a required table/columns), but still a recognizable attempt at the required structure.\n- 0.0: Not an Excel file OR missing multiple required sheets/tables such that verification would not be possible.\n\nDo not check values or technical accuracy. Only verify that the structure exists to enable verification.", "expectation": "A single Excel workbook with the exact sheets and tables specified so that automated checks can parse timings, tracking data, masking summary, color grade parameters, FX events, and a deliverables manifest."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Technical Correctness Verification", "description": "Automated checks validate plausibility and internal consistency using the structured workbook and presence of the final output video.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Deliverables Presence via Manifest", "description": "Verify an output composite video is listed in the manifest and actually exists among outputs (.mp4 or .mov).", "weight": 1.2, "code": "import re, pandas as pd, numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        primary = context.get_primary_output()\n        if not primary or not primary.is_spreadsheet:\n            return 0.0, \"Primary output missing or not a spreadsheet.\"\n        # Find the Delivery Manifest sheet (flexible matching)\n        file_path = context.files.get_path(primary.id)\n        xls = pd.ExcelFile(file_path)\n        sheet_name = None\n        for s in xls.sheet_names:\n            if 'manifest' in s.lower() or ('delivery' in s.lower() and 'manifest' in s.lower()):\n                sheet_name = s\n                break\n        if sheet_name is None:\n            # try exact\n            for s in xls.sheet_names:\n                if s.strip().lower() in ['delivery manifest','manifest']:\n                    sheet_name = s\n                    break\n        if sheet_name is None:\n            return 0.0, \"Delivery Manifest sheet not found.\"\n        df = context.files.read_excel(primary.id, sheet_name=sheet_name)\n        # Normalize columns\n        df_cols = [str(c).strip().lower() for c in df.columns]\n        df.columns = df_cols\n        score = 0.0\n        feedback = []\n        # Attempt to locate Files table by looking for columns Role | Filename\n        has_role = any('role' == c or 'file role' in c for c in df.columns)\n        has_filename = any('filename' == c or 'file name' in c or 'path' == c for c in df.columns)\n        output_name = None\n        if has_role and has_filename:\n            role_col = [c for c in df.columns if c == 'role' or 'file role' in c][0]\n            fn_col = [c for c in df.columns if c in ['filename','file name','path']][0]\n            # look for Output Composite row\n            candidates = df[df[role_col].astype(str).str.lower().str.contains('output')]\n            if not candidates.empty:\n                output_name = str(candidates.iloc[0][fn_col]).strip()\n        else:\n            # Fallback: search any cell for .mp4 or .mov\n            text = '\\n'.join(df.astype(str).fillna('')).lower()\n            m = re.search(r\"([\\w\\-\\/\\\\.]+\\.(mp4|mov))\", text)\n            output_name = m.group(1) if m else None\n        # Check if any output file exists that matches extension and optionally name\n        all_outputs = context.get_all_outputs() or []\n        video_found = False\n        matched_name = False\n        for r in all_outputs:\n            try:\n                p = context.files.get_path(r.id)\n                if p.suffix.lower() in ['.mp4','.mov']:\n                    video_found = True\n                    if output_name and p.name.lower() == output_name.lower():\n                        matched_name = True\n                        break\n            except Exception:\n                continue\n        if matched_name:\n            score = 1.2\n            feedback.append(\"Output composite found and matches manifest filename.\")\n        elif video_found:\n            score = 0.9\n            feedback.append(\"Video deliverable found, but filename does not match manifest exactly.\")\n        elif output_name:\n            score = 0.3\n            feedback.append(\"Manifest lists an output composite file, but no matching video deliverable found among outputs.\")\n        else:\n            score = 0.0\n            feedback.append(\"No video deliverable found and manifest lacks a clear output composite filename.\")\n        return score, ' '.join(feedback)\n    except Exception as e:\n        return 0.0, f\"Error in deliverables check: {e}\""}, {"type": "code", "name": "Segment Durations Plausibility (\u22486s and \u22483s)", "description": "Check that the Edit & Timing sheet lists two segments: Selected Performance (~6s) and Post-Duck (~3s). Uses Duration (s) column.", "weight": 1.2, "code": "import re, pandas as pd, numpy as np\n\ndef evaluate(workflow, context):\n    def find_sheet(xls_names, keys):\n        for s in xls_names:\n            low = s.lower()\n            if all(k in low for k in keys):\n                return s\n        # fallback exact\n        for s in xls_names:\n            if s.strip().lower() in ['edit & timing','edit and timing','timing']:\n                return s\n        return None\n    try:\n        primary = context.get_primary_output()\n        if not primary or not primary.is_spreadsheet:\n            return 0.0\n        xls = pd.ExcelFile(context.files.get_path(primary.id))\n        sname = find_sheet(xls.sheet_names, ['edit','tim'])\n        if not sname:\n            return 0.0\n        df = context.files.read_excel(primary.id, sheet_name=sname)\n        df.columns = [str(c).strip().lower() for c in df.columns]\n        required_cols = ['segment','duration (s)']\n        if not all(any(rc == c or rc in c for c in df.columns) for rc in required_cols):\n            return 0.0\n        seg_col = [c for c in df.columns if c == 'segment' or 'segment' in c][0]\n        dur_col = [c for c in df.columns if c.startswith('duration')][0]\n        # Normalize segments\n        df[seg_col] = df[seg_col].astype(str).str.lower()\n        want = {\n            'selected performance': (5.0, 7.0, 0.6),\n            'post-duck': (2.5, 3.5, 0.6)\n        }\n        score = 0.0\n        for seg, (lo, hi, w) in want.items():\n            rows = df[df[seg_col].str.contains(seg)]\n            if rows.empty:\n                continue\n            try:\n                durs = pd.to_numeric(rows[dur_col], errors='coerce').dropna()\n                if durs.empty:\n                    continue\n                dur = float(durs.iloc[0])\n                if lo <= dur <= hi:\n                    score += w\n                else:\n                    # partial credit if close within +/\u2212 1.5s\n                    if (dur >= lo - 1.5) and (dur <= hi + 1.5):\n                        score += w * 0.5\n            except Exception:\n                continue\n        return min(score, 1.2)\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Post-Duck Start Time \u2248 20s in Overlay", "description": "Verify that the Post-Duck segment starts around 20 seconds (\u00b15s tolerance, partial up to \u00b110s).", "weight": 0.6, "code": "import re, pandas as pd, numpy as np\n\ndef evaluate(workflow, context):\n    def parse_timecode(tc):\n        if tc is None:\n            return None\n        s = str(tc).strip()\n        # Try seconds float\n        try:\n            return float(s)\n        except:\n            pass\n        # HH:MM:SS:FF or HH:MM:SS.mmm or MM:SS\n        m = re.match(r'^(?:(\\d{1,2}):)?(\\d{1,2}):(\\d{2})(?::(\\d{2}))?(?:\\.(\\d{1,3}))?$', s)\n        if m:\n            h = m.group(1)\n            mnt = m.group(2)\n            sec = m.group(3)\n            ff = m.group(4)\n            ms = m.group(5)\n            hours = int(h) if h else 0\n            minutes = int(mnt)\n            seconds = int(sec)\n            total = hours*3600 + minutes*60 + seconds\n            if ff:\n                # assume 24 fps if frames given\n                total += int(ff)/24.0\n            if ms:\n                total += int(ms)/1000.0\n            return float(total)\n        return None\n    try:\n        primary = context.get_primary_output()\n        if not primary or not primary.is_spreadsheet:\n            return 0.0\n        xls = pd.ExcelFile(context.files.get_path(primary.id))\n        # find Edit & Timing\n        sname = None\n        for s in xls.sheet_names:\n            if 'edit' in s.lower() and 'tim' in s.lower():\n                sname = s\n                break\n        if not sname:\n            return 0.0\n        df = context.files.read_excel(primary.id, sheet_name=sname)\n        df.columns = [str(c).strip().lower() for c in df.columns]\n        if not any('segment' == c for c in df.columns):\n            return 0.0\n        seg_col = 'segment'\n        # find start timecode col\n        stc_col = None\n        for c in df.columns:\n            if 'start' in c and 'time' in c:\n                stc_col = c\n                break\n        if not stc_col:\n            return 0.0\n        row = df[df[seg_col].astype(str).str.lower().str.contains('post-duck')]\n        if row.empty:\n            return 0.0\n        tc = parse_timecode(row.iloc[0][stc_col])\n        if tc is None:\n            return 0.0\n        target = 20.0\n        diff = abs(tc - target)\n        if diff <= 5.0:\n            return 0.6\n        elif diff <= 10.0:\n            return 0.3\n        else:\n            return 0.0\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Tracking Data Adequacy", "description": "Ensure Tracking & Matchmove sheet has reasonable keyframe rows and transform fields (tx, ty, scale, rotation) plus corner pin or homography-like fields.", "weight": 1.0, "code": "import re, pandas as pd, numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        primary = context.get_primary_output()\n        if not primary or not primary.is_spreadsheet:\n            return 0.0\n        xls = pd.ExcelFile(context.files.get_path(primary.id))\n        # find tracking sheet\n        sname = None\n        for s in xls.sheet_names:\n            low = s.lower()\n            if 'tracking' in low or 'matchmove' in low:\n                sname = s\n                break\n        if not sname:\n            return 0.0\n        df = context.files.read_excel(primary.id, sheet_name=sname)\n        df.columns = [str(c).strip().lower() for c in df.columns]\n        required = ['tx','ty','scale','rotation']\n        has_core = sum(1 for r in required if any(r == c or r in c for c in df.columns))\n        # Check for corner pin or homography\n        corner_cols = ['x1','y1','x2','y2','x3','y3','x4','y4']\n        has_corners = sum(1 for cc in corner_cols if any(cc == c for c in df.columns))\n        homo_cols = [c for c in df.columns if re.match(r'h\\d{2}', c)]\n        rows = len(df.index)\n        score = 0.0\n        # core transform presence\n        score += 0.4 if has_core >= 3 else (0.2 if has_core >= 2 else 0.0)\n        # structure: corners or homography\n        if has_corners >= 6 or len(homo_cols) >= 4:\n            score += 0.3\n        elif has_corners >= 4 or len(homo_cols) >= 3:\n            score += 0.15\n        # sample size of keyframes\n        if rows >= 10:\n            score += 0.3\n        elif rows >= 5:\n            score += 0.15\n        return min(score, 1.0)\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Masking & Keying Completeness", "description": "Validate masking summary includes alpha channel yes and reasonable keyframe count; feather/edge blur present.", "weight": 0.8, "code": "import re, pandas as pd, numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        primary = context.get_primary_output()\n        if not primary or not primary.is_spreadsheet:\n            return 0.0\n        xls = pd.ExcelFile(context.files.get_path(primary.id))\n        sname = None\n        for s in xls.sheet_names:\n            if 'mask' in s.lower() or 'key' in s.lower():\n                sname = s\n                break\n        if not sname:\n            return 0.0\n        df = context.files.read_excel(primary.id, sheet_name=sname)\n        df.columns = [str(c).strip().lower() for c in df.columns]\n        # Expect columns: Property | Value\n        if not ('property' in df.columns and 'value' in df.columns):\n            return 0.0\n        props = dict(zip(df['property'].astype(str).str.lower(), df['value']))\n        score = 0.0\n        # Alpha Channel yes\n        alpha = str(props.get('alpha channel','')).strip().lower()\n        if alpha in ['yes','true','y']:\n            score += 0.3\n        # keyframes count\n        try:\n            kf = float(props.get('keyframes (count)', props.get('keyframes', np.nan)))\n            if np.isfinite(kf):\n                if kf >= 20:\n                    score += 0.3\n                elif kf >= 10:\n                    score += 0.15\n        except Exception:\n            pass\n        # feather and edge blur present\n        if any('feather' in k for k in props.keys()):\n            score += 0.1\n        if any('edge blur' in k for k in props.keys()):\n            score += 0.1\n        return min(score, 0.8)\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Color Grade Parameters Coverage", "description": "Check Color Grade sheet includes at least three parameters among Lift, Gamma, Gain, Exposure, Temperature, Tint, LUT.", "weight": 0.6, "code": "import re, pandas as pd, numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        primary = context.get_primary_output()\n        if not primary or not primary.is_spreadsheet:\n            return 0.0\n        xls = pd.ExcelFile(context.files.get_path(primary.id))\n        sname = None\n        for s in xls.sheet_names:\n            if 'color' in s.lower() or 'grade' in s.lower():\n                sname = s\n                break\n        if not sname:\n            return 0.0\n        df = context.files.read_excel(primary.id, sheet_name=sname)\n        df.columns = [str(c).strip().lower() for c in df.columns]\n        if not ('parameter' in df.columns):\n            return 0.0\n        params = df['parameter'].astype(str).str.lower().tolist()\n        target = ['lift','gamma','gain','exposure','temperature','tint','lut']\n        count = sum(1 for t in target if any(t in p for p in params))\n        if count >= 5:\n            return 0.6\n        elif count >= 3:\n            return 0.45\n        elif count >= 2:\n            return 0.2\n        else:\n            return 0.0\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "FX Events Coverage and Licensing", "description": "Ensure Teleport Flash exists and at least one Smoke element includes Source/URL and License indicating royalty-free usage.", "weight": 0.6, "code": "import re, pandas as pd, numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        primary = context.get_primary_output()\n        if not primary or not primary.is_spreadsheet:\n            return 0.0\n        xls = pd.ExcelFile(context.files.get_path(primary.id))\n        sname = None\n        for s in xls.sheet_names:\n            if 'fx' in s.lower() or 'events' in s.lower():\n                sname = s\n                break\n        if not sname:\n            return 0.0\n        df = context.files.read_excel(primary.id, sheet_name=sname)\n        df.columns = [str(c).strip().lower() for c in df.columns]\n        # Flexible column matching\n        def col_like(key):\n            for c in df.columns:\n                if key in c:\n                    return c\n            return None\n        event_col = col_like('event')\n        url_col = col_like('url') or col_like('source')\n        lic_col = col_like('license')\n        if not event_col:\n            return 0.0\n        ev = df[event_col].astype(str).str.lower()\n        has_flash = ev.str.contains('flash').any()\n        smoke_rows = df[ev.str.contains('smoke', na=False)] if 'smoke' in ev.to_string().lower() else pd.DataFrame()\n        score = 0.0\n        if has_flash:\n            score += 0.3\n        if not smoke_rows.empty and url_col and lic_col:\n            valid = False\n            for _, r in smoke_rows.iterrows():\n                url = str(r.get(url_col, '')).lower()\n                lic = str(r.get(lic_col, '')).lower()\n                if (url.startswith('http') or '://' in url) and any(k in lic for k in ['royalty-free','cc0','public domain','permissive']):\n                    valid = True\n                    break\n            if valid:\n                score += 0.3\n            elif not smoke_rows.empty:\n                score += 0.15\n        return min(score, 0.6)\n    except Exception:\n        return 0.0"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality Assessment", "description": "LLM judge evaluates the clarity, coherence, and professional rigor of the documentation and plan, not the visual quality of the video itself.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Documentation Clarity and Professional Rigor", "description": "Judge the workbook (and any attached PDF breakdown if provided) for professional completeness, clear methodology, and alignment with the VFX brief.", "weight": 2.0, "judge_prompt": "Assess the professional quality of the documentation ONLY (Excel workbook and any included PDF breakdown if present). Do not attempt to judge the visual outcome of the video. Consider:\n- Clarity: Are the steps (stabilization, masking, stitching, tracking/matchmove, color grade, FX timing) clearly described and easy to follow?\n- Completeness: Are all required process elements covered (segments defined, tracking data present, masking with alpha, color grade parameters, flash and smoke events with sources/licenses)?\n- Plausibility: Do the chosen timings and methods seem reasonable for making a believable teleportation moment? Are file specs and roles coherent?\n- Professionalism: Naming conventions, units (s/frames), timecodes, and notes that would allow another editor to reproduce the shot.\n\nScoring Guidance:\n- 2.0: Exceptionally clear, complete, and reproducible with coherent, industry-standard documentation.\n- 1.2: Generally clear and complete with minor gaps or ambiguities.\n- 0.6: Partial coverage; notable ambiguities but some usable structure.\n- 0.0: Vague or disorganized; insufficient to guide implementation.\n\nBe forgiving on minor naming variations; focus on whether a competent editor could reproduce the shot from this documentation.", "expectation": "A clean, reproducible package documenting the full VFX workflow with clear segments, tracking approach, masking choices, grading parameters, and FX event timing aligned to the brief."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "8c8fc328-69fc-4559-a13f-82087baef0a1", "rubric": {"category_name": "Information \u2014 Film and Video Editors \u2014 Basic Documentary Script (Pre-Papercut)", "rationale": "This rubric enforces a self-documenting, file-based deliverable for a basic documentary script. Stage 1 (LLM-only) strictly gates the required DOCX format and structural elements (title, metadata, timestamps, generalized scene list) so later checks are trivial. Stage 2 uses code to verify objective constraints enabled by the shape (title presence, timestamps order and bounds, scene/VO patterns, word-count proxy). Stage 3 uses LLM judgment to assess brand tone, dual-audience appropriateness, narrative flow, and professional polish.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Structured Script Format Gate", "description": "MANDATORY shape enforcement for a basic script in DOCX with required sections and timestamped scenes.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "DOCX + Required Structure Present", "description": "Checks the output is a .docx basic script with mandated sections and elements that make verification possible.", "weight": 4.0, "judge_prompt": "You are evaluating whether the candidate output meets the REQUIRED SHAPE for a basic documentary script deliverable. Only check PRESENCE and STRUCTURE, not writing quality or accuracy.\n\nDeliverable Requirements (must be verifiable from the rendered DOCX):\n1) File Format:\n   - Must be a DOCX file (not PDF/Markdown/Excel).  \n   - Total length must be under 5 pages.\n\n2) Mandatory Title and Header Metadata (near the top):\n   - Exact film title visible: \"Unseen Realms: The Microscopic Marvels\" (minor punctuation variance OK).  \n   - A short metadata block including all of the following labels or clearly equivalent terms:  \n     \u2022 Duration or Runtime target (total documentary length)  \n     \u2022 Audience (should mention children 6\u201312 and adults 25\u201334 together or equivalently)  \n     \u2022 Distribution: Broadcast and Internet (or both clearly referenced)  \n     \u2022 Tone/Brand alignment (calm, enriching, trustworthy, intellectually stimulating; flexible wording OK)\n\n3) Script Overview section (a brief paragraph) summarizing the documentary focus and intent.\n\n4) Basic Script section with generalized scenes (NOT a shooting script):\n   - A numbered list of scenes or sequences (e.g., \"Scene 1\", \"Sequence 1\", etc.), minimum of 4 entries.  \n   - Each scene includes:  \n     \u2022 A timestamp marker in mm:ss (e.g., 00:00, 01:30, etc.).  \n     \u2022 A generalized scene description (no specific shot lists/camera directions).  \n     \u2022 At least occasional VO cues (e.g., lines prefixed with \"VO:\" or labeled voiceover).  \n   - The overall timestamps should indicate a total runtime between 2\u20138 minutes (either via a clear end timestamp or stated Duration in metadata).\n\nScoring (STRUCTURE ONLY):\n- 4.0: DOCX under 5 pages AND all required elements present (title; metadata with duration, audience, distribution, tone; Script Overview; Basic Script with \u22654 timestamped generalized scenes including VO cues; runtime implied 2\u20138 min).  \n- 3.0: DOCX under 5 pages and structure mostly present, but missing exactly one minor element (e.g., Overview or Tone label) while still having title, audience, distribution, duration, and valid Basic Script with timestamps.  \n- 2.0: DOCX present but missing multiple required structural elements OR scenes lack timestamps/VO cues OR fewer than 4 scenes.  \n- 0.0: Not DOCX, over 5 pages, or the shape is fundamentally wrong (e.g., freeform text without structured scenes/timestamps).  \n\nImportant: Be flexible with section naming (e.g., \"Overview\" vs. \"Synopsis\"). Only check structure, not quality or factual accuracy.", "expectation": "A clearly structured DOCX script with title, metadata, overview, and a timestamped, generalized scene list that implies a 2\u20138 minute runtime."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Constraint Verification", "description": "Objective checks enabled by the enforced shape: title presence, timestamps coverage and order, scene structure, VO cues, and length proxy.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Title + Runtime Bounds", "description": "Verify exact title presence and total runtime implied by timestamps within 2\u20138 minutes (with graceful partial credit).", "weight": 1.5, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_document:\\n            return 0.0\\n        # Try to read DOCX text; if it fails, return 0\\n        try:\\n            text = context.files.read_docx_text(output.id)\\n        except Exception:\\n            return 0.0\\n        if not text:\\n            return 0.0\\n        score = 0.0\\n        # Title check (case-insensitive, punctuation-tolerant)\\n        title_ok = re.search(r\"unseen\\\\s+realms\\s*:\\s*the\\\\s+microscopic\\\\s+marvels\", text, re.IGNORECASE) is not None\\n        if title_ok:\\n            score += 0.5\\n        # Timestamp extraction mm:ss\\n        times = []\\n        for m in re.finditer(r\"\\\\b(\\\\d{1,2}):([0-5]\\\\d)\\\\b\", text):\\n            mm = int(m.group(1))\\n            ss = int(m.group(2))\\n            times.append(mm*60 + ss)\\n        if times:\\n            max_t = max(times)\\n            # Presence of 00:00 helps indicate proper start\\n            has_zero = '00:00' in text\\n            # Full credit if max within 120..480 and has 00:00\\n            if 120 <= max_t <= 480 and has_zero:\\n                score += 1.0\\n            # Partial if close to bounds or missing 00:00\\n            elif 90 <= max_t <= 540:\\n                score += 0.6\\n            else:\\n                score += 0.2\\n        # Clip to weight\\n        return min(score, 1.5)\\n    except Exception:\\n        return 0.0"}, {"type": "code", "name": "Timestamp Order Coherence", "description": "Check that timestamps appear in non-decreasing order and there are enough timestamped beats.", "weight": 1.0, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_document:\\n            return 0.0\\n        try:\\n            text = context.files.read_docx_text(output.id)\\n        except Exception:\\n            return 0.0\\n        if not text:\\n            return 0.0\\n        iters = list(re.finditer(r\"\\\\b(\\\\d{1,2}):([0-5]\\\\d)\\\\b\", text))\\n        if not iters:\\n            return 0.0\\n        times = []\\n        for m in iters:\\n            mm = int(m.group(1)); ss = int(m.group(2))\\n            times.append(mm*60 + ss)\\n        n = len(times)\\n        if n < 3:\\n            # Some timestamps present, but thin coverage\\n            return 0.3\\n        nondec = sum(1 for i in range(1, n) if times[i] >= times[i-1])\\n        frac = nondec / (n-1) if n > 1 else 0.0\\n        # Map to score: perfect -> 1.0, mostly -> 0.7, partial -> 0.4\\n        if frac >= 0.95:\\n            return 1.0\\n        elif frac >= 0.7:\\n            return 0.7\\n        elif frac > 0.0:\\n            return 0.4\\n        else:\\n            return 0.0\\n    except Exception:\\n        return 0.0"}, {"type": "code", "name": "Scene Structure + VO Cues", "description": "Verify presence of multiple generalized scenes and recurring VO cues.", "weight": 1.0, "code": "import re\\n\\nHEAD_RX = re.compile(r\"^(?:\\t|\\s)*(scene|sequence|section)\\s*\\d+\\b\", re.IGNORECASE)\\n\\ndef evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_document:\\n            return 0.0\\n        try:\\n            text = context.files.read_docx_text(output.id)\\n        except Exception:\\n            return 0.0\\n        if not text:\\n            return 0.0\\n        lines = [ln.strip() for ln in text.splitlines()]\\n        scene_heads = sum(1 for ln in lines if HEAD_RX.search(ln))\\n        # Count VO cues (VO: or Voiceover)\\n        vo_count = len(re.findall(r\"\\b(?:VO\\s*:|Voice\\s*over|Voiceover)\\b\", text, re.IGNORECASE))\\n        score = 0.0\\n        # Scene heads\\n        if scene_heads >= 6:\\n            score += 0.6\\n        elif scene_heads >= 4:\\n            score += 0.5\\n        elif scene_heads >= 2:\\n            score += 0.3\\n        # VO cues\\n        if vo_count >= 5:\\n            score += 0.4\\n        elif vo_count >= 3:\\n            score += 0.3\\n        elif vo_count >= 1:\\n            score += 0.15\\n        return min(score, 1.0)\\n    except Exception:\\n        return 0.0"}, {"type": "code", "name": "Length Proxy (Word Count)", "description": "Soft-check that the script is a plausible length for <5 pages: 300\u20132000 words ideal.", "weight": 0.5, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_document:\\n            return 0.0\\n        try:\\n            text = context.files.read_docx_text(output.id)\\n        except Exception:\\n            return 0.0\\n        words = re.findall(r\"\\\\w+\", text or \"\")\\n        wc = len(words)\\n        # Heuristic: 300\u20132000 words ~ up to ~5 pages in standard formatting\\n        if wc >= 300 and wc <= 2000:\\n            return 0.5\\n        elif wc < 300 and wc >= 120:\\n            return 0.3\\n        else:\\n            return 0.0\\n    except Exception:\\n        return 0.0"}, {"type": "llm_judge", "name": "Not a Shooting Script + Distribution Mention", "description": "Check that the script stays at the basic/pre-papercut level (no shot lists/camera directions) and mentions both broadcast and internet distribution somewhere.", "weight": 0.5, "judge_prompt": "Evaluate the candidate document for TWO structural constraints (not quality):\\n1) It is NOT a shooting script: avoid detailed shot lists or camera/lens directions (e.g., CU, ECU, dolly, 35mm, pan/tilt lists). Generalized scene descriptions are acceptable.\\n2) The document mentions both distribution channels: broadcast and internet (or clearly equivalent terms).\\n\\nScoring (structure only):\\n- 0.5: No shooting-script markers detected AND both broadcast and internet distribution are mentioned.\\n- 0.3: One of the two conditions met (either no shooting details OR distribution mentions only one channel).\\n- 0.0: Shooting-script style is present OR no distribution mention at all.\\n\\nDo not evaluate tone, wording quality, or correctness of content here\u2014only these structural constraints.", "expectation": "A basic script without camera/shot directives that explicitly references broadcast and internet distribution."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Audience Fit", "description": "Holistic LLM assessment of tone, clarity, pacing, and professional presentation for dual audiences.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Brand Tone + Dual-Audience Appropriateness", "description": "Assess whether the writing voice aligns with calm, enriching, trustworthy, intellectually stimulating branding and is suitable for children 6\u201312 and adults 25\u201334.", "weight": 1.0, "judge_prompt": "Judge the script\u2019s overall tone and audience suitability. Consider:  \n- Brand alignment: calm, enriching, trustworthy, intellectually stimulating (educational clarity without hype).  \n- Dual-audience appropriateness: accessible to children 6\u201312 (clear, non-frightening, engaging metaphors) while still appealing to adults 25\u201334 (informative, not simplistic).  \n- Avoids jargon overload and maintains an inviting, curiosity-forward feel.\n\nScoring:\n- 1.0: Strongly matches brand tone and clearly serves both audiences well.  \n- 0.7: Generally on-tone with minor lapses; broadly suitable for both audiences.  \n- 0.4: Mixed tone or skewed to only one audience.  \n- 0.0: Tone conflicts with brand or is inappropriate for the age ranges.", "expectation": "Calm, enriching, trustworthy, intellectually stimulating voice that engages kids and adults together."}, {"type": "llm_judge", "name": "Narrative Flow and Pacing (2\u20138 min)", "description": "Evaluate clarity of the story arc, transitions, and pacing appropriate to the implied runtime.", "weight": 0.6, "judge_prompt": "Assess narrative flow and pacing given a short-form documentary (2\u20138 minutes). Look for:  \n- Clear beginning/establishing context, middle/development, end/closure.  \n- Logical transitions between scenes/sequences.  \n- Appropriate density for the implied runtime (not rushed or meandering).  \n- VO cues support the structure without overcrowding.\n\nScoring:\n- 0.6: Strong, coherent arc with appropriate pacing and smooth transitions.  \n- 0.4: Generally coherent with a few rough spots.  \n- 0.2: Choppy/uneven pacing or unclear arc.  \n- 0.0: Disorganized or incoherent.", "expectation": "A concise, coherent arc with smooth transitions suitable for a 2\u20138 minute runtime."}, {"type": "llm_judge", "name": "Professional Formatting and Clarity", "description": "Assess readability, section labeling, and overall presentation polish for a client-facing basic script.", "weight": 0.4, "judge_prompt": "Evaluate presentation quality:  \n- Clear section labels (Metadata/Overview/Scenes) and consistent timestamping.  \n- Readable formatting (headings, lists, spacing) without clutter.  \n- Minimal typos/grammar issues that would distract a client.  \n- Concise, client-ready phrasing for a pre-papercut script.\n\nScoring:\n- 0.4: Clean, professional, and easy to navigate.  \n- 0.2: Minor issues but generally professional.  \n- 0.0: Sloppy or confusing formatting.", "expectation": "Clean headings and lists, consistent timestamps, and client-ready readability."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "85d95ce5-b20c-41e2-834e-e788ce9622b6", "rubric": {"category_name": "Social Developmental History Report (K-5 School Social Work)", "rationale": "Pattern B (Document). The deliverable is a polished, professional Social Developmental History report saved as a PDF titled \"J.S.\" for an IEP context. Stage 1 uses an LLM judge to strictly enforce format and structural shape (sections, length, headings, placeholders) so that verification is trivial. Stage 2 mixes code checks (file naming, required mentions, counts and section-specific constraints) with lightweight LLM checks where structure enables them. Stage 3 assesses professional quality and actionability for the intended school audience.", "max_total_score": 10.0, "stages": [{"name": "Stage 1: Format and Structural Gate", "description": "Gate: Verify the final deliverable is a properly structured Social Developmental History report in PDF format with required sections, page count, and mandated placeholders/fields. Only structure and presence\u2014no content quality judgments.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Document Requirements (Gate)", "description": "Check the output is a PDF with 8\u201315 pages and contains all required sections/structural elements per the task. Confirm use of the SCHOOL placeholder and blank fields on the first page.", "weight": 4.0, "judge_prompt": "You are evaluating whether the candidate output satisfies strict structural requirements for a Social Developmental History report. Only assess PRESENCE and STRUCTURE (not content quality or correctness). Be flexible with section naming when obviously synonymous.\n\nCheck the following:\n1) File Format and Length\n- Must be a PDF (not Word, not plain text).\n- Length must be between 8 and 15 pages inclusive.\n\n2) Core Document Structure (headers/sections must be clearly visible)\nRequired sections (flexible naming allowed; examples provided):\n- Student Information or Identifiers (should show student name; school must appear as \"SCHOOL\").\n- Reason for Referral or Referral Concerns.\n- Family and Household or Home Environment.\n- Developmental and Medical/Behavioral Health History.\n- Educational History and Current Functioning/Performance.\n- Attendance and Discipline/Behavioral Data.\n- Social-Emotional/Behavioral Functioning in School Settings.\n- Strengths and Interests/Motivators.\n- Cultural/Linguistic Background and Home Language (or similar equity/cultural context section).\n- Community Services/Agencies and Supports (outside providers/services).\n- School Social Work Impressions (narrative section).\n- School Social Work Recommendations (must be a numbered list; target 10\u201312 items).\n- Contacts/Interview Sources/Records Reviewed.\n- Signatures and Dates (or equivalent closing attestation block).\n\n3) Mandated Placeholders and Fields\n- The school name must be shown as the placeholder word \"SCHOOL\" (do not accept a real school name).\n- First-page fields for social worker name and address, and the student's address, must be left blank (visibly empty on the title/cover area or first page fields).\n\n4) Date Presence\n- The evaluation date 9/27/23 must appear somewhere in the document.\n\nScoring (return a single score only):\n- 4.0: PDF with 8\u201315 pages; all required sections present; recommendations are numbered (10\u201312 items visible); SCHOOL placeholder used; first-page specified fields are blank; evaluation date 9/27/23 appears.\n- 3.0: PDF and 8\u201315 pages; School Social Work Impressions and Recommendations sections present; recommendations are numbered; SCHOOL placeholder used; first-page specified fields blank; but missing up to two non-critical sections OR minor deviation (e.g., recommendations show 9 or 13 items) OR date formatting variant is present but clearly indicates 9/27/23.\n- 2.0: PDF and at least 6 pages; shows a recognizable Social Developmental History with the Impressions and Recommendations sections, but multiple required sections missing or page range outside 8\u201315; SCHOOL placeholder or blank fields not consistently applied; or recommendations list present but not clearly numbered.\n- 0.0: Not a PDF; fewer than 6 pages; missing Impressions or Recommendations sections; no SCHOOL placeholder; or blank field requirement clearly violated.\n\nImportant: Do NOT judge quality, accuracy of facts, or writing style here\u2014only structure, presence, formatting, and mandated placeholders.", "expectation": "A PDF, 8\u201315 pages, with all the specified sections, SCHOOL placeholder usage, blank first-page fields, visible evaluation date 9/27/23, and a 10\u201312 item numbered recommendations list."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Correctness and Compliance Checks", "description": "Verifies specific compliance details now that the structure is guaranteed. Uses code rules for deterministic checks of presence, counts, placeholders, and section content length; avoids subjective quality judgments.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "PDF is correctly named 'J.S.'", "description": "Confirm the primary output file is a PDF named exactly 'J.S.pdf' (case-insensitive match).", "weight": 0.5, "code": "import re\nfrom pathlib import Path\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output:\n            return 0.0, \"No output found.\"\n        p = context.files.get_path(output.id)\n        name = p.name.lower()\n        if name == 'j.s.pdf':\n            return 0.5, \"Filename matches 'J.S.pdf'.\"\n        return 0.0, f\"Filename '{p.name}' does not exactly match 'J.S.pdf'.\"\n    except Exception as e:\n        return 0.0, f\"Error checking filename: {e}\""}, {"type": "code", "name": "Student name and evaluation date present", "description": "Check that 'John Smith' (any case) and the evaluation date (9/27/23 or common variants) appear in the PDF text.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document output.\"\n        text = context.files.read_pdf_text(output.id)\n        if not text:\n            return 0.0, \"No extractable text.\"\n        t = text.lower()\n        has_name = 'john smith' in t or 'john  smith' in t\n        # Date variants: 9/27/23, 09/27/2023, 9-27-23, 9.27.23, September 27, 2023, Sept 27, 2023\n        date_patterns = [\n            r\"\\b0?9[/-]27[/-]23\\b\",\n            r\"\\b0?9[/-]27[/-]2023\\b\",\n            r\"\\b0?9[.]27[.]23\\b\",\n            r\"\\b(sept|september)\\s+27,\\s*2023\\b\",\n        ]\n        has_date = any(re.search(p, t, flags=re.I) for p in date_patterns)\n        score = 0.0\n        feedback = []\n        if has_name:\n            score += 0.4\n        else:\n            feedback.append(\"Name 'John Smith' not found.\")\n        if has_date:\n            score += 0.4\n        else:\n            feedback.append(\"Evaluation date 9/27/23 (or variant) not found.\")\n        return score, \"; \".join(feedback) if feedback else \"Name and date found.\"\n    except Exception as e:\n        return 0.0, f\"Error reading PDF text: {e}\""}, {"type": "code", "name": "Uses 'SCHOOL' placeholder consistently", "description": "Ensure the placeholder word 'SCHOOL' is used and appears multiple times, indicating consistent masking of the school's name.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document output.\"\n        text = context.files.read_pdf_text(output.id)\n        if not text:\n            return 0.0, \"No extractable text.\"\n        # Count exact uppercase placeholder occurrences\n        count = len(re.findall(r\"\\bSCHOOL\\b\", text))\n        if count >= 3:\n            return 0.6, f\"Found 'SCHOOL' {count} times.\"\n        elif count >= 1:\n            return 0.3, f\"Found 'SCHOOL' {count} time(s); consider using consistently.\"\n        else:\n            return 0.0, \"Placeholder 'SCHOOL' not found.\"\n    except Exception as e:\n        return 0.0, f\"Error scanning for placeholder: {e}\""}, {"type": "code", "name": "Recommendations count is 10\u201312 and numbered", "description": "Count numbered items in the 'School Social Work Recommendations' section; award full credit if between 10 and 12 inclusive.", "weight": 1.1, "code": "import re\n\ndef extract_section(text, start_header_patterns, end_header_patterns=None):\n    start_idx = None\n    for pat in start_header_patterns:\n        m = re.search(pat, text, flags=re.I)\n        if m:\n            start_idx = m.start()\n            break\n    if start_idx is None:\n        return None\n    if not end_header_patterns:\n        return text[start_idx:]\n    end_positions = []\n    for pat in end_header_patterns:\n        m = re.search(pat, text[start_idx:], flags=re.I)\n        if m:\n            end_positions.append(start_idx + m.start())\n    if end_positions:\n        return text[start_idx:min(end_positions)]\n    return text[start_idx:]\n\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document output.\"\n        text = context.files.read_pdf_text(output.id)\n        if not text:\n            return 0.0, \"No extractable text.\"\n        section = extract_section(\n            text,\n            [r\"School\\s+Social\\s+Work\\s+Recommendations\", r\"Recommendations\\b\"],\n            [r\"Signatures?\\b\", r\"Contacts?\\b\", r\"Appendix\\b\", r\"References?\\b\", r\"Acknowledg\"]\n        )\n        target = section if section else text\n        # Count lines that look like numbered list items (1. or 1) )\n        matches = re.findall(r\"(?m)^\\s*(?:\\d{1,2})[\\.)]\\s+\", target)\n        count = len(matches)\n        if 10 <= count <= 12:\n            return 1.1, f\"Detected {count} numbered recommendations.\"\n        elif 8 <= count <= 14:\n            return 0.6, f\"Detected {count} items (near target of 10\u201312).\"\n        else:\n            return 0.0, f\"Detected {count} items; expected 10\u201312 numbered recommendations.\"\n    except Exception as e:\n        return 0.0, f\"Error counting recommendations: {e}\""}, {"type": "code", "name": "Impressions section length and concluding opinion", "description": "Verify the 'School Social Work Impressions' section is at least ~150 words and includes a concluding recommendation/support statement near the end.", "weight": 1.0, "code": "import re\n\ndef extract_section(text, start_header_patterns, end_header_patterns=None):\n    start_idx = None\n    for pat in start_header_patterns:\n        m = re.search(pat, text, flags=re.I)\n        if m:\n            start_idx = m.start()\n            break\n    if start_idx is None:\n        return None\n    if not end_header_patterns:\n        return text[start_idx:]\n    end_positions = []\n    for pat in end_header_patterns:\n        m = re.search(pat, text[start_idx:], flags=re.I)\n        if m:\n            end_positions.append(start_idx + m.start())\n    if end_positions:\n        return text[start_idx:min(end_positions)]\n    return text[start_idx:]\n\n\ndef count_words(s):\n    return len(re.findall(r\"\\b\\w+\\b\", s))\n\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document output.\"\n        text = context.files.read_pdf_text(output.id)\n        if not text:\n            return 0.0, \"No extractable text.\"\n        section = extract_section(\n            text,\n            [r\"School\\s+Social\\s+Work\\s+Impressions\"],\n            [r\"School\\s+Social\\s+Work\\s+Recommendations\", r\"Recommendations\\b\", r\"Signatures?\\b\", r\"Contacts?\\b\"]\n        )\n        if not section:\n            return 0.0, \"Impressions section not found.\"\n        # Remove the header line itself for word count\n        section_body = re.sub(r\"^.*?\\n\", \"\", section, count=1, flags=re.S)\n        words = count_words(section_body)\n        score = 0.0\n        fb = []\n        if words >= 150:\n            score += 0.6\n        else:\n            fb.append(f\"Impressions length ~{words} words (<150).\")\n        # Check concluding statement in the last ~2 sentences\n        sentences = re.split(r\"(?<=[.!?])\\s+\", section_body.strip())\n        tail = \" \".join([s for s in sentences[-2:] if s]).lower()\n        if any(k in tail for k in [\"recommend\", \"support\", \"needs\", \"should\", \"services\", \"accommodations\", \"iep\", \"504\"]):\n            score += 0.4\n        else:\n            fb.append(\"No clear concluding support/recommendation statement near end.\")\n        return score, \"; \".join(fb) if fb else \"Impressions meets length and conclusion criteria.\"\n    except Exception as e:\n        return 0.0, f\"Error evaluating Impressions section: {e}\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3: Professional Quality and Actionability", "description": "Holistic quality assessment appropriate for a K\u20135 school setting: clarity, tone, person-first language, and actionable recommendations.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Tone and Readability", "description": "Assess if the document uses professional, trauma-informed, person-first, and culturally responsive language; is well-organized and readable for an IEP audience.", "weight": 1.0, "judge_prompt": "Evaluate the final PDF for overall professional quality suitable for a K\u20135 multidisciplinary IEP meeting. Consider:\n- Professional tone (objective, trauma-informed, avoids judgmental language, uses person-first phrasing).\n- Organization and readability (clear headings, logical flow, appropriate paragraphing, minimal errors in grammar/usage).\n- Appropriateness for school stakeholders (parents/guardians, teachers, administrators).\n\nScoring:\n- 1.0: Highly professional, clear, and appropriate throughout.\n- 0.7: Generally professional with minor issues.\n- 0.4: Mixed professionalism/clarity with noticeable issues.\n- 0.0: Unprofessional, confusing, or error-ridden.\n\nFocus on overall presentation quality, not factual accuracy.", "expectation": "A clear, professional, parent-friendly report with person-first, objective, and culturally respectful language."}, {"type": "llm_judge", "name": "Actionability of Recommendations", "description": "Assess whether the 10\u201312 recommendations are concrete, feasible in a school context, and aligned with described needs.", "weight": 1.0, "judge_prompt": "Review the 'School Social Work Recommendations' list. Judge whether items are specific, actionable, and feasible within an elementary school context, and aligned to the described concerns and impressions. Consider clarity of who does what, suggested frequency/duration where relevant, and alignment to observed needs.\n\nScoring:\n- 1.0: Recommendations are specific, feasible, and clearly actionable.\n- 0.7: Mostly actionable with minor vagueness.\n- 0.4: Several are generic or unclear.\n- 0.0: Largely vague or misaligned.\n\nDo not re-check counts; focus on actionability and school fit.", "expectation": "10\u201312 numbered recommendations that are specific, implementable in a K\u20135 school, and aligned with student needs."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "27e8912c-8bd5-44ba-ad87-64066ea05264", "rubric": {"category_name": "Ergonomics Material Preparation", "rationale": "This rubric evaluates the preparation of ergonomic materials intended to support HR and facilities teams in addressing staff complaints about workstation setups. The task requires the creation of structured documents that facilitate both individual ergonomic assessments and organizational tracking of action items. The rubric focuses on enforcing specific document structures, verifying correctness of content, and assessing the overall quality.", "max_total_score": 10.0, "stages": [{"name": "Format and Structure Validation Gate", "description": "Ensure deliverables are structured correctly with required sections and formats.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.0, "rules": [{"type": "llm_judge", "name": "Workstation Ergonomics Checklist Structure", "description": "Validation of the PDF structure for the ergonomics checklist.", "weight": 1.5, "judge_prompt": "Review the deliverable to ensure it is a PDF with no more than five pages and contains the following sections: \n- Stated goal at the beginning\n- Fields for name, position, email, and date\n- Ergonomic checklist focusing on office chair, keyboard and mouse, and work surface setup\n- Images or appendix with ergonomic setup images from credible sources\n- Ensure all sections are clearly visible and labeled.", "expectation": null}, {"type": "llm_judge", "name": "Organizational Action Items Document Structure", "description": "Validation of the Word document structure for organizational action tracking.", "weight": 1.5, "judge_prompt": "Check the Word document to ensure it includes the following:\n- A table with columns for: Ergonomic Assessment, Action Items, Status/Comments\n- Fields for employee/workstation details: Name, Department, Email, Date, Resolve details\n- A process section with steps: Available equipment, Vendor review, Ordering procedure, Resolution confirmation\n- Ensure all sections and fields are properly labeled and present.", "expectation": null}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Correctness Verification", "description": "Verify the correctness and credibility of ergonomic checklist content and action items table.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 2.0, "rules": [{"type": "code", "name": "Ergonomic Checklist Content Verification", "description": "Check the content and credibility of the ergonomic checklist against a credible source.", "weight": 2.0, "code": "def evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = context.files.read_pdf_text(output.id)\n    credible_source_indicator = 'National Institutes of Health' in text\n    chair_keywords = all(keyword in text.lower() for keyword in ['chair', 'support', 'height', 'adjust'])\n    keyboard_mouse_keywords = all(keyword in text.lower() for keyword in ['keyboard', 'mouse', 'position', 'distance'])\n    work_surface_keywords = all(keyword in text.lower() for keyword in ['work surface', 'height', 'placement'])\n    credibility_score = 0.5 if credible_source_indicator else 0.0\n    content_score = (chair_keywords + keyboard_mouse_keywords + work_surface_keywords) / 3.0\n    return credibility_score + content_score"}, {"type": "code", "name": "Action Items Table and Process Verification", "description": "Ensure the Word document effectively tracks ergonomic assessment actions and includes a complete process section.", "weight": 2.0, "code": "def evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = context.files.read_docx_text(output.id)\n    has_action_table = 'action item' in text.lower() and 'status' in text.lower()\n    process_steps = ['available', 'vendor', 'order', 'resolution']\n    has_process_section = all(step in text.lower() for step in process_steps)\n    return (has_action_table + has_process_section) / 2.0"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Quality Assessment", "description": "Evaluate the overall quality, presentation, and utility of the deliverables.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 1.5, "rules": [{"type": "llm_judge", "name": "Workstation Ergonomics Checklist Quality", "description": "Assess the professional quality and utility of the ergonomics checklist.", "weight": 1.5, "judge_prompt": "Consider the ergonomics checklist. Is it professionally formatted, clear, and easy to use for ergonomic assessments? Does it provide value and clarity to HR and facilities staff when addressing workstation setup concerns?", "expectation": "A high-quality checklist should be cleanly formatted, with clear guidance and useful images."}, {"type": "llm_judge", "name": "Organizational Action Items and Process Quality", "description": "Assess the clarity and practicality of the organizational action items document.", "weight": 1.5, "judge_prompt": "Evaluate the clarity and practicality of the organizational action items document. Does it effectively track actions and facilitate resolution? Is the process section clear and actionable?", "expectation": "The document should provide a clear framework for tracking ergonomics-related actions and should offer a practical guide for implementing resolutions."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "17111c03-aac7-45c2-857d-c06d8223d6ad", "rubric": {"category_name": "Gov Admin: Blight Cleanup Memo + Schedule (Mixed Outputs)", "rationale": "This is a mixed-output task: a professionally formatted memo (PDF/DOCX) plus a structured Excel schedule that staff can reference and share. The rubric enforces a verifiable shape first (Stage 1 LLM gate), then checks correctness/consistency with code and LLM (Stage 2), and finally evaluates professional quality (Stage 3).", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2013 Structured Output Gate (MUST PASS)", "description": "Validate that the submission includes BOTH deliverables with the exact, verifiable structure so downstream checks are trivial: a memo document (PDF preferred, DOCX acceptable) and an Excel schedule with a clear tabular structure.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Deliverables and Structure Present", "description": "Check presence and structure of BOTH required files: (1) a memo as PDF (preferred) or DOCX; (2) an Excel (.xlsx) schedule with specified sheet and columns. Verify memo has memo headers and required sections; verify Excel has a schedule table with key columns and sufficient rows. Only check structure/presence, not content quality or accuracy.", "weight": 4.0, "judge_prompt": "You are validating SHAPE ONLY. Review the candidate submission. Consider ALL available output files together, not just one. Confirm that BOTH deliverables exist and meet the following structural requirements:\n\nA) MEMO DOCUMENT (PDF preferred; DOCX acceptable)\nFormat requirements:\n- File type: PDF or DOCX (not plain text; not Excel)\n- Memo format with a visible header block including clear fields for: To, From, Date, Subject (flexible on exact labels, e.g., \"To:\" or \"To -\" etc.)\n- Professionally formatted paragraphs (not bullet-only or a single line)\nRequired sections (flexible naming, but all must be present):\n1) Purpose/Overview: states this is to inform admin staff of the tentative cleanup schedule for volunteers.\n2) Background/Context: mentions historic challenges (understaffing, lack of process, work left unfinished) and goal of a set/rotating schedule to improve blight remediation.\n3) Schedule Reference: explicitly references an attached Excel schedule that staff can use to inform volunteers about when/where crews will be working.\n4) Disruption Protocol: guidance for emergencies or severe weather, noting crews may temporarily shift areas and how to return/reschedule missed locations.\n5) Customer Service/Call Handling: states staff can provide estimates or timing to callers reporting debris/illegal dumping.\n6) Signature/Sign-off: sender identity and title (Administrative Services Manager or equivalent) visible. \n- Placeholder text must be replaced (no \"Your Name\", \"Date\", \"Subject\", \"TBD\", or obvious placeholders).\n\nB) EXCEL SCHEDULE (.xlsx)\nFormat requirements:\n- File type: Excel (.xlsx)\n- Contains a primary sheet named something like \"Cleanup Schedule\" (accept similar names: \"Schedule\", \"Blight Cleanup Schedule\", \"Crew Schedule\").\n- On that sheet, there is a clear tabular schedule with column headers. Required columns (flexible naming, but concepts must be present):\n  \u2022 Date or Week (e.g., Date, Week, Week Start)\n  \u2022 Area/Region (e.g., Area, Region, Zone, District, Neighborhood)\n  \u2022 Start/End or Shift timing (e.g., Start Time and End Time, or Shift)\n  \u2022 Crew lead/supervisor (e.g., Lead, Supervisor, Foreman)\n  \u2022 Volunteer suitability/availability (e.g., Volunteers, Volunteer Friendly, Allowed) \n  \u2022 Notes/Remarks\n- At least 4 scheduled rows (4 or more entries).\n- No placeholder text in the table (no \"TBD\", \"Lorem\", \"Your Name\").\n- Optional: A secondary sheet for legend/notes is acceptable but not required.\n\nScoring (STRUCTURE only):\n- 4.0: Both files present; memo has all required sections and memo headers; Excel is .xlsx with a schedule sheet (named as above), includes all required column concepts, and has >=4 rows; no placeholders.\n- 3.0: Both files present and mostly correct; minor omissions (e.g., missing one memo subsection OR one schedule column concept), but still clearly usable.\n- 2.0: One deliverable is significantly incomplete (e.g., memo missing disruption protocol, or Excel missing multiple required column concepts or <4 rows).\n- 1.0: Only one of the two deliverables present but structurally sound.\n- 0.0: Wrong formats or missing deliverables. \n\nOnly evaluate presence/structure. Do NOT judge calculation accuracy or writing quality.", "expectation": "A properly formatted memo (PDF/DOCX) with headers and required sections, plus an Excel schedule sheet with key columns and at least 4 rows, no placeholders."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness and Consistency Checks", "description": "With shape enforced, verify key correctness elements using code and LLM: files are parseable, structure aligns with intent, memo fields are filled with today\u2019s date and correct role, schedule integrity (dates, ordering, volunteer indicator), and memo policy coverage clarity.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Files Found + Excel Structure Parse", "description": "Confirm both a document (PDF/DOCX) and an Excel file exist; parse the Excel; check a likely schedule sheet exists; verify required column concepts are present and there are at least 4 data rows.", "weight": 1.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    # Locate outputs\n    outputs = context.get_all_outputs() or []\n    doc = None\n    xls = None\n    for r in outputs:\n        try:\n            if getattr(r, 'is_spreadsheet', False) and xls is None:\n                xls = r\n            if getattr(r, 'is_document', False) and doc is None:\n                doc = r\n        except Exception:\n            continue\n\n    score = 0.0\n    feedback = []\n\n    # 0.5 pts: both files found\n    have_both = (doc is not None) and (xls is not None)\n    if have_both:\n        score += 0.5\n    else:\n        if not doc:\n            feedback.append('Document (PDF/DOCX) not found.')\n        if not xls:\n            feedback.append('Excel schedule (.xlsx) not found.')\n        return max(0.0, min(score, 1.5)), '; '.join(feedback) if feedback else None\n\n    # Parse Excel\n    try:\n        xls_path = context.files.get_path(xls.id)\n        xf = pd.ExcelFile(xls_path)\n        sheet_names = [s for s in xf.sheet_names]\n        # pick schedule-like sheet if present\n        sched_sheet = None\n        for s in sheet_names:\n            ls = s.lower()\n            if any(k in ls for k in ['schedule','cleanup','blight','crew']):\n                sched_sheet = s\n                break\n        if sched_sheet is None and sheet_names:\n            sched_sheet = sheet_names[0]\n        df = xf.parse(sched_sheet)\n    except Exception as e:\n        feedback.append(f'Failed to open/parse Excel: {e}')\n        return max(0.0, min(score, 1.5)), '; '.join(feedback)\n\n    # Normalize columns\n    def norm(s):\n        s = str(s).strip().lower()\n        s = re.sub(r'[^a-z0-9]+',' ', s)\n        return s\n\n    cols = [norm(c) for c in df.columns]\n\n    # Column concept detection\n    def has_any(keys):\n        for c in cols:\n            for k in keys:\n                if k in c:\n                    return True\n        return False\n\n    req_concepts = {\n        'date_or_week': ['date','week','week start','day'],\n        'area_region': ['area','region','zone','district','neighborhood','ward'],\n        'start_time': ['start time','shift start','begin','from'],\n        'end_time': ['end time','shift end','until','to'],\n        'crew_lead': ['crew lead','supervisor','foreman','lead'],\n        'volunteer': ['volunteer','volunteers','public','allowed','friendly'],\n        'notes': ['notes','remarks','comments']\n    }\n\n    present = {k: has_any(v) for k,v in req_concepts.items()}\n    concepts_present = sum(1 for v in present.values() if v)\n\n    # 0.5 pts: sufficient columns present (>=4 concept buckets)\n    if concepts_present >= 4:\n        score += 0.5\n    else:\n        feedback.append(f'Only {concepts_present} of 7 schedule column concepts detected; need at least 4.')\n\n    # 0.5 pts: at least 4 data rows\n    data_rows = int(df.shape[0])\n    if data_rows >= 4:\n        score += 0.5\n    else:\n        feedback.append(f'Only {data_rows} schedule rows; need at least 4.')\n\n    return max(0.0, min(score, 1.5)), '; '.join(feedback) if feedback else None"}, {"type": "code", "name": "Memo Date, Role/Signature, and Placeholder-Free", "description": "Check memo text contains today\u2019s date in a common format, includes the role title (Administrative Services Manager), and is free of obvious placeholders like \"Your Name\", \"TBD\".", "weight": 0.8, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs() or []\n    doc = None\n    for r in outputs:\n        try:\n            if getattr(r, 'is_document', False):\n                doc = r\n                break\n        except Exception:\n            continue\n\n    score = 0.0\n    feedback = []\n\n    if not doc:\n        return 0.0, 'No memo document found.'\n\n    # Extract text\n    text = ''\n    try:\n        # Prefer PDF; if not, try DOCX\n        if str(getattr(doc, 'extension', '')).lower().endswith('pdf'):\n            text = context.files.read_pdf_text(doc.id) or ''\n        else:\n            text = context.files.read_docx_text(doc.id) or ''\n    except Exception:\n        # Try both fallbacks\n        try:\n            text = context.files.read_pdf_text(doc.id) or ''\n        except Exception:\n            try:\n                text = context.files.read_docx_text(doc.id) or ''\n            except Exception:\n                text = ''\n\n    lt = text.lower()\n\n    # Today date patterns\n    today = pd.Timestamp.now(tz=None)\n    # Build patterns: Month DD, YYYY; MM/DD/YYYY; YYYY-MM-DD\n    month_name = today.strftime('%B')\n    mdy_long = f\"{month_name} {today.day}, {today.year}\"\n    mdy_slash = today.strftime('%m/%d/%Y')\n    ymd_dash = today.strftime('%Y-%m-%d')\n\n    date_present = any(p in text for p in [mdy_long, mdy_slash, ymd_dash])\n    if date_present:\n        score += 0.3\n    else:\n        feedback.append('Today\\'s date not found in memo (common formats checked).')\n\n    # Role/title presence\n    if 'administrative services manager' in lt:\n        score += 0.3\n    else:\n        feedback.append('Role/title \"Administrative Services Manager\" not detected.')\n\n    # Placeholder checks\n    placeholders = ['your name','your title','tbd','lorem','xx/xx/xxxx','subject','[date]','<date>']\n    # Allow legitimate label words like 'subject:' if followed by real content; we do a simple heuristic: if 'subject' appears but also at least 5 words later\n    has_placeholder = False\n    for ph in placeholders:\n        if ph in lt:\n            has_placeholder = True\n            break\n    if not has_placeholder:\n        score += 0.2\n    else:\n        feedback.append('Placeholder-like text detected (e.g., Your Name/TBD/xx/xx/xxxx).')\n\n    return max(0.0, min(score, 0.8)), '; '.join(feedback) if feedback else None"}, {"type": "code", "name": "Schedule Temporal Integrity and Volunteer Flags", "description": "Verify the schedule has a usable date/week field with >=4 unique dates/weeks, generally ascending order, and a volunteer indicator column with recognizable values (Yes/No/True/False/etc.).", "weight": 0.7, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs() or []\n    xls = None\n    for r in outputs:\n        try:\n            if getattr(r, 'is_spreadsheet', False):\n                xls = r\n                break\n        except Exception:\n            continue\n\n    if not xls:\n        return 0.0, 'No Excel schedule found.'\n\n    feedback = []\n    score = 0.0\n\n    # Load Excel\n    try:\n        xls_path = context.files.get_path(xls.id)\n        xf = pd.ExcelFile(xls_path)\n        sheet_names = xf.sheet_names\n        sched_sheet = None\n        for s in sheet_names:\n            if any(k in s.lower() for k in ['schedule','cleanup','blight','crew']):\n                sched_sheet = s\n                break\n        if sched_sheet is None and sheet_names:\n            sched_sheet = sheet_names[0]\n        df = xf.parse(sched_sheet)\n    except Exception as e:\n        return 0.0, f'Failed to parse Excel: {e}'\n\n    if df.empty:\n        return 0.0, 'Schedule sheet appears empty.'\n\n    # Identify date/week column by best guess\n    def norm(s):\n        s = str(s).strip().lower()\n        s = re.sub(r'[^a-z0-9]+',' ', s)\n        return s\n\n    candidates = []\n    for c in df.columns:\n        nc = norm(c)\n        if any(k in nc for k in ['date','week','day']):\n            candidates.append(c)\n    date_col = candidates[0] if candidates else df.columns[0]\n\n    # Parse to datetime; handle week labels by coercing\n    series = pd.to_datetime(df[date_col], errors='coerce', infer_datetime_format=True)\n    unique_vals = series.dropna().dt.normalize().nunique()\n\n    # 0.3 pts: >=4 unique dates/weeks\n    if unique_vals >= 4:\n        score += 0.3\n    else:\n        feedback.append(f'Only {unique_vals} unique parsed dates/weeks; need at least 4.')\n\n    # 0.2 pts: generally ascending order (ignoring NaT)\n    parsed = series.dropna()\n    if not parsed.empty and parsed.is_monotonic_increasing:\n        score += 0.2\n    else:\n        # Try a relaxed check: sorted equality within non-NaT subset\n        if not parsed.empty and parsed.equals(parsed.sort_values().reset_index(drop=True)):\n            score += 0.2\n        else:\n            feedback.append('Dates/weeks not in ascending order.')\n\n    # 0.2 pts: volunteer indicator recognizable\n    cols = [norm(c) for c in df.columns]\n    vol_idx = None\n    for i, c in enumerate(cols):\n        if any(k in c for k in ['volunteer','public','allowed','friendly']):\n            vol_idx = i\n            break\n    if vol_idx is not None:\n        vals = df.iloc[:, vol_idx].astype(str).str.lower().str.strip()\n        ok_vals = vals.apply(lambda v: any(x in v for x in ['yes','no','true','false','y','n','allowed','open']))\n        share_ok = ok_vals.mean() if len(ok_vals) else 0.0\n        if share_ok >= 0.6:\n            score += 0.2\n        else:\n            feedback.append('Volunteer column values not in recognizable yes/no style for most rows.')\n    else:\n        feedback.append('No volunteer indicator column detected.')\n\n    return max(0.0, min(score, 0.7)), '; '.join(feedback) if feedback else None"}, {"type": "llm_judge", "name": "Policy Coverage and Operational Guidance", "description": "Check the memo text for clear coverage of: purpose, background issues, goal of rotating schedule, how staff should communicate with volunteers, disruption protocol (emergency/severe weather, temporary shifts, and return/reschedule plan), and reference to attached Excel schedule.", "weight": 1.0, "judge_prompt": "Review the memo document (PDF/DOCX). Does it explicitly include the following elements (flexible wording allowed)?\n1) Purpose/Overview: notifying admin staff about the tentative cleanup schedule to support volunteer communications.\n2) Background/Context: the historic issues (understaffing, lack of process, unfinished jobs) and goal of a set/rotating schedule to reduce weekly debris.\n3) Reference to the attached Excel schedule for detailed dates/areas.\n4) Volunteer communication guidance: what to tell callers, that staff can provide an estimate of abatement timing.\n5) Disruption protocol: addresses emergencies or severe weather, clarifies that crews may temporarily shift areas, and states plan to return to original locations or reschedule missed areas.\n\nScoring:\n- 1.0: All 5 elements clearly present.\n- 0.7: Missing one minor element.\n- 0.4: Missing two elements or coverage is very cursory.\n- 0.2: Mentions only 1\u20132 elements.\n- 0.0: Does not include these elements or memo is missing.\n\nEvaluate presence and clarity of these topics only; do not grade writing style here.", "expectation": "Memo covers all five content elements and clearly references the attached Excel schedule."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Professional Quality", "description": "Holistic assessment of professionalism, clarity, and usefulness for administrative staff and volunteer coordination.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Fitness for Purpose", "description": "Assess whether the memo is professionally written and formatted for a government admin audience and whether the schedule is easy to reference.", "weight": 2.0, "judge_prompt": "Evaluate overall professional quality and usefulness:\n- Clarity and concision appropriate for administrative staff; correct tone for a government memo.\n- Organization: clear headers (To/From/Date/Subject), logical flow, readable paragraphs, and a clear sign-off/signature.\n- Consistency: terms used in the memo align with the schedule (area/region names, timing references).\n- Usability: Excel schedule is easy to read (sensible column order, unambiguous labels), and the memo points admins to how to use it.\n- Mechanics: grammar, spelling, and formatting quality.\n\nScoring:\n- 2.0: Highly professional, clear, consistent with schedule, immediately usable.\n- 1.5: Generally professional with minor issues.\n- 1.0: Adequate but several issues reduce clarity/usability.\n- 0.5: Poorly presented; hard to use.\n- 0.0: Unprofessional or unusable.\n\nDo not re-check structure already validated in Stage 1; focus on quality and fitness for purpose.", "expectation": "A clear, well-formatted government memo paired with an easy-to-use Excel schedule that staff can confidently use when speaking with volunteers."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "aad21e4c-1d43-45fc-899a-97754a1b1b63", "rubric": {"category_name": "Legal Drafting: Share Subscription Agreement (NoxaPulse)", "rationale": "This staged rubric enforces a self-documenting structure for a U.S. startup share subscription agreement in Word format. Stage 1 (LLM-only) mandates an exact, reviewable document shape with required sections, deal terms, and a capitalization schedule. Stage 2 mixes code and LLM to verify presence and consistency of key terms, parties, investor protections, and the pre/post cap table using flexible pattern checks. Stage 3 assesses professional drafting quality, coherence, and appropriateness for a Delaware corporation and a minority angel investor.", "max_total_score": 10.0, "stages": [{"name": "Stage 1: Word Agreement Shape Gate", "description": "Gate: Validate the candidate produced a DOCX subscription agreement with all required sections, schedules, and placeholders to enable verification.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Document and Section Presence (DOCX only)", "description": "Check that the output is a Word document (DOCX) with all required sections, signature blocks, and a capitalization schedule showing pre- and post-issuance figures.", "weight": 4.0, "judge_prompt": "You are reviewing a candidate output for strict shape and structure (not content quality). Only pass if the output is a single DOCX Word document that contains all required sections below. Be flexible with exact headings but ensure each element clearly exists. Do NOT judge correctness of calculations or drafting quality here.\n\nFormat Requirements (MANDATORY):\n- File type: DOCX (Microsoft Word). Not PDF, not Markdown, not plain text, not Excel.\n- A professional agreement format with a clear title such as \u201cShare Subscription Agreement,\u201d \u201cStock Subscription Agreement,\u201d or \u201cSubscription Agreement.\u201d\n- Parties identified near the beginning: NoxaPulse Technologies, Inc. (a Delaware corporation) and Alan Gane (an individual).\n- Use bracketed placeholders for unknowns (e.g., [Date], [Address], [Title]).\n- Signature blocks for both Company and Investor with names/placeholders.\n\nRequired Sections (must be clearly visible; flexible naming allowed):\n1) Definitions or Defined Terms\n2) Subscription and Purchase Terms (the core deal terms)\n3) Company Representations and Warranties\n4) Investor Representations and Warranties (including accredited investor status)\n5) Covenants/Undertakings\n6) Information and Inspection Rights (minority investor information rights; no board/observer seat)\n7) Pre-emptive Rights (pro rata participation in future issuances)\n8) Minimum Ownership / Anti-Dilution Protection (maintain at least 10% fully diluted; includes a top-up mechanism and carve-outs for exempt/Excluded Issuances)\n9) Investor Consent/Protective Provisions over extraordinary actions (change of control, liquidation, adverse charter/bylaw amendments, material indebtedness, dividends/repurchases, material changes to management or business)\n10) Miscellaneous/General Provisions (e.g., Notices, Governing Law, Entire Agreement, Amendments, Counterparts)\n\nRequired Schedule (MANDATORY):\n- A clearly labeled schedule or exhibit titled like \u201cSchedule A \u2013 Capitalization\u201d or similar, showing the company\u2019s capitalization before and after the issuance. It should include at least: authorized common stock of 10,000,000, pre-issuance outstanding of 5,000,000 (owned by Eleanor Byrne), issuance of 1,000,000 shares to Alan, and post-issuance outstanding of 6,000,000 with post-ownership percentages (e.g., Eleanor ~83.33%, Alan ~16.67%). A table format is preferred but not strictly required if the information is laid out clearly in rows.\n\nScoring Guidance (STRUCTURE ONLY):\n- 4.0: DOCX present AND all sections + required schedule present, including placeholders and signature blocks.\n- 3.0: DOCX present; missing exactly one required rights section or minor schedule detail (but schedule exists and shows pre- and post-issuance figures).\n- 2.0: DOCX present; several core sections present but missing the capitalization schedule OR multiple key rights sections.\n- 0.0: Not a DOCX OR missing multiple core sections and the schedule.\n\nOnly evaluate presence and structure, not legal accuracy or calculation correctness.", "expectation": "A DOCX agreement with all listed sections, signature blocks, bracketed placeholders, and a clear pre/post capitalization schedule reflecting the specified numbers."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Correctness and Compliance Verification", "description": "Verify presence and consistency of key deal terms, parties, protections, and capitalization details using code-based checks and an LLM cross-check.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Core Deal Terms Present (Shares, Price, Common)", "description": "Verify presence of 1,000,000 common shares and $500,000 purchase price; confirm reference to common stock/shares.", "weight": 0.7, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = ''\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0\n    t = text.lower()\n\n    shares_patterns = [r\"\\b1[, ]?0{3}[, ]?0{3}\\b\", r\"\\b1\\s*million\\b\"]\n    price_patterns = [r\"\\$\\s*500[, ]?000\\b\", r\"\\b500[, ]?000\\b\"]\n    common_patterns = [r\"common stock\", r\"common shares\"]\n\n    def found_any(patterns):\n        return any(re.search(p, t) for p in patterns)\n\n    found_shares = found_any(shares_patterns)\n    found_price = found_any(price_patterns)\n    found_common = found_any(common_patterns)\n\n    found = sum([found_shares, found_price, found_common])\n    score = found / 3.0\n    return score"}, {"type": "code", "name": "Parties and Identities Present", "description": "Verify references to NoxaPulse Technologies, Inc., Delaware corporation, Alan Gane, and Eleanor Byrne.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0\n    t = text.lower()\n\n    checks = [\n        ('noxapulse technologies', 'company name'),\n        ('delaware', 'delaware mention'),\n        ('alan gane', 'investor name'),\n        ('eleanor byrne', 'founder name'),\n    ]\n    found = 0\n    for needle, _ in checks:\n        if needle in t:\n            found += 1\n    return found / len(checks)"}, {"type": "code", "name": "Section Coverage (Reps, Covenants, Rights)", "description": "Check for core sections: definitions, subscription/purchase terms, company reps, investor reps (accredited), covenants, information/inspection rights, pre-emptive rights, anti-dilution/minimum ownership/top-up, consent/protective provisions, and miscellaneous/governing law.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0\n    t = text.lower()\n\n    buckets = [\n        [\"definition\"],\n        [\"subscription\", \"purchase\"],\n        [\"representations and warranties of the company\", \"company representations\"],\n        [\"representations and warranties of the investor\", \"investor representations\", \"accredited investor\"],\n        [\"covenant\"],\n        [\"information rights\", \"inspection rights\", \"books and records\"],\n        [\"pre-emptive\", \"preemptive\"],\n        [\"anti-dilution\", \"antidilution\", \"minimum ownership\", \"top-up\", \"top up\"],\n        [\"protective provisions\", \"consent rights\", \"prior consent\"],\n        [\"miscellaneous\", \"governing law\", \"entire agreement\"]\n    ]\n\n    def bucket_found(keys):\n        return any(k in t for k in keys)\n\n    found = sum(bucket_found(b) for b in buckets)\n    return found / len(buckets)"}, {"type": "code", "name": "Minority Info/Inspection Rights; No Board/Observer Seat", "description": "Verify information and inspection rights for the investor, mention of material developments notices, and absence or negation of board/observer seat rights.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0\n    t = text.lower()\n\n    info = any(k in t for k in [\"information rights\", \"financial statements\", \"reporting\", \"material developments\", \"material development\", \"notice of material\"])\n    inspect = any(k in t for k in [\"inspection rights\", \"books and records\", \"inspect\", \"examine records\"])\n\n    # Evidence of no board/observer seat: either explicit negation or absence of grant\n    explicit_no_board = any(k in t for k in [\"no right to appoint\", \"no right to designate\", \"no board seat\", \"no observer rights\", \"no board observer\"])\n    # Weak signal if the document never mentions observer/director appointment at all (avoid penalizing absence)\n    mentions_observer = \"observer\" in t or \"board seat\" in t\n\n    score = 0\n    if info:\n        score += 1\n    if inspect:\n        score += 1\n    if explicit_no_board or not mentions_observer:\n        score += 1\n    return score / 3.0"}, {"type": "code", "name": "Anti-Dilution Minimum Ownership, Top-up, Exempt Issuances, and Pre-emptive Pro Rata", "description": "Verify minimum 10% fully diluted ownership protection with top-up and exempt issuances carve-out, and presence of pre-emptive/pro rata participation rights.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0\n    t = text.lower()\n\n    min_own = (\"minimum ownership\" in t or \"maintain\" in t) and (\"10%\" in t or \"ten percent\" in t) and (\"fully diluted\" in t)\n    top_up = (\"top-up\" in t or \"top up\" in t or \"true-up\" in t or \"true up\" in t)\n    exempt = (\"exempt issuance\" in t or \"excluded issuance\" in t or \"excluded issuances\" in t or \"exempt issuances\" in t)\n    preempt = (\"pre-emptive\" in t or \"preemptive\" in t) and (\"pro rata\" in t or \"pro-rata\" in t)\n\n    found = sum([min_own, top_up, exempt, preempt])\n    return found / 4.0"}, {"type": "code", "name": "Consent/Protective Provisions Coverage", "description": "Verify consent rights over extraordinary actions: change of control, liquidation, adverse charter/bylaw amendments, material indebtedness, dividends/repurchases, and material changes to management or business.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0\n    t = text.lower()\n\n    checks = [\n        any(k in t for k in [\"change of control\", \"sale of all or substantially all\", \"merger\", \"consolidation\"]),\n        any(k in t for k in [\"liquidation\", \"dissolution\", \"winding up\"]),\n        any(k in t for k in [\"amend\", \"modif\"] ) and any(k in t for k in [\"certificate of incorporation\", \"charter\", \"bylaws\", \"by-laws\"]) and any(k in t for k in [\"adverse\", \"adversely affect\", \"adversely change\"]),\n        any(k in t for k in [\"material indebtedness\", \"incur debt\", \"borrowings\", \"indebtedness\"]),\n        any(k in t for k in [\"dividends\", \"distributions\", \"repurchases\", \"redemptions\", \"buyback\"]),\n        any(k in t for k in [\"material change\", \"material changes\"]) and any(k in t for k in [\"management\", \"business\"]) \n    ]\n    found = sum(1 for c in checks if c)\n    return found / len(checks)"}, {"type": "code", "name": "Capitalization Schedule Details (Pre/Post)", "description": "Verify presence of a schedule with pre/post capitalization including authorized 10,000,000, outstanding 5,000,000 pre, issuance of 1,000,000, and post-outstanding ~6,000,000.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0\n    t = text.lower()\n\n    has_schedule = (\"schedule\" in t or \"exhibit\" in t) and (\"capitalization\" in t or \"cap table\" in t)\n\n    auth_ok = bool(re.search(r\"authorized[^\\n\\r]*10[, ]?000[, ]?000\", t))\n    pre_out_ok = bool(re.search(r\"outstanding[^\\n\\r]*5[, ]?000[, ]?000\", t))\n    issuance_ok = bool(re.search(r\"1[, ]?000[, ]?000\", t))\n    post_out_ok = bool(re.search(r\"post[^\\n\\r]*6[, ]?000[, ]?000\", t)) or (\"6,000,000\" in t or \"6000000\" in t)\n\n    found = sum([has_schedule, auth_ok, pre_out_ok, issuance_ok, post_out_ok])\n    # Cap at 1.0\n    return min(1.0, found / 5.0)"}, {"type": "code", "name": "Accredited Investor and Securities Law Legends", "description": "Verify the investor is stated as an accredited investor and typical securities law legends/acknowledgments are present (e.g., not registered, Securities Act references, Reg D/Rule 506, restricted securities).", "weight": 0.3, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0\n    t = text.lower()\n\n    accredited = \"accredited investor\" in t\n    sec_act = (\"securities act of 1933\" in t) or (\"regulation d\" in t) or (\"rule 506\" in t) or (\"restricted securities\" in t) or (\"not registered\" in t)\n\n    found = sum([accredited, sec_act])\n    return found / 2.0"}, {"type": "code", "name": "Bracketed Placeholders Present", "description": "Verify bracketed placeholders like [Date], [Address], [Title], or [\u25cf] exist.", "weight": 0.2, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0\n\n    t = text\n    has_brackets = bool(re.search(r\"\\[[^\\]]+\\]\", t)) or (\"[\\u25cf]\" in t) or (\"\\u25cf\" in t) or (\"[\u25cf]\" in t)\n    return 1.0 if has_brackets else 0.0"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3: Professional Quality Assessment", "description": "Holistic legal drafting quality, coherence, and appropriateness for a Delaware startup and a minority angel investor.", "is_required": false, "max_points": 1.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Drafting Quality, Coherence, and Appropriateness", "description": "Assess whether the agreement is professionally drafted, cohesive, and fit-for-purpose for the client and the context.", "weight": 1.0, "judge_prompt": "Evaluate the professional quality and appropriateness of the Word agreement. Consider:\n- Clarity and cohesion of defined terms, internal references, and numbering.\n- Whether the protective provisions and anti-dilution mechanics are clearly drafted (e.g., how the top-up works, timing, calculations, exempt issuances), and do not inadvertently overreach or create ambiguity.\n- Information and inspection rights appropriately provide visibility without implying management or board/observer rights (ensure no contradictory grant elsewhere).\n- Pre-emptive rights are practical and include typical notice/timing mechanics.\n- Governing law and other boilerplate are standard for a Delaware corporation; notices, amendments, counterparts, electronic signatures present and reasonable.\n- The capitalization schedule is readable and aligns narratively with the described transaction (do not recompute math; just check clarity and alignment of descriptions).\n\nScoring guidance:\n- 1.0: High professional quality: clear, cohesive, internally consistent, and appropriately tailored for a minority angel in a Delaware startup.\n- 0.5: Adequate quality with minor issues in clarity or completeness, but overall usable.\n- 0.0: Low quality: confusing, inconsistent, or inappropriate for the stated context.\n", "expectation": "A polished, cohesive, and practical subscription agreement suitable for a Delaware startup and a minority angel investor, with clear rights and mechanisms and no contradictory grants."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "b1a79ce1-86b0-41fb-97dc-9206dfd7b044", "rubric": {"category_name": "Producer/Director Creative Artifact \u2014 Music Video Moodboard (PNG)", "rationale": "Task Type = Document-like visual artifact (Pattern B) with a single image deliverable. Stage 1 forces a rigid, visually-verifiable layout inside the PNG so downstream checks are trivial. Stage 2 mixes lightweight code (file validity, size, dimensions, naming) with an LLM rule to verify alignment to the brief now that the structure exists. Stage 3 is a holistic LLM quality assessment emphasizing professional polish, cohesion, and production usability.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 STRUCTURE GATE (LLM-only)", "description": "Enforce exact, visually-verifiable structure of a single PNG moodboard so verification is possible.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 5.6, "rules": [{"type": "llm_judge", "name": "Moodboard Structural Requirements (PNG)", "description": "Check the delivered artifact is a single PNG moodboard that follows the required layout and includes the mandated sections to enable verification.", "weight": 8.0, "judge_prompt": "You are validating the SHAPE ONLY of the candidate output. Do NOT judge quality or correctness beyond presence/structure. You are looking at a single image file (PNG). Verify the following structural requirements are visibly present on the moodboard itself:\n\nFormat\n- The output is a single PNG image (not a PDF/DOCX/Excel or multiple files).\n- The layout is clearly organized into sections (title/header area, references grid, palette strip, notes/captions panel).\n\nRequired Sections (be flexible on exact titles but the content must be visibly there):\n1) Title/Header\n   - A visible title/header indicating this is a moodboard for a music video (e.g., project name, \u201cMoodboard\u201d, or similar). It should be clearly identifiable near the top.\n\n2) Color Palette with Hex Codes\n   - A palette strip/area with 5\u20138 distinct color swatches.\n   - Each swatch is labeled with a HEX code in #RRGGBB format (e.g., #1A2233). Swatch labels must be visible on the board.\n\n3) Visual References Grid\n   - A grid/cluster of at least 6 reference images (photography, film stills, decor, wardrobe, lighting, etc.).\n   - Images are neatly arranged (rows/columns or tidy clusters) and not just a single collage without discernible grouping.\n\n4) Thematic Labels\n   - At least 3 short text labels attached to clusters/images or in a legend referring to these themes: Elegance, Theatrical, Tension, Vulnerability, Orchestral, Inner Conflict, or Contrast. Flexible wording is fine (e.g., \u201cElegant\u201d instead of \u201cElegance\u201d).\n\n5) Direction Notes Panel\n   - A small notes/captions panel with at least 3 bullet points or short lines calling out production-relevant cues (e.g., lighting, wardrobe, set dressing, camera feel). Plain text on the board is fine.\n\nScoring (0\u20138 points):\n- 8: PNG and all five sections present, clearly visible, well laid out.\n- 6: PNG and 4 of 5 sections present (only one missing or clearly insufficient).\n- 4: PNG and 3 of 5 sections present.\n- 2: PNG but only 1\u20132 sections present.\n- 0: Not a PNG, or structure missing/unclear (multiple sections absent).\n\nOnly evaluate presence/structure, not quality or correctness.", "expectation": "A single PNG moodboard containing: a visible title/header; a 5\u20138 swatch color palette with HEX codes; a grid of 6+ reference images; at least 3 thematic labels; and a notes/captions panel with 3+ bullets."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 VERIFICATION (Content + Technical)", "description": "Now that the shape exists, verify technical validity with code and check content alignment with the brief using LLM.", "is_required": true, "max_points": 7.0, "min_score_to_pass": 3.5, "rules": [{"type": "code", "name": "PNG Validity and Readability", "description": "Verify the primary output is a PNG image with valid header, reasonable dimensions, and non-trivial file size so on-board text is likely readable.", "weight": 2.0, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float (score) or tuple[float, str]\n    \"\"\"\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No primary output found.\"\n    if not getattr(output, 'is_image', False):\n        return 0.0, \"Primary output is not an image.\"\n\n    # Initialize\n    score = 0.0\n    feedback_parts = []\n\n    try:\n        path = context.files.get_path(output.id)\n        suffix_ok = str(path.suffix).lower() == '.png'\n        if not suffix_ok:\n            feedback_parts.append(\"File extension is not .png\")\n        # Read PNG signature and IHDR to get dimensions without external libs\n        width = height = None\n        with open(path, 'rb') as f:\n            sig = f.read(8)\n            is_png_sig = sig == b'\\x89PNG\\r\\n\\x1a\\n'\n            # Read first chunk (should be IHDR)\n            len_bytes = f.read(4)\n            ctype = f.read(4)\n            # IHDR data is 13 bytes\n            data = f.read(13) if ctype == b'IHDR' and len(len_bytes) == 4 else b''\n            if ctype == b'IHDR' and len(data) == 13:\n                width = int.from_bytes(data[0:4], 'big')\n                height = int.from_bytes(data[4:8], 'big')\n            else:\n                feedback_parts.append(\"IHDR chunk not found/invalid; cannot confirm dimensions.\")\n        # Subscores within 2.0 total\n        # 1) PNG format (ext + signature): up to 1.0\n        is_png = suffix_ok and ('is_png_sig' in locals() and is_png_sig)\n        score += 1.0 if is_png else 0.0\n        if is_png:\n            feedback_parts.append(\"PNG signature and extension OK.\")\n        else:\n            feedback_parts.append(\"PNG signature/extension check failed.\")\n        # 2) Dimensions: prefer >=1200x800; partial for >=1000x700\n        dim_sub = 0.0\n        if isinstance(width, int) and isinstance(height, int):\n            if width >= 1200 and height >= 800:\n                dim_sub = 0.7\n                feedback_parts.append(f\"Dimensions OK: {width}x{height}.\")\n            elif width >= 1000 and height >= 700:\n                dim_sub = 0.35\n                feedback_parts.append(f\"Dimensions marginal but acceptable: {width}x{height}.\")\n            else:\n                feedback_parts.append(f\"Dimensions too small: {width}x{height}.\")\n        else:\n            feedback_parts.append(\"Could not read dimensions from IHDR.\")\n        score += dim_sub\n        # 3) File size between ~150KB and 30MB: up to 0.3\n        try:\n            size_bytes = path.stat().st_size\n            if 150_000 <= size_bytes <= 30_000_000:\n                score += 0.3\n                feedback_parts.append(f\"File size OK: {size_bytes} bytes.\")\n            else:\n                feedback_parts.append(f\"File size out of expected range: {size_bytes} bytes.\")\n        except Exception:\n            feedback_parts.append(\"Could not read file size.\")\n        # Cap to weight\n        score = min(score, 2.0)\n        return score, \"; \".join(feedback_parts)\n    except Exception as e:\n        return 0.0, f\"Error verifying PNG validity: {e}\""}, {"type": "code", "name": "Helpful File Naming", "description": "Encourage descriptive naming that signals artifact purpose.", "weight": 0.5, "code": "def evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No primary output found.\"\n    try:\n        path = context.files.get_path(output.id)\n        name = path.stem.lower()\n        keywords = [\"mood\", \"board\", \"moodboard\", \"palette\"]\n        if any(k in name for k in keywords):\n            return 0.5, f\"Descriptive filename detected: '{path.name}'.\"\n        return 0.0, f\"Filename '{path.name}' lacks moodboard-related keywords.\"\n    except Exception as e:\n        return 0.0, f\"Error checking filename: {e}\""}, {"type": "llm_judge", "name": "Brief Alignment and Content Presence", "description": "Check that the structured content aligns with the creative brief and includes the mandated content now that structure exists.", "weight": 4.5, "judge_prompt": "Assess the PNG moodboard for alignment with the brief of a slow-building orchestral ballad that is elegant yet tense and vulnerable, leaning into a rich, theatrical visual style and the contrast between outer beauty and inner conflict.\n\nVerify the following (content-level, not just shape):\n- Palette: 5\u20138 swatches with visible HEX codes that fit the vibe (e.g., jewel tones like deep burgundy, emerald, sapphire, gold/brass accents; balanced with black/ivory/graphite). Overly neon or cartoonish palettes are misaligned.\n- References: At least 6 reference images coherently grouped/arranged. Imagery should plausibly communicate elegance + theatricality + tension/vulnerability (e.g., drapery/velvet, spotlight/shadows, mirrors/reflections, veils/smoke/rain, stage architecture, classical instruments or orchestral cues, poised wardrobe with subtle distress).\n- Thematic labels: At least 3 labels clearly map to themes like Elegance, Theatrical, Tension, Vulnerability, Orchestral, Inner Conflict, or Contrast (synonyms acceptable).\n- Notes panel: 3+ short notes give actionable production direction (lighting, wardrobe, set design, camera feel).\n\nScoring (0\u20134.5):\n- 4.5: All four checks pass and the content is clearly aligned with the brief.\n- 3.5: Three checks pass with minor misalignments.\n- 2.5: Two checks pass or alignment is mixed.\n- 1.0: Only one check passes or alignment is weak.\n- 0.0: Misaligned overall or required content missing.\nProvide a 1\u20133 sentence rationale.", "expectation": "A visually coherent, brief-aligned board: jewel/neutral palette with HEX codes, 6+ references expressing elegance, theatricality, tension/vulnerability, 3+ thematic labels tied to those ideas, and 3+ practical production notes."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 QUALITY (Holistic LLM)", "description": "Professional presentation, cohesion, and production usefulness.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professionalism, Cohesion, and Utility", "description": "Holistic evaluation of design quality and usefulness to a production team.", "weight": 5.0, "judge_prompt": "Evaluate the overall professional quality of the PNG moodboard:\n- Visual cohesion: Are type, spacing, and grid/layout clean and consistent? Are captions/labels legible on their backgrounds?\n- Color craft: Do the chosen colors harmonize and support the intended mood without clashing or over-saturation?\n- Narrative clarity: Does the board clearly communicate the contrast between outer beauty and inner conflict, and the elegant/theatrical/tense mood of a slow-building orchestral ballad?\n- Production utility: Would this board effectively align a director/art director/DP/wardrobe team (clarity, specificity without clutter)?\n\nScoring (0\u20135):\n- 5: Polished, cohesive, and highly useful to crew; communicates mood and contrast unmistakably.\n- 4: Strong and production-ready with minor issues.\n- 3: Adequate; communicates intent but could be clearer or more refined.\n- 2: Rough; uneven layout or unclear story; limited utility.\n- 0\u20131: Low quality or confusing; not practically useful.\nProvide 1\u20133 sentence justification.", "expectation": "A polished, legible, cohesive board that a film crew could confidently reference for look-and-feel decisions."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "bbe0a93b-ebf0-40b0-98dc-8d9243099034", "rubric": {"category_name": "Government | Child, Family, and School Social Workers: Bilingual Needs Assessment + Kent County Resource Guide", "rationale": "This rubric enforces a self-documenting workflow: Stage 1 (LLM-only) mandates exact file shapes and section/table structures needed for verification. Stage 2 mixes code and LLM checks to validate content presence, bilingual coverage, contact info sufficiency, and cross-document consistency, leveraging text extraction from PDFs. Stage 3 assesses professional presentation, usability for frontline staff, and local relevance of the resource guide.", "max_total_score": 20.0, "stages": [{"name": "Stage 1: Format and Structural Gate (LLM-only)", "description": "Gate that verifies the required outputs exist as separate PDFs with the mandated structures and tables, enabling downstream verification.", "is_required": true, "max_points": 10.0, "min_score_to_pass": 8.0, "rules": [{"type": "llm_judge", "name": "Bilingual Needs Assessment Structure (2 separate PDFs)", "description": "Verify there are TWO separate needs assessment PDFs: one in English and one in Spanish. Each must include a screening table with 3 columns (Area of Need Questions, Yes, No) and a follow-up tracking table with specified labels. This is structure-only, not quality or correctness.", "weight": 6.0, "judge_prompt": "You are evaluating the candidate outputs (all files) for required STRUCTURE ONLY. Do not judge writing quality or correctness here.\n\nCheck across all submitted files for TWO separate PDF documents that are needs assessments: one in English and one in Spanish. Each assessment must include BOTH of these elements on the pages:\n\n1) A screening table with exactly three columns:\n   - Column for questions/areas of need (flexible header names allowed like: \"Questions related to areas of need\", \"Area of Need\", or similar)\n   - A column labeled Yes (English: \"Yes\"; Spanish: \"S\u00ed\" or \"Si\" accepted)\n   - A column labeled No (English: \"No\"; Spanish: \"No\")\n   The table must include rows for the following need areas (flexible phrasing allowed): income, food, housing/shelter, clothing, education, financial literacy, transportation, employment, legal assistance, pet services, and healthcare.\n\n2) A follow-up tracking table in the same assessment document with columns labeled exactly or very closely to: \"Individual or Family Need\", \"Resource Given\", and \"Follow-Up Needed\". For the Spanish assessment, either the English labels or accurate Spanish equivalents (e.g., \"Necesidad individual o familiar\", \"Recurso proporcionado\", \"Se necesita seguimiento\") are acceptable.\n\nAdditional requirements:\n- The English and Spanish assessments must be two separate PDF files (not a single bilingual PDF). Minor bilingual notes on either file are okay, but the main content should be in the stated language.\n- Each assessment should be at least 1 page and clearly show the two required tables.\n\nScoring (0-6):\n- 6: Two separate assessment PDFs found (one English, one Spanish). Both show the 3-column screening table (with Yes/No) AND the follow-up tracking table with the required labels. The screening table includes most or all listed need areas (allow up to 1-2 minor omissions if structure is otherwise correct).\n- 4-5: Both files present and separate, but minor structural gaps (e.g., one table mislabeled, 1-2 columns unclear, or 2-3 need areas missing); OR a single bilingual PDF that clearly duplicates all required structures for both languages.\n- 2-3: Only one language found or key table absent in one file (e.g., missing follow-up table or missing Yes/No columns), but there is clear evidence of the assessment intent and some required structure.\n- 0-1: Assessments not in PDF, not separate, or required tables/columns largely missing.\n\nOnly evaluate structure and presence. Do not judge translation quality or content accuracy here.", "expectation": "Two separate assessment PDFs (English and Spanish) each with: (a) 3-column screening table [Needs | Yes | No] covering listed categories; (b) tracking table [Individual or Family Need | Resource Given | Follow-Up Needed]."}, {"type": "llm_judge", "name": "Resource Guide Structure (Separate PDF)", "description": "Verify a separate Resource Guide PDF exists, organized by service categories, each with resource entries containing contact info (at least phone, website, email, or address). Structure-only check.", "weight": 4.0, "judge_prompt": "Evaluate all submitted files for a separate PDF that serves as a Resource Guide for Kent County, Michigan.\n\nRequired structure for the Resource Guide PDF:\n- Clearly organized by service categories (headings). Flexible naming allowed, but should cover many of: Financial Assistance, Transportation, Food Pantry, Employment, Clothing, Healthcare, Counseling, Legal Services, Pregnancy Support (additional relevant categories welcome).\n- Each category lists multiple resources with at least: organization name and at least one contact method (phone number, website, email, or street address). Phone numbers should resemble common US formats; URLs/emails/addresses acceptable substitutes.\n- The guide should explicitly appear to target Kent County, MI (e.g., mentions of Kent County or Grand Rapids or MI context).\n\nScoring (0-4):\n- 4: Separate PDF resource guide found, clearly categorized with multiple categories and multiple resources per category; entries include organization names and contact info; Kent County targeting is evident.\n- 3: Present but missing 1-2 key categories, or many entries lack contact info, or locality is implied but not explicit.\n- 2: Present but sparsely populated (few categories and few entries) OR poorly structured list with weak category separation.\n- 0-1: No separate resource guide PDF or wrong format.\n\nOnly check presence and structure; do not validate correctness of contact details.", "expectation": "One separate PDF resource guide organized by categories with names and contact info, clearly for Kent County/Grand Rapids, MI."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Verification and Consistency Checks", "description": "Programmatic and targeted LLM checks to verify content presence, bilingual coverage, contact info sufficiency, and cross-document alignment.", "is_required": false, "max_points": 7.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "File Count and Types (PDF presence)", "description": "Verify at least three PDF documents exist (English assessment, Spanish assessment, Resource Guide). Partial credit per PDF found.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    WEIGHT = 1.0\n    outputs = context.get_all_outputs() or []\n    if not outputs:\n        return 0.0, \"No outputs.\"\n    pdfs = []\n    for r in outputs:\n        try:\n            if getattr(r, 'is_document', False):\n                path = context.files.get_path(r.id)\n                if str(path).lower().endswith('.pdf'):\n                    pdfs.append(r)\n        except Exception:\n            continue\n    count = len(pdfs)\n    score = min(count, 3) / 3.0 * WEIGHT\n    return score, f\"Found {count} PDF document(s).\""}, {"type": "code", "name": "English Assessment Content Signals", "description": "Confirm an English needs assessment PDF includes Yes/No columns, screening categories, and the follow-up table column labels.", "weight": 2.0, "code": "import re\n\ndef evaluate(workflow, context):\n    WEIGHT = 2.0\n    outputs = context.get_all_outputs() or []\n    texts = []\n    for r in outputs:\n        try:\n            if getattr(r, 'is_document', False):\n                path = context.files.get_path(r.id)\n                if str(path).lower().endswith('.pdf'):\n                    t = context.files.read_pdf_text(r.id) or ''\n                    texts.append(t)\n        except Exception:\n            continue\n    if not texts:\n        return 0.0, \"No PDF text extracted.\"\n\n    # Identify likely English assessment by keywords\n    en_keywords = [\n        'needs assessment', 'area of need', 'questions', 'yes', 'no',\n        'individual or family need', 'resource given', 'follow-up needed'\n    ]\n    categories = ['income','food','housing','shelter','clothing','education','financial literacy','transportation','employment','legal','pet','healthcare','health care','medical']\n\n    best_score = 0.0\n    best_feedback = ''\n    for t in texts:\n        low = t.lower()\n        # skip if looks Spanish-dominant\n        if sum(1 for w in ['s\u00ed','si','necesidad','salud','empleo','transporte','alimentos','vivienda','seguimiento'] if w in low) > 4:\n            continue\n        has_yes = 'yes' in low\n        has_no = 'no' in low\n        follow_cols = sum(1 for w in ['individual or family need','resource given','follow-up needed'] if w in low)\n        cat_hits = sum(1 for c in categories if c in low)\n        # heuristic score components\n        comp = 0\n        comp += 1 if (has_yes and has_no) else 0\n        comp += (follow_cols/3)\n        comp += min(cat_hits/10.0, 1.0)\n        # average of three components\n        score_ratio = comp / 3.0\n        if score_ratio > best_score:\n            best_score = score_ratio\n            best_feedback = f\"Yes/No: {has_yes and has_no}, follow-up cols found: {follow_cols}/3, category hits: {cat_hits}.\"\n\n    return WEIGHT*best_score, best_feedback if best_feedback else \"No English-like assessment detected.\""}, {"type": "code", "name": "Spanish Assessment Content Signals", "description": "Confirm a Spanish needs assessment PDF includes S\u00ed/No, Spanish category terms, and follow-up table labels (Spanish or English accepted).", "weight": 2.0, "code": "import re\n\ndef evaluate(workflow, context):\n    WEIGHT = 2.0\n    outputs = context.get_all_outputs() or []\n    texts = []\n    for r in outputs:\n        try:\n            if getattr(r, 'is_document', False):\n                path = context.files.get_path(r.id)\n                if str(path).lower().endswith('.pdf'):\n                    t = context.files.read_pdf_text(r.id) or ''\n                    texts.append(t)\n        except Exception:\n            continue\n    if not texts:\n        return 0.0, \"No PDF text extracted.\"\n\n    spanish_terms = ['s\u00ed', 'si', 'no', 'evaluaci\u00f3n', 'necesidades', 'ingresos', 'alimentos', 'comida', 'vivienda', 'refugio', 'ropa', 'educaci\u00f3n', 'alfabetizaci\u00f3n financiera', 'finanzas', 'transporte', 'empleo', 'trabajo', 'asistencia legal', 'servicios legales', 'mascotas', 'servicios para mascotas', 'salud', 'atenci\u00f3n m\u00e9dica', 'm\u00e9dico', 'cl\u00ednica']\n    followup_es = ['necesidad individual o familiar','recurso proporcionado','se necesita seguimiento','seguimiento necesario']\n    followup_en = ['individual or family need','resource given','follow-up needed']\n\n    best_score = 0.0\n    best_feedback = ''\n    for t in texts:\n        low = t.lower()\n        # Spanish-likeness: multiple Spanish terms\n        es_likeness = sum(1 for w in spanish_terms if w in low)\n        if es_likeness < 4:\n            continue\n        has_si = ('s\u00ed' in low) or (re.search(r'\\bsi\\b', low) is not None)\n        has_no = ' no ' in f' {low} '\n        follow_cols = max(sum(1 for w in followup_es if w in low), sum(1 for w in followup_en if w in low))\n        cat_hits = sum(1 for w in ['ingresos','alimentos','comida','vivienda','refugio','ropa','educaci\u00f3n','alfabetizaci\u00f3n financiera','finanzas','transporte','empleo','trabajo','asistencia legal','mascotas','salud','atenci\u00f3n m\u00e9dica','cl\u00ednica','m\u00e9dico'] if w in low)\n        comp = 0\n        comp += 1 if (has_si and has_no) else 0\n        comp += (follow_cols/3)\n        comp += min(cat_hits/10.0, 1.0)\n        score_ratio = comp/3.0\n        if score_ratio > best_score:\n            best_score = score_ratio\n            best_feedback = f\"S\u00ed/No: {has_si and has_no}, follow-up cols found: {follow_cols}/3, Spanish category hits: {cat_hits}.\"\n\n    return WEIGHT*best_score, best_feedback if best_feedback else \"No Spanish assessment detected.\""}, {"type": "code", "name": "Resource Guide: Contact Info Sufficiency and Locality", "description": "Check the resource guide contains multiple contact methods (phones/URLs) and references to Kent County/Grand Rapids, MI.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    WEIGHT = 1.5\n    outputs = context.get_all_outputs() or []\n    texts = []\n    for r in outputs:\n        try:\n            if getattr(r, 'is_document', False):\n                path = context.files.get_path(r.id)\n                if str(path).lower().endswith('.pdf'):\n                    t = context.files.read_pdf_text(r.id) or ''\n                    texts.append(t)\n        except Exception:\n            continue\n    if not texts:\n        return 0.0, \"No PDF text extracted.\"\n\n    # Identify likely guide texts by category words and avoiding strict assessment cues\n    guide_candidates = []\n    cat_words = ['financial assistance','transportation','food','pantry','employment','clothing','healthcare','counseling','legal','pregnancy','support','housing']\n    for t in texts:\n        low = t.lower()\n        cat_hits = sum(1 for w in cat_words if w in low)\n        assesment_cues = sum(1 for w in ['individual or family need','resource given','follow-up needed','yes','no'] if w in low)\n        if cat_hits >= 4 and assesment_cues < 3:\n            guide_candidates.append(low)\n\n    if not guide_candidates:\n        # fallback: pick the longest text\n        guide_candidates = [max(texts, key=lambda x: len(x)).lower()]\n\n    phones = set()\n    urls = set()\n    locality = False\n    phone_re = re.compile(r'(\\(\\d{3}\\)\\s*\\d{3}[\\s.-]?\\d{4}|\\b\\d{3}[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b)')\n    url_re = re.compile(r'(https?://\\S+|www\\.\\S+)', re.IGNORECASE)\n\n    for low in guide_candidates:\n        for m in phone_re.findall(low):\n            phones.add(m)\n        for m in url_re.findall(low):\n            urls.add(m)\n        if any(w in low for w in ['kent county', 'grand rapids', 'mi ', ' mi\\n', ' michigan']):\n            locality = True\n\n    contact_count = len(phones) + len(urls)\n    # Expect at least 8 contacts for full credit\n    base_ratio = min(contact_count/8.0, 1.0)\n    if not locality:\n        base_ratio *= 0.5\n    score = WEIGHT * base_ratio\n    feedback = f\"Phones: {len(phones)}, URLs: {len(urls)}, Locality: {locality}.\"\n    return score, feedback"}, {"type": "code", "name": "Cross-Document Category Alignment", "description": "Check that categories present in the assessments overlap with categories mentioned in the resource guide.", "weight": 0.5, "code": "def evaluate(workflow, context):\n    WEIGHT = 0.5\n    outputs = context.get_all_outputs() or []\n    texts = []\n    for r in outputs:\n        try:\n            if getattr(r, 'is_document', False):\n                path = context.files.get_path(r.id)\n                if str(path).lower().endswith('.pdf'):\n                    t = context.files.read_pdf_text(r.id) or ''\n                    texts.append(t.lower())\n        except Exception:\n            continue\n    if not texts:\n        return 0.0, \"No PDF text extracted.\"\n\n    # Canonical categories\n    canon = ['financial assistance','transportation','food','pantry','employment','clothing','healthcare','counseling','legal services','pregnancy support','housing','shelter','education','financial literacy','pet']\n\n    # Assessments: look for Yes/No and follow-up labels\n    assess_texts = [t for t in texts if ('individual or family need' in t and 'resource given' in t) or ('s\u00ed' in t and 'necesidad' in t)]\n    guide_texts = [t for t in texts if any(w in t for w in ['resource guide','financial assistance','grand rapids','kent county'])]\n    if not guide_texts:\n        guide_texts = texts\n\n    assess_cats = set([c for c in canon if any(c in t for t in assess_texts)])\n    guide_cats = set([c for c in canon if any(c in t for t in guide_texts)])\n    if not assess_cats or not guide_cats:\n        return 0.0, f\"Assessment cats: {len(assess_cats)}, Guide cats: {len(guide_cats)}.\"\n\n    overlap = len(assess_cats & guide_cats)\n    target = min(len(assess_cats), 8)  # normalize against 8 core categories\n    ratio = min(overlap / max(target, 1), 1.0)\n    return WEIGHT * ratio, f\"Overlap: {overlap}, Assess cats: {len(assess_cats)}, Guide cats: {len(guide_cats)}.\""}, {"type": "llm_judge", "name": "Spanish Comprehensibility and Accuracy Spot-Check", "description": "Judge whether the Spanish assessment is coherent and faithful to the English intent (basic check only).", "weight": 1.0, "judge_prompt": "From the submitted files, locate the Spanish needs assessment PDF. Briefly judge if the Spanish is coherent and natural (acceptable for client-facing use), not obviously machine-translated word-for-word, and that the key screening areas and Yes/No prompts are understandable in Spanish. Minor typos are acceptable. You may compare to the English assessment to ensure the same intent is conveyed.\n\nScoring (0-1):\n- 1.0: Spanish reads naturally, conveys the same screening categories and instructions as English, and uses clear Yes/No prompts (S\u00ed/No acceptable).\n- 0.5: Mostly understandable, but awkward phrasing or several literal translations; still usable by staff.\n- 0.0: Spanish not present or largely incomprehensible/incorrect.\n\nReturn a score 0-1 only for this criterion.", "expectation": "A usable, coherent Spanish assessment that mirrors the English intent with clear S\u00ed/No prompts."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3: Presentation and Practical Quality", "description": "Holistic LLM review of professional presentation, usability for frontline staff and clients, and local relevance/value.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Usability", "description": "Assess layout, readability, and practical usability of the assessments and guide for a nonprofit social work setting.", "weight": 2.0, "judge_prompt": "Evaluate overall presentation and usability of the two assessment PDFs and the resource guide PDF:\n- Clarity and readability (plain language, trauma-informed and culturally sensitive tone)\n- Clean, consistent layout with clear headers, space for checkmarks/notes, and printable formatting\n- Tables are easy to use in the field; logical flow; bilingual pairing is obvious and accessible\n\nScoring (0-2):\n- 2: Professional, clear, easy to use/print; logical flow and spacing; staff can administer efficiently.\n- 1: Generally usable but minor layout or clarity issues.\n- 0: Poorly formatted, confusing, or hard to use in practice.", "expectation": "Clean, readable, printable forms with clear headers and space for staff to log responses; practical for field use."}, {"type": "llm_judge", "name": "Strategic Value and Local Completeness of Resource Guide", "description": "Judge whether the resource guide is sufficiently comprehensive and actionable for Kent County, MI needs.", "weight": 1.0, "judge_prompt": "Assess the Resource Guide PDF for strategic value:\n- Coverage of key service categories relevant to low-income and barriered clients in Kent County (e.g., financial assistance, transportation, food pantries, clothing, healthcare, legal services, counseling, employment, pregnancy support)\n- Actionability: entries have enough info (names + contact methods) for quick referrals\n- Local relevance: clearly tailored to Kent County/Grand Rapids, MI\n\nScoring (0-1):\n- 1: Broad coverage across major categories, actionable contact details, and clear Kent County focus.\n- 0.5: Adequate but misses several common categories or has thin details.\n- 0: Sparse, generic, or not clearly local.", "expectation": "A practical, locally relevant guide with multiple entries per major category and actionable contact info."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "9a0d8d36-6233-4c76-9107-0d1f783c7340", "rubric": {"category_name": "Finance & Insurance \u2014 Personal Financial Advisors: ISO vs NSO Tax Implications Presentation", "rationale": "This is a Mixed task (Pattern C): a client-facing slide document that must include embedded analytical content and step-by-step calculations. Stage 1 uses an LLM gate to strictly enforce a verifiable, presentation-specific structure. Stage 2 mixes code rules (lightweight, robust text parsing for key terms and numbers when the file is PDF/DOCX) with an LLM judge for calculation logic and tax treatment consistency. Stage 3 uses LLM judges to assess clarity, professionalism, and client appropriateness for an executive HNW audience. To keep verification practical, Stage 1 strongly prefers a PDF export of the slide deck (PPTX allowed), and mandates explicit calculation tables/sections so LLMs can verify structure and content presence.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Slide Deck Shape Gate", "description": "Gate: Verify that the deliverable is a short presentation (PDF preferred or PPTX allowed) with all required sections enabling verification of assumptions, step-by-step calculations, and comparison between ISOs and NSOs.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 2.8, "rules": [{"type": "llm_judge", "name": "Structured Presentation Format Requirement (Gate)", "description": "Check that the output is a slide deck (PDF preferred or PPTX allowed) with the exact structural elements needed for verification.", "weight": 4.0, "judge_prompt": "You are evaluating whether the candidate output is a properly structured slide presentation that enables verification of ISO vs NSO tax treatment and calculations.\n\nACCEPTABLE FORMATS:\n- Preferred: PDF export of the slide deck\n- Allowed: PPTX (PowerPoint)\n\nREJECT if:\n- Not PDF or PPTX\n- Fewer than 6 slides\n\nREQUIRED STRUCTURE (be flexible about exact slide titles, but these elements must be clearly present):\n1) Title slide:\n   - Mentions the topic (ISO vs NSO) and the institution (CrawBank) or advisor context.\n2) Overview / Executive Summary slide:\n   - High-level differences and key takeaway for an executive HNW client.\n3) Definitions / \u201cWhat are ISOs vs NSOs\u201d slide(s):\n   - Explains incentive stock options and non-qualified stock options and their high-level differences.\n4) Assumptions slide (must show a table):\n   - Hypothetical inputs shown in a table for both ISO and NSO scenarios, including at minimum:\n     \u2022 Number of options/shares\n     \u2022 Exercise/Strike Price\n     \u2022 FMV at Exercise\n     \u2022 Ordinary income tax rate (federal)\n     \u2022 Capital gains rate assumption\n     \u2022 AMT note/rate for ISO\n     \u2022 Payroll tax treatment for NSO (e.g., FICA/Medicare) \u2014 can be noted textually\n5) ISO Exercise Walkthrough slide(s):\n   - Step-by-step calculations for ISO including:\n     \u2022 Bargain element\n     \u2022 AMT adjustment at exercise\n     \u2022 Net cash at exercise\n     \u2022 Sale tax outcomes for qualifying vs disqualifying dispositions (timing noted)\n6) NSO Exercise Walkthrough slide(s):\n   - Step-by-step calculations for NSO including:\n     \u2022 Ordinary income at exercise\n     \u2022 Withholding/payroll taxes at exercise\n     \u2022 Net cash at exercise\n     \u2022 Subsequent capital gains at sale\n7) Comparative Summary slide (must show a side-by-side table):\n   - Side-by-side ISO vs NSO comparison including net proceeds (at exercise and after sale), and major tax components.\n8) Tax Treatment Summary slide:\n   - Clear bullets on the different tax implications and holding period requirements (qualifying vs disqualifying disposition for ISOs).\n9) Next Steps / Recommendations slide:\n   - Educational guidance tailored to the client (options vest in ~1 year), caveats/disclaimers to consult a tax professional.\n\nPRESENTATION QUALITY REQUIREMENTS:\n- Professional slide formatting\n- Clear section headers\n- Tables for Assumptions and Comparative Summary are visible and labeled\n\nSCORING (map to the rule weight):\n- 1.00: All required slide types present, with Assumptions and Comparative Summary as tables; professional formatting.\n- 0.80: Missing exactly one required slide type OR one required table is presented but not clearly labeled.\n- 0.50: Missing two required slide types OR tables are not clearly visible.\n- 0.00: Wrong format (not PDF/PPTX), or fewer than 6 slides, or missing three or more core elements.\n\nOnly assess structure, presence, and format\u2014not correctness of calculations.", "expectation": "A professionally formatted PDF (or PPTX) with clearly labeled slides for Title, Overview, Definitions, Assumptions (table), ISO steps, NSO steps, Comparative Summary (table), Tax Treatment, and Next Steps."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification: Calculation Logic and Tax Treatment", "description": "Verify technical correctness and completeness of the analytical content given the mandated structure. Code rules perform text-based checks on PDFs/DOCX; an LLM judge evaluates the logical consistency of the calculations and tax timing/treatment.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Key Tax Terms Present", "description": "Detect presence of essential tax terms and concepts indicating correct coverage: ISO vs NSO, AMT, withholding/payroll taxes, dispositions/holding periods, ordinary income vs capital gains.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output file.\"\n\n    text = \"\"\n    try:\n        if output.is_document:\n            # Try PDF first, then DOCX\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    text = \"\"\n        elif output.is_text_format:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n        # If PPTX or other non-parsable format, gracefully return 0 with feedback\n    except Exception:\n        text = \"\"\n\n    if not text:\n        return 0.0, \"Could not parse text (likely PPTX or unextractable). LLM rules will still assess logic.\"\n\n    lt = text.lower()\n\n    groups = []\n    groups.append(any(k in lt for k in [\"incentive stock option\", \"iso \", \" iso\", \"(iso)\"]))\n    groups.append(any(k in lt for k in [\"non-qualified stock option\", \"nonqualified stock option\", \"nso \", \" nso\", \"(nso)\"]))\n    groups.append(any(k in lt for k in [\"amt\", \"alternative minimum tax\"]))\n    groups.append(any(k in lt for k in [\"withholding\", \"withheld\", \"payroll tax\", \"fica\", \"medicare\"]))\n    groups.append(any(k in lt for k in [\"qualifying disposition\", \"disqualifying disposition\", \"holding period\"]))\n    groups.append(any(k in lt for k in [\"ordinary income\", \"capital gain\", \"capital gains\"]))\n\n    matched = sum(1 for g in groups if g)\n    score = (matched / len(groups)) * 0.8\n    feedback = f\"Matched {matched}/6 key term groups.\"\n    return score, feedback"}, {"type": "code", "name": "Assumptions and Numbers Detected", "description": "Confirm the presence of numeric assumptions: currency amounts, tax rates, and share counts/option units; also references to strike/exercise price.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output file.\"\n\n    text = \"\"\n    try:\n        if output.is_document:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    text = \"\"\n        elif output.is_text_format:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    except Exception:\n        text = \"\"\n\n    if not text:\n        return 0.0, \"Could not parse text for numeric checks (likely PPTX).\"\n\n    lt = text.lower()\n\n    # Find percentages (e.g., 22%, 37.5%)\n    percents = re.findall(r\"\\b\\d{1,2}(?:\\.\\d+)?\\s?%\", lt)\n    # Find currency amounts (e.g., $100,000.00)\n    currency = re.findall(r\"\\$\\s?\\d{1,3}(?:,\\d{3})*(?:\\.\\d{2})?\", text)\n\n    has_shares = (\"share\" in lt or \"shares\" in lt or \"option\" in lt or \"options\" in lt)\n    has_price_terms = (\"strike\" in lt or \"exercise price\" in lt or \"exercise\" in lt)\n\n    # Sub-scores: 4 checks each worth 0.25 of raw; scaled to weight 0.6\n    raw = 0.0\n    raw += 0.25 if len(percents) >= 2 else 0.0\n    raw += 0.25 if len(currency) >= 3 else 0.0\n    raw += 0.25 if has_shares else 0.0\n    raw += 0.25 if has_price_terms else 0.0\n\n    score = raw * 0.6\n    feedback = f\"Detected {len(percents)} percent(s), {len(currency)} currency amount(s); shares term: {has_shares}; price term: {has_price_terms}.\"\n    return score, feedback"}, {"type": "llm_judge", "name": "Calculation Consistency and Tax Logic", "description": "Evaluate whether the step-by-step calculations are logically consistent with the assumptions and correctly distinguish ISO vs NSO tax timing and treatment.", "weight": 2.2, "judge_prompt": "Evaluate the technical correctness and consistency of the step-by-step calculations and tax logic in the presentation.\n\nCriteria:\n1) Assumptions usage: Do the numerical examples (shares, strike, FMV, tax rates) flow into the ISO and NSO walkthroughs?\n2) ISO logic: Correctly identifies that regular tax is not due at exercise, but an AMT adjustment (bargain element) may apply; distinguishes qualifying vs disqualifying disposition at sale, with holding period noted and appropriate capital gains/ordinary treatment.\n3) NSO logic: Correctly identifies ordinary income at exercise on the bargain element, subject to withholding/payroll taxes; subsequent gains at sale treated as capital gains based on holding period from exercise.\n4) Net proceeds clarity: Shows net cash at exercise and after-sale outcomes for both ISO and NSO; timing differences are explicit.\n\nScoring:\n- 1.0 to 1.0 (normalized, scaled to weight): Perfectly consistent with correct U.S. tax treatment, assumptions clearly flow through, timing distinctions are explicit, proceeds tables match logic.\n- ~0.7: Mostly correct with minor omissions (e.g., mentions AMT but lacks a brief holding period note, or withholds taxes for NSO but omits FICA label) and still reasonably consistent.\n- ~0.4: Partially correct but with notable gaps or confusion in timing/treatment (e.g., unclear proceeds sequencing, missing disposition discussion for ISOs).\n- 0.0: Materially incorrect logic (e.g., states ISOs are taxed as ordinary income at exercise under regular tax) or no usable calculation steps.\n\nJudge for correctness and consistency; do not assess slide aesthetics here.", "expectation": "A logically consistent walkthrough: ISO shows AMT adjustment and sale-based taxation with proper holding period distinctions; NSO shows ordinary income and payroll withholding at exercise; both show net proceeds at exercise and after sale."}, {"type": "code", "name": "Net Proceeds Comparison Mention", "description": "Verify that the document mentions a net proceeds comparison and indicates a side-by-side or versus framing between ISO and NSO.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output file.\"\n\n    text = \"\"\n    try:\n        if output.is_document:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    text = \"\"\n        elif output.is_text_format:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    except Exception:\n        text = \"\"\n\n    if not text:\n        return 0.0, \"Could not parse text to confirm net proceeds comparison (likely PPTX).\"\n\n    lt = text.lower()\n\n    has_net = (\"net proceed\" in lt)\n    has_iso = (\" iso\" in lt or \"iso \" in lt or \"incentive stock option\" in lt)\n    has_nso = (\" nso\" in lt or \"nso \" in lt or \"non-qualified\" in lt or \"nonqualified\" in lt)\n    has_compare = (\" vs \" in lt or \"versus\" in lt or \"comparison\" in lt or \"side-by-side\" in lt or \"side by side\" in lt)\n\n    parts = [has_net, (has_iso and has_nso), has_compare]\n    score = (sum(1 for p in parts if p) / 3.0) * 0.4\n    fb = f\"net:{has_net}, iso+nso:{has_iso and has_nso}, compare:{has_compare}\"\n    return score, fb"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Client Fit", "description": "Holistic assessment of clarity, professionalism, and appropriateness for an executive high-net-worth client in an in-person meeting context.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity, Visual Design, and Readability", "description": "Assess whether the slides are clear, concise, and visually organized for rapid executive comprehension; check that tables/charts are legible and layouts guide the client to key insights.", "weight": 1.0, "judge_prompt": "Assess the presentation\u2019s clarity and visual quality for an executive client meeting:\n- Are headings concise and informative?\n- Are tables/charts legible and properly labeled (especially Assumptions and Comparative Summary)?\n- Is information density appropriate (not overcrowded; key points stand out)?\n- Is the flow logical from overview \u2192 details \u2192 comparison \u2192 next steps?\n\nScoring:\n- 1.0: Professional, polished, easy to scan; tables/charts are clear; flow is intuitive.\n- 0.6: Generally clear with minor layout or readability issues.\n- 0.3: Noticeable clutter or unclear flow; tables hard to read.\n- 0.0: Poor readability, confusing structure, or unprofessional appearance.", "expectation": "A clean, executive-ready deck with clear headings, legible tables, intuitive flow, and focused messaging."}, {"type": "llm_judge", "name": "Actionability and Client Appropriateness", "description": "Evaluate whether the content is tailored for an executive HNW client, includes relevant caveats, and offers concrete next steps given the one-year vesting timeline.", "weight": 1.0, "judge_prompt": "Evaluate the deck\u2019s actionability and client fit:\n- Tailors content to an executive HNW client (concise, decision-oriented tone; highlights risks like AMT exposure and concentration risk).\n- Notes that options are not yet vested (\u22481 year) and frames the content as educational.\n- Includes reasonable caveats/disclaimers (e.g., consult tax advisor/CPA; actual outcomes depend on personal tax situation).\n- Provides clear next steps for the client (e.g., tracking vest dates/holding periods, tax projection for AMT, cash needs for exercise, diversification plan).\n\nScoring:\n- 1.0: Strongly tailored with concrete next steps and appropriate caveats.\n- 0.6: Mostly tailored; some next steps or caveats are brief or generic.\n- 0.3: Weak tailoring; minimal next steps; missing key caveats.\n- 0.0: Not tailored; no next steps; no caveats.", "expectation": "Client-centered, action-oriented recommendations acknowledging one-year vesting, with appropriate disclaimers and next steps."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "7de33b48-5163-4f50-b5f3-8deea8185e57", "rubric": {"category_name": "WCAG AA ScreenReader Status Utility (React + TypeScript)", "rationale": "This rubric forces a self-documenting, verifiable artifact: a single distributable .zip containing a React+TypeScript utility, tests, CSS, package.json, and README. Stage 1 (LLM-only) enforces the exact file/package shape required to make automated verification tractable. Stage 2 uses code rules to deterministically validate contents: archive structure, TypeScript/JSX patterns (role=\"status\", visible prop, aria-hidden), presence of queueing logic hints, CSS visually-hidden rules, ARIA22-oriented tests (React Testing Library + Sinon), package scripts/deps, and README instructions. Stage 3 judges professional quality and accessibility appropriateness.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 Gate \u2014 Archive and File Structure (LLM-only)", "description": "Enforce exact output shape: a single .zip containing the specified files with high-level, checkable structure. Do NOT judge correctness of logic or test pass/fail here.", "is_required": true, "max_points": 2.5, "min_score_to_pass": 1.5, "rules": [{"type": "llm_judge", "name": "ZIP Package Shape and Required Files Present", "description": "Check that the primary output is a single .zip archive containing the exact required files at the top level (or within a single root folder), and that each file has the expected high-level structure so that automated checks are possible in Stage 2.", "weight": 2.5, "judge_prompt": "You are validating the STRUCTURE ONLY. Do not assess quality or whether tests pass. Inspect the candidate output and verify the following:\n\nFormat and Packaging\n- Primary output must be a .zip archive. If the agent instead provided the files unzipped as separate top-level files, you may accept it, but prefer a single .zip.\n\nRequired Files (present exactly once each, either at zip root or inside a single root folder):\n1) ScreenReaderStatusMessage.tsx \u2014 a React+TypeScript component/utility file\n   - Should appear to export a component or utility named ScreenReaderStatusMessage (default or named export acceptable)\n   - Mentions role=\"status\" or live region usage somewhere\n   - Mentions a visible prop or equivalent for the special case described\n2) ScreenReaderStatusMessage.test.tsx \u2014 tests using React Testing Library and Sinon\n   - Should include imports from '@testing-library/react' and 'sinon' (or equivalents)\n   - Should contain tests covering the 3 ARIA22 checks plus the visible prop functionality\n3) ScreenReaderStatusMessage.css \u2014 CSS for visually hiding the status region\n   - Should include a class like .visually-hidden or .sr-only with typical offscreen rules\n4) package.json \u2014 NPM manifest\n   - Should include a test script (e.g., \"test\": \"...\"), and dependencies/devDependencies suitable for React+TS and Testing Library + Sinon\n5) README.md \u2014 usage and test instructions\n   - Should describe how to install and run tests\n\nScoring (STRUCTURE ONLY)\n- 2.5: Valid .zip (or clearly equivalent set of files) AND all 5 required files present with the expected high-level content markers listed above.\n- 1.8: Valid .zip (or equivalent) with 4/5 required files present OR all files present but one file clearly missing its expected high-level markers.\n- 1.0: Valid .zip (or equivalent) with 3/5 required files present.\n- 0.0: Not a .zip (nor equivalent separate files), OR fewer than 3 required files present, OR clearly wrong artifact type.\n\nOnly check PRESENCE and HIGH-LEVEL STRUCTURE. Do NOT judge implementation correctness, accessibility depth, or whether tests pass.", "expectation": "A single .zip containing the 5 required files with plausible content structures that enable automated verification."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Automated Correctness and Content Verification (Code + Heuristics)", "description": "Deterministic checks on archive contents: file presence, TSX structure (role=\"status\", visible prop, aria-hidden), queueing hints, CSS visually-hidden rules, ARIA22-oriented tests coverage, package.json scripts/deps, and README instructions.", "is_required": false, "max_points": 6.5, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Archive Contains Required Files", "description": "Verify the .zip (or alternative provided files) includes the 5 required files by exact filename. Partial credit by count present.", "weight": 1.2, "code": "def evaluate(workflow, context):\n    import os, zipfile\n    from pathlib import Path\n\n    required = {\n        'ScreenReaderStatusMessage.tsx',\n        'ScreenReaderStatusMessage.test.tsx',\n        'ScreenReaderStatusMessage.css',\n        'package.json',\n        'README.md',\n    }\n\n    def list_zip_basenames(zip_path: Path):\n        try:\n            with zipfile.ZipFile(zip_path, 'r') as z:\n                return {Path(n).name for n in z.namelist() if not n.endswith('/')}\n        except Exception:\n            return set()\n\n    def list_all_output_basenames():\n        names = set()\n        for r in context.get_all_outputs():\n            try:\n                p = context.files.get_path(r.id)\n                names.add(p.name)\n            except Exception:\n                continue\n        return names\n\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n\n    present = set()\n    try:\n        p = context.files.get_path(output.id)\n        if str(p).lower().endswith('.zip'):\n            present = list_zip_basenames(p)\n        else:\n            present = list_all_output_basenames()\n    except Exception:\n        present = list_all_output_basenames()\n\n    found = len(required.intersection(present))\n    score = (found / len(required)) * 1.2\n    return max(0.0, min(1.2, score))"}, {"type": "code", "name": "TSX Component Structure (role=\"status\", visible prop, message types)", "description": "Parse ScreenReaderStatusMessage.tsx to verify React+TS patterns: export, role=\"status\", visible prop handling, message types (string or ReactNode/JSX.Element), and visually-hidden class usage.", "weight": 1.2, "code": "def evaluate(workflow, context):\n    import os, re, zipfile\n    from pathlib import Path\n\n    TSX_NAME = 'ScreenReaderStatusMessage.tsx'\n\n    def read_text_from_outputs(filename):\n        # Try primary zip\n        output = context.get_primary_output()\n        if output:\n            try:\n                p = context.files.get_path(output.id)\n                if str(p).lower().endswith('.zip'):\n                    with zipfile.ZipFile(p, 'r') as z:\n                        # match by basename\n                        for n in z.namelist():\n                            if Path(n).name == filename:\n                                return z.read(n).decode('utf-8', errors='ignore')\n            except Exception:\n                pass\n        # Fallback: scan other outputs\n        for r in context.get_all_outputs():\n            try:\n                rp = context.files.get_path(r.id)\n                if rp.name == filename:\n                    return rp.read_text(encoding='utf-8', errors='ignore')\n            except Exception:\n                continue\n        return ''\n\n    text = read_text_from_outputs(TSX_NAME)\n    if not text:\n        return 0.0, f\"Missing {TSX_NAME}\"\n\n    checks = 0\n    total = 6\n\n    # React/TSX import\n    if re.search(r\"from\\\\s+['\\\"]react['\\\"]\", text):\n        checks += 1\n\n    # Exported component/utility\n    if re.search(r\"export\\\\s+(default\\\\s+)?(function|const|class)\\\\s+ScreenReaderStatusMessage\", text) or \\\n       re.search(r\"export\\\\s+\\{\\\\s*ScreenReaderStatusMessage\\\\s*\\}\", text):\n        checks += 1\n\n    # role=\"status\" or role={\"status\"}\n    if re.search(r\"role\\\\s*=\\\\s*['\\\"]status['\\\"]\", text) or re.search(r\"role\\\\s*=\\\\s*\\{['\\\"]status['\\\"]\\}\", text):\n        checks += 1\n\n    # visible prop presence\n    if re.search(r\"\\bvisible\\b\\s*[:?]\", text) or re.search(r\"props\\\\.visible\", text) or re.search(r\"\\bvisible\\b\", text):\n        checks += 1\n\n    # message accepts string or ReactNode/JSX.Element\n    if re.search(r\":\\\\s*(React\\\\.ReactNode|ReactNode|JSX\\\\.Element|ReactElement)\\\\b\", text) or re.search(r\"\\bstring\\b\", text):\n        checks += 1\n\n    # visually-hidden class usage (in TSX markup)\n    if re.search(r\"class(Name)?\\\\s*=\\\\s*['\\\"][^'\\\"]*(visually-hidden|sr-only|screen-reader-only)[^'\\\"]*['\\\"]\", text):\n        checks += 1\n\n    score = (checks/total) * 1.2\n    return max(0.0, min(1.2, score))"}, {"type": "code", "name": "CSS Visually Hidden Rules Present", "description": "Verify ScreenReaderStatusMessage.css contains a visually hidden class (.visually-hidden or .sr-only) with typical offscreen properties.", "weight": 0.6, "code": "def evaluate(workflow, context):\n    import re, zipfile\n    from pathlib import Path\n\n    CSS_NAME = 'ScreenReaderStatusMessage.css'\n\n    def read_text(filename):\n        output = context.get_primary_output()\n        if output:\n            try:\n                p = context.files.get_path(output.id)\n                if str(p).lower().endswith('.zip'):\n                    with zipfile.ZipFile(p, 'r') as z:\n                        for n in z.namelist():\n                            if Path(n).name == filename:\n                                return z.read(n).decode('utf-8', errors='ignore')\n            except Exception:\n                pass\n        for r in context.get_all_outputs():\n            try:\n                rp = context.files.get_path(r.id)\n                if rp.name == filename:\n                    return rp.read_text(encoding='utf-8', errors='ignore')\n            except Exception:\n                continue\n        return ''\n\n    css = read_text(CSS_NAME)\n    if not css:\n        return 0.0\n\n    has_class = bool(re.search(r\"\\.(visually-hidden|sr-only)\\b\", css))\n    props = 0\n    # count typical properties\n    if re.search(r\"position\\s*:\\s*(absolute|fixed)\", css):\n        props += 1\n    if re.search(r\"(width|height)\\s*:\\s*1(px)?\", css) and re.search(r\"height\\s*:\\s*1(px)?\", css):\n        props += 1\n    if re.search(r\"overflow\\s*:\\s*hidden\", css):\n        props += 1\n    if re.search(r\"clip(-path)?\\s*:\\s*(rect|inset)\", css):\n        props += 1\n    if re.search(r\"margin\\s*:\\s*-?1(px)?\", css) or re.search(r\"border\\s*:\\s*0\", css):\n        props += 1\n\n    base = 0.3 if has_class else 0.0\n    score = base + min(props, 3) * 0.1  # up to +0.3\n    return max(0.0, min(0.6, score))"}, {"type": "code", "name": "Tests Cover ARIA22 Checks + Visible Prop", "description": "Verify ScreenReaderStatusMessage.test.tsx imports React Testing Library and Sinon and includes tests targeting ARIA22 items 1\u20133 plus the visible prop functionality.", "weight": 1.2, "code": "def evaluate(workflow, context):\n    import re, zipfile\n    from pathlib import Path\n\n    TEST_NAME = 'ScreenReaderStatusMessage.test.tsx'\n\n    def read_text(filename):\n        output = context.get_primary_output()\n        if output:\n            try:\n                p = context.files.get_path(output.id)\n                if str(p).lower().endswith('.zip'):\n                    with zipfile.ZipFile(p, 'r') as z:\n                        for n in z.namelist():\n                            if Path(n).name == filename:\n                                return z.read(n).decode('utf-8', errors='ignore')\n            except Exception:\n                pass\n        for r in context.get_all_outputs():\n            try:\n                rp = context.files.get_path(r.id)\n                if rp.name == filename:\n                    return rp.read_text(encoding='utf-8', errors='ignore')\n            except Exception:\n                continue\n        return ''\n\n    t = read_text(TEST_NAME)\n    if not t:\n        return 0.0\n\n    checks = 0\n    total = 6\n\n    # Imports\n    if re.search(r\"from\\\\s+['\\\"]@testing-library/react['\\\"]\", t):\n        checks += 1\n    if re.search(r\"from\\\\s+['\\\"]sinon['\\\"]\", t) or re.search(r\"require\\\\(\\s*['\\\"]sinon['\\\"]\\s*\\)\", t):\n        checks += 1\n\n    # Test content hints\n    # 1) container has role=status before message\n    if re.search(r\"getByRole\\(.*status.*\\)|queryByRole\\(.*status.*\\)\", t):\n        checks += 1\n    # 2) when message triggered, it is inside the container\n    if re.search(r\"(getByText|findByText|queryByText)\\(.*\\)\\s*\\)\\s*;?\", t) and re.search(r\"within|container|closest\", t):\n        checks += 1\n    # 3) equivalent info elements (e.g., image alt text) also reside in the container\n    if re.search(r\"getByAltText|alt=\", t):\n        checks += 1\n    # 4) visible prop functionality\n    if re.search(r\"visible\", t) and (re.search(r\"aria-hidden\", t) or re.search(r\"getByText|queryByText\", t)):\n        checks += 1\n\n    score = (checks/total) * 1.2\n    return max(0.0, min(1.2, score))"}, {"type": "code", "name": "package.json Scripts and Dependencies", "description": "Parse package.json to ensure a test script is present and key deps/devDeps exist for React+TS, Testing Library, and Sinon (Jest or Vitest acceptable).", "weight": 0.6, "code": "def evaluate(workflow, context):\n    import json, zipfile\n    from pathlib import Path\n\n    PKG = 'package.json'\n\n    def read_text(filename):\n        output = context.get_primary_output()\n        if output:\n            try:\n                p = context.files.get_path(output.id)\n                if str(p).lower().endswith('.zip'):\n                    with zipfile.ZipFile(p, 'r') as z:\n                        for n in z.namelist():\n                            if Path(n).name == filename:\n                                return z.read(n).decode('utf-8', errors='ignore')\n            except Exception:\n                pass\n        for r in context.get_all_outputs():\n            try:\n                rp = context.files.get_path(r.id)\n                if rp.name == filename:\n                    return rp.read_text(encoding='utf-8', errors='ignore')\n            except Exception:\n                continue\n        return ''\n\n    txt = read_text(PKG)\n    if not txt:\n        return 0.0\n    try:\n        data = json.loads(txt)\n    except Exception:\n        return 0.0\n\n    score = 0.0\n\n    scripts = data.get('scripts', {})\n    if any(k == 'test' for k in scripts.keys()):\n        score += 0.2\n\n    deps = {}\n    for k in ('dependencies','devDependencies','peerDependencies'):\n        if isinstance(data.get(k), dict):\n            deps.update(data[k])\n\n    def has_any(keys):\n        return any(k in deps for k in keys)\n\n    if has_any(['react','react-dom']):\n        score += 0.1\n    if has_any(['typescript']):\n        score += 0.1\n    if has_any(['@testing-library/react']):\n        score += 0.1\n    if has_any(['@testing-library/jest-dom']):\n        score += 0.05\n    if has_any(['sinon']):\n        score += 0.1\n    if has_any(['jest','ts-jest','babel-jest','vitest']):\n        score += 0.15\n\n    return max(0.0, min(0.6, score))"}, {"type": "code", "name": "README: Install and Test Instructions", "description": "README.md contains clear instructions to install dependencies and run tests, and mentions the utility name.", "weight": 0.4, "code": "def evaluate(workflow, context):\n    import re, zipfile\n    from pathlib import Path\n\n    NAME = 'README.md'\n\n    def read_text(filename):\n        output = context.get_primary_output()\n        if output:\n            try:\n                p = context.files.get_path(output.id)\n                if str(p).lower().endswith('.zip'):\n                    with zipfile.ZipFile(p, 'r') as z:\n                        for n in z.namelist():\n                            if Path(n).name == filename:\n                                return z.read(n).decode('utf-8', errors='ignore')\n            except Exception:\n                pass\n        for r in context.get_all_outputs():\n            try:\n                rp = context.files.get_path(r.id)\n                if rp.name == filename:\n                    return rp.read_text(encoding='utf-8', errors='ignore')\n            except Exception:\n                continue\n        return ''\n\n    t = read_text(NAME)\n    if not t:\n        return 0.0\n\n    score = 0.0\n    if re.search(r\"npm\\s+install|yarn\\s+install|pnpm\\s+install\", t, re.I):\n        score += 0.15\n    if re.search(r\"npm\\s+test|yarn\\s+test|pnpm\\s+test\", t, re.I):\n        score += 0.15\n    if re.search(r\"ScreenReaderStatusMessage\", t):\n        score += 0.1\n\n    return max(0.0, min(0.4, score))"}, {"type": "code", "name": "Visible Prop Twin Rendering Semantics", "description": "In TSX, when visible prop is used, a visible sibling is rendered with aria-hidden or equivalent to avoid duplication in the accessibility tree.", "weight": 0.8, "code": "def evaluate(workflow, context):\n    import re, zipfile\n    from pathlib import Path\n\n    TSX_NAME = 'ScreenReaderStatusMessage.tsx'\n\n    def read_text(filename):\n        output = context.get_primary_output()\n        if output:\n            try:\n                p = context.files.get_path(output.id)\n                if str(p).lower().endswith('.zip'):\n                    with zipfile.ZipFile(p, 'r') as z:\n                        for n in z.namelist():\n                            if Path(n).name == filename:\n                                return z.read(n).decode('utf-8', errors='ignore')\n            except Exception:\n                pass\n        for r in context.get_all_outputs():\n            try:\n                rp = context.files.get_path(r.id)\n                if rp.name == filename:\n                    return rp.read_text(encoding='utf-8', errors='ignore')\n            except Exception:\n                continue\n        return ''\n\n    t = read_text(TSX_NAME)\n    if not t:\n        return 0.0\n\n    score = 0.0\n    # Evidence of conditional visible rendering and aria-hidden on the visible twin\n    has_visible = bool(re.search(r\"\\bvisible\\b\", t))\n    has_aria_hidden = bool(re.search(r\"aria-hidden\\\\s*=\\\\s*(\\{\\s*true\\s*\\}|['\\\"]true['\\\"])\", t))\n    # sibling or twin rendering hint (two elements, or conditional rendering fragments)\n    has_sibling_hint = bool(re.search(r\"<>|</>|\n|\\{\\s*visible\\s*\\?\\s*\", t))\n\n    if has_visible and has_aria_hidden:\n        score += 0.6\n    if has_sibling_hint:\n        score += 0.2\n\n    return max(0.0, min(0.8, score))"}, {"type": "code", "name": "Queueing or Re-Announce Mechanism Present", "description": "Heuristic check that the utility attempts to queue or force re-announcement (e.g., queue array, setTimeout, alternating live regions, toggling keys).", "weight": 0.5, "code": "def evaluate(workflow, context):\n    import re, zipfile\n    from pathlib import Path\n\n    TSX_NAME = 'ScreenReaderStatusMessage.tsx'\n\n    def read_text(filename):\n        output = context.get_primary_output()\n        if output:\n            try:\n                p = context.files.get_path(output.id)\n                if str(p).lower().endswith('.zip'):\n                    with zipfile.ZipFile(p, 'r') as z:\n                        for n in z.namelist():\n                            if Path(n).name == filename:\n                                return z.read(n).decode('utf-8', errors='ignore')\n            except Exception:\n                pass\n        for r in context.get_all_outputs():\n            try:\n                rp = context.files.get_path(r.id)\n                if rp.name == filename:\n                    return rp.read_text(encoding='utf-8', errors='ignore')\n            except Exception:\n                continue\n        return ''\n\n    t = read_text(TSX_NAME)\n    if not t:\n        return 0.0\n\n    # Look for queueing or re-announce hints\n    patterns = [\n        r\"\\bqueue\\b\", r\"\\bmessages?\\b\", r\"setTimeout\\(\", r\"requestAnimationFrame\\(\", r\"useEffect\\(\",\n        r\"key=\\{\\w+\\}\", r\"aria-live\", r\"aria-atomic\", r\"useRef\\(\", r\"setState\\(\", r\"useState\\(\\[\", r\"useState\\(.*queue\"\n    ]\n    hits = sum(1 for pat in patterns if re.search(pat, t))\n\n    if hits >= 3:\n        return 0.5\n    elif hits == 2:\n        return 0.35\n    elif hits == 1:\n        return 0.2\n    else:\n        return 0.0"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Accessibility Appropriateness (LLM)", "description": "Holistic assessment of code clarity, TypeScript typing, React patterns, accessibility semantics, and documentation quality. Not about pass/fail of tests; judge overall professional quality.", "is_required": false, "max_points": 1.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Overall Code/Docs Quality and A11y Soundness", "description": "Evaluate clarity, maintainability, and appropriateness for enterprise use: TypeScript types, component API design, accessibility semantics (role=status, duplication avoidance via aria-hidden), visible prop design, and README clarity.", "weight": 1.0, "judge_prompt": "Holistically evaluate the submission for professional quality and accessibility soundness. Consider:\n- Code clarity and maintainability: TypeScript typing for props (message supports string or ReactNode/JSX.Element), sensible defaults, naming, comments.\n- Accessibility correctness: Using role=\"status\" (or equivalent live region), avoiding duplicate announcements via aria-hidden on visible twin, reasonable approach to queuing/re-announcements for multiple concurrent updates.\n- React/TS best practices: hooks usage, side-effect management, minimal re-renders, testable design.\n- Test readability/coverage: Are tests meaningful, aligned to ARIA22 checks 1\u20133 and visible prop behavior?\n- README quality: Clear usage, install and test steps, brief rationale of the approach.\n\nScoring guidance:\n- 1.0: Professional, clear, and appropriate for enterprise use; design demonstrates strong accessibility understanding.\n- 0.7: Solid with minor issues or omissions.\n- 0.4: Basic but serviceable; noticeable gaps in clarity or a11y reasoning.\n- 0.0: Poor quality or significantly misguided from an accessibility standpoint.\n\nDo not re-check structure already validated; focus on qualitative aspects.", "expectation": "A clean, well-typed TSX component with thoughtful a11y semantics, meaningful tests, and a clear README suitable for enterprise teams."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "d3d255b2-f5f2-4841-9f62-2083ec9ef3da", "rubric": {"category_name": "Real Estate Seller Advisory Report (Offer Review and Strategy)", "rationale": "This rubric enforces a self-documenting, verifiable PDF/DOCX report for a real estate seller advisory task. Stage 1 (LLM-only) mandates an exact document structure that makes verification trivial. Stage 2 mixes code rules (text extraction from PDF/DOCX) and LLM judgment to check factual inclusion (offer terms), numeric coherence (counteroffer within a reasonable band), and alignment to stated market context. Stage 3 evaluates professional quality and client-facing clarity. The structure ensures the agent proves correctness via clearly labeled sections and explicit data references.", "max_total_score": 20.0, "stages": [{"name": "Stage 1: Structured Report Gate (FORMAT + SECTIONS)", "description": "LLM-only gate enforcing exact document shape so later verification is possible.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Document Format and Structural Completeness", "description": "Check if output is a 2\u20133 page PDF/DOCX with all required sections and explicit, verifiable elements.", "weight": 8.0, "judge_prompt": "You are verifying ONLY the structure/format (not content quality) of the candidate output.\n\nFormat Requirements:\n- Must be a PDF or DOCX (not plain text, not Excel)\n- Approximately 2\u20133 pages\n- Professionally formatted with clear section headers\n\nRequired Sections (flexible on exact names, but headers must be visible):\n1) Executive Summary (or Overview)\n2) Offer Summary (or Offer Details) \u2014 must present: price $500,000, cash, as-is, no contingencies, 30-day close\n3) Market Analysis Summary (reference to an attached/used market analysis/CMA) \u2014 should indicate at least 3 comps in a table or bulleted list (addresses/IDs and sale/list prices visible)\n4) Pricing Guidance (positioning vs. market, noting property is slightly overpriced and has minor repairs)\n5) Negotiation Strategy (tactics/options)\n6) Recommended Counteroffer (a specific numeric price)\n7) Risks and Next Steps\n8) Appendix (or Supporting Data) \u2014 includes a small table or bullet list of the comps referenced (at least 3)\n\nScoring (structure only):\n- 1.0: Valid PDF/DOCX, 2\u20133 pages, and all 8 sections present with clearly labeled headers and the Offer Summary explicitly including the listed offer terms (500k cash, as-is, no contingencies, 30-day close). Comps list/table shows \u22653 items.\n- 0.8: Valid format and pages; missing only the Appendix comp list/table OR has only 2 comps listed.\n- 0.6: Valid format and pages; missing one major section (e.g., Negotiation Strategy or Recommended Counteroffer) OR Offer Summary missing 1 of the required offer terms.\n- 0.3: Valid format but wrong length (under 2 pages or over 3.5) OR missing two or more major sections.\n- 0.0: Not a PDF/DOCX, or basically unstructured (no clear headers), or under 1 page.\n\nOnly check PRESENCE and STRUCTURE, not correctness of analysis or writing quality.", "expectation": "A professional 2\u20133 page PDF/DOCX with the specified sections, an explicit Offer Summary including all key terms, and an Appendix or in-body comps list (\u22653)."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Correctness and Consistency Checks", "description": "Verify inclusion and consistency of key facts, numeric logic for counteroffer, and alignment to described market conditions. Mix of code rules and LLM judgment.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Offer Terms Presence and Consistency", "description": "Verify that the report text explicitly includes the key offer facts: $500,000 price, cash, as-is, no contingencies, and 30-day close; and mentions the original list price $525,000.", "weight": 2.5, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return (0.0, \"No document output to evaluate.\")\n        # Extract text from PDF or DOCX\n        text = \"\"\n        try:\n            if output.file_extension.lower().endswith('.pdf'):\n                text = context.files.read_pdf_text(output.id) or \"\"\n            else:\n                text = context.files.read_docx_text(output.id) or \"\"\n        except Exception:\n            # Fallback: try both\n            try:\n                text = context.files.read_pdf_text(output.id) or \"\"\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(output.id) or \"\"\n                except Exception:\n                    text = \"\"\n        if not text:\n            return (0.0, \"Unable to extract text from document.\")\n        low = text.lower()\n\n        def has_money(val):\n            # Matches 500,000 or 500000 with optional $ and spaces\n            s = str(val)\n            parts = [s, f\"{int(val):,}\"]\n            for p in parts:\n                if p in text or f\"${p}\" in text:\n                    return True\n            # Regex fallback\n            pattern = re.compile(r\"\\$?\\s*\" + re.escape(parts[1]) + r\"(?!\\d)\")\n            if pattern.search(text):\n                return True\n            return False\n\n        # Checks\n        checks = []\n        missing = []\n\n        # Offer $500,000\n        offer_present = has_money(500000)\n        checks.append(offer_present)\n        if not offer_present:\n            missing.append(\"offer price $500,000\")\n\n        # List price $525,000\n        list_present = has_money(525000)\n        checks.append(list_present)\n        if not list_present:\n            missing.append(\"list price $525,000\")\n\n        # cash\n        cash_present = \"cash\" in low\n        checks.append(cash_present)\n        if not cash_present:\n            missing.append(\"cash (payment type)\")\n\n        # as-is variants\n        as_is_present = (\"as-is\" in low) or (\"as is\" in low) or (\"asis\" in low)\n        checks.append(as_is_present)\n        if not as_is_present:\n            missing.append(\"as-is condition\")\n\n        # no contingencies (look for 'no/without ... contingenc')\n        cont_present = bool(re.search(r\"(no|without|waiv\\w*)\\s+[^\\n\\r\\.;:,]{0,20}contingenc\", low)) or (\"no contingenc\" in low)\n        checks.append(cont_present)\n        if not cont_present:\n            missing.append(\"no contingencies\")\n\n        # 30-day close\n        days_present = bool(re.search(r\"30\\s*[- ]?\\s*day\\b\", low)) or bool(re.search(r\"close\\w*\\s+in\\s+30\\s*day\", low))\n        checks.append(days_present)\n        if not days_present:\n            missing.append(\"30-day close\")\n\n        score_ratio = sum(1 for c in checks if c) / len(checks)\n        feedback = \"All key offer terms found.\" if score_ratio == 1.0 else (\"Missing: \" + \", \".join(missing))\n        return (score_ratio, feedback)\n    except Exception as e:\n        return (0.0, f\"Rule error: {e}\")"}, {"type": "code", "name": "Counteroffer Presence and Reasonable Range", "description": "Find a recommended counteroffer price and check it is numeric and within a plausible range between $500,000 and $525,000 (inclusive), and not above list nor below offer when those are found.", "weight": 2.5, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return (0.0, \"No document output to evaluate.\")\n        # Extract text\n        text = \"\"\n        try:\n            if output.file_extension.lower().endswith('.pdf'):\n                text = context.files.read_pdf_text(output.id) or \"\"\n            else:\n                text = context.files.read_docx_text(output.id) or \"\"\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(output.id) or \"\"\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(output.id) or \"\"\n                except Exception:\n                    text = \"\"\n        if not text:\n            return (0.0, \"Unable to extract text from document.\")\n        low = text.lower()\n\n        # Helper to extract nearest dollar amount around keywords\n        kw_pattern = re.compile(r\"(counter[- ]?offer|\\bcounter\\b|recommend(?:ed|ation)?|suggest(?:ion|ed)?|propos\\w+)\", re.IGNORECASE)\n        money_pattern = re.compile(r\"\\$\\s*([0-9]{1,3}(?:,[0-9]{3})+|[0-9]{3,})(?:\\.[0-9]{2})?\")\n\n        # Try to find labeled list and offer values (optional)\n        def find_near_amount(label_words, span=120):\n            for m in re.finditer(label_words, low):\n                start = max(0, m.start()-span)\n                end = min(len(text), m.end()+span)\n                window = text[start:end]\n                m2 = money_pattern.search(window)\n                if m2:\n                    val = int(m2.group(1).replace(',', ''))\n                    return val\n            return None\n\n        list_val = find_near_amount(r\"list(ed)?\\s+price|asking\\s+price|listed\\s+at\")\n        offer_val = find_near_amount(r\"offer|contract\\s+price\")\n\n        # Find counteroffer amount\n        counter_val = None\n        for m in kw_pattern.finditer(text):\n            start = max(0, m.start()-160)\n            end = min(len(text), m.end()+160)\n            window = text[start:end]\n            m2 = money_pattern.search(window)\n            if m2:\n                try:\n                    counter_val = int(m2.group(1).replace(',', ''))\n                    break\n                except Exception:\n                    continue\n\n        if counter_val is None:\n            return (0.0, \"No numeric counteroffer/recommended price found near relevant keywords.\")\n\n        # Plausibility checks\n        in_band = 500000 <= counter_val <= 525000\n        near_band = 495000 <= counter_val <= 535000\n\n        # Consistency with list/offer when available\n        consistency_ok = True\n        issues = []\n        if offer_val is not None and counter_val < offer_val:\n            consistency_ok = False\n            issues.append(\"counteroffer below offer\")\n        if list_val is not None and counter_val > list_val:\n            consistency_ok = False\n            issues.append(\"counteroffer above list price\")\n\n        # Score logic\n        if in_band and consistency_ok:\n            score = 1.0\n        elif near_band and consistency_ok:\n            score = 0.6\n        elif near_band:\n            score = 0.4\n        else:\n            score = 0.2 if consistency_ok else 0.1\n\n        fb = f\"Counteroffer detected: ${counter_val:,}. \"\n        if issues:\n            fb += \"Issues: \" + \", \".join(issues) + \".\"\n        else:\n            fb += \"Range/consistency acceptable.\"\n        return (score, fb)\n    except Exception as e:\n        return (0.0, f\"Rule error: {e}\")"}, {"type": "code", "name": "Market Context References (Overpriced, DOM, Repair Condition)", "description": "Check for acknowledgment of being slightly overpriced, time on market (months/DOM), and minor repairs/condition as factors in strategy.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return (0.0, \"No document output to evaluate.\")\n        # Extract text\n        text = \"\"\n        try:\n            if output.file_extension.lower().endswith('.pdf'):\n                text = context.files.read_pdf_text(output.id) or \"\"\n            else:\n                text = context.files.read_docx_text(output.id) or \"\"\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(output.id) or \"\"\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(output.id) or \"\"\n                except Exception:\n                    text = \"\"\n        if not text:\n            return (0.0, \"Unable to extract text from document.\")\n        low = text.lower()\n\n        overpriced = any(k in low for k in [\"overprice\", \"above market\", \"priced high\", \"price reduction\", \"reduce the price\", \"slightly over\"])\n        dom = bool(re.search(r\"days\\s+on\\s+market|\\bdom\\b|months?\\s+on\\s+market|several\\s+months\", low))\n        repairs = any(k in low for k in [\"minor repairs\", \"repairs\", \"deferred maintenance\", \"as-is condition\", \"condition issues\"]) or (\"as-is\" in low or \"as is\" in low)\n\n        score = (overpriced + dom + repairs) / 3.0\n        missing = []\n        if not overpriced:\n            missing.append(\"overpriced/price positioning\")\n        if not dom:\n            missing.append(\"time on market/DOM\")\n        if not repairs:\n            missing.append(\"repairs/condition\")\n        fb = \"Market context elements present.\" if score == 1.0 else (\"Missing: \" + \", \".join(missing))\n        return (score, fb)\n    except Exception as e:\n        return (0.0, f\"Rule error: {e}\")"}, {"type": "code", "name": "Length Reasonableness (Word Count)", "description": "Approximate 2\u20133 pages by word count. Full credit for ~400\u20131200 words; partial for near-miss.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        # Extract text\n        text = \"\"\n        try:\n            if output.file_extension.lower().endswith('.pdf'):\n                text = context.files.read_pdf_text(output.id) or \"\"\n            else:\n                text = context.files.read_docx_text(output.id) or \"\"\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(output.id) or \"\"\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(output.id) or \"\"\n                except Exception:\n                    text = \"\"\n        words = len(text.split()) if text else 0\n        if 400 <= words <= 1200:\n            return (1.0, f\"Word count OK: {words}\")\n        elif 250 <= words < 400 or 1200 < words <= 1500:\n            return (0.5, f\"Word count near-miss: {words}\")\n        else:\n            return (0.0, f\"Word count out of range: {words}\")\n    except Exception as e:\n        return (0.0, f\"Rule error: {e}\")"}, {"type": "llm_judge", "name": "Advice Alignment and Negotiation Logic", "description": "LLM validates that the negotiation strategy leverages cash/as-is certainty and 30-day close, fairly presents pros/cons, and preserves fiduciary duty without pressuring the seller.", "weight": 2.5, "judge_prompt": "Evaluate the report for correctness of advisory alignment (not writing polish):\n\nRequired elements to consider:\n- Uses the cash/as-is/no-contingency/30-day close facts to articulate benefits (speed, certainty, lower risk of fall-through, no repair credits).\n- Provides a clear, defensible negotiation strategy (e.g., counter at a justifiable price; consider timeline tweaks; reasoning tied to market analysis and condition/DOM).\n- Recommends a specific counteroffer and explains why it is reasonable.\n- States risks/trade-offs (e.g., staying listed vs. accepting now, potential further price reductions, holding costs, buyer leverage due to repairs/overpricing).\n- Upholds fiduciary duty: presents options and pros/cons, explicitly leaves decision to the seller (no undue pressure).\n\nScoring:\n- 1.0: All five elements clearly present and consistent with facts.\n- 0.7: Minor gaps (missing 1 element or shallow rationale).\n- 0.4: Several gaps (2 elements missing or weak, strategy unclear).\n- 0.1: Mentions some facts but strategy/rationale largely unsupported.\n- 0.0: Advice ignores key facts or pressures the seller inappropriately.\n\nJudge only alignment and logic, not grammar/style.", "expectation": "Balanced, fact-grounded negotiation plan that leverages the buyer\u2019s terms, explains a reasonable counter, and preserves the seller\u2019s decision authority."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3: Professional Quality and Presentation", "description": "Holistic LLM assessment of client-facing clarity, tone, organization, and actionability.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professionalism, Clarity, and Actionability", "description": "Evaluate presentation quality for a seller-facing advisory report.", "weight": 4.0, "judge_prompt": "Assess the overall quality of the report as a client-facing deliverable:\n- Professional tone and trust-building voice appropriate for a seller client\n- Clear organization and readability (headers, logical flow, concise narrative)\n- Actionable next steps and concrete recommendations (e.g., counter number, timing, steps to respond)\n- Proper use of headings/formatting and minimal fluff\n- Absence of obvious factual contradictions within the document\n\nScoring:\n- 1.0: Highly professional, clear, and actionable; easy for the seller to understand what to do next.\n- 0.7: Generally strong with minor clarity/structure issues.\n- 0.4: Mixed\u2014readable but lacks polish or actionability.\n- 0.2: Hard to follow; weak organization; limited actionable guidance.\n- 0.0: Unprofessional or confusing.\n\nFocus on presentation quality, not structure (Stage 1) or factual alignment (Stage 2).", "expectation": "A polished, seller-ready 2\u20133 page report with clear structure, actionable steps, and a professional tone."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "7bbfcfe9-132d-4194-82bb-d6f29d001b01", "rubric": {"category_name": "Government Compliance \u2013 SCRA Audit Question Set (50 U.S.C. \u00a73937 and \u00a73919)", "rationale": "This rubric enforces a self-documenting, verifiable Excel deliverable containing exactly 10 yes/no/NA audit questions tied to specific SCRA statutes. Stage 1 (LLM-only) mandates the precise spreadsheet shape, column structure, IDs, and citations to make verification trivial. Stage 2 uses code rules to deterministically validate IDs, response option constraints, and statute/citation consistency, plus an LLM topical alignment check. Stage 3 holistically judges clarity, audit utility, and professional presentation.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2013 Shape Enforcement Gate (LLM ONLY)", "description": "Verifies the output is an Excel spreadsheet with the exact structural requirements that enable verification: required columns, required IDs, presence of statute/citation, response option constraints. Structure only \u2013 do not judge content quality or legal correctness.", "is_required": true, "max_points": 5.0, "min_score_to_pass": 4.0, "rules": [{"type": "llm_judge", "name": "Structured Excel Format Requirement", "description": "Check that the deliverable is an Excel (.xlsx) file containing a single questions table with the specified columns, exactly 10 rows with the specified IDs, per-row citations referencing \u00a73937 or \u00a73919, and response options limited to Yes/No/Not Applicable.", "weight": 5.0, "judge_prompt": "You are verifying STRUCTURE ONLY for an Excel deliverable. Use the rendered spreadsheet view. Do NOT judge content quality or legal accuracy; only check the presence and layout required below.\n\nFormat Requirements (must all be satisfied for full credit):\n- File must be an Excel spreadsheet (.xlsx). Not CSV, not PDF, not DOCX.\n- There must be a visible table on a sheet (any reasonable sheet name) with the following columns (flexible to close variants; header order does not matter):\n  1) ID (or Identifier / Question ID)\n  2) Statute (or Section / Law / USC Section)\n  3) Citation (or Legal Citation / USC Citation)\n  4) Question (or Test Question / Prompt)\n  5) Response Options (or Allowed Responses / Permitted Responses)\n- The table must contain exactly 10 question rows (each on a separate row) with ALL of these unique IDs present:\n  \u2022 SCRA-12a, SCRA-12b, SCRA-12c, SCRA-12d (these are for \u00a73937)\n  \u2022 SCRA-13, SCRA-14, SCRA-15, SCRA-16, SCRA-17, SCRA-18 (these are for \u00a73919)\n- Each of the 10 rows must include a citation that clearly references the applicable statute number (e.g., includes 3937 or 3919, with Title 50 U.S.C. or \u00a7 notation). It is acceptable if the statute is reflected in either the Statute or Citation column, as long as it is unambiguous per row.\n- Response Options must be restricted to Yes / No / Not Applicable (accept case and common variants like N/A). The restriction must be explicitly visible in the sheet (e.g., a column indicating allowed responses per row or a clearly labeled field applying to all rows). You do NOT need to verify actual data validation\u2014only that the allowed responses are clearly stated and limited to those three.\n\nScoring (STRUCTURE ONLY):\n- 5.0: Excel format; required columns present (allowing close variants); exactly 10 rows; all required IDs present and unique; each row has a clear citation to the correct statute (3937 for SCRA-12a\u201312d and 3919 for SCRA-13\u201318); response options explicitly limited to Yes/No/Not Applicable.\n- 4.0: Excel format; minor header naming variance or layout issues but clearly the five required columns exist; 10 rows with all required IDs; citations present but slightly inconsistent placement; response options clearly limited to Yes/No/Not Applicable.\n- 2.5: Excel format but missing one required column OR has 10 rows but missing 1\u20132 required IDs or contains duplicates; citations present but unclear for several rows; or response options not clearly restricted for all rows.\n- 1.0: Excel format but significant structural gaps (e.g., multiple missing columns, many missing IDs, unclear citations, or unclear response restrictions).\n- 0.0: Not an Excel file OR no recognizable questions table.\n\nOnly evaluate the presence and structure necessary to enable later verification. Do NOT assess the legal accuracy or quality of the questions.", "expectation": "A single Excel worksheet with a clearly labeled table having the five columns, exactly ten rows with the specified IDs, clear per-row citations that reference \u00a73937 or \u00a73919, and response options limited to Yes/No/Not Applicable."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness Verification (Deterministic + LLM)", "description": "Now that structure is enforced, verify deterministically: required IDs present and unique, allowed response options limited to Yes/No/Not Applicable, and statute/citation alignment with IDs. Also use an LLM check to ensure topical alignment of the questions to \u00a73937 and \u00a73919 subject matter.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Columns, IDs, and Row Count Validation", "description": "Check for presence of required columns (flexible names), exactly 10 rows, all required IDs present and unique.", "weight": 1.5, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output or wrong file type.\"\n    # Try reading the first sheet robustly\n    df = None\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheet_name = xls.sheet_names[0]\n        df = pd.read_excel(path, sheet_name=sheet_name)\n    except Exception:\n        try:\n            df = context.files.read_excel(output.id, sheet_name=0)\n        except Exception as e:\n            return 0.0, f\"Failed to read Excel: {e}\"\n    if df is None or df.empty:\n        return 0.0, \"Empty or unreadable sheet.\"\n    # Drop fully empty rows\n    df = df.dropna(how='all')\n    if df.empty:\n        return 0.0, \"Sheet contains no data rows.\"\n    # Normalize columns\n    norm_cols = [str(c).strip().lower() for c in df.columns]\n    df.columns = norm_cols\n\n    def find_col(candidates):\n        for c in df.columns:\n            for cand in candidates:\n                if cand in c:\n                    return c\n        return None\n\n    id_col = find_col(['id', 'identifier', 'question id', 'item id', 'scra'])\n    statute_col = find_col(['statute', 'section', 'usc', 'law'])\n    citation_col = find_col(['citation', 'cite', 'usc citation', 'legal citation'])\n    question_col = find_col(['question', 'test question', 'prompt'])\n    responses_col = find_col(['response options', 'allowed responses', 'permitted responses', 'responses', 'allowed answers'])\n\n    present_cols = [id_col, statute_col, citation_col, question_col, responses_col]\n    cols_present_count = sum(c is not None for c in present_cols)\n    cols_score = cols_present_count / 5.0  # [0..1]\n\n    feedback_parts = []\n    if cols_present_count < 5:\n        missing = []\n        if id_col is None: missing.append('ID')\n        if statute_col is None: missing.append('Statute')\n        if citation_col is None: missing.append('Citation')\n        if question_col is None: missing.append('Question')\n        if responses_col is None: missing.append('Response Options')\n        feedback_parts.append(f\"Missing columns: {', '.join(missing)}\")\n\n    # ID checks\n    required_ids = [\n        'scra-12a','scra-12b','scra-12c','scra-12d',\n        'scra-13','scra-14','scra-15','scra-16','scra-17','scra-18'\n    ]\n    id_score = 0.0\n    dup_score = 0.0\n    if id_col is not None and id_col in df.columns:\n        ids = df[id_col].astype(str).str.strip().str.lower().tolist()\n        # Only count non-empty\n        ids = [x for x in ids if x and x != 'nan']\n        total_rows = len(ids)\n        exact_10 = 1.0 if total_rows == 10 else max(0.0, 1.0 - abs(total_rows - 10)/10.0)\n        present_required = sum(1 for rid in required_ids if rid in ids)\n        id_score = present_required / 10.0  # fraction of required IDs found\n        no_dups = 1.0 if len(ids) == len(set(ids)) else max(0.0, 1.0 - (len(ids) - len(set(ids))) / max(1, len(ids)))\n        # Combine presence of exact count and no duplicates\n        dup_score = 0.5 * no_dups + 0.5 * exact_10\n        if total_rows != 10:\n            feedback_parts.append(f\"Row count is {total_rows}, expected 10.\")\n        missing_ids = [rid for rid in required_ids if rid not in ids]\n        if missing_ids:\n            feedback_parts.append(f\"Missing IDs: {', '.join(missing_ids)}\")\n        if len(ids) != len(set(ids)):\n            feedback_parts.append(\"Duplicate IDs detected.\")\n    else:\n        feedback_parts.append(\"ID column not found; cannot validate IDs, duplicates, or row count.\")\n\n    # Weighted combination within [0..1]\n    score = 0.5 * cols_score + 0.3 * id_score + 0.2 * dup_score\n    feedback = \"; \".join(feedback_parts) if feedback_parts else \"Columns and IDs look valid.\"\n    return max(0.0, min(1.0, score)), feedback"}, {"type": "code", "name": "Allowed Responses Constraint", "description": "Verify that response options are limited to Yes, No, and Not Applicable (allow common variants like N/A, NA).", "weight": 0.8, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output or wrong file type.\"\n    # Read first sheet\n    df = None\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheet_name = xls.sheet_names[0]\n        df = pd.read_excel(path, sheet_name=sheet_name)\n    except Exception:\n        try:\n            df = context.files.read_excel(output.id, sheet_name=0)\n        except Exception as e:\n            return 0.0, f\"Failed to read Excel: {e}\"\n    if df is None or df.empty:\n        return 0.0, \"Empty or unreadable sheet.\"\n\n    # Normalize columns\n    df = df.dropna(how='all')\n    df.columns = [str(c).strip().lower() for c in df.columns]\n\n    def find_col(candidates):\n        for c in df.columns:\n            for cand in candidates:\n                if cand in c:\n                    return c\n        return None\n\n    responses_col = find_col(['response options', 'allowed responses', 'permitted responses', 'responses', 'allowed answers'])\n    if responses_col is None:\n        return 0.0, \"Response Options column missing.\"\n\n    allowed = {\"yes\", \"no\", \"not applicable\", \"n/a\", \"na\"}\n\n    def parse_tokens(val):\n        if pd.isna(val):\n            return set()\n        s = str(val).strip().lower()\n        # Split on common delimiters\n        parts = re.split(r\"[,/;|\\\\]+\", s)\n        parts = [p.strip() for p in parts if p.strip()]\n        # Normalize common forms\n        norm = []\n        for p in parts:\n            if p in {\"n/a\", \"n.a.\", \"n a\", \"na\"}: p = \"n/a\"\n            if p in {\"not applicable\", \"not-applicable\"}: p = \"not applicable\"\n            norm.append(p)\n        return set(norm)\n\n    # Evaluate across unique values in the column\n    unique_vals = df[responses_col].dropna().unique().tolist()\n    if not unique_vals:\n        return 0.0, \"Response Options column has no values.\"\n\n    subset_ok = True\n    has_yes = False\n    has_no = False\n    has_na = False\n    bad_examples = []\n    for v in unique_vals:\n        toks = parse_tokens(v)\n        if not toks:\n            subset_ok = False\n            bad_examples.append(str(v))\n            continue\n        if not toks.issubset(allowed):\n            subset_ok = False\n            bad = \", \".join(sorted(toks - allowed))\n            bad_examples.append(f\"{v} (invalid: {bad})\")\n        has_yes = has_yes or (\"yes\" in toks)\n        has_no = has_no or (\"no\" in toks)\n        has_na = has_na or (\"not applicable\" in toks or \"n/a\" in toks or \"na\" in toks)\n\n    score = 1.0 if subset_ok and has_yes and has_no and has_na else 0.0\n    fb_bits = []\n    if not subset_ok:\n        fb_bits.append(\"Found values outside allowed set\")\n    if not has_yes: fb_bits.append(\"'Yes' not indicated\")\n    if not has_no: fb_bits.append(\"'No' not indicated\")\n    if not has_na: fb_bits.append(\"'Not Applicable' not indicated\")\n    feedback = \"; \".join(fb_bits) if fb_bits else \"Response options correctly limited to Yes/No/Not Applicable.\"\n    return score, feedback"}, {"type": "code", "name": "Statute/Citation Alignment with IDs", "description": "Ensure SCRA-12a\u201312d rows reference \u00a73937 and SCRA-13\u201318 rows reference \u00a73919 in statute/citation text.", "weight": 0.4, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output or wrong file type.\"\n    # Read first sheet\n    df = None\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheet_name = xls.sheet_names[0]\n        df = pd.read_excel(path, sheet_name=sheet_name)\n    except Exception:\n        try:\n            df = context.files.read_excel(output.id, sheet_name=0)\n        except Exception as e:\n            return 0.0, f\"Failed to read Excel: {e}\"\n    if df is None or df.empty:\n        return 0.0, \"Empty or unreadable sheet.\"\n\n    df = df.dropna(how='all')\n    df.columns = [str(c).strip().lower() for c in df.columns]\n\n    def find_col(candidates):\n        for c in df.columns:\n            for cand in candidates:\n                if cand in c:\n                    return c\n        return None\n\n    id_col = find_col(['id', 'identifier', 'question id', 'item id', 'scra'])\n    statute_col = find_col(['statute', 'section', 'usc', 'law'])\n    citation_col = find_col(['citation', 'cite', 'usc citation', 'legal citation'])\n\n    if id_col is None:\n        return 0.0, \"ID column missing.\"\n\n    good = 0\n    total = 0\n    fb_mismatches = []\n\n    for idx, row in df.iterrows():\n        rid = str(row.get(id_col, '')).strip().lower()\n        if not rid or rid == 'nan':\n            continue\n        if not (rid.startswith('scra-12') or rid.startswith('scra-1')):\n            # Only evaluate rows that look like SCRA IDs\n            continue\n        total += 1\n        # Combine statute + citation text for flexible matching\n        stat_text = str(row.get(statute_col, '')) if statute_col else ''\n        cit_text = str(row.get(citation_col, '')) if citation_col else ''\n        combo = (stat_text + ' ' + cit_text).lower()\n        has_3937 = '3937' in combo or '\u00a73937' in combo\n        has_3919 = '3919' in combo or '\u00a73919' in combo\n        if rid.startswith('scra-12'):\n            if has_3937 and not has_3919:\n                good += 1\n            else:\n                fb_mismatches.append(rid)\n        else:\n            # scra-13..18 should map to 3919\n            if has_3919 and not has_3937:\n                good += 1\n            else:\n                fb_mismatches.append(rid)\n\n    if total == 0:\n        return 0.0, \"No recognizable SCRA IDs to validate alignment.\"\n\n    score = good / total\n    feedback = \"Mismatched or unclear statute/citation for: \" + \", \".join(fb_mismatches) if fb_mismatches else \"All IDs align with the correct statute/citation.\"\n    return score, feedback"}, {"type": "llm_judge", "name": "Topical Alignment to Statutes", "description": "Check that the \u00a73937 questions address the maximum rate of interest on pre-service debts and related compliance elements (e.g., 6% cap, qualifying period, recalculation/refund), and that the \u00a73919 questions address non-discrimination in future financial transactions due to exercising SCRA rights.", "weight": 0.3, "judge_prompt": "Evaluate whether the questions are TOPICALLY aligned to the statutes (not legal perfection). Use the IDs:\n- SCRA-12a\u2013SCRA-12d correspond to 50 U.S.C. \u00a73937 (maximum rate of interest on pre-service debts).\n- SCRA-13\u2013SCRA-18 correspond to 50 U.S.C. \u00a73919 (exercise of rights under chapter not to affect certain future financial transactions).\n\nGuidance:\n- \u00a73937 items should reasonably probe: applicability to pre-service obligations, interest/fees cap (commonly understood 6% unless otherwise specified by law), timing/coverage during active duty, documentation/notice requirements, adjustments/refunds, and non-capitalization of excess interest.\n- \u00a73919 items should reasonably probe: that exercising SCRA rights is not used to deny, refuse, revoke, or adversely alter terms for future credit/financial transactions; no negative credit reporting or adverse annotations due to exercising those rights; and that such status is not used as a negative factor.\n\nScoring:\n- 1.0: All \u00a73937 questions are clearly about interest-rate cap compliance on pre-service debts and related mechanics; all \u00a73919 questions clearly address non-discrimination in future transactions due to SCRA rights.\n- 0.6: Mostly aligned with minor drifts or 1 unclear item.\n- 0.3: Multiple items off-topic or vague.\n- 0.0: Mostly unrelated topics.\n\nOnly judge topical fit, not drafting quality or legal exactness.", "expectation": "Each question\u2019s focus matches the core concepts of \u00a73937 or \u00a73919 according to its ID group."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Quality and Professionalism (LLM)", "description": "Holistic assessment of clarity, audit utility, and presentation quality for real-world compliance testing by a regulator/servicer audience.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity and Audit Utility", "description": "Assess whether each question is clear, concise, unambiguous, and suitable for Yes/No/Not Applicable answers, and whether the set is usable as an audit template.", "weight": 1.2, "judge_prompt": "Judge the overall clarity and audit utility of the question set:\n- Are questions phrased to be answerable strictly as Yes/No/Not Applicable without ambiguity?\n- Are there redundancies or overlapping questions that would reduce audit efficiency?\n- Are the IDs correctly used to make referencing easy?\n- Would this be usable as a template for regulators/servicers reviewing accounts for SCRA compliance?\n\nScoring:\n- 1.0: Clear, concise yes/no/NA phrasing for all items; no redundancy; immediately usable as an audit template.\n- 0.6: Minor ambiguity or redundancy but still usable with slight edits.\n- 0.3: Several items need rework for yes/no/NA clarity; limited usability.\n- 0.0: Poorly structured for yes/no/NA and not usable as an audit template.\n\nDo not re-check structure or legal correctness\u2014focus on clarity and practical audit utility.", "expectation": "A crisp, unambiguous, non-overlapping set of 10 questions that auditors can directly use."}, {"type": "llm_judge", "name": "Professional Presentation", "description": "Evaluate formatting and labeling for a professional regulatory deliverable.", "weight": 0.8, "judge_prompt": "Assess professional presentation of the spreadsheet:\n- Are headers readable and professional? Does the sheet look organized (consistent capitalization, no obvious typos, sensible ordering)?\n- Are statute and citation values presented consistently and professionally (e.g., 50 U.S.C. \u00a73937, 50 U.S.C. \u00a73919)?\n- Is there sufficient spacing and alignment for readability?\n- Are there distracting artifacts (empty columns, random notes) that would confuse reviewers?\n\nScoring:\n- 1.0: Professional, clean, and consistent presentation.\n- 0.6: Minor cosmetic issues but professional overall.\n- 0.3: Noticeable formatting inconsistencies.\n- 0.0: Unprofessional or messy.\n\nDo not re-check structural requirements; focus on presentation quality.", "expectation": "A clean, well-formatted spreadsheet suitable for regulator-facing documentation."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "ffed32d8-d192-4e3f-8cd4-eda5a730aec3", "rubric": {"category_name": "Retail Pharmacist: Auto-Refill Cost-Effectiveness Decision", "rationale": "Pattern C (Mixed): The deliverable is a 1\u20132 page PDF decision memo, but verification requires a companion data workbook. Stage 1 uses an LLM judge to strictly enforce the presence and structure of both the PDF report and a machine-readable Excel/CSV workbook. Stage 2 uses code rules to verify calculations, internal consistency, and correctness of the final decision against the 2% ($16,000) threshold. Stage 3 uses an LLM judge for professional quality and clarity of the recommendation for an owner-operator audience.", "max_total_score": 24.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (LLM ONLY)", "description": "Gate: Verify required file types and exact structural elements exist to enable verification. Must include BOTH a professionally formatted PDF/DOCX report AND a companion Excel/CSV workbook with defined sheets/tables.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Structured Output Format Requirement (PDF/DOCX + Workbook)", "description": "Check presence and structure of BOTH the report (PDF/DOCX) and the companion workbook (Excel/CSV) with the mandated sections/tables that make verification trivial.", "weight": 8.0, "judge_prompt": "You are evaluating whether the candidate outputs meet the REQUIRED STRUCTURE for verification. Look across ALL provided files.\n\nREQUIRED FILES:\n1) A report in PDF or DOCX format (not plain text, not Excel). Target length: 1\u20132 pages.\n2) A companion data workbook in Excel (.xlsx preferred) or CSV, containing the analysis data used in the report.\n\nREPORT (PDF/DOCX) \u2014 Required structure:\n- Professional formatting, clearly readable.\n- Sections (flexible names allowed, but content must be present):\n  a) Executive Summary / Summary of Findings and Recommendation (must appear on the first page). Should include the final decision whether to switch to 100-day fills or maintain 90-day fills.\n  b) Methodology & Assumptions: must explicitly reference these points:\n     \u2022 300 patients per medication\n     \u2022 Once daily dosing (1 tablet/day)\n     \u2022 Fill frequencies: 90-day \u2192 4 fills/year (360 days), 100-day \u2192 3 fills/year (300 days)\n     \u2022 Decision threshold: 2% of $800,000 = $16,000\n     \u2022 Data source references (Wholesale Price.pdf, Reimbursement.pdf) or equivalent sourcing notes\n  c) Comparative Results Table: a single table that compares both 90-day and 100-day models for each of the 10 listed maintenance medications (Atorvastatin 10/20 mg, Amlodipine 5/10 mg, Rosuvastatin 5/10 mg, Losartan 25/50 mg, Metformin 500 mg, Tamsulosin 0.4 mg). The table must visibly include, per medication and per model (90 vs 100):\n     \u2022 Drug name and strength\n     \u2022 Model (90-day vs 100-day) or Days Supply\n     \u2022 Patients (should show 300)\n     \u2022 Fills per Year (4 vs 3)\n     \u2022 Tablets per Fill (90 vs 100)\n     \u2022 Drug cost and vial/supply cost (per fill)\n     \u2022 Total expense annual\n     \u2022 Reimbursement per fill and total reimbursement annual\n     \u2022 Annual revenue (reimbursement minus expense)\n     \u2022 Annual revenue difference (100-day minus 90-day) clearly shown per drug OR a separate summary row that computes this difference across models\n  d) Final Recommendation: Explicit statement applying the 2%/$16,000 threshold to the computed revenue difference to choose between switching to 100-day fills vs maintaining 90-day fills.\n\nCOMPANION WORKBOOK (Excel/CSV) \u2014 Required structure:\n- Must be a separate machine-readable file accompanying the PDF/DOCX. Flexible sheet names allowed, but it must contain:\n  1) An Assumptions sheet/table with the following named items (flexible naming OK, but values must be present):\n     \u2022 Total Annual Revenue = $800,000\n     \u2022 Threshold Percentage = 2%\n     \u2022 Threshold Absolute (should be $16,000)\n     \u2022 Patients per Drug = 300\n     \u2022 Fills per Year: 90-day = 4, 100-day = 3\n  2) A Drug Analysis sheet/table listing BOTH 90-day and 100-day rows for each of the 10 medications (20 rows total). Columns required (flexible names OK):\n     \u2022 Drug name, Strength\n     \u2022 Model/Days Supply (90 vs 100)\n     \u2022 Patients\n     \u2022 Fills per Year\n     \u2022 Tablets per Fill\n     \u2022 Drug unit cost or drug cost per fill (either is acceptable, but one must be present)\n     \u2022 Vial/supply cost per fill\n     \u2022 Total expense annual\n     \u2022 Reimbursement per fill\n     \u2022 Total reimbursement annual\n     \u2022 Annual revenue (reimbursement minus expense)\n     \u2022 Revenue difference (100 \u2212 90) per drug (can also be shown in a Summary sheet if not per-row)\n  3) A Summary sheet/table that aggregates totals across all drugs and shows:\n     \u2022 Total annual revenue under 90-day and 100-day models\n     \u2022 Total difference (100 \u2212 90)\n     \u2022 Final Recommendation text that references the $16,000 threshold logic\n\nSCORING (8 points total):\n- 8.0: Both files present; Report has all required sections; Comparative table includes required data; Workbook contains Assumptions, Drug Analysis (20 rows covering all 10 medications with both models), and Summary with totals and final recommendation.\n- 6.0\u20137.5: Minor omissions (e.g., per-drug difference shown only in Summary, or one supporting field missing) but structure is sufficient for verification.\n- 3.0\u20135.5: Significant structural gaps (e.g., missing Assumptions sheet, incomplete Drug Analysis columns, or report lacks a key section) but some verifiable structure exists.\n- 0.0\u20132.5: Missing report or workbook; wrong formats; or structure too incomplete to verify.\n\nIMPORTANT: Only judge structure and presence, not correctness of numbers.", "expectation": "A 1\u20132 page PDF/DOCX report with the specified sections, plus a companion Excel/CSV workbook with Assumptions, Drug Analysis (20 rows), and Summary with totals and final recommendation."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Verification (Code + LLM)", "description": "Verify numerical correctness, internal consistency, coverage of all 10 medications and both models, and that the final decision matches the 2% ($16,000) rule based on the workbook data.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Workbook Present and Parseable", "description": "Find a spreadsheet among outputs and confirm it can be opened and at least one sheet read.", "weight": 1.0, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        # Find a spreadsheet (xlsx or csv)\n        wb = None\n        for res in context.get_all_outputs() or []:\n            if getattr(res, 'is_spreadsheet', False):\n                wb = res\n                break\n            # Fallback: CSV sometimes marked as text\n            if getattr(res, 'is_text_format', False) and str(res.name or '').lower().endswith('.csv'):\n                wb = res\n                break\n        if not wb:\n            return 0.0, 'No spreadsheet/CSV found.'\n        # Try reading sheet names (xlsx) or CSV\n        path = context.files.get_path(wb.id)\n        score = 0.0\n        try:\n            if str(path).lower().endswith('.xlsx'):\n                xls = pd.ExcelFile(path)\n                if len(xls.sheet_names) > 0:\n                    df = pd.read_excel(path, sheet_name=xls.sheet_names[0])\n                    if df is not None:\n                        score = 1.0\n            else:\n                # CSV path\n                df = pd.read_csv(path)\n                if df is not None and df.shape[0] >= 1:\n                    score = 1.0\n        except Exception as e:\n            return 0.0, f'Failed to read workbook: {e}'\n        return float(score)\n    except Exception as e:\n        return 0.0, f'Exception: {e}'"}, {"type": "code", "name": "Assumptions Sheet Integrity", "description": "Validate presence of key assumptions: $800,000 revenue, 2% threshold, $16,000 absolute threshold, 300 patients, and fills/year mapping (90->4, 100->3). Flexible parsing of key-value sheets.", "weight": 2.0, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    weight = 2.0\n    try:\n        # Helper to find spreadsheet\n        def find_wb():\n            for res in context.get_all_outputs() or []:\n                if getattr(res, 'is_spreadsheet', False):\n                    return res\n                if getattr(res, 'is_text_format', False) and str(res.name or '').lower().endswith('.csv'):\n                    return res\n            return None\n        wb = find_wb()\n        if not wb:\n            return 0.0, 'No workbook.'\n        path = context.files.get_path(wb.id)\n        # Load sheets\n        sheets = {}\n        if str(path).lower().endswith('.xlsx'):\n            xls = pd.ExcelFile(path)\n            for s in xls.sheet_names:\n                try:\n                    sheets[s] = pd.read_excel(path, sheet_name=s)\n                except Exception:\n                    pass\n        else:\n            # CSV fallback: treat as a single sheet named 'Drug Analysis'\n            try:\n                sheets['Drug Analysis'] = pd.read_csv(path)\n            except Exception:\n                return 0.0, 'CSV not readable.'\n        # Find assumptions-like sheet\n        target_sheet = None\n        for name, df in sheets.items():\n            lname = name.lower()\n            if any(k in lname for k in ['assumption','input','parameter','config']):\n                target_sheet = df\n                break\n        # If not found, attempt to detect a 2-col key-value sheet\n        if target_sheet is None:\n            for df in sheets.values():\n                if df.shape[1] in [2,3] and df.shape[0] >= 3:\n                    # Heuristic: first col strings, second col numbers/strings\n                    if df.iloc[:,0].astype(str).str.len().mean() > 3:\n                        target_sheet = df\n                        break\n        if target_sheet is None:\n            return 0.4, 'No explicit Assumptions sheet; minimal credit.'\n        # Build key-value map\n        kv = {}\n        df = target_sheet\n        df = df.dropna(how='all')\n        df.columns = [str(c).strip() for c in df.columns]\n        # Try row-wise key-value\n        for _, row in df.iterrows():\n            items = [row.get(c) for c in df.columns]\n            # Key in first cell\n            key = str(items[0]).strip().lower()\n            vals = [v for v in items[1:] if pd.notna(v)]\n            if not key or key == 'nan':\n                continue\n            if len(vals) == 1:\n                kv[key] = vals[0]\n            elif len(vals) > 1:\n                kv[key] = vals\n        # Normalization helpers\n        def find_num(keys):\n            for k, v in kv.items():\n                for want in keys:\n                    if want in k:\n                        try:\n                            if isinstance(v, list):\n                                for vv in v:\n                                    x = pd.to_numeric(str(vv).replace(',','').replace('$','').replace('%',''), errors='coerce')\n                                    if pd.notna(x):\n                                        return float(x)\n                            else:\n                                x = pd.to_numeric(str(v).replace(',','').replace('$','').replace('%',''), errors='coerce')\n                                if pd.notna(x):\n                                    return float(x)\n                        except Exception:\n                            pass\n            return None\n        def find_text(keys):\n            for k, v in kv.items():\n                for want in keys:\n                    if want in k:\n                        return str(v).lower()\n            return ''\n        # Extract values\n        total_rev = find_num(['total annual revenue','annual revenue','total revenue'])\n        thr_pct = find_num(['threshold percentage','threshold %','percent threshold'])\n        thr_abs = find_num(['threshold absolute','threshold amount','threshold usd'])\n        patients = find_num(['patients per drug','patients','enrolled'])\n        fills90 = find_num(['fills per year (90)','90-day fills','fills 90'])\n        fills100 = find_num(['fills per year (100)','100-day fills','fills 100'])\n        # Scoring\n        score = 0.0\n        # Total revenue ~ 800000 (+/- 5%)\n        if total_rev is not None and abs(total_rev - 800000.0) <= 0.05*800000:\n            score += 0.5\n        # Threshold percent ~ 2\n        if thr_pct is not None:\n            if thr_pct > 1.0:\n                # assume given as percent\n                if abs(thr_pct - 2.0) <= 0.2:\n                    score += 0.4\n            else:\n                if abs(thr_pct - 0.02) <= 0.005:\n                    score += 0.4\n        # Threshold absolute ~ 16000 (or derivable)\n        derived_thr_abs = None\n        if total_rev is not None and thr_pct is not None:\n            p = thr_pct/100.0 if thr_pct > 1.0 else thr_pct\n            derived_thr_abs = total_rev * p\n        if thr_abs is not None and abs(thr_abs - 16000.0) <= 800:  # 5%\n            score += 0.4\n        elif derived_thr_abs is not None and abs(derived_thr_abs - 16000.0) <= 800:\n            score += 0.3\n        # Patients = 300\n        if patients is not None and abs(patients - 300) <= 1:\n            score += 0.3\n        # Fills/year mapping\n        if (fills90 is not None and int(round(fills90)) == 4) and (fills100 is not None and int(round(fills100)) == 3):\n            score += 0.4\n        return min(score, weight)\n    except Exception as e:\n        return 0.0, f'Exception: {e}'"}, {"type": "code", "name": "Drug Table Structure Coverage", "description": "Verify the Drug Analysis table exists with both models per drug, and required columns are present (flexible names accepted).", "weight": 2.0, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    weight = 2.0\n    try:\n        def find_wb():\n            for res in context.get_all_outputs() or []:\n                if getattr(res, 'is_spreadsheet', False):\n                    return res\n                if getattr(res, 'is_text_format', False) and str(res.name or '').lower().endswith('.csv'):\n                    return res\n            return None\n        wb = find_wb()\n        if not wb:\n            return 0.0, 'No workbook.'\n        path = context.files.get_path(wb.id)\n        data_sheets = {}\n        if str(path).lower().endswith('.xlsx'):\n            xls = pd.ExcelFile(path)\n            for s in xls.sheet_names:\n                try:\n                    df = pd.read_excel(path, sheet_name=s)\n                    data_sheets[s] = df\n                except Exception:\n                    pass\n        else:\n            try:\n                data_sheets['Drug Analysis'] = pd.read_csv(path)\n            except Exception:\n                return 0.0, 'CSV not readable.'\n        # Find the main analysis sheet by name or columns\n        target = None\n        for name, df in data_sheets.items():\n            lname = name.lower()\n            cols = [str(c).lower() for c in df.columns]\n            if any(k in lname for k in ['analysis','drug','result','table','data']) and len(cols) >= 6:\n                target = df\n                break\n        if target is None:\n            # heuristic: sheet with most rows/cols\n            target = max(data_sheets.values(), key=lambda d: d.shape[0]*max(1,d.shape[1]))\n        df = target.copy()\n        df = df.dropna(how='all')\n        cols = [str(c).lower() for c in df.columns]\n        # Required logical columns (fuzzy)\n        req_groups = {\n            'drug':['drug','medication','product','name'],\n            'strength':['strength','dose','mg'],\n            'model':['model','days supply','days','fill'],\n            'patients':['patients','enrolled','count'],\n            'fills_per_year':['fills per year','fills/year','annual fills','fills yr'],\n            'tablets_per_fill':['tablets per fill','qty per fill','quantity per fill','qty'],\n            'drug_cost_fill':['drug cost per fill','medication cost per fill','drug cost/fill','rx cost per fill','cost per fill','fill cost'],\n            'unit_cost':['unit cost','cost per tablet','tablet cost','drug unit cost'],\n            'vial_cost':['vial','supply','container','packaging'],\n            'reimb_per_fill':['reimbursement per fill','reimb/fill','ins reimbursement/fill','reimbursement/fill'],\n            'total_exp_annual':['total expense annual','annual expense','yearly expense','annual cost'],\n            'total_reimb_annual':['total reimbursement annual','annual reimbursement','yearly reimbursement'],\n            'annual_revenue':['annual revenue','profit','net revenue','margin'],\n            'rev_diff':['difference','delta','rev diff','revenue difference','100-90','delta revenue']\n        }\n        def has_any(keywords):\n            for c in cols:\n                for k in keywords:\n                    if all(tok in c for tok in k.split()):\n                        return True\n            return False\n        present = 0\n        keys_to_check = ['drug','strength','model','patients','fills_per_year','tablets_per_fill','vial_cost','reimb_per_fill','total_exp_annual','total_reimb_annual','annual_revenue']\n        for k in keys_to_check:\n            if has_any(req_groups[k]):\n                present += 1\n        # Either drug_cost_fill or unit_cost must be present\n        has_cost_basis = has_any(req_groups['drug_cost_fill']) or has_any(req_groups['unit_cost'])\n        if has_cost_basis:\n            present += 1\n        # Coverage of rows: expect ~20 rows (10 drugs x 2 models). Allow 18-24.\n        rows_ok = 18 <= df.shape[0] <= 24\n        # Scoring: columns presence (up to 1.5), rows coverage (0.5)\n        col_score = 1.5 * (present / (len(keys_to_check) + 1))\n        row_score = 0.5 if rows_ok else 0.0\n        return min(weight, col_score + row_score)\n    except Exception as e:\n        return 0.0, f'Exception: {e}'"}, {"type": "code", "name": "Model-to-Frequency Consistency (90\u21924, 100\u21923) and Tablets per Fill", "description": "Check that rows labeled 90-day have 4 fills/year and ~90 tablets/fill; rows labeled 100-day have 3 fills/year and ~100 tablets/fill.", "weight": 1.5, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    weight = 1.5\n    try:\n        def find_wb():\n            for res in context.get_all_outputs() or []:\n                if getattr(res, 'is_spreadsheet', False):\n                    return res\n                if getattr(res, 'is_text_format', False) and str(res.name or '').lower().endswith('.csv'):\n                    return res\n            return None\n        wb = find_wb()\n        if not wb:\n            return 0.0\n        path = context.files.get_path(wb.id)\n        if str(path).lower().endswith('.xlsx'):\n            xls = pd.ExcelFile(path)\n            # pick likely analysis sheet\n            target_name = None\n            for s in xls.sheet_names:\n                if any(k in s.lower() for k in ['analysis','drug','data','results']):\n                    target_name = s\n                    break\n            if target_name is None:\n                target_name = xls.sheet_names[0]\n            df = pd.read_excel(path, sheet_name=target_name)\n        else:\n            df = pd.read_csv(path)\n        if df is None or df.shape[0] == 0:\n            return 0.0\n        d = df.copy()\n        d.columns = [str(c).lower() for c in d.columns]\n        def col_like(keys):\n            for c in d.columns:\n                for k in keys:\n                    if all(tok in c for tok in k.split()):\n                        return c\n            return None\n        c_model = col_like(['model','days','fill'])\n        c_fills = col_like(['fills per year','fills/year','annual fills','fills yr'])\n        c_qty = col_like(['tablets per fill','qty per fill','quantity per fill','qty'])\n        if not (c_model and c_fills and c_qty):\n            return 0.3  # minimal credit if structure missing\n        total = len(d)\n        ok = 0\n        for _, r in d.iterrows():\n            m = str(r.get(c_model, '')).lower()\n            fy = pd.to_numeric(r.get(c_fills), errors='coerce')\n            q = pd.to_numeric(r.get(c_qty), errors='coerce')\n            if pd.isna(fy) or pd.isna(q):\n                continue\n            if '90' in m or ' 90' in m or '90-day' in m or (q >= 85 and q <= 95):\n                if int(round(fy)) == 4 and 85 <= q <= 95:\n                    ok += 1\n            elif '100' in m or '100-day' in m or (q >= 95 and q <= 105):\n                if int(round(fy)) == 3 and 95 <= q <= 105:\n                    ok += 1\n        ratio = ok / max(1, total)\n        return weight * ratio\n    except Exception as e:\n        return 0.0, f'Exception: {e}'"}, {"type": "code", "name": "Annual Math Consistency (Per-Row)", "description": "Verify annual totals: Total Reimb Annual \u2248 Reimb/Fill * Fills/Year * Patients; Total Expense Annual \u2248 (DrugCost/Fill + VialCost/Fill) * Fills/Year * Patients (or UnitCost*Qty + Vial); Annual Revenue = Reimb \u2212 Expense. Tolerance 2%.", "weight": 2.5, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    weight = 2.5\n    try:\n        def find_wb():\n            for res in context.get_all_outputs() or []:\n                if getattr(res, 'is_spreadsheet', False):\n                    return res\n                if getattr(res, 'is_text_format', False) and str(res.name or '').lower().endswith('.csv'):\n                    return res\n            return None\n        wb = find_wb()\n        if not wb:\n            return 0.0\n        path = context.files.get_path(wb.id)\n        if str(path).lower().endswith('.xlsx'):\n            xls = pd.ExcelFile(path)\n            target_name = None\n            for s in xls.sheet_names:\n                if any(k in s.lower() for k in ['analysis','drug','data','results']):\n                    target_name = s\n                    break\n            if target_name is None:\n                target_name = xls.sheet_names[0]\n            df = pd.read_excel(path, sheet_name=target_name)\n        else:\n            df = pd.read_csv(path)\n        d = df.copy()\n        d = d.dropna(how='all')\n        d.columns = [str(c).lower() for c in d.columns]\n        def col_like(keys):\n            for c in d.columns:\n                for k in keys:\n                    if all(tok in c for tok in k.split()):\n                        return c\n            return None\n        c_pat = col_like(['patients','enrolled','count'])\n        c_fy = col_like(['fills per year','fills/year','annual fills','fills yr'])\n        c_qty = col_like(['tablets per fill','qty per fill','quantity per fill','qty'])\n        c_unit = col_like(['unit cost','cost per tablet','tablet cost','drug unit cost'])\n        c_cost_fill = col_like(['drug cost per fill','medication cost per fill','drug cost/fill','rx cost per fill','cost per fill','fill cost'])\n        c_vial = col_like(['vial','supply','container','packaging'])\n        c_reimb_fill = col_like(['reimbursement per fill','reimb/fill','ins reimbursement/fill','reimbursement/fill'])\n        c_total_exp = col_like(['total expense annual','annual expense','yearly expense','annual cost'])\n        c_total_reimb = col_like(['total reimbursement annual','annual reimbursement','yearly reimbursement'])\n        c_ann_rev = col_like(['annual revenue','profit','net revenue','margin'])\n        if not (c_pat and c_fy and c_reimb_fill and c_total_exp and c_total_reimb and c_ann_rev and c_vial):\n            return 0.6  # partial if structure lacking\n        tol = 0.02\n        n = 0\n        pass_all = 0\n        for _, r in d.iterrows():\n            try:\n                patients = float(pd.to_numeric(r.get(c_pat), errors='coerce'))\n                fy = float(pd.to_numeric(r.get(c_fy), errors='coerce'))\n                reimb_fill = float(pd.to_numeric(r.get(c_reimb_fill), errors='coerce'))\n                vial = float(pd.to_numeric(r.get(c_vial), errors='coerce'))\n                total_exp = float(pd.to_numeric(r.get(c_total_exp), errors='coerce'))\n                total_reimb = float(pd.to_numeric(r.get(c_total_reimb), errors='coerce'))\n                ann_rev = float(pd.to_numeric(r.get(c_ann_rev), errors='coerce'))\n                if any(pd.isna(x) for x in [patients, fy, reimb_fill, vial, total_exp, total_reimb, ann_rev]):\n                    continue\n                # compute cost per fill\n                if c_cost_fill:\n                    dcpf = float(pd.to_numeric(r.get(c_cost_fill), errors='coerce'))\n                    drug_cost_fill = dcpf\n                elif c_unit and c_qty:\n                    unit = float(pd.to_numeric(r.get(c_unit), errors='coerce'))\n                    qty = float(pd.to_numeric(r.get(c_qty), errors='coerce'))\n                    if not pd.isna(unit) and not pd.isna(qty):\n                        drug_cost_fill = unit * qty\n                    else:\n                        continue\n                else:\n                    continue\n                exp_fill = drug_cost_fill + vial\n                exp_ann_exp = exp_fill * fy * patients\n                exp_ann_reimb = reimb_fill * fy * patients\n                exp_ann_rev = exp_ann_reimb - exp_ann_exp\n                # Compare with tolerance (relative tolerance to scale)\n                def close(a, b):\n                    denom = max(1.0, abs(b))\n                    return abs(a - b) <= tol * denom\n                all_ok = close(total_reimb, exp_ann_reimb) and close(total_exp, exp_ann_exp) and close(ann_rev, exp_ann_rev)\n                n += 1\n                if all_ok:\n                    pass_all += 1\n            except Exception:\n                continue\n        if n == 0:\n            return 0.6\n        ratio = pass_all / n\n        return weight * ratio\n    except Exception as e:\n        return 0.0, f'Exception: {e}'"}, {"type": "code", "name": "Coverage: 10 Medications, Both Models", "description": "Ensure at least 10 unique drug+strength combinations exist and each has both 90 and 100 day models.", "weight": 1.5, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    weight = 1.5\n    try:\n        def find_wb():\n            for res in context.get_all_outputs() or []:\n                if getattr(res, 'is_spreadsheet', False):\n                    return res\n                if getattr(res, 'is_text_format', False) and str(res.name or '').lower().endswith('.csv'):\n                    return res\n            return None\n        wb = find_wb()\n        if not wb:\n            return 0.0\n        path = context.files.get_path(wb.id)\n        if str(path).lower().endswith('.xlsx'):\n            xls = pd.ExcelFile(path)\n            target_name = None\n            for s in xls.sheet_names:\n                if any(k in s.lower() for k in ['analysis','drug','data','results']):\n                    target_name = s\n                    break\n            if target_name is None:\n                target_name = xls.sheet_names[0]\n            df = pd.read_excel(path, sheet_name=target_name)\n        else:\n            df = pd.read_csv(path)\n        d = df.dropna(how='all').copy()\n        d.columns = [str(c).lower() for c in d.columns]\n        def col_like(keys):\n            for c in d.columns:\n                for k in keys:\n                    if all(tok in c for tok in k.split()):\n                        return c\n            return None\n        c_drug = col_like(['drug','medication','product','name'])\n        c_strength = col_like(['strength','dose','mg'])\n        c_model = col_like(['model','days supply','days','fill'])\n        if not (c_drug and c_strength and c_model):\n            return 0.5\n        # Normalize models as 90 vs 100 by inspecting text/numbers\n        key = d[[c_drug, c_strength]].astype(str).agg(' | '.join, axis=1)\n        models = []\n        for _, r in d.iterrows():\n            m = str(r.get(c_model, '')).lower()\n            qty = None\n            # qty as backup\n            for c in d.columns:\n                if 'qty' in c or 'tablets per fill' in c:\n                    qty = pd.to_numeric(r.get(c), errors='coerce')\n                    break\n            if '90' in m or (qty is not None and 85 <= qty <= 95):\n                models.append('90')\n            elif '100' in m or (qty is not None and 95 <= qty <= 105):\n                models.append('100')\n            else:\n                models.append('other')\n        tmp = d.copy()\n        tmp['__key__'] = key\n        tmp['__model__'] = models\n        uniq = tmp['__key__'].nunique()\n        # Score components\n        comp1 = 0.6 if uniq >= 10 else 0.0\n        # Both models per key\n        bykey = tmp.groupby('__key__')['__model__'].apply(lambda x: set(x))\n        has_both = sum(1 for s in bykey if '90' in s and '100' in s)\n        comp2 = 0.9 * (has_both / max(1, uniq))\n        return min(weight, comp1 + comp2)\n    except Exception as e:\n        return 0.0, f'Exception: {e}'"}, {"type": "code", "name": "Decision Matches Threshold Logic", "description": "Compute total revenue difference (100 \u2212 90) from the workbook and verify the stated final recommendation aligns with the 2%/$16,000 rule. If no explicit recommendation text is found, partial credit.", "weight": 1.5, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    weight = 1.5\n    try:\n        # Find workbook and optional PDF/DOCX\n        wb = None\n        doc = None\n        for res in context.get_all_outputs() or []:\n            if wb is None and getattr(res, 'is_spreadsheet', False):\n                wb = res\n            if doc is None and getattr(res, 'is_document', False):\n                doc = res\n        if wb is None:\n            return 0.0, 'No workbook.'\n        # Read assumptions (threshold)\n        path = context.files.get_path(wb.id)\n        sheets = {}\n        if str(path).lower().endswith('.xlsx'):\n            xls = pd.ExcelFile(path)\n            for s in xls.sheet_names:\n                try:\n                    sheets[s] = pd.read_excel(path, sheet_name=s)\n                except Exception:\n                    pass\n        else:\n            try:\n                sheets['Drug Analysis'] = pd.read_csv(path)\n            except Exception:\n                return 0.0, 'CSV not readable.'\n        # defaults\n        total_rev_base = 800000.0\n        thr_abs = 16000.0\n        thr_pct = 0.02\n        # Try to get from assumptions\n        for name, df in sheets.items():\n            if any(k in name.lower() for k in ['assumption','input','parameter','config']):\n                dfa = df.dropna(how='all').copy()\n                for _, row in dfa.iterrows():\n                    try:\n                        k = str(row.iloc[0]).lower()\n                        v = row.iloc[1] if row.shape[0] > 1 else None\n                        if 'annual' in k and 'revenue' in k:\n                            x = pd.to_numeric(str(v).replace(',','').replace('$',''), errors='coerce')\n                            if pd.notna(x): total_rev_base = float(x)\n                        if 'threshold' in k and ('%' in k or 'percent' in k or 'percentage' in k):\n                            x = pd.to_numeric(str(v).replace('%',''), errors='coerce')\n                            if pd.notna(x): thr_pct = float(x)/100.0 if float(x)>1 else float(x)\n                        if 'threshold' in k and any(t in k for t in ['absolute','amount','usd']):\n                            x = pd.to_numeric(str(v).replace(',','').replace('$',''), errors='coerce')\n                            if pd.notna(x): thr_abs = float(x)\n                    except Exception:\n                        pass\n        if thr_abs is None and total_rev_base is not None and thr_pct is not None:\n            thr_abs = total_rev_base * thr_pct\n        # Find analysis sheet\n        analysis = None\n        for name, df in sheets.items():\n            if any(k in name.lower() for k in ['analysis','drug','data','results']):\n                analysis = df\n                break\n        if analysis is None:\n            analysis = list(sheets.values())[0]\n        d = analysis.dropna(how='all').copy()\n        d.columns = [str(c).lower() for c in d.columns]\n        def col_like(keys):\n            for c in d.columns:\n                for k in keys:\n                    if all(tok in c for tok in k.split()):\n                        return c\n            return None\n        c_drug = col_like(['drug','medication','product','name'])\n        c_strength = col_like(['strength','dose','mg'])\n        c_model = col_like(['model','days supply','days','fill'])\n        c_ann_rev = col_like(['annual revenue','profit','net revenue','margin'])\n        if not (c_drug and c_strength and c_model and c_ann_rev):\n            return 0.3\n        # Normalize keys and models\n        d['__key__'] = d[[c_drug, c_strength]].astype(str).agg(' | '.join, axis=1)\n        # models via text and quantity fallback\n        c_qty = col_like(['tablets per fill','qty per fill','quantity per fill','qty'])\n        models = []\n        for _, r in d.iterrows():\n            m = str(r.get(c_model, '')).lower()\n            q = pd.to_numeric(r.get(c_qty), errors='coerce') if c_qty else np.nan\n            if '90' in m or (pd.notna(q) and 85 <= q <= 95):\n                models.append('90')\n            elif '100' in m or (pd.notna(q) and 95 <= q <= 105):\n                models.append('100')\n            else:\n                models.append('other')\n        d['__model__'] = models\n        # Aggregate revenue by model\n        g = d.pivot_table(index='__key__', columns='__model__', values=c_ann_rev, aggfunc='sum')\n        # Sum across keys\n        rev90 = float(g.get('90', pd.Series([0.0])).sum())\n        rev100 = float(g.get('100', pd.Series([0.0])).sum())\n        diff = rev100 - rev90\n        # Extract declared recommendation from Summary sheet or document text\n        rec_text = ''\n        for name, df in sheets.items():\n            if any(k in name.lower() for k in ['summary','overview','decision']):\n                try:\n                    txt = ' '.join(df.astype(str).fillna('').values.ravel().tolist()).lower()\n                    rec_text += ' ' + txt\n                except Exception:\n                    pass\n        if not rec_text and doc is not None:\n            try:\n                if str(doc.name or '').lower().endswith('.pdf'):\n                    rec_text = (context.files.read_pdf_text(doc.id) or '').lower()\n                else:\n                    rec_text = (context.files.read_docx_text(doc.id) or '').lower()\n            except Exception:\n                rec_text = ''\n        # Determine expected decision\n        expected_switch = abs(diff) < float(thr_abs)\n        # Interpret declared\n        declared_switch = None\n        if rec_text:\n            if 'switch' in rec_text or '100-day' in rec_text or '100 day' in rec_text:\n                # look for maintain language\n                if any(w in rec_text for w in ['maintain 90', 'keep 90', 'stay with 90', 'do not switch']):\n                    # conflicting, leave None -> partial\n                    declared_switch = None\n                else:\n                    # If it says recommend implement 100-day, treat as switch\n                    if any(w in rec_text for w in ['recommend', 'recommending', 'we will', 'we should', 'decision']):\n                        declared_switch = True\n            if any(w in rec_text for w in ['maintain 90','keep 90','stay with 90','retain 90','continue 90']):\n                declared_switch = False\n        # Scoring\n        if declared_switch is None:\n            # No clear declaration found\n            return 0.8 if expected_switch is not None else 0.3\n        else:\n            return weight if declared_switch == expected_switch else 0.3\n    except Exception as e:\n        return 0.0, f'Exception: {e}'"}, {"type": "code", "name": "Per-Drug Revenue Difference Visibility", "description": "Check if a per-drug revenue difference (100\u221290) is present either as a column or in a summary table.", "weight": 1.0, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    weight = 1.0\n    try:\n        def find_wb():\n            for res in context.get_all_outputs() or []:\n                if getattr(res, 'is_spreadsheet', False):\n                    return res\n                if getattr(res, 'is_text_format', False) and str(res.name or '').lower().endswith('.csv'):\n                    return res\n            return None\n        wb = find_wb()\n        if not wb:\n            return 0.0\n        path = context.files.get_path(wb.id)\n        sheets = {}\n        if str(path).lower().endswith('.xlsx'):\n            xls = pd.ExcelFile(path)\n            for s in xls.sheet_names:\n                try:\n                    sheets[s] = pd.read_excel(path, sheet_name=s)\n                except Exception:\n                    pass\n        else:\n            try:\n                sheets['Drug Analysis'] = pd.read_csv(path)\n            except Exception:\n                return 0.0\n        found = False\n        for name, df in sheets.items():\n            cols = [str(c).lower() for c in df.columns]\n            for c in cols:\n                if any(all(tok in c for tok in k.split()) for k in ['revenue difference','rev diff','difference','delta','100-90']):\n                    found = True\n                    break\n            if found:\n                break\n        if found:\n            return weight\n        # Fallback: detect a summary with per-drug diffs embedded in text\n        for name, df in sheets.items():\n            if any(k in name.lower() for k in ['summary','overview','decision']):\n                txt = ' '.join(df.astype(str).fillna('').values.ravel().tolist()).lower()\n                if any(w in txt for w in ['difference','delta','100-90','100 \u2013 90','100 minus 90']):\n                    return 0.6\n        return 0.2\n    except Exception as e:\n        return 0.0, f'Exception: {e}'"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Presentation and Professional Quality (LLM)", "description": "Evaluate the PDF/DOCX report for professionalism, clarity, and audience appropriateness. Confirm that the threshold is explicitly applied and the recommendation is clearly stated for an owner-operator decision.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Report Format and Section Completeness", "description": "PDF/DOCX report quality: 1\u20132 pages, clear sections, visually readable table, and an explicit recommendation.", "weight": 3.0, "judge_prompt": "Assess the PDF/DOCX report for professional quality and completeness. Requirements:\n- Length 1\u20132 pages\n- Clear section headers\n- Contains Executive Summary / Summary of Findings with a clear final recommendation\n- Contains Methodology & Assumptions explicitly mentioning: 300 patients/med, 1 tablet/day, 90\u21924 vs 100\u21923 fills/year, and the 2% of $800,000 ($16,000) threshold\n- Contains a Comparative Results Table that is visually readable\n\nScoring (3 points):\n- 3.0: All above present; professional formatting and clear layout\n- 2.0: One minor element missing or somewhat weak formatting\n- 1.0: Multiple elements missing but still coherent\n- 0.0: Poor format or missing key sections\n\nFocus on presentation and completeness, not numerical correctness.", "expectation": "A crisp, owner-ready 1\u20132 page report with required sections and an explicit recommendation."}, {"type": "llm_judge", "name": "Comparative Table Readability and Relevance", "description": "Judge if the comparative table clearly contrasts 90-day vs 100-day across all 10 drugs with relevant financial fields.", "weight": 1.5, "judge_prompt": "Evaluate the primary comparative table in the report:\n- Does it clearly show both 90-day and 100-day models for each of the 10 medications?\n- Are key fields visible: patients, fills per year, tablets per fill, costs, reimbursements, annual revenue, and revenue difference?\n- Is it legible and appropriately labeled?\n\nScoring (1.5 points):\n- 1.5: Table is clear, includes both models and key fields for all medications\n- 1.0: Minor omissions but overall readable and comparable\n- 0.5: Significant omissions or readability issues\n- 0.0: No usable table present\n\nDo not check math; only readability and completeness.", "expectation": "A legible comparative table with both models and key cost/revenue fields for all 10 drugs."}, {"type": "llm_judge", "name": "Clarity of Threshold Application and Rationale", "description": "Judge whether the report explicitly applies the 2% ($16,000) threshold to the computed revenue difference and states a clear, defensible recommendation with brief rationale.", "weight": 1.5, "judge_prompt": "Check the text for decisiveness and threshold application:\n- Does it explicitly apply the 2% of $800,000 threshold ($16,000) to the computed revenue difference?\n- Does it clearly say whether to switch to 100-day fills or maintain 90-day fills?\n- Does it briefly note that adherence/operational benefits may outweigh small revenue differences if under threshold?\n\nScoring (1.5 points):\n- 1.5: All elements present and clearly articulated\n- 1.0: Mostly present, some vagueness\n- 0.5: Weak or implicit threshold application\n- 0.0: No clear recommendation or threshold discussion\n\nAgain, do not grade numerical correctness here\u2014only clarity and rationale.", "expectation": "A clear statement of the decision tied to the $16,000 threshold and brief rationale."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "a10ec48c-168e-476c-8fe3-23b2a5f616ac", "rubric": {"category_name": "Concierge Restaurant Recommendations (Sarasota Downtown)", "rationale": "Task Type: Pattern B (Document). Output is a Word/PDF document intended for concierge use. Stage 1 uses an LLM judge to strictly enforce the required document and table structure so later checks are trivial. Stage 2 uses code rules to verify deterministic textual elements (headers, address, categories, hours patterns, and citations) made possible by Stage 1\u2019s structure. Stage 3 uses an LLM judge for holistic quality suited to luxury residential concierge needs.", "max_total_score": 11.0, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate", "description": "Gate: Verify the document is a properly structured Word/PDF with the mandated sections, titles, tables, and columns that enable verification.", "is_required": true, "max_points": 5.0, "min_score_to_pass": 3.5, "rules": [{"type": "llm_judge", "name": "Structured Document and Tables Requirement", "description": "Validate the candidate output has the exact structure needed for verification, using flexible matching for names while ensuring all required elements are present.", "weight": 5.0, "judge_prompt": "You are evaluating whether the submitted file is a valid DOCX or PDF document that follows these structural requirements. Only check PRESENCE and STRUCTURE, not content accuracy.\n\nRequired overall format:\n- File must be a DOCX or PDF (not Excel/CSV/plain text).\n- Document must include a clear main title near the top: \"Concierge Local Restaurant Recommendations (Sarasota Downtown)\" (minor variations in capitalization or punctuation are acceptable).\n- Include a short introductory passage (at least 3 sentences) explaining purpose, audience (luxury residential concierge/residents), scope (Downtown Sarasota, FL), and referencing data sources (\"downtownsarasota.com\" and \"Google Maps\").\n\nTables requirement:\n- One or more tables titled exactly: \"Sarasota Downtown Restaurant Recommendations\".\n- Each such table must include an immediately visible cuisine subtitle (e.g., \"American/Continental\", \"Asian\", \"Italian\", \"Seafood\", \"Mexican/Latin\", \"Mediterranean\", \"Steakhouse\", \"Vegan/Vegetarian\", \"Cafe/Bakery\", or similar). Subtitle can appear immediately above or below the table title.\n- Each table must have the following 5 column headers (exact names):\n  1) \"Restaurant Name\"\n  2) \"Business Hours\"\n  3) \"Description\"\n  4) \"Directions\"\n  5) \"Category\"\n- Each table should contain at least 3 data rows.\n- Across the entire document, there should be at least 8 distinct restaurants in total (sum of all tables).\n\nPer-cell expectations (STRUCTURE only):\n- In the \"Restaurant Name\" column, the visible text should be the restaurant name and be formatted as a hyperlink to the restaurant\u2019s website (judge visually if links are present; do not test the URLs).\n- In the \"Directions\" column, text should reference the origin \"1991 Main Street, Sarasota, Florida 34236\" (minor abbreviations like \"1991 Main St\" are fine) and describe how to get there.\n- In the \"Category\" column, values should come from the allowed set: {Quick Service, Fast Casual, Casual Dining, Family Style, Upscale Casual, Fine Dining, Michelin-Starred, Pop-Up/Concept}. Allow close matches like \"Pop-Up/Concept\" vs \"Pop-Up / Concept\".\n\nSourcing notes:\n- Document should include a visible \"Sources\" or \"Notes\" area mentioning: (1) downtownsarasota.com (the restaurant list source) and (2) Google Maps (additional info).\n\nScoring (structure only):\n- 5.0: Valid DOCX/PDF, clear title and intro, at least 2 cuisine-subtitled tables, each with all 5 required column headers, at least 8 total restaurants, restaurant names visibly hyperlinked, directions reference the origin address, categories use the allowed set, and sources section cites both downtownsarasota.com and Google Maps.\n- 4.0: Valid format with all core elements except ONE minor omission (e.g., only one cuisine table but with 8+ rows total; or sources cite only one of the two; or hyperlinks visible for most but not all rows).\n- 2.0: Valid format but missing multiple required structural elements (e.g., no cuisine subtitles, wrong/missing column headers, too few rows, missing title or intro).\n- 0.0: Not a DOCX/PDF or lacks the table-based structure entirely.\n\nOnly assess the presence of structure and formatting elements. Do NOT judge factual correctness, up-to-dateness, or whether restaurants are open/closed at this stage.", "expectation": "A professional DOCX/PDF with the specified title, intro, cuisine-subtitled recommendation tables using the exact 5 columns, visible hyperlinks in Restaurant Name cells, directions referencing 1991 Main Street, valid category labels, and a sources note citing downtownsarasota.com and Google Maps."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Deterministic Correctness and Consistency Checks", "description": "Now that the document has the mandated shape, verify deterministic textual and structural signals: headers, address origin, category labels, hours patterns, and citations.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Headers and Titles Present", "description": "Confirm title string, table title, and the 5 required column names appear in extracted text (flexible, case-insensitive).", "weight": 1.5, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_document:\\n            return 0.0, 'No document output'\\n\\n        # Read text from DOCX or PDF\\n        text = ''\\n        try:\\n            if str(output.mime_type).lower().endswith('word') or (hasattr(output, 'extension') and str(output.extension).lower() == '.docx') or 'docx' in str(output.mime_type).lower():\\n                text = context.files.read_docx_text(output.id) or ''\\n            else:\\n                text = context.files.read_pdf_text(output.id) or ''\\n        except Exception:\\n            # Fallback attempts\\n            try:\\n                text = context.files.read_pdf_text(output.id) or ''\\n            except Exception:\\n                try:\\n                    text = context.files.read_docx_text(output.id) or ''\\n                except Exception:\\n                    text = ''\\n        if not text:\\n            return 0.0, 'No extractable text'\\n\\n        low = text.lower()\\n        score_parts = 0\\n        total_parts = 3\\n\\n        # Title presence (flexible)\\n        title_ok = ('concierge local restaurant recommendations' in low and 'sarasota downtown' in low)\\n        if title_ok:\\n            score_parts += 1\\n\\n        # Table title presence\\n        table_title_ok = 'sarasota downtown restaurant recommendations' in low\\n        if table_title_ok:\\n            score_parts += 1\\n\\n        # Column headers presence (flexible, require at least 4 of 5 to accommodate extraction losses)\\n        headers = ['restaurant name', 'business hours', 'description', 'directions', 'category']\\n        found = sum(1 for h in headers if h in low)\\n        if found >= 4:\\n            score_parts += 1\\n\\n        return score_parts / total_parts, f'Headers score: {score_parts}/{total_parts}'\\n    except Exception as e:\\n        return 0.0, f'Error: {e}'"}, {"type": "code", "name": "Origin Address and Directions Signal", "description": "Verify the origin address and direction phrasing exist.", "weight": 0.8, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_document:\\n            return 0.0\\n        # Extract text\\n        text = ''\\n        try:\\n            text = context.files.read_docx_text(output.id) or ''\\n        except Exception:\\n            try:\\n                text = context.files.read_pdf_text(output.id) or ''\\n            except Exception:\\n                text = ''\\n        low = text.lower()\\n        if not low:\\n            return 0.0\\n\\n        addr_ok = re.search(r'1991\\s+main\\s+st(reet)?', low) and ('sarasota' in low) and ('34236' in low or 'fl' in low)\\n        # Check presence of direction verbs near address words\\n        dir_phrases = ['from', 'head', 'walk', 'drive', 'via', 'east', 'west', 'north', 'south', 'turn', 'mile', 'minutes']\\n        dir_ok = any(p in low for p in dir_phrases)\\n\\n        score = 0.0\\n        if addr_ok and dir_ok:\\n            score = 1.0\\n        elif addr_ok or dir_ok:\\n            score = 0.6\\n        else:\\n            score = 0.0\\n        return score\\n    except Exception:\\n        return 0.0"}, {"type": "code", "name": "Allowed Category Labels Presence", "description": "Confirm the document uses the allowed set of category labels; award more credit for broader coverage.", "weight": 1.0, "code": "def evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_document:\\n            return 0.0\\n        try:\\n            text = context.files.read_docx_text(output.id) or ''\\n        except Exception:\\n            try:\\n                text = context.files.read_pdf_text(output.id) or ''\\n            except Exception:\\n                text = ''\\n        low = text.lower()\\n        allowed = [\\n            'quick service','fast casual','casual dining','family style','upscale casual','fine dining','michelin-starred','pop-up/concept','pop-up / concept','pop-up', 'concept'\\n        ]\\n        hits = 0\\n        for a in allowed:\\n            if a in low:\\n                hits += 1\\n        # Normalize: at least 3 distinct labels -> full credit; 2 -> 0.7; 1 -> 0.4; else 0\\n        if hits >= 3:\\n            return 1.0\\n        elif hits == 2:\\n            return 0.7\\n        elif hits == 1:\\n            return 0.4\\n        else:\\n            return 0.0\\n    except Exception:\\n        return 0.0"}, {"type": "code", "name": "Sources Cited and Hours Pattern Check", "description": "Check that downtownsarasota.com and Google Maps are cited, and that business hours-like time patterns appear.", "weight": 0.7, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_document:\\n            return 0.0\\n        try:\\n            text = context.files.read_docx_text(output.id) or ''\\n        except Exception:\\n            try:\\n                text = context.files.read_pdf_text(output.id) or ''\\n            except Exception:\\n                text = ''\\n        low = text.lower()\\n        # Sources presence\\n        src1 = 'downtownsarasota.com' in low\\n        src2 = 'google maps' in low\\n        src_score = 0.0\\n        if src1 and src2:\\n            src_score = 1.0\\n        elif src1 or src2:\\n            src_score = 0.6\\n        else:\\n            src_score = 0.0\\n        # Business hours pattern (look for day + time with am/pm)\\n        pattern = re.compile(r'(mon|monday|tue|tues|tuesday|wed|weds|wednesday|thu|thurs|thursday|fri|friday|sat|saturday|sun|sunday)[^\\n]{0,40}?\\b(\\d{1,2}(:\\d{2})?\\s*[ap]\\.?.?m\\.?)(\\s*-\\s*\\d{1,2}(:\\d{2})?\\s*[ap]\\.?.?m\\.?)?', re.IGNORECASE)\\n        matches = pattern.findall(text) if text else []\\n        hours_score = 1.0 if len(matches) >= 3 else (0.6 if len(matches) >= 1 else 0.0)\\n        # Combine: 60% weight sources, 40% weight hours in this rule\\n        combined = 0.6 * src_score + 0.4 * hours_score\\n        return combined\\n    except Exception:\\n        return 0.0"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Concierge Usefulness", "description": "Holistic assessment of clarity, organization, and usefulness for a luxury residential concierge audience.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professionalism and Usefulness", "description": "Assess writing quality, organization, and practical usefulness for concierge interactions.", "weight": 2.0, "judge_prompt": "Evaluate the document for overall professional quality and usefulness to a luxury residential concierge serving residents. Consider:\n- Clarity and concision of the introduction and any guidance notes (e.g., disclaimers like \"verified as of [date]\").\n- Organization and readability of cuisine-subtitled tables; consistent use of the 5 required columns.\n- Actionability of Directions (are they specific and understandable for guests from 1991 Main Street?).\n- Appropriateness and consistency of Category assignments (e.g., Fine Dining vs Upscale Casual) for a discerning audience.\n- Coverage breadth (multiple cuisines, balanced selection) and curation (avoid obviously closed venues; if closures are noted, they are excluded and/or a data freshness note is present).\n- Overall polish suitable for a luxury property (tone, formatting, minimal typos).\n\nScoring:\n- 2.0: Excellent presentation; tables are easy to use; directions are actionable; categories appropriate; breadth and curation are strong; tone is polished.\n- 1.4: Good overall with minor issues (e.g., small inconsistencies or minor clarity/style issues).\n- 0.8: Adequate but with noticeable shortcomings in clarity, organization, or usefulness.\n- 0.0: Poor quality; hard to use; unclear or unprofessional.\n\nDo not re-check structural presence already covered in Stage 1; focus on quality and usefulness.", "expectation": "A polished, easy-to-use concierge-ready document with actionable directions, accurate-feeling categorization, balanced cuisine coverage, and refined presentation for luxury residents."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "c94452e4-39cd-4846-b73a-ab75933d1ad7", "rubric": {"category_name": "Broadcast Spot \u2014 \u201cCare Not Cutbacks\u201d (15s) \u2014 Video Edit and Self-Documentation", "rationale": "Task Type: Mixed (Pattern C). Primary deliverable is an MP4 broadcast spot; verification requires structured, file-based artifacts (shot list, cue sheet, tech specs, editor packet). Stage 1 forces a strict, self-documenting shape to make Stage 2 verification trivial. Stage 2 mixes code rules (timings, coverage, specs, licensing) and an LLM cross-check against the Editor Packet. Stage 3 assesses creative quality and broadcast readiness.", "max_total_score": 12.0, "stages": [{"name": "Stage 1 \u2014 Format and Shape Gate (LLM)", "description": "Gate check: Candidate must provide the video and the self-documenting artifact set with exact structure so verification is possible.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Required Deliverables and Structure Present", "description": "Confirms presence of MP4 and required supporting artifacts with specified section/column structures. Only verify presence/structure, not correctness.", "weight": 4.0, "judge_prompt": "You are evaluating whether the submission includes the required outputs in the correct, verifiable shape. Consider ALL files provided.\n\nRequired outputs (ALL must be present for full credit):\n1) Final Spot: An H.264 .mp4 video export intended for broadcast review.\n   - Target: 1920x1080, exactly 15 seconds. Do NOT verify specs here; just confirm an .mp4 exists as the final spot file.\n\n2) Editor Packet (PDF or DOCX) \u2014 a professionally formatted document with these clearly labeled sections/headers:\n   A. Overview (objective, creative approach, tone) \n   B. Timeline Summary or Shot Plan (high-level sequence explanation)\n   C. Supers Plan (list the exact supers used, in order of appearance)\n   D. Music Plan or Cue Sheet (describe chosen track, source, and edit approach, beginning/ending)\n   E. Export Specs (claimed technical specs of the delivered MP4)\n   F. QC Checklist (bullet list confirming: initial shot has no super; each super placed on a unique shot; supers displayed long enough to read; overall duration claim = 15s)\n   G. Sources & Licensing (footage/music providers and URLs; note on watermarks acceptable for review)\n\n3) Shot List file (CSV or XLSX) \u2014 must be a data table with columns that cover at minimum:\n   - Shot number/ID\n   - Start timecode or seconds\n   - End timecode or seconds (or an explicit duration)\n   - Super text for the shot (blank/none for the first shot)\n   Flexible column names are allowed (e.g., Start/TC In, End/TC Out, Duration_sec, Super, etc.).\n\n4) Music Cues file (CSV or XLSX) \u2014 must be a data table with at minimum:\n   - Cue start time\n   - Cue end time\n   - Fade in/out durations (or notes about how beginning/ending are shaped)\n   - Track title and source/provider or URL\n   Flexible column names are allowed (e.g., Start, End, FadeIn_sec, FadeOut_sec, Title, Source_URL).\n\n5) Tech Specs JSON (.json) \u2014 machine-readable export summary that claims:\n   - duration_sec\n   - width and height (or resolution string like 1920x1080)\n   - codec (should claim H.264)\n   - fps\n   - audio_sample_rate and audio_channels\n\nOptional (nice to have; do not penalize if missing):\n6) Thumbnail Contact Sheet image (PNG/JPG) \u2014 e.g., 1 frame per second or per shot.\n\nScoring guidance (STRUCTURE ONLY \u2014 do not judge content quality or calculation correctness):\n- 4.0: MP4 present + Editor Packet with all 7 sections + Shot List (CSV/XLSX) present + Music Cues (CSV/XLSX) present + Tech Specs JSON present. Optional contact sheet may be present.\n- 3.5: Missing only the optional contact sheet (if everything else is present) \u2014 still give 4.0; otherwise if any required item is slightly incomplete (e.g., Editor Packet missing 1 minor subsection), 3.5 is acceptable.\n- 3.0: Exactly one required item missing or Editor Packet missing 2 subsections.\n- 2.0: Two required items missing or Editor Packet missing 3+ subsections.\n- 0.0: No MP4, or clearly wrong format (e.g., only text), or multiple required items missing making verification impossible.\n\nBe flexible with exact section/column names as long as the structure is clearly present and labeled. Do NOT evaluate correctness of timings, specs, or effectiveness; only presence and structural completeness.", "expectation": "A complete, self-documenting package: MP4 + Editor Packet (with all sections) + Shot List (CSV/XLSX) + Music Cues (CSV/XLSX) + Tech Specs JSON. Optional contact sheet."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification and Compliance", "description": "Now that the structure exists, verify technical compliance, timing integrity, music coverage, color/pacing adjustments, and licensing using the self-documenting artifacts. Mix of code checks and an LLM cross-check against the Editor Packet.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Tech Specs JSON Validity", "description": "Validate duration, resolution, codec, and audio basics using the provided Tech Specs JSON and presence of an MP4.", "weight": 1.2, "code": "import json\nimport re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    # Weight: 1.2\n    weight = 1.2\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0\n\n    # Find MP4 and Tech Specs JSON\n    mp4 = None\n    tech = None\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            name = str(p.name).lower()\n        except Exception:\n            continue\n        if name.endswith('.mp4'):\n            mp4 = r\n        if name.endswith('.json') and any(k in name for k in ['tech', 'spec', 'export']):\n            tech = r\n\n    score = 0.0\n    feedback_parts = []\n\n    if mp4 is None:\n        feedback_parts.append('No MP4 file found.')\n    else:\n        feedback_parts.append('MP4 present.')\n        # small credit for having mp4 at all\n        score += 0.1\n\n    if tech is None:\n        feedback_parts.append('No Tech Specs JSON found.')\n        return (min(score, weight), '; '.join(feedback_parts))\n\n    # Read Tech Specs JSON\n    try:\n        txt = context.files.read_text(tech.id)\n        data = json.loads(txt)\n    except Exception as e:\n        feedback_parts.append(f'Tech Specs JSON unreadable: {e}')\n        return (min(score, weight), '; '.join(feedback_parts))\n\n    # Helper getters\n    def get_num(keys, default=None):\n        for k in keys:\n            if k in data and isinstance(data[k], (int, float)):\n                return float(data[k])\n        return default\n\n    def get_str(keys):\n        for k in keys:\n            if k in data and isinstance(data[k], str):\n                return data[k]\n        return None\n\n    # Checks and partial scoring\n    # Duration\n    dur = get_num(['duration_sec', 'duration'])\n    if dur is not None:\n        if 14.95 <= dur <= 15.05:\n            score += 0.5\n            feedback_parts.append(f'Duration OK ({dur:.3f}s).')\n        else:\n            feedback_parts.append(f'Duration out of range ({dur:.3f}s).')\n    else:\n        feedback_parts.append('Duration missing.')\n\n    # Resolution\n    width = get_num(['width'])\n    height = get_num(['height'])\n    res = get_str(['resolution'])\n    res_ok = False\n    if width and height:\n        if int(width) == 1920 and int(height) == 1080:\n            res_ok = True\n    elif res:\n        if re.match(r'\\b1920\\s*x\\s*1080\\b', res.replace(' ', '')):\n            res_ok = True\n    if res_ok:\n        score += 0.25\n        feedback_parts.append('Resolution OK (1920x1080).')\n    else:\n        feedback_parts.append('Resolution not confirmed as 1920x1080.')\n\n    # Codec\n    codec = get_str(['codec', 'video_codec'])\n    if codec and ('264' in codec.lower() or 'h.264' in codec.lower() or 'avc' in codec.lower()):\n        score += 0.15\n        feedback_parts.append(f'Codec claims H.264 ({codec}).')\n    else:\n        feedback_parts.append('Codec not confirmed as H.264.')\n\n    # Audio basics\n    sr = get_num(['audio_sample_rate', 'audio_rate_hz'])\n    ch = get_num(['audio_channels', 'channels'])\n    if sr and (abs(sr-44100) < 100 or abs(sr-48000) < 100):\n        score += 0.1\n        feedback_parts.append(f'Audio sample rate plausible ({sr}).')\n    else:\n        feedback_parts.append('Audio sample rate missing or implausible.')\n\n    if ch and (int(round(ch)) in [1,2]):\n        score += 0.1\n        feedback_parts.append(f'Audio channels plausible ({int(round(ch))}).')\n    else:\n        feedback_parts.append('Audio channels missing or implausible.')\n\n    return (min(score, weight), '; '.join(feedback_parts))"}, {"type": "code", "name": "Shot List Integrity and Super Placement", "description": "Validate that the shot list durations total ~15s, the first shot has no super, each subsequent shot has a super, and each super has adequate on-screen time.", "weight": 2.4, "code": "import json\nimport re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    # Weight: 2.4\n    weight = 2.4\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0\n\n    # Find shot list file (CSV or XLSX)\n    shot_res = None\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            name = str(p.name).lower()\n        except Exception:\n            continue\n        if (name.endswith('.csv') or name.endswith('.xlsx')) and any(k in name for k in ['shot', 'edl', 'timeline']):\n            shot_res = r\n            break\n    if shot_res is None:\n        return 0.0\n\n    # Read as DataFrame\n    try:\n        if str(context.files.get_path(shot_res.id)).lower().endswith('.csv'):\n            df = context.files.read_csv(shot_res.id)\n        else:\n            # Try first sheet\n            df = context.files.read_excel(shot_res.id)\n    except Exception:\n        return 0.0\n\n    # Normalize columns\n    cols = {c.lower().strip(): c for c in df.columns}\n    def col(*names):\n        for n in names:\n            if n in cols:\n                return cols[n]\n        return None\n\n    c_shot = col('shot', 'shot_number', 'shot id', 'id', '#')\n    c_start = col('start', 'tc in', 'tc_in', 'in', 'start_time')\n    c_end = col('end', 'tc out', 'tc_out', 'out', 'end_time')\n    c_dur = col('duration_sec', 'duration (s)', 'duration', 'dur_sec')\n    c_super = col('super', 'super_text', 'title', 'graphic', 'caption')\n    c_speed = col('speed_pct', 'speed percent', 'speed', 'speed%')\n    c_desat = col('color_desat_pct', 'desat', 'desaturation', 'saturation_change_pct')\n\n    if c_shot is None or (c_dur is None and (c_start is None or c_end is None)) or c_super is None:\n        # Not enough structure to verify\n        return 0.0\n\n    # Time parsing\n    def parse_time(val):\n        if pd.isna(val):\n            return None\n        if isinstance(val, (int, float)):\n            return float(val)\n        s = str(val).strip()\n        if re.match(r'^\\d+(\\.\\d+)?$', s):\n            return float(s)\n        parts = s.split(':')\n        try:\n            parts = [float(p) for p in parts]\n        except Exception:\n            return None\n        if len(parts) == 3:\n            h, m, sec = parts\n            return h*3600 + m*60 + sec\n        if len(parts) == 2:\n            m, sec = parts\n            return m*60 + sec\n        if len(parts) == 1:\n            return float(parts[0])\n        return None\n\n    # Build per-shot durations\n    starts = df[c_start].apply(parse_time) if c_start else pd.Series([None]*len(df))\n    ends = df[c_end].apply(parse_time) if c_end else pd.Series([None]*len(df))\n    durs = df[c_dur].astype(float, errors='ignore') if c_dur else None\n\n    durations = []\n    for i in range(len(df)):\n        if c_dur:\n            try:\n                d = float(df.iloc[i][c_dur])\n            except Exception:\n                d = None\n        else:\n            s = starts.iloc[i]\n            e = ends.iloc[i]\n            d = (e - s) if (s is not None and e is not None) else None\n        durations.append(d)\n\n    # Compute metrics\n    valid_durs = [d for d in durations if isinstance(d, (int, float)) and d > 0]\n    total_dur = sum(valid_durs) if valid_durs else None\n\n    score = 0.0\n    feedback = []\n\n    # Total duration approx 15s\n    if total_dur is not None:\n        if 14.9 <= total_dur <= 15.1:\n            score += 0.8\n            feedback.append(f'Total timeline duration OK ({total_dur:.2f}s).')\n        else:\n            feedback.append(f'Total duration off ({total_dur:.2f}s).')\n    else:\n        feedback.append('Unable to compute total duration.')\n\n    # First shot super must be blank/none\n    first_super = str(df.iloc[0][c_super]).strip().lower() if len(df) > 0 else None\n    if first_super in [None, '', 'nan', 'none', 'null'] or len(str(df.iloc[0][c_super]).strip()) == 0:\n        score += 0.5\n        feedback.append('First shot has no super (as required).')\n    else:\n        feedback.append('First shot incorrectly has a super.')\n\n    # All subsequent shots must have a super\n    if len(df) >= 2:\n        rest = df.iloc[1:][c_super]\n        non_empty = rest.apply(lambda x: (not pd.isna(x)) and str(x).strip() != '')\n        ratio = non_empty.mean() if len(rest) else 0.0\n        if ratio >= 0.95:\n            score += 0.5\n            feedback.append('Supers present on all subsequent shots.')\n        else:\n            feedback.append(f'Supers missing on some subsequent shots (coverage={ratio:.0%}).')\n    else:\n        feedback.append('Shot list has fewer than 2 shots.')\n\n    # Each super duration >= 1.2s (readability)\n    ok_read = 0\n    total_super = 0\n    for i in range(1, len(df)):\n        stxt = str(df.iloc[i][c_super]) if not pd.isna(df.iloc[i][c_super]) else ''\n        if stxt.strip() == '':\n            continue\n        total_super += 1\n        d = durations[i]\n        if isinstance(d, (int, float)) and d >= 1.2:\n            ok_read += 1\n    if total_super > 0:\n        ratio = ok_read / total_super\n        if ratio >= 0.9:\n            score += 0.4\n            feedback.append('Super on-screen times meet readability threshold.')\n        else:\n            feedback.append(f'Some supers may be too brief (readability={ratio:.0%}).')\n    else:\n        feedback.append('No supers detected beyond the first shot.')\n\n    # Minimum count of supers (expect at least 3)\n    super_count = 0\n    for i in range(1, len(df)):\n        stxt = '' if pd.isna(df.iloc[i][c_super]) else str(df.iloc[i][c_super]).strip()\n        if stxt:\n            super_count += 1\n    if super_count >= 3:\n        score += 0.2\n        feedback.append(f'Sufficient number of supers ({super_count}).')\n    else:\n        feedback.append(f'Too few supers ({super_count}).')\n\n    # Speed sanity (50%\u2013100%) if available\n    if c_speed is not None:\n        try:\n            speeds = pd.to_numeric(df[c_speed], errors='coerce')\n            valid = speeds.dropna()\n            if len(valid) > 0:\n                within = ((valid >= 50) & (valid <= 100)).mean()\n                if within >= 0.9:\n                    score += 0.0  # informational; main pacing judged elsewhere\n                else:\n                    feedback.append('Some clip speeds outside 50\u2013100%.')\n        except Exception:\n            pass\n\n    return (min(score, weight), '; '.join(feedback))"}, {"type": "code", "name": "Music Coverage and Shaping", "description": "Validate music cue coverage from ~0s to ~15s with fade-in/out present and a plausible strong ending.", "weight": 0.9, "code": "import json\nimport re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    # Weight: 0.9\n    weight = 0.9\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0\n\n    # Find music cues (CSV/XLSX)\n    cues_res = None\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            name = str(p.name).lower()\n        except Exception:\n            continue\n        if (name.endswith('.csv') or name.endswith('.xlsx')) and any(k in name for k in ['music', 'cue', 'audio', 'score']):\n            cues_res = r\n            break\n    if cues_res is None:\n        return 0.0\n\n    try:\n        if str(context.files.get_path(cues_res.id)).lower().endswith('.csv'):\n            df = context.files.read_csv(cues_res.id)\n        else:\n            df = context.files.read_excel(cues_res.id)\n    except Exception:\n        return 0.0\n\n    cols = {c.lower().strip(): c for c in df.columns}\n    def col(*names):\n        for n in names:\n            if n in cols:\n                return cols[n]\n        return None\n\n    c_start = col('start', 'start_sec', 'in', 'tc_in')\n    c_end = col('end', 'end_sec', 'out', 'tc_out')\n    c_fi = col('fade_in_sec', 'fadein', 'fade in')\n    c_fo = col('fade_out_sec', 'fadeout', 'fade out')\n\n    if c_start is None or c_end is None:\n        return 0.0\n\n    def parse_time(v):\n        if pd.isna(v):\n            return None\n        if isinstance(v, (int, float)):\n            return float(v)\n        s = str(v).strip()\n        if re.match(r'^\\d+(\\.\\d+)?$', s):\n            return float(s)\n        parts = s.split(':')\n        try:\n            parts = [float(p) for p in parts]\n        except Exception:\n            return None\n        if len(parts) == 3:\n            h, m, sec = parts\n            return h*3600 + m*60 + sec\n        if len(parts) == 2:\n            m, sec = parts\n            return m*60 + sec\n        return None\n\n    starts = df[c_start].apply(parse_time)\n    ends = df[c_end].apply(parse_time)\n\n    score = 0.0\n    fb = []\n\n    # Coverage check: any cue spanning near 0 to 15s\n    has_begin = any(s is not None and s <= 0.1 for s in starts)\n    has_end = any(e is not None and abs(e - 15.0) <= 0.1 for e in ends)\n    if has_begin and has_end:\n        score += 0.5\n        fb.append('Music covers ~0s to ~15s.')\n    else:\n        fb.append('Music may not cover full 0\u201315s range.')\n\n    # Fades presence\n    fades_ok = 0\n    fades_total = 0\n    if c_fi is not None:\n        fi = pd.to_numeric(df[c_fi], errors='coerce').fillna(0)\n        fades_total += 1\n        if (fi >= 0.2).any():\n            fades_ok += 1\n    if c_fo is not None:\n        fo = pd.to_numeric(df[c_fo], errors='coerce').fillna(0)\n        fades_total += 1\n        if (fo >= 0.2).any():\n            fades_ok += 1\n    if fades_total > 0:\n        if fades_ok == fades_total:\n            score += 0.3\n            fb.append('Fade-in and fade-out present (>=0.2s).')\n        elif fades_ok > 0:\n            score += 0.15\n            fb.append('Some fades present.')\n        else:\n            fb.append('No adequate fades detected.')\n    else:\n        fb.append('Fade columns not provided; cannot verify shape.')\n\n    # Minor credit for listing source/title columns\n    has_source = any(k in cols for k in ['source', 'source_url', 'provider', 'url'])\n    has_title = any(k in cols for k in ['title', 'track', 'name'])\n    if has_source and has_title:\n        score += 0.1\n        fb.append('Track title and source listed.')\n\n    return (min(score, weight), '; '.join(fb))"}, {"type": "code", "name": "Color Grading and Pacing Fields", "description": "From the shot list, verify inclusion and plausible values for desaturation/darkening and pacing adjustments (if provided).", "weight": 0.7, "code": "import json\nimport re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    # Weight: 0.7\n    weight = 0.7\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0\n\n    # Find shot list\n    shot_res = None\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            name = str(p.name).lower()\n        except Exception:\n            continue\n        if (name.endswith('.csv') or name.endswith('.xlsx')) and any(k in name for k in ['shot', 'edl', 'timeline']):\n            shot_res = r\n            break\n    if shot_res is None:\n        return 0.0\n\n    try:\n        if str(context.files.get_path(shot_res.id)).lower().endswith('.csv'):\n            df = context.files.read_csv(shot_res.id)\n        else:\n            df = context.files.read_excel(shot_res.id)\n    except Exception:\n        return 0.0\n\n    cols = {c.lower().strip(): c for c in df.columns}\n    def col(*names):\n        for n in names:\n            if n in cols:\n                return cols[n]\n        return None\n\n    c_desat = col('color_desat_pct', 'desat', 'desaturation', 'saturation_change_pct')\n    c_bright = col('brightness_ev', 'exposure_adjust_ev', 'brightness_adjust', 'exposure')\n    c_speed = col('speed_pct', 'speed percent', 'speed', 'speed%')\n\n    score = 0.0\n    fb = []\n\n    # Desaturation presence and values\n    if c_desat is not None:\n        des = pd.to_numeric(df[c_desat], errors='coerce')\n        valid = des.dropna()\n        if len(valid) > 0:\n            share = (valid >= 5).mean()\n            if share >= 0.6:\n                score += 0.3\n                fb.append('Desaturation applied on majority of shots (>=5%).')\n            elif share > 0:\n                score += 0.15\n                fb.append('Some desaturation present.')\n            else:\n                fb.append('No meaningful desaturation detected.')\n        else:\n            fb.append('Desaturation column present but values missing.')\n    else:\n        fb.append('No desaturation column provided.')\n\n    # Brightness/darkening presence and values (negative EV indicates darkening)\n    if c_bright is not None:\n        br = pd.to_numeric(df[c_bright], errors='coerce')\n        valid = br.dropna()\n        if len(valid) > 0:\n            share_dark = (valid < 0).mean()\n            if share_dark >= 0.5:\n                score += 0.2\n                fb.append('Darkening applied on at least half of shots.')\n            elif share_dark > 0:\n                score += 0.1\n                fb.append('Some darkening present.')\n            else:\n                fb.append('No darkening detected.')\n        else:\n            fb.append('Brightness column present but values missing.')\n    else:\n        fb.append('No brightness/darkening column provided.')\n\n    # Pacing via speed percentage (should be between 50% and 100%; slowdown optional but allowed)\n    if c_speed is not None:\n        sp = pd.to_numeric(df[c_speed], errors='coerce')\n        valid = sp.dropna()\n        if len(valid) > 0:\n            within = ((valid >= 50) & (valid <= 100)).mean()\n            if within >= 0.9:\n                score += 0.2\n                fb.append('Clip speeds within 50\u2013100% for most shots (slow, dramatic pacing).')\n            else:\n                fb.append('Some clip speeds outside 50\u2013100%.')\n        else:\n            fb.append('Speed column present but values missing.')\n    else:\n        fb.append('No speed column provided.')\n\n    return (min(score, weight), '; '.join(fb))"}, {"type": "code", "name": "Licensing Sources Validity", "description": "Confirm that listed sources for footage/music are plausible royalty-free stock providers and URLs are well-formed.", "weight": 0.3, "code": "import json\nimport re\nimport pandas as pd\nimport numpy as np\nfrom urllib.parse import urlparse\n\ndef evaluate(workflow, context):\n    # Weight: 0.3\n    weight = 0.3\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0\n\n    # Prefer a dedicated sources CSV/XLSX; otherwise fall back to shot list\n    src_res = None\n    shot_res = None\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            name = str(p.name).lower()\n        except Exception:\n            continue\n        if (name.endswith('.csv') or name.endswith('.xlsx')) and any(k in name for k in ['source', 'licens', 'attribution']):\n            src_res = r\n        if (name.endswith('.csv') or name.endswith('.xlsx')) and any(k in name for k in ['shot', 'edl', 'timeline']):\n            shot_res = r\n    target = src_res or shot_res\n    if target is None:\n        return 0.0\n\n    try:\n        if str(context.files.get_path(target.id)).lower().endswith('.csv'):\n            df = context.files.read_csv(target.id)\n        else:\n            df = context.files.read_excel(target.id)\n    except Exception:\n        return 0.0\n\n    cols = {c.lower().strip(): c for c in df.columns}\n    def col(*names):\n        for n in names:\n            if n in cols:\n                return cols[n]\n        return None\n\n    c_url = col('source_url', 'url', 'provider_url', 'link')\n    c_provider = col('provider', 'source', 'vendor')\n    c_license = col('license', 'licence', 'royalty_free', 'license_type')\n\n    if c_url is None and c_provider is None and c_license is None:\n        return 0.0\n\n    known_domains = [\n        'pond5.com','shutterstock.com','istockphoto.com','gettyimages.com',\n        'stock.adobe.com','pexels.com','pixabay.com','videvo.net','storyblocks.com'\n    ]\n\n    score = 0.0\n    fb = []\n\n    # URL validity and domain match\n    if c_url is not None:\n        urls = df[c_url].dropna().astype(str)\n        if len(urls) > 0:\n            good = 0\n            for u in urls:\n                try:\n                    pr = urlparse(u)\n                    domain = pr.netloc.lower()\n                    if pr.scheme in ['http','https'] and any(d in domain for d in known_domains):\n                        good += 1\n                except Exception:\n                    pass\n            ratio = good / len(urls)\n            if ratio >= 0.7:\n                score += 0.2\n                fb.append('Most source URLs point to known stock providers.')\n            elif ratio > 0:\n                score += 0.1\n                fb.append('Some source URLs valid/known; others unclear.')\n            else:\n                fb.append('Source URLs present but not recognized as stock providers.')\n        else:\n            fb.append('No source URLs listed.')\n    else:\n        fb.append('No URL column provided.')\n\n    # License string sanity\n    if c_license is not None:\n        lic = df[c_license].dropna().astype(str).str.lower()\n        if len(lic) > 0:\n            rf_ratio = lic.str.contains('royalty|rf|royalty-free|royalty free').mean()\n            if rf_ratio >= 0.7:\n                score += 0.1\n                fb.append('License entries indicate royalty-free for most items.')\n            elif rf_ratio > 0:\n                score += 0.05\n                fb.append('Some items indicate royalty-free.')\n            else:\n                fb.append('Licenses present but not clearly royalty-free.')\n    else:\n        fb.append('No license column provided.')\n\n    return (min(score, weight), '; '.join(fb))"}, {"type": "llm_judge", "name": "Editor Packet Cross-Check (Structural Correctness)", "description": "Use the Editor Packet to confirm claimed structural requirements: initial shot without super, one super per subsequent shot, and legibility notes. Do not judge creative quality here.", "weight": 0.5, "judge_prompt": "Open the Editor Packet (PDF/DOCX) included in the submission. Cross-check the QC Checklist and Supers Plan sections for the following claims:\n- Initial shot has no super (first beat is clean footage). \n- Every subsequent shot has exactly one super, and supers change only when the shot changes.\n- Supers are stated to be long enough to read (e.g., QC or Supers Plan mentions minimum on-screen times).\n\nScore only on whether the Editor Packet clearly and explicitly documents these elements. Do not assess the actual video content or creative quality here.\n\nScoring:\n- 0.5: Editor Packet explicitly documents all three items above.\n- 0.3: Documents any two of the three.\n- 0.1: Documents any one of the three, or mentions them vaguely.\n- 0.0: Not documented or Editor Packet missing/too incomplete.\n", "expectation": "QC Checklist and Supers Plan clearly state: first shot has no super; thereafter one super per shot; supers long enough to read."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Creative Quality and Broadcast Readiness (LLM)", "description": "Professional polish assessment: tone, pacing, typography legibility, audio shaping, and overall broadcast readiness.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Creative Quality and Readiness", "description": "Evaluate the video\u2019s creative execution and presentation quality.", "weight": 2.0, "judge_prompt": "Watch the 15-second MP4 and consider the Editor Packet. Judge creative quality and broadcast readiness. Criteria:\n1) Pacing and Tone: Slow, dramatic pacing that conveys urgency and gravity. Avoids rushed cuts; music supports emotion.\n2) Visual Treatment: Slight desaturation and darkening to create somber mood; tasteful reframing/slowdown as needed; clean transitions.\n3) Supers and Legibility: High-contrast, safe-margin placement; readable font choice/size; one super per unique shot; no super on first shot.\n4) Music Edit: Feels like a real beginning and a strong ending within 15s (fade-in/out or musical cadence is convincing); music level appropriate vs. supers.\n5) Overall Broadcast Readiness: Professional export look, no obvious artifacts, coherent narrative, appropriate sensitivity to subject matter.\n\nScoring guidance:\n- 2.0: Excellent across all criteria; broadcast-ready.\n- 1.5: Strong overall with only minor issues in one area.\n- 1.0: Adequate but noticeable weaknesses in two areas.\n- 0.5: Multiple weaknesses; not yet client-ready.\n- 0.0: Poor quality or not aligned with brief.\n", "expectation": "A dramatic, emotionally resonant 15s spot with clear, readable supers, supportive dramatic music edit, tasteful grading, and professional polish."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "9e8607e7-a38a-491f-ace1-e5ea7dc477cb", "rubric": {"category_name": "LatAm Fintech Strategy Deck (PDF) \u2013 Self-Documenting Evaluation", "rationale": "Pattern C (Mixed): The deliverable is a presentation (PDF) that combines document structure with data-driven content. Stage 1 uses an LLM gate to enforce a precise, presentation-like structure that makes verification possible. Stage 2 uses code rules to check coverage of regions/segments, time-stamping and sources, client-oriented actionability, and structural cues\u2014leveraging the mandated shape. Stage 3 applies an LLM quality assessment for professionalism and strategic value.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2013 Presentation Shape Enforcement (GATE)", "description": "LLM-only structural validation that the output is a presentation-style PDF exported from PowerPoint with the exact required sections and layout, enabling downstream verification.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Presentation Requirements (PDF)", "description": "Verify the deliverable is a presentation exported to PDF with clearly delineated sections and expected slide components.", "weight": 4.0, "judge_prompt": "You are evaluating whether the candidate output meets the REQUIRED structural format. Only check presence/format, not correctness of content.\n\nDeliverable must be a presentation exported to PDF with approximately ~30 slides (acceptable range 25\u201335; lenient 20\u201340). It must be clearly organized with section divider slides and the following sections and elements:\n\nFormat Requirements:\n- File is PDF (not Word, not Excel, not plain text).\n- Slides are apparent (landscape pages, slide-like formatting). Approximate length target: 25\u201335 slides (20\u201340 acceptable).\n- Clear section headers and divider slides for each of the three core sections.\n- Visuals appear across the deck (charts, tables, or market maps/logos) \u2013 at least one in each core section.\n\nRequired Slides/Sections:\n1) Title Slide (first slide): Includes title indicating LatAm focus, presenter/org, and date or quarter (e.g., Fall 2023 / 2023).\n2) Agenda or Table of Contents: Lists the three core sections.\n3) Section Divider: \"Latin America Macro Overview\", or close variant (e.g., \"LatAm Macro Overview\").\n   - Content: At least 6\u20138 slides covering macro topics such as GDP, inflation, FX/FX risk, interest rates/monetary policy, demographics, internet/mobile penetration, and regulatory/business climate. Include at least one chart/table within this section.\n4) Section Divider: \"State of LatAm Technology and Venture Markets\", or close variant (e.g., \"LatAm Tech & VC Landscape\").\n   - Content: At least 6\u20138 slides covering venture funding volumes, deal counts, stage mix, top countries, exits, valuations, notable investors. Include at least one chart/table within this section.\n5) Section Divider: \"Latin America Fintech Landscape\", or close variant (e.g., \"LatAm Fintech Landscape\").\n   - Content: At least 10\u201312 slides organized by sub-segments (e.g., Payments, Lending/Credit/BNPL, Neobanks/Digital Banks, Infrastructure/Open Banking/BaaS, Remittances/Cross-Border/FX, Wealth/Brokerage, Insurtech, Regtech, Crypto/Web3). Preferably include a market map or category slides; include at least one regulatory considerations slide for key markets.\n6) Closing: 1\u20133 slides with \"Implications for Client\" and/or \"Operating vs Investing\" considerations and \"Recommendations/Next Steps\" for the upcoming quarter.\n7) Optional Appendix (acceptable but not required).\n\nScoring (STRUCTURE ONLY):\n- 4.0: Valid PDF; 25\u201335 slides (20\u201340 acceptable); all three core sections present with divider slides; each core section has the required content volume; visuals present in each core section; includes Title, Agenda, and Closing (Recommendations/Next Steps). \n- 3.0: Valid PDF; 20\u201340 slides; all three sections present with divider slides; minor deviations (e.g., one section slightly under target or one section missing a visual) but still structurally complete overall. \n- 2.0: Valid PDF but missing at least one major structural element (e.g., no section dividers, or one core section too short/absent, or no closing), or slide count far outside target (\u226415 or \u226550). \n- 1.0: Valid PDF but only 1\u20132 core sections covered, minimal structure, or very short (<15 slides). \n- 0.0: Not a PDF presentation or missing multiple core sections.\n\nOnly evaluate presence and format against these requirements. Do not assess factual accuracy or quality.", "expectation": "A 25\u201335 slide PDF deck with Title, Agenda, three section dividers (Macro, Tech/Venture, Fintech), adequate content volume per section, visuals in each core section, and a Closing with Recommendations/Next Steps."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification Checks (Coverage, Rigor, Actionability)", "description": "Code-based verification of topical coverage, time-stamping and sourcing, client-oriented actionability, and structural cues using extracted PDF text.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Coverage of Regions and Fintech Segments", "description": "Checks that key LatAm countries and fintech sub-segments are covered in the text.", "weight": 1.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    def read_text(res):\n        if not res:\n            return ''\n        try:\n            if res.is_document:\n                # Prefer PDF text\n                try:\n                    return context.files.read_pdf_text(res.id) or ''\n                except Exception:\n                    pass\n                # Fallback to DOCX if available\n                try:\n                    return context.files.read_docx_text(res.id) or ''\n                except Exception:\n                    pass\n            if res.is_text_format:\n                try:\n                    return context.files.read_text(res.id) or ''\n                except Exception:\n                    return ''\n        except Exception:\n            return ''\n        return ''\n\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    if not output.is_document:\n        return 0.0\n\n    text = read_text(output).lower()\n    if not text:\n        return 0.0\n\n    # Region coverage\n    core_countries = [\"brazil\", \"mexico\", \"colombia\", \"chile\", \"argentina\", \"peru\"]\n    region_markers = [\"latin america\", \"latam\", \"lat\u00edn am\u00e9rica\", \"am\u00e9rica latina\"]\n    regions_present = set([c for c in core_countries if c in text])\n    has_latam_marker = any(m in text for m in region_markers)\n\n    # Fintech segments (broad and synonyms)\n    segments = [\n        \"payments\", \"acquiring\", \"merchant acquiring\", \"payment gateway\", \"pix\",\n        \"lending\", \"credit\", \"bnpl\", \"microcredit\", \"invoice financing\",\n        \"neobank\", \"digital bank\",\n        \"open banking\", \"open finance\", \"banking-as-a-service\", \"baas\",\n        \"infrastructure\", \"fintech infrastructure\", \"core banking\",\n        \"remittance\", \"cross-border\", \"fx\", \"forex\",\n        \"wealth\", \"brokerage\", \"investing app\",\n        \"insurtech\", \"regtech\",\n        \"crypto\", \"blockchain\", \"web3\", \"stablecoin\",\n        \"wallet\"\n    ]\n\n    seg_present = set([s for s in segments if s in text])\n\n    # Scoring: Encourage breadth\n    # Regions: full score at 5+ countries present and a LatAm marker; partial otherwise\n    region_score = 0.0\n    if has_latam_marker:\n        region_score = min(1.0, len(regions_present) / 5.0)\n    else:\n        # penalize missing LatAm anchor\n        region_score = min(0.7, len(regions_present) / 6.0)\n\n    # Segments: full score at 8+ segment keywords\n    seg_score = min(1.0, len(seg_present) / 8.0)\n\n    score = 0.5 * region_score + 0.5 * seg_score\n\n    feedback = (\n        f\"Regions found: {sorted(list(regions_present))} | LatAm marker: {has_latam_marker}; \"\n        f\"Segments found: {len(seg_present)}\"\n    )\n    return (score, feedback)\n"}, {"type": "code", "name": "Time-Stamping and Source Citations", "description": "Checks for recent time references (years/quarters) and presence of data sources or citations.", "weight": 1.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    def read_text(res):\n        if not res:\n            return ''\n        try:\n            if res.is_document:\n                try:\n                    return context.files.read_pdf_text(res.id) or ''\n                except Exception:\n                    pass\n                try:\n                    return context.files.read_docx_text(res.id) or ''\n                except Exception:\n                    pass\n            if res.is_text_format:\n                try:\n                    return context.files.read_text(res.id) or ''\n                except Exception:\n                    return ''\n        except Exception:\n            return ''\n        return ''\n\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = read_text(output)\n    if not text:\n        return 0.0\n\n    low = text.lower()\n\n    # Years and quarter/half markers\n    years = set(re.findall(r\"\\\\b(2019|2020|2021|2022|2023)\\\\b\", text))\n    period_markers = [\"q1\", \"q2\", \"q3\", \"q4\", \"h1\", \"h2\", \"as of\", \"ltm\", \"yoy\", \"y/y\"]\n    periods_found = sum(1 for p in period_markers if p in low)\n\n    # Sources: generic 'source' plus known providers\n    source_terms = [\n        \"source:\", \"sources:\", \"data source\", \"footnote\", \"citation\",\n        \"world bank\", \"imf\", \"oecd\", \"eclac\", \"un\", \"statista\",\n        \"lavca\", \"pitchbook\", \"cb insights\", \"dealroom\", \"crunchbase\",\n        \"bain\", \"mckinsey\", \"bcg\",\n        \"bloomberg\", \"refinitiv\",\n        \"banco central do brasil\", \"bacen\", \"cnbv\", \"sfc\", \"cmf\", \"bcra\",\n        \"inegi\", \"ibge\"\n    ]\n    sources_present = set([s for s in source_terms if s in low])\n\n    # Scoring\n    year_score = min(1.0, len(years) / 3.0)  # full credit if 3+ distinct years referenced\n    period_score = 1.0 if periods_found >= 2 else (0.5 if periods_found == 1 else 0.0)\n    time_stamp_score = 0.6 * year_score + 0.4 * period_score\n\n    src_score = 1.0 if len(sources_present) >= 3 else (0.7 if len(sources_present) == 2 else (0.4 if len(sources_present) == 1 else 0.0))\n\n    score = 0.5 * time_stamp_score + 0.5 * src_score\n\n    feedback = (\n        f\"Years: {sorted(list(years))} | Period markers: {periods_found} | Sources matched: {len(sources_present)}\"\n    )\n    return (score, feedback)\n"}, {"type": "code", "name": "Client-Oriented Actionability (Operate + Invest)", "description": "Checks for practical, client-oriented content: operating and investing pathways, regulatory and risk considerations, synergies, and next steps.", "weight": 1.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    def read_text(res):\n        if not res:\n            return ''\n        try:\n            if res.is_document:\n                try:\n                    return context.files.read_pdf_text(res.id) or ''\n                except Exception:\n                    pass\n                try:\n                    return context.files.read_docx_text(res.id) or ''\n                except Exception:\n                    pass\n            if res.is_text_format:\n                try:\n                    return context.files.read_text(res.id) or ''\n                except Exception:\n                    return ''\n        except Exception:\n            return ''\n        return ''\n\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = read_text(output).lower()\n    if not text:\n        return 0.0\n\n    operating = [\n        \"operating entity\", \"local entity\", \"market entry\", \"entry strategy\", \"go-to-market\",\n        \"licensing\", \"localization\", \"hiring\", \"talent\", \"distribution\", \"partnerships\"\n    ]\n    investing = [\n        \"investing entity\", \"investment criteria\", \"deal pipeline\", \"minority\", \"majority\",\n        \"valuation\", \"term sheet\", \"due diligence\", \"portfolio construction\"\n    ]\n    regulatory = [\n        \"regulatory\", \"compliance\", \"k yc\", \"kyc\", \"a ml\", \"aml\", \"data privacy\", \"lgpd\",\n        \"habeas data\", \"consumer protection\", \"sandbox\", \"licence\", \"license\",\n        \"banco central do brasil\", \"bacen\", \"cnbv\", \"sfc\", \"cmf\", \"bcra\"\n    ]\n    risks = [\n        \"currency risk\", \"fx hedging\", \"macro volatility\", \"political risk\",\n        \"credit risk\", \"fraud\", \"operational risk\", \"regulatory risk\"\n    ]\n    next_steps = [\n        \"next steps\", \"90-day plan\", \"timeline\", \"workplan\", \"roadmap\"\n    ]\n    synergies = [\n        \"synergy\", \"synergies\", \"build vs buy vs partner\", \"partner\", \"partnership\", \"m&a\",\n        \"acquire\", \"acquisition\", \"joint venture\", \"jv\", \"co-invest\", \"integration\"\n    ]\n\n    def bucket_score(terms, min_hits=1):\n        hits = sum(1 for t in terms if t in text)\n        return 1.0 if hits >= max(1, min_hits) else 0.0\n\n    scores = []\n    scores.append(bucket_score(operating, 2))\n    scores.append(bucket_score(investing, 2))\n    scores.append(bucket_score(regulatory, 2))\n    scores.append(bucket_score(risks, 1))\n    scores.append(bucket_score(next_steps, 1))\n    scores.append(bucket_score(synergies, 1))\n\n    score = sum(scores) / len(scores)\n\n    feedback = (\n        f\"Buckets satisfied (operating, investing, regulatory, risks, next_steps, synergies): {scores}\"\n    )\n    return (score, feedback)\n"}, {"type": "code", "name": "Structural Cues in Text (Agenda/Sections/Appendix)", "description": "Sanity-check for presence of common presentation structural markers in extracted text.", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    def read_text(res):\n        if not res:\n            return ''\n        try:\n            if res.is_document:\n                try:\n                    return context.files.read_pdf_text(res.id) or ''\n                except Exception:\n                    pass\n                try:\n                    return context.files.read_docx_text(res.id) or ''\n                except Exception:\n                    pass\n            if res.is_text_format:\n                try:\n                    return context.files.read_text(res.id) or ''\n                except Exception:\n                    return ''\n        except Exception:\n            return ''\n        return ''\n\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = read_text(output).lower()\n    if not text:\n        return 0.0\n\n    # Flexible markers\n    markers = [\n        \"agenda\", \"table of contents\", \"executive summary\", \"key takeaways\",\n        \"macro overview\", \"latam macro\", \"technology and venture\", \"tech & venture\",\n        \"fintech landscape\", \"recommendations\", \"next steps\", \"appendix\"\n    ]\n\n    found = sum(1 for m in markers if m in text)\n    # Expect at least 5 distinct markers if structure is present in extracted text\n    if found >= 7:\n        score = 1.0\n    elif found >= 5:\n        score = 0.7\n    elif found >= 3:\n        score = 0.4\n    else:\n        score = 0.0\n\n    feedback = f\"Structural markers found: {found}\"\n    return (score, feedback)\n"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Professional Quality and Strategic Value", "description": "LLM assessment of clarity, visual professionalism, strategic framing, and client relevance given a global consumer internet company expanding in LatAm fintech.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Presentation Quality and Strategic Fit", "description": "Evaluate professional polish, clarity, and usefulness for a client discussion.", "weight": 2.0, "judge_prompt": "Assess the overall quality and strategic value of the PDF presentation for a Managing Director advising a publicly traded global consumer internet client expanding in LatAm fintech. Consider:\n- Clarity and structure: logical flow across sections; concise, executive-ready slides; clear headlines and takeaways.\n- Visual quality: professional formatting, readable charts/tables/maps, consistent design.\n- Strategic value: framing of opportunities/threats; prioritization; practicality for operating and investing decisions; reasonable country sequencing and segment focus.\n- Tailoring: relevance for a global consumer internet company seeking LatAm operating and investing entities; attention to regulatory/licensing, partnership models, and synergy pathways.\n- Right level of detail for a 30-slide, 30\u201360 minute discussion.\n\nScoring:\n- 2.0: Highly professional and strategically impactful; would enable a strong client conversation with clear next steps.\n- 1.5: Solid quality with good strategic framing; minor gaps but very usable.\n- 1.0: Adequate but uneven; helpful context with limited strategic guidance.\n- 0.5: Weak polish or unclear; limited utility for decision-making.\n- 0.0: Poor quality or largely irrelevant to the client needs.\n\nDo not re-check Stage 1 structure; focus on quality and usefulness.", "expectation": "A polished, executive-ready deck that clearly frames LatAm fintech opportunities, provides practical operating/investing guidance, and supports a productive client meeting with clear next steps."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "ec591973-04d5-48c0-981c-1ab2fcec2dc1", "rubric": {"category_name": "One-Page Channel-Differentiated Strategy Slide (Prestige Cosmetics)", "rationale": "This rubric enforces a self-documenting, verifiable one-slide PowerPoint deliverable. Stage 1 strictly gates on format and structural completeness so later checks are trivial. Stage 2 verifies correctness and logical coverage of required business challenges and channel differentiation, blending deterministic file checks with LLM cross-referencing. Stage 3 assesses executive quality and persuasion for the leadership audience.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (STRUCTURE ONLY)", "description": "Validate that the output is a single-slide PowerPoint deck with the exact structural elements required to enable verification. Do not assess content quality or strategy strength here\u2014only presence/structure.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "PowerPoint One-Page Structure and Required Sections", "description": "LLM checks that the deliverable is a PPT/PPTX with exactly one slide and contains the mandated structural elements to enable verification of strategy, channel differentiation, challenges, KPIs, and speaker notes.", "weight": 4.0, "judge_prompt": "You are evaluating a single candidate output. Only assess SHAPE/STRUCTURE and FORMAT, not the quality of ideas. The expected deliverable is ONE PowerPoint slide (PPTX or PPT file), suitable for a Monthly Business Review.\n\nCheck the following strictly as presence/structure:\n\nA) File format and length\n- Must be a PowerPoint file (.pptx or .ppt)\n- Must contain exactly ONE slide\n\nB) Required visible slide sections (headers can be flexible but must be clearly present):\n1) Slide Title with a clear thesis about differentiated investment to protect client retention and brand health (e.g., a strong headline).\n2) Channel Strategy section that differentiates at minimum the three channels: Open-sell, Specialty, and Brand Boutiques. This section must present channel roles and show differentiation across at least two of these dimensions: Assortment, Marketing Programs (activations/CRM/loyalty/collateral), Services/GWP/Value. Accept as a structured grid, multi-column layout, or clearly boxed sections.\n3) Investment/Prioritization Principles (e.g., resource allocation criteria, door-tiering, ROI focus). A compact list or mini-framework is acceptable.\n4) Risks/Challenges section mapping to the provided themes (can be a compact list): corporate store closures (esp. specialty), resource efficiency in staffing/activations, weak brand expertise in open-sell at high price points, low ROI/over-assorted low-volume doors with stockouts, avoid over-investment in unsustainable locations.\n5) KPIs/Success Metrics (at least 3 metrics visible), e.g., retention/repeat rate, basket size, margin, in-stock rate, loyalty enrollment, event ROI.\n6) Optional but preferred: Next Steps/Action Summary (brief 2\u20134 bullets). If missing, do not penalize more than a minor deduction.\n\nC) Speaker notes\n- The slide should include speaker notes that support a ~5-minute elevator pitch (do not judge exact length here; just confirm that notes exist and are non-empty).\n\nD) Professional slide conventions\n- Executive layout with clear headers and readable hierarchy. Brevity (bullets or concise phrases) and a footer (date/audience or brand) are preferred but treat footer as minor if missing.\n\nScoring (STRUCTURE ONLY):\n- 1.0: PPT/PPTX with exactly one slide AND all items in B1\u2013B5 present AND speaker notes present (C). B6 and footer are optional minor enhancements.\n- 0.8: PPT/PPTX with exactly one slide; all core items present EXCEPT one minor structural element missing (e.g., KPIs OR explicit challenges section OR speaker notes missing). Only one such miss allowed for 0.8.\n- 0.5: PPT/PPTX with exactly one slide, but multiple core structural elements missing (two or more from B1\u2013B5 or C). Still a valid shape, but incomplete.\n- 0.0: Not PowerPoint, OR more than one slide, OR missing most core sections (does not enable verification).\n\nImportant: Be flexible with exact header wording; focus on whether the sections clearly exist. Do not evaluate content correctness or design quality\u2014only presence/structure.", "expectation": "A one-slide PPT/PPTX with title, channel-differentiated strategy (covering open-sell, specialty, boutiques), investment/prioritization principles, risks/challenges aligned to the prompt, KPIs, and non-empty speaker notes. Exact header names may vary; structure must be clearly visible."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Verification", "description": "Now that structure is verified, assess whether the strategy content correctly addresses the required differentiation by channel and ties back to stated business challenges and lifecycle retention/value tailoring. Includes light deterministic file checks and LLM cross-references.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Deterministic File Check: PowerPoint and Single Slide", "description": "Programmatically verify PPTX/PPT and attempt to confirm exactly one slide (pptx reliably; ppt slide count falls back to extension-only).", "weight": 0.75, "code": "import re\nfrom pathlib import Path\nimport zipfile\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float (0..1) or (float, str)\n    \"\"\"\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output resource found.\"\n    try:\n        p = context.files.get_path(output.id)\n    except Exception as e:\n        return 0.0, f\"Failed to access file path: {e}\"\n\n    suffix = p.suffix.lower()\n    if suffix not in [\".pptx\", \".ppt\"]:\n        return 0.0, f\"Wrong extension {suffix}; expected .pptx or .ppt.\"\n\n    # For PPTX, count slides via zip structure. For PPT (binary), we cannot reliably count without extra libs.\n    if suffix == \".pptx\":\n        try:\n            with zipfile.ZipFile(p, 'r') as zf:\n                slide_files = [n for n in zf.namelist() if n.startswith('ppt/slides/slide') and n.endswith('.xml')]\n                slide_count = len(slide_files)\n            if slide_count == 1:\n                return 1.0, \"PPTX with exactly one slide.\"\n            elif slide_count == 2:\n                return 0.5, f\"PPTX has {slide_count} slides; expected 1.\"\n            else:\n                return 0.0, f\"PPTX has {slide_count} slides; expected 1.\"\n        except Exception as e:\n            return 0.5, f\"PPTX detected but failed to count slides robustly: {e}\"\n    else:\n        # .ppt legacy: accept extension but cannot count reliably\n        return 0.7, \".ppt detected; extension valid but slide count not verified.\"\n"}, {"type": "code", "name": "Deterministic Check: Speaker Notes Presence and Reasonable Length", "description": "For PPTX files, extract speaker notes text and verify non-empty and plausible length to support a 5-minute pitch. Partial credit if present but short/long. For PPT, best-effort cannot parse\u2014award minimal credit if file is .ppt and Stage 1 already saw notes.", "weight": 0.75, "code": "import re\nimport zipfile\nfrom pathlib import Path\nimport xml.etree.ElementTree as ET\n\ndef _extract_notes_text_from_pptx(path):\n    texts = []\n    with zipfile.ZipFile(path, 'r') as zf:\n        note_files = [n for n in zf.namelist() if n.startswith('ppt/notesSlides/notesSlide') and n.endswith('.xml')]\n        if not note_files:\n            return \"\"\n        # Use first notes slide (single-slide deck expected)\n        data = zf.read(note_files[0])\n        try:\n            root = ET.fromstring(data)\n            # Collect all text nodes\n            for elem in root.iter():\n                if elem.text and elem.text.strip():\n                    texts.append(elem.text.strip())\n        except ET.ParseError:\n            # Fallback: strip tags via regex\n            txt = re.sub(rb\"<[^>]+>\", b\" \", data)\n            try:\n                texts.append(txt.decode('utf-8', errors='ignore'))\n            except Exception:\n                texts.append(\"\")\n    return \" \".join(texts)\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output resource found.\"\n    try:\n        p = context.files.get_path(output.id)\n    except Exception as e:\n        return 0.0, f\"Failed to access file path: {e}\"\n\n    suffix = p.suffix.lower()\n    if suffix == \".pptx\":\n        try:\n            notes_text = _extract_notes_text_from_pptx(p)\n            if not notes_text or not notes_text.strip():\n                return 0.0, \"No speaker notes detected in PPTX.\"\n            # Word count\n            words = re.findall(r\"[A-Za-z0-9']+\", notes_text)\n            wc = len(words)\n            # For a 5-min pitch, ~600-750 words is typical; allow a generous band.\n            if 250 <= wc <= 800:\n                return 1.0, f\"Speaker notes present with plausible length (~{wc} words).\"\n            elif 120 <= wc < 250 or 800 < wc <= 1200:\n                return 0.6, f\"Speaker notes present but short/long (~{wc} words).\"\n            else:\n                return 0.3, f\"Speaker notes present but implausible length (~{wc} words).\"\n        except Exception as e:\n            return 0.3, f\"Error parsing speaker notes: {e}\"\n    elif suffix == \".ppt\":\n        # Cannot parse reliably; minimal credit for presence will be handled by Stage 1 LLM\n        return 0.3, \".ppt detected; unable to parse notes; awarding minimal credit.\"\n    else:\n        return 0.0, f\"Unsupported format {suffix}; expected PPT/PPTX.\"\n"}, {"type": "llm_judge", "name": "Challenge Linkage and Channel Differentiation Coverage", "description": "Verify that each stated business challenge is explicitly tied to strategic responses, and that channel differentiation covers the three channels with concrete levers across assortment, marketing programs, and services/value.", "weight": 1.5, "judge_prompt": "Evaluate the slide content for correctness of coverage and linkage (not design quality). Check:\n1) CHANNEL COVERAGE: Does the slide explicitly address all three channels: Open-sell, Specialty, and Brand Boutiques? Are there concrete differentiators per channel across at least two levers: Assortment; Marketing Programs (activations/CRM/loyalty/collateral); Services/GWP/Value? Preferably all three levers.\n2) CHALLENGE LINKAGE: For each challenge listed in the prompt, is there a visible, explicit response or mitigation on the slide? Challenges to check: (a) Corporate store closures (esp. specialty); (b) Resource efficiency in staffing and activations; (c) Weak brand expertise in open-sell for high-price products; (d) Low ROI in over-assorted, low-volume doors with stockouts; (e) Avoid over-investment in weak locations. Look for mapping: bullet-to-challenge or section-to-challenge alignment.\n3) LIFECYCLE/RETENTION ORIENTATION: Are responses oriented toward protecting client retention and long-term brand health, not just short-term sales?\n\nScoring:\n- 1.0: All channels covered with concrete differentiators across at least two levers per channel AND all challenges have explicit linked responses AND retention/brand health orientation is clear.\n- 0.7: Minor gaps (e.g., one channel lacks a lever or one challenge only implicitly addressed).\n- 0.4: Multiple gaps (e.g., missing a channel or most levers are vague; several challenges unaddressed).\n- 0.0: Little to no linkage; channels/challenges largely missing.", "expectation": "Clear, explicit mapping from each challenge to a concrete strategic response, and differentiated tactics for open-sell vs. specialty vs. boutiques across assortment, programs, and services/value, framed for long-term retention."}, {"type": "llm_judge", "name": "Value Tailoring and Loyalty Mechanics by Channel", "description": "Check that the slide proposes tailored value by channel (e.g., curated sets, exclusive services, or GWP strategies) and ties them to loyalty/CRM and lifecycle retention.", "weight": 0.5, "judge_prompt": "Assess whether the slide articulates tailored value by channel and ties it to loyalty/CRM and lifecycle retention.\n- Look for specifics such as curated product sets, exclusive service menus, appointment/concierge experiences, or GWP thresholds differentiated by channel.\n- Confirm tie-ins to CRM/loyalty (e.g., enrollment, tiers, offers) and how value is sequenced across the client lifecycle.\n\nScoring:\n- 1.0: Clear, channel-specific value constructs with loyalty/CRM linkage and lifecycle framing.\n- 0.6: Present but thin (one channel weak or lifecycle link vague).\n- 0.3: Mostly generic; limited channel tailoring or no explicit loyalty linkage.\n- 0.0: Not present.", "expectation": "Distinct value mechanics per channel, integrated with loyalty/CRM and lifecycle retention (e.g., onboarding, growth, reactivation)."}, {"type": "llm_judge", "name": "Resource Optimization and Prioritization Logic", "description": "Confirm the strategy includes a principled resource allocation approach (e.g., door tiering, ROI thresholds, exit/hold/invest decisions) aligned to risks like closures, stockouts, and low-volume doors.", "weight": 0.5, "judge_prompt": "Evaluate whether a clear investment framework is stated:\n- Door/prioritization tiers or criteria (e.g., A/B/C, ROI hurdles, productivity per square foot, in-stock reliability, brand expertise coverage).\n- Explicit guidance to avoid over-investment in low-return or frequently out-of-stock doors, addressing closures and staffing/activation efficiency.\n- Indications of reallocation (shift budget from low ROI doors to high-potential channels) and sustaining brand health.\n\nScoring:\n- 1.0: Concrete framework with criteria and clear invest/hold/exit guidance tied to challenges.\n- 0.6: Framework present but criteria shallow or links to challenges partial.\n- 0.3: Vague principles without actionable thresholds.\n- 0.0: No prioritization logic.", "expectation": "A concise prioritization model that operationalizes where to invest, maintain, or reduce exposure, tied to ROI, expertise coverage, and stock/inventory health."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Executive Quality and Persuasive Readout", "description": "Assess the professional polish, clarity, and persuasive power for a leadership monthly business review. Focus on readability, succinctness, and executive relevance.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Executive Clarity and Slide Craft", "description": "Evaluate whether the slide is scannable and executive-ready: crisp headline, concise bullets, clear hierarchy, minimal clutter.", "weight": 1.0, "judge_prompt": "Judge the slide on executive readability and craft:\n- Is there a strong, crisp headline that states the thesis?\n- Are sections concise (bullets/phrases vs. paragraphs), with clear visual hierarchy?\n- Is the content scannable in under 10 seconds and suitable for a leadership MBR?\n- Avoids jargon overload; focuses on what leaders need to decide/endorse.\n\nScoring:\n- 1.0: Highly scannable, crisp, and executive-ready.\n- 0.7: Generally clear with minor density/layout issues.\n- 0.4: Hard to scan or overly verbose.\n- 0.0: Not executive-appropriate.", "expectation": "A clean, succinct slide with a strong headline and well-structured sections suitable for a leadership forum."}, {"type": "llm_judge", "name": "Persuasiveness and Leadership Readiness", "description": "Assess whether the slide supports a compelling 5-minute verbal pitch with a clear ask and expected outcomes.", "weight": 0.6, "judge_prompt": "Evaluate persuasiveness:\n- Do the speaker notes and slide together support a compelling 5-minute narrative?\n- Is there a clear leadership-relevant ask or decision (e.g., endorse differentiated investment approach, approve resource reallocation)?\n- Are expected outcomes or success measures stated at a high level?\n\nScoring:\n- 1.0: Clear, compelling narrative with an explicit ask and outcomes.\n- 0.6: Generally persuasive; ask/outcomes implied but not explicit.\n- 0.3: Weak narrative; unclear ask.\n- 0.0: Not persuasive.", "expectation": "Concise, outcome-oriented narrative with a clear leadership ask and success signals."}, {"type": "llm_judge", "name": "Brand Tone and Category Appropriateness", "description": "Check if tone and examples fit a prestige cosmetic brand and wholesale trade context.", "weight": 0.4, "judge_prompt": "Assess tone and contextual fit:\n- Do examples and language fit a prestige cosmetics brand (e.g., clienteling, services, luxury cues) within wholesale channel realities?\n- Does the content maintain brand elevation while addressing operational realities (open-sell constraints, specialty closures)?\n\nScoring:\n- 1.0: Strongly on-brand and contextually appropriate.\n- 0.6: Mostly appropriate with minor mismatches.\n- 0.3: Generic or mismatched tone.\n- 0.0: Inappropriate tone or context.", "expectation": "Elevated yet practical tone aligned to prestige cosmetics and multi-channel wholesale distribution."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "b3573f20-5d3e-4954-948f-9461fda693d2", "rubric": {"category_name": "Brand Onboarding Form (Wholesale Trade - Sales Managers)", "rationale": "Pattern B (Document). The required output is a 3-page, text-based PDF titled \"Brand Data Gathering\". Stage 1 uses an LLM judge to strictly enforce the document's shape and structural completeness (format, length, title presence, and topic coverage in a question-based format). Stage 2 uses code rules to verify textual coverage and verifiable elements (keyword/topic coverage, question density, contact info fields, and visible answer-space cues). Stage 3 uses LLM judges for holistic quality (clarity, flow, and audience appropriateness). This self-documenting structure forces the agent to produce a verifiable, well-structured PDF fit for internal onboarding.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 - Shape Enforcement Gate", "description": "LLM-only gate to verify the output is a text-based, 3-page PDF titled \"Brand Data Gathering\" with clear, question-based prompts covering core onboarding topics and visible space for answers.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 2.5, "rules": [{"type": "llm_judge", "name": "Document Format and Structural Completeness", "description": "Checks that the candidate output is a properly structured, text-based PDF with the mandated title, length, and topic coverage in a question-based format.", "weight": 4.0, "judge_prompt": "You are evaluating a candidate output for a Sales Manager onboarding artifact.\n\nTask summary:\n- Create a 3-page, text-based PDF titled \"Brand Data Gathering\".\n- It must contain clear, question-based prompts to collect operational and sales information from potential/new brand partners.\n- No design, branding, or embedded form fields required. Section headers are optional; focus is on clear content and logical flow.\n- The form should be easy to read and complete, with clear labels and sufficient space for answers.\n\nWhat to check (STRUCTURE ONLY):\n1) File format and length:\n   - Must be a PDF (not Word, not Excel, not plain text image).\n   - Should be 3 pages. If not exactly 3, at least 3 pages is acceptable for partial credit.\n   - Must be text-based (selectable text; not a scanned image of text).\n2) Title and first page:\n   - The exact phrase \"Brand Data Gathering\" appears prominently on the first page as the document title.\n3) Question-based format:\n   - The content is primarily prompts/questions (e.g., lines with '?' or ':' labels) intended to be filled in by the brand\u2019s Operations or Sales teams.\n   - There should be many prompts distributed across the document (dozens), not just a few.\n4) Visible answer space:\n   - There should be visible space to write answers (e.g., blank lines/underscores, brackets, or clearly separated whitespace after prompts). Exact form fields are not required.\n5) Topic coverage (headers may vary or be implicit, but the prompts must clearly cover these areas):\n   - Company profile and contacts (company name, website, primary contact, email, phone)\n   - Product catalog and identifiers (SKUs, variants, UPC/GTIN, case/inner packs)\n   - Pricing and commercial terms (MSRP, wholesale, MAP, minimums, payment terms)\n   - Logistics and operations (ship-from location, lead times, warehouse/3PL, carton/pallet details)\n   - Compliance and documentation (COI/insurance, W-9/tax ID, MSDS/SDS, Prop 65, trademarks/brand registry)\n   - Sales channels and distribution (current retailers/markets, online/marketplaces, channel strategy)\n   - Marketing assets (images, copy, brand guidelines, UPC barcodes quality)\n   - Forecasting and supply (forecast volumes, production capacity, MOQs)\n   - Onboarding/retailer readiness (EDI, portals, routing guides, chargebacks, RMA/returns)\n\nScoring (focus on presence/structure, not writing quality):\n- 4.0: PDF, exactly 3 pages, text-based, title \"Brand Data Gathering\" on page 1, clearly question-based with ample prompts and visible answer space, and covers ALL listed topic areas.\n- 3.0: PDF with 3+ pages, text-based, title present, clearly question-based with visible answer space, but missing 1\u20132 topic areas OR slightly sparse prompts.\n- 2.0: Valid PDF and mostly text-based but only 2 pages OR question prompts are too few OR missing several topic areas (3\u20134 missing) OR limited visible answer space.\n- 1.0: Valid document but wrong format (not PDF) OR only 1\u20132 pages OR barely any question prompts.\n- 0.0: Not a document, unreadable, or no relevant structure.\n\nOnly judge structure/format and presence of the required elements above. Do NOT assess quality, writing style, or correctness of details.", "expectation": "A 3-page, text-based PDF titled \"Brand Data Gathering\" on page 1 with question-based prompts spanning all core topics and visible space for answers."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 - Verification (Content Coverage via Code)", "description": "Code-based checks to verify text coverage, prompt density, presence of key fields, and visible answer-space cues based on the enforced structure.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Core Domain Coverage (Keywords)", "description": "Checks the PDF text for coverage of core onboarding domains using keyword families. Awards proportional credit for domains detected.", "weight": 1.0, "code": "import re\\nimport json\\n\\nDOMAINS = {\\n    'company_profile': ['company name', 'legal name', 'website', 'year founded', 'headquarters', 'address'],\\n    'contacts': ['primary contact', 'sales contact', 'operations contact', 'ops contact', 'email', 'phone'],\\n    'product_catalog': ['sku', 'product', 'variant', 'upc', 'gtin', 'case pack', 'inner pack'],\\n    'pricing_terms': ['msrp', 'map', 'wholesale', 'cost', 'payment terms', 'net', 'minimum order', 'moq', 'discount'],\\n    'logistics_ops': ['lead time', 'ship from', 'warehouse', '3pl', 'pallet', 'dimensions', 'weight', 'incoterms', 'fob', 'freight', 'carton'],\\n    'compliance_docs': ['coi', 'insurance', 'certificate', 'msds', 'sds', 'prop 65', 'trademark', 'brand registry', 'w-9', 'tax id'],\\n    'sales_channels': ['channel', 'distribution', 'retailer', 'marketplace', 'amazon', 'd2c', 'ecommerce', 'online'],\\n    'marketing_assets': ['images', 'photography', 'assets', 'brand guidelines', 'copy', 'descriptions'],\\n    'forecasting_supply': ['forecast', 'production capacity', 'allocation', 'safety stock', 'backorder'],\\n    'onboarding_ops': ['edi', 'portal', 'routing guide', 'chargeback', 'rma', 'returns', 'label', 'barcode']\\n}\\n\\ndef _read_text(context, output):\\n    text = ''\\n    try:\\n        if output.is_document:\\n            try:\\n                text = context.files.read_pdf_text(output.id) or ''\\n            except Exception:\\n                text = ''\\n            if not text:\\n                try:\\n                    text = context.files.read_docx_text(output.id) or ''\\n                except Exception:\\n                    pass\\n        if (not text) and output.is_text_format:\\n            try:\\n                text = context.files.read_text(output.id) or ''\\n            except Exception:\\n                pass\\n    except Exception:\\n        text = ''\\n    return text\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output:\\n        return 0.0, 'No output resource.'\\n    text = _read_text(context, output)\\n    if not text:\\n        return 0.0, 'Could not read text from output.'\\n    low = text.lower()\\n\\n    covered = 0\\n    details = []\\n    for domain, kws in DOMAINS.items():\\n        found = any(kw in low for kw in kws)\\n        if found:\\n            covered += 1\\n            details.append(f\"{domain}: yes\")\\n        else:\\n            details.append(f\"{domain}: no\")\\n\\n    # Expect broad coverage; full credit at >=8 domains, partial otherwise\\n    score = min(1.0, covered / 8.0)\\n    feedback = f\"Domains covered: {covered}/10. Details: \" + '; '.join(details)\\n    return score, feedback\\n"}, {"type": "code", "name": "Question/Prompt Density", "description": "Counts question-style lines (ending with '?' or label-style with ':') to ensure the document is truly a fillable prompt list.", "weight": 1.0, "code": "import re\\n\\nLABEL_MAX_LEN = 140\\n\\ndef _read_text(context, output):\\n    text = ''\\n    try:\\n        if output.is_document:\\n            try:\\n                text = context.files.read_pdf_text(output.id) or ''\\n            except Exception:\\n                text = ''\\n            if not text:\\n                try:\\n                    text = context.files.read_docx_text(output.id) or ''\\n                except Exception:\\n                    pass\\n        if (not text) and output.is_text_format:\\n            try:\\n                text = context.files.read_text(output.id) or ''\\n            except Exception:\\n                pass\\n    except Exception:\\n        text = ''\\n    return text\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output:\\n        return 0.0, 'No output resource.'\\n    text = _read_text(context, output)\\n    if not text:\\n        return 0.0, 'Could not read text.'\\n\\n    lines = [ln.strip() for ln in text.splitlines()]\\n    qmark = sum(1 for ln in lines if ln.endswith('?'))\\n    colon = sum(1 for ln in lines if (':' in ln and len(ln.split(':',1)[0]) <= LABEL_MAX_LEN))\\n    # Approximate distinct prompts\\n    prompts = qmark + colon\\n\\n    # Scale: 35+ prompts = 1.0; 25 = 0.8; 15 = 0.6; 8 = 0.4; else proportional\\n    if prompts >= 35:\\n        score = 1.0\\n    elif prompts >= 25:\\n        score = 0.8\\n    elif prompts >= 15:\\n        score = 0.6\\n    elif prompts >= 8:\\n        score = 0.4\\n    else:\\n        score = max(0.0, min(1.0, prompts / 35.0))\\n\\n    return score, f\"Detected ~{prompts} prompts (?:{qmark}, :{colon}).\"\\n"}, {"type": "code", "name": "Contact and Company Info Fields", "description": "Checks for presence of key company and contact fields critical to onboarding.", "weight": 1.0, "code": "import re\\n\\nFIELD_GROUPS = {\\n    'company_name': ['company name', 'legal name'],\\n    'website': ['website', 'url'],\\n    'primary_contact': ['primary contact', 'main contact', 'contact name'],\\n    'email': ['email'],\\n    'phone': ['phone', 'telephone'],\\n    'address': ['address', 'street'],\\n    'city_state_zip': ['city', 'state', 'province', 'zip', 'postal'],\\n    'country': ['country'],\\n}\\n\\ndef _read_text(context, output):\\n    text = ''\\n    try:\\n        if output.is_document:\\n            try:\\n                text = context.files.read_pdf_text(output.id) or ''\\n            except Exception:\\n                text = ''\\n            if not text:\\n                try:\\n                    text = context.files.read_docx_text(output.id) or ''\\n                except Exception:\\n                    pass\\n        if (not text) and output.is_text_format:\\n            try:\\n                text = context.files.read_text(output.id) or ''\\n            except Exception:\\n                pass\\n    except Exception:\\n        text = ''\\n    return text\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output:\\n        return 0.0, 'No output resource.'\\n    text = _read_text(context, output)\\n    if not text:\\n        return 0.0, 'Could not read text.'\\n    low = text.lower()\\n\\n    found = {}\\n    hits = 0\\n    for group, synonyms in FIELD_GROUPS.items():\\n        present = any(s in low for s in synonyms)\\n        found[group] = present\\n        if present:\\n            hits += 1\\n\\n    total = len(FIELD_GROUPS)\\n    score = hits / total if total else 0.0\\n    detail = ', '.join(f\"{k}:{'yes' if v else 'no'}\" for k,v in found.items())\\n    return score, f\"Contact fields present {hits}/{total}. {detail}\"\\n"}, {"type": "code", "name": "Answer Space Cues", "description": "Detects common visual cues for answer space (underscores, brackets, or blank placeholders) indicating sufficient room to write answers.", "weight": 1.0, "code": "import re\\n\\nUNDER_RE = re.compile(r'_{5,}')\\nBRACK_RE = re.compile(r'\\[(\\s|_)\\]|\\(\\s*\\)')\\nBLANK_LINE_RE = re.compile(r'^\\s*$')\\n\\ndef _read_text(context, output):\\n    text = ''\\n    try:\\n        if output.is_document:\\n            try:\\n                text = context.files.read_pdf_text(output.id) or ''\\n            except Exception:\\n                text = ''\\n            if not text:\\n                try:\\n                    text = context.files.read_docx_text(output.id) or ''\\n                except Exception:\\n                    pass\\n        if (not text) and output.is_text_format:\\n            try:\\n                text = context.files.read_text(output.id) or ''\\n            except Exception:\\n                pass\\n    except Exception:\\n        text = ''\\n    return text\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output:\\n        return 0.0, 'No output resource.'\\n    text = _read_text(context, output)\\n    if not text:\\n        return 0.0, 'Could not read text.'\\n\\n    underscores = len(UNDER_RE.findall(text))\\n    brackets = len(BRACK_RE.findall(text))\\n    # Heuristic: blank lines indicate space too, but avoid inflating score\\n    blank_lines = sum(1 for ln in text.splitlines() if BLANK_LINE_RE.match(ln))\\n\\n    # Scoring heuristic: prioritize explicit placeholders\\n    raw = min(1.0, (underscores/10.0) + (brackets/10.0))\\n    # If no explicit placeholders but many blank lines, grant small partial credit\\n    if raw < 0.3 and blank_lines >= 20:\\n        raw = 0.3\\n    return raw, f\"Underscore placeholders: {underscores}; bracket placeholders: {brackets}; blank lines: {blank_lines}.\"\\n"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 - Quality and Audience Fit", "description": "LLM-based holistic assessment of clarity, flow, and suitability for brand Operations/Sales teams.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity, Readability, and Flow", "description": "Evaluates whether the document reads clearly, is logically organized, and uses consistent prompting that is easy to follow.", "weight": 1.0, "judge_prompt": "Evaluate the overall clarity and organization of the document (PDF). Consider: logical flow from company info to operations/logistics to pricing to compliance and onboarding; clear, concise prompts; consistent labeling; and readable spacing. Ignore visual design and branding. Scoring: 1.0 = very clear, logically ordered, consistent prompting and spacing; 0.7 = generally clear with minor inconsistencies; 0.4 = somewhat confusing or disorganized; 0.0 = hard to follow, poor flow, inconsistent prompting.", "expectation": "A clean, logically ordered prompt list that reads easily from basics to operational detail, with consistent labels and whitespace."}, {"type": "llm_judge", "name": "Audience Appropriateness and Ease of Completion", "description": "Assesses whether prompts are appropriate and actionable for brand Operations/Sales teams and can be completed without specialized internal knowledge.", "weight": 1.0, "judge_prompt": "Judge whether the prompts are suitable for brand-side Operations/Sales teams. Consider: avoidance of internal company jargon; inclusion of practical, answerable questions (e.g., lead times, case packs, payment terms); brief guidance where needed; and an overall sense that a typical brand contact could complete it efficiently. Scoring: 1.0 = highly appropriate and easy to complete; 0.7 = appropriate with minor gaps; 0.4 = several prompts unclear or overly internal; 0.0 = largely inappropriate or confusing for the intended audience.", "expectation": "Prompts should be practical, unambiguous, and minimize internal jargon so brand teams can complete them readily."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "a328feea-47db-4856-b4be-2bdc63dd88fb", "rubric": {"category_name": "Government Administrative Policy Creation", "rationale": "This rubric evaluates the structured creation of a policy document within a government administrative context, focusing on the critical requirements for absentee reporting procedures. It leverages LLM judges for format and structure compliance and mixes code with LLM evaluations to verify correctness and assess quality.", "max_total_score": 10.0, "stages": [{"name": "Format Compliance Gate", "description": "Ensure document is correctly formatted as per task description.", "is_required": true, "max_points": 2.0, "min_score_to_pass": 2.0, "rules": [{"type": "llm_judge", "name": "Document Format and Structure Verification", "description": "Check if output is a Word or PDF document with specified sections.", "weight": 2.0, "judge_prompt": "Check if the provided document is either a Word or PDF file and includes the following required sections:\n\n1. 'Purpose' - Clearly stating the reason for the document.\n2. 'Scope' - Defining who the procedure applies to within the organization.\n3. 'Definitions' - Important terms explained for clarity.\n4. 'Procedures' - Clear steps for reporting unscheduled absences or lateness via phone call, addressing issues like inconsistency, timeliness, contact method, and MFA communication.\n\n- The document should not exceed one page.\n\n**Scoring:**\n- 2.0: All sections present in correct format.\n- 1.0: Missing one section.\n- 0.0: Missing two or more sections, wrong format, or exceeds one page.", "expectation": "A one-page document in Word or PDF format with the specified sections."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Correctness Verification", "description": "Check the correctness of the procedures and clarity of definitions.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 2.5, "rules": [{"type": "code", "name": "Definitions Clarity and Completeness", "description": "Verify all required terms are defined and explained.", "weight": 2.0, "code": "def evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, 'Output is not a valid document.'\n    text = context.files.read_docx_text(output.id) if output.id.endswith('.docx') else context.files.read_pdf_text(output.id)\n    definitions_section = text.lower().split('definitions')[-1]\n    required_terms = ['unscheduled absence', 'lateness', 'medical or family assistance (mfa)']\n    defined_terms = [term for term in required_terms if term in definitions_section]\n    score = len(defined_terms) / len(required_terms)\n    feedback = f\"Defined terms: {', '.join(defined_terms)}\"\n    return score, feedback"}, {"type": "llm_judge", "name": "Procedure Completeness and Coherence", "description": "Evaluate if the procedures logically address the issues raised and are coherent.", "weight": 3.0, "judge_prompt": "Review the 'Procedures' section of the document. Ensure it logically addresses each of the following issues:\n\n1. Staff not notifying lateness timely.\n2. Staff using inconsistent communication methods.\n3. Proper timing for reporting absences.\n4. Supervisors needing to provide support when more details are required.\n5. Communication of MFA file information to HR.\n\nConsider if the procedures are clear, actionable, and logically sequenced.\n\n**Scoring:**\n- 3.0: All issues addressed with clear, coherent procedure.\n- 1.5: Some issues addressed but lack coherence or clarity.\n- 0.0: Procedures are unclear or only address few issues.", "expectation": "Procedures that clearly and logically address the outlined issues."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Quality Assessment", "description": "Evaluate overall document quality, including professionalism and strategic insight.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 1.5, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Tone", "description": "Assess the overall professionalism of the document's presentation and tone.", "weight": 1.5, "judge_prompt": "Evaluate the document for professionalism in presentation and appropriateness in tone for a government administrative setting. Consider elements such as clarity, formality, and visual organization.\n\n**Scoring:**\n- 1.5: Exceptionally professional with appropriate tone and well-organized.\n- 0.75: Generally professional but with minor tone or organization issues.\n- 0.0: Lacks professionalism and has significant tone or organization issues.", "expectation": "Document is clear, formal, and well-organized."}, {"type": "llm_judge", "name": "Strategic Insight and Audience Appropriateness", "description": "Evaluate the policy for strategic value and relevance to intended audience.", "weight": 1.5, "judge_prompt": "Assess the document's strategic insight and relevance for the intended audience - Regional Branches' staff. The policy should demonstrate an understanding of the operational environment and audience needs.\n\n**Scoring:**\n- 1.5: Strong strategic insight with clear relevance to the audience.\n- 0.75: Some strategic insight but lacks full audience relevance.\n- 0.0: Lacks strategic insight or is poorly targeted to the audience.", "expectation": "The policy should be strategically valuable and relevant to the intended audience."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "dfb4e0cd-a0b7-454e-b943-0dd586c2764c", "rubric": {"category_name": "Grants Compliance: Spending Rate Risk Analysis (2 CFR Part 200)", "rationale": "Pattern A: Analytical. The output must be an Excel workbook with a single, verifiable table of flagged awards and precise columns. Stage 1 uses an LLM judge to enforce the exact file shape so that Stage 2 code rules can deterministically verify calculations and flags. Stage 3 assesses presentation quality and usefulness for compliance outreach.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (LLM ONLY)", "description": "Verify the candidate produced a properly structured Excel workbook suitable for automated verification.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Excel Structured Output Requirement", "description": "Output must be a single Excel workbook containing a primary sheet with the required columns and only the requested award rows.", "weight": 4.0, "judge_prompt": "You are validating the STRUCTURE ONLY of the submitted file for a grants spending-rate risk analysis.\n\nAcceptable format:\n- Must be an Excel workbook (.xlsx). Not PDF, not DOCX, not plain text.\n\nPrimary Sheet Requirements (be flexible with the sheet name, e.g., \"Spending Rate Analysis\", \"Flagged Awards\", \"Analysis\"):\n- Must contain a tabular dataset with column headers visible in the first row.\n- The table must include ALL of the following columns (exact or very close synonyms are acceptable; minor variations in punctuation/case are fine):\n  1) \"Recipient Award Number\"\n  2) \"Start Date\"\n  3) \"End Date\"\n  4) \"% Time Elapsed\"\n  5) \"Total Awarded Amt\"\n  6) \"FFR Expenditure Amt\"\n  7) \"% of Funds Spent\"\n  8) \"Spending Rate Analysis\"\n- The \"Spending Rate Analysis\" column must contain categorical values that clearly indicate either \"Fast Spending\" or \"Slow Spending\" for each row.\n- The sheet should include only awards that meet at least one of the two criteria, as of the date 03/31/2025:\n  (a) Over 50% of funds expended AND 25% or less of project time elapsed; or\n  (b) Under 25% of funds expended AND 75% or more of project time elapsed.\n\nHelpful but optional (do NOT penalize if missing):\n- A visible note or header indicating the as-of date (e.g., \"As of 03/31/2025\").\n- A short methodology or footnote indicating how percentages were calculated.\n\nScoring (Structure only \u2014 do NOT verify calculation correctness):\n- 4.0: Excel workbook present; primary sheet with all 8 required columns; appears to only include awards that match the defined criteria; spending labels present (Fast/Slow); table is clearly organized.\n- 3.2: Excel workbook present; primary sheet has all 8 columns but minor naming/order variations; or includes the required table but there is minor ambiguity in labels. Overall structure enables verification.\n- 2.0: Excel workbook present but missing exactly one required column OR mixed with unflagged awards; still plausibly verifiable.\n- 1.0: Excel workbook present but missing multiple required columns or table structure is unclear.\n- 0.0: Not an Excel workbook, or no identifiable table with the required columns.\n\nOnly assess PRESENCE and STRUCTURE. Do not check the math or quality.", "expectation": "A clean .xlsx with a primary sheet holding the specified 8 columns and only rows flagged as Fast or Slow, enabling automated verification."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness Verification (Code + Deterministic Checks)", "description": "Verify calculations and flags using deterministic code checks against the mandated structure.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Parsability and Column Mapping", "description": "Ensure the primary output is an Excel file and locate a sheet with the required columns (allowing flexible synonyms).", "weight": 0.8, "code": "def evaluate(workflow, context):\n    import pandas as pd\n    import numpy as np\n    import re\n    \n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output detected.\"\n    \n    # Define expected logical columns with common aliases\n    aliases = {\n        'recipient_award_number': [\n            'recipient award number', 'award number', 'recipient award no', 'recipient award #', 'award id', 'recipient award id'\n        ],\n        'start_date': ['start date', 'project start date', 'start'],\n        'end_date': ['end date', 'project end date', 'end'],\n        'pct_time': ['% time elapsed', 'percent time elapsed', 'time elapsed %', 'time elapsed', 'pct time'],\n        'total_awarded': ['total awarded amt', 'total award amount', 'award amount', 'total award', 'total awarded amount'],\n        'ffr_expended': ['ffr expenditure amt', 'ffr expended amt', 'expenditure amount', 'ffr expenditure amount', 'expended amount'],\n        'pct_funds': ['% of funds spent', 'percent funds spent', 'funds spent %', '% funds', 'pct funds', 'percent of funds spent'],\n        'analysis': ['spending rate analysis', 'spending analysis', 'rate analysis', 'spending rate']\n    }\n\n    def norm(s):\n        return re.sub(r\"\\s+\", \" \", str(s).strip().lower())\n\n    # Try to find a sheet containing the required columns\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        best = None\n        best_map = None\n        for sheet in xls.sheet_names:\n            try:\n                df = pd.read_excel(path, sheet_name=sheet)\n            except Exception:\n                continue\n            cols = [norm(c) for c in df.columns]\n            col_map = {}\n            # Map each logical column to a physical column if alias matches\n            for key, names in aliases.items():\n                found = None\n                for i, c in enumerate(cols):\n                    if c in [norm(a) for a in names]:\n                        found = df.columns[i]\n                        break\n                if found is not None:\n                    col_map[key] = found\n            # Score this sheet by how many required columns found\n            required_keys = ['recipient_award_number','start_date','end_date','pct_time','total_awarded','ffr_expended','pct_funds','analysis']\n            have = sum(1 for k in required_keys if k in col_map)\n            if best is None or have > best:\n                best = have\n                best_map = (sheet, df, col_map)\n        if best is None:\n            return 0.0, \"No readable sheets.\"\n        sheet, df, col_map = best_map\n        required_keys = ['recipient_award_number','start_date','end_date','pct_time','total_awarded','ffr_expended','pct_funds','analysis']\n        have = sum(1 for k in required_keys if k in col_map)\n        # Score proportionally: all 8 = full credit, fewer reduce score\n        score = (have / 8.0) * 0.8\n        return score, f\"Found sheet '{sheet}' with {have}/8 required columns.\"\n    except Exception as e:\n        return 0.0, f\"Error reading Excel: {e}\""}, {"type": "code", "name": "% Time Elapsed Calculation Accuracy", "description": "Recompute % Time Elapsed as of 2025-03-31 using Start/End Date and compare to provided values within tolerance.", "weight": 1.2, "code": "def evaluate(workflow, context):\n    import pandas as pd\n    import numpy as np\n    import re\n    from datetime import datetime\n\n    def norm(s):\n        return re.sub(r\"\\s+\", \" \", str(s).strip().lower())\n\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output detected.\"\n\n    # Helper to locate sheet/columns similar to previous rule\n    aliases = {\n        'start_date': ['start date', 'project start date', 'start'],\n        'end_date': ['end date', 'project end date', 'end'],\n        'pct_time': ['% time elapsed', 'percent time elapsed', 'time elapsed %', 'time elapsed', 'pct time']\n    }\n\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        best = None\n        best_tuple = None\n        for sheet in xls.sheet_names:\n            try:\n                df = pd.read_excel(path, sheet_name=sheet)\n            except Exception:\n                continue\n            cols = [norm(c) for c in df.columns]\n            col_map = {}\n            for key, names in aliases.items():\n                for i, c in enumerate(cols):\n                    if c in [norm(a) for a in names]:\n                        col_map[key] = df.columns[i]\n                        break\n            have = len(col_map)\n            if best is None or have > best:\n                best = have\n                best_tuple = (sheet, df, col_map)\n        if best_tuple is None:\n            return 0.0, \"No usable sheet.\"\n        sheet, df, col_map = best_tuple\n        if len(col_map) < 3:\n            return 0.0, \"Missing one or more of Start, End, % Time columns.\"\n\n        # Parse dates\n        start = pd.to_datetime(df[col_map['start_date']], errors='coerce')\n        end = pd.to_datetime(df[col_map['end_date']], errors='coerce')\n        as_of = pd.Timestamp('2025-03-31')\n\n        # Compute % time elapsed, clamped 0..1\n        total_days = (end - start).dt.days\n        elapsed_days = (as_of - start).dt.days\n        # Clamp elapsed between 0 and total_days\n        elapsed_days = elapsed_days.clip(lower=0)\n        # Avoid division by zero or negative durations\n        valid = (total_days > 0)\n        pct_calc = pd.Series(np.nan, index=df.index, dtype=float)\n        pct_calc[valid] = (elapsed_days[valid] / total_days[valid]).clip(lower=0, upper=1)\n\n        # Provided values may be 0..1 or 0..100\n        provided = pd.to_numeric(df[col_map['pct_time']], errors='coerce')\n        # Heuristic: if median > 1.5, interpret as percent out of 100\n        scale = 100.0 if provided.median(skipna=True) and provided.median(skipna=True) > 1.5 else 1.0\n        provided_norm = provided / (100.0 if scale == 100.0 else 1.0)\n\n        # Compare within tolerance of 2 percentage points\n        tol = 0.02\n        comparable = valid & provided_norm.notna() & pct_calc.notna()\n        if comparable.sum() == 0:\n            return 0.0, \"No comparable rows for % Time check.\"\n        diff = (provided_norm[comparable] - pct_calc[comparable]).abs()\n        ok = (diff <= tol)\n        frac_ok = ok.mean()\n        score = frac_ok * 1.2\n        return score, f\"% Time Elapsed within tolerance on {ok.sum()}/{comparable.sum()} rows (tol=2pp).\"\n    except Exception as e:\n        return 0.0, f\"Error computing % Time: {e}\""}, {"type": "code", "name": "% of Funds Spent Calculation Accuracy", "description": "Recompute % of Funds Spent as FFR Expenditure Amt / Total Awarded Amt and compare to provided values within tolerance.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    import pandas as pd\n    import numpy as np\n    import re\n\n    def norm(s):\n        return re.sub(r\"\\s+\", \" \", str(s).strip().lower())\n\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output detected.\"\n\n    aliases = {\n        'total_awarded': ['total awarded amt', 'total award amount', 'award amount', 'total award', 'total awarded amount'],\n        'ffr_expended': ['ffr expenditure amt', 'ffr expended amt', 'expenditure amount', 'ffr expenditure amount', 'expended amount'],\n        'pct_funds': ['% of funds spent', 'percent funds spent', 'funds spent %', '% funds', 'pct funds', 'percent of funds spent']\n    }\n\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        best = None\n        best_tuple = None\n        for sheet in xls.sheet_names:\n            try:\n                df = pd.read_excel(path, sheet_name=sheet)\n            except Exception:\n                continue\n            cols = [norm(c) for c in df.columns]\n            col_map = {}\n            for key, names in aliases.items():\n                for i, c in enumerate(cols):\n                    if c in [norm(a) for a in names]:\n                        col_map[key] = df.columns[i]\n                        break\n            have = len(col_map)\n            if best is None or have > best:\n                best = have\n                best_tuple = (sheet, df, col_map)\n        if best_tuple is None:\n            return 0.0, \"No usable sheet.\"\n        sheet, df, col_map = best_tuple\n        if len(col_map) < 3:\n            return 0.0, \"Missing one or more of Total Awarded, FFR Expenditure, % Funds columns.\"\n\n        total = pd.to_numeric(df[col_map['total_awarded']], errors='coerce')\n        exp = pd.to_numeric(df[col_map['ffr_expended']], errors='coerce')\n        provided = pd.to_numeric(df[col_map['pct_funds']], errors='coerce')\n\n        # Handle zeros and negatives sanely\n        valid = total > 0\n        pct_calc = pd.Series(np.nan, index=df.index, dtype=float)\n        pct_calc[valid] = (exp[valid] / total[valid]).astype(float)\n\n        # Normalize provided (0..1 vs 0..100)\n        med = provided.median(skipna=True)\n        scale = 100.0 if (med is not None and pd.notna(med) and med > 1.5) else 1.0\n        provided_norm = provided / (100.0 if scale == 100.0 else 1.0)\n\n        comparable = valid & provided_norm.notna() & pct_calc.notna()\n        if comparable.sum() == 0:\n            return 0.0, \"No comparable rows for % Funds check.\"\n\n        tol = 0.01  # 1 percentage point when expressed as 0..1\n        diff = (provided_norm[comparable] - pct_calc[comparable]).abs()\n        ok = (diff <= tol)\n        frac_ok = ok.mean()\n\n        # Additional plausibility: expenditures should not be wildly above total; allow up to 105%\n        plausible = (pct_calc[comparable] <= 1.05)\n        plaus_frac = plausible.mean()\n\n        # Score: 80% weight on accuracy, 20% on plausibility\n        score = (frac_ok * 0.8 + plaus_frac * 0.2) * 1.0\n        return score, f\"% Funds within tolerance on {ok.sum()}/{comparable.sum()} rows; plausibility ok on {plausible.sum()}/{comparable.sum()} rows.\"\n    except Exception as e:\n        return 0.0, f\"Error computing % Funds: {e}\""}, {"type": "code", "name": "Criteria and Flag Consistency", "description": "Check that all listed rows meet the Fast/Slow criteria as of 2025-03-31 and that the Spending Rate Analysis labels match.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    import pandas as pd\n    import numpy as np\n    import re\n    from datetime import datetime\n\n    def norm(s):\n        return re.sub(r\"\\s+\", \" \", str(s).strip().lower())\n\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output detected.\"\n\n    aliases = {\n        'start_date': ['start date', 'project start date', 'start'],\n        'end_date': ['end date', 'project end date', 'end'],\n        'pct_time': ['% time elapsed', 'percent time elapsed', 'time elapsed %', 'time elapsed', 'pct time'],\n        'total_awarded': ['total awarded amt', 'total award amount', 'award amount', 'total award', 'total awarded amount'],\n        'ffr_expended': ['ffr expenditure amt', 'ffr expended amt', 'expenditure amount', 'ffr expenditure amount', 'expended amount'],\n        'pct_funds': ['% of funds spent', 'percent funds spent', 'funds spent %', '% funds', 'pct funds', 'percent of funds spent'],\n        'analysis': ['spending rate analysis', 'spending analysis', 'rate analysis', 'spending rate']\n    }\n\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        best = None\n        best_tuple = None\n        for sheet in xls.sheet_names:\n            try:\n                df = pd.read_excel(path, sheet_name=sheet)\n            except Exception:\n                continue\n            cols = [norm(c) for c in df.columns]\n            col_map = {}\n            for key, names in aliases.items():\n                for i, c in enumerate(cols):\n                    if c in [norm(a) for a in names]:\n                        col_map[key] = df.columns[i]\n                        break\n            have = len(col_map)\n            if best is None or have > best:\n                best = have\n                best_tuple = (sheet, df, col_map)\n        if best_tuple is None:\n            return 0.0, \"No usable sheet.\"\n        sheet, df, m = best_tuple\n        required = ['start_date','end_date','pct_time','total_awarded','ffr_expended','pct_funds','analysis']\n        if not all(k in m for k in required):\n            return 0.0, \"Missing required columns for criteria check.\"\n\n        # Normalize % columns (0..1 vs 0..100)\n        pct_time = pd.to_numeric(df[m['pct_time']], errors='coerce')\n        pct_funds = pd.to_numeric(df[m['pct_funds']], errors='coerce')\n        if pct_time.median(skipna=True) is not None and pd.notna(pct_time.median()) and pct_time.median() > 1.5:\n            pct_time = pct_time / 100.0\n        if pct_funds.median(skipna=True) is not None and pd.notna(pct_funds.median()) and pct_funds.median() > 1.5:\n            pct_funds = pct_funds / 100.0\n\n        # Determine which rows meet fast/slow criteria\n        is_fast = (pct_funds > 0.50) & (pct_time <= 0.25)\n        is_slow = (pct_funds < 0.25) & (pct_time >= 0.75)\n        meets = is_fast | is_slow\n\n        # Compare to labels\n        labels = df[m['analysis']].astype(str).str.strip().str.lower()\n        labeled_fast = labels.str.contains('fast')\n        labeled_slow = labels.str.contains('slow')\n        label_ok = (is_fast & labeled_fast) | (is_slow & labeled_slow)\n\n        # Only rows meeting criteria should be present\n        if len(df) == 0:\n            return 0.0, \"No rows present.\"\n\n        meet_frac = meets.mean()\n        label_frac = label_ok.mean()\n\n        # Score components: 50% presence of only qualifying rows, 50% correct labels\n        score = (0.5 * meet_frac + 0.5 * label_frac) * 1.0\n        return score, f\"Criteria met on {int(meets.sum())}/{len(df)} rows; labels consistent on {int(label_ok.sum())}/{len(df)} rows.\"\n    except Exception as e:\n        return 0.0, f\"Error checking criteria/labels: {e}\""}, {"type": "code", "name": "Identifier and Amount Plausibility", "description": "Basic sanity checks: Recipient Award Number present, nonnegative amounts, and expenditures not grossly exceeding total.", "weight": 0.2, "code": "def evaluate(workflow, context):\n    import pandas as pd\n    import numpy as np\n    import re\n\n    def norm(s):\n        return re.sub(r\"\\s+\", \" \", str(s).strip().lower())\n\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output detected.\"\n\n    aliases = {\n        'recipient_award_number': ['recipient award number', 'award number', 'recipient award no', 'recipient award #', 'award id', 'recipient award id'],\n        'total_awarded': ['total awarded amt', 'total award amount', 'award amount', 'total award', 'total awarded amount'],\n        'ffr_expended': ['ffr expenditure amt', 'ffr expended amt', 'expenditure amount', 'ffr expenditure amount', 'expended amount']\n    }\n\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        best = None\n        best_tuple = None\n        for sheet in xls.sheet_names:\n            try:\n                df = pd.read_excel(path, sheet_name=sheet)\n            except Exception:\n                continue\n            cols = [norm(c) for c in df.columns]\n            col_map = {}\n            for key, names in aliases.items():\n                for i, c in enumerate(cols):\n                    if c in [norm(a) for a in names]:\n                        col_map[key] = df.columns[i]\n                        break\n            have = len(col_map)\n            if best is None or have > best:\n                best = have\n                best_tuple = (sheet, df, col_map)\n        if best_tuple is None:\n            return 0.0, \"No usable sheet.\"\n        sheet, df, m = best_tuple\n        if not all(k in m for k in ['recipient_award_number','total_awarded','ffr_expended']):\n            return 0.0, \"Missing columns for plausibility checks.\"\n\n        ids_ok = df[m['recipient_award_number']].notna() & (df[m['recipient_award_number']].astype(str).str.strip() != '')\n        tot = pd.to_numeric(df[m['total_awarded']], errors='coerce')\n        exp = pd.to_numeric(df[m['ffr_expended']], errors='coerce')\n        amounts_ok = (tot.notna() & (tot > 0) & exp.notna() & (exp >= 0))\n        # Expenditures normally should not exceed 110% (allow slight anomalies)\n        ratio = pd.Series(np.nan, index=df.index)\n        mask = amounts_ok\n        ratio[mask] = exp[mask] / tot[mask]\n        ratio_ok = (ratio <= 1.10) | ratio.isna()\n\n        if len(df) == 0:\n            return 0.0, \"No rows present.\"\n        frac = (ids_ok & amounts_ok & ratio_ok).mean()\n        score = frac * 0.2\n        return score, f\"Plausibility passed on {int((ids_ok & amounts_ok & ratio_ok).sum())}/{len(df)} rows.\"\n    except Exception as e:\n        return 0.0, f\"Error in plausibility checks: {e}\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Presentation Quality and Utility", "description": "Assess whether the workbook is professionally presented and immediately usable for compliance outreach.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Actionability", "description": "Evaluate formatting, clarity, and usefulness for contacting recipients regarding 2 CFR Part 200 compliance.", "weight": 2.0, "judge_prompt": "Assess the Excel workbook for professional presentation and immediate utility to a grants compliance officer:\n\nLook for the following (do not re-check math):\n- Clear, descriptive sheet name (e.g., \"Spending Rate Analysis\" or similar) and an optional visible note indicating \"As of 03/31/2025\".\n- Clean header row, readable column names, frozen header or filterable table (if visible), and consistent formatting.\n- Dates appear as dates, currency columns (Total Awarded Amt, FFR Expenditure Amt) appear as currency, and percent columns appear as percentages.\n- The \"Spending Rate Analysis\" labels are clear and consistent (e.g., \"Fast Spending\", \"Slow Spending\").\n- Optional but helpful: a brief footnote or note on methodology/criteria.\n\nScoring:\n- 2.0: Professionally formatted; date/percent/currency formatting appears correct; clear labeling; contains as-of date or concise note; immediately usable for outreach.\n- 1.0: Generally readable; minor formatting inconsistencies (e.g., some numeric formatting off) but usable.\n- 0.5: Cluttered or inconsistent presentation; requires cleanup before use.\n- 0.0: Poor structure/presentation; difficult to use.\n\nJudge only presentation/utility \u2014 not calculation correctness.", "expectation": "A clean, well-formatted analysis sheet with professional headings, correct numeric formats, and clear flags suitable for contacting recipients."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "ed2bc14c-99ac-4a2a-8467-482a1a5d67f3", "rubric": {"category_name": "Tenant Retention Strategy (Property Management)", "rationale": "Mixed-output task: a concise business memo (DOCX/PDF) that embeds verifiable, data-referenced analysis and structured plans. Stage 1 uses an LLM judge to strictly enforce memo shape so later verification is trivial. Stage 2 uses code rules to verify key requirements and internal consistency from the extracted document text. Stage 3 uses an LLM judge for professional quality and strategic appropriateness.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate (LLM)", "description": "Gate: Output must be a 1\u20132 page professional business memo in DOCX (preferred) or PDF with explicit, verifiable sections and tables enabling downstream checks.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 2.8, "rules": [{"type": "llm_judge", "name": "Required Memo Structure Present", "description": "Verify the candidate produced a 1\u20132 page professional memo (DOCX or PDF) with all required sections, tables, and elements that make verification trivial. Only check presence/structure, not content correctness.", "weight": 4.0, "judge_prompt": "You are evaluating whether the output is a properly structured business memo suitable for verification. Only check format and presence of required elements, not the correctness of content.\n\nAcceptable formats: DOCX (preferred) or PDF. Reject plain text or spreadsheets.\n\nLength: 1\u20132 pages (flexible by layout; 1\u20133 pages OK if dense formatting, but aim for concise memo).\n\nRequired structure (be flexible with exact header names but insist on these elements):\n\nA) Memo Header Block on first page:\n- To, From, Date, Subject lines. Subject must reference \u201cTenant Retention Strategy\u201d (or very close) and Harborview Flats.\n\nB) Executive Summary (first page):\n- 3\u20135 bullet highlights including the retention goal: increase resident retention by 10% in the next 6 months.\n\nC) Section 1: Departure Reasons Analysis:\n- Mentions analysis of Exit Survey Feedback.xlsx (method summary is fine).\n- Categorization into exactly five reasons (e.g., rent increase too high, lack of community, etc.).\n- A small table labeled like \u201cExit Survey Categorization\u201d with columns similar to: [Category | Definition | Count | Percent of Responses | Example Comment].\n- A callout identifying the Top 2 reasons and a brief analysis (2\u20134 sentences total or per reason).\n\nD) Section 2: Tiered Renewal Offer Structure:\n- A table summarizing three tiers: Early Bird (90 days), Standard (60 days), and Month-to-Month premium.\n- Columns similar to: [Trigger | Offer Elements | Eligibility/Exclusions | Est. Cost/Unit | Est. Uptake].\n- Shows at least one numeric ($ or %) for each tier.\n\nE) Section 3: Communication Plan:\n- A timeline or table for 90-day, 60-day, and 30-day renewal emails.\n- For each, include: Subject line, 2\u20134 key message points, and a clear CTA.\n- Include short draft snippets or phrasing that demonstrate tone alignment (may reference Current Renewal Letter.docx).\n\nF) Section 4: Community Engagement Initiatives:\n- Two low-cost (target <$500 each) high-impact event ideas with basic logistics and a success metric to measure retention impact.\n\nG) Section 5: KPI & Measurement:\n- States target: +10% retention in 6 months.\n- Defines at least 3 KPIs (e.g., early renewals, event participation, make-ready cost, loss to lease) with baseline/target fields or placeholders.\n\nH) Appendix A: Data and Assumptions:\n- Notes data sources (Exit Survey Feedback.xlsx, Current Renewal Letter.docx if referenced), and any assumptions or external sources used.\n\nScoring for this gate (structure only):\n- 4.0: All required sections A\u2013H present; memo format (DOCX/PDF); clear tables where specified; concise/ professional layout.\n- 3.2: Missing exactly one non-core element (either KPI & Measurement OR Appendix), all other sections present.\n- 2.4: Missing one core section among C, D, or E OR memo runs far beyond 2 pages without concise structure.\n- 1.2: Missing multiple core sections, or format issues (not clearly a memo, no tables where required).\n- 0.0: Not DOCX/PDF, or grossly wrong format (e.g., spreadsheet/markdown only), or lacks most sections.\n\nOnly assess presence/format, not correctness of analysis or values.", "expectation": "A concise DOCX/PDF memo with required sections and tables enabling verification in later stages."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification of Requirements and Internal Consistency (Code)", "description": "Deterministic checks on key task requirements using extracted document text. Flexible, fuzzy matching; robust to minor wording variations.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Targets and Timeline Signals", "description": "Verify presence of retention target (+10%) and 6-month horizon, and the 90/60/30-day cadence.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No primary document output.\"\n    text = \"\"\n    try:\n        if str(output.mime_type or '').lower().endswith('pdf') or (hasattr(output, 'extension') and str(output.extension).lower()=='.pdf'):\n            text = context.files.read_pdf_text(output.id)\n        else:\n            text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                text = \"\"\n    if not text:\n        return 0.0, \"Unable to extract document text.\"\n    t = text.lower()\n\n    # Retention target ~10%\n    ten_percent = bool(re.search(r\"(\\b10\\s*%\\b|\\b10\\s*percent\\b|\\bten\\s*percent\\b)\", t))\n    retention_ctx = (\"retention\" in t)\n\n    # 6-month horizon\n    six_months = bool(re.search(r\"\\b(6|six)\\s*(months|mos|month)\\b\", t))\n\n    # Cadence 90/60/30 days\n    def has_day(n):\n        return bool(re.search(rf\"\\b{n}\\s*[- ]?\\s*day(s)?\\b\", t))\n    d90, d60, d30 = has_day(90), has_day(60), has_day(30)\n\n    checks = [ten_percent and retention_ctx, six_months, d90, d60, d30]\n    score = sum(1.0 for c in checks if c) / len(checks) * 1.0\n    fb = []\n    if not (ten_percent and retention_ctx): fb.append(\"Missing explicit +10% retention target.\")\n    if not six_months: fb.append(\"Missing 6-month horizon.\")\n    if not d90: fb.append(\"Missing 90-day reference.\")\n    if not d60: fb.append(\"Missing 60-day reference.\")\n    if not d30: fb.append(\"Missing 30-day reference.\")\n    return score, \"; \".join(fb) if fb else \"All target/timeline signals present.\""}, {"type": "code", "name": "Tiered Offer Numeric Plausibility", "description": "Check that each tier (Early 90-day, Standard 60-day, Month-to-Month premium) includes numeric incentives/premiums ($ or %) and correct timing references.", "weight": 1.2, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = \"\"\n    t = text.lower()\n\n    def has_num_near(keyword, window=180):\n        idx = t.find(keyword)\n        if idx == -1:\n            return False\n        start = max(0, idx - window)\n        end = min(len(t), idx + window)\n        seg = t[start:end]\n        return bool(re.search(r\"(\\$\\s?\\d{1,3}(,\\d{3})*(\\.\\d+)?|\\b\\d{1,2}\\s?%\\b)\", seg))\n\n    # Early Bird @ 90 days with numeric\n    has_early = (\"early\" in t or \"early bird\" in t) and bool(re.search(r\"\\b90\\b\", t)) and (has_num_near(\"early\") or has_num_near(\"90\"))\n\n    # Standard @ 60 days with numeric\n    std_kw = (\"standard\" in t or \"renewal offer\" in t or \"standard offer\" in t)\n    has_std = std_kw and bool(re.search(r\"\\b60\\b\", t)) and (has_num_near(\"standard\") or has_num_near(\"60\") or has_num_near(\"renewal offer\"))\n\n    # Month-to-Month with premium numeric\n    mtm_kw = (\"month-to-month\" in t or \"mtm\" in t)\n    has_mtm = mtm_kw and (\"premium\" in t or \"surcharge\" in t or \"markup\" in t) and (has_num_near(\"month-to-month\") or has_num_near(\"mtm\") or has_num_near(\"premium\"))\n\n    checks = [has_early, has_std, has_mtm]\n    score = sum(1.0 for c in checks if c) / 3.0 * 1.2\n    return score"}, {"type": "code", "name": "Departure Reasons: Five Categories and Top Two Identified", "description": "Verify the memo names five categories for departure reasons and explicitly calls out the top two reasons with a brief analysis.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = \"\"\n    t = text.lower()\n\n    # Evidence of five categories\n    five_phrase = bool(re.search(r\"\\b(five|5)\\s+categor\", t))\n    table_hint = (\"category\" in t and \"percent\" in t) or (\"category\" in t and \"count\" in t)\n    enum_list = bool(re.search(r\"\\b1[\\).]\\s|\\b2[\\).]\\s|\\b3[\\).]\\s|\\b4[\\).]\\s|\\b5[\\).]\\s\", t))\n    has_five = five_phrase or table_hint or enum_list\n\n    # Top two reasons explicitly identified\n    top_two_phrase = bool(re.search(r\"top\\s*(two|2)\\s+reason\", t)) or bool(re.search(r\"\\btop\\s*2\\b\", t))\n\n    # Brief analysis presence heuristic: a couple of sentences around 'top'\n    analysis = False\n    m = re.search(r\"top\\s*(two|2).{0,300}\", t)\n    if m:\n        seg = m.group(0)\n        analysis = seg.count('.') >= 1 or seg.count(';') >= 1\n\n    checks = [has_five, top_two_phrase, analysis]\n    score = sum(1.0 for c in checks if c) / 3.0 * 1.0\n    return score"}, {"type": "code", "name": "Communication Drafts Coverage", "description": "Check the presence of 90/60/30-day email drafts with Subject lines and CTAs.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = \"\"\n    t = text.lower()\n\n    def has_day_block(n):\n        return bool(re.search(rf\"\\b{n}\\s*[- ]?day\", t))\n\n    have_90 = has_day_block(90)\n    have_60 = has_day_block(60)\n    have_30 = has_day_block(30)\n\n    subjects = len(re.findall(r\"subject\\s*:\\s*\", t)) >= 2  # allow flexible\n    ctas = (\"cta\" in t) or (\"call to action\" in t) or (\"renew now\" in t) or (\"apply now\" in t)\n\n    checks = [have_90, have_60, have_30, subjects, ctas]\n    score = sum(1.0 for c in checks if c) / len(checks) * 0.5\n    return score"}, {"type": "code", "name": "Community Events: Low-Cost and Measurable", "description": "Confirm at least two low-cost event proposals (costs <= $500 each) and a measurable success metric is mentioned.", "weight": 0.3, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = \"\"\n    t = text.lower()\n\n    # Dollar amounts\n    amounts = []\n    for m in re.finditer(r\"\\$\\s*([0-9]{1,3}(?:,[0-9]{3})*(?:\\.[0-9]{1,2})?|[0-9]{1,3}(?:\\.[0-9]{1,2})?)\", t):\n        try:\n            val = float(m.group(1).replace(',', ''))\n            amounts.append(val)\n        except Exception:\n            pass\n    low_cost_counts = sum(1 for v in amounts if v <= 500)\n    have_two_low_cost = low_cost_counts >= 2\n\n    # Metrics keywords\n    metric = any(k in t for k in [\"attendance\", \"rsvp\", \"survey\", \"nps\", \"participation\", \"renewal conversion\", \"retention rate\"])\n\n    checks = [have_two_low_cost, metric]\n    score = sum(1.0 for c in checks if c) / len(checks) * 0.3\n    return score"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Strategic Fit (LLM)", "description": "Holistic assessment of professionalism, feasibility, property context fit, and strategic value.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professionalism, Feasibility, and Strategic Value", "description": "Judge overall memo quality: clarity, tone, feasibility of offers/timeline/events, alignment with Harborview Flats context (Stamford, amenities), and non-discriminatory, resident-friendly communication.", "weight": 2.0, "judge_prompt": "Evaluate the submitted memo on overall professional quality and strategic value. Consider:\n\n1) Professional presentation: concise 1\u20132 page memo, clear headings, tables readable, no obvious grammar issues, actionable tone.\n2) Strategic feasibility: Renewal tiers are realistic (cost-aware), month-to-month premium is sensible, early/standard timing aligns with leasing norms, and communication cadence is practical.\n3) Context fit: Tactics leverage Harborview Flats context (Stamford, 200 units, amenities like lounge, WFH zones, front lawn) and address tenant turnover causes.\n4) Risk and compliance: Avoids discriminatory language; suitable for broad resident audience; respectful tone.\n5) Measurability: KPIs are meaningful and tied to the goal; includes how to track (e.g., early renewals, event participation, make-ready cost).\n\nScoring (0\u20132.0):\n- 2.0: Highly professional, strategically strong, feasible, well-aligned with context, and clearly measurable.\n- 1.4: Generally professional and feasible with minor gaps (e.g., one weak KPI or vague tactic).\n- 0.8: Mixed quality; some actionable elements but notable weaknesses or unclear feasibility.\n- 0.0: Unprofessional or unrealistic; misaligned with task context or tone.\n\nProvide a brief justification for the score.", "expectation": "A polished, resident-friendly, and actionable memo tailored to Harborview Flats, with feasible tactics and clear KPIs."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
