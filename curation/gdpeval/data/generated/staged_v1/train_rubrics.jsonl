{"task_id": "40a8c4b1-b169-4f92-a38b-7f79685037ec", "rubric": {"category_name": "Grand Rounds 2025 Schedule (Otolaryngology)", "rationale": "This rubric enforces a self-documenting Excel deliverable for the 2025 grand rounds schedule. Stage 1 (LLM-only) mandates a very specific workbook shape that enables automated checks. Stage 2 uses code rules to verify correctness on key requirements (dates, late-February In-Service Study Session, semi-annual All Periop, MS4 presence, and unused optional topic consistency), plus an LLM check for yellow highlighting. Stage 3 assesses professional quality and readability for stakeholders.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Structural Gate (LLM only)", "description": "Gate: Verify the output is a single Excel workbook with the exact structural elements needed for verification. Only presence/format/structure, not correctness.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.2, "rules": [{"type": "llm_judge", "name": "Workbook Format and Required Sheets/Sections", "description": "Check the candidate output is an Excel workbook with the mandated sheets and structures to enable verification.", "weight": 4.0, "judge_prompt": "You are evaluating whether the submitted file has the required structure to enable verification. Only assess presence/format/structure; do not judge correctness of content or calculations.\n\nRequirements (be flexible with minor naming variations, but the intent must be clear):\n\n1) File format\n- Must be an Excel workbook (.xlsx). Not PDF/DOCX/CSV.\n\n2) Sheet: \"2025 Grand Rounds Schedule\"\n- The sheet name should explicitly say 2025 and Grand Rounds Schedule (e.g., exactly \"2025 Grand Rounds Schedule\").\n- It should represent weekly Wednesday sessions across the 2025 calendar year (Jan\u2013Dec), excluding holidays if marked.\n- Column C must be the Topic/Session column and its bordered cells (excluding row 1 header) must be populated with session names. You can verify by visually inspecting that column C\u2019s bordered cells contain text entries for sessions.\n- The sheet should clearly convey: Date, that sessions are Wednesdays, and the 7:00\u20139:00 AM time window (either in a dedicated Time column, header note, or visible label). Minor variations in exact column headers are acceptable if the structure is clear.\n- The following session labels should appear somewhere in this schedule sheet (case-insensitive, flexible phrasing): \"All Periop\", \"MS4 Talks\", and \"In-Service Study Session\" (just check presence, not counts or placement at this stage).\n\n3) Sheet: \"Topics & Labs\"\n- Contains two clearly delineated sections/tables: Required Topics & Labs and Optional Topics & Labs. Each must include at least a Title/Topic name column with visible entries.\n- Optional topics or labs not used in the schedule must be highlighted in yellow in this sheet, OR there must be a clearly labeled companion listing of unused optional topics (see item 4). At this stage, just confirm that the highlighting or companion list exists; do not validate completeness.\n\n4) Sheet: Companion structure for auditability\n- Either a sheet named \"Unused Optional Topics\" listing optional items not used, or a clearly labeled section within Topics & Labs that lists unused optional items.\n- A \"Compliance Log\" or similarly named sheet/section that includes a simple Scheduling Log with at least: Date, Topic/Session, Source Category (Scheduled Meeting | Required | Optional). It can be a compact table; do not assess content quality here\u2014just presence.\n\nScoring\n- 4.0: All above items present and clearly structured; column C on schedule filled; required sheets/sections exist; presence of the named sessions visible; and a compliance log plus unused optional listing/highlights exist.\n- 3.0: Minor structural omissions (e.g., compliance log present but minimal; unused optional visible only in one form; or time labeling implicit but still clearly 7\u20139 AM).\n- 2.0: Major omissions but workbook still plausibly structured (e.g., Topics & Labs lacks clear separation of required vs optional, or schedule sheet present but column C isn\u2019t clearly the session column).\n- 1.0: Workbook exists but required sheets/sections largely missing or mislabeled (cannot reliably verify later).\n- 0.0: Not an Excel workbook, or the key schedule sheet is missing.\n\nOnly check structure, presence, and formatting\u2014not correctness or counts.", "expectation": "A single .xlsx workbook containing: a properly named 2025 Grand Rounds Schedule sheet with column C populated for sessions and visible 7\u20139 AM timing; a Topics & Labs sheet with Required and Optional sections; unused optional items highlighted in yellow or listed; and a simple Compliance Log/Scheduling Log."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness Verification", "description": "Now that the structure exists, verify key scheduling requirements with code and targeted LLM checks.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Wednesday/Time/Blank Sanity", "description": "Verify schedule rows are Wednesdays in 2025, 7\u20139 AM is indicated, and topics are filled (no blank Topic in dated rows).", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        # Find schedule sheet\n        target_names = [s for s in xls.sheet_names if '2025' in s.lower() and 'grand' in s.lower() and 'round' in s.lower() and 'schedule' in s.lower()]\n        sheet_name = target_names[0] if target_names else xls.sheet_names[0]\n        df = pd.read_excel(path, sheet_name=sheet_name)\n    except Exception as e:\n        return 0.0, f\"Error reading workbook: {e}\"\n\n    # Helper: find date column\n    date_col = None\n    for col in df.columns:\n        try:\n            ser = pd.to_datetime(df[col], errors='coerce')\n            if ser.notna().sum() >= max(3, int(len(df)*0.2)):\n                date_col = col\n                break\n        except Exception:\n            continue\n    if date_col is None:\n        return 0.1, \"Could not identify date column\"\n\n    # Topic column: prefer column C (index 2) if exists; else by header keywords; else fallback to widest text column\n    topic_col = None\n    if len(df.columns) >= 3:\n        topic_col = df.columns[2]\n    # Try header keywords\n    for col in df.columns:\n        name = str(col).lower()\n        if any(k in name for k in ['topic', 'session', 'talk', 'title', 'activity']):\n            topic_col = col\n            break\n\n    dates = pd.to_datetime(df[date_col], errors='coerce')\n    mask_2025 = dates.dt.year == 2025\n    valid = dates[mask_2025 & dates.notna()]\n    if valid.empty:\n        return 0.2, \"No 2025 dates found\"\n\n    # Wednesday check\n    weds_ratio = (valid.dt.weekday == 2).mean()  # 0=Mon, 2=Wed\n\n    # Time check: look for 7-9 AM signals in headers and first ~10 rows\n    text_snips = []\n    head_text = ' '.join([str(c) for c in df.columns])\n    text_snips.append(head_text)\n    sample_rows = df.head(10).astype(str).apply(lambda r: ' '.join(r.values), axis=1).tolist()\n    text_snips.extend(sample_rows)\n    blob = ' '.join(text_snips).lower()\n    time_ok = any(tok in blob for tok in ['7:00-9:00', '7:00 \u2013 9:00', '7\u20139 am', '7-9 am', '7:00 am - 9:00 am', '7:00am-9:00am', '7 to 9 am'])\n\n    # Topic fill check\n    fill_ratio = 1.0\n    if topic_col is not None:\n        topics = df.loc[dates.notna() & mask_2025, topic_col]\n        if len(topics) > 0:\n            filled = topics.astype(str).str.strip().replace({'nan':'', 'None':''}).astype(bool).mean()\n            fill_ratio = float(filled)\n\n    # Combine: weights within this 0.5 rule\n    score = 0.0\n    score += 0.25 * min(1.0, max(0.0, weds_ratio))\n    score += 0.15 * (1.0 if time_ok else 0.0)\n    score += 0.10 * min(1.0, fill_ratio)\n    feedback = f\"Wed ratio={weds_ratio:.2f}, time_ok={time_ok}, topic_fill={fill_ratio:.2f}\"\n    return score, feedback"}, {"type": "code", "name": "In-Service Study Session in late February", "description": "Ensure 'In-Service Study Session' is scheduled on the last or second-to-last Wednesday of February 2025 (Feb 19 or Feb 26).", "weight": 2.0, "code": "import re\nimport pandas as pd\nfrom datetime import datetime\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        # Find schedule sheet\n        cand = [s for s in xls.sheet_names if '2025' in s.lower() and 'grand' in s.lower() and 'round' in s.lower() and 'schedule' in s.lower()]\n        sheet_name = cand[0] if cand else xls.sheet_names[0]\n        df = pd.read_excel(path, sheet_name=sheet_name)\n    except Exception as e:\n        return 0.0, f\"Read error: {e}\"\n\n    # Date column\n    date_col = None\n    for col in df.columns:\n        ser = pd.to_datetime(df[col], errors='coerce')\n        if ser.notna().sum() >= max(3, int(len(df)*0.2)):\n            date_col = col\n            break\n    if date_col is None:\n        return 0.0, \"No date column\"\n\n    # Topic column\n    topic_col = df.columns[2] if len(df.columns) >= 3 else None\n    for col in df.columns:\n        name = str(col).lower()\n        if any(k in name for k in ['topic', 'session', 'talk', 'title', 'activity']):\n            topic_col = col\n            break\n    if topic_col is None:\n        return 0.0, \"No topic column\"\n\n    dates = pd.to_datetime(df[date_col], errors='coerce')\n    topics = df[topic_col].astype(str).str.lower()\n\n    target_dates = {pd.Timestamp('2025-02-19'), pd.Timestamp('2025-02-26')}\n    hits = 0\n    for idx, d in dates.items():\n        if pd.isna(d):\n            continue\n        if d in target_dates:\n            t = topics.iloc[idx]\n            if ('in-service' in t or 'in service' in t) and ('study' in t) and ('session' in t):\n                hits += 1\n    score = 2.0 if hits >= 1 else 0.0\n    fb = \"OK\" if hits >= 1 else \"Missing required In-Service Study Session on Feb 19 or 26\"\n    return score, fb"}, {"type": "code", "name": "All Periop semiannual and MS4 presence", "description": "Check at least two 'All Periop' sessions in 2025 and presence of 'MS4 Talks' (or similar) at least once.", "weight": 1.5, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        cand = [s for s in xls.sheet_names if '2025' in s.lower() and 'grand' in s.lower() and 'round' in s.lower() and 'schedule' in s.lower()]\n        sheet_name = cand[0] if cand else xls.sheet_names[0]\n        df = pd.read_excel(path, sheet_name=sheet_name)\n    except Exception as e:\n        return 0.0, f\"Read error: {e}\"\n\n    # Identify topic column\n    topic_col = df.columns[2] if len(df.columns) >= 3 else None\n    for col in df.columns:\n        if any(k in str(col).lower() for k in ['topic', 'session', 'talk', 'title', 'activity']):\n            topic_col = col\n            break\n    if topic_col is None:\n        return 0.0, \"No topic column\"\n\n    topics = df[topic_col].astype(str).str.lower().fillna('')\n\n    # Count All Periop variants\n    periop_count = topics.str.contains('periop', regex=False).sum()\n\n    # MS4 presence (various variants)\n    ms4_patterns = [r'\\bms4\\b', r'\\bms 4\\b', r'\\bm4\\b', r'med(ical)? student.*(talk|present|presentation)', r'fourth[- ]year.*(talk|present|presentation)']\n    ms4_present = any(topics.str.contains(pat, regex=True, case=False, na=False).any() for pat in ms4_patterns)\n\n    # Scoring: 60% for periop>=2, 40% for ms4 presence\n    periop_score = 0.9 if periop_count >= 2 else min(0.9, 0.45 * periop_count)  # partial if 1 occurrence\n    ms4_score = 0.6 if ms4_present else 0.0\n    score = min(1.5, (periop_score + ms4_score))\n    feedback = f\"All Periop count={periop_count}, MS4 present={ms4_present}\"\n    return score, feedback"}, {"type": "code", "name": "Unused Optional Topics not scheduled", "description": "Optional items listed as unused should not appear in the scheduled topics.", "weight": 1.0, "code": "import pandas as pd\nimport re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        # Find schedule\n        sched_names = [s for s in xls.sheet_names if '2025' in s.lower() and 'grand' in s.lower() and 'round' in s.lower() and 'schedule' in s.lower()]\n        sched_sheet = sched_names[0] if sched_names else xls.sheet_names[0]\n        df_sched = pd.read_excel(path, sheet_name=sched_sheet)\n        # Find unused optional sheet\n        unused_names = [s for s in xls.sheet_names if 'unused' in s.lower() and 'optional' in s.lower()]\n        if not unused_names:\n            # Try within Topics & Labs by looking for a sheet with that name\n            tab_candidates = [s for s in xls.sheet_names if 'topics' in s.lower() and 'lab' in s.lower()]\n            if not tab_candidates:\n                return 0.2, \"No 'Unused Optional' or 'Topics & Labs' sheet\"\n            df_tab = pd.read_excel(path, sheet_name=tab_candidates[0])\n            # Heuristic: look for a column header containing 'unused' and 'optional'\n            possible_cols = [c for c in df_tab.columns if 'unused' in str(c).lower() and 'optional' in str(c).lower()]\n            if possible_cols:\n                unused_list = df_tab[possible_cols[0]].dropna().astype(str).str.strip().tolist()\n            else:\n                return 0.2, \"Could not locate unused optional list\"\n        else:\n            df_unused = pd.read_excel(path, sheet_name=unused_names[0])\n            # Use first text-like column\n            text_cols = [c for c in df_unused.columns if df_unused[c].astype(str).str.strip().ne('').sum() > 0]\n            if not text_cols:\n                return 0.2, \"Unused optional sheet empty\"\n            unused_list = df_unused[text_cols[0]].dropna().astype(str).str.strip().tolist()\n\n        # Topic column in schedule\n        topic_col = df_sched.columns[2] if len(df_sched.columns) >= 3 else None\n        for col in df_sched.columns:\n            if any(k in str(col).lower() for k in ['topic', 'session', 'talk', 'title', 'activity']):\n                topic_col = col\n                break\n        if topic_col is None:\n            return 0.2, \"No topic column in schedule\"\n        sched_topics = df_sched[topic_col].dropna().astype(str).str.lower().tolist()\n\n        if not unused_list:\n            return 0.5, \"No unused optional items listed\"\n\n        checks = []\n        for u in unused_list:\n            u_low = u.lower()\n            present = any(u_low in t for t in sched_topics)\n            checks.append(0 if present else 1)\n        ratio = sum(checks) / len(checks) if checks else 1.0\n        score = ratio * 1.0\n        fb = f\"Unused optional not scheduled ratio={ratio:.2f} (1.0 is best)\"\n        return score, fb"}, {"type": "llm_judge", "name": "Yellow highlighting of unused optional topics", "description": "Visually verify that optional topics not used are highlighted in yellow within the Topics & Labs sheet (or that a clearly labeled unused list is present if highlighting is not feasible).", "weight": 0.5, "judge_prompt": "Open the workbook. Inspect the \"Topics & Labs\" sheet (and any clearly labeled unused optional section or sheet). Determine if optional topics/labs that were NOT used in the schedule are visibly highlighted in yellow. If a companion sheet/section lists unused optional items instead of highlighting, accept that as adequate evidence but award slightly less than full credit.\n\nScoring:\n- 0.5: Unused optional items are clearly highlighted in yellow in Topics & Labs (or equivalent visual marking), and this appears consistent.\n- 0.3: No visible yellow highlighting, but there is a clearly labeled companion list of unused optional topics that appears consistent.\n- 0.0: Neither yellow highlighting nor a clear unused optional list is present.\n\nDo not check completeness or cross-reference counts here\u2014just the visual presence of highlighting or a clear list.", "expectation": "Yellow highlighting of unused optional topics in Topics & Labs, or a clear unused list if highlighting is infeasible."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Presentation Quality", "description": "Holistic quality and professional suitability for departmental use.", "is_required": false, "max_points": 1.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Usability", "description": "Assess the professional formatting, readability, and usefulness for stakeholders.", "weight": 1.0, "judge_prompt": "Judge the workbook\u2019s professional quality and usability for Medical Education Administration:\n- Is the schedule easy to read and navigate (clear dates, Wednesdays, 7\u20139 AM labeled, consistent formatting)?\n- Are required items (All Periop, MS4 Talks, In-Service Study Session) clearly labeled and not hidden?\n- Is the Compliance Log/Scheduling Log understandable and sufficient for quick audit (source category labeling clear)?\n- Are colors, borders, and highlights used sparingly and consistently (e.g., yellow only for unused optionals)?\n\nScoring:\n- 1.0: Clean, professional, easy to audit and use.\n- 0.5: Generally readable with minor issues (e.g., clutter, inconsistent labels).\n- 0.0: Hard to use, confusing structure, poor labeling.", "expectation": "A professional, readable workbook with clear scheduling and simple audit aids suitable for department leadership and coordinators."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "94925f49-36bc-42da-b45b-61078d329300", "rubric": {"category_name": "Buyer School Reports and Nearby Homes \u2014 Real Estate Sales Agent", "rationale": "This rubric enforces a self-documenting, verifiable structure for five school-focused PDF reports (one per specified elementary school) with embedded, checkable listings under the buyer\u2019s price cap. Stage 1 (LLM-only) mandates the exact report structure so that verification is trivial. Stage 2 uses code to check coverage, price-cap compliance, presence of key metrics and sources, and proximity evidence. Stage 3 uses an LLM judge for professional quality and decision usefulness. The rubric is flexible about file organization (five separate PDFs or a single consolidated PDF) but strict about required sections and tables to enable verification.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Structured PDF Reports Gate", "description": "LLM-only gate to ensure outputs are valid PDF(s) with five school-specific reports, each \u226410 pages, and containing required sections and tables to enable automated verification.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "PDF Structure and Section Completeness", "description": "Verify that the submission consists of PDF report(s) with one report per school (5 total), each \u226410 pages, and containing all required sections and tables to enable verification. Flexible on exact header names but strict on presence and structure.", "weight": 4.0, "judge_prompt": "You are evaluating whether the candidate outputs satisfy strict STRUCTURAL requirements. Only check FORMAT and PRESENCE, not the correctness of numbers. Consider ALL provided outputs (there may be 5 separate PDFs or a single consolidated PDF containing all five reports). Be flexible with section names but strict about required elements.\n\nSchools to cover (each must have its own clearly delimited report):\n- Floral Park-Bellerose School\n- John Lewis Childs School\n- Hillside Grade School\n- Manor Oaks School\n- Garden City Park School\n\nAcceptable organization:\n- Either 5 separate PDFs (one per school), or\n- 1 consolidated PDF with 5 clearly separated reports (each starting with a school header/title), or\n- A small set of PDFs that together cover all five schools (each report still clearly delimited).\n\nFor EACH school\u2019s report, check the following REQUIRED structural elements:\n1) Report Header (first section of that report)\n   - Contains the school name as a visible heading/title\n   - Includes the district name if available\n   - Shows a date stamp (e.g., \u201cJuly 2025\u201d or similar)\n\n2) School Snapshot (can be called Overview/Key Facts/Summary)\n   - A short paragraph and/or bullet list summarizing the school and district context\n\n3) Academic & Staffing Metrics TABLE (tabular, not just prose). Columns/rows must visibly include the following fields (exact labels may vary slightly):\n   - Grades served\n   - Enrollment\n   - Student\u2013teacher ratio (or \u201cstudents per teacher\u201d)\n   - Academic statistics (e.g., test scores/ratings)\n   - % Gifted (or gifted/enrichment indicator)\n   - Average teacher salary (or teacher salary)\n\n4) Nearby Homes Under $1,250,000 TABLE (must be a table). Flexible column naming, but the table must show at least these columns:\n   - Property Address\n   - List Price (currency shown)\n   - Beds\n   - Baths\n   - Square Feet (or note N/A)\n   - Distance to School (e.g., in miles/minutes)\n   - Days on Market (or list date)\n   - Source/Link (e.g., MLS # or URL)\n   - At least 2 listings per school must be shown (structure check only; do NOT validate prices here)\n\n5) Sources/Notes section\n   - Lists at least one reputable SCHOOL info source (e.g., niche.com, greatschools.org)\n   - Lists at least one reputable REAL ESTATE source (e.g., mlsli.com, realtor.com, zillow.com, redfin.com)\n\nFormatting constraints:\n- Reports must be PDF(s), not Word/Excel/plain text\n- Each school\u2019s report is no more than 10 pages; if combined into one PDF, total length should be \u2264 50 pages (approx. 10 per school)\n- Tables must be visually identifiable as tables\n\nScoring:\n- 4.0: All five schools present; each report contains all REQUIRED sections and both required tables; PDF format; page limits respected; clear report boundaries for each school\n- 3.5: All five schools present and mostly complete; one minor omission (e.g., missing Days on Market column for one school) OR one report slightly exceeds page cap\n- 3.0: All five schools present but with multiple minor omissions across reports (e.g., missing Distance for several homes tables) OR noticeably weak delimitation between reports\n- 2.0: Only 3\u20134 schools have clearly structured reports OR homes table missing for more than one school\n- 0.0: Not PDF, fewer than 3 schools covered, or structure fundamentally missing (no tables/sections). \n\nOnly judge STRUCTURE and FORMAT, not whether values are correct.", "expectation": "Five well-structured PDF reports (\u226410 pages each) or one consolidated PDF with five clearly separated sections. Each includes a School Snapshot, an Academic & Staffing Metrics table with key fields, a Nearby Homes Under $1,250,000 table with required columns and \u22652 listings, and a Sources/Notes section with credible domains."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Code + LLM-friendly checks)", "description": "Code-based verification of coverage, price-cap compliance evidence, presence of metrics and sources, and proximity evidence using extracted text from PDFs/DOCX.", "is_required": false, "max_points": 4.5, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "School Coverage and Homes Section Presence", "description": "Check that each of the five school names appears in the documents and that a nearby homes section is indicated near each (e.g., the word 'home', 'listings', or 'for sale').", "weight": 1.2, "code": "import re\n\ndef evaluate(workflow, context):\n    schools = [\n        'Floral Park-Bellerose School',\n        'John Lewis Childs School',\n        'Hillside Grade School',\n        'Manor Oaks School',\n        'Garden City Park School',\n    ]\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0, 'No outputs.'\n\n    # Collect text from all document-like outputs\n    texts = []\n    for r in outputs:\n        try:\n            if r.is_document:\n                if str(r.name).lower().endswith('.pdf'):\n                    t = context.files.read_pdf_text(r.id)\n                elif str(r.name).lower().endswith('.docx'):\n                    t = context.files.read_docx_text(r.id)\n                else:\n                    # Fallback for other document mime types\n                    t = context.files.read_pdf_text(r.id)\n                if t:\n                    texts.append(t)\n        except Exception:\n            continue\n\n    if not texts:\n        return 0.0, 'No readable document text.'\n\n    all_text = ('\\n'.join(texts)).lower()\n\n    present_count = 0\n    homes_nearby_count = 0\n    missing = []\n\n    for s in schools:\n        s_lower = s.lower()\n        idx = all_text.find(s_lower)\n        if idx != -1:\n            present_count += 1\n            window = all_text[idx: idx + 8000]\n            if re.search(r'\\b(home|homes|listing|listings|for sale|nearby homes)\\b', window):\n                homes_nearby_count += 1\n        else:\n            missing.append(s)\n\n    coverage_fraction = present_count / len(schools)\n    homes_fraction = homes_nearby_count / len(schools)\n\n    score = 1.2 * (0.85 * coverage_fraction + 0.15 * homes_fraction)\n    feedback = f\"Schools found: {present_count}/5; homes section near schools: {homes_nearby_count}/5. Missing: {', '.join(missing) if missing else 'None'}.\"\n    return score, feedback"}, {"type": "code", "name": "Listings Under $1,250,000 Evidence per School", "description": "For each school, within a reasonable window after the school header, count list prices and award credit if multiple listings under the cap are present.", "weight": 1.6, "code": "import re\n\ndef evaluate(workflow, context):\n    CAP = 1250000\n    schools = [\n        'Floral Park-Bellerose School',\n        'John Lewis Childs School',\n        'Hillside Grade School',\n        'Manor Oaks School',\n        'Garden City Park School',\n    ]\n    outputs = context.get_all_outputs()\n    texts = []\n    for r in outputs:\n        try:\n            if r.is_document:\n                name = str(r.name).lower()\n                if name.endswith('.pdf'):\n                    t = context.files.read_pdf_text(r.id)\n                elif name.endswith('.docx'):\n                    t = context.files.read_docx_text(r.id)\n                else:\n                    t = context.files.read_pdf_text(r.id)\n                if t:\n                    texts.append(t)\n        except Exception:\n            continue\n\n    if not texts:\n        return 0.0, 'No readable document text.'\n\n    all_text = ('\\n'.join(texts))\n    all_text_lower = all_text.lower()\n\n    price_re = re.compile(r\"\\$\\s*([0-9]{1,3}(?:,[0-9]{3})+|[0-9]{4,})(?:\\.[0-9]{2})?\")\n\n    per_school_scores = []\n    details = []\n\n    for s in schools:\n        s_lower = s.lower()\n        idx = all_text_lower.find(s_lower)\n        under_cap = 0\n        total_found = 0\n        if idx != -1:\n            window = all_text[idx: idx + 10000]\n            for m in price_re.finditer(window):\n                val = m.group(1).replace(',', '')\n                try:\n                    price = int(val)\n                    total_found += 1\n                    if price <= CAP:\n                        under_cap += 1\n                except Exception:\n                    continue\n        # Scoring per school: 0 if none, 0.5 if 1-2 under cap, 1.0 if >=3 under cap\n        if under_cap >= 3:\n            ps = 1.0\n        elif under_cap >= 1:\n            ps = 0.5\n        else:\n            ps = 0.0\n        per_school_scores.append(ps)\n        details.append(f\"{s}: {under_cap} under-cap (of {total_found} prices in window)\")\n\n    avg_score = sum(per_school_scores) / len(schools)\n    score = 1.6 * avg_score\n    feedback = '; '.join(details)\n    return score, feedback"}, {"type": "code", "name": "Metrics and Sources Presence", "description": "Check for presence of key metrics terms and reputable sources across the documents.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    texts = []\n    for r in outputs:\n        try:\n            if r.is_document:\n                name = str(r.name).lower()\n                if name.endswith('.pdf'):\n                    t = context.files.read_pdf_text(r.id)\n                elif name.endswith('.docx'):\n                    t = context.files.read_docx_text(r.id)\n                else:\n                    t = context.files.read_pdf_text(r.id)\n                if t:\n                    texts.append(t)\n        except Exception:\n            continue\n\n    if not texts:\n        return 0.0, 'No readable document text.'\n\n    all_text = ('\\n'.join(texts)).lower()\n\n    metric_terms = [\n        r'grades',\n        r'enrollment',\n        r'student[- ]?teacher',\n        r'ratio',\n        r'gifted',\n        r'teacher salary',\n    ]\n\n    found_metrics = 0\n    for pat in metric_terms:\n        if re.search(pat, all_text):\n            found_metrics += 1\n\n    metrics_frac = found_metrics / len(metric_terms) if metric_terms else 0.0\n\n    school_sources = ['niche.com', 'greatschools.org']\n    re_sources = ['mlsli.com', 'realtor.com', 'zillow.com', 'redfin.com', 'streeteasy.com', 'trulia.com', 'compass.com', 'elliman.com', 'coldwellbanker.com']\n\n    has_school_source = any(src in all_text for src in school_sources)\n    has_re_source = any(src in all_text for src in re_sources)\n\n    sources_frac = (int(has_school_source) + int(has_re_source)) / 2.0\n\n    # Weighting: 70% metrics, 30% sources\n    score = 1.0 * (0.7 * metrics_frac + 0.3 * sources_frac)\n    fb = f\"Metrics present: {found_metrics}/6; School source: {has_school_source}; RE source: {has_re_source}.\"\n    return score, fb"}, {"type": "code", "name": "Proximity Evidence (Distances)", "description": "Check for distance/proximity expressions near each school\u2019s homes section.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    schools = [\n        'Floral Park-Bellerose School',\n        'John Lewis Childs School',\n        'Hillside Grade School',\n        'Manor Oaks School',\n        'Garden City Park School',\n    ]\n    outputs = context.get_all_outputs()\n    texts = []\n    for r in outputs:\n        try:\n            if r.is_document:\n                name = str(r.name).lower()\n                if name.endswith('.pdf'):\n                    t = context.files.read_pdf_text(r.id)\n                elif name.endswith('.docx'):\n                    t = context.files.read_docx_text(r.id)\n                else:\n                    t = context.files.read_pdf_text(r.id)\n                if t:\n                    texts.append(t)\n        except Exception:\n            continue\n\n    if not texts:\n        return 0.0, 'No readable document text.'\n\n    all_text = ('\\n'.join(texts))\n    all_text_lower = all_text.lower()\n\n    dist_re = re.compile(r\"\\b([0-9]+(?:\\.[0-9]+)?)\\s*(mi|mile|miles|km|kilometer|kilometers|mins?|minutes?)\\b\")\n\n    with_distance = 0\n    for s in schools:\n        s_lower = s.lower()\n        idx = all_text_lower.find(s_lower)\n        if idx != -1:\n            window = all_text_lower[idx: idx + 8000]\n            if dist_re.search(window):\n                with_distance += 1\n\n    frac = with_distance / len(schools)\n    score = 0.5 * frac\n    fb = f\"Schools with distance evidence: {with_distance}/5.\"\n    return score, fb"}, {"type": "code", "name": "Date Presence (July 2025)", "description": "Check that the reports include a current date reference (e.g., July 2025).", "weight": 0.2, "code": "import re\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    texts = []\n    for r in outputs:\n        try:\n            if r.is_document:\n                name = str(r.name).lower()\n                if name.endswith('.pdf'):\n                    t = context.files.read_pdf_text(r.id)\n                elif name.endswith('.docx'):\n                    t = context.files.read_docx_text(r.id)\n                else:\n                    t = context.files.read_pdf_text(r.id)\n                if t:\n                    texts.append(t)\n        except Exception:\n            continue\n\n    if not texts:\n        return 0.0\n\n    all_text = ('\\n'.join(texts)).lower()\n\n    has_year = '2025' in all_text\n    has_month_year = ('july 2025' in all_text) or bool(re.search(r'\\b(07/\\s*2025|07-\\s*2025|jul\\.?\\s*2025)\\b', all_text))\n\n    if has_month_year or has_year:\n        return 0.2, 'Date found.'\n    return 0.0, 'No 2025 date found.'"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Decision Usefulness", "description": "LLM judge evaluates presentation, clarity, and usefulness for the buyer\u2019s decision, without penalizing for minor stylistic differences.", "is_required": false, "max_points": 1.5, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Buyer Usefulness", "description": "Assess whether the reports are professionally formatted, readable, and useful for a buyer prioritizing elementary schools and nearby home targets.", "weight": 1.5, "judge_prompt": "Evaluate the final PDF report(s) for overall professional quality and buyer usefulness. Do not re-check structure already tested in Stage 1; instead, judge clarity and presentation.\n\nConsider:\n- Readability and layout: clear headings, legible tables, consistent formatting, page numbers.\n- Decision usefulness: do the reports make it easy to compare school metrics and identify viable home options? Are key figures (ratio, enrollment, academic indicators) easy to find? Are listings readable with addresses, prices, beds/baths, and distances visible?\n- Appropriateness for a residential buyer in Floral Park/New Hyde Park: tone, brevity (\u226410 pages per school), and localized context.\n- Light synthesis: brief takeaways, cautions, or notes that help the buyer interpret data (optional, but rewarded if present without exceeding scope).\n\nScoring:\n- 1.5: Highly professional, clear, and directly useful for choosing a school and shortlisting homes.\n- 1.0: Generally professional and useful with minor issues (e.g., small inconsistencies or clutter).\n- 0.5: Adequate but somewhat hard to use (e.g., cluttered tables, unclear formatting).\n- 0.0: Poor presentation or confusing; hard for a buyer to act on.", "expectation": "Clean, consistent formatting with readable tables and succinct narrative that helps a buyer prioritize schools and shortlist sub-$1.25M homes near each school."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "fccaa4a1-1c39-49ac-b701-55361a19966b", "rubric": {"category_name": "Real Estate and Rental and Leasing \u2014 Concierges: VIP Statue of Liberty & Ellis Island Tour Plan (PDF)", "rationale": "Pattern B (Document). The deliverable is a two-page, styled PDF tour brochure/plan for UHNW clients. Stage 1 (LLM only) strictly enforces the PDF format and visible structure (title, required section headers, itinerary, icons, image, meeting/end points). Stage 2 mixes code rules (text extraction from PDF) and an LLM rule to verify factual presence/consistency (location, duration, highlights, inclusions/requirements, operator attribution, itinerary feasibility within 4 hours). Stage 3 uses an LLM judge for holistic quality, tone, and suitability for a luxury clientele.", "max_total_score": 12.0, "stages": [{"name": "Stage 1 \u2014 PDF Format and Structure Gate", "description": "LLM-only gate to enforce exact shape and visible structure so subsequent verification is trivial.", "is_required": true, "max_points": 5.0, "min_score_to_pass": 3.5, "rules": [{"type": "llm_judge", "name": "PDF Tour Brochure Format and Structure Requirements", "description": "Verify the candidate produced a properly structured, styled PDF tour brochure with all required sections and visual elements.", "weight": 5.0, "judge_prompt": "You are evaluating whether the SUBMITTED OUTPUT is a properly structured, styled PDF tour brochure for a VIP early-access Statue of Liberty & Ellis Island tour. Only check FORMAT, STRUCTURE, and PRESENCE of required elements. Do not judge writing quality or correctness of facts.\n\nRequirements (be flexible with exact phrasing, but headers must match these labels exactly where specified):\n\nA) File/Format\n- Must be a PDF (not DOCX/Google Doc/Excel/plain text).\n- At least 2 pages total. The itinerary is intended to be two pages in length; accept if the overall PDF is 2+ pages and the itinerary occupies a substantial, clearly step-by-step portion.\n- Clean, styled layout intended for clients (icons used to visually organize details like location, time/duration, overview, inclusions, requirements).\n- Includes a small photo/image of the Statue of Liberty (royalty-free source or similar); image presence is required.\n\nB) Title and Headers\n- Title on page 1: \u201cEarly Access Statue of Liberty & Ellis Island Tour\u201d (allow minor punctuation/case variations; must be clearly the main title).\n- Section headers must include the following exact labels somewhere in the document: \u201coverview of activities\u201d, \u201cinclusions\u201d, \u201crequirements\u201d.\n\nC) Core Sections Present\n- Location: \u201cNew York City, United States\u201d (or equivalent like NYC, USA) visibly stated.\n- Duration: \u201c4 hours\u201d visibly stated.\n- Highlights list including items like: first group of the day / early access; licensed New York tour guide; small group (max 25); visiting Ellis Island and seeing NYC skyline. (Exact phrasing can vary; must be recognizably present as a highlights list.)\n- Description of the tour operator and sites visited, with visible attribution to www.TakeWalks.com (or TakeWalks.com) as the source.\n- Itinerary: a step-by-step tour plan.\n- Meeting Location: \u201cHarbor House, 22 Battery Place, in Battery Park\u201d.\n- End Point: \u201cEllis Island\u201d or an option to take the ferry back to Battery Park with a Guide.\n\nScoring (STRUCTURE ONLY):\n- 5.0: Valid PDF; 2+ pages; clearly styled; icons present; small Statue of Liberty photo present; title present; all three required headers (\u201coverview of activities\u201d, \u201cinclusions\u201d, \u201crequirements\u201d) present; ALL core sections present (location, duration, highlights, operator description referencing TakeWalks.com, step-by-step itinerary, meeting location, end point).\n- 4.0: Valid PDF; 2+ pages; all required headers and core sections present, but missing either icons OR the photo.\n- 3.0: Valid PDF; 2+ pages; most core sections present but missing one required header or one core section (e.g., no meeting location or no highlights list).\n- 1.0: Valid PDF but missing multiple core sections or only 1 page.\n- 0.0: Not a PDF, blank/unreadable, or grossly wrong format.\n\nOnly evaluate presence/structure. Do NOT check factual accuracy beyond presence. Return a score from 0.0 to 5.0.", "expectation": "A 2+ page, styled PDF brochure with icons, a small Statue of Liberty image, the exact title, exact section headers, and all core sections visibly present."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification of Content Presence and Consistency", "description": "Programmatic and LLM checks of factual presence and internal consistency, leveraging Stage 1\u2019s enforced structure.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Core Facts Present: Location, Duration, Title", "description": "Check for presence of location (NYC/United States), duration (4 hours), and the required title (allow &/and).", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n    # Prefer PDF text extraction per Stage 1 requirement\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id) or \"\"\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id) or \"\"\n        except Exception:\n            text = \"\"\n    if not text:\n        return 0.0, \"Could not extract document text\"\n    tl = text.lower()\n    # Location\n    has_location = (\"new york city\" in tl) or (\"nyc\" in tl)\n    # Duration (handle 4 hours variants)\n    has_duration = bool(re.search(r\"\\b(4\\s*-?\\s*hour|four\\s+hour)s?\\b\", tl))\n    # Title (handle & vs and)\n    title_and = \"early access statue of liberty and ellis island tour\" in tl\n    title_amp = \"early access statue of liberty & ellis island tour\" in tl\n    has_title = title_and or title_amp\n    checks = [has_location, has_duration, has_title]\n    score = (sum(checks) / len(checks)) * 1.0\n    feedback = f\"Location={has_location}, Duration={has_duration}, Title={has_title}\"\n    return score, feedback"}, {"type": "code", "name": "Highlights Coverage", "description": "Verify presence of the requested highlights: early access/first group, licensed NY guide, small group max 25, Ellis Island and NYC skyline.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id) or \"\"\n    except Exception:\n        text = \"\"\n    tl = text.lower()\n    early = (\"early access\" in tl) or (\"first group\" in tl) or (\"first entry\" in tl) or (\"early entry\" in tl)\n    licensed = (\"licensed new york tour guide\" in tl) or (\"licensed guide\" in tl) or (\"expert guide\" in tl)\n    small_group = (\"small group\" in tl) and (\"25\" in tl or \"twenty five\" in tl or \"twenty-five\" in tl or \"maximum\" in tl)\n    skyline_ellis = (\"ellis island\" in tl) and (\"skyline\" in tl or \"nyc skyline\" in tl)\n    checks = [early, licensed, small_group, skyline_ellis]\n    score = (sum(checks) / len(checks)) * 1.0\n    return score, f\"early={early}, licensed={licensed}, small_group={small_group}, skyline_ellis={skyline_ellis}\""}, {"type": "code", "name": "Meeting Location and End Point", "description": "Check for exact meeting location and end point language (ferry option back to Battery Park with a guide).", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id) or \"\"\n    except Exception:\n        text = \"\"\n    tl = text.lower()\n    meet_harbor = (\"harbor house\" in tl) and (\"22 battery place\" in tl) and (\"battery park\" in tl)\n    # End point variations\n    end_ellis = \"ellis island\" in tl\n    ferry_back = (\"ferry\" in tl) and (\"battery park\" in tl)\n    with_guide = (\"with a guide\" in tl) or (\"with guide\" in tl) or (\"guided\" in tl)\n    end_ok = end_ellis and ferry_back and with_guide\n    # Score: 0.5 for meeting location, 0.5 for end point phrase\n    score = 0.0\n    if meet_harbor:\n        score += 0.5\n    if end_ok:\n        score += 0.5\n    return score, f\"meeting={meet_harbor}, end_point={end_ok}\""}, {"type": "code", "name": "Inclusions and Requirements Completed", "description": "Verify language (English), not-included items (Crown/Pedestal access, gratuities, hotel pick-up/drop-off), difficulty (moderate), restrictions (only clear containers/bottles), and age requirements (2\u201314).", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id) or \"\"\n    except Exception:\n        text = \"\"\n    tl = text.lower()\n    has_language = \"english\" in tl\n    not_included_crown_ped = (\"crown\" in tl) or (\"pedestal\" in tl)\n    not_included_grat = \"gratuities\" in tl or \"gratuity\" in tl\n    not_included_hotel = (\"hotel pick\" in tl) or (\"hotel pickup\" in tl) or (\"hotel pick-up\" in tl) or (\"hotel drop\" in tl) or (\"drop-off\" in tl) or (\"drop off\" in tl)\n    difficulty_moderate = \"moderate\" in tl\n    restrictions_clear = (\"only clear\" in tl) or (\"clear container\" in tl) or (\"clear bottle\" in tl) or (\"clear bag\" in tl)\n    age_req = bool(re.search(r\"\\b2\\s*(?:-|\u2013|to)\\s*14\\b\", tl)) or (\"2-14\" in tl) or (\"2 \u2013 14\" in tl)\n    checks = [has_language, not_included_crown_ped, not_included_grat, not_included_hotel, difficulty_moderate, restrictions_clear, age_req]\n    score = (sum(checks) / len(checks)) * 1.0\n    fb = {\n        \"language\": has_language,\n        \"crown/pedestal\": not_included_crown_ped,\n        \"gratuities\": not_included_grat,\n        \"hotel_pickup_dropoff\": not_included_hotel,\n        \"difficulty_moderate\": difficulty_moderate,\n        \"restrictions_clear\": restrictions_clear,\n        \"age_2_14\": age_req,\n    }\n    return score, str(fb)"}, {"type": "code", "name": "Operator Attribution Present", "description": "Verify attribution/source to TakeWalks.com is present in text.", "weight": 0.5, "code": "def evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id) or \"\"\n    except Exception:\n        text = \"\"\n    tl = text.lower()\n    ok = (\"takewalks.com\" in tl) or (\"take walks\" in tl)\n    return (0.5 if ok else 0.0), f\"operator_attribution={ok}\""}, {"type": "llm_judge", "name": "Itinerary Feasibility and Early/VIP Access Consistency", "description": "LLM checks that the step-by-step itinerary plausibly fits within ~4 hours and clearly reflects early/VIP access intent.", "weight": 0.5, "judge_prompt": "Read the PDF itinerary section(s). Judge ONLY feasibility and consistency, not writing style.\n\nChecks:\n1) Time-bounded: The step-by-step plan plausibly fits in ~4 hours (240 minutes), including ferry time and security lines (assume expedited/early access reduces wait). Look for timestamps or time estimates per step or a clearly time-aware sequence.\n2) Early/VIP access reflected: Mentions early access/first group/VIP priority that aligns with starting before general crowds.\n\nScoring:\n- 0.5: Both conditions clearly satisfied (timeline/sequence is plausible within ~4 hours and early/VIP access is explicit).\n- 0.3: Implied feasibility but missing explicit time cues OR early/VIP access only loosely implied.\n- 0.0: No meaningful timeline or clearly exceeds 4 hours; no sign of early/VIP access.\n\nReturn a score from 0.0 to 0.5.", "expectation": "A step-by-step itinerary with time awareness that reasonably fits ~4 hours and shows early/VIP access."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Presentation and Client Fit Quality", "description": "LLM judge evaluates professional polish, clarity, and suitability for UHNW clients seeking VIP service.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Quality, Clarity, and VIP Appropriateness", "description": "Assess design and writing quality for UHNW clientele.", "weight": 2.0, "judge_prompt": "Evaluate the overall quality of the PDF as a luxury concierge brochure for UHNW clients.\nConsider:\n- Visual polish: clean, elegant layout; consistent typography; effective use of icons; tasteful photo placement; good spacing.\n- Clarity and usefulness: succinct overview; clearly labeled sections; easy-to-scan inclusions/requirements; unambiguous meeting point/end point.\n- VIP/concierge tone: bespoke, reassuring, premium (no hype); avoids clutter; anticipates client needs.\n- Cohesion and readability: minimal errors; logical flow; professional formatting (headers, bullets, hierarchy).\n\nScoring:\n- 2.0: Premium, publication-ready; exceptional clarity and polish suitable for UHNW clients.\n- 1.5: Strong and professional with minor issues.\n- 1.0: Adequate but with noticeable lapses in clarity/design/tone.\n- 0.5: Weak structure or multiple design/language issues.\n- 0.0: Unprofessional or confusing.\n\nReturn a score from 0.0 to 2.0.", "expectation": "A polished, premium-feeling two-page PDF brochure appropriate for UHNW clients, with clear sections and elegant design."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "b5d2e6f1-62a2-433a-bcdd-95b260cdd860", "rubric": {"category_name": "Wholesale Trade \u2013 Order Clerks: Sales Performance Pivot Workbook", "rationale": "This rubric enforces a self-documenting Excel deliverable that enables trivial verification. Stage 1 (LLM-only) mandates an exact workbook structure with three sheets and specific headers, creating the contract for verification. Stage 2 uses code rules to validate column coverage, ST% formula correctness, grand totals, numeric sanity, and cross-sheet consistency. Stage 3 assesses professional quality and stakeholder readiness.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2013 Workbook Structure Gate", "description": "LLM-only gate: verify the candidate produced an Excel workbook with required sheets and structures enabling verification. Only structure and format, not calculation accuracy.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Pivot Workbook Requirement", "description": "Output must be an Excel file with required sheets and tables, enabling verification of sales performance by brand and by store.", "weight": 4.0, "judge_prompt": "You are checking ONLY structure/format, not correctness of numbers. Inspect the candidate's primary output.\n\nFormat Requirements:\n- Must be an Excel spreadsheet (.xlsx or .xls). Not a PDF/DOCX/CSV.\n- Workbook must contain at least three sheets with the following roles and names (flexible matching on capitalization and minor variants is OK):\n  1) A sheet named exactly \"Data\" (or extremely close like \"DATA\"/\"Data Table\"). This is the baseline data sheet derived from the provided weekly sales data.\n  2) A sheet named \"Sales by Brand\" (or very close, e.g., \"Brand Sales\", \"Sales_by_Brand\").\n  3) A sheet named \"Sales by Store\" (or very close, e.g., \"Store Sales\", \"Sales_by_Store\").\n\nData Sheet Structure (\"Data\"):\n- Should present a tabular dataset with at least these columns visible as headers (flexible on minor naming, spacing, and punctuation):\n  - Brand or Brand Name\n  - Store or Store Name/Number\n  - WTD Sales Quantity\n  - WTD Sales $\n  - WTD Stock On Hand\n  - MTD Sales Quantity\n  - MTD Sales $\n  - MTD Stock On Hand\n  - YTD Sales Quantity\n  - YTD Sales $\n  - YTD Stock On Hand\n  - Optional: WTD ST%, MTD ST%, YTD ST% (sell-through percent)\n- Values should be visible (not entirely hidden behind formulas or empty).\n\nSheet: \"Sales by Brand\"\n- Should compile totals by brand (one row per brand).\n- Must include the following column headers (allowing small variations like extra words such as \"Total\" before Sales $):\n  Brand; WTD Sales Quantity; WTD Sales $; WTD Stock On Hand; WTD ST%; MTD Sales Quantity; MTD Sales $; MTD Stock On Hand; MTD ST%; YTD Sales Quantity; YTD Sales $; YTD Stock On Hand; YTD ST%.\n- Include grand total(s) (e.g., a Grand Total row and/or total column typical of PivotTables).\n- This should be a pivot table or an equivalent aggregated table with the same structure.\n\nSheet: \"Sales by Store\"\n- Should show totals by store for each brand (hierarchical: Store then Brand Name or columns that clearly identify both store and brand).\n- Must include these column headers (minor variations acceptable):\n  Store; Brand Name; WTD Sales Quantity; WTD Total Sales $; WTD Stock On Hand; WTD ST%; MTD Sales Quantity; MTD Total Sales $; MTD Stock On Hand; MTD ST%; YTD Sales Quantity; YTD Total Sales $; YTD Stock On Hand; YTD ST%.\n- Include grand total(s) (e.g., Grand Total row and/or total column typical of PivotTables).\n\nScoring (structure only):\n- 4.0: Valid Excel + all three required sheets present; each sheet has the required columns/structure; Brand and Store tabs show totals and contain grand totals.\n- 3.0: Valid Excel + all three sheets present, but a minor omission (e.g., a single ST% column missing on one tab or slightly incomplete Data columns). Grand totals still present on both summary tabs.\n- 2.0: Valid Excel, but one required summary sheet (\u201cSales by Brand\u201d or \u201cSales by Store\u201d) is clearly missing required columns or no grand totals on either tab; or the Data sheet is missing several core columns.\n- 1.0: Valid Excel but major deviations: missing a required sheet, or summary tabs not recognizable as aggregated by brand/store.\n- 0.0: Not an Excel file OR only one sheet with unstructured data OR multiple required sheets missing.\n\nOnly check presence/format/structure. Do not validate numbers, formulas, or calculation accuracy.", "expectation": "A 3-sheet Excel workbook: Data, Sales by Brand, Sales by Store. Brand and Store sheets show totals with specified headers and grand totals; Data sheet includes at least brand, store, and WTD/MTD/YTD sales and stock columns."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness and Consistency Verification", "description": "Code-based checks leveraging the mandated structure to verify headers, formulas, totals, and cross-sheet consistency. Flexible matching, robust parsing, and tolerant bounds.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Column Coverage \u2013 Sales by Brand", "description": "Verify the 'Sales by Brand' sheet contains the expected headers (flexible matching) for Brand and WTD/MTD/YTD metrics (Sales Qty, Sales $, Stock On Hand, ST%).", "weight": 1.0, "code": "import re, json, pandas as pd, numpy as np\n\ndef normalize(s):\n    if s is None:\n        return ''\n    s = str(s)\n    s = s.replace('\\n',' ').lower()\n    s = re.sub(r'[%$]+', lambda m: ' '+m.group(0)+' ', s)\n    s = re.sub(r'[^a-z0-9%$]+', ' ', s)\n    s = re.sub(r'\\s+', ' ', s).strip()\n    return s\n\ndef read_with_inferred_header(file_path, sheet_name):\n    try:\n        raw = pd.read_excel(file_path, sheet_name=sheet_name, header=None)\n    except Exception:\n        return None\n    # Find the most likely header row among first 10 rows\n    header_tokens = ['brand', 'store', 'wtd', 'mtd', 'ytd', 'sales', 'stock', 'st']\n    best_row = 0\n    best_score = -1\n    for r in range(min(10, len(raw))):\n        row_vals = [normalize(v) for v in list(raw.iloc[r].values)]\n        score = sum(any(tok in cell for tok in header_tokens) for cell in row_vals)\n        if score > best_score:\n            best_score = score\n            best_row = r\n    df = raw.copy()\n    df.columns = [str(c) for c in raw.iloc[best_row].values]\n    df = df.iloc[best_row+1:].reset_index(drop=True)\n    # Drop empty columns\n    df = df.loc[:, [not pd.isna(c) and str(c).strip() != '' for c in df.columns]]\n    # Normalize column names\n    df.columns = [normalize(c) for c in df.columns]\n    return df\n\ndef find_sheet_by_keywords(xl, keywords):\n    keys = [k.lower() for k in keywords]\n    for s in xl.sheet_names:\n        ns = s.lower()\n        if all(k in ns for k in keys):\n            return s\n    # fallback: partial any match\n    for s in xl.sheet_names:\n        ns = s.lower()\n        if any(k in ns for k in keys):\n            return s\n    return None\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, 'No spreadsheet output.'\n    file_path = context.files.get_path(output.id)\n    xl = pd.ExcelFile(file_path)\n    sheet = find_sheet_by_keywords(xl, ['brand']) or find_sheet_by_keywords(xl, ['sales','brand'])\n    if not sheet:\n        return 0.0, 'Could not locate Sales by Brand sheet.'\n    df = read_with_inferred_header(file_path, sheet)\n    if df is None or df.empty:\n        return 0.0, 'Sales by Brand sheet unreadable or empty.'\n    # Expected columns\n    expected = {\n        'brand': ['brand', 'brand name'],\n    }\n    periods = ['wtd','mtd','ytd']\n    metrics = {\n        'qty': ['sales quantity','qty','units','sales qty'],\n        'sales$': ['sales $','total sales $','$ sales','sales dollars'],\n        'stock': ['stock on hand','soh','stock']\n    }\n    metrics_pct = ['st %','st%','sell through %','sell thru %','sell through','sell thru']\n\n    found = 0\n    total = 1 + 3*4  # brand + (qty, sales$, stock, st%) for WTD/MTD/YTD\n\n    cols = list(df.columns)\n\n    # brand\n    if any(any(v in c for v in expected['brand']) for c in cols):\n        found += 1\n\n    for p in periods:\n        # qty\n        if any((p in c) and any(v in c for v in metrics['qty']) for c in cols):\n            found += 1\n        # sales $\n        if any((p in c) and any(v in c for v in metrics['sales$']) for c in cols):\n            found += 1\n        # stock\n        if any((p in c) and any(v in c for v in metrics['stock']) for c in cols):\n            found += 1\n        # st%\n        if any((p in c) and any(v in c for v in metrics_pct) for c in cols):\n            found += 1\n\n    score = (found/total)\n    feedback = f'Found {found} of {total} expected columns on Sales by Brand.'\n    return score, feedback"}, {"type": "code", "name": "Column Coverage \u2013 Sales by Store", "description": "Verify the 'Sales by Store' sheet contains expected headers (flexible matching) for Store, Brand, and WTD/MTD/YTD metrics.", "weight": 1.0, "code": "import re, json, pandas as pd, numpy as np\n\ndef normalize(s):\n    if s is None:\n        return ''\n    s = str(s)\n    s = s.replace('\\n',' ').lower()\n    s = re.sub(r'[%$]+', lambda m: ' '+m.group(0)+' ', s)\n    s = re.sub(r'[^a-z0-9%$]+', ' ', s)\n    s = re.sub(r'\\s+', ' ', s).strip()\n    return s\n\ndef read_with_inferred_header(file_path, sheet_name):\n    try:\n        raw = pd.read_excel(file_path, sheet_name=sheet_name, header=None)\n    except Exception:\n        return None\n    header_tokens = ['brand', 'store', 'wtd', 'mtd', 'ytd', 'sales', 'stock', 'st']\n    best_row = 0\n    best_score = -1\n    for r in range(min(10, len(raw))):\n        row_vals = [normalize(v) for v in list(raw.iloc[r].values)]\n        score = sum(any(tok in cell for tok in header_tokens) for cell in row_vals)\n        if score > best_score:\n            best_score = score\n            best_row = r\n    df = raw.copy()\n    df.columns = [str(c) for c in raw.iloc[best_row].values]\n    df = df.iloc[best_row+1:].reset_index(drop=True)\n    df = df.loc[:, [not pd.isna(c) and str(c).strip() != '' for c in df.columns]]\n    df.columns = [normalize(c) for c in df.columns]\n    return df\n\ndef find_sheet_by_keywords(xl, keywords):\n    keys = [k.lower() for k in keywords]\n    for s in xl.sheet_names:\n        ns = s.lower()\n        if all(k in ns for k in keys):\n            return s\n    for s in xl.sheet_names:\n        ns = s.lower()\n        if any(k in ns for k in keys):\n            return s\n    return None\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, 'No spreadsheet output.'\n    file_path = context.files.get_path(output.id)\n    xl = pd.ExcelFile(file_path)\n    sheet = find_sheet_by_keywords(xl, ['store']) or find_sheet_by_keywords(xl, ['sales','store'])\n    if not sheet:\n        return 0.0, 'Could not locate Sales by Store sheet.'\n    df = read_with_inferred_header(file_path, sheet)\n    if df is None or df.empty:\n        return 0.0, 'Sales by Store sheet unreadable or empty.'\n\n    expected_base = {\n        'store': ['store','store name','store number'],\n        'brand': ['brand','brand name']\n    }\n    periods = ['wtd','mtd','ytd']\n    metrics = {\n        'qty': ['sales quantity','qty','units','sales qty'],\n        'sales$': ['total sales $','sales $','$ sales','sales dollars'],\n        'stock': ['stock on hand','soh','stock']\n    }\n    metrics_pct = ['st %','st%','sell through %','sell thru %','sell through','sell thru']\n\n    cols = list(df.columns)\n    found = 0\n    total = 2 + 3*4  # store+brand + (qty, sales$, stock, st%) * 3 periods\n\n    if any(any(v in c for v in expected_base['store']) for c in cols):\n        found += 1\n    if any(any(v in c for v in expected_base['brand']) for c in cols):\n        found += 1\n\n    for p in periods:\n        if any((p in c) and any(v in c for v in metrics['qty']) for c in cols):\n            found += 1\n        if any((p in c) and any(v in c for v in metrics['sales$']) for c in cols):\n            found += 1\n        if any((p in c) and any(v in c for v in metrics['stock']) for c in cols):\n            found += 1\n        if any((p in c) and any(v in c for v in metrics_pct) for c in cols):\n            found += 1\n\n    score = (found/total)\n    feedback = f'Found {found} of {total} expected columns on Sales by Store.'\n    return score, feedback"}, {"type": "code", "name": "ST% Formula Validation (Brand & Store)", "description": "Check that ST% \u2248 Sales/Stock On Hand for WTD, MTD, YTD on Brand and Store sheets (tolerant to unit or $ numerator and 0\u2013100 vs 0\u20131 scaling).", "weight": 1.0, "code": "import re, json, pandas as pd, numpy as np\n\n# Helpers\ndef norm(s):\n    if s is None:\n        return ''\n    s = str(s).lower()\n    s = s.replace('\\n',' ')\n    s = re.sub(r'[%$]+', lambda m: ' '+m.group(0)+' ', s)\n    s = re.sub(r'[^a-z0-9%$]+',' ', s)\n    s = re.sub(r'\\s+',' ', s).strip()\n    return s\n\ndef read_sheet(file_path, sheet_name):\n    raw = pd.read_excel(file_path, sheet_name=sheet_name, header=None)\n    # infer header row\n    header_tokens = ['brand','store','wtd','mtd','ytd','sales','stock','st']\n    best_r, best_score = 0, -1\n    for r in range(min(10, len(raw))):\n        vals = [norm(v) for v in list(raw.iloc[r].values)]\n        score = sum(any(tok in v for tok in header_tokens) for v in vals)\n        if score > best_score:\n            best_score, best_r = score, r\n    df = raw.copy()\n    df.columns = [str(c) for c in raw.iloc[best_r].values]\n    df = df.iloc[best_r+1:].reset_index(drop=True)\n    df = df.loc[:, [not pd.isna(c) and str(c).strip()!='' for c in df.columns]]\n    df.columns = [norm(c) for c in df.columns]\n    return df\n\ndef find_sheet(xl, keys):\n    keys = [k.lower() for k in keys]\n    for s in xl.sheet_names:\n        ns = s.lower()\n        if all(k in ns for k in keys):\n            return s\n    for s in xl.sheet_names:\n        ns = s.lower()\n        if any(k in ns for k in keys):\n            return s\n    return None\n\n# Map finder\n\ndef locate_columns(cols, period):\n    cols = [norm(c) for c in cols]\n    def pick(cands):\n        for c in cols:\n            if (period in c) and any(v in c for v in cands):\n                return c\n        return None\n    qty = pick(['sales quantity','qty','units','sales qty'])\n    dol = pick(['sales $','total sales $','$ sales','sales dollars'])\n    stock = pick(['stock on hand','soh','stock'])\n    st = pick(['st %','st%','sell through %','sell thru %','sell through','sell thru'])\n    return qty, dol, stock, st\n\n\ndef coerce_num(series):\n    if series is None: return None\n    s = pd.to_numeric(series, errors='coerce')\n    return s\n\n\ndef validate_st(df):\n    periods = ['wtd','mtd','ytd']\n    total_rows = 0\n    ok_rows = 0\n    for p in periods:\n        qty_col, dol_col, stock_col, st_col = locate_columns(df.columns, p)\n        if st_col and stock_col and (qty_col or dol_col):\n            qty = coerce_num(df.get(qty_col)) if qty_col else None\n            dol = coerce_num(df.get(dol_col)) if dol_col else None\n            stock = coerce_num(df.get(stock_col))\n            st = coerce_num(df.get(st_col))\n            # normalize percent scale (0-1 or 0-100)\n            if st is not None:\n                st_norm = st.copy()\n                # if median > 1.5, assume percent\n                med = np.nanmedian(st_norm.values) if np.isfinite(np.nanmedian(st_norm.values)) else np.nan\n                if pd.notna(med) and med > 1.5:\n                    st_norm = st_norm/100.0\n                # compute ratios\n                r_cands = []\n                if qty is not None:\n                    with np.errstate(divide='ignore', invalid='ignore'):\n                        r_cands.append(qty/stock)\n                if dol is not None:\n                    with np.errstate(divide='ignore', invalid='ignore'):\n                        r_cands.append(dol/stock)\n                if not r_cands:\n                    continue\n                r = r_cands[0]\n                if len(r_cands) > 1:\n                    # choose closer candidate per row\n                    r = pd.concat(r_cands, axis=1)\n                    # pick column with min abs error\n                    diffs = (r.subtract(st_norm, axis=0)).abs()\n                    best = diffs.idxmin(axis=1)\n                    r = r.lookup(r.index, best) if hasattr(r, 'lookup') else r.to_numpy()[np.arange(len(r)), np.nanargmin(diffs.to_numpy(), axis=1)]\n                    r = pd.Series(r, index=st_norm.index)\n                # compare\n                mask = (~st_norm.isna()) & (~pd.isna(r)) & (~stock.isna()) & (stock != 0)\n                total_rows += int(mask.sum())\n                if total_rows == 0:\n                    continue\n                diff = (st_norm[mask] - r[mask]).abs()\n                ok_rows += int((diff <= 0.05).sum())  # 5 percentage points tolerance\n    if total_rows == 0:\n        return 0.0\n    return ok_rows/total_rows\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, 'No spreadsheet output.'\n    file_path = context.files.get_path(output.id)\n    xl = pd.ExcelFile(file_path)\n    # Sales by Brand\n    brand_sheet = find_sheet(xl, ['brand']) or find_sheet(xl, ['sales','brand'])\n    store_sheet = find_sheet(xl, ['store']) or find_sheet(xl, ['sales','store'])\n    if not brand_sheet or not store_sheet:\n        return 0.0, 'Missing Brand or Store sheet.'\n    try:\n        dfb = read_sheet(file_path, brand_sheet)\n        dfs = read_sheet(file_path, store_sheet)\n    except Exception:\n        return 0.0, 'Failed to read sheets.'\n    sb = validate_st(dfb)\n    ss = validate_st(dfs)\n    score = (sb + ss)/2.0\n    return float(score), f'ST% match rate: Brand={sb:.2f}, Store={ss:.2f}.'"}, {"type": "code", "name": "Grand Totals and Numeric Sanity", "description": "Check presence of grand totals on Brand and Store tabs and ensure metrics are mostly non-negative and ST% is within a plausible range.", "weight": 1.0, "code": "import re, json, pandas as pd, numpy as np\n\ndef norm(s):\n    s = '' if s is None else str(s).lower()\n    s = s.replace('\\n',' ')\n    s = re.sub(r'[^a-z0-9%$ ]+',' ', s)\n    s = re.sub(r'\\s+',' ', s).strip()\n    return s\n\ndef read_df(file_path, sheet):\n    raw = pd.read_excel(file_path, sheet_name=sheet, header=None)\n    # infer header\n    tokens = ['brand','store','wtd','mtd','ytd','sales','stock','st']\n    best, br = -1, 0\n    for r in range(min(10, len(raw))):\n        vals = [norm(v) for v in raw.iloc[r].values]\n        score = sum(any(t in v for t in tokens) for v in vals)\n        if score > best:\n            best, br = score, r\n    df = raw.copy()\n    df.columns = [str(c) for c in raw.iloc[br].values]\n    df = df.iloc[br+1:].reset_index(drop=True)\n    df = df.loc[:, [not pd.isna(c) and str(c).strip()!='' for c in df.columns]]\n    df.columns = [norm(c) for c in df.columns]\n    return df\n\ndef find_sheet(xl, keys):\n    keys = [k.lower() for k in keys]\n    for s in xl.sheet_names:\n        ns = s.lower()\n        if all(k in ns for k in keys):\n            return s\n    for s in xl.sheet_names:\n        ns = s.lower()\n        if any(k in ns for k in keys):\n            return s\n    return None\n\n\ndef has_grand_total(df):\n    # Look for a cell containing 'grand total' or 'total' in first two columns\n    sub = df.iloc[:, :min(2, df.shape[1])]\n    text = sub.astype(str).applymap(lambda x: norm(x)).values.ravel().tolist()\n    return any(('grand total' in t) or (t.strip()=='total') or (' total' in t) for t in text)\n\n\ndef metric_cols(df):\n    cols = df.columns\n    numeric_cands = []\n    for c in cols:\n        if any(k in c for k in ['sales','stock','qty','quantity','units','st %','st%','sell thru','sell through']):\n            numeric_cands.append(c)\n    return numeric_cands\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, 'No spreadsheet output.'\n    file_path = context.files.get_path(output.id)\n    xl = pd.ExcelFile(file_path)\n    brand_sheet = find_sheet(xl, ['brand']) or find_sheet(xl, ['sales','brand'])\n    store_sheet = find_sheet(xl, ['store']) or find_sheet(xl, ['sales','store'])\n    if not brand_sheet or not store_sheet:\n        return 0.0, 'Missing required sheets.'\n    dfb = read_df(file_path, brand_sheet)\n    dfs = read_df(file_path, store_sheet)\n\n    # Grand totals detection\n    gt_brand = has_grand_total(dfb)\n    gt_store = has_grand_total(dfs)\n    gt_score = (1.0 if gt_brand else 0.5) + (1.0 if gt_store else 0.5) - 0.5  # 1.5 if both, 0.5 if one, 0 if none\n    gt_score = max(0.0, min(1.0, gt_score))\n\n    # Numeric sanity\n    cols_b = metric_cols(dfb)\n    cols_s = metric_cols(dfs)\n    nb = pd.concat([pd.to_numeric(dfb[c], errors='coerce') for c in cols_b], axis=1) if cols_b else pd.DataFrame()\n    ns = pd.concat([pd.to_numeric(dfs[c], errors='coerce') for c in cols_s], axis=1) if cols_s else pd.DataFrame()\n    frames = []\n    if not nb.empty:\n        frames.append(nb.values.ravel())\n    if not ns.empty:\n        frames.append(ns.values.ravel())\n    if not frames:\n        return gt_score*0.7, 'No numeric columns detected; partial for grand totals.'\n    arr = np.concatenate(frames)\n    arr = arr[~np.isnan(arr)]\n    if arr.size == 0:\n        return gt_score*0.7, 'No numeric values; partial for grand totals.'\n    nonneg_rate = float((arr >= 0).sum())/float(arr.size)\n    # ST% bounds\n    st_cols_b = [c for c in dfb.columns if 'st' in c]\n    st_cols_s = [c for c in dfs.columns if 'st' in c]\n    st_vals = []\n    for c in st_cols_b + st_cols_s:\n        s = pd.to_numeric(pd.concat([dfb.get(c, pd.Series(dtype=float)), dfs.get(c, pd.Series(dtype=float))]), errors='coerce')\n        st_vals.append(s)\n    st_vals = pd.concat(st_vals, axis=0) if st_vals else pd.Series(dtype=float)\n    st_vals = st_vals[~st_vals.isna()]\n    st_ok = True\n    if not st_vals.empty:\n        med = np.nanmedian(st_vals)\n        if med > 1.5:\n            st_vals = st_vals/100.0\n        st_ok = ((st_vals >= 0).mean() >= 0.95) and ((st_vals <= 5).mean() >= 0.95)\n    sanity = 0.0\n    if nonneg_rate >= 0.95:\n        sanity += 0.5\n    if st_ok:\n        sanity += 0.5\n    score = 0.5*gt_score + 0.5*sanity\n    feedback = f'Grand totals: brand={gt_brand}, store={gt_store}; Non-negative rate={nonneg_rate:.2f}; ST% plausibility={st_ok}.'\n    return float(score), feedback"}, {"type": "code", "name": "Cross-Sheet Consistency (Store aggregates match Brand totals)", "description": "Aggregate the Store sheet by Brand and compare totals vs the Brand sheet for YTD Sales Quantity, YTD Sales $, and YTD Stock On Hand.", "weight": 1.0, "code": "import re, json, pandas as pd, numpy as np\n\ndef norm(s):\n    s = '' if s is None else str(s).lower()\n    s = s.replace('\\n',' ')\n    s = re.sub(r'[%$]+', lambda m: ' '+m.group(0)+' ', s)\n    s = re.sub(r'[^a-z0-9%$]+',' ', s)\n    s = re.sub(r'\\s+',' ', s).strip()\n    return s\n\ndef read_df(file_path, sheet):\n    raw = pd.read_excel(file_path, sheet_name=sheet, header=None)\n    tokens = ['brand','store','wtd','mtd','ytd','sales','stock','st']\n    best, br = -1, 0\n    for r in range(min(10, len(raw))):\n        vals = [norm(v) for v in raw.iloc[r].values]\n        score = sum(any(t in v for t in tokens) for v in vals)\n        if score > best:\n            best, br = score, r\n    df = raw.copy()\n    df.columns = [str(c) for c in raw.iloc[br].values]\n    df = df.iloc[br+1:].reset_index(drop=True)\n    df = df.loc[:, [not pd.isna(c) and str(c).strip()!='' for c in df.columns]]\n    df.columns = [norm(c) for c in df.columns]\n    return df\n\ndef find_sheet(xl, keys):\n    keys = [k.lower() for k in keys]\n    for s in xl.sheet_names:\n        ns = s.lower()\n        if all(k in ns for k in keys):\n            return s\n    for s in xl.sheet_names:\n        ns = s.lower()\n        if any(k in ns for k in keys):\n            return s\n    return None\n\n\ndef best_brand_col(cols):\n    for c in cols:\n        if 'brand name' in c:\n            return c\n    for c in cols:\n        if 'brand' in c:\n            return c\n    return None\n\n\ndef get_metric_col(cols, period, label_variants):\n    for c in cols:\n        if period in c and any(v in c for v in label_variants):\n            return c\n    return None\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, 'No spreadsheet output.'\n    file_path = context.files.get_path(output.id)\n    xl = pd.ExcelFile(file_path)\n    brand_sheet = find_sheet(xl, ['brand']) or find_sheet(xl, ['sales','brand'])\n    store_sheet = find_sheet(xl, ['store']) or find_sheet(xl, ['sales','store'])\n    if not brand_sheet or not store_sheet:\n        return 0.0, 'Missing Brand/Store sheets.'\n    dfb = read_df(file_path, brand_sheet)\n    dfs = read_df(file_path, store_sheet)\n\n    b_brand = best_brand_col(dfb.columns)\n    s_brand = best_brand_col(dfs.columns)\n    if not b_brand or not s_brand:\n        return 0.0, 'Brand column not found.'\n\n    # Identify YTD metric columns\n    qty_labels = ['sales quantity','qty','units','sales qty']\n    dol_labels = ['sales $','total sales $','$ sales','sales dollars']\n    stk_labels = ['stock on hand','soh','stock']\n\n    b_qty = get_metric_col(dfb.columns, 'ytd', qty_labels)\n    b_dol = get_metric_col(dfb.columns, 'ytd', dol_labels)\n    b_stk = get_metric_col(dfb.columns, 'ytd', stk_labels)\n    s_qty = get_metric_col(dfs.columns, 'ytd', qty_labels)\n    s_dol = get_metric_col(dfs.columns, 'ytd', dol_labels)\n    s_stk = get_metric_col(dfs.columns, 'ytd', stk_labels)\n\n    needed = [b_qty,b_dol,b_stk,s_qty,s_dol,s_stk]\n    if any(x is None for x in needed):\n        # Not enough for this check\n        return 0.4, 'Incomplete YTD columns for full consistency check; partial credit.'\n\n    # Aggregate store by brand\n    dfs_num = dfs.copy()\n    for c in [s_qty,s_dol,s_stk]:\n        dfs_num[c] = pd.to_numeric(dfs_num[c], errors='coerce')\n    agg = dfs_num.groupby(s_brand)[[s_qty,s_dol,s_stk]].sum(min_count=1).reset_index()\n\n    dfb_num = dfb[[b_brand,b_qty,b_dol,b_stk]].copy()\n    for c in [b_qty,b_dol,b_stk]:\n        dfb_num[c] = pd.to_numeric(dfb_num[c], errors='coerce')\n\n    merged = pd.merge(dfb_num, agg, left_on=b_brand, right_on=s_brand, how='inner', suffixes=('_b','_s'))\n    if merged.empty:\n        return 0.2, 'No overlapping brands between sheets.'\n\n    def match_rate(b, s):\n        bvals = merged[b].astype(float)\n        svals = merged[s].astype(float)\n        # relative tolerance 5% or absolute 1 if values are small\n        denom = np.where(np.abs(bvals)>1, np.abs(bvals), 1)\n        rel = np.abs(bvals - svals)/denom\n        good = (rel <= 0.05) | (np.abs(bvals - svals) <= 1.0)\n        return float(good.mean())\n\n    r_qty = match_rate(b_qty, s_qty)\n    r_dol = match_rate(b_dol, s_dol)\n    r_stk = match_rate(b_stk, s_stk)\n\n    score = (r_qty + r_dol + r_stk)/3.0\n    return float(score), f'Consistency match rates \u2013 YTD Qty={r_qty:.2f}, YTD $={r_dol:.2f}, YTD Stock={r_stk:.2f}.'"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Presentation Quality and Stakeholder Readiness", "description": "Evaluate professional formatting, readability, and fitness for the buying team and DMM to make decisions.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Usability", "description": "Assess clarity, formatting, and usability of the workbook for executive decision-making.", "weight": 2.0, "judge_prompt": "Evaluate the Excel workbook for presentation quality and usability. Do NOT check numeric correctness\u2014only whether the file is professionally prepared and decision-ready.\n\nConsider:\n- Clear sheet naming and tab order: Data first, then Sales by Brand, then Sales by Store (or similarly logical).\n- Pivot tables are well-formatted: column headers readable, no truncated labels, filters/slicers (optional) or at least visible filter drop-downs.\n- Number formatting: Sales $ formatted as currency; ST% formatted as percent; quantities as integers where appropriate.\n- Grand totals are visually clear; subtotals by store/brand are sensible; no obvious layout issues (e.g., misaligned rows, blank header rows, merged cells harming usability).\n- Readability: freeze panes or header rows visible; reasonable column widths; minimal extraneous clutter.\n- Audience appropriateness: succinct, executive-friendly summaries enabling quick comparison across brands and stores.\n\nScoring:\n- 2.0: Highly professional and decision-ready; consistent number formats; clear totals; easy to navigate.\n- 1.0: Generally usable with minor formatting issues (e.g., a few columns not formatted, cramped widths) but still decision-supportive.\n- 0.5: Several formatting problems reduce clarity, though structure exists.\n- 0.0: Poorly formatted or hard to read; not suitable for stakeholder review.", "expectation": "A clean, well-formatted workbook with proper currency/percent formatting, clear totals, and logical tab order suitable for buying team and DMM review."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "01d7e53e-0513-4109-a242-8ccaf442cd21", "rubric": {"category_name": "Government \u2013 Recreation Workers: RecFit Agreement Draft (City of Recreare x Recreare University)", "rationale": "This rubric enforces a self-documenting contract deliverable. Stage 1 (LLM-only) mandates a specific DOCX/PDF structure so verification is trivial. Stage 2 mixes deterministic code checks (dates, duties, indemnity, exhibits, signatures) with one LLM cross-check for contacts/compliance. Stage 3 assesses professional quality and readiness for City Attorney review.", "max_total_score": 25.0, "stages": [{"name": "Stage 1 \u2013 Document Shape and Structural Gate", "description": "Gate: Verify the candidate delivered a professionally structured agreement document (DOCX/PDF) with the exact sections and artifacts needed for subsequent verification.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Structured Agreement Format Requirement (Gate)", "description": "Check if the output is a DOCX or PDF agreement with required structure for a municipal-university program agreement. Do NOT judge legal correctness\u2014only structural presence and formatting.", "weight": 8.0, "judge_prompt": "You are evaluating whether the submitted file is a properly structured agreement document for the RecFit program between the City of Recreare and Recreare University (RU). Only check FORMAT and PRESENCE of sections, not content correctness.\n\nFormat requirements:\n- File type: Must be DOCX or PDF (not Excel/CSV/plain text).\n- Length: At least 3 pages.\n- Professional formatting: clear title, headings, section numbering or bold section headers.\n\nRequired structural elements (flexible naming allowed; look for clear equivalents):\n1) Title and Parties/Recitals\n   - Title indicating an Agreement between City of Recreare and Recreare University (RU)\n   - A Parties or Recitals section (e.g., \u201cWHEREAS\u201d clauses) referencing RecFit and the Recreare Recreation and Aquatic Center.\n2) Program Scope/Description\n   - Section that describes the RecFit program and purpose.\n3) Facilities and Scheduling\n   - Section dedicated to space allocation at the Recreare Recreation and Aquatic Center (Fitness Center as principal space) and program scheduling.\n4) Term and Renewal\n   - A section labeled Term/Term and Renewal/Term; must explicitly discuss initial term and renewal options (don\u2019t check dates here).\n5) Roles and Responsibilities\n   - Duties for both parties in one or more sections.\n6) Insurance and Indemnification\n   - A section addressing insurance and mutual indemnification between the parties.\n7) Compliance with Laws/Regulations\n   - Section referencing applicable federal/state/city requirements or compliance language.\n8) Reporting/Data\n   - Section mentioning program reporting or data sharing obligations.\n9) Notices and Primary Contacts\n   - A section listing day-to-day primary contacts or notices information for both parties.\n10) Miscellaneous\n   - A Miscellaneous/General Provisions section that states/reflects inclusion of the City of Recreare\u2019s standard contract language (from \u201cRecreare_Official_Contract_Language.docx\u201d).\n11) Exhibits/Attachments\n   - Clear references to Exhibit A and Exhibit B, plus a separate \u201cEquipment Liability\u201d exhibit/section.\n12) Signature Blocks\n   - Signature blocks for both parties with the named signatories: Beth Cobb (City Clerk), Robert Howell, CPRE (Director of Parks and Recreation), Steve Southgate, MD (Chief of General Pediatrics), and Mark Coleman, Ph.D (Executive Vice Dean of Administration). Names can appear within signature blocks or an execution section.\n\nScoring (out of 8 points):\n- 8: Valid DOCX/PDF, \u22653 pages, professional formatting, and ALL 12 structural elements present.\n- 7: Valid format + \u226510 structural elements present (including Title/Parties, Term, Facilities/Scheduling, Insurance/Indemnification, Exhibits, Signature Blocks).\n- 6: Valid format + \u22659 structural elements present, including Title/Parties and Signature Blocks.\n- 0\u20135: Wrong format (not DOCX/PDF), <3 pages, or missing multiple core sections (Title/Parties, Term, Facilities/Scheduling, Insurance/Indemnification, Exhibits, Signature Blocks). Score proportionally to presence.\n\nDo not judge the correctness of dates, duties, or legal sufficiency\u2014only the existence of the sections and blocks that enable verification.", "expectation": "A well-structured, multi-page DOCX/PDF agreement with all mandated sections, exhibits, and signature blocks, ready for legal review."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification of Key Requirements (Content Checks)", "description": "Deterministic verification of required terms/duties/dates plus a small LLM cross-check for contacts and compliance. Uses Stage 1\u2019s structure to locate content via text search.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Term Dates and Renewal Options", "description": "Verify the agreement includes the exact initial term dates and renewal options: Jan 1, 2026 through Dec 31, 2027, with the option for two additional one-year renewals.", "weight": 2.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = ''\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = ''\n    lt = text.lower()\n    # Date patterns (flexible formats)\n    jan1_2026 = re.search(r'(january\\s*1,\\s*2026|\\b01/0?1/2026\\b|\\b1/0?1/2026\\b|\\b2026-01-01\\b)', lt)\n    dec31_2027 = re.search(r'(december\\s*31,\\s*2027|\\b12/3?1/2027\\b|\\b2027-12-31\\b)', lt)\n    renewal = re.search(r'\\b(two|2)\\s+(additional\\s+)?(one[-\\s]?year)\\s+(renewal|renewals|extension|extensions)\\b', lt)\n\n    hits = 0\n    hits += 1 if jan1_2026 else 0\n    hits += 1 if dec31_2027 else 0\n    hits += 1 if renewal else 0\n\n    score = (hits/3.0) * 2.0\n    return score"}, {"type": "code", "name": "Program Scheduling Requirements", "description": "Verify presence of minimum scheduling commitments: two hours twice per week on weeknights, and two hours on a weekend day.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = ''\n    lt = text.lower()\n\n    two_hours = re.search(r'\\b((two|2)\\s*(hours|hrs))\\b', lt)\n    twice_per_week = re.search(r'(twice\\s*(per\\s*)?week|two\\s*times\\s*(per\\s*)?week|2x\\s*per\\s*week|2\\s*days\\s*per\\s*week)', lt)\n    weeknights = re.search(r'weeknight', lt)\n    weekend = re.search(r'weekend\\s*day|saturday|sunday', lt)\n\n    hits = sum(1 for m in [two_hours, twice_per_week, weeknights, weekend] if m)\n    return (hits/4.0) * 1.0"}, {"type": "code", "name": "RU Obligations Present", "description": "Check RU obligations: manage grant funding; staffing; allow City staff to volunteer; cover all program expenses; provide annual participant report.", "weight": 2.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = ''\n    lt = text.lower()\n\n    # Manage grants\n    grant = re.search(r'grant', lt) and re.search(r'manag|administ', lt)\n    # Staffing\n    staffing = re.search(r'\\bstaff(ing)?\\b|personnel|\\bprovide\\s+staff\\b', lt)\n    # Allow City staff to volunteer\n    volunteer = re.search(r'\\bvolunteer', lt) and re.search(r'city\\s+staff', lt)\n    # Cover all program expenses / no cost to City\n    expenses = re.search(r'all\\s+program\\s+expenses|cover\\s+all\\s+expenses|no\\s+cost\\s+to\\s+the?\\s*city', lt)\n    # Annual participant report\n    annual_report = re.search(r'annual', lt) and re.search(r'report', lt) and re.search(r'participant', lt)\n\n    items = [grant, staffing, volunteer, expenses, annual_report]\n    hits = sum(1 for it in items if it)\n    return (hits/5.0) * 2.0"}, {"type": "code", "name": "City Obligations Present", "description": "Check City obligations: Fitness Center as principal space; locked storage closet; master calendar provided three times per year.", "weight": 2.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = ''\n    lt = text.lower()\n\n    # Fitness Center as principal/primary space\n    fitness_center = re.search(r'fitness\\s+center', lt)\n    principal = re.search(r'principal\\s+space|primary\\s+space|main\\s+space', lt)\n    fc_principal = bool(fitness_center and principal)\n\n    # Locked storage closet\n    storage = re.search(r'locked\\s+storage\\s+closet|lock(ed)?\\s+closet', lt)\n\n    # Master calendar three times per year\n    master_cal = re.search(r'master\\s+calendar', lt)\n    three_times = re.search(r'(three|3)\\s+(times\\s+per\\s+year|times/year|per\\s+year)', lt) or re.search(r'tri-?annual', lt)\n    calendar_ok = bool(master_cal and three_times)\n\n    items = [fc_principal, bool(storage), bool(calendar_ok)]\n    hits = sum(1 for it in items if it)\n    return (hits/3.0) * 2.0"}, {"type": "code", "name": "Mutual Indemnification and Self-Insurance", "description": "Verify mutual indemnification language and that both parties are self-insured.", "weight": 2.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = ''\n    lt = text.lower()\n\n    indemn = re.search(r'indemnif|hold\\s+harmless', lt)\n    # Attempt to infer mutuality: look for words indicating reciprocity or both parties named near indemnity\n    mutual = re.search(r'mutual|reciprocal|each\\s+other', lt)\n    both_named = re.search(r'city\\s+of\\s+recreare', lt) and (re.search(r'recreare\\s+university', lt) or re.search(r'\\bru\\b', lt))\n    mutuality = bool(mutual) or bool(indemn and both_named)\n\n    self_ins = re.search(r'self[-\\s]?insured|self[-\\s]?insurance', lt)\n\n    hits = sum(1 for it in [bool(indemn), bool(mutuality), bool(self_ins)] if it)\n    return (hits/3.0) * 2.0"}, {"type": "code", "name": "Parties and Signature Blocks", "description": "Verify presence of the four named signatories. Partial credit if some names are present.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = ''\n    lt = text.lower()\n\n    names = [\n        ('beth', 'cobb'),\n        ('robert', 'howell'),\n        ('steve', 'southgate'),\n        ('mark', 'coleman')\n    ]\n    hits = 0\n    for first, last in names:\n        if re.search(r'\\b' + first + r'\\b', lt) and re.search(r'\\b' + last + r'\\b', lt):\n            hits += 1\n    return (hits/4.0) * 1.0"}, {"type": "code", "name": "Exhibits and Equipment Liability Present", "description": "Verify references to Exhibit A, Exhibit B, and a specific Equipment Liability section.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = ''\n    lt = text.lower()\n\n    ex_a = re.search(r'exhibit\\s*a\\b', lt)\n    ex_b = re.search(r'exhibit\\s*b\\b', lt)\n    equip = re.search(r'equipment\\s+liabilit(y|ies)', lt)\n\n    hits = sum(1 for m in [ex_a, ex_b, equip] if m)\n    return (hits/3.0) * 1.0"}, {"type": "llm_judge", "name": "Contacts and Compliance Cross-Check", "description": "LLM verification that primary contacts (day-to-day) are listed for each party with at least phone or email, and that compliance language references federal/state/city requirements (ideally North Carolina).", "weight": 1.0, "judge_prompt": "Review the agreement and check ONLY for the following:\nA) Primary Contacts/Notices: Does the document list day-to-day primary contacts for BOTH parties (City and University/RU) with at least one of: email or phone for each party? Section labeling can vary (e.g., Notices, Contacts, Primary Contacts).\nB) Compliance language: Is there a section that references compliance with applicable federal/state/city laws or regulations? Prefer mention of North Carolina, but allow general references.\n\nScoring (1.0 total):\n- 1.0 = Both A and B are clearly present.\n- 0.5 = Only one of A or B is clearly present.\n- 0.0 = Neither is present.\n\nDo not judge quality\u2014only presence of these items.", "expectation": "Both parties have contacts with phone/email and there is explicit compliance language referencing applicable laws (ideally North Carolina)."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Professional Quality and Readiness for Legal Review", "description": "Holistic LLM assessment of clarity, organization, drafting professionalism, and review-readiness for the City Attorney and University Counsel.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professionalism, Clarity, and Review-Readiness", "description": "Assess whether the agreement reads like a professional municipal-university program agreement ready for attorney review.", "weight": 5.0, "judge_prompt": "Evaluate the agreement\u2019s overall quality and readiness for legal review. Consider:\n1) Organization and Formatting: Clear title, consistent defined terms (e.g., \u201cCity,\u201d \u201cUniversity\u201d/\u201cRU\u201d), numbered Articles/Sections, consistent cross-references to Exhibits.\n2) Clarity and Precision: Plain, precise drafting; removal of placeholders like [TBD]; no contradictory statements; obligations/rights are unambiguous.\n3) Legal Drafting Professionalism: Proper recitals, signature blocks, execution dating, headings, and standard Miscellaneous provisions; references to the City\u2019s standard contract language incorporated appropriately.\n4) Consistency: Roles and responsibilities align across sections (e.g., RU manages grants and staffing; City provides facility and calendar). Exhibit labels match references in the body.\n5) Appropriateness for Audience: Tone fits a City Attorney review; avoids extraneous marketing language; includes reasonable protective clauses (indemnity, compliance, insurance acknowledgment) without overreaching.\n\nScoring (0\u20135):\n- 5: Exemplary: well-organized, professionally drafted, consistent, clean of placeholders/contradictions, and appears attorney-ready.\n- 4: Strong: minor issues (typos, small inconsistencies) but overall ready.\n- 3: Adequate: understandable but with several drafting/formatting issues to correct before review.\n- 2: Weak: disorganized or ambiguous; significant edits needed before legal review.\n- 0\u20131: Poor: unprofessional or confusing; not suitable for legal review.\n", "expectation": "A clean, consistent, professionally formatted agreement that appears ready for City Attorney review."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "0353ee0c-18b5-4ad3-88e8-e001d223e1d7", "rubric": {"category_name": "PACT Act Presumptive Eligibility Guide (Consolidated PDF)", "rationale": "This rubric forces a self-documenting, verifiable PDF/DOCX deliverable. Stage 1 is a strict LLM-only gate that mandates exact section headers and pipe-delimited table headers so automated code checks in Stage 2 can reliably parse extracted text. Stage 2 mixes deterministic code rules (headers, tables, source crosswalk linkage, date formats, revision log) with an LLM cross-reference check. Stage 3 assesses presentation quality and audience suitability for veterans seeking PACT Act information.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Structure Gate (LLM Only)", "description": "MANDATORY structure and format gate for a consolidated PACT Act guide. This stage enforces exact headers and table shapes that make verification trivial. Do NOT judge factual correctness.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Document Format and Structural Completeness", "description": "Verify the candidate output is a properly structured PDF/DOCX guide that follows the exact required section headers and table header rows below. Only check PRESENCE/FORMAT, not correctness of claims.", "weight": 4.0, "judge_prompt": "You are evaluating whether the candidate output is a structured, clinic-ready PACT Act guide. Only check the structure/format. Do NOT judge factual correctness. The output MUST meet all required elements below. If not PDF/DOCX or major elements missing, score low.\n\nFormat requirements:\n- File type: PDF or DOCX (not plain text, not spreadsheet)\n- Minimum length: 6 pages\n- Professional, readable formatting with clear section headers\n\nRequired top-level sections (use exact header text, case-insensitive match is OK, but the text itself must appear):\n1) \"How to Use This Guide\"\n2) \"Eligibility Overview: Presumptive vs Non\u2011Presumptive\"\n3) \"Table A: Presumptive Exposures\"\n4) \"Table B: Presumptive Conditions \u2014 Cancer\"\n5) \"Table C: Presumptive Conditions \u2014 Non\u2011Cancer\"\n6) \"Table D: Service Location and Date Matrix\"\n7) \"How to File a Claim (Step\u2011by\u2011Step)\"\n8) \"Documentation Checklist\"\n9) \"Sources and Crosswalk\"\n10) \"Revision Log\"\n11) \"Glossary\"\n\nRequired table header rows (these exact pipe-delimited headers must be visible so automated parsing can work; case-insensitive is acceptable but columns and order must match):\n- For Table A (Presumptive Exposures):\n  \"Exposure Name | Service Location/Operation | Service Dates | Eligibility Notes | Source ID\"\n- For Table B (Presumptive Conditions \u2014 Cancer):\n  \"Condition Name | Exposure Link | Service Location/Operation | Service Dates | Notes/Limitations | Source ID\"\n- For Table C (Presumptive Conditions \u2014 Non\u2011Cancer):\n  \"Condition Name | Exposure Link | Service Location/Operation | Service Dates | Notes/Limitations | Source ID\"\n- For Table D (Service Location and Date Matrix):\n  \"Service Era/Location | Covered Exposures | Covered Conditions (count) | Service Dates | Notes | Source ID\"\n\nAdditional required structural elements:\n- \"Sources and Crosswalk\" section lists sources as enumerated IDs in the format [S1], [S2], \u2026 each with: Title, URL, Accessed Date.\n- \"Revision Log\" includes at least one entry with Version, Date, Preparer, and Summary of Changes.\n- A brief, 4\u20138 step list under \"How to File a Claim (Step\u2011by\u2011Step)\".\n\nScoring (structure only):\n- 4.0: PDF/DOCX, \u22656 pages, all 11 sections present with exact titles AND all 4 table header rows exactly as specified, plus Sources/Crosswalk and Revision Log present with their required fields.\n- 3.0\u20133.9: PDF/DOCX, \u22656 pages, missing up to 2 required elements (a section header or one table header row), but core tables and Sources/Crosswalk are present.\n- 1.5\u20132.9: Valid format but missing 3\u20135 required elements, or uses different table columns/order.\n- 0.1\u20131.4: Valid format but severely incomplete structure (missing most sections/tables) or <6 pages.\n- 0.0: Not PDF/DOCX or no recognizable required sections.\n\nOnly evaluate presence/format of these items. Do NOT judge whether the medical content is correct.", "expectation": "A clinic-ready PDF/DOCX guide with the exact section and table headers specified, enabling automated verification in Stage 2."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Code + LLM)", "description": "Now that the document shape is enforced, verify internal consistency and machine-checkable elements using extracted text, plus a focused LLM consistency check. Do not judge real-world factual correctness beyond internal consistency.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Section Headers Present (Exact)", "description": "Check that all required section headers appear in the extracted text (case-insensitive). Returns proportion present.", "weight": 0.9, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output.\"\n    text = \"\"\n    try:\n        if output.file_ext.lower().endswith('.pdf'):\n            text = context.files.read_pdf_text(output.id)\n        elif output.file_ext.lower().endswith('.docx'):\n            text = context.files.read_docx_text(output.id)\n        else:\n            text = context.files.read_text(output.id)\n    except Exception as e:\n        return 0.0, f\"Text extraction failed: {e}\"\n    t = text.lower()\n    required_headers = [\n        \"how to use this guide\",\n        \"eligibility overview: presumptive vs non\",\n        \"table a: presumptive exposures\",\n        \"table b: presumptive conditions \u2014 cancer\",\n        \"table c: presumptive conditions \u2014 non\",\n        \"table d: service location and date matrix\",\n        \"how to file a claim (step\",\n        \"documentation checklist\",\n        \"sources and crosswalk\",\n        \"revision log\",\n        \"glossary\",\n    ]\n    hits = 0\n    missing = []\n    for h in required_headers:\n        if h in t:\n            hits += 1\n        else:\n            missing.append(h)\n    score = hits / len(required_headers)\n    feedback = f\"Found {hits}/{len(required_headers)} headers. Missing: {', '.join(missing) if missing else 'None'}\"\n    return score, feedback"}, {"type": "code", "name": "Table Headers and Minimal Row Counts", "description": "Verify pipe-delimited table headers exist exactly and each table has a minimal number of data rows (lines containing pipes). Returns average across four tables.", "weight": 1.1, "code": "import re\n\ndef evaluate(workflow, context):\n    def get_text(output, context):\n        if output.file_ext.lower().endswith('.pdf'):\n            return context.files.read_pdf_text(output.id)\n        elif output.file_ext.lower().endswith('.docx'):\n            return context.files.read_docx_text(output.id)\n        else:\n            return context.files.read_text(output.id)\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output.\"\n    try:\n        text = get_text(output, context)\n    except Exception as e:\n        return 0.0, f\"Text extraction failed: {e}\"\n    t = text\n    # Define headers and minimum rows\n    tables = [\n        (\"Exposure Name | Service Location/Operation | Service Dates | Eligibility Notes | Source ID\", 8),\n        (\"Condition Name | Exposure Link | Service Location/Operation | Service Dates | Notes/Limitations | Source ID\", 10),\n        (\"Condition Name | Exposure Link | Service Location/Operation | Service Dates | Notes/Limitations | Source ID\", 8),\n        (\"Service Era/Location | Covered Exposures | Covered Conditions (count) | Service Dates | Notes | Source ID\", 5),\n    ]\n    table_names = [\"Table A\", \"Table B\", \"Table C\", \"Table D\"]\n    achieved = []\n    notes = []\n    for (header, min_rows), name in zip(tables, table_names):\n        hidx = t.lower().find(header.lower())\n        if hidx == -1:\n            achieved.append(0.0)\n            notes.append(f\"{name}: header not found\")\n            continue\n        # Take substring after header to approximate the table body\n        sub = t[hidx: hidx + 20000]  # large window\n        # Count data lines with >= number of pipes as header and not the header itself\n        header_pipes = header.count('|')\n        lines = sub.splitlines()\n        row_count = 0\n        for ln in lines[1:]:\n            if '|' in ln and ln.count('|') >= header_pipes and (ln.strip().lower() != header.lower()):\n                row_count += 1\n        ok = 1.0 if row_count >= min_rows else max(0.0, row_count / max(1, min_rows))\n        achieved.append(ok)\n        notes.append(f\"{name}: {row_count}/{min_rows} rows\")\n    score = sum(achieved) / len(achieved)\n    feedback = \"; \".join(notes)\n    return score, feedback"}, {"type": "code", "name": "Sources Crosswalk Coverage", "description": "Check that source IDs used in tables [S#] are defined in the Sources and Crosswalk section with URLs. Scores based on coverage and minimum count.", "weight": 0.9, "code": "import re\n\ndef evaluate(workflow, context):\n    def get_text(output, context):\n        if output.file_ext.lower().endswith('.pdf'):\n            return context.files.read_pdf_text(output.id)\n        elif output.file_ext.lower().endswith('.docx'):\n            return context.files.read_docx_text(output.id)\n        else:\n            return context.files.read_text(output.id)\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output.\"\n    try:\n        text = get_text(output, context)\n    except Exception as e:\n        return 0.0, f\"Text extraction failed: {e}\"\n    t_low = text.lower()\n    # Identify Sources section window\n    start = t_low.find(\"sources and crosswalk\")\n    end_candidates = [t_low.find(\"revision log\", start+1), t_low.find(\"glossary\", start+1)]\n    end = len(t_low)\n    for c in end_candidates:\n        if c != -1:\n            end = min(end, c)\n    sources_block = text[start:end] if start != -1 else \"\"\n    used_ids = set(m.lower() for m in re.findall(r\"\\[s(\\d+)\\]\", text, flags=re.IGNORECASE))\n    defined = set(m.lower() for m in re.findall(r\"\\[s(\\d+)\\]\\s*[:\\-\u2013]\\s*(?:https?://\\S+|www\\.\\S+)\", sources_block, flags=re.IGNORECASE))\n    used_count = len(used_ids)\n    defined_count = len(defined)\n    if used_count == 0:\n        return 0.0, \"No [S#] source IDs used in tables.\"\n    coverage = len(used_ids & defined) / used_count if used_count else 0.0\n    # Encourage at least 5 sources defined\n    qty_factor = min(1.0, defined_count / 5) if defined_count else 0.0\n    score = coverage * 0.7 + qty_factor * 0.3\n    feedback = f\"Used IDs: {used_count}, Defined: {defined_count}, Coverage: {coverage:.2f}\"\n    return score, feedback"}, {"type": "code", "name": "Service Dates Format Coverage", "description": "Detect presence of service date formats and ranges within the document to ensure entries include date qualifiers.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output.\"\n    try:\n        if output.file_ext.lower().endswith('.pdf'):\n            text = context.files.read_pdf_text(output.id)\n        elif output.file_ext.lower().endswith('.docx'):\n            text = context.files.read_docx_text(output.id)\n        else:\n            text = context.files.read_text(output.id)\n    except Exception as e:\n        return 0.0, f\"Text extraction failed: {e}\"\n    # Patterns: year ranges and Month Year\n    year_range = re.findall(r\"(19|20)\\d{2}\\s*[\\-\u2013to]+\\s*(19|20)\\d{2}\", text, flags=re.IGNORECASE)\n    month_year = re.findall(r\"\\b(jan|feb|mar|apr|may|jun|jul|aug|sep|sept|oct|nov|dec)[a-z]*\\s+(19|20)\\d{2}\\b\", text, flags=re.IGNORECASE)\n    count = len(year_range) + len(month_year)\n    target = 8\n    score = min(1.0, count / target)\n    return score, f\"Found {count} date expressions (ranges and month-year).\""}, {"type": "code", "name": "Revision Log Has Version and Date", "description": "Ensure the Revision Log includes a version tag and a 2024/2025 date.", "weight": 0.2, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output.\"\n    try:\n        if output.file_ext.lower().endswith('.pdf'):\n            text = context.files.read_pdf_text(output.id)\n        elif output.file_ext.lower().endswith('.docx'):\n            text = context.files.read_docx_text(output.id)\n        else:\n            text = context.files.read_text(output.id)\n    except Exception as e:\n        return 0.0, f\"Text extraction failed: {e}\"\n    # Slice around revision log\n    t_low = text.lower()\n    start = t_low.find(\"revision log\")\n    block = text[start:start+3000] if start != -1 else \"\"\n    has_version = re.search(r\"\\b(v(?:ersion)?\\s*\\d+(?:\\.\\d+)*)\\b\", block, flags=re.IGNORECASE) is not None\n    has_date = re.search(r\"\\b(2024|2025)\\b\", block) is not None\n    score = 1.0 if (has_version and has_date) else 0.0\n    fb = f\"Version: {'yes' if has_version else 'no'}, Date(2024/2025): {'yes' if has_date else 'no'}\"\n    return score, fb"}, {"type": "llm_judge", "name": "Cross-Reference Consistency (LLM)", "description": "Check internal consistency: rows in Tables B/C include Service Location/Operation, Service Dates, and Source ID; exposure names referenced in Tables B/C appear in Table A; and Sources/Crosswalk entries correspond to Source IDs used. Do not judge factual truthfulness, only internal linkage and completeness.", "weight": 0.5, "judge_prompt": "Review the document for internal consistency ONLY (no external fact checking). Assess:\n- In Tables B and C, do typical rows include values for Service Location/Operation, Service Dates, and Source ID?\n- Do exposure names referenced in Tables B/C (under Exposure Link) appear as rows in Table A (names can match case-insensitively; minor wording variations are OK)?\n- Do Source IDs used in tables correspond to entries in the Sources and Crosswalk section?\n\nScoring:\n- 0.5: Strong internal consistency: Tables B/C rows are well-populated; Exposure Link items are represented in Table A; Source IDs in tables are clearly defined in Sources/Crosswalk.\n- 0.3\u20130.4: Mostly consistent with some gaps (e.g., a few missing Service Dates or Source IDs, or a few Exposure Links not found in Table A).\n- 0.1\u20130.2: Many inconsistencies or missing fields.\n- 0.0: Tables B/C lack key fields, or Exposure Links largely absent from Table A, or Source IDs not defined.\n\nOnly judge internal cross-references and presence, not correctness of medical content.", "expectation": "Tables B/C rows have complete fields; Exposure Link values map to Table A entries; Source IDs used are defined in the Sources/Crosswalk."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Utility (LLM)", "description": "Professionalism, accessibility, and usefulness to veterans seeking to file PACT Act claims. Do not judge factual correctness beyond surface plausibility.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Presentation and Accessibility", "description": "Assess layout clarity, readability, and accessibility features.", "weight": 1.0, "judge_prompt": "Evaluate presentation quality of the PDF/DOCX:\n- Clear hierarchy: headings, spacing, and consistent typography.\n- Readable tables: column widths, wrapping, and alignment that avoid truncation.\n- Navigability: optional TOC, page headers/footers, section numbering.\n- Accessibility cues: descriptive section titles, minimal jargon, legends for abbreviations, high-contrast colors if used.\n\nScoring:\n- 1.0: Professional, clean, accessible; tables are easy to read; document feels clinic-ready.\n- 0.6\u20130.9: Generally good with minor readability issues.\n- 0.3\u20130.5: Noticeable formatting issues or clutter that slow comprehension.\n- 0.0\u20130.2: Poor formatting, hard to read or navigate.", "expectation": "Clinic-ready formatting with clear hierarchy and readable tables."}, {"type": "llm_judge", "name": "Audience Fit and Practical Utility", "description": "Assess usefulness for veterans filing PACT Act claims.", "weight": 1.0, "judge_prompt": "Judge the practical utility for veterans:\n- Does \"How to File a Claim (Step\u2011by\u2011Step)\" give actionable steps without jargon?\n- Is the \"Documentation Checklist\" practical (e.g., service records, medical records, provider letters)?\n- Does the guide avoid introducing novel claims beyond sources while still being comprehensive and de-duplicated?\n- Is tone supportive and clear for non-expert readers?\n\nScoring:\n- 1.0: Highly useful, actionable, and appropriate for veterans.\n- 0.6\u20130.9: Useful with minor gaps.\n- 0.3\u20130.5: Somewhat helpful but misses key practical details.\n- 0.0\u20130.2: Not useful or confusing.", "expectation": "Actionable steps, practical checklist, supportive tone tailored to veterans."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "61f546a8-c374-467f-95cc-d0d9b5656eb6", "rubric": {"category_name": "TR Apartments Turn Plan Report (Counter and Rental Clerks)", "rationale": "This rubric enforces a self-documenting PDF report with two clearly structured sections plus machine-readable CSV appendices embedded in the PDF text. Stage 1 (LLM-only) is a strict format/structure gate to make downstream verification trivial. Stage 2 uses code rules to parse the embedded CSV blocks from the PDF text and verify scheduling constraints and cross-section consistency. Stage 3 assesses professional quality and managerial usefulness.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate (LLM-only)", "description": "Enforce the exact output shape so verification is trivial. The report must be a PDF with two sections, required tables, and machine-readable CSV appendices delimited by markers, plus a brief assumptions/methodology note.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured PDF with Required Sections and CSV Appendices", "description": "Verify the candidate produced a properly structured PDF containing the two required sections with tables and embedded CSV appendices with exact markers and headers.", "weight": 4.0, "judge_prompt": "You are evaluating whether the submitted output is a properly structured PDF report for a turn plan at TR Apartments. Only check PRESENCE and STRUCTURE, not correctness of the dates or logic.\n\nFormat requirements (must-have):\n- Must be a PDF file (not Word, not Excel, not plain text)\n- At least 1 page\n- Professional section headers visible\n\nRequired Sections (header names can be close variants):\n1) Section 1: \"Vendor Work Plan\" (or similar like \"By Vendor\", \"Vendor Schedule\"). This section must include a table that lists contracted services by vendor and apartment, indicates whether any new appliances need to be ordered, includes the scheduled service date, and indicates whether the make-ready date needs to change.\n   - The table must visibly contain columns covering: Vendor, Apartment, Service Type, Appliance Order Needed (Yes/No), Scheduled Date, Make-Ready Date Change (Yes/No), Notes. Column names can be close variants as long as meaning is clear.\n\n2) Section 2: \"Unit Work Schedule\" (or similar like \"By Apartment\", \"Apartment Work Plan\"). This section must include a table by apartment listing type of work required, who will do it (for on-site staff, show \"our staff\"), any appliance deliveries, and the date of work.\n   - The table must visibly contain columns covering: Apartment, Work Type, Provider (should say \"our staff\" for internal team), Date, Is Appliance Delivery (Yes/No), Notes. Column names can be close variants as long as meaning is clear.\n\nAssumptions/Methodology note (brief paragraph or bullet list):\n- Must state the key scheduling guidelines in some form: (a) No two vendors can work in the unit on the same day; (b) Our on-site staff need a total of 2 days and work Mon\u2013Fri; (c) Any appliance installation adds an extra day except hot water tank installs; (d) Delivery orders can't be scheduled on holidays or weekends; (e) Move-outs occurred on 6/30/25.\n\nMachine-Readable Appendices (strict markers to enable code checks):\n- The PDF text must include TWO CSV blocks, delimited exactly as follows (use these literal markers on their own lines):\n\n<<<BEGIN Vendor_Schedule_CSV>>>\nVendor,Apartment,Service_Type,Appliance_Order_Needed,Scheduled_Date,Make_Ready_Date_Change,Notes\n...data rows here with no commas inside fields...\n<<<END Vendor_Schedule_CSV>>>\n\n<<<BEGIN Unit_Schedule_CSV>>>\nApartment,Work_Type,Provider,Date,Is_Appliance_Delivery,Notes\n...data rows here with no commas inside fields...\n<<<END Unit_Schedule_CSV>>>\n\nCSV formatting requirements:\n- Use exactly the headers shown (spelling and order as above)\n- Dates in ISO format YYYY-MM-DD\n- Values for Yes/No fields should be Yes or No\n- No embedded commas in fields (simple CSV)\n\nScoring (Structure only):\n- 4.0: PDF format AND both required sections with appropriate tables AND an assumptions/methodology note AND BOTH CSV blocks present with exact markers and exact headers.\n- 3.2: PDF with both sections and tables AND BOTH CSV blocks present, but minor omission (e.g., assumptions note missing or one column header is a close variant in the visual tables). CSV headers must still match exactly.\n- 2.0: PDF has both sections and tables, but CSV appendices are missing or only one block present.\n- 0.0: Not a PDF, OR missing one of the two required sections entirely, OR tables absent.\n\nOnly evaluate structure, not correctness of scheduling or dates.", "expectation": "A one- to three-page professional PDF with the two specified sections and tables, a short assumptions note documenting constraints, and two machine-readable CSV blocks with exact markers and headers."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Scheduling and Consistency Verification (Code + lightweight checks)", "description": "Parse the embedded CSV appendices from the PDF text and verify scheduling rules, date validity, and cross-section consistency. Partial credit awarded per check.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "CSV Blocks Parsed + Required Columns Present", "description": "Extract both CSV blocks from the PDF text using the exact markers. Verify both parse successfully with all required columns.", "weight": 0.8, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output.\"\n    try:\n        text = context.files.read_pdf_text(output.id) if output.ext.lower()=='.pdf' else context.files.read_docx_text(output.id)\n    except Exception as e:\n        return 0.0, f\"Failed to read document text: {e}\"\n\n    def extract_block(t, name):\n        m = re.search(rf\"<<<BEGIN {name}>>>\\n(.*?)\\n<<<END {name}>>>\", t, flags=re.DOTALL)\n        return m.group(1) if m else None\n\n    def parse_simple_csv(block):\n        if not block:\n            return None\n        lines = [ln.strip() for ln in block.strip().splitlines() if ln.strip()]\n        if not lines:\n            return None\n        headers = [h.strip() for h in lines[0].split(',')]\n        rows = []\n        for ln in lines[1:]:\n            parts = [p.strip() for p in ln.split(',')]\n            if len(parts) != len(headers):\n                # skip malformed lines\n                continue\n            rows.append(parts)\n        try:\n            df = pd.DataFrame(rows, columns=headers)\n            return df\n        except Exception:\n            return None\n\n    vendor_block = extract_block(text, 'Vendor_Schedule_CSV')\n    unit_block = extract_block(text, 'Unit_Schedule_CSV')\n\n    score = 0.0\n    feedback = []\n\n    required_vendor_cols = ['Vendor','Apartment','Service_Type','Appliance_Order_Needed','Scheduled_Date','Make_Ready_Date_Change','Notes']\n    required_unit_cols = ['Apartment','Work_Type','Provider','Date','Is_Appliance_Delivery','Notes']\n\n    vend_df = parse_simple_csv(vendor_block)\n    unit_df = parse_simple_csv(unit_block)\n\n    if vend_df is not None and all(c in vend_df.columns for c in required_vendor_cols):\n        score += 0.4\n    else:\n        feedback.append('Vendor CSV missing or columns incorrect.')\n\n    if unit_df is not None and all(c in unit_df.columns for c in required_unit_cols):\n        score += 0.4\n    else:\n        feedback.append('Unit CSV missing or columns incorrect.')\n\n    return score, ('; '.join(feedback) if feedback else 'Both CSV blocks parsed with required columns.')"}, {"type": "code", "name": "Date Validity and After Move-Out", "description": "Dates must be valid ISO YYYY-MM-DD. All scheduled dates should be on or after 2025-07-01 (the day after 6/30/25 move-outs). Partial credit by fraction valid.", "weight": 0.6, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id) if output.ext.lower()=='.pdf' else context.files.read_docx_text(output.id)\n    except Exception:\n        return 0.0\n\n    def extract_block(t, name):\n        m = re.search(rf\"<<<BEGIN {name}>>>\\n(.*?)\\n<<<END {name}>>>\", t, flags=re.DOTALL)\n        return m.group(1) if m else None\n    def parse_simple_csv(block):\n        if not block:\n            return None\n        lines = [ln.strip() for ln in block.strip().splitlines() if ln.strip()]\n        if not lines:\n            return None\n        headers = [h.strip() for h in lines[0].split(',')]\n        rows = []\n        for ln in lines[1:]:\n            parts = [p.strip() for p in ln.split(',')]\n            if len(parts) != len(headers):\n                continue\n            rows.append(parts)\n        try:\n            return pd.DataFrame(rows, columns=headers)\n        except Exception:\n            return None\n\n    vendor_block = extract_block(text, 'Vendor_Schedule_CSV')\n    unit_block = extract_block(text, 'Unit_Schedule_CSV')\n\n    score = 0.0\n\n    # Parse and validate dates\n    vend_df = parse_simple_csv(vendor_block)\n    unit_df = parse_simple_csv(unit_block)\n    if vend_df is None or unit_df is None:\n        return 0.0\n\n    def pct_valid_dates(series):\n        if series is None or len(series)==0:\n            return 0.0\n        dt = pd.to_datetime(series, format='%Y-%m-%d', errors='coerce')\n        return float((~dt.isna()).mean())\n\n    pv = pct_valid_dates(vend_df.get('Scheduled_Date'))\n    pu = pct_valid_dates(unit_df.get('Date'))\n\n    # Valid ISO date formats\n    score_iso = 0.3 * ((pv + pu) / 2.0)\n\n    # On or after 2025-07-01\n    mv = pd.to_datetime(vend_df.get('Scheduled_Date'), format='%Y-%m-%d', errors='coerce')\n    mu = pd.to_datetime(unit_df.get('Date'), format='%Y-%m-%d', errors='coerce')\n    cutoff = pd.Timestamp('2025-07-01')\n    good_v = (mv.notna() & (mv >= cutoff)).mean() if len(vend_df)>0 else 0.0\n    good_u = (mu.notna() & (mu >= cutoff)).mean() if len(unit_df)>0 else 0.0\n    score_after = 0.3 * ((good_v + good_u) / 2.0)\n\n    return score_iso + score_after"}, {"type": "code", "name": "Staff Scheduling Constraints", "description": "Our on-site staff must work Mon\u2013Fri only and require a total of 2 distinct days per unit. Partial credit by fraction compliant.", "weight": 0.8, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id) if output.ext.lower()=='.pdf' else context.files.read_docx_text(output.id)\n    except Exception:\n        return 0.0\n\n    def extract_block(t, name):\n        m = re.search(rf\"<<<BEGIN {name}>>>\\n(.*?)\\n<<<END {name}>>>\", t, flags=re.DOTALL)\n        return m.group(1) if m else None\n    def parse_simple_csv(block):\n        if not block:\n            return None\n        lines = [ln.strip() for ln in block.strip().splitlines() if ln.strip()]\n        if not lines:\n            return None\n        headers = [h.strip() for h in lines[0].split(',')]\n        rows = []\n        for ln in lines[1:]:\n            parts = [p.strip() for p in ln.split(',')]\n            if len(parts) != len(headers):\n                continue\n            rows.append(parts)\n        try:\n            return pd.DataFrame(rows, columns=headers)\n        except Exception:\n            return None\n\n    unit_block = extract_block(text, 'Unit_Schedule_CSV')\n    unit_df = parse_simple_csv(unit_block)\n    if unit_df is None or 'Provider' not in unit_df.columns or 'Date' not in unit_df.columns or 'Apartment' not in unit_df.columns:\n        return 0.0\n\n    # Normalize\n    udf = unit_df.copy()\n    udf['Provider_l'] = udf['Provider'].astype(str).str.strip().str.lower()\n    udf['Date_dt'] = pd.to_datetime(udf['Date'], format='%Y-%m-%d', errors='coerce')\n\n    staff = udf[udf['Provider_l'] == 'our staff'].copy()\n    if len(staff) == 0:\n        return 0.0\n\n    # Staff only on weekdays (Mon-Fri)\n    weekday_ok = (staff['Date_dt'].notna() & (staff['Date_dt'].dt.weekday <= 4)).mean()\n    score_weekdays = 0.4 * float(weekday_ok)\n\n    # At least 2 distinct staff days per apartment\n    def has_two_days(g):\n        d = g['Date_dt'].dropna().dt.normalize().nunique()\n        return 1.0 if d >= 2 else 0.0\n    per_unit = staff.groupby(staff['Apartment']).apply(has_two_days) if len(staff)>0 else pd.Series(dtype=float)\n    score_two_days = 0.4 * (float(per_unit.mean()) if len(per_unit)>0 else 0.0)\n\n    return score_weekdays + score_two_days"}, {"type": "code", "name": "Appliance Delivery and Installation Constraints", "description": "Delivery orders cannot be on weekends or holidays. Any non\u2013hot water tank appliance installation should add at least one extra day to the unit\u2019s overall timeline (>=3 distinct dates including staff). Partial credit by fraction compliant.", "weight": 0.8, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id) if output.ext.lower()=='.pdf' else context.files.read_docx_text(output.id)\n    except Exception:\n        return 0.0\n\n    def extract_block(t, name):\n        m = re.search(rf\"<<<BEGIN {name}>>>\\n(.*?)\\n<<<END {name}>>>\", t, flags=re.DOTALL)\n        return m.group(1) if m else None\n    def parse_simple_csv(block):\n        if not block:\n            return None\n        lines = [ln.strip() for ln in block.strip().splitlines() if ln.strip()]\n        if not lines:\n            return None\n        headers = [h.strip() for h in lines[0].split(',')]\n        rows = []\n        for ln in lines[1:]:\n            parts = [p.strip() for p in ln.split(',')]\n            if len(parts) != len(headers):\n                continue\n            rows.append(parts)\n        try:\n            return pd.DataFrame(rows, columns=headers)\n        except Exception:\n            return None\n\n    unit_block = extract_block(text, 'Unit_Schedule_CSV')\n    unit_df = parse_simple_csv(unit_block)\n    if unit_df is None:\n        return 0.0\n\n    df = unit_df.copy()\n    for col in ['Work_Type','Is_Appliance_Delivery','Date','Apartment']:\n        if col not in df.columns:\n            return 0.0\n    df['Date_dt'] = pd.to_datetime(df['Date'], format='%Y-%m-%d', errors='coerce')\n    df['work_l'] = df['Work_Type'].astype(str).str.lower()\n    df['is_delivery'] = df['Is_Appliance_Delivery'].astype(str).str.strip().str.lower().isin(['yes','y','true']) | df['work_l'].str.contains('delivery')\n\n    # US Federal holidays (2025) relevant to mid-year scheduling\n    holidays = set(pd.to_datetime([\n        '2025-01-01','2025-01-20','2025-02-17','2025-05-26','2025-06-19','2025-07-04',\n        '2025-09-01','2025-10-13','2025-11-11','2025-11-27','2025-12-25'\n    ]))\n\n    # 1) Deliveries not on weekends/holidays\n    deliveries = df[df['is_delivery'] & df['Date_dt'].notna()].copy()\n    if len(deliveries) == 0:\n        score_deliv = 0.4  # no violations possible\n    else:\n        ok = ((deliveries['Date_dt'].dt.weekday <= 4) & (~deliveries['Date_dt'].isin(holidays))).mean()\n        score_deliv = 0.4 * float(ok)\n\n    # 2) Non-HWT appliance installation adds extra day (>= 3 distinct dates for that unit)\n    # Identify units with appliance installation that is NOT hot water tank\n    install_units = df[df['work_l'].str.contains('appliance') & ~df['work_l'].str.contains('hot\\s*water\\s*tank')]['Apartment'].unique().tolist()\n    if len(install_units) == 0:\n        score_extra = 0.4  # nothing to enforce\n    else:\n        ok_list = []\n        for apt in install_units:\n            sub = df[df['Apartment'] == apt]\n            distinct_days = sub['Date_dt'].dropna().dt.normalize().nunique()\n            ok_list.append(1.0 if distinct_days >= 3 else 0.0)\n        score_extra = 0.4 * (sum(ok_list)/len(ok_list) if ok_list else 0.0)\n\n    return score_deliv + score_extra"}, {"type": "code", "name": "No Two Vendors Same Day per Unit", "description": "Within a unit, no two vendors on the same date. Multiple rows by the same vendor are allowed, and overlapping with our staff is not prohibited by the rule. Partial credit by fraction of compliant dates.", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id) if output.ext.lower()=='.pdf' else context.files.read_docx_text(output.id)\n    except Exception:\n        return 0.0\n\n    def extract_block(t, name):\n        m = re.search(rf\"<<<BEGIN {name}>>>\\n(.*?)\\n<<<END {name}>>>\", t, flags=re.DOTALL)\n        return m.group(1) if m else None\n    def parse_simple_csv(block):\n        if not block:\n            return None\n        lines = [ln.strip() for ln in block.strip().splitlines() if ln.strip()]\n        if not lines:\n            return None\n        headers = [h.strip() for h in lines[0].split(',')]\n        rows = []\n        for ln in lines[1:]:\n            parts = [p.strip() for p in ln.split(',')]\n            if len(parts) != len(headers):\n                continue\n            rows.append(parts)\n        try:\n            return pd.DataFrame(rows, columns=headers)\n        except Exception:\n            return None\n\n    unit_block = extract_block(text, 'Unit_Schedule_CSV')\n    unit_df = parse_simple_csv(unit_block)\n    if unit_df is None:\n        return 0.0\n\n    df = unit_df.copy()\n    needed = {'Apartment','Provider','Date'}\n    if not needed.issubset(df.columns):\n        return 0.0\n    df['Date_dt'] = pd.to_datetime(df['Date'], format='%Y-%m-%d', errors='coerce')\n    df['prov_l'] = df['Provider'].astype(str).str.strip().str.lower()\n\n    # Identify vendor rows (not our staff)\n    vendors = df[(df['prov_l'] != 'our staff') & df['Date_dt'].notna()].copy()\n    if len(vendors) == 0:\n        return 0.5  # no possible violations\n\n    # For each (Apartment, Date), check number of distinct providers\n    grp = vendors.groupby([vendors['Apartment'], vendors['Date_dt'].dt.normalize()])['prov_l'].nunique()\n    # Compliant if <= 1\n    compliant = (grp <= 1).mean() if len(grp)>0 else 0.0\n    return 0.5 * float(compliant)"}, {"type": "code", "name": "Cross-Section Consistency (Vendor \u2194 Unit)", "description": "Every vendor-scheduled apartment/date in the Vendor CSV should appear in the Unit CSV with the same vendor and date. Partial credit by match rate.", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id) if output.ext.lower()=='.pdf' else context.files.read_docx_text(output.id)\n    except Exception:\n        return 0.0\n\n    def extract_block(t, name):\n        m = re.search(rf\"<<<BEGIN {name}>>>\\n(.*?)\\n<<<END {name}>>>\", t, flags=re.DOTALL)\n        return m.group(1) if m else None\n    def parse_simple_csv(block):\n        if not block:\n            return None\n        lines = [ln.strip() for ln in block.strip().splitlines() if ln.strip()]\n        if not lines:\n            return None\n        headers = [h.strip() for h in lines[0].split(',')]\n        rows = []\n        for ln in lines[1:]:\n            parts = [p.strip() for p in ln.split(',')]\n            if len(parts) != len(headers):\n                continue\n            rows.append(parts)\n        try:\n            return pd.DataFrame(rows, columns=headers)\n        except Exception:\n            return None\n\n    vendor_block = extract_block(text, 'Vendor_Schedule_CSV')\n    unit_block = extract_block(text, 'Unit_Schedule_CSV')\n    vend_df = parse_simple_csv(vendor_block)\n    unit_df = parse_simple_csv(unit_block)\n    if vend_df is None or unit_df is None:\n        return 0.0\n\n    # Normalize\n    vend = vend_df.copy()\n    unit = unit_df.copy()\n    for c in ['Vendor','Apartment','Scheduled_Date']:\n        if c not in vend.columns:\n            return 0.0\n    for c in ['Provider','Apartment','Date']:\n        if c not in unit.columns:\n            return 0.0\n\n    vend['sched_dt'] = pd.to_datetime(vend['Scheduled_Date'], format='%Y-%m-%d', errors='coerce')\n    unit['date_dt'] = pd.to_datetime(unit['Date'], format='%Y-%m-%d', errors='coerce')\n    vend['vendor_l'] = vend['Vendor'].astype(str).str.strip().str.lower()\n    unit['prov_l'] = unit['Provider'].astype(str).str.strip().str.lower()\n\n    if len(vend)==0:\n        return 0.5\n\n    # Build set of tuples from unit: (apt, provider, date)\n    unit_keys = set(zip(unit['Apartment'].astype(str), unit['prov_l'], unit['date_dt'].dt.normalize()))\n\n    matches = 0\n    total = 0\n    for _, r in vend.iterrows():\n        apt = str(r['Apartment'])\n        v = r['vendor_l']\n        d = r['sched_dt']\n        if pd.isna(d):\n            total += 1\n            continue\n        total += 1\n        if (apt, v, pd.Timestamp(d.date())) in unit_keys:\n            matches += 1\n    if total == 0:\n        return 0.5\n    return 0.5 * (matches/total)"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Managerial Usefulness", "description": "Assess presentation, clarity, and usefulness for a manager planning turn timelines. Do not re-check structure or strict correctness; judge overall professional quality.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity, Professionalism, and Actionability", "description": "Evaluate whether the report is professionally formatted, easy to read, and actionable for a property manager. Look for clear schedules, readable tables, concise notes, and realistic sequencing rationale.", "weight": 2.0, "judge_prompt": "Evaluate the overall quality of the PDF report for a manager who needs to plan turn timelines. Do not re-check strict structure or correctness already handled in earlier stages. Focus on clarity, professionalism, and usefulness.\n\nConsider:\n- Clear, descriptive title and date context (e.g., references 6/30/25 move-outs)\n- Readable, well-labeled tables with consistent apartment and vendor names\n- Concise notes explaining sequencing choices and any make-ready date changes\n- Logical flow between vendor plan and unit schedule\n- Overall presentation quality (typography, spacing, headings) and managerial actionability\n\nScoring:\n- 2.0: Highly professional, very clear and actionable; easy to brief a manager\n- 1.2: Generally clear with minor formatting/clarity issues\n- 0.6: Understandable but lacks polish or some key labeling/notes\n- 0.0: Hard to follow or unprofessional formatting\n", "expectation": "A clean, manager-ready PDF with clear tables and notes that make the turn plan easy to execute."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "4c18ebae-dfaa-4b76-b10c-61fcdf26734c", "rubric": {"category_name": "SAR Preparation and Transaction Support (Government \u2022 Compliance Officers)", "rationale": "This rubric enforces a self-documenting, verifiable workflow for producing a FinCEN-aligned Suspicious Activity Report (SAR) and a supporting transactions workbook. Stage 1 (LLM-only) mandates exact structural outputs (DOCX/PDF SAR \u22644 pages and an Excel workbook with specific sheets/columns). Stage 2 (code + LLM) verifies correctness via deterministic checks (columns present, suspect names, structuring and interstate indicators, narrative cross-references). Stage 3 (LLM) evaluates professional quality and managerial readiness. Without the Stage 1 structure, verification is impossible, so the category is zeroed.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (LLM only)", "description": "MANDATORY structure check. Output must include BOTH: (1) a FinCEN-style SAR narrative as a DOCX/PDF no longer than 4 pages, and (2) a transactions workbook in Excel with the specified sheets and column structure. Only presence and format are checked here, not correctness.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "SAR Document Structure Requirement (DOCX/PDF)", "description": "Verify the SAR narrative is a DOCX or PDF, \u22644 pages, and includes all required sections per FinCEN guidance.", "weight": 2.0, "judge_prompt": "You are validating ONLY the STRUCTURE of the candidate outputs for a Suspicious Activity Report (SAR). Do not judge quality or correctness here, only presence/format/sections. Confirm ALL of the following for the SAR document:\n\nFormat requirements:\n- File is a PDF or DOCX (not Excel/CSV/plain text).\n- Length is no more than 4 pages.\n- Professional report formatting with clear section headers.\n\nRequired SAR sections (flexible on exact titles if clearly equivalent):\n1) Executive Summary or Overview (first page, concise case synopsis)\n2) Subjects and Entities (list suspects: Bluehaven Collective LLC, Owen Tavery, Silverleaf Partners LLC, Victor Curcun; include any known addresses such as 6903 Oakridge Way, Suite 1)\n3) Financial Institution and Accounts (The Golden Apple Bank; relevant account numbers or descriptors)\n4) Narrative of Suspicious Activity \u2014 follows FinCEN guidance (who, what, when, where, why/how) and cites indicators such as human trafficking/illegal prostitution, structuring, interstate cash activity, and any open-source signals (e.g., Bedpage, AMPReviews)\n5) Timeframe of Activity (e.g., Sept 2023\u2013Aug 2024, or similar)\n6) Law Enforcement Tip/Contact and Bank\u2019s Actions (e.g., LE tip received, whether LE contacted, internal actions taken)\n7) Recommended Actions for Senior Management (e.g., file SAR, EDD, maintain/exit relationship)\n8) Attachments/References section indicating an Excel transactions workbook is provided\n\nScoring (structure only):\n- 2.0: Valid PDF/DOCX \u22644 pages and all 8 sections present with clear headers.\n- 1.6: Valid format; 1 minor supporting section missing (e.g., Attachments/References) but core narrative, subjects, timeframe, and indicators present.\n- 1.2: Valid format; up to 2 required sections missing or combined ambiguously but core FinCEN narrative and subjects are clearly present.\n- 0.6: Valid format but only 3\u20134 sections identifiable.\n- 0.0: Not PDF/DOCX OR exceeds 4 pages OR fewer than 3 identifiable sections.\n\nOnly evaluate structure and presence, not correctness of facts or analysis.", "expectation": "A 3\u20134 page professional SAR narrative (PDF/DOCX) with the listed sections, clearly labeled, referencing the attached transactions workbook."}, {"type": "llm_judge", "name": "Transactions Workbook Structure Requirement (Excel)", "description": "Verify the transactions workbook is an Excel file with the required sheets and columns enabling verification.", "weight": 2.0, "judge_prompt": "You are validating ONLY the STRUCTURE of the transactions workbook. Confirm the following:\n\nFormat requirements:\n- File is an Excel workbook (.xlsx or .xls). Not CSV/Google Sheets/PDF.\n\nRequired sheets (flexible naming allowed if clearly equivalent):\nA) \"Transactions\" or \"Detailed Transactions\"\n   Required columns (flexible header synonyms OK if clearly equivalent):\n   - Date (e.g., Transaction Date)\n   - Account Number (or Account ID)\n   - Entity/Customer/Subject (party name)\n   - Counterparty (optional but preferred)\n   - Amount\n   - Currency\n   - Transaction Type (e.g., cash deposit, withdrawal)\n   - Channel/Instrument (e.g., ATM, teller)\n   - Location City\n   - Location State\n   - Notes/Description/Memo\n   - Red Flag/Indicator Tag(s)\n   Expect at least several rows of data (e.g., 10+ rows) visible, not just headers.\n\nB) \"Entity Summary\" or \"Summary\" or \"Aggregation\"\n   - Table summarizing totals by entity and counts of key red flags (e.g., cash deposits, suspected structuring, interstate activity).\n\nC) \"Methodology & Flags\" or \"Notes & Assumptions\" (optional but preferred)\n   - Brief text explaining data sources, filters, and how red flags were tagged.\n\nScoring (structure only):\n- 2.0: Valid Excel; required sheets A and B present; Transactions sheet has all required columns and \u226510 rows; optional Methodology present.\n- 1.6: Valid Excel; required sheets A and B present; Transactions has most required columns (minor omissions) and \u22655 rows; optional sheet may be missing.\n- 1.0: Valid Excel; Transactions present but missing multiple required columns and/or Summary sheet is weak or mislabeled; still usable for verification.\n- 0.0: Not Excel OR missing the Transactions sheet OR Transactions has almost no visible data.\n\nOnly evaluate structure and presence, not data correctness.", "expectation": "An Excel workbook with a populated Transactions sheet and a Summary sheet that rolls up activity by entity and flags; an optional methodology sheet is preferred."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Code + LLM)", "description": "Deterministic checks on data presence, plausibility, and cross-references, enabled by Stage 1 structure. These rules verify that essential elements are actually present and consistent with the scenario and FinCEN typologies.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Excel Has Required Columns And Rows (Flexible Match)", "description": "Checks Transactions sheet exists (flexibly named), essential columns are present via synonym matching, and has at least a few rows.", "weight": 1.2, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    # Find a spreadsheet output\n    outputs = context.get_all_outputs()\n    xls_res = None\n    for r in outputs:\n        try:\n            if getattr(r, 'is_spreadsheet', False):\n                xls_res = r\n                break\n        except Exception:\n            pass\n    if not xls_res:\n        return 0.0, 'No spreadsheet resource found.'\n\n    # Identify Transactions sheet by fuzzy name\n    try:\n        fp = context.files.get_path(xls_res.id)\n        xfile = pd.ExcelFile(fp)\n        sheet_names = [s for s in xfile.sheet_names]\n    except Exception as e:\n        return 0.0, f'Failed to open Excel: {e}'\n\n    def pick_transactions_sheet(names):\n        cand = None\n        for n in names:\n            ln = n.lower()\n            if 'trans' in ln or 'detail' in ln:\n                cand = n\n                break\n        return cand or (names[0] if names else None)\n\n    sheet = pick_transactions_sheet(sheet_names)\n    if not sheet:\n        return 0.0, 'No sheets found in workbook.'\n\n    try:\n        df = pd.read_excel(fp, sheet_name=sheet)\n    except Exception as e:\n        return 0.0, f'Failed reading sheet {sheet}: {e}'\n\n    if df is None or df.empty:\n        return 0.0, 'Transactions sheet empty.'\n\n    # Normalize columns\n    cols = [str(c).strip().lower() for c in df.columns]\n\n    # Synonym mapping\n    synonyms = {\n        'date': ['date','transaction date','txn date','posted date'],\n        'account': ['account','account number','acct','acct #','account id'],\n        'entity': ['entity','customer','subject','name','business','party'],\n        'counterparty': ['counterparty','beneficiary','originator','payee','payer'],\n        'amount': ['amount','usd amount','amt','transaction amount'],\n        'currency': ['currency','curr'],\n        'type': ['type','transaction type','txn type','activity type'],\n        'channel': ['channel','instrument','method','via'],\n        'city': ['city','location city','branch city'],\n        'state': ['state','location state','state/province','st'],\n        'notes': ['notes','description','memo','remarks','details'],\n        'flag': ['red flag','indicator','flag','tag']\n    }\n\n    def has_any(syns):\n        return any(any(s in c for s in syns) for c in cols)\n\n    essential_keys = ['date','amount','type','entity','state']\n    optional_keys = ['account','city','notes','flag','currency','channel','counterparty']\n\n    essential_hit = sum(1 for k in essential_keys if has_any(synonyms[k]))\n    optional_hit = sum(1 for k in optional_keys if has_any(synonyms[k]))\n\n    # Base score from essential coverage\n    essential_ratio = essential_hit / max(1, len(essential_keys))\n\n    # Row count factor\n    row_factor = 1.0\n    try:\n        nrows = len(df)\n        if nrows >= 10:\n            row_factor = 1.0\n        elif nrows >= 5:\n            row_factor = 0.8\n        elif nrows >= 1:\n            row_factor = 0.5\n        else:\n            row_factor = 0.0\n    except Exception:\n        row_factor = 0.6\n\n    # Bonus from optional columns (up to +20%)\n    optional_ratio = optional_hit / max(1, len(optional_keys))\n    bonus = 0.2 * optional_ratio\n\n    score = max(0.0, min(1.0, (essential_ratio * row_factor) + bonus))\n    return score, f'Essential columns hit: {essential_hit}/{len(essential_keys)}, optional hit: {optional_hit}/{len(optional_keys)}, rows: {len(df)}.'"}, {"type": "code", "name": "Suspects Present In Data", "description": "Verifies that suspect names/entities appear in the Transactions data (Entity/Notes columns).", "weight": 0.6, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    xls_res = None\n    for r in outputs:\n        if getattr(r, 'is_spreadsheet', False):\n            xls_res = r\n            break\n    if not xls_res:\n        return 0.0, 'No spreadsheet found.'\n\n    fp = context.files.get_path(xls_res.id)\n    try:\n        xfile = pd.ExcelFile(fp)\n        sheet = None\n        for s in xfile.sheet_names:\n            if 'trans' in s.lower() or 'detail' in s.lower():\n                sheet = s\n                break\n        sheet = sheet or xfile.sheet_names[0]\n        df = pd.read_excel(fp, sheet_name=sheet)\n    except Exception as e:\n        return 0.0, f'Cannot read transactions sheet: {e}'\n\n    if df is None or df.empty:\n        return 0.0, 'Empty transactions.'\n\n    df_cols = {str(c).strip().lower(): c for c in df.columns}\n    def find_col(keys):\n        for k in list(df_cols.keys()):\n            for key in keys:\n                if key in k:\n                    return df_cols[k]\n        return None\n\n    entity_col = find_col(['entity','customer','subject','name','business','party'])\n    notes_col = find_col(['notes','description','memo','remarks','details'])\n\n    suspects = [\n        'bluehaven collective',\n        'owen tavery',\n        'silverleaf partners',\n        'victor curcun',\n        'serenya spa'\n    ]\n\n    text_series = []\n    if entity_col is not None:\n        text_series.append(df[entity_col].astype(str))\n    if notes_col is not None:\n        text_series.append(df[notes_col].astype(str))\n\n    if not text_series:\n        return 0.0, 'No entity/notes columns to search.'\n\n    combined = ('\\n'.join(s.fillna('') for s in text_series)).lower()\n\n    found = 0\n    found_list = []\n    for s in suspects:\n        if s in combined:\n            found += 1\n            found_list.append(s)\n\n    # Score: 1.0 if 4+ suspects found, 0.7 if 3, 0.5 if 2, 0.3 if 1, else 0\n    mapping = {4:1.0, 3:0.7, 2:0.5, 1:0.3, 0:0.0}\n    score = mapping.get(min(found,4), 0.0)\n    return score, f'Suspects found: {found_list}'"}, {"type": "code", "name": "Structuring Pattern Detected (Cash Deposits < $10k)", "description": "Detects clusters of cash deposits just under $10,000 suggesting structuring.", "weight": 0.8, "code": "import re\nimport pandas as pd\nimport numpy as np\nfrom datetime import timedelta\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    xls_res = None\n    for r in outputs:\n        if getattr(r, 'is_spreadsheet', False):\n            xls_res = r\n            break\n    if not xls_res:\n        return 0.0, 'No spreadsheet found.'\n\n    fp = context.files.get_path(xls_res.id)\n    try:\n        xfile = pd.ExcelFile(fp)\n        sheet = None\n        for s in xfile.sheet_names:\n            if 'trans' in s.lower() or 'detail' in s.lower():\n                sheet = s\n                break\n        sheet = sheet or xfile.sheet_names[0]\n        df = pd.read_excel(fp, sheet_name=sheet)\n    except Exception as e:\n        return 0.0, f'Cannot read transactions: {e}'\n\n    if df is None or df.empty:\n        return 0.0, 'Empty transactions.'\n\n    # Identify columns\n    cols = {str(c).strip().lower(): c for c in df.columns}\n\n    def col_like(options):\n        for k,v in cols.items():\n            if any(opt in k for opt in options):\n                return v\n        return None\n\n    date_col = col_like(['date'])\n    amt_col = col_like(['amount','amt'])\n    type_col = col_like(['type'])\n    notes_col = col_like(['notes','description','memo','details'])\n    acct_col = col_like(['account'])\n\n    if amt_col is None or date_col is None:\n        return 0.0, 'Missing date/amount columns.'\n\n    # Coerce types\n    df[amt_col] = pd.to_numeric(df[amt_col], errors='coerce')\n    df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n\n    # Cash deposit filter using type/notes heuristics\n    def is_cash_deposit(row):\n        t = str(row.get(type_col, '')) if type_col else ''\n        n = str(row.get(notes_col, '')) if notes_col else ''\n        text = (t + ' ' + n).lower()\n        if 'cash' in text and 'withdraw' not in text:\n            if 'deposit' in text or 'dep' in text or 'credit' in text:\n                return True\n        return False\n\n    df['__cash_dep__'] = df.apply(is_cash_deposit, axis=1)\n    df['__under10__'] = (df[amt_col] >= 8000) & (df[amt_col] < 10000)\n\n    cand = df[df['__cash_dep__'] & df['__under10__']].copy()\n\n    if cand.empty:\n        return 0.0, 'No sub-$10k cash deposits detected.'\n\n    # Group by account and look for clusters within a 14-day window\n    if date_col not in cand.columns:\n        return 0.0, 'No dates for candidate transactions.'\n\n    cand.sort_values(by=[c for c in [acct_col, date_col] if c], inplace=True)\n\n    cluster_count = 0\n    for acct, grp in cand.groupby(acct_col) if acct_col else [(None, cand)]:\n        dates = grp[date_col].dropna().sort_values().tolist()\n        # sliding window: count how many within any 14-day span\n        i = 0\n        while i < len(dates):\n            j = i\n            while j < len(dates) and (dates[j] - dates[i]).days <= 14:\n                j += 1\n            window_size = j - i\n            if window_size >= 3:\n                cluster_count += 1\n            i += 1\n\n    # Scoring by strength of evidence\n    if cluster_count >= 2:\n        score = 1.0\n    elif cluster_count == 1:\n        score = 0.7\n    else:\n        # If no clusters but at least 2 such deposits overall, partial credit\n        score = 0.4 if len(cand) >= 2 else 0.2\n\n    return score, f'Under-$10k cash deposit clusters (14-day windows): {cluster_count}; total candidates: {len(cand)}.'"}, {"type": "code", "name": "Interstate Activity Detected (I-95 corridor)", "description": "Checks for activity across multiple states consistent with interstate movement (I-95 corridor emphasis).", "weight": 0.6, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    xls_res = None\n    for r in outputs:\n        if getattr(r, 'is_spreadsheet', False):\n            xls_res = r\n            break\n    if not xls_res:\n        return 0.0, 'No spreadsheet found.'\n\n    fp = context.files.get_path(xls_res.id)\n    try:\n        xfile = pd.ExcelFile(fp)\n        sheet = None\n        for s in xfile.sheet_names:\n            if 'trans' in s.lower() or 'detail' in s.lower():\n                sheet = s\n                break\n        sheet = sheet or xfile.sheet_names[0]\n        df = pd.read_excel(fp, sheet_name=sheet)\n    except Exception as e:\n        return 0.0, f'Cannot read transactions: {e}'\n\n    if df is None or df.empty:\n        return 0.0, 'Empty transactions.'\n\n    cols = {str(c).strip().lower(): c for c in df.columns}\n    def find_col(keys):\n        for k,v in cols.items():\n            for key in keys:\n                if key in k:\n                    return v\n        return None\n\n    state_col = find_col(['state','location state','st'])\n    if state_col is None:\n        return 0.0, 'No state column.'\n\n    states = (\n        df[state_col].astype(str).str.upper().str.replace(r'[^A-Z]', '', regex=True)\n    )\n\n    unique_states = set(s for s in states if s and s != 'NAN')\n\n    i95_states = set(['FL','GA','SC','NC','VA','DC','MD','DE','NJ','PA','NY','CT','RI','MA','NH','ME'])\n    i95_present = unique_states & i95_states\n\n    # Scoring logic\n    if len(i95_present) >= 2:\n        score = 1.0\n    elif 'FL' in unique_states and len(unique_states) >= 2:\n        score = 0.6\n    elif 'FL' in unique_states:\n        score = 0.3\n    else:\n        score = 0.0\n\n    return score, f'States detected: {sorted(unique_states)}; I-95 states: {sorted(i95_present)}.'"}, {"type": "code", "name": "Narrative Cross-References Key Facts and Red Flags", "description": "Verifies SAR document text mentions suspects, address, typologies (human trafficking, structuring), and open-source indicators (e.g., Bedpage, AMPReviews).", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    # Find document (PDF/DOCX)\n    outputs = context.get_all_outputs()\n    doc = None\n    for r in outputs:\n        if getattr(r, 'is_document', False):\n            doc = r\n            break\n    if not doc:\n        return 0.0, 'No SAR document found.'\n\n    text = ''\n    try:\n        # Try PDF first, then DOCX\n        if str(getattr(doc, 'extension', '')).lower() in ['.pdf','pdf']:\n            text = context.files.read_pdf_text(doc.id)\n        else:\n            # Will work for DOCX; if PDF detected above fails we still try\n            try:\n                text = context.files.read_docx_text(doc.id)\n            except Exception:\n                text = context.files.read_pdf_text(doc.id)\n    except Exception:\n        # Fallback generic text read (may work for some docs)\n        try:\n            text = context.files.read_text(doc.id)\n        except Exception:\n            text = ''\n\n    if not text:\n        return 0.0, 'Unable to extract document text.'\n\n    t = text.lower()\n\n    checks = {\n        'bluehaven collective': any(s in t for s in ['bluehaven collective','blue haven collective']),\n        'owen tavery': 'owen tavery' in t,\n        'silverleaf partners': 'silverleaf partners' in t,\n        'victor curcun': 'victor curcun' in t,\n        'address 6903 oakridge way': '6903 oakridge way' in t,\n        'bedpage': 'bedpage' in t,\n        'ampreviews': 'ampreviews' in t or 'amp reviews' in t,\n        'human trafficking/prostitution': any(s in t for s in ['human trafficking','prostitution','sex trafficking','illicit sexual services']),\n        'structuring': 'structuring' in t or 'under $10,000' in t or 'under 10000' in t,\n        'interstate/i-95': 'i-95' in t or 'interstate 95' in t or 'interstate' in t,\n        'timeframe 2023/2024': any(s in t for s in ['2023','2024','september 2023','august 2024']),\n        'fincen referenced': 'fincen' in t or 'sar' in t,\n        'excel attachment referenced': any(s in t for s in ['see attached transactions','attached excel','transactions workbook','see attached records'])\n    }\n\n    hits = sum(1 for k,v in checks.items() if v)\n    total = len(checks)\n\n    # Score proportional with floors to reflect key elements\n    key_items = ['bluehaven collective','silverleaf partners','human trafficking/prostitution','structuring','timeframe 2023/2024']\n    key_hits = sum(1 for k in key_items if checks.get(k, False))\n\n    base = hits / total\n    # Ensure minimum thresholds if key items satisfied\n    if key_hits >= 4:\n        base = max(base, 0.8)\n    elif key_hits >= 3:\n        base = max(base, 0.6)\n\n    score = max(0.0, min(1.0, base))\n    return score, f'Cross-reference hits: {hits}/{total}; key hits: {key_hits}/{len(key_items)}.'"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism (LLM)", "description": "Holistic quality assessment for senior management readiness and analytical clarity. This stage focuses on writing quality, clarity, usefulness, and professional presentation.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "SAR Narrative Quality (FinCEN-aligned, concise, decision-useful)", "description": "Assesses clarity, concision (\u22644 pages), FinCEN-aligned who/what/when/where/why/how narrative, explicit red flags, recommended actions, and suitability for senior management.", "weight": 1.0, "judge_prompt": "Assess the SAR document\u2019s professional quality (not structure). Consider:\n- Clarity and concision (\u22644 pages, minimal fluff, precise language)\n- FinCEN-aligned narrative: clearly addresses who/what/when/where/why/how\n- Explicit articulation of red flags (human trafficking typologies, structuring, interstate cash activity) and how they were identified\n- Clear timeframe, subjects, and linkage to accounts/institution\n- Decision-useful recommendations for senior management (e.g., file SAR, file timing, ongoing monitoring, EDD, relationship disposition)\n- Professional tone, organized layout, and absence of unsubstantiated conclusions\n\nScoring:\n- 1.0: Highly clear, concise, FinCEN-aligned, and decision-useful; recommendations are specific and actionable.\n- 0.7: Generally clear and useful with minor gaps; recommendations present but could be sharper.\n- 0.4: Mixed clarity or missing some narrative elements; recommendations vague.\n- 0.2: Poorly organized or overly speculative; limited usefulness.\n- 0.0: Not decision-useful or severely unclear.", "expectation": "A crisp, FinCEN-style narrative that enables senior management to approve filing and next steps confidently."}, {"type": "llm_judge", "name": "Workbook Usability and Traceability", "description": "Assesses whether the Excel workbook is professional, readable, and supports traceability from summary to detail and flags.", "weight": 1.0, "judge_prompt": "Evaluate the Excel workbook\u2019s professional usability (not just presence):\n- Clear, consistent headers; appropriate data types for dates/amounts\n- Readable formatting (filters, number formats, frozen headers)\n- Summary sheet clearly ties to Transactions (e.g., totals by entity, counts of cash deposits/structuring/interstate hits) and is easy to audit back to detail\n- Red flag/indicator tags are intelligible and used consistently; brief methodology or notes present (if included)\n\nScoring:\n- 1.0: Highly usable and auditable; summary and flags clearly map to detail; formatting aids review.\n- 0.7: Generally usable with minor formatting or traceability gaps.\n- 0.4: Usable but hard to audit; inconsistent tags or weak summary.\n- 0.2: Minimal formatting, poor readability.\n- 0.0: Not usable or lacks coherent structure.", "expectation": "A clean, auditable workbook where reviewers can quickly trace summaries and flags back to underlying transactions."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "664a42e5-3240-413a-9a57-ea93c6303269", "rubric": {"category_name": "Finance and Insurance \u2014 Personal Financial Advisors \u2014 ILIT Client Presentation", "rationale": "This rubric enforces a self-documenting, verifiable presentation. Stage 1 (LLM-only) mandates a precise slide-deck structure in a PDF/DOCX format so downstream checks are possible. Stage 2 mixes code and LLM rules to verify factual and structural correctness enabled by Stage 1\u2019s shape. Stage 3 evaluates professional quality and client-appropriateness for HNW clients ($5\u201310M).", "max_total_score": 25.0, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate", "description": "LLM-only gate ensuring the output is a slide-style document (PDF/DOCX) with all required sections for an ILIT implementation presentation.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.0, "rules": [{"type": "llm_judge", "name": "Structured Slide Deck Requirement (PDF/DOCX)", "description": "Verify the candidate output is a properly structured slide-style document with all required ILIT content sections present.", "weight": 6.0, "judge_prompt": "You are evaluating whether the submitted output is a properly structured slide-style client presentation suitable for a meeting. Only check FORMAT and PRESENCE of required elements, not correctness of the technical content.\n\nAcceptable formats: PDF or DOCX only (exported slide deck preferred). Reject plain text, spreadsheets, images only, or other formats.\n\nMinimum structure requirements:\n- Slide-style document with visible slide titles (not a single block of paragraphs)\n- 8\u201320 slides recommended. If fewer than 8, score very low. If more than 20 but otherwise complete, allow minor deduction.\n\nRequired slides/sections (flexible naming, but all concepts must be present):\n1) Title Slide: includes \u201cIrrevocable Life Insurance Trust\u201d or \u201cILIT\"\n2) Agenda/Overview\n3) ILIT Basics & Parties: explains ILIT purpose and identifies parties (Grantor/Settlor, Trustee, Beneficiaries, Insured)\n4) Step-by-Step Implementation Process: clearly numbered or staged steps from engage counsel \u2192 draft trust \u2192 obtain TIN/bank \u2192 apply for policy or transfer \u2192 funding mechanics \u2192 Crummey notices \u2192 premium payment \u2192 ongoing admin\n5) Funding & Gift Mechanics: gifts to trust, annual gift tax exclusion, how premiums are paid by the ILIT\n6) Crummey Powers: purpose, notices, temporary withdrawal right, lapse/administration\n7) Policy Types in ILIT: single-life vs survivorship (second-to-die) and typical product types (e.g., term, whole life, UL)\n8) Death Benefit & Distribution Flow: proceeds estate-exclusion concept if properly structured; trust distribution and liquidity for estate/beneficiaries\n9) Crummey Timeline (2025): a time-cycle for notices, contribution, withdrawal window, premium payment; must explicitly reference 2025 annual exclusion amount\n10) Side-by-Side Comparison: including vs excluding an ILIT (table or clearly delineated two-column comparison with rows like estate tax exposure, liquidity, control, complexity, costs/admin, flexibility)\n11) Key Considerations/Risks: trustee duties, administration, coordination with attorney/CPA, potential downsides\n12) Disclosures/Next Steps (OPTIONAL but recommended): not legal/tax advice, coordinate with counsel/CPA; action plan for client\n\nScoring guidance (STRUCTURE ONLY):\n- 6.0: Valid PDF/DOCX slide-style; \u226510 slides; all 10 core content sections (1\u201310) present; \u201cKey Considerations/Risks\u201d present; Crummey timeline references 2025\n- 5.0: Valid PDF/DOCX; \u22658 slides; missing only 1 core content section OR timeline present without a visual but text describes 2025 cycle\n- 4.0: Valid PDF/DOCX; \u22658 slides; has 7/10 core content sections; implementation steps or comparison present but one of timeline or policy types missing\n- 2.0: Valid PDF/DOCX but only 5\u20136 core sections, or <8 slides\n- 0.0: Not PDF/DOCX, not slide-style, or missing the majority of core sections\n\nBe flexible with exact slide titles, but strict about the presence of the listed concepts. Do not assess technical correctness here\u2014only structure and presence.", "expectation": "A PDF/DOCX slide deck with the specified sections, including a 2025 Crummey timeline and a side-by-side ILIT vs no-ILIT comparison."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Compliance Verification", "description": "Verify key technical elements are present and plausible using code checks and an LLM logic check enabled by Stage 1\u2019s shape.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "2025 Annual Gift Exclusion Mention and Amount", "description": "Checks the document text for an explicit 2025 annual gift tax exclusion amount near the year reference. Full credit if 2025 is mentioned near $19,000 (or 19k). Partial if 2025 is paired with $18,000 or if $19,000 is mentioned without 2025.", "weight": 2.0, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float in [0,1] (engine scales by rule weight)\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n\n    t = text.lower()\n\n    def has_amount_near_year(year_pat, amount_pats, window=80):\n        positions_year = [m.start() for m in re.finditer(year_pat, t)]\n        if not positions_year:\n            return False\n        for ap in amount_pats:\n            for m in re.finditer(ap, t):\n                pos = m.start()\n                for y in positions_year:\n                    if abs(pos - y) <= window:\n                        return True\n        return False\n\n    year_2025 = r\"2025\"\n    amt_19k = [r\"\\$?19[, ]?000\\b\", r\"\\b19k\\b\", r\"\\$19k\\b\"]\n    amt_18k = [r\"\\$?18[, ]?000\\b\", r\"\\b18k\\b\", r\"\\$18k\\b\"]\n\n    near_19k_2025 = has_amount_near_year(year_2025, amt_19k)\n    near_18k_2025 = has_amount_near_year(year_2025, amt_18k)\n\n    mentions_19k = bool(re.search(r\"\\$?19[, ]?000\\b|\\b19k\\b\", t))\n    mentions_annual_excl = (\"annual\" in t and \"exclusion\" in t and \"gift\" in t)\n\n    # Scoring\n    if near_19k_2025:\n        return 1.0\n    if near_18k_2025:\n        return 0.6\n    if mentions_19k and mentions_annual_excl:\n        return 0.7\n    return 0.0\n"}, {"type": "code", "name": "Parties of the Trust Identified", "description": "Checks for presence of key parties: Grantor/Settlor, Trustee, Beneficiaries, and Insured.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n    t = text.lower()\n\n    has_grantor = (\"grantor\" in t) or (\"settlor\" in t)\n    has_trustee = \"trustee\" in t\n    has_benef = \"beneficiar\" in t  # beneficiaries/beneficiary\n    has_insured = \"insured\" in t\n\n    count = sum([has_grantor, has_trustee, has_benef, has_insured])\n    return min(1.0, count / 4.0)\n"}, {"type": "code", "name": "Policy Types Mentioned (Single/Survivorship; Product Types)", "description": "Confirms mention of survivorship/second-to-die and at least one common product type (term/whole/UL).", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n\n    t = text.lower()\n    has_survivorship = (\"survivorship\" in t) or (\"second-to-die\" in t) or (\"second to die\" in t)\n    has_product = any(k in t for k in [\"term life\", \"whole life\", \"universal life\", \"iul\", \"ul \", \" wl \", \" variable universal \"])\n\n    score = 0.0\n    if has_survivorship:\n        score += 0.5\n    if has_product:\n        score += 0.5\n    return score\n"}, {"type": "code", "name": "Crummey Mechanics (Notices and Window)", "description": "Checks for Crummey powers, notice/withdrawal right, and a plausible notice window (e.g., 30\u201360 days).", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n\n    t = text.lower()\n    has_crummey = \"crummey\" in t\n    has_notice = (\"notice\" in t) or (\"withdrawal right\" in t) or (\"right to withdraw\" in t)\n    window_match = re.search(r\"\\b(2[0-9]|3[0-9]|4[0-9]|5[0-9]|6[0-9])\\b\\s*(day|days)\", t)\n\n    if not has_crummey:\n        return 0.0\n    score = 0.4\n    if has_notice:\n        score += 0.3\n    if window_match:\n        score += 0.3\n    return min(score, 1.0)\n"}, {"type": "code", "name": "Funding and Premium Payment Mechanics", "description": "Verifies mentions of gifts to trust, use of annual exclusion, and that ILIT pays premiums with contributed funds; ownership/incidents of ownership separation signal.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n\n    t = text.lower()\n    has_gift = (\"gift\" in t) or (\"contribution\" in t) or (\"fund\" in t)\n    has_excl = (\"annual\" in t and \"exclusion\" in t)\n    has_premium = \"premium\" in t\n    has_ownership_sep = (\"trust owns\" in t) or (\"owner is the trust\" in t) or (\"incidents of ownership\" in t) or (\"policy owner: trust\" in t)\n\n    score = 0.0\n    if has_gift:\n        score += 0.3\n    if has_excl:\n        score += 0.3\n    if has_premium:\n        score += 0.2\n    if has_ownership_sep:\n        score += 0.2\n    return min(1.0, score)\n"}, {"type": "code", "name": "Distribution & Estate-Tax Treatment", "description": "Checks for the estate-exclusion concept (if properly structured), death benefit distribution by the trust, and tax characterization cues (e.g., income tax-free under IRC 101(a)).", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n\n    t = text.lower()\n    estate_excl = any(phrase in t for phrase in [\n        \"outside of the taxable estate\", \"excluded from the estate\", \"not included in the grantor's estate\", \"avoid estate inclusion\", \"no incidents of ownership\"\n    ])\n    distribution = (\"trust distributes\" in t) or (\"distribution to beneficiaries\" in t) or (\"liquidity for estate expenses\" in t) or (\"pay estate taxes\" in t) or (\"beneficiaries receive\" in t)\n    income_tax_free = (\"101(a)\" in t) or (\"income tax-free\" in t) or (\"generally income-tax free\" in t)\n\n    score = 0.0\n    if estate_excl:\n        score += 0.4\n    if distribution:\n        score += 0.4\n    if income_tax_free:\n        score += 0.2\n    return min(1.0, score)\n"}, {"type": "code", "name": "Advanced Tax Considerations (3-year rule / GST)", "description": "Awards partial credit for mentioning IRC \u00a72035 three-year rule on policy transfers and/or GST tax considerations.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n\n    t = text.lower()\n    three_year = (\"2035\" in t) or (\"3-year\" in t) or (\"three-year\" in t)\n    gst = (\"generation-skipping\" in t) or (\"gst tax\" in t) or (\"skip person\" in t)\n\n    score = 0\n    if three_year:\n        score += 1\n    if gst:\n        score += 1\n    return min(1.0, score / 2.0)\n"}, {"type": "llm_judge", "name": "Logical Mechanics and Comparison Accuracy", "description": "LLM verifies that the process, Crummey cycle, and ILIT vs. no-ILIT comparison are logically accurate and consistent for a $5\u201310M client.", "weight": 4.0, "judge_prompt": "Evaluate the document\u2019s technical correctness and internal consistency (not formatting). Consider:\n- Step-by-step implementation logic (engage counsel, draft ILIT, obtain TIN/bank, apply or transfer policy, fund trust, send Crummey notices, pay premiums, maintain records)\n- Crummey mechanics coherence (what is the right, notice timing, lapse, application to premium payments)\n- Estate-tax treatment (proceeds excluded from insured\u2019s estate if structured properly; no incidents of ownership by insured)\n- ILIT funding accuracy (gifts, annual exclusion, alignment with premium amounts)\n- ILIT vs. No-ILIT comparison accuracy (estate tax exposure, liquidity, control, complexity, costs/admin)\n- Suitability for a $5\u201310M net worth client context\n\nScoring guidance:\n- 4.0: Accurate, coherent throughout; no material errors or contradictions; comparison reflects real trade-offs\n- 2.5\u20133.5: Generally accurate; minor omissions/ambiguities; no major errors\n- 1.0\u20132.0: Some inaccuracies or unclear mechanics but partially correct overall\n- 0.0: Material inaccuracies or contradictions (e.g., suggests insured retains ownership; misstates Crummey purpose)\n\nDo not judge visual design. Focus on correctness and logical alignment.", "expectation": "Clear, correct mechanics for ILIT implementation and realistic comparison tailored to HNW clients."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Client Fit", "description": "Assess overall professionalism, clarity, and suitability for HNW client presentation; ensure actionable next steps and compliance tone.", "is_required": false, "max_points": 7.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Presentation Design and Readability", "description": "Evaluate clarity of slide titles, scannability, use of visuals (timeline/flow/chart/table), and slide economy (no wall-of-text).", "weight": 3.5, "judge_prompt": "Evaluate presentation design quality:\n- Clear slide titles and logical flow\n- Readable bullets; avoids dense walls of text\n- Visual aids (timeline/flowchart for Crummey cycle; side-by-side table for comparison)\n- Appropriate length (8\u201320 slides) and pacing\nScoring: 3.5 = excellent clarity with helpful visuals; 2.5\u20133.0 = good with minor issues; 1.0\u20132.0 = mixed readability; 0.0\u20130.5 = poor/overly dense.", "expectation": "A concise, visually aided deck that a client can follow easily."}, {"type": "llm_judge", "name": "Client Appropriateness, Actionability, and Compliance Tone", "description": "Evaluate whether content matches $5\u201310M HNW clients, offers concrete next steps, and includes compliance-minded framing (coordinate with attorney/CPA; not legal/tax advice).", "weight": 3.5, "judge_prompt": "Assess:\n- Targeting for $5\u201310M clients (estate liquidity, estate-tax exposure, trustee/admin burden)\n- Actionable next steps (what the client should do before/after the meeting)\n- Compliance tone (not legal/tax advice; coordinate with estate attorney and CPA)\nScoring: 3.5 = well-targeted, actionable, and compliant; 2.5\u20133.0 = generally solid; 1.0\u20132.0 = partially appropriate; 0.0\u20130.5 = not targeted or lacks compliance framing.", "expectation": "Action-oriented, HNW-appropriate, with clear coordination and disclosure language."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "a74ead3b-f67d-4b1c-9116-f6bb81b29d4f", "rubric": {"category_name": "Government \u2014 Child, Family, and School Social Workers: Session 13 & 14 Slide Decks (Nurturing Parenting Program)", "rationale": "Pattern B (Document). Deliverable is two visual slide presentations for Sessions 13 and 14, best evaluated as PDF slide exports for reliable reading. Stage 1 uses an LLM gate to enforce precise structural shape (two session decks, required slides). Stage 2 uses code to verify presence of both sessions, section markers, contextual alignment with substance use recovery/reunification, and adequate length. Stage 3 uses LLM to assess clarity, trauma-informed tone, and visual quality. The rubric is self-documenting by requiring explicit, checkable slide structures and session labeling.", "max_total_score": 14.0, "stages": [{"name": "Stage 1 \u2014 Presentation Structure Gate (LLM-Only)", "description": "Gate: Verify the candidate provided two complete session slide decks (preferably exported as PDFs) with the required sections for Session 13 and Session 14.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Slide Decks for Sessions 13 and 14", "description": "Check that the output consists of two visually engaging presentation files (preferably PDFs) for Session 13 and Session 14 of the Nurturing Parenting Program for Families in Substance Abuse Treatment and Recovery. Each deck must include the required sections and basic structure so later verification is possible.", "weight": 4.0, "judge_prompt": "You are evaluating whether the candidate delivered TWO complete, presentation-style slide decks for Session 13 and Session 14 of the Nurturing Parenting Program for Families in Substance Abuse Treatment and Recovery.\n\nFORMAT REQUIREMENTS (Gate):\n- Preferred format: Two separate PDFs, one for Session 13 and one for Session 14. Acceptable alternatives: a single PDF clearly split into two labeled sections (Session 13 and Session 14), or two PPTX files exported/attached. If only one combined document is provided, it MUST clearly contain both sessions as distinct sections.\n- Each session deck should look like slides (titles + bullet points) and be professionally formatted. Aim for at least ~6 slides per session (flexible), with clear section headers.\n- Neutral, non-stigmatizing imagery: at least 1\u20132 images or icons per session (flexible on exact count if overall visual design is evident).\n\nREQUIRED SECTIONS PER SESSION (must be visibly labeled or obviously present):\n1) Title slide including the session number (e.g., \u201cSession 13 \u2026\u201d or \u201cSession Fourteen \u2026\u201d).\n2) Icebreaker / Warm-up / Check-in slide.\n3) Key Session Points (may be multiple slides; bullet points acceptable).\n4) Wrap-up / Closing / Next steps / Homework slide.\n\nSCORING (STRUCTURE ONLY \u2014 do not judge content quality or correctness):\n- 4.0: Two distinct session decks present (or one combined PDF with two clearly labeled session sections), each with all four required sections. Visual slide formatting is evident. Imagery/icons present across the decks.\n- 3.0: Both sessions present but missing one minor section across either session OR imagery is minimal/unclear while slides are otherwise structured.\n- 2.0: Only one complete session deck present with required sections OR both sessions present but missing multiple required sections.\n- 1.0: A document is present but not clearly a slide deck, or sections are largely missing/unclear.\n- 0.0: Wrong format (e.g., plain text only), not slide-like, or sessions 13/14 not identifiable.\n\nOnly assess presence/format/structure, not the accuracy of content.\n", "expectation": "Two separate, slide-formatted PDFs (or clearly separated sections in one PDF) labeled Session 13 and Session 14, each containing Title, Icebreaker, Key Points, and Wrap-up sections with neutral imagery."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Code + LLM)", "description": "Verification of structural correctness and minimal contextual alignment using code checks enabled by the Stage 1 shape.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Two Distinct Sessions Present", "description": "Verify that both Session 13 and Session 14 are present across outputs. Prefer two separate document outputs; accept a single combined file that clearly contains both sessions.", "weight": 2.0, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs() or []\n        doc_candidates = []\n        for r in outputs:\n            if getattr(r, 'is_document', False) or getattr(r, 'is_text_format', False):\n                doc_candidates.append(r)\n        # Fallback to primary output if nothing found\n        if not doc_candidates:\n            primary = context.get_primary_output()\n            if primary and (getattr(primary, 'is_document', False) or getattr(primary, 'is_text_format', False)):\n                doc_candidates = [primary]\n        if not doc_candidates:\n            return 0.0, \"No document-like outputs found.\"\n\n        def read_text(resource):\n            # Try PDF then DOCX then plain text\n            txt = \"\"\n            try:\n                txt = context.files.read_pdf_text(resource.id)\n            except Exception:\n                pass\n            if not txt:\n                try:\n                    txt = context.files.read_docx_text(resource.id)\n                except Exception:\n                    pass\n            if not txt and getattr(resource, 'is_text_format', False):\n                try:\n                    txt = context.files.read_text(resource.id)\n                except Exception:\n                    pass\n            return txt or \"\"\n\n        texts = []\n        for r in doc_candidates:\n            t = read_text(r)\n            if t:\n                texts.append(t)\n        corpus = \"\\n\".join(texts).lower()\n        if not corpus.strip():\n            return 0.0, \"Could not extract text from document outputs.\"\n\n        s13 = bool(re.search(r\"session\\s*13\\b|session\\s*thirteen\\b|\\bs13\\b\", corpus))\n        s14 = bool(re.search(r\"session\\s*14\\b|session\\s*fourteen\\b|\\bs14\\b\", corpus))\n\n        # Prefer two separate docs\n        two_docs = len(doc_candidates) >= 2\n\n        if s13 and s14 and two_docs:\n            score = 1.0\n            fb = \"Both sessions detected across at least two documents.\"\n        elif s13 and s14:\n            score = 0.6\n            fb = \"Both sessions detected but not clearly split across two documents.\"\n        elif s13 or s14:\n            score = 0.4\n            fb = \"Only one session detected.\"\n        else:\n            score = 0.0\n            fb = \"Neither Session 13 nor Session 14 detected.\"\n\n        return score, fb\n    except Exception as e:\n        return 0.0, f\"Execution error: {e}\""}, {"type": "code", "name": "Required Sections Present (Titles, Icebreaker, Key Points, Wrap-up)", "description": "Check for presence of basic required sections across the provided decks using flexible keyword matching.", "weight": 2.0, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs() or []\n        doc_candidates = [r for r in outputs if getattr(r, 'is_document', False) or getattr(r, 'is_text_format', False)]\n        if not doc_candidates:\n            primary = context.get_primary_output()\n            if primary and (getattr(primary, 'is_document', False) or getattr(primary, 'is_text_format', False)):\n                doc_candidates = [primary]\n        if not doc_candidates:\n            return 0.0, \"No document outputs to check sections.\"\n\n        def read_text(resource):\n            txt = \"\"\n            try:\n                txt = context.files.read_pdf_text(resource.id)\n            except Exception:\n                pass\n            if not txt:\n                try:\n                    txt = context.files.read_docx_text(resource.id)\n                except Exception:\n                    pass\n            if not txt and getattr(resource, 'is_text_format', False):\n                try:\n                    txt = context.files.read_text(resource.id)\n                except Exception:\n                    pass\n            return txt or \"\"\n\n        corpus = \"\\n\".join([read_text(r) for r in doc_candidates]).lower()\n        if not corpus.strip():\n            return 0.0, \"No extractable text.\"\n\n        # Session titles\n        has_title13 = bool(re.search(r\"session\\s*13\\b|session\\s*thirteen\\b|\\bs13\\b\", corpus))\n        has_title14 = bool(re.search(r\"session\\s*14\\b|session\\s*fourteen\\b|\\bs14\\b\", corpus))\n\n        # Icebreaker synonyms\n        ice_count = len(re.findall(r\"ice\\s*breaker|icebreaker|warm[- ]?up|opener|check[- ]?in\", corpus))\n        # Key points synonyms\n        key_count = len(re.findall(r\"key\\s*points|key\\s*takeaways|learning\\s*objectives|objectives|agenda|overview\", corpus))\n        # Wrap-up synonyms\n        wrap_count = len(re.findall(r\"wrap[- ]?up|closing|close\\b|summary|next\\s*steps|homework|practice\\s*plan\", corpus))\n\n        # Expect at least once per session (2 occurrences) for ice/key/wrap\n        def two_or_more(n):\n            return 1.0 if n >= 2 else (0.5 if n == 1 else 0.0)\n\n        components = []\n        components.append(1.0 if (has_title13 and has_title14) else (0.5 if (has_title13 or has_title14) else 0.0))\n        components.append(two_or_more(ice_count))\n        components.append(two_or_more(key_count))\n        components.append(two_or_more(wrap_count))\n\n        score = sum(components) / 4.0\n        fb = f\"Titles(13/14): {has_title13}/{has_title14}; ice:{ice_count}, key:{key_count}, wrap:{wrap_count}.\"\n        return score, fb\n    except Exception as e:\n        return 0.0, f\"Execution error: {e}\""}, {"type": "code", "name": "Program Context Alignment", "description": "Verify minimal alignment with program context: Nurturing Parenting Program; substance use and recovery; reunification/foster context; child age relevance.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs() or []\n        doc_candidates = [r for r in outputs if getattr(r, 'is_document', False) or getattr(r, 'is_text_format', False)]\n        if not doc_candidates:\n            primary = context.get_primary_output()\n            if primary and (getattr(primary, 'is_document', False) or getattr(primary, 'is_text_format', False)):\n                doc_candidates = [primary]\n        if not doc_candidates:\n            return 0.0, \"No document outputs.\"\n\n        def read_text(resource):\n            txt = \"\"\n            try:\n                txt = context.files.read_pdf_text(resource.id)\n            except Exception:\n                pass\n            if not txt:\n                try:\n                    txt = context.files.read_docx_text(resource.id)\n                except Exception:\n                    pass\n            if not txt and getattr(resource, 'is_text_format', False):\n                try:\n                    txt = context.files.read_text(resource.id)\n                except Exception:\n                    pass\n            return txt or \"\"\n\n        corpus = \"\\n\".join([read_text(r) for r in doc_candidates]).lower()\n        if not corpus.strip():\n            return 0.0, \"No extractable text.\"\n\n        program_flag = bool(re.search(r\"nurturing\\s*parenting|families\\s*in\\s*substance\\s*abuse\\s*treatment\\s*and\\s*recovery|ntcrc\", corpus))\n        substance_recovery = bool(re.search(r\"substance|addiction|recovery|relapse|sobriety\", corpus))\n        reunification = bool(re.search(r\"reunification|foster|case\\s*plan|court|visitation\", corpus))\n        age5 = bool(re.search(r\"age\\s*5\\b|5[- ]?year[- ]?old|kindergarten|preschool\", corpus))\n\n        flags = [program_flag, substance_recovery, reunification, age5]\n        score = sum(1.0 for f in flags if f) / 4.0\n        fb = f\"Program:{program_flag}, Recovery:{substance_recovery}, Reunification:{reunification}, Age5:{age5}\"\n        return score, fb\n    except Exception as e:\n        return 0.0, f\"Execution error: {e}\""}, {"type": "code", "name": "Sufficient Length for Two Sessions", "description": "Approximate sufficiency by total extracted words across decks. Full credit if ~800+ words total (both sessions). Partial at ~400. Scaled if only one session detected.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs() or []\n        doc_candidates = [r for r in outputs if getattr(r, 'is_document', False) or getattr(r, 'is_text_format', False)]\n        if not doc_candidates:\n            primary = context.get_primary_output()\n            if primary and (getattr(primary, 'is_document', False) or getattr(primary, 'is_text_format', False)):\n                doc_candidates = [primary]\n        if not doc_candidates:\n            return 0.0, \"No document outputs.\"\n\n        def read_text(resource):\n            txt = \"\"\n            try:\n                txt = context.files.read_pdf_text(resource.id)\n            except Exception:\n                pass\n            if not txt:\n                try:\n                    txt = context.files.read_docx_text(resource.id)\n                except Exception:\n                    pass\n            if not txt and getattr(resource, 'is_text_format', False):\n                try:\n                    txt = context.files.read_text(resource.id)\n                except Exception:\n                    pass\n            return txt or \"\"\n\n        texts = [read_text(r) for r in doc_candidates]\n        corpus = \"\\n\".join(texts)\n        words = len(corpus.split())\n\n        lower_corpus = corpus.lower()\n        s13 = bool(re.search(r\"session\\s*13\\b|session\\s*thirteen\\b|\\bs13\\b\", lower_corpus))\n        s14 = bool(re.search(r\"session\\s*14\\b|session\\s*fourteen\\b|\\bs14\\b\", lower_corpus))\n        sessions_detected = (1 if s13 else 0) + (1 if s14 else 0)\n        sessions_detected = max(sessions_detected, 1)  # avoid divide-by-zero\n\n        # Expect ~400 words per session for adequate detail (slides are concise)\n        target = 400 * sessions_detected\n        # Full at 800 words for two sessions; scale otherwise\n        score = min(words / (target * 2.0), 1.0) if sessions_detected >= 2 else min(words / 400.0, 1.0)\n        fb = f\"Words: {words}, Sessions detected: {sessions_detected}\"\n        return score, fb\n    except Exception as e:\n        return 0.0, f\"Execution error: {e}\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Fitness for Purpose (LLM)", "description": "Holistic assessment of clarity, accessibility, trauma-informed tone, visual design, and practical usefulness for a parent in recovery working toward reunification.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity and Accessibility", "description": "Assess clarity of language (plain, jargon-free), organization, and suitability for a parent audience with varying literacy levels. Check if content is easy to follow, with concise bullets and clear headers.", "weight": 1.5, "judge_prompt": "Assess the clarity and accessibility of the slide deck(s):\n- Is the language plain and easy to understand (6th\u20138th grade reading level feel)?\n- Are slides well-organized with clear headers, concise bullet points, and logical flow?\n- Would a parent in recovery unfamiliar with jargon understand the main points?\nScoring:\n- 1.5: Very clear, plain language, excellent organization.\n- 1.0: Generally clear with minor jargon or density.\n- 0.5: Mixed clarity; some slides hard to follow.\n- 0.0: Confusing, jargon-heavy, poor organization.\n", "expectation": "Plain, well-structured slides with clear headers and concise bullets appropriate for a diverse parent audience."}, {"type": "llm_judge", "name": "Trauma-Informed, Non-Stigmatizing Tone", "description": "Evaluate whether the language is strengths-based, respectful, and non-judgmental; avoids stigmatizing terms about substance use; and aligns with a supportive, trauma-informed approach.", "weight": 1.0, "judge_prompt": "Evaluate tone and sensitivity:\n- Is the language strengths-based, empathetic, and non-judgmental?\n- Does it avoid stigmatizing terms about substance use or parenting?\n- Does it reflect trauma-informed principles (choice, collaboration, safety)?\nScoring:\n- 1.0: Consistently trauma-informed, non-stigmatizing throughout.\n- 0.7: Mostly appropriate with minor slips.\n- 0.3: Mixed; notable stigmatizing or directive language.\n- 0.0: Inappropriate or stigmatizing tone.\n", "expectation": "Consistently respectful, empowering, and non-stigmatizing language."}, {"type": "llm_judge", "name": "Visual Design and Neutral Imagery", "description": "Judge visual appeal and appropriateness of imagery. Look for neutral, non-triggering images/icons; readable fonts; consistent layout; and adequate contrast.", "weight": 1.0, "judge_prompt": "Assess visual quality:\n- Do slides use a clean, consistent template with readable fonts and good contrast?\n- Are images/icons neutral and appropriate (avoid triggering/stigmatizing visuals)?\n- Overall visual engagement without clutter?\nScoring:\n- 1.0: Strong visuals, consistent template, appropriate neutral imagery.\n- 0.7: Generally good visuals with minor issues.\n- 0.3: Inconsistent design or marginal readability.\n- 0.0: Poor readability or inappropriate imagery.\n", "expectation": "Readable, consistent design with neutral imagery suited to sensitive topics."}, {"type": "llm_judge", "name": "Actionability and Engagement", "description": "Evaluate whether slides include concrete activities, examples, or homework and a clear wrap-up with next steps/resources to support practice between sessions.", "weight": 0.5, "judge_prompt": "Assess practical engagement:\n- Are there concrete activities or examples (e.g., role-plays, practice steps)?\n- Is there a clear wrap-up with next steps, homework, or resources?\nScoring:\n- 0.5: Clear, actionable activities and wrap-up.\n- 0.3: Some actionability but limited or vague.\n- 0.0: Little to no actionable guidance.\n", "expectation": "Slides include tangible activities/practice and a clear wrap-up with next steps/resources."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "b9665ca1-4da4-4ff9-86f2-40b9a8683048", "rubric": {"category_name": "Manufacturing \u2014 Industrial Engineering: Safety Circuit Schematic (PDF)", "rationale": "This rubric enforces a self-documenting, verifiable 1-page safety circuit schematic PDF for an automated packaging/sealing machine. Stage 1 is an LLM-only shape gate that mandates the exact document structure and visible labels so later verification is trivial. Stage 2 mixes code and LLM checks to validate required labels, naming conventions, and wiring topology implied by those labels. Stage 3 assesses professional presentation and standards compliance.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (LLM only)", "description": "Validate the deliverable\u2019s format and required visible structural elements. This is a hard gate: if not a properly structured 1-page schematic PDF with the specified relay, pins, labels, and button/actuator annotations, the entire category receives 0.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Schematic PDF Requirement", "description": "Output must be a 1-page landscape 11x17 (Tabloid) PDF schematic using IEC symbols with specific relay, pins, labels, and button/actuator annotations.", "weight": 4.0, "judge_prompt": "You are evaluating whether the submitted file is a properly structured 1-page schematic PDF that includes all required elements. Only check PRESENCE/STRUCTURE/FORMAT, not electrical correctness.\n\nCheck the following in the PDF (be flexible on exact phrasing of labels, but the intent must be clearly present on the drawing):\n\nFormat and Page Layout:\n- File is a PDF, exactly 1 page\n- Orientation: Landscape\n- Paper size: 11x17 (Tabloid). If the exact size is unclear but visually close to 11x17 and landscape, accept with minor deduction\n- Title block present and legible, containing: Title: \"E-Stop Circuit\"\n- Uses IEC-style symbols for safety relay, E-Stops, push buttons\n- Connector/line width approximately 0.625\u20130.875 mm and minimum spacing between components about 5 mm (visual reasonableness check; allow small deviations)\n\nRelay and Power:\n- AutomationDirect safety relay model LG 5925-48-61-24 is shown and labeled (flexible on the exact brand text but model must be recognizable)\n- Relay pins labeled: A1, A2, S11, S12, S22, S33, S34, 13, 14 are visible\n- 24V power labels to relay: ES.24V+ to A1(+) and ES.24V- to A2(-)\n\nE-Stop Chain and Reset:\n- Exactly four E-Stop buttons (ES0, ES1, ES2, ES3) shown and labeled\n- E-Stops wired as a 2-channel emergency stop without cross-fault monitoring, with the four E-Stops in series across the channels between S11, S12, and S22 (naming flexibility allowed but series intent must be clear)\n- A normally-open manual reset push button is drawn across S33 and S34 and labeled Reset.S33 and Reset.S34 (flexible on exact label formatting but the mapping must be clear)\n\nOutputs and Grounding:\n- Devices connected to the relay\u2019s normally open contact 14 are shown and labeled: indexer 1 servo motor contactor (ES.1SD-), indexer 2 servo motor contactor (accept ES.2SD- or ES.SD-), seal module heater contactor (ES.3-), form module heater contactor (ES.6-), weld module heater contactor (ES.10-), actuator soft start valve, and stir motor contactor (ES.STIR)\n- Pin 13 (ES.13) is shown connected to 24V ground/0V\n\nButton Boxes and Signal/Indicator Wiring:\n- Three button boxes (BB1, BB2, BB3) each with 3-channel E-Stop (2NC safety channels K1/K2 and 1NO signal channel). The 1NO channel labels ES1.SIG, ES2.SIG, ES3.SIG are present\n- E-Stop in cabinet is ES0. Wire labels for ES0 across the two NC channels (K1, K2) follow the convention: ES0.K1-1, ES0.K1-2, ES0.K2-1, ES0.K2-2; similar conventions for ES1, ES2, ES3 are visible\n- Four STOP push buttons are shown in parallel, each with one NO contact and a pilot light. Enclosure STOP labeled STP.DI (NO) and STP.IND (pilot). BB1/BB2/BB3 STOPs labeled BB1.STP & BB1.IND, BB2.STP & BB2.IND, BB3.STP & BB3.IND\n- Enclosure START push button labeled STR.DI (NO) and STR.IND (pilot)\n- Enable push button on the enclosure is shown to clear E-Stop condition (exact label flexible). Treat this as OPTIONAL for scoring if the rest of the safety circuit clearly reflects the enable function at the relay\n\nScoring:\n- 4.0: Valid 1-page landscape PDF, title block with Title: \"E-Stop Circuit\", IEC symbols, relay LG 5925-48-61-24 with required pins visible, ES.24V+/ES.24V- labeled to A1/A2, four E-Stops in 2-channel series, reset across S33/S34, all listed outputs on pin 14 and pin 13 to 0V/GND, button box 3-channel conventions and all specified wire labels (ESx.SIG, ESx.K1/K2 pairs), STOP/START with indicators as specified. Line width and spacing visually reasonable\n- 3.0: Valid 1-page landscape PDF and core circuit present (relay with pins, ES.24V+/-, 4 E-Stops in series 2-channel chain, reset across S33/S34, at least 5 of the 7 listed outputs shown on pin 14, ES.13 to ground) with minor missing labels or partial button-box labeling\n- 2.0: Valid PDF but missing multiple required structural items (e.g., fewer than 4 E-Stops, missing reset, or most output devices not shown). Still obviously a safety circuit schematic\n- 0.0: Not a PDF, not a schematic, or more than 1 page", "expectation": "A single-page, landscape 11x17 PDF schematic using IEC symbols, with LG 5925-48-61-24 relay and all required pins/labels; four E-Stops in a 2-channel series chain; reset across S33/S34; specified outputs on pin 14; pin 13 to 0V; complete button-box signal and pilot-light labeling; clear title block with Title: E-Stop Circuit."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Mixed: Code + LLM)", "description": "Now that the schematic is in a verifiable shape, check correctness via label coverage and wiring topology reasonableness. Code rules validate presence of critical tokens and label conventions in the PDF text; LLM rule checks plausible topology and mapping to relay pins.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Core Relay/Power/Title Tokens Present", "description": "Verify that the PDF text includes core relay model/pin and power/title tokens required for the safety circuit.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No primary output.\"\n    if not output.is_document:\n        return 0.0, \"Primary output is not a document/PDF.\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception as e:\n        return 0.0, f\"Failed to read PDF text: {e}\"\n    if not text:\n        return 0.0, \"No extractable text from PDF.\"\n    t = text.lower()\n\n    # Core tokens (flexible grounding recognition)\n    tokens = [\n        \"lg 5925-48-61-24\",  # relay model\n        \"a1\", \"a2\", \"s11\", \"s12\", \"s22\", \"s33\", \"s34\", \"13\", \"14\",\n        \"es.24v+\", \"es.24v-\", \"es.13\",\n        \"e-stop circuit\",  # title content\n    ]\n\n    # Ground synonyms\n    ground_ok = any(g in t for g in [\"24v gnd\", \"gnd\", \"0v\", \"24v 0v\", \"0 v\", \"24vdc 0v\", \"24 v gnd\"]) \n\n    found = 0\n    missing = []\n    for tok in tokens:\n        if tok in t:\n            found += 1\n        else:\n            missing.append(tok)\n\n    # Count ground if found\n    if ground_ok:\n        found += 1\n    else:\n        missing.append(\"ground (GND/0V) label\")\n\n    total = len(tokens) + 1  # +1 for ground\n    score = (found / total) * 1.5\n    score = max(0.0, min(1.5, score))\n\n    feedback = f\"Found {found}/{total} core tokens. Missing: {', '.join(missing[:10]) if missing else 'None'}\"\n    return score, feedback"}, {"type": "code", "name": "Button-Box and Stop/Start Label Coverage", "description": "Check presence of ESx.SIG, K1/K2 wire labels for ES0\u2013ES3, STOP/START signals with pilot lights per naming conventions.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No primary output.\"\n    if not output.is_document:\n        return 0.0, \"Primary output is not a document/PDF.\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception as e:\n        return 0.0, f\"Failed to read PDF text: {e}\"\n    if not text:\n        return 0.0, \"No extractable text from PDF.\"\n    t = text.lower()\n\n    tokens = [\n        # Button-box NO signal channels\n        \"es1.sig\", \"es2.sig\", \"es3.sig\",\n        # K1/K2 channels for ES0-ES3\n        \"es0.k1-1\", \"es0.k1-2\", \"es0.k2-1\", \"es0.k2-2\",\n        \"es1.k1-1\", \"es1.k1-2\", \"es1.k2-1\", \"es1.k2-2\",\n        \"es2.k1-1\", \"es2.k1-2\", \"es2.k2-1\", \"es2.k2-2\",\n        \"es3.k1-1\", \"es3.k1-2\", \"es3.k2-1\", \"es3.k2-2\",\n        # Stop/Start signals & indicators\n        \"stp.di\", \"stp.ind\",\n        \"bb1.stp\", \"bb1.ind\", \"bb2.stp\", \"bb2.ind\", \"bb3.stp\", \"bb3.ind\",\n        \"str.di\", \"str.ind\",\n    ]\n\n    found = [tok for tok in tokens if tok in t]\n    ratio = len(found) / len(tokens)\n    score = ratio * 1.0\n    score = max(0.0, min(1.0, score))\n\n    missing = [tok for tok in tokens if tok not in found]\n    feedback = f\"Found {len(found)}/{len(tokens)} label tokens. Missing (sample): {', '.join(missing[:12]) if missing else 'None'}\"\n    return score, feedback"}, {"type": "code", "name": "Outputs on Pin 14 and Devices Presence", "description": "Validate presence of output device labels and pin 14 reference; accept ES.2SD- or ES.SD- for Indexer 2. Also check soft start valve mention.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No primary output.\"\n    if not output.is_document:\n        return 0.0, \"Primary output is not a document/PDF.\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception as e:\n        return 0.0, f\"Failed to read PDF text: {e}\"\n    if not text:\n        return 0.0, \"No extractable text from PDF.\"\n    t = text.lower()\n\n    # Each device contributes 1 if any acceptable alias is present\n    device_aliases = [\n        (\"es.1sd-\",),\n        (\"es.2sd-\", \"es.sd-\"),\n        (\"es.3-\",),\n        (\"es.6-\",),\n        (\"es.10-\",),\n        (\"es.stir\",),\n        (\"soft start\", \"soft-start\", \"actuator soft start valve\"),\n    ]\n\n    devices_found = 0\n    missing_devices = []\n    for aliases in device_aliases:\n        if any(a in t for a in aliases):\n            devices_found += 1\n        else:\n            missing_devices.append(\"/\".join(aliases))\n\n    # Pin 14 presence (cannot verify net association via text alone)\n    pin14 = (\" pin 14\" in t) or re.search(r\"\\b14\\b\", t) is not None\n\n    total = len(device_aliases) + 1  # +1 for pin 14 presence\n    found = devices_found + (1 if pin14 else 0)\n\n    score = (found / total) * 1.0\n    score = max(0.0, min(1.0, score))\n\n    miss = []\n    if not pin14:\n        miss.append(\"pin 14 reference\")\n    miss += missing_devices\n    feedback = f\"Outputs/pin check: found {found}/{total}. Missing (sample): {', '.join(miss[:10]) if miss else 'None'}\"\n    return score, feedback"}, {"type": "llm_judge", "name": "Wiring Topology and Relay Mapping Reasonableness", "description": "LLM verifies that the depicted topology is plausible: four E-Stops in 2-channel series (no cross-fault monitoring), reset across S33/S34, outputs on 14 and 13 to 0V/GND, and NO signal channels to PLC labeled ESx.SIG.", "weight": 0.5, "judge_prompt": "Check if the schematic\u2019s topology appears correct and consistent with the intended LG 5925-48-61-24 configuration:\n- Four E-Stops (ES0\u2013ES3) in a 2-channel series loop without cross-fault monitoring (series per channel), connected across S11/S12 and S22 appropriately\n- The normally-open manual reset is placed across S33 and S34\n- Output devices are shown on relay contact 14; pin 13 returns to 0V/GND\n- The third channel (1NO) on each remote button box is used as a PLC signal and labeled ES1.SIG, ES2.SIG, ES3.SIG\n\nScore 0.5 if all topology elements are correctly depicted; 0.3 if mostly correct with a minor inconsistency; 0.1 if partially correct but key elements are wrong; 0 if largely incorrect or ambiguous. Provide a brief justification.", "expectation": "A plausible 2-channel series E-Stop topology with reset and outputs mapped to the specified relay pins, and clear ESx.SIG signal usage."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality", "description": "Assess presentation quality, standards usage, and readiness to hand off to assembly technicians.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity, Standards, and Hand-off Readiness", "description": "Evaluate readability, IEC symbol consistency, annotation quality, and dimensional/format details (landscape 11x17, title block clarity, line weights, spacing).", "weight": 2.0, "judge_prompt": "Evaluate the schematic\u2019s professional quality for handoff to assembly technicians:\n- Readability: clear labels, legible text, tidy signal routing, consistent line weights; minimal crossing lines\n- Standards: IEC-style symbols used consistently for safety relay, E-Stops, push buttons, contactors\n- Documentation quality: clear title block with Title: E-Stop Circuit; optional fields (date, revision, drafter) helpful but not required\n- Format fidelity: 1-page landscape 11x17; connector/line width roughly 0.625\u20130.875 mm; component spacing near 5 mm (visual approximation acceptable)\n- Completeness: all key devices and labels present; notes or legends where helpful\n\nScoring:\n- 2.0: Professional, clean, fully readable, strong standards adherence, format and spacing convincingly correct\n- 1.0: Generally good with minor readability/standards/format issues\n- 0.0: Messy, unclear, or poorly formatted; would risk assembly errors", "expectation": "A clean, readable, IEC-consistent 1-page landscape 11x17 schematic with clear labeling suitable for technician handoff."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "5f6c57dd-feb6-4e70-b152-4969d92d1608", "rubric": {"category_name": "Finance & Insurance \u2014 Financial Managers: Standardized Branch Reporting Package (Excel)", "rationale": "Pattern A (Analytical). The rubric enforces a strict, self-documenting Excel shape first (LLM-only Stage 1), then verifies correctness and plausibility with robust, fuzzy code checks and targeted LLM verification (Stage 2). Finally, Stage 3 assesses presentation and managerial usefulness.", "max_total_score": 16.0, "stages": [{"name": "Stage 1 \u2014 Structured Output Format Gate (LLM Only)", "description": "Gate: Output must be a single Excel workbook with 5 required worksheets, each with mandated structure and visible branch/aggregate dropdown selectors. No calculation checking here\u2014just presence/shape.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.0, "rules": [{"type": "llm_judge", "name": "Workbook Shape and Required Sheets/Sections", "description": "Verify the candidate produced a single Excel workbook with 5 distinct worksheets that each contain the specified tables, headers, and dropdown selectors enabling branch and aggregate views. Only check structure/presence, not numeric correctness.", "weight": 6.0, "judge_prompt": "You are verifying STRUCTURE ONLY for an Excel workbook meant to be a standardized management reporting package. Do NOT check math or evaluate quality; just confirm the file is Excel and the required sheets, sections, tables, and selectors exist in a reasonable form. Be flexible with sheet names that clearly indicate the intended content.\n\nFORMAT REQUIREMENTS (hard requirements):\n- File type: Excel workbook (.xlsx or .xls). Not PDF/DOCX/CSV.\n- Exactly one workbook containing 5 analysis worksheets (tabs). Accept additional hidden/control/data tabs, but the 5 deliverables must be present as visible worksheets.\n- Each of the 5 worksheets must include a visible dropdown selector (e.g., Data Validation list or Slicer) labeled similarly to \u201cBranch Selector\u201d or \u201cBranch/Company Selector\u201d, allowing:\n  1) Choosing a specific branch\n  2) An aggregate/company-wide view (\u201cAll Company\u201d, \u201cCompany Total\u201d, or similar)\n- The workbook must include a visible note/legend explaining that periods M1\u2013M12 = months 1\u201312 of 2023 and M13\u2013M24 = months 1\u201312 of 2024 (this can be on a cover, notes area, or any sheet).\n\nREQUIRED WORKSHEETS (flexible with naming, but each intent must be clear):\n1) Income Statement \u2014 MoM and YoY Comparison\n   Must include a table with, at minimum:\n   - Account lines including: Revenue, COGS, SG&A, Gross Margin, EBITDA\n   - Columns for M23 and M24\n   - Columns for MoM variance in $ and in %\n   - Columns for FY 2023 and FY 2024 totals\n   - Columns for YoY variance in $ and in %\n   - A visible note or legend indicating variance sign conventions: revenue increases positive; increases in COGS/SG&A negative\n\n2) Monthly Trended Income Statement \u2014 2024\n   Must include:\n   - Monthly columns for M13 through M24 (2024 months)\n   - FY 2023 and FY 2024 summary columns/sections for comparison\n   - Same core lines as above (Revenue, COGS, SG&A, Gross Margin, EBITDA)\n\n3) Branch Ranking \u2014 2023\u20132024\n   Must include a table ranking 10 branches with these columns (or close equivalents):\n   - Branch\n   - YoY % Sales Growth\n   - 2024 ARPU (Average Revenue per Unit)\n   - Sales $ per Headcount\n   - YoY % Gross Margin Growth\n   - YoY % Order Growth\n   - An overall Rank column or clearly sorted ranking by a stated method\n\n4) Regional Comparison \u2014 2023 vs 2024\n   Must compare Regions A through G with at least these metrics by year:\n   - Revenue, SG&A Expenses, Allocations, and EBITDA\n   - A table structure showing both 2023 and 2024\n\n5) 2024 KPI Metrics \u2014 Monthly and Yearly (Efficiency, Volume, Profitability)\n   Must include sections with metric names visible and values shown by month (M13\u2013M24) and a yearly total/summary for 2024:\n   - Efficiency: (1) Implementation Headcount Hours per Implementation Headcount; (2) Revenue per Direct Labor Headcount; (3) Revenue per Sales Headcount; (4) Expenses per Total Headcount\n   - Volume: (1) Backlog Turn Rate % (Revenue units / Project Backlog units); (2) Backlog Days (Project Backlog units / units closed per day)\n   - Profitability: (1) ARPU; (2) COGS per Rev Unit; (3) EBITDA as % of Total Revenue\n   - Brief formula notes/definitions visible for each metric category\n\nSCORING (Structure Presence Only):\n- 6.0: Excel file + all 5 worksheets present with the specified tables/columns/sections; dropdown selector on each sheet for branch vs aggregate; M1\u2013M24 mapping note visible; variance sign legend present on the Income Statement sheet.\n- 5.0: All 5 worksheets present and well-structured; one minor support element missing (e.g., mapping note OR one sheet\u2019s dropdown label unclear but clearly a selector is present).\n- 4.0: Core sheets and tables present but up to 2 support elements missing (e.g., a missing dropdown on one sheet, or missing variance legend, or missing FY summaries on Trended IS).\n- 3.0: One required worksheet missing or severely incomplete, or multiple selectors missing.\n- 0.0: Not an Excel file OR fewer than 3 required worksheets present OR structure not recognizable.\n\nOnly evaluate structure/presence. Do not verify calculations, data accuracy, or design quality at this stage.", "expectation": "A single Excel workbook with 5 clearly structured analysis worksheets, each with a branch/company selector and specified tables, plus visible period mapping and variance sign notes."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification of Correctness and Plausibility (Code + LLM)", "description": "Now that the workbook shape is enforced, verify the presence of key fields, month mappings, metrics, and basic numeric plausibility using flexible, robust checks. Also verify selectors and sign conventions are documented (LLM).", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Spreadsheet Presence & Minimum Sheet Count", "description": "Confirm the primary output is an Excel workbook with at least 5 sheets.", "weight": 0.2, "code": "import pandas as pd\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_spreadsheet:\\n        return 0.0, \"No spreadsheet output detected.\"\\n    try:\\n        path = context.files.get_path(output.id)\\n        xl = pd.ExcelFile(path)\\n        sheet_count = len(xl.sheet_names)\\n        if sheet_count >= 5:\\n            return 0.2, f\"Spreadsheet with {sheet_count} sheets found.\"\\n        else:\\n            return 0.0, f\"Only {sheet_count} sheets found (<5).\"\\n    except Exception as e:\\n        return 0.0, f\"Error opening spreadsheet: {e}\""}, {"type": "code", "name": "Income Statement Variance Columns (M23/M24 + FY YoY)", "description": "Flexibly detect that the Income Statement sheet contains M23, M24, MoM $/%, FY 2023, FY 2024, YoY $/%, and core account lines (Revenue, COGS, SG&A).", "weight": 1.2, "code": "import pandas as pd, re\\n\\nREQ_TOKENS = {\\n    'm23': ['m23'],\\n    'm24': ['m24'],\\n    'fy2023': ['fy 2023','2023 total','2023'],\\n    'fy2024': ['fy 2024','2024 total','2024'],\\n    'mom_dollar': ['mom var $','mom $','mom variance $','month-over-month $','m/m $','mom value'],\\n    'mom_percent': ['mom %','mom pct','mom percent','month-over-month %','m/m %'],\\n    'yoy_dollar': ['yoy var $','yoy $','yoy variance $','year-over-year $'],\\n    'yoy_percent': ['yoy %','yoy pct','yoy percent','year-over-year %'],\\n    'revenue': ['revenue'],\\n    'cogs': ['cogs','cost of goods'],\\n    'sga': ['sg&a','sga']\\n}\\n\\nALT_HINTS = {\\n    'mom_dollar': ['mom var','mom variance','mom change'],\\n    'yoy_dollar': ['yoy var','yoy variance','yoy change']\\n}\\n\\nSEARCH_ROWS = 20\\n\\ndef has_any(cell_texts, variants):\\n    t = ' \\n'.join(cell_texts)\\n    for v in variants:\\n        if v in t:\\n            return True\\n    return False\\n\\ndef evaluate(workflow, context):\\n    out = context.get_primary_output()\\n    if not out or not out.is_spreadsheet:\\n        return 0.0, \"No spreadsheet.\"\\n    path = context.files.get_path(out.id)\\n    try:\\n        xl = pd.ExcelFile(path)\\n        # Prefer sheet with 'income' or 'statement' in name\\n        cand = None\\n        for s in xl.sheet_names:\\n            sl = s.lower()\\n            if ('income' in sl or 'statement' in sl or sl.startswith('1')):\\n                cand = s\\n                break\\n        if cand is None:\\n            cand = xl.sheet_names[0]\\n        df = pd.read_excel(path, sheet_name=cand, header=None, dtype=str)\\n        df = df.fillna('')\\n        head = df.iloc[:SEARCH_ROWS, :].astype(str).applymap(lambda x: x.lower().strip())\\n        cell_texts = list(head.values.ravel())\\n        # Score components\\n        checks = {}\\n        for key, variants in REQ_TOKENS.items():\\n            checks[key] = has_any(cell_texts, variants)\\n        # Flexible fallbacks for variance labels\\n        if not checks['mom_dollar'] and has_any(cell_texts, ALT_HINTS['mom_dollar']):\\n            checks['mom_dollar'] = True\\n        if not checks['yoy_dollar'] and has_any(cell_texts, ALT_HINTS['yoy_dollar']):\\n            checks['yoy_dollar'] = True\\n        needed = ['m23','m24','fy2023','fy2024','mom_dollar','mom_percent','yoy_dollar','yoy_percent','revenue','cogs','sga']\\n        got = sum(1 for k in needed if checks.get(k, False))\\n        score = (got/len(needed)) * 1.2\\n        missing = [k for k in needed if not checks.get(k, False)]\\n        return score, f\"Income Statement tokens present: {got}/{len(needed)}; missing: {missing}; sheet='{cand}'.\"\\n    except Exception as e:\\n        return 0.0, f\"Error reading Income Statement: {e}\""}, {"type": "code", "name": "Trended IS \u2014 2024 Months M13\u2013M24 and FY Summaries", "description": "Detect columns/labels for M13..M24 and FY 2023 / FY 2024 in the 2024 Trended Income Statement.", "weight": 0.8, "code": "import pandas as pd\\n\\nSEARCH_ROWS = 20\\n\\ndef evaluate(workflow, context):\\n    out = context.get_primary_output()\\n    if not out or not out.is_spreadsheet:\\n        return 0.0, \"No spreadsheet.\"\\n    try:\\n        path = context.files.get_path(out.id)\\n        xl = pd.ExcelFile(path)\\n        # Find a sheet with 'trend' or 'monthly' or '2024' or '2'\\n        cand = None\\n        for s in xl.sheet_names:\\n            sl = s.lower()\\n            if any(t in sl for t in ['trend','trended','monthly','2024']) or sl.startswith('2'):\\n                cand = s\\n                break\\n        if cand is None:\\n            cand = xl.sheet_names[0]\\n        df = pd.read_excel(path, sheet_name=cand, header=None, dtype=str).fillna('')\\n        head = df.iloc[:SEARCH_ROWS, :].astype(str).applymap(lambda x: x.lower().strip())\\n        cell_texts = list(head.values.ravel())\\n        tokens = set(cell_texts)\\n        months = [f\"m{i}\" for i in range(13,25)]\\n        m_count = sum(1 for m in months if any(m in c for c in cell_texts))\\n        fy23 = any('fy 2023' in c or '2023' == c for c in cell_texts)\\n        fy24 = any('fy 2024' in c or '2024' == c for c in cell_texts)\\n        total_need = 12 + 2\\n        got = m_count + (1 if fy23 else 0) + (1 if fy24 else 0)\\n        score = (got/total_need) * 0.8\\n        return score, f\"Trended months found: {m_count}/12; FY2023: {fy23}; FY2024: {fy24}; sheet='{cand}'.\"\\n    except Exception as e:\\n        return 0.0, f\"Error reading Trended IS: {e}\""}, {"type": "code", "name": "Branch Ranking \u2014 Metrics and 10 Branches", "description": "Detect ranking sheet with columns for required metrics; also detect ~10 branches listed.", "weight": 1.2, "code": "import pandas as pd, re, numpy as np\\n\\nSEARCH_ROWS = 30\\n\\nMETRIC_HINTS = {\\n    'yoy_sales_growth': ['yoy % sales','yoy percent sales','yoy sales growth','year-over-year % sales'],\\n    'arpu_2024': ['arpu','2024'],\\n    'sales_per_headcount': ['sales per headcount','sales $ per headcount','sales/headcount'],\\n    'yoy_gm_growth': ['yoy % gross margin','yoy gross margin growth','year-over-year % gross margin'],\\n    'yoy_order_growth': ['yoy % order','yoy order growth','year-over-year % order']\\n}\\n\\ndef contains_any(texts, needles):\\n    t = ' \\n'.join(texts)\\n    return any(n in t for n in needles)\\n\\ndef evaluate(workflow, context):\\n    out = context.get_primary_output()\\n    if not out or not out.is_spreadsheet:\\n        return 0.0, \"No spreadsheet.\"\\n    path = context.files.get_path(out.id)\\n    try:\\n        xl = pd.ExcelFile(path)\\n        # Find ranking sheet\\n        cand = None\\n        for s in xl.sheet_names:\\n            sl = s.lower()\\n            if any(k in sl for k in ['rank','ranking']) or sl.startswith('3'):\\n                cand = s\\n                break\\n        if cand is None:\\n            cand = xl.sheet_names[0]\\n        df0 = pd.read_excel(path, sheet_name=cand, header=None, dtype=str).fillna('')\\n        head = df0.iloc[:SEARCH_ROWS, :].applymap(lambda x: str(x).lower().strip())\\n        cell_texts = list(head.values.ravel())\\n        # Metric coverage\\n        coverage = 0\\n        for key, needles in METRIC_HINTS.items():\\n            if contains_any(cell_texts, needles):\\n                coverage += 1\\n        coverage_score = (coverage/5) * 0.9  # up to 0.9 for metrics\\n        # Branch count via a plausible 'Branch' column\\n        branch_score = 0.3\\n        try:\\n            # Attempt with header=0 for structure\\n            df = pd.read_excel(path, sheet_name=cand)\\n            cols = [str(c).lower() for c in df.columns]\\n            if any('branch' in c for c in cols):\\n                bcol = [c for c in df.columns if 'branch' in str(c).lower()][0]\\n                uniq = pd.Series(df[bcol]).dropna().astype(str).str.strip().unique()\\n                # score proportional up to 10\\n                cnt = sum(1 for u in uniq if u)\\n                branch_score = min(cnt, 10)/10 * 0.3\\n        except Exception:\\n            pass\\n        total = min(1.2, coverage_score + branch_score)\\n        return total, f\"Ranking metrics matched: {coverage}/5; branch score: {branch_score:.2f}; sheet='{cand}'.\"\\n    except Exception as e:\\n        return 0.0, f\"Error reading Ranking: {e}\""}, {"type": "code", "name": "Regional Comparison \u2014 Regions A\u2013G and Key Metrics by Year", "description": "Detect regional comparison with Regions A\u2013G and metrics Revenue, SG&A, Allocations, EBITDA for both 2023 and 2024.", "weight": 1.0, "code": "import pandas as pd\\n\\nSEARCH_ROWS = 30\\nREGIONS = [f'region {c}' for c in list('abcdefg')]\\nMETRICS = ['revenue','sg&a','sga','allocations','ebitda']\\n\\ndef evaluate(workflow, context):\\n    out = context.get_primary_output()\\n    if not out or not out.is_spreadsheet:\\n        return 0.0, \"No spreadsheet.\"\\n    try:\\n        path = context.files.get_path(out.id)\\n        xl = pd.ExcelFile(path)\\n        cand = None\\n        for s in xl.sheet_names:\\n            sl = s.lower()\\n            if 'region' in sl or sl.startswith('4'):\\n                cand = s\\n                break\\n        if cand is None:\\n            cand = xl.sheet_names[0]\\n        df = pd.read_excel(path, sheet_name=cand, header=None, dtype=str).fillna('')\\n        head = df.iloc[:SEARCH_ROWS, :].applymap(lambda x: str(x).lower().strip())\\n        cells = list(head.values.ravel())\\n        # Regions coverage\\n        reg_count = sum(1 for r in REGIONS if any(r in c for c in cells))\\n        # Metrics coverage\\n        met_count = 0\\n        for m in METRICS:\\n            if any(m in c for c in cells):\\n                met_count += 1\\n        # Years\\n        y23 = any('2023' in c for c in cells)\\n        y24 = any('2024' in c for c in cells)\\n        # Score: regions (>=5 good), metrics, years\\n        reg_score = min(reg_count/7, 1.0) * 0.4\\n        met_score = min(met_count/4, 1.0) * 0.4  # sg&a/sga overlap\\n        year_score = (1.0 if (y23 and y24) else 0.0) * 0.2\\n        total = reg_score + met_score + year_score\\n        return total, f\"Regions matched: {reg_count}/7; metrics matched: {met_count}/4; years: 2023={y23}, 2024={y24}; sheet='{cand}'.\"\\n    except Exception as e:\\n        return 0.0, f\"Error reading Regional Comparison: {e}\""}, {"type": "code", "name": "KPI Metrics (Efficiency, Volume, Profitability) \u2014 Monthly and Yearly 2024", "description": "Detect presence of specified KPI metric names and that months M13\u2013M24 appear along with a 2024 summary/total.", "weight": 1.2, "code": "import pandas as pd\\n\\nSEARCH_ROWS = 40\\nEFF_HINTS = [\\n    'implementation headcount hours per implementation headcount',\\n    'revenue per direct labor headcount',\\n    'revenue per sales headcount',\\n    'expenses per total headcount'\\n]\\nVOL_HINTS = ['backlog turn rate','backlog days']\\nPROF_HINTS = ['arpu','cogs per rev unit','ebitda as %','ebitda %']\\n\\ndef evaluate(workflow, context):\\n    out = context.get_primary_output()\\n    if not out or not out.is_spreadsheet:\\n        return 0.0, \"No spreadsheet.\"\\n    try:\\n        path = context.files.get_path(out.id)\\n        xl = pd.ExcelFile(path)\\n        cand = None\\n        for s in xl.sheet_names:\\n            sl = s.lower()\\n            if any(k in sl for k in ['kpi','metrics']) or sl.startswith('5'):\\n                cand = s\\n                break\\n        if cand is None:\\n            cand = xl.sheet_names[0]\\n        df = pd.read_excel(path, sheet_name=cand, header=None, dtype=str).fillna('')\\n        head = df.iloc[:SEARCH_ROWS, :].applymap(lambda x: str(x).lower().strip())\\n        cells = list(head.values.ravel())\\n        def cover(hints):\\n            return sum(1 for h in hints if any(h in c for c in cells))\\n        eff = cover(EFF_HINTS)\\n        vol = cover(VOL_HINTS)\\n        prof = cover(PROF_HINTS)\\n        m_count = sum(1 for i in range(13,25) if any(f'm{i}' in c for c in cells))\\n        fy24 = any('fy 2024' in c or '2024 total' in c or 'annual 2024' in c for c in cells)\\n        # weights: metrics 0.9, months+year 0.3\\n        metric_total = (eff/4 + vol/2 + prof/3) / 3.0  # average coverage across groups\\n        metric_score = metric_total * 0.9\\n        time_score = ( (m_count/12)*0.2 + (0.1 if fy24 else 0.0) )\\n        total = min(1.2, metric_score + time_score)\\n        return total, f\"Efficiency {eff}/4, Volume {vol}/2, Profitability {prof}/3; months {m_count}/12; FY2024 summary={fy24}; sheet='{cand}'.\"\\n    except Exception as e:\\n        return 0.0, f\"Error reading KPI Metrics: {e}\""}, {"type": "code", "name": "ARPU Plausibility Check (Recompute if possible)", "description": "If both Revenue and Units columns exist with an ARPU column on any relevant sheet, check ARPU \u2248 Revenue/Units within 10% for at least a few rows.", "weight": 0.6, "code": "import pandas as pd, numpy as np\\n\\nCAND_SHEETS_HINTS = ['rank','ranking','kpi','metrics']\\n\\ndef try_check(path, sheet):\\n    try:\\n        df = pd.read_excel(path, sheet_name=sheet)\\n        cols = {str(c).lower(): c for c in df.columns}\\n        arpu_col = next((cols[c] for c in cols if 'arpu' in c), None)\\n        rev_col = next((cols[c] for c in cols if 'revenue' in c and '%' not in c), None)\\n        unit_col = next((cols[c] for c in cols if ('unit' in c or 'volume' in c) and '%' not in c), None)\\n        if not (arpu_col and rev_col and unit_col):\\n            return None\\n        sub = df[[arpu_col, rev_col, unit_col]].dropna()\\n        if sub.empty:\\n            return None\\n        sub = sub[(sub[unit_col] != 0)]\\n        if sub.empty:\\n            return None\\n        calc = sub[rev_col] / sub[unit_col]\\n        arpu = sub[arpu_col]\\n        # handle potential percent or string\\n        try:\\n            arpu = pd.to_numeric(arpu, errors='coerce')\\n            calc = pd.to_numeric(calc, errors='coerce')\\n        except Exception:\\n            return 0.0\\n        valid = (np.abs(arpu - calc) <= 0.1 * (np.abs(calc)+1e-9))\\n        if valid.count() == 0:\\n            return 0.0\\n        frac = valid.mean()\\n        return frac\\n    except Exception:\\n        return None\\n\\ndef evaluate(workflow, context):\\n    out = context.get_primary_output()\\n    if not out or not out.is_spreadsheet:\\n        return 0.0, \"No spreadsheet.\"\\n    path = context.files.get_path(out.id)\\n    try:\\n        xl = pd.ExcelFile(path)\\n        # prioritize candidate sheets then fallback to all\\n        candidates = [s for s in xl.sheet_names if any(h in s.lower() for h in CAND_SHEETS_HINTS)] or xl.sheet_names\\n        best = None\\n        for s in candidates:\\n            res = try_check(path, s)\\n            if res is not None:\\n                best = (s, res)\\n                break\\n        if best is None:\\n            # Not applicable: award small partial credit for structure presence in Stage 1; here 0.2/0.6\\n            return 0.2, \"ARPU recompute not applicable (columns not found).\"\\n        sheet, frac = best\\n        score = max(0.0, min(1.0, frac)) * 0.6\\n        return score, f\"ARPU match fraction {frac:.2f} on sheet '{sheet}'.\"\\n    except Exception as e:\\n        return 0.0, f\"Error checking ARPU: {e}\""}, {"type": "code", "name": "EBITDA Margin Sanity (Within -100% to +100%)", "description": "If EBITDA and Revenue columns exist together, check EBITDA/Revenue is within [-1.0, +1.0] for most rows.", "weight": 0.5, "code": "import pandas as pd, numpy as np\\n\\ndef try_margin(path, sheet):\\n    try:\\n        df = pd.read_excel(path, sheet_name=sheet)\\n        cols = [str(c).lower() for c in df.columns]\\n        if not any('ebitda' in c for c in cols) or not any('revenue' in c for c in cols):\\n            return None\\n        e_col = [c for c in df.columns if 'ebitda' in str(c).lower()][0]\\n        r_col = [c for c in df.columns if 'revenue' in str(c).lower()][0]\\n        sub = df[[e_col, r_col]].dropna()\\n        if sub.empty:\\n            return 0.0\\n        e = pd.to_numeric(sub[e_col], errors='coerce')\\n        r = pd.to_numeric(sub[r_col], errors='coerce')\\n        sub = pd.DataFrame({'e': e, 'r': r}).dropna()\\n        sub = sub[sub['r'] != 0]\\n        if sub.empty:\\n            return 0.0\\n        marg = sub['e'] / sub['r']\\n        ok = (marg >= -1.0) & (marg <= 1.0)\\n        if ok.count() == 0:\\n            return 0.0\\n        return ok.mean()\\n    except Exception:\\n        return None\\n\\ndef evaluate(workflow, context):\\n    out = context.get_primary_output()\\n    if not out or not out.is_spreadsheet:\\n        return 0.0, \"No spreadsheet.\"\\n    path = context.files.get_path(out.id)\\n    try:\\n        xl = pd.ExcelFile(path)\\n        best = 0.0\\n        best_sheet = None\\n        for s in xl.sheet_names:\\n            res = try_margin(path, s)\\n            if res is not None:\\n                if res > best:\\n                    best = res; best_sheet = s\\n        score = max(0.0, min(1.0, best)) * 0.5\\n        if best_sheet is None:\\n            return 0.1, \"EBITDA/Revenue columns not found together (minimal credit).\"\\n        return score, f\"EBITDA margin plausibility {best:.2f} on '{best_sheet}'.\"\\n    except Exception as e:\\n        return 0.0, f\"Error checking EBITDA margin: {e}\""}, {"type": "llm_judge", "name": "Selectors and Period Mapping \u2014 Functional Visibility", "description": "Visually confirm that each required worksheet shows a drop-down selector for branch vs aggregate, and that the M1\u2013M24 = 2023\u20132024 mapping is documented somewhere visible.", "weight": 0.8, "judge_prompt": "Check visually for selector controls and a clear mapping note. Requirements:\\n- Each of the 5 required analysis worksheets should have a clearly labeled dropdown or slicer (e.g., Branch Selector) allowing a specific branch selection and an aggregate/company view.\\n- A visible note explains that M1\u2013M12 = 2023 months and M13\u2013M24 = 2024 months.\\nScoring:\\n- 0.8: All 5 worksheets have visible selectors + mapping note present.\\n- 0.5: 3\u20134 worksheets show selectors + mapping note present.\\n- 0.3: 1\u20132 worksheets show selectors or mapping note is present without selectors everywhere.\\n- 0.0: No visible selectors and no mapping note.\\nOnly assess visible presence; do not test functionality or math.", "expectation": "Selectors visible on every analysis sheet and a clear M1\u2013M24 mapping note."}, {"type": "llm_judge", "name": "Variance Sign Convention Clearly Documented", "description": "Check that the Income Statement sheet clearly documents sign conventions: increases in revenue positive; increases in COGS/SG&A negative.", "weight": 0.5, "judge_prompt": "On the Income Statement comparison worksheet, check for a visible note/legend stating that an increase in revenue is positive, and increases in COGS or SG&A are negative in variance calculations. Score structure presence only.\\nScoring:\\n- 0.5: Clear, explicit note or legend is visible.\\n- 0.2: Implicit or partially stated (e.g., color legend suggests it, but text unclear).\\n- 0.0: Not documented.", "expectation": "A concise legend/note describing variance sign conventions is present on the Income Statement sheet."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Presentation and Managerial Usefulness (LLM)", "description": "Holistic quality assessment: usability, standardization, clarity for senior management decision-making.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Quality and Usability", "description": "Evaluate formatting, consistency, and management-readiness.", "weight": 2.0, "judge_prompt": "Assess the overall professional quality and usability for senior management. Consider:\\n- Consistent styling, clear headings, readable number formats (currency, %), frozen headers, and print-friendly layout.\\n- Clear labeling of selectors, sections, and time periods.\\n- Brief methodology/assumptions notes where helpful (e.g., formula definitions on KPI sheet).\\n- Standardization across sheets (same account order, consistent color scheme, coherent navigation).\\n- Practicality for recurring reporting (e.g., minimal manual steps, obvious refresh points).\\nScoring:\\n- 2.0: Highly professional, consistent, and immediately useful; excellent readability and standardization.\\n- 1.3: Generally good with minor inconsistencies or formatting gaps.\\n- 0.7: Adequate but rough; several readability/consistency issues.\\n- 0.0: Poorly formatted or confusing; not suitable for senior management.", "expectation": "A polished, standardized workbook that senior management can easily navigate and interpret."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "0ed38524-a4ad-405f-9dee-7b2252659aad", "rubric": {"category_name": "Finance & Insurance \u2014 ECID CSR: Constituent Summary + Board Talking Points", "rationale": "Self-documenting, staged rubric enforcing two structured PDFs: (1) a one-page constituent summary by district derived from an Excel tracking log, and (2) staff talking points for a board meeting. Stage 1 is an LLM-only shape gate mandating exact structures. Stage 2 mixes code + LLM to verify correctness/consistency leveraging the enforced shape. Stage 3 judges professional quality and meeting readiness.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (LLM ONLY)", "description": "Gate that verifies the presence and exact structural format of BOTH required PDFs: (A) one-page ECID constituent summary by district, and (B) ECID board meeting talking points. Do not judge content quality or numerical correctness here\u2014only structure, layout, and presence of required sections.", "is_required": true, "max_points": 2.0, "min_score_to_pass": 2.0, "rules": [{"type": "llm_judge", "name": "Constituent Summary PDF \u2014 Required Structure", "description": "Verifies that a one-page PDF exists summarizing constituent comments by district with mandated sections and visible structure.", "weight": 1.0, "judge_prompt": "You are the Stage 1 shape gate. Review all outputs from the last task. Confirm there is a PDF that is a ONE-PAGE constituent comments summary by district for ECID. Check only structure and format\u2014not correctness of numbers.\n\nFormat requirements (flexible with naming but structure must be visible on the rendered page):\n- File must be a PDF (not Word, not Excel, not plain text).\n- Exactly one page in length (minor overrun like spill-over footer still counts as one page; two or more pages = partial at best).\n- Professional title including ECID and a summary concept, e.g., \u201cECID Constituent Summary\u201d or \u201cDistrict Constituent Comments Summary\u201d.\n- A visible \u201cPrepared\u201d or date line (e.g., Prepared: <date>) somewhere on the page.\n- A visible data source line referencing the ECID Constituent Feedback Tracking Log (flexible wording like \u201cSource: ECID Constituent Feedback Tracking Log\u201d or similar).\n- Four clearly delineated district sections (flexible names like \u201cDistrict 1/2/3/4\u201d, or the district\u2019s actual names). Each district block should include:\n  - 2\u20135 concise bullet points capturing key concerns/themes in that district; bullets may use \u2022, \u2013, \u2014, or similar markers.\n  - At least one quantitative indicator (e.g., count of comments, % share, or tally) visible either per district or in a small summary area on the page.\n- Clean layout with headings and bullets readable on the single page.\n\nScoring (structure only):\n- 1.0: All bullets above satisfied. One-page PDF with title, date, source line, and four district sections with bullets; includes at least one visible quantitative indicator.\n- 0.7: One-page PDF with title and four district sections with bullets, but missing either the date or explicit source line OR lacks a clearly visible quantitative indicator.\n- 0.4: PDF exists but missing one or more district sections (only 2\u20133 districts present), or it spills to 2 pages while otherwise structured.\n- 0.0: Not a PDF, or no identifiable single-page district-structured summary.\nOnly check presence/format, not the correctness of content.", "expectation": "A one-page, district-structured ECID constituent summary PDF with title, date, source line, 4 district sections with bullets, and at least one quantitative indicator."}, {"type": "llm_judge", "name": "Board Meeting Talking Points PDF \u2014 Required Structure", "description": "Verifies that a separate PDF exists with structured staff talking points for the board meeting.", "weight": 1.0, "judge_prompt": "You are the Stage 1 shape gate. Review all outputs from the last task. Confirm there is a PDF with structured talking points for an ECID board meeting. Check only structure and format\u2014not content quality.\n\nFormat requirements (flexible with naming but structure must be visible):\n- File must be a PDF (not Word, not Excel, not plain text).\n- 1\u20132 pages in length (2 pages acceptable; 3+ pages = partial at best).\n- Clear title referencing ECID and talking points, e.g., \u201cECID Board Meeting Talking Points\u201d.\n- Required sections (headers can vary; judge flexibly by meaning):\n  1) Opening/Context (e.g., Overview, Purpose, Framing)\n  2) Key Themes by District (bulleted, grouped by district)\n  3) Cross-Cutting Issues (themes that span districts)\n  4) Recommended Actions / Next Steps\n  5) Anticipated Questions (Q&A) with brief answers\n  6) Data Notes/Assumptions/Methodology (brief caveats)\n- Professional, scannable bullets; suitable for live reference during a meeting.\n\nScoring (structure only):\n- 1.0: Valid PDF, 1\u20132 pages, clear title, and all 6 sections present with readable bullets.\n- 0.7: Valid PDF, 1\u20132 pages, clear title, but missing exactly one required section.\n- 0.4: Valid PDF but missing two or more required sections or exceeds 2 pages.\n- 0.0: Not a PDF or lacks recognizable talking points structure.\nOnly check presence/format, not content quality.", "expectation": "A 1\u20132 page PDF with clear title and six required sections: Opening/Context, Key Themes by District, Cross-Cutting Issues, Recommended Actions/Next Steps, Anticipated Questions (Q&A), and Data Notes/Assumptions."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Structure-derived, Code + LLM)", "description": "Now that the output shape is enforced, verify correctness and internal consistency using deterministic checks and light LLM consistency review. No scoring for prose polish here; focus on plausibility and alignment with the required structure.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Two PDFs Produced", "description": "Verify that at least two PDF documents exist among the final task outputs.", "weight": 0.3, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs() or []\n        pdfs = []\n        for r in outputs:\n            try:\n                p = context.files.get_path(r.id)\n                if r.is_document and str(p).lower().endswith('.pdf'):\n                    pdfs.append(r)\n            except Exception:\n                continue\n        return 1.0 if len(pdfs) >= 2 else 0.0\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Summary Length \u2248 One Page (by word count)", "description": "Heuristic word-count check that the constituent summary stays concise (~1 page).", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    def classify_pdfs():\n        outs = context.get_all_outputs() or []\n        items = []\n        for r in outs:\n            try:\n                p = context.files.get_path(r.id)\n                if r.is_document and str(p).lower().endswith('.pdf'):\n                    txt = context.files.read_pdf_text(r.id) or ''\n                    items.append((r, txt))\n            except Exception:\n                pass\n        if not items:\n            return None, None\n        # Identify summary vs talking points\n        summary = None\n        talking = None\n        for r, t in items:\n            lt = t.lower()\n            if ('talking points' in lt) or ('q&a' in lt) or ('anticipated questions' in lt):\n                talking = (r, t)\n            if ('summary' in lt and 'district' in lt) or ('constituent' in lt and 'district' in lt):\n                summary = (r, t)\n        if not summary and len(items) == 2:\n            # Fallback: shorter text assumed to be one-page summary\n            items_sorted = sorted(items, key=lambda x: len(x[1]))\n            summary = items_sorted[0]\n            talking = items_sorted[1]\n        return summary, talking\n\n    try:\n        summary, _ = classify_pdfs()\n        if not summary:\n            return 0.0\n        text = summary[1]\n        words = re.findall(r\"\\b\\w+\\b\", text)\n        w = len(words)\n        if w == 0:\n            return 0.0\n        # Scoring: ideal 150\u2013600 words (single, dense page). Allow partial up to 800.\n        if 150 <= w <= 600:\n            return 1.0\n        if 100 <= w <= 800:\n            return 0.6\n        # Very short or very long -> poor\n        return 0.2 if 50 <= w < 100 or 800 < w <= 1000 else 0.0\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Summary Contains Four District Sections (fuzzy)", "description": "Check that the summary text references 4 distinct districts with clear labels/headers.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    def get_summary_text():\n        outs = context.get_all_outputs() or []\n        pdfs = []\n        for r in outs:\n            try:\n                p = context.files.get_path(r.id)\n                if r.is_document and str(p).lower().endswith('.pdf'):\n                    txt = context.files.read_pdf_text(r.id) or ''\n                    pdfs.append((r, txt))\n            except Exception:\n                pass\n        # classify\n        summary = None\n        for r, t in pdfs:\n            lt = t.lower()\n            if ('summary' in lt and 'district' in lt) or ('constituent' in lt and 'district' in lt):\n                summary = t\n                break\n        if not summary and len(pdfs) == 2:\n            pdfs_sorted = sorted(pdfs, key=lambda x: len(x[1]))\n            summary = pdfs_sorted[0][1]\n        return summary\n\n    try:\n        text = get_summary_text()\n        if not text:\n            return 0.0\n        lt = text.lower()\n        # Extract district labels like \"District 1\", \"District A\", etc.\n        labels = re.findall(r\"district\\s*([a-z0-9-]+)\", lt)\n        uniq = set([lab.strip() for lab in labels if lab.strip()])\n        # Also consider titled lines that start with 'District'\n        lines = [l.strip().lower() for l in lt.splitlines()]\n        header_like = set()\n        for l in lines:\n            if l.startswith('district'):\n                m = re.match(r\"district\\s*([a-z0-9-]+)\", l)\n                if m:\n                    header_like.add(m.group(1))\n        uniq |= header_like\n        count = len(uniq)\n        if count >= 4:\n            return 1.0\n        if count == 3:\n            return 0.75\n        if count == 2:\n            return 0.5\n        if 'district' in lt:\n            return 0.25\n        return 0.0\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Summary Mentions Data Source", "description": "Check that the summary references the ECID Constituent Feedback Tracking Log (flexible phrasing).", "weight": 0.3, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        outs = context.get_all_outputs() or []\n        for r in outs:\n            try:\n                p = context.files.get_path(r.id)\n                if not (r.is_document and str(p).lower().endswith('.pdf')):\n                    continue\n                text = (context.files.read_pdf_text(r.id) or '').lower()\n                if 'ecid' in text and (('tracking log' in text) or ('feedback tracking' in text) or ('constituent feedback' in text)):\n                    if 'source' in text or 'data source' in text or 'prepared' in text or 'methodology' in text:\n                        return 1.0\n                    return 0.7\n        return 0.0\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Talking Points \u2014 Required Sections Present (fuzzy)", "description": "Check for key sections in the talking points using flexible keyword matching.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    def get_talking_points_text():\n        outs = context.get_all_outputs() or []\n        cands = []\n        for r in outs:\n            try:\n                p = context.files.get_path(r.id)\n                if r.is_document and str(p).lower().endswith('.pdf'):\n                    t = context.files.read_pdf_text(r.id) or ''\n                    cands.append((r, t))\n            except Exception:\n                pass\n        # pick by keywords\n        for r, t in cands:\n            lt = t.lower()\n            if ('talking points' in lt) or ('board meeting' in lt) or ('q&a' in lt) or ('anticipated questions' in lt):\n                return t\n        if len(cands) == 2:\n            # fallback: longer doc is likely the talking points\n            cands_sorted = sorted(cands, key=lambda x: len(x[1]), reverse=True)\n            return cands_sorted[0][1]\n        return None\n\n    try:\n        text = get_talking_points_text()\n        if not text:\n            return 0.0\n        lt = text.lower()\n        checks = 0\n        total = 6\n        # 1) Opening/Context\n        if any(k in lt for k in ['opening', 'context', 'overview', 'purpose', 'framing']):\n            checks += 1\n        # 2) Key Themes by District\n        if ('district' in lt and 'theme' in lt) or ('by district' in lt) or ('per district' in lt):\n            checks += 1\n        # 3) Cross-Cutting Issues\n        if any(k in lt for k in ['cross-cutting', 'cross cutting', 'common themes', 'across districts', 'systemic']):\n            checks += 1\n        # 4) Recommended Actions / Next Steps\n        if any(k in lt for k in ['recommended actions', 'recommendations', 'next steps', 'action items', 'proposed actions']):\n            checks += 1\n        # 5) Anticipated Questions (Q&A)\n        if any(k in lt for k in ['q&a', 'anticipated questions', 'possible questions', 'questions & answers', 'faq']):\n            checks += 1\n        # 6) Data Notes/Assumptions\n        if any(k in lt for k in ['data notes', 'assumptions', 'methodology', 'limitations', 'caveats']):\n            checks += 1\n        return checks / total\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Cross-Document District Alignment", "description": "District labels mentioned in the summary also appear in the talking points (fuzzy match).", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    def get_texts():\n        outs = context.get_all_outputs() or []\n        pdfs = []\n        for r in outs:\n            try:\n                p = context.files.get_path(r.id)\n                if r.is_document and str(p).lower().endswith('.pdf'):\n                    t = context.files.read_pdf_text(r.id) or ''\n                    pdfs.append((r, t))\n            except Exception:\n                pass\n        # classify\n        summary = None\n        talking = None\n        for r, t in pdfs:\n            lt = t.lower()\n            if ('summary' in lt and 'district' in lt) or ('constituent' in lt and 'district' in lt):\n                summary = t\n            if ('talking points' in lt) or ('q&a' in lt) or ('anticipated questions' in lt):\n                talking = t\n        if (not summary or not talking) and len(pdfs) == 2:\n            pdfs_sorted = sorted(pdfs, key=lambda x: len(x[1]))\n            summary = summary or pdfs_sorted[0][1]\n            talking = talking or pdfs_sorted[1][1]\n        return summary, talking\n\n    def extract_district_set(txt):\n        if not txt:\n            return set()\n        lt = txt.lower()\n        labs = re.findall(r\"district\\s*([a-z0-9-]+)\", lt)\n        return set([x.strip() for x in labs if x.strip()])\n\n    try:\n        summary, talking = get_texts()\n        sset = extract_district_set(summary)\n        tset = extract_district_set(talking)\n        if not sset or not tset:\n            return 0.0\n        inter = len(sset & tset)\n        target = min(4, max(len(sset), len(tset)))\n        if target == 0:\n            return 0.0\n        return max(0.0, min(1.0, inter / target))\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Bullets Under Districts (Summary)", "description": "Heuristic check that the summary uses bullets and likely provides multiple bullets per district.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        outs = context.get_all_outputs() or []\n        texts = []\n        for r in outs:\n            try:\n                p = context.files.get_path(r.id)\n                if r.is_document and str(p).lower().endswith('.pdf'):\n                    t = context.files.read_pdf_text(r.id) or ''\n                    texts.append(t)\n            except Exception:\n                pass\n        if not texts:\n            return 0.0\n        # Assume shorter doc is the one-page summary\n        text = sorted(texts, key=lambda x: len(x))[0]\n        lt = text.lower()\n        if 'district' not in lt:\n            return 0.0\n        bullet_chars = ['\\u2022', '\u2022', '-', '\u2013', '*']\n        bullet_count = 0\n        for ch in bullet_chars:\n            bullet_count += text.count(ch)\n        # Scoring: 8+ bullets -> full (approx 2 per each of 4 districts); 4 bullets -> half.\n        if bullet_count >= 8:\n            return 1.0\n        if bullet_count >= 4:\n            return 0.5\n        return bullet_count / 8.0\n    except Exception:\n        return 0.0"}, {"type": "llm_judge", "name": "Consistency: Talking Points Reflect Summary", "description": "LLM check that the talking points accurately reflect the themes/district focus presented in the summary (no obvious contradictions).", "weight": 0.4, "judge_prompt": "Review the two PDFs produced: (1) the one-page district-by-district constituent summary and (2) the board meeting talking points. Judge consistency only\u2014do the talking points reflect and align with the themes/issues and districts emphasized in the summary? Ignore prose quality. Examples of misalignment: talking points focus on districts not present in the summary, claim opposite trends, or omit all major themes from the summary.\n\nScoring:\n- 1.0: Strong alignment\u2014key districts and themes from the summary clearly appear in the talking points with no contradictions.\n- 0.6: Mostly aligned\u2014minor omissions or small emphasis shifts, but overall consistent.\n- 0.3: Weak alignment\u2014several omissions or mismatches; partial overlap only.\n- 0.0: Misaligned or cannot verify alignment (e.g., only one PDF present).", "expectation": "Talking points that faithfully mirror districts and themes from the one-page summary."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality Assessment (LLM)", "description": "Holistic professional quality for board use: clarity, concision, actionability, tone, and audience appropriateness.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity and Concision", "description": "Are the documents clear, concise, and easy to scan quickly in a meeting?", "weight": 1.4, "judge_prompt": "Evaluate both PDFs for clarity and concision. Is the one-page summary scannable and crisp with clear headings and bullets? Are the talking points succinct and logically organized for quick reference during a meeting? Avoid judging factual correctness; focus on readability and brevity.\n\nScoring:\n- 1.0: Very clear and concise; fast to scan\n- 0.7: Generally clear; minor verbosity or clutter\n- 0.4: Some clarity issues; hard to scan or overly wordy\n- 0.0: Unclear or disorganized", "expectation": "Highly scannable one-page summary and succinct, well-structured talking points."}, {"type": "llm_judge", "name": "Actionability and Meeting Readiness", "description": "Assess whether talking points drive productive board discussion with concrete, prioritized actions.", "weight": 1.2, "judge_prompt": "Evaluate the talking points for actionability and meeting readiness. Do they present prioritized, concrete recommendations or next steps tied to the concerns? Are responsibilities or timelines suggested? Do sections help guide a productive board discussion?\n\nScoring:\n- 1.0: Highly actionable and meeting-ready\n- 0.7: Some actionable guidance but could be sharper\n- 0.4: Mostly descriptive with limited actionable value\n- 0.0: Not actionable", "expectation": "Clear, prioritized actions/next steps that staff can use during the board meeting."}, {"type": "llm_judge", "name": "Professional Formatting and Tone", "description": "Professional presentation suitable for elected board members and staff.", "weight": 0.9, "judge_prompt": "Assess formatting and tone across both PDFs. Are titles, headers, bullets, and spacing professional? Is tone neutral, service-oriented, and appropriate for a public-sector board audience? Ignore whether content is correct; focus on professionalism.\n\nScoring:\n- 1.0: Highly professional formatting and tone\n- 0.7: Generally professional with minor issues\n- 0.4: Several issues detract from professionalism\n- 0.0: Unprofessional presentation or tone", "expectation": "Clean, consistent formatting and a neutral, professional tone."}, {"type": "llm_judge", "name": "Audience Appropriateness", "description": "Fit for ECID board audience; avoids jargon and includes needed context.", "weight": 0.5, "judge_prompt": "Assess whether both documents are appropriate for the ECID board audience, avoiding unnecessary jargon while providing enough context to understand constituent concerns and proposed actions.\n\nScoring:\n- 1.0: Fully appropriate for the board\n- 0.7: Mostly appropriate; minor over/under-explanation\n- 0.4: Significant jargon or missing context\n- 0.0: Poorly matched to audience", "expectation": "Right level of context and plain language for board members."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "0112fc9b-c3b2-4084-8993-5a4abb1f54f1", "rubric": {"category_name": "Healthcare Documentation - Pediatric SOAP Note (Head Injury)", "rationale": "This rubric enforces a self-documenting SOAP note deliverable for a pediatric minor head injury visit. Stage 1 (LLM-only) mandates a strict, verifiable document structure so later checks are trivial. Stage 2 uses code rules to verify factual consistency, clinical decision rule references, vitals plausibility, and safety elements. Stage 3 uses LLM judges to assess clinical reasoning appropriateness and professional presentation.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 - Structure and Format Gate", "description": "MANDATORY gate. Output must be a professionally formatted SOAP note document with the exact structure specified below so verification is possible.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.0, "rules": [{"type": "llm_judge", "name": "SOAP Note Structure and Formatting Requirement", "description": "Check that the output is a properly structured SOAP note document with all required sections and subsections present, in a verifiable format.", "weight": 6.0, "judge_prompt": "You are the Stage 1 gatekeeper for STRUCTURE ONLY (not content quality). Review the candidate output and score based on the following requirements. Be flexible with exact header wording, but the structure must be clearly visible with headers/bold formatting or equivalent.\n\nACCEPTABLE FORMATS\n- PDF or DOCX preferred; Markdown (.md) acceptable if clearly sectioned with headers. Not spreadsheets.\n- At least ~1 page of content equivalent. Professional clinical formatting (clear headers, bullet lists or tables where applicable).\n\nREQUIRED TOP-LEVEL SECTIONS (SOAP)\n1) Subjective\n   Must include clearly labeled subsections:\n   - Chief Complaint (CC)\n   - History of Present Illness (HPI): Should indicate mechanism of injury and timing\n   - Review of Systems (ROS) pertinent to neuro/head injury\n   - Past Medical/Surgical History (PMH/PSH)\n   - Medications\n   - Allergies\n   - Family History (FHx)\n   - Social History (SHx)\n2) Objective\n   Must include:\n   - Vitals presented together (prefer a small table or clear list)\n   - Physical Exam with, at minimum: HEENT and Neurologic; Gait/coordination noted\n   - Optional: Any tests/decision tools used (listed, even if none ordered)\n3) Assessment\n   Must include:\n   - Primary problem/diagnosis statement for this visit\n   - Differential diagnosis list (\u22652 items) with brief justification per item (even short phrases)\n   - Optional: Diagnosis Codes (ICD-10) as a small list or table\n4) Plan\n   Must include labeled plan components:\n   - Diagnostics/Imaging decision with a named decision rule reference (e.g., PECARN) or a clear statement of rule used/considered\n   - Treatment/Medications (analgesia, etc.)\n   - Patient/Family Education & Return Precautions (red flags)\n   - Activity/School/Sport guidance (e.g., no driving/contact sports; return-to-play/learn guidance)\n   - Follow-up timeline/criteria\n   - Optional: Minor/guardian notification/consent note\n\nSCORING (STRUCTURE ONLY)\n- 6.0: Acceptable document format + all 4 SOAP sections present + all required subsections/components as listed under each section. Optional items (ICD-10, guardian note) may be missing.\n- 5.0: One required subsection/component missing across the SOAP sections.\n- 4.0: Two required subsections/components missing across the SOAP sections.\n- 2.0: Three to four required subsections/components missing OR format is weak but still a document (e.g., plain text with unclear headers), yet SOAP headers are present.\n- 0.0: Not a PDF/DOCX/Markdown document, SOAP headers missing, or grossly incomplete note.\n\nOnly evaluate STRUCTURE and PRESENCE. Do not judge medical accuracy or decision appropriateness here.", "expectation": "A PDF/DOCX/MD SOAP note with clear Subjective, Objective, Assessment, Plan sections and all required subsections/components present so that Stage 2 can verify details."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 - Verification and Correctness Checks", "description": "Code-based and light LLM checks that leverage the enforced structure to validate factual consistency, clinical safety elements, and plausibility.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Text Extractability + Key Facts Present", "description": "Ensure text is extractable and includes the patient age (16) and encounter date (3/1/2024 variants).", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n\n    text = \"\"\n    try:\n        if getattr(output, 'is_document', False):\n            # Try PDF then DOCX\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                text = \"\"\n            if not text:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    text = \"\"\n        if not text and getattr(output, 'is_text_format', False):\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    except Exception:\n        text = \"\"\n\n    if not text:\n        return 0.0\n\n    score = 0.0\n    # Age 16 patterns\n    age_patterns = [\n        r\"\\b16\\s*(?:yo|y/o|yr|yrs|year|years)[- ]?old\\b\",\n        r\"\\b16[- ]?year[- ]?old\\b\",\n        r\"\\b16\\s*yo\\b\",\n        r\"\\b16\\s*y/o\\b\",\n    ]\n    age_ok = any(re.search(p, text, flags=re.IGNORECASE) for p in age_patterns) or bool(re.search(r\"\\b16\\s*[- ]?year\\b\", text, re.IGNORECASE))\n\n    # Date 3/1/2024 variants\n    date_ok = bool(re.search(r\"\\b(?:0?3|March)\\s*[\\/-\\s]?\\s*0?1\\s*[\\/-\\s]?\\s*2024\\b\", text, re.IGNORECASE)) or bool(re.search(r\"\\b2024[-/.]0?3[-/.]0?1\\b\", text))\n\n    if age_ok and date_ok:\n        score = 1.5\n    elif age_ok or date_ok:\n        score = 0.9\n    else:\n        # Give minimal credit if substantial text exists\n        score = 0.3 if len(text) > 100 else 0.0\n    return score"}, {"type": "code", "name": "Vitals Presence and Plausibility", "description": "Detect vitals (Temp, HR, BP, RR, Weight, Height). Award more if present and within plausible adolescent ranges.", "weight": 2.5, "code": "import re\n\ndef _parse_float(s):\n    try:\n        return float(s)\n    except Exception:\n        return None\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    text = \"\"\n    try:\n        if getattr(output, 'is_document', False):\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                text = \"\"\n            if not text:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    text = \"\"\n        if not text and getattr(output, 'is_text_format', False):\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    except Exception:\n        text = \"\"\n    if not text:\n        return 0.0\n\n    found = 0\n    points = 0.0\n\n    # Temperature\n    m = re.search(r\"\\b(?:temp|temperature)\\s*[:\\-]?\\s*(\\d{2,3}(?:\\.\\d)?)\\s*([cfCF])?\\b\", text)\n    if m:\n        found += 1\n        val = _parse_float(m.group(1))\n        unit = (m.group(2) or 'F').upper()\n        if val is not None:\n            if unit == 'C':\n                plausible = 35.0 <= val <= 40.5\n            else:\n                plausible = 95.0 <= val <= 105.0\n            points += 1.0 if plausible else 0.5\n\n    # Heart Rate\n    m = re.search(r\"\\b(?:HR|heart\\s*rate)\\s*[:\\-]?\\s*(\\d{2,3})\\b\", text, re.IGNORECASE)\n    if m:\n        found += 1\n        val = _parse_float(m.group(1))\n        plausible = 40 <= (val or 0) <= 140\n        points += 1.0 if plausible else 0.5\n\n    # Blood Pressure\n    m = re.search(r\"\\b(?:BP|blood\\s*pressure)\\s*[:\\-]?\\s*(\\d{2,3})\\s*/\\s*(\\d{2,3})\\b\", text, re.IGNORECASE)\n    if m:\n        found += 1\n        sys = _parse_float(m.group(1))\n        dia = _parse_float(m.group(2))\n        plausible = (90 <= (sys or 0) <= 160) and (50 <= (dia or 0) <= 100)\n        points += 1.0 if plausible else 0.5\n\n    # Respiratory Rate\n    m = re.search(r\"\\b(?:RR|respiratory\\s*rate)\\s*[:\\-]?\\s*(\\d{1,2})\\b\", text, re.IGNORECASE)\n    if m:\n        found += 1\n        val = _parse_float(m.group(1))\n        plausible = 10 <= (val or 0) <= 28\n        points += 1.0 if plausible else 0.5\n\n    # Weight (kg or lbs)\n    m = re.search(r\"\\b(?:Wt|weight)\\s*[:\\-]?\\s*(\\d{2,3}(?:\\.\\d)?)\\s*(kg|kgs|kilograms|lb|lbs|pounds)?\\b\", text, re.IGNORECASE)\n    if m:\n        found += 1\n        val = _parse_float(m.group(1)) or 0\n        unit = (m.group(2) or '').lower()\n        if unit in ['lb', 'lbs', 'pounds']:\n            kg = val * 0.453592\n        else:\n            kg = val\n        plausible = 35 <= kg <= 120\n        points += 1.0 if plausible else 0.5\n\n    # Height (feet and inches)\n    m = re.search(r\"\\b(?:Ht|height)\\s*[:\\-]?\\s*(\\d)\\s*[\\'\u2019]\\s*(\\d{1,2})\\b\", text, re.IGNORECASE)\n    if m:\n        found += 1\n        feet = _parse_float(m.group(1)) or 0\n        inches = _parse_float(m.group(2)) or 0\n        total_inches = feet * 12 + inches\n        plausible = 54 <= total_inches <= 78\n        points += 1.0 if plausible else 0.5\n    else:\n        # Alternate pattern like 5\u20197\u201c without label\n        m2 = re.search(r\"\\b(\\d)\\s*[\\'\u2019]\\s*(\\d{1,2})\\b\", text)\n        if m2:\n            found += 1\n            feet = _parse_float(m2.group(1)) or 0\n            inches = _parse_float(m2.group(2)) or 0\n            total_inches = feet * 12 + inches\n            plausible = 54 <= total_inches <= 78\n            points += 1.0 if plausible else 0.5\n\n    # Normalize score: each vital worth ~ (2.5 / 6)\n    if found == 0:\n        return 0.0\n    max_items = 6\n    per_item = 2.5 / max_items\n    return min(2.5, points * per_item)"}, {"type": "code", "name": "Neuro and HEENT Exam Elements Present", "description": "Verify presence of key neurologic and HEENT exam elements, including cranial nerves, pupils/EOM, and gait/coordination.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    text = \"\"\n    try:\n        if getattr(output, 'is_document', False):\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                text = \"\"\n            if not text:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    text = \"\"\n        if not text and getattr(output, 'is_text_format', False):\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    except Exception:\n        text = \"\"\n    if not text:\n        return 0.0\n\n    features = 0\n    if re.search(r\"\\bcranial\\s*nerves?\\b|\\b\\bCN\\s*(?:II|III|IV|V|VI|VII|VIII|IX|X|XI|XII)\\b\", text, re.IGNORECASE):\n        features += 1\n    if re.search(r\"\\bpupils?\\s*(?:equal|PERRL|PERRLA)|\\bEOMI\\b|extraocular\", text, re.IGNORECASE):\n        features += 1\n    if re.search(r\"\\bgait\\b|heel[- ]?(?:toe|walking)|Romberg|finger[- ]?to[- ]?nose|coordination\", text, re.IGNORECASE):\n        features += 1\n\n    # Score proportionally: 0.5 per feature up to 1.5\n    return min(1.5, 0.5 * features)"}, {"type": "code", "name": "Head Imaging/Concussion Decision Rule Referenced", "description": "Check for explicit mention of a recognized decision rule (PECARN preferred).", "weight": 2.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    text = \"\"\n    try:\n        if getattr(output, 'is_document', False):\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                text = \"\"\n            if not text:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    text = \"\"\n        if not text and getattr(output, 'is_text_format', False):\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    except Exception:\n        text = \"\"\n    if not text:\n        return 0.0\n\n    if re.search(r\"\\bPECARN\\b|Pediatric\\s*Emergency\\s*Care\\s*Applied\\s*Research\\s*Network\", text, re.IGNORECASE):\n        return 2.0\n    if re.search(r\"Canadian\\s*CT\\s*Head\\s*Rule|ACE\\b|CDC\\s*(Heads\\s*Up|concussion)|New\\s*Orleans\\s*Criteria\", text, re.IGNORECASE):\n        return 1.2\n    if re.search(r\"decision\\s*rule|clinical\\s*guideline\", text, re.IGNORECASE):\n        return 0.6\n    return 0.0"}, {"type": "code", "name": "Differentials and Diagnostic Coding Present", "description": "Verify a differential diagnosis list and at least one ICD-10 code are documented in Assessment.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    text = \"\"\n    try:\n        if getattr(output, 'is_document', False):\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                text = \"\"\n            if not text:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    text = \"\"\n        if not text and getattr(output, 'is_text_format', False):\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    except Exception:\n        text = \"\"\n    if not text:\n        return 0.0\n\n    score = 0.0\n    if re.search(r\"\\bdifferential\\b|\\bddx\\b|\\bdifferentials\\b\", text, re.IGNORECASE):\n        score += 0.8\n    # ICD-10 code pattern (exclude U,S? Keep general but avoid MDM artifacts). Accept standard code formats.\n    if re.search(r\"\\b[A-TV-Z][0-9][0-9A-Z](?:\\.[0-9A-Z]{1,4})?\\b\", text):\n        score += 0.7\n    return min(1.5, score)"}, {"type": "code", "name": "Education/Return Precautions and Activity Restrictions", "description": "Check that the Plan includes return precautions/red flags and activity/school/sport guidance (e.g., no sports/driving, graded return).", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    text = \"\"\n    try:\n        if getattr(output, 'is_document', False):\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                text = \"\"\n            if not text:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    text = \"\"\n        if not text and getattr(output, 'is_text_format', False):\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    except Exception:\n        text = \"\"\n    if not text:\n        return 0.0\n\n    precautions = bool(re.search(r\"return\\s*precautions|red\\s*flags|seek\\s*(?:medical|emergency)\\s*care|go\\s*to\\s*(?:ED|ER)|911|worsen|worsening\", text, re.IGNORECASE))\n    activity = bool(re.search(r\"no\\s*(?:sports|PE|physical\\s*education|driving)|return[- ]to[- ]play|graded\\s*return|return[- ]to[- ]learn|school\\s*accommodations\", text, re.IGNORECASE))\n\n    score = 0.0\n    if precautions:\n        score += 0.5\n    if activity:\n        score += 0.5\n    return min(1.0, score)"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 - Holistic Quality Assessment", "description": "LLM-judged assessment of clinical reasoning appropriateness, safety, and professional communication quality.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clinical Reasonableness and Safety", "description": "Does the note reflect appropriate assessment and safe plan for a 16-year-old with minor head injury without LOC, using an evidence-based decision rule and appropriate education/follow-up?", "weight": 2.0, "judge_prompt": "Judge the clinical reasoning and safety of the SOAP note for a 16-year-old male with minor head injury, no LOC, mild nausea, normal neuro exam aside from mild gait/heel-walk coordination deficit, and stable vitals. Consider:\n- Appropriate consideration/mention of a pediatric head imaging decision rule (PECARN preferred) and whether the plan aligns with low-risk features\n- Reasonableness of assessment and differential (e.g., concussion, scalp/face contusion, less likely ICH, skull fracture) with brief justifications\n- Safety: clear return precautions (red flags), activity restrictions (no driving/contact sports initially), and specific follow-up timing\n- Medication guidance (e.g., acetaminophen; avoidance of sedating meds initially) and school/sports guidance\n\nScoring:\n- 2.0: Sound, guideline-consistent reasoning; safe and specific plan\n- 1.2: Generally appropriate with minor omissions\n- 0.6: Partially appropriate but notable gaps in safety/logic\n- 0.0: Inappropriate or unsafe plan/assessment for this presentation", "expectation": "A safe, guideline-aligned plan referencing PECARN (or equivalent), with clear precautions and follow-up."}, {"type": "llm_judge", "name": "Professional Presentation and Documentation Quality", "description": "Organization, clarity, clinical tone, and documentation completeness suitable for the medical record.", "weight": 2.0, "judge_prompt": "Evaluate professional presentation quality:\n- Organization: clear headers, readable layout, concise clinical language\n- Clarity: minimal ambiguity, uses standard medical abbreviations correctly, no contradictory statements\n- Documentation completeness: includes key elements from the visit context (mechanism, timing, ROS pertinent negatives, exam elements), and is appropriate for a pediatric minor\n- Professional tone: objective, nonjudgmental, suitable for EHR\n\nScoring:\n- 2.0: Highly professional, well-organized, clear, and complete\n- 1.2: Mostly professional with minor clarity/organization issues\n- 0.6: Understandable but inconsistent or poorly organized\n- 0.0: Unprofessional or confusing documentation", "expectation": "A polished, EHR-ready SOAP note with clear clinical communication."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "bf68f2ad-eac5-490a-adec-d847eb45bd6f", "rubric": {"category_name": "Manufacturing | First-Line Supervisors of Production and Operating Workers | MIG Welding Catch-Up Plan", "rationale": "This rubric enforces a self-documenting, verifiable Excel-based catch-up plan starting at week 4 plus a brief manager-ready summary. Stage 1 (LLM-only) strictly gates structure and required artifacts. Stage 2 (code) verifies quantitative correctness using flexible, tolerant checks informed by the mandated shape. Stage 3 (LLM) assesses presentation quality and managerial usefulness, including alignment between the spreadsheet and the brief summary.", "max_total_score": 10.0, "stages": [{"name": "Stage 1: Shape Enforcement Gate", "description": "MANDATORY structural validation of deliverables. Output must be an Excel catch-up plan with specific sheets/sections/columns starting at week 4, plus a brief manager summary as a separate file (or a clearly labeled sheet if not separate). No calculation quality checks here\u2014structure only.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.4, "rules": [{"type": "llm_judge", "name": "Required Files and Spreadsheet Structure", "description": "Check that the primary output is a well-structured Excel file with the required sheets, sections, and columns starting at week 4, and that a brief manager summary (2\u20135 sentences) is provided as a separate document (preferred) or as a clearly labeled sheet.", "weight": 4.0, "judge_prompt": "You are validating STRUCTURE ONLY. Review all candidate outputs. Apply these rules:\n\nFORMAT REQUIREMENTS:\n- The primary output MUST be an Excel file (.xlsx preferred) containing a detailed MIG welding catch-up plan.\n- There MUST be a brief manager summary of 2\u20135 sentences as a separate file (PDF/DOCX/MD). If not separate, it may be accepted as a clearly labeled sheet named like \"Email Summary\" or \"Manager Summary\".\n\nEXCEL STRUCTURE REQUIREMENTS (be flexible with exact names but require the intent):\n1) Sheet: \"Catch-Up Plan\" (or similar like \"Week 4 Catch-Up Plan\", \"Production Catch-up Plan\", etc.)\n   - A tabular plan starting at week 4 (first plan row should be Week 4) with columns (flexible naming but clearly present):\n     \u2022 Week (number or date; must explicitly include week 4)\n     \u2022 Demand Hours (weekly)\n     \u2022 Days per Week (planned; typically 4\u20136)\n     \u2022 Shift Hours per Week (should correspond to Days \u00d7 10 hours; label may be \"Shift Hours\" or similar)\n     \u2022 Standard Capacity Hours (should correspond to Days \u00d7 30 standard hours)\n     \u2022 Weekly Balance (Capacity \u2212 Demand or Demand \u2212 Capacity; clearly indicated)\n     \u2022 Cumulative Backlog/Buffer (running total; label may be \"Cumulative Balance\", \"Backlog\", or \"Buffer\")\n     \u2022 Status/Notes (brief notes on transitions like moving to 5-day or 4-day weeks)\n\n2) Sheet: \"Assumptions & Rules\" (or similar like \"Assumptions\", \"Methodology\")\n   - Must include a Parameters table with columns like [Parameter | Value | Source/Justification], clearly listing at minimum:\n     \u2022 Standard hours per day = 30\n     \u2022 Regular schedule = 4 days/week (regular time)\n     \u2022 Max schedule = 6 days/week (10-hour shifts)\n     \u2022 Definition of \"caught up\"\n     \u2022 Starting backlog at week 4 = 438.81 standard hours (label may be \"past due\", \"starting backlog\", or similar)\n   - A short text explanation (\u2265 3 sentences) describing calculation rules and how overtime, balances, and backlog are computed.\n\n3) Sheet: \"Summary\" (or similar like \"Executive Summary\", \"Plan Summary\")\n   - Must explicitly state:\n     \u2022 Number of weeks to catch up (backlog reaches \u2264 0 standard hours)\n     \u2022 The first week returning to 5-day weeks\n     \u2022 The first week returning to 4-day regular time\n     \u2022 A statement about overtime usage while catching up\n\nOPTIONAL but encouraged:\n- A \"Scenarios\" or \"Sensitivity\" sheet showing consequences of reducing days without demand drop and/or alternative pacing.\n\nSCORING (STRUCTURE ONLY):\n- 1.0: Excel present with all required sheets/columns/sections AND a separate 2\u20135 sentence manager summary file. Plan starts at week 4.\n- 0.9: Excel meets all required structure, but the manager summary is included as a clearly labeled sheet (not a separate file). Plan starts at week 4.\n- 0.5: Excel present but missing one required sheet/section/column OR the plan does not clearly start at week 4 OR the manager summary is missing.\n- 0.0: Not an Excel file, or missing multiple required sheets/sections, or structure too vague to verify.\n\nDo NOT check calculation correctness or content quality. Only verify presence, format, and structural completeness to enable later verification.", "expectation": "A well-structured Excel model with Catch-Up Plan, Assumptions & Rules, and Summary sheets, starting at week 4, plus a short separate manager summary document."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Quantitative Correctness and Consistency", "description": "Code-based verification of plan logic given the mandated structure. Flexible matching and tolerant numerical checks leveraging the required shape.", "is_required": false, "max_points": 4.5, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Plan Sheets and Columns (Flexible Match)", "description": "Verify presence of key sheets and columns using tolerant matching to enable subsequent checks.", "weight": 0.8, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        out = context.get_primary_output()\n        if not out or not out.is_spreadsheet:\n            return 0.0, \"Primary output not a spreadsheet.\"\n        # Get sheet names\n        xlf = pd.ExcelFile(context.files.get_path(out.id))\n        sheet_names = [str(s) for s in xlf.sheet_names]\n        low_sheets = [s.lower() for s in sheet_names]\n        # Find plan sheet\n        def find_sheet(cands):\n            for i, s in enumerate(low_sheets):\n                if any(c in s for c in cands):\n                    return sheet_names[i]\n            return None\n        plan_sheet = find_sheet([\"catch\", \"plan\"])\n        if plan_sheet is None:\n            plan_sheet = find_sheet([\"capacity\", \"plan\"]) or find_sheet([\"production\", \"plan\"]) or find_sheet([\"week 4\", \"plan\"]) or find_sheet([\"plan\"])\n        if plan_sheet is None:\n            return 0.0, \"No plan sheet found.\"\n        df = pd.read_excel(xlf, sheet_name=plan_sheet)\n        cols = [str(c).strip() for c in df.columns]\n        low = [c.lower() for c in cols]\n        # Helper to find a column by keywords\n        def has_col(*keys):\n            for i, c in enumerate(low):\n                if all(k in c for k in keys):\n                    return True\n            return False\n        checks = {\n            'week': any(('week' in c) or ('wk' in c) for c in low),\n            'demand': has_col('demand') or has_col('req') or has_col('load'),\n            'days_per_week': (has_col('days','week') or has_col('day','week') or has_col('days')),\n            'shift_hours': (has_col('shift','hour') or has_col('hours','week') or has_col('shift')),\n            'std_capacity': (has_col('standard','capacity') or has_col('capacity') or has_col('total','capacity')),\n            'weekly_balance': ('balance' in ' '.join(low)) or has_col('weekly','balance'),\n            'cumulative': (has_col('cumulative') or has_col('backlog') or has_col('buffer')),\n        }\n        score = sum(1.0 if v else 0.0 for v in checks.values())/7.0\n        return score * 0.8, f\"Found columns: {checks} (sheet: {plan_sheet})\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Identity and Bounds Checks per Row", "description": "Verify Days per Week in [4,6], Shift Hours \u2248 Days\u00d710, Standard Capacity \u2248 Days\u00d730, and Weekly Balance \u2248 Capacity \u2212 Demand (tolerant).", "weight": 1.2, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        out = context.get_primary_output()\n        if not out or not out.is_spreadsheet:\n            return 0.0\n        xlf = pd.ExcelFile(context.files.get_path(out.id))\n        # Find plan sheet\n        def find_sheet(names):\n            for s in xlf.sheet_names:\n                sl = str(s).lower()\n                if any(k in sl for k in names):\n                    return s\n            return xlf.sheet_names[0]\n        plan_sheet = find_sheet([\"catch\", \"plan\"]) or find_sheet([\"plan\"]) or find_sheet([\"capacity\"]) \n        df = pd.read_excel(xlf, sheet_name=plan_sheet)\n        # Normalize columns\n        cols = {str(c).strip().lower(): c for c in df.columns}\n        def pick(*alts):\n            for a in alts:\n                for k in cols:\n                    if all(t in k for t in a):\n                        return cols[k]\n            return None\n        col_week = pick((\"week\",))\n        col_demand = pick((\"demand\",), (\"req\",), (\"load\",))\n        col_days = pick((\"day\",\"week\"), (\"days\",))\n        col_shift = pick((\"shift\",\"hour\"), (\"hours\",\"week\"), (\"shift\",))\n        col_cap = pick((\"standard\",\"capacity\"), (\"total\",\"capacity\"), (\"capacity\",))\n        col_bal = pick((\"weekly\",\"balance\"), (\"balance\",))\n        req = [col_demand, col_days, col_shift, col_cap]\n        if any(c is None for c in req):\n            return 0.0, \"Missing key numeric columns for identity checks.\"\n        df2 = df.copy()\n        # Coerce numerics\n        for c in [col_demand, col_days, col_shift, col_cap] + ([col_bal] if col_bal else []):\n            if c is None: continue\n            df2[c] = pd.to_numeric(df2[c], errors='coerce')\n        df2 = df2.dropna(subset=[col_demand, col_days, col_shift, col_cap])\n        if len(df2) == 0:\n            return 0.0, \"No numeric rows.\"\n        tol_shift = 0.6\n        tol_cap = 1.2\n        tol_bal = 2.0\n        checks = []\n        for _, r in df2.iterrows():\n            d = r[col_days]\n            sh = r[col_shift]\n            cap = r[col_cap]\n            dem = r[col_demand]\n            ok_days = (4 - 1e-6) <= d <= (6 + 1e-6)\n            ok_shift = (abs(sh - d*10) <= tol_shift)\n            ok_cap = (abs(cap - d*30) <= tol_cap)\n            ok_bal = True\n            if col_bal and pd.notna(r[col_bal]):\n                bal = r[col_bal]\n                # Accept either definition: cap - dem or dem - cap\n                ok_bal = (abs(bal - (cap - dem)) <= tol_bal) or (abs(bal - (dem - cap)) <= tol_bal)\n            checks.append(ok_days and ok_shift and ok_cap and ok_bal)\n        frac = sum(checks)/len(checks)\n        return frac * 1.2, f\"Row identity pass rate: {frac:.2%}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Backlog Evolution and Catch-Up", "description": "Verify cumulative backlog calculation aligns with plan and that backlog reduces toward \u2264 0. Uses starting backlog 438.81 if not found on Assumptions sheet.", "weight": 1.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        out = context.get_primary_output()\n        if not out or not out.is_spreadsheet:\n            return 0.0\n        xlf = pd.ExcelFile(context.files.get_path(out.id))\n        # Helper: find sheets by keywords\n        def find_sheet(keys):\n            for s in xlf.sheet_names:\n                sl = str(s).lower()\n                if all(k in sl for k in keys):\n                    return s\n            for s in xlf.sheet_names:\n                sl = str(s).lower()\n                if any(k in sl for k in keys):\n                    return s\n            return None\n        plan_sheet = find_sheet([\"catch\"]) or find_sheet([\"plan\"]) or xlf.sheet_names[0]\n        df = pd.read_excel(xlf, sheet_name=plan_sheet)\n        cols = {str(c).strip().lower(): c for c in df.columns}\n        def pick(*alts):\n            for a in alts:\n                for k in cols:\n                    if all(t in k for t in a):\n                        return cols[k]\n            return None\n        col_demand = pick((\"demand\",), (\"req\",), (\"load\",))\n        col_cap = pick((\"standard\",\"capacity\"), (\"total\",\"capacity\"), (\"capacity\",))\n        col_cum = pick((\"cumulative\",), (\"backlog\",), (\"buffer\",))\n        if col_demand is None or col_cap is None:\n            return 0.0, \"Missing demand/capacity columns.\"\n        df2 = df.copy()\n        for c in [col_demand, col_cap] + ([col_cum] if col_cum else []):\n            df2[c] = pd.to_numeric(df2[c], errors='coerce')\n        df2 = df2.dropna(subset=[col_demand, col_cap])\n        # Starting backlog from assumptions or default\n        start_backlog = 438.81\n        # Try to read assumptions sheet for a number near that or labeled\n        as_sheet = find_sheet([\"assumption\"]) or find_sheet([\"method\"]) or None\n        try:\n            if as_sheet:\n                as_df = pd.read_excel(xlf, sheet_name=as_sheet, header=None)\n                # Search labeled entries\n                candidate_vals = []\n                for i in range(min(200, as_df.size)):\n                    pass\n                for _, row in as_df.iterrows():\n                    for i, v in row.items():\n                        sv = str(v).lower()\n                        # look for lines indicating starting backlog\n                        if any(k in sv for k in [\"starting backlog\",\"past due\",\"week 4\",\"start backlog\",\"backlog at week 4\"]):\n                            # try to grab a number in the row\n                            nums = re.findall(r\"[-+]?[0-9]*\\.?[0-9]+\", ' '.join(map(str, row.values)))\n                            for n in nums:\n                                try:\n                                    candidate_vals.append(float(n))\n                                except:\n                                    pass\n                # fallback: scan all numbers and pick closest to 438.81\n                if not candidate_vals:\n                    nums = []\n                    for v in as_df.values.flatten():\n                        try:\n                            nums.append(float(v))\n                        except:\n                            pass\n                    if nums:\n                        candidate_vals = nums\n                if candidate_vals:\n                    # choose the value closest to 438.81 if any over 100\n                    vals_over_50 = [v for v in candidate_vals if abs(v) >= 50]\n                    pool = vals_over_50 or candidate_vals\n                    start_backlog = sorted(pool, key=lambda x: abs(abs(x) - 438.81))[0]\n        except Exception:\n            pass\n        # Compute cumulative backlog two ways depending on sign convention\n        bal_series = df2[col_cap] - df2[col_demand]\n        cum1 = start_backlog + bal_series.cumsum()\n        cum2 = start_backlog + (-bal_series).cumsum()\n        score = 0.0\n        feedback = []\n        if col_cum is not None and df2[col_cum].notna().any():\n            y = df2[col_cum].dropna().reset_index(drop=True)\n            y1 = cum1.iloc[:len(y)]\n            y2 = cum2.iloc[:len(y)]\n            err1 = float(np.nanmean(np.abs(y1 - y)))\n            err2 = float(np.nanmean(np.abs(y2 - y)))\n            err = min(err1, err2)\n            # map error to score: <=3 -> full, <=10 -> partial, else low\n            if err <= 3:\n                score += 0.7\n            elif err <= 10:\n                score += 0.4\n            else:\n                score += 0.1\n            feedback.append(f\"Cumulative alignment error ~{err:.2f}\")\n        else:\n            # no provided cumulative: partial credit if computed backlog seems to trend to <=0\n            pass\n        # Catch-up check\n        caught1 = (cum1 <= 0).any()\n        caught2 = (cum2 <= 0).any()\n        if caught1 or caught2:\n            score += 0.3\n            feedback.append(\"Catch-up achievable (backlog <= 0 detected).\")\n        else:\n            feedback.append(\"No catch-up point (backlog never <= 0) detected.\")\n        score = min(score, 1.0)\n        return score * 1.0, \"; \".join(feedback) if feedback else \"Backlog checks complete.\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Policy Transitions After Catch-Up", "description": "Check that after backlog reaches \u2264 0, the plan transitions to 5-day weeks and eventually to 4-day regular time.", "weight": 1.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        out = context.get_primary_output()\n        if not out or not out.is_spreadsheet:\n            return 0.0\n        xlf = pd.ExcelFile(context.files.get_path(out.id))\n        # Find plan sheet\n        def find_sheet(keys):\n            for s in xlf.sheet_names:\n                sl = str(s).lower()\n                if any(k in sl for k in keys):\n                    return s\n            return xlf.sheet_names[0]\n        plan_sheet = find_sheet([\"catch\",\"plan\"]) or find_sheet([\"plan\"]) or xlf.sheet_names[0]\n        df = pd.read_excel(xlf, sheet_name=plan_sheet)\n        cols = {str(c).strip().lower(): c for c in df.columns}\n        def pick(*alts):\n            for a in alts:\n                for k in cols:\n                    if all(t in k for t in a):\n                        return cols[k]\n            return None\n        col_demand = pick((\"demand\",), (\"req\",), (\"load\",))\n        col_cap = pick((\"standard\",\"capacity\"), (\"total\",\"capacity\"), (\"capacity\",))\n        col_days = pick((\"day\",\"week\"), (\"days\",))\n        if col_demand is None or col_cap is None or col_days is None:\n            return 0.0, \"Missing key columns.\"\n        df2 = df[[col_demand, col_cap, col_days]].copy()\n        df2[col_demand] = pd.to_numeric(df2[col_demand], errors='coerce')\n        df2[col_cap] = pd.to_numeric(df2[col_cap], errors='coerce')\n        df2[col_days] = pd.to_numeric(df2[col_days], errors='coerce')\n        df2 = df2.dropna()\n        if len(df2) == 0:\n            return 0.0, \"No usable rows.\"\n        # Compute backlog cumsum with starting backlog 438.81 (sign choice that best indicates catch-up)\n        start_backlog = 438.81\n        bal = df2[col_cap] - df2[col_demand]\n        cum1 = start_backlog + bal.cumsum()\n        cum2 = start_backlog + (-bal).cumsum()\n        cum = cum1 if (cum1 <= 0).any() else cum2\n        if not (cum <= 0).any():\n            return 0.3, \"No catch-up; transitions cannot be verified.\"\n        idx_catch = int(np.argmax((cum <= 0).values))\n        days_after = df2[col_days].iloc[idx_catch:idx_catch+4].values if len(df2) > idx_catch else np.array([])\n        ever_5 = np.any(days_after <= 5) if days_after.size>0 else False\n        ever_4 = np.any(df2[col_days].iloc[idx_catch:].values == 4)\n        score = 0.0\n        fb = []\n        if ever_5:\n            score += 0.5\n            fb.append(\"Transition to 5-day weeks detected after catch-up.\")\n        else:\n            fb.append(\"No 5-day transition within ~4 weeks after catch-up.\")\n        if ever_4:\n            score += 0.5\n            fb.append(\"Eventual return to 4-day regular time detected.\")\n        else:\n            fb.append(\"No eventual return to 4-day regular time detected.\")\n        return score * 1.0, \"; \".join(fb)\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Plan Starts at Week 4", "description": "Confirm the planning horizon clearly starts at week 4.", "weight": 0.5, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    try:\n        out = context.get_primary_output()\n        if not out or not out.is_spreadsheet:\n            return 0.0\n        xlf = pd.ExcelFile(context.files.get_path(out.id))\n        # Find plan sheet\n        def find_sheet(names):\n            for s in xlf.sheet_names:\n                sl = str(s).lower()\n                if any(n in sl for n in names):\n                    return s\n            return xlf.sheet_names[0]\n        plan_sheet = find_sheet([\"catch\",\"plan\"]) or find_sheet([\"plan\"]) or xlf.sheet_names[0]\n        df = pd.read_excel(xlf, sheet_name=plan_sheet)\n        cols = {str(c).strip().lower(): c for c in df.columns}\n        week_col = None\n        for k, v in cols.items():\n            if (\"week\" in k) or (\"wk\" in k):\n                week_col = v\n                break\n        if week_col is None:\n            return 0.0, \"No week column.\"\n        w = df[week_col].astype(str).str.lower().str.extract(r\"(\\d+)\")[0]\n        w = pd.to_numeric(w, errors='coerce').dropna()\n        if len(w)==0:\n            return 0.0, \"No numeric week identifiers.\"\n        # Accept either explicit first row==4 or inclusion of 4 near the start\n        first_is_4 = (w.iloc[0] == 4)\n        contains_4 = (w == 4).any()\n        if first_is_4:\n            return 0.5, \"First plan row is week 4.\"\n        elif contains_4:\n            return 0.3, \"Week 4 present but not first row.\"\n        else:\n            return 0.0, \"Week 4 not found.\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3: Professional Quality and Managerial Usefulness", "description": "LLM-based assessment of presentation quality, clarity, and managerial value of the spreadsheet and the brief summary, including alignment between the two.", "is_required": false, "max_points": 1.5, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Spreadsheet Professionalism and Clarity", "description": "Assess whether the Excel workbook is professional, readable, and decision-ready for a first-line supervisor context.", "weight": 0.8, "judge_prompt": "Assess the EXCEL workbook only (not calculation correctness). Consider:\n- Clear, professional labeling of sheets and columns (units like standard hours are evident).\n- Readable tabular layout with obvious headers and consistent number formatting.\n- Assumptions & Rules are understandable and self-contained (parameters are justified in brief notes).\n- Summary is concise and highlights: weeks to catch up, when to move to 5-day weeks, when to return to 4-day regular time, and notes on overtime usage.\n\nSCORING:\n- 0.8: Highly professional and clear; easy for a supervisor/manager to act on.\n- 0.5: Generally clear with minor issues (some labels unclear or minor formatting confusion).\n- 0.2: Barely readable; labels/units unclear; reviewer must infer intent.\n- 0.0: Disorganized or confusing; difficult to interpret structure.", "expectation": "A clean, well-labeled workbook with an understandable assumptions sheet and a crisp summary sheet."}, {"type": "llm_judge", "name": "Manager Summary Quality and Alignment", "description": "Evaluate the brief manager summary (2\u20135 sentences) for clarity, brevity, and alignment with the Excel Summary sheet.", "weight": 0.7, "judge_prompt": "Review the short manager summary (PDF/DOCX/MD) together with the Excel file's Summary sheet. Check:\n- Length is 2\u20135 sentences and suitable for an email.\n- It states the number of weeks to catch up and the planned transitions (6\u21925 days, then 5\u21924 days) with approximate timing.\n- It briefly notes overtime usage during catch-up and the definition of \"caught up\".\n- Numbers and timing are consistent with the Excel \"Summary\" sheet (allow minor rounding); no contradictions.\n\nSCORING:\n- 0.7: Clear, concise, action-oriented, and consistent with the Excel Summary.\n- 0.4: Mostly clear but missing one element or minor inconsistencies.\n- 0.2: Vague, too long/short, or multiple inconsistencies.\n- 0.0: Missing or not aligned with the Excel content.", "expectation": "A crisp 2\u20135 sentence email-ready note that mirrors the key outcomes shown in the workbook Summary."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "ff85ee58-bc9f-4aa2-806d-87edeabb1b81", "rubric": {"category_name": "Audio Integration and Final Mix \u2014 \u201cTavarua\u201d (Experimental Rock Interlude)", "rationale": "Self-documenting rubric tailored for audio engineering. Stage 1 forces a structured, verifiable deliverable set beyond the audio file: a Technical Specs CSV, a Timing Alignment Log CSV, a Mix Notes document, and visual evidence images. This makes Stage 2 feasible with deterministic code checks for format, timing, and loudness targets. Stage 3 uses an LLM judge to assess the professionalism and plausibility of the documented process and mix decisions without requiring audio decoding.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Structured Deliverables Gate", "description": "LLM-only gate verifying the presence and structure of required deliverables enabling verification: final WAV, specs CSV, timing alignment CSV, mix notes document, and visual evidence images. Only check presence/structure, not correctness.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Deliverable Shape and Structure Check", "description": "Verify the candidate produced a complete, structured verification pack that makes subsequent checks trivial.", "weight": 4.0, "judge_prompt": "You are validating the SHAPE (not correctness) of the submission for an audio mixing task. Examine ALL submitted files/resources. Confirm the following deliverables exist with the described structure. Be flexible with exact names but strict about categories and required columns/sections.\n\nRequired Core Deliverables:\n1) Final Mix Audio (WAV):\n   - A stereo WAV file representing the final mix of \u201cTavarua\u201d.\n   - Should be clearly identifiable as the final mix (e.g., name contains \u201cFINAL\u201d or \u201cTAVARUA\u201d).\n   - Must be 24-bit, 48 kHz stereo. You do NOT need to verify values are correct here, only that the file appears to be the final WAV deliverable.\n\n2) Technical Specs CSV (or XLSX):\n   - A tabular file with at least these columns (flexible naming ok):\n     \u2022 file_name\n     \u2022 sample_rate_hz (aka sample_rate/sr_hz)\n     \u2022 bit_depth (e.g., 24)\n     \u2022 channels (e.g., 2)\n     \u2022 duration_sec\n     \u2022 integrated_lufs (aka lufs_i/loudness_integrated)\n     \u2022 true_peak_dbfs (aka true_peak/dBTP)\n     \u2022 loudness_range_lu (aka LRA)\n     \u2022 measurement_standard (e.g., EBU R128 or ITU-R BS.1770)\n     \u2022 meter_tool and/or meter_version\n     \u2022 analysis_date\n   - Must contain one row referring to the final mix audio file.\n\n3) Timing Alignment Log CSV (or XLSX):\n   - A tabular file documenting sax resync with columns (flexible naming ok):\n     \u2022 event (or marker/section)\n     \u2022 reference_time_sec (from MP3 reference)\n     \u2022 raw_sax_time_sec\n     \u2022 applied_offset_sec\n     \u2022 aligned_time_sec\n     \u2022 target_grid_time_sec (1/8-note grid at 50 BPM)\n     \u2022 error_sec (difference from target grid)\n   - Should have multiple rows demonstrating alignment across the sax passage.\n\n4) Mix Notes Document (PDF or DOCX):\n   - A professionally formatted document with clear section headers including:\n     \u2022 Sources & Files Used (lists the three provided sources)\n     \u2022 Resync Methodology (how the sax was aligned using the MP3 reference)\n     \u2022 Editing & Tightening (following 1/8-note at 50 BPM with \u00b11/16-note tolerance)\n     \u2022 FX Chain & Settings (reverb/delay details, plugin chain, key parameters)\n     \u2022 Loudness & Dynamics Strategy (how targets were met)\n     \u2022 QA Checklist & Versioning (what was checked; version/date)\n   - Minimum length: at least 1 full page; clear headings required.\n\n5) Visual Evidence (Images):\n   - At least two images (PNG/JPG):\n     \u2022 A waveform overview annotated or clearly showing the sax region.\n     \u2022 A spectrogram (or spectrum view) with time axis; sax entry/region indicated.\n\nScoring (structure only; do not judge audio quality or value correctness):\n- 4.0: All five categories present. Specs CSV has key columns; Timing Log has required columns; Mix Notes has all sections; at least two distinct images (waveform + spectrogram). Final WAV clearly identified.\n- 3.0: Core items present (Final WAV, Specs CSV with key columns, Timing Log with columns, Mix Notes doc) but missing one supporting element (e.g., only one image) OR minor structural omission (one missing column OR one missing Mix Notes section).\n- 2.0: Missing one core deliverable (e.g., no Specs CSV or no Timing Log) OR multiple structural omissions across docs.\n- 1.0: Only the audio file and minimal documentation appear; most structured items missing.\n- 0.0: No valid final WAV OR submission lacks any of the structured items entirely.\n\nOnly evaluate presence/structure, not the correctness of technical values or timing. Be lenient on naming but strict about categories and tabular/section structure.", "expectation": "A complete verification pack: final WAV + specs CSV + timing log CSV + mix notes PDF/DOCX + two images (waveform and spectrogram)."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification of Specs, Timing, and Documentation", "description": "Deterministic code checks using the structured artifacts enforced by Stage 1. Verifies technical targets, timing tolerance, and basic documentation references.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Specs CSV Validity and Target Checks", "description": "Validate that the Technical Specs table claims the correct format and loudness/peak targets for the final mix.", "weight": 1.8, "code": "import re\nimport json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    # Helper: find primary audio output path\n    primary = context.get_primary_output()\n    final_audio_name = None\n    if primary is not None:\n        try:\n            pth = context.files.get_path(primary.id)\n            final_audio_name = pth.name\n        except Exception:\n            final_audio_name = None\n    \n    # Find a candidate specs CSV/XLSX\n    outputs = context.get_all_outputs() or []\n\n    def try_read_specs(resource):\n        # Return DataFrame or None\n        try:\n            path = context.files.get_path(resource.id)\n            suffix = path.suffix.lower()\n            if suffix == '.csv':\n                df = context.files.read_csv(resource.id)\n                return df\n            if suffix in ['.xlsx', '.xlsm', '.xls']:\n                # Try first sheet by default\n                try:\n                    df = context.files.read_excel(resource.id)\n                except Exception:\n                    # Try a sheet named 'Specs' as fallback\n                    try:\n                        df = context.files.read_excel(resource.id, sheet_name='Specs')\n                    except Exception:\n                        return None\n                return df\n        except Exception:\n            return None\n        return None\n\n    candidate_specs = None\n    for r in outputs:\n        try:\n            path = context.files.get_path(r.id)\n            name = path.name.lower()\n            if path.suffix.lower() in ['.csv', '.xlsx', '.xlsm', '.xls']:\n                if any(k in name for k in ['spec', 'tech', 'loud', 'meter']):\n                    df = try_read_specs(r)\n                    if isinstance(df, pd.DataFrame) and len(df.columns) >= 3 and len(df) >= 1:\n                        candidate_specs = (r, df)\n                        break\n        except Exception:\n            continue\n    # If not found by name, fall back to any CSV/XLSX containing integrated loudness\n    if candidate_specs is None:\n        for r in outputs:\n            try:\n                path = context.files.get_path(r.id)\n                if path.suffix.lower() in ['.csv', '.xlsx', '.xlsm', '.xls']:\n                    df = try_read_specs(r)\n                    if isinstance(df, pd.DataFrame):\n                        cols = [c.lower() for c in df.columns.astype(str)]\n                        if any('lufs' in c or 'loudness' in c for c in cols):\n                            candidate_specs = (r, df)\n                            break\n            except Exception:\n                continue\n\n    if candidate_specs is None:\n        return 0.0, 'No Technical Specs CSV/XLSX found.'\n\n    _, df = candidate_specs\n    # Normalize columns\n    df_cols = {c.lower().strip(): c for c in df.columns}\n\n    def get_col(options):\n        for opt in options:\n            if opt in df_cols:\n                return df_cols[opt]\n        # partial contains\n        for c_low, c_orig in df_cols.items():\n            for opt in options:\n                if opt in c_low:\n                    return c_orig\n        return None\n\n    col_fname = get_col(['file_name','filename','file'])\n    col_sr = get_col(['sample_rate_hz','sample_rate','sr_hz','sr'])\n    col_bd = get_col(['bit_depth','bitdepth','depth_bits'])\n    col_ch = get_col(['channels','channel_count','num_channels'])\n    col_dur = get_col(['duration_sec','duration','length_sec'])\n    col_lufs = get_col(['integrated_lufs','lufs_i','loudness_integrated','integrated loudness'])\n    col_tp = get_col(['true_peak_dbfs','true_peak','dbtp'])\n    col_lra = get_col(['loudness_range_lu','lra'])\n\n    # Select the row pertaining to the final audio if possible\n    row = None\n    if col_fname and final_audio_name is not None:\n        try:\n            mask = df[col_fname].astype(str).str.lower().str.contains(final_audio_name.lower())\n            if mask.any():\n                row = df[mask].iloc[0]\n        except Exception:\n            row = None\n    if row is None:\n        row = df.iloc[0]\n\n    score = 0.0\n    feedback_parts = []\n\n    # Check sample rate 48000 Hz (\u00b11 Hz)\n    try:\n        sr_val = float(row[col_sr]) if col_sr else None\n        if sr_val is not None and abs(sr_val - 48000) <= 1:\n            score += 0.3\n        else:\n            feedback_parts.append(f'sample_rate not 48000: {sr_val}')\n    except Exception:\n        feedback_parts.append('sample_rate missing')\n\n    # Check bit depth 24\n    try:\n        bd_val = float(row[col_bd]) if col_bd else None\n        if bd_val is not None and abs(bd_val - 24) < 0.51:\n            score += 0.3\n        else:\n            feedback_parts.append(f'bit_depth not 24: {bd_val}')\n    except Exception:\n        feedback_parts.append('bit_depth missing')\n\n    # Check channels == 2\n    try:\n        ch_val = float(row[col_ch]) if col_ch else None\n        if ch_val is not None and int(round(ch_val)) == 2:\n            score += 0.2\n        else:\n            feedback_parts.append(f'channels not 2: {ch_val}')\n    except Exception:\n        feedback_parts.append('channels missing')\n\n    # Check integrated loudness -16 \u00b1 1 LUFS => between -17 and -15\n    try:\n        lufs_val = float(row[col_lufs]) if col_lufs else None\n        if lufs_val is not None and (-17.0 <= lufs_val <= -15.0):\n            score += 0.5\n        else:\n            feedback_parts.append(f'integrated_lufs out of range: {lufs_val}')\n    except Exception:\n        feedback_parts.append('integrated_lufs missing')\n\n    # Check true peak not exceeding -0.1 dBFS (<= -0.1)\n    try:\n        tp_val = float(row[col_tp]) if col_tp else None\n        if tp_val is not None and tp_val <= -0.1:\n            score += 0.5\n        else:\n            feedback_parts.append(f'true_peak_dbfs above -0.1: {tp_val}')\n    except Exception:\n        feedback_parts.append('true_peak_dbfs missing')\n\n    # Clamp to weight\n    score = float(min(score, 1.8))\n    feedback = '; '.join(feedback_parts) if feedback_parts else 'OK'\n    return score, feedback"}, {"type": "code", "name": "Timing Alignment Within Tolerance", "description": "Verify that the Timing Alignment Log shows sax edits aligned to the 1/8-note grid at 50 BPM within \u00b11/16-note (<= 0.3 s error).", "weight": 1.2, "code": "import re\nimport json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs() or []\n\n    def try_read_table(resource):\n        try:\n            path = context.files.get_path(resource.id)\n            suffix = path.suffix.lower()\n            if suffix == '.csv':\n                return context.files.read_csv(resource.id)\n            if suffix in ['.xlsx', '.xlsm', '.xls']:\n                try:\n                    return context.files.read_excel(resource.id)\n                except Exception:\n                    try:\n                        return context.files.read_excel(resource.id, sheet_name='Alignment')\n                    except Exception:\n                        return None\n        except Exception:\n            return None\n        return None\n\n    cand = None\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            name = p.name.lower()\n            if p.suffix.lower() in ['.csv', '.xlsx', '.xlsm', '.xls'] and any(k in name for k in ['align', 'timing', 'grid', 'edit']):\n                df = try_read_table(r)\n                if isinstance(df, pd.DataFrame) and len(df) >= 1:\n                    cand = df\n                    break\n        except Exception:\n            continue\n    if cand is None:\n        # fallback: any table with columns that look like alignment\n        for r in outputs:\n            df = try_read_table(r)\n            if isinstance(df, pd.DataFrame):\n                cols = [c.lower() for c in df.columns.astype(str)]\n                if any('aligned' in c for c in cols) and any('grid' in c for c in cols):\n                    cand = df\n                    break\n    if cand is None:\n        return 0.0, 'No Timing Alignment Log found.'\n\n    df = cand\n    cols_map = {c.lower(): c for c in df.columns}\n    def get_col(options):\n        for opt in options:\n            if opt in cols_map:\n                return cols_map[opt]\n        for k,v in cols_map.items():\n            for opt in options:\n                if opt in k:\n                    return v\n        return None\n\n    c_err = get_col(['error_sec','timing_error','err_sec'])\n    c_aligned = get_col(['aligned_time_sec','aligned'])\n    c_grid = get_col(['target_grid_time_sec','grid_time_sec','grid'])\n\n    if c_err is not None and c_err in df.columns:\n        try:\n            err = df[c_err].astype(float).abs().max()\n            max_err = float(err)\n        except Exception:\n            max_err = None\n    else:\n        # compute from aligned and grid times if available\n        max_err = None\n        if c_aligned and c_grid and c_aligned in df.columns and c_grid in df.columns:\n            try:\n                errs = (df[c_aligned].astype(float) - df[c_grid].astype(float)).abs()\n                max_err = float(errs.max())\n            except Exception:\n                max_err = None\n\n    if max_err is None:\n        return 0.0, 'Alignment columns present but could not compute max error.'\n\n    # 1/16 note at 50 BPM is 0.3 s tolerance\n    # Full credit <=0.3; partial if <=0.4 or <=0.6\n    if max_err <= 0.3:\n        return 1.2, f'Max error {max_err:.3f}s within tolerance.'\n    elif max_err <= 0.4:\n        return 0.8, f'Max error {max_err:.3f}s slightly above tolerance.'\n    elif max_err <= 0.6:\n        return 0.5, f'Max error {max_err:.3f}s high but plausible.'\n    else:\n        return 0.0, f'Max error {max_err:.3f}s exceeds tolerance.'"}, {"type": "code", "name": "Documentation Mentions Tempo, Grid, and FX", "description": "Confirm Mix Notes document references the required tempo/grid and time-based FX; checks for 50 BPM, 1/8th grid, \u00b11/16th tolerance, and use of reverb/delay.", "weight": 0.6, "code": "import re\nimport json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs() or []\n\n    def read_doc_text(r):\n        try:\n            path = context.files.get_path(r.id)\n            if path.suffix.lower() == '.pdf':\n                return context.files.read_pdf_text(r.id)\n            if path.suffix.lower() in ['.docx']:\n                return context.files.read_docx_text(r.id)\n        except Exception:\n            return ''\n        return ''\n\n    text = ''\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            name = p.name.lower()\n            if p.suffix.lower() in ['.pdf', '.docx'] and any(k in name for k in ['mix', 'notes', 'method', 'verification', 'report']):\n                t = read_doc_text(r)\n                if t and len(t.strip()) > 0:\n                    text = t\n                    break\n        except Exception:\n            continue\n\n    if not text:\n        # fallback: any doc\n        for r in outputs:\n            try:\n                p = context.files.get_path(r.id)\n                if p.suffix.lower() in ['.pdf', '.docx']:\n                    text = read_doc_text(r)\n                    if text:\n                        break\n            except Exception:\n                continue\n\n    if not text:\n        return 0.0, 'No Mix Notes document found/readable.'\n\n    tl = text.lower()\n    score = 0.0\n    fb = []\n\n    # 50 BPM\n    if '50 bpm' in tl or re.search(r'bpm\\s*[:=]?\\s*50', tl):\n        score += 0.15\n    else:\n        fb.append('Missing explicit 50 BPM mention')\n\n    # 1/8 note grid mention\n    if '1/8' in tl or 'eighth' in tl or 'eighth-note' in tl or 'eighth note' in tl:\n        score += 0.15\n    else:\n        fb.append('Missing 1/8-note grid mention')\n\n    # \u00b11/16 tolerance mention\n    if '1/16' in tl or 'sixteenth' in tl or 'sixteenth-note' in tl or 'sixteenth note' in tl:\n        score += 0.15\n    else:\n        fb.append('Missing \u00b11/16-note tolerance mention')\n\n    # Reverb and delay mentioned\n    has_reverb = 'reverb' in tl\n    has_delay = 'delay' in tl or 'echo' in tl\n    if has_reverb and has_delay:\n        score += 0.15\n    else:\n        fb.append('Missing reverb and/or delay discussion')\n\n    score = float(min(max(score, 0.0), 0.6))\n    return score, ('; '.join(fb) if fb else 'OK')"}, {"type": "code", "name": "Presence of Visual Evidence Files", "description": "Check that at least two images exist: one waveform overview and one spectrogram/spectrum image.", "weight": 0.4, "code": "import re\nimport json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs() or []\n    waveform_imgs = 0\n    spectro_imgs = 0\n    total_imgs = 0\n\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            suffix = p.suffix.lower()\n            if suffix in ['.png', '.jpg', '.jpeg']:\n                total_imgs += 1\n                name = p.name.lower()\n                if any(k in name for k in ['wave', 'waveform']):\n                    waveform_imgs += 1\n                if any(k in name for k in ['spectro', 'spectrogram', 'spectrum']):\n                    spectro_imgs += 1\n        except Exception:\n            continue\n\n    if waveform_imgs >= 1 and spectro_imgs >= 1:\n        return 0.4, 'Waveform and spectrogram images present.'\n    if total_imgs >= 2:\n        return 0.2, 'Two images present but not clearly waveform+spectrogram.'\n    if total_imgs >= 1:\n        return 0.1, 'Only one image present.'\n    return 0.0, 'No visual evidence images found.'"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Cohesion (LLM)", "description": "Holistic LLM assessment of the documented process, plausibility of mix decisions, and how well the sax is likely integrated given the described FX chain, timing edits, and loudness strategy.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professionalism and Cohesive Mix Strategy", "description": "Assess the Mix Notes document and visuals for professionalism, plausibility, and alignment with the task brief (experimental interlude, tasteful reverb/delay, clarity, immersion, and loudness strategy).", "weight": 2.0, "judge_prompt": "Review the Mix Notes document (PDF/DOCX) and any provided visuals (waveform/spectrogram). Evaluate the professionalism and plausibility of the approach for integrating a virtual sax into an experimental rock interlude. Consider:\n- Resync and Editing: Is the described method for aligning the sax to the MP3 reference and tightening to the 1/8-note grid at 50 BPM (\u00b11/16) clear and technically sound?\n- FX Chain and Spatial Design: Are reverb/delay choices appropriate for an experimental interlude, enhancing width/immersion without likely muddying the existing mix? Are parameters (pre-delay, decay, filters, stereo placement) thoughtfully justified?\n- Loudness and Dynamics: Is the plan to achieve around -16 LUFS integrated (\u00b11 dB) coherent, with attention to true-peak headroom and not over-limiting?\n- Documentation Quality: Are sections complete, clearly written, with tables/settings where relevant? Do visuals support the narrative (markers at sax entries, spectral energy consistent with sax register)?\n\nScoring:\n- 2.0: Highly professional and coherent. Clear, technically plausible alignment/editing, well-justified FX, and a sensible loudness workflow. Visuals support the approach.\n- 1.0: Generally adequate but with gaps or vague justifications; still plausible for the brief.\n- 0.0: Superficial or implausible; lacks clear methodology or contradicts the brief.\n\nYou are not listening to audio; judge based on documentation and visuals only.", "expectation": "A credible, well-structured rationale demonstrating expert alignment, tasteful spatial effects, and a controlled loudness strategy appropriate for the genre."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "46b34f78-6c06-4416-87e2-77b6d8b20ce9", "rubric": {"category_name": "Finance/Insurance \u2014 Energy Desk H1 2025 Strategy (Financial & Investment Analysts)", "rationale": "Pattern C (Mixed): A professional DOCX report with embedded analysis. Stage 1 uses an LLM gate to enforce strict document structure so verification is trivial. Stage 2 mixes code checks (text parsing for constraints, issuers, bond fields, numeric specificity, sources) with LLM cross-checks for alignment and feasibility. Stage 3 assesses overall professional quality and actionability for MD/Director audience.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (STRUCTURE ONLY)", "description": "LLM-only gate verifying the candidate produced a DOCX report (\u226410 pages) with all mandated sections, subsections, and key tables that enable verification. Do NOT judge correctness or quality\u2014only presence and format.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.0, "rules": [{"type": "llm_judge", "name": "Structured DOCX Report Requirement (Gate)", "description": "Check that the output is a DOCX report (\u226410 pages) with exact structural elements required for subsequent verification.", "weight": 3.0, "judge_prompt": "You are validating ONLY the structure/format of the candidate\u2019s output. Do not judge correctness of analysis or quality of writing. Check the following strictly:\n\nFormat requirements:\n- File must be a DOCX (Microsoft Word). Not PDF/Excel/markdown.\n- Length must be 2\u201310 pages inclusive.\n- Professional report formatting with clear section headers and logical layout.\n\nRequired sections and elements (flexible with similar names, but all must be present unless marked optional):\n1) Executive Summary \u2014 must appear on page 1.\n2) Portfolio Constraints and Objectives \u2014 must explicitly restate these constraints and context: $300M portfolio size; current 10% energy-linked bonds; max 20% high-yield (HY) allocation; 3\u20135 year duration for HY bonds; diversification across fixed income products; 5-year total return objective measured in absolute dollars.\n3) Energy Market Overview (Oil and Natural Gas) \u2014 with distinct oil and natural gas subsections (e.g., \u201cOil Market\u201d and \u201cNatural Gas Market\u201d), focusing on 2025 H1 outlook, supply/demand drivers, and volatility context.\n4) Issuer Bond Analyses (exactly two issuers):\n   - One issuer tied to oil, one issuer tied to natural gas.\n   - Each issuer subsection must include:\n     a) A Bond Snapshot table with columns (or very close variants): [Issuer | Ticker/CUSIP | Bond Name | Coupon | Maturity | Rating | Price | Yield-to-Maturity | Spread vs Treasuries | Duration]\n     b) A Thesis/Recommendation paragraph and a Risks paragraph/section.\n5) Trading Strategy (H1 2025) \u2014 concrete trade ideas with entry level/indicative price or spread, size/notional guidance, stop-loss/hedges, and expected P&L logic.\n6) Sales Strategy (H1 2025) \u2014 target client segments, pitch angles, product mix (cash bonds/CDS/options), and a pipeline or call plan.\n7) Risk Management and Scenario Analysis \u2014 market/rate/credit/liquidity risks and stress/scenario framing.\n8) Data Sources and Appendix (optional for appendix) \u2014 list at least 3 public sources (e.g., EIA, IEA, OPEC, CME/ICE, FERC, NOAA, FRED). Appendix may include source excerpts or tables.\n\nScoring (map to the rule\u2019s full weight):\n- 3.0: DOCX format; 2\u201310 pages; ALL required sections present; each issuer has a Bond Snapshot table and Thesis+Risks; oil and gas clearly separated.\n- 2.5: Same as above but missing only the Appendix/source list depth (fewer than 3 sources) OR one minor structural element.\n- 2.0: Missing one core section or one issuer subsection element (e.g., missing Risks or missing the Snapshot table for one issuer).\n- 1.0: Valid DOCX but missing multiple core sections or issuer detail; or oil/gas not clearly separated.\n- 0.0: Not a DOCX, or fewer than 2 pages, or structure so incomplete that verification is impossible.\n\nOnly evaluate presence/format. Do NOT evaluate correctness of data, calculations, or strategy quality.", "expectation": "A \u226410-page DOCX containing all sections, two issuer analyses (one oil, one gas) each with a bond snapshot table and risks, plus trading and sales strategy sections, and sources/appendix."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness, Alignment, Specificity)", "description": "Now that the structure is enforced, verify constraint alignment, presence of critical details, temporal relevance, and evidence linkage. Mix of code rules (deterministic text checks) and LLM rules (reasoned consistency).", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Portfolio Constraints Referenced and Correct", "description": "Verify the report text explicitly references key constraints and context: $300M size, current 10% energy-linked bonds, HY cap 20%, HY duration 3\u20135 years, and diversification across fixed income.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = ''\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n    t = text.lower()\n\n    score = 0\n    weight = 1.0\n    items = 5\n\n    # $300M portfolio size\n    if (re.search(r\"\\$\\s*300\\s*(m|mm|mn|million)\\b\", t) or\n        (\"300\" in t and \"million\" in t and \"portfolio\" in t)):\n        score += 1\n\n    # Current 10% energy-linked bonds\n    if (\"10%\" in t or re.search(r\"10\\s*percent\", t)) and (\"energy\" in t) and (\"bond\" in t):\n        score += 1\n\n    # Max 20% HY\n    if ((\"20%\" in t or re.search(r\"20\\s*percent\", t)) and (\"high-yield\" in t or re.search(r\"\\bhy\\b\", t))):\n        score += 1\n\n    # 3\u20135 year duration for HY bonds\n    if (re.search(r\"(3\\s*[-\u2013]\\s*5|3\\s*to\\s*5)\\s*(year|yr)s?\\s*(duration|matur)\", t)):\n        score += 1\n\n    # Diversification across fixed income products\n    if (\"diversif\" in t and (\"fixed income\" in t or \"across\" in t)):\n        score += 1\n\n    return weight * (score / items)\n"}, {"type": "code", "name": "H1 2025 Timing Relevance", "description": "Confirm the strategy period is clearly H1 2025.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0\n    t = text.lower()\n\n    weight = 0.4\n    full = bool(re.search(r\"h1[^a-z0-9]{0,10}2025\", t) or re.search(r\"first\\s*half\\s*(of\\s*)?2025\", t))\n    partial = (\"2025\" in t and (\"h1\" in t or \"first half\" in t))\n\n    if full:\n        return weight\n    elif partial:\n        return weight * 0.5\n    else:\n        return 0.0\n"}, {"type": "code", "name": "Two Issuer Analyses (Oil and Gas)", "description": "Detect presence of two issuer-focused sections with clear oil and natural gas association.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0\n    t = text.lower()\n    lines = [ln.strip() for ln in t.splitlines() if ln.strip()]\n\n    weight = 0.8\n\n    def near_keyword(lines, key_a, key_b):\n        for i, ln in enumerate(lines):\n            if key_a in ln:\n                window = \" \".join(lines[i:i+3])\n                if key_b in window:\n                    return True\n        return False\n\n    # Heuristics\n    issuer1_2 = (re.search(r\"issuer\\s*1\", t) or re.search(r\"issuer\\s*2\", t))\n    oil_near_issuer = near_keyword(lines, \"issuer\", \"oil\") or near_keyword(lines, \"company\", \"oil\")\n    gas_near_issuer = near_keyword(lines, \"issuer\", \"gas\") or near_keyword(lines, \"company\", \"gas\") or near_keyword(lines, \"issuer\", \"natural gas\")\n\n    if issuer1_2 and oil_near_issuer and gas_near_issuer:\n        return weight\n    elif oil_near_issuer and gas_near_issuer:\n        return weight * 0.875\n    elif ((\"oil\" in t and (\"gas\" in t or \"natural gas\" in t)) and (t.count(\"issuer\") + t.count(\"company\")) >= 2):\n        return weight * 0.5\n    else:\n        return 0.0\n"}, {"type": "code", "name": "Bond Detail Fields Present", "description": "Check for presence of key bond fields across issuer analyses: coupon, maturity, rating, price, yield, spread, duration.", "weight": 0.8, "code": "def evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0\n    t = text.lower()\n\n    terms = [\"coupon\", \"maturity\", \"rating\", \"price\", \"yield\", \"spread\", \"duration\", \"ytm\"]\n    found = 0\n    seen = set()\n    for term in terms:\n        if term in t and term not in seen:\n            seen.add(term)\n            found += 1\n    total_needed = 7  # count toward 7 unique concepts; ytm counts toward yield if missing\n\n    # If 'yield' missing but 'ytm' present, treat as covered\n    if 'yield' not in seen and 'ytm' in seen:\n        found = max(found, 6)  # approximate credit for yield via ytm\n\n    covered = min(found, total_needed)\n    return 0.8 * (covered / total_needed)\n"}, {"type": "code", "name": "Strategy Numeric Specificity (Targets/Sizing/Risk)", "description": "Check for numeric specificity in Trading/Sales Strategy: targets, entries/exits, stop-loss/hedges, bps/%/sizes.", "weight": 0.25, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0\n    t = text.lower()\n\n    weight = 0.25\n    categories = 0\n\n    if re.search(r\"\\b\\d{1,3}\\s?%\\b\", t) or \"bps\" in t:\n        categories += 1\n    if any(k in t for k in [\"target\", \"entry\", \"exit\", \"stop\", \"take profit\", \"tp\", \"sl\"]):\n        categories += 1\n    if re.search(r\"\\$\\s?\\d{1,3}\\s*(m|mm|mn|million)\\b\", t) or re.search(r\"\\bsize\\b|\\bnotional\\b\", t):\n        categories += 1\n    if any(k in t for k in [\"hedge\", \"cds\", \"option\", \"futures\", \"swap\"]):\n        categories += 1\n\n    return weight * min(categories / 4.0, 1.0)\n"}, {"type": "code", "name": "Public Data Sources Listed", "description": "Validate at least two recognized public sources are cited (e.g., EIA, IEA, OPEC, CME/ICE, FERC, NOAA, FRED/BEA/IMF/World Bank). Appendix or Sources section preferred.", "weight": 0.25, "code": "def evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0\n    t = text.lower()\n\n    sources = [\n        \"eia\", \"iea\", \"opec\", \"cme\", \"ice\", \"ferc\", \"noaa\", \"fred\", \"bea\", \"imf\", \"world bank\", \"doe\"\n    ]\n    count = 0\n    for s in sources:\n        if s in t:\n            count += 1\n    if count >= 2:\n        return 0.25\n    elif count == 1:\n        return 0.1\n    else:\n        return 0.0\n"}, {"type": "llm_judge", "name": "Constraint Alignment and Feasibility", "description": "Do proposed trades respect the HY \u226420% cap, 3\u20135 year HY duration, diversification, and plausible position sizing vs a $300M portfolio for H1 2025?", "weight": 0.9, "judge_prompt": "Evaluate whether the recommendations adhere to the explicit portfolio constraints and practical feasibility:\n- HY exposure kept at or below 20% of total portfolio.\n- HY bond selections predominantly within 3\u20135 year maturities/durations.\n- Diversification across fixed income products maintained (not concentrated in a single product).\n- Position sizes are realistic relative to a $300M portfolio and the H1 2025 horizon.\n- Recommendations refer to H1 2025 timing.\n\nScoring (0 to 0.9):\n- 0.9: Fully adherent to all constraints with clear, plausible sizing.\n- 0.6: Generally adherent with minor gaps (e.g., one unclear sizing or maturity detail).\n- 0.3: Several ambiguities or probable breaches.\n- 0.0: Clearly violates constraints or ignores feasibility.\n\nFocus on alignment; do not re-assess structural format.", "expectation": "Trades/allocations demonstrably within HY cap and 3\u20135Y HY duration, diversified, and scaled to a $300M book."}, {"type": "llm_judge", "name": "Analysis\u2013Recommendation Consistency", "description": "Check that market views (oil and gas) clearly support each issuer\u2019s trade thesis, with risks and hedges logically tied to the analysis.", "weight": 0.6, "judge_prompt": "Assess consistency between the Energy Market Overview and the Issuer Bond Analyses and Strategy sections:\n- Do oil and gas outlooks (supply/demand, volatility drivers) explicitly support the two issuer trade theses?\n- Are risks acknowledged, and are hedges/mitigations logically connected to identified risks?\n- Do the trading and sales strategies flow logically from the issuer theses and market outlook?\n\nScoring (0 to 0.6):\n- 0.6: Strong, explicit linkage and coherent risk/hedge logic.\n- 0.4: Generally consistent with minor gaps.\n- 0.2: Weak linkage; recommendations feel disconnected.\n- 0.0: No clear linkage.\n\nDo not judge formatting or page count here.", "expectation": "Clear causal thread from market outlook \u2192 issuer theses \u2192 trades/sales with risks and hedges aligned."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Actionability", "description": "Holistic LLM assessment of presentation quality, strategic value, and audience fit for MDs/Directors on an energy desk.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Overall Professional Quality", "description": "Evaluate clarity, structure, tone, visuals/tables, and actionability for the target audience (MDs/Directors).", "weight": 2.0, "judge_prompt": "Judge the overall professional quality and actionability for a sell-side energy desk audience (MDs/Directors):\n- Clarity and concision suitable for senior stakeholders; prioritization of key takeaways.\n- Cohesive structure, clean formatting, appropriate use of tables/figures.\n- Actionable recommendations with clear next steps for trading and sales teams.\n- Risk awareness and compliance-conscious tone.\n- Adherence to length guidance (\u226410 pages).\n\nScoring (0 to 2.0):\n- 2.0: Highly professional, concise, and action-oriented; excellent presentation.\n- 1.4: Strong overall with minor issues.\n- 0.8: Adequate but notable clarity/organization gaps.\n- 0.3: Low professionalism or weak actionability.\n- 0.0: Unusable quality.\n\nDo not re-judge formal structure already handled in Stage 1.", "expectation": "A polished, executive-ready memo with actionable trade/sales plans and clear tables/charts where appropriate."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "eb54f575-93f9-408b-b9e0-f1208a0b6759", "rubric": {"category_name": "Police Procurement Report: Patrol Rifles (Quantity and Caliber Recommendation)", "rationale": "Pattern C (Mixed): A formal PDF report with structured narrative sections plus an embedded quantitative calculation. Stage 1 (LLM-only) enforces the precise document shape so verification is trivial. Stage 2 mixes code checks (text extraction from the PDF) with an LLM cross-check to verify consistency and plausibility of calculations and caliber justification. Stage 3 assesses professional quality and executive suitability.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2013 Shape Enforcement Gate", "description": "Strictly enforce required PDF format, exact title, and presence of all mandated sections with key structural elements to enable verification. LLM-only per philosophy.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured PDF Report Requirement", "description": "Output must be a professionally formatted PDF with exact title and five required sections containing specific structural elements that enable later verification.", "weight": 4.0, "judge_prompt": "You are evaluating whether the submitted output satisfies strict FORMAT and STRUCTURE requirements for a formal PDF report. Do NOT judge correctness of analysis; only check presence/shape. Be flexible with minor header wording, but required elements must be clearly present.\n\nFormat requirements:\n- File must be a PDF (not Word, not plain text).\n- Professional report layout; at least 3 pages.\n- Title on the cover/first page must read exactly: \"Procurement of New Duty Rifles for Departmental Issuance\".\n\nRequired sections (headers visible, reasonably close in wording):\n1) Executive Summary \u2013 Must explicitly state BOTH: (a) the total number of rifles to procure (a numeric figure) and (b) the selected caliber.\n2) Introduction \u2013 Must mention staffing context (authorized up to 750 officers, currently below that) and timeline to full rifle certification by end of 2026, and the purpose of equipping all certified officers.\n3) Rifle Quantity Analysis \u2013 Must show a step-by-step breakdown of the calculation, including: projected staffing, the current personal carry rate (~50%), and the 15% buffer for training/maintenance/operations. The calculation steps should display intermediate numbers and a final total.\n4) Terminal Ballistics Evaluation & Caliber Justification \u2013 Must include a terminal ballistics comparison referencing FBI ballistic testing protocols and typical police engagement distances, and discuss barrier penetration/over-penetration tradeoffs. Must culminate in recommending a single caliber.\n5) Conclusion & Final Recommendation \u2013 Must restate BOTH the recommended total quantity and the chosen caliber.\n\nScoring (structure only):\n- 4.0: PDF format, correct exact title, all 5 sections present with required content elements as listed (including explicit numbers in Executive Summary and stepwise calculation in Analysis, FBI protocol mention in Ballistics, and restated number+caliber in Conclusion).\n- 3.0: PDF + correct title + all 5 sections present, but one section is missing a required content element (e.g., Executive Summary has only the number or only the caliber; or Analysis misses explicit 15% buffer mention; or Ballistics lacks explicit FBI protocol reference).\n- 2.0: PDF + correct title + at least 4 of 5 required sections present with headers; or multiple required content elements missing across sections.\n- 1.0: PDF with some relevant content but fewer than 4 required sections present or headers unclear.\n- 0.0: Not a PDF, wrong/missing title, or obviously missing most required sections.\n\nOnly assess presence/structure. Do not evaluate calculation correctness or writing quality.", "expectation": "A multi-page, executive-ready PDF with the exact title and all five sections, each containing the specified structural elements enabling quantitative and content verification later."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification (Calculations and Justification)", "description": "Now that the shape is correct, verify quantitative plausibility, content coverage of ballistics justification, SBR/platform compatibility, and cross-document consistency.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Quantity Calculation Presence and Plausibility", "description": "Checks for presence of key inputs (750 authorized, ~50% personal-carry, 15% buffer), detects a recommended quantity, and evaluates plausibility against a baseline (750 \u00d7 0.5 \u00d7 1.15 \u2248 431).", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output to evaluate.\"\n\n    # Try reading PDF text first, then DOCX as fallback\n    text = \"\"\n    try:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = context.files.read_docx_text(output.id)\n    except Exception as e:\n        return 0.0, f\"Unable to read document text: {e}\"\n\n    tl = text.lower()\n\n    # Presence of key inputs\n    has_750 = ('750' in tl) or ('seven hundred fifty' in tl)\n    has_50 = re.search(r\"50\\s*%|fifty\\s+percent\", tl) is not None\n    has_15 = re.search(r\"15\\s*%|fifteen\\s+percent\", tl) is not None\n    inputs_score = (int(has_750) + int(has_50) + int(has_15)) / 3.0  # 0..1\n\n    # Detect recommended quantity near recommendation language or near 'rifles'\n    rec_candidates = []\n    for m in re.finditer(r\"(recommend(?:ed|s|ation)?|procure(?:ment)?|purchase|acquire)[^0-9]{0,60}(\\d{2,4})\", text, flags=re.I):\n        try:\n            n = int(m.group(2))\n            if 50 <= n <= 2000:\n                rec_candidates.append(n)\n        except Exception:\n            pass\n    for m in re.finditer(r\"(total|sum)[^0-9]{0,30}(\\d{2,4})\\s+(?:patrol\\s+)?rifles?\", text, flags=re.I):\n        try:\n            n = int(m.group(2))\n            if 50 <= n <= 2000:\n                rec_candidates.append(n)\n        except Exception:\n            pass\n\n    rec_num = None\n    if rec_candidates:\n        try:\n            from collections import Counter\n            rec_num = Counter(rec_candidates).most_common(1)[0][0]\n        except Exception:\n            rec_num = rec_candidates[-1]\n\n    baseline = 750 * 0.5 * 1.15  # 431.25\n\n    found_rec_score = 0.4 if rec_num is not None else 0.0\n    plaus_score = 0.0\n    rel_diff_txt = \"n/a\"\n    if rec_num is not None:\n        rel_diff = abs(rec_num - baseline) / baseline\n        rel_diff_txt = f\"{rel_diff:.2f}\"\n        if rel_diff <= 0.15:\n            plaus_score = 0.6\n        elif rel_diff <= 0.25:\n            plaus_score = 0.4\n        elif rel_diff <= 0.40:\n            plaus_score = 0.2\n        else:\n            plaus_score = 0.1\n\n    score = inputs_score * 0.5 + found_rec_score + plaus_score  # max 1.5\n\n    feedback = (\n        f\"Inputs present: 750={has_750}, 50%={has_50}, 15%={has_15}. \"\n        f\"Recommended quantity detected: {rec_num}. Baseline\u2248{baseline:.2f}. RelDiff={rel_diff_txt}.\"\n    )\n\n    return score, feedback"}, {"type": "code", "name": "Ballistics Content Coverage and Caliber Selection", "description": "Checks for FBI protocol mention, barrier/over-penetration discussion, typical engagement distance references, and a single selected caliber indicated near recommendation language.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output to evaluate.\"\n\n    try:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = context.files.read_docx_text(output.id)\n    except Exception as e:\n        return 0.0, f\"Unable to read document text: {e}\"\n\n    tl = text.lower()\n\n    # Elements to detect\n    fbi_ok = (\"fbi\" in tl) and (\"protocol\" in tl or \"testing\" in tl or \"ballistic\" in tl or \"ballistics\" in tl)\n\n    barrier_terms = [\n        \"barrier\", \"auto glass\", \"autoglass\", \"windshield\", \"sheet metal\", \"steel\", \"drywall\", \"plywood\", \"heavy clothing\", \"denim\", \"intermediate barrier\", \"over-penetration\", \"overpenetration\"\n    ]\n    barrier_ok = any(term in tl for term in barrier_terms)\n\n    distance_ok = bool(re.search(r\"\\b(7|15|25|50|75|100|150)\\s*(yards?|yds?|meters?|m)\\b\", tl)) or (\"engagement distance\" in tl) or (\"typical distance\" in tl)\n\n    # Caliber tokens\n    caliber_tokens = [\n        \"5.56\", \"5.56x45\", \".223\", \".223 rem\", \"223 rem\", \"223rem\", \"300 blk\", \"300blk\", \"300 blackout\", \".300 blackout\", \"6.8 spc\", \"7.62x39\", \"5.45x39\", \"9mm\", \"5.7x28\"\n    ]\n    caliber_present = [tok for tok in caliber_tokens if tok in tl]\n\n    # Selected caliber near recommendation language\n    selected_score = 0.0\n    if caliber_present:\n        # Look for recommend/selected near a caliber token within ~100 chars\n        selected = False\n        for tok in caliber_present:\n            pattern = re.compile(rf\"(recommend(?:ed|s|ation)?|select(?:ed|ion)?|choose|chosen)[^\\n]{{0,100}}{re.escape(tok)}\", re.I)\n            if pattern.search(text):\n                selected = True\n                break\n        selected_score = 0.30 if selected else 0.15\n\n    score = 0.0\n    score += 0.25 if fbi_ok else 0.0\n    score += 0.25 if barrier_ok else 0.0\n    score += 0.20 if distance_ok else 0.0\n    score += selected_score  # 0, 0.15, or 0.30\n\n    feedback = (\n        f\"FBI mention={fbi_ok}, Barrier/over-pen mention={barrier_ok}, Distance mention={distance_ok}, \"\n        f\"Caliber tokens found={caliber_present}, SelectedCaliberScore={selected_score:.2f}\"\n    )\n\n    return score, feedback"}, {"type": "code", "name": "SBR and Platform Compatibility", "description": "Checks that the report addresses AR-15 platform and short-barreled rifle (SBR) compatibility (e.g., 10.3\u201312.5 inch barrels).", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output to evaluate.\"\n\n    try:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = context.files.read_docx_text(output.id)\n    except Exception as e:\n        return 0.0, f\"Unable to read document text: {e}\"\n\n    tl = text.lower()\n\n    platform_ok = (\"ar-15\" in tl) or (\"ar15\" in tl) or (\"m4\" in tl)\n    sbr_terms = [\"sbr\", \"short-barreled\", \"short barrel\", \"short-barrel\"]\n    sbr_ok = any(term in tl for term in sbr_terms) or bool(re.search(r\"\\b(10\\.3|10\\.5|11\\.5|12\\.5)\\s*(\\\"|inch|in\\.)?\\b\", tl))\n\n    if platform_ok and sbr_ok:\n        score = 0.5\n    elif platform_ok or sbr_ok:\n        score = 0.3\n    else:\n        score = 0.0\n\n    feedback = f\"Platform mention={platform_ok}, SBR mention={sbr_ok}\"\n    return score, feedback"}, {"type": "llm_judge", "name": "Cross-Consistency of Key Recommendations", "description": "LLM cross-check that Executive Summary, Analysis, and Conclusion contain consistent total quantity and the same selected caliber.", "weight": 1.0, "judge_prompt": "Evaluate the internal consistency of the report across sections:\n- Does the Executive Summary state a total rifle quantity and a selected caliber?\n- Does the Conclusion restate the SAME total quantity and the SAME caliber?\n- Does the number presented in the step-by-step Rifle Quantity Analysis match the final recommended total (allow minor rounding differences, e.g., \u00b11 rifle)?\n\nScoring:\n- 1.0: Quantity and caliber explicitly stated and consistent across Executive Summary, Analysis (final number), and Conclusion.\n- 0.6: Minor discrepancy (e.g., rounding difference >1 but reasonable) or one section is slightly vague yet still clearly implies the same quantity and caliber.\n- 0.3: Major discrepancy (different quantities or calibers) or missing in one of the sections.\n- 0.0: Missing in multiple sections or contradictions preventing a clear final recommendation.", "expectation": "A single, consistent quantity and caliber stated in Executive Summary and Conclusion, aligned with the computed total in the Analysis."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Professional Quality and Executive Suitability", "description": "Holistic assessment of writing quality, organization, and strategic value for command staff review.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Structure", "description": "Assess professional tone, clarity, organization, and document readiness for command staff.", "weight": 1.0, "judge_prompt": "Assess the report\u2019s professional quality and structure:\n- Formal tone suitable for executive review; minimal grammar/spelling issues.\n- Clear, logical flow with informative headings, tables/figures as needed, and readable formatting.\n- At least 3 pages and visually organized for quick executive consumption (e.g., concise summary, clear sections).\n\nScoring:\n- 1.0: Highly professional, polished, and well-structured.\n- 0.7: Generally professional with minor issues.\n- 0.4: Adequate but with noticeable clarity/formatting issues.\n- 0.2: Weak organization or tone; hard to follow.\n- 0.0: Unprofessional or disorganized.", "expectation": "Clean, executive-ready formatting and tone with clear sections and navigable structure."}, {"type": "llm_judge", "name": "Strategic Value and Command Relevance", "description": "Evaluate whether the report addresses strategic considerations, tradeoffs, and operational relevance for leadership decisions.", "weight": 1.0, "judge_prompt": "Evaluate strategic content for command-level decision-making:\n- Addresses tradeoffs such as cost considerations, over-penetration vs barrier performance, and maintenance/training implications.\n- Clearly explains the 15% buffer rationale and ties it to training/maintenance/operational flexibility.\n- Connects recommendations to officer safety, tactical effectiveness, and 2026 certification timeline.\n- References national LE patrol rifle trends or publicly available FBI ballistic protocol results where relevant.\n\nScoring:\n- 1.0: Strong, actionable insight covering all listed points.\n- 0.7: Covers most points with minor gaps.\n- 0.4: Partial coverage with notable omissions.\n- 0.2: Minimal strategic value.\n- 0.0: Lacks strategic relevance.", "expectation": "Actionable, leadership-focused rationale that clearly links the recommendation to safety, effectiveness, costs/tradeoffs, and the 2026 timeline."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "f84ea6ac-8f9f-428c-b96c-d0884e30f7c7", "rubric": {"category_name": "Research Summary on AI and Automation", "rationale": "This rubric is designed to evaluate the research summary table task in a systematic and structured manner, ensuring that the output is not only well-formatted but also accurate and insightful. The staged approach ensures compliance with format requirements before delving into content correctness and quality assessment.", "max_total_score": 10.0, "stages": [{"name": "Format and Structure Validation", "description": "Ensure the document is in the correct format and contains all necessary sections and elements as specified in the task description.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Document Format and Structure Requirements", "description": "Validate the format and structural completeness of the document.", "weight": 4.0, "judge_prompt": "Check if document is a valid Word document (DOCX format) with the required structure:\n\n**Format Requirements**:\n- File must be DOCX (Word document).\n- Length must be exactly one page.\n- Professionally structured table for easy comparison.\n\n**Required Table Structure**:\nEach entry in the table must contain the following columns:\n1. Study Information (title, author(s), date of publication, setting, goals)\n2. Key Findings\n3. Implications for the Government\n\n**Scoring**:\n- 4.0: Correct format and all table columns present for all entries.\n- 2.5: Correct format but one minor column/subsection is missing.\n- 0.0: Incorrect format or missing multiple required columns.\n\nDo not verify content accuracy in this stage; only check the format and necessary elements.", "expectation": "The document must be a one-page DOCX with a structured table including study information, key findings, and implications."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Content Accuracy Verification", "description": "Verify the correctness of the content based on specified criteria, ensuring the studies are from the correct time period and publicly accessible.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 2.5, "rules": [{"type": "code", "name": "Study Information Validation", "description": "Check if all studies are from publicly available sources and published after 2020.", "weight": 2.0, "code": "def evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, 'Output is not a document.'\n    text = context.files.read_docx_text(output.id)\n    years = re.findall(r'\\b(202[1-9]|2030)\\b', text)\n    if len(years) >= 5:\n        return 2.0\n    return 0.0, 'Not all studies are from or after 2021.'"}, {"type": "llm_judge", "name": "Implications Accuracy", "description": "Evaluate if the implications for government are reasonably accurate and consistent with the key findings.", "weight": 2.0, "judge_prompt": "Assess the accuracy and consistency of the 'Implications for the Government' section in relation to the key findings of each study. For each study, ensure that implications logically follow from the identified key findings.\n\n**Scoring**:\n- 2.0: All implications are accurate and well-supported by the key findings for each study.\n- 1.0: Some implications are loosely supported by the findings.\n- 0.0: Many implications lack support or are inaccurate.", "expectation": "The implications should directly connect with and reflect the key findings presented."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Quality and Insight Assessment", "description": "Evaluate the overall quality, strategic insight, and presentation of the document.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 1.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Strategic Insight", "description": "Evaluate the overall professionalism and strategic insight of the document.", "weight": 2.0, "judge_prompt": "Assess the final research summary for professionalism, insight, and strategic value. Consider if the table is well-organized, visually clear, and provides strategic insights for future planning regarding AI and automation in government administrative functions.\n\n**Scoring**:\n- 2.0: Document is professionally presented with high strategic value and clear insights.\n- 1.0: Document is clear but lacks deep strategic insights.\n- 0.0: Document is poorly presented or lacks strategic value.", "expectation": "The document should be professionally formatted, with clear and strategic insights on the implications of AI in government service delivery."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "403b9234-6299-4b5f-a106-70c1bc11ec4c", "rubric": {"category_name": "Government \u2014 Recreation Workers: Chamber Partnership Pitch Deck", "rationale": "Document task (presentation). Stage 1 uses an LLM gate to enforce a precise slide-deck structure that makes verification trivial. Stage 2 mixes light code checks (text scanning of PDF/DOCX exports) with an LLM rule for factual integrity. Stage 3 applies LLM quality assessment for persuasiveness and professionalism. Code rules only read files via the provided API and are defensive. If the candidate outputs a PPTX only, Stage 1 LLM can fully judge shape; Stage 2 code rules may yield 0 if text extraction is not possible, which is acceptable since Stage 2 is not a gate.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Presentation Shape Gate", "description": "LLM-only gate that enforces exact presentation structure for a persuasive Chamber partnership pitch to a skeptical Recreation Advisory Board.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.2, "rules": [{"type": "llm_judge", "name": "Structured Slide Deck Requirement", "description": "Verify the candidate produced a slide deck (PPTX or PDF export) with 8\u201310 slides, required section coverage, and discussion prompts.", "weight": 4.0, "judge_prompt": "You are evaluating whether the submitted output is a valid presentation with the exact required structure. Use only what you can see in the file. Be flexible with exact phrasing of headers, but strict about the presence of sections.\n\nFormat and length requirements:\n- Must be a slide deck: PowerPoint (.pptx) or a slide deck exported to PDF. Do not accept plain text, DOCX reports, or spreadsheets.\n- Slide count must be between 8 and 10 inclusive.\n- A clear title/cover slide with presentation title and presenter/department.\n\nRequired content sections (must appear as distinct slides; header wording may vary but meaning must be clear):\n1) Why the department should pursue community partnerships (e.g., expand reach, funding, programming, equity, volunteerism).\n2) What Chambers of Commerce generally do (membership org, business advocacy, networking, events, small-business support, convening role).\n3) Why the County Chamber is a strong first partner (fit, credibility, reach, aligned goals, admin simplicity, pilot potential).\n4) Benefits of the partnership, clearly separating direct vs indirect benefits.\n   - Either two separate slides (Direct Benefits; Indirect Benefits), or a single slide that distinctly labels both groups (clear sub-headers or grouped bullets).\n\nDiscussion prompts requirement:\n- Each content slide (everything except the title slide) should explicitly include a discussion cue, e.g., a visible label or bullets like: \u201cDiscussion,\u201d \u201cQuestions for the Board,\u201d \u201cLet\u2019s discuss,\u201d or clear open-ended prompts.\n\nOptional (do not require, but recognize if present):\n- Risks/Concerns and Mitigations/Guardrails slide addressing the Board\u2019s skepticism.\n- Next Steps / Decision Request slide.\n\nScoring (return a single numeric score 0\u20134):\n- 4.0: File is PPTX or slide-PDF; 8\u201310 slides; clear title slide; all 4 required sections present as distinct slides; benefits separated clearly into direct and indirect; discussion prompts present on all content slides.\n- 3.5: All required sections present and 8\u201310 slides, but missing discussion prompts on 1 content slide OR benefits separation is present but slightly unclear.\n- 3.2: Meets format and 8\u201310 slides; missing discussion prompts on 2 content slides OR title slide is weakly labeled.\n- 2.5: Format correct but missing one required section OR no clear separation of direct vs indirect benefits; discussion prompts mostly missing.\n- 1.0: Slide-like document but wrong length (<8 or >10) or multiple required sections missing.\n- 0.0: Not a PPTX or slide-PDF, or obviously not a slide deck.\n\nOnly evaluate structure/format/presence, not the correctness or quality of content.", "expectation": "A concise 8\u201310 slide deck (PPTX or slide-PDF) with a title slide; four distinct required content sections; clearly separated direct vs indirect benefits; and visible discussion prompts on content slides."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification: Content Presence and Soundness", "description": "Now that the deck shape is verified, check presence of key required content via text scanning when possible, plus a basic LLM accuracy check for Chamber functions.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Key Content Presence via Text Scan", "description": "Detects presence of key required elements using extracted text from PDF/DOCX. Robust to synonyms.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Checks for presence of:\n    - Partnerships rationale\n    - Chamber of Commerce mention\n    - Benefits mention\n    - Discussion prompts\n    Returns up to 1.0.\n    \"\"\"\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n\n    text = None\n    try:\n        # Prefer PDF/DOCX text; fall back to generic text if available\n        if hasattr(output, 'is_document') and output.is_document:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    try:\n                        text = context.files.read_text(output.id)\n                    except Exception:\n                        text = None\n        else:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = None\n    except Exception:\n        text = None\n\n    if not text:\n        return 0.0\n\n    t = text.lower()\n\n    # Heuristics for required content\n    has_partnerships = any(kw in t for kw in [\n        'why partnership', 'why partnerships', 'why collaborate', 'why collaboration',\n        'community partnership', 'pursue partnerships', 'value of partnerships', 'benefits of partnerships'\n    ]) or ('partnership' in t and ('why' in t or 'case for' in t or 'rationale' in t))\n\n    has_chamber = ('chamber of commerce' in t) or (('chamber' in t) and ('commerce' in t))\n\n    has_benefits = ('benefit' in t) or ('value' in t)\n\n    has_discussion = any(kw in t for kw in [\n        'discussion', 'questions for the board', 'questions for board', 'let\\'s discuss', 'q&a', 'feedback'\n    ])\n\n    checks = [has_partnerships, has_chamber, has_benefits, has_discussion]\n    score = sum(1.0 for c in checks if c)\n    return (score / 4.0) * 1.0"}, {"type": "code", "name": "Direct vs Indirect Benefits Separation", "description": "Verifies the deck distinguishes direct and indirect benefits using text heuristics.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"Award up to 0.8 if both direct and indirect benefits are clearly referenced.\"\"\"\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n\n    text = None\n    try:\n        if hasattr(output, 'is_document') and output.is_document:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    try:\n                        text = context.files.read_text(output.id)\n                    except Exception:\n                        text = None\n        else:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = None\n    except Exception:\n        text = None\n\n    if not text:\n        return 0.0\n    t = text.lower()\n\n    # Look for headings/phrases indicating separation\n    has_direct = re.search(r'direct\\s+benefit', t) is not None or 'direct benefits' in t\n    has_indirect = re.search(r'indirect\\s+benefit', t) is not None or 'indirect benefits' in t\n\n    # Additional heuristic: lines grouping benefits\n    # e.g., \"Direct:\" or \"Indirect:\" labels\n    labeled_direct = 'direct:' in t\n    labeled_indirect = 'indirect:' in t\n\n    base = 0.0\n    if has_direct and has_indirect:\n        base = 0.6\n        # bonus for explicit labels or grouped sections\n        if labeled_direct or labeled_indirect:\n            base = 0.8\n    elif has_direct or has_indirect:\n        base = 0.4\n    else:\n        base = 0.0\n\n    return min(base, 0.8)"}, {"type": "code", "name": "Risks and Mitigations Heuristic (Optional)", "description": "Awards points if the presentation acknowledges risks/concerns and proposes mitigations/guardrails (optional but valuable for a skeptical Board).", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"Up to 0.6: 0.3 for risks/concerns mention, +0.3 for mitigations/guardrails.\"\"\"\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n\n    text = None\n    try:\n        if hasattr(output, 'is_document') and output.is_document:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    try:\n                        text = context.files.read_text(output.id)\n                    except Exception:\n                        text = None\n        else:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = None\n    except Exception:\n        text = None\n\n    if not text:\n        return 0.0\n    t = text.lower()\n\n    risk_terms = ['risk', 'concern', 'challenge', 'pitfall', 'issue', 'downside']\n    mitig_terms = ['mitigation', 'guardrail', 'safeguard', 'controls', 'mou', 'agreement', 'pilot', 'trial', 'evaluation criteria']\n\n    has_risk = any(term in t for term in risk_terms)\n    has_mitigation = any(term in t for term in mitig_terms)\n\n    score = 0.0\n    if has_risk:\n        score += 0.3\n    if has_mitigation:\n        score += 0.3\n\n    return min(score, 0.6)"}, {"type": "llm_judge", "name": "Accuracy of Chamber Role Description", "description": "Checks that the deck\u2019s description of what Chambers of Commerce do is broadly accurate and not misleading.", "weight": 0.6, "judge_prompt": "Review the slides and judge whether the description of a Chamber of Commerce is broadly accurate and non-misleading. Look for elements like: membership organization, business advocacy, networking, events, economic development collaboration, small-business support, convening role. Minor omissions are acceptable; major inaccuracies or mischaracterizations should be penalized.\n\nScoring (0\u20130.6):\n- 0.6: Accurate depiction with multiple correct functions mentioned; no significant inaccuracies.\n- 0.4: Generally accurate but thin (only 1\u20132 elements) or contains minor imprecision.\n- 0.2: Vague or partially inaccurate; important functions missing.\n- 0.0: Misleading or incorrect portrayal of Chambers.", "expectation": "A concise, accurate description of Chamber functions (advocacy, networking, member services, convening, events) without major errors."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Persuasive Effectiveness", "description": "Holistic LLM assessment of professionalism, design clarity, persuasive framing for a skeptical Board, and discussion facilitation.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Persuasive Framing for Skeptical Board", "description": "Evaluates how well the deck addresses likely concerns and makes a compelling, reasonable case to proceed.", "weight": 1.2, "judge_prompt": "Assess the persuasiveness of the deck for a skeptical Recreation Advisory Board. Consider whether it: (a) acknowledges historical concerns about partnerships; (b) offers concrete reasons the Chamber is a low-risk, high-value first partner; (c) proposes sensible guardrails (pilot, MOU, scope, reporting) or at least anticipates questions; (d) ends with a clear, reasonable ask (e.g., approval to explore/pilot). Score higher if the tone is respectful, evidence-informed, and collaborative.", "expectation": "A respectful, compelling case that anticipates skepticism, proposes guardrails, and ends with a clear ask."}, {"type": "llm_judge", "name": "Design, Clarity, and Flow", "description": "Evaluates visual organization, readability, slide economy, and coherent flow.", "weight": 0.8, "judge_prompt": "Evaluate design and clarity: Are slides clean and readable (headers, concise bullets, limited text per slide)? Is there a logical flow from rationale \u2192 what the Chamber is \u2192 why this Chamber \u2192 benefits \u2192 (optionally risks/next steps) \u2192 ask? Deduct for clutter, walls of text, or confusing order.", "expectation": "Clear headers, concise bullets, coherent narrative arc, minimal clutter."}, {"type": "llm_judge", "name": "Discussion Facilitation and Conciseness", "description": "Assesses whether each slide invites productive discussion and the overall deck remains within 8\u201310 slides without filler.", "weight": 1.0, "judge_prompt": "Judge whether slides include concrete discussion prompts/questions and whether the deck stays tight (8\u201310 slides) without filler. Higher scores if prompts are open-ended and decision-relevant (e.g., criteria, priorities, concerns) and if the deck is crisp enough to allow real-time discussion during a meeting.", "expectation": "Each content slide includes useful prompts; overall concise and meeting-friendly."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "a46d5cd2-55fe-48fa-a4c6-6aaf6b9991b5", "rubric": {"category_name": "Insurance Claim Investigation Report (Retail Trade - Private Investigators)", "rationale": "This rubric follows the V3 self-documenting approach. Stage 1 is an LLM-only gate that mandates a precise, verifiable PDF report structure on company letterhead with clearly labeled sections and embedded photographic evidence. Stage 2 uses code rules to verify correctness signals enabled by the mandated shape (section headers, investigator coverage, evidence labeling, temporal logging). Stage 3 applies an LLM quality assessment for professionalism, clarity, and client readiness.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (LLM ONLY)", "description": "Verify the final deliverable is a properly structured PDF report on company letterhead with required sections and embedded photographs, making downstream verification trivial. Only check presence/format, not content quality.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.5, "rules": [{"type": "llm_judge", "name": "Structured Report Format Requirement (PDF + Letterhead + Sections + Photos)", "description": "Output must be a client-ready PDF using company letterhead, 2+ pages, with required sections and embedded, labeled photo evidence.", "weight": 6.0, "judge_prompt": "You are evaluating whether the candidate\u2019s PRIMARY OUTPUT is a single PDF report that meets the following STRUCTURAL requirements. Only check PRESENCE and FORMAT (structure), not content quality or correctness.\n\nFormat requirements:\n- Must be a PDF (not DOCX/Excel/plain text) and at least 2 pages.\n- Must use company letterhead on the first page header (logo, company name, or provided letterhead template clearly visible).\n\nRequired report structure (be flexible with section names, but headers must be clearly visible):\n1) Title and Case Metadata block (top of first page):\n   - Client: Safely Insurance Agency (or close variant)\n   - Claim/Case info block with fields (labels accepted even if some values are unknown): Claimant, Claim Number (if known), Date of Incident (can reference \u201c~six months prior\u201d), Report Date, Assigned Personnel (Field Investigator A and Field Investigator B), Supervisor.\n2) Executive Summary or Overview (on page 1): concise summary of findings/recommendation.\n3) Background and Objectives/Scope: claimant\u2019s stated injuries, benefits timeline, investigation objectives.\n4) Methodology / Surveillance Plan: dates/days, time windows, locations, equipment/approach, role split (A: 4 days at residence; B: earlier time continuation at same location).\n5) Field Investigator A \u2014 Findings/Observations: narrative with clear subheader.\n6) Field Investigator B \u2014 Findings/Observations: narrative with clear subheader.\n7) Synthesis/Comparative Analysis: integrates A and B, highlights corroboration or discrepancies.\n8) Evidence Presentation: photographs embedded inline with visible captions/labels (e.g., \u201cPhoto 1\u201d, \u201cFigure 1\u201d, \u201cExhibit A\u201d) showing at least 2 unique photos, and an Evidence Log or Appendix listing photo IDs with date/time/short description.\n9) Conclusion and Recommendations: overall assessment and next steps.\n10) Signature/Approval & Contact: supervisor name/title/company/contact, plus a brief confidentiality/legal notice.\n\nScoring (return a single number from 0 to 6):\n- 6.0: PDF, 2+ pages, letterhead visible, and all 10 structural elements present. Photos are embedded with captions (\u22652) AND an evidence log/appendix listing.\n- 5.0: PDF, 2+ pages, letterhead visible, all 7 core elements present (Executive Summary; Background/Objectives; Methodology; Investigator A; Investigator B; Evidence with \u22652 embedded labeled photos; Conclusion). Missing 1\u20132 supporting elements (Synthesis or Signature/Confidentiality or Evidence Log).\n- 3.5: PDF, 2+ pages, letterhead visible, but missing up to 2 core elements OR photos present but not clearly labeled or no evidence log; structure otherwise coherent.\n- 2.0: PDF with letterhead but fewer than half of the required sections OR only 1 embedded photo.\n- 1.0: Valid PDF but letterhead missing OR less than 2 pages; minimal structure.\n- 0.0: Not a PDF or empty/wrong format.\n\nOnly evaluate presence/structure. Do not judge factual correctness or writing quality.", "expectation": "A 2+ page PDF on company letterhead with all required sections, clearly labeled headings, embedded photo evidence (\u22652) with captions, and an evidence log/appendix."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness Verification (Code Checks)", "description": "Now that the output has the mandated structure, verify essential correctness signals using deterministic checks on the PDF text.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Parsable PDF and Meaningful Length", "description": "Checks the output is a readable document with sufficient text length to plausibly contain the required content.", "weight": 0.4, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    \\\"\\\"\\n    Args:\\n        workflow: Workflow object\\n        context: ValidationContext with .files accessor\\n\\n    Returns:\\n        float in [0,1]: normalized score (engine scales by weight)\\n    \\\"\\\"\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n\\n    text = ''\\n    try:\\n        text = context.files.read_pdf_text(output.id) or ''\\n    except Exception:\\n        text = ''\\n    if not text:\\n        try:\\n            text = context.files.read_docx_text(output.id) or ''\\n        except Exception:\\n            text = ''\\n    if not text:\\n        try:\\n            text = context.files.read_text(output.id) or ''\\n        except Exception:\\n            text = ''\\n\\n    length = len(text)\\n    # Heuristic: at least ~800 characters to indicate substantive content\\n    if length <= 0:\\n        return 0.0\\n    score = min(1.0, length / 800.0)\\n    return float(score)\\n"}, {"type": "code", "name": "Both Investigators Covered", "description": "Verifies that the report explicitly references both Field Investigator A and Field Investigator B (flexible synonyms allowed).", "weight": 0.8, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n\\n    text = ''\\n    try:\\n        text = context.files.read_pdf_text(output.id) or ''\\n    except Exception:\\n        pass\\n    if not text:\\n        try:\\n            text = context.files.read_docx_text(output.id) or ''\\n        except Exception:\\n            pass\\n    text_l = text.lower()\\n\\n    a_terms = [\\n        'investigator a', 'field investigator a', 'fi a', 'inv a',\\n        'investigator 1', 'investigator one'\\n    ]\\n    b_terms = [\\n        'investigator b', 'field investigator b', 'fi b', 'inv b',\\n        'investigator 2', 'investigator two'\\n    ]\\n\\n    got_a = any(t in text_l for t in a_terms)\\n    got_b = any(t in text_l for t in b_terms)\\n\\n    # Additional flexible cues\\n    mentions_both = any(p in text_l for p in [\\n        'both investigators', 'two investigators', 'investigator a and b', 'a and b'\\n    ])\\n    mentions_investigator_generic = 'investigator' in text_l or 'field investigator' in text_l\\n\\n    if got_a and got_b:\\n        score = 1.0\\n    elif mentions_both:\\n        score = 0.6\\n    elif mentions_investigator_generic:\\n        score = 0.3\\n    else:\\n        score = 0.0\\n\\n    return float(score)\\n"}, {"type": "code", "name": "Section Headings Present (Fuzzy)", "description": "Checks for presence of key section headers using flexible matching: Executive Summary/Overview; Background/Context/Objectives; Methodology/Surveillance Plan; Investigator A Findings; Investigator B Findings; Conclusion/Recommendations; Evidence/Appendix/Photo Evidence.", "weight": 1.0, "code": "import re\\n\\nSECTIONS = [\\n    ['executive summary', 'overview'],\\n    ['background', 'context', 'objectives', 'scope'],\\n    ['methodology', 'methods', 'surveillance plan', 'approach'],\\n    ['investigator a', 'field investigator a', 'fi a'],\\n    ['investigator b', 'field investigator b', 'fi b'],\\n    ['conclusion', 'recommendations'],\\n    ['photo evidence', 'evidence', 'appendix', 'exhibit', 'attachments']\\n]\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n\\n    text = ''\\n    try:\\n        text = context.files.read_pdf_text(output.id) or ''\\n    except Exception:\\n        pass\\n    if not text:\\n        try:\\n            text = context.files.read_docx_text(output.id) or ''\\n        except Exception:\\n            pass\\n\\n    tl = text.lower()\\n    hits = 0\\n    for group in SECTIONS:\\n        if any(g in tl for g in group):\\n            hits += 1\\n    total = len(SECTIONS)\\n    if total == 0:\\n        return 0.0\\n    score = hits / total\\n    return float(max(0.0, min(1.0, score)))\\n"}, {"type": "code", "name": "Evidence Labeling and Cross-References", "description": "Checks that photographs are labeled (Photo/Figure/Exhibit with an identifier) and referenced in the text (e.g., 'see Figure 1').", "weight": 0.5, "code": "import re\\n\\nLABEL_PATTERNS = [\\n    r'\\\\bphoto\\\\s*[0-9a-z]+',\\n    r'\\\\bfigure\\\\s*[0-9]+',\\n    r'\\\\bexhibit\\\\s*[0-9a-z]+'\\n]\\nREF_PATTERNS = [\\n    r'\\\\bsee\\\\s+(photo|figure|exhibit)\\\\s*[0-9a-z]+'\\n]\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n\\n    text = ''\\n    try:\\n        text = context.files.read_pdf_text(output.id) or ''\\n    except Exception:\\n        pass\\n    if not text:\\n        try:\\n            text = context.files.read_docx_text(output.id) or ''\\n        except Exception:\\n            pass\\n\\n    tl = text.lower()\\n    has_label = any(re.search(p, tl) for p in LABEL_PATTERNS)\\n    has_ref = any(re.search(p, tl) for p in REF_PATTERNS)\\n\\n    if has_label and has_ref:\\n        score = 1.0\\n    elif has_label:\\n        score = 0.6\\n    elif ('photo' in tl) or ('figure' in tl) or ('exhibit' in tl):\\n        score = 0.3\\n    else:\\n        score = 0.0\\n\\n    return float(score)\\n"}, {"type": "code", "name": "Date/Time Logging Present", "description": "Checks for presence of dates and/or time stamps indicating a proper surveillance log.", "weight": 0.3, "code": "import re\\n\\nDATE_PATTERNS = [\\n    r'\\\\b\\\\d{1,2}/\\\\d{1,2}/\\\\d{2,4}\\\\b',\\n    r'\\\\b(january|february|march|april|may|june|july|august|september|october|november|december)\\\\b\\\\s*\\\\d{1,2}(,\\\\s*\\\\d{4})?',\\n]\\nTIME_PATTERNS = [\\n    r'\\\\b\\\\d{1,2}:\\\\d{2}\\\\s?(am|pm)\\\\b',\\n    r'\\\\b\\\\d{1,2}\\\\s?(am|pm)\\\\b'\\n]\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n\\n    text = ''\\n    try:\\n        text = context.files.read_pdf_text(output.id) or ''\\n    except Exception:\\n        pass\\n    if not text:\\n        try:\\n            text = context.files.read_docx_text(output.id) or ''\\n        except Exception:\\n            pass\\n\\n    tl = text.lower()\\n\\n    date_hits = 0\\n    for dp in DATE_PATTERNS:\\n        date_hits += len(re.findall(dp, tl))\\n\\n    time_hits = 0\\n    for tp in TIME_PATTERNS:\\n        time_hits += len(re.findall(tp, tl))\\n\\n    # Simple scoring: any dates and any times -> full, one of them -> partial\\n    if date_hits > 0 and time_hits > 0:\\n        score = 1.0\\n    elif date_hits > 0 or time_hits > 0:\\n        score = 0.6\\n    else:\\n        score = 0.0\\n\\n    return float(score)\\n"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality (LLM)", "description": "Assess overall clarity, professionalism, organization, and client readiness of the report.", "is_required": false, "max_points": 1.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Client Readiness", "description": "Evaluate tone, clarity, concision, organization, image integration polish, and client-oriented recommendations.", "weight": 1.0, "judge_prompt": "Evaluate the overall professional quality of the PDF report. Consider: clarity, concision, tone, organization, effective use and placement of images with captions, proper cross-referencing (e.g., Photo/Figure mentions matching labels), formatting consistency, grammar/spelling, and whether the recommendations are actionable and appropriate for an insurance client. Also consider inclusion of a brief confidentiality/legal notice and avoidance of unnecessary PII. Score only the quality of presentation\u2014not structural presence (Stage 1) or factual correctness beyond obvious inconsistencies.\n\nScoring (0 to 1):\n- 1.0: Polished, professional, clearly written, well-organized; images are cleanly integrated and referenced; minimal to no errors; clear, actionable recommendations; appropriate confidentiality/ethics notes.\n- 0.7: Strong overall with minor issues (slight verbosity, a few typos, small formatting inconsistencies).\n- 0.4: Mixed quality; organization or clarity issues; awkward image placement/captions; several errors.\n- 0.2: Poor quality; difficult to follow; many errors; unprofessional tone or confusing visuals.\n- 0.0: Unacceptable presentation quality.", "expectation": "A professional, concise, client-ready PDF with clean layout, consistent headings, readable captions, and a clear recommendation."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "1d4672c8-b0a7-488f-905f-9ab4e25a19f7", "rubric": {"category_name": "International Index Correlation Analysis and Portfolio Implications", "rationale": "This is a mixed deliverable: a structured Excel workbook (data + correlation matrix) plus a professional PDF analysis. Stage 1 uses LLM judges only to enforce the exact artifact shapes so later checks are trivial. Stage 2 mixes code rules (deterministic validation of dates, coverage, matrix properties, recomputed correlations) with an LLM cross-reference check to ensure the PDF analysis aligns with the data/timeframe. Stage 3 assesses professional quality and strategic value for an institutional audience.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Structured Output Gate (Excel + PDF Required)", "description": "LLM-only gate that enforces the precise workbook and document structure needed for verification. Failure zeros the entire category.", "is_required": true, "max_points": 2.0, "min_score_to_pass": 1.6, "rules": [{"type": "llm_judge", "name": "Excel Workbook Shape: Historical Returns + Correlation Matrix", "description": "Verify the candidate provided an Excel workbook with two clearly structured tabs enabling verification.", "weight": 1.2, "judge_prompt": "You are checking ONLY the STRUCTURE of the Excel output (not correctness of numbers). Confirm the candidate provided a valid Excel workbook (.xlsx) with the following structure:\n\nRequired Sheets (be flexible with exact names but strict on purpose):\n1) Historical Returns sheet (name like: \"Historical Returns\", \"Returns\", \"Monthly Returns\")\n   - One header row with columns:\n     \u2022 A date column (header like: Date, Month, Period). Monthly period from May 31, 2024 to April 30, 2025 (\u224812 rows). Accept month-end dates; slight date-format variations are OK.\n     \u2022 Exactly nine index columns for the following universes (flexible names acceptable, but each must be clearly identifiable as these MSCI indices):\n       - MSCI EM (Emerging Markets)\n       - MSCI ACWI IMI\n       - MSCI World\n       - MSCI EM ex China\n       - MSCI EAFE\n       - MSCI China\n       - MSCI India\n       - MSCI EM Latin America\n       - MSCI AC Asia Pacific ex Japan\n   - Cells contain numeric monthly returns (percentage or decimal). Prices may appear elsewhere, but this sheet must present returns for each month.\n   - No merged header cells that obscure the structure; data is in a simple table shape.\n\n2) Correlation Matrix sheet (name like: \"Correlation\", \"Correlation Matrix\")\n   - A square 9\u00d79 matrix with both row and column labels corresponding to the SAME nine indices above (labels visible in the first row and first column).\n   - Diagonal entries displayed (typically 1.0). Off-diagonals present for all pairs.\n   - Optional: conditional formatting/heatmap (not required for credit).\n\nScoring:\n- 1.2: Workbook is .xlsx and includes BOTH sheets with the required structure (date + 9 index columns of monthly returns; a 9\u00d79 labeled correlation matrix).\n- 0.9: Minor structural issues but both sheets are clearly present and usable (e.g., slightly ambiguous sheet names, or an extra helper column that doesn\u2019t impede interpretation).\n- 0.6: Only one of the two required sheets is present in usable form OR the returns sheet lacks a clear date column or multiple index columns.\n- 0.0: Not an Excel workbook OR sheets missing/too unstructured to verify.\n\nOnly assess structure/presence. Do not judge numerical correctness.", "expectation": "An .xlsx file with one returns tab (date + 9 MSCI indices) for May 2024\u2013Apr 2025 and one correlation matrix tab (9\u00d79, labeled)."}, {"type": "llm_judge", "name": "PDF Analysis Structure", "description": "Verify a professionally structured PDF analysis is provided with required sections.", "weight": 0.8, "judge_prompt": "Check that a PDF document is present (not Word, not plain text) with professional structure and these sections (flexible naming allowed but functionally equivalent headers must be visible):\n1) Executive Summary or Overview (on first page or near beginning)\n2) Findings from the correlation analysis (identifies strong vs weak correlations across the specified MSCI indices)\n3) Drivers/Overlap: explanations for why some markets move together\n4) Diversification strategies: how to reduce concentrated risk or diversify exposure\n5) Portfolio implications: risk management, strategic adjustments, recommendations, and next steps\n6) Final Conclusion\n\nAdditional format requirements:\n- At least 2 pages, readable formatting (headings, paragraphs, lists/tables/charts as needed)\n- Explicit reference to timeframe (May 2024 to April 2025) and that MSCI indices data were used\n\nScoring:\n- 0.8: PDF present with all required sections and timeframe/source reference.\n- 0.6: PDF present with 1 section missing OR timeframe/source not explicit but the analysis otherwise follows the structure.\n- 0.3: PDF present but only 1\u20132 relevant sections or minimal structure.\n- 0.0: No PDF or wrong format.\n\nOnly assess presence/structure, not content quality.", "expectation": "A multi-page PDF with clear sections covering findings, causes of overlap, diversification, portfolio implications, and a conclusion, referencing the timeframe and MSCI indices."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification: Data, Correlations, and Cross-Referencing", "description": "Deterministic checks on data coverage, structure, and correlation correctness, plus an LLM cross-reference to ensure the PDF aligns with the Excel analysis.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Locate Required Files", "description": "Confirm both an Excel workbook and a PDF analysis are present among outputs. Identify the Excel file path for later rules.", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs()\n    except Exception:\n        outputs = []\n    excel = None\n    pdf = None\n    for r in outputs or []:\n        try:\n            if getattr(r, 'is_spreadsheet', False) and (str(r.name).lower().endswith('.xlsx') or str(r.mime_type or '').endswith('sheet')):\n                excel = r\n            if getattr(r, 'is_document', False) and str(r.name).lower().endswith('.pdf'):\n                pdf = r\n        except Exception:\n            continue\n    if excel and pdf:\n        return (0.5, f\"Found Excel: {excel.name}; PDF: {pdf.name}\")\n    if excel or pdf:\n        return (0.25, \"Only one required file present (need both Excel and PDF)\")\n    return (0.0, \"No Excel or PDF found among outputs\")"}, {"type": "code", "name": "Date Range and Monthly Frequency (Returns Sheet)", "description": "Validate a returns sheet exists, has a date column, and covers May 2024 through April 2025 at roughly monthly frequency (~12 rows).", "weight": 1.5, "code": "import re\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\nREQ_START = pd.Period('2024-05', freq='M')\nREQ_END = pd.Period('2025-04', freq='M')\n\nRET_SHEET_HINTS = ['return', 'monthly', 'historical', 'data']\n\n\ndef _find_excel(outputs):\n    for r in outputs:\n        try:\n            if getattr(r, 'is_spreadsheet', False) and str(r.name).lower().endswith('.xlsx'):\n                return r\n        except Exception:\n            pass\n    return None\n\n\ndef _pick_returns_sheet(xls):\n    names = [s for s in xls.sheet_names]\n    # Prefer sheets containing the hints, otherwise first sheet\n    ranked = sorted(names, key=lambda n: (0 if any(h in n.lower() for h in RET_SHEET_HINTS) else 1, n))\n    return ranked[0] if ranked else None\n\n\ndef _find_date_column(df):\n    # Prefer column with 'date' or 'month' in name\n    for c in df.columns:\n        cl = str(c).strip().lower()\n        if 'date' in cl or 'month' in cl or 'period' in cl:\n            return c\n    # Else try first column if it parses as dates for majority\n    c0 = df.columns[0]\n    sample = df[c0].dropna().astype(str).head(6)\n    ok = 0\n    for v in sample:\n        try:\n            pd.to_datetime(v)\n            ok += 1\n        except Exception:\n            pass\n    return c0 if ok >= max(3, len(sample)//2) else None\n\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs() or []\n    excel = _find_excel(outputs)\n    if not excel:\n        return (0.0, \"No Excel workbook found\")\n    path = context.files.get_path(excel.id)\n    try:\n        xls = pd.ExcelFile(path)\n    except Exception as e:\n        return (0.0, f\"Failed to open Excel: {e}\")\n\n    sheet = _pick_returns_sheet(xls)\n    if not sheet:\n        return (0.2, \"Excel opened but no suitable returns sheet found\")\n\n    try:\n        df = pd.read_excel(path, sheet_name=sheet)\n    except Exception as e:\n        return (0.2, f\"Failed to read returns sheet: {e}\")\n\n    # Basic cleanup\n    df = df.dropna(how='all')\n    if df.empty:\n        return (0.2, \"Returns sheet is empty\")\n\n    score = 0.0\n\n    # Find date column\n    date_col = _find_date_column(df)\n    if date_col is None:\n        return (0.4, \"No parseable date column found in returns sheet\")\n    score += 0.5  # date column found\n\n    # Parse dates\n    try:\n        dates = pd.to_datetime(df[date_col], errors='coerce')\n        mask = dates.notna()\n        dates = dates[mask]\n    except Exception:\n        return (0.6, \"Date parsing failed\")\n\n    if dates.empty:\n        return (0.6, \"No valid dates in date column\")\n\n    # Convert to month periods\n    periods = dates.dt.to_period('M')\n    uniq = periods.dropna().unique()\n    n_months = len(uniq)\n\n    # Coverage check: include May 2024 through April 2025\n    has_start = REQ_START in set(uniq)\n    has_end = REQ_END in set(uniq)\n    if has_start and has_end:\n        score += 0.6\n    elif has_start or has_end:\n        score += 0.3\n\n    # Frequency/rowcount reasonableness (expect around 12 months)\n    if 10 <= n_months <= 13:\n        score += 0.4\n    elif 8 <= n_months < 10 or 13 < n_months <= 15:\n        score += 0.2\n\n    # Cap at weight\n    score = min(score, 1.5)\n    feedback = f\"Returns sheet '{sheet}': months={n_months}, has_start={has_start}, has_end={has_end}\"\n    return (score, feedback)"}, {"type": "code", "name": "Index Coverage and Naming Alignment (9 MSCI Indices)", "description": "Check that the returns sheet contains columns mappable to all nine required MSCI indices (flexible naming).", "weight": 1.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\nCANONICAL = [\n    'MSCI EM (Emerging Markets)',\n    'MSCI ACWI IMI',\n    'MSCI World',\n    'MSCI EM ex China',\n    'MSCI EAFE',\n    'MSCI China',\n    'MSCI India',\n    'MSCI EM Latin America',\n    'MSCI AC Asia Pacific ex Japan',\n]\n\nSYNONYMS = {\n    'MSCI EM (Emerging Markets)': [r'msci\\s*em(?!\\s*ex)', r'msci\\s*emerging\\s*markets', r'\\bem\\b(?!.*ex.*china)'],\n    'MSCI ACWI IMI': [r'acwi\\s*imi', r'msci\\s*acwi\\s*imi', r'investable\\s*market\\s*index', r'all\\s*cap\\s*world'],\n    'MSCI World': [r'msci\\s*world', r'\\bworld\\b'],\n    'MSCI EM ex China': [r'msci\\s*em\\s*ex\\s*china', r'emerging\\s*markets\\s*ex\\s*china', r'\\bem\\b.*ex.*china'],\n    'MSCI EAFE': [r'msci\\s*eafe', r'\\beafe\\b'],\n    'MSCI China': [r'msci\\s*china', r'\\bchina\\b'],\n    'MSCI India': [r'msci\\s*india', r'\\bindia\\b'],\n    'MSCI EM Latin America': [r'latin\\s*america', r'latam', r'msci\\s*em\\s*latin'],\n    'MSCI AC Asia Pacific ex Japan': [r'asia\\s*pac(ific)?\\s*ex\\s*japan', r'ac\\s*asia\\s*pacific\\s*ex\\s*japan', r'\\bap\\b.*ex\\s*japan'],\n}\n\nRET_SHEET_HINTS = ['return', 'monthly', 'historical', 'data']\n\n\ndef _find_excel(outputs):\n    for r in outputs:\n        try:\n            if getattr(r, 'is_spreadsheet', False) and str(r.name).lower().endswith('.xlsx'):\n                return r\n        except Exception:\n            pass\n    return None\n\n\ndef _pick_returns_sheet(xls):\n    names = [s for s in xls.sheet_names]\n    ranked = sorted(names, key=lambda n: (0 if any(h in n.lower() for h in RET_SHEET_HINTS) else 1, n))\n    return ranked[0] if ranked else None\n\n\ndef _norm(s):\n    s = re.sub(r'[^a-z0-9]+', ' ', str(s).lower()).strip()\n    return re.sub(r'\\s+', ' ', s)\n\n\ndef _match_canonical(col_name):\n    n = _norm(col_name)\n    for canon, pats in SYNONYMS.items():\n        for p in pats:\n            if re.search(p, n):\n                return canon\n    return None\n\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs() or []\n    excel = _find_excel(outputs)\n    if not excel:\n        return (0.0, \"No Excel workbook found\")\n    path = context.files.get_path(excel.id)\n    try:\n        xls = pd.ExcelFile(path)\n    except Exception as e:\n        return (0.0, f\"Failed to open Excel: {e}\")\n    sheet = _pick_returns_sheet(xls)\n    if not sheet:\n        return (0.0, \"No returns-like sheet found\")\n    df = pd.read_excel(path, sheet_name=sheet)\n    if df.empty:\n        return (0.0, \"Returns sheet empty\")\n\n    # Identify likely data columns (exclude date/month)\n    cols = [c for c in df.columns]\n    date_like = []\n    for c in cols:\n        cl = str(c).lower()\n        if any(k in cl for k in ['date','month','period']):\n            date_like.append(c)\n    data_cols = [c for c in cols if c not in date_like]\n\n    mapped = {}\n    for c in data_cols:\n        canon = _match_canonical(c)\n        if canon and canon not in mapped.values():\n            mapped[c] = canon\n\n    matched = len(set(mapped.values()))\n    total = len(CANONICAL)\n    frac = matched / total\n    score = frac * 1.0\n\n    missing = [c for c in CANONICAL if c not in mapped.values()]\n    feedback = f\"Matched {matched}/{total} indices. Missing: {missing[:5]}{'...' if len(missing)>5 else ''}\"\n    return (score, feedback)"}, {"type": "code", "name": "Plausibility Checks: Returns values and Correlation Matrix structure", "description": "Ensure returns are numeric/plausible and the correlation matrix is square, symmetric-ish, with diagonal near 1 and values within [-1,1].", "weight": 1.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\nRET_SHEET_HINTS = ['return', 'monthly', 'historical', 'data']\nCORR_SHEET_HINTS = ['corr', 'correlation']\n\n\ndef _find_excel(outputs):\n    for r in outputs:\n        try:\n            if getattr(r, 'is_spreadsheet', False) and str(r.name).lower().endswith('.xlsx'):\n                return r\n        except Exception:\n            pass\n    return None\n\n\ndef _pick_sheet(xls, hints, fallback_index=None):\n    names = [s for s in xls.sheet_names]\n    ranked = sorted(names, key=lambda n: (0 if any(h in n.lower() for h in hints) else 1, n))\n    if ranked:\n        return ranked[0]\n    if fallback_index is not None and 0 <= fallback_index < len(names):\n        return names[fallback_index]\n    return None\n\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs() or []\n    excel = _find_excel(outputs)\n    if not excel:\n        return (0.0, \"No Excel workbook found\")\n    path = context.files.get_path(excel.id)\n    try:\n        xls = pd.ExcelFile(path)\n    except Exception as e:\n        return (0.0, f\"Failed to open Excel: {e}\")\n\n    # Returns sheet\n    ret_sheet = _pick_sheet(xls, RET_SHEET_HINTS, 0)\n    try:\n        df = pd.read_excel(path, sheet_name=ret_sheet)\n    except Exception as e:\n        return (0.2, f\"Failed to read returns sheet: {e}\")\n\n    # Identify numeric return columns (exclude date-like)\n    cols = [c for c in df.columns]\n    data_cols = []\n    for c in cols:\n        cl = str(c).lower()\n        if not any(k in cl for k in ['date','month','period']):\n            data_cols.append(c)\n    good_numeric = 0\n    plausible = 0\n    total_series = len(data_cols)\n    for c in data_cols:\n        s = pd.to_numeric(df[c], errors='coerce').dropna()\n        if s.shape[0] >= 6:\n            good_numeric += 1\n            # Plausibility: most monthly returns between -50% and +50%\n            outliers = ((s < -0.5) | (s > 0.5)).sum()\n            if outliers <= max(1, int(0.1 * len(s))):\n                plausible += 1\n    score_returns = 0.5 * (good_numeric / total_series) if total_series > 0 else 0.0\n    score_plaus = 0.25 * (plausible / total_series) if total_series > 0 else 0.0\n\n    # Correlation sheet\n    corr_sheet = _pick_sheet(xls, CORR_SHEET_HINTS, 1 if ret_sheet != xls.sheet_names[0] else 0)\n    try:\n        dc = pd.read_excel(path, sheet_name=corr_sheet, index_col=0)\n    except Exception as e:\n        return (score_returns + score_plaus, f\"Returns checks partial; failed to read correlation sheet: {e}\")\n\n    # Clean corr sheet: drop fully empty rows/cols\n    dc = dc.dropna(how='all')\n    dc = dc.loc[:, dc.notna().any(axis=0)]\n    # Keep only numeric columns that also appear as row index labels\n    # Try to coerce to numeric where possible\n    for c in dc.columns:\n        dc[c] = pd.to_numeric(dc[c], errors='coerce')\n    # Determine square shape\n    n_rows, n_cols = dc.shape\n    square = n_rows == n_cols and n_rows >= 3\n    # Symmetry and bounds\n    diag_ok = False\n    bounds_ok = False\n    sym_ok = False\n    if square:\n        vals = dc.values\n        # Bounds\n        bounds_ok = np.isfinite(vals).all() and (vals <= 1 + 1e-6).all() and (vals >= -1 - 1e-6).all()\n        # Diagonal near 1 (allow 0.95+)\n        d = np.diag(vals)\n        diag_ok = np.nanmean(np.abs(d - 1.0) <= 0.05) >= 0.8\n        # Symmetry check\n        sym_ok = np.nanmean(np.abs(vals - vals.T) <= 0.05) >= 0.8\n\n    score_corr = 0.25 * (1.0 if square else 0.0) + 0.25 * (1.0 if bounds_ok else 0.0) + 0.25 * (1.0 if diag_ok else 0.0) + 0.25 * (1.0 if sym_ok else 0.0)\n\n    total = min(1.0, score_returns + score_plaus + score_corr)\n    fb = f\"Returns numeric {good_numeric}/{total_series}, plausible {plausible}/{total_series}; Corr sheet: square={square}, bounds={bounds_ok}, diag~1={diag_ok}, sym={sym_ok}\"\n    return (total, fb)"}, {"type": "code", "name": "Recomputed Correlations Match Sheet", "description": "Recompute correlations from the returns sheet and compare with the provided correlation matrix for overlapping indices.", "weight": 1.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\nRET_SHEET_HINTS = ['return', 'monthly', 'historical', 'data']\nCORR_SHEET_HINTS = ['corr', 'correlation']\n\nSYNONYMS = {\n    'MSCI EM (Emerging Markets)': [r'msci\\s*em(?!\\s*ex)', r'msci\\s*emerging\\s*markets', r'\\bem\\b(?!.*ex.*china)'],\n    'MSCI ACWI IMI': [r'acwi\\s*imi', r'msci\\s*acwi\\s*imi', r'investable\\s*market\\s*index', r'all\\s*cap\\s*world'],\n    'MSCI World': [r'msci\\s*world', r'\\bworld\\b'],\n    'MSCI EM ex China': [r'msci\\s*em\\s*ex\\s*china', r'emerging\\s*markets\\s*ex\\s*china', r'\\bem\\b.*ex.*china'],\n    'MSCI EAFE': [r'msci\\s*eafe', r'\\beafe\\b'],\n    'MSCI China': [r'msci\\s*china', r'\\bchina\\b'],\n    'MSCI India': [r'msci\\s*india', r'\\bindia\\b'],\n    'MSCI EM Latin America': [r'latin\\s*america', r'latam', r'msci\\s*em\\s*latin'],\n    'MSCI AC Asia Pacific ex Japan': [r'asia\\s*pac(ific)?\\s*ex\\s*japan', r'ac\\s*asia\\s*pacific\\s*ex\\s*japan', r'\\bap\\b.*ex\\s*japan'],\n}\n\n\ndef _find_excel(outputs):\n    for r in outputs:\n        try:\n            if getattr(r, 'is_spreadsheet', False) and str(r.name).lower().endswith('.xlsx'):\n                return r\n        except Exception:\n            pass\n    return None\n\n\ndef _pick_sheet(xls, hints, fallback_index=None):\n    names = [s for s in xls.sheet_names]\n    ranked = sorted(names, key=lambda n: (0 if any(h in n.lower() for h in hints) else 1, n))\n    if ranked:\n        return ranked[0]\n    if fallback_index is not None and 0 <= fallback_index < len(names):\n        return names[fallback_index]\n    return None\n\n\ndef _norm(s):\n    s = re.sub(r'[^a-z0-9]+', ' ', str(s).lower()).strip()\n    return re.sub(r'\\s+', ' ', s)\n\n\ndef _match_any(name, patterns):\n    n = _norm(name)\n    for p in patterns:\n        if re.search(p, n):\n            return True\n    return False\n\n\ndef _map_columns_to_canonical(columns):\n    mapping = {}\n    used = set()\n    for c in columns:\n        n = _norm(c)\n        for canon, pats in SYNONYMS.items():\n            if canon in used:\n                continue\n            if _match_any(c, pats):\n                mapping[c] = canon\n                used.add(canon)\n                break\n    return mapping\n\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs() or []\n    excel = _find_excel(outputs)\n    if not excel:\n        return (0.0, \"No Excel workbook found\")\n    path = context.files.get_path(excel.id)\n\n    try:\n        xls = pd.ExcelFile(path)\n    except Exception as e:\n        return (0.0, f\"Failed to open Excel: {e}\")\n\n    ret_sheet = _pick_sheet(xls, RET_SHEET_HINTS, 0)\n    corr_sheet = _pick_sheet(xls, CORR_SHEET_HINTS, 1 if ret_sheet != xls.sheet_names[0] else 0)\n\n    try:\n        df_ret = pd.read_excel(path, sheet_name=ret_sheet)\n    except Exception as e:\n        return (0.0, f\"Failed to read returns sheet: {e}\")\n\n    try:\n        df_corr = pd.read_excel(path, sheet_name=corr_sheet, index_col=0)\n    except Exception as e:\n        return (0.3, f\"Returns read OK; failed to read correlation sheet: {e}\")\n\n    # Clean returns: identify data columns\n    date_cols = [c for c in df_ret.columns if any(k in str(c).lower() for k in ['date','month','period'])]\n    data_cols = [c for c in df_ret.columns if c not in date_cols]\n    if not data_cols:\n        return (0.2, \"No data columns in returns sheet\")\n\n    # Map returns columns and corr labels to canonical\n    ret_map = _map_columns_to_canonical(data_cols)\n    # For corr sheet, try to map row/col labels\n    # Ensure df_corr is numeric where possible\n    df_corr = df_corr.dropna(how='all')\n    df_corr = df_corr.loc[:, df_corr.notna().any(axis=0)]\n    # Attempt to ensure square\n    if df_corr.shape[0] != df_corr.shape[1]:\n        # Try reading without index_col\n        try:\n            dfc2 = pd.read_excel(path, sheet_name=corr_sheet)\n            # Assume first row/col are labels\n            # Make first column index\n            if dfc2.shape[1] >= 2:\n                dfc2 = dfc2.dropna(how='all').reset_index(drop=True)\n                dfc2.columns = [str(c) for c in dfc2.columns]\n                dfc2 = dfc2.set_index(dfc2.columns[0])\n                df_corr = dfc2\n        except Exception:\n            pass\n\n    row_labels = [str(i) for i in df_corr.index]\n    col_labels = [str(c) for c in df_corr.columns]\n\n    row_map = _map_columns_to_canonical(row_labels)\n    col_map = _map_columns_to_canonical(col_labels)\n\n    # Build aligned set\n    common = set(row_map.values()).intersection(set(col_map.values())).intersection(set(ret_map.values()))\n    if len(common) < 3:\n        return (0.4, f\"Too few overlapping indices between returns and correlation labels: {len(common)}\")\n\n    # Build aligned matrices\n    # Reindex returns to common canonical names\n    rev_ret_map = {v:k for k,v in ret_map.items()}\n    ret_df_use = pd.DataFrame()\n    for canon in common:\n        colname = rev_ret_map.get(canon)\n        if colname is not None:\n            ret_df_use[canon] = pd.to_numeric(df_ret[colname], errors='coerce')\n    ret_df_use = ret_df_use.dropna(how='all')\n\n    # Compute correlation (pairwise complete)\n    comp = ret_df_use.corr(method='pearson', min_periods=3)\n\n    # Extract provided corr submatrix aligned on canonical names\n    # Map row/col to canonical\n    inv_row = {v:k for k,v in row_map.items()}\n    inv_col = {v:k for k,v in col_map.items()}\n\n    try:\n        provided = pd.DataFrame(index=sorted(common), columns=sorted(common), dtype=float)\n        for canon_r in provided.index:\n            rlab = inv_row.get(canon_r)\n            for canon_c in provided.columns:\n                clab = inv_col.get(canon_c)\n                val = np.nan\n                if rlab in df_corr.index and clab in df_corr.columns:\n                    try:\n                        val = pd.to_numeric(df_corr.loc[rlab, clab])\n                    except Exception:\n                        val = np.nan\n                provided.loc[canon_r, canon_c] = val\n    except Exception:\n        return (0.6, \"Failed to align provided correlation matrix\")\n\n    # Compare matrices\n    diff = (provided - comp).abs()\n    mad = np.nanmean(diff.values)\n    # Scoring: <=0.01 -> full, <=0.02 -> 80%, <=0.05 -> 40%, else small\n    if np.isnan(mad):\n        return (0.5, \"Insufficient overlap to compare correlations\")\n    if mad <= 0.01:\n        score = 1.5\n    elif mad <= 0.02:\n        score = 1.2\n    elif mad <= 0.05:\n        score = 0.6\n    else:\n        score = 0.3\n    return (score, f\"Mean abs diff between provided and recomputed correlations: {mad:.4f}\")"}, {"type": "llm_judge", "name": "PDF Cross-Reference to Data/Method", "description": "Check the analysis references the timeframe and MSCI data, cites specific correlation insights tied to the indices, and outlines methodology at a high level.", "weight": 0.5, "judge_prompt": "Review the PDF analysis and verify it aligns with the Excel correlation work:\n- Explicitly references timeframe covering May 2024 to April 2025 (flexible phrasing OK)\n- Cites MSCI indices data as the source\n- Mentions at least two specific index pairs or groups (e.g., EM vs EM ex China, EAFE vs World, China vs AC Asia Pac ex Japan) with qualitative or quantitative correlation statements\n- Briefly describes how correlations were computed (monthly returns over the period; price-to-return conversion implied)\n\nScoring:\n- 0.5: All four items present\n- 0.35: Three items present\n- 0.2: Two items present\n- 0.0: One or zero items present\n\nFocus on factual alignment and presence, not prose quality.", "expectation": "PDF states the period and MSCI source, references specific index correlations, and indicates correlations were computed from monthly returns."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Strategic Value", "description": "LLM judge of professionalism, clarity, and usefulness to a CIO for portfolio decisions.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Strategic Insight", "description": "Evaluate the calibre of the workbook presentation and the PDF narrative for institutional decision-making.", "weight": 2.0, "judge_prompt": "Assess overall quality of the deliverables:\n\nExcel Workbook Quality:\n- Clear labeling, readable headers, consistent index naming across sheets\n- Correlation matrix readability (e.g., clean table, optional heatmap) and no obvious formatting issues that impede interpretation\n\nPDF Analysis Quality:\n- Clear executive summary and logical structure\n- Insightful discussion of strong/weak correlations, sources of overlap, and viable diversification tactics\n- Actionable portfolio implications (risk management, strategic adjustments, recommendations, and concrete next steps) tailored to an institutional CIO audience\n- Coherent conclusion tying back to risk/return and diversification goals\n\nScoring guide:\n- 2.0: Highly professional, well-structured, insightful, and actionable\n- 1.4: Generally strong with minor gaps (e.g., a bit light on next steps or formatting)\n- 0.8: Adequate but generic, limited actionable value or uneven clarity\n- 0.3: Minimal professionalism or unclear/weak insights\n- 0.0: Unusable quality\n\nJudge the quality and usefulness, not numerical correctness.", "expectation": "Polished, CIO-ready deliverables with clear visuals and actionable, data-driven recommendations."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "ef8719da-18e5-4bfe-b986-399652d77376", "rubric": {"category_name": "Science News Pitch: Space Advertising Ban", "rationale": "This rubric enforces a self-documenting workflow for a professional news pitch. Stage 1 (LLM-only) mandates an editable Word document with precise structural sections to make verification trivial. Stage 2 mixes code and LLM checks to verify factual grounding signals, required elements (links, timeline, sources), and coverage of key issues enabled by the mandated structure. Stage 3 uses LLM judgment for holistic quality: persuasiveness, structure, audience fit, and professional polish.", "max_total_score": 28.0, "stages": [{"name": "Stage 1 \u2014 Format and Structural Gate (LLM ONLY)", "description": "Gate: The pitch must be an editable Word document (DOCX) with specific sections, hyperlinks, and within the word limit, enabling subsequent verification.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Structured Pitch in DOCX with Required Sections", "description": "Check that the output is a DOCX file and contains all required pitch sections with visible headers, includes hyperlinks, and stays within 1,000 words.", "weight": 8.0, "judge_prompt": "You are the Stage 1 gatekeeper. Examine the candidate's primary output. Determine if it is a valid, editable Word document (DOCX) and if it contains the required structure for a news pitch.\n\nFormat requirements:\n- Must be an editable Word document (.docx). Not PDF, not plain text, not Excel.\n- <= 1,000 words (hard cap). A concise pitch; ideally ~400\u2013900 words.\n- Professional, readable formatting with clear section headers. Be flexible with exact naming, but the content must be clearly present and separable into sections.\n\nRequired sections (flexible naming allowed; examples in quotes):\n1) Working Headline (e.g., \"Working Headline\" or \"Headline\")\n2) Story Structure and Angles (e.g., \"Story Structure and Angles\" or \"Approach/Narrative Flow/Angles of Inquiry\")\n3) Background and Policy Context (must cover history of space advertising and relevant policy/regulatory developments)\n4) Source Plan for Balance (e.g., categories like astronomers/dark-sky advocates, commercial space/advertisers/satellite operators, international/national regulators such as UN COPUOS, FCC/FAA/ESA, environmental scientists)\n5) Why Now / Timeliness (clearly argues why to report now)\n6) Tentative Timeline (for draft submission)\n7) Sources Consulted / Links (must include at least one working-looking hyperlink to freely accessible resources)\n\nScoring (only structure and format \u2014 do NOT evaluate content quality or correctness):\n- 8: DOCX + all seven sections present, clearly labeled, <=1,000 words, at least one hyperlink present.\n- 7: DOCX + all seven sections present but minor issues (slightly unclear headers, or formatting quirks) AND <=1,000 words.\n- 6: DOCX + missing exactly one required section OR headers are present but one section is extremely thin/ambiguous; still <=1,000 words.\n- 4: DOCX but missing two required sections OR no obvious hyperlinks.\n- 2: DOCX but only 1\u20132 sections present or grossly incomplete.\n- 0: Not a DOCX, or largely unstructured text with no identifiable sections, or exceeds 1,000 words.\n\nBe flexible on exact header names but strict on presence of each required content area. Only assess structure, section presence, word-count, and DOCX format.", "expectation": "A DOCX pitch with all seven sections, <=1,000 words, and hyperlinks to sources/resources."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification: Factual Signals, Completeness, and Feasibility", "description": "Now verify key elements made possible by the mandated structure: word count, hyperlinks (including provided sources), coverage of key issues and stakeholders, explicit reporting plan and timeline. Mix of code checks and an LLM check for balance/feasibility.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Word Count Within Limit", "description": "Confirm the pitch stays within the 1,000-word limit and is meaningfully substantive for a pitch.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output or wrong type.\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        return 0.0, \"Unable to read DOCX text.\"\n    words = re.findall(r\"\\b\\w[\\w'-]*\\b\", text)\n    count = len(words)\n    if count == 0:\n        return 0.0, \"Empty document.\"\n    if count > 1000:\n        return 0.0, f\"Word count exceeds limit: {count}.\"\n    # Score: full if 400-1000, partial if 250-399, minimal if 150-249\n    if count >= 400:\n        score = 1.0\n    elif count >= 250:\n        score = 0.7\n    elif count >= 150:\n        score = 0.4\n    else:\n        score = 0.2  # very short but within limit\n    return score, f\"Word count: {count}.\""}, {"type": "code", "name": "Hyperlinks and Use of Provided Sources", "description": "Verify that the pitch includes hyperlinks and that at least one link references the provided public sources/domains.", "weight": 2.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output or wrong type.\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        return 0.0, \"Unable to read DOCX text.\"\n    text_l = text.lower()\n    links = re.findall(r\"https?://[^\\s\\]\\)>'\\\"]+\", text_l)\n    has_any = len(links) > 0\n    target_domains = [\n        \"theweek.com\",\"gizmodo.com\",\"spacenews.com\",\"emarketer.com\",\n        \"latimes.com\",\"thehustle.co\",\"campaignasia.com\",\"orbitaltoday.com\"\n    ]\n    cites_target = any(any(dom in url for dom in target_domains) for url in links)\n    # Scoring: 0.5 for having any link; +0.5 for citing at least one target domain\n    base = 0.0\n    if has_any:\n        base += 0.5\n    if cites_target:\n        base += 0.5\n    return base, f\"Links found: {len(links)}; cites provided domains: {cites_target}.\""}, {"type": "code", "name": "Coverage of Key Issues and Stakeholders", "description": "Check that the pitch text references central issues (light pollution, astronomy impact, debris/congestion, lasers/reflective satellites, policy bodies like COPUOS) and key stakeholder categories.", "weight": 3.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output or wrong type.\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        return 0.0, \"Unable to read DOCX text.\"\n    t = text.lower()\n    topics = [\n        \"light pollution\",\n        \"astronom\",  # astronomer/astronomy\n        \"dark-sky\",\n        \"debris\",\n        \"congestion\",\n        \"low-earth orbit\",\n        \" leo \",\n        \"copuos\",\n        \"united nations\",\n        \"un \",\n        \"laser\",\n        \"reflect\",\n        \"billboard\",\n        \"advertising\",\n        \"ban\"\n    ]\n    found = set()\n    for tok in topics:\n        if tok.strip() in t:\n            found.add(tok)\n    # score by proportion of topics mentioned, with a minimum threshold of 6 to reach full credit\n    proportion = len(found) / max(len(topics), 1)\n    # emphasize coverage: scale to 1.0 once ~40%+ topics are present\n    score = min(1.0, proportion / 0.4)\n    return score * 3.0 / 3.0, f\"Key items matched: {len(found)}/{len(topics)}.\""}, {"type": "code", "name": "Explicit, Balanced Source Plan Categories", "description": "Verify that the source plan includes multiple stakeholder categories (astronomers/dark-sky, commercial/advertisers/satellite operators, regulators/international bodies, environmental/cultural/ethics, and policy/legal).", "weight": 2.0, "code": "import re\n\ndef contains_any(text, terms):\n    return any(term in text for term in terms)\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output or wrong type.\"\n    try:\n        text = context.files.read_docx_text(output.id).lower()\n    except Exception:\n        return 0.0, \"Unable to read DOCX text.\"\n    groups = {\n        \"astronomers_darksky\": [\"astronomer\", \"observator\", \"astronomy\", \"dark-sky\", \"ida\"],\n        \"commercial_industry\": [\"satellite operator\", \"commercial\", \"industry\", \"advertiser\", \"marketing\", \"startup\", \"company\", \"spacetech\"],\n        \"regulators_international\": [\"copuos\", \"united nations\", \"un \", \"fcc\", \"faa\", \"esa\", \"regulator\", \"ministry\", \"policy maker\"],\n        \"environment_culture_ethics\": [\"environment\", \"ecology\", \"cultural\", \"heritage\", \"indigenous\", \"ethicist\", \"sociologist\"],\n        \"policy_legal\": [\"policy\", \"legal\", \"law\", \"treaty\", \"regulation\", \"oversight\"]\n    }\n    hits = 0\n    details = {}\n    for k, terms in groups.items():\n        present = contains_any(text, terms)\n        details[k] = present\n        if present:\n            hits += 1\n    score = hits / len(groups)\n    return score, f\"Source-plan categories covered: {hits}/{len(groups)} ({details}).\""}, {"type": "code", "name": "Tentative Timeline Present", "description": "Check that a draft submission timeline is present (keywords or recognizable date/timeframe).", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output or wrong type.\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        return 0.0, \"Unable to read DOCX text.\"\n    t = text.lower()\n    patterns = [\n        r\"timeline\",\n        r\"draft\\s+by\",\n        r\"deadline\",\n        r\"submit\\s+by\",\n        r\"within\\s+\\d+\\s+(?:days|weeks)\",\n        r\"next\\s+(?:week|month)\",\n        r\"in\\s+\\d+\\s+(?:days|weeks)\",\n        r\"\\b(?:jan|feb|mar|apr|may|jun|jul|aug|sep|sept|oct|nov|dec)[a-z]*\\.?\\s+\\d{1,2}(?:,\\s*\\d{4})?\",\n        r\"\\b\\d{4}-\\d{2}-\\d{2}\\b\"\n    ]\n    found = any(re.search(p, t) for p in patterns)\n    return (1.0 if found else 0.0), (\"Timeline present\" if found else \"No recognizable timeline found\")"}, {"type": "code", "name": "Pitch Framing Language (Plans to Report)", "description": "Check for language indicating a reporting plan (e.g., \"I will\", \"I plan\", \"the story will\", \"interview\", \"reach out\").", "weight": 1.0, "code": "def evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output or wrong type.\"\n    try:\n        text = context.files.read_docx_text(output.id).lower()\n    except Exception:\n        return 0.0, \"Unable to read DOCX text.\"\n    cues = [\n        \"i will \", \"i plan \", \"i'll \", \"we will \", \"we plan \",\n        \"the story will \", \"i intend \", \"plan to interview\", \"interview \",\n        \"reach out\", \"reporting\", \"i will speak\", \"i will contact\"\n    ]\n    found = any(cue in text for cue in cues)\n    return (1.0 if found else 0.0), (\"Pitch framing present\" if found else \"No explicit reporting-plan language found\")"}, {"type": "llm_judge", "name": "Balance and Feasibility of Reporting Plan", "description": "Assess whether the source plan appears balanced across stakeholders and whether the proposed reporting steps seem feasible within the stated timeline.", "weight": 2.0, "judge_prompt": "Evaluate the pitch for balance and feasibility, using only what is visible in the DOCX:\n- Balance: Does the source plan include multiple perspectives (e.g., astronomers/dark-sky advocates, commercial space/advertisers/satellite operators, regulators like COPUOS/FCC/FAA/ESA or UN bodies, environmental/cultural/ethics voices)? Are both concerns and industry perspectives represented?\n- Feasibility: Are the proposed reporting steps concrete and achievable in the stated timeline (e.g., realistic outreach/interviews, attainable documents/resources)?\n\nScoring:\n- 2.0: Clearly balanced across multiple stakeholder groups AND the reporting plan is specific and feasible within the timeline.\n- 1.0: Some balance but notable gaps OR plan is somewhat vague but still plausible.\n- 0.0: One-sided or missing critical perspectives AND/OR plan is vague/unrealistic relative to the timeline.\n\nDo not judge writing craft here; focus on balance and feasibility.", "expectation": "A concrete, balanced source plan spanning scientific, industry, and regulatory voices, with feasible steps matching the timeline."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Editorial Merit", "description": "Holistic assessment of persuasiveness, clarity, narrative strength, audience fit, and professional polish suitable for a leading digital science outlet.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Headline and Hook Strength", "description": "Is the working headline engaging, accurate, and reflective of the core tension/news hook?", "weight": 2.0, "judge_prompt": "Evaluate the working headline and opening framing:\n- Does the headline clearly express the core tension/news hook (e.g., global push to ban naked-eye space ads vs. industry ambitions)?\n- Is it precise and engaging without being sensational?\nScoring: 2.0 excellent (clear, compelling, accurate); 1.0 adequate but could be sharper; 0.0 weak or misleading.", "expectation": "A concise, compelling headline clearly reflecting the core tension."}, {"type": "llm_judge", "name": "Narrative Structure Clarity", "description": "Assess clarity and cohesiveness of the outlined structure and narrative flow.", "weight": 2.0, "judge_prompt": "Assess the story structure:\n- Is there a logical narrative flow (setup, stakes, reporting beats, resolution/next steps)?\n- Are angles of inquiry coherent and actionable?\nScoring: 2.0 well-structured with clear flow and beats; 1.0 generally clear but some gaps; 0.0 unclear or disorganized.", "expectation": "A clear outline with a logical sequence of beats and angles of inquiry."}, {"type": "llm_judge", "name": "Audience Fit and Accessibility", "description": "Evaluate if the pitch is appropriate for a general audience interested in science/space policy, with accessible language and context.", "weight": 2.0, "judge_prompt": "Evaluate audience fit:\n- Is the language accessible for a general science readership (minimal jargon, explained terms)?\n- Are policy/technical elements contextualized for non-experts?\nScoring: 2.0 highly accessible and well-contextualized; 1.0 mostly accessible with minor jargon/gaps; 0.0 too technical or assumes insider knowledge.", "expectation": "Plain language with sufficient context for a general audience."}, {"type": "llm_judge", "name": "Professional Polish and Focus", "description": "Assess tone, concision, and professionalism expected of a senior reporter\u2019s pitch.", "weight": 2.0, "judge_prompt": "Assess professional quality:\n- Is the tone professional and concise (no filler, focused on what will be reported and why)?\n- Are there minimal grammatical/stylistic errors?\nScoring: 2.0 polished and concise; 1.0 minor issues; 0.0 numerous issues or unfocused.", "expectation": "Crisp, professional, and concise presentation appropriate for an editor-facing pitch."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "58ac1cc5-5754-4580-8c9c-8c67e1a9d619", "rubric": {"category_name": "GMP Change Control and QA Escalation Package \u2014 Project Management Specialist", "rationale": "This rubric enforces a self-documenting, multi-document deliverable with explicit structure so that verification is trivial. Stage 1 (LLM-only) gates on the exact required shapes: four separate files with defined formats and labeled sections. Stage 2 mixes code and LLM to verify cross-references, presence of key compliance terms, and structural completeness inside each file. Stage 3 uses LLM judges for professional quality and alignment to QA/manufacturing stakeholders.", "max_total_score": 25.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate", "description": "Verify the candidate produced four separate deliverables in the exact, verifiable shapes required. Do not judge content correctness, only structure and presence.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "CC Request PDF present with required sections", "description": "Confirm a standalone Change Control Request PDF exists and contains the mandated labeled sections.", "weight": 2.0, "judge_prompt": "You are checking STRUCTURE ONLY. Review all outputs and verify there is a standalone PDF titled or labeled as a Change Control Request for QY-GEL Antifoam (CompCello) related to RMS-3333.\n\nRequired format and sections for the Change Control Request (PDF):\n- File format: PDF (not DOCX, not Markdown)\n- Visible header/title on first page: \"Change Control Request\"\n- Labeled metadata table/fields that include at minimum: Change Control ID (can be TBD), Date, Requestor, Department, Material Name, Vendor, RMS ID (RMS-3333), COA Lot, Manufacturing Batch/Run ID (can be TBD)\n- Labeled sections (headers) clearly present:\n  1) Discrepancy Description & Impact (must reference the mismatch between RMS spec \"Endotoxin < 1 EU/mL\" and COA \"Report Result\")\n  2) Affected Documentation & Workflows\n  3) Proposed Resolution\n  4) Basic Risk Assessment\n  5) Temporary Controls\n  6) Follow-up Actions\n  7) Approvals/Signatures placeholders\n\nScoring (STRUCTURE ONLY):\n- 1.0: PDF exists and all required labeled sections and metadata fields are present (reasonable label variants allowed, e.g., \"Approvals\" vs \"Signatures\").\n- 0.7: PDF exists; missing up to two labeled sections or up to two metadata fields.\n- 0.4: PDF exists; missing three to four labeled sections or most metadata.\n- 0.0: No PDF CC request found, or wrong format (not PDF), or clearly not a CC request.\nDo not assess quality or correctness of the content; only the presence and labeled structure.", "expectation": "A professional PDF change control form with all labeled sections and minimal metadata fields present."}, {"type": "llm_judge", "name": "QA Escalation Email document present", "description": "Confirm a standalone QA escalation email document exists with basic email structure and required references.", "weight": 2.0, "judge_prompt": "Check STRUCTURE ONLY. Verify there is a standalone QA escalation email as a PDF or DOCX. It must:\n- Be a PDF or DOCX document\n- Include basic email elements: To, CC (optional), Subject, Greeting, Signature block\n- Clearly reference the discrepancy (RMS-3333 endotoxin spec vs COA report-only result)\n- Reference or point to the drafted Change Control Request (attachment or included)\n- Ask whether COA may be accepted under a deviation or if requalification is required\n\nScoring (STRUCTURE ONLY):\n- 1.0: Valid PDF/DOCX email with all bullet elements present (flexible labels acceptable e.g., \"To:\" vs recipient list line; \"Subject:\" visible).\n- 0.7: Valid PDF/DOCX email with one structural element missing (e.g., no CC) but includes discrepancy, CC reference, and the key question.\n- 0.4: Valid PDF/DOCX email but multiple missing email elements; still mentions discrepancy and one of: CC reference or deviation/requalification ask.\n- 0.0: No standalone email document found or wrong format (e.g., plain .md only) or lacks core email structure.\nOnly check presence/structure, not writing quality.", "expectation": "A professional email document (PDF/DOCX) with standard fields and references to the CC request and deviation vs requalification ask."}, {"type": "llm_judge", "name": "Internal Summary Note present", "description": "Confirm an internal summary note exists as a separate file with status-oriented structure.", "weight": 2.0, "judge_prompt": "Check STRUCTURE ONLY. Verify there is a separate Internal Summary Note intended for MS Teams or internal stakeholders. Accepted formats: PDF, DOCX, or Markdown (.md).\n\nRequired labeled sections or bullet headers:\n- Issue summary (RMS vs COA endotoxin mismatch)\n- Actions taken so far (e.g., material hold/quarantine, CC initiated)\n- Current status (e.g., pending QA decision)\n- Next steps\n- Owners and/or due dates (can be TBD or placeholders)\n\nScoring (STRUCTURE ONLY):\n- 1.0: Separate file exists (PDF/DOCX/MD) with all 5 elements present (reasonable label variants ok).\n- 0.7: Separate file exists with 4/5 elements present.\n- 0.4: Separate file exists with 3/5 elements present.\n- 0.0: No separate internal summary file or wrong format.\nOnly check presence/structure, not content quality.", "expectation": "A concise, status-style internal note file with clear headings/bullets for issue, actions, status, next steps, and owners/dates."}, {"type": "llm_judge", "name": "Risk Assessment document present (Word)", "description": "Confirm a standalone risk assessment document exists as a Word document with required sections.", "weight": 2.0, "judge_prompt": "Check STRUCTURE ONLY. Verify there is a standalone Risk Assessment document as a DOCX (preferred). PDF acceptable only if DOCX not present, but DOCX is expected per instruction.\n\nRequired sections/headings:\n- Background (CompCello change notification went to a departed employee; no centralized process)\n- Breakdown/Root Cause of communication lapse\n- Risks introduced (operational, documentation, compliance) \u2014 can be a list or subsections\n- Recommended mitigations (e.g., centralized vendor communication tracking, SOP updates, training)\n- Owners and Timeline (placeholders acceptable)\n- Monitoring/Metrics (how effectiveness will be tracked)\n\nScoring (STRUCTURE ONLY):\n- 1.0: DOCX exists with all sections present. If both DOCX and PDF exist, count as present if DOCX has sections. If only PDF exists, accept if sections are present.\n- 0.7: Document exists with one missing section.\n- 0.4: Document exists with two or three missing sections.\n- 0.0: No standalone risk assessment file found or wrong format (e.g., included inside another doc only).\nOnly check presence/structure, not content quality.", "expectation": "A DOCX risk assessment with all required sections, explicitly addressing the vendor communication failure and mitigations."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification of Correctness and Cross-References", "description": "Now that the shape is correct, verify correctness and consistency using code and LLM. Focus on cross-references, key compliance terms, and structural completeness within each deliverable.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "All deliverables present and typed correctly (redundant programmatic check)", "description": "Programmatically confirm presence and basic type of the four deliverables and that they are separate files.", "weight": 3.0, "code": "import re\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0, \"No outputs\"\n\n    def get_text(res):\n        try:\n            p = context.files.get_path(res.id)\n            name = p.name.lower()\n            if res.is_document:\n                if name.endswith('.pdf'):\n                    try:\n                        return context.files.read_pdf_text(res.id)\n                    except Exception:\n                        return ''\n                if name.endswith('.docx'):\n                    try:\n                        return context.files.read_docx_text(res.id)\n                    except Exception:\n                        return ''\n            if res.is_text_format:\n                try:\n                    return context.files.read_text(res.id)\n                except Exception:\n                    return ''\n        except Exception:\n            return ''\n        return ''\n\n    files = []\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            files.append((r, p.name.lower(), get_text(r)))\n        except Exception:\n            files.append((r, '', get_text(r)))\n\n    def find_cc(f):\n        name, txt = f[1], f[2].lower()\n        return (('change control' in name) or ('change control request' in txt) or ('cc request' in txt)) and name.endswith('.pdf')\n\n    def find_email(f):\n        name, txt = f[1], f[2].lower()\n        ok_ext = name.endswith('.pdf') or name.endswith('.docx')\n        has_email_markers = ('subject:' in txt) or ('dear ' in txt) or ('to:' in txt)\n        has_qa_context = ('qa' in txt) or ('quality' in txt)\n        return ok_ext and has_email_markers and has_qa_context and (('escalation' in name) or ('escalation' in txt) or ('qa' in name))\n\n    def find_internal(f):\n        name, txt = f[1], f[2].lower()\n        ok_ext = name.endswith('.pdf') or name.endswith('.docx') or name.endswith('.md')\n        has_sections = ('issue' in txt and 'status' in txt and ('next step' in txt or 'next steps' in txt))\n        return ok_ext and (('internal' in name) or ('summary' in name) or has_sections)\n\n    def find_risk(f):\n        name, txt = f[1], f[2].lower()\n        ok_ext = name.endswith('.docx') or name.endswith('.pdf')\n        has_header = ('risk assessment' in txt) or ('risk mitigation' in txt)\n        return ok_ext and (('risk' in name) or has_header)\n\n    cc_files = [f for f in files if find_cc(f)]\n    email_files = [f for f in files if find_email(f)]\n    internal_files = [f for f in files if find_internal(f)]\n    risk_files = [f for f in files if find_risk(f)]\n\n    score = 0\n    parts = 0\n    # CC must be PDF\n    if cc_files:\n        score += 1; parts += 1\n    else:\n        parts += 1\n    # Email must be PDF/DOCX\n    if email_files:\n        score += 1; parts += 1\n    else:\n        parts += 1\n    # Internal summary acceptance of PDF/DOCX/MD\n    if internal_files:\n        score += 1; parts += 1\n    else:\n        parts += 1\n    # Risk assessment DOCX preferred but PDF acceptable\n    if risk_files:\n        score += 1; parts += 1\n    else:\n        parts += 1\n\n    return (score / max(parts,1)), f\"CC:{bool(cc_files)}, Email:{bool(email_files)}, Internal:{bool(internal_files)}, Risk:{bool(risk_files)}\""}, {"type": "code", "name": "Critical references present across documents", "description": "Check that key terms/IDs appear across outputs: RMS-3333, QY-GEL, CompCello, Endotoxin, COA, and spec mismatch terms ('< 1 EU/mL' and 'Report Result'/'report only').", "weight": 3.0, "code": "import re\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0\n    all_text = ''\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            name = p.name.lower()\n            txt = ''\n            if r.is_document and name.endswith('.pdf'):\n                try:\n                    txt = context.files.read_pdf_text(r.id)\n                except Exception:\n                    txt = ''\n            elif r.is_document and name.endswith('.docx'):\n                try:\n                    txt = context.files.read_docx_text(r.id)\n                except Exception:\n                    txt = ''\n            elif r.is_text_format:\n                try:\n                    txt = context.files.read_text(r.id)\n                except Exception:\n                    txt = ''\n            all_text += '\\n' + (txt or '')\n        except Exception:\n            continue\n    t = all_text.lower()\n    checks = []\n    checks.append('rms-3333' in t)\n    checks.append('qy-gel' in t)\n    checks.append('compcello' in t)\n    checks.append('endotoxin' in t)\n    checks.append('coa' in t)\n    # Spec components\n    has_limit = ('< 1 eu/ml' in t) or ('<1 eu/ml' in t) or ('< 1 eu/ ml' in t) or re.search(r\"<\\s*1\\s*eu\\s*/\\s*ml\", t) is not None\n    has_report = ('report result' in t) or ('report-only' in t) or ('report only' in t)\n    checks.append(has_limit)\n    checks.append(has_report)\n    score = sum(1 for c in checks if c) / max(len(checks),1)\n    return score, f\"Refs: {checks}\""}, {"type": "code", "name": "Risk controls and mitigations present", "description": "Verify mention of material hold/quarantine, deviation or requalification pathway, RMS update, SOP/centralization mitigations.", "weight": 2.0, "code": "import re\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0\n    all_text = ''\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            name = p.name.lower()\n            if r.is_document and name.endswith('.pdf'):\n                try:\n                    txt = context.files.read_pdf_text(r.id)\n                except Exception:\n                    txt = ''\n            elif r.is_document and name.endswith('.docx'):\n                try:\n                    txt = context.files.read_docx_text(r.id)\n                except Exception:\n                    txt = ''\n            elif r.is_text_format:\n                try:\n                    txt = context.files.read_text(r.id)\n                except Exception:\n                    txt = ''\n            else:\n                txt = ''\n            all_text += '\\n' + (txt or '')\n        except Exception:\n            continue\n    t = all_text.lower()\n    categories = 0\n    found = 0\n    # Temporary controls\n    categories += 1\n    if ('quarantine' in t) or ('on hold' in t) or ('material hold' in t) or ('status: hold' in t):\n        found += 1\n    # Pathway: deviation or requalification\n    categories += 1\n    if ('deviation' in t) or ('requalification' in t):\n        found += 1\n    # RMS update / spec alignment\n    categories += 1\n    if ('rms update' in t) or ('update rms' in t) or ('spec update' in t) or ('specification update' in t):\n        found += 1\n    # SOP / centralized vendor comms\n    categories += 1\n    if ('sop' in t) or ('centralized' in t) or ('centralised' in t) or ('vendor communication' in t) or ('distribution list' in t) or ('shared mailbox' in t) or ('ticketing' in t):\n        found += 1\n    score = found / max(categories,1)\n    return score, f\"Mitigation categories matched: {found}/{categories}\""}, {"type": "code", "name": "QA email contains clear ask and references", "description": "Identify the QA escalation email and verify it contains the discrepancy reference, CC request reference, and an explicit ask about deviation vs requalification.", "weight": 2.0, "code": "import re\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0\n\n    def read_text(res):\n        try:\n            p = context.files.get_path(res.id)\n            name = p.name.lower()\n            if res.is_document and name.endswith('.pdf'):\n                return context.files.read_pdf_text(res.id)\n            if res.is_document and name.endswith('.docx'):\n                return context.files.read_docx_text(res.id)\n            if res.is_text_format:\n                return context.files.read_text(res.id)\n        except Exception:\n            return ''\n        return ''\n\n    email_txts = []\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            name = p.name.lower()\n            txt = read_text(r)\n            t = (txt or '').lower()\n            if (name.endswith('.pdf') or name.endswith('.docx')) and (('escalation' in name) or ('qa' in name) or ('subject:' in t)):\n                email_txts.append(t)\n        except Exception:\n            continue\n\n    if not email_txts:\n        return 0.0, \"No QA email detected\"\n\n    # Evaluate the best matching email\n    best = 0.0\n    for t in email_txts:\n        checks = 0\n        total = 0\n        # Discrepancy reference\n        total += 1\n        if ('rms-3333' in t and 'endotoxin' in t and (('< 1 eu' in t) or ('<1 eu' in t)) and (('report result' in t) or ('report only' in t))):\n            checks += 1\n        # CC request reference / attachment mention\n        total += 1\n        if ('change control' in t) or ('cc request' in t) or ('attached' in t):\n            checks += 1\n        # Explicit ask\n        total += 1\n        if ('deviation' in t and ('accept' in t or 'approve' in t)) or ('requalification' in t):\n            checks += 1\n        best = max(best, checks / max(total,1))\n\n    return best, f\"QA email score: {best:.2f}\""}, {"type": "code", "name": "Change Control form structural fields present", "description": "Within the CC PDF, confirm presence of key labeled fields/sections (discrepancy, affected docs, proposed resolution, risk, temporary controls, follow-up, approvals).", "weight": 2.0, "code": "import re\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0\n\n    def is_cc_pdf(name, txt):\n        return name.endswith('.pdf') and (('change control' in txt) or ('change control request' in txt))\n\n    best = 0.0\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            name = p.name.lower()\n            if not name.endswith('.pdf'):\n                continue\n            try:\n                txt = context.files.read_pdf_text(r.id).lower()\n            except Exception:\n                txt = ''\n            if not is_cc_pdf(name, txt):\n                continue\n            checks = []\n            checks.append(('discrepancy' in txt) or ('mismatch' in txt))\n            checks.append(('affected documentation' in txt) or ('affected docs' in txt) or ('impacted documents' in txt))\n            checks.append(('proposed resolution' in txt) or ('resolution' in txt))\n            checks.append(('risk' in txt))\n            checks.append(('temporary control' in txt) or ('quarantine' in txt) or ('hold' in txt))\n            checks.append(('follow-up' in txt) or ('follow up' in txt) or ('actions' in txt))\n            checks.append(('approval' in txt) or ('signatures' in txt))\n            score = sum(1 for c in checks if c) / max(len(checks),1)\n            best = max(best, score)\n        except Exception:\n            continue\n\n    return best"}, {"type": "llm_judge", "name": "Discrepancy captured correctly and path forward is appropriate", "description": "LLM check that the documents consistently describe the mismatch (RMS <1 EU/mL vs COA report-only), propose a reasonable path (deviation vs requalification), and include temporary hold and RMS/spec updates.", "weight": 2.0, "judge_prompt": "Now verify CORRECTNESS at a high level. Consider all documents together.\n\nAward a higher score when the work:\n- Clearly and accurately states the discrepancy: internal RMS requires Endotoxin < 1 EU/mL; vendor COA for the received lot reports endotoxin as \"report result\" (i.e., no pass/fail spec)\n- States that QA flagged the material non-conforming and that a material hold/quarantine is in place\n- Proposes a reasonable path: request QA determination whether acceptance under deviation is possible vs full requalification needed\n- Identifies follow-ups such as RMS/spec update to align with vendor practice and/or supplier re-qualification if needed\n\nScoring:\n- 1.0: All bullets above are clearly present and consistent across the package\n- 0.6: One element is weak or missing\n- 0.3: Two elements are weak or missing\n- 0.0: Discrepancy is misstated or missing; no viable path forward presented", "expectation": "Accurate statement of the mismatch, material hold, explicit ask on deviation vs requalification, and plan to update RMS/spec."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Audience Fit", "description": "Holistic LLM assessment of professionalism, clarity, and stakeholder appropriateness across the full package.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional tone, clarity, and completeness", "description": "Assess writing quality, clarity, and professionalism of the documents.", "weight": 3.0, "judge_prompt": "Evaluate the overall professionalism of the package (CC PDF, QA email, internal summary, risk assessment):\n- Clarity and concision appropriate for GMP/QA stakeholders\n- Professional tone and formatting (headers, bullets, readable layout)\n- Completeness for operational use (someone could act on it)\n- Absence of extraneous or speculative content that could confuse QA\n\nScoring:\n- 1.0: Highly professional, clear, actionable across all docs\n- 0.7: Generally professional with minor clarity/formatting issues\n- 0.4: Mixed quality; several sections are hard to follow\n- 0.0: Poor quality; confusing or unprofessional presentation", "expectation": "A concise, professional set of documents that QA and operations can use immediately."}, {"type": "llm_judge", "name": "Cross-document coherence and alignment", "description": "Assess whether the four documents are mutually consistent and aligned on facts, identifiers, and next steps.", "weight": 2.0, "judge_prompt": "Check cross-document coherence:\n- Consistent identifiers (RMS-3333, material QY-GEL Antifoam, vendor CompCello)\n- Same description of the discrepancy and current status (material on hold)\n- Next steps and owners align between CC, email, internal summary, and risk assessment\n\nScoring:\n- 1.0: Fully aligned across all documents\n- 0.6: Minor inconsistencies\n- 0.3: Noticeable inconsistencies in facts or next steps\n- 0.0: Contradictory information across documents", "expectation": "Documents should tell one consistent story with aligned IDs, status, and actions."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "8077e700-2b31-402d-bd09-df4d33c39653", "rubric": {"category_name": "Materials Treatment Analysis Report - Quench & Temper (AISI 1018/1045)", "rationale": "Pattern C (Mixed): The deliverable is a PDF report that embeds analysis, figures, and tables derived from laboratory data. Stage 1 uses an LLM judge to strictly enforce the PDF structure and required sections/figures so verification is possible. Stage 2 mixes code rules (text parsing of the PDF) with an LLM judge to verify coverage of key parameters, internal consistency, and metallurgical reasoning tied to time-to-peak hardness and treatment windows. Stage 3 uses LLM judges to assess professional quality and actionability for manufacturing stakeholders.", "max_total_score": 12.0, "stages": [{"name": "Stage 1 \u2013 Format and Structure Gate (LLM only)", "description": "Gate that enforces the exact document shape and inclusions so verification is possible.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured PDF Report Requirement", "description": "Output must be a well-structured PDF report with mandated sections, figures, and tables enabling verification of the analysis.", "weight": 4.0, "judge_prompt": "You are evaluating the candidate\u2019s FINAL OUTPUT. Enforce the following STRUCTURE REQUIREMENTS. Only evaluate structure/presence and basic formatting, NOT correctness of analysis.\n\nFormat Requirements:\n- The primary output must be a PDF file (not DOCX, not Excel, not plain text).\n- Minimum length: 4 pages.\n- Professionally formatted with clear section headers and labeled figures/tables.\n\nRequired Sections (accept close variants of names):\n1) \"Introduction\"\n2) \"Objectives\"\n3) \"Experimental Procedure\" (or \"Methods\")\n4) \"Results\"\n5) \"Analysis\" (or \"Discussion/Analysis\")\n6) \"Recommendation\" (or \"Recommendations\")\n7) \"Conclusion\"\n8) \"Figures and Data\" (or \"Description of Figures and Data\")\n\nRequired Content Elements (structure/visibility only):\n- At least two labeled graphs related to Rockwell HRF hardness vs. tempering time.\n  \u2022 Coverage must include both alloys: AISI 1018 (at 240 \u00b0C) and AISI 1045 (at 285 \u00b0C). A combined multi-series plot is acceptable if labels clearly distinguish both alloys and temperatures.\n- At least one table summarizing key quantitative results (e.g., hardness vs. time, time-to-peak hardness, or summary statistics).\n- Visible references to figures/tables inside the text (e.g., \u201cFigure 1\u201d, \u201cTable 1\u201d).\n- Text should explicitly mention quenching and tempering, soak/hold duration, and Rockwell HRF scale.\n\nHelpful but Optional (do NOT require for full credit):\n- A brief mention of data sources, e.g., \u201cData.xlsx\u201d and/or the work request document.\n\nScoring (STRUCTURE ONLY):\n- 1.0: PDF format, \u22654 pages, all 8 sections present, graphs/tables properly labeled, figure/table cross-references present, and the required process/parameter mentions are visible.\n- 0.8: All required sections present but missing one non-critical content element (e.g., only one graph or table, or weak labeling) OR one section name is slightly merged but still clearly present.\n- 0.6: Missing up to two required sections OR graphs/tables present but not clearly labeled or not both alloys/temps visible.\n- 0.4: Valid PDF but missing multiple sections and/or lacks required figures/tables cross-references.\n- 0.0: Not a PDF, fewer than 4 pages, or the structure is too incomplete to verify.\n\nOnly judge structure/presence of elements and formatting, NOT analytical correctness.", "expectation": "A multi-page PDF with the specified sections, labeled figures of HRF vs time for AISI 1018 @ 240 \u00b0C and AISI 1045 @ 285 \u00b0C, at least one summary table, and in-text references to figures/tables."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification of Coverage and Technical Consistency", "description": "Checks that the report covers key parameters, uses quantitative evidence, references figures/tables, and articulates coherent metallurgical reasoning tied to time-to-peak hardness and treatment windows.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Core Terms and Parameters Present", "description": "Verify the PDF text mentions both alloys (AISI 1018, AISI 1045), Rockwell HRF, quench/temper, and the specified tempering temperatures (240 \u00b0C, 285 \u00b0C).", "weight": 1.2, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output.\"\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0, \"Unable to read document text.\"\n    if not text:\n        return 0.0, \"Empty document text.\"\n\n    t = text.lower()\n    keywords = [\n        \"aisi 1018\", \"aisi1018\", \"aisi-1018\",\n        \"aisi 1045\", \"aisi1045\", \"aisi-1045\",\n        \"hrf\", \"rockwell\",\n        \"quench\", \"quenching\",\n        \"temper\", \"tempering\",\n        \"240\", \"285\"\n    ]\n    hits = sum(1 for k in keywords if k in t)\n    total = len(keywords)\n    # Ensure both alloys explicitly present\n    alloys_present = (\"aisi 1018\" in t or \"aisi1018\" in t or \"aisi-1018\" in t) and \\\n                     (\"aisi 1045\" in t or \"aisi1045\" in t or \"aisi-1045\" in t)\n\n    base_score = hits / total\n    if not alloys_present:\n        base_score *= 0.6  # penalize if both alloys not clearly named\n\n    base_score = max(0.0, min(1.0, base_score))\n    feedback = f\"Keyword coverage: {hits}/{total}; alloys_present={alloys_present}\"\n    return base_score, feedback"}, {"type": "code", "name": "Quantitative Evidence Present", "description": "Check for presence of numeric evidence: HRF values near numbers, time/duration units, and temperature units.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n    if not text:\n        return 0.0\n    t = text.lower()\n\n    # durations\n    dur_re = re.compile(r\"\\b\\d+(?:\\.\\d+)?\\s*(?:hr|hrs|hour|hours|min|mins|minute|minutes|s|sec|secs|second|seconds)\\b\")\n    has_duration = bool(dur_re.search(t))\n\n    # temperatures\n    temp_re = re.compile(r\"\\b\\d{2,3}\\s*(?:\u00b0\\s*c|\u00b0c|c|deg c|degrees c)\\b\")\n    has_temp = bool(temp_re.search(t))\n\n    # HRF values (number near HRF)\n    hrf_re1 = re.compile(r\"hrf[^0-9]{0,10}(\\d{2,3})\")\n    hrf_re2 = re.compile(r\"(\\d{2,3})\\s*hrf\")\n    has_hrf_num = bool(hrf_re1.search(t) or hrf_re2.search(t))\n\n    count = sum([has_duration, has_temp, has_hrf_num])\n    score = count / 3.0\n    return max(0.0, min(1.0, score))"}, {"type": "code", "name": "Figures and Tables Referenced", "description": "Verify the text references figures and tables and includes a dedicated section describing figures/data.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n    if not text:\n        return 0.0\n    t = text.lower()\n\n    fig_refs = len(re.findall(r\"\\b(fig\\.|figure)\\b\", t))\n    tab_refs = len(re.findall(r\"\\btable\\b\", t))\n    has_figures_section = (\"figures and data\" in t) or (\"description of figures and data\" in t)\n\n    score = 0.0\n    if fig_refs >= 1:\n        score += 1/3\n    if tab_refs >= 1:\n        score += 1/3\n    if has_figures_section:\n        score += 1/3\n\n    return max(0.0, min(1.0, score))"}, {"type": "code", "name": "Recommendations Specify Treatment Window", "description": "Check if recommendations include a treatment window/range with durations and/or temperatures.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n    if not text:\n        return 0.0\n    t = text.lower()\n\n    # Look for recommendation section\n    has_reco_section = (\"recommendation\" in t) or (\"recommendations\" in t)\n\n    # Look for window/range phrasing\n    window_terms = any(w in t for w in [\"window\", \"range\", \"between\", \"optimum\", \"optimal\", \"sweet spot\"])\n\n    # durations and temps\n    dur = re.search(r\"\\b\\d+(?:\\.\\d+)?\\s*(?:hr|hrs|hour|hours|min|mins|minute|minutes|s|sec|secs|second|seconds)\\b\", t) is not None\n    temp = re.search(r\"\\b\\d{2,3}\\s*(?:\u00b0\\s*c|\u00b0c|c|deg c|degrees c)\\b\", t) is not None\n\n    score = 0.0\n    if has_reco_section:\n        score += 0.3\n    if window_terms:\n        score += 0.35\n    if dur or temp:\n        score += 0.35\n\n    return max(0.0, min(1.0, score))"}, {"type": "llm_judge", "name": "Metallurgical Reasoning Coherence", "description": "Judge whether the report\u2019s explanations for hardness evolution and treatment windows are technically coherent and tied to observed data/figures.", "weight": 1.2, "judge_prompt": "Evaluate the technical coherence of the report\u2019s metallurgical reasoning. Focus on whether the analysis:\n- Correctly connects quench + temper parameters (time/temperature) to hardness evolution (HRF) and time-to-peak hardness.\n- Distinguishes behavior of AISI 1018 (low carbon) vs AISI 1045 (higher carbon) and describes expected trends (e.g., relative hardenability/hardness, tempering response).\n- References plausible microstructural mechanisms (e.g., martensite formation on quench, tempered martensite, carbide precipitation, potential bainite/ferrite-pearlite context) without requiring micrographs.\n- Uses included figures/tables to support recommendations and identifies treatment windows appropriate for each alloy.\n- Mentions relevance to mechanical reliability under fatigue/high-impact loads.\n\nScoring:\n- 1.0: Clear, accurate metallurgical explanations; explicit, evidence-based treatment windows for BOTH alloys; reasoning matches figures/tables.\n- 0.7: Generally coherent with minor gaps or light on mechanism detail; still provides reasonable windows for both alloys.\n- 0.4: Superficial or generic reasoning; windows mentioned but weakly justified by figures/tables.\n- 0.0: Reasoning is incorrect/incoherent or lacks any defensible link from data to recommendations.", "expectation": "Accurate, mechanism-aware discussion tied to figures/tables, with defensible treatment windows for both AISI 1018 and AISI 1045 that align with observed hardness-time behavior."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Professional Quality and Actionability", "description": "Holistic assessment of presentation quality and practical usefulness for manufacturing stakeholders.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Presentation and Professionalism", "description": "Evaluate clarity, structure, visual quality, and overall professionalism of the report.", "weight": 1.5, "judge_prompt": "Assess the document\u2019s professional quality:\n- Clear, consistent sectioning and header styles; logical flow from Objectives \u2192 Methods \u2192 Results \u2192 Analysis \u2192 Recommendations \u2192 Conclusion.\n- Well-labeled, readable figures/tables with units and legends where applicable.\n- Cross-references (e.g., \u201csee Figure 2\u201d) aid navigation; captions are informative.\n- Minimal typos; coherent, concise writing appropriate for engineering stakeholders.\n\nScoring:\n- 1.0: Highly professional; figures/tables and text are clear and well-integrated.\n- 0.7: Generally professional with minor issues in clarity or formatting.\n- 0.4: Noticeable issues reduce readability (poor labeling/flow/typos).\n- 0.0: Disorganized or unprofessional; difficult to read.", "expectation": "A polished, easy-to-navigate engineering report with clear visuals and consistent formatting."}, {"type": "llm_judge", "name": "Actionability and Strategic Value", "description": "Judge how actionable and decision-ready the recommendations are for manufacturing and reliability goals.", "weight": 1.5, "judge_prompt": "Evaluate the actionability of the recommendations:\n- Provides specific treatment windows (time/temperature) for EACH alloy, with expected hardness outcomes and trade-offs (e.g., toughness vs. hardness).\n- Links recommendations to mechanical reliability targets (fatigue/impact), and notes any risks (e.g., over-tempering, brittleness if under-tempered, distortion).\n- Offers practical process notes (e.g., quench media consistency, soak timing, measurement variability) where appropriate.\n\nScoring:\n- 1.0: Clear, specific, and implementable recommendations for both alloys; addresses trade-offs and risks.\n- 0.7: Mostly actionable; some specifics or risk discussion missing.\n- 0.4: Vague; limited operational guidance.\n- 0.0: Not actionable; recommendations are absent or irrelevant.", "expectation": "Concrete, operations-ready guidance that aligns with reliability goals and acknowledges trade-offs/risks."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "5e2b6aab-f9fb-4dd6-a1a5-874ef1743909", "rubric": {"category_name": "Manufacturing | Mechanical Engineering | Rugged Flashlight Concept (Toasty)", "rationale": "Mixed task: CAD artifacts (STEP/ZIP) plus engineering drawings (PDF). Stage 1 uses an LLM gate to enforce the exact structural shape of deliverables in the PDFs (assemblies/sub-assemblies, exploded and assembled views, BOM with materials, title block with tolerances, ANSI B landscape). Stage 2 runs code rules to verify presence of STEP/ZIP and text-extractable drawing content (BOM/materials, sealing/IP cues, exploded/title-block signals), plus LLM checks for usability constraints and CNC manufacturability. Stage 3 is an LLM quality assessment of the overall concept, manufacturability, and communication quality.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Deliverables Shape Gate (LLM-only)", "description": "Gate: Verify that the output set includes professional engineering drawings with the exact required structure so downstream verification is possible. This gate checks drawing-format structure and essential drawing content only (not correctness).", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Drawing Set Shape and Required Elements", "description": "LLM checks that the provided drawing PDFs for Toasty have the mandated sections/format and that CAD deliverables are referenced.", "weight": 8.0, "judge_prompt": "You are evaluating deliverables for a rugged flashlight concept called \"Toasty\". Consider ALL provided files. Focus only on format/structure and visible presence of required elements, not on whether the design is correct.\n\nRequired drawing deliverables and structure (be flexible with exact wording of headers, but the elements must be visibly present):\n\nA) PDF Drawings Set\n- Final Assembly drawing PDF must exist.\n- Each Sub-Assembly must have its own PDF drawing (if multiple sub-assemblies, there should be one per sub-assembly; exact naming may vary such as \"Head Assembly\", \"Tailcap Assembly\", etc.).\n- Each assembly/sub-assembly PDF must be ANSI B (11x17) in landscape orientation.\n- Each assembly/sub-assembly PDF must include BOTH:\n  1) An assembled view\n  2) An exploded view\n- Each drawing must contain a professional engineering title block with visible tolerance specifications (e.g., a general tolerance block or a section stating tolerances such as \"UNLESS OTHERWISE SPECIFIED: ... TOLERANCES ...\").\n- Each drawing must show a clear SCALE (e.g., \"SCALE: 1:2\"), either in the title block or near the view.\n- Each drawing must include a Bill of Materials (BOM) table with at least these columns visible (or clear equivalents): Item, Quantity (QTY), Part Number/Name, Material. The BOM items must be called out by balloons (numbered callouts) that link to the table items.\n- The product name \"Toasty\" or clear equivalent project identification must appear in the title block or drawing notes.\n\nB) CAD Models Indicator\n- The submission must either include STEP model files or a ZIP containing STEP models. Because we cannot open non-PDF files here, we will accept explicit indications in the drawing set that STEP models are provided (e.g., a note or a small table listing associated STEP filenames). This can be on the assembly drawing(s) or on a simple cover page/index sheet.\n\nScoring guidance (0\u20138):\n- 8.0: Final assembly + all sub-assembly PDFs present; each shows assembled and exploded views; ANSI B landscape; title block with tolerances; scale; BOM table with materials and balloons; and an explicit note/table referencing associated STEP models (or a ZIP of STEP models is present among files).\n- 7.0: All core drawing elements present (assembly + sub-assemblies; assembled + exploded; ANSI B; title block with tolerances; scale; BOM with materials + balloons) but no explicit STEP model reference in drawings (other files may still include STEP/ZIP).\n- 5.0: Missing one major element (e.g., no exploded view, or no BOM materials column, or no tolerance info) OR sub-assembly coverage incomplete; otherwise professionally structured.\n- 3.0: Only a partial drawing set (e.g., only final assembly without sub-assemblies) or multiple structural elements missing; basic professional structure still visible.\n- 0.0: Not PDF/DOCX drawings, or not engineering-style drawings, or less than one assembly drawing, or clearly wrong format.\n\nImportant: Do NOT judge design correctness or manufacturability; only verify the structural presence and format of the deliverables.", "expectation": "A professionally formatted assembly and sub-assembly drawing set in ANSI B landscape with assembled and exploded views, title block with tolerances, scale, and BOM (with materials + balloons). Clear indication that STEP models are included (explicitly referenced or provided as files)."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Code + LLM)", "description": "Now that the structure exists, verify key requirements with deterministic checks and LLM validation for aspects not trivially machine-checked.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Deliverable Set Completeness and ZIP Compliance", "description": "Verify that STEP/ZIP and assembly PDFs are present, and that exploded views are indicated by text. Penalize if >5 STEP files are present without a ZIP.", "weight": 2.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    # Consider all outputs\n    resources = context.get_all_outputs() or []\n\n    # Counters\n    step_files = 0\n    zip_present = False\n    pdf_resources = []\n\n    # Gather resources by type/extension\n    for r in resources:\n        try:\n            p = context.files.get_path(r.id)\n            ext = (p.suffix or '').lower()\n            if ext in ['.step', '.stp']:\n                step_files += 1\n            elif ext == '.zip':\n                zip_present = True\n            elif ext == '.pdf' or (r.is_document and ext == '.pdf'):\n                pdf_resources.append(r)\n        except Exception:\n            continue\n\n    # Read PDF texts\n    pdf_texts = []\n    for r in pdf_resources:\n        try:\n            txt = context.files.read_pdf_text(r.id) or ''\n            pdf_texts.append(txt.lower())\n        except Exception:\n            pdf_texts.append('')\n\n    combined = ' '.join(pdf_texts)\n\n    # Heuristic checks in PDFs\n    has_assembly_text = bool(re.search(r'\\bassembly\\b|\\basm\\b', combined))\n    has_exploded_text = 'exploded' in combined or 'explode' in combined\n\n    # Scoring (max 2.0)\n    score = 0.0\n    feedback_parts = []\n\n    if step_files > 0 or zip_present:\n        score += 0.8\n        feedback_parts.append(f\"STEP/ZIP present (STEP count={step_files}, ZIP={zip_present}).\")\n    else:\n        feedback_parts.append(\"No STEP or ZIP file detected.\")\n\n    if len(pdf_resources) > 0:\n        score += 0.6\n        feedback_parts.append(f\"PDF drawings detected (count={len(pdf_resources)}).\")\n    else:\n        feedback_parts.append(\"No PDF drawings detected.\")\n\n    if has_assembly_text:\n        score += 0.3\n        feedback_parts.append(\"Assembly text found in drawings.\")\n    else:\n        feedback_parts.append(\"No clear 'assembly' text found.\")\n\n    if has_exploded_text:\n        score += 0.3\n        feedback_parts.append(\"Exploded view referenced in drawings.\")\n    else:\n        feedback_parts.append(\"No 'exploded' reference found.\")\n\n    if step_files > 5 and not zip_present:\n        score -= 0.3\n        feedback_parts.append(\">5 STEP files but no ZIP provided (penalty).\")\n\n    # Clamp to [0, 2]\n    score = max(0.0, min(2.0, score))\n    return score, ' '.join(feedback_parts)\n"}, {"type": "code", "name": "BOM Materials \u2014 Corrosion-Resistant Coverage", "description": "From PDF text, verify a BOM exists, has a Material column, and includes corrosion-resistant selections (e.g., Aluminum 6061/7075, SS 316, Titanium, polymer optics/insulators, elastomers like NBR/Viton, or coatings like anodize).", "weight": 2.0, "code": "import re\n\ndef evaluate(workflow, context):\n    resources = context.get_all_outputs() or []\n    pdf_texts = []\n    for r in resources:\n        try:\n            p = context.files.get_path(r.id)\n            if (p.suffix or '').lower() == '.pdf':\n                txt = context.files.read_pdf_text(r.id) or ''\n                pdf_texts.append(txt.lower())\n        except Exception:\n            continue\n\n    combined = ' '.join(pdf_texts)\n\n    # Signals\n    has_bom = ('bill of materials' in combined) or bool(re.search(r'\\bbom\\b', combined))\n    has_material_col = 'material' in combined\n\n    # Material keywords\n    metals = [\n        'aluminum', 'aluminium', '6061', '7075', 'anodize', 'anodised', 'hardcoat',\n        'stainless', 'ss316', '316', '316l', 'titanium', 'ti-6al-4v', 'ti6al4v'\n    ]\n    polymers = [\n        'polycarbonate', 'pc', 'acetal', 'delrin', 'pom', 'peek', 'ptfe', 'nylon', 'pa6', 'pa66'\n    ]\n    elastomers = [\n        'o-ring', 'oring', 'nitrile', 'nbr', 'viton', 'fkm', 'epdm', 'silicone'\n    ]\n\n    found_metal = any(k in combined for k in metals)\n    found_polymer = any(k in combined for k in polymers)\n    found_elastomer = any(k in combined for k in elastomers)\n\n    # Scoring (max 2.0)\n    score = 0.0\n    fb = []\n\n    if has_bom:\n        score += 0.5\n        fb.append('BOM detected.')\n    else:\n        fb.append('No BOM detected.')\n\n    if has_material_col:\n        score += 0.5\n        fb.append('Material column/mentions detected.')\n    else:\n        fb.append('No clear Material column.')\n\n    # Credit for corrosion-resistant selections\n    if found_metal:\n        score += 0.5\n        fb.append('Corrosion-resistant metal indicated.')\n    else:\n        fb.append('No corrosion-resistant metal keyword found.')\n\n    if found_elastomer or found_polymer:\n        score += 0.5\n        fb.append('Complementary material (polymer and/or elastomer) indicated.')\n    else:\n        fb.append('No polymer/elastomer keyword found.')\n\n    score = max(0.0, min(2.0, score))\n    return score, ' '.join(fb)\n"}, {"type": "code", "name": "Sealing and IP/Ingress Cues", "description": "From PDF text, verify presence of sealing features and/or ingress protection cues (O-rings/gaskets, IP ratings, seal notes).", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    resources = context.get_all_outputs() or []\n    pdf_texts = []\n    for r in resources:\n        try:\n            p = context.files.get_path(r.id)\n            if (p.suffix or '').lower() == '.pdf':\n                txt = context.files.read_pdf_text(r.id) or ''\n                pdf_texts.append(txt.lower())\n        except Exception:\n            continue\n\n    combined = ' '.join(pdf_texts)\n\n    seals = ['o-ring', 'oring', 'gasket', 'seal ring', 'as568', 'gland']\n    ip_ratings = ['ip67', 'ip68', 'ipx7', 'ipx8']\n    ingress_terms = ['ingress', 'waterproof', 'sealed', 'seal']\n\n    has_seal = any(k in combined for k in seals)\n    has_ip = any(k in combined for k in ip_ratings)\n    has_ingress = any(k in combined for k in ingress_terms)\n\n    score = 0.0\n    fb = []\n\n    if has_seal:\n        score += 0.9\n        fb.append('Sealing feature (O-ring/gasket) referenced.')\n    else:\n        fb.append('No explicit O-ring/gasket reference found.')\n\n    if has_ip:\n        score += 0.3\n        fb.append('IP rating mentioned.')\n    if has_ingress:\n        score += 0.3\n        fb.append('Ingress/waterproof sealing notes present.')\n\n    score = max(0.0, min(1.5, score))\n    return score, ' '.join(fb)\n"}, {"type": "code", "name": "Exploded/Assembled Views and Title-Block Signals", "description": "From PDF text, verify presence of exploded/assembly mentions, BOM callout signals, scale, tolerance/title block, and ANSI B cues.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    resources = context.get_all_outputs() or []\n    pdf_texts = []\n    for r in resources:\n        try:\n            p = context.files.get_path(r.id)\n            if (p.suffix or '').lower() == '.pdf':\n                txt = context.files.read_pdf_text(r.id) or ''\n                pdf_texts.append(txt.lower())\n        except Exception:\n            continue\n\n    combined = ' '.join(pdf_texts)\n\n    score = 0.0\n    fb = []\n\n    if 'exploded' in combined or 'explode' in combined:\n        score += 0.4\n        fb.append('Exploded view reference found.')\n    else:\n        fb.append('No exploded reference found.')\n\n    if 'item' in combined and ('qty' in combined or 'quantity' in combined):\n        score += 0.2\n        fb.append('BOM callout signals (ITEM/QTY) present.')\n    else:\n        fb.append('BOM callout signals not clearly detected.')\n\n    if 'tolerance' in combined or 'unless otherwise specified' in combined:\n        score += 0.2\n        fb.append('Tolerance/title-block language found.')\n    else:\n        fb.append('No obvious tolerance language detected.')\n\n    if 'scale' in combined:\n        score += 0.2\n        fb.append('Scale indicated.')\n    else:\n        fb.append('No scale text found.')\n\n    if 'ansi b' in combined or 'size b' in combined or '11x17' in combined or '11 x 17' in combined:\n        score += 0.2\n        fb.append('ANSI B / 11x17 cue detected.')\n    else:\n        fb.append('No ANSI B/11x17 cue detected.')\n\n    score = max(0.0, min(1.0, score))\n    return score, ' '.join(fb)\n"}, {"type": "llm_judge", "name": "Usability and Field-Service Requirements Evidenced", "description": "LLM checks if drawings and notes visibly support key requirements: two 18650 cells in series, user-replaceable in the field without tools while wearing gloves; glove-operable switch; grip feature; interchangeable metal belt clip.", "weight": 1.0, "judge_prompt": "Check the drawing set for visible evidence of the following requirements (do not judge beauty, only presence/evidence):\n1) Power source: Two 18650 cells in series (look for notes like \"2x 18650\", battery bay length, series/pack notation, polarity indicators, or a battery compartment in exploded view).\n2) Field replacement without tools while wearing gloves: Look for a threaded or quarter-turn tailcap/body interface with O-ring, a manual latch, tabs/knurling/large grip features on the cap, and explicit notes like \"no tools required\" or glove-friendly instructions.\n3) Glove-operable switch: Look for a large tail or side switch, raised boot, guard, or note that it is operable with gloves.\n4) Grip feature on the flashlight body: Look for knurling, scallops, overmold-like grooves, or machined texture called out in notes.\n5) Interchangeable metal belt clip: Look for a metal clip item in the BOM and depiction in assembled/exploded views, with mounting method (screws, groove, or ring).\n\nScoring (0\u20131.0):\n- 1.0: Clear evidence for at least 4 of the 5 items above (must include the battery arrangement OR explicit two 18650 mention).\n- 0.7: Clear evidence for 3 items.\n- 0.4: Evidence for 2 items.\n- 0.1: Evidence for only 1 item.\n- 0.0: No visible evidence for these items.\n\nBe conservative but fair; rely only on what is visible in the drawings/notes/BOM.", "expectation": "Drawings show battery arrangement, glove-friendly access and switch, body grip features, and an interchangeable metal belt clip with mounting details."}, {"type": "llm_judge", "name": "CNC Prototyping Manufacturability Signals", "description": "LLM checks if parts appear manufacturable by common CNC prototyping methods (machinable features, reasonable tolerances/threads, fewer hidden undercuts, accessible geometries) based on the drawings.", "weight": 0.5, "judge_prompt": "From the assembly/sub-assembly drawings, assess whether the concept is plausible for CNC prototyping:\n- Look for machinable features (turned body/tailcap/head, milled pockets, standard threads like UNC/UNF/Metric with callouts, standard O-ring gland callouts).\n- Avoidance of impossible undercuts without special tooling; split lines that make sense for assembly and machining.\n- Reasonable general tolerances for prototype work (not overly tight without cause).\n- Materials called out are common for CNC (e.g., 6061-T6, 7075-T6, 316 SS, Titanium) and thicknesses/stock sizes seem plausible.\n\nScoring (0\u20130.5):\n- 0.5: Strong evidence the assembly can be prototyped with common CNC processes.\n- 0.3: Generally plausible but some ambiguous features.\n- 0.1: Weak/uncertain; many unclear features.\n- 0.0: Not plausible for typical CNC prototyping from the drawings.", "expectation": "Evidence of CNC-suitable geometry, realistic tolerances, and standard threads/feature callouts."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Overall Quality (LLM)", "description": "Professionalism, coherence of the concept, clarity of communication, and likely performance sufficiency for the specified environment.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Concept Quality and Communication", "description": "LLM assesses overall design quality: clarity, completeness, environmental suitability, and presentation polish.", "weight": 4.0, "judge_prompt": "Holistically assess the submission quality for the Toasty rugged flashlight concept. Consider:\n- Coherence of the overall assembly architecture and sub-assembly breakdown.\n- Likely environmental suitability: materials and sealing likely to resist corrosion and water ingress; visible thermal management features (heat sinking, fins, massing) appropriate for operation in -20\u00b0C to 40\u00b0C; no obvious overheating risk under typical flashlight loads indicated by the design notes/features.\n- Professional communication: drawings are clear, views/readability balanced, balloons legible and consistent with BOM, titles/notes unambiguous, and scale/tolerances sensible for prototypes.\n- Appropriateness to the concept phase: not over-specified on production details, but sufficient to CNC prototype.\n\nScoring (0\u20134):\n- 4: Excellent across all criteria; very professional and concept seems ready for CNC prototyping.\n- 3: Strong overall with minor issues.\n- 2: Adequate but with notable gaps.\n- 1: Poorly communicated or questionable feasibility.\n- 0: Unclear or not meeting the spirit of the task.\n\nDo not penalize for missing final production details or detailed electronics; focus on mechanical concept and communication quality.", "expectation": "Clear, professional, CNC-prototyping-ready concept drawings with credible environmental/thermal considerations and strong overall communication."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "552b7dd0-96f4-437c-a749-0691e0e4b381", "rubric": {"category_name": "Manufacturing | Shipping/Receiving/Inventory Clerks \u2014 Incident Analysis Presentation", "rationale": "Pattern C (Mixed). The task requires a data-driven presentation built from incident data. Stage 1 strictly enforces a PowerPoint/PDF slide structure that makes verification trivial. Stage 2 mixes code (for workbook shape and numeric consistency if a companion analysis workbook is provided) with LLM checks (cross-slide consistency). Stage 3 evaluates professional quality and managerial utility.", "max_total_score": 25.0, "stages": [{"name": "Stage 1 \u2014 Presentation Structure and Format Gate", "description": "Gate: The output must be a properly structured presentation file that makes verification possible. LLM-only check of file format and slide structure.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Structured Presentation Requirements", "description": "Verify the candidate produced a PowerPoint-style presentation with the exact structural elements needed for later verification.", "weight": 8.0, "judge_prompt": "You are evaluating whether the candidate\u2019s primary output is a properly structured presentation for an annual incident analysis. Only check presence/format/structure, not content correctness.\n\nAcceptable formats: PowerPoint (.pptx) or a PDF exported from slides.\n\nMinimum length: 7 slides.\n\nRequired structure (flexible on exact slide titles, but the content must be clearly present):\n1) Title Slide (must be first slide)\n   - Includes a clear title mentioning \u201cIncident Analysis\u201d and a year.\n   - Includes company/warehouse context or team/author.\n\n2) Incidents by Supplier (counts)\n   - A slide that shows the number of incidents per supplier.\n   - Must include a tabular view of counts or clearly labeled values.\n   - Must include a visual (bar/column chart preferred) representing counts by supplier.\n\n3) Incident Share by Supplier (percent of total)\n   - A slide that presents percentage share of incidents by supplier compared to total incidents.\n   - Must include a visual (pie, stacked bar, or labeled bar) that communicates percentages.\n   - Percent values should be visible as labels or in a simple table.\n\n4) Total Cost of Resolving Incidents\n   - A slide that states an overall total cost figure for the year.\n   - May also show cost by supplier or top cost drivers (table or chart). Visual not mandatory here but preferred.\n\n5) Resolution Time (Overall and by Type)\n   - A slide that shows the average time to resolve incidents overall.\n   - Also shows separate statistics for RMAs and Work Orders (e.g., average days for each; can be in a table or clearly labeled figures). A simple visual (e.g., grouped bars) is preferred.\n\n6) Summary and Recommendations (Conclusion)\n   - A final slide that synthesizes the findings into 3\u20136 key takeaways.\n   - Must explicitly reference common or recurring themes from incident descriptions (e.g., recurring damage types, recurring supplier issues, packaging defects).\n   - Must include actionable recommendations addressed to supplier and warehouse management.\n\n7) Appendix / Methodology / Data Notes (may be a single slide)\n   - Briefly lists data source (year-long incident data), definitions (RMA vs Work Order), and any assumptions or caveats.\n\nScoring (return a fractional score from 0.0 to 8.0 based on these criteria):\n- 8.0: Valid PPTX or slide PDF, >=7 slides, and all 7 required elements present (allow minor title wording differences). Contains the two visuals for counts and percent share.\n- 6.0\u20137.5: Valid format and length, missing exactly one required element OR one of the two visuals.\n- 3.0\u20135.5: Valid format but 2\u20133 required elements missing or visuals absent; structure is incomplete but clearly a slide deck.\n- 1.0\u20132.5: Valid format but only 1\u20132 key slides present, or largely unstructured.\n- 0.0: Not a PPT/PDF deck, or fewer than 7 slides, or grossly wrong format.\n\nOnly evaluate presence and structure, not whether numbers are correct.", "expectation": "A professional PPTX/PDF deck with clearly labeled slides for counts by supplier, percentage share, total cost, resolution time (overall + by type), and a concluding summary with recommendations, plus an appendix/methods slide."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification: Data Shape and Numerical Consistency", "description": "Now that the presentation structure exists, verify correctness/consistency using a companion analysis workbook if provided and cross-check internal logic.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Companion Analysis Workbook \u2014 Presence and Shape", "description": "Detect a spreadsheet among outputs and verify it contains reasonably structured sheets to support the presentation: incidents by supplier, percent share, costs, and durations/statistics.", "weight": 4.0, "code": "import re\nimport json\nimport pandas as pd\nimport numpy as np\n\ndef _get_spreadsheets(context):\n    spreadsheets = []\n    for r in context.get_all_outputs():\n        try:\n            if getattr(r, 'is_spreadsheet', False):\n                spreadsheets.append(r)\n        except Exception:\n            continue\n    return spreadsheets\n\ndef _read_excel_sheets(path):\n    try:\n        xl = pd.ExcelFile(path)\n        sheets = {}\n        for sn in xl.sheet_names:\n            try:\n                df = xl.parse(sn)\n                sheets[sn] = df\n            except Exception:\n                continue\n        return sheets\n    except Exception:\n        return None\n\ndef _std_cols(df):\n    cols = []\n    for c in df.columns:\n        try:\n            cols.append(str(c).strip().lower())\n        except Exception:\n            cols.append('')\n    return cols\n\ndef _find_sheet_by_keywords(sheet_names, *keywords):\n    lname_map = {sn: sn.lower() for sn in sheet_names}\n    for sn, ln in lname_map.items():\n        if all(k in ln for k in keywords):\n            return sn\n    # fallback: any sheet containing at least one of the keywords\n    for sn, ln in lname_map.items():\n        if any(k in ln for k in keywords):\n            return sn\n    return None\n\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Returns up to 4.0 points based on presence/shape of a companion spreadsheet.\n    \"\"\"\n    spreadsheets = _get_spreadsheets(context)\n    if not spreadsheets:\n        return (0.0, 'No spreadsheet output found to verify analysis shape.')\n\n    best_score = 0.0\n    best_feedback = ''\n\n    for sp in spreadsheets:\n        try:\n            path = context.files.get_path(sp.id)\n            sheets = _read_excel_sheets(path)\n            if sheets is None or len(sheets) == 0:\n                continue\n            sheet_names = list(sheets.keys())\n\n            incidents_sn = _find_sheet_by_keywords(sheet_names, 'supplier') or _find_sheet_by_keywords(sheet_names, 'incident')\n            percent_sn = _find_sheet_by_keywords(sheet_names, 'percent') or incidents_sn\n            costs_sn = _find_sheet_by_keywords(sheet_names, 'cost')\n            dur_sn = _find_sheet_by_keywords(sheet_names, 'duration') or _find_sheet_by_keywords(sheet_names, 'resolution') or _find_sheet_by_keywords(sheet_names, 'time')\n            summary_sn = _find_sheet_by_keywords(sheet_names, 'summary') or _find_sheet_by_keywords(sheet_names, 'metric') or _find_sheet_by_keywords(sheet_names, 'overview')\n\n            score = 0.0\n            details = []\n\n            # Incidents by supplier\n            if incidents_sn:\n                df_inc = sheets[incidents_sn]\n                cols = _std_cols(df_inc)\n                has_supplier = any('supplier' in c for c in cols)\n                has_count = any(('incident' in c and ('count' in c or 'no' in c or '#' in c)) or c in ['count','incidents'] for c in cols)\n                # percent may be in same sheet\n                has_percent = any('%' in c or 'percent' in c or 'share' in c for c in cols)\n                if has_supplier and has_count:\n                    # Base credit\n                    score += 1.0\n                    details.append(f\"Incidents sheet '{incidents_sn}' present with supplier and count.\")\n                    # small extra partial if percent also here\n                    if has_percent:\n                        score += 0.25\n                        details.append('Percent column also present in incidents sheet.')\n                else:\n                    score += 0.5  # partial: sheet exists but columns weak\n                    details.append(f\"Incidents sheet '{incidents_sn}' found but missing clear supplier/count columns.\")\n            else:\n                details.append('No incidents-by-supplier sheet located.')\n\n            # Percent share (if separate)\n            if percent_sn and percent_sn != incidents_sn:\n                df_pct = sheets[percent_sn]\n                cols = _std_cols(df_pct)\n                has_supplier = any('supplier' in c for c in cols)\n                has_percent = any('%' in c or 'percent' in c or 'share' in c for c in cols)\n                if has_supplier and has_percent:\n                    score += 0.75\n                    details.append(f\"Percent/share sheet '{percent_sn}' present with supplier and percent.\")\n                else:\n                    score += 0.25\n                    details.append(f\"Percent/share sheet '{percent_sn}' found but missing clear columns.\")\n\n            # Costs\n            if costs_sn:\n                df_cost = sheets[costs_sn]\n                cols = _std_cols(df_cost)\n                has_supplier = any('supplier' in c for c in cols)\n                has_cost = any('cost' in c or '$' in c for c in cols)\n                if has_supplier and has_cost:\n                    score += 1.0\n                    details.append(f\"Costs sheet '{costs_sn}' present with supplier and cost.\")\n                else:\n                    score += 0.5\n                    details.append(f\"Costs sheet '{costs_sn}' found but missing clear supplier/cost columns.\")\n            else:\n                details.append('No costs sheet located.')\n\n            # Durations\n            if dur_sn:\n                df_d = sheets[dur_sn]\n                cols = _std_cols(df_d)\n                # Look for overall and by type columns or rows\n                has_overall = any(('avg' in c or 'mean' in c) and ('day' in c or 'duration' in c or 'time' in c) for c in cols)\n                has_rma = any(('rma' in c) and ('day' in c or 'duration' in c or 'time' in c or 'avg' in c) for c in cols) or ('rma' in ' '.join(cols))\n                has_wo = any(('work' in c and 'order' in c) and ('day' in c or 'duration' in c or 'time' in c or 'avg' in c) for c in cols) or ('work order' in ' '.join(cols))\n                if has_overall and has_rma and has_wo:\n                    score += 1.0\n                    details.append(f\"Durations sheet '{dur_sn}' present with overall and by-type stats (RMA & Work Order).\")\n                elif has_overall and (has_rma or has_wo):\n                    score += 0.75\n                    details.append(f\"Durations sheet '{dur_sn}' present with overall and partial by-type stats.\")\n                else:\n                    score += 0.5\n                    details.append(f\"Durations sheet '{dur_sn}' found but missing overall/by-type clarity.\")\n            else:\n                details.append('No durations/resolution time sheet located.')\n\n            # Summary (optional but preferred)\n            if summary_sn:\n                score += 0.5\n                details.append(f\"Summary/metrics sheet '{summary_sn}' present (bonus).\")\n\n            # Cap score at 4.0\n            score = max(0.0, min(4.0, score))\n\n            if score > best_score:\n                best_score = score\n                best_feedback = '; '.join(details)\n        except Exception as e:\n            continue\n\n    if best_score <= 0:\n        return (0.0, 'Spreadsheet(s) present but could not validate required sheets/columns.')\n    return (best_score, best_feedback)\n"}, {"type": "code", "name": "Numerical Plausibility and Cross-Table Consistency", "description": "If a companion workbook is present, verify that key aggregates and percentages are plausible and internally consistent.", "weight": 3.0, "code": "import re\nimport json\nimport pandas as pd\nimport numpy as np\n\n\ndef _get_spreadsheets(context):\n    sps = []\n    for r in context.get_all_outputs():\n        try:\n            if getattr(r, 'is_spreadsheet', False):\n                sps.append(r)\n        except Exception:\n            pass\n    return sps\n\ndef _read_excel(path):\n    try:\n        xl = pd.ExcelFile(path)\n        return {sn: xl.parse(sn) for sn in xl.sheet_names}\n    except Exception:\n        return None\n\ndef _std_cols(df):\n    return [str(c).strip().lower() for c in df.columns]\n\ndef _find_sheet(sheets, *keys_any):\n    for sn in sheets.keys():\n        ln = sn.lower()\n        if any(k in ln for k in keys_any):\n            return sn\n    return None\n\ndef _to_num(x):\n    if pd.isna(x):\n        return np.nan\n    try:\n        if isinstance(x, (int, float, np.integer, np.floating)):\n            return float(x)\n        s = str(x)\n        s = s.replace(',', '')\n        s = re.sub(r'[^0-9\\.\\-]', '', s)\n        if s in ['', '-', '.', '-.']:\n            return np.nan\n        return float(s)\n    except Exception:\n        return np.nan\n\ndef _extract_metric(df, key_words):\n    # Search anywhere in the frame; if a cell contains all keywords, grab next cell in same row that parses numeric\n    try:\n        for r in range(min(len(df), 200)):\n            for c in range(min(len(df.columns), 20)):\n                val = str(df.iat[r, c]).strip().lower()\n                if all(kw in val for kw in key_words):\n                    # try right cell, then next columns in row\n                    for cc in range(c+1, min(c+5, len(df.columns))):\n                        num = _to_num(df.iat[r, cc])\n                        if pd.notna(num):\n                            return num\n    except Exception:\n        pass\n    return np.nan\n\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Returns up to 3.0 points based on numeric plausibility and consistency checks.\n    \"\"\"\n    sps = _get_spreadsheets(context)\n    if not sps:\n        return (0.0, 'No companion workbook to verify numeric consistency.')\n\n    best = 0.0\n    best_fb = ''\n\n    for sp in sps:\n        try:\n            path = context.files.get_path(sp.id)\n            sheets = _read_excel(path)\n            if not sheets:\n                continue\n\n            incidents_sn = _find_sheet(sheets, 'supplier', 'incident')\n            percent_sn = _find_sheet(sheets, 'percent', 'share') or incidents_sn\n            costs_sn = _find_sheet(sheets, 'cost')\n            dur_sn = _find_sheet(sheets, 'duration') or _find_sheet(sheets, 'resolution') or _find_sheet(sheets, 'time')\n            summary_sn = _find_sheet(sheets, 'summary') or _find_sheet(sheets, 'metric') or _find_sheet(sheets, 'overview')\n\n            checks = 0\n            passes = 0\n            notes = []\n\n            # Percent sum ~ 100%\n            if percent_sn:\n                dfp = sheets[percent_sn]\n                cols = _std_cols(dfp)\n                pct_col = None\n                for c in dfp.columns:\n                    lc = str(c).lower()\n                    if ('%' in lc) or ('percent' in lc) or ('share' in lc):\n                        pct_col = c\n                        break\n                if pct_col is None:\n                    # maybe percent in incidents sheet columns\n                    if incidents_sn and percent_sn != incidents_sn:\n                        dfp = sheets[incidents_sn]\n                        cols = _std_cols(dfp)\n                        for c in dfp.columns:\n                            lc = str(c).lower()\n                            if ('%' in lc) or ('percent' in lc) or ('share' in lc):\n                                pct_col = c\n                                break\n                if pct_col is not None:\n                    checks += 1\n                    vals = [_to_num(v) for v in dfp[pct_col].values]\n                    vals = [v for v in vals if pd.notna(v)]\n                    if len(vals) >= 2:\n                        s = np.nansum(vals)\n                        # handle 0-1 vs 0-100 scale\n                        ok = (98 <= s <= 102) or (0.98 <= s <= 1.02)\n                        if ok:\n                            passes += 1\n                            notes.append(f\"Percent sum plausible ({s:.2f}).\")\n                        else:\n                            notes.append(f\"Percent sum off ({s:.2f}).\")\n            \n            # Total incidents consistency\n            total_inc = np.nan\n            if summary_sn:\n                total_inc = _extract_metric(sheets[summary_sn], ['total', 'incident'])\n            if incidents_sn:\n                dfi = sheets[incidents_sn]\n                cols = _std_cols(dfi)\n                cnt_col = None\n                for c in dfi.columns:\n                    lc = str(c).lower()\n                    if ('incident' in lc and ('count' in lc or 'no' in lc or '#' in lc)) or lc in ['count','incidents','incident count']:\n                        cnt_col = c\n                        break\n                if cnt_col is None:\n                    # fallback: any numeric column other than supplier\n                    for c in dfi.columns:\n                        lc = str(c).lower()\n                        if 'supplier' in lc:\n                            continue\n                        if pd.api.types.is_numeric_dtype(dfi[c]) or np.nanmean([_to_num(x) for x in dfi[c].head(10)]) >= 0:\n                            cnt_col = c\n                            break\n                if cnt_col is not None:\n                    checks += 1\n                    s_cnt = np.nansum([_to_num(v) for v in dfi[cnt_col].values])\n                    if pd.notna(total_inc):\n                        if abs(s_cnt - total_inc) <= max(2, 0.02 * max(1, total_inc)):\n                            passes += 1\n                            notes.append(f\"Total incidents match (sum {s_cnt:.0f} vs summary {total_inc:.0f}).\")\n                        else:\n                            notes.append(f\"Total incidents mismatch (sum {s_cnt:.0f} vs summary {total_inc:.0f}).\")\n                    else:\n                        # No summary; accept plausibility if integer and >0\n                        if s_cnt > 0:\n                            passes += 1\n                            notes.append(f\"Total incidents plausible (sum {s_cnt:.0f}).\")\n                        else:\n                            notes.append(\"Total incidents not positive.\")\n\n            # Costs non-negative and total aligns\n            if costs_sn:\n                dfc = sheets[costs_sn]\n                cols = _std_cols(dfc)\n                cost_col = None\n                for c in dfc.columns:\n                    lc = str(c).lower()\n                    if 'cost' in lc or '$' in lc:\n                        cost_col = c\n                        break\n                if cost_col is not None:\n                    checks += 1\n                    vals = [_to_num(v) for v in dfc[cost_col].values]\n                    if all((pd.isna(v) or v >= 0) for v in vals):\n                        passes += 1\n                        notes.append(\"Costs non-negative.\")\n                    else:\n                        notes.append(\"Negative cost detected.\")\n\n                    # Compare to summary total cost if available\n                    total_cost = np.nan\n                    if summary_sn:\n                        total_cost = _extract_metric(sheets[summary_sn], ['total', 'cost'])\n                    if pd.notna(total_cost):\n                        checks += 1\n                        s_cost = np.nansum([v for v in vals if pd.notna(v)])\n                        if abs(s_cost - total_cost) <= max(100.0, 0.02 * max(1.0, total_cost)):\n                            passes += 1\n                            notes.append(f\"Total cost matches (sum {s_cost:,.2f} vs summary {total_cost:,.2f}).\")\n                        else:\n                            notes.append(f\"Total cost mismatch (sum {s_cost:,.2f} vs summary {total_cost:,.2f}).\")\n\n            # Durations plausible (overall + by type)\n            if dur_sn:\n                dfd = sheets[dur_sn]\n                cols = _std_cols(dfd)\n                # try to find numeric columns with likely duration meaning\n                cand_cols = []\n                for c in dfd.columns:\n                    lc = str(c).lower()\n                    if any(k in lc for k in ['avg','mean','days','duration','resolution','time']):\n                        cand_cols.append(c)\n                # Identify RMA and Work Order scopes\n                has_rma = any('rma' in ' '.join(cols) for _ in [0])\n                has_wo = any(('work' in ' '.join(cols)) and ('order' in ' '.join(cols)) for _ in [0])\n\n                # If rows encode types, try to compute stats\n                def plausible_duration(x):\n                    x = _to_num(x)\n                    return pd.notna(x) and (0 < x <= 366)\n\n                if cand_cols:\n                    checks += 1\n                    # check any plausible duration in data\n                    vals = []\n                    for c in cand_cols:\n                        series = dfd[c].head(50)\n                        vals.extend([_to_num(v) for v in series])\n                    plaus = [plausible_duration(v) for v in vals if pd.notna(v)]\n                    if any(plaus):\n                        passes += 1\n                        notes.append(\"At least one duration metric within 0\u2013366 days.\")\n                    else:\n                        notes.append(\"No plausible duration metrics found (0\u2013366 days).\")\n\n                    # bonus check: by-type presence\n                    checks += 1\n                    if has_rma and has_wo:\n                        passes += 1\n                        notes.append(\"By-type (RMA & Work Order) duration stats present.\")\n                    else:\n                        notes.append(\"By-type duration stats not clearly identified.\")\n\n            total_checks = max(1, checks)\n            frac = passes / total_checks\n            score = 3.0 * frac\n\n            if score > best:\n                best = score\n                best_fb = '; '.join(notes) if notes else 'Numeric checks computed.'\n        except Exception:\n            continue\n\n    if best <= 0:\n        return (0.0, 'Could not run numeric consistency checks on any spreadsheet output.')\n    return (best, best_fb)\n"}, {"type": "llm_judge", "name": "Slide-Level Internal Consistency", "description": "Check whether counts, percentages, and time/cost figures shown across slides are internally consistent and not contradictory. If a workbook is present among outputs, use it as a reference; otherwise, check for internal coherency within the deck.", "weight": 3.0, "judge_prompt": "Evaluate the presentation for internal consistency of the reported metrics. If a companion workbook is attached among outputs, prefer it as a reference. Otherwise, judge internal consistency only. Do not penalize for minor rounding.\n\nCheck:\n- The counts by supplier and the percentage shares align (e.g., the shares sum to ~100% when interpreted across suppliers; the highest count generally has the highest share).\n- The stated total incident count and total cost, if shown in multiple slides, agree across the deck.\n- The average resolution time figures are sensible and consistently reported (overall vs. RMA vs. Work Order); e.g., overall average should fall between the two type-specific averages when weighted; no impossible values (negative days).\n\nScoring (0\u20133 points):\n- 3.0: All figures are coherent and mutually consistent (shares ~100%, totals match across slides, durations make sense).\n- 2.0: Minor discrepancies or rounding differences that do not change conclusions.\n- 1.0: Noticeable inconsistencies (e.g., shares far from 100%, conflicting totals) but a majority of metrics still align.\n- 0.0: Major contradictions or nonsensical values.\n\nAssess consistency only; do not evaluate design quality or narrative strength.", "expectation": "Numbers agree across slides and reflect a coherent story; percentages aggregate properly; averages by type and overall are plausible."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Presentation Quality and Managerial Value", "description": "Holistic assessment of communication quality, insightfulness, and suitability for supplier and warehouse management audiences.", "is_required": false, "max_points": 7.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity and Professionalism", "description": "Assess visual clarity, slide organization, labeling, and overall professionalism.", "weight": 3.0, "judge_prompt": "Evaluate the deck\u2019s clarity and professionalism:\n- Are charts readable with labeled axes/legends and supplier names visible?\n- Are tables cleanly formatted with clear headings and units (e.g., $ for costs, days for durations)?\n- Is the deck visually consistent (fonts, colors) and free of clutter/typos?\n\nScoring (0\u20133):\n- 3: Highly professional, clear charts/tables, excellent readability.\n- 2: Generally clear with minor formatting issues.\n- 1: Readable but rough or inconsistent.\n- 0: Sloppy or confusing presentation.", "expectation": "A clean, readable, and professional slide deck with correctly labeled visuals and consistent styling."}, {"type": "llm_judge", "name": "Analytical Insight and Recommendations", "description": "Evaluate the depth of insights and the actionability of recommendations grounded in recurring incident themes from descriptions.", "weight": 3.0, "judge_prompt": "Assess the insightfulness of the conclusion/summary slide(s):\n- Do the key takeaways clearly synthesize recurring themes from incident descriptions (e.g., packaging damage patterns, recurring supplier defects, process issues at issuance)?\n- Are recommendations specific, actionable, and directed to both supplier management and warehouse management (e.g., vendor quality audits, packaging spec changes, handling SOP updates, training, corrective action requests)?\n- Does the reasoning explicitly tie data patterns to recommendations?\n\nScoring (0\u20133):\n- 3: Clear, data-driven insights with concrete, prioritized actions for suppliers and warehouse teams.\n- 2: Mostly relevant insights with some actionability; minor vagueness.\n- 1: Generic or weak recommendations only loosely tied to data.\n- 0: Little to no insight; recommendations missing or unrelated.", "expectation": "Synthesis identifies specific recurring issues and translates them into targeted, actionable recommendations for both suppliers and warehouse operations."}, {"type": "llm_judge", "name": "Audience Appropriateness", "description": "Evaluate whether the content is tailored to supplier and warehouse management audiences and stays succinct and decision-oriented.", "weight": 1.0, "judge_prompt": "Judge audience fit:\n- Are the findings and recommendations framed for management decision-making (e.g., impact, priority, expected benefit)?\n- Is the narrative concise and focused, avoiding raw data dumps?\n\nScoring (0\u20131):\n- 1: Well-calibrated for management with clear decisions/next steps.\n- 0.5: Mostly appropriate, some verbosity or gaps.\n- 0: Not tailored to management; unclear implications.", "expectation": "Concise, decision-oriented communication appropriate for supplier and warehouse leaders."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "cebf301e-5ea7-41ae-b117-ad8f43e7ac22", "rubric": {"category_name": "CTO Design Document \u2013 Customer Portal (CleanTech) \u2013 Structured, Verified, and Actionable", "rationale": "Pattern B (Document). The deliverable is a 2\u20133 page Word document (DOCX) design brief for a senior engineering team. Stage 1 uses an LLM judge to strictly enforce a precise document structure and format that makes verification possible. Stage 2 mixes code rules to verify coverage of required decisions, constraints, and cross-references (e.g., TOTP now, social later; CI/CD with GitHub Actions; IaC; Node.js/TypeScript API via containers or serverless; integration points with the existing React/Express/PostgreSQL stack). Stage 3 uses LLM judges to assess professional quality, actionability, and strategic alignment.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2013 Format and Structure Gate (LLM Only)", "description": "Gate that enforces exact document shape, sections, and page length so subsequent verification is trivial.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 2.8, "rules": [{"type": "llm_judge", "name": "Document Format and Structural Completeness", "description": "Verify DOCX format and required section structure for the CTO design document.", "weight": 4.0, "judge_prompt": "You are validating a CTO design document for a new Customer Portal. Only check FORMAT and STRUCTURE, not content correctness.\n\nFormat requirements:\n- Must be a Word document (DOCX). PDF is not acceptable for this task.\n- Length guideline: approximately 2\u20133 pages. Accept 2\u20134 pages; penalize if <2 or >4.\n- Professional formatting with clear section headers.\n\nRequired sections (flexible on exact names, but all topics must be clearly present as distinct, headed sections):\n1) Purpose and Goals (or Executive Summary / Overview)\n2) Scope and MVP for V1 (6-week plan) with Out-of-scope explicitly stated\n3) Functional Requirements (V1)\n4) Non-Functional Requirements (security, scalability, performance, mobile/responsive, accessibility)\n5) Architecture & Technical Decisions (must include: React framework recommendation; TOTP-based auth now; plan for future social logins; strict access control; data storage: PostgreSQL for metadata and object storage like S3 for assets; in-browser PDF export; session tokens/cookies with reasonable expiration)\n6) Integration with Existing Sales System (React front-end, Express REST API, PostgreSQL) \u2013 specify integration points\n7) Infrastructure & Deployment (IaC; API in Node.js/TypeScript; via containers or serverless; CI/CD with GitHub Actions)\n8) Data Model Overview (key entities like Lead, User/Customer, Proposal, Asset)\n9) Risks and Mitigations\n10) Open Questions\n11) Timeline & Milestones (enabling six-week launch)\n\nScoring:\n- 4.0: DOCX + clearly headed sections covering all 11 items; 2\u20134 pages.\n- 3.2: DOCX + 9\u201310 items present; minor structural gaps; 2\u20134 pages.\n- 2.4: DOCX + 7\u20138 items present OR length slightly off (1 or >4 pages) but otherwise structured.\n- 1.2: DOCX but only 4\u20136 items present or poorly structured/unclear headers.\n- 0.0: Not DOCX OR fewer than 4 required sections present.\n\nBe flexible with section names (e.g., \"Executive Summary\" vs \"Purpose and Goals\") but ensure the required topics are distinctly represented. Do not evaluate content accuracy; only verify presence and structure.", "expectation": "A 2\u20133 page DOCX with clear headers for all listed sections, enabling automated checks in Stage 2."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness and Coverage Verification (Code + LLM)", "description": "Deterministic checks to verify that required decisions, constraints, and cross-references are explicitly covered and consistent with the task.", "is_required": false, "max_points": 12.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Hard Requirements Coverage", "description": "Check explicit coverage of key hard requirements: TOTP now; social login later; strict access control; Postgres metadata + object storage (S3) for assets; in-browser PDF export; responsive/mobile; sessions with expiry; scalability; GitHub Actions CI/CD; repo strategy; React framework recommendation; integration points with existing system; Infrastructure as Code; Node.js/TypeScript API via containers or serverless.", "weight": 4.0, "code": "def evaluate(workflow, context):\n    import re\n    def get_text(output):\n        text = ''\n        try:\n            if output.is_document:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    try:\n                        text = context.files.read_pdf_text(output.id)\n                    except Exception:\n                        text = ''\n            elif output.is_text_format:\n                text = context.files.read_text(output.id)\n        except Exception:\n            text = ''\n        return text or ''\n\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output file provided.\"\n\n    text = get_text(output).lower()\n    if not text:\n        return 0.0, \"Could not read document text.\"\n\n    checks = []\n    # 1 TOTP now\n    checks.append((bool(re.search(r\"\\b(totp|time[- ]?based one[- ]?time password)\\b\", text)), \"TOTP-based authentication (now)\"))\n    # 2 Social login later\n    checks.append((('social' in text or 'oauth' in text or 'openid' in text or 'google' in text or 'facebook' in text) and ('future' in text or 'later' in text or 'phase 2' in text or 'v2' in text), \"Social login planned for later\"))\n    # 3 Strict access control\n    checks.append(((('access control' in text) or ('authorization' in text) or ('rbac' in text) or ('row-level' in text) or ('scoped' in text)) and ('proposal' in text or 'lead' in text or 'customer' in text)), \"Strict access control per-customer/proposal\"))\n    # 4 Storage: Postgres + object storage/S3\n    checks.append(((('postgres' in text or 'postgresql' in text) and ('s3' in text or 'object storage' in text or 'blob storage' in text)), \"PostgreSQL for metadata and S3/object storage for assets\"))\n    # 5 In-browser PDF export\n    checks.append(((('pdf' in text) and ('export' in text or 'print' in text or 'download' in text)), \"In-browser PDF export\"))\n    # 6 Responsive/mobile\n    checks.append(((\"responsive\" in text or \"mobile\" in text or \"breakpoint\" in text or \"mobile-first\" in text), \"Responsive/mobile support\"))\n    # 7 Sessions with expiry\n    checks.append(((('session' in text or 'cookie' in text or 'jwt' in text) and ('expiry' in text or 'expiration' in text or 'ttl' in text or 'max-age' in text)), \"Session tokens/cookies with expiration\"))\n    # 8 Scalability\n    checks.append(((\"scalab\" in text or 'autoscal' in text or 'horizontal' in text or 'load' in text or 'throughput' in text), \"Scalability plan\"))\n    # 9 GitHub Actions CI/CD\n    checks.append(((\"github actions\" in text and (\"ci/cd\" in text or 'pipeline' in text or 'deploy' in text)), \"GitHub Actions CI/CD\"))\n    # 10 Repo strategy\n    checks.append(((\"monorepo\" in text or \"polyrepo\" in text or \"new repo\" in text), \"Repo strategy recommendation\"))\n    # 11 React framework recommendation\n    checks.append(((\"next.js\" in text or 'remix' in text or 'vite' in text or 'create react app' in text or 'cra' in text), \"React framework recommendation\"))\n    # 12 Integration points with existing system\n    checks.append((((\"integration\" in text or \"integrate\" in text) and (\"rest\" in text or 'api' in text) and (\"express\" in text or 'node' in text) and (\"postgres\" in text or 'postgresql' in text)), \"Integration with existing Express/REST/PostgreSQL\"))\n    # 13 Infrastructure as Code\n    checks.append(((\"terraform\" in text or \"pulumi\" in text or \"cloudformation\" in text or \"cdk\" in text or \"iac\" in text), \"Infrastructure as Code\"))\n    # 14 Node.js/TypeScript API via containers or serverless\n    checks.append(((('node.js' in text or 'nodejs' in text or 'node' in text) and ('typescript' in text or 'ts' in text) and ('container' in text or 'docker' in text or 'serverless' in text or 'lambda' in text or 'cloud run' in text or 'app service' in text)), \"Node.js/TypeScript API via containers or serverless\"))\n\n    passed = [c for c, _ in checks if c]\n    ratio = len(passed) / len(checks)\n    missing = [desc for c, desc in checks if not c]\n    feedback = f\"Covered {len(passed)}/{len(checks)} hard requirements. Missing: \" + (\", \".join(missing) if missing else \"None\")\n    return ratio, feedback\n"}, {"type": "code", "name": "V1 vs Future Scope Clarity", "description": "Verify that the document explicitly distinguishes V1 (6-week MVP) from future features (e.g., social login, contract signing, deposit in-portal).", "weight": 1.5, "code": "def evaluate(workflow, context):\n    import re\n    def get_text(output):\n        try:\n            if output.is_document:\n                try:\n                    return context.files.read_docx_text(output.id)\n                except Exception:\n                    return context.files.read_pdf_text(output.id)\n            elif output.is_text_format:\n                return context.files.read_text(output.id)\n        except Exception:\n            return ''\n        return ''\n\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    text = (get_text(output) or '').lower()\n    if not text:\n        return 0.0\n\n    mentions_v1 = any(k in text for k in [\"v1\", \"mvp\", \"phase 1\", \"first release\", \"initial release\"])\n    mentions_future = any(k in text for k in [\"future\", \"later\", \"phase 2\", \"v2\"])\n    social_future = (\"social\" in text or \"oauth\" in text or \"google\" in text or \"facebook\" in text) and mentions_future\n    contract_or_deposit_future = ((\"contract\" in text or \"signing\" in text or \"deposit\" in text or \"payment\" in text) and mentions_future)\n    six_week = (\"6-week\" in text or \"six-week\" in text or re.search(r\"\\b6\\s*weeks?\\b\", text) is not None)\n\n    score_parts = [mentions_v1, mentions_future, social_future, contract_or_deposit_future, six_week]\n    ratio = sum(1 for x in score_parts if x) / len(score_parts)\n    feedback = f\"Signals \u2013 V1:{mentions_v1}, Future:{mentions_future}, SocialLater:{social_future}, Contract/DepositFuture:{contract_or_deposit_future}, 6-week:{six_week}\"\n    return ratio, feedback\n"}, {"type": "code", "name": "Security Considerations", "description": "Check for explicit security practices related to web sessions, auth, and app security (e.g., CSRF, CORS, encryption, RBAC, audit logging, rate limiting).", "weight": 1.2, "code": "def evaluate(workflow, context):\n    def get_text(output):\n        try:\n            if output.is_document:\n                try:\n                    return context.files.read_docx_text(output.id)\n                except Exception:\n                    return context.files.read_pdf_text(output.id)\n            elif output.is_text_format:\n                return context.files.read_text(output.id)\n        except Exception:\n            return ''\n        return ''\n\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    text = (get_text(output) or '').lower()\n    if not text:\n        return 0.0\n\n    keywords = [\n        'csrf', 'cors', 'encryption', 'encrypt', 'tls', 'https', 'rback', 'rbac', 'least privilege',\n        'rate limit', 'rate-limit', 'throttle', 'brute force', 'ip allowlist', 'audit log', 'logging', 'monitoring', 'owasp'\n    ]\n    hits = sum(1 for k in keywords if k in text)\n    # require at least 3 distinct mentions\n    ratio = 1.0 if hits >= 5 else 0.7 if hits >= 3 else 0.3 if hits >= 1 else 0.0\n    return ratio, f\"Security mentions found: {hits}\"\n"}, {"type": "code", "name": "Open Questions Section Depth", "description": "Check for an 'Open Questions' section and ensure at least 3 explicit questions are surfaced.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    import re\n    def get_text(output):\n        try:\n            if output.is_document:\n                try:\n                    return context.files.read_docx_text(output.id)\n                except Exception:\n                    return context.files.read_pdf_text(output.id)\n            elif output.is_text_format:\n                return context.files.read_text(output.id)\n        except Exception:\n            return ''\n        return ''\n\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    text_raw = get_text(output) or ''\n    text = text_raw.lower()\n    has_section = ('open questions' in text) or ('questions' in text)\n    q_marks = text_raw.count('?')\n    # Count bullet lines containing a question mark as a proxy for distinct questions\n    bullets = re.findall(r\"(?m)^(?:\\s*[-*\u2022]|\\d+\\.)\\s.*\\?$\", text_raw)\n    distinct = max(len(bullets), min(q_marks, 10))\n    if not has_section:\n        return 0.0, \"Missing 'Open Questions' section.\"\n    ratio = 1.0 if distinct >= 3 else 0.6 if distinct >= 1 else 0.2\n    return ratio, f\"Detected ~{distinct} questions.\"\n"}, {"type": "code", "name": "Risks and Mitigations", "description": "Check for a 'Risks' section and explicit mention of mitigations.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    import re\n    def get_text(output):\n        try:\n            if output.is_document:\n                try:\n                    return context.files.read_docx_text(output.id)\n                except Exception:\n                    return context.files.read_pdf_text(output.id)\n            elif output.is_text_format:\n                return context.files.read_text(output.id)\n        except Exception:\n            return ''\n        return ''\n\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    text = (get_text(output) or '').lower()\n    if not text:\n        return 0.0\n    has_risks = ('risk' in text)\n    has_mitigations = ('mitigation' in text or 'mitigate' in text or 'contingency' in text)\n    # Count likely risk items\n    items = len(re.findall(r\"(?m)^(\\s*[-*\u2022]|\\d+\\.)\\s.*risk\", text)) + len(re.findall(r\"risk:\\s\", text))\n    if not has_risks:\n        return 0.0, \"No risks section detected.\"\n    ratio = 1.0 if has_mitigations and items >= 3 else 0.7 if has_mitigations and items >= 1 else 0.4\n    return ratio, f\"Risks present: {has_risks}, Mitigations present: {has_mitigations}, Items: {items}\"\n"}, {"type": "code", "name": "Timeline and Milestones", "description": "Check for a six-week timeline with milestones.", "weight": 0.8, "code": "def evaluate(workflow, context):\n    import re\n    def get_text(output):\n        try:\n            if output.is_document:\n                try:\n                    return context.files.read_docx_text(output.id)\n                except Exception:\n                    return context.files.read_pdf_text(output.id)\n            elif output.is_text_format:\n                return context.files.read_text(output.id)\n        except Exception:\n            return ''\n        return ''\n\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    text = (get_text(output) or '').lower()\n    if not text:\n        return 0.0\n    six_week = bool(re.search(r\"\\b6\\s*weeks?\\b\", text) or (\"6-week\" in text) or (\"six-week\" in text))\n    milestones = len(re.findall(r\"(?m)^(\\s*[-*\u2022]|\\d+\\.)\\s*(week|milestone)\", text))\n    ratio = 1.0 if six_week and milestones >= 3 else 0.7 if six_week and milestones >= 1 else 0.3 if six_week else 0.0\n    return ratio, f\"Six-week mentioned: {six_week}, milestone lines: {milestones}\"\n"}, {"type": "code", "name": "Integration Specificity with Existing Stack", "description": "Ensure integration details reference the existing React UI, Express REST API, and PostgreSQL, with clear integration points.", "weight": 0.8, "code": "def evaluate(workflow, context):\n    def get_text(output):\n        try:\n            if output.is_document:\n                try:\n                    return context.files.read_docx_text(output.id)\n                except Exception:\n                    return context.files.read_pdf_text(output.id)\n            elif output.is_text_format:\n                return context.files.read_text(output.id)\n        except Exception:\n            return ''\n        return ''\n\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    t = (get_text(output) or '').lower()\n    if not t:\n        return 0.0\n    has_react = 'react' in t\n    has_express = 'express' in t\n    has_rest = 'rest' in t or 'api' in t\n    has_pg = 'postgres' in t or 'postgresql' in t\n    has_integration_point = ('webhook' in t or 'endpoint' in t or 'event' in t or 'synchron' in t or 'sync' in t or 'integration point' in t)\n    score = sum([has_react, has_express, has_rest, has_pg, has_integration_point]) / 5\n    return score, f\"React:{has_react}, Express:{has_express}, REST/API:{has_rest}, PostgreSQL:{has_pg}, IntegrationPoint:{has_integration_point}\"\n"}, {"type": "code", "name": "CI/CD and IaC Explicitness", "description": "Verify that GitHub Actions CI/CD and Infrastructure-as-Code tooling are both named.", "weight": 0.7, "code": "def evaluate(workflow, context):\n    def get_text(output):\n        try:\n            if output.is_document:\n                try:\n                    return context.files.read_docx_text(output.id)\n                except Exception:\n                    return context.files.read_pdf_text(output.id)\n            elif output.is_text_format:\n                return context.files.read_text(output.id)\n        except Exception:\n            return ''\n        return ''\n\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    t = (get_text(output) or '').lower()\n    if not t:\n        return 0.0\n    gha = 'github actions' in t\n    iac = any(k in t for k in ['terraform', 'pulumi', 'cloudformation', 'cdk', 'iac'])\n    if gha and iac:\n        return 1.0, \"GitHub Actions and IaC mentioned.\"\n    if gha or iac:\n        return 0.6, \"Only one of GitHub Actions or IaC mentioned.\"\n    return 0.0, \"Neither GitHub Actions nor IaC mentioned.\"\n"}, {"type": "code", "name": "Repo Strategy Clarity", "description": "Check for an explicit recommendation on monorepo vs new repo (polyrepo) with rationale.", "weight": 0.5, "code": "def evaluate(workflow, context):\n    def get_text(output):\n        try:\n            if output.is_document:\n                try:\n                    return context.files.read_docx_text(output.id)\n                except Exception:\n                    return context.files.read_pdf_text(output.id)\n            elif output.is_text_format:\n                return context.files.read_text(output.id)\n        except Exception:\n            return ''\n        return ''\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    t = (get_text(output) or '').lower()\n    if not t:\n        return 0.0\n    mentions = any(k in t for k in ['monorepo', 'polyrepo', 'new repo', 'separate repo'])\n    rationale = any(k in t for k in ['because', 'trade-off', 'tradeoff', 'pros', 'cons', 'rationale', 'reason'])\n    if mentions and rationale:\n        return 1.0, \"Repo strategy with rationale present.\"\n    if mentions:\n        return 0.6, \"Repo strategy mentioned but rationale unclear.\"\n    return 0.0, \"Repo strategy not found.\"\n"}, {"type": "code", "name": "Data Model Presence", "description": "Check for a brief data model: at least two key entities mentioned (e.g., Lead, User/Customer, Proposal, Asset).", "weight": 0.5, "code": "def evaluate(workflow, context):\n    def get_text(output):\n        try:\n            if output.is_document:\n                try:\n                    return context.files.read_docx_text(output.id)\n                except Exception:\n                    return context.files.read_pdf_text(output.id)\n            elif output.is_text_format:\n                return context.files.read_text(output.id)\n        except Exception:\n            return ''\n        return ''\n\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    t = (get_text(output) or '').lower()\n    if not t:\n        return 0.0\n    entities = [\n        ('lead' in t),\n        ('user' in t or 'customer' in t),\n        ('proposal' in t),\n        ('asset' in t or 'file' in t or 'document' in t)\n    ]\n    count = sum(1 for e in entities if e)\n    ratio = 1.0 if count >= 3 else 0.6 if count == 2 else 0.0\n    return ratio, f\"Entities mentioned: {count}\"\n"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Professional Quality and Strategic Value (LLM)", "description": "LLM judges assess clarity, actionability, prioritization, and strategic alignment with constraints and long-term goals.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Clarity", "description": "Assess whether the document is concise, well-structured, and readable by a senior engineering team.", "weight": 1.5, "judge_prompt": "Evaluate the document's professional presentation and clarity for a senior engineering audience. Criteria:\n- Clear, concise writing without unnecessary verbosity (aiming for 2\u20133 pages of dense, actionable content).\n- Logical flow with headings, bullets, and succinct explanations.\n- Avoids deep tutorials; focuses on decisions and direction.\nScoring:\n- 1.5: Highly professional, concise, and clear.\n- 1.0: Generally clear with minor verbosity or organization issues.\n- 0.5: Somewhat rambling or hard to follow but usable.\n- 0.0: Confusing or unprofessional presentation.", "expectation": "Polished, succinct, well-structured guidance suitable for senior engineers."}, {"type": "llm_judge", "name": "Actionability and Prioritization", "description": "Assess whether the plan is directly actionable with clear priorities and near-term milestones for a 6-week MVP.", "weight": 1.5, "judge_prompt": "Evaluate how actionable and prioritized the plan is for a 6-week MVP. Criteria:\n- Clear MVP scope and out-of-scope items.\n- Concrete near-term milestones and ownership hints.\n- Explicit decisions that unblock ticket creation (framework choice, auth approach, storage, CI/CD, IaC).\nScoring:\n- 1.5: Immediately actionable with clear priorities and milestones.\n- 1.0: Mostly actionable; some priorities or milestones are vague.\n- 0.5: High-level only; significant translation needed for tickets.\n- 0.0: Not actionable.", "expectation": "A short plan that can be decomposed into tickets right away."}, {"type": "llm_judge", "name": "Strategic Alignment and Future Extensibility", "description": "Assess alignment with constraints and the roadmap (TOTP now; social login later; contracts and deposits in future; scalable architecture choices).", "weight": 1.0, "judge_prompt": "Evaluate strategic alignment and extensibility:\n- Explicitly respects constraints: TOTP in initial release, social login later, in-browser PDF export, mobile usage assumptions.\n- Anticipates future additions (contract signing, deposit/payment) without over-scoping V1.\n- Picks technologies and integration points that scale and fit the existing stack.\nScoring:\n- 1.0: Strong alignment and thoughtful future-proofing.\n- 0.7: Generally aligned; minor mismatches.\n- 0.4: Some conflicts with constraints.\n- 0.0: Major misalignment with requirements.", "expectation": "Decisions that meet the 6-week goal and enable future features without rework."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "7d7fc9a7-21a7-4b83-906f-416dea5ad04f", "rubric": {"category_name": "Amortization Schedule Preparation", "rationale": "This rubric evaluates the preparation of detailed amortization schedules for Aurisic's prepaid expenses and insurance, assessing structure, correctness, and quality. The task is analytical, requiring precise data organization and calculation, making it suitable for a structured Excel output evaluation.", "max_total_score": 10.0, "stages": [{"name": "Format and Structure Validation", "description": "Ensure the Excel workbook has required sheets with correct structural organization.", "is_required": true, "min_score_to_pass": 0.6, "rules": [{"type": "llm_judge", "name": "Excel Structure Validation", "description": "Check if the Excel file has the required sheets and sections as specified.", "weight": 1.0, "judge_prompt": "Check if the submitted Excel file meets the following structure requirements:\n\n1. **Sheet: 'Prepaid Summary'**\n   - Must include: Total Prepaid Expenses, Total Prepaid Insurance, YTD Prepaid Expenses, Total Amortization YTD, Ending Balance as of 4/30/2025.\n   - Company name and reporting period in the header.\n\n2. **Sheet: 'Prepaid Expenses (Account #1250)'**\n   - Amortization schedule for 2025 prepaid services invoices.\n   - Columns: [Vendor, Original Amount, Amortization Period, Monthly Expense, Remaining Balance by Month].\n   - Monthly activity and ending balances summary.\n\n3. **Sheet: 'Prepaid Insurance (Account #1251)'**\n   - Amortization schedule for prepaid insurance invoices.\n   - Same columns as Prepaid Expenses tab.\n\nScoring Criteria:\n- 1.0: All sheets and sections correctly structured.\n- 0.7: Missing minor details or formatting inconsistencies.\n- 0.0: Missing sheets or major sections, incorrect format.\n\nDO NOT check the correctness of calculations, focus on structure only.", "expectation": null}], "max_points": 2.0, "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Correctness Verification", "description": "Verify the accuracy of calculations and reconciliation with general ledger balances.", "is_required": false, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Expenses Reconciliation", "description": "Verify if Prepaid Expenses schedule reconciles with provided GL balances.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        df_expenses = context.files.read_excel(output.id, sheet_name='Prepaid Expenses (Account #1250)')\n        # Extract the summary total row\n        summary_totals = df_expenses[df_expenses.vendor.str.lower() == 'summary'].iloc[0]\n        # Check against GL balances\n        gl_balances = {\n            'Jan': 518934.86, 'Feb': 426673.13, 'Mar': 473655.55, 'Apr': 559377.61\n        }\n        # For each month\n        for month, gl_balance in gl_balances.items():\n            if not np.isclose(float(summary_totals[month]), gl_balance, atol=10.0):\n                return 0.5, f'Mismatch in {month}: expected {gl_balance}, found {summary_totals[month]}'\n        return 1.0\n    except Exception as e:\n        return 0.0, f'Error in calculation verification: {str(e)}'"}, {"type": "code", "name": "Insurance Reconciliation", "description": "Ensure Prepaid Insurance schedule reconciles to GL balances.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        df_insurance = context.files.read_excel(output.id, sheet_name='Prepaid Insurance (Account #1251)')\n        # Extract the summary total row\n        summary_totals = df_insurance[df_insurance.vendor.str.lower() == 'summary'].iloc[0]\n        # Check against GL balances\n        gl_balances = {\n            'Jan': 506657.98, 'Feb': 461097.55, 'Mar': 415537.13, 'Apr': 369976.70\n        }\n        # For each month\n        for month, gl_balance in gl_balances.items():\n            if not np.isclose(float(summary_totals[month]), gl_balance, atol=10.0):\n                return 0.5, f'Mismatch in {month}: expected {gl_balance}, found {summary_totals[month]}'\n        return 1.0\n    except Exception as e:\n        return 0.0, f'Error in calculation verification: {str(e)}'"}], "max_points": 4.0, "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Quality and Presentation Assessment", "description": "Evaluate the overall quality, clarity, and professionalism of the workbook.", "is_required": false, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Clarity", "description": "Assess the quality and professionalism of the Excel workbook.", "weight": 1.0, "judge_prompt": "Evaluate the overall quality and clarity of the Excel workbook. Consider:\n- Professional presentation: Is the workbook well-organized, easy to navigate, and visually appealing?\n- Clarity: Are titles and headings clear and informative? Is the information presented logically?\n- Completeness: Does the workbook cover all aspects of the task comprehensively?\n- Appropriateness for the target audience.\n\nScoring Criteria:\n- 1.0: Exceptionally clear, well-organized, highly professional.\n- 0.7: Generally good, minor issues in organization or clarity.\n- 0.4: Several issues in clarity or organization, lacks professionalism.\n- 0.0: Poor presentation and clarity, unprofessional.", "expectation": null}], "max_points": 4.0, "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "a4a9195c-5ebe-4b8d-a0c2-4a6b7a49da8b", "rubric": {"category_name": "Manufacturing \u2014 Shipping/Receiving/Inventory Clerks \u2014 SOP: Handling & Storage of ESD-Sensitive Items", "rationale": "Three-stage, self-documenting rubric. Stage 1 is a strict LLM gate enforcing a verifiable DOCX/PDF SOP structure (sections, document control, appendix checklist) and page-limit so downstream checks are trivial. Stage 2 mixes code and LLM-friendly validations to verify correctness signals enabled by the structure (reference to IPC-A-610G, breadth of ESD controls, actionable steps, training/records, revision control, checklist). Stage 3 judges overall professionalism, clarity, and fitness for warehouse use.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (LLM ONLY)", "description": "Document-format and structure requirements for a concise, training-ready SOP. This is the critical gate; if the SOP is not structured as required, verification is impossible.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured SOP Format and Section Presence", "description": "Check that the candidate output is a compact (<=5 pages) SOP in DOCX or PDF with required sections, document control block, procedures, and an appendix checklist.", "weight": 4.0, "judge_prompt": "You are verifying ONLY the format/shape and structural completeness of a Standard Operating Procedure (SOP). Do not judge content quality or technical correctness here.\n\nFormat requirements:\n- File type: DOCX (preferred) or PDF. If PDF clearly appears to be a Word-style document, treat as acceptable.\n- Length: No more than 5 pages total.\n- Professional SOP layout: clear headers, lists/tables where appropriate.\n\nRequired visible elements (flexible header names allowed; judge by intent):\nA) Title block on page 1 indicating it is an SOP for handling and storage of ESD-sensitive items (e.g., \u201cStandard Operating Procedure: ESD Handling and Storage\u201d).\nB) Document control block on page 1 containing at least three of the following fields: Document ID/Number, Revision, Effective Date, Owner/Process Owner, Approver/Approval, Page x of y.\nC) Required sections with clear headers:\n   1) Purpose\n   2) Scope\n   3) References (must list IPC-A-610G or a very close variant, e.g., \"IPC A-610G\" or the provided link)\n   4) Definitions/Acronyms (e.g., ESD, EPA, MBB, HIC)\n   5) Roles and Responsibilities (for warehouse roles such as Receiving, Kitting/Picking, QA, Manager)\n   6) Required Equipment/Controls (e.g., wrist straps, heel grounders, ESD mats, ionizers, shielding/packaging)\n   7) Procedures (broken into practical subsections covering at least: Receiving/Inspection; Storage; Handling within EPA; Picking/Kitting/Issuing; Transport; Labeling/Traceability; Nonconforming Material)\n   8) Training and Competency\n   9) Records and Forms\n   10) Appendix: ESD Compliance Checklist (a checklist with multiple items intended for daily/weekly use)\n\nScoring (STRUCTURE ONLY):\n- 4.0: DOCX or acceptable PDF; \u22645 pages; title block; document control (\u22653 fields); all 10 required sections present and clearly labeled; Appendix includes a visible checklist with multiple items.\n- 3.5: All above but missing 1 required section OR document control shows only 2 required fields.\n- 3.0: Valid format and \u22645 pages; at least 8/10 sections present including Procedures and Appendix checklist.\n- 2.0: Valid format but >5 pages OR only 6\u20137 required sections present OR Appendix present but no visible checklist.\n- 1.0: Valid file but only 3\u20135 sections or missing Procedures section.\n- 0.0: Not DOCX/PDF, or obviously not an SOP, or fewer than 3 sections.\n\nOnly assess presence/structure and page-limit; do not assess technical accuracy.", "expectation": "A 3\u20135 page DOCX/PDF SOP with a document-control front matter, the ten required sections, and an actionable checklist in the appendix."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification: Correctness Signals (Code + Flexible Text Checks)", "description": "Deterministic checks leveraging Stage 1 structure. Validates references, breadth of ESD controls, actionable steps, training/records, revision/document control, and presence of a real checklist.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Reference to IPC-A-610G Present", "description": "Verify the SOP references IPC-A-610G (or close variant) in the References section or elsewhere.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 0.5\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = ''\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        text = ''\n    if not text:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = ''\n    if not text:\n        return 0.0\n    t = text.lower()\n    patterns = [\n        r\"ipc\\s*-?a\\s*-?610g\",  # IPC-A-610G\n        r\"ipc\\s*a\\s*[- ]?610g\",\n        r\"ipc\\s*-?a\\s*-?610\\b\",  # tolerate missing G\n        r\"pc\\s*-?a\\s*-?610g\",    # task typo tolerance\n        r\"ipc\\s*a\\s*-?610\\s*g\"\n    ]\n    found = any(re.search(p, t) for p in patterns)\n    return weight if found else 0.0"}, {"type": "code", "name": "Actionable, Numbered Procedure Steps", "description": "Check for presence and reasonable count of numbered steps across procedure sections to ensure actionability.", "weight": 0.7, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 0.7\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = ''\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        text = ''\n    if not text:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = ''\n    if not text:\n        return 0.0\n    lines = [ln.strip() for ln in text.splitlines()]\n    step_pat = re.compile(r\"^\\s*(?:\\d+|[ivx]+)[\\.)]\\s+\", re.IGNORECASE)\n    step_count = sum(1 for ln in lines if step_pat.search(ln))\n    # Scale: 0 steps -> 0; 10 or more steps -> full credit; linear in between\n    score = min(step_count, 10) / 10.0 * weight\n    return score"}, {"type": "code", "name": "Breadth of ESD Controls and Concepts", "description": "Confirm coverage across key ESD control categories (grounding/PPE, worksurfaces/EPA, packaging/storage, labeling, environmental/ionization, test/verification, handling/transport).", "weight": 1.1, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 1.1\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = ''\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        text = ''\n    if not text:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = ''\n    if not text:\n        return 0.0\n    t = text.lower()\n    categories = {\n        'grounding_ppe': [\"wrist strap\", \"heel grounder\", \"esd smock\", \"ground cord\", \"common point ground\", \"cpg\"],\n        'worksurfaces_epa': [\"esd mat\", \"work surface\", \"epa\", \"esd protected area\", \"bench mat\"],\n        'packaging_storage': [\"moisture barrier bag\", \"mbb\", \"hic\", \"humidity indicator card\", \"desiccant\", \"dry cabinet\", \"shielding bag\", \"esd-safe bin\", \"conductive tote\"],\n        'labeling': [\"esd symbol\", \"yellow/black\", \"labeling\", \"caution static sensitive\", \"marking\"],\n        'environment_ionization': [\"humidity\", \"rh\", \"relative humidity\", \"ionizer\", \"temperature\"],\n        'test_verification': [\"wrist strap tester\", \"esd tester\", \"test log\", \"record of test\", \"verification\", \"calibration\"],\n        'handling_transport': [\"cart\", \"transport\", \"grounded cart\", \"bag and tag\", \"transfer\", \"material handling\"]\n    }\n    hits = 0\n    for kws in categories.values():\n        if any(kw in t for kw in kws):\n            hits += 1\n    total = len(categories)\n    frac = hits / total if total else 0\n    return frac * weight"}, {"type": "code", "name": "Training and Records Presence with Frequency/Retention", "description": "Verify presence of both Training and Records concepts, including frequency/retention cues (e.g., annual, monthly, 12 months, retention).", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 0.6\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = ''\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        text = ''\n    if not text:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = ''\n    if not text:\n        return 0.0\n    t = text.lower()\n    has_training = (\"training\" in t) or (\"competency\" in t) or (\"train\" in t)\n    freq_pat = re.compile(r\"annual|annually|monthly|quarterly|semi[- ]?annual|every\\s+\\d+\\s*(days|weeks|months|years)|\\b12\\s*months\\b\")\n    has_freq = bool(freq_pat.search(t))\n    has_records = any(k in t for k in [\"records\", \"forms\", \"log\", \"retention\", \"recordkeeping\", \"document retention\"])\n    # 0.2 for training mention, 0.2 for records mention, 0.2 for frequency/retention cue\n    score = 0.0\n    if has_training:\n        score += 0.2\n    if has_records:\n        score += 0.2\n    if has_freq:\n        score += 0.2\n    return min(score, 0.6)"}, {"type": "code", "name": "Appendix Checklist Detected with Multiple Items", "description": "Detect a real checklist in the appendix (e.g., lines starting with [ ], - [ ], or checkbox symbols).", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 0.6\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = ''\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        text = ''\n    if not text:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = ''\n    if not text:\n        return 0.0\n    lines = [ln.strip() for ln in text.splitlines()]\n    # Checklist indicators: [ ], - [ ], \u25a1, \u2610, ( )\n    pat = re.compile(r\"^(?:-\\s*)?(\\[\\s*\\]|\u2610|\u25a1|\\(\\s*\\))\\s+\", re.UNICODE)\n    count = sum(1 for ln in lines if pat.search(ln))\n    # Require at least 6 items for full credit; linear scaling\n    score = min(count, 6) / 6.0 * weight\n    return score"}, {"type": "code", "name": "Revision History and Document Control Signals", "description": "Detect presence of a revision history/document control block including multiple fields (Revision, Effective Date, Owner, Approver).", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 0.5\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = ''\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        text = ''\n    if not text:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = ''\n    if not text:\n        return 0.0\n    t = text.lower()\n    present = 0\n    if any(ph in t for ph in [\"revision history\", \"document control\", \"doc control\", \"change history\"]):\n        present += 1\n    if \"revision\" in t or re.search(r\"\\brev[:\\s]\", t):\n        present += 1\n    if \"effective date\" in t or re.search(r\"\\beff(?:ective)?\\s*date\\b\", t):\n        present += 1\n    if \"owner\" in t or \"process owner\" in t or \"document owner\" in t:\n        present += 1\n    if \"approver\" in t or \"approved by\" in t or \"approval\" in t:\n        present += 1\n    # Scale: need at least 3 signals for partial, 4+ for full\n    if present >= 4:\n        return weight\n    elif present == 3:\n        return weight * 0.6\n    elif present == 2:\n        return weight * 0.3\n    else:\n        return 0.0"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Fitness for Use (LLM)", "description": "Holistic assessment of the SOP\u2019s clarity, actionability, and professionalism for warehouse staff and training integration.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professionalism, Clarity, and Actionability", "description": "Evaluate whether the SOP is concise (\u22645 pages), readable by warehouse staff, uses unambiguous steps, and is training-ready.", "weight": 2.0, "judge_prompt": "Assess overall quality and fitness for use of the SOP (not just structure). Consider:\n- Clarity and concision for warehouse personnel (plain language, minimal jargon, \u22645 pages maintained without padding).\n- Actionability: concrete, step-by-step instructions; clear acceptance criteria (e.g., pass/fail for strap tests), measurable conditions (e.g., humidity ranges), and unambiguous responsibilities.\n- Professional presentation: consistent headers, lists/tables, document control details filled, reasonable visuals or symbols as needed.\n- Training readiness: procedures align with onboarding and recurring training; records and checklist are usable in daily/weekly routines.\n- Risk awareness: emphasizes ESD-safe practices that prevent damage during receiving, storage, kitting, transport, and issuing.\n\nScoring Guide:\n- 2.0: Highly clear, concise, and actionable; easy to train from; well-formatted and consistent; measurable criteria present.\n- 1.0: Generally clear with some gaps; mostly actionable but missing a few measurable criteria or minor formatting issues.\n- 0.5: Partially useful; noticeable ambiguity or length/structure issues; hard to train from without edits.\n- 0.0: Unclear, poorly structured, or not suitable for training without major rework.", "expectation": "A crisp, training-ready SOP with unambiguous steps, measurable controls, and a usable checklist, suitable for warehouse daily use."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "ae0c1093-5ea8-4b84-a81e-53ebf7a4321d", "rubric": {"category_name": "Undercover Operations Guide and Observation Form (Retail Investigations)", "rationale": "Pattern B (Document). The deliverables are two PDFs: a professionally structured guide and a printable observation form. Stage 1 (LLM-only) enforces exact file/structure so verification is possible. Stage 2 (code) performs deterministic text checks (titles, purpose objective, section coverage, headers). Stage 3 (LLM) assesses professional quality and suitability for private investigators in retail contexts.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 Gate: Document Format and Structural Compliance", "description": "LLM-only verification that both required PDFs exist with correct titles and mandated structural elements. This is a hard gate; if structure is wrong, the category is not scorable.", "is_required": true, "max_points": 5.0, "min_score_to_pass": 4.0, "rules": [{"type": "llm_judge", "name": "Guide PDF: Structural Requirements", "description": "Verify the 'Undercover Operations Guide: Employee Evaluation' PDF exists and is structurally complete.", "weight": 3.0, "judge_prompt": "You are validating the STRUCTURE ONLY of a PDF, not content quality. Check the outputs and confirm the following for the guide document:\n\nTarget document:\n- Title must be exactly: \"Undercover Operations Guide: Employee Evaluation\"\n\nFormat Requirements:\n- Must be a PDF (not DOCX, not plain text)\n- At least 3 pages long\n- Professionally formatted with clear section headers\n\nRequired Sections (flexible naming allowed if clearly equivalent):\n1) Title Page (can include company name and date)\n2) Purpose (must explicitly state objective to discreetly observe and assess employee behavior within the organization)\n3) Legal and Ethical Considerations (privacy, recording/consent, policy boundaries)\n4) Roles and Cover Identity Development\n5) Pre-Operation Planning (briefing, schedule, equipment)\n6) Observation Criteria and Indicators (e.g., theft/red flags, cash handling, POS anomalies, policy compliance, customer interactions)\n7) Evidence Collection and Chain of Custody\n8) Documentation Protocol and Reporting Template/Instructions\n9) Communication Plan (during operation)\n10) Risk Management and Safety Protocols\n11) Operational Workflow (Pre-Op, During/Execution, Post-Op/After-Action)\n12) Appendices or Checklists (at least one checklist, template, or sample script)\n\nScoring (STRUCTURE ONLY):\n- 3.0: PDF format, correct title visible, \u22653 pages, and 10\u201312 required sections present with clear headers\n- 2.5: PDF, correct title, \u22653 pages, and 8\u20139 sections present\n- 1.5: PDF, correct title, \u22652 pages, and 6\u20137 sections present\n- 0.0: Wrong format (not PDF), title missing/incorrect, or fewer than 6 sections\n\nOnly check presence/format and section coverage. Do not judge writing quality or accuracy of legal guidance.", "expectation": "A multi-page PDF guide with the exact title and clearly labeled sections covering purpose, legal/ethical boundaries, planning, observation criteria, evidence handling, documentation, communication, risk/safety, workflow, and appendices/checklists."}, {"type": "llm_judge", "name": "Observation Form PDF: Structural Requirements (Headers + Lines)", "description": "Verify the 'Undercover Observation Form' PDF exists and includes headers with three solid lines under each header for handwritten notes.", "weight": 2.0, "judge_prompt": "You are validating the STRUCTURE ONLY of a PDF, not content quality. Check the outputs and confirm the following for the form document:\n\nTarget document:\n- Title must be exactly: \"Undercover Observation Form\"\n\nFormat Requirements:\n- Must be a PDF (not DOCX, not plain text)\n- At least 1 full page; multi-page is acceptable\n- Clean, printable layout for handwriting\n\nRequired Header Blocks (flexible naming allowed if clearly equivalent). Each header must be immediately followed by three solid horizontal lines spanning the writing area:\n- Date / Time\n- Location / Store ID\n- Investigator Alias (or Investigator Name/Alias)\n- Assignment / Objective\n- Shift Start / End\n- Persons Observed (or Subjects Observed)\n- Activities / Incidents\n- Cash Handling Observations (or Till / POS Observations)\n- Policy Deviations (or Policy/Procedure Compliance Notes)\n- Customer Interaction Quality\n- Red Flags / Suspicious Behaviors\n- Evidence Collected (with space for IDs/attachments)\n- Witnesses (if any)\n- Supervisor/Manager Interactions\n- Follow-ups / Next Steps\n- Summary / Recommendations\n- Signature and Date (signature line acceptable)\n\nScoring (STRUCTURE ONLY):\n- 2.0: PDF format, correct title visible, and \u226512 of the above headers present with three solid lines under each\n- 1.5: PDF, correct title, and 9\u201311 headers present with three solid lines under each\n- 1.0: PDF, correct title, and 6\u20138 headers present with three solid lines under each\n- 0.0: Wrong format (not PDF), title missing/incorrect, or most headers missing OR lines not present under headers\n\nOnly check presence/format and the visual requirement of three solid lines under each header. Do not judge aesthetics beyond these structural checks.", "expectation": "A printable PDF form with the exact title, clear headers, and three solid lines after each header to facilitate handwritten notes."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Correctness and Consistency Verification (Code + LLM-friendly checks)", "description": "Deterministic checks leveraging the enforced shape from Stage 1: title presence, purpose objective phrasing, coverage of key guide sections, and presence of required form headers.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Both PDFs Detected with Correct Titles in Text", "description": "Confirm both PDFs exist among outputs and their titles appear in extracted text.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Validate that two document outputs exist and their texts include the exact titles:\n    - \"Undercover Operations Guide: Employee Evaluation\"\n    - \"Undercover Observation Form\"\n    Returns proportional credit based on titles found.\n    \"\"\"\n    try:\n        outputs = context.get_all_outputs() or []\n    except Exception:\n        outputs = []\n    if not outputs:\n        return 0.0, \"No outputs found.\"\n\n    docs = [r for r in outputs if getattr(r, 'is_document', False)]\n    if not docs:\n        return 0.0, \"No document outputs detected.\"\n\n    def extract_text(res):\n        # Try PDF first, then DOCX as fallback\n        txt = \"\"\n        try:\n            txt = context.files.read_pdf_text(res.id)\n            if isinstance(txt, bytes):\n                txt = txt.decode('utf-8', errors='ignore')\n        except Exception:\n            pass\n        if not txt:\n            try:\n                txt = context.files.read_docx_text(res.id)\n            except Exception:\n                pass\n        return txt or \"\"\n\n    title_guide = \"undercover operations guide: employee evaluation\"\n    title_form = \"undercover observation form\"\n\n    found_guide = False\n    found_form = False\n    details = []\n\n    for d in docs:\n        text = extract_text(d).lower()\n        if not text:\n            continue\n        if title_guide in text:\n            found_guide = True\n        if title_form in text:\n            found_form = True\n\n    found_count = int(found_guide) + int(found_form)\n    score = (found_count / 2.0) * 0.8\n    fb = []\n    fb.append(f\"Guide title found: {found_guide}\")\n    fb.append(f\"Form title found: {found_form}\")\n    return score, \"; \".join(fb)\n"}, {"type": "code", "name": "Guide Purpose States Required Objective", "description": "Check that the guide's Purpose section explicitly indicates the objective to discreetly observe and assess employee behavior within the organization.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Locate the guide document and verify the Purpose section mentions key intent:\n    - discreet(ly) observe\n    - assess/evaluate\n    - employee behavior/conduct\n    - within the organization/company/business\n    Award partial credit if Purpose exists but phrasing is incomplete.\n    \"\"\"\n    try:\n        outputs = context.get_all_outputs() or []\n    except Exception:\n        outputs = []\n    docs = [r for r in outputs if getattr(r, 'is_document', False)]\n    if not docs:\n        return 0.0, \"No document outputs found.\"\n\n    def extract_text(res):\n        txt = \"\"\n        try:\n            txt = context.files.read_pdf_text(res.id)\n            if isinstance(txt, bytes):\n                txt = txt.decode('utf-8', errors='ignore')\n        except Exception:\n            pass\n        if not txt:\n            try:\n                txt = context.files.read_docx_text(res.id)\n            except Exception:\n                pass\n        return (txt or \"\").lower()\n\n    guide_text = \"\"\n    for d in docs:\n        t = extract_text(d)\n        if \"undercover operations guide\" in t or \"employee evaluation\" in t:\n            guide_text = t\n            break\n    if not guide_text:\n        # Fallback: choose the longer text as the guide\n        texts = [extract_text(d) for d in docs]\n        guide_text = max(texts, key=len) if texts else \"\"\n    if not guide_text:\n        return 0.0, \"Unable to extract guide text.\"\n\n    # Find a Purpose section window\n    idx = guide_text.find(\"purpose\")\n    window = guide_text[max(0, idx-100): idx+600] if idx != -1 else guide_text[:1200]\n\n    has_discreet_observe = bool(re.search(r\"discreet\\w*\\s+observe\", window)) or (\"discreet\" in window and \"observe\" in window)\n    has_assess = (\"assess\" in window) or (\"evaluate\" in window)\n    has_employee_behavior = (\"employee behavior\" in window) or (\"employee conduct\" in window)\n    has_within_org = any(k in window for k in [\"within the organization\", \"within the organisation\", \"within the company\", \"within the business\", \"inside the organization\"])\n\n    hits = sum([has_discreet_observe, has_assess, has_employee_behavior, has_within_org])\n\n    if idx != -1 and hits == 4:\n        return 0.8, \"Purpose section contains full required objective.\"\n    if idx != -1 and hits >= 2:\n        return 0.8 * 0.6, f\"Purpose found with partial objective coverage ({hits}/4 elements).\"\n    if idx != -1:\n        return 0.8 * 0.3, \"Purpose heading found but objective language weak or missing.\"\n    return 0.0, \"Purpose section not detected.\"\n"}, {"type": "code", "name": "Guide Key Sections Coverage", "description": "Verify the guide includes a broad set of operational sections (legal, cover, planning, criteria, evidence/chain, documentation/reporting, communication, risk/safety, workflow, appendices/checklists).", "weight": 0.9, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Score based on presence of multiple key sections in the guide text.\n    Up to 10 checks, proportional scoring.\n    \"\"\"\n    try:\n        outputs = context.get_all_outputs() or []\n    except Exception:\n        outputs = []\n    docs = [r for r in outputs if getattr(r, 'is_document', False)]\n    if not docs:\n        return 0.0, \"No document outputs found.\"\n\n    def extract_text(res):\n        txt = \"\"\n        try:\n            txt = context.files.read_pdf_text(res.id)\n            if isinstance(txt, bytes):\n                txt = txt.decode('utf-8', errors='ignore')\n        except Exception:\n            pass\n        if not txt:\n            try:\n                txt = context.files.read_docx_text(res.id)\n            except Exception:\n                pass\n        return (txt or \"\").lower()\n\n    guide_text = \"\"\n    for d in docs:\n        t = extract_text(d)\n        if \"undercover operations guide\" in t or \"employee evaluation\" in t:\n            guide_text = t\n            break\n    if not guide_text:\n        texts = [extract_text(d) for d in docs]\n        guide_text = max(texts, key=len) if texts else \"\"\n    if not guide_text:\n        return 0.0, \"Unable to extract guide text.\"\n\n    checks = {\n        \"legal_ethics\": any(k in guide_text for k in [\"legal\", \"ethic\", \"consent\", \"privacy\", \"recording\"]),\n        \"cover_identity\": any(k in guide_text for k in [\"cover identity\", \"legend\", \"alias\", \"backstory\"]),\n        \"pre_op_planning\": any(k in guide_text for k in [\"pre-operation\", \"pre operation\", \"pre-op\", \"planning\", \"briefing\"]),\n        \"observation_criteria\": any(k in guide_text for k in [\"observation criteria\", \"indicators\", \"red flags\", \"behavioural indicators\", \"behavioral indicators\"]),\n        \"cash_pos\": any(k in guide_text for k in [\"cash handling\", \"till\", \"register\", \"pos\", \"point of sale\"]),\n        \"policy_compliance\": any(k in guide_text for k in [\"policy compliance\", \"procedure compliance\", \"policy deviations\", \"violations\"]),\n        \"evidence_chain\": any(k in guide_text for k in [\"evidence\", \"chain of custody\", \"custody form\", \"preserve evidence\"]),\n        \"documentation_reporting\": any(k in guide_text for k in [\"documentation\", \"reporting\", \"report template\", \"observation log\"]),\n        \"communication_plan\": any(k in guide_text for k in [\"communication plan\", \"check-in\", \"comms\", \"radio\", \"contact protocol\"]),\n        \"risk_safety\": any(k in guide_text for k in [\"risk\", \"safety\", \"threat\", \"de-escalation\", \"emergency\"]),\n        \"workflow\": any(k in guide_text for k in [\"workflow\", \"during the operation\", \"execution phase\", \"post-operation\", \"after-action\", \"pre-op\"]),\n        \"appendix_checklist\": any(k in guide_text for k in [\"appendix\", \"appendices\", \"checklist\", \"template\", \"sample script\"])\n    }\n\n    # Score up to 10; prioritize 10 core checks; consider appendix as bonus within cap\n    keys_priority = [\n        \"legal_ethics\", \"cover_identity\", \"pre_op_planning\", \"observation_criteria\", \"cash_pos\",\n        \"policy_compliance\", \"evidence_chain\", \"documentation_reporting\", \"communication_plan\", \"risk_safety\"\n    ]\n    hits = sum(1 for k in keys_priority if checks.get(k))\n    # Include workflow or appendix as substitutes if under 10\n    if hits < 10 and checks.get(\"workflow\"):\n        hits += 1\n    if hits < 10 and checks.get(\"appendix_checklist\"):\n        hits += 1\n\n    ratio = min(1.0, hits / 10.0)\n    score = ratio * 0.9\n    feedback = f\"Guide section coverage: {hits}/10 core checks met.\"\n    return score, feedback\n"}, {"type": "code", "name": "Form Headers Presence (Text-based)", "description": "Confirm that the observation form includes most of the required headers as extractable text. Visual line checks are handled in Stage 1.", "weight": 0.5, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Identify the form and check presence of a set of expected headers using flexible matching.\n    \"\"\"\n    try:\n        outputs = context.get_all_outputs() or []\n    except Exception:\n        outputs = []\n    docs = [r for r in outputs if getattr(r, 'is_document', False)]\n    if not docs:\n        return 0.0, \"No document outputs found.\"\n\n    def extract_text(res):\n        txt = \"\"\n        try:\n            txt = context.files.read_pdf_text(res.id)\n            if isinstance(txt, bytes):\n                txt = txt.decode('utf-8', errors='ignore')\n        except Exception:\n            pass\n        if not txt:\n            try:\n                txt = context.files.read_docx_text(res.id)\n            except Exception:\n                pass\n        return (txt or \"\").lower()\n\n    # Pick the form by title, else the shorter document as fallback\n    form_text = \"\"\n    texts = []\n    for d in docs:\n        t = extract_text(d)\n        texts.append(t)\n        if \"undercover observation form\" in t:\n            form_text = t\n            break\n    if not form_text and texts:\n        form_text = min(texts, key=len)\n    if not form_text:\n        return 0.0, \"Unable to extract form text.\"\n\n    headers = {\n        \"date_time\": [\"date / time\", \"date/time\", \"date and time\", \"time\"],\n        \"location\": [\"location\", \"store id\", \"store#\", \"site\"],\n        \"alias\": [\"investigator alias\", \"alias\", \"investigator name\"],\n        \"objective\": [\"assignment / objective\", \"assignment\", \"objective\"],\n        \"shift\": [\"shift start\", \"shift end\", \"shift\"],\n        \"persons\": [\"persons observed\", \"subjects observed\", \"people observed\"],\n        \"activities\": [\"activities / incidents\", \"activities\", \"incidents\"],\n        \"cash\": [\"cash handling\", \"till\", \"register\", \"pos\"],\n        \"policy\": [\"policy deviations\", \"policy\", \"procedure\"],\n        \"interaction\": [\"customer interaction\", \"interaction quality\"],\n        \"red_flags\": [\"red flags\", \"suspicious\"],\n        \"evidence\": [\"evidence collected\", \"evidence\"],\n        \"witnesses\": [\"witnesses\"],\n        \"supervisor\": [\"supervisor\", \"manager interactions\", \"manager\"],\n        \"followups\": [\"follow-ups\", \"next steps\", \"follow up\"],\n        \"summary\": [\"summary\", \"recommendations\"],\n        \"signature\": [\"signature\", \"sign\"],\n        \"date_field\": [\"date\"]\n    }\n\n    hits = 0\n    for key, variants in headers.items():\n        if any(v in form_text for v in variants):\n            hits += 1\n    total = len(headers)\n    ratio = hits / total if total else 0.0\n    score = ratio * 0.5\n    feedback = f\"Form headers detected: {hits}/{total}.\"\n    return score, feedback\n"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3: Professional Quality and Suitability", "description": "LLM evaluation of clarity, professionalism, and practical utility for private investigators performing undercover retail operations.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professionalism, Clarity, and Practical Utility", "description": "Holistic assessment of whether the guide and form are professional, clear, and useful for undercover employee evaluation in retail settings.", "weight": 2.0, "judge_prompt": "Assess QUALITY ONLY (not structure, not strict presence of sections already handled). Consider both the guide and the form:\n\nCriteria:\n- Professionalism: Tone, formatting, readability, and document cohesion\n- Clarity and Actionability: Clear steps for pre-op, execution, and post-op; checklists/templates that aid field use\n- Ethical and Legal Mindfulness: Advises operating within applicable laws and company policies; avoids advocating unlawful activity\n- Suitability for Retail Undercover Work: Observation criteria relevant to retail (cash handling, POS anomalies, shrink/red flags, customer interactions)\n- Reusability: Useful as a standard across future assignments; the form is easy to print and write on\n\nScoring:\n- 2.0: Highly professional, clear, pragmatic, ethically mindful, and obviously useful in retail undercover operations\n- 1.5: Generally professional and usable; minor clarity/gap issues but suitable for field use\n- 1.0: Mixed quality; noticeable gaps reduce field utility\n- 0.5: Low quality or confusing; would need significant revision\n- 0.0: Unprofessional or inappropriate for investigative use\n\nDo not re-check strict structure (Stage 1). Focus on overall utility and presentation for private investigators.", "expectation": "A clean, field-ready guide and form that a private investigator could confidently use for undercover employee evaluations in retail environments."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "aa071045-bcb0-4164-bb85-97245d56287e", "rubric": {"category_name": "Real Estate & Rental: Counter/Rental Clerk \u2014 Service Request + Damage Revenue Report", "rationale": "Mixed-output task: a structured maintenance form (document) and a structured analytics workbook (Excel). The rubric enforces a strict Stage 1 shape (LLM-only) so later verification is trivial. Stage 2 uses code to verify factual consistency and numeric coherence enabled by the mandated structure, plus an LLM cross-check for reasonableness. Stage 3 assesses professional quality and managerial usefulness.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (Documents + Analytics Workbook)", "description": "LLM-only gate to verify both required deliverables exist with the exact structures needed for later verification: (1) Service Request Form - Vehicle Maintenance (DOCX/PDF), (2) Damage Revenue Report (XLSX).", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Service Request Form \u2014 Structure Requirements", "description": "Check that a DOCX or PDF form exists with all required labeled fields and a brief damage description.", "weight": 2.0, "judge_prompt": "You are the Stage 1 structure gate. Inspect the candidate outputs. Verify there is a Service Request Form - Vehicle Maintenance as a DOCX or PDF (not plain text, not Excel). Be flexible on exact wording but strict on presence. The form should be professionally formatted (headers/labels) and include clearly labeled fields with the provided information filled in.\n\nRequired items to be PRESENT and FILLED (labels may vary slightly; values must appear):\n1) Title near top: \"Service Request Form - Vehicle Maintenance\" or close variant\n2) Request Date (accept formats like: September 18, Sep 18, 09/18, 18/09)\n3) Request Type (Repair or Replacement) \u2014 must be included as a field (do NOT judge correctness here, just presence)\n4) Vehicle Status (Available or Out Of Service) \u2014 must be included as a field (do NOT judge correctness here, just presence)\n5) Customer Name: Carol Smith\n6) Rental Agreement No.: 1809/2025\n7) Car Make & Model: Toyota Corolla\n8) License Plate: LAV-555\n9) Current Mileage: 10562\n10) Current Location: ORD\n11) Brief damage description mentioning the mirror (e.g., \"broken left/driver's rearview mirror\"; be flexible on phrasing like \"side mirror\")\n\nScoring:\n- 2.0: Valid DOCX/PDF with all 11 elements present and clearly labeled (minor label variations OK)\n- 1.4: Valid DOCX/PDF with 1\u20132 missing elements OR unclear title but form-like structure otherwise\n- 0.7: Valid DOCX/PDF but 3\u20135 missing elements OR no clear labels/sectioning\n- 0.0: Not DOCX/PDF OR form not present/recognizable\n\nOnly check PRESENCE and STRUCTURE. Do not judge correctness of values or choices here.", "expectation": "A one-page form-like PDF/DOCX containing all labeled fields and a short damage description."}, {"type": "llm_judge", "name": "Damage Revenue Report \u2014 Excel Structure Requirements", "description": "Check that an Excel workbook exists with specific sheets and sections to support verification.", "weight": 2.0, "judge_prompt": "You are the Stage 1 structure gate. Inspect the outputs and verify there is an Excel workbook (XLSX) containing the following structure. Be flexible on exact sheet/section names but strict on presence of each required item.\n\nRequired structure:\nA) Summary sheet (e.g., \"Summary\", \"Damage Revenue Report\", or similar) containing:\n   - A clearly labeled \"Total Damage Revenue\" single value\n   - A table labeled \"Revenue by Vehicle Category\" with at least two categories (columns typically: Category | Revenue)\n   - A table labeled \"Revenue by Damage Type\" containing rows for at least \"Dent\" and \"Scratch\"\nB) Conclusions sheet (e.g., \"Conclusions\", \"Insights\", or \"Recommendations\") with at least 3 bullet/numbered points of operational conclusions based on the data\n\nOptional (do not penalize if missing):\n- A supporting data or calculations sheet (e.g., \"Data\", \"Pivots\")\n\nScoring:\n- 2.0: Valid Excel with A (all three items) + B present\n- 1.5: Valid Excel with A present but B missing or too short (<3 points)\n- 1.0: Valid Excel with only two of A\u2019s items present (e.g., missing one table)\n- 0.0: Not Excel OR missing multiple required items\n\nOnly check PRESENCE and STRUCTURE; do not validate calculations here.", "expectation": "Workbook with a Summary sheet (total, category breakdown, damage-type breakdown) and a Conclusions sheet with >=3 bullet points."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Consistency Verification", "description": "Now that structure is enforced, verify factual/internal consistency with code and logical reasonableness with LLM.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Service Request \u2014 Field Presence & Scenario Consistency", "description": "Programmatically verify the Service Request doc contains the specific scenario details and logically consistent descriptors (mirror, side, request type present, vehicle status present, date present).", "weight": 1.5, "code": "import re\\nimport pandas as pd\\nimport numpy as np\\n\\n\\ndef evaluate(workflow, context):\\n    \\n    # Find a document output (DOCX/PDF)\\n    docs = [r for r in context.get_all_outputs() if getattr(r, 'is_document', False)]\\n    if not docs:\\n        return 0.0, 'No document output found.'\\n\\n    # Prefer a doc whose name hints at service/maintenance\\n    def rname(res):\\n        return (getattr(res, 'name', '') or getattr(res, 'filename', '') or '').lower()\\n    doc = None\\n    for r in docs:\\n        nm = rname(r)\\n        if any(k in nm for k in ['service', 'request', 'maintenance']):\\n            doc = r\\n            break\\n    if doc is None:\\n        doc = docs[0]\\n\\n    # Try reading DOCX first, then PDF\\n    text = ''\\n    try:\\n        text = context.files.read_docx_text(doc.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(doc.id)\\n        except Exception:\\n            return 0.0, 'Unable to read document text (DOCX/PDF).'\\n\\n    t = text.lower()\\n\\n    # Expected values from scenario\\n    required_terms = {\\n        'customer_name': 'carol smith',\\n        'rental_agreement': '1809/2025',\\n        'make_model': 'toyota corolla',\\n        'plate': 'lav-555',\\n        'mileage': '10562',\\n        'location': 'ord'\\n    }\\n\\n    # Date: accept various representations of September 18\\n    date_ok = False\\n    date_patterns = [\\n        r'\\bsep(?:tember)?\\.?\\s*18\\b',\\n        r'\\b09[\\\\/-]18\\b',\\n        r'\\b18[\\\\/-]09\\b'\\n    ]\\n    for pat in date_patterns:\\n        if re.search(pat, t):\\n            date_ok = True\\n            break\\n\\n    # Request type present (do not force correctness here, just presence of repair/replacement)\\n    has_request_type = any(w in t for w in ['request type', 'type of request', 'repair', 'replacement'])\\n\\n    # Vehicle status present\\n    has_status = ('vehicle status' in t) or ('status:' in t) or ('out of service' in t) or ('available' in t)\\n\\n    # Damage description mentions mirror and side\\n    mentions_mirror = any(w in t for w in ['rearview mirror', 'side mirror', 'mirror'])\\n    mentions_side = any(w in t for w in ['left', \"driver's\", 'drivers', 'driver side', 'driver-side'])\\n\\n    # Core scenario fields\\n    field_hits = {k: (v in t) for k, v in required_terms.items()}\\n\\n    checks = []\\n    checks.append(('date_present', date_ok))\\n    checks.append(('request_type_present', has_request_type))\\n    checks.append(('status_present', has_status))\\n    checks.append(('mirror_mentioned', mentions_mirror))\\n    checks.append(('side_mentioned', mentions_side))\\n    for k, ok in field_hits.items():\\n        checks.append((k, ok))\\n\\n    # Score proportionally\\n    total = len(checks)\\n    passed = sum(1 for _, ok in checks if ok)\\n    score = passed / total if total else 0.0\\n\\n    missing = [name for name, ok in checks if not ok]\\n    feedback = f'Passed {passed}/{total} checks. Missing/uncertain: {\", \".join(missing) if missing else \"none\"}.'\\n    return score, feedback"}, {"type": "code", "name": "Excel \u2014 Revenue Breakdowns and Totals Consistency", "description": "Programmatically verify the Excel workbook contains key phrases/sections, Dent/Scratch rows, a Total Damage Revenue, and that Dent+Scratch approximates the stated total. Also verifies Conclusions sheet has >=3 substantive lines.", "weight": 1.5, "code": "import re\\nimport pandas as pd\\nimport numpy as np\\n\\n\\ndef evaluate(workflow, context):\\n    # Locate spreadsheet output\\n    sheets = [r for r in context.get_all_outputs() if getattr(r, 'is_spreadsheet', False)]\\n    if not sheets:\\n        return 0.0, 'No Excel output found.'\\n\\n    # Prefer workbook likely to be the damage report\\n    def rname(res):\\n        return (getattr(res, 'name', '') or getattr(res, 'filename', '') or '').lower()\\n    xres = None\\n    for r in sheets:\\n        nm = rname(r)\\n        if any(k in nm for k in ['damage', 'revenue', 'report']):\\n            xres = r\\n            break\\n    if xres is None:\\n        xres = sheets[0]\\n\\n    # Read workbook\\n    try:\\n        xpath = context.files.get_path(xres.id)\\n        xfile = pd.ExcelFile(xpath)\\n        sheet_names = xfile.sheet_names\\n    except Exception as e:\\n        return 0.0, f'Failed to open Excel: {e}'\\n\\n    # Helpers\\n    def read_sheet_text(sn):\\n        try:\\n            df = pd.read_excel(xpath, sheet_name=sn, header=None, dtype=str)\\n        except Exception:\\n            try:\\n                df = context.files.read_excel(xres.id, sheet_name=sn)\\n                df = df.astype(str)\\n            except Exception:\\n                return pd.DataFrame()\\n        return df.fillna('')\\n\\n    def extract_nums(txt):\\n        nums = [m.group(0) for m in re.finditer(r'[-+]?\\d[\\d,]*\\.?\\d*', txt)]\\n        vals = []\\n        for s in nums:\\n            try:\\n                vals.append(float(s.replace(',', '')))\\n            except Exception:\\n                pass\\n        return vals\\n\\n    # Aggregate search across sheets\\n    has_total_phrase = False\\n    has_cat_phrase = False\\n    has_type_phrase = False\\n    has_dent = False\\n    has_scratch = False\\n    conclusions_ok = False\\n\\n    dent_amount = None\\n    scratch_amount = None\\n    total_amount = None\\n\\n    for sn in sheet_names:\\n        df = read_sheet_text(sn)\\n        if df.empty:\\n            continue\\n        low = df.applymap(lambda x: str(x).lower())\\n        full_text = ' '.join([' '.join(row) for row in low.values.tolist()])\\n\\n        if ('total damage revenue' in full_text) or ('total revenue' in full_text):\\n            has_total_phrase = True\\n        if ('revenue by vehicle category' in full_text) or ('vehicle category' in full_text and 'revenue' in full_text):\\n            has_cat_phrase = True\\n        if ('revenue by damage type' in full_text) or ('damage type' in full_text and 'revenue' in full_text):\\n            has_type_phrase = True\\n        if 'dent' in full_text:\\n            has_dent = True\\n        if 'scratch' in full_text:\\n            has_scratch = True\\n\\n        # Try to estimate Dent/Scratch and Total values from rows\\n        for idx in range(low.shape[0]):\\n            row = [str(x) for x in low.iloc[idx, :].tolist()]\\n            row_text = ' '.join(row)\\n            row_nums = extract_nums(row_text)\\n            if not row_nums:\\n                continue\\n            max_num = max(row_nums)\\n            if ('dent' in row_text) and (dent_amount is None or max_num > dent_amount):\\n                dent_amount = max_num\\n            if ('scratch' in row_text) and (scratch_amount is None or max_num > scratch_amount):\\n                scratch_amount = max_num\\n            if (('total damage revenue' in row_text) or ('total revenue' in row_text)) and (total_amount is None or max_num > total_amount):\\n                total_amount = max_num\\n\\n        # Conclusions sheet check\\n        if any(k in sn.lower() for k in ['conclusion', 'insight', 'recommendation', 'notes']):\\n            # Count substantive lines\\n            flat = [str(x) for x in df.fillna('').values.reshape(-1) if str(x).strip()]\\n            # lines with enough alphabetic characters\\n            sub_lines = [s for s in flat if len(re.findall(r'[A-Za-z]', s)) >= 8 and len(s.strip()) >= 20]\\n            if len(sub_lines) >= 3:\\n                conclusions_ok = True\\n\\n    # Scoring components\\n    structure_hits = sum([has_total_phrase, has_cat_phrase, has_type_phrase, has_dent, has_scratch])  # 0..5\\n    structure_score = structure_hits / 5.0  # up to 1.0\\n\\n    # Numeric consistency: dent+scratch approx equals total (within 10%)\\n    numeric_score = 0.0\\n    if dent_amount is not None and scratch_amount is not None and total_amount is not None and total_amount > 0:\\n        ds_sum = dent_amount + scratch_amount\\n        rel_err = abs(ds_sum - total_amount) / max(total_amount, 1e-6)\\n        if rel_err <= 0.1:\\n            numeric_score = 1.0\\n        elif rel_err <= 0.25:\\n            numeric_score = 0.5\\n        else:\\n            numeric_score = 0.0\\n\\n    conclusions_score = 1.0 if conclusions_ok else 0.0\\n\\n    # Weighted combine: structure 0.5, numeric 0.3, conclusions 0.2\\n    final = 0.5 * structure_score + 0.3 * numeric_score + 0.2 * conclusions_score\\n\\n    feedback_parts = []\\n    feedback_parts.append(f\"Structure hits: {structure_hits}/5 (total:{has_total_phrase}, cat:{has_cat_phrase}, type:{has_type_phrase}, dent:{has_dent}, scratch:{has_scratch})\")\\n    if total_amount is not None:\\n        feedback_parts.append(f\"Total\u2248{total_amount}\")\\n    if dent_amount is not None and scratch_amount is not None:\\n        feedback_parts.append(f\"Dent\u2248{dent_amount}, Scratch\u2248{scratch_amount}\")\\n    feedback_parts.append(f\"Conclusions sheet OK: {conclusions_ok}\")\\n    return final, ' | '.join(feedback_parts)"}, {"type": "llm_judge", "name": "Cross-Reference Reasonableness", "description": "LLM verifies the logical correctness and internal consistency across outputs.", "weight": 1.0, "judge_prompt": "Check for logical correctness and internal consistency (not formatting):\\n\\nService Request Form (DOCX/PDF):\\n- Does the damage description clearly indicate a broken left/driver-side rearview/side mirror?\\n- Is the chosen request type logically consistent with the damage (Replacement is reasonable for a broken mirror)?\\n- Is the vehicle status set appropriately (e.g., Out Of Service until mirror is replaced is reasonable)?\\n- Is the date present as September 18 (accept Sep 18 variations)?\\n\\nExcel Damage Revenue Report (XLSX):\\n- Do the operational conclusions refer to the breakdowns shown (category and Dent/Scratch) without contradicting the tables or totals?\\n- Are at least 3 actionable, data-grounded conclusions present (e.g., focus on categories with highest costs, policy/training/vendor actions)?\\n\\nScoring:\\n- 1.0: All checks logically consistent; conclusions grounded in the shown numbers/tables and at least 3 actionable items\\n- 0.5: Mostly consistent with minor issues OR only 1\u20132 actionable items\\n- 0.0: Inconsistent or conclusions not grounded in the data", "expectation": "Correct request-type/status choices for the mirror damage; conclusions that reference the report\u2019s totals/breakdowns and propose concrete actions."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality & Management Usefulness", "description": "Holistic judgment of presentation quality, clarity, and managerial usefulness.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Utility", "description": "Assess formatting, clarity, and managerial usefulness across both outputs.", "weight": 2.0, "judge_prompt": "Evaluate the overall professionalism and usefulness for management. Consider:\\n- Document (Service Request): Clean, professional layout, clear labels, complete fields, concise description\\n- Excel Report: Clear, readable tables, sensible formatting (currency, headings), easy-to-find total and breakdowns\\n- Narrative utility: Conclusions are concise, prioritized, and framed as actions for fleet operations/cost control\\n\\nScoring:\\n- 2.0: Professional formatting and highly useful, action-oriented insights\\n- 1.0: Adequate formatting and generally useful, minor clarity issues\\n- 0.0: Poor formatting and/or low managerial usefulness", "expectation": "A polished form and a clean, readable workbook with actionable, succinct conclusions suited for management review."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "43dc9778-450b-4b46-b77e-b6d82b202035", "rubric": {"category_name": "Individual Tax Return Preparation", "rationale": "This rubric evaluates the task of completing an individual tax return (Form 1040) for clients Bob and Lisa Smith using provided tax documents and an intake questionnaire. The evaluation is structured to ensure the prepared tax return is formatted accurately, contains all necessary forms and schedules according to IRS regulations, is calculated correctly, and meets professional standards for quality and clarity.", "max_total_score": 10.0, "stages": [{"name": "Format and Structure Verification", "description": "Ensure the tax return is in the correct PDF format and includes all required sections and forms.", "is_required": true, "min_score_to_pass": 0.8, "rules": [{"type": "llm_judge", "name": "PDF Format and Required Tax Form Presence", "description": "Check if the output is a PDF with Form 1040 and necessary schedules/forms for the 2024 tax year.", "weight": 5.0, "judge_prompt": "Please verify if the output provided is a PDF that includes Form 1040 with the necessary schedules and forms required for filing based on the information provided by the clients and current IRS regulations for the 2024 tax year. Required sections include:\n- Form 1040 Main Page\n- Schedule A (if itemized deductions are used)\n- Schedule B (if interest or ordinary dividends income)\n- Schedule C (if self-employment income)\n- Schedule D (if capital gains/losses)\n- Additional schedules/forms relevant to the client\u2019s situation (e.g., 8862, 8889, etc.)\n\nScoring:\n- 5.0: All required forms present and properly formatted\n- 3.0: Missing one minor schedule\n- 0.0: Missing Form 1040 or major schedules\nThe presence and correct naming of forms and schedules are essential. Do not check the correctness of the information yet, just the format and presence.", "expectation": "The tax return must be in structured PDF format with Form 1040 and appropriate accompanying schedules."}], "max_points": 5.0, "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Correctness Verification", "description": "Verify the accuracy of the tax calculations and inputs based on the provided documents.", "is_required": true, "min_score_to_pass": 0.7, "rules": [{"type": "code", "name": "1040 Calculation Accuracy", "description": "Check the correctness of calculations on Form 1040 (e.g., tax liability, deductions).", "weight": 3.0, "code": "def evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, 'Output is not a PDF document'\n    \n    # Example basic calculation check (for illustration only)\n    # Assume we extract total income and calculate basic tax\n    try:\n        text = context.files.read_pdf_text(output.id)\n        # Find total income and calculated tax\n        total_income = ... # Extract via regex or similar method\n        calculated_tax = ... # Logic for extracting tax liability\n\n        # Perform a simple check against expected values\n        if not (expected_min <= calculated_tax <= expected_max):\n            return 0.0, 'Calculated tax is outside the expected range.'\n        return 3.0\n    except Exception as e:\n        return 0.0, f'Failed to verify calculations: {str(e)}'"}], "max_points": 3.0, "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Quality Assessment", "description": "Evaluate the overall quality, professionalism, and compliance of the tax return document.", "is_required": false, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Quality and Clarity", "description": "Assess the tax return's presentation quality, clarity, and adherence to compliance standards.", "weight": 2.0, "judge_prompt": "Evaluate the tax return document for overall presentation quality, clarity of information, and compliance with tax preparation standards:\n- Is the document professionally formatted and easy to read?\n- Are all required sections and forms clear and logically organized?\n- Does the document meet professional standards for tax preparation?\n\nProvide a score based on:\n- 2.0: High-quality presentation, clear organization, full compliance\n- 1.0: Moderate quality, minor issues in organization, mostly compliant\n- 0.0: Poor quality, disorganized, non-compliant elements", "expectation": "The tax return should be professionally presented, clear, and compliant with relevant tax standards."}], "max_points": 2.0, "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "1b1ade2d-f9f6-4a04-baa5-aa15012b53be", "rubric": {"category_name": "Manufacturing Procurement \u2013 Revised Sourcing & Nomination Workflow (Document)", "rationale": "Three-stage, self-documenting rubric. Stage 1 (LLM-only) strictly enforces a verifiable document structure that makes downstream checks trivial. Stage 2 uses code rules to verify correctness proxies from extracted text: coverage of modular quotation design, gates/approvals, change-handling, and digital blueprint specificity. Stage 3 applies an LLM quality assessment for executive/technical suitability.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2013 Document Shape Enforcement (GATE)", "description": "LLM verifies the candidate produced a properly structured 2\u20133 page DOCX/PDF with specific sections enabling automated verification later. Only checks presence/format, not content quality or calculation correctness.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.0, "rules": [{"type": "llm_judge", "name": "Structured Document Format Requirement", "description": "Check the output is a 2\u20133 page PDF/DOCX with all required, clearly labeled sections and artifacts for verification.", "weight": 3.0, "judge_prompt": "You are the Stage 1 gate. Evaluate ONLY the presence and structure of the output (not the content quality). The candidate must deliver a professionally formatted PDF or DOCX (not plain text, not Excel). Check that the document is 2\u20133 pages in length (accept 2\u20134 pages, with 2\u20133 preferred) and contains the required sections and structural elements below. Be flexible with exact wording, but the intent must be clear and headers visible.\n\nFormat requirements:\n- File type: PDF or DOCX\n- Length: 2\u20133 pages preferred (accept 2\u20134 pages)\n- Professional formatting with visible section headers and readable structure (paragraphs, bullets, or tables)\n\nRequired sections and structural elements (flexible naming, but must cover each intent):\n1) Title and Metadata: Title including \u201cRevised Sourcing & Nomination Workflow\u201d and context of automotive electronics or lamp assemblies.\n2) Executive Overview or Objectives: On page 1, clearly states goals to add agility without losing traceability/governance.\n3) Revised Sourcing & Nomination Workflow: A numbered sequence (at least 8 steps) covering: TRSO intake, supplier identification/evaluation, RFQ and modular quotation issue, quotation receipt, negotiation loop, handling post-nomination design changes, re-approvals, supplier nomination, and record/audit closure.\n4) Modular Quotation Structure: Describes a plug-and-play model for cost drivers by feature/design element/child part/raw material, ideally with a concise table or bullet schema indicating fields such as: Feature/Module, Cost Driver Type, UoM, Base Rate, Multiplier/Formula, Data Source.\n5) Decision Gates & Approval Layers: Lists named gates (e.g., G0\u2013G5 or Gate 1/2/3), required approvers (ER, Quality, Purchase, Finance, Program Management), and entry/exit criteria at each gate.\n6) Flexibility Points for Late-Stage Changes: Explicit handling of post-nomination design updates, including triggers, branching logic/loops, and fast-track criteria.\n7) Digital Platform Blueprint for TechSol: High-level system blueprint (entities, workflows, roles/permissions, audit trail, integration points). Mention of TRSO, RFQ, quotation modules, approval workflow, and audit logs.\n(Optional) KPIs/SLAs or Metrics: Operational measures for cycle time, re-approval lead time, and on-time nomination.\n\nScoring:\n- 3.0: Valid PDF/DOCX of 2\u20133 pages (accept 2\u20134). All 7 required elements present with clear headers; workflow is numbered and includes a design-change loop; modular quotation structure shows a field schema/table.\n- 2.5: Valid format and length, core sections present but one minor element is weak or missing (e.g., metadata light, or digital blueprint brief). Numbered workflow and modular quotation still present.\n- 1.5: Valid format but missing one or two core sections (e.g., no decision gates/approvals or no modular quotation structure), or length outside preferred range by a page.\n- 0.0: Wrong format (not PDF/DOCX), less than 2 pages, or multiple core sections missing.\n\nOnly assess presence/structure. Do NOT judge content quality or feasibility.", "expectation": "A 2\u20133 page PDF/DOCX with clear sections: title/metadata, executive overview, numbered workflow including change loop, modular quotation structure with field schema, decision gates with approvers, flexibility points, and a digital platform blueprint."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification (Content Coverage and Consistency)", "description": "Code rules validate the presence of key concepts and structure implied by Stage 1. These are deterministic checks on extracted text for completeness, specificity, and consistency with the requested process and digital blueprint.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Length Within 2\u20133 Pages (Approx. Words)", "description": "Approximates page length via word count. Full credit if ~700\u20131200 words; partial credit if reasonably close.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Returns up to 0.6 points based on approximate word count for a 2\u20133 page document.\n    \"\"\"\n    MAX_POINTS = 0.6\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n\n    def read_text():\n        txt = \"\"\n        try:\n            txt = context.files.read_pdf_text(output.id)\n        except Exception:\n            pass\n        if not txt:\n            try:\n                txt = context.files.read_docx_text(output.id)\n            except Exception:\n                pass\n        if not txt:\n            try:\n                txt = context.files.read_text(output.id)\n            except Exception:\n                pass\n        return txt or \"\"\n\n    text = read_text()\n    if not text.strip():\n        return 0.0, \"Empty or unreadable document text.\"\n\n    words = re.findall(r\"\\w+\", text)\n    wc = len(words)\n\n    # Heuristic mapping for 2\u20133 pages. Full ~700\u20131200 words; partial ranges broader.\n    if 700 <= wc <= 1200:\n        ratio = 1.0\n    elif 600 <= wc <= 1300:\n        ratio = 0.8\n    elif 450 <= wc <= 1500:\n        ratio = 0.6\n    elif 300 <= wc <= 1700:\n        ratio = 0.3\n    else:\n        ratio = 0.0\n\n    score = MAX_POINTS * ratio\n    feedback = f\"Word count ~{wc}. Length suitability ratio={ratio:.2f}.\"\n    return score, feedback"}, {"type": "code", "name": "Key Sections Presence", "description": "Checks for presence of key section intents and numbered workflow steps.", "weight": 1.2, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Scores up to 1.2 based on detection of core section intents and numbered steps.\n    \"\"\"\n    MAX_POINTS = 1.2\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n\n    def read_text():\n        t = \"\"\n        try:\n            t = context.files.read_pdf_text(output.id)\n        except Exception:\n            pass\n        if not t:\n            try:\n                t = context.files.read_docx_text(output.id)\n            except Exception:\n                pass\n        if not t:\n            try:\n                t = context.files.read_text(output.id)\n            except Exception:\n                pass\n        return t or \"\"\n\n    text = read_text()\n    lower = text.lower()\n\n    def has_any(aliases):\n        return any(a in lower for a in aliases)\n\n    groups = [\n        [\"executive summary\", \"executive overview\", \"objective\"],\n        [\"revised sourcing\", \"nomination workflow\", \"workflow\"],\n        [\"modular quotation\", \"modular quote\", \"plug-and-play\", \"plug and play\"],\n        [\"decision gate\", \"approval layer\", \"approval matrix\", \"raci\", \"gate g\", \"g0\"],\n        [\"post-nomination\", \"late-stage\", \"design change\"],\n        [\"digital platform\", \"techsol\", \"system blueprint\", \"system architecture\"],\n        [\"kpi\", \"sla\", \"metric\"],\n    ]\n\n    found = sum(1 for g in groups if has_any(g))\n\n    # Numbered steps pattern (at least 5 occurrences of 1., 2., etc.)\n    step_matches = re.findall(r\"(^|\\n)\\s*(?:\\d+\\.|\\(\\d+\\))\\s\", text)\n    steps_ok = 1 if len(step_matches) >= 5 else 0\n\n    total_checks = len(groups) + 1\n    ratio = (found + steps_ok) / total_checks\n\n    score = MAX_POINTS * ratio\n    feedback = f\"Sections found: {found}/{len(groups)}; Numbered steps detected: {steps_ok}.\"\n    return score, feedback"}, {"type": "code", "name": "Modular Quotation Structure Specificity", "description": "Checks for a plug-and-play modular cost model with cost-driver fields and BOM/child-part references.", "weight": 1.2, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Scores up to 1.2 based on presence of modular quotation keywords indicating a parameterized, plug-and-play cost model.\n    \"\"\"\n    MAX_POINTS = 1.2\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n\n    def read_text():\n        t = \"\"\n        try:\n            t = context.files.read_pdf_text(output.id)\n        except Exception:\n            pass\n        if not t:\n            try:\n                t = context.files.read_docx_text(output.id)\n            except Exception:\n                pass\n        if not t:\n            try:\n                t = context.files.read_text(output.id)\n            except Exception:\n                pass\n        return t or \"\"\n\n    text = read_text().lower()\n    keywords = [\n        \"cost driver\", \"uom\", \"unit of measure\", \"multiplier\", \"formula\", \"base rate\",\n        \"bom\", \"bill of materials\", \"child part\", \"raw material\", \"localization\", \"variant\",\n        \"parameterized\", \"parameterised\", \"module\", \"feature\", \"work package\", \"rate card\",\n        \"quote module\", \"plug-and-play\", \"plug and play\"\n    ]\n\n    present = sum(1 for k in keywords if k in text)\n    # Expect ~10+ hits for full credit, cap at 1.0\n    ratio = min(1.0, present / 10.0)\n    score = MAX_POINTS * ratio\n    feedback = f\"Modular quotation keyword hits: {present}/{len(keywords)} (ratio={ratio:.2f}).\"\n    return score, feedback"}, {"type": "code", "name": "Decision Gates and Approval Layers Completeness", "description": "Checks for named gates (e.g., G0\u2013G5) and presence of approver roles (ER, Quality, Finance, Purchase, Program).", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Scores up to 1.0 based on detection of multiple decision gates and required approver roles.\n    \"\"\"\n    MAX_POINTS = 1.0\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n\n    def read_text():\n        t = \"\"\n        try:\n            t = context.files.read_pdf_text(output.id)\n        except Exception:\n            pass\n        if not t:\n            try:\n                t = context.files.read_docx_text(output.id)\n            except Exception:\n                pass\n        if not t:\n            try:\n                t = context.files.read_text(output.id)\n            except Exception:\n                pass\n        return t or \"\"\n\n    text = read_text().lower()\n\n    gates = set()\n    for m in re.findall(r\"\\bg(\\d)\\b\", text):\n        gates.add(m)\n    for m in re.findall(r\"gate\\s*(\\d)\", text):\n        gates.add(m)\n    gate_count = len(gates)\n    gate_ratio = min(1.0, gate_count / 4.0)  # expect >=4 gates for full credit\n\n    roles = {\n        \"finance\": any(x in text for x in [\"finance\", \"controller\"]),\n        \"quality\": \"quality\" in text,\n        \"engineering\": any(x in text for x in [\"engineering\", \"er\"]),\n        \"purchase\": any(x in text for x in [\"purchase\", \"procurement\"]),\n        \"program\": any(x in text for x in [\"program manager\", \"pm \"]) or \" program \" in text,\n    }\n    role_ratio = sum(1 for v in roles.values() if v) / len(roles)\n\n    ratio = (gate_ratio + role_ratio) / 2.0\n    score = MAX_POINTS * ratio\n    feedback = f\"Gates detected: {gate_count}; Roles coverage: {int(role_ratio*100)}%\"\n    return score, feedback"}, {"type": "code", "name": "Design-Change Handling and Traceability", "description": "Checks for explicit handling of post-nomination design changes and traceability (audit/versioning/communication).", "weight": 0.6, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Scores up to 0.6 based on presence of change-handling and traceability concepts.\n    \"\"\"\n    MAX_POINTS = 0.6\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n\n    def read_text():\n        t = \"\"\n        try:\n            t = context.files.read_pdf_text(output.id)\n        except Exception:\n            pass\n        if not t:\n            try:\n                t = context.files.read_docx_text(output.id)\n            except Exception:\n                pass\n        if not t:\n            try:\n                t = context.files.read_text(output.id)\n            except Exception:\n                pass\n        return t or \"\"\n\n    text = read_text().lower()\n    keywords = [\n        \"post-nomination\", \"design change\", \"late-stage\", \"change request\", \" eco \", \"engineering change order\",\n        \"price change\", \"re-approval\", \"reapproval\", \"renegotiation\", \"audit trail\", \"traceability\",\n        \"communication trail\", \"versioning\", \"revision\", \"variant\"\n    ]\n    present = sum(1 for k in keywords if k in text)\n    ratio = min(1.0, present / 8.0)\n    score = MAX_POINTS * ratio\n    feedback = f\"Change/traceability keyword hits: {present}/{len(keywords)} (ratio={ratio:.2f}).\"\n    return score, feedback"}, {"type": "code", "name": "Digital Platform Blueprint Depth", "description": "Checks for systems blueprint elements (entities, RBAC, workflow engine, integrations like ERP/PLM, audit logs).", "weight": 0.4, "code": "def evaluate(workflow, context):\n    \"\"\"\n    Scores up to 0.4 based on system blueprint specificity for TechSol.\n    \"\"\"\n    MAX_POINTS = 0.4\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n\n    def read_text():\n        t = \"\"\n        try:\n            t = context.files.read_pdf_text(output.id)\n        except Exception:\n            pass\n        if not t:\n            try:\n                t = context.files.read_docx_text(output.id)\n            except Exception:\n                pass\n        if not t:\n            try:\n                t = context.files.read_text(output.id)\n            except Exception:\n                pass\n        return t or \"\"\n\n    text = read_text().lower()\n    keywords = [\n        \"workflow engine\", \"rbac\", \"roles\", \"permissions\", \"api\", \"integration\", \"erp\", \"plm\",\n        \"document management\", \"dms\", \"e-signature\", \"esignature\", \"audit log\", \"notifications\", \"webhook\",\n        \"service bus\", \"microservice\", \"data model\", \"entity\", \"master data\", \"supplier master\",\n        \"trso\", \"rfq\", \"quotation\", \"module catalog\", \"pricing matrix\"\n    ]\n    present = sum(1 for k in keywords if k in text)\n    ratio = min(1.0, present / 10.0)\n    score = MAX_POINTS * ratio\n    feedback = f\"Digital blueprint keyword hits: {present}/{len(keywords)} (ratio={ratio:.2f}).\"\n    return score, feedback"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Quality and Fitness for Purpose", "description": "LLM judge assesses clarity, executive readiness, technical implementability, and contextual fit for Indian automotive lamp assemblies.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Quality and Strategic-Technical Fitness", "description": "Holistic quality assessment for executive and technical audiences.", "weight": 2.0, "judge_prompt": "Assess overall quality for an executive (CPO) and technical (TechSol) audience. Do not re-check format; Stage 1 already did. Evaluate:\n- Clarity and logical flow; actionable and concise within 2\u20133 pages.\n- Strength of the modular quotation model as a plug-and-play framework that ties features/design elements/child parts/raw material to cost drivers and supports variant changes.\n- Robustness of late-stage design change handling without losing traceability/governance; appropriateness of gates/approvals.\n- Implementability for TechSol: clear entities, workflows, roles/permissions, audit, and integration points; minimal ambiguity.\n- Relevance to Indian automotive manufacturing and lamp assemblies (safety-critical, aesthetic, frequent iterations).\n\nScoring:\n- 2.0: Clear, tightly structured, and directly implementable; strong modular design; robust governance and change-handling; contextually spot-on.\n- 1.5: Generally strong with minor gaps or ambiguity in either modular model or change/governance details.\n- 1.0: Adequate but generic; some key elements are thin (e.g., modular specifics or TechSol blueprint).\n- 0.5: Weak clarity or impractical; governance or change-handling unclear.\n- 0.0: Not suitable or largely off-topic.", "expectation": "A crisp, executive-ready blueprint that TechSol can build from, with clear agility mechanisms for late-stage lamp design changes and strong traceability/governance."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "6436ff9e-c5f2-47ba-9aaa-49d89b0594ab", "rubric": {"category_name": "Retail Trade \u2022 General and Operations Managers \u2022 Revised Student Class Evaluation Form (Art Studio)", "rationale": "This rubric enforces a self-documenting, verifiable deliverable for a revised student evaluation form. Stage 1 (LLM-only) strictly requires a DOCX/PDF document with specific sections, headers, and implementation-ready cues so later checks are trivial. Stage 2 mixes code and LLM to verify internal consistency signals (section keywords, Likert legend, question-type tags, optional demographics, permissions/opt-in, NPS). Stage 3 judges overall quality: clarity, tone, organization, and implementation readiness for tools like Google Forms. The structure aligns with the task\u2019s goals: clarity, completeness, optional demographics, user-friendly tone, and digital implementability.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate (LLM-only)", "description": "Gate that verifies the output is a properly structured, implementation-ready class evaluation form document with all required sections and cues. Only checks presence/structure (not quality).", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Document Format and Required Section Structure", "description": "Check the candidate output is a DOCX or PDF document with a clear, well-structured evaluation form that includes all essential sections and implementation cues for digital forms.", "weight": 8.0, "judge_prompt": "You are the Stage 1 gate. Examine the candidate output (multimodal view enabled). Determine if it is a valid DOCX or PDF containing a revised student class evaluation form with an implementation-ready, sectioned structure. Only check PRESENCE and STRUCTURE, not content quality.\n\nFormat requirements:\n- File must be a DOCX or PDF document (not spreadsheet/plain text/code).\n- The document should be clearly formatted as a form with a visible title and section headers.\n- Include concise instructions at the top (2\u20134 sentences) mentioning approximate time, optional demographics, and privacy/anonymous note or reassurance.\n\nRequired form components and sections (be flexible with exact header names; synonyms acceptable):\nA. Top of form\n  - Form Title that includes \u201cClass Evaluation,\u201d \u201cCourse Evaluation,\u201d or \u201cStudent Evaluation.\u201d\n  - Brief Instructions block (2\u20134 sentences) including: time to complete, optional nature of demographic items, privacy/anonymous reassurance.\nB. Class & Instructor Details\n  - Fields for: Class Title/Name, Date, Time, Instructor Name; also allow Location/Studio as a field.\nC. Student Information\n  - Fields for participant name (or anonymous option), email and/or phone, and an explicit newsletter/updates opt-in checkbox.\nD. Demographics (Optional)\n  - Clearly labeled as optional. Include items such as: age range, ZIP code/neighborhood, occupation/role or experience level with art mediums. Must include a \u201cPrefer not to say\u201d option.\nE. Class Feedback (Core section)\n  - A Likert scale matrix (1\u20135 or equivalent) with at least 6 items covering: overall satisfaction, clarity of objectives, pacing, materials/space, value for money (or pricing), and facility/environment.\n  - A visible Likert legend (endpoint labels like \u201c1 = Strongly Disagree \u2026 5 = Strongly Agree\u201d or comparable wording).\n  - At least two open-ended prompts (e.g., What worked well? What could improve?).\nF. Instructor Evaluation\n  - Likert items on knowledge, clarity/communication, approachability, and feedback/helpfulness; plus an optional comment prompt.\nG. Future Interests\n  - Multi-select topics/skills or future classes; preferred days/times; and a willingness-to-recommend item (e.g., NPS 0\u201310 or equivalent likelihood to recommend question).\nH. Marketing/Discovery\n  - \u201cHow did you hear about us?\u201d with multi-select choices such as Instagram, Facebook, Email Newsletter, Search, Friend/Word-of-mouth, Walk-in, Other (with write-in).\nI. Testimonials & Permissions\n  - A testimonial/quote prompt and explicit permission checkboxes for publishing testimonial/name (and optional photo release).\nJ. Footer/Submission\n  - A short thank-you and submission instructions or contact info.\n\nImplementation readiness cues:\n- Each question should be labeled with an implementation-friendly question type tag, e.g., [Single-select], [Multiple-select], [Likert 1\u20135], [Short Text], [Long Text], [Checkbox], [Date], [Email], [Phone], [NPS 0\u201310].\n- Section headers clearly separate related items; overall layout appears easily portable to Google Forms or similar tools.\n\nScoring (0.0 to 8.0):\n- 0.6: Valid DOCX/PDF + clear title and instructions present\n- 0.7: Class & Instructor Details fields\n- 0.6: Student Information fields incl. opt-in checkbox\n- 0.5: Demographics section labeled optional with a \u201cPrefer not to say\u201d option\n- 1.6: Class Feedback with \u22656 Likert items, visible legend, and \u22652 open-ended prompts\n- 0.6: Instructor Evaluation Likert items + optional comment\n- 0.4: Future Interests (topics/schedule + recommend/NPS)\n- 0.5: Marketing/Discovery (how-heard with multi-select and Other)\n- 0.6: Testimonials & Permissions with explicit publishing consent (and optional photo release)\n- 0.4: Footer/Submission (thank-you + how to submit/contact)\n- 1.5: Implementation readiness (question-type tags visible throughout; sections clearly delineated)\n\nAward partial credit if most elements of a bullet are present. If the file is not a DOCX/PDF, or is missing multiple core sections (e.g., Class Feedback, Instructor Evaluation, Testimonials/Permissions), score near 0. Do not judge writing quality or correctness\u2014only structure/presence.", "expectation": "A well-structured class evaluation form (DOCX/PDF) with all required sections, clear headers, question-type tags, visible Likert legend, and implementation-ready organization."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Code + LLM)", "description": "Now that the structure exists, verify internal consistency and implementation readiness via programmatic checks and a focused LLM consistency review.", "is_required": false, "max_points": 7.5, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Section Keywords Programmatic Check", "description": "Programmatically confirm presence of key section signals in extracted text: Student Info, Demographics, Class Feedback, Instructor Evaluation, Future Interests, Marketing/Discovery, Testimonials/Permissions, Thank You/Footer.", "weight": 2.0, "code": "import re\\n\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0, 'No document output detected.'\\n\\n    text = ''\\n    # Try DOCX then PDF\\n    try:\\n        text = context.files.read_docx_text(output.id) or ''\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id) or ''\\n        except Exception:\\n            return 0.0, 'Unable to read DOCX or PDF text.'\\n\\n    t = text.lower()\\n\\n    # Define section keyword groups (flexible synonyms)\\n    groups = {\\n        'student_info': ['student information', 'participant information', 'contact information', 'your information', 'about you'],\\n        'demographics': ['demographics', 'demographic', 'age range', 'zip code', 'neighborhood'],\\n        'class_feedback': ['class feedback', 'course feedback', 'overall satisfaction', 'class experience'],\\n        'instructor_eval': ['instructor evaluation', 'instructor feedback', 'teacher evaluation', 'facilitator feedback'],\\n        'future_interests': ['future classes', 'future interests', 'what would you like to learn', 'interests'],\\n        'marketing': ['how did you hear', 'discovery', 'marketing', 'word-of-mouth', 'referral'],\\n        'testimonials_permissions': ['testimonial', 'quote', 'permission', 'consent', 'release'],\\n        'thank_you_footer': ['thank you', 'thanks', 'how to submit', 'submission instructions', 'contact us']\\n    }\\n\\n    found = {}\\n    hits = 0\\n    for key, kws in groups.items():\\n        present = any(kw in t for kw in kws)\\n        found[key] = present\\n        if present:\\n            hits += 1\\n\\n    coverage = hits / len(groups)\\n    feedback = f\"Section coverage: {hits}/{len(groups)} present. Missing: \" + \", \".join([k for k,v in found.items() if not v])\\n\\n    return coverage, feedback"}, {"type": "code", "name": "Likert Legend and Item Volume", "description": "Check that a Likert legend is present and that there are enough Likert-tagged items to meaningfully assess the class and instructor.", "weight": 1.5, "code": "import re\\n\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0, 'No document output detected.'\\n\\n    text = ''\\n    try:\\n        text = context.files.read_docx_text(output.id) or ''\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id) or ''\\n        except Exception:\\n            return 0.0, 'Unable to read DOCX or PDF text.'\\n\\n    t = text.lower()\\n\\n    # Legend signals\\n    legend_signals = [\\n        'strongly disagree', 'strongly agree',\\n        '1 =', '5 =', '1\u20135', '1-5', 'likert 1-5', 'likert 1\u20135'\\n    ]\\n    has_legend = any(sig in t for sig in legend_signals)\\n\\n    # Count Likert items by tags or typical patterns\\n    likert_tag_count = len(re.findall(r\"\\\\[likert[\\\\s\\n_-]*1-?5\\\\]\", t))\\n    # Fallback heuristics: count lines mentioning agreement scale items\\n    agree_terms = ['agree', 'disagree', 'neither', 'neutral']\\n    agree_hits = sum(t.count(term) for term in agree_terms)\\n    est_items = likert_tag_count if likert_tag_count > 0 else (agree_hits // 3)\\n\\n    # Score: 0.5 for legend, up to 1.0 for having >= 8 items (scaled)\\n    score = 0.0\\n    if has_legend:\\n        score += 0.5\\n    score += min(est_items / 8.0, 1.0) * 1.0\\n\\n    # Normalize to 0..1 for framework (weight handled externally)\\n    score = min(max(score / 1.5, 0.0), 1.0)\\n    fb = f\"Legend: {'yes' if has_legend else 'no'}; Likert items estimated: {est_items}.\"\\n    return score, fb"}, {"type": "code", "name": "Question-Type Tags Coverage", "description": "Verify that implementation-friendly question-type tags exist and are used broadly across the document (e.g., [Single-select], [Multiple-select], [Likert 1\u20135], [Short Text], [Long Text], [Checkbox], [Date], [Email], [Phone], [NPS 0\u201310]).", "weight": 1.5, "code": "import re\\n\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0, 'No document output detected.'\\n\\n    text = ''\\n    try:\\n        text = context.files.read_docx_text(output.id) or ''\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id) or ''\\n        except Exception:\\n            return 0.0, 'Unable to read DOCX or PDF text.'\\n\\n    t = text.lower()\\n\\n    tags = [\\n        '[single-select]', '[multiple-select]', '[checkbox]', '[likert',\\n        '[short text]', '[long text]', '[date]', '[email]', '[phone]', '[nps', '[nps 0-10]'\\n    ]\\n\\n    # Count unique tag types present\\n    unique_present = set()\\n    total_instances = 0\\n    for tag in tags:\\n        cnt = t.count(tag)\\n        if cnt > 0:\\n            unique_present.add(tag)\\n            total_instances += cnt\\n\\n    uniq_score = len(unique_present) / max(len(tags), 1)  # coverage\\n    volume_score = min(total_instances / 12.0, 1.0)  # at least 12 tagged items for full credit\\n\\n    combined = 0.4 * uniq_score + 0.6 * volume_score\\n    fb = f\"Question-type tags \u2014 unique: {len(unique_present)}/{len(tags)}; total instances: {total_instances}.\"\\n    return max(min(combined, 1.0), 0.0), fb"}, {"type": "code", "name": "Optional Demographics with Privacy Controls", "description": "Check that demographics are clearly optional and include a \u201cPrefer not to say\u201d option or equivalent privacy choice.", "weight": 1.0, "code": "import re\\n\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0, 'No document output detected.'\\n\\n    text = ''\\n    try:\\n        text = context.files.read_docx_text(output.id) or ''\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id) or ''\\n        except Exception:\\n            return 0.0, 'Unable to read DOCX or PDF text.'\\n\\n    t = text.lower()\\n\\n    has_demo = ('demographic' in t) or ('demographics' in t) or ('age range' in t)\\n    has_optional_flag = 'optional' in t\\n    has_prefer_not_say = 'prefer not to say' in t or 'rather not say' in t\\n\\n    # Scoring: 1.0 if demographics present + optional flag + prefer-not-to-say; 0.5 if any two; else 0.0\\n    parts = sum([has_demo, has_optional_flag, has_prefer_not_say])\\n    score = 1.0 if parts == 3 else (0.5 if parts == 2 else 0.0)\\n    fb = f\"Demographics present: {has_demo}; optional flag: {has_optional_flag}; prefer-not-to-say: {has_prefer_not_say}.\"\\n    return score, fb"}, {"type": "code", "name": "Consent/Permissions and Opt-In", "description": "Ensure the form includes explicit permission/consent language for testimonials/publicity and a marketing/email opt-in.", "weight": 1.0, "code": "import re\\n\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0, 'No document output detected.'\\n\\n    text = ''\\n    try:\\n        text = context.files.read_docx_text(output.id) or ''\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id) or ''\\n        except Exception:\\n            return 0.0, 'Unable to read DOCX or PDF text.'\\n\\n    t = text.lower()\\n\\n    permission_terms = ['permission', 'consent', 'i consent', 'i give permission', 'release', 'photo release', 'publish my testimonial', 'use my testimonial']\\n    optin_terms = ['opt-in', 'opt in', 'subscribe', 'join the newsletter', 'email updates', 'mailing list']\\n\\n    has_permission = any(term in t for term in permission_terms)\\n    has_optin = any(term in t for term in optin_terms)\\n\\n    score = 1.0 if (has_permission and has_optin) else (0.5 if (has_permission or has_optin) else 0.0)\\n    fb = f\"Permission terms: {has_permission}; Opt-in terms: {has_optin}.\"\\n    return score, fb"}, {"type": "code", "name": "Recommend/NPS Presence", "description": "Confirm presence of a willingness-to-recommend item (e.g., NPS 0\u201310 or equivalent).", "weight": 0.5, "code": "import re\\n\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0, 'No document output detected.'\\n\\n    text = ''\\n    try:\\n        text = context.files.read_docx_text(output.id) or ''\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id) or ''\\n        except Exception:\\n            return 0.0, 'Unable to read DOCX or PDF text.'\\n\\n    t = text.lower()\\n\\n    signals = [\\n        'likelihood to recommend', 'how likely are you to recommend', 'nps', 'net promoter', '0-10', '0\u201310', 'not at all likely', 'extremely likely'\\n    ]\\n    present = any(sig in t for sig in signals)\\n    return (1.0 if present else 0.0), f\"Recommend/NPS present: {present}.\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism (LLM)", "description": "Holistic assessment of writing quality, clarity, friendliness, organization, de-duplication, and digital implementability.", "is_required": false, "max_points": 4.5, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity, Tone, Organization, and Implementability", "description": "Evaluate professional quality: friendliness and clarity of wording, lack of redundancy, visual organization, and ease of implementing in Google Forms or similar.", "weight": 4.5, "judge_prompt": "Assess the document\u2019s overall quality now that structure exists. Consider:\n- Clarity and friendliness of wording (typo-free, concise, plain language, professional yet welcoming tone).\n- Reduction of redundancy (no repetitive questions; sections don\u2019t duplicate each other).\n- Visual organization (clear headers, spacing, logical grouping, readable tables/option lists).\n- Implementability (question-type tags consistently used; Likert legend is clear; sections and items are ready to port to Google Forms with minimal editing).\n- Appropriateness to an art studio context and audience.\n\nScoring (0.0 to 4.5):\n- 4.0\u20134.5: Highly professional, friendly, typo-free, minimal redundancy, clearly laid out with precise question-type tags and obvious implementability.\n- 3.0\u20133.9: Strong overall with minor issues (a few awkward phrases or small redundancies) but still easy to implement.\n- 2.0\u20132.9: Adequate but with noticeable clarity/tone or organization issues; needs edits before digitization.\n- 1.0\u20131.9: Weak clarity/tone and/or disorganized; non-trivial effort to implement.\n- 0.0\u20130.9: Poor quality; confusing, error-prone, or largely unusable as a digital form.\n\nDo not re-judge format presence already gated by Stage 1; focus on quality and usability.", "expectation": "A polished, welcoming, and well-organized form with consistent tags and minimal redundancy, ready to paste into Google Forms."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "f3351922-dbdd-45da-85c5-e7110696bbe5", "rubric": {"category_name": "Professional Email: TSP Funds and Transition Benefits", "rationale": "This rubric enforces a self-documenting deliverable: a professional email as a PDF/DOCX with a strict, verifiable structure. Stage 1 uses an LLM gate to ensure the exact shape needed for verification (email format, subject line, and required sections). Stage 2 uses code rules for deterministic checks (subject correctness, presence of all TSP funds, coverage of transition benefits, citations, and compliance language) plus an LLM rule for factual reasonableness. Stage 3 assesses overall communication quality and suitability for a transitioning military client.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Email Format and Structural Completeness Gate", "description": "Gate: Verify the candidate produced a properly structured professional email as a PDF/DOCX with the mandated sections and headings, enabling objective verification in later stages. Only check presence/structure, not correctness of content.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Email Requirements (PDF/DOCX)", "description": "Check that the output is a professionally formatted email document (PDF or DOCX) with the exact required structure and sections. Only verify presence/format, not factual accuracy.", "weight": 4.0, "judge_prompt": "You are the Stage 1 gatekeeper. Evaluate ONLY the presence and structure of the output (not the factual accuracy). The candidate must submit a single professional email as a PDF or DOCX. Use multimodal view to inspect the rendered document.\n\nFormat Requirements (must all be present):\n- The file must be a PDF or DOCX (not plain text, not Excel).\n- It must read as an email with: a clear Subject line, greeting, body, and sign-off/signature block.\n- Subject line MUST match exactly: \"Comprehensive overview of TSP investment funds and benefits for transitioning service members\" (allow with or without a preceding \"Subject:\" label, but the text must match exactly otherwise).\n- At least 1 full page equivalent of content (roughly >250 words), professionally formatted and readable.\n\nRequired Sections in the body (flexible on exact header wording, but they must be clearly identifiable):\n1) \"Overview of TSP Investment Funds\" (or similar) that individually covers all of the following with at least 1-2 sentences each:\n   - G Fund\n   - F Fund\n   - C Fund\n   - S Fund\n   - I Fund\n   - L Funds (Lifecycle Funds)\n2) \"Benefits for transitioning service members\" (or similar) addressing uniformed services to federal civilian transition\u2014presented as a list or clear subsections with at least 5 distinct, scannable benefit points.\n3) \"Next steps\" or \"How I can help\" (or similar) with a clear call to action (e.g., offer to schedule time, invite questions).\n4) \"References\" or \"Resources\" with at least one external link (URL) to an authoritative source.\n5) \"Disclaimer\" indicating general information/not individualized financial advice (flexible phrasing is acceptable).\n6) Professional sign-off/signature block (name and role/title at minimum).\n\nScoring (structure only):\n- 4.0: Valid PDF/DOCX. Exact subject present. All 6 required section areas present and clearly delineated, each with the minimum content listed.\n- 3.0: Valid PDF/DOCX. Exact subject present. Missing exactly 1 required section area OR one fund (of G/F/C/S/I/L) not individually covered.\n- 2.0: Valid PDF/DOCX with the exact subject, but missing 2 required section areas OR missing 2+ individual fund coverages.\n- 1.0: Valid PDF/DOCX, but the structure is largely incomplete (e.g., only some sections, no clear subdivision of funds, or no call to action).\n- 0.0: Not PDF/DOCX; Subject text not exact; or email structure absent.\n\nImportant: Judge the presence and structure only. Do NOT evaluate factual correctness or writing quality here.\n", "expectation": "A properly structured email document (PDF/DOCX) with the exact subject line, complete fund-by-fund overview, transition benefits section, next steps, references, disclaimer, and a professional signature."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Coverage Verification", "description": "Now that the shape is enforced, verify content coverage and basic correctness signals using deterministic code checks and a light LLM factual reasonableness review.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Exact Subject Line Present", "description": "Checks that the exact required subject line appears in the document text (case-insensitive), with or without a preceding 'Subject:' label.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Ensure the exact subject string is present in the document (case-insensitive),\n    with or without a 'Subject:' prefix.\n    \"\"\"\n    required = 0.5\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    # Only proceed if document-like\n    if not output.is_document:\n        return 0.0\n\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    if not text:\n        return 0.0\n\n    subject = \"Comprehensive overview of TSP investment funds and benefits for transitioning service members\"\n    t = text.lower()\n    s = subject.lower()\n\n    # Match either exact subject line anywhere, or prefixed with 'Subject:'\n    found = (s in t) or (f\"subject: {s}\" in t)\n    return required if found else 0.0"}, {"type": "code", "name": "All Fund Sections Mentioned", "description": "Checks for presence of each TSP core fund (G, F, C, S, I) and L Funds/Lifecycle Funds by name, anywhere in the document. Awards partial credit based on count.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 1.0\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    if not output.is_document:\n        return 0.0\n\n    # Read text\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    if not text:\n        return 0.0\n    t = text.lower()\n\n    patterns = {\n        'g fund': r\"\\bg\\s*funds?\\b|\\bg fund\\b\",\n        'f fund': r\"\\bf\\s*funds?\\b|\\bf fund\\b\",\n        'c fund': r\"\\bc\\s*funds?\\b|\\bc fund\\b\",\n        's fund': r\"\\bs\\s*funds?\\b|\\bs fund\\b\",\n        'i fund': r\"\\bi\\s*funds?\\b|\\bi fund\\b\",\n        'l funds': r\"\\bl\\s*funds?\\b|lifecycle\\s*funds?\"\n    }\n    found = 0\n    for key, pat in patterns.items():\n        if re.search(pat, t):\n            found += 1\n    return weight * (found / len(patterns))"}, {"type": "code", "name": "Transition Benefits Coverage Breadth", "description": "Checks that the email discusses multiple distinct benefits/topics relevant to transitioning from uniformed services to federal civilian TSP participation (e.g., combining accounts, rollovers/transfers, agency matching under FERS, Roth vs. Traditional, catch-up contributions, loans, vesting, contribution limits). Partial credit based on distinct topics matched.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 1.0\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    # Read text\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    if not text:\n        return 0.0\n    t = text.lower()\n\n    topic_patterns = {\n        'combine_accounts': r\"combine|merge|consolidat(e|ion).*account|combine.*tsp\",\n        'rollover_transfer': r\"roll\\s*over|rollover|roll\\s*in|roll\\s*out|transfer\\s*(in|out)\",\n        'agency_match_fers': r\"agency\\s*match|matching\\s*contributions|fers\",\n        'roth_traditional': r\"roth|traditional\",\n        'catch_up': r\"catch[- ]?up|age\\s*50\",\n        'loans': r\"loan|general\\s*purpose\\s*loan|residential\\s*loan\",\n        'vesting': r\"vesting\",\n        'contribution_limits': r\"irs\\s*limit|contribution\\s*limit|annual\\s*addition|elective\\s*deferral\",\n        'separation_transition': r\"separate|separation|transition|leav(e|ing)\\s*service|entering\\s*civilian\"\n    }\n\n    matched_topics = 0\n    for k, pat in topic_patterns.items():\n        if re.search(pat, t):\n            matched_topics += 1\n\n    # Reward breadth. Full credit at 5+ topics; scale linearly.\n    score = min(matched_topics, 5) / 5.0\n    return weight * score"}, {"type": "code", "name": "Authoritative Sources Cited", "description": "Awards credit for citing at least one authoritative source, prioritizing tsp.gov. Partial credit for other reputable government sources (opm.gov, dfas.mil, irs.gov).", "weight": 0.3, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 0.3\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    if not text:\n        return 0.0\n    t = text.lower()\n\n    has_tsp = 'tsp.gov' in t\n    other_domains = any(d in t for d in ['opm.gov', 'dfas.mil', 'irs.gov', 'sec.gov', 'usa.gov'])\n\n    if has_tsp and other_domains:\n        return weight\n    if has_tsp:\n        return weight\n    if other_domains:\n        return weight * 0.6\n    return 0.0"}, {"type": "code", "name": "Compliance/Non-Advice Language", "description": "Checks for presence of general-information/not-financial-advice language or similar compliance-oriented disclaimer.", "weight": 0.2, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 0.2\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    if not text:\n        return 0.0\n    t = text.lower()\n\n    patterns = [\n        r\"not\\s+(intended|meant)\\s+as\\s+financial\\s+advice\",\n        r\"does\\s+not\\s+constitute\\s+(financial|investment)\\s+advice\",\n        r\"for\\s+informational\\s+purposes\\s+only\",\n        r\"consult\\s+(a|your)\\s+financial\\s+(advisor|professional)\"\n    ]\n    for pat in patterns:\n        if re.search(pat, t):\n            return weight\n    return 0.0"}, {"type": "llm_judge", "name": "Factual Reasonableness (Funds and Benefits)", "description": "Light factual check: Are the fund descriptions directionally correct (e.g., G Fund = government securities, C Fund ~ S&P 500, S Fund ~ completion index, I Fund ~ international developed, F Fund ~ U.S. aggregate bonds, L Funds = target-date mixes)? Do the transition benefits align with common TSP guidance (e.g., agency matching in civilian FERS roles, ability to consolidate/transfer, Roth vs. Traditional considerations), without glaring inaccuracies?", "weight": 1.0, "judge_prompt": "Review the email content for basic factual reasonableness (not perfection). Provide a score based on the following:\n- TSP Fund Descriptions: Each of G, F, C, S, I, and L Funds should be directionally accurate at a high level:\n  \u2022 G Fund: special-issue U.S. Treasury securities/principal safety concept.\n  \u2022 F Fund: broad U.S. investment-grade bond market (e.g., Bloomberg U.S. Aggregate) concept.\n  \u2022 C Fund: large-cap U.S. equities (S&P 500) concept.\n  \u2022 S Fund: U.S. mid/small-cap completion index concept.\n  \u2022 I Fund: international developed markets (e.g., MSCI EAFE) concept.\n  \u2022 L Funds: target-date/lifecycle mixes of the core funds with glide path.\n- Transition Benefits: Mentions are consistent with TSP norms for moving from uniformed services to civilian (e.g., eligibility for agency matching under FERS, options to consolidate/transfer accounts, Roth vs. Traditional carryover, contribution limits/catch-up, loan considerations).\n- No glaring misinformation (e.g., claiming guaranteed high returns, mislabeling fund indices, stating civilians get BRS matching, etc.).\n\nScoring:\n- 1.0: All funds and benefits are described in a directionally accurate way with no major errors.\n- 0.6: Minor omissions or mild imprecision but generally correct and safe.\n- 0.3: Several inaccuracies or confusing statements, but some parts are correct.\n- 0.0: Major factual errors or misleading claims.\n", "expectation": "High-level correctness for fund descriptions and transition benefits without material inaccuracies."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professionalism, Clarity, and Client-Centric Quality", "description": "Holistic quality assessment of the communication: tone, clarity, usefulness, and tailoring to a long-tenured military member transitioning to civilian federal service.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Writing Quality and Client Fit", "description": "Evaluate professionalism, clarity, empathy, and actionability for the intended audience. Check that the tone suits a government agency representative assisting a long-tenured military member moving to civilian service.", "weight": 2.0, "judge_prompt": "Evaluate the email on overall quality for a transitioning military-to-civilian federal employee:\n- Professional tone appropriate for a government agency representative\n- Clear, concise explanations; well-organized with scannable lists where helpful\n- Empathy and respect for long-tenured military service; supportive and encouraging\n- Concrete next steps and availability for assistance (contact, scheduling)\n- Formatting and grammar/punctuation quality (no glaring errors)\n\nScoring:\n- 2.0: Excellent professional polish, audience-appropriate, actionable and empathetic.\n- 1.4: Strong overall with minor issues.\n- 0.8: Adequate but could be clearer, more tailored, or better organized.\n- 0.3: Weak clarity/tone/organization or limited usefulness.\n- 0.0: Unprofessional or confusing.\n", "expectation": "A clear, respectful, and actionable email that reads professionally and supports the client\u2019s transition."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "bb863dd9-31c2-4f64-911a-ce11f457143b", "rubric": {"category_name": "Wholesale Trade | Technical & Scientific Products | IEHK 2017 Module Quotation (MedWholeGroup \u2192 Inter-Aid)", "rationale": "Mixed-output task: a structured Excel quotation (data + commercial terms). The rubric enforces a precise, verifiable Excel shape (Stage 1 LLM gate), then validates correctness and policy compliance with deterministic code checks (Stage 2), and finally judges presentation quality (Stage 3). The shape ensures verification is trivial, shifting the burden of proof to the agent via self-documenting artifacts.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2013 Structured Output Gate (LLM-only)", "description": "Gate that enforces exact output shape and structural completeness of the Excel quotation so subsequent code checks are trivial.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Excel Quotation Shape and Content Presence", "description": "Verify the candidate produced the required Excel file with the mandated filename and a complete, verifiable structure for the IEHK 2017 module quotation.", "weight": 4.0, "judge_prompt": "You are the Stage 1 gatekeeper. Examine the candidate's PRIMARY OUTPUT. Only assess STRUCTURE and PRESENCE, not calculation correctness.\n\nPass criteria: An Excel (.xlsx) file named exactly: \"Quotation Q6533211 - BO-757820 (Inter-Aid).xlsx\" containing a professional quotation for IEHK 2017 modules with the structure below.\n\nREQUIRED FORMAT AND STRUCTURE:\n- File type: Excel (.xlsx), not PDF/Word/plain text.\n- Filename: exactly \"Quotation Q6533211 - BO-757820 (Inter-Aid).xlsx\" (case-insensitive ok).\n- Professional layout suitable for client delivery.\n\nREQUIRED CONTENT (can be split across sheets; names can vary but must be clearly identifiable):\n1) Cover/Summary section (e.g., sheet named \"Cover\", \"Summary\", or top section of main sheet) containing ALL of:\n   - Company: MedWholeGroup (or MedWhole Group) clearly indicated\n   - Client: Inter-Aid clearly indicated\n   - Project reference number: BO-757820 present\n   - Quotation number: Q6533211 present\n   - Incoterm: EXW (Ex-Works / ex warehouse) clearly indicated\n   - Payment condition: 100% prepayment indicated\n   - Offer validity: 30 days from date of quotation indicated\n   - A helpful WHO reference link so the client can understand IEHK structure (e.g., a visible URL like who.int or iris.who.int). A functioning hyperlink OR a visible URL in text both count as present.\n\n2) Quotation table for IEHK 2017 MODULES (NOT item-by-item contents) with the following columns visible as headers (flexible wording allowed, but meaning must be clear):\n   - Item Description (module name)\n   - Article Number (SKU/Code)\n   - Quantity\n   - Unit Price (USD)\n   - Line Total (USD)\n   - Shelf Life (from internal doc)\n   - Lead Time (from internal doc)\n\n   Table content requirements (presence only, not correctness):\n   - Rows listing the requested modules at module-level, not individual kit items\n   - Includes a row with quantity 10 for the \u201cBasic\u201d module (name can vary, e.g., \"Basic Module\")\n   - Includes additional rows with quantity 1 for each other IEHK 2017 module (at least several are present; exact full set not verified here)\n   - A clear Subtotal/Total (USD) shown at the end of the table or a nearby summary box\n\nSCORING (STRUCTURE ONLY):\n- 4.0: Correct .xlsx file name exactly as specified + all required cover info present + quotation table present with the specified columns + a 10-quantity Basic module row + several (quantity=1) other module rows + visible Total.\n- 3.2\u20133.9: Minor omissions (e.g., one of cover info elements missing OR one supporting column like Shelf Life/Lead Time missing) but overall verifiable shape is present.\n- 2.0\u20133.1: Major structural gaps (e.g., missing multiple cover elements OR table missing multiple required columns OR no clear Total) but still an Excel quotation resembling the required structure.\n- 0.0\u20131.9: Not an Excel file, wrong filename, or lacks a recognizable quotation table; cannot proceed with verification.\n\nOnly judge PRESENCE and STRUCTURE. Do NOT verify calculation correctness or business logic.", "expectation": "A properly named Excel file with a cover/summary section containing all key commercial terms and identifiers, plus a clearly structured module-level quotation table with the specified columns, including a 10x Basic module line, other modules at 1x, and a visible total."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness and Policy Verification (Code + LLM-amenable checks)", "description": "Deterministic checks for filename, table structure, calculations, totals, and policy text. Uses flexible matching and tolerance-based validations.", "is_required": false, "max_points": 4.5, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Spreadsheet and Filename Compliance", "description": "Verify output is a spreadsheet and filename matches the required convention.", "weight": 0.7, "code": "import os\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No primary output.\"\n    score = 0.0\n    msgs = []\n    # Check spreadsheet type\n    if getattr(output, 'is_spreadsheet', False):\n        score += 0.3\n    else:\n        return 0.0, \"Output is not a spreadsheet.\"\n    # Filename checks\n    try:\n        path = context.files.get_path(output.id)\n        fname = os.path.basename(str(path))\n        fname_lower = fname.lower()\n        exact = fname_lower == 'quotation q6533211 - bo-757820 (inter-aid).xlsx'\n        has_tokens = ('q6533211' in fname_lower) and ('bo-757820' in fname_lower)\n        ext_ok = fname_lower.endswith('.xlsx')\n        if exact:\n            score += 0.3\n            msgs.append('Exact filename match.')\n        elif has_tokens:\n            score += 0.15\n            msgs.append('Filename contains key tokens.')\n        if ext_ok and not exact:\n            score += 0.05\n        # Cap at weight\n        score = min(score, 0.7)\n    except Exception as e:\n        msgs.append(f'Filename check error: {e}')\n    return score, '; '.join(msgs)"}, {"type": "code", "name": "Quotation Table Structure and Line Calculations", "description": "Locate the quotation sheet, verify required columns via fuzzy matching, and test line total \u2248 quantity \u00d7 unit price for most rows.", "weight": 1.6, "code": "import re\nimport pandas as pd\nimport numpy as np\n\nALIASES = {\n    'desc': ['item description','description','module','product','module description','item'],\n    'article': ['article number','article','sku','code','item number','art','product code'],\n    'qty': ['quantity','qty','q\\'ty'],\n    'unit_price': ['unit price','price','unit cost','unit (usd)','unit usd','unit price (usd)'],\n    'line_total': ['line total','total','amount','extended price','subtotal'],\n    'shelf_life': ['shelf life','shelf','expiry','expiration','exp.'],\n    'lead_time': ['lead time','lead','availability','eta','delivery time']\n}\n\nKEY_SHEET_HINTS = ['quot', 'iehk', 'module']\n\ndef _read_all_sheets(path):\n    xls = pd.ExcelFile(path)\n    dfs = {}\n    for sn in xls.sheet_names:\n        try:\n            dfs[sn] = pd.read_excel(path, sheet_name=sn)\n        except Exception:\n            try:\n                dfs[sn] = pd.read_excel(path, sheet_name=sn, header=None)\n            except Exception:\n                pass\n    return dfs\n\ndef _pick_quote_sheet(dfs):\n    names = list(dfs.keys())\n    if not names:\n        return None, None\n    # Prefer sheets with hints\n    for sn in names:\n        s = sn.lower()\n        if any(h in s for h in KEY_SHEET_HINTS):\n            return sn, dfs[sn]\n    # Fallback to first\n    return names[0], dfs[names[0]]\n\ndef _normalize_columns(df):\n    cols = []\n    for c in df.columns:\n        try:\n            s = str(c).strip().lower()\n        except Exception:\n            s = ''\n        cols.append(s)\n    return cols\n\ndef _fuzzy_find(cols, keys):\n    for i, c in enumerate(cols):\n        for k in keys:\n            if k in c:\n                return i\n    return None\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not getattr(output, 'is_spreadsheet', False):\n        return 0.0, 'No spreadsheet.'\n    path = context.files.get_path(output.id)\n    try:\n        dfs = _read_all_sheets(path)\n        if not dfs:\n            return 0.0, 'No sheets found.'\n        sn, df = _pick_quote_sheet(dfs)\n        if df is None or df.shape[1] < 2:\n            return 0.0, 'No usable quotation sheet.'\n        # Try header repair if unnamed columns\n        cols = _normalize_columns(df)\n        missing_headers = sum(1 for c in cols if c in ('unnamed: 0', 'unnamed: 1', ''))\n        if missing_headers > max(1, len(cols)//2):\n            # Try using first row as header\n            df2 = df.copy()\n            df2.columns = df2.iloc[0].astype(str)\n            df2 = df2.iloc[1:].reset_index(drop=True)\n            df = df2\n            cols = _normalize_columns(df)\n        # Fuzzy map columns\n        idx = {}\n        for k, aliases in ALIASES.items():\n            i = _fuzzy_find(cols, aliases)\n            idx[k] = i\n        score = 0.0\n        feedback_parts = [f\"Sheet: {sn}\"]\n        # Structure scoring\n        core_ok = sum(1 for k in ['desc','article','qty','unit_price'] if idx.get(k) is not None)\n        if core_ok >= 4:\n            score += 0.6\n        elif core_ok >= 3:\n            score += 0.4\n        elif core_ok >= 2:\n            score += 0.2\n        if idx.get('line_total') is not None:\n            score += 0.2\n        if idx.get('shelf_life') is not None:\n            score += 0.2\n        if idx.get('lead_time') is not None:\n            score += 0.2\n        # Calculation consistency\n        try:\n            q = pd.to_numeric(df.iloc[:, idx['qty']]) if idx.get('qty') is not None else pd.Series([], dtype=float)\n            up = pd.to_numeric(df.iloc[:, idx['unit_price']]) if idx.get('unit_price') is not None else pd.Series([], dtype=float)\n            lt = pd.to_numeric(df.iloc[:, idx['line_total']]) if idx.get('line_total') is not None else pd.Series([], dtype=float)\n            n = min(len(q), len(up), len(lt))\n            if n > 0:\n                q = q.astype(float).values[:n]\n                up = up.astype(float).values[:n]\n                lt = lt.astype(float).values[:n]\n                calc = q * up\n                with np.errstate(divide='ignore', invalid='ignore'):\n                    rel_err = np.where(np.abs(calc) > 1e-6, np.abs(lt - calc) / (np.abs(calc) + 1e-9), np.abs(lt - calc))\n                ok = np.isfinite(rel_err) & ((rel_err <= 0.02) | (np.abs(lt - calc) <= 0.02))\n                frac_ok = ok.mean() if n > 0 else 0.0\n                # up to 0.4 points for calc correctness\n                score += 0.4 * float(frac_ok)\n                feedback_parts.append(f\"Line calc OK fraction: {frac_ok:.2f}\")\n        except Exception as e:\n            feedback_parts.append(f\"Calc check error: {e}\")\n        return min(score, 1.6), '; '.join(feedback_parts)\n    except Exception as e:\n        return 0.0, f'Error reading spreadsheet: {e}'"}, {"type": "code", "name": "Grand Total Consistency", "description": "Find a Grand Total/Total and compare to sum of line totals within tolerance.", "weight": 0.8, "code": "import re\nimport pandas as pd\nimport numpy as np\n\nTOTAL_KEYS = ['grand total', 'total', 'subtotal']\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not getattr(output, 'is_spreadsheet', False):\n        return 0.0\n    path = context.files.get_path(output.id)\n    try:\n        xls = pd.ExcelFile(path)\n        best_total = None\n        best_sum = None\n        found_label = False\n        for sn in xls.sheet_names:\n            try:\n                df = pd.read_excel(path, sheet_name=sn)\n            except Exception:\n                continue\n            cols_lower = [str(c).lower() for c in df.columns]\n            # Identify totals by scanning last 10 rows for labels\n            tail = df.tail(12)\n            # Attempt to find line total column\n            lt_col = None\n            for i, c in enumerate(cols_lower):\n                if any(k in c for k in ['line total', 'total', 'amount', 'subtotal']):\n                    lt_col = i\n                    break\n            # Sum of line totals if available\n            line_sum = None\n            if lt_col is not None:\n                try:\n                    lt_vals = pd.to_numeric(df.iloc[:, lt_col], errors='coerce')\n                    line_sum = float(lt_vals.dropna().sum())\n                except Exception:\n                    line_sum = None\n            # Find a labeled total cell\n            total_val = None\n            for r in range(tail.shape[0]):\n                row = tail.iloc[r]\n                for c in range(min(3, len(row))):\n                    cell = str(row.iloc[c]).lower()\n                    if any(k in cell for k in TOTAL_KEYS):\n                        # pick adjacent numeric in same row\n                        for c2 in range(len(row)):\n                            if c2 == c:\n                                continue\n                            try:\n                                val = float(pd.to_numeric(row.iloc[c2]))\n                                total_val = val\n                                found_label = True\n                                break\n                            except Exception:\n                                continue\n                        if total_val is not None:\n                            break\n                if total_val is not None:\n                    break\n            if total_val is not None and line_sum is not None:\n                best_total = total_val\n                best_sum = line_sum\n                break\n        if best_total is None or best_sum is None:\n            # Partial credit if a labeled total was found anywhere\n            return (0.3 if found_label else 0.0), 'No comparable total and line sum found.'\n        # Compare within 2% or $1\n        diff = abs(best_total - best_sum)\n        tol = max(1.0, 0.02 * max(abs(best_sum), 1.0))\n        ok = diff <= tol\n        return (0.8 if ok else 0.4), f'Total={best_total:.2f}, SumLines={best_sum:.2f}, Diff={diff:.2f}, Tol={tol:.2f}'\n    except Exception as e:\n        return 0.0, f'Error: {e}'"}, {"type": "code", "name": "Policy and Reference Text Presence", "description": "Verify presence of key commercial terms and references across all sheets: 100% prepayment, 30-day validity, EXW, project and quotation numbers, and WHO link.", "weight": 1.0, "code": "import re\nimport pandas as pd\n\nTERMS = [\n    ('prepayment', [r'100\\%\\s*prepayment', r'prepayment\\s*100\\%', r'100\\%\\s*advance', r'advance\\s*payment\\s*100\\%']),\n    ('validity_30_days', [r'valid(?:ity)?[^\\n]{0,40}30\\s*day', r'30\\s*day[^\\n]{0,40}valid']),\n    ('incoterm_exw', [r'\\bexw\\b', r'ex[- ]?works', r'ex\\s*warehouse']),\n    ('project_ref', [r'bo-757820']),\n    ('quote_no', [r'q6533211']),\n    ('who_link', [r'who\\.int', r'iris\\.who\\.int'])\n]\n\ndef _sheet_texts(path):\n    xls = pd.ExcelFile(path)\n    texts = []\n    for sn in xls.sheet_names:\n        try:\n            df = pd.read_excel(path, sheet_name=sn, header=None, dtype=str)\n            s = ' '.join(df.fillna('').astype(str).values.ravel().tolist()).lower()\n            texts.append(s)\n        except Exception:\n            pass\n    return ' \\n '.join(texts)\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not getattr(output, 'is_spreadsheet', False):\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        blob = _sheet_texts(path)\n        points = 0.0\n        found = []\n        for name, patterns in TERMS:\n            hit = any(re.search(p, blob, flags=re.IGNORECASE) for p in patterns)\n            if hit:\n                found.append(name)\n                points += 1.0/6.0\n        # Cap at weight\n        points = min(points, 1.0)\n        return points, 'Found: ' + ', '.join(found)\n    except Exception as e:\n        return 0.0, f'Error scanning text: {e}'"}, {"type": "code", "name": "Module Quantities Sanity", "description": "Check that a Basic module row has quantity >=10 and that several other modules are listed with quantity 1.", "weight": 0.4, "code": "import re\nimport pandas as pd\nimport numpy as np\n\nHINTS = ['quot', 'iehk', 'module']\n\ndef _pick_sheet(path):\n    xls = pd.ExcelFile(path)\n    for sn in xls.sheet_names:\n        s = sn.lower()\n        if any(h in s for h in HINTS):\n            return sn\n    return xls.sheet_names[0]\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not getattr(output, 'is_spreadsheet', False):\n        return 0.0\n    path = context.files.get_path(output.id)\n    try:\n        sn = _pick_sheet(path)\n        df = pd.read_excel(path, sheet_name=sn)\n        cols = [str(c).lower() for c in df.columns]\n        # Find probable description and qty columns\n        desc_idx = None\n        qty_idx = None\n        for i, c in enumerate(cols):\n            if any(k in c for k in ['description','module','item','product']):\n                desc_idx = i if desc_idx is None else desc_idx\n            if any(k in c for k in ['quantity','qty','q\\'ty']):\n                qty_idx = i if qty_idx is None else qty_idx\n        if desc_idx is None or qty_idx is None:\n            return 0.0, 'Missing description/quantity columns.'\n        desc = df.iloc[:, desc_idx].astype(str).str.lower()\n        qty = pd.to_numeric(df.iloc[:, qty_idx], errors='coerce')\n        qty1_count = int((qty == 1).sum())\n        basic_ok = bool(((desc.str.contains('basic')) & (qty >= 10)).any())\n        score = 0.0\n        if basic_ok:\n            score += 0.3\n        if qty1_count >= 3:\n            score += 0.1\n        return min(score, 0.4), f'basic_ok={basic_ok}, qty1_rows={qty1_count}'\n    except Exception as e:\n        return 0.0, f'Error: {e}'"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Presentation and Client Fit (LLM)", "description": "Holistic quality assessment of formatting, clarity, and client-appropriateness. Does not re-check structure or math.", "is_required": false, "max_points": 1.5, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Readability", "description": "Assess if the Excel quotation looks professional, is easy to read, and is client-ready.", "weight": 1.0, "judge_prompt": "Review the Excel quotation for professional presentation. Consider: clear branding/headers, readable table formatting (aligned columns, consistent currency formatting USD, thousand separators/decimals), visible totals and any summary, and overall client-ready appearance. Do not judge correctness of numbers. Scoring: 1.0 = highly professional and clean; 0.6 = acceptable with minor formatting issues; 0.3 = cluttered or inconsistent but readable; 0.0 = poor/ confusing presentation.", "expectation": "A clean, well-formatted, client-ready quotation with consistent currency formatting and clear totals."}, {"type": "llm_judge", "name": "Client-Centered Helpfulness", "description": "Evaluate how helpful the quotation is for Inter-Aid: clarity of module-level descriptions, inclusion of WHO reference link, and concise commercial notes about EXW, validity, payment.", "weight": 0.5, "judge_prompt": "Evaluate helpfulness for the NGO client: Are module names clear and unambiguous? Is a WHO reference link included so they can understand IEHK structure? Are EXW incoterms, 30-day validity, and 100% prepayment stated in a concise, client-friendly manner? Scoring: 0.5 = excellent clarity and all elements; 0.3 = generally helpful with small gaps; 0.1 = limited helpfulness; 0.0 = missing or confusing.", "expectation": "Concise, client-friendly notes and a visible WHO link to IEHK guidance, with clear module naming."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "e996036e-8287-4e7f-8d0a-90a57cb53c45", "rubric": {"category_name": "Wholesale Trade \u2014 First-Line Supervisors of Non-Retail Sales Workers: Terms Scenario Plan (Cosmetics Retail Account)", "rationale": "Pattern C (Mixed): The deliverable is an Excel financial scenario model (analytical) that must also include a written executive summary and a visual inside the workbook (document/communication). Stage 1 enforces an exact, LLM-checked workbook structure so verification is trivial. Stage 2 uses code to validate arithmetic, bounds, and timing logic now that structure is known. Stage 3 judges overall quality and strategic fitness for executive consumption.", "max_total_score": 11.0, "stages": [{"name": "Stage 1 \u2014 Structured Excel Gate", "description": "LLM-only gate to enforce exact workbook structure enabling verification.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Excel Scenario Plan Requirement", "description": "Output must be a single Excel workbook with required sheets/sections, tables, embedded chart, and 5\u20136 sentence summary inside the workbook.", "weight": 4.0, "judge_prompt": "You are checking ONLY the presence/structure of the candidate output. Do not check calculation correctness.\n\nConfirm the output is a valid Excel workbook (.xlsx) and contains the following structure (allow minor naming variations, e.g., 'Summary' vs 'Executive Summary', 'Scenario Comparison' vs 'Scenarios'):\n\nRequired Sheets and Sections\n1) Assumptions & Inputs (or similar)\n   - Section: Base Assumptions (text bullets or small table) explicitly stating:\n     \u2022 MSRP is followed\n     \u2022 Retailer assumes markdown responsibility\n   - Section: Year 1 Quarterly Projections with a visible table having rows for Q1, Q2, Q3, Q4 and columns for both:\n     \u2022 Projected Retail Sales by Quarter (total should be $200,000)\n     \u2022 Projected Shipments at Retail Value by Quarter (total should be $225,000)\n   - Totals for both series must be visible in the sheet.\n\n2) Scenario Comparison (or similar)\n   - A table comparing at least THREE distinct scenarios (A/B/C or 1/2/3) with these columns:\n     \u2022 Scenario Name\n     \u2022 Retailer Margin % (between 40% and 50%)\n     \u2022 Payment Terms (Net 30 or Net 60)\n     \u2022 Marketing Allowance Rate % (up to 4%)\n     \u2022 Shipments (Retail Value) Total\n     \u2022 Wholesale Revenue Total\n     \u2022 Marketing Allowance Total\n     \u2022 Net Wholesale Revenue Total\n   - One row per scenario.\n\n3) Cash Flow Timing (or similar)\n   - A quarterly schedule that ties to each scenario (wide or long format acceptable). Must include columns:\n     \u2022 Scenario\n     \u2022 Quarter (Q1\u2013Q4)\n     \u2022 Shipments (Retail Value)\n     \u2022 Retailer Margin % or Wholesale Revenue\n     \u2022 Payment Terms\n     \u2022 Cash Receipts (by quarter)\n     \u2022 Marketing Allowance booked (by quarter)\n\n4) Summary & Visualization (or similar)\n   - A chart/visual inside the workbook that compares scenario favorability (e.g., bar/column chart of Net Wholesale Revenue by scenario or similar).\n   - A written executive summary paragraph INSIDE the workbook (in a cell range or text box) of 5\u20136 sentences that:\n     \u2022 Selects the most favorable scenario\n     \u2022 Explains balance of profitability, company objectives (brand awareness/retailer-led activations), and retailer concerns (cash flow)\n     \u2022 Notes any considerations or compromises\n\nScoring (structure only):\n- 4.0: Valid .xlsx AND all 4 sheets/sections present with required tables, visible quarterly breakdowns, 3 scenarios table with required columns, cash flow schedule, embedded chart, and 5\u20136 sentence summary in-workbook.\n- 3.2: All required sheets/sections present but one minor element missing (e.g., totals not visibly shown, or the cash flow schedule lacks one supporting column but still clearly shows timing by quarter).\n- 2.4: Missing one required sheet/section OR scenario table has fewer than 3 scenarios.\n- 1.2: Workbook present but structure largely incomplete (e.g., no quarterly table, no scenario comparison table).\n- 0.0: Not an Excel file or no recognizable required structure.\n\nOnly check PRESENCE/FORMAT, not correctness.", "expectation": "A single .xlsx workbook with the specified sheets/sections, three-scenario comparison table, quarterly cash flow schedule, an embedded chart, and a 5\u20136 sentence executive summary paragraph inside the workbook."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness Verification (Code + LLM)", "description": "Deterministic checks for math, bounds, timing consistency, and summary content completeness. Flexible matching is used because Stage 1 enforced the shape.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Quarterly Totals Match Reference", "description": "Find quarterly tables and verify totals: Retail Sales sum \u2248 200,000 and Shipments (Retail Value) sum \u2248 225,000 across Q1\u2013Q4.", "weight": 0.6, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n    except Exception as e:\n        return 0.0, f\"Failed to open Excel: {e}\"\n\n    target_sales = 200000\n    target_ship = 225000\n    tol = 0.01  # 1%\n\n    def norm(s):\n        if pd.isna(s):\n            return \"\"\n        return str(s).strip().lower()\n\n    def to_num(x):\n        if pd.isna(x):\n            return np.nan\n        s = str(x).replace(\",\", \"\").replace(\"$\", \"\").strip()\n        try:\n            return float(s)\n        except:\n            return np.nan\n\n    found_sales = False\n    found_ship = False\n    notes = []\n\n    for sheet in xls.sheet_names:\n        try:\n            df = pd.read_excel(path, sheet_name=sheet, header=0)\n        except:\n            continue\n        # Normalize column names\n        df_cols = [norm(c) for c in df.columns]\n        # Look for quarter labels in first column, or anywhere\n        q_labels = {\"q1\",\"q2\",\"q3\",\"q4\"}\n        # Try to locate a column with quarters\n        quarter_col_idx = None\n        for i, c in enumerate(df_cols):\n            col_vals = set(norm(v) for v in df.iloc[:, i].dropna().astype(str).str.lower().str.strip().values[:50])\n            if len(q_labels.intersection(col_vals)) >= 2:\n                quarter_col_idx = i\n                break\n        if quarter_col_idx is None:\n            continue\n        # Once found, try to identify numeric columns for retail sales and shipments\n        for j, c in enumerate(df_cols):\n            if j == quarter_col_idx:\n                continue\n            col_name = df_cols[j]\n            if any(k in col_name for k in [\"retail sales\", \"sales (retail)\", \"projected retail\", \"sales\"]):\n                vals = pd.to_numeric(df.iloc[:, j].apply(to_num), errors='coerce')\n                total = vals.sum(skipna=True)\n                if np.isfinite(total) and abs(total - target_sales) <= tol * target_sales:\n                    found_sales = True\n            if any(k in col_name for k in [\"shipments\", \"shipments (retail)\", \"shipments retail value\", \"retail shipments\"]):\n                vals = pd.to_numeric(df.iloc[:, j].apply(to_num), errors='coerce')\n                total = vals.sum(skipna=True)\n                if np.isfinite(total) and abs(total - target_ship) <= tol * target_ship:\n                    found_ship = True\n    score = 0.0\n    if found_sales and found_ship:\n        score = 0.6\n        msg = \"Found both quarterly totals matching references.\"\n    elif found_sales or found_ship:\n        score = 0.3\n        which = \"sales\" if found_sales else \"shipments\"\n        msg = f\"Found matching quarterly total for {which} only.\"\n    else:\n        msg = \"No matching quarterly totals found.\"\n    return score, msg"}, {"type": "code", "name": "Wholesale Revenue Math Consistency", "description": "For each scenario in the comparison table, check Wholesale Revenue \u2248 Shipments*(1 - Margin%). Pass if at least 2 scenarios match within 2%.", "weight": 1.2, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n    except Exception as e:\n        return 0.0, f\"Failed to open Excel: {e}\"\n\n    def norm(s):\n        if pd.isna(s):\n            return \"\"\n        return str(s).strip().lower()\n\n    def to_num(x):\n        if pd.isna(x):\n            return np.nan\n        s = str(x).strip()\n        s = s.replace(\"$\", \"\").replace(\",\", \"\")\n        try:\n            return float(s)\n        except:\n            return np.nan\n\n    def to_rate(x):\n        if pd.isna(x):\n            return np.nan\n        s = str(x).strip().replace(\"%\",\"\")\n        s = s.replace(\",\",\"\")\n        try:\n            v = float(s)\n            if v > 1.0:\n                v = v / 100.0\n            return v\n        except:\n            return np.nan\n\n    scenario_df = None\n    for sheet in xls.sheet_names:\n        try:\n            df = pd.read_excel(path, sheet_name=sheet)\n        except:\n            continue\n        cols = [norm(c) for c in df.columns]\n        if any(\"scenario\" in c for c in cols) and sum(any(k in c for k in [\"margin\", \"payment\", \"allowance\", \"shipments\", \"wholesale\", \"net\"]) for c in cols) >= 3:\n            scenario_df = df.copy()\n            scenario_df.columns = cols\n            break\n    if scenario_df is None:\n        return 0.0, \"Scenario comparison table not found\"\n\n    # Map likely columns\n    def find_col(keys):\n        for c in scenario_df.columns:\n            if any(k in c for k in keys):\n                return c\n        return None\n\n    col_ship = find_col([\"shipments\", \"shipments retail value\", \"retail shipments\"])\n    col_margin = find_col([\"margin\"])  # retailer margin %\n    col_wh = find_col([\"wholesale revenue\", \"wholesale total\", \"gross wholesale\"])\n\n    if not col_ship or not col_margin or not col_wh:\n        return 0.0, \"Missing required columns (shipments/margin/wholesale) in scenario table\"\n\n    tol = 0.02\n    matches = 0\n    checked = 0\n    details = []\n    for idx, row in scenario_df.iterrows():\n        ship = to_num(row[col_ship])\n        margin = to_rate(row[col_margin])\n        wh = to_num(row[col_wh])\n        if not np.isfinite(ship) or not np.isfinite(margin) or not np.isfinite(wh):\n            continue\n        exp_wh = ship * (1 - margin)\n        checked += 1\n        if exp_wh == 0:\n            continue\n        rel_err = abs(wh - exp_wh) / max(abs(exp_wh), 1e-9)\n        details.append(f\"row {idx}: wh={wh:.2f}, exp={exp_wh:.2f}, err={rel_err:.3f}\")\n        if rel_err <= tol:\n            matches += 1\n    if checked == 0:\n        return 0.0, \"No valid numeric scenario rows to check\"\n    if matches >= 2:\n        return 1.2, f\"{matches}/{checked} scenarios match wholesale math within 2%.\"\n    elif matches >= 1:\n        return 0.6, f\"{matches}/{checked} scenarios match wholesale math within 2%.\"\n    else:\n        return 0.0, f\"No scenarios match wholesale math within 2%. Details: {'; '.join(details[:5])}\""}, {"type": "code", "name": "Marketing Allowance Cap and Math", "description": "Check each scenario\u2019s Marketing Allowance Rate \u2264 4% and Allowance Amount \u2248 Shipments*Rate within 2%.", "weight": 0.8, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n    except Exception as e:\n        return 0.0, f\"Failed to open Excel: {e}\"\n\n    def norm(s):\n        if pd.isna(s):\n            return \"\"\n        return str(s).strip().lower()\n\n    def to_num(x):\n        if pd.isna(x):\n            return np.nan\n        s = str(x).strip().replace(\"$\",\"\" ).replace(\",\", \"\")\n        try:\n            return float(s)\n        except:\n            return np.nan\n\n    def to_rate(x):\n        if pd.isna(x):\n            return np.nan\n        s = str(x).strip().replace(\"%\",\"\")\n        s = s.replace(\",\",\"\")\n        try:\n            v = float(s)\n            if v > 1.0:\n                v = v / 100.0\n            return v\n        except:\n            return np.nan\n\n    scenario_df = None\n    for sheet in xls.sheet_names:\n        try:\n            df = pd.read_excel(path, sheet_name=sheet)\n        except:\n            continue\n        cols = [norm(c) for c in df.columns]\n        if any(\"scenario\" in c for c in cols) and sum(any(k in c for k in [\"allowance\", \"shipments\", \"rate\", \"retail value\"]) for c in cols) >= 2:\n            scenario_df = df.copy()\n            scenario_df.columns = cols\n            break\n    if scenario_df is None:\n        return 0.0, \"Scenario table not found\"\n\n    col_ship = next((c for c in scenario_df.columns if \"shipment\" in c), None)\n    col_rate = next((c for c in scenario_df.columns if \"allowance\" in c and \"%\" in c) or (c for c in scenario_df.columns if \"allowance rate\" in c), None)\n    if col_rate is None:\n        # fallback: any column with 'rate' and 'allowance' keywords\n        for c in scenario_df.columns:\n            if \"allowance\" in c and (\"rate\" in c or \"%\" in c):\n                col_rate = c\n                break\n    col_amt = next((c for c in scenario_df.columns if \"allowance\" in c and any(k in c for k in [\"total\",\"amount\"]) and \"%\" not in c), None)\n\n    if not col_ship or not col_rate or not col_amt:\n        return 0.0, \"Missing shipments/allowance rate/allowance amount columns\"\n\n    tol = 0.02\n    good = 0\n    checked = 0\n    cap_ok = True\n    for _, row in scenario_df.iterrows():\n        ship = to_num(row[col_ship])\n        rate = to_rate(row[col_rate])\n        amt = to_num(row[col_amt])\n        if not np.isfinite(ship) or not np.isfinite(rate) or not np.isfinite(amt):\n            continue\n        checked += 1\n        if rate > 0.04 + 1e-6:\n            cap_ok = False\n        exp_amt = ship * rate\n        if exp_amt == 0:\n            continue\n        rel_err = abs(amt - exp_amt) / max(abs(exp_amt), 1e-9)\n        if rel_err <= tol:\n            good += 1\n    if checked == 0:\n        return 0.0, \"No valid scenario rows for allowance math\"\n    score = 0.0\n    if good >= 2:\n        score += 0.6\n    elif good == 1:\n        score += 0.3\n    if cap_ok:\n        score += 0.2\n    return score, f\"Allowance math OK in {good}/{checked} scenarios; cap_ok={cap_ok}\""}, {"type": "code", "name": "Net Wholesale Revenue Calculation", "description": "Verify Net Wholesale Revenue \u2248 Wholesale Revenue \u2212 Marketing Allowance within 2% for scenarios.", "weight": 0.6, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n    except Exception as e:\n        return 0.0, f\"Failed to open Excel: {e}\"\n\n    def norm(s):\n        if pd.isna(s):\n            return \"\"\n        return str(s).strip().lower()\n\n    def to_num(x):\n        if pd.isna(x):\n            return np.nan\n        s = str(x).strip().replace(\"$\",\"\" ).replace(\",\", \"\")\n        try:\n            return float(s)\n        except:\n            return np.nan\n\n    scenario_df = None\n    for sheet in xls.sheet_names:\n        try:\n            df = pd.read_excel(path, sheet_name=sheet)\n        except:\n            continue\n        cols = [norm(c) for c in df.columns]\n        if any(\"scenario\" in c for c in cols) and sum(any(k in c for k in [\"wholesale\", \"net\", \"allowance\"]) for c in cols) >= 2:\n            scenario_df = df.copy()\n            scenario_df.columns = cols\n            break\n    if scenario_df is None:\n        return 0.0, \"Scenario table not found\"\n\n    col_wh = next((c for c in scenario_df.columns if \"wholesale\" in c and \"net\" not in c), None)\n    col_allow = next((c for c in scenario_df.columns if \"allowance\" in c and \"%\" not in c), None)\n    col_net = next((c for c in scenario_df.columns if \"net\" in c and \"wholesale\" in c), None)\n    if not col_wh or not col_allow or not col_net:\n        return 0.0, \"Missing wholesale/allowance/net columns\"\n\n    tol = 0.02\n    good = 0\n    checked = 0\n    for _, row in scenario_df.iterrows():\n        wh = to_num(row[col_wh])\n        al = to_num(row[col_allow])\n        net = to_num(row[col_net])\n        if not np.isfinite(wh) or not np.isfinite(al) or not np.isfinite(net):\n            continue\n        checked += 1\n        exp_net = wh - al\n        if exp_net == 0:\n            continue\n        rel_err = abs(net - exp_net) / max(abs(exp_net), 1e-9)\n        if rel_err <= tol:\n            good += 1\n    if checked == 0:\n        return 0.0, \"No valid rows to check\"\n    if good >= 2:\n        return 0.6, f\"Net wholesale matches in {good}/{checked} scenarios\"\n    elif good == 1:\n        return 0.3, f\"Net wholesale matches in {good}/{checked} scenarios\"\n    else:\n        return 0.0, \"Net wholesale mismatch across scenarios\""}, {"type": "code", "name": "Cash Flow Timing Lag Check (Net 60)", "description": "Using the cash flow schedule, confirm that for a Net 60 scenario, Q1 cash receipts are materially less than Q1 wholesale revenue (if Q1 shipments exist) and total receipts equal total wholesale revenue across quarters.", "weight": 0.8, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n    except Exception as e:\n        return 0.0, f\"Failed to open Excel: {e}\"\n\n    def norm(s):\n        if pd.isna(s):\n            return \"\"\n        return str(s).strip().lower()\n\n    def to_num(x):\n        if pd.isna(x):\n            return np.nan\n        s = str(x).strip().replace(\"$\",\"\" ).replace(\",\", \"\")\n        try:\n            return float(s)\n        except:\n            return np.nan\n\n    schedule_df = None\n    for sheet in xls.sheet_names:\n        try:\n            df = pd.read_excel(path, sheet_name=sheet)\n        except:\n            continue\n        cols = [norm(c) for c in df.columns]\n        if sum(any(k in c for k in [\"quarter\", \"q1\", \"q2\", \"q3\", \"q4\"]) for c in cols) >= 1 and \\\n           sum(any(k in c for k in [\"cash receipt\", \"receipts\", \"cash in\"]) for c in cols) >= 1 and \\\n           sum(any(k in c for k in [\"wholesale\", \"revenue\"]) for c in cols) >= 1 and \\\n           any(\"payment\" in c and \"term\" in c for c in cols):\n            schedule_df = df.copy()\n            schedule_df.columns = cols\n            break\n    if schedule_df is None:\n        return 0.0, \"Cash flow schedule not found\"\n\n    # Identify columns\n    col_scen = next((c for c in schedule_df.columns if \"scenario\" in c), None)\n    col_q = next((c for c in schedule_df.columns if \"quarter\" in c), None)\n    if col_q is None:\n        # Try to reconstruct quarter from a column with Q1..Q4 values\n        for c in schedule_df.columns:\n            vals = schedule_df[c].astype(str).str.lower().str.strip().values\n            if any(v in {\"q1\",\"q2\",\"q3\",\"q4\"} for v in vals):\n                col_q = c\n                break\n    col_terms = next((c for c in schedule_df.columns if \"payment\" in c and \"term\" in c), None)\n    col_wh = next((c for c in schedule_df.columns if \"wholesale\" in c and \"net\" not in c), None)\n    col_cash = next((c for c in schedule_df.columns if \"cash\" in c and (\"receipt\" in c or \"receipts\" in c or \"cash in\" in c)), None)\n\n    if not col_q or not col_terms or not col_wh or not col_cash:\n        return 0.0, \"Missing required columns in cash flow schedule\"\n\n    df = schedule_df.copy()\n    # Normalize quarter labels\n    def q_to_num(v):\n        s = str(v).strip().lower()\n        if s.startswith('q') and len(s) >= 2 and s[1].isdigit():\n            try:\n                return int(s[1])\n            except:\n                return np.nan\n        return np.nan\n\n    df['qnum'] = df[col_q].apply(q_to_num)\n    # Filter Net 60 rows\n    net60_mask = df[col_terms].astype(str).str.lower().str.contains('60')\n    df60 = df[net60_mask].copy()\n    if df60.empty:\n        return 0.2, \"No Net 60 scenario found; partial credit\"\n\n    # Aggregate by quarter\n    g = df60.groupby('qnum')\n    wh_by_q = g[col_wh].apply(lambda s: pd.to_numeric(s.apply(to_num), errors='coerce').sum())\n    cash_by_q = g[col_cash].apply(lambda s: pd.to_numeric(s.apply(to_num), errors='coerce').sum())\n\n    total_wh = pd.to_numeric(df60[col_wh].apply(to_num), errors='coerce').sum()\n    total_cash = pd.to_numeric(df60[col_cash].apply(to_num), errors='coerce').sum()\n\n    # Check totals equality within 1%\n    totals_ok = False\n    if total_wh > 0:\n        totals_ok = abs(total_cash - total_wh) <= 0.01 * total_wh\n\n    # Check lag in Q1 if shipments exist\n    q1_wh = wh_by_q.get(1, np.nan)\n    q1_cash = cash_by_q.get(1, np.nan)\n    lag_ok = False\n    if np.isfinite(q1_wh) and q1_wh > 0 and np.isfinite(q1_cash):\n        lag_ok = q1_cash <= 0.9 * q1_wh  # materially less (>=10% lower)\n\n    score = 0.0\n    if totals_ok:\n        score += 0.4\n    if lag_ok:\n        score += 0.4\n    if score == 0.0 and (np.isfinite(total_wh) and total_wh > 0 and np.isfinite(total_cash)):\n        score = 0.2  # partial if at least totals exist\n    return score, f\"Totals_ok={totals_ok}, Q1_lag_ok={lag_ok}\""}, {"type": "llm_judge", "name": "Executive Summary Content Completeness", "description": "Check the in-workbook paragraph (5\u20136 sentences) selects a scenario, explains profitability vs. objectives vs. retailer concerns, and notes tradeoffs.", "weight": 1.0, "judge_prompt": "Evaluate ONLY the written summary paragraph INSIDE the Excel workbook (ignore any external text). Confirm:\n1) It is a single coherent paragraph of 5\u20136 sentences.\n2) It clearly names or identifies the selected scenario as most favorable.\n3) It explains why it balances profitability (e.g., wholesale/net revenue), company objectives (brand awareness via retailer activations/social/live shows), and retailer concerns (cash flow risk; payment terms).\n4) It notes at least one consideration or compromise (e.g., higher margin offset by better terms/marketing cap; choosing Net 30 vs Net 60; allowance tradeoffs).\n\nScoring:\n- 1.0: All 4 criteria satisfied.\n- 0.7: Meets 3 criteria (minor omissions).\n- 0.4: Meets 2 criteria or paragraph length outside 5\u20136 but still substantive.\n- 0.2: Mentions a preferred scenario but lacks depth.\n- 0.0: No identifiable paragraph in the workbook or does not address the selection/reasoning.", "expectation": "A concise 5\u20136 sentence paragraph inside the workbook that selects a scenario and clearly justifies it across profitability, growth/awareness objectives, and retailer cash flow considerations, noting any compromises."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Communication", "description": "LLM appraisal of professional polish, clarity, and strategic usefulness for executives.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Strategic Value", "description": "Holistic quality: formatting, readability, chart clarity, executive readiness, and strategic insightfulness.", "weight": 2.0, "judge_prompt": "Assess the overall professionalism and executive readiness of the workbook:\n- Formatting and readability: clean layout, labeled tables, consistent units/currency/% formats.\n- Visualization quality: chart type appropriate, titles/labels/legends clear; it directly compares scenario favorability.\n- Clarity of scenario comparison: scenarios are easy to distinguish; totals are visible; assumptions trace to outputs.\n- Strategic value: the chosen scenario and discussion reflect nuanced understanding of profitability, brand-awareness goals, and retailer constraints (cash flow); recommendations feel actionable for leadership.\n\nScoring:\n- 2.0: Highly professional, clear, strategically strong; minimal to no ambiguities.\n- 1.2: Generally professional with minor issues; insight is solid but could be tighter.\n- 0.6: Adequate but rough edges (formatting or clarity); limited strategic framing.\n- 0.0: Poorly formatted, confusing, or lacks strategic usefulness.", "expectation": "A clear, well-formatted workbook with an effective comparative chart and an executive-ready narrative that demonstrates strategic tradeoffs and actionable guidance."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "be830ca0-b352-4658-a5bd-57139d6780ba", "rubric": {"category_name": "Manufacturing \u2014 Industrial Engineering \u2014 Analyze Tollgate Presentation (LLS Processing Rate)", "rationale": "Pattern C (Mixed): A presentation (document) that must embed analytics and verifiable statistical artifacts. Stage 1 uses an LLM gate to enforce a precise presentation structure and a required companion Excel workbook with standardized sheets. Stage 2 uses code rules to verify statistical correctness signals and date ranges from the companion workbook. Stage 3 uses an LLM judge for overall quality and professional effectiveness for leadership consumption.", "max_total_score": 30.0, "stages": [{"name": "Stage 1 \u2014 Structure Gate (Presentation + Companion Workbook)", "description": "LLM-only gate verifying the output is a PowerPoint (or PDF export) Analyze tollgate presentation with a precise slide structure AND an accompanying Excel workbook with standardized analysis sheets enabling verification. This is a hard gate: if structure is wrong or workbook missing, the entire category scores 0.", "is_required": true, "max_points": 12.0, "min_score_to_pass": 9.0, "rules": [{"type": "llm_judge", "name": "PowerPoint/PDF Structure and Companion Workbook Presence", "description": "Check if the candidate delivered a PowerPoint (.pptx) or PDF export of the Analyze tollgate presentation with the required slide set and also provided a companion Excel file named clearly (e.g., Analyze_Artifacts.xlsx) containing the mandated sheets for verification.", "weight": 12.0, "judge_prompt": "You are evaluating the SHAPE ONLY. Do not grade content quality or numerical correctness.\n\nAcceptable primary document formats: PowerPoint (.pptx) preferred, or a PDF export of the slides. In addition, there MUST be a companion Excel workbook (e.g., \"Analyze_Artifacts.xlsx\") that contains the analysis outputs in a structured, readable way (no images only). The workbook is required to enable verification in Stage 2.\n\nEvaluate if ALL of the following structural requirements are satisfied:\n\nA) Presentation File (PPTX or PDF of slides)\nRequired slides/sections (be flexible with exact titles but headers must be clear):\n1) First slide is a Project Charter that includes ALL of:\n   - Project overview\n   - Problem/purpose (clearly written)\n   - Goal with measurable target (raise average UPR from ~3189 to 3400 UPR by 04/01/2025)\n   - Rationale explaining operational and financial impact of sub-3400 UPR\n   - Scope\n   - Project schedule indicating start/end of each DMAIC phase\n2) A One-Way ANOVA interval plot evaluating UPR by day of week, with brief interpretation.\n3) An I-MR Control Chart over 01/04/2025\u201303/01/2025 with interpretation (control limits, center line/average, special causes).\n4) A Linear Regression analysis using Time of Day as a continuous predictor (mention that categorical predictors would need dummies if used), with interpretation of direction/strength and staffing implications.\n5) A 1-Sample Hypothesis Test comparing mean UPR vs the 3400 UPR target with p-value and confidence interval noted.\n6) A Process Capability Analysis (histogram with Cp and Cpk) assessing capability vs 3400 UPR (treated as LSL or target as appropriate).\n7) An A3 Summary including the required sections:\n   - Background\n   - Project purpose\n   - Current conditions (MUST include a separate I-MR chart using only 01/04/2025\u201302/21/2025 data)\n   - Goals\n   - Analysis results (should reference all five charts above)\n   - Follow-up\n8) Final slide: A project timeline with all DMAIC phase dates, indicating completed tollgates and phases in progress. Dates expected (can be shown directly or via a timeline graphic):\n   - Define start: 02/08/2025\n   - Measure start: 02/15/2025\n   - Analyze start: 02/22/2025 and ends 03/01/2025\n   - Improve: TBD\n   - Control: TBD\n9) Clear reference that all analysis is based on \"Processing Data.xlsx\" for dates 01/04/2025\u201303/01/2025.\n\nB) Companion Excel Workbook (e.g., \"Analyze_Artifacts.xlsx\")\nMust exist alongside the presentation and include readable, structured sheets (table format, not just screenshots). Flexible naming allowed, but each sheet\u2019s purpose must be unambiguous:\n   - \"ANOVA\" (or similar): includes day-of-week groups, group means, confidence intervals, and p-value.\n   - \"IMR_Full\" (or similar): includes Date and UPR columns and control chart tabular data (center line and UCL/LCL or enough to infer them) for 01/04/2025\u201303/01/2025.\n   - \"IMR_Baseline\" (or similar): same structure as above but restricted to 01/04/2025\u201302/21/2025.\n   - \"Regression\" (or similar): includes at minimum a coefficients table where Time of Day is a predictor, plus R-squared and p-values.\n   - \"OneSample\" (or similar): includes target of 3400 UPR, sample mean, n, stdev, test statistic, p-value, and CI bounds.\n   - \"Capability\" (or similar): includes Cp and Cpk, mean, stdev, and the spec basis (LSL or target) set at 3400 UPR.\n   - \"DMAIC_Timeline\" (or similar): tabular listing of DMAIC phases with dates and completion/in-progress/TBD status.\n   - Optional: \"Data Summary\" and/or \"ReadMe/Method\" explaining data source and steps.\n\nScoring (STRUCTURE ONLY):\n- 12: Presentation file present (PPTX or PDF) AND companion Excel workbook present with all required sheets (or clearly equivalent names) AND all required slides/sections listed above are present.\n- 10: Presentation + workbook present; exactly one minor missing item (e.g., one required sheet OR one required slide element missing), everything else present.\n- 8: Presentation + workbook present; two required items missing across slides/sheets.\n- 6: Presentation present, but workbook missing OR workbook present but major slide sections missing (e.g., no A3, no Charter, or missing two or more required charts).\n- 0: Not a PPTX/PDF presentation OR grossly wrong format OR no companion Excel workbook.\n\nOnly check presence/structure, not the correctness of numbers.", "expectation": "A PPTX (or PDF) that follows the Analyze tollgate structure and an Excel workbook with ANOVA, IMR (full and baseline), Regression, OneSample test, Capability, and DMAIC timeline sheets enabling verification."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Data, Dates, Stats Presence)", "description": "Code-based verification of key correctness signals using the companion Excel workbook. Flexible, resilient checks confirm date ranges, presence of target/specs, and plausible stats fields. Not assessing deep statistical validity\u2014just verifying that the mandated artifacts are present and internally consistent enough for audit.", "is_required": false, "max_points": 15.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "IMR Date Range Validation (Full and Baseline)", "description": "Verify IMR_Full covers 2025-01-04 to at least 2025-03-01, and IMR_Baseline covers 2025-01-04 to at most 2025-02-21, with Date and UPR-like columns present.", "weight": 3.0, "code": "import re, json\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\ndef evaluate(workflow, context):\n    def find_spreadsheet():\n        for r in context.get_all_outputs() or []:\n            try:\n                if getattr(r, 'is_spreadsheet', False):\n                    return r\n            except Exception:\n                continue\n        return None\n\n    def find_sheet(excel_file, names, fallback_contains=None):\n        # names: list of substrings to match; fallback_contains: list of looser tokens\n        try:\n            sheets = [s for s in excel_file.sheet_names]\n            sl = [s.lower() for s in sheets]\n            # Strong match: all tokens present\n            for i, s in enumerate(sl):\n                if all(tok in s for tok in names):\n                    return sheets[i]\n            # Fallback: any token\n            if fallback_contains:\n                for i, s in enumerate(sl):\n                    if any(tok in s for tok in fallback_contains):\n                        return sheets[i]\n        except Exception:\n            pass\n        return None\n\n    def to_datetime_series(col):\n        return pd.to_datetime(col, errors='coerce', infer_datetime_format=True)\n\n    def find_date_column(df):\n        for c in df.columns:\n            if 'date' in str(c).lower():\n                return c\n        # try parse any column that parses to many valid dates\n        best = None; best_valid = 0\n        for c in df.columns:\n            ser = to_datetime_series(df[c])\n            valid = ser.notna().sum()\n            if valid > best_valid and valid >= max(3, int(0.3*len(df))):\n                best, best_valid = c, valid\n        return best\n\n    def has_upr_column(df):\n        for c in df.columns:\n            cl = str(c).lower()\n            if any(k in cl for k in ['upr','processing rate','rate']):\n                return True\n        # fallback: any numeric column\n        num_cols = df.select_dtypes(include=[np.number]).columns\n        return len(num_cols) > 0\n\n    output = find_spreadsheet()\n    if not output:\n        return 0.0, 'No spreadsheet output found.'\n\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n    except Exception as e:\n        return 0.0, f'Failed to open workbook: {e}'\n\n    # Locate IMR full and baseline\n    sheet_full = find_sheet(xls, names=['imr','full'], fallback_contains=['imr','i-mr','control'])\n    sheet_base = find_sheet(xls, names=['imr','baseline'], fallback_contains=['baseline','define','measure'])\n\n    score = 0.0\n    feedback = []\n\n    # Check Full\n    try:\n        if sheet_full:\n            df_full = pd.read_excel(path, sheet_name=sheet_full)\n            dcol = find_date_column(df_full)\n            if dcol is not None and has_upr_column(df_full):\n                dts = to_datetime_series(df_full[dcol]).dropna()\n                if not dts.empty:\n                    dmin = dts.min(); dmax = dts.max()\n                    cond = (dmin <= pd.Timestamp('2025-01-04')) and (dmax >= pd.Timestamp('2025-03-01'))\n                    if cond:\n                        score += 1.5\n                    feedback.append(f'IMR_Full dates: {dmin.date() if pd.notna(dmin) else None} to {dmax.date() if pd.notna(dmax) else None}')\n            else:\n                feedback.append('IMR_Full missing date or UPR-like column.')\n        else:\n            feedback.append('IMR_Full-like sheet not found.')\n    except Exception as e:\n        feedback.append(f'Error reading IMR_Full: {e}')\n\n    # Check Baseline\n    try:\n        if sheet_base:\n            df_base = pd.read_excel(path, sheet_name=sheet_base)\n            dcol = find_date_column(df_base)\n            if dcol is not None and has_upr_column(df_base):\n                dts = to_datetime_series(df_base[dcol]).dropna()\n                if not dts.empty:\n                    dmin = dts.min(); dmax = dts.max()\n                    cond = (dmin >= pd.Timestamp('2025-01-04')) and (dmax <= pd.Timestamp('2025-02-21'))\n                    if cond:\n                        score += 1.5\n                    feedback.append(f'IMR_Baseline dates: {dmin.date() if pd.notna(dmin) else None} to {dmax.date() if pd.notna(dmax) else None}')\n            else:\n                feedback.append('IMR_Baseline missing date or UPR-like column.')\n        else:\n            feedback.append('IMR_Baseline-like sheet not found.')\n    except Exception as e:\n        feedback.append(f'Error reading IMR_Baseline: {e}')\n\n    return min(score, 3.0), '; '.join(feedback)"}, {"type": "code", "name": "ANOVA Structure and Day-of-Week Coverage", "description": "Verify ANOVA sheet includes day-of-week type grouping (>=5 distinct groups), group stats, and a p-value/ANOVA test indicator.", "weight": 2.5, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    def find_spreadsheet():\n        for r in context.get_all_outputs() or []:\n            if getattr(r, 'is_spreadsheet', False):\n                return r\n        return None\n\n    def find_sheet(xls):\n        sl = [s.lower() for s in xls.sheet_names]\n        for i, s in enumerate(sl):\n            if 'anova' in s or 'one-way' in s or 'one way' in s or 'interval' in s:\n                return xls.sheet_names[i]\n        return None\n\n    output = find_spreadsheet()\n    if not output:\n        return 0.0, 'No spreadsheet found.'\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n    except Exception as e:\n        return 0.0, f'Open error: {e}'\n\n    sheet = find_sheet(xls)\n    if not sheet:\n        return 0.0, 'ANOVA-like sheet not found.'\n\n    df = pd.read_excel(path, sheet_name=sheet)\n\n    # Find group column (day-of-week or similar)\n    group_col = None\n    for c in df.columns:\n        cl = str(c).lower()\n        if any(k in cl for k in ['day','weekday','group']):\n            group_col = c; break\n    # Count groups (aim for ~7)\n    groups_ok = False\n    if group_col is not None:\n        uniq = df[group_col].dropna().astype(str).str.strip().str.lower().unique()\n        groups_ok = len(uniq) >= 5\n    # p-value presence\n    text_grid = ' '.join(df.astype(str).fillna('').values.ravel()).lower()\n    p_ok = ('p-value' in text_grid) or ('p value' in text_grid) or re.search(r'\\bp\\s*[<=>]', text_grid)\n\n    score = 0.0\n    if groups_ok:\n        score += 1.25\n    if p_ok:\n        score += 1.25\n\n    fb = f\"Groups_ok={groups_ok}, pvalue_ok={p_ok}\"\n    return score, fb"}, {"type": "code", "name": "One-Sample Test vs 3400 Presence", "description": "Verify the one-sample test sheet compares mean UPR to the 3400 target and includes p-value and/or CI.", "weight": 2.0, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    def find_spreadsheet():\n        for r in context.get_all_outputs() or []:\n            if getattr(r, 'is_spreadsheet', False):\n                return r\n        return None\n\n    def find_sheet(xls):\n        sl = [s.lower() for s in xls.sheet_names]\n        for i, s in enumerate(sl):\n            if 'one' in s and 'sample' in s:\n                return xls.sheet_names[i]\n            if 't-test' in s or 'hypothesis' in s:\n                return xls.sheet_names[i]\n        return None\n\n    output = find_spreadsheet()\n    if not output:\n        return 0.0, 'No spreadsheet found.'\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n    except Exception as e:\n        return 0.0, f'Open error: {e}'\n\n    sheet = find_sheet(xls)\n    if not sheet:\n        return 0.0, 'OneSample-like sheet not found.'\n\n    df = pd.read_excel(path, sheet_name=sheet)\n    text = ' '.join(df.astype(str).fillna('').values.ravel()).lower()\n\n    has_target = ('3400' in text) or any(abs(v-3400) < 1e-6 for v in pd.to_numeric(df.stack(dropna=False), errors='coerce').dropna().values)\n    has_p_or_ci = ('p-value' in text) or ('p value' in text) or ('confidence interval' in text) or re.search(r'\\bci\\b', text)\n\n    score = 0.0\n    if has_target:\n        score += 1.0\n    if has_p_or_ci:\n        score += 1.0\n\n    return score, f\"target_3400={has_target}, p_or_ci={has_p_or_ci}\""}, {"type": "code", "name": "Regression Includes Time Predictor and Fit Stats", "description": "Verify the regression sheet contains a coefficient for a time-of-day predictor and provides an R-squared within [0,1] or a percentage convertible to [0,1].", "weight": 2.5, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    def find_spreadsheet():\n        for r in context.get_all_outputs() or []:\n            if getattr(r, 'is_spreadsheet', False):\n                return r\n        return None\n\n    def find_sheet(xls):\n        sl = [s.lower() for s in xls.sheet_names]\n        for i, s in enumerate(sl):\n            if 'regress' in s or 'model' in s or 'linear' in s:\n                return xls.sheet_names[i]\n        return None\n\n    output = find_spreadsheet()\n    if not output:\n        return 0.0, 'No spreadsheet found.'\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n    except Exception as e:\n        return 0.0, f'Open error: {e}'\n\n    sheet = find_sheet(xls)\n    if not sheet:\n        return 0.0, 'Regression-like sheet not found.'\n\n    df = pd.read_excel(path, sheet_name=sheet)\n    text = ' '.join(df.astype(str).fillna('').values.ravel()).lower()\n\n    # R-squared detection\n    r2_match = re.search(r'(r[-\\s]?sq(?:uared)?)[^0-9%]*([0-9\\.]+)\\s*%?', text)\n    r2_ok = False\n    if r2_match:\n        try:\n            val = float(r2_match.group(2))\n            if val > 1.0:  # treat as percentage\n                val = val/100.0\n            r2_ok = 0.0 <= val <= 1.0\n        except Exception:\n            r2_ok = False\n\n    # Time predictor presence in coefficients table\n    time_ok = False\n    # Try to find a predictor column\n    preds = None\n    for c in df.columns:\n        if any(k in str(c).lower() for k in ['predictor','term','variable']):\n            preds = df[c].astype(str).str.lower()\n            break\n    if preds is None:\n        preds = pd.Series([], dtype=str)\n    if any(any(k in str(p) for k in ['time','hour']) for p in preds):\n        time_ok = True\n    else:\n        # fallback: look anywhere\n        if 'time' in text or 'hour' in text:\n            time_ok = True\n\n    score = 0.0\n    if time_ok:\n        score += 1.25\n    if r2_ok:\n        score += 1.25\n\n    return score, f\"time_predictor={time_ok}, r2_ok={r2_ok}\""}, {"type": "code", "name": "Capability Metrics and Spec Alignment (3400 as LSL/Target)", "description": "Verify Capability sheet includes Cp and Cpk, and references 3400 as LSL or target. Values should be plausible (0 < Cp,Cpk < 5).", "weight": 3.0, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    def find_spreadsheet():\n        for r in context.get_all_outputs() or []:\n            if getattr(r, 'is_spreadsheet', False):\n                return r\n        return None\n\n    def find_sheet(xls):\n        sl = [s.lower() for s in xls.sheet_names]\n        for i, s in enumerate(sl):\n            if 'capability' in s or 'cpk' in s or 'cp' in s:\n                return xls.sheet_names[i]\n        return None\n\n    output = find_spreadsheet()\n    if not output:\n        return 0.0, 'No spreadsheet found.'\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n    except Exception as e:\n        return 0.0, f'Open error: {e}'\n\n    sheet = find_sheet(xls)\n    if not sheet:\n        return 0.0, 'Capability-like sheet not found.'\n\n    df = pd.read_excel(path, sheet_name=sheet)\n    text = ' '.join(df.astype(str).fillna('').values.ravel()).lower()\n\n    nums = pd.to_numeric(df.stack(dropna=False), errors='coerce').dropna().values if not df.empty else []\n\n    def find_metric(label):\n        # Try to get value near a label in text\n        m = re.search(label + r\"[^0-9\\.]*([0-9\\.]+)\", text)\n        if m:\n            try:\n                return float(m.group(1))\n            except Exception:\n                return None\n        # fallback: use any plausible numeric in table but we can't know which is which\n        return None\n\n    cp = find_metric(r'\\bcp\\b')\n    cpk = find_metric(r'\\bcpk\\b')\n\n    cp_ok = (cp is not None and 0.0 < cp < 5.0)\n    cpk_ok = (cpk is not None and 0.0 < cpk < 5.0)\n\n    lsl_ok = ('lsl' in text and '3400' in text) or ('spec' in text and '3400' in text) or ('target' in text and '3400' in text)\n\n    score = 0.0\n    if cp_ok:\n        score += 1.0\n    if cpk_ok:\n        score += 1.0\n    if lsl_ok:\n        score += 1.0\n\n    return score, f\"cp_ok={cp_ok}, cpk_ok={cpk_ok}, spec3400={lsl_ok}\""}, {"type": "code", "name": "DMAIC Timeline Dates and Status", "description": "Verify the DMAIC timeline sheet includes required dates (Define 02/08/2025, Measure 02/15/2025, Analyze 02/22/2025\u201303/01/2025) and indicates Improve/Control as TBD or in-progress appropriately.", "weight": 1.5, "code": "import re, json\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\ndef evaluate(workflow, context):\n    def find_spreadsheet():\n        for r in context.get_all_outputs() or []:\n            if getattr(r, 'is_spreadsheet', False):\n                return r\n        return None\n\n    def find_sheet(xls):\n        for s in xls.sheet_names:\n            sl = s.lower()\n            if 'dmaic' in sl or 'timeline' in sl or 'schedule' in sl:\n                return s\n        return None\n\n    def parse_all_dates(df):\n        vals = []\n        for v in df.astype(str).fillna('').values.ravel():\n            t = v.strip()\n            # try multiple formats\n            for fmt in ['%Y-%m-%d','%m/%d/%Y','%m/%d/%y','%m-%d-%Y','%m-%d-%y']:\n                try:\n                    dt = datetime.strptime(t, fmt)\n                    vals.append(dt.date())\n                    break\n                except Exception:\n                    continue\n        return set(vals)\n\n    def has_tbd(df):\n        text = ' '.join(df.astype(str).fillna('').values.ravel()).lower()\n        return 'tbd' in text or 'to be determined' in text\n\n    output = find_spreadsheet()\n    if not output:\n        return 0.0, 'No spreadsheet found.'\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n    except Exception as e:\n        return 0.0, f'Open error: {e}'\n\n    sheet = find_sheet(xls)\n    if not sheet:\n        return 0.0, 'DMAIC timeline sheet not found.'\n\n    df = pd.read_excel(path, sheet_name=sheet)\n    dates = parse_all_dates(df)\n\n    needed = {datetime(2025,2,8).date(), datetime(2025,2,15).date(), datetime(2025,2,22).date(), datetime(2025,3,1).date()}\n    have_all = needed.issubset(dates)\n    tbd_ok = has_tbd(df)\n\n    score = 0.0\n    if have_all:\n        score += 1.0\n    if tbd_ok:\n        score += 0.5\n\n    return score, f\"dates_ok={have_all}, tbd_ok={tbd_ok}\""}, {"type": "code", "name": "Data Provenance Mentions \"Processing Data.xlsx\"", "description": "Verify that the workbook text includes a reference to the source file name \"Processing Data.xlsx\".", "weight": 0.5, "code": "import re, json\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    def find_spreadsheet():\n        for r in context.get_all_outputs() or []:\n            if getattr(r, 'is_spreadsheet', False):\n                return r\n        return None\n\n    output = find_spreadsheet()\n    if not output:\n        return 0.0, 'No spreadsheet found.'\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n    except Exception as e:\n        return 0.0, f'Open error: {e}'\n\n    target = 'processing data.xlsx'\n    found = False\n    try:\n        for s in xls.sheet_names:\n            df = pd.read_excel(path, sheet_name=s)\n            text = ' '.join(df.astype(str).fillna('').values.ravel()).lower()\n            if target in text:\n                found = True\n                break\n    except Exception:\n        pass\n\n    return (0.5 if found else 0.0), f\"provenance_found={found}\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Presentation Quality and Professionalism", "description": "LLM assessment of executive readiness, clarity of narrative, Lean Six Sigma appropriateness, and actionability for leadership.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Quality, Insight, and Actionability", "description": "Judge overall communication quality, clarity of analysis interpretations, linkage to operational/financial impact, and appropriateness for a Green Belt Analyze tollgate audience.", "weight": 3.0, "judge_prompt": "Assess the presentation (PPTX or PDF) for professional quality and leadership readiness. Do NOT recalculate statistics; judge clarity and usefulness.\n\nCriteria (Holistic, award partial as appropriate):\n- Clarity and flow: Clear executive summary tone, logical progression from problem to analysis to implications.\n- Interpretations: Brief, accurate interpretations accompany each chart (ANOVA, I-MR, Regression, 1-Sample, Capability). Mentions p-values/CI at a high level without contradictions.\n- Lean Six Sigma fit: Uses correct Analyze-phase framing, ties results to variation, stability, capability, and root-cause exploration.\n- Actionability: Identifies what findings imply for staffing/modeling and next steps toward Improve/Control.\n- Visuals: Clean, readable charts and A3; consistent formatting; minimal clutter.\n\nScoring:\n- 3: Strong across all criteria; executive-ready and actionable.\n- 2: Generally solid; minor gaps in clarity or actionability.\n- 1: Several weaknesses; hard to follow or weak interpretations.\n- 0: Unprofessional or incoherent for intended audience.", "expectation": "A crisp, executive-ready Analyze tollgate deck with concise interpretations, A3 summary alignment, and clear next steps into Improve/Control."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "74d6e8b0-f334-4e7e-af55-c095d5d4d1a6", "rubric": {"category_name": "MenoHelp HT Prescribing Guidelines (Virtual Menopause Care)", "rationale": "Three-stage, self-documenting rubric ensuring: (1) strict document structure enabling verification, (2) mixed code + LLM checks for clinical correctness and telehealth feasibility, and (3) overall professional quality and usability for clinicians in a virtual-first setting.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate (LLM-only)", "description": "MANDATORY structural requirements for a clinical guideline document. If the output is not a well-structured DOCX/PDF with all required sections, evaluation stops.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Document Format and Required Sections", "description": "Verify the candidate output is a DOCX or PDF clinical guideline with the exact sections and elements required to enable deterministic verification in Stage 2.", "weight": 4.0, "judge_prompt": "You are the Stage 1 structural gate. Inspect the single candidate output.\n\nAcceptable formats: DOCX (preferred) or PDF. Reject plain text/Excel. The document must be professionally formatted and suitable for clinicians. Minimum length: 8 pages (be flexible if content density is high but under 8 pages; see scoring).\n\nCheck for the following REQUIRED front matter and core sections (exact names may vary but intent must be clear and headings visible):\n\nFront matter and document control:\n- Title that clearly states: \u201cMenoHelp\u201d AND \u201cHormone Therapy (HT) Prescribing Guidelines\u201d (or near-equivalent)\n- Version control: Version number, Effective date, Author(s)/Owner, Approver, Next review date (or Review cycle)\n- Intended Use/Scope: Virtual menopause care for low- to moderate-risk patients; includes population and care setting\n- Table of Contents\n\nCore sections (must be present):\n1) Executive Summary or Overview (key principles of HT, when appropriate)\n2) Eligibility and Contraindications (absolute and relative; explicit list)\n3) Baseline Evaluation & Risk Stratification (history, vitals/BMI, breast/CV/VTE risk, baseline tests/labs as applicable)\n4) Treatment Options (systemic estrogen preparations/routes; progestogen requirement for intact uterus; local/vaginal therapy)\n5) Dosing & Titration (standard starting doses with units and ranges; routes: oral, transdermal patch/gel/spray, vaginal; include at least one dosing table)\n6) Initiation Protocol and Informed Consent/Shared Decision-Making (with documentation requirements)\n7) Monitoring & Follow-up (initial and maintenance intervals; what to assess)\n8) Risk Counseling & Safety (VTE, stroke, breast cancer, endometrial protection, liver/gallbladder, mitigation strategies)\n9) Special Populations (e.g., POI/early menopause, perimenopause vs postmenopause, hysterectomy vs uterus present, migraines, obesity, VTE risk, smoking)\n10) Drug Interactions & Switching/Discontinuation (including tapering/recurrence)\n11) Nonhormonal Alternatives/Adjuncts (e.g., SSRIs/SNRIs, gabapentin, clonidine, oxybutynin; vaginal moisturizers)\n12) Virtual Care Workflow (telehealth screening, remote vitals, labs ordering, red flags, escalation to in-person/ED)\n13) Documentation Checklists & Templates (eligibility/contraindication checklist, consent template, follow-up checklist)\n14) References/Bibliography (end of document; numbered or author-year; in-text or footnote citations present)\n15) Appendices (at least one: dosing tables, algorithms/flowcharts, patient education handout)\n\nScoring (STRUCTURE ONLY; do not judge medical accuracy):\n- 4.0: DOCX/PDF; appears ~8+ pages; all front matter elements present; all 15 sections present with clear headers; at least one dosing table visible.\n- 3.5: DOCX/PDF; front matter present (may be missing one element like Next review date); 14 of 15 sections present (only one missing or merged); dosing table present.\n- 3.0: DOCX/PDF; front matter present but missing up to two elements; 12\u201313 of 15 sections present; dosing table present OR clear dosing list with units.\n- 2.0: DOCX/PDF; major gaps: only 9\u201311 sections or missing dosing table and references section; OR under 8 pages but still substantial.\n- 0.0: Not DOCX/PDF; or fewer than 9 sections; or missing BOTH Eligibility/Contraindications AND Dosing; or no References. If extreme brevity (e.g., brochure) or purely narrative without headers, score 0.\n\nBe flexible on exact headings but strict on intent and presence. Only evaluate presence/structure, not correctness.", "expectation": "A DOCX/PDF clinical guideline with document control, all core sections, dosing tables, telehealth workflow, and references."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Verification (Code + LLM)", "description": "Now that structure is enforced, verify clinical completeness, plausibility of dosing, presence of contraindications, telehealth workflow elements, and adequacy/diversity of citations.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Reference Count and Source Diversity", "description": "Check that the References section exists with sufficient items (>=10) and diversity including at least two recognized authority sources (e.g., NAMS, ACOG, Endocrine Society, NICE, USPSTF).", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    text = \"\"\n    try:\n        if output.is_document:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                text = \"\"\n            if not text:\n                try:\n                    text = context.files.read_pdf_text(output.id)\n                except Exception:\n                    text = \"\"\n        if (not text) and getattr(output, 'is_text_format', False):\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    except Exception:\n        text = \"\"\n    if not text:\n        return 0.0\n    lower = text.lower()\n\n    # Attempt to isolate references section (from the first occurrence of 'references' or 'bibliography')\n    ref_start = None\n    for kw in [\"\\nreferences\", \"\\nbibliography\", \"\\nworks cited\"]:\n        idx = lower.find(kw)\n        if idx != -1:\n            ref_start = idx\n            break\n    refs_text = lower[ref_start:] if ref_start is not None else lower\n\n    # Count references heuristically: lines containing a year or doi or http with scholarly cues\n    lines = [l.strip() for l in refs_text.splitlines() if l.strip()]\n    ref_like = 0\n    authority_hits = 0\n    authority_terms = [\"north american menopause society\", \"nams\", \"acog\", \"american college of obstetricians\", \"endocrine society\", \"nice\", \"uspstf\", \"cochrane\"]\n    year_re = re.compile(r\"\\b(19|20)\\d{2}\\b\")\n    for l in lines:\n        if (\"doi\" in l) or (\"http\" in l) or year_re.search(l) or (\",\" in l and \")\" in l):\n            ref_like += 1\n        if any(a in l for a in authority_terms):\n            authority_hits += 1\n\n    # Also consider inline citations in body if no clear references section\n    inline_years = len(re.findall(r\"\\((?:[A-Z][A-Za-z-]+,?\\s*)?(19|20)\\d{2}(?:[a-z])?\\)\", text))\n    est_total_refs = max(ref_like, inline_years)\n\n    # Score logic\n    score = 0.0\n    if est_total_refs >= 10 and authority_hits >= 2:\n        score = 1.0\n    elif est_total_refs >= 6 and authority_hits >= 1:\n        score = 0.7\n    elif est_total_refs >= 3:\n        score = 0.4\n    else:\n        score = 0.0\n    return score"}, {"type": "code", "name": "Dosing Presence and Plausible Ranges", "description": "Verify dosing details exist for key HT agents/routes with units and plausible ranges (e.g., oral estradiol 0.5\u20132 mg, transdermal patch 0.025\u20130.1 mg/day, micronized progesterone 100\u2013200 mg, MPA 2.5\u201310 mg).", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    text = \"\"\n    try:\n        if output.is_document:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                text = \"\"\n            if not text:\n                try:\n                    text = context.files.read_pdf_text(output.id)\n                except Exception:\n                    text = \"\"\n        if (not text) and getattr(output, 'is_text_format', False):\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    except Exception:\n        text = \"\"\n    if not text:\n        return 0.0\n    t = text.lower()\n\n    # Helper to check pattern co-occurrence of drug and a dose with mg/mcg/ug and daily/weekly hints\n    def has_dose(drug_terms, units=(\"mg\", \"mcg\", \"\u00b5g\"), window=120):\n        for m in re.finditer('|'.join(map(re.escape, drug_terms)), t):\n            start = max(0, m.start()-window)\n            end = min(len(t), m.end()+window)\n            seg = t[start:end]\n            if re.search(r\"\\b\\d+(?:\\.\\d+)?\\s?(?:\" + \"|\".join(units) + r\")\\b\", seg):\n                return True\n        return False\n\n    oral_e2 = has_dose([\"oral estradiol\", \"estradiol (oral)\", \"estradiol tablets\", \"estradiol po\", \"estradiol\" ])\n    td_e2 = has_dose([\"transdermal estradiol\", \"estradiol patch\", \"patch\", \"gel\", \"spray\"]) and (\"patch\" in t or \"transdermal\" in t or \"gel\" in t or \"spray\" in t)\n    prog = has_dose([\"micronized progesterone\", \"progesterone\", \"prometrium\"]) or has_dose([\"medroxyprogesterone\", \"mpa\"]) \n    mpa = has_dose([\"medroxyprogesterone\", \"mpa\"]) or (\"medroxyprogesterone\" in t)\n    vaginal_local = has_dose([\"vaginal estradiol\", \"vaginal tablet\", \"estradiol cream\", \"ring\", \"vagifem\", \"estrace cream\", \"e2 vaginal\"]) or (\"vaginal\" in t and re.search(r\"\\b(10|25)\\s?(mcg|\u00b5g)\\b\", t))\n\n    titration_cues = any(k in t for k in [\"titrate\", \"increase\", \"decrease\", \"adjust\", \"stepwise\", \"after\", \"every\", \"weeks\", \"follow-up\", \"reassess\"])\n\n    # Score from subcriteria (5 items): oral_e2, td_e2, prog, mpa, vaginal_local; + titration cue for completeness\n    score_parts = 0\n    for flag in [oral_e2, td_e2, prog, mpa, vaginal_local]:\n        score_parts += 1 if flag else 0\n    score = min(1.0, (score_parts * 0.18) + (0.1 if titration_cues else 0.0))  # up to ~1.0\n    return score"}, {"type": "code", "name": "Monitoring Intervals and Content", "description": "Check for initial and maintenance follow-up intervals and monitoring content (e.g., early 2\u201312 weeks; maintenance 3\u201312 months; symptom control, BP, adverse effects).", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    text = \"\"\n    try:\n        if output.is_document:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                text = \"\"\n            if not text:\n                try:\n                    text = context.files.read_pdf_text(output.id)\n                except Exception:\n                    text = \"\"\n        if (not text) and getattr(output, 'is_text_format', False):\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    except Exception:\n        text = \"\"\n    if not text:\n        return 0.0\n    t = text.lower()\n\n    early = bool(re.search(r\"(follow[- ]?up|reassess|check|review).{0,40}(2|3|4|6|8|10|12)\\s*(week|wk)s?\", t))\n    maintenance = bool(re.search(r\"(follow[- ]?up|reassess|check|review).{0,40}(3|4|6|9|12)\\s*(month|mo|mth)s?\", t) or re.search(r\"annual|yearly\", t))\n    content_hits = sum([1 for k in [\"symptom\", \"bp\", \"blood pressure\", \"adverse\", \"side effect\", \"bleeding\", \"breast\", \"lipid\", \"mammogram\", \"endometrium\", \"uterine\"] if k in t])\n\n    if early and maintenance and content_hits >= 3:\n        return 1.0\n    if (early or maintenance) and content_hits >= 2:\n        return 0.7\n    if early or maintenance:\n        return 0.4\n    return 0.0"}, {"type": "code", "name": "Contraindications Coverage", "description": "Ensure common absolute/strong contraindications are enumerated: e.g., breast cancer, estrogen-dependent neoplasia, unexplained vaginal bleeding, active liver disease, VTE/DVT/PE, stroke, coronary disease/MI, pregnancy, known thrombophilia.", "weight": 0.5, "code": "def evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    text = \"\"\n    try:\n        if output.is_document:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                text = \"\"\n            if not text:\n                try:\n                    text = context.files.read_pdf_text(output.id)\n                except Exception:\n                    text = \"\"\n        if (not text) and getattr(output, 'is_text_format', False):\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    except Exception:\n        text = \"\"\n    if not text:\n        return 0.0\n    t = text.lower()\n\n    terms = [\n        \"breast cancer\", \"estrogen-dependent\", \"endometrial cancer\", \"unexplained vaginal bleeding\",\n        \"active liver\", \"severe liver\", \"venous thromboembolism\", \"vte\", \"dvt\", \"deep vein thrombosis\",\n        \"pulmonary embolism\", \"pe\", \"stroke\", \"cerebrovascular\", \"coronary\", \"mi\", \"myocardial infarction\",\n        \"ischemic heart\", \"pregnancy\", \"thrombophilia\", \"porphyria\", \"liver tumor\"\n    ]\n    hits = sum(1 for term in terms if term in t)\n    if hits >= 8:\n        return 1.0\n    if hits >= 6:\n        return 0.7\n    if hits >= 4:\n        return 0.4\n    return 0.0"}, {"type": "llm_judge", "name": "Evidence Alignment and Cross-Consistency", "description": "Judge whether recommendations align with mainstream guidance (NAMS/ACOG/Endocrine/NICE), correctly require progestogen for intact uterus with systemic estrogen, present risk tradeoffs fairly (e.g., lower VTE risk with transdermal), and include nonhormonal alternatives; confirm telehealth workflow addresses feasibility and escalation.", "weight": 1.0, "judge_prompt": "Evaluate the document for evidence alignment and internal consistency (not presentation quality). Consider:\n- Does it broadly align with well-accepted recommendations from NAMS/ACOG/Endocrine Society/NICE (no need to name them explicitly if content matches standard practice)?\n- Does it state that individuals with an intact uterus using systemic estrogen require endometrial protection with an adequate progestogen regimen?\n- Are dosing/regimen examples clinically plausible and not obviously unsafe or obsolete? (e.g., includes oral and transdermal estradiol, typical dose ranges; discourages compounded bioidentical hormones as first-line; recognizes vaginal estrogen for GSM.)\n- Are risk statements balanced and correct (e.g., VTE/stroke risk; comparatively lower VTE risk with transdermal; breast/endometrial considerations; timing hypothesis nuance is acceptable)?\n- Are nonhormonal alternatives listed where HT is contraindicated or declined?\n- Does the telehealth/virtual workflow address remote vitals/labs, red-flag symptoms, and escalation to in-person/ED when indicated?\n\nScoring:\n- 1.0: Consistent with mainstream guidance across all bullets; no major clinical errors or omissions.\n- 0.5: Generally aligned but with 1\u20132 notable gaps or ambiguities; no unsafe recommendations.\n- 0.0: Significant misalignment or unsafe/incorrect guidance (e.g., systemic estrogen without progestogen for uterus, no risk counseling, no escalation criteria).", "expectation": "Clinically sound, standard-aligned guidance with coherent telehealth workflow and safety measures."}, {"type": "code", "name": "Telehealth Workflow Elements", "description": "Verify presence of virtual care workflow elements: remote vitals, labs ordering, informed consent/identity, red flags and escalation to in-person/ED.", "weight": 0.5, "code": "def evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    text = \"\"\n    try:\n        if output.is_document:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                text = \"\"\n            if not text:\n                try:\n                    text = context.files.read_pdf_text(output.id)\n                except Exception:\n                    text = \"\"\n        if (not text) and getattr(output, 'is_text_format', False):\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    except Exception:\n        text = \"\"\n    if not text:\n        return 0.0\n    t = text.lower()\n\n    telehealth = any(k in t for k in [\"telehealth\", \"telemedicine\", \"virtual\", \"video visit\", \"remote\"])\n    vitals = any(k in t for k in [\"blood pressure\", \"bp\", \"vital\", \"heart rate\", \"bmi\", \"weight\"]) and (\"home\" in t or \"remote\" in t or \"self\" in t or \"reported\" in t)\n    labs = any(k in t for k in [\"lab\", \"labs\", \"laboratory\", \"order\", \"quest\", \"labcorp\", \"testing\"]) and any(k in t for k in [\"mail\", \"fax\", \"e-order\", \"electronic order\", \"draw station\", \"collection site\"]) or (\"order labs\" in t)\n    escalation = any(k in t for k in [\"red flag\", \"escalate\", \"in-person\", \"emergency\", \"911\", \"er\", \"ed\", \"urgent care\"]) or (\"warning sign\" in t)\n\n    score = 0.0\n    score += 0.25 if telehealth else 0.0\n    score += 0.25 if vitals else 0.0\n    score += 0.25 if labs else 0.0\n    score += 0.25 if escalation else 0.0\n    return min(1.0, score)"}, {"type": "code", "name": "Document Control Elements", "description": "Check for versioning and governance metadata: Version number, Effective date, Approver, Next review date, Author/Owner.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    text = \"\"\n    try:\n        if output.is_document:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                text = \"\"\n            if not text:\n                try:\n                    text = context.files.read_pdf_text(output.id)\n                except Exception:\n                    text = \"\"\n        if (not text) and getattr(output, 'is_text_format', False):\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    except Exception:\n        text = \"\"\n    if not text:\n        return 0.0\n    t = text.lower()\n\n    version = bool(re.search(r\"\\b(v(er(sion)?)?\\s*\\d+(?:\\.\\d+){0,2})\\b\", t)) or (\"version\" in t and any(ch.isdigit() for ch in t))\n    effective = (\"effective date\" in t) or (\"effective\" in t and re.search(r\"\\b(\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}|(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)[a-z]*\\s+\\d{1,2},?\\s+\\d{4})\\b\", t))\n    approver = (\"approved by\" in t) or (\"medical director\" in t and \"approved\" in t) or (\"governance\" in t)\n    review = (\"next review\" in t) or (\"review cycle\" in t) or (\"last reviewed\" in t)\n    owner = (\"author\" in t) or (\"owner\" in t) or (\"document owner\" in t)\n\n    hits = sum([version, effective, approver, review, owner])\n    if hits >= 4:\n        return 1.0\n    if hits >= 3:\n        return 0.7\n    if hits >= 2:\n        return 0.4\n    return 0.0"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Clinical Usability and Presentation Quality (LLM)", "description": "Professional polish, readability, and clinician usability for a virtual-first program.", "is_required": false, "max_points": 1.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Usability, Clarity, and Actionability", "description": "Assess professional presentation and practical usability for busy clinicians working in virtual menopause care.", "weight": 1.0, "judge_prompt": "Assess the document\u2019s overall professional quality and clinical usability (not clinical correctness already checked in Stage 2):\n- Organization: Logical flow from eligibility to initiation, dosing, monitoring, and escalation; headers and subheaders make navigation easy; TOC works.\n- Actionability: Clear checklists, templates, and step-by-step workflows; dosing tables easy to read; callouts for red flags.\n- Clarity and brevity: Concise, unambiguous guidance; consistent terminology; minimal duplication.\n- Visual aids: Presence of tables/algorithms/appendices that enhance execution (don\u2019t penalize if only text but still clearly actionable).\n- Virtual-first practicality: Instructions feasible in telehealth (e.g., remote vitals, labs, documentation steps).\n\nScoring:\n- 1.0: Highly usable, clearly organized, actionable with checklists/tables; easy for clinicians to apply.\n- 0.5: Generally usable but with moderate issues (navigation, redundancy, or sparse checklists/tables).\n- 0.0: Hard to follow, poorly organized, or not actionable in a virtual setting.", "expectation": "A polished, easy-to-navigate guideline with clear checklists/tables and virtual-first practicality."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "8079e27d-b6f3-4f75-a9b5-db27903c798d", "rubric": {"category_name": "Finance and Insurance \u2014 Financial and Investment Analysts \u2014 S&P 500 P/E Deep Dive (Excel)", "rationale": "Self-documenting, shape-first rubric for an analytical Excel deliverable. Stage 1 (LLM-only) mandates an exact workbook structure so verification is trivial. Stage 2 mixes code rules (bounds, consistency, and cross-sheet checks) with a light LLM screen check. Stage 3 assesses professional quality and client usability.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Structured Output Format Gate (LLM-only)", "description": "Gate: Verify the candidate produced a single Excel workbook with the exact, verifiable structure needed for analysis and screening.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Workbook Shape and Required Sections", "description": "LLM checks the Excel file structure, sheet presence, and required columns/sections for company-level and sub-sector analysis, plus data sources and overview.", "weight": 4.0, "judge_prompt": "You are checking ONLY the structure/format of the submitted output. Do not judge correctness of numbers. Treat minor naming variations flexibly (e.g., \"Sub-sector\" vs \"Subsector\"; \"Forward P/E\" vs \"NTM P/E\"). The output MUST be a single Excel workbook (.xlsx), not CSV or PDF.\n\nCheck the following structural requirements:\n\nA) Format\n- Must be an Excel file (.xlsx). Not CSV, PDF, DOCX, or images.\n- Workbook contains clearly labeled sheets with tables (header row visible). Preferably filter-enabled tables and frozen header row (judge visually if possible).\n\nB) Required Sheets (all 4 must be present)\n1) \"Company Detail\" (or very similar, e.g., \"Companies\", \"Constituents\", \"Company-Level Detail\")\n   Required columns (flexible naming, but meaning must be clear):\n   - Ticker (or Symbol)\n   - Company Name\n   - GICS Sector (or Sector)\n   - GICS Sub-Industry or Sub-Sector (accept \"Subsector\", \"Sub-Industry\", etc.)\n   - LTM P/E (Trailing P/E)\n   - NTM P/E (Forward P/E)\n   - Dividend Yield\n   - EPS (CY+1) \u2014 next full calendar year EPS\n   - EPS (CQ+1) \u2014 next calendar quarter EPS\n   - Market Capitalization (USD)\n   - % of Index (company weight in S&P 500)\n   - Above/Below Index Avg (LTM) \u2014 classification vs 15\u201320x historical S&P 500 range\n   - Above/Below Index Avg (NTM) \u2014 classification vs 15\u201320x historical S&P 500 range\n\n2) \"Subsector Summary\" (or similar, e.g., \"Sub-Industry Summary\")\n   Required columns:\n   - Sub-Sector (name)\n   - No. of Companies\n   - Total Market Cap (USD)\n   - % of Index (sub-sector weight)\n   - Avg LTM P/E\n   - Avg NTM P/E\n   - Avg Dividend Yield\n\n3) \"Index Overview\" (or similar, e.g., \"Overview\", \"Dashboard\")\n   Must include on the page:\n   - Statement of historical S&P 500 P/E range (about 15\u201320x) and mean-reversion framing\n   - As-of date for the data (should be near April 11, 2025)\n   - Counts of companies above and below historical average (for both LTM and NTM)\n   - Short instructions on how to sort/filter the workbook\n\n4) \"Data Sources & Methodology\" (or similar)\n   - A structured text/table with at least these columns or clear fields: [Element | Source/URL | As-of Date | Notes]\n   - At least 3 sentences describing data collection and calculation approach (mentioning LTM, NTM, EPS, yields, market cap, and the sub-sector taxonomy used, e.g., GICS)\n\nC) Optional but preferred (do not penalize if missing):\n- \"Pivots & Screens\" (or similar) with example pivot tables and filters for above/below historical average.\n- Professional numeric formatting (P/E as numbers, yields and weights as %).\n\nScoring (out of 4.0):\n- 4.0: Valid Excel + all 4 required sheets present + each required sheet has the listed required columns/sections + both classification columns (LTM and NTM) are present in Company Detail + Overview includes historical range, as-of date, and counts.\n- 3.0: Valid Excel + all required sheets present, but one minor requirement missing (e.g., only one of the two classification columns, or Overview missing one of the listed bullets).\n- 2.0: Valid Excel but missing one required sheet OR multiple required columns absent in a required sheet.\n- 1.0: Valid Excel but structure is far from the specification (e.g., only a flat company list without sub-sector summary or sources).\n- 0.0: Not an Excel file OR workbook is missing most of the required structure.\n\nOnly evaluate structure/presence, not the correctness of values.", "expectation": "A single Excel workbook with the four required sheets and specified columns/sections so downstream verification is trivial."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification and Consistency Checks", "description": "After shape is confirmed, verify internal consistency, reasonableness of key metrics, and cross-sheet alignment between company-level and sub-sector summaries.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Company Coverage and Required Columns (Code)", "description": "Verify the workbook is a spreadsheet; locate the Company Detail sheet; confirm presence of core columns via fuzzy matching; and ensure coverage ~S&P500 scale (>= 450 rows).", "weight": 1.4, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n    try:\n        xls_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(xls_path)\n        sheet_names = [s for s in xls.sheet_names]\n    except Exception as e:\n        return 0.0, f\"Failed to open Excel: {e}\"\n\n    # Find a likely Company Detail sheet\n    def norm(s):\n        return re.sub(r\"[^a-z0-9]+\", \" \", str(s).lower()).strip()\n    name_scores = []\n    for s in sheet_names:\n        ns = norm(s)\n        score = 0\n        if any(k in ns for k in [\"company\", \"companies\", \"constituent\", \"detail\", \"holdings\", \"equity\", \"issuer\"]):\n            score += 1\n        if \"detail\" in ns or \"constituent\" in ns:\n            score += 0.5\n        name_scores.append((score, s))\n    name_scores.sort(reverse=True)\n    company_sheet = name_scores[0][1] if name_scores and name_scores[0][0] > 0 else sheet_names[0]\n\n    try:\n        df = pd.read_excel(xls_path, sheet_name=company_sheet)\n    except Exception as e:\n        return 0.0, f\"Failed to read company sheet: {e}\"\n\n    # Normalize columns\n    cols = [norm(c) for c in df.columns]\n    colmap = {c: i for i, c in enumerate(cols)}\n\n    def has_col(patterns=None, any_substr=None, must_all=None, custom=None):\n        # patterns: list of exact normalized names\n        # any_substr: list of substrings; true if any present in a column\n        # must_all: list of substrings that must all appear in the same column name\n        # custom: callable(df) -> bool\n        if custom:\n            return bool(custom(df))\n        if patterns:\n            if any(p in colmap for p in patterns):\n                return True\n        if any_substr:\n            for c in cols:\n                if any(s in c for s in any_substr):\n                    return True\n        if must_all:\n            for c in cols:\n                if all(s in c for s in must_all):\n                    return True\n        return False\n\n    # Required semantic columns (fuzzy)\n    req_checks = {\n        \"ticker\": has_col(any_substr=[\"ticker\", \"symbol\", \"ric\"]),\n        \"company_name\": has_col(any_substr=[\"company name\", \"issuer\", \"security\", \"name\"]),\n        \"sector\": has_col(any_substr=[\"gics sector\", \"sector\"]),\n        \"subsector\": has_col(any_substr=[\"sub sector\", \"subsector\", \"sub industry\", \"sub industry\", \"gics sub\", \"sub industry\"]),\n        \"ltm_pe\": has_col(any_substr=[\"ltm p\", \"trailing p\", \"ttm p\", \"pe ltm\", \"p e ltm\", \"trailing pe\"]),\n        \"ntm_pe\": has_col(any_substr=[\"ntm p\", \"forward p\", \"pe ntm\", \"p e ntm\", \"forward pe\"]),\n        \"div_yield\": has_col(any_substr=[\"dividend\", \"div yield\", \"yield %\", \"div%\", \"dyield\"]),\n        \"eps_cy1\": has_col(custom=lambda d: any((\"eps\" in c and any(x in c for x in [\"cy+1\", \"fy+1\", \"+1\", \"next year\", \"2025e\", \"2026e\"])) and (\"q\" not in c) for c in cols)),\n        \"eps_cq1\": has_col(custom=lambda d: any((\"eps\" in c and any(x in c for x in [\"cq+1\", \"q+1\", \"next quarter\", \"nq\"])) for c in cols)),\n        \"market_cap\": has_col(any_substr=[\"market cap\", \"market capitalization\", \"mkt cap\", \"marketcap\"]),\n        \"%index\": has_col(any_substr=[\"% of index\", \"% index\", \"index weight\", \"weight\", \"spx weight\", \"s&p weight\"]),\n        \"above_below_ltm\": has_col(any_substr=[\"above/below\", \"above below\", \"classification\", \"ltm\"]),\n        \"above_below_ntm\": has_col(any_substr=[\"above/below\", \"above below\", \"classification\", \"ntm\"]) or has_col(custom=lambda d: any((\"above\" in c or \"below\" in c) and (\"ntm\" in c) for c in cols)),\n    }\n\n    present = sum(1 for k,v in req_checks.items() if v)\n    total_req = len(req_checks)\n\n    # Coverage score (rows)\n    nrows = len(df)\n    coverage = 1.0 if nrows >= 450 else (nrows / 450.0)\n    coverage = max(0.0, min(1.0, coverage))\n\n    # Columns score\n    col_score = present / total_req\n    score = (0.7 * col_score + 0.3 * coverage) * 1.4\n\n    feedback = f\"Company sheet: '{company_sheet}'. Columns present {present}/{total_req}. Rows={nrows}.\"\n    return max(0.0, min(1.4, float(score))), feedback"}, {"type": "code", "name": "Plausibility and Normalization Checks (Code)", "description": "Sanity-check ranges for LTM/NTM P/E, Dividend Yield, Market Cap; normalize percent units; check company % of Index sums reasonably (~100%).", "weight": 1.4, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n    xls_path = context.files.get_path(output.id)\n    try:\n        xls = pd.ExcelFile(xls_path)\n        sheet_names = xls.sheet_names\n    except Exception as e:\n        return 0.0, f\"Failed to open Excel: {e}\"\n\n    def norm(s):\n        return re.sub(r\"[^a-z0-9]+\", \" \", str(s).lower()).strip()\n\n    # Pick company sheet\n    best = None\n    for s in sheet_names:\n        ns = norm(s)\n        if any(k in ns for k in [\"company\", \"companies\", \"constituent\", \"detail\", \"holdings\", \"issuer\"]):\n            best = s if best is None else best\n    if best is None:\n        best = sheet_names[0]\n\n    try:\n        df = pd.read_excel(xls_path, sheet_name=best)\n    except Exception as e:\n        return 0.0, f\"Failed to read company sheet: {e}\"\n\n    cols = {norm(c): c for c in df.columns}\n\n    def find_col(keys, contains=None):\n        if contains:\n            for k in cols:\n                if all(s in k for s in contains):\n                    return cols[k]\n        for key in keys:\n            for k in cols:\n                if key in k:\n                    return cols[k]\n        return None\n\n    col_ltm = find_col([], contains=[\"p\", \"e\", \"ltm\"]) or find_col([\"trailing p\", \"ttm p\", \"pe ltm\", \"trailing pe\"]) \n    col_ntm = find_col([], contains=[\"p\", \"e\", \"ntm\"]) or find_col([\"forward p\", \"pe ntm\", \"forward pe\"]) \n    col_yld = find_col([\"dividend\", \"div yield\", \"yield %\", \"div%\", \"dyield\"]) \n    col_mcap = find_col([\"market cap\", \"market capitalization\", \"mkt cap\", \"marketcap\"]) \n    col_wt = find_col([\"% of index\", \"% index\", \"index weight\", \"weight\", \"spx weight\", \"s&p weight\"]) \n\n    # Helper to coerce numeric\n    def to_num(s):\n        return pd.to_numeric(s, errors='coerce')\n\n    metrics = []\n    # P/E LTM\n    if col_ltm and col_ltm in df:\n        s = to_num(df[col_ltm])\n        valid = s.dropna()\n        if len(valid) > 0:\n            ok = valid.between(-100, 500).mean()  # allow negative for losses, cap extreme\n            metrics.append(ok)\n    # P/E NTM\n    if col_ntm and col_ntm in df:\n        s = to_num(df[col_ntm])\n        valid = s.dropna()\n        if len(valid) > 0:\n            ok = valid.between(-50, 200).mean()\n            metrics.append(ok)\n    # Dividend Yield\n    if col_yld and col_yld in df:\n        s = to_num(df[col_yld])\n        valid = s.dropna()\n        if len(valid) > 0:\n            # normalize if in percent units\n            med = valid.median()\n            if med > 1.5:  # likely %\n                valid = valid / 100.0\n            ok = valid.between(0, 0.25).mean()  # up to 25%\n            metrics.append(ok)\n    # Market Cap\n    if col_mcap and col_mcap in df:\n        s = to_num(df[col_mcap]).abs()\n        valid = s.dropna()\n        if len(valid) > 0:\n            ok = valid.between(1e6, 5e13).mean()\n            metrics.append(ok)\n    # % of Index sum ~ 100%\n    sum_score = None\n    if col_wt and col_wt in df:\n        s = to_num(df[col_wt])\n        valid = s.dropna()\n        if len(valid) > 0:\n            # Normalize percent vs decimal\n            if valid.max() > 1.5:\n                valid = valid / 100.0\n            total = float(valid.sum())\n            # Accept within +/- 5% due to rounding/exclusions\n            diff = abs(total - 1.0)\n            if diff <= 0.05:\n                sum_score = 1.0\n            elif diff <= 0.10:\n                sum_score = 0.5\n            else:\n                sum_score = 0.0\n            metrics.append(sum_score)\n\n    if not metrics:\n        return 0.0, \"Could not locate numeric columns to sanity-check.\"\n\n    avg_ok = float(np.mean(metrics))\n    score = max(0.0, min(1.4, 1.4 * avg_ok))\n    feedback = f\"Metrics checked: {len(metrics)}; average pass rate={avg_ok:.2f}.\"\n    return score, feedback"}, {"type": "code", "name": "Subsector Summary Cross-Checks (Code)", "description": "Compare Subsector Summary to Company Detail: company counts, market cap sums, and % of Index \u2014 within tolerances. Also check total number of distinct companies ~500.", "weight": 0.7, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n    xls_path = context.files.get_path(output.id)\n    try:\n        xls = pd.ExcelFile(xls_path)\n    except Exception as e:\n        return 0.0, f\"Failed to open Excel: {e}\"\n\n    def norm(s):\n        return re.sub(r\"[^a-z0-9]+\", \" \", str(s).lower()).strip()\n\n    # Identify sheets\n    sheets = xls.sheet_names\n    comp_sheet = None\n    sub_sheet = None\n    for s in sheets:\n        ns = norm(s)\n        if comp_sheet is None and any(k in ns for k in [\"company\", \"companies\", \"constituent\", \"detail\", \"holdings\"]):\n            comp_sheet = s\n        if sub_sheet is None and any(k in ns for k in [\"subsector\", \"sub sector\", \"sub industry\", \"sub industry\", \"sub industry summary\", \"subsector summary\", \"sub industry summary\", \"subindustry\"]):\n            sub_sheet = s\n    if comp_sheet is None:\n        comp_sheet = sheets[0]\n    if sub_sheet is None:\n        return 0.0, \"No Subsector Summary sheet found.\"\n\n    try:\n        dfc = pd.read_excel(xls_path, sheet_name=comp_sheet)\n        dfs = pd.read_excel(xls_path, sheet_name=sub_sheet)\n    except Exception as e:\n        return 0.0, f\"Failed reading sheets: {e}\"\n\n    # Map columns (fuzzy)\n    ccols = {norm(c): c for c in dfc.columns}\n    scols = {norm(c): c for c in dfs.columns}\n\n    def find(cols_map, contains=None, any_sub=None):\n        if contains:\n            for k in cols_map:\n                if all(s in k for s in contains):\n                    return cols_map[k]\n        if any_sub:\n            for k in cols_map:\n                if any(s in k for s in any_sub):\n                    return cols_map[k]\n        return None\n\n    c_sub = find(ccols, any_sub=[\"sub sector\", \"subsector\", \"sub industry\", \"gics sub\"]) or list(dfc.columns)[0]\n    c_mcap = find(ccols, any_sub=[\"market cap\", \"market capitalization\", \"mkt cap\"]) \n    c_wt   = find(ccols, any_sub=[\"% of index\", \"% index\", \"index weight\", \"weight\", \"spx weight\"]) \n    c_tick = find(ccols, any_sub=[\"ticker\", \"symbol\", \"ric\"]) or list(dfc.columns)[0]\n\n    s_sub = find(scols, any_sub=[\"sub sector\", \"subsector\", \"sub industry\", \"gics sub\"]) or list(dfs.columns)[0]\n    s_nco = find(scols, any_sub=[\"no of companies\", \"# companies\", \"count\", \"companies\"]) \n    s_mcap= find(scols, any_sub=[\"market cap\", \"market capitalization\", \"mkt cap\"]) \n    s_wt  = find(scols, any_sub=[\"% of index\", \"% index\", \"index weight\", \"weight\"]) \n\n    # Prepare groups\n    dfc_sub = dfc[[c_sub] + ([c_mcap] if c_mcap in dfc else []) + ([c_wt] if c_wt in dfc else []) + ([c_tick] if c_tick in dfc else [])].copy()\n    dfc_sub.columns = [\"sub\", \"mcap\" if c_mcap in dfc else \"mcap_\", \"wt\" if c_wt in dfc else \"wt_\", \"tic\"][:len(dfc_sub.columns)]\n\n    # Company counts\n    comp_counts = dfc_sub.groupby(\"sub\")[\"tic\"].nunique() if \"tic\" in dfc_sub else dfc_sub.groupby(\"sub\").size()\n\n    # Market cap sums\n    if \"mcap\" in dfc_sub:\n        mc = pd.to_numeric(dfc_sub[\"mcap\"], errors='coerce').dropna()\n        dfc_sub.loc[mc.index, \"mcap\"] = mc\n        mcap_sum = dfc_sub.groupby(\"sub\")[\"mcap\"].sum()\n    else:\n        mcap_sum = None\n\n    # Weights sums\n    if \"wt\" in dfc_sub:\n        wt = pd.to_numeric(dfc_sub[\"wt\"], errors='coerce').dropna()\n        if len(wt) and wt.max() > 1.5:\n            dfc_sub.loc[wt.index, \"wt\"] = wt / 100.0\n        wts = pd.to_numeric(dfc_sub[\"wt\"], errors='coerce')\n        wt_sum = dfc_sub.assign(w=wts).groupby(\"sub\")[\"w\"].sum()\n    else:\n        wt_sum = None\n\n    # Build summary frame\n    dfs2_cols = {}\n    for k,v in scols.items():\n        dfs2_cols[k] = v\n    sub_key = s_sub\n    df_sum = dfs[[s_sub] + ([s_nco] if s_nco in dfs else []) + ([s_mcap] if s_mcap in dfs else []) + ([s_wt] if s_wt in dfs else [])].copy()\n    df_sum.columns = [\"sub\", \"nco\" if s_nco in dfs else \"nco_\", \"mcap\" if s_mcap in dfs else \"mcap_\", \"wt\" if s_wt in dfs else \"wt_\"][:len(df_sum.columns)]\n\n    # Join and compare\n    joined = df_sum.set_index(\"sub\").copy()\n    parts = []\n\n    # Count comparison\n    if \"nco\" in joined:\n        comp_series = comp_counts.reindex(joined.index).fillna(0)\n        nco = pd.to_numeric(joined[\"nco\"], errors='coerce').fillna(0)\n        cnt_ok = (comp_series == nco).mean() if len(joined) else 0\n        parts.append(cnt_ok)\n    # Market cap comparison (10% tolerance)\n    if \"mcap\" in joined and mcap_sum is not None and len(mcap_sum) > 0:\n        left = mcap_sum.reindex(joined.index)\n        right = pd.to_numeric(joined[\"mcap\"], errors='coerce')\n        both = left.notna() & right.notna()\n        rel_err = (abs(left[both] - right[both]) / right[both].replace(0, np.nan)).replace([np.inf, -np.inf], np.nan).dropna()\n        mc_ok = (rel_err <= 0.10).mean() if len(rel_err) else 0\n        parts.append(mc_ok)\n    # Weight comparison (5% tolerance on absolute diff)\n    if \"wt\" in joined and wt_sum is not None and len(wt_sum) > 0:\n        left = wt_sum.reindex(joined.index)\n        right = pd.to_numeric(joined[\"wt\"], errors='coerce')\n        # normalize right if likely in %\n        if right.max() > 1.5:\n            right = right / 100.0\n        both = left.notna() & right.notna()\n        abs_diff = (left[both] - right[both]).abs()\n        wt_ok = (abs_diff <= 0.05).mean() if both.any() else 0\n        parts.append(wt_ok)\n\n    # Sum of summary weights ~ 100%\n    sum_ok = 0\n    if \"wt\" in joined:\n        w = pd.to_numeric(joined[\"wt\"], errors='coerce').dropna()\n        if len(w):\n            if w.max() > 1.5:\n                w = w / 100.0\n            tot = float(w.sum())\n            sum_ok = 1.0 if abs(tot - 1.0) <= 0.02 else (0.5 if abs(tot - 1.0) <= 0.05 else 0.0)\n            parts.append(sum_ok)\n\n    # Distinct company count ~ 500\n    distinct = dfc[c_tick].nunique() if c_tick in dfc else len(dfc)\n    coverage_ok = 1.0 if distinct >= 480 else (0.5 if distinct >= 430 else 0.0)\n    parts.append(coverage_ok)\n\n    if not parts:\n        return 0.0, \"Insufficient overlapping columns to compare.\"\n\n    avg = float(np.mean(parts))\n    score = max(0.0, min(0.7, 0.7 * avg))\n    fb = f\"Subsector checks combined={avg:.2f}; distinct companies={distinct}.\"\n    return score, fb"}, {"type": "code", "name": "Data Sources Freshness and Methodology Signals (Code)", "description": "Validate that the Data Sources & Methodology sheet includes URLs, dates (2025), and mentions of LTM/NTM and the sub-sector taxonomy.", "weight": 0.3, "code": "import re, json\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n    xls_path = context.files.get_path(output.id)\n    try:\n        xls = pd.ExcelFile(xls_path)\n    except Exception as e:\n        return 0.0, f\"Failed to open Excel: {e}\"\n\n    def norm(s):\n        import re\n        return re.sub(r\"[^a-z0-9]+\", \" \", str(s).lower()).strip()\n\n    # Find sources/method sheet\n    cand = None\n    for s in xls.sheet_names:\n        ns = norm(s)\n        if any(k in ns for k in [\"source\", \"method\", \"methodology\"]):\n            cand = s\n            break\n    if cand is None:\n        return 0.0, \"No Data Sources & Methodology sheet found.\"\n\n    try:\n        df = pd.read_excel(xls_path, sheet_name=cand, dtype=str)\n    except Exception as e:\n        return 0.0, f\"Failed to read sources sheet: {e}\"\n\n    # Concatenate all text\n    txt = \" \".join([\" \".join(map(str, row.dropna().tolist())) for _, row in df.iterrows()])\n    low = txt.lower()\n\n    # Checks\n    url_ok = bool(re.search(r\"https?://\", low))\n    year_ok = (\"2025\" in low)\n    ltm_ok = (\"ltm\" in low or \"trailing\" in low or \"ttm\" in low)\n    ntm_ok = (\"ntm\" in low or \"forward\" in low)\n    gics_ok = (\"gics\" in low or \"sub-sector\" in low or \"subsector\" in low or \"sub-industry\" in low)\n\n    parts = [url_ok, year_ok, ltm_ok and ntm_ok, gics_ok]\n    avg = sum(1 for p in parts if p) / 4.0\n    score = 0.3 * avg\n    fb = f\"Sources flags: URL={url_ok}, year2025={year_ok}, LTM&NTM={ltm_ok and ntm_ok}, taxonomy={gics_ok}.\"\n    return score, fb"}, {"type": "llm_judge", "name": "Mean-Reversion Screen Presence (LLM)", "description": "Check that the workbook visibly includes Above/Below classification fields for both LTM and NTM relative to the historical 15\u201320x range, enabling quick screening.", "weight": 0.2, "judge_prompt": "Check the Excel workbook (visually) for screening elements tied to mean reversion:\n- In the Company Detail sheet, look for columns that classify whether each company is ABOVE or BELOW the historical index average range (around 15\u201320x) for BOTH LTM and NTM (e.g., columns named \"Above/Below Index Avg (LTM)\" and \"Above/Below Index Avg (NTM)\").\n- Alternatively, a dedicated screen/pivot that clearly flags ABOVE vs BELOW for both LTM and NTM is acceptable if it\u2019s clearly labeled and filterable.\n\nScoring (out of 0.2):\n- 0.2: Both LTM and NTM classifications are present and clearly labeled for screening.\n- 0.1: Only one of LTM or NTM classification present.\n- 0.0: No visible classification related to the 15\u201320x historical range.", "expectation": "Clear, filterable ABOVE/BELOW classification for both LTM and NTM mean-reversion screens."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Client Usability", "description": "Holistic quality: presentation polish, usability for seniors/clients, and clarity for independent use.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Usability (LLM)", "description": "Assess the workbook for polish, readability, and client-ready usability.", "weight": 2.0, "judge_prompt": "Evaluate overall presentation quality and usability of the Excel workbook. Consider:\n- Clarity and professionalism: consistent fonts, readable headers, proper number formats (P/E as numbers, yields and weights as %), thousand separators for Market Cap.\n- Usability: filters enabled, freeze panes on header rows, sensible default sorting (e.g., by Sector/Sub-Sector then Company), and clear instructions in the Overview.\n- Navigability: descriptive sheet names, minimal clutter, and clear labeling.\n- Insight facilitation: counts of above/below historical average in the Overview, optional Pivots & Screens that make exploring easy, and presence of simple conditional formatting to highlight above/below.\n- Currency and disclaimers: visible as-of date (around April 11, 2025) and a short disclaimer about data sources/limitations.\n\nScoring (out of 2.0):\n- 2.0: Highly professional, well-formatted, filterable, and immediately useful to client/seniors.\n- 1.0: Generally usable with minor formatting or navigation issues.\n- 0.0: Sloppy formatting, difficult to use, or missing basic usability elements.", "expectation": "Client-ready Excel with clean formatting, filters, clear overview, and easy navigation."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "55ddb773-23a4-454c-8704-d432fe1b99d9", "rubric": {"category_name": "HOA Violation Inspection Questionnaire Form (Master Association)", "rationale": "This rubric enforces a self-documenting, verifiable PDF questionnaire form tailored for a master association and seven sub associations. Stage 1 (LLM-only) mandates exact structural elements and visible form features so that verification is possible. Stage 2 applies deterministic code checks on extracted PDF text to confirm required prompts, Y/N circle options, and section presence. Stage 3 uses LLM judgment for professional quality, breadth, and applicability across mixed association types (single-family, condo, townhouse). The rubric prioritizes structure first, then correctness of inclusion, then overall quality and usability for a third-party inspection company.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (STRUCTURE ONLY)", "description": "LLM-only gate to confirm the output is a properly structured PDF questionnaire form with required sections, fields, and visual fill-in elements. Do not assess content quality or calculation correctness\u2014only presence and format.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.2, "rules": [{"type": "llm_judge", "name": "PDF Form Structure and Required Sections Present", "description": "Checks if the output is a PDF questionnaire form with all mandated sections and structural elements to enable verification and use by an inspection vendor.", "weight": 6.0, "judge_prompt": "You are evaluating whether the candidate output is a properly structured PDF questionnaire form suitable for sub associations to complete and provide to a master association and its inspection company. Only check the presence/structure of elements\u2014do not assess content quality beyond existence and basic layout.\n\nFormat Requirements:\n- Must be a PDF (not Word, not Excel, not plain text). A multi-page layout is acceptable but not required. Professional, readable formatting.\n- The document should look like a fillable questionnaire form with lined spaces or clear input areas for responses.\n\nRequired Sections and Fields (accept close synonyms for headers):\nA) Association Information (or similar):\n   - Lined space or input fields for ALL of the following:\n     1) Sub Association Name\n     2) Type of Association (e.g., single-family, condo, townhouse)\n     3) Access Codes (e.g., gate, entry, call box)\n     4) CAM Name (Community Association Manager) and Phone Number\n     5) Number of Homes/Units\n     6) Whether the community fines: include Y/N with an instruction to \u201ccircle one\u201d (or checkbox/radio equivalents clearly labeled for Yes/No)\n     7) Picture requirement for violations: include Y/N with an instruction to \u201ccircle one\u201d (or checkbox/radio equivalents clearly labeled for Yes/No)\n\nB) Violations Catalog (or Violations Matrix/Types/Guideline):\n   - A section that enumerates violation types/categories, each with clearly visible sub-lines under the main violation for qualifying questions and/or details (e.g., prompts like \u201cIf yes, specify:\u201d, \u201cDetails:\u201d, or blank lines under each item for association-specific parameters). The intent is that each violation type has its own space for additional questions/details.\n\nC) Architectural Regulations:\n   - Each architectural item/question listed on its own line (line-per-item format), with several additional blank lines for association-specific architectural items.\n\nD) Community-Specific Additions:\n   - A short section with a few blank lines for any extra, community-specific rules or instructions not otherwise captured.\n\nScoring Guidance (structure only):\n- 6.0: PDF format AND all sections A\u2013D present with correct structural elements (lined spaces/fields, Y/N with circle-one or equivalent, violations listed with subordinate fill-in lines, architectural items line-by-line, and blank lines for community-specific additions).\n- 4.5\u20135.5: PDF format and nearly complete; one minor structural element missing (e.g., circle-one text present but only on one of the Y/N items, or slightly fewer blank lines than requested).\n- 2.0\u20134.0: PDF format but multiple structural elements missing (e.g., no clear Violations Catalog structure with sub-lines, architectural items not line-by-line, or missing several required header fields).\n- 0.0: Not a PDF OR missing most core sections, making verification impossible.\n\nOnly check presence and structural completeness sufficient for later verification\u2014do not judge content depth or writing quality.", "expectation": "A cleanly structured PDF questionnaire form with: Association Information (all fields + Y/N circle-one items), a Violations Catalog listing types with per-type fill-in lines for qualifying details, an Architectural Regulations list with line-per-item and extra blank lines, and a Community-Specific Additions area with blank lines."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness Verification (Textual Deterministic Checks)", "description": "Code-based checks to confirm the presence of required prompts, Y/N options with circle-one guidance, violations coverage cues, and architectural section cues using extracted PDF text. Flexible matching and robust error handling.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Header Fields Presence", "description": "Verify the PDF text includes prompts for all required header fields (name, type, access codes, CAM name, phone, number of homes/units).", "weight": 2.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output found.\"\n\n    # Extract text (try PDF, then DOCX, then plain text fallback)\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        text = \"\"\n    if not text:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text and getattr(output, 'is_text_format', False):\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text:\n        return 0.0, \"Unable to extract text from document.\"\n\n    s = text.lower()\n\n    def any_in(keys):\n        return any(k in s for k in keys)\n\n    checks = {\n        'sub_association_name': any_in(['sub association name', 'sub-association name', 'association name', 'community name']),\n        'association_type': any_in(['type of association', 'association type', 'community type']),\n        'access_codes': any_in(['access code', 'gate code', 'entry code', 'call box code', 'access codes']),\n        'cam_name': any_in(['cam name', 'community association manager', 'manager name']),\n        'phone': any_in(['phone', 'telephone', 'tel:']),\n        'number_of_homes': any_in(['number of homes', 'number of units', 'unit count', 'total homes', 'total units', 'home count'])\n    }\n\n    passed = sum(1 for v in checks.values() if v)\n    total = len(checks)\n    score = passed / total if total else 0.0\n\n    missing = [k for k, v in checks.items() if not v]\n    feedback = f\"Header fields found {passed}/{total}. Missing: {', '.join(missing) if missing else 'None'}.\"\n    return score, feedback"}, {"type": "code", "name": "Y/N Circle-One Options for Fines and Picture Requirement", "description": "Verify presence of Y/N (or Yes/No) prompts with an instruction to circle/select, for both fines and picture requirements.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output found.\"\n\n    # Extract text\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        text = \"\"\n    if not text:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text:\n        return 0.0, \"Unable to extract text from document.\"\n\n    s = text.lower()\n\n    yn_present = any(x in s for x in ['y/n', 'yes/no', 'yes / no'])\n    circle_present = any(x in s for x in ['circle one', 'please circle', 'select one'])\n\n    fines_present = re.search(r'\\bfines?\\b', s) is not None\n    picture_present = re.search(r'\\b(picture|photo|photograph|image)s?\\b', s) is not None and any(x in s for x in ['require', 'requirement', 'required'])\n\n    fines_ok = fines_present and yn_present and circle_present\n    pics_ok = picture_present and yn_present and circle_present\n\n    score = (1.0 if fines_ok else 0.0 + 1.0 if pics_ok else 0.0) / 2.0\n\n    detail = []\n    if not fines_ok:\n        detail.append('Fines Y/N with circle/select not clearly found')\n    if not pics_ok:\n        detail.append('Picture requirement Y/N with circle/select not clearly found')\n    feedback = 'Both Y/N circle-one items present' if not detail else '; '.join(detail)\n\n    return score, feedback"}, {"type": "code", "name": "Violations Catalog Structure Signals", "description": "Check for a violations catalog/list with evidence of qualifying questions or details prompts, and coverage of typical HOA categories.", "weight": 2.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output found.\"\n\n    # Extract text\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        text = \"\"\n    if not text:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text:\n        return 0.0, \"Unable to extract text from document.\"\n\n    s = text.lower()\n\n    header_ok = ('violation' in s and any(w in s for w in ['catalog', 'types', 'matrix', 'guideline', 'list'])) or 'violations catalog' in s or 'violation types' in s\n\n    question_signals = s.count('?')\n    if_yes = s.count('if yes')\n    detail_signals = sum(s.count(w) for w in ['details:', 'specify:', 'describe:', 'notes:'])\n    questions_ok = (question_signals >= 5) or (if_yes >= 2) or (detail_signals >= 3)\n\n    categories = ['landscaping', 'parking', 'trash', 'exterior', 'signage', 'pets', 'noise', 'rental', 'vehicle', 'architectural', 'balcony', 'patio', 'fence', 'roof', 'paint', 'window', 'door']\n    category_hits = sum(1 for c in categories if c in s)\n    coverage_ok = category_hits >= 5\n\n    # Weighted within this rule: header 0.4, questions 0.3, coverage 0.3\n    score = (0.4 if header_ok else 0.0) + (0.3 if questions_ok else 0.0) + (0.3 if coverage_ok else 0.0)\n\n    feedback = f\"Header:{'ok' if header_ok else 'missing'}, Questions/Details:{'ok' if questions_ok else 'weak'}, Coverage hits:{category_hits}\"\n    return score, feedback"}, {"type": "code", "name": "Architectural Regulations Listed Line-by-Line Signals", "description": "Confirm an Architectural section exists and suggests line-by-line items and additional space for extras.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output found.\"\n\n    # Extract text\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        text = \"\"\n    if not text:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text:\n        return 0.0, \"Unable to extract text from document.\"\n\n    s = text.lower()\n\n    arch_header = any(w in s for w in ['architectural regulations', 'architectural', 'arc', 'design review'])\n    # Look for multiple likely architectural item keywords as a proxy for line-by-line listing\n    arch_items = ['fence', 'roof', 'paint', 'color', 'window', 'door', 'satellite', 'solar', 'landscape', 'hardscape', 'material', 'finish', 'trim']\n    item_hits = sum(1 for w in arch_items if w in s)\n    items_ok = item_hits >= 3\n\n    extras_ok = any(w in s for w in ['additional', 'other', 'extra']) and 'architect' in s\n\n    # Weighted: header 0.5, items list 0.4, extras space 0.1\n    score = (0.5 if arch_header else 0.0) + (0.4 if items_ok else 0.0) + (0.1 if extras_ok else 0.0)\n\n    feedback = f\"Architectural header:{'ok' if arch_header else 'missing'}, item keywords:{item_hits}, extras:{'ok' if extras_ok else 'missing'}\"\n    return score, feedback"}, {"type": "code", "name": "Community-Specific Additions Blank Lines Indicator", "description": "Check for an additional section for community-specific items and indicators for space to fill (e.g., prompts like Notes/Additional/Other).", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output found.\"\n\n    # Extract text\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        text = \"\"\n    if not text:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text:\n        return 0.0, \"Unable to extract text from document.\"\n\n    s = text.lower()\n\n    has_additional = any(w in s for w in ['additional', 'community-specific', 'other']) and any(w in s for w in ['lines', 'notes', 'comments', 'space', 'provide details', 'specify'])\n\n    score = 1.0 if has_additional else 0.0\n    feedback = 'Community-specific additions section present' if has_additional else 'No clear community-specific additions section found'\n    return score, feedback"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Fitness for Purpose", "description": "LLM judges evaluate professional presentation, clarity, and breadth for mixed association types and inspection vendor usability.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Layout and Fillability", "description": "Assess if the PDF looks professional, is easy to complete, and contains clear instructions, labels, and user-friendly elements (lined fields, checkboxes/radios, consistent spacing).", "weight": 3.0, "judge_prompt": "Evaluate the PDF questionnaire for professional presentation and ease of completion. Consider:\n- Visual clarity and professional formatting (readable fonts, alignment, spacing, headers)\n- Clear labeling of every field and section\n- Obvious input affordances (lined spaces, boxes, checkfields, yes/no circle or check prompts)\n- Brief instructions where needed (e.g., \u201ccircle one,\u201d \u201cattach photos,\u201d \u201cif yes, specify\u201d)\n- Consistent page layout and section hierarchy\nScoring:\n- 3.0: Highly professional, clear, well-labeled, and easy to complete throughout\n- 2.0: Generally professional and usable with minor issues\n- 1.0: Usable but several clarity/formatting issues\n- 0.0: Sloppy or confusing layout that impedes completion", "expectation": "A professional, easy-to-complete PDF form with clear labels, visible input areas, consistent formatting, and succinct instructions."}, {"type": "llm_judge", "name": "Coverage and Applicability to Mixed Association Types", "description": "Judge whether the form\u2019s Violations Catalog and Architectural sections are broadly applicable across sub associations (single-family, condo, townhouse) and operationally useful to a third-party inspection vendor.", "weight": 3.0, "judge_prompt": "Assess the breadth and applicability of the questionnaire content to a master association overseeing diverse sub associations (single-family, condos, townhomes). Consider:\n- Does the Violations Catalog cover a wide range of common HOA categories (e.g., landscaping/yard, parking/vehicles, trash/enclosures, exterior maintenance, signage, pets, noise, rentals/leases, balconies/patios, fences, colors/paint, windows/doors, roofs, satellite/solar where applicable)?\n- For each category, are there clearly placed prompts for qualifying details (e.g., time limits, dimensions, allowed/prohibited conditions, exceptions, photo requirements)?\n- Does the Architectural Regulations section present items line-by-line and enable association-specific parameters (blank lines for adds)?\n- Would a third-party inspection vendor reasonably be able to use this form as a guideline without additional clarification?\nScoring:\n- 3.0: Broad, well-structured coverage for mixed association types with clear qualifying prompts usable by vendors\n- 2.0: Good coverage with a few gaps but still broadly usable\n- 1.0: Limited coverage or unclear prompts, vendor usability questionable\n- 0.0: Narrow or misaligned coverage; not useful for diverse sub associations", "expectation": "Comprehensive, cross-type violation coverage with per-type qualifying prompts and architectural items line-by-line plus room for association-specific additions, usable by inspection vendors."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "8a7b6fca-60cc-4ae3-b649-971753cbf8b9", "rubric": {"category_name": "Manufacturing \u2022 Industrial Engineering \u2014 Inbound Parcel Process Mapping (Automation vs Manual)", "rationale": "This rubric follows the self-documenting, staged approach. Stage 1 is a strict LLM gate that enforces a precise, verifiable diagram structure in a PDF/DOCX. Stage 2 mixes code and LLM checks to verify logical routing and the presence of critical concepts (compatibility decision, failure reroute, scanning). Stage 3 assesses professional presentation and stakeholder readiness. The output is a single file-based artifact (PDF preferred) suitable for leadership alignment and standardization.", "max_total_score": 15.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (Diagram Structure)", "description": "LLM-only gate ensuring the output is a proper process map diagram in a PDF/DOCX with the required structural elements: lanes for Automation and Manual, start/end, tasks (including scanning), decisions (compatibility and failure), clear routing including reroute to Manual on failure.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.5, "rules": [{"type": "llm_judge", "name": "Process Map Format and Structure Requirements", "description": "Verify that the submission is a valid PDF/DOCX containing a process map that clearly separates automation-compatible and manual processing paths, with standard flowchart elements and explicit failure rerouting to manual.", "weight": 6.0, "judge_prompt": "You are evaluating whether the candidate output satisfies strict STRUCTURAL requirements for a professional process map. Only check FORMAT and PRESENCE of required elements (do not judge content quality or logic beyond presence). Use the rendered PDF/DOCX for visual inspection.\n\nREQUIREMENTS (all refer to the same diagram):\nFormat\n- The file must be a PDF or DOCX. PDF preferred (no penalty if DOCX is used).\n- It must contain a process map diagram (not just paragraphs of text). A single page is acceptable.\n\nCore Structure\n1) Two distinct processing lanes (swimlanes or clearly separated areas), labeled or visibly identifiable as:\n   - Automation (e.g., \u201cAutomation\u201d, \u201cAutomated Systems\u201d, \u201cConveyor/Sorter\u201d)\n   - Manual (e.g., \u201cManual\u201d, \u201cManual Handling/Processing\u201d)\n2) Start and End points shown with standard terminator symbols (ovals/rounded rectangles) or clearly labeled start/end.\n3) Tasks (rectangles) that include at least: receiving/unloading/induction and scanning (barcode/OCR/RFID) as separate or clearly indicated steps.\n4) At least two decision points (diamond shapes or labeled decision boxes):\n   a) Automation compatibility decision (e.g., compatible vs incompatible/non-conveyable/oversize/fragile).\n   b) Automation failure decision (e.g., no-read/jam/overflow/breakdown) with a branch to manual.\n5) Routing and handoffs:\n   - A branch that routes compatible pieces through the automation lane toward automated sortation and outbound.\n   - A branch that routes incompatible or failed pieces into the manual lane.\n   - A visible cross-lane handoff arrow/connector from automation to manual for failure handling.\n\nOptional but Helpful (do NOT require for full credit):\n- A legend explaining symbols.\n- A clear title (e.g., \u201cClearbend Logistics Hub \u2013 Inbound Piece Flow\u201d).\n\nSCORING (Score from 0.0 to 6.0):\n- 6.0: Valid PDF/DOCX with a clear diagram including BOTH lanes; visible Start/End; distinct scanning and receiving/induction tasks; TWO decision points (compatibility and failure); clear routing of compatible items through automation; clear reroute of failed/incompatible items to manual; cross-lane handoff visible.\n- 5.0: All required elements present except one minor omission (e.g., lanes not explicitly labeled but obviously separated; or Start/End symbols present but unlabeled).\n- 3.0: Diagram present but missing one major required element (e.g., no failure reroute to manual OR no compatibility decision OR no Start/End OR no distinct scanning task).\n- 1.0\u20132.0: Diagram present but missing multiple major elements or lanes are not separated.\n- 0.0: Not a PDF/DOCX; or no diagram (text-only); or diagram is unrelated to the described process.\n\nReturn only a numeric score per the above, based strictly on structural presence and format.", "expectation": "A single PDF process map with clear Automation and Manual lanes, Start/End, tasks including scanning, two decision diamonds (compatibility and failure), and explicit reroute from automation to manual on failure."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness Verification (Logic + Evidence)", "description": "Now that the diagram shape is validated, verify with code and LLM that key logical elements are present: compatibility decision, failure rerouting to manual, scanning, and end-to-end flow validity.", "is_required": false, "max_points": 4.5, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Textual Signal Coverage (Automation vs Manual, Decisions, Scanning)", "description": "Checks PDF/DOCX extracted text for required conceptual signals to corroborate the diagram: Automation lane terms, Manual lane terms, compatibility cues, failure/reroute cues, scanning, and start/end indicators.", "weight": 2.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n\n    Returns:\n        float (score) or tuple[float, str] (score, feedback)\n    \"\"\"\n    weight = 2.0\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No primary document output detected. Expected PDF/DOCX process map.\"\n\n    text = \"\"\n    try:\n        # Prefer PDF text; fall back to DOCX\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = context.files.read_docx_text(output.id)\n    except Exception:\n        return 0.0, \"Could not extract text from the document.\"\n\n    if not text or not isinstance(text, str):\n        return 0.0, \"Empty or unreadable document text content.\"\n\n    t = text.lower()\n\n    # Token families\n    automation_tokens = [\n        'automation', 'automated', 'conveyor', 'sorter', 'auto sort', 'induct', 'singulate'\n    ]\n    manual_tokens = [\n        'manual', 'hand', 'non-conveyable', 'non conveyable', 'ncv', 'exception lane', 'manual induction', 'manual handling'\n    ]\n    compat_tokens = [\n        'compat', 'incompatible', 'nonconvey', 'non-convey', 'oversize', 'overweight', 'irregular', 'fragile', 'spec'\n    ]\n    failure_tokens = [\n        'fail', 'failure', 'jam', 'no-read', 'noread', 'mis-sort', 'missort', 'divert', 'diversion', 'reject', 'reroute', 'overflow', 'breakdown'\n    ]\n    scan_tokens = [\n        'scan', 'barcode', 'label', 'ocr', 'rfid', 'read'\n    ]\n    start_tokens = ['start', 'begin', 'inbound', 'receive', 'receiving', 'unload', 'induction']\n    end_tokens = ['end', 'finish', 'complete', 'outbound', 'ship', 'dispatch', 'staging', 'sorted']\n\n    def present(tokens):\n        return any(tok in t for tok in tokens)\n\n    checks = {\n        'automation_lane_terms': present(automation_tokens),\n        'manual_lane_terms': present(manual_tokens),\n        'compatibility_decision_terms': present(compat_tokens),\n        'failure_reroute_terms': present(failure_tokens),\n        'scanning_terms': present(scan_tokens),\n        'start_present': present(start_tokens),\n        'end_present': present(end_tokens),\n    }\n\n    # Score: 6 sub-features counted as follows: require both start and end as two separate checks\n    # Count fulfilled signals\n    total_signals = 7\n    hits = sum(1 for v in checks.values() if v)\n    frac = hits / total_signals\n    score = round(weight * frac, 4)\n\n    feedback = (\n        f\"Signals found {hits}/{total_signals}. \"\n        f\"Automation:{checks['automation_lane_terms']} Manual:{checks['manual_lane_terms']} \"\n        f\"Compat:{checks['compatibility_decision_terms']} Failure/Reroute:{checks['failure_reroute_terms']} \"\n        f\"Scan:{checks['scanning_terms']} Start:{checks['start_present']} End:{checks['end_present']}\"\n    )\n\n    return score, feedback\n"}, {"type": "llm_judge", "name": "Logical Routing Verification (Automation to Manual on Failure)", "description": "Check that the logic is correct: items are classified for automation compatibility; failed or incompatible items are routed into manual processing; automated flow reaches a valid end state; manual flow reaches a valid end state.", "weight": 2.5, "judge_prompt": "Evaluate the diagram\u2019s logical routing (not aesthetics). Confirm these conditions are visibly satisfied:\n\nRequired logical behaviors:\n1) Intake classification for automation compatibility (explicit Yes/No or equivalent) with incompatible items directed to the Manual lane.\n2) Automation failure handling: a decision/event for no-read/jam/overflow/etc. that visibly reroutes affected items into the Manual lane (a cross-lane handoff or equivalent connector).\n3) Automation lane path: Receiving/Induction \u2192 Scan \u2192 Automated Sort \u2192 Outbound/End (or equivalent step sequence reaching an end state).\n4) Manual lane path: Intake from incompatibility AND from automation failure \u2192 Manual processing steps (e.g., inspect/repack/label/scan) \u2192 Outbound/End (or a clearly labeled end state).\n5) No dead ends for normal flows; each path reaches an End.\n\nScoring (0.0 to 2.5):\n- 2.5: All five behaviors are clearly present.\n- 1.5: Four behaviors are clearly present; one is ambiguous or missing.\n- 0.5: Only two\u2013three behaviors present; multiple issues.\n- 0.0: One or zero behaviors present; or logic contradicts requirements.\n\nOnly judge logical routing, not presentation quality.", "expectation": "Diagram shows both classification and failure reroute leading to Manual, with both lanes terminating in valid end states and no dead ends."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Communication", "description": "Professional presentation quality and stakeholder readiness for cross-functional alignment and leadership review.", "is_required": false, "max_points": 4.5, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Visual Clarity and Professionalism", "description": "Assess whether the diagram is clean, readable, and uses professional conventions (consistent symbols, labels, minimal line crossings, clear lane headers, optional legend/title).", "weight": 2.5, "judge_prompt": "Assess presentation quality of the process map:\n- Readability: text legible at normal zoom, spacing adequate, minimal overlaps.\n- Conventions: consistent use of standard symbols (terminators, rectangles for tasks, diamonds for decisions), clear connectors/arrowheads.\n- Structure: lanes clearly labeled (Automation vs Manual) and well-aligned, minimal crossing lines, logical left-to-right or top-to-bottom flow.\n- Documentation: clear title, optional legend, version/date (nice-to-have, not required for passing).\n\nScoring (0.0 to 2.5):\n- 2.5: Highly professional, clear, and easy to follow.\n- 1.5: Generally clear with minor clutter or minor inconsistencies.\n- 0.5: Hard to read or several inconsistencies but still decipherable.\n- 0.0: Messy/unreadable or not using recognizable symbols.", "expectation": "A clean, professional PDF map with clear lane labels, consistent symbols, readable labels, and tidy connectors."}, {"type": "llm_judge", "name": "Stakeholder Readiness and Operational Usefulness", "description": "Evaluate whether the diagram is immediately useful for aligning cross-functional teams and briefing operational leadership.", "weight": 2.0, "judge_prompt": "Judge the diagram\u2019s usefulness for cross-functional alignment and leadership briefing:\n- Scope clarity: shows the full inbound flow from arrival/receiving to outbound/end, with both automation and manual paths.\n- Handoffs: responsibilities and transitions between automation and manual processing are obvious; exception/failure handling is explicit.\n- Actionability: suitable as a reference for standardizing the workflow; adequately labeled steps that would support SOP creation/training.\n\nScoring (0.0 to 2.0):\n- 2.0: Strong alignment tool; scope and handoffs are clear and it could anchor SOPs/training.\n- 1.0: Generally useful but missing some clarity (e.g., vague step labels or partial scope).\n- 0.0: Too vague or incomplete to aid alignment/leadership.", "expectation": "A diagram that clearly communicates the end-to-end inbound flow and handoffs so teams can align and standardize the process."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "0419f1c3-d669-45d0-81cd-f4d5923b06a5", "rubric": {"category_name": "Real Estate Property Management \u2014 PIP for Superintendent (NY, Multi-family)", "rationale": "This rubric enforces a self-documenting, verifiable Performance Improvement Plan (PIP) deliverable. Stage 1 uses an LLM gate to mandate a precise DOCX/PDF structure with tables, objectives, training, timeline, consequences, and signatures. Stage 2 mixes code rules (procedural compliance, dates, standards coverage, objectives count, training validity, signatures) with an LLM alignment check (do objectives and training address the identified gaps). Stage 3 evaluates professional quality and appropriateness for a property management audience. The approach ensures the agent proves correctness via artifacts that are trivial to verify.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Document Shape Gate (MANDATORY)", "description": "LLM-only gate that verifies the output is a properly structured 2\u20133 page PIP document (DOCX or PDF) with required sections, tables, training plan from allowed resources, 90-day timeline with required milestones, consequences, and signature block. This is a structure-only check, not a content-quality check.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.1, "rules": [{"type": "llm_judge", "name": "PIP Structure and Format Requirement", "description": "Verify the candidate delivered a DOCX or PDF PIP (about 2\u20133 pages) with exact structural elements needed for later verification.", "weight": 3.0, "judge_prompt": "You are evaluating whether the OUTPUT FILE is a properly structured Performance Improvement Plan (PIP) document for superintendent John Miller. Only check PRESENCE and STRUCTURE, not the quality of the content or correctness of calculations.\n\nFormat requirements:\n- Must be a DOCX or PDF document (not Excel, not plain text).\n- Length target: about 2\u20133 pages. Be flexible: 2\u20134 pages is acceptable. Reject 1 page.\n- Professionally formatted with clear section headers.\n\nRequired sections and elements (use flexible matching on section names):\n1) Factual Summary of Performance Gaps\n   - Must reference both: \"Work Order Log\" and \"Resident Complaint Log\" (by name).\n   - Include a KPI summary table (or clearly formatted table) for maintenance performance. The table should have columns covering, at minimum: Metric, Definition (or How Measured), Standard/Benchmark, Current Period Value, Variance (or Gap), Evidence Source. Column names can vary but meanings must be present.\n   - Include a complaint/themes table with columns covering: Theme, Count, % of Total (or share), Representative Examples (or notes).\n2) Specific, Measurable Objectives (3\u20135 items)\n   - A clearly enumerated list (numbered preferred) of 3 to 5 SMART-style objectives.\n3) Support and Resources (Training Plan)\n   - Must list one or more training modules selected ONLY from this allowed list:\n     \u2022 Advanced Plumbing Diagnostics (Online Module)\n     \u2022 HVAC Fundamentals (Online Module)\n     \u2022 NFPA 70E Electrical Safety (Online Module)\n     \u2022 Customer Service & Professionalism (Video Library)\n   - For each selected module, include at least a brief justification linking it to a gap.\n4) Review Period and Timeline\n   - Explicit 90-day PIP period with clear dates using today\u2019s date as start: 07/13/2025.\n   - State the PIP end date (90 days from start): 10/11/2025.\n   - State that all assigned training must be completed within the first 30 days; include the 30-day completion deadline date: 08/12/2025.\n   - Include a weekly 30-minute check-in cadence.\n5) Consequences\n   - A clear statement of consequences if objectives are not met.\n6) Signature Section\n   - Signature lines/blocks for Manager, Employee, and Witness, each with a Date line.\n\nScoring (return a single numeric score for this rule):\n- 3.0: DOCX/PDF, ~2\u20133 pages, and ALL required sections/elements are present (including both tables, allowed training modules, explicit dates 07/13/2025, 08/12/2025, 10/11/2025, weekly 30-minute check-ins, consequences, and 3 signature lines with dates).\n- 2.3: DOCX/PDF and mostly correct structure but 1\u20132 minor elements missing (e.g., one table missing a column, or one of the three dates not explicitly shown, or one signature line missing).\n- 1.5: DOCX/PDF and generally structured, but multiple required elements missing (e.g., missing a required table, missing training plan from allowed list, missing check-in cadence).\n- 0.0: Not DOCX/PDF, or only 1 page, or missing multiple core sections (Performance Gaps, Objectives, Training Plan, Timeline, Consequences, Signature Block).\n\nOnly evaluate presence/structure. Do not judge correctness of data or writing quality.", "expectation": "A 2\u20133 page DOCX/PDF PIP with all required sections, two tables (KPI and Complaints), allowed training modules, explicit 90-day timeline with required dates, consequences, and signature block."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification of Compliance and Internal Consistency", "description": "Code rules validate procedural compliance with company standards, timelines and allowed resources; LLM judge checks cross-referencing between gaps, objectives, and training. Assumes Stage 1 shape passed.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Standards Coverage (4 hours, 72 hours, <5% redo)", "description": "Verify the document explicitly references company standards: acknowledgement within 4 business hours, completion within 72 hours, and a redo rate target below 5%.", "weight": 1.2, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0, 'No document output'\\n\\n    text = ''\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            return 0.0, 'Unable to read document text'\\n\\n    t = text.lower()\\n\\n    # Acknowledgement within 4 business hours\\n    ack_phrase = bool(re.search(r'acknowledg\\w*', t)) and bool(re.search(r'\\b(4|four)\\s+(business\\s+)?hours?\\b', t))\\n\\n    # Completion within 72 hours\\n    comp_phrase = bool(re.search(r'\\b(72|seventy[-\\s]?two)\\s+hours?\\b', t))\\n\\n    # Redo rate < 5%\\n    redo_present = 'redo' in t\\n    five_percent = bool(re.search(r'(\\b<\\s*5%\\b|\\b<=\\s*5%\\b|below\\s+5\\s*%|under\\s+5\\s*%|less\\s+than\\s+5\\s*%|\\b5\\s*percent\\b)', t))\\n    redo_clause = redo_present and five_percent\\n\\n    hits = sum([ack_phrase, comp_phrase, redo_clause])\\n    score = hits / 3.0\\n    feedback = f'Ack 4h: {ack_phrase}, Complete 72h: {comp_phrase}, Redo<5%: {redo_clause}'\\n    return score, feedback\\n"}, {"type": "code", "name": "Timeline Compliance with Standard PIP Procedures", "description": "Verify explicit dates for 90-day period starting 07/13/2025, end 10/11/2025, training due by 08/12/2025, and weekly 30-minute check-ins.", "weight": 1.2, "code": "import re\\n\\nSTART_DATES = ['07/13/2025','7/13/2025','2025-07-13','july 13, 2025','13 july 2025']\\nEND_DATES = ['10/11/2025','10/11/25','2025-10-11','october 11, 2025','11 october 2025']\\nTRAIN_DATES = ['08/12/2025','8/12/2025','2025-08-12','august 12, 2025','12 august 2025']\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0, 'No document output'\\n\\n    text = ''\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            return 0.0, 'Unable to read document text'\\n\\n    t = text.lower()\\n\\n    start_ok = any(s in t for s in START_DATES)\\n    end_ok = any(s in t for s in END_DATES)\\n    train_ok = any(s in t for s in TRAIN_DATES)\\n\\n    checkin_ok = False\\n    if re.search(r'weekly', t) and re.search(r'30\\s*-?\\s*minute', t) and (re.search(r'check[-\\s]?in', t) or 'meeting' in t):\\n        checkin_ok = True\\n\\n    hits = sum([start_ok, end_ok, train_ok, checkin_ok])\\n    score = hits / 4.0\\n    feedback = f'Start:{start_ok}, End:{end_ok}, Training<=30d:{train_ok}, Weekly 30-min check-ins:{checkin_ok}'\\n    return score, feedback\\n"}, {"type": "code", "name": "Training Modules Validity (Allowed List Only)", "description": "Check that at least one recommended training module is from the allowed list; reward more for two or more. Penalize if none are present.", "weight": 0.8, "code": "import re\\n\\nALLOWED = [\\n    'advanced plumbing diagnostics',\\n    'hvac fundamentals',\\n    'nfpa 70e electrical safety',\\n    'customer service & professionalism'\\n]\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0, 'No document output'\\n\\n    text = ''\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            return 0.0, 'Unable to read document text'\\n\\n    t = text.lower()\\n\\n    found = []\\n    for name in ALLOWED:\\n        if name in t:\\n            found.append(name)\\n\\n    if len(found) >= 2:\\n        score = 1.0\\n    elif len(found) == 1:\\n        score = 0.7\\n    else:\\n        # If they at least mention training/modules generically, give small partial\\n        generic = bool(re.search(r'training|module|course', t))\\n        score = 0.3 if generic else 0.0\\n\\n    feedback = f'Allowed modules referenced: {len(found)} \u2014 {found}'\\n    return score, feedback\\n"}, {"type": "code", "name": "Objectives Count and Measurability Signals", "description": "Verify there are 3\u20135 enumerated objectives and that measurable signals (percentages, hours/days, or deadlines) are present.", "weight": 1.0, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0, 'No document output'\\n\\n    text = ''\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            return 0.0, 'Unable to read document text'\\n\\n    t = text\\n    tl = t.lower()\\n\\n    # Count objectives like \"Objective 1\", \"Objective #2\", etc.\\n    obj_matches = re.findall(r'objective\\s*#?\\s*(\\d+)', tl)\\n    nums = set()\\n    for m in obj_matches:\\n        try:\\n            nums.add(int(m))\\n        except Exception:\\n            pass\\n    count = len(nums)\\n\\n    # Alternative: count strong numbered lines if 'objective' not used\\n    if count == 0:\\n        lines = [ln.strip() for ln in tl.splitlines()]\\n        numbered = [ln for ln in lines if re.match(r'^(\\d+)[\\.)]\\s', ln)]\\n        # Heuristic: assume first 3-6 numbered items in an objectives section qualify\\n        if 'objective' in tl or 'objectives' in tl:\\n            count = min(6, len(numbered))\\n\\n    # Measurability presence: look for % or hours/days or date formats anywhere in document\\n    measurable = False\\n    if re.search(r'\\d+\\s*%', tl) or re.search(r'\\b(\\d+\\s*hours?|\\d+\\s*days?)\\b', tl) or re.search(r'\\b\\d{1,2}/\\d{1,2}/\\d{2,4}\\b', tl) or re.search(r'\\b(january|february|march|april|may|june|july|august|september|october|november|december)\\b\\s+\\d{1,2},\\s*\\d{4}', tl):\\n        measurable = True\\n\\n    base = 0.0\\n    if 3 <= count <= 5:\\n        base = 0.8\\n    elif count == 2:\\n        base = 0.5\\n    elif count >= 6:\\n        base = 0.6\\n    elif count == 1:\\n        base = 0.2\\n    else:\\n        base = 0.0\\n\\n    score = min(1.0, base + (0.2 if measurable else 0.0))\\n    feedback = f'Objectives counted: {count}; Measurable signals: {measurable}'\\n    return score, feedback\\n"}, {"type": "code", "name": "Signature Block Presence", "description": "Verify signature lines for Manager, Employee, and Witness with dates.", "weight": 0.3, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0, 'No document output'\\n\\n    text = ''\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            return 0.0, 'Unable to read document text'\\n\\n    tl = text.lower()\\n\\n    def has_sig(role_keywords):\\n        return any((rk in tl and 'signature' in tl) or (rk in tl and 'signed' in tl) for rk in role_keywords)\\n\\n    manager_ok = has_sig(['manager'])\\n    employee_ok = has_sig(['employee'])\\n    witness_ok = has_sig(['witness'])\\n\\n    # Date presence near signatures not robust via text extraction; check generic 'date' token\\n    date_token = 'date' in tl\\n\\n    parts = [manager_ok, employee_ok, witness_ok]\\n    score = sum(parts) / 3.0\\n    # small bump if 'date' token exists\\n    if date_token and score > 0:\\n        score = min(1.0, score + 0.05)\\n\\n    feedback = f'Manager:{manager_ok}, Employee:{employee_ok}, Witness:{witness_ok}, Date token:{date_token}'\\n    return score, feedback\\n"}, {"type": "llm_judge", "name": "Cross-Reference Alignment (Gaps \u2192 Objectives \u2192 Training)", "description": "Judge whether objectives and training selections are clearly and credibly tied to the identified performance gaps and evidence references.", "weight": 0.5, "judge_prompt": "Assess how well the PIP internally aligns its parts. Focus on traceable linkage, not writing quality. Consider:\\n- Do the SMART objectives directly address the specific gaps surfaced in the KPI table and complaint themes?\\n- Do the selected training modules (must be from allowed list) have a clear justification tied to the gaps (e.g., plumbing/HVAC quality issues \u2192 technical training; communication/tenants \u2192 customer service; safety/compliance \u2192 NFPA 70E)?\\n- Is there a clear path from: Gaps (metrics, themes) \u2192 Objectives \u2192 Training/Support \u2192 Timeline milestones?\\n\\nScoring (0.0\u20130.5 for this rule):\\n- 0.5: Strong, explicit cross-references throughout (objectives cite metrics/themes; each training justified by specific gap; timeline milestones aligned to objectives).\\n- 0.35: Mostly aligned with minor gaps (some objectives or one training weakly justified).\\n- 0.2: Partial alignment (objectives generic or training generic).\\n- 0.0: Little to no alignment (objectives unrelated to stated gaps; training off-list or unjustified).", "expectation": "Clear, traceable linkage from evidence to objectives and training, forming a coherent improvement path over the 90-day period."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality", "description": "Evaluate clarity, professionalism, and appropriateness for a property management context in New York multi-family operations.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professionalism, Clarity, and Fitness for Purpose", "description": "Holistic quality assessment: clarity, tone, professional formatting, length, and practicality for use with an employee.", "weight": 2.0, "judge_prompt": "Evaluate overall professional quality of the PIP document. Consider:\\n- Clarity, organization, and readability; suitable 2\u20133 page length.\\n- Professional, fair, and objective tone for a property manager in NY multi-family context.\\n- Practicality: Are instructions actionable? Is the cadence (weekly 30-minute check-ins) and timeline usable?\\n- Formatting: clear headings, tables readable, signature block usable.\\n\\nScoring (0.0\u20132.0 for this rule):\\n- 2.0: Highly professional, clear, and ready for real-world use.\\n- 1.4: Generally professional with minor issues.\\n- 0.8: Adequate but needs refinement.\\n- 0.0: Poorly organized or unprofessional.\\n\\nDo not reassess structural presence (Stage 1) or strict compliance details (Stage 2). Focus on presentation quality and fitness for purpose.", "expectation": "A clear, fair, professional PIP suitable for delivery to the superintendent and HR."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "99ac6944-4ec6-4848-959c-a460ac705c6f", "rubric": {"category_name": "IEM Mobile Setup Proposal (Audio/Video Technicians)", "rationale": "This rubric enforces a self-documenting, verifiable deliverable: a multi-section PDF proposal that includes explicit structure, embedded PNG artifacts (signal-flow diagram and spreadsheet image), retailer links, and a budget-constrained bill of materials. Stage 1 (LLM-only) strictly gates structural compliance. Stage 2 uses code rules to verify key correctness elements that are machine-checkable from the PDF text (budget compliance, retailer links, feature/requirements coverage, component counts, BOM robustness). Stage 3 assesses professional quality and suitability for a touring context.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Structure and Format Gate (LLM Only)", "description": "Gate: Verify the candidate produced a properly structured PDF proposal with the mandated sections, embedded images, and key structural elements enabling verification.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "PDF Format and Structural Completeness", "description": "Check that the output is a PDF with required sections, tables, links, and embedded PNG images per task requirements.", "weight": 4.0, "judge_prompt": "You are evaluating whether the candidate output satisfies STRICT structure and format requirements. Only assess PRESENCE and STRUCTURE, not correctness of prices or technical choices.\n\nRequired file/format:\n- The primary output must be a PDF document (not DOCX/Excel/Markdown).\n- At least 2 pages long.\n- Professionally formatted with clear section headers.\n\nRequired sections and structural elements (flexible with names but content must be clearly present):\n1) Title and Executive Summary / Overview (first page) that restates the core requirements (touring IEM rig for two singers, mid-sized venues/festivals, west coast Summer 2023, portable, under $3,000 budget, 2 IEM packs, analog mixing board acceptable with onboard/built-in digital effects).\n2) Requirements Recap or Constraints section listing functional parameters:\n   - Two singers only (two dynamic vocal mics as inputs).\n   - Analog mixing board preference; onboard digital effects acceptable.\n   - Effects required: compression, reverb, delay (independent IEM mixes).\n   - RF-based IEM operation; portability; usable side-stage/FOH/farther locations; portable surface noted.\n3) Proposed System Overview summarizing the overall solution and topology.\n4) Mixing Board Selection and justification, clearly indicating analog mixer and onboard/built-in effects (compression, reverb, and delay) are available.\n5) IEM System Selection with model(s) and inclusion of 2 bodypack receivers (IEM packs). Include the transmitter. Include explicit counts.\n6) Bill of Materials / Pricing table with columns similar to: [Item | Model | Qty | Unit Price | Extended Price | Retailer/URL]. Must include clickable web links to retailers for both the mixing board and the IEM system (at least two product links overall).\n7) Portability and Setup Plan (e.g., mobile surface/stand/tray/case, power, cable management, typical venue layout considerations).\n8) Wiring and Signal Flow Diagram: an embedded PNG image showing a simple input/output flow (two dynamic vocal mics into mixer; processing/effects to IEM transmitter; two independent mixes to two IEM packs on-stage). This must appear visibly as an image within the PDF (not just described in text).\n9) Cost Breakdown Spreadsheet Image: a PNG image of an Excel-like spreadsheet that shows a full cost breakdown with columns for price and quantity and a total. The image should include a visible total estimated cost in USD and a visible reference to the budget ($3,000). This image must be embedded on the final page of the PDF.\n\nFlexible naming is fine (e.g., \"Overview\" vs \"Executive Summary\"), but the above elements must be clearly present. Do NOT check calculation correctness\u2014only that the structure and artifacts are present.\n\nScoring (return a number from 0.0 to 1.0; the system will weight it):\n- 1.0: PDF, >=2 pages, and ALL nine structural elements present. Both PNG images (signal flow and spreadsheet) are embedded and clearly visible. BOM has retailer links for mixer and IEM system.\n- 0.85: PDF and pages OK; exactly one minor element missing (e.g., one image present but the other missing OR BOM present but missing one of the two required retailer links).\n- 0.7: PDF and pages OK; two structural elements missing or unclear.\n- 0.4: PDF but missing multiple (3\u20134) required elements OR images not embedded (both missing).\n- 0.2: Wrong format (not PDF) OR single-page document OR extremely incomplete structure (5+ elements missing).\n- 0.0: No valid deliverable.", "expectation": "A multi-page PDF containing the specified sections, embedded PNG diagram and PNG spreadsheet image, and a structured BOM with retailer links\u2014ready for quantitative verification in Stage 2."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness Verification (Code + LLM Optional)", "description": "With the structure guaranteed by Stage 1, verify key correctness elements: budget compliance, retailer links presence, coverage of required features, component counts, and BOM robustness.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Total Estimated Cost Within $3,000 Budget", "description": "Parse the PDF text to locate the final/total estimated cost (USD) and verify it is <= $3,000.", "weight": 2.0, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Returns a score in [0, 1] based on whether a clear total estimated cost is present\n    and <= $3,000. Partial credit if a plausible total is found but slightly over budget\n    (<= 5% over).\n    \"\"\"\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        text = context.files.read_pdf_text(output.id)\n        if not text:\n            return 0.0\n        low = text.lower()\n        # Prefer totals that are clearly labeled as total estimated cost / grand total\n        patterns = [\n            r\"total\\s+(estimated\\s+)?(cost|price|amount)[^\\$\\d]{0,20}(\\$?\\s*[\\d,]+(?:\\.\\d{2})?)\",\n            r\"grand\\s+total[^\\$\\d]{0,20}(\\$?\\s*[\\d,]+(?:\\.\\d{2})?)\",\n            r\"final\\s+total[^\\$\\d]{0,20}(\\$?\\s*[\\d,]+(?:\\.\\d{2})?)\",\n        ]\n        def parse_usd(s):\n            s = s.replace('$','').replace(',','').strip()\n            try:\n                return float(s)\n            except:\n                return None\n        totals = []\n        for pat in patterns:\n            for m in re.finditer(pat, low, flags=re.IGNORECASE):\n                # last group should be the amount\n                amt = m.groups()[-1]\n                val = parse_usd(amt)\n                if val is not None:\n                    totals.append(val)\n        # If nothing matched, try a generic 'total' line heuristic\n        if not totals:\n            for m in re.finditer(r\"\\btotal\\b[^\\$\\d]{0,20}(\\$?\\s*[\\d,]+(?:\\.\\d{2})?)\", low, flags=re.IGNORECASE):\n                val = parse_usd(m.group(1))\n                if val is not None:\n                    totals.append(val)\n        if not totals:\n            # As a fallback, award small partial credit if there are many USD prices present\n            dollars = re.findall(r\"\\$\\s*[\\d,]+(?:\\.\\d{2})?\", text)\n            if len(dollars) >= 6:\n                return 0.3\n            return 0.0\n        # Choose the maximum total found (often the grand total is the largest)\n        est_total = max(totals)\n        budget = 3000.0\n        if est_total <= budget:\n            return 1.0\n        # partial if within 5%\n        if est_total <= budget * 1.05:\n            return 0.6\n        return 0.0\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Retailer Links Present (Mixer and IEM System)", "description": "Check the PDF text includes at least two product URLs from known pro-audio retailers (e.g., Sweetwater, B&H, Guitar Center, Thomann, Amazon, Musicians Friend, Reverb).", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Returns score in [0,1] based on presence of at least two retailer product links.\n    \"\"\"\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        text = context.files.read_pdf_text(output.id)\n        if not text:\n            return 0.0\n        urls = re.findall(r\"https?://[^\\s)]+\", text)\n        if not urls:\n            return 0.0\n        domains = [\n            'sweetwater.com','bhphotovideo.com','bhphoto.com','guitarcenter.com','thomann.de',\n            'amazon.com','musiciansfriend.com','reverb.com','adorama.com'\n        ]\n        def is_retail(u):\n            u_low = u.lower()\n            return any(d in u_low for d in domains)\n        retail_links = [u for u in urls if is_retail(u)]\n        # Require >= 2 total retailer links (ideally mixer + IEM system)\n        n = len(retail_links)\n        if n >= 2:\n            return 1.0\n        if n == 1:\n            return 0.5\n        return 0.0\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Mixer Requirements Coverage (Analog + Effects)", "description": "Verify the PDF text indicates an analog mixing board and mentions all three effects: compression, reverb, and delay.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Score components (analog mixer mention + effects mentions). Full credit if\n    'analog' + all of 'compression','reverb','delay' are present in the text.\n    \"\"\"\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        text = context.files.read_pdf_text(output.id)\n        if not text:\n            return 0.0\n        low = text.lower()\n        analog_ok = bool(re.search(r\"analog[\\w\\s]{0,20}(mixer|mixing board|console)\", low)) or ('analog' in low and ('mixer' in low or 'mixing board' in low or 'console' in low))\n        comp_ok = ('compress' in low)  # matches compress/compressor/compression\n        reverb_ok = ('reverb' in low)\n        delay_ok = ('delay' in low)\n        score = 0\n        score += 1 if analog_ok else 0\n        score += 1 if comp_ok else 0\n        score += 1 if reverb_ok else 0\n        score += 1 if delay_ok else 0\n        return score / 4.0\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "IEM Components Counted (2 Packs + Transmitter)", "description": "Verify the PDF text explicitly indicates two IEM bodypack receivers (packs) and includes a transmitter.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Full credit if text mentions a transmitter and exactly two IEM receivers/packs.\n    Partial credit if only one of these conditions is met.\n    \"\"\"\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        text = context.files.read_pdf_text(output.id)\n        if not text:\n            return 0.0\n        low = text.lower()\n        has_tx = ('transmitter' in low)\n        # Look for quantity 2 near pack/receiver terms\n        pack_patterns = [\n            r\"(2|two)\\s+(iem\\s+)?(packs?|bodypacks?|receivers?)\",\n            r\"(packs?|bodypacks?|receivers?)\\s*[:x\\-]?\\s*(2|two)\",\n            r\"qty\\s*[:x\\-]?\\s*2\\s+(iem\\s+)?(packs?|bodypacks?|receivers?)\",\n        ]\n        has_two_packs = any(re.search(p, low) for p in pack_patterns)\n        if has_tx and has_two_packs:\n            return 1.0\n        if has_tx or has_two_packs:\n            return 0.5\n        return 0.0\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "BOM Robustness (Qty + Prices Across Multiple Lines)", "description": "Heuristic check: ensure the BOM includes multiple line items with quantities and USD prices.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Score based on count of lines that include both a quantity token (qty/quantity or xN)\n    and a USD price. Full credit for >=5 such lines, partial for 3-4.\n    \"\"\"\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        text = context.files.read_pdf_text(output.id)\n        if not text:\n            return 0.0\n        lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n        count = 0\n        qty_re = re.compile(r\"\\b(qty|quantity)\\b\\s*[:x\\-]?\\s*(\\d+)|\\bx\\s*(\\d+)\\b\", re.IGNORECASE)\n        usd_re = re.compile(r\"\\$\\s*[\\d,]+(?:\\.\\d{2})?\")\n        for ln in lines:\n            if qty_re.search(ln) and usd_re.search(ln):\n                count += 1\n        if count >= 5:\n            return 1.0\n        if count >= 3:\n            return 0.6\n        if count >= 1:\n            return 0.3\n        return 0.0\n    except Exception:\n        return 0.0"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Suitability", "description": "Assess clarity, professionalism, and practicality for a touring IEM workflow in varied venues. This is a holistic LLM judgment of presentation and usefulness.", "is_required": false, "max_points": 1.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Practicality", "description": "Evaluate if the document is professionally written, visually clear, and practically actionable for real touring scenarios.", "weight": 1.0, "judge_prompt": "Judge the professional quality and practical usefulness of the PDF. Consider:\n- Clarity, organization, and readability of sections, tables, and figures (are the PNG images legible at print or screen scale?).\n- Practicality for touring: portability plan (surface/stand/case), power and cable management, quick setup steps, labeling, spares/consumables, troubleshooting notes, and venue layout contingencies (side stage/FOH/farther locations).\n- Appropriateness to audience: two-vocalist band expecting independent mixes with compression/reverb/delay on an analog-preferred board; wireless RF IEM operation.\n- Link usability: links appear relevant and usable for purchasing.\n- Overall cohesion: does it feel like a one-stop summary the band can approve and the IEM tech can deploy quickly?\n\nScoring (0.0 to 1.0):\n- 1.0: Highly professional, clean visuals, clear and actionable touring guidance, strong cohesion.\n- 0.7: Generally good; minor clarity gaps or light on operational details but usable.\n- 0.4: Acceptable but unpolished or missing several practical considerations.\n- 0.2: Hard to follow, unclear visuals, weak practical guidance.\n- 0.0: Low-effort or unusable quality.", "expectation": "A clear, polished, and practically actionable proposal suitable for immediate adoption by a touring band and their IEM tech."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "bb499d9c-0263-4684-9238-75e8e86077b1", "rubric": {"category_name": "Finance/Insurance \u2014 Sales Operations Process (Fintech Marketplace)", "rationale": "This rubric forces a self-documenting, verifiable Word/PDF deliverable with strict structural requirements (Stage 1 LLM gate), objective correctness checks via code on key elements aligned to the requested sections (Stage 2), and a final holistic quality assessment for senior-management-ready polish and strategic value (Stage 3). The task is a professional document with visual process models (Pattern B: Document). Stage 1 mandates exact sections and diagrams so later checks are trivial. Stage 2 uses flexible, robust text parsing to validate coverage of compliance, metrics, issuer customization, and reporting. Stage 3 judges clarity, practicality, and executive suitability.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Document Format and Structure Gate", "description": "LLM-only gate that enforces exact output shape and structural completeness for a professional Sales Operation Process document with diagrams.", "is_required": true, "max_points": 2.5, "min_score_to_pass": 2.0, "rules": [{"type": "llm_judge", "name": "Structured Document Requirements (DOCX/PDF with specific sections and diagrams)", "description": "Check that the candidate output is a properly structured DOCX or PDF not exceeding 25 pages, containing all required sections, tables/lists, and both process model diagrams with textual breakdowns.", "weight": 2.5, "judge_prompt": "You are evaluating a professional Sales Operation Process deliverable for a fintech two-sided marketplace (asset issuers and retail investors). Only verify PRESENCE and STRUCTURE, not content quality. Be flexible with exact phrasing of headings.\n\nFORMAT REQUIREMENTS:\n- File must be a DOCX or PDF (not Excel/CSV/plaintext).  \n- Length: no longer than 25 pages.  \n- Professional document formatting with clear, scannable section headers.  \n\nREQUIRED SECTIONS (headers must be visible; synonymous headers accepted):\n1) Overview \u2014 purpose, scope, audience.  \n2) Stakeholders \u2014 internal teams and external parties. Should be a structured list or table.  \n3) Process Definition \u2014 must include the following labeled subsections:\n   - Process Goal\n   - Trigger Event\n   - Preconditions\n   - Inputs\n   - Output (deliverables)\n   - Success end condition\n   - Failure end condition\n   - Compliance (regulations/policies)  \n4) Key Roles \u2014 roles/responsibilities of internal stakeholders.  \n5) Key Forms \u2014 essential documents (e.g., NDAs, KYC forms).  \n6) Key Metrics \u2014 include both volume/outcome metrics (e.g., AUM, ARR) and efficiency metrics (e.g., margins, retention).  \n7) Key Reports \u2014 regular reports to monitor metrics and decisions.  \n8) Potential Risks and Mitigation Controls \u2014 risks with controls/mitigations.  \n9) Asset Issuers Process model \u2014 includes:\n   - A visible flow chart/diagram image for selling to issuers.  \n   - A textual breakdown of each stage in the chart.  \n   - Customization by issuer groups (e.g., private companies, private funds, public listings, banks/asset managers/originators).  \n10) Retail Investors Process model \u2014 includes:\n   - A visible flow chart/diagram image for selling to retail investors.  \n   - A textual breakdown of each stage in the chart.  \n\nSCORING (STRUCTURE ONLY):\n- 2.5: Valid DOCX/PDF, <=25 pages, all 10 required sections present with clear headers; Process Definition has all 8 subsections; both process model diagrams are present with textual breakdowns; issuer model shows group customization.  \n- 2.2: All core pieces present but one minor omission (e.g., missing one Process Definition subsection OR issuer customization is implied but not clearly labeled).  \n- 1.5: Missing two\u2013three required elements (e.g., one diagram missing OR multiple Process Definition subsections missing).  \n- 0.8: Valid format but several missing elements; structure is incomplete.  \n- 0.0: Not DOCX/PDF, or grossly incomplete (e.g., <5 sections), or obviously >25 pages.\n\nOnly evaluate structure/format, not correctness of content or calculations.", "expectation": "A cleanly structured Word/PDF (<=25 pages) including all 10 sections, both process model diagrams with textual breakdowns, and the full set of Process Definition subsections."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification and Coverage Checks", "description": "Code-based verification of section coverage, subsections, compliance depth, metrics/reports inclusion, issuer customization, diagram captions, and approximate length. Flexible substring matching and robust parsing.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 2.5, "rules": [{"type": "code", "name": "Section Headers Presence", "description": "Verify presence of the 10 requested main sections using flexible matching.", "weight": 1.2, "code": "import re\\n\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n\\n    text = ''\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_text(output.id)\\n            except Exception:\\n                text = ''\\n    if not text:\\n        return 0.0\\n\\n    tl = re.sub(r\"\\s+\", \" \", text).lower()\\n\\n    checks = [\\n        any(k in tl for k in ['overview']),\\n        any(k in tl for k in ['stakeholders']),\\n        any(k in tl for k in ['process definition', 'sales process', 'process design', 'process overview']),\\n        any(k in tl for k in ['key roles', 'roles and responsibilities', 'roles & responsibilities', 'roles']),\\n        any(k in tl for k in ['key forms', 'forms', 'documents']),\\n        any(k in tl for k in ['key metrics', 'metrics', 'kpis', 'performance metrics']),\\n        any(k in tl for k in ['key reports', 'reports', 'reporting', 'reporting cadence']),\\n        any(k in tl for k in ['potential risks', 'risks', 'risk and mitigation', 'risk management']),\\n        any(k in tl for k in ['asset issuers process model', 'issuer process model', 'issuers process', 'issuer flow', 'issuer journey']),\\n        any(k in tl for k in ['retail investors process model', 'investor process model', 'retail investor flow', 'investor journey'])\\n    ]\\n    coverage = sum(1 for c in checks if c) / len(checks)\\n    return coverage * 1.2\\n"}, {"type": "code", "name": "Process Definition Subsections", "description": "Verify presence of the eight Process Definition subsections via fuzzy matching.", "weight": 1.0, "code": "import re\\n\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n    text = ''\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_text(output.id)\\n            except Exception:\\n                text = ''\\n    if not text:\\n        return 0.0\\n    tl = re.sub(r\"\\s+\", \" \", text).lower()\\n\\n    subs = {\\n        'goal': ['process goal', 'objective', 'objectives'],\\n        'trigger': ['trigger event', 'trigger', 'entry criteria'],\\n        'preconditions': ['preconditions', 'prerequisites', 'pre-requisites'],\\n        'inputs': ['inputs', 'required inputs', 'resources needed'],\\n        'output': ['output', 'deliverables', 'outcomes'],\\n        'success': ['success end condition', 'definition of success', 'exit criteria (success)'],\\n        'failure': ['failure end condition', 'lost opportunity', 'exit criteria (failure)'],\\n        'compliance': ['compliance', 'regulatory', 'policies']\\n    }\\n\\n    hits = 0\\n    for _, variants in subs.items():\\n        if any(v in tl for v in variants):\\n            hits += 1\\n    return (hits / 8.0) * 1.0\\n"}, {"type": "code", "name": "Compliance Coverage Depth", "description": "Check for breadth of key compliance/regulatory keywords relevant to fintech sales to issuers and retail investors.", "weight": 0.8, "code": "import re\\n\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n    text = ''\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            text = ''\\n    if not text:\\n        return 0.0\\n    tl = re.sub(r\"\\s+\", \" \", text).lower()\\n\\n    keywords = [\\n        'kyc', 'know your customer', 'aml', 'anti-money laundering', 'sanctions', 'ofac',\\n        'customer due diligence', 'cdd', 'enhanced due diligence', 'edd',\\n        'suitability', 'appropriateness', 'accredited investor',\\n        'reg d', 'reg s', 'reg a', 'reg cf',\\n        'sec', 'finra', 'sipc', 'broker-dealer', 'transfer agent', 'custody',\\n        'mifid', 'gdpr', 'privacy', 'data protection', 'recordkeeping'\\n    ]\\n    found = set(k for k in keywords if k in tl)\\n    # Full credit at 8+ unique hits, scale otherwise\\n    score = min(len(found) / 8.0, 1.0) * 0.8\\n    return score\\n"}, {"type": "code", "name": "Key Metrics Coverage", "description": "Verify inclusion of core volume/outcome and efficiency metrics.", "weight": 0.7, "code": "import re\\n\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n    text = ''\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            text = ''\\n    if not text:\\n        return 0.0\\n    tl = re.sub(r\"\\s+\", \" \", text).lower()\\n\\n    metrics = [\\n        'aum', 'assets under management', 'arr', 'annual recurring revenue',\\n        'cac', 'customer acquisition cost', 'ltv', 'lifetime value',\\n        'conversion rate', 'win rate', 'pipeline coverage', 'sales velocity',\\n        'average deal size', 'sales cycle', 'retention', 'churn',\\n        'margin', 'gross margin', 'nps', 'gmv', 'take rate'\\n    ]\\n    found = set(m for m in metrics if m in tl)\\n    score = min(len(found) / 8.0, 1.0) * 0.7\\n    return score\\n"}, {"type": "code", "name": "Key Reports Coverage", "description": "Verify presence of standard reporting cadence and report types.", "weight": 0.5, "code": "import re\\n\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n    text = ''\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            text = ''\\n    if not text:\\n        return 0.0\\n    tl = re.sub(r\"\\s+\", \" \", text).lower()\\n\\n    reports = [\\n        'pipeline report', 'forecast report', 'board report', 'kpi dashboard',\\n        'cohort analysis', 'funnel report', 'churn report', 'compliance report', 'risk report',\\n        'weekly', 'monthly', 'quarterly'\\n    ]\\n    found = set(r for r in reports if r in tl)\\n    score = min(len(found) / 6.0, 1.0) * 0.5\\n    return score\\n"}, {"type": "code", "name": "Issuer Group Customization", "description": "Check that issuer process references multiple issuer groups (e.g., private companies, private funds, public listings, banks/asset managers/originators).", "weight": 0.5, "code": "import re\\n\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n    text = ''\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            text = ''\\n    if not text:\\n        return 0.0\\n    tl = re.sub(r\"\\s+\", \" \", text).lower()\\n\\n    groups = [\\n        'private companies', 'private funds', 'public listings', 'listed companies',\\n        'banks', 'asset managers', 'fund gps', 'gps', 'originators', 'debt originators'\\n    ]\\n    found = set(g for g in groups if g in tl)\\n    score = min(len(found) / 4.0, 1.0) * 0.5\\n    return score\\n"}, {"type": "code", "name": "Flowchart Captions/References", "description": "Verify textual references to diagrams/flowcharts for issuer and investor models (captions or nearby descriptors).", "weight": 0.2, "code": "import re\\n\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n    text = ''\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            text = ''\\n    if not text:\\n        return 0.0\\n    tl = re.sub(r\"\\s+\", \" \", text).lower()\\n    # Look for references like \"flowchart\", \"flow chart\", or \"diagram\" near issuer/investor\\n    has_issuer = bool(re.search(r'(issuer|issuers).{0,50}(flow ?chart|diagram|figure)', tl)) or bool(re.search(r'(flow ?chart|diagram|figure).{0,50}(issuer|issuers)', tl))\\n    has_investor = bool(re.search(r'(investor|investors).{0,50}(flow ?chart|diagram|figure)', tl)) or bool(re.search(r'(flow ?chart|diagram|figure).{0,50}(investor|investors)', tl))\\n    return (0.1 * has_issuer) + (0.1 * has_investor)\\n"}, {"type": "code", "name": "Approximate Length Check (<=25 pages)", "description": "Approximate page length via word count. Full credit if <=10,000 words; partial if <=13,000; else 0.", "weight": 0.1, "code": "import re\\n\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n    text = ''\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            text = ''\\n    if not text:\\n        return 0.0\\n    words = re.findall(r'\\\\w+', text)\\n    wc = len(words)\\n    if wc <= 10000:\\n        return 0.1\\n    elif wc <= 13000:\\n        return 0.05\\n    else:\\n        return 0.0\\n"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Executive Quality and Practicality", "description": "LLM judgment of professional polish, managerial clarity, practicality, and strategic fit for senior management approval.", "is_required": false, "max_points": 2.5, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professionalism, Clarity, and Actionability", "description": "Assess whether the document reads like a senior-management-ready Level 1 Sales Operation Process, with clear step-by-step guidance and practical implementation details.", "weight": 2.5, "judge_prompt": "Evaluate the document holistically for executive readiness and practical value. Consider only quality (not structure already checked). Criteria:\\n- Clarity and organization: Clear, step-by-step process; navigation and labeling are intuitive.\\n- Practicality: Concrete handoffs, ownership, and dependencies across Sales, Marketing, Legal/Compliance, Product/Eng, Risk, Finance/RevOps, and Support.\\n- Strategic fit: Tailored to a two-sided fintech marketplace selling to issuers and retail investors; acknowledges dual go-to-market.\\n- Operational readiness: Defines cadences, SLAs, RACI/ownership cues, and how metrics/reports are used in operating rhythm (e.g., weekly pipeline/forecast, monthly board).\\n- Risk and compliance posture: Balanced, realistic controls integrated into the flow, not bolted on.\\n- Visuals and explanations: Diagrams are readable, and each stage has a succinct textual breakdown that aligns with the visuals.\\n- Senior management suitability: Tone, completeness, and decision-enabling content appropriate for approval; avoids fluff.\\n\\nScoring Guidance:\\n- 2.5: Exceptional executive-ready artifact: crisp, actionable, integrated across teams; diagrams and text align; clear ownership and operating rhythm.\\n- 2.0: Strong with minor gaps (e.g., some ownership/cadence details light or minor inconsistencies).\\n- 1.5: Adequate but generic or partially actionable; several sections lack depth.\\n- 1.0: Superficial; limited practical guidance; unclear operating model.\\n- 0.5: Poorly organized and not ready for leadership review.\\n- 0.0: Incoherent or irrelevant.\\n", "expectation": "An executive-ready, pragmatic Level 1 Sales Operation Process with actionable steps, clear ownership and cadences, aligned issuer/investor flows, and integrated compliance controls."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "c44e9b62-7cd8-4f72-8ad9-f8fbddb94083", "rubric": {"category_name": "Government \u2014 Administrative Services Managers \u2014 FTE Reduction Package", "rationale": "Mixed-output task requiring a self-documenting package: a revised org chart (PDF), an FTE plan/report (Excel), and a briefing note (Word). Stage 1 (LLM-only) strictly enforces deliverable shapes and structures so later verification is trivial. Stage 2 mixes code checks for quantitative consistency and document mentions tied to instructions. Stage 3 assesses overall professional quality and suitability for leadership review.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate", "description": "LLM-only check that ALL three deliverables exist with exact, verifiable structures enabling downstream validation. No correctness checks here.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.0, "rules": [{"type": "llm_judge", "name": "Structured Deliverables Presence and Format", "description": "Check that the candidate produced all three required deliverables with mandated structures.", "weight": 6.0, "judge_prompt": "You are verifying the presence and structure (not correctness) of a 3-part deliverable set for a government Administrative Services Manager.\n\nReview ALL provided outputs (files). Only evaluate format/structure. Do not judge correctness or content quality.\n\nRequired deliverables and structural requirements:\n\n1) Revised Organizational Chart (PDF)\n- File format: PDF only.\n- Title or header includes: \"Administrative Support Services Branch\" AND indicates this is revised for next fiscal year (e.g., \"Revised\", \"Next Fiscal Year\").\n- Contains both Central and Regional structures; the \"Regional Support Services Supervisor\" node should be visible.\n- Visual highlighting for positions to be reduced (e.g., color, halo, strikethrough). A small legend or note explains the highlight convention (e.g., \"Red outline = reduction\").\n- A footer/note or subtitle indicating the effective period (next fiscal year) is present.\n\n2) FTE Report (Excel)\n- File format: Excel (.xlsx) only.\n- Must include these sheets:\n  a) \"FTE Plan\" sheet with a tabular layout and columns covering, at minimum: Position Title, Current FTE, Planned FTE, and a change indicator (e.g., Change (FTE) and/or Change (%)). Additional helpful columns such as Unit and Supervisor are acceptable.\n  b) \"Summary\" sheet that explicitly lists: Total Current FTE, Total Planned FTE, Total Change (FTE), and Reduction % (computed).\n- Optional but encouraged: an \"Assumptions\" or \"Notes\" sheet listing key assumptions (e.g., 10% regional reduction due to 10\u21929 offices; attrition/vacancy/leave drivers).\n\n3) Briefing Note (Word)\n- File format: DOCX only (not PDF, not plain text).\n- Has a header block with: To:, From:, Date:, Subject: (reasonable equivalents acceptable).\n- Subject references FTE reductions for the Administrative Support Services Branch.\n- Clear, labeled sections including (flexibly-named but identifiable):\n  \u2022 Background\n  \u2022 Proposed Reductions (or Analysis of Reductions)\n  \u2022 Alignment to Budget Planning Principles (explicitly mentions Principle #7)\n  \u2022 Risks and Mitigations\n  \u2022 Implementation Timeline/Next Steps\n  \u2022 Recommendation (or Decision Requested)\n- Includes a brief numeric summary (current FTE, planned FTE, reduction count and percentage) and references/attaches the Excel report and Org Chart PDF.\n\nScoring (structure-only):\n- 6.0: All three deliverables present, correct formats (PDF/XLSX/DOCX), and each contains the required structural elements above (optional Assumptions sheet may be missing without penalty).\n- 5.0: All three present and in correct formats, but one minor structural element missing (e.g., no legend on PDF, or missing one header field in briefing note, or Summary sheet labels present but not all four listed).\n- 4.0: All three present but multiple structural elements missing across deliverables, or one deliverable has format friction (e.g., Excel has FTE Plan but Summary is too skeletal). Still clearly usable for verification.\n- 3.0: Only two deliverables present OR one deliverable is in the wrong file format.\n- 0.0\u20131.5: Only one deliverable present, or formats do not match requirements.\n\nBe flexible with exact section/tab/column names, but ensure the above structures clearly exist. Do NOT assess calculation correctness or writing quality\u2014only presence/structure.", "expectation": "A complete package: PDF org chart with legend and next-fiscal-year context; Excel with FTE Plan and Summary sheets; DOCX briefing note with required sections and explicit mention of Principle #7."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Consistency Verification", "description": "Code-based checks validating quantitative plausibility and instruction adherence using the enforced shapes. Flexible matching and robust parsing.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Excel Reduction >= 4% and Non-Negative, Planned <= Current", "description": "Verify Excel (FTE Plan + Summary) shows at least 4% reduction overall; planned totals not exceeding current; no negative planned FTE.", "weight": 2.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    # Find spreadsheet output\n    excel_res = None\n    for r in context.get_all_outputs():\n        try:\n            if getattr(r, 'is_spreadsheet', False):\n                excel_res = r\n                break\n        except Exception:\n            pass\n    if not excel_res:\n        return 0.0, 'No spreadsheet output found.'\n\n    feedback = []\n    file_path = context.files.get_path(excel_res.id)\n    try:\n        xls = pd.ExcelFile(file_path)\n        sheet_names = [s.lower() for s in xls.sheet_names]\n    except Exception as e:\n        return 0.0, f'Failed to open Excel: {e}'\n\n    # Try to locate FTE Plan sheet\n    target_sheet = None\n    for s in xls.sheet_names:\n        sl = s.lower()\n        if 'fte plan' in sl or 'plan' in sl or 'fte' in sl:\n            target_sheet = s\n            break\n    if target_sheet is None:\n        # fallback to first sheet\n        target_sheet = xls.sheet_names[0]\n        feedback.append('FTE Plan sheet not clearly found; using first sheet.')\n\n    try:\n        df = pd.read_excel(file_path, sheet_name=target_sheet)\n    except Exception as e:\n        return 0.0, f'Failed to read FTE Plan sheet: {e}'\n\n    # Normalize columns\n    cols = [str(c).strip().lower() for c in df.columns]\n    def find_col(cands):\n        # cands: list of lists of keywords; returns index if all keywords in a column name\n        for i,c in enumerate(cols):\n            for kws in cands:\n                if all(kw in c for kw in kws):\n                    return i\n        return None\n\n    current_idx = find_col([[ 'current','fte' ], ['current'], ['baseline','fte'], ['baseline']])\n    planned_idx = find_col([[ 'planned','fte' ], ['planned'], ['target','fte'], ['target'], ['next','fte'], ['next']])\n\n    if current_idx is None or planned_idx is None:\n        return 0.5, 'Could not confidently identify Current/Planned FTE columns.'\n\n    curr = pd.to_numeric(df.iloc[:, current_idx], errors='coerce')\n    plan = pd.to_numeric(df.iloc[:, planned_idx], errors='coerce')\n\n    # Drop entirely NaN rows\n    mask = ~(curr.isna() & plan.isna())\n    curr = curr[mask].fillna(0)\n    plan = plan[mask].fillna(0)\n\n    total_current = float(curr.sum())\n    total_planned = float(plan.sum())\n\n    if total_current <= 0:\n        return 0.8, 'Total current FTE not positive; cannot compute reduction % reliably.'\n\n    reduction = total_current - total_planned\n    reduction_pct = reduction / total_current\n\n    # Checks weighting\n    score = 0.0\n    weight = 2.5\n\n    # 0.4: parsed totals\n    score += 0.4 * weight\n\n    # 0.3: planned <= current\n    if total_planned <= total_current + 1e-6:\n        score += 0.3 * weight\n    else:\n        feedback.append('Planned total exceeds current total.')\n\n    # 0.2: non-negative planned values\n    if (plan >= -1e-6).all():\n        score += 0.2 * weight\n    else:\n        feedback.append('Found negative Planned FTE entries.')\n\n    # 0.1: reduction >= 4%\n    if reduction_pct >= 0.04 - 1e-6:\n        score += 0.1 * weight\n    else:\n        feedback.append(f'Reduction % below 4% (found {reduction_pct:.2%}).')\n\n    fb = f\"Totals \u2014 Current: {total_current:.2f}, Planned: {total_planned:.2f}, Reduction: {reduction:.2f} ({reduction_pct:.2%}). \"\n    if feedback:\n        fb += ' | ' + ' '.join(feedback)\n    return score, fb"}, {"type": "code", "name": "Excel Column Structure Present", "description": "Check that FTE Plan contains key columns: Position Title, Current FTE, Planned FTE; optional helpful fields (Unit, Supervisor) boost clarity.", "weight": 1.5, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    excel_res = None\n    for r in context.get_all_outputs():\n        try:\n            if getattr(r, 'is_spreadsheet', False):\n                excel_res = r\n                break\n        except Exception:\n            pass\n    if not excel_res:\n        return 0.0, 'No spreadsheet output found.'\n\n    file_path = context.files.get_path(excel_res.id)\n    try:\n        xls = pd.ExcelFile(file_path)\n        # choose FTE Plan-like sheet\n        target_sheet = None\n        for s in xls.sheet_names:\n            sl = s.lower()\n            if 'fte plan' in sl or 'plan' in sl or 'fte' in sl:\n                target_sheet = s\n                break\n        if target_sheet is None:\n            target_sheet = xls.sheet_names[0]\n        df = pd.read_excel(file_path, sheet_name=target_sheet)\n    except Exception as e:\n        return 0.0, f'Failed to open/read Excel: {e}'\n\n    cols = [str(c).strip().lower() for c in df.columns]\n    def has_col(substrs_any):\n        return any(all(sub in c for sub in substrs_any) for c in cols)\n\n    # Required logical columns\n    has_position = any(('position' in c) or ('title' in c) for c in cols)\n    has_current = any(('current' in c) for c in cols)\n    has_planned = any(('planned' in c) or ('target' in c) or ('next' in c) for c in cols)\n\n    # Optional helpful columns\n    has_unit = any('unit' in c or 'branch' in c for c in cols)\n    has_supervisor = any('supervisor' in c or 'manager' in c for c in cols)\n    has_change = any('change' in c or '%'+'' in c for c in cols)\n\n    base_count = sum([has_position, has_current, has_planned])\n    if base_count == 0:\n        return 0.0, 'Missing Position, Current, and Planned columns.'\n\n    base_score = (base_count/3.0) * (1.2)  # up to 1.2 of 1.5\n    optional_score = (sum([has_unit, has_supervisor, has_change])/3.0) * (0.3)\n    score = (base_score + optional_score) * (1.5)  # scale to weight\n\n    score = min(score, 1.5)\n    feedback = f\"Columns present \u2014 Position:{has_position}, Current:{has_current}, Planned:{has_planned}, Unit:{has_unit}, Supervisor:{has_supervisor}, Change:{has_change}.\"\n    return score, feedback"}, {"type": "code", "name": "Briefing Note Mentions Principle #7 and Budget Planning Principles", "description": "DOCX briefing note should explicitly reference Budget Planning Principles and emphasize Principle #7.", "weight": 2.0, "code": "import re\n\ndef evaluate(workflow, context):\n    # Gather document texts (docx preferred; pdf fallback)\n    texts = []\n    for r in context.get_all_outputs():\n        try:\n            if getattr(r, 'is_document', False):\n                # Try DOCX first\n                try:\n                    t = context.files.read_docx_text(r.id)\n                    if t and t.strip():\n                        texts.append(('docx', t))\n                        continue\n                except Exception:\n                    pass\n                try:\n                    t = context.files.read_pdf_text(r.id)\n                    if t and t.strip():\n                        texts.append(('pdf', t))\n                except Exception:\n                    pass\n        except Exception:\n            pass\n\n    if not texts:\n        return 0.0, 'No document text available.'\n\n    # Prefer DOCX content for briefing note\n    doc_text = ' '.join([t for kind,t in texts if kind=='docx'])\n    if not doc_text:\n        # Fallback to all\n        doc_text = ' '.join([t for kind,t in texts])\n\n    txt = doc_text.lower()\n    has_bpp = ('budget planning principles' in txt) or ('planning principles' in txt)\n    # Principle #7 variants\n    has_p7 = ('principle #7' in txt) or ('principle 7' in txt) or ('principle seven' in txt)\n\n    score = 0.0\n    weight = 2.0\n    if has_bpp:\n        score += 0.9 * weight\n    if has_p7:\n        score += 0.1 * weight\n\n    # Cap score\n    score = min(score, weight)\n    fb = f\"Mentions \u2014 BPP:{has_bpp}, Principle7:{has_p7}.\"\n    return score, fb"}, {"type": "code", "name": "Briefing Note References Drivers and Roles", "description": "Briefing note should mention drivers (attrition/leave/retirement/vacancy/resignation) and specific roles/structures cited in instructions.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    texts = []\n    for r in context.get_all_outputs():\n        try:\n            if getattr(r, 'is_document', False):\n                try:\n                    t = context.files.read_docx_text(r.id)\n                    if t and t.strip():\n                        texts.append(t)\n                        continue\n                except Exception:\n                    pass\n                try:\n                    t = context.files.read_pdf_text(r.id)\n                    if t and t.strip():\n                        texts.append(t)\n                except Exception:\n                    pass\n        except Exception:\n            pass\n\n    if not texts:\n        return 0.0, 'No document text available.'\n\n    full = ' '.join(texts).lower()\n\n    role_terms = ['data clerk', 'clerk ii', 'facilitator', 'regional support services supervisor']\n    driver_terms = ['attrition', 'retirement', 'retirements', 'leave', 'leaves', 'vacancy', 'vacant', 'resignation', 'resignations', 'voluntary']\n\n    roles_hit = sum(1 for rt in role_terms if rt in full)\n    drivers_hit = sum(1 for dt in driver_terms if dt in full)\n\n    # Scoring: up to 1.0 for roles (>=3 hits for full), up to 0.5 for drivers (>=2 hits for full)\n    weight = 1.5\n    role_score = min(roles_hit/3.0, 1.0) * 1.0\n    driver_score = min(drivers_hit/2.0, 1.0) * 0.5\n\n    score = (role_score + driver_score) * weight / 1.5\n\n    fb = f\"Role mentions: {roles_hit}, Driver mentions: {drivers_hit}.\"\n    return score, fb"}, {"type": "code", "name": "Org Chart PDF Mentions Branch and Reduction Legend", "description": "Org chart PDF should refer to the branch and indicate reductions/legend/highlighting convention.", "weight": 0.5, "code": "def evaluate(workflow, context):\n    pdf_texts = []\n    for r in context.get_all_outputs():\n        try:\n            if getattr(r, 'is_document', False):\n                try:\n                    t = context.files.read_pdf_text(r.id)\n                    if t and t.strip():\n                        pdf_texts.append(t)\n                except Exception:\n                    pass\n        except Exception:\n            pass\n\n    if not pdf_texts:\n        return 0.0, 'No PDF text available.'\n\n    full = ' '.join(pdf_texts).lower()\n    has_branch = 'administrative support services branch' in full\n    has_reduction_hint = any(k in full for k in ['revised','next fiscal year','reduction','reduced','removed','eliminated','legend','highlight'])\n\n    score = 0.0\n    weight = 0.5\n    if has_branch:\n        score += 0.3 * weight\n    if has_reduction_hint:\n        score += 0.7 * weight\n\n    return score, f\"Branch:{has_branch}, Reduction/Legend hint:{has_reduction_hint}.\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Fitness for Purpose", "description": "LLM judge assesses clarity, professionalism, leadership readiness, and strategic alignment.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Overall Quality and Leadership Readiness", "description": "Assess professionalism, clarity, and usefulness for leadership decision-making across the 3 deliverables.", "weight": 6.0, "judge_prompt": "Evaluate the overall professional quality and leadership readiness of the full package (PDF org chart, Excel FTE report, DOCX briefing note). Focus on clarity, cohesion, and usefulness for decision-making\u2014not raw structure (already checked) nor arithmetic precision (lightly checked elsewhere).\n\nConsider:\n- Org Chart (PDF): visually clear, reductions easy to spot; legend understandable; central and regional structures readable; typography/layout professional.\n- FTE Report (Excel): readable tabular design; summary clearly reports totals and Reduction %; reasonable labeling; filters/sorts or formatting that aid understanding; consistent units.\n- Briefing Note (DOCX): concise, executive-friendly; clear narrative linking background \u2192 proposed reductions \u2192 alignment to Budget Planning Principles (with emphasis on Principle #7); risks/mitigations and next steps practical; references to attachments; quantitative summary presented clearly.\n- Cross-deliverable coherence: the numbers and narrative feel consistent at a high level (no glaring contradictions visible to a human reviewer).\n\nScoring guidance:\n- 6.0: Highly professional and leadership-ready; excellent clarity/visuals; compelling and coherent narrative tightly aligned to principles.\n- 4.5: Strong overall; minor clarity or presentation issues; fully usable by leadership with minimal edits.\n- 3.0: Adequate but with noticeable issues (formatting, clarity, or cohesion) requiring edits before use.\n- 1.5: Weak; difficult to follow or lacks executive focus.\n- 0.0: Not professionally usable.\n\nUse holistic judgment of presentation and executive usefulness.", "expectation": "A polished, coherent package that a senior leader can read quickly to understand the what/why/how of the FTE reductions, with clear visuals and succinct, principled narrative."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "38889c3b-e3d4-49c8-816a-3cc8e5313aba", "rubric": {"category_name": "Audio Production \u2014 Instrumental and Stems Delivery (Music Producer)", "rationale": "This rubric forces a self-documenting deliverable: a clearly structured Delivery Report (PDF/DOCX) plus a ZIP containing the Master and required stems with machine-verifiable metadata and maps. Stage 1 (LLM-only) mandates an auditable document structure that inventories files, defines tempo/key/section timing, and states export specs. Stage 2 uses code to verify file presence, technical specs (48 kHz, 24-bit/float), approximate duration, alignment, and the tempo/key window for the bridge. Stage 3 assesses professional quality and fit-to-brief based on the documented production rationale and arrangement notes.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Delivery Report Structure Gate (LLM)", "description": "Gate ensuring candidate provides a self-documenting Delivery Report with exact sections enabling later verification.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.1, "rules": [{"type": "llm_judge", "name": "Structured Delivery Report (Manifest) Requirement", "description": "Output must include a professionally formatted Delivery Report (PDF or DOCX) as a distinct document that inventories the ZIP contents and declares verifiable specs.", "weight": 3.0, "judge_prompt": "You are evaluating whether the candidate provided a self-documenting Delivery Report in proper format and structure. Only judge structure and presence, not audio quality.\n\nAcceptable Formats:\n- PDF or DOCX only (not plain text, not Excel). The Delivery Report must be visible as a separate document (not just inside a ZIP).\n- At least 1 full page with clearly labeled section headers.\n\nRequired Sections (titles can vary but meaning must match):\n1) File Manifest\n   - Must list the ZIP filename and every included audio file to be delivered to the recording engineer.\n   - Must include at least these expected files (exact names may vary): Master track (contains \"master\" or similar), and stems for Guitars, Synths, Bridge, and Bass (filenames indicating each part).\n   - Encouraged: sizes or checksums (optional, not required for full credit).\n\n2) Technical Specs (Export Settings)\n   - Must explicitly state: sample rate 48,000 Hz and bit depth 24-bit float (or 24-bit PCM/32-bit float acceptable if explicitly stated).\n   - Must state WAV format and channel count (stereo or mono per stem/master).\n\n3) Tempo & Sync\n   - Must state BPM = 140 and that all instrumentation is tightly synchronized to the provided \u201cDRUM REFERENCE TRACK.WAV\u201d.\n   - Must declare zero (or exact) start offset/alignment approach (e.g., all stems render from 00:00:00).\n\n4) Structure & Keys Map\n   - Must include a timeline with sections and times.\n   - Must explicitly document that main sections are in G major and the bridge spans approximately 1:22\u20131:49 in Ab major.\n   - This can be in prose, a table, or both.\n\n5) Stem Definitions\n   - A short description of what is contained in each required stem: Guitars, Synths, Bridge, Bass.\n\n6) Sampling Compliance Note\n   - A short statement that any samples used adhere to the Tracklib sampling guidelines, ideally referencing https://www.tracklib.com/blog/music-sampling-guide\n\nOptional (do not reduce score if missing):\n- Marker/Cue table (CSV referenced)\n- Alignment/Tempo-Key map CSV references\n- Peak headroom/LUFS targets\n\nScoring:\n- 3.0: All 6 required sections present, clearly labeled, and readable; format is PDF/DOCX.\n- 2.5: Missing minor details in one section but all 6 sections clearly present.\n- 2.1: One required section missing or too vague, but document otherwise meets format and length requirements.\n- 1.0: Valid PDF/DOCX but only 2\u20133 sections present or very incomplete.\n- 0.0: No PDF/DOCX Delivery Report, or fewer than 2 sections, or purely unstructured text.\n\nOnly judge the presence and structure. Do not verify correctness of audio or calculations.", "expectation": "A professional Delivery Report (PDF/DOCX) with File Manifest, Technical Specs (48 kHz, 24-bit float WAV), Tempo & Sync (140 BPM with drum reference), Structure & Keys (G major main, Ab major bridge 1:22\u20131:49), Stem Definitions (Guitars, Synths, Bridge, Bass), and Sampling Compliance Note."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Technical and Structural Verification (Code + Heuristics)", "description": "Deterministic checks using ZIP/audio headers and CSV maps declared in Stage 1.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "ZIP and Required Files Presence", "description": "Verify a ZIP is provided and contains a master track and stems for Guitars, Synths, Bridge, and Bass (WAV files). Flexible filename matching.", "weight": 1.5, "code": "import os, io, zipfile, re\nfrom typing import List, Dict, Tuple\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs()\n        if not outputs:\n            return 0.0, \"No outputs found.\"\n        # Find at least one ZIP\n        zips = []\n        for r in outputs:\n            try:\n                p = context.files.get_path(r.id)\n                if str(p).lower().endswith('.zip'):\n                    zips.append((r, p))\n            except Exception:\n                continue\n        # Also collect any top-level WAVs (in case candidate didn't zip)\n        loose_wavs = []\n        for r in outputs:\n            try:\n                p = context.files.get_path(r.id)\n                if str(p).lower().endswith('.wav'):\n                    loose_wavs.append((r, p))\n            except Exception:\n                continue\n        if not zips and not loose_wavs:\n            return 0.0, \"No ZIP or WAV files detected. Expected a delivery ZIP with audio files.\"\n\n        # Gather all filenames from zip(s) and loose wavs\n        audio_files = []  # list of tuples: (name_lower, source, content_bytes or path)\n        for r, zp in zips:\n            try:\n                with zipfile.ZipFile(zp, 'r') as zf:\n                    for name in zf.namelist():\n                        if name.lower().endswith('.wav') and not name.endswith('/'):\n                            audio_files.append((name.lower(), ('zip', zp), None))\n            except Exception:\n                pass\n        for r, wp in loose_wavs:\n            audio_files.append((os.path.basename(str(wp)).lower(), ('loose', wp), None))\n\n        if not audio_files:\n            return 0.0, \"No WAV files found in ZIP or as loose files.\"\n\n        # Heuristic matching\n        def has_file(preds: List[str]) -> Tuple[bool, List[str]]:\n            matches = [n for (n,_,_) in audio_files if any(pr in n for pr in preds)]\n            return (len(matches) > 0, matches)\n\n        # Master track\n        master_ok, master_matches = has_file(['master', 'mix', 'final'])\n        # Stems\n        guitars_ok, guitars_matches = has_file(['guitar', 'guitars', 'gtr'])\n        synths_ok, synths_matches = has_file(['synth'])\n        bridge_ok, bridge_matches = has_file(['bridge'])\n        bass_ok, bass_matches = has_file(['bass'])\n\n        required = {\n            'master': (master_ok, master_matches),\n            'guitars': (guitars_ok, guitars_matches),\n            'synths': (synths_ok, synths_matches),\n            'bridge': (bridge_ok, bridge_matches),\n            'bass': (bass_ok, bass_matches)\n        }\n\n        # Scoring: 5 required files, each worth 0.3 up to 1.5\n        present_count = sum(1 for k,(ok,_) in required.items() if ok)\n        score = 0.3 * present_count\n        missing = [k for k,(ok,_) in required.items() if not ok]\n        feedback = f\"Found {present_count}/5 required files. Missing: {', '.join(missing) if missing else 'none'}.\"\n        return min(score, 1.5), feedback\n    except Exception as e:\n        return 0.0, f\"Exception in presence check: {e}\""}, {"type": "code", "name": "Sample Rate and Bit Depth Compliance", "description": "Check WAV headers for 48 kHz sample rate and 24-bit depth (allow partial credit for 32-bit float if detected as 4-byte).", "weight": 1.2, "code": "import os, io, zipfile, wave, contextlib\n\ndef _collect_audio_bytes(context):\n    audio = []  # list of tuples: (display_name, bytes_like)\n    outputs = context.get_all_outputs()\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n        except Exception:\n            continue\n        pl = str(p).lower()\n        if pl.endswith('.zip'):\n            try:\n                with zipfile.ZipFile(p, 'r') as zf:\n                    for name in zf.namelist():\n                        if name.lower().endswith('.wav') and not name.endswith('/'):\n                            try:\n                                data = zf.read(name)\n                                audio.append((f\"zip:{os.path.basename(str(p))}/{name}\", io.BytesIO(data)))\n                            except Exception:\n                                continue\n            except Exception:\n                continue\n        elif pl.endswith('.wav'):\n            try:\n                with open(p, 'rb') as f:\n                    audio.append((os.path.basename(str(p)), io.BytesIO(f.read())))\n            except Exception:\n                continue\n    return audio\n\n\ndef evaluate(workflow, context):\n    try:\n        audio = _collect_audio_bytes(context)\n        if not audio:\n            return 0.0, \"No WAV files available to verify sample rate/bit depth.\"\n\n        # Target: 48kHz and 24-bit (3 bytes). 32-bit float (4 bytes) gets partial credit.\n        total = 0\n        count = 0\n        details = []\n        for name, bio in audio:\n            try:\n                bio.seek(0)\n                with contextlib.closing(wave.open(bio, 'rb')) as w:\n                    sr = w.getframerate()\n                    sw = w.getsampwidth()  # bytes per sample\n                    # Score per file: SR must be 48000; bit-depth 3 bytes = full, 4 bytes = partial.\n                    if sr == 48000:\n                        if sw == 3:\n                            total += 1.0\n                            details.append(f\"{name}: 48kHz, 24-bit OK\")\n                        elif sw == 4:\n                            total += 0.75\n                            details.append(f\"{name}: 48kHz, 32-bit float (partial)\")\n                        else:\n                            details.append(f\"{name}: 48kHz, {sw*8}-bit (non-compliant)\")\n                    else:\n                        details.append(f\"{name}: {sr}Hz (non-compliant)\")\n                    count += 1\n            except Exception as e:\n                details.append(f\"{name}: unreadable ({e})\")\n                count += 1\n        if count == 0:\n            return 0.0, \"No readable WAV headers.\"\n        # Average score across files, scaled to weight 1.2\n        avg = total / max(count, 1)\n        score = avg * 1.2\n        return min(max(score, 0.0), 1.2), \"; \".join(details)\n    except Exception as e:\n        return 0.0, f\"Exception in SR/bit-depth check: {e}\""}, {"type": "code", "name": "Duration and Alignment Consistency", "description": "Verify master ~2:17 long and stems closely match master duration (tight render-from-zero alignment).", "weight": 1.2, "code": "import os, io, zipfile, wave, contextlib\n\ndef _load_wavs(context):\n    # Returns dict name_lower -> (frames, sr)\n    files = {}\n    outputs = context.get_all_outputs()\n    def add(name, bio):\n        with contextlib.closing(wave.open(bio, 'rb')) as w:\n            files[name.lower()] = (w.getnframes(), w.getframerate())\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n        except Exception:\n            continue\n        pl = str(p).lower()\n        if pl.endswith('.zip'):\n            try:\n                with zipfile.ZipFile(p, 'r') as zf:\n                    for name in zf.namelist():\n                        if name.lower().endswith('.wav') and not name.endswith('/'):\n                            try:\n                                bio = io.BytesIO(zf.read(name))\n                                add(name, bio)\n                            except Exception:\n                                continue\n            except Exception:\n                continue\n        elif pl.endswith('.wav'):\n            try:\n                with open(p, 'rb') as f:\n                    add(os.path.basename(str(p)), io.BytesIO(f.read()))\n            except Exception:\n                continue\n    return files\n\n\ndef evaluate(workflow, context):\n    try:\n        files = _load_wavs(context)\n        if not files:\n            return 0.0, \"No WAV files to evaluate duration/alignment.\"\n        # Identify master candidate\n        names = list(files.keys())\n        master_candidates = [n for n in names if any(k in n for k in ['master', 'mix', 'final'])]\n        if not master_candidates:\n            return 0.0, \"No master track detected (name contains 'master'/'mix'/'final').\"\n        master_name = master_candidates[0]\n        m_frames, m_sr = files[master_name]\n        m_dur = (m_frames / m_sr) if m_sr else 0\n        # Expect ~137s. Allow 132\u2013142 seconds as approx 2:17.\n        approx_ok = 132.0 <= m_dur <= 142.0\n        # Stems to check\n        stem_keys = {\n            'guitars': ['guitar','guitars','gtr'],\n            'synths': ['synth'],\n            'bridge': ['bridge'],\n            'bass': ['bass']\n        }\n        deltas = []\n        present = 0\n        for label, preds in stem_keys.items():\n            candidates = [n for n in names if any(p in n for p in preds)]\n            if candidates:\n                s_name = candidates[0]\n                s_frames, s_sr = files[s_name]\n                s_dur = (s_frames / s_sr) if s_sr else 0\n                deltas.append(abs(s_dur - m_dur))\n                present += 1\n        # Alignment tightness: stems duration within 0.2s of master\n        align_ok_frac = 0.0 if present == 0 else sum(1 for d in deltas if d <= 0.2) / present\n\n        # Scoring: 40% for master approx duration, 60% for alignment tightness\n        score = (0.4 * (1.0 if approx_ok else 0.0)) + (0.6 * align_ok_frac)\n        return min(max(score * 1.2, 0.0), 1.2), f\"Master duration: {m_dur:.2f}s (approx OK: {approx_ok}); Stem alignment fraction <=0.2s: {align_ok_frac:.2f}\"\n    except Exception as e:\n        return 0.0, f\"Exception in duration/alignment check: {e}\""}, {"type": "code", "name": "Tempo and Key Map Verification", "description": "Verify a Tempo/Key CSV map exists and encodes: BPM ~140 and Bridge section 1:22\u20131:49 in Ab Major; other sections G Major.", "weight": 0.7, "code": "import os, io, zipfile, re\nimport pandas as pd\n\ndef _find_csvs(context):\n    csvs = []\n    outputs = context.get_all_outputs()\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n        except Exception:\n            continue\n        pl = str(p).lower()\n        if pl.endswith('.zip'):\n            try:\n                with zipfile.ZipFile(p, 'r') as zf:\n                    for name in zf.namelist():\n                        if name.lower().endswith('.csv') and any(k in name.lower() for k in ['tempo', 'key', 'map']):\n                            try:\n                                data = zf.read(name)\n                                csvs.append((f\"zip:{os.path.basename(str(p))}/{name}\", io.BytesIO(data)))\n                            except Exception:\n                                continue\n            except Exception:\n                continue\n        elif pl.endswith('.csv'):\n            try:\n                with open(p, 'rb') as f:\n                    csvs.append((os.path.basename(str(p)), io.BytesIO(f.read())))\n            except Exception:\n                continue\n    return csvs\n\n\ndef _to_seconds(v):\n    if pd.isna(v): return None\n    s = str(v).strip()\n    m = re.match(r\"^(\\d+):(\\d{1,2})(?::(\\d{1,2}))?$\", s)\n    if m:\n        parts = [int(x) for x in m.groups() if x is not None]\n        if len(parts) == 2:\n            return parts[0]*60 + parts[1]\n        elif len(parts) == 3:\n            return parts[0]*3600 + parts[1]*60 + parts[2]\n    try:\n        return float(s)\n    except:\n        return None\n\n\ndef evaluate(workflow, context):\n    try:\n        csvs = _find_csvs(context)\n        if not csvs:\n            return 0.0, \"No Tempo/Key CSV found (e.g., Tempo_Key_Map.csv).\"\n        # Use the first relevant CSV\n        name, bio = csvs[0]\n        bio.seek(0)\n        try:\n            df = pd.read_csv(bio)\n        except Exception:\n            return 0.0, f\"Tempo/Key CSV unreadable: {name}\"\n        cols = {c.lower().strip(): c for c in df.columns}\n        # Flexible matching\n        sec_col = next((cols[c] for c in cols if 'section' in c), None)\n        start_col = next((cols[c] for c in cols if 'start' in c), None)\n        end_col = next((cols[c] for c in cols if 'end' in c), None)\n        key_col = next((cols[c] for c in cols if 'key' in c), None)\n        bpm_col = next((cols[c] for c in cols if 'bpm' in c or 'tempo' in c), None)\n        if not all([sec_col, start_col, end_col, key_col]):\n            return 0.0, f\"CSV missing required columns (need Section, Start, End, Key).\"\n\n        # Normalize\n        df['_section'] = df[sec_col].astype(str).str.lower()\n        df['_start_s'] = df[start_col].apply(_to_seconds)\n        df['_end_s'] = df[end_col].apply(_to_seconds)\n        df['_key'] = df[key_col].astype(str).str.lower().str.replace(' major','').str.replace('maj','').str.strip()\n        if bpm_col:\n            df['_bpm'] = pd.to_numeric(df[bpm_col], errors='coerce')\n        else:\n            df['_bpm'] = None\n\n        # Checks\n        score = 0.0\n        fb = []\n        # Bridge in Ab major around 1:22\u20131:49\n        bridge = df[df['_section'].str.contains('bridge', na=False)]\n        if not bridge.empty:\n            row = bridge.iloc[0]\n            start_ok = (row['_start_s'] is not None) and (abs(row['_start_s'] - 82) <= 3)\n            end_ok = (row['_end_s'] is not None) and (abs(row['_end_s'] - 109) <= 3)\n            key_ok = ('ab' in row['_key'])\n            # 0.4 points for bridge correctness\n            bridge_score = (1.0 if (start_ok and end_ok and key_ok) else 0.0)\n            score += 0.4 * bridge_score\n            fb.append(f\"Bridge start {row['_start_s']}s ok={start_ok}, end {row['_end_s']}s ok={end_ok}, key {row['_key']} ok={key_ok}\")\n        else:\n            fb.append(\"No bridge section found in CSV.\")\n\n        # Main sections in G major (anything not bridge)\n        non_bridge = df[~df['_section'].str.contains('bridge', na=False)]\n        if not non_bridge.empty:\n            # 0.2 points if all non-bridge rows contain G\n            keys = non_bridge['_key'].dropna().tolist()\n            if keys and all(('g' in k) for k in keys):\n                score += 0.2\n                fb.append(\"Non-bridge keys all indicate G.\")\n            else:\n                fb.append(f\"Non-bridge keys not all G: {keys}\")\n        else:\n            fb.append(\"No non-bridge sections to verify G major.\")\n\n        # BPM check ~140\n        if '_bpm' in df.columns:\n            # 0.1 points if median BPM within [139,141]\n            bpm_vals = df['_bpm'].dropna().tolist()\n            if bpm_vals:\n                med = float(pd.Series(bpm_vals).median())\n                if 139.0 <= med <= 141.0:\n                    score += 0.1\n                    fb.append(f\"BPM median {med} within 140\u00b11.\")\n                else:\n                    fb.append(f\"BPM median {med} outside 140\u00b11.\")\n            else:\n                fb.append(\"No BPM values present.\")\n        else:\n            fb.append(\"No BPM column present.\")\n\n        return min(score, 0.7), \"; \".join(fb)\n    except Exception as e:\n        return 0.0, f\"Exception in Tempo/Key verification: {e}\""}, {"type": "code", "name": "Drum Sync and Compliance Declarations", "description": "Check Delivery Report text for drum reference usage, BPM 140 acknowledgment, and Tracklib compliance statement.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs()\n        # Find a delivery report document among outputs\n        doc_texts = []\n        for r in outputs:\n            try:\n                if r.is_document:\n                    # Try PDF then DOCX\n                    text = None\n                    p = None\n                    try:\n                        text = context.files.read_pdf_text(r.id)\n                    except Exception:\n                        try:\n                            text = context.files.read_docx_text(r.id)\n                        except Exception:\n                            text = None\n                    if text:\n                        doc_texts.append(text.lower())\n                elif r.is_text_format:\n                    try:\n                        t = context.files.read_text(r.id)\n                        if t:\n                            doc_texts.append(t.lower())\n                    except Exception:\n                        pass\n            except Exception:\n                continue\n        if not doc_texts:\n            return 0.0, \"No readable Delivery Report text found.\"\n        text = \"\\n\".join(doc_texts)\n\n        hits = 0\n        fb = []\n        # Drum reference usage\n        if 'drum reference track' in text or 'drum reference' in text or 'drum track' in text:\n            hits += 1\n            fb.append(\"Mentions drum reference.\")\n        else:\n            fb.append(\"No explicit drum reference mention.\")\n        # BPM 140 acknowledgment\n        if re.search(r\"\\b140\\s*bpm\\b\", text) or re.search(r\"bpm\\s*[:=]\\s*140\", text):\n            hits += 1\n            fb.append(\"Mentions 140 BPM.\")\n        else:\n            fb.append(\"No explicit 140 BPM mention.\")\n        # Tracklib compliance\n        if 'tracklib' in text:\n            hits += 1\n            fb.append(\"Mentions Tracklib compliance.\")\n        else:\n            fb.append(\"No Tracklib compliance mention.\")\n\n        score = (hits / 3.0) * 0.4\n        return min(score, 0.4), \"; \".join(fb)\n    except Exception as e:\n        return 0.0, f\"Exception in declarations check: {e}\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Production Fit and Documentation Quality (LLM)", "description": "Holistic assessment of how well the documented production meets the brief and the professionalism of the delivery.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Style Fit, Arrangement, and Delivery Professionalism", "description": "Evaluate the Delivery Report for evidence that the production matches the creative brief and is studio-ready.", "weight": 2.0, "judge_prompt": "Evaluate the Delivery Report (PDF/DOCX/text) for the following. You are not judging audio quality directly; rely on documented intent and specifics:\n\nCriteria (consider section evidence, be lenient on naming):\n- Creative Fit to Brief: Does the documentation describe an uptempo, bright, tightly looped bossa-influenced groove with a crisp, modern, punchy, high-energy beat? Look for explicit references to groove pattern, syncopation, percussive accents, and time-based effects creating drive.\n- Instrumentation: Clear use of Guitars, Synths (optionally named DX7, Prophet 5, ARP 2600), and a punchy Bass (MiniMoog acceptable). Are choices justified to meet inspiration?\n- Keys and Structure: Main sections in G Major; bridge at ~1:22\u20131:49 in Ab Major. The document should explain how this supports the vocalist.\n- Tempo and Sync: 140 BPM with tight grid alignment to the provided drum reference. Any mention of render-from-zero, sample-accurate start, or offset checks is positive.\n- Studio-Readiness: Stem naming clarity, file manifest clarity, technical specs, headroom/gain staging, and any notes for the recording engineer.\n\nScoring (0\u20132):\n- 2.0: Strong, specific alignment with the brief; clear instrumentation rationale; keys/structure explained; impeccable delivery details.\n- 1.2: Generally aligned but missing depth in one area (e.g., limited arrangement notes or vague effect usage).\n- 0.6: Partial alignment with several gaps or vague/marketing language without specifics.\n- 0.0: Does not demonstrate fit to the brief or lacks professional delivery clarity.\n\nJudge based on the document contents only (no need to open audio).", "expectation": "A clear, specific, and professional Delivery Report that convincingly documents a bright, punchy bossa-influenced production at 140 BPM, with stems and specs ready for a recording session."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "2fa8e956-7b35-4c13-95dc-027f02be318b", "rubric": {"category_name": "Concierge Document: Napa Wineries Within One Hour (Word, Structured, Verifiable)", "rationale": "This rubric enforces a self-documenting, verifiable Word document that a lifestyle manager would provide to an ultra-high-net-worth member. Stage 1 (LLM-only) strictly mandates a DOCX format, page limit, footer, fonts/colors, a royalty-free image with attribution, and a clearly labeled per-winery structure that makes verification trivial. Stage 2 uses code rules to parse the DOCX text and verify field presence, numeric/time bounds, grape variety counts, phone formats, and Google Maps links. Stage 3 evaluates overall presentation quality and member suitability.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Format and Structured Document Gate", "description": "LLM-only gate to enforce exact deliverable shape: Word (DOCX), 1\u20134 pages, footer text and fonts, embedded royalty-free Napa image with attribution, and per-winery labeled fields enabling automated verification.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Word Doc Structure and Formatting Requirement", "description": "Check that the output is a Microsoft Word document with the mandated layout, labels, and visual formatting so later code checks can read it.", "weight": 4.0, "judge_prompt": "You are evaluating a single candidate output. Only evaluate the primary output of the workflow. This is a SHAPE gate. Verify the document is a Microsoft Word file (DOCX) and has the exact structural format below. Do NOT judge content quality or correctness here\u2014only presence/format.\n\nRequired Format and Layout (must all be visible in the rendered document):\n1) File type and length:\n   - Must be a DOCX (Microsoft Word) file (not PDF/Excel/plain text).\n   - Length: 1\u20134 pages total.\n\n2) Footer (all pages):\n   - Footer text exactly: \"Napa Valley Wineries\".\n   - Footer font: Georgia, 14 pt.\n\n3) Body text fonts/colors:\n   - Default body text font: Georgia, 9 pt, black.\n   - The label \"Grape Varieties:\" and the grape variety values must be purple, Georgia 9 pt.\n\n4) Title and context:\n   - A clear title near the top similar to: \"Wineries Within One Hour of The Westin Verasa Napa\" (flexible wording OK but must include the hotel name and one-hour context).\n\n5) Royalty-free image requirement:\n   - Include at least one embedded, relevant photo of Napa Valley vineyards.\n   - Provide an attribution/caption that indicates a royalty-free source (e.g., Unsplash, Pexels, Wikimedia Commons, Pixabay) with a source/URL visible.\n\n6) Per-winery structured blocks (exact labels):\n   - The document must list wineries as repeated, clearly separated blocks. Each block must use the following exact labels (case-insensitive match OK) each on its own line or with the value on the same line:\n     \u2022 Name:\n     \u2022 Grape Varieties:\n     \u2022 Description: (1\u20132 sentences)\n     \u2022 Visiting Hours:\n     \u2022 Address:\n     \u2022 Phone:\n     \u2022 Distance from hotel: (in miles)\n     \u2022 Estimated drive time: (in minutes or hr/min)\n     \u2022 Sources: (must include at least one winery/reputable source link AND one Google Maps link)\n   - A minimum of 8 winery blocks are required.\n\n7) Optional sections:\n   - A brief note on how distances and drive times were obtained (e.g., from Google Maps) is helpful but not required if each block has Sources.\n\nScoring Guidelines (STRUCTURE ONLY):\n- 4.0: DOCX + 1\u20134 pages + footer present with correct text and font + grape varieties purple Georgia 9 + body text Georgia 9 black + at least one embedded Napa vineyards photo with royalty-free attribution/URL + title with hotel + \u22658 winery blocks with ALL required labels including Sources (with a Maps link).\n- 3.0: Minor deviations in fonts/colors OR one winery block missing one non-critical label OR image attribution present but not perfectly formatted; all other requirements met.\n- 2.0: Several structural issues: e.g., fewer than 8 blocks OR missing footer OR no image attribution OR multiple missing labels across blocks.\n- 1.0: DOCX present but major structure missing (no labeled blocks, no footer, no image, or >4 pages).\n- 0.0: Not a DOCX OR impossible to identify the required structure.\n\nImportant: Do NOT check factual correctness of distances, hours, or tasting details. Only verify the presence and formatting/structure that enables later automated checks.", "expectation": "A 1\u20134 page DOCX with footer text \u201cNapa Valley Wineries\u201d (Georgia 14), body text Georgia 9 black, grape varieties in purple Georgia 9, a royalty-free Napa vineyards image with attribution/URL, a clear title referencing The Westin Verasa Napa and one-hour context, and \u22658 winery blocks using the exact labels including Sources with at least one Google Maps link per winery."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification and Consistency Checks", "description": "Code-based checks on correctness and internal consistency made possible by the Stage 1 structure.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Labeled Field Presence per Winery", "description": "Verify each winery block contains all required labeled fields.", "weight": 1.3, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        text = None\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                return 0.0\n        if not text:\n            return 0.0\n        tl = text\n        # Split into winery blocks by 'Name:' label\n        parts = re.split(r\"(?i)\\n\\s*name:\\s*\", tl)\n        if len(parts) <= 1:\n            return 0.0\n        blocks = []\n        for p in parts[1:]:\n            blocks.append(\"Name: \" + p)\n        required_labels = [\n            r\"(?i)name:\\s*\",\n            r\"(?i)grape varieties:\\s*\",\n            r\"(?i)description:\\s*\",\n            r\"(?i)visiting hours:\\s*\",\n            r\"(?i)address:\\s*\",\n            r\"(?i)phone:\\s*\",\n            r\"(?i)distance from hotel:\\s*\",\n            r\"(?i)estimated drive time:\\s*\",\n            r\"(?i)sources:\\s*\",\n        ]\n        if len(blocks) == 0:\n            return 0.0\n        scores = []\n        for b in blocks:\n            present = 0\n            for lab in required_labels:\n                if re.search(lab, b):\n                    present += 1\n            scores.append(present / len(required_labels))\n        avg = sum(scores) / len(scores) if scores else 0.0\n        return 1.3 * avg\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Drive Time Within 60 Minutes", "description": "Parse 'Estimated drive time:' for each winery and score the fraction at or under 60 minutes.", "weight": 0.8, "code": "import re\n\ndef _parse_minutes(val: str):\n    s = val.lower()\n    # 1: patterns like '45 min' or '45 minutes'\n    m = re.search(r\"(\\d{1,3})\\s*(?:min|minutes)\\b\", s)\n    if m:\n        return int(m.group(1))\n    # 2: '1 hr 5 min' or '1 hour 5 minutes'\n    m = re.search(r\"(\\d{1,2})\\s*(?:h|hr|hrs|hour|hours)\\b\\s*(\\d{1,2})?\\s*(?:min|minutes)?\", s)\n    if m:\n        h = int(m.group(1))\n        mins = int(m.group(2)) if m.group(2) else 0\n        return h * 60 + mins\n    # 3: '1 hr' or '2 hours'\n    m = re.search(r\"(\\d{1,2})\\s*(?:h|hr|hrs|hour|hours)\\b\", s)\n    if m:\n        return int(m.group(1)) * 60\n    # 4: time-like '0:45' or '1:05'\n    m = re.search(r\"\\b(\\d{1,2})[:h](\\d{1,2})\\b\", s)\n    if m:\n        return int(m.group(1)) * 60 + int(m.group(2))\n    return None\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        text = None\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                return 0.0\n        if not text:\n            return 0.0\n        # Extract Estimated drive time lines per block\n        parts = re.split(r\"(?i)\\n\\s*name:\\s*\", text)\n        if len(parts) <= 1:\n            return 0.0\n        blocks = [\"Name: \" + p for p in parts[1:]]\n        total = 0\n        ok = 0\n        for b in blocks:\n            m = re.search(r\"(?i)estimated drive time:\\s*([^\\n]+)\", b)\n            if m:\n                total += 1\n                mins = _parse_minutes(m.group(1))\n                if mins is not None and mins <= 60:\n                    ok += 1\n        if total == 0:\n            return 0.0\n        frac = ok / total\n        return 0.8 * frac\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Distance Plausibility (<= 60 miles, non-negative)", "description": "Check that 'Distance from hotel:' values are present, numeric, and <= 60 miles.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                return 0.0\n        if not text:\n            return 0.0\n        parts = re.split(r\"(?i)\\n\\s*name:\\s*\", text)\n        if len(parts) <= 1:\n            return 0.0\n        blocks = [\"Name: \" + p for p in parts[1:]]\n        total = 0\n        ok = 0\n        for b in blocks:\n            m = re.search(r\"(?i)distance from hotel:\\s*([^\\n]+)\", b)\n            if m:\n                total += 1\n                s = m.group(1)\n                n = re.search(r\"(\\d+(?:\\.\\d+)?)\\s*(?:mi|miles)\\b\", s.lower())\n                if n:\n                    miles = float(n.group(1))\n                    if miles >= 0 and miles <= 60:\n                        ok += 1\n        if total == 0:\n            return 0.0\n        return 0.5 * (ok / total)\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Grape Variety Count (>= 3 per winery)", "description": "Ensure each winery lists at least three grape varieties.", "weight": 0.6, "code": "import re\n\ndef _count_varieties(s):\n    # Split on commas, semicolons, slashes, ' and '\n    t = s.strip()\n    # Normalize separators\n    t = re.sub(r\"\\s+/\\s+\", \",\", t)\n    t = t.replace(\";\", \",\")\n    t = t.replace(\" and \", \",\")\n    items = [x.strip() for x in t.split(\",\") if x.strip()]\n    # Deduplicate case-insensitive\n    uniq = set([i.lower() for i in items])\n    return len([u for u in uniq if u])\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                return 0.0\n        if not text:\n            return 0.0\n        parts = re.split(r\"(?i)\\n\\s*name:\\s*\", text)\n        if len(parts) <= 1:\n            return 0.0\n        blocks = [\"Name: \" + p for p in parts[1:]]\n        total = 0\n        ok = 0\n        for b in blocks:\n            m = re.search(r\"(?i)grape varieties:\\s*([^\\n]+)\", b)\n            if m:\n                total += 1\n                cnt = _count_varieties(m.group(1))\n                if cnt >= 3:\n                    ok += 1\n        if total == 0:\n            return 0.0\n        return 0.6 * (ok / total)\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "US Phone Number Format", "description": "Check that listed phone numbers look like valid US patterns per winery.", "weight": 0.3, "code": "import re\n\ndef _is_us_phone(s):\n    s = s.strip()\n    # Accept formats: (707) 555-1234, 707-555-1234, 707.555.1234, 707 555 1234\n    return bool(re.search(r\"\\b\\(?\\d{3}\\)?[-\\.\\s]?\\d{3}[-\\.\\s]\\d{4}\\b\", s))\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                return 0.0\n        if not text:\n            return 0.0\n        parts = re.split(r\"(?i)\\n\\s*name:\\s*\", text)\n        if len(parts) <= 1:\n            return 0.0\n        blocks = [\"Name: \" + p for p in parts[1:]]\n        total = 0\n        ok = 0\n        for b in blocks:\n            m = re.search(r\"(?i)phone:\\s*([^\\n]+)\", b)\n            if m:\n                total += 1\n                if _is_us_phone(m.group(1)):\n                    ok += 1\n        if total == 0:\n            return 0.0\n        return 0.3 * (ok / total)\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Google Maps Link Presence in Sources", "description": "For each winery, ensure Sources contains at least one Google Maps link.", "weight": 0.3, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                return 0.0\n        if not text:\n            return 0.0\n        parts = re.split(r\"(?i)\\n\\s*name:\\s*\", text)\n        if len(parts) <= 1:\n            return 0.0\n        blocks = [\"Name: \" + p for p in parts[1:]]\n        total = 0\n        ok = 0\n        for b in blocks:\n            m = re.search(r\"(?i)sources:\\s*(.+?)(?:\\n\\s*name:|\\Z)\", b, flags=re.S)\n            if m:\n                total += 1\n                s = m.group(1)\n                if re.search(r\"https?://(?:www\\.)?google\\.[^/]+/maps|https?://goo\\.gl/maps\", s, flags=re.I):\n                    ok += 1\n        if total == 0:\n            return 0.0\n        return 0.3 * (ok / total)\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Tasting Mention Present", "description": "Check that each winery block mentions tasting (experience) in Description or Hours.", "weight": 0.2, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                return 0.0\n        if not text:\n            return 0.0\n        parts = re.split(r\"(?i)\\n\\s*name:\\s*\", text)\n        if len(parts) <= 1:\n            return 0.0\n        blocks = [\"Name: \" + p for p in parts[1:]]\n        total = 0\n        ok = 0\n        for b in blocks:\n            total += 1\n            if re.search(r\"(?i)tasting\", b):\n                ok += 1\n        if total == 0:\n            return 0.0\n        return 0.2 * (ok / total)\n    except Exception:\n        return 0.0"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Presentation Quality and Member Fit", "description": "LLM assessment of professionalism, clarity, visual appeal, and suitability for an UHNW member.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Utility", "description": "Holistic assessment of clarity, concision, visual hierarchy, and decision-enabling value.", "weight": 2.0, "judge_prompt": "Evaluate the overall professionalism and usefulness of the DOCX deliverable for an ultra-high-net-worth member. Assume Stage 1 structure exists; focus on quality, not raw presence.\n\nConsider:\n- Readability and scannability: Are winery blocks easy to scan? Are labels aligned, spacing consistent, and information concise (1\u20132 sentence descriptions)?\n- Visual polish: Consistent fonts/sizes, effective use of purple for grape varieties without overuse; tasteful placement/size of the vineyard photo; footer unobtrusive and consistent.\n- Member relevance: Selections reflect a variety of grape types and experiences; mix of well-known and distinctive options; hours and contact details presented clearly for quick action.\n- Concision: Stays within 1\u20134 pages while being complete; no unnecessary verbosity.\n- Actionability: Distances and drive times are easy to compare; sources/links make it easy to navigate to maps and winery pages.\n\nScoring:\n- 2.0: Highly professional, polished, concise, and decision-ready.\n- 1.3: Good quality with minor layout or clarity issues but still member-ready.\n- 0.7: Adequate but cluttered or inconsistently formatted; requires edits.\n- 0.0: Poorly presented, hard to read, or not useful for quick decision-making.", "expectation": "A polished, concise, and visually consistent 1\u20134 page DOCX that a concierge could confidently send to an UHNW member to quickly choose among wineries."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "e6429658-4de1-42dd-a9e0-2d2b9b02fb10", "rubric": {"category_name": "Medical Necessity Appeal and Patient Assistance Application", "rationale": "This rubric enforces a self-documenting, two-deliverable workflow: (1) a 2\u20134 page DOCX medical necessity appeal letter for Vraylar 1.5 mg and (2) a digitally completed AbbVie patient assistance application PDF. Stage 1 is an LLM-only gate that verifies each artifact\u2019s required format and structure so later verification is trivial. Stage 2 mixes code and LLM checks to validate factual/structural correctness and cross-document consistency (drug, dose, patient identity, rationale, prior trials). Stage 3 assesses professional quality and audience appropriateness.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate", "description": "LLM-only verification that BOTH required deliverables exist in the exact, verifiable structure: a 2\u20134 page DOCX appeal letter named like 'Vraylar Appeal for RP' and a digitally filled AbbVie assistance application PDF named like 'RP Financial Assistance Application'.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Appeal Letter Format and Structure (DOCX)", "description": "Verify the medical necessity appeal letter exists as a 2\u20134 page DOCX, named like 'Vraylar Appeal for RP', with core sections enabling verification.", "weight": 2.5, "judge_prompt": "You are checking ALL provided outputs. Confirm there is a Word (DOCX) appeal letter for Vraylar.\n\nFormat and file requirements:\n- File must be DOCX (not PDF, not plain text).\n- File name should closely match: \u201cVraylar Appeal for RP\u201d (flexible: minor variations/case ok if it clearly indicates Vraylar + Appeal + RP).\n- Length: 2\u20134 pages.\n\nRequired structural elements (flexible headings, but content must be clearly present):\n1) Header and Addressing: Date and addressee (payer or \u201cTo Whom It May Concern\u201d).\n2) Patient Identifiers: Patient full name and at least one of age/DOB/member ID.\n3) Diagnosis and History: Treatment-resistant depression and brief relevant psychiatric history.\n4) Current Therapy: Vraylar (cariprazine) 1.5 mg once daily and 2-year stability.\n5) Prior Trials/Intolerance: Failed or intolerant prior antidepressant trials.\n6) Medical Necessity Rationale: Continuity of care, risk of relapse, prior side effects/lack of efficacy, financial hardship, and request for tier/formulary exception or coverage determination.\n7) Provider Block: Clinician name/credentials (e.g., NP/APRN), contact info, and signature/attestation.\n\nScoring (return a score from 0.0 to 2.5):\n- 2.5: DOCX present, 2\u20134 pages, and all 7 structural elements clearly present.\n- 2.0: DOCX present, 2\u20134 pages, missing 1 structural element OR minor naming deviation only.\n- 1.0: DOCX present but missing 2\u20133 structural elements OR length slightly off (e.g., short of 2 pages) but still substantive.\n- 0.0: Not DOCX, not present, or egregiously wrong shape (e.g., 0\u20131 page notes, no identifiable sections). Do NOT judge content quality\u2014only presence and structure.", "expectation": "A professionally formatted, 2\u20134 page DOCX appeal letter named like \u201cVraylar Appeal for RP,\u201d containing patient identifiers, diagnosis/history, prior trials, rationale for Vraylar 1.5 mg, and a provider signature block."}, {"type": "llm_judge", "name": "AbbVie Patient Assistance Application (PDF)", "description": "Verify the AbbVie assistance application PDF is present and digitally completed using available information.", "weight": 1.5, "judge_prompt": "You are checking ALL outputs. Confirm there is a digitally completed AbbVie Patient Assistance Application PDF for Vraylar.\n\nFormat and file requirements:\n- Must be a PDF of the AbbVie Patient Assistance Application form (the layout/title typically includes AbbVie branding and \u201cPatient Assistance Program\u201d).\n- File name should closely match: \u201cRP Financial Assistance Application\u201d (flexible: minor variations/case ok if it clearly indicates RP + Financial Assistance/Application).\n- Visible typed entries (digital completion). Signatures may be typed/electronic.\n\nRequired sections to be at least partially completed (leave blank only where details not provided):\n1) Patient Information (name; and any available DOB, address, phone).\n2) Insurance/Financial info (plan/member details, or indication if no coverage; household income if available).\n3) Prescriber/Clinic information (provider name/credentials, clinic, contact; NPI if available).\n4) Medication details (Vraylar/cariprazine; 1.5 mg strength; quantity/frequency if available).\n5) Attestations/authorizations (as applicable) with typed names/signatures if required.\n\nScoring (return a score from 0.0 to 1.5):\n- 1.5: PDF of the AbbVie form, clearly digitally completed with typed entries; Vraylar listed with strength; patient and prescriber sections reasonably filled; other sections completed where info is available.\n- 1.0: PDF present and mostly filled but missing 1\u20132 of the listed sections or medication strength not explicit.\n- 0.5: PDF present but minimally filled (mostly blank) or uncertain if digitally completed.\n- 0.0: Not a PDF, wrong form, or essentially unfilled.\n\nOnly verify structure and visible completion, not the correctness of specific values.", "expectation": "A digitally completed AbbVie patient assistance PDF for Vraylar with patient, insurance/financial, prescriber, and medication sections populated where information is available."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification of Correctness and Consistency", "description": "Deterministic code checks and an LLM cross-check ensure the outputs contain the required clinical claims, structure, and cross-document consistency (drug, dose, patient identity), and that the AbbVie PDF appears digitally filled.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "File Naming Compliance", "description": "Checks filenames for close compliance with requested names for both deliverables.", "weight": 0.4, "code": "import re\nfrom pathlib import Path\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0, \"No outputs found.\"\n    score = 0.0\n    fb = []\n\n    def norm(s):\n        return re.sub(r\"[^a-z0-9]+\", \" \", s.lower()).strip()\n\n    letter_tokens = {\"vraylar\", \"appeal\", \"rp\"}\n    app_tokens = {\"rp\", \"financial\", \"assistance\", \"application\"}\n\n    letter_score = 0.0\n    app_score = 0.0\n\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            name = p.name\n            n = norm(name)\n            if p.suffix.lower() == \".docx\":\n                hits = sum(1 for t in letter_tokens if t in n)\n                if hits == len(letter_tokens):\n                    letter_score = max(letter_score, 0.2)\n                elif hits >= 2:\n                    letter_score = max(letter_score, 0.12)\n                fb.append(f\"DOCX '{name}' token matches: {hits}/{len(letter_tokens)}\")\n            if p.suffix.lower() == \".pdf\":\n                hits = sum(1 for t in app_tokens if t in n)\n                # Allow 'abbvie' + 'application' as an alternative\n                alt_hits = (\"abbvie\" in n) + (\"application\" in n)\n                if hits == len(app_tokens) or alt_hits == 2:\n                    app_score = max(app_score, 0.2)\n                elif hits >= 2 or alt_hits == 1:\n                    app_score = max(app_score, 0.12)\n                fb.append(f\"PDF '{name}' token matches: {hits}/{len(app_tokens)}; alt_hits={alt_hits}\")\n        except Exception as e:\n            fb.append(f\"Error inspecting resource: {e}\")\n\n    score = letter_score + app_score\n    score = min(score, 0.4)\n    return score, \"; \".join(fb) if fb else \"Checked filenames.\""}, {"type": "code", "name": "Appeal Letter \u2014 Key Clinical Content Present", "description": "DOCX contains core clinical elements: Vraylar, cariprazine, dose, medical necessity, TRD, continuity-of-care, financial hardship, etc.", "weight": 1.2, "code": "import re\n\ndef evaluate(workflow, context):\n    # Find DOCX appeal letter and scan for key content\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0, \"No outputs found.\"\n\n    letter = None\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            if p.suffix.lower() == \".docx\":\n                letter = r\n                break\n        except Exception:\n            continue\n\n    if not letter:\n        return 0.0, \"No DOCX appeal letter found.\"\n\n    try:\n        text = context.files.read_docx_text(letter.id) or \"\"\n    except Exception as e:\n        return 0.0, f\"Failed to read DOCX: {e}\"\n\n    t = text.lower()\n    keywords = [\n        \"vraylar\", \"cariprazine\", \"1.5 mg\", \"1.5mg\", \"medical necessity\",\n        \"treatment-resistant\", \"trd\", \"depression\", \"continuity of care\",\n        \"financial hardship\", \"tier exception\", \"formulary exception\",\n        \"coverage determination\", \"appeal\", \"once daily\", \"stable\", \"insurer\", \"insurance\"\n    ]\n\n    matched = sum(1 for k in keywords if k in t)\n    # Expect at least 8 matches for full credit; scale up to weight\n    ratio = min(1.0, matched / 8.0)\n    score = ratio * 1.2\n    missing = [k for k in keywords if k not in t]\n    fb = f\"Matched {matched}/{len(keywords)} key elements. Missing examples: {', '.join(missing[:5])}\"\n    return score, fb"}, {"type": "code", "name": "Appeal Letter \u2014 Prior Trials/Intolerance Evidence", "description": "Detects mention of failed/inadequate/intolerant trials and at least one medication/class reference.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0, \"No outputs found.\"\n    letter = None\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            if p.suffix.lower() == \".docx\":\n                letter = r\n                break\n        except Exception:\n            continue\n    if not letter:\n        return 0.0, \"No DOCX appeal letter found.\"\n\n    try:\n        t = (context.files.read_docx_text(letter.id) or \"\").lower()\n    except Exception as e:\n        return 0.0, f\"Failed to read DOCX: {e}\"\n\n    trial_terms = [\"failed\", \"inadequate response\", \"nonresponse\", \"non-response\", \"intolerant\", \"side effect\", \"adverse effect\", \"trial\", \"switched\"]\n    meds = [\"ssri\", \"snri\", \"bupropion\", \"mirtazapine\", \"trazodone\", \"tca\", \"maoi\", \"aripiprazole\", \"quetiapine\", \"lithium\", \"venlafaxine\", \"duloxetine\", \"sertraline\", \"fluoxetine\", \"escitalopram\", \"paroxetine\", \"citalopram\"]\n\n    has_trial_lang = any(term in t for term in trial_terms)\n    med_hits = sum(1 for m in meds if m in t)\n\n    score = 0.0\n    fb_parts = []\n    if has_trial_lang:\n        score += 0.3\n        fb_parts.append(\"Found trial/intolerance language\")\n    else:\n        fb_parts.append(\"No clear trial/intolerance language detected\")\n    if med_hits > 0:\n        score += 0.3\n        fb_parts.append(f\"Medication/class mentions: {med_hits}\")\n    else:\n        fb_parts.append(\"No medication/class mentions detected\")\n\n    return score, \"; \".join(fb_parts)"}, {"type": "code", "name": "Appeal Letter \u2014 Structured Sections", "description": "Checks for presence of common section anchors (patient info, diagnosis, history, justification, request, signature).", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0, \"No outputs found.\"\n    letter = None\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            if p.suffix.lower() == \".docx\":\n                letter = r\n                break\n        except Exception:\n            continue\n    if not letter:\n        return 0.0, \"No DOCX appeal letter found.\"\n\n    try:\n        t = (context.files.read_docx_text(letter.id) or \"\").lower()\n    except Exception as e:\n        return 0.0, f\"Failed to read DOCX: {e}\"\n\n    anchors = [\n        \"patient\", \"diagnosis\", \"history\", \"treatment\", \"medication\",\n        \"justification\", \"rationale\", \"request\", \"appeal\", \"sincerely\",\n        \"signature\", \"npi\", \"contact\"\n    ]\n    hits = sum(1 for a in anchors if a in t)\n    # Expect at least 6 anchors for full credit\n    ratio = min(1.0, hits / 6.0)\n    score = ratio * 0.6\n    return score, f\"Section anchor hits: {hits}/{len(anchors)}\""}, {"type": "code", "name": "Cross-Document Consistency (Drug, Dose, Patient)", "description": "Verifies both documents consistently reference Vraylar and 1.5 mg, and attempts to match patient identity across documents.", "weight": 0.9, "code": "import re\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0, \"No outputs found.\"\n    letter = None\n    app = None\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            if p.suffix.lower() == \".docx\":\n                letter = r\n            elif p.suffix.lower() == \".pdf\":\n                app = r\n        except Exception:\n            continue\n\n    if not letter or not app:\n        return 0.0, \"Both DOCX letter and PDF application are required for this check.\"\n\n    try:\n        lt = (context.files.read_docx_text(letter.id) or \"\").lower()\n    except Exception as e:\n        return 0.0, f\"Failed to read DOCX: {e}\"\n\n    try:\n        pt = (context.files.read_pdf_text(app.id) or \"\").lower()\n    except Exception as e:\n        return 0.0, f\"Failed to read PDF: {e}\"\n\n    score = 0.0\n    fb = []\n\n    # Vraylar present in both\n    if \"vraylar\" in lt and \"vraylar\" in pt:\n        score += 0.3\n        fb.append(\"Vraylar present in both documents\")\n    else:\n        fb.append(\"Vraylar missing in one or both documents\")\n\n    # Dose present in both (1.5 mg)\n    dose_patterns = [\"1.5 mg\", \"1.5mg\"]\n    has_dose_letter = any(d in lt for d in dose_patterns)\n    has_dose_pdf = any(d in pt for d in dose_patterns)\n    if has_dose_letter and has_dose_pdf:\n        score += 0.3\n        fb.append(\"Dose 1.5 mg present in both documents\")\n    else:\n        fb.append(\"Dose 1.5 mg missing in one or both documents\")\n\n    # Patient identity \u2014 try to match 'robert palen' specifically, else any likely full name overlap\n    name_score = 0.0\n    target_first = \"robert\"\n    target_last = \"palen\"\n    if target_first in lt and target_last in lt and target_first in pt and target_last in pt:\n        name_score = 0.3\n        fb.append(\"Patient name 'Robert Palen' appears in both documents\")\n    else:\n        # fallback: attempt generic two-word capitalized name extraction from letter and check presence in pdf\n        # This is very heuristic and lenient\n        letter_names = set()\n        for m in re.finditer(r\"\\b([A-Z][a-z]+)\\s+([A-Z][a-z]+)\\b\", context.files.read_docx_text(letter.id) or \"\"):\n            letter_names.add((m.group(1).lower(), m.group(2).lower()))\n        found = False\n        for fn, ln in list(letter_names)[:20]:\n            if fn in pt and ln in pt:\n                found = True\n                break\n        if found:\n            name_score = 0.2\n            fb.append(\"Detected a likely matching full name across documents\")\n        else:\n            fb.append(\"Could not confirm matching patient name across documents\")\n    score += name_score\n\n    return min(score, 0.9), \"; \".join(fb)"}, {"type": "code", "name": "PDF Digital Completion Signals", "description": "Heuristics that the AbbVie PDF was digitally filled: presence of AbbVie branding, medication text, and typical typed data patterns (dates/phones/IDs).", "weight": 0.3, "code": "import re\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0, \"No outputs found.\"\n    app = None\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            if p.suffix.lower() == \".pdf\":\n                app = r\n                break\n        except Exception:\n            continue\n    if not app:\n        return 0.0, \"No application PDF found.\"\n\n    try:\n        t = (context.files.read_pdf_text(app.id) or \"\")\n        tl = t.lower()\n    except Exception as e:\n        return 0.0, f\"Failed to read PDF: {e}\"\n\n    score = 0.0\n    fb = []\n\n    if (\"abbvie\" in tl) or (\"patient assistance\" in tl):\n        score += 0.1\n        fb.append(\"AbbVie/PAP branding detected\")\n    else:\n        fb.append(\"No clear AbbVie branding text detected\")\n\n    if \"vraylar\" in tl or \"cariprazine\" in tl:\n        score += 0.1\n        fb.append(\"Medication name detected\")\n    else:\n        fb.append(\"Medication name not detected in PDF text\")\n\n    # Typed data signals: dates, phones, or an address-like pattern\n    has_date = re.search(r\"\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b\", t) is not None\n    has_phone = re.search(r\"\\(?\\d{3}\\)?[\\s-]?\\d{3}-\\d{4}\", t) is not None\n    has_addr = re.search(r\"\\b\\d{1,5}\\s+[A-Za-z][A-Za-z\\s]+\\b\", t) is not None\n\n    if has_date or has_phone or has_addr:\n        score += 0.1\n        fb.append(\"Detected typical typed data (date/phone/address)\")\n    else:\n        fb.append(\"No clear typed data patterns detected\")\n\n    return min(score, 0.3), \"; \".join(fb)"}, {"type": "llm_judge", "name": "Clinical Reasoning and Justification Check", "description": "LLM verifies the appeal articulates an evidence-based justification for continued Vraylar with prior failures/intolerance, continuity-of-care, and risk-of-relapse arguments; and that the application aligns with the clinical request.", "weight": 1.0, "judge_prompt": "Review both deliverables (the DOCX appeal letter and the AbbVie PDF application). Assess the clinical reasoning for continuing Vraylar 1.5 mg:\n\nLook for:\n- Clear diagnosis of treatment-resistant depression with concise psychiatric history.\n- Prior failed/intolerant trials or lack of efficacy with examples.\n- Rationale for medical necessity: stability on Vraylar for ~2 years, continuity of care, risk of relapse, patient-specific harms of switching, and financial hardship.\n- Appropriateness of request (tier/formulary exception or coverage determination) and professional, non-misleading clinical claims.\n- Cross-document alignment: The application\u2019s drug/strength and patient identity match the letter\u2019s request.\n\nScoring (0.0\u20131.0):\n- 1.0: Sound, specific, and consistent reasoning on all points; application aligns.\n- 0.7: Generally sound but missing one element (e.g., sparse trial details or weak continuity-of-care rationale).\n- 0.4: Superficial reasoning with vague or generic claims; limited evidence of prior trials; partial alignment.\n- 0.0: Reasoning absent, clinically inappropriate, or documents conflict on key details.", "expectation": "A coherent, patient-specific, and ethically appropriate justification for continued Vraylar 1.5 mg that aligns across both documents."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality", "description": "LLM assessment of professionalism, formatting, tone, readability, and audience-appropriate presentation across both documents.", "is_required": false, "max_points": 1.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Readability", "description": "Checks tone, formatting, readability, and completeness (letterhead style, coherent paragraphs, salutation/closing; application legibility).", "weight": 1.0, "judge_prompt": "Evaluate the overall professional quality of both outputs:\n- Appeal letter: Formal tone, logical flow, well-structured paragraphs, correct grammar/spelling, appropriate headers/salutation/closing, contact block, and 2\u20134 pages readability without filler.\n- AbbVie application: Clearly legible typed entries, neatness, and consistency.\n\nScoring (0.0\u20131.0):\n- 1.0: Highly professional, polished, and easy to read; excellent formatting.\n- 0.7: Professional with minor issues (typos, minor formatting lapses) but overall clear.\n- 0.4: Noticeably rough or inconsistent formatting or readability issues.\n- 0.0: Unprofessional, confusing, or sloppy presentation.", "expectation": "Polished, professional documents suitable for submission to an insurer and a manufacturer assistance program."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "9a8c8e28-ce76-408b-83c3-488422892e58", "rubric": {"category_name": "Editorial Accessibility Pack (Guide, Checklist, Quiz)", "rationale": "This is a document-heavy task (Pattern B). The rubric enforces a self-documenting shape: three separate PDFs with clearly defined sections that enable automated verification. Stage 1 (LLM-only) gates on structure and format, ensuring the deliverables are in a verifiable layout. Stage 2 mixes code rules that validate presence of key legal references, WCAG versions, contact/training notes, CMS/dev disclaimers, checklist and quiz structures, and bibliography links. Stage 3 judges overall quality, practicality for UK newsroom editors/journalists, and compliance orientation. Failure at Stage 1 zeros the category, per the self-documenting philosophy.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Structured Output Format Gate", "description": "GATE: Verify the candidate produced THREE separate PDFs with required structures: an Editorial Accessibility Guide, an Accessibility Checklist, and an Accessibility Quiz. Only structure/format is checked here, not content quality or calculation correctness.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.5, "rules": [{"type": "llm_judge", "name": "Structured Document Set (3 PDFs)", "description": "Check presence and structural completeness of three PDFs: Guide, Checklist, Quiz. Be flexible with section titles but strict about structural presence.", "weight": 4.0, "judge_prompt": "You are evaluating whether the submission satisfies the REQUIRED SHAPE for this task. Review ALL submitted files.\n\nGoal: There must be THREE separate PDF documents tailored to UK online journalism accessibility:\n  1) A practical framework Guide for editorial staff\n  2) A day-to-day Checklist\n  3) A multiple-choice Quiz\n\nAccepted formats: PDF only. No Word/Excel/plain text for the final deliverables.\n\nDocument 1 \u2014 Editorial Accessibility Guide (PDF): Required structural elements\n- Title page or clearly visible title (e.g., \"Editorial Accessibility Guide\" or similar), with date/version\n- Audience/scope statement that this is for editors/journalists/editorial staff\n- Legal context section referencing UK legislation (Equality Act 2010 AND Public Sector Bodies (Websites and Mobile Applications) Accessibility Regulations 2018) and international context (EU and North America)\n- WCAG overview section referencing WCAG 2.1 and WCAG 2.2 and the POUR principles (Perceivable, Operable, Understandable, Robust)\n- Practical guidance mapped to editorial tasks, including at minimum: headings/semantics, links, images/graphics alt text, video captions and audio transcripts, color/contrast, tables/data viz, readability/plain language, social embeds, and publishing workflow\n- Clear note that any CMS changes beyond basic editorial actions (text, basic semantic formatting, links, alt text, embedding images/graphics/video) are in hand and will be actioned by the dev team\n- Contact and training: instructions to contact the section editor for questions; note training will be available, with dates to be announced in the Slack editorial advice channel\n- Bibliography or Further Reading with clickable links\n- Length expectation: at least 6 pages total\n\nDocument 2 \u2014 Accessibility Checklist (PDF): Required structural elements\n- Title indicating it is a Checklist\n- Organized, scannable checklist items (checkboxes or similar markers) grouped across stages of editorial workflow (e.g., writing, media, publishing/review)\n- Items touching on: headings, links, alt text, images/graphics, video/audio captions/transcripts, color contrast, tables/infographics, plain language/readability, and final review\n- Reference back to the guide (e.g., pointers or section references) is acceptable but not required\n- Length expectation: 1\u20132 pages (can be longer if needed)\n\nDocument 3 \u2014 Accessibility Quiz (PDF): Required structural elements\n- Title indicating it is a Quiz or Assessment\n- Minimum of 10 multiple-choice questions\n- Each question has EXACTLY four options (A/B/C/D or equivalent)\n- An Answer Key with explanations\n- A scoring guide describing how to interpret results (e.g., thresholds for pass/retrain)\n- Length expectation: typically 2+ pages to accommodate questions and the answer key\n\nScoring (STRUCTURE ONLY \u2014 do not assess content quality/correctness):\n- 4.0: All THREE PDFs present, correctly formatted, and each meets the required structural elements (including page-length expectations, presence of WCAG 2.1 and 2.2, both UK legal references, contact/Slack/training, CMS dev-note in the Guide; clear, scannable Checklist; 10+ MCQs with four options, answer key with explanations, and scoring guide in the Quiz).\n- 3.2: Three PDFs present and correctly formatted; minor structural omissions in ONE document only (e.g., Guide missing one minor sub-element like explicit POUR mention OR Quiz missing explicit scoring guide text but otherwise structurally complete; or Guide is 5 pages but clearly comprehensive).\n- 2.0: Structure partially present: only TWO of the required PDFs are present OR one PDF is missing multiple core elements (e.g., Guide lacks both legal references and WCAG versions) OR the Quiz lacks an answer key.\n- 0.0: Not PDF format; only one document present; or structure is fundamentally non-compliant (e.g., no checklist, no quiz, or unreadable files).\n\nBe flexible with section titles and phrasing (e.g., \"Overview\" vs. \"Executive Summary\") but strict about the presence of required elements. Do not judge content correctness or writing quality \u2014 only structure and format.", "expectation": "Three separate, well-structured PDFs with all required sections and elements present so that automated checks are possible in later stages."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification: References and Structural Consistency", "description": "Automated checks for key references, structural markers, and required elements across the three PDFs. Uses flexible keyword matching and counts.", "is_required": false, "max_points": 4.5, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "PDF Count and Separation", "description": "Verify at least three distinct PDFs are present. Partial credit if only one PDF but it appears to contain Guide, Checklist, and Quiz sections.", "weight": 0.2, "code": "import re\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0\n    pdf_texts = []\n    for r in outputs:\n        if getattr(r, 'is_document', False):\n            try:\n                txt = context.files.read_pdf_text(r.id)\n                if txt is not None and len(txt.strip()) > 0:\n                    pdf_texts.append((r, txt))\n            except Exception:\n                # Not a PDF or unreadable\n                pass\n    pdf_count = len(pdf_texts)\n    if pdf_count >= 3:\n        return 0.2\n    # Partial if a single PDF seems to contain all three parts\n    if pdf_count == 1:\n        t = pdf_texts[0][1].lower()\n        has_guide = ('guide' in t) or ('executive summary' in t) or ('overview' in t and 'wcag' in t)\n        has_checklist = 'checklist' in t\n        has_quiz = ('quiz' in t) or (re.search(r'question\\s*1', t) is not None)\n        if has_guide and has_checklist and has_quiz:\n            return 0.1\n    return 0.0"}, {"type": "code", "name": "Legal and Standards Coverage (Guide)", "description": "Check mention of WCAG 2.1 and 2.2, Equality Act 2010, and Public Sector Bodies (Websites and Mobile Applications) Accessibility Regulations 2018; plus international context (EU and North America).", "weight": 1.2, "code": "import re\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0\n    all_text = ''\n    for r in outputs:\n        if getattr(r, 'is_document', False):\n            try:\n                txt = context.files.read_pdf_text(r.id) or ''\n            except Exception:\n                txt = ''\n            all_text += '\\n' + txt\n    t = all_text.lower()\n    score = 0.0\n    # WCAG 2.1\n    if 'wcag' in t and ('2.1' in t or '2\\u00b71' in t):\n        score += 0.3\n    # WCAG 2.2\n    if 'wcag' in t and '2.2' in t:\n        score += 0.3\n    # Equality Act 2010\n    if 'equality act 2010' in t:\n        score += 0.3\n    # Public Sector Bodies Regulations 2018 (full or close variant)\n    regs_patterns = [\n        'public sector bodies (websites and mobile applications) accessibility regulations 2018',\n        'public sector bodies accessibility regulations 2018',\n        'websites and mobile applications accessibility regulations 2018'\n    ]\n    if any(p in t for p in regs_patterns):\n        score += 0.3\n    # International context\n    intl = 0.0\n    if ('eu' in t) or ('european' in t):\n        intl += 0.15\n    if ('north america' in t) or ('united states' in t) or ('usa' in t) or ('us' in t) or ('canada' in t):\n        intl += 0.15\n    score += min(intl, 0.3)\n    return min(score, 1.2)"}, {"type": "code", "name": "Contact and Training Instructions", "description": "Confirm presence of instructions to contact the section editor and mention of training with dates to be announced in the Slack editorial advice channel.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0\n    all_text = ''\n    for r in outputs:\n        if getattr(r, 'is_document', False):\n            try:\n                txt = context.files.read_pdf_text(r.id) or ''\n            except Exception:\n                txt = ''\n            all_text += '\\n' + txt\n    t = all_text.lower()\n    score = 0.0\n    # Contact section editor\n    if ('contact' in t and 'section editor' in t) or ('contact your' in t and 'editor' in t):\n        score += 0.25\n    # Training and Slack editorial advice channel mention\n    if 'slack' in t and 'advice' in t and 'channel' in t:\n        score += 0.25\n    return score"}, {"type": "code", "name": "CMS Changes Handled by Dev Team", "description": "Verify there is a clear note that CMS changes beyond basic editorial actions will be handled by the dev team.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0\n    all_text = ''\n    for r in outputs:\n        if getattr(r, 'is_document', False):\n            try:\n                txt = context.files.read_pdf_text(r.id) or ''\n            except Exception:\n                txt = ''\n            all_text += '\\n' + txt\n    t = all_text.lower()\n    has_cms = 'cms' in t or 'content management system' in t\n    has_dev = ('dev team' in t) or ('development team' in t) or ('engineering team' in t)\n    # Phrases indicating responsibility/hand-off\n    handoff = any(p in t for p in ['in hand', 'will be actioned', 'will be handled', 'handled by', 'to be implemented'])\n    if has_cms and has_dev and handoff:\n        return 0.4\n    if has_cms and has_dev:\n        return 0.2\n    return 0.0"}, {"type": "code", "name": "Checklist Structure", "description": "Detect presence of a checklist PDF with multiple actionable checkbox items across editorial stages.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    def count_check_items(text):\n        # Count lines with common checkbox markers or bullet patterns\n        lines = [ln.strip() for ln in text.splitlines()]\n        cnt = 0\n        for ln in lines:\n            ll = ln.lower()\n            if re.match(r'^(\\[\\s?\\]|\\[\\]|\u2610|\u25a1|\\- \\[ \\]|\\- \\[x\\]|\\* \\[ \\]|\\* \\[x\\]|\\u2610|\\u25a1|\\u2713|\\u2714)', ll):\n                cnt += 1\n            elif ('[ ]' in ll) or ('[]' in ll) or ('\u2610' in ll) or ('\u25a1' in ll):\n                cnt += 1\n            elif ll.startswith('\u2022 ') or ll.startswith('- '):\n                # Heuristic: count bullets that include accessibility keywords\n                if any(k in ll for k in ['alt text', 'caption', 'contrast', 'heading', 'link', 'table', 'transcript', 'plain language', 'readability', 'keyboard']):\n                    cnt += 1\n        return cnt\n\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0\n    best = 0\n    for r in outputs:\n        if getattr(r, 'is_document', False):\n            try:\n                txt = context.files.read_pdf_text(r.id) or ''\n            except Exception:\n                continue\n            t = txt.lower()\n            if 'checklist' in t or 'check list' in t or 'pre-publish' in t:\n                items = count_check_items(txt)\n                best = max(best, items)\n    # Score by number of detected items\n    if best >= 12:\n        return 0.8\n    if best >= 8:\n        return 0.6\n    if best >= 5:\n        return 0.4\n    if best >= 3:\n        return 0.2\n    return 0.0"}, {"type": "code", "name": "Quiz Structure and Key", "description": "Verify the quiz has >=10 questions, four options per question pattern, an answer key with explanations, and a scoring guide.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0\n    best = {'q':0, 'opts':0, 'has_key':False, 'has_scoring':False}\n    for r in outputs:\n        if getattr(r, 'is_document', False):\n            try:\n                txt = context.files.read_pdf_text(r.id) or ''\n            except Exception:\n                continue\n            t = txt.lower()\n            q_count = len(re.findall(r'\\bquestion\\s*\\d+', t))\n            # Options markers (A) B) C) D) or A. B. etc.)\n            opt_markers = len(re.findall(r'\\b[abcd][\\)\\.\\]]\\s', t))\n            has_key = ('answer key' in t) or ('answers:' in t) or ('correct answers' in t)\n            # Look for explanations (e.g., \"because\", \"explanation\") near key\n            has_scoring = ('scoring guide' in t) or ('scoring' in t) or ('how to score' in t) or ('interpret your score' in t)\n            cand = {'q': q_count, 'opts': opt_markers, 'has_key': has_key, 'has_scoring': has_scoring}\n            # Choose the candidate with the most questions\n            if q_count > best['q']:\n                best = cand\n    q = best['q']\n    opts = best['opts']\n    has_key = best['has_key']\n    has_scoring = best['has_scoring']\n    # Heuristic: at least ~3 options markers per question on average suggests 4-option MCQs are present in structure\n    enough_opts = (q > 0 and (opts / max(q,1)) >= 3)\n    if q >= 10 and has_key and has_scoring and enough_opts:\n        return 1.0\n    if q >= 8 and has_key and (has_scoring or enough_opts):\n        return 0.6\n    if q >= 5:\n        return 0.3\n    return 0.0"}, {"type": "code", "name": "Bibliography with Links", "description": "Confirm the guide includes a bibliography or further reading section with multiple links.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    url_re = re.compile(r'https?://\\S+', re.IGNORECASE)\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0\n    best_url_count = 0\n    for r in outputs:\n        if getattr(r, 'is_document', False):\n            try:\n                txt = context.files.read_pdf_text(r.id) or ''\n            except Exception:\n                continue\n            t = txt.lower()\n            if ('bibliography' in t) or ('further reading' in t) or ('references' in t):\n                urls = set(u.strip(').,;') for u in url_re.findall(txt))\n                best_url_count = max(best_url_count, len(urls))\n    if best_url_count >= 5:\n        return 0.4\n    if best_url_count >= 3:\n        return 0.2\n    return 0.0"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Fitness for Purpose", "description": "Holistic LLM assessment of professionalism, editorial fit, clarity, and compliance orientation for UK newsroom staff with mixed technical literacy.", "is_required": false, "max_points": 1.5, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Practicality and Editorial Fit", "description": "Are the materials actionable for editors/journalists with mixed technical literacy, reflecting UK newsroom realities and resistance to change?", "weight": 0.7, "judge_prompt": "Evaluate the overall practicality and editorial fit of the three PDFs for a UK online newsroom with mixed technical literacy and some resistance to change.\nConsider:\n- Actionability: clear, step-by-step guidance mapped to daily editorial tasks (writing, editing, media handling, publishing).\n- Relevance: tailored to editors/journalists (not developers); uses newsroom examples; references the CMS only at the level of editorial actions.\n- Change management: empathetic tone, pragmatic advice, low-friction practices, and a workflow/checklist that\u2019s easy to adopt.\n- Risk framing: emphasizes reputational and legal risk in a constructive way.\nScoring:\n- 0.7: Highly actionable and tailored; excellent change-management tone.\n- 0.4: Generally helpful but uneven or occasionally too technical.\n- 0.2: Some useful parts but not well-adapted to editorial users.\n- 0.0: Unhelpful or misaligned with newsroom needs.", "expectation": "A practical, newsroom-ready pack that editors can adopt immediately with minimal friction."}, {"type": "llm_judge", "name": "Clarity and Accessibility of Writing", "description": "Assess clarity, plain language, structure, and scannability of the documents themselves (headings, bullets, examples) consistent with accessibility good practice.", "weight": 0.5, "judge_prompt": "Judge the clarity and accessibility of the writing across the three PDFs:\n- Plain language, concise phrasing, and appropriate reading level\n- Logical structure with descriptive headings, lists, and examples\n- Visual clarity and scannability (even when printed)\n- Consistency in terminology and formatting across the set\nScoring:\n- 0.5: Very clear, accessible, consistent, and easy to scan.\n- 0.3: Generally clear with minor inconsistencies.\n- 0.1: Understandable but verbose or poorly structured.\n- 0.0: Hard to understand or poorly structured.", "expectation": "Clear, consistent, accessible writing with strong structure and scannability."}, {"type": "llm_judge", "name": "Compliance Orientation and Credibility", "description": "Does the pack strongly anchor on WCAG 2.1/2.2 and UK legal obligations, provide credible references, and clearly explain how to stay compliant?", "weight": 0.3, "judge_prompt": "Evaluate how well the materials orient the team toward compliance:\n- Accurate emphasis on WCAG 2.1/2.2 principles and success criteria relevant to editorial work\n- Clear explanation of UK legal context and international considerations\n- Credible references and links; sound, non-misleading statements\n- Practical guidance for avoiding legal/reputational risk day to day\nScoring:\n- 0.3: Strong, accurate compliance guidance with credible references.\n- 0.2: Mostly accurate with minor gaps.\n- 0.1: Useful but light on references or specifics.\n- 0.0: Misleading or missing compliance orientation.", "expectation": "Accurate, credible compliance framing with actionable guidance for daily practice."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "1bff4551-1d54-4e37-b2e0-d5c3f2ea4a45", "rubric": {"category_name": "Government \u00b7 Recreation Workers \u2014 Celestial Solstice Black Rock Set List (PDF)", "rationale": "Self-documenting design: Stage 1 mandates a tightly structured PDF set list with a clearly labeled table and supporting sections that make later verification trivial. Stage 2 uses code to verify objective constraints (original song inclusion, YouTube links, duration near 45 minutes, headers presence, NMAAHC collection references, instrumentation and suitability statements, and year diversity) plus a light LLM diversity check. Stage 3 holistically assesses professionalism, educational value, and audience fit.", "max_total_score": 14.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate", "description": "LLM-only verification that the output is a single, well-structured PDF containing a specifically formatted set list table and required sections to enable automated verification later.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Set List PDF Requirement", "description": "Validate that the candidate output is a single PDF with the required sections and a clearly structured set list table containing all mandated columns.", "weight": 4.0, "judge_prompt": "You are grading ONLY the STRUCTURE/FORMAT of the delivered file (not the content quality). Inspect the candidate output.\n\nFormat requirements (strict):\n- The deliverable must be a single PDF file (not Word, not Excel, not plain text).\n- The PDF should be professionally formatted and readable. One page is acceptable if all required elements fit; multiple pages are also acceptable.\n\nRequired sections (flexible on exact header wording, but the elements must be clearly present):\n1) Program Overview\n   - Should mention this is for the Jarred Premton Institute\u2019s \u201cCelestial Solstice\u201d event (minor naming variations acceptable)\n   - State approximate performance length (~45 minutes)\n   - State standard band instrumentation (lead guitar, rhythm guitar, bass, drums, keyboard, vocalist) or an equivalent statement that a standard band can perform the program\n   - Explicitly confirm inclusion of the original song \u201cFistful of Flyers\u201d by the vocalist \u201crex\u201d\n\n2) Set List (as a single table, or clearly tabular layout that looks like a table)\n   - Table columns (minor naming variations acceptable):\n     \u2022 Order/#\n     \u2022 Song Title\n     \u2022 Artist\n     \u2022 Year\n     \u2022 Sub-genre\n     \u2022 Duration (mm:ss)\n     \u2022 YouTube Link\n     \u2022 Context Summary (2\u20134 sentences per entry acceptable; can be a concise paragraph in a cell)\n     \u2022 NMAAHC Collection? (Yes/No)\n     \u2022 Collection Reference (a URL to the NMAAHC collection search or a search term/reference)\n   - There should be multiple entries forming a complete 45-minute program (typically 8\u201312 selections). The table must be visibly structured so columns are identifiable.\n\n3) Content and Performance Notes\n   - A short section affirming no heavy curse words / suitable for general audiences\n   - Brief rehearsal/arrangement feasibility notes (e.g., keys/tempos/standard band-ready with limited rehearsal)\n\n4) Sources and Links\n   - YouTube links (already in the table) are acceptable here as a summary list or cross-reference\n   - Include at least one reference or link to the NMAAHC collection search page (https://nmaahc.si.edu/explore/collection/search) or mention of NMAAHC collection sourcing for artists\n\nScoring (structure only):\n- 4.0: PDF format AND all 4 sections present with the table containing all required columns\n- 3.5: PDF format AND all 4 sections present but table missing 1 non-core support column (e.g., Collection Reference)\n- 3.0: PDF format AND Set List table with core columns present (Order, Song Title, Artist, Year, Sub-genre, Duration, YouTube Link, Context) AND at least one of the two collection columns present; may be missing one supporting section\n- 2.0: PDF format AND Set List table exists but is clearly incomplete (missing 3+ required columns) OR only 1\u20132 sections outside the table present\n- 0.0: Not a PDF OR no recognizable set list table\n\nOnly evaluate presence/shape, not correctness of details or calculations.", "expectation": "A clean PDF with an Overview, a Set List table with the specified columns, Content/Performance Notes, and Sources/Links including NMAAHC reference."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification Checks (Correctness)", "description": "Automated and LLM-supported verification of key constraints now that the structure exists.", "is_required": false, "max_points": 7.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Original Song Included: \u201cFistful of Flyers\u201d by rex", "description": "Confirms the PDF text includes the original song title and the vocalist name, indicating it is on the set list.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n    tl = text.lower()\n    has_title = 'fistful of flyers' in tl\n    has_rex = re.search(r'\\brex\\b', tl) is not None\n    return 1.0 if (has_title and has_rex) else 0.0"}, {"type": "code", "name": "YouTube Links Count in Reasonable Range", "description": "Counts YouTube links and rewards typical set sizes for a ~45-minute show.", "weight": 1.0, "code": "import re\n\nyt_pattern = r'https?://(?:www\\.)?(?:youtube\\.com/watch\\?v=[\\w-]+|youtu\\.be/[\\w-]+)[^\\s)]*'\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n    links = re.findall(yt_pattern, text, flags=re.IGNORECASE)\n    n = len(links)\n    # Typical 45-min set: 8-12 songs\n    if 9 <= n <= 12:\n        return 1.0\n    if 8 <= n <= 14:\n        return 0.6\n    if 5 <= n <= 15:\n        return 0.2\n    return 0.0"}, {"type": "code", "name": "Total Duration Approximately 45 Minutes", "description": "Parses mm:ss durations and checks that the sum is near 45 minutes with tolerance.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n    tl = text.lower()\n    matches = re.findall(r'\\b(\\d{1,2}):(\\d{2})\\b', tl)\n    secs = []\n    for m in matches:\n        mm = int(m[0]); ss = int(m[1])\n        # Filter out likely non-song times (e.g., > 12 min or seconds >= 60 handled by regex)\n        total = mm*60 + ss\n        if 60 <= total <= 12*60:\n            secs.append(total)\n    if not secs:\n        return 0.0\n    total_min = sum(secs) / 60.0\n    diff = abs(total_min - 45.0)\n    # Reward closer totals; also require a reasonable count of durations\n    n = len(secs)\n    coverage_factor = min(1.0, n / 7.0)  # expect ~8\u201312 songs; tolerate if slightly fewer\n    if diff <= 3:\n        base = 1.0\n    elif diff <= 5:\n        base = 0.7\n    elif diff <= 8:\n        base = 0.4\n    else:\n        base = 0.0\n    return base * coverage_factor"}, {"type": "code", "name": "Set List Headers Present", "description": "Checks that expected header keywords appear somewhere in the PDF text (flexible naming).", "weight": 0.8, "code": "import re\n\ndef any_present(t, options):\n    return any(o in t for o in options)\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n    tl = text.lower()\n    checks = [\n        any_present(tl, ['order', '#']),\n        any_present(tl, ['song title', 'title', 'track']),\n        any_present(tl, ['artist', 'band', 'act']),\n        any_present(tl, ['year', 'release year']),\n        any_present(tl, ['sub-genre', 'subgenre', 'genre']),\n        any_present(tl, ['duration', 'length', 'time']),\n        any_present(tl, ['youtube']),\n        any_present(tl, ['context', 'notes', 'rationale']),\n        any_present(tl, ['nmaahc', 'collection'])\n    ]\n    score = sum(1 for c in checks if c) / len(checks)\n    return score"}, {"type": "code", "name": "NMAAHC Collection Mapping Evidence", "description": "Confirms that the document references the NMAAHC collection search and/or includes an In-Collection indicator.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n    tl = text.lower()\n    signals = 0\n    if 'nmaahc.si.edu/explore/collection/search' in tl or 'nmaahc.si.edu' in tl:\n        signals += 1\n    if 'nmaahc' in tl:\n        signals += 1\n    if 'smithsonian' in tl:\n        signals += 1\n    if 'in collection' in tl or 'collection reference' in tl or 'collection link' in tl:\n        signals += 1\n    # Map signals to score; require at least two signals for partial credit\n    if signals >= 3:\n        return 1.0\n    if signals == 2:\n        return 0.6\n    if signals == 1:\n        return 0.2\n    return 0.0"}, {"type": "code", "name": "Standard Band Instrumentation Mentioned", "description": "Checks that the document states standard band feasibility (lead, rhythm, bass, drums, keys, vocals).", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n    tl = text.lower()\n    terms = [\n        ('lead guitar',),\n        ('rhythm guitar',),\n        ('bass', 'bass guitar'),\n        ('drum', 'drum kit', 'drums'),\n        ('keyboard', 'keys'),\n        ('vocal', 'singer', 'vocals')\n    ]\n    present = 0\n    for group in terms:\n        if any(g in tl for g in group):\n            present += 1\n    return present / len(terms)"}, {"type": "code", "name": "Content Suitability Statement Present", "description": "Verifies an explicit statement about suitability for general audiences / no heavy curse words.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n    tl = text.lower()\n    phrases = [\n        'no heavy curse', 'no explicit language', 'clean lyrics',\n        'suitable for general audiences', 'family-friendly', 'appropriate for all ages'\n    ]\n    return 1.0 if any(p in tl for p in phrases) else 0.0"}, {"type": "code", "name": "Year Diversity and Era Coverage (Code)", "description": "Extracts years and checks for diversity across decades (span and count).", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n    years = re.findall(r'\\b(19[5-9]\\d|20[0-2]\\d)\\b', text)\n    years = sorted({int(y) for y in years})\n    if not years:\n        return 0.0\n    span = years[-1] - years[0] if len(years) > 1 else 0\n    diversity = len(years)\n    # Reward broader span and more distinct years\n    score = 0.0\n    if diversity >= 6 and span >= 30:\n        score = 1.0\n    elif diversity >= 5 and span >= 20:\n        score = 0.7\n    elif diversity >= 4 and span >= 10:\n        score = 0.4\n    else:\n        score = 0.1 if diversity >= 3 else 0.0\n    return score"}, {"type": "llm_judge", "name": "Era and Sub-genre Coverage (LLM)", "description": "Qualitative check that the selections span multiple rock eras and sub-genres by Black artists/bands, consistent with headings and context blurbs.", "weight": 0.4, "judge_prompt": "Based on the structured Set List table and context text, judge the factual reasonableness of coverage across eras and sub-genres for Black artists/bands in rock. You are NOT scoring writing quality here\u2014only whether, taken at face value, the entries appear to represent multiple eras (e.g., classic rock/rock & roll foundations through later decades) and multiple sub-genres (e.g., rock & roll, blues rock, funk rock, punk/new wave, hard rock/alt, etc.).\n\nScoring:\n- 0.4: Clearly spans early foundations and later decades with multiple sub-genres represented\n- 0.25: Spans at least two distinct eras and more than one sub-genre but somewhat narrow\n- 0.1: Minimal diversity (mostly one era or one sub-genre)\n- 0.0: Not apparent or inconsistent with the brief\n\nIgnore formatting; assume Stage 1 already validated structure.", "expectation": "A set list touching multiple decades and sub-genres of rock by Black artists/bands."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Audience Fit", "description": "Holistic LLM assessment of presentation quality, educational value, flow, and feasibility for the event and audience.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professionalism, Clarity, Educational Value, and Feasibility", "description": "Evaluates how well the document reads as a professional set list handoff for organizers and band members, with clear context and practical performance readiness.", "weight": 3.0, "judge_prompt": "Assess the overall quality of the PDF as a professional deliverable for the Jarred Premton Institute\u2019s Celestial Solstice program:\n- Professional presentation: clear title, logical layout, clean typography, easy to read\n- Clarity and usefulness for band prep: concise contexts per song, clear links, keys/tempos if provided, realistic for limited rehearsal\n- Educational value: context blurbs explain historical/musical significance of each selection\n- Audience appropriateness: suitable for general audiences in an evening museum program; tone fits institutional setting\n- Flow and pacing: set order makes sense musically and historically; transitions feel intentional\n\nScoring:\n- 3.0: Excellent across all dimensions; ready to send as-is\n- 2.0: Solid; minor improvements possible\n- 1.0: Adequate but needs revision for clarity or practicality\n- 0.0: Poorly presented or not useful to organizers/band", "expectation": "Polished, useful, educational, and performance-feasible set list suited to a general audience evening program."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "045aba2e-4093-42aa-ab7f-159cc538278c", "rubric": {"category_name": "California Community Pharmacy Compliance Checklists (Daily / Weekly+Monthly / Quarterly+Annual)", "rationale": "This rubric enforces a self-documenting, verifiable structure for three printer-friendly compliance checklists in PDF form, aligned to California Board of Pharmacy expectations. Stage 1 (LLM-only) strictly gates the exact structural shape across three one-page PDFs. Stage 2 uses code rules to verify detectable elements (file presence, titles, categories, citation markers, structural fields, and versioning) using text extraction. Stage 3 evaluates professional quality and practical usability for ongoing compliance operations.", "max_total_score": 22.0, "stages": [{"name": "Stage 1 \u2013 Shape Enforcement Gate (LLM ONLY)", "description": "Validate the deliverables have the exact required structure: three separate one-page PDFs with specific titles, obvious checklist layout with key columns/fields, section headers, and a compliance citations footer.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.5, "rules": [{"type": "llm_judge", "name": "Three One-Page PDFs with Required Structure", "description": "LLM verifies the presence of exactly three separate one-page PDF checklists with required titles, structure, fields, and citations footer.", "weight": 6.0, "judge_prompt": "You are auditing the candidate outputs for strict structure. Only evaluate SHAPE and FORMAT, not content quality. Review ALL produced files.\n\nPASS CRITERIA (Full credit if all are met):\nA) Exactly 3 separate PDF files are present (no DOCX/Excel substitutes), each ONE PAGE only.\nB) Each PDF\u2019s title clearly matches one of the following (flexible on punctuation/case/spacing but must be unmistakable):\n   1) \"California Pharmacy Compliance Checklist \u2013 Daily\"\n   2) \"California Pharmacy Compliance Checklist \u2013 Weekly & Monthly\" (or a single page that combines Weekly and Monthly)\n   3) \"California Pharmacy Compliance Checklist \u2013 Quarterly & Annual\" (combined on one page)\nC) Each page is a printer-friendly checklist with a clear table or grid-like layout and visible checkbox indicators (e.g., \u2610 or [ ]). The checklist must have column headers covering at minimum the following concepts (synonyms acceptable):\n   - Requirement/Task/Item\n   - Citation/Authority/Reference\n   - Responsible/Role/Owner\n   - Verification fields: Date and/or Initials/Signature\n   - Notes/Comments/Exceptions\nD) Each page contains visible section headers (category groupings) appropriate to the frequency. Be flexible with exact names, but categories should include multiple areas such as: Controlled Substances, Prescription Processing/Dispensing, Inventory/Storage/Labeling, Records/Reporting, Facility/Security, Temperature Logs/Equipment, Clinical Services (e.g., Immunizations/CLIA), Privacy/HIPAA, PIC Oversight/Training, Recalls/Expirations. Daily should reasonably include temperature/operations items; Weekly+Monthly and Quarterly+Annual should include periodic items.\nE) Each page includes a small header area with pharmacy identifiers fields (e.g., Pharmacy Name, Permit/License #, PIC, Date) and a footer noting sources. Footer must reference both:\n   - 2025 Lawbook for Pharmacy (California) and\n   - Community Pharmacy Self-Assessment (form 17M-13)\n   URLs or explicit names acceptable.\n\nSCORING:\n- 6.0: All three PDFs exist and are exactly one page each; titles/frequencies correct; table-like checklist with the required column concepts; section headers present; header fields present; footer references both the 2025 Lawbook and the Self-Assessment (17M-13).\n- 5.0\u20135.5: Minor deviations (e.g., checkbox symbols absent but table clearly a checklist; slight title variations; one minor header field missing) while all three one-page PDFs and core structure remain correct.\n- 4.0\u20134.9: Meaningful gaps but core intent mostly intact (e.g., only two pages meet the exact frequency titling; or section headers thin; or footer cites only one source). Still three PDFs and largely checklists.\n- 2.0\u20133.9: Partial delivery (e.g., only two PDFs; or one is multipage; or missing most required columns/fields), but some recognizable checklist structure exists.\n- 0.0\u20131.9: Not PDFs; wrong number of files; no clear checklist structure; or cannot identify frequencies.\n\nReturn a numeric score only based on the above. Do NOT judge content accuracy\u2014just shape and format.", "expectation": "Three distinct, one-page PDF checklists with clear titles for Daily, Weekly+Monthly, and Quarterly+Annual; each with a table-like checklist including columns for Requirement, Citation, Responsible, Date/Initials, Notes; visible checkbox indicators; category sections; header with identifiers; and a footer citing the 2025 Lawbook and the Community Pharmacy Self-Assessment (17M-13)."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification (Code + LLM-amenable checks)", "description": "Now that shape is enforced, verify presence and consistency of key elements detectable via text extraction and flexible matching across the three PDFs.", "is_required": false, "max_points": 9.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Exactly Three Separate PDF Files Present", "description": "Checks that there are exactly three PDF outputs among the final artifacts.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs() or []\n        pdfs = []\n        for r in outputs:\n            try:\n                if not getattr(r, 'is_document', False):\n                    continue\n                p = context.files.get_path(r.id)\n                if str(p).lower().endswith('.pdf'):\n                    pdfs.append(r)\n            except Exception:\n                continue\n        n = len(pdfs)\n        if n == 3:\n            return 1.0, \"Found exactly 3 PDF files.\"\n        elif n == 2 or n == 4:\n            return 0.5, f\"Found {n} PDF files; expected 3.\"\n        elif n == 1:\n            return 0.25, \"Found 1 PDF file; expected 3.\"\n        else:\n            return 0.0, f\"Found {n} PDF files; expected 3.\"\n    except Exception as e:\n        return 0.0, f\"Error during PDF count: {e}\""}, {"type": "code", "name": "Titles and Frequency Identification", "description": "Verifies that across the three PDFs, one is Daily, one combines Weekly & Monthly, and one combines Quarterly & Annual, and that each includes a recognizable master title.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    def read_doc_text(res):\n        try:\n            return (context.files.read_pdf_text(res.id) or '').lower()\n        except Exception:\n            try:\n                return (context.files.read_docx_text(res.id) or '').lower()\n            except Exception:\n                try:\n                    return (context.files.read_text(res.id) or '').lower()\n                except Exception:\n                    return ''\n    try:\n        outputs = context.get_all_outputs() or []\n        pdfs = []\n        for r in outputs:\n            try:\n                if not getattr(r, 'is_document', False):\n                    continue\n                p = context.files.get_path(r.id)\n                if str(p).lower().endswith('.pdf'):\n                    pdfs.append(r)\n            except Exception:\n                continue\n        if not pdfs:\n            return 0.0, \"No PDFs to evaluate titles/frequencies.\"\n        # classify\n        has_daily = False\n        has_wm = False\n        has_qa = False\n        title_hits = 0\n        for r in pdfs:\n            t = read_doc_text(r)\n            if not t:\n                continue\n            if 'california pharmacy compliance checklist' in t:\n                title_hits += 1\n            # frequency classification (flexible, try to avoid overmatching)\n            contains_daily = 'daily' in t\n            contains_weekly = 'weekly' in t\n            contains_monthly = 'monthly' in t\n            contains_quarterly = 'quarterly' in t\n            contains_annual = 'annual' in t\n            # Daily: mentions daily and does not strongly suggest combined periodic sets\n            if contains_daily and not (contains_weekly or contains_monthly or contains_quarterly or contains_annual):\n                has_daily = True\n            # Weekly+Monthly combined\n            if contains_weekly and contains_monthly and not contains_daily and not contains_quarterly and not contains_annual:\n                has_wm = True\n            # Quarterly+Annual combined\n            if contains_quarterly and contains_annual and not contains_daily and not contains_weekly and not contains_monthly:\n                has_qa = True\n        score = 0.0\n        parts = []\n        # Title presence: require at least 2 titles across 3 PDFs for partial, 3 for full\n        if title_hits >= 3:\n            score += 0.5\n            parts.append('All PDFs include master title text.')\n        elif title_hits == 2:\n            score += 0.35\n            parts.append('Two PDFs include master title text.')\n        elif title_hits == 1:\n            score += 0.2\n            parts.append('One PDF includes master title text.')\n        else:\n            parts.append('No master title detected.')\n        # Frequency mapping\n        freq_earned = 0.0\n        freq_earned += 0.333 if has_daily else 0.0\n        freq_earned += 0.333 if has_wm else 0.0\n        freq_earned += 0.334 if has_qa else 0.0\n        score += min(freq_earned * 1.0, 1.0)  # up to 1.0 for frequencies\n        # total weight 1.5\n        final = min(score, 1.5)\n        fb = f\"Title hits: {title_hits}; Daily:{has_daily}, Weekly+Monthly:{has_wm}, Quarterly+Annual:{has_qa}.\"\n        return final, fb\n    except Exception as e:\n        return 0.0, f\"Error during title/frequency check: {e}\""}, {"type": "code", "name": "Category Coverage Breadth", "description": "Checks for presence of key compliance categories across the combined text of all PDFs. Scores by proportion of categories detected.", "weight": 2.5, "code": "import re\n\ndef evaluate(workflow, context):\n    def read_doc_text(res):\n        try:\n            return (context.files.read_pdf_text(res.id) or '').lower()\n        except Exception:\n            try:\n                return (context.files.read_docx_text(res.id) or '').lower()\n            except Exception:\n                try:\n                    return (context.files.read_text(res.id) or '').lower()\n                except Exception:\n                    return ''\n    try:\n        outputs = context.get_all_outputs() or []\n        texts = []\n        for r in outputs:\n            try:\n                if not getattr(r, 'is_document', False):\n                    continue\n                p = context.files.get_path(r.id)\n                if str(p).lower().endswith('.pdf'):\n                    texts.append(read_doc_text(r))\n            except Exception:\n                continue\n        all_text = ' \\n '.join([t for t in texts if t])\n        if not all_text:\n            return 0.0, \"No PDF text available for category checks.\"\n        categories = {\n            'controlled substances': [r'controlled substances?', r'cs', r'dea', r'cures'],\n            'prescription processing/dispensing': [r'prescription processing', r'dispensing', r'verification', r'patient counseling'],\n            'inventory/storage/labeling': [r'inventory', r'storage', r'labeling', r'quarantine'],\n            'records/reporting': [r'records?', r'documentation', r'report(ing)?', r'retention'],\n            'facility/security': [r'facility', r'security', r'alarms?', r'keys?'],\n            'temperature/equipment': [r'temperature', r'log', r'refrigerator|fridge|freezer', r'calibration'],\n            'clinical services': [r'immunization', r'vaccine', r'clia', r'protocol'],\n            'privacy/hipaa': [r'hipaa', r'privacy', r'phi'],\n            'pic oversight/training': [r'pic', r'pharmacist-in-charge', r'training', r'competency'],\n            'recalls/expired': [r'recall', r'expired', r'returns?', r'destruction']\n        }\n        found = 0\n        details = []\n        for cname, pats in categories.items():\n            matched = any(re.search(p, all_text) for p in pats)\n            if matched:\n                found += 1\n                details.append(cname)\n        ratio = found / len(categories)\n        score = ratio * 2.5\n        return score, f\"Detected categories ({found}/{len(categories)}): {', '.join(details)}\"\n    except Exception as e:\n        return 0.0, f\"Error during category coverage: {e}\""}, {"type": "code", "name": "Legal Citation Presence (CA Lawbook and Self-Assessment)", "description": "Checks that legal/authority references appear, including California B&P Code, CCR Title 16, HSC, and Self-Assessment references.", "weight": 2.0, "code": "import re\n\ndef evaluate(workflow, context):\n    def read_doc_text(res):\n        try:\n            return (context.files.read_pdf_text(res.id) or '').lower()\n        except Exception:\n            try:\n                return (context.files.read_docx_text(res.id) or '').lower()\n            except Exception:\n                try:\n                    return (context.files.read_text(res.id) or '').lower()\n                except Exception:\n                    return ''\n    try:\n        outputs = context.get_all_outputs() or []\n        texts = []\n        for r in outputs:\n            try:\n                if not getattr(r, 'is_document', False):\n                    continue\n                p = context.files.get_path(r.id)\n                if str(p).lower().endswith('.pdf'):\n                    texts.append(read_doc_text(r))\n            except Exception:\n                continue\n        if not texts:\n            return 0.0, \"No PDF text for citation checks.\"\n        def has_marker(t):\n            markers = [\n                r'bus(iness)?\\.?\\s*&\\s*prof(essions)?\\.?\\s*code|bpc',\n                r'california code of regulations|ccr',\n                r'title\\s*16',\n                r'health and safety code|hsc',\n                r'\\u00a7|section\\s*\\d',\n                r'17m-13|community pharmacy self-assessment',\n                r'pharmacy\\.ca\\.gov'\n            ]\n            return any(re.search(p, t) for p in markers)\n        docs_with_markers = sum(1 for t in texts if has_marker(t))\n        # Bonus for specific section patterns like \"16 CCR 1707.1\" or \"BPC 4113\"\n        section_pat = r'(?:16\\s*ccr\\s*\\d{3,4}|ccr\\s*title\\s*16\\s*\\d{3,4}|bpc\\s*\\d{3,4}|business and professions code\\s*\\d{3,4}|hsc\\s*\\d{4,6})'\n        specific_hits = sum(1 for t in texts if re.search(section_pat, t))\n        base = (docs_with_markers / max(1, len(texts))) * 1.5\n        bonus = min(0.5, 0.25 * specific_hits)\n        score = min(2.0, base + bonus)\n        fb = f\"Docs with legal markers: {docs_with_markers}/{len(texts)}; specific hits: {specific_hits}.\"\n        return score, fb\n    except Exception as e:\n        return 0.0, f\"Error during legal citation checks: {e}\""}, {"type": "code", "name": "Checklist Columns / Fields Present", "description": "Detects key column/field labels within each PDF: Requirement, Citation, Responsible, Date/Initials (or Signature/Verified By), Notes.", "weight": 2.0, "code": "import re\n\ndef evaluate(workflow, context):\n    def read_doc_text(res):\n        try:\n            return (context.files.read_pdf_text(res.id) or '').lower()\n        except Exception:\n            try:\n                return (context.files.read_docx_text(res.id) or '').lower()\n            except Exception:\n                try:\n                    return (context.files.read_text(res.id) or '').lower()\n                except Exception:\n                    return ''\n    try:\n        outputs = context.get_all_outputs() or []\n        pdf_texts = []\n        for r in outputs:\n            try:\n                if not getattr(r, 'is_document', False):\n                    continue\n                p = context.files.get_path(r.id)\n                if str(p).lower().endswith('.pdf'):\n                    pdf_texts.append(read_doc_text(r))\n            except Exception:\n                continue\n        if not pdf_texts:\n            return 0.0, \"No PDF text for field detection.\"\n        cols = {\n            'requirement': [r'requirement', r'task', r'item'],\n            'citation': [r'citation', r'authority', r'reference'],\n            'responsible': [r'responsible', r'role', r'owner', r'pic'],\n            'verify': [r'date', r'initials', r'signature', r'verified by'],\n            'notes': [r'notes?', r'comments?', r'exceptions?']\n        }\n        per_doc_scores = []\n        for t in pdf_texts:\n            present = 0\n            for name, pats in cols.items():\n                if any(re.search(p, t) for p in pats):\n                    present += 1\n            per_doc_scores.append(present / len(cols))\n        # average across PDFs and scale to weight 2.0\n        avg = sum(per_doc_scores) / len(per_doc_scores)\n        score = avg * 2.0\n        return score, f\"Avg column coverage: {avg:.2f} across {len(per_doc_scores)} PDFs.\"\n    except Exception as e:\n        return 0.0, f\"Error during column/field checks: {e}\""}, {"type": "code", "name": "Versioning and Source Footer", "description": "Checks for version/revision/effective date indicators and explicit source references in footers or elsewhere.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    def read_doc_text(res):\n        try:\n            return (context.files.read_pdf_text(res.id) or '').lower()\n        except Exception:\n            try:\n                return (context.files.read_docx_text(res.id) or '').lower()\n            except Exception:\n                try:\n                    return (context.files.read_text(res.id) or '').lower()\n                except Exception:\n                    return ''\n    try:\n        outputs = context.get_all_outputs() or []\n        pdf_texts = []\n        for r in outputs:\n            try:\n                if not getattr(r, 'is_document', False):\n                    continue\n                p = context.files.get_path(r.id)\n                if str(p).lower().endswith('.pdf'):\n                    pdf_texts.append(read_doc_text(r))\n            except Exception:\n                continue\n        if not pdf_texts:\n            return 0.0, \"No PDF text for version/footer checks.\"\n        version_markers = [r'version', r'rev\\.?', r'updated', r'effective']\n        date_pat = r'(\\b20(2|3|4)\\d\\b|\\b\\d{1,2}/\\d{1,2}/\\d{2,4}\\b)'\n        source_markers = [r'2025 lawbook', r'lawbook\\.pdf', r'community pharmacy self-assessment', r'17m-13']\n        v_hits = 0\n        s_hits = 0\n        for t in pdf_texts:\n            has_version = any(re.search(p, t) for p in version_markers) or re.search(date_pat, t)\n            if has_version:\n                v_hits += 1\n            if any(re.search(p, t) for p in source_markers):\n                s_hits += 1\n        # version credit up to 0.5, source footer up to 0.5\n        v_score = 0.5 * (v_hits / len(pdf_texts))\n        s_score = 0.5 * (s_hits / len(pdf_texts))\n        score = v_score + s_score\n        fb = f\"Version markers in {v_hits}/{len(pdf_texts)}; source refs in {s_hits}/{len(pdf_texts)}.\"\n        return score, fb\n    except Exception as e:\n        return 0.0, f\"Error during version/footer checks: {e}\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Quality and Professionalism (LLM)", "description": "Holistic assessment of usability, completeness for key risk areas, and professional presentation for frontline compliance use.", "is_required": false, "max_points": 7.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Usability and Print-Readiness", "description": "Evaluates whether each page is clear, concise, scannable, and immediately usable on paper by staff.", "weight": 2.0, "judge_prompt": "Evaluate ONLY quality/usability, not structure. Consider all three PDFs together.\nScore higher if:\n- Clear, concise checklist items (actionable vs. vague statements).\n- Readable, printer-friendly layout (adequate spacing, consistent fonts, obvious checkboxes, page fits on one sheet without clutter).\n- Field areas for initials/date/notes are easy to use.\n- Logical flow that suits the frequency (daily items short and operational; periodic items grouped sensibly).\nScoring:\n- 2.0: Highly usable; concise, well-formatted, and immediately deployable.\n- 1.2\u20131.8: Generally usable; some minor crowding or verbosity but workable.\n- 0.5\u20131.1: Usable with noticeable friction (cluttered/unclear phrasing or layout issues).\n- 0.0\u20130.4: Hard to use as a practical paper checklist.", "expectation": "Staff can print and use each page without edits; items are short, checkable, and well-spaced."}, {"type": "llm_judge", "name": "Coverage of High-Risk Compliance Areas", "description": "Evaluates whether major CA Board of Pharmacy risk areas are substantively covered across the three pages.", "weight": 2.0, "judge_prompt": "Judge whether the three checklists collectively address high-risk areas for a California community pharmacy. Look for: controlled substances (DEA, CURES), prescription processing/labeling, inventory/storage/quarantine/expired returns, records/reporting (retention timelines), facility/security, temperature/equipment logs and calibration, clinical services (immunizations/CLIA if applicable), privacy/HIPAA, PIC oversight/training, recalls. Be flexible on naming.\nScoring:\n- 2.0: Comprehensive coverage of all or nearly all critical risk areas with appropriate frequency placement.\n- 1.2\u20131.8: Most areas covered; a few notable omissions or misplaced items.\n- 0.5\u20131.1: Several gaps; limited utility for preventing violations.\n- 0.0\u20130.4: Major omissions; insufficient for meaningful compliance oversight.", "expectation": "All key risk domains appear with appropriate frequency placement."}, {"type": "llm_judge", "name": "Accuracy of Citations and Terminology", "description": "Assesses whether legal references and terms (e.g., BPC, CCR Title 16, HSC, 17M-13) appear correct and non-misleading.", "weight": 1.5, "judge_prompt": "Look at the citations/terminology used. Are the references plausible and non-misleading for California pharmacy rules (e.g., BPC sections, CCR Title 16, HSC, DEA/CURES references, Self-Assessment 17M-13)? You do NOT need to verify exact statute text, but penalize obviously wrong jurisdictions or irrelevant codes.\nScoring:\n- 1.5: Consistently appropriate CA-specific references; nothing obviously incorrect.\n- 0.8\u20131.4: Minor inconsistencies but overall correct.\n- 0.1\u20130.7: Several mismatches or unclear references.\n- 0.0: Mostly incorrect or unrelated citations.", "expectation": "Citations reflect CA BPC, CCR Title 16, HSC, DEA/CURES, and 17M-13 where appropriate."}, {"type": "llm_judge", "name": "Consistency and Professional Presentation", "description": "Assesses uniformity of headings, typography, spacing, and alignment across the three PDFs.", "weight": 1.5, "judge_prompt": "Evaluate consistency across the three checklists: matching title style, headers/footers, column labels, typography, spacing, and alignment. Prefer a cohesive template.\nScoring:\n- 1.5: Highly consistent and professional across all three.\n- 0.9\u20131.3: Mostly consistent with minor discrepancies.\n- 0.4\u20130.8: Noticeable inconsistencies that distract but still usable.\n- 0.0\u20130.3: Inconsistent and unprofessional presentation.", "expectation": "All three pages look like one cohesive system with consistent styles and fields."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "5ad0c554-a7a2-48cd-b41a-ebc1bff4a9de", "rubric": {"category_name": "Real Estate Sales Agents \u2014 Double-Sided Buyer Brochure (Sarasota, FL)", "rationale": "This staged rubric enforces a self-documenting, verifiable brochure deliverable. Stage 1 (LLM-only) mandates a DOCX/PDF brochure with exact structural sections and citations, creating a stable shape for verification. Stage 2 mixes code checks to verify correctness and compliance signals (NAR 2024 BBA requirement, local context, citations to the '132 Things' list). Stage 3 uses an LLM judge to assess professional quality, clarity for first-time buyers, and design effectiveness.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (Document Structure and Format)", "description": "Gate: Validate the brochure is a 2+ page DOCX/PDF with brochure-like structure, required sections, and verifiable citations to the '132 Things Realtors Do For Buyers' list. Only check structure/presence, not quality or correctness.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Document Format and Required Sections Present", "description": "Checks if the output is a properly structured brochure document with required sections and visible citations, enabling later verification.", "weight": 4.0, "judge_prompt": "You are validating the STRUCTURE ONLY (not content quality or factual accuracy) of the candidate output. Use the rendered document (DOCX/PDF) to assess the following brochure-format requirements. Be flexible with synonym section names, but ensure clear headers and structure exist.\n\nFormat Requirements:\n- Must be a DOCX or PDF document (not plain text, not Excel).\n- Must be at least 2 pages (double-sided brochure equivalent). Page count is required.\n- Brochure-like layout: visually organized into panels, columns, or clearly delineated sections suitable for a handout.\n- At least one image or visual element included (e.g., photo, icon, graphic).\n\nRequired Sections (as visible headers; accept close variants/synonyms):\n1) A localized intro/callout referencing Sarasota, Florida and the buyers\u2019 context (gated community with amenities and need for financing).\n2) A dedicated section explaining the Buyer\u2019s Broker Agreement (BBA) that references the 2024 NAR settlement/requirements and states clearly that agents cannot show or tour properties without a signed buyer-broker agreement.\n3) Five milestone sections with headings (accept close variants):\n   - Buyer consultation\n   - The home search process\n   - Pre-offer details\n   - The offer process\n   - Contract to closing\n4) For EACH of the five milestone sections: include at least 3 bulleted/action items that explicitly cite the external resource \"132 Things REALTORS Do For Buyers\" using item numbers or clear references (e.g., \u201c#12\u201d, \u201cItem 12\u201d, or \u201c(12)\u201d), and include a reference to the source (the title and/or the URL https://www.bubbleinfo.com/wp-content/uploads/2024/02/132-Things-REALTORS-Do-For-Buyers.pdf).\n\nScoring (STRUCTURE PRESENCE ONLY):\n- 4.0: Valid DOCX/PDF; 2+ pages; brochure-like layout; image/visual present; BBA/NAR 2024 section present and clearly states no showings without signed BBA; all five milestone sections present; each milestone includes \u22653 cited items with recognizable item numbers; source reference included.\n- 3.0: Valid format and 2+ pages with brochure-like layout and BBA/NAR 2024 section; all five milestones present; but one of these is missing or incomplete: (a) the image/visual, or (b) some milestones have fewer than 3 cited items, or (c) source reference is present but not clearly labeled.\n- 2.0: Valid format and 2+ pages; but missing the BBA/NAR 2024 section OR one milestone section entirely; OR citations are present but only sporadically (e.g., fewer than 6 total item-number citations across the brochure).\n- 1.0: Valid format but not clearly brochure-like and/or fewer than 2 pages, or multiple core sections missing (e.g., several milestones absent, no BBA section, and no citations).\n- 0.0: Not a DOCX/PDF, or obviously not a brochure (e.g., plain note), or less than 2 pages and lacks core sections.\n\nOnly check presence/structure as listed above. Do NOT judge accuracy or quality of the content.", "expectation": "A 2+ page DOCX/PDF brochure with clear section headers, a BBA/NAR 2024 compliance section, five milestone sections, and explicit numbered citations to the '132 Things' list, plus at least one visual."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Compliance Verification", "description": "Code and LLM checks that leverage the enforced shape: verify milestone coverage, NAR/BBA compliance statements, proper citation patterns to the '132 Things' list, and Sarasota/gated community/financing context signals.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Milestone Headings Present (Text Signals)", "description": "Detects presence of the five milestone sections using flexible keyword matching in extracted text.", "weight": 1.2, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output\"\n\n    # Try to extract text from DOCX or PDF\n    text = \"\"\n    try:\n        if output.is_document:\n            # Try DOCX first\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_pdf_text(output.id)\n                except Exception:\n                    pass\n        if not text:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    except Exception:\n        text = \"\"\n\n    if not text:\n        return 0.0, \"Could not extract text\"\n\n    t = text.lower()\n\n    categories = {\n        'buyer_consultation': [\"buyer consultation\", \"initial consultation\", \"consultation\", \"onboarding\", \"needs assessment\"],\n        'home_search': [\"home search\", \"search process\", \"property search\", \"touring homes\", \"showings\", \"home-search\"],\n        'pre_offer': [\"pre-offer\", \"pre offer\", \"before you write an offer\", \"preoffer\", \"due diligence\", \"cma\", \"comparative market analysis\", \"disclosures review\"],\n        'offer_process': [\"offer process\", \"making an offer\", \"write an offer\", \"submit an offer\", \"negotiation\", \"negotiate\"],\n        'contract_to_closing': [\"contract to closing\", \"from contract to close\", \"under contract\", \"escrow\", \"closing\", \"contingencies\", \"final walk-through\", \"final walkthrough\"]\n    }\n\n    found = 0\n    missing = []\n    for key, variants in categories.items():\n        matched = any(v in t for v in variants)\n        if matched:\n            found += 1\n        else:\n            missing.append(key)\n\n    score = found / 5.0  # normalized 0..1\n    feedback = f\"Milestone sections found: {found}/5. Missing: {', '.join(missing) if missing else 'None'}\"\n    return score, feedback"}, {"type": "code", "name": "NAR 2024 and Buyer\u2019s Broker Agreement Compliance Statement", "description": "Checks text for presence of BBA mention, NAR/2024, and a clear no-showings-without-signed-agreement statement.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output\"\n\n    text = \"\"\n    try:\n        if output.is_document:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_pdf_text(output.id)\n                except Exception:\n                    pass\n        if not text:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    except Exception:\n        text = \"\"\n\n    if not text:\n        return 0.0, \"Could not extract text\"\n\n    t = text.lower()\n\n    # Buyer Broker Agreement synonyms\n    bba_terms = [\n        \"buyer broker agreement\", \"buyer\u2019s broker agreement\", \"buyers broker agreement\", \"buyer brokerage agreement\",\n        \"buyer representation agreement\", \"buyer agency agreement\", \"bba\"\n    ]\n    has_bba = any(term in t for term in bba_terms)\n\n    nar_terms = [\"nar\", \"national association of realtors\", \"realtors\u00ae\", \"realtors(r)\"]\n    has_nar = any(term in t for term in nar_terms)\n\n    has_2024 = \"2024\" in t\n\n    # Prohibition phrasing\n    prohibition_patterns = [\n        r\"cannot\\s+show\", r\"can\\'t\\s+show\", r\"can\u2019t\\s+show\", r\"not\\s+allowed\\s+to\\s+show\",\n        r\"no\\s+showings\\s+without\", r\"must\\s+have\\s+a\\s+signed\", r\"require[d]?\\s+\\w*\\s+signed\\s+buyer\",\n        r\"before\\s+touring\\s+any\\s+home\", r\"prior\\s+to\\s+showings\"\n    ]\n    has_prohibition = any(re.search(p, t) for p in prohibition_patterns)\n\n    components = [has_bba, has_nar, has_2024, has_prohibition]\n    count = sum(1 for c in components if c)\n    score = count / 4.0\n\n    details = []\n    if not has_bba: details.append(\"BBA mention missing\")\n    if not has_nar: details.append(\"NAR mention missing\")\n    if not has_2024: details.append(\"2024 mention missing\")\n    if not has_prohibition: details.append(\"No-showings-without-signed-agreement phrasing missing\")\n\n    feedback = f\"Compliance components present: {count}/4. Missing: {', '.join(details) if details else 'None'}\"\n    return score, feedback"}, {"type": "code", "name": "Citations to '132 Things REALTORS Do For Buyers' with Item Numbers", "description": "Checks for explicit source reference and multiple item-number citations (1\u2013132) using patterns like '#12', 'Item 12', or '(12)'.", "weight": 1.2, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output\"\n\n    text = \"\"\n    try:\n        if output.is_document:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_pdf_text(output.id)\n                except Exception:\n                    pass\n        if not text:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    except Exception:\n        text = \"\"\n\n    if not text:\n        return 0.0, \"Could not extract text\"\n\n    t = text.lower()\n\n    # Source reference signals\n    has_url = \"bubbleinfo.com\" in t\n    has_132_phrase = \"132 things\" in t\n    has_source = has_url or has_132_phrase\n\n    # Find item-number-like citations bounded to 1..132\n    pattern = re.compile(r\"(?:item\\s*#?\\s*(\\d{1,3}))|(?:#\\s*(\\d{1,3}))|(?:\\((\\d{1,3})\\))\")\n    nums = set()\n    for m in pattern.finditer(t):\n        for g in m.groups():\n            if g is None: continue\n            try:\n                v = int(g)\n                if 1 <= v <= 132:\n                    nums.add(v)\n            except Exception:\n                pass\n\n    distinct = len(nums)\n\n    # Score logic: encourage at least ~10-12 distinct citations and explicit source mention\n    base = min(distinct / 12.0, 1.0)  # normalized 0..1\n    if not has_source:\n        base *= 0.6  # penalize missing explicit source reference\n\n    feedback = f\"Distinct item citations detected: {distinct}. Source referenced: {has_source}.\"\n    return base, feedback"}, {"type": "code", "name": "Local Buyer Context Signals (Sarasota, Gated Community, Financing)", "description": "Checks for Sarasota/FL context, gated community with amenities, financing/pre-approval, and single-family home cues.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output\"\n\n    text = \"\"\n    try:\n        if output.is_document:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_pdf_text(output.id)\n                except Exception:\n                    pass\n        if not text:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    except Exception:\n        text = \"\"\n\n    if not text:\n        return 0.0, \"Could not extract text\"\n\n    t = text.lower()\n\n    # Define five signal buckets\n    signals = {\n        'sarasota': any(k in t for k in [\"sarasota\"]),\n        'gated': any(k in t for k in [\"gated community\", \"gated communities\"]),\n        'amenities': any(k in t for k in [\"amenities\", \"clubhouse\", \"pool\", \"fitness center\", \"hoa amenities\"]),\n        'financing': any(k in t for k in [\"financing\", \"pre-approval\", \"preapproval\", \"mortgage\", \"lender\", \"loan pre-approval\", \"loan preapproval\"]),\n        'single_family': any(k in t for k in [\"single family\", \"single-family\"]),\n    }\n\n    count = sum(1 for v in signals.values() if v)\n    score = count / 5.0\n    missing = [k for k, v in signals.items() if not v]\n\n    feedback = f\"Local buyer signals present: {count}/5. Missing: {', '.join(missing) if missing else 'None'}\"\n    return score, feedback"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Audience Fit", "description": "LLM judge assesses overall design polish, clarity for first-time buyers, effective use of visuals, and professional tone consistent with real estate marketing.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Design, Clarity, and Practical Value", "description": "Holistic quality assessment: layout, readability, tone for first-time buyers, local relevance, and professionalism.", "weight": 2.0, "judge_prompt": "Assess the brochure\u2019s overall professional quality and suitability for first-time home buyers in Sarasota, FL. The document should already meet structural requirements; now evaluate its effectiveness and presentation.\n\nConsider:\n- Visual design: clear hierarchy, scannable panels/columns, adequate whitespace, readable fonts, consistent styles; appropriate, non-distracting images/graphics.\n- Clarity and buyer-friendliness: plain-language explanations, avoids jargon, explains steps and expectations for each milestone succinctly.\n- Practical usefulness: includes actionable bullets, clear calls to action, and helpful details for buyers (e.g., what to bring to the consultation, how to get pre-approved, what to expect during inspections/closing).\n- Localization and alignment: Sarasota/gated-community context and financing needs are woven naturally; messaging appears compliant and sets expectations (BBA section is clear and supportive rather than intimidating). Include a professional contact/footer block if present.\n- Cohesion: the citations to the '132 Things' are integrated meaningfully (not random lists), and the brochure reads as a coherent, branded handout.\n\nScoring:\n- 2.0: Highly professional and clear; visually strong; tailored to first-time buyers; compelling, cohesive, and useful.\n- 1.0: Adequate but with noticeable issues (e.g., dense text, awkward flow, weak visuals); still usable for buyers.\n- 0.0: Poorly presented, confusing, or not suitable for first-time buyers.\n\nFocus on quality and audience fit, not structure or strict correctness.", "expectation": "A polished, buyer-friendly brochure that looks professional, reads clearly, and would be effective in a real client meeting."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "11dcc268-cb07-4d3a-a184-c6d7a19349bc", "rubric": {"category_name": "Manufacturing \u2014 Shipping, Receiving, and Inventory Clerks: Daily Location Report (Excel)", "rationale": "This rubric enforces a self-documenting workflow for building a populated Location Report from a Receiving Log and assigned location list. Stage 1 (LLM gate) mandates an Excel workbook shape that enables verification: a Location Report sheet with specific columns, an Assigned Location Map, a Reconciliation summary, and a Methodology sheet documenting sources and assumptions (including the partial receipt rule for P11-P09457-01). Stage 2 uses code rules to verify quantitative correctness, mapping consistency, and special-case handling, leveraging the known shape. Stage 3 evaluates professional presentation and operational suitability for material handlers.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Structured Excel Output Gate", "description": "LLM-only gate to enforce exact workbook structure that enables verification.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Excel Structure and Required Sheets", "description": "Verify the candidate produced a properly structured Excel workbook with all required sheets and columns.", "weight": 4.0, "judge_prompt": "You are evaluating whether the submitted OUTPUT is a valid Excel workbook with the exact structure needed for verification. Only check presence/format/structure \u2014 do NOT judge correctness of numbers.\n\nCheck the following (be flexible with closely similar names):\n\nFORMAT REQUIREMENTS:\n- Must be an Excel spreadsheet (.xlsx). Not PDF, not CSV, not DOCX.\n- Workbook must contain at least 4 sheets with the following roles:\n  1) Location Report (primary) \u2014 or similar name like: \"Location Report\", \"Putaway Report\", \"Move to Line\", \"Location Moves\".\n  2) Assigned Location Map \u2014 or similar: \"Assigned Locations\", \"Item \u2192 Location Map\", \"Inv on line\", \"Line Locations\", \"Item Locations\".\n  3) Reconciliation \u2014 or similar: \"Recon\", \"Summary\", \"Daily Totals\", \"Roll-up\".\n  4) Methodology \u2014 or similar: \"Notes\", \"Assumptions & Sources\", \"How Calculated\", \"Data Lineage\".\n\nLOCATION REPORT SHEET STRUCTURE (must be a table with headers in row 1):\n- Required columns (flexible synonyms allowed, but must be clearly identifiable):\n  \u2022 Item Number (e.g., Item, Part Number, SKU)\n  \u2022 Description\n  \u2022 Supplier (e.g., Vendor)\n  \u2022 Received Qty (e.g., Qty Received)\n  \u2022 Moved Qty (e.g., Putaway Qty)\n  \u2022 UOM (Unit of Measure)\n  \u2022 Moved From (e.g., Source/Staging/Receiving Dock)\n  \u2022 Moved To (Assigned/Line/Destination Location)\n  \u2022 Move Timestamp (e.g., Date/Time, Moved At)\n  \u2022 Handler/Initials (e.g., Moved By/User)\n  \u2022 Reference (e.g., Receiving Log ID/PO/Receipt)\n  \u2022 Balance Remaining (e.g., Qty Remaining in Staging)\n- Data should be in a grid with one row per moved line.\n\nASSIGNED LOCATION MAP SHEET:\n- Table mapping Item Number \u2192 Assigned Location. Required columns:\n  \u2022 Item Number\n  \u2022 Assigned Location (or Line Location)\n\nRECONCILIATION SHEET:\n- Item-level summary with at least these columns:\n  \u2022 Item Number\n  \u2022 Received Today\n  \u2022 Moved to Location (or similar)\n  \u2022 Balance Remaining\n- Include overall totals (either an explicit Total row or a clearly labeled totals section).\n\nMETHODOLOGY SHEET:\n- Short narrative (at least 3 sentences) that:\n  \u2022 Lists data sources used (Daily Receiving Log, Inv on line, Blank Template).\n  \u2022 States that staging/phantom locations in \"Moved From\" are temporary.\n  \u2022 Explains handling of partial receipts and that only half the quantity of item P11-P09457-01 was received and moved to its line location today, with the remaining balance left for next day.\n\nSCORING:\n- 4.0: Correct file type AND all 4 sheets present with the structures above (Location Report has all required columns; Assigned Location Map has both required columns; Reconciliation has required columns + totals; Methodology meets narrative requirements including the P11-P09457-01 note).\n- 3.0: Correct file type; all required sheets exist but one sheet is missing 1-2 required columns/elements OR the Methodology is present but missing one required narrative element.\n- 2.0: Correct file type; at least the Location Report + one supporting sheet exist, but substantial structural elements missing across others.\n- 1.0: Correct file type but only a single relevant sheet exists or structure is largely incomplete.\n- 0.0: Not an Excel file or the Location Report sheet is missing.\n\nOnly evaluate presence/format/structure per above. Do NOT verify calculations or data correctness.", "expectation": "A cleanly structured .xlsx with the four sheets and specified columns/sections so that code checks can verify correctness later."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Consistency Verification", "description": "Code-based checks leveraging the enforced shape to verify quantities, mappings, and reconciliation logic.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Core Sheets and Columns (Light Structural Recheck)", "description": "Fuzzy-detect critical sheets and core columns on Location Report; award proportional credit for presence.", "weight": 0.8, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not getattr(output, 'is_spreadsheet', False):\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheet_names = [str(s) for s in xls.sheet_names]\n    except Exception:\n        return 0.0\n\n    def find_sheet(cands):\n        # Return first sheet whose name contains all tokens from any candidate pattern\n        low = [s.lower() for s in sheet_names]\n        for s in low:\n            for tokens in cands:\n                if all(t in s for t in tokens):\n                    return sheet_names[low.index(s)]\n        return None\n\n    # Likely sheets\n    loc_sheet = find_sheet([\n        ['location','report'], ['putaway','report'], ['location','moves'], ['move','to','line'], ['location']\n    ])\n    map_sheet = find_sheet([\n        ['assigned','location'], ['location','map'], ['item','location'], ['inv','on','line'], ['line','location']\n    ])\n    rec_sheet = find_sheet([\n        ['recon'], ['summary'], ['daily','totals'], ['roll'], ['reconciliation']\n    ])\n    meth_sheet = find_sheet([\n        ['method'], ['assumption'], ['notes'], ['lineage']\n    ])\n\n    present = [loc_sheet is not None, map_sheet is not None, rec_sheet is not None, meth_sheet is not None]\n\n    score = 0.0\n    # Core columns on Location Report\n    if loc_sheet is not None:\n        try:\n            df = pd.read_excel(path, sheet_name=loc_sheet)\n            cols = [str(c).strip().lower() for c in df.columns]\n            def has_any(keys):\n                return any(any(k in c for k in keys) for c in cols)\n            requirements = {\n                'item': ['item','part','sku','pn'],\n                'desc': ['desc'],\n                'supplier': ['supplier','vendor'],\n                'recv': ['received qty','qty received','received','recv','rcvd'],\n                'moved': ['moved qty','qty moved','putaway','moved'],\n                'uom': ['uom','unit of measure','unit'],\n                'from': ['moved from','from location','source','staging','dock','receiv'],\n                'to': ['moved to','to location','assigned','line','destination'],\n                'ts': ['timestamp','moved at','date','time','datetime'],\n                'handler': ['handler','moved by','user','operator','initials'],\n                'ref': ['reference','receiving','receipt','po','grn','asn','ref'],\n                'bal': ['balance remaining','remaining','qty remaining','left in staging','balance']\n            }\n            needed = list(requirements.keys())\n            have = []\n            for k, keys in requirements.items():\n                have.append(has_any(keys))\n            present_cols_ratio = sum(have) / len(have)\n        except Exception:\n            present_cols_ratio = 0.0\n    else:\n        present_cols_ratio = 0.0\n\n    # Weighting: 50% for sheets presence, 50% for columns presence\n    sheet_ratio = sum(present) / 4.0\n    total_ratio = 0.5*sheet_ratio + 0.5*present_cols_ratio\n    total_ratio = max(0.0, min(1.0, total_ratio))\n    return 0.8 * total_ratio"}, {"type": "code", "name": "Quantity Validity and Non-Negative Bounds", "description": "Checks non-negative quantities, and Moved Qty <= Received Qty on Location Report; also ensures From != To for most rows.", "weight": 1.2, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not getattr(output, 'is_spreadsheet', False):\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheet_names = [s for s in xls.sheet_names]\n    except Exception:\n        return 0.0\n\n    def find_loc_sheet(names):\n        low = [n.lower() for n in names]\n        for i, s in enumerate(low):\n            if ('location' in s and ('report' in s or 'move' in s or 'putaway' in s)) or s.strip()=='location report':\n                return names[i]\n        for i, s in enumerate(low):\n            if 'location' in s:\n                return names[i]\n        return None\n\n    loc = find_loc_sheet(sheet_names)\n    if not loc:\n        return 0.0\n\n    try:\n        df = pd.read_excel(path, sheet_name=loc)\n    except Exception:\n        return 0.0\n\n    cols = {str(c).strip().lower(): c for c in df.columns}\n    def pick_col(syns):\n        for want in syns:\n            for lc, orig in cols.items():\n                if want in lc:\n                    return orig\n        return None\n\n    c_item = pick_col(['item','part','sku','pn'])\n    c_recv = pick_col(['received qty','qty received','received','recv','rcvd'])\n    c_moved = pick_col(['moved qty','qty moved','putaway','moved'])\n    c_from = pick_col(['moved from','from location','source','staging','dock','receiv'])\n    c_to = pick_col(['moved to','to location','assigned','line','destination'])\n\n    if c_recv is None or c_moved is None:\n        return 0.0\n\n    r = pd.to_numeric(df[c_recv], errors='coerce')\n    m = pd.to_numeric(df[c_moved], errors='coerce')\n\n    # Consider only rows with at least one quantity present\n    mask = ~(r.isna() & m.isna())\n    r = r[mask]\n    m = m[mask]\n    sub = df.loc[mask]\n\n    if len(sub) == 0:\n        return 0.0\n\n    nonneg = ((r >= 0) & (m >= 0)).mean()\n    not_excess = (m <= r + 1e-9).mean()  # small tolerance\n\n    # From != To and From looks like staging (~temporary) for most rows\n    if c_from is not None and c_to is not None:\n        fromv = sub[c_from].astype(str).str.lower().fillna('')\n        tov = sub[c_to].astype(str).str.lower().fillna('')\n        neq = (fromv != tov).mean()\n        staging_keywords = ['dock','recv','receiv','staging','temp','qc','inbound','phantom']\n        looks_staging = fromv.apply(lambda x: any(k in x for k in staging_keywords)).mean()\n    else:\n        neq = 0.5\n        looks_staging = 0.5\n\n    frac = np.mean([nonneg, not_excess, neq, looks_staging])\n    frac = float(np.clip(frac, 0, 1))\n    return 1.2 * frac"}, {"type": "code", "name": "Special Case: P11-P09457-01 Half Received and Moved", "description": "Verify that for item P11-P09457-01, Moved Qty is approximately half of Received Qty for the day, with remaining balance indicated.", "weight": 1.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not getattr(output, 'is_spreadsheet', False):\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        names = [s for s in xls.sheet_names]\n    except Exception:\n        return 0.0\n\n    # Find location report\n    loc = None\n    for s in names:\n        sl = s.lower()\n        if ('location' in sl and ('report' in sl or 'move' in sl or 'putaway' in sl)) or sl=='location report':\n            loc = s\n            break\n    if loc is None:\n        for s in names:\n            if 'location' in s.lower():\n                loc = s\n                break\n    if loc is None:\n        return 0.0\n\n    try:\n        df = pd.read_excel(path, sheet_name=loc)\n    except Exception:\n        return 0.0\n\n    # Column picks\n    cols = {str(c).strip().lower(): c for c in df.columns}\n    def pick_col(syns):\n        for want in syns:\n            for lc, orig in cols.items():\n                if want in lc:\n                    return orig\n        return None\n    c_item = pick_col(['item','part','sku','pn'])\n    c_recv = pick_col(['received qty','qty received','received','recv','rcvd'])\n    c_moved = pick_col(['moved qty','qty moved','putaway','moved'])\n    c_bal = pick_col(['balance remaining','remaining','qty remaining','left in staging','balance'])\n\n    if c_item is None or c_recv is None or c_moved is None:\n        return 0.0\n\n    items = df[c_item].astype(str).str.upper().str.strip()\n    mask = items == 'P11-P09457-01'\n    if not mask.any():\n        # Item should be present per task; if missing, score 0\n        return 0.0\n\n    r = pd.to_numeric(df.loc[mask, c_recv], errors='coerce').fillna(0)\n    m = pd.to_numeric(df.loc[mask, c_moved], errors='coerce').fillna(0)\n    r_sum = float(r.sum())\n    m_sum = float(m.sum())\n\n    if r_sum <= 0:\n        return 0.0\n\n    ratio = m_sum / r_sum if r_sum != 0 else 0.0\n    # Accept within +/-5% as full; within +/-10% as partial\n    if 0.45 <= ratio <= 0.55:\n        base = 1.0\n    elif 0.40 <= ratio <= 0.60:\n        base = 0.5\n    else:\n        base = 0.0\n\n    # Extra small boost if a balance column indicates remaining qty > 0\n    bonus = 0.0\n    if c_bal is not None:\n        bal = pd.to_numeric(df.loc[mask, c_bal], errors='coerce').fillna(0)\n        if float(bal.sum()) > 0 and ratio < 1.0:\n            bonus = 0.1\n    score = min(1.0, base + bonus)\n    return 1.0 * score"}, {"type": "code", "name": "Assigned Location Mapping Consistency", "description": "Verify that Moved To locations match the Assigned Location Map for the item for most rows, and From != To.", "weight": 0.8, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not getattr(output, 'is_spreadsheet', False):\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        names = [s for s in xls.sheet_names]\n    except Exception:\n        return 0.0\n\n    def find_sheet(names, want_any):\n        for s in names:\n            sl = s.lower()\n            if all(tok in sl for tok in want_any):\n                return s\n        return None\n\n    # Find sheets\n    loc = None\n    for s in names:\n        sl = s.lower()\n        if ('location' in sl and ('report' in sl or 'move' in sl or 'putaway' in sl)) or sl=='location report':\n            loc = s; break\n    if loc is None:\n        for s in names:\n            if 'location' in s.lower():\n                loc = s; break\n    map_sheet = find_sheet(names, ['assigned','location']) or find_sheet(names, ['location','map']) or find_sheet(names, ['inv','on','line']) or find_sheet(names, ['item','location']) or find_sheet(names, ['line','location'])\n    if loc is None or map_sheet is None:\n        return 0.0\n\n    try:\n        df = pd.read_excel(path, sheet_name=loc)\n        mp = pd.read_excel(path, sheet_name=map_sheet)\n    except Exception:\n        return 0.0\n\n    def canon_cols(df):\n        return {str(c).strip().lower(): c for c in df.columns}\n    c1 = canon_cols(df)\n    c2 = canon_cols(mp)\n\n    def pick(colmap, syns):\n        for s in syns:\n            for lc, orig in colmap.items():\n                if s in lc:\n                    return orig\n        return None\n\n    item_col = pick(c1, ['item','part','sku','pn'])\n    to_col = pick(c1, ['moved to','to location','assigned','line','destination'])\n    map_item = pick(c2, ['item','part','sku','pn'])\n    map_loc = pick(c2, ['assigned','line location','location'])\n\n    if item_col is None or to_col is None or map_item is None or map_loc is None:\n        return 0.0\n\n    # Build mapping\n    mp2 = mp[[map_item, map_loc]].copy()\n    mp2[map_item] = mp2[map_item].astype(str).str.upper().str.strip()\n    mp2[map_loc] = mp2[map_loc].astype(str).str.upper().str.strip()\n    mapping = dict(zip(mp2[map_item], mp2[map_loc]))\n\n    sub = df[[item_col, to_col]].copy()\n    sub[item_col] = sub[item_col].astype(str).str.upper().str.strip()\n    sub[to_col] = sub[to_col].astype(str).str.upper().str.strip()\n\n    # Compare\n    has_map = sub[item_col].isin(mapping.keys())\n    if has_map.sum() == 0:\n        return 0.0\n\n    matched = (sub.loc[has_map, to_col] == sub.loc[has_map, item_col].map(mapping)).mean()\n\n    # Also ensure From != To for mapped rows if available\n    from_col = pick(c1, ['moved from','from location','source','staging','dock','receiv'])\n    if from_col is not None:\n        sub[from_col] = df[from_col].astype(str).str.upper().str.strip()\n        neq = (sub.loc[has_map, from_col] != sub.loc[has_map, to_col]).mean()\n    else:\n        neq = 0.5\n\n    frac = float(np.mean([matched, neq]))\n    return 0.8 * max(0.0, min(1.0, frac))"}, {"type": "code", "name": "Reconciliation Agreement with Detail", "description": "Check that the Reconciliation sheet\u2019s per-item numbers agree with the summed detail rows for a sample of items.", "weight": 0.2, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not getattr(output, 'is_spreadsheet', False):\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        names = [s for s in xls.sheet_names]\n    except Exception:\n        return 0.0\n\n    # Find sheets\n    loc = None\n    for s in names:\n        sl = s.lower()\n        if ('location' in sl and ('report' in sl or 'move' in sl or 'putaway' in sl)) or sl=='location report':\n            loc = s; break\n    if loc is None:\n        for s in names:\n            if 'location' in s.lower():\n                loc = s; break\n    rec = None\n    for s in names:\n        sl = s.lower()\n        if 'recon' in sl or 'summary' in sl or 'daily' in sl or 'roll' in sl:\n            rec = s; break\n\n    if loc is None or rec is None:\n        return 0.0\n\n    try:\n        d = pd.read_excel(path, sheet_name=loc)\n        r = pd.read_excel(path, sheet_name=rec)\n    except Exception:\n        return 0.0\n\n    def canon(df):\n        return {str(c).strip().lower(): c for c in df.columns}\n    c1 = canon(d); c2 = canon(r)\n\n    def pick(cmap, syns):\n        for s in syns:\n            for lc, orig in cmap.items():\n                if s in lc:\n                    return orig\n        return None\n\n    item1 = pick(c1, ['item','part','sku','pn'])\n    recv1 = pick(c1, ['received qty','qty received','received','recv','rcvd'])\n    moved1 = pick(c1, ['moved qty','qty moved','putaway','moved'])\n\n    item2 = pick(c2, ['item','part','sku','pn'])\n    recv2 = pick(c2, ['received today','received','recv','rcvd'])\n    moved2 = pick(c2, ['moved to location','moved','putaway'])\n\n    if None in [item1, recv1, moved1, item2, recv2, moved2]:\n        return 0.0\n\n    d2 = d[[item1, recv1, moved1]].copy()\n    d2[item1] = d2[item1].astype(str).str.upper().str.strip()\n    d2[recv1] = pd.to_numeric(d2[recv1], errors='coerce').fillna(0)\n    d2[moved1] = pd.to_numeric(d2[moved1], errors='coerce').fillna(0)\n    grp = d2.groupby(item1).agg({recv1:'sum', moved1:'sum'}).reset_index()\n\n    r2 = r[[item2, recv2, moved2]].copy()\n    r2[item2] = r2[item2].astype(str).str.upper().str.strip()\n    r2[recv2] = pd.to_numeric(r2[recv2], errors='coerce').fillna(0)\n    r2[moved2] = pd.to_numeric(r2[moved2], errors='coerce').fillna(0)\n\n    merged = pd.merge(grp, r2, left_on=item1, right_on=item2, how='inner', suffixes=('_det','_rec'))\n    if len(merged) == 0:\n        return 0.0\n\n    # Tolerance: absolute 1 unit OR 1% of detailed sum, whichever larger\n    def agree(a, b):\n        tol = max(1.0, 0.01*max(abs(a), abs(b)))\n        return abs(a-b) <= tol\n\n    recv_ok = merged.apply(lambda x: agree(x[recv1+'_det'], x[recv2]), axis=1)\n    moved_ok = merged.apply(lambda x: agree(x[moved1+'_det'], x[moved2]), axis=1)\n    frac = float(np.mean([(recv_ok.mean() if len(merged)>0 else 0.0), (moved_ok.mean() if len(merged)>0 else 0.0)]))\n    return 0.2 * max(0.0, min(1.0, frac))"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Presentation and Operational Readiness", "description": "LLM judge assesses if the workbook is clear, professionally formatted, and practically usable by material handlers.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Quality and Usability", "description": "Assess formatting, clarity, and readiness for use on the production floor.", "weight": 2.0, "judge_prompt": "Evaluate the overall presentation and operational readiness of this Excel workbook for material handlers. Do not re-check structure; focus on quality and usability.\n\nConsider:\n- Clarity and labeling of headers; freeze panes and filters on the Location Report table; consistent units (UOM) and date/time formats.\n- Sorting/grouping that makes picking efficient (e.g., grouped by destination line location or by item), and visibility of reference IDs.\n- Reasonable, human-friendly formatting: number formatting for quantities, text wrapping for descriptions, no obviously hidden critical data.\n- Moved From looks like staging/phantom locations; Moved To looks like line/assigned locations.\n- Presence of a concise notes area or highlights to draw attention to partials/backlog (e.g., remaining balances) without clutter.\n\nScoring (0-2):\n- 2.0: Professional, clean, filterable/sortable, clearly labeled, easy to use for picking.\n- 1.0: Generally clear but some formatting/labeling inconsistencies or minor friction points.\n- 0.0: Messy, confusing, or not practically usable by material handlers.\n", "expectation": "A clean, filterable Location Report with clear headers and consistent formatting that a handler can use immediately."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "1752cb53-5983-46b6-92ee-58ac85a11283", "rubric": {"category_name": "Manufacturing \u2014 Week One Test Plan (Wire Extrusion)", "rationale": "This rubric enforces a self-documenting, verifiable Excel planning artifact suitable for first-run validations on two new extrusion presses. Stage 1 uses an LLM judge to mandate and verify the exact spreadsheet shape needed to enable deterministic checks. Stage 2 applies code-based validations that leverage the mandated structure to verify correctness and internal consistency without requiring the hidden reference documents. Stage 3 evaluates professional quality and readiness for cross-functional use (maintenance, quality, engineering).", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (LLM Only)", "description": "Output must be a properly structured Excel workbook for the Week One Test Plan. This gate verifies the exact structure and section presence so Stage 2 can run deterministic checks.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Structured Excel Output Requirement", "description": "Verify the candidate produced a single Excel workbook with the exact planning structure required for a Week One Test Plan across two presses. Only check presence/format, not correctness of numbers.", "weight": 8.0, "judge_prompt": "You are validating the SHAPE of the output only. Do not judge calculation correctness.\n\nCheck the candidate's primary output. It MUST be an Excel file (.xlsx) with a professionally formatted Week One Test Plan for running initial validations on two presses (Press 1 and Press 2). Be flexible with exact sheet names (minor variations acceptable), but the following structure must be clearly present:\n\nRequired Format:\n- File type: Excel (.xlsx). Not PDF/DOCX/CSV/MD.\n- Contains clearly labeled sheets with the following roles. Accept close variants of names.\n\nRequired Sheets and Core Structure:\n1) Plan Overview (a.k.a. Overview, Summary, or Executive Summary)\n   - A top-level summary table for Week One that includes:\n     \u2022 Total planned finished goods quantity (or units) per press\n     \u2022 Total planned run hours per press\n     \u2022 Number of SKUs planned\n     \u2022 Total changeover time per press (or at least visible changeovers summarized)\n   - A short status/assumptions area noting this is for first validation runs.\n\n2) Press Schedule (a.k.a. Schedule, Production Schedule, Week One Schedule)\n   - A table with columns (exact names may vary, but they must be visibly present):\n     \u2022 Date\n     \u2022 Shift\n     \u2022 Press (values reflect Press 1 and Press 2)\n     \u2022 SKU (finished good)\n     \u2022 FG Description\n     \u2022 Planned Qty\n     \u2022 Run Rate (units/hour)\n     \u2022 Planned Run Hours\n     \u2022 Start Time\n     \u2022 End Time\n     \u2022 Changeover Minutes\n     \u2022 Tooling ID\n   - Schedule rows for at least two SKUs total and use of both presses.\n\n3) Labor Plan (a.k.a. Staffing Plan, Labor Schedule)\n   - A table with columns:\n     \u2022 Date\n     \u2022 Shift\n     \u2022 Press\n     \u2022 Role (e.g., Operator, QA, Maintenance, Engineering)\n     \u2022 Person/Name\n     \u2022 Start Time\n     \u2022 End Time\n     \u2022 Hours\n\n4) Material Plan (a.k.a. Raw Material Plan, Components Plan)\n   - A table with columns:\n     \u2022 SKU\n     \u2022 Raw Material/Component ID (or Part Number)\n     \u2022 Description\n     \u2022 Unit\n     \u2022 Per-FG Qty (per unit requirement)\n     \u2022 Planned FG Qty (linked to schedule quantity)\n     \u2022 Total Required Qty\n     \u2022 On Hand\n     \u2022 Shortage/To Buy\n\n5) BOM Summary (a.k.a. FG BOM Requirement)\n   - A table with columns:\n     \u2022 SKU\n     \u2022 FG Description\n     \u2022 Component/Material\n     \u2022 Per-FG Qty\n     \u2022 Tooling ID\n     \u2022 Std Run Rate (or similar capability/run standard)\n\n6) Tooling & Changeovers (a.k.a. Tooling, Setup Times)\n   - A table documenting:\n     \u2022 Tooling ID\n     \u2022 SKU(s)\n     \u2022 Expected Changeover Minutes\n     \u2022 Compatible Presses\n\n7) Assumptions & Notes (a.k.a. Methodology/Guidelines)\n   - Brief explanatory text stating planning assumptions, constraints, and that only yellow input cells were edited as instructed.\n\nYellow-Cell Convention:\n- The spreadsheet should indicate yellow input cells for user-populated fields. Check that yellow-highlighted input fields are filled and non-highlighted areas appear unedited where applicable. Be flexible; if explicit highlighting isn\u2019t visible, there must be clear labeling like \u201cYellow cells = inputs\u201d and those input fields are filled while unused cells are left blank.\n\nScoring (do not assess calculation correctness):\n- 8: Excel workbook present with all 7 sheet roles accounted for, Press Schedule includes all the listed columns, yellow-cell convention present and followed.\n- 7: Minor omissions (e.g., one supporting sheet lightly abbreviated) but core Press Schedule, Labor Plan, and Material Plan are complete and yellow-cell rule is clear.\n- 6: One required sheet missing or Press Schedule missing 1-2 columns, but overall structure still enables verification.\n- 4: Multiple required sheets missing or Press Schedule missing several core columns.\n- 0: Not an Excel file, or structure too incomplete to verify.\n\nOnly check the presence and structure, not whether numbers are right.", "expectation": "A complete, well-structured Excel plan with clearly labeled sheets and input areas enabling deterministic verification in Stage 2."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Consistency Verification (Code + Deterministic Checks)", "description": "Deterministic checks leveraging the mandated structure to verify schedule integrity, press usage, changeovers, material tie-out, labor coverage, and basic arithmetic consistency.", "is_required": false, "max_points": 9.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Press Schedule \u2014 Structural Integrity", "description": "Find a Press Schedule-like sheet and verify presence of essential columns to enable subsequent checks.", "weight": 1.5, "code": "def evaluate(workflow, context):\n    import re, json\n    import pandas as pd\n    import numpy as np\n\n    weight = 1.5\n\n    out = context.get_primary_output()\n    if not out or not out.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n\n    try:\n        xls_path = context.files.get_path(out.id)\n        ef = pd.ExcelFile(xls_path)\n        sheets = ef.sheet_names\n    except Exception as e:\n        return 0.0, f\"Failed to open Excel: {e}\"\n\n    essential = {\n        'date': ['date','day'],\n        'shift': ['shift'],\n        'press': ['press','machine','line'],\n        'sku': ['sku','fg','fg sku','part','part number'],\n        'planned_qty': ['planned qty','plan qty','quantity','planned quantity','fg qty'],\n        'run_rate': ['run rate','u/hr','units/hr','rate'],\n        'planned_hours': ['planned hours','run hours','planned run hours','hrs'],\n        'start_time': ['start time','start','run start'],\n        'end_time': ['end time','end','run end','finish'],\n        'changeover_min': ['changeover','c/o','changeover min','changeover minutes','setup min','setup time'],\n        'tooling_id': ['tooling','tooling id','die id','tool id','mold id']\n    }\n\n    best_present = 0\n    best_sheet = None\n\n    for s in sheets:\n        try:\n            df = context.files.read_excel(out.id, sheet_name=s)\n            cols = [str(c).strip().lower() for c in df.columns]\n            present = 0\n            for key, alts in essential.items():\n                if any(a in '|'.join(cols) or a in cols for a in alts):\n                    present += 1\n            if present > best_present:\n                best_present = present\n                best_sheet = s\n        except Exception:\n            continue\n\n    # Score proportional to coverage of essential fields; require at least a reasonable subset\n    # Full credit when >= 9 of 11 are present\n    score_ratio = min(1.0, best_present / 9.0)\n    # If fewer than 5 essentials, consider too weak to be useful\n    if best_present < 5:\n        score_ratio *= 0.5\n\n    feedback = f\"Best schedule-like sheet: {best_sheet}; essentials present: {best_present}/11\"\n    return weight * score_ratio, feedback"}, {"type": "code", "name": "Both Presses Utilized + No Overlap per Press", "description": "Verify that both Press 1 and Press 2 are scheduled and that, within each press, time blocks do not overlap.", "weight": 2.0, "code": "def evaluate(workflow, context):\n    import re, json\n    import pandas as pd\n    import numpy as np\n\n    weight = 2.0\n\n    out = context.get_primary_output()\n    if not out or not out.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n\n    def find_schedule_sheet():\n        try:\n            xls_path = context.files.get_path(out.id)\n            ef = pd.ExcelFile(xls_path)\n        except Exception:\n            return None, None\n        best = None\n        best_df = None\n        best_hits = -1\n        essential = ['date','shift','press','sku','planned qty','start','end','run rate','planned hours','changeover','tooling']\n        for s in ef.sheet_names:\n            try:\n                df = context.files.read_excel(out.id, sheet_name=s)\n                cols = [str(c).lower() for c in df.columns]\n                hits = sum(any(k in c for c in cols) for k in essential)\n                if hits > best_hits:\n                    best_hits = hits\n                    best = s\n                    best_df = df\n            except Exception:\n                continue\n        return best, best_df\n\n    sched_name, df = find_schedule_sheet()\n    if df is None or len(df) == 0:\n        return 0.0, \"No identifiable schedule sheet.\"\n\n    # Normalize columns\n    def pick(df, options):\n        cols = {c.lower(): c for c in df.columns}\n        for opt in options:\n            for k, v in cols.items():\n                if opt in k:\n                    return v\n        return None\n\n    press_col = pick(df, ['press','machine','line'])\n    start_col = pick(df, ['start time','start'])\n    end_col = pick(df, ['end time','end','finish'])\n    date_col = pick(df, ['date','day'])\n\n    if press_col is None:\n        return 0.0, \"No press column found.\"\n\n    # Both presses used?\n    presses = df[press_col].astype(str).str.lower()\n    has_p1 = presses.str.contains('press\\s*1').any() or presses.str.fullmatch('1').any() or presses.str.contains('\\b1\\b').any()\n    has_p2 = presses.str.contains('press\\s*2').any() or presses.str.fullmatch('2').any() or presses.str.contains('\\b2\\b').any()\n\n    score_use = 1.0 if (has_p1 and has_p2) else (0.5 if presses.nunique()>=2 else 0.0)\n\n    # Overlap check per press\n    def to_ts(date_series, time_series):\n        # Return numeric minutes from an arbitrary origin per row; if date is parseable, combine\n        d = pd.to_datetime(date_series, errors='coerce') if date_series is not None else None\n        t = pd.to_datetime(time_series, errors='coerce')\n        # If times are durations (e.g., 8:00), pandas sets a date; we only need minutes-of-day\n        # Build minutes since epoch per row using date if available\n        mins = []\n        for i in range(len(time_series)):\n            ti = t.iloc[i] if len(t)>i else pd.NaT\n            di = d.iloc[i] if (d is not None and len(d)>i) else pd.NaT\n            if pd.isna(ti) and not pd.isna(time_series.iloc[i]):\n                # try as float hours\n                try:\n                    mins.append(float(str(time_series.iloc[i]))*60.0)\n                    continue\n                except Exception:\n                    mins.append(np.nan)\n                    continue\n            if pd.isna(ti):\n                mins.append(np.nan)\n                continue\n            if not pd.isna(di):\n                base = pd.Timestamp(di.date())\n                mins.append((ti - pd.Timestamp(base.date())).total_seconds()/60.0)\n            else:\n                # strip date, keep time\n                mins.append(ti.hour*60.0 + ti.minute + ti.second/60.0)\n        return pd.Series(mins)\n\n    overlap_ok = 0.5  # default partial if we cannot evaluate\n    if start_col is not None and end_col is not None:\n        df_local = df.copy()\n        df_local['__start_m'] = to_ts(df_local[date_col] if date_col else None, df_local[start_col])\n        df_local['__end_m'] = to_ts(df_local[date_col] if date_col else None, df_local[end_col])\n\n        valid = df_local.dropna(subset=['__start_m','__end_m'])\n        overlap_free = True\n        for pv, grp in valid.groupby(press_col):\n            g = grp.sort_values(['__start_m','__end_m'])\n            prev_end = -1e9\n            for _, r in g.iterrows():\n                s = r['__start_m']\n                e = r['__end_m']\n                if pd.notna(s) and pd.notna(e):\n                    if s < prev_end - 1e-6:  # overlap\n                        overlap_free = False\n                        break\n                    prev_end = max(prev_end, e)\n            if not overlap_free:\n                break\n        overlap_ok = 1.0 if overlap_free else 0.0\n\n    total_score = weight * (0.5*score_use + 0.5*overlap_ok)\n    fb = f\"Both presses used: {has_p1 and has_p2}; Overlap-free: {overlap_ok==1.0}; Sheet={sched_name}\"\n    return total_score, fb"}, {"type": "code", "name": "Changeover Logic by SKU", "description": "When consecutive jobs on the same press change SKU, there should be non-zero changeover minutes; otherwise zero or blank.", "weight": 1.5, "code": "def evaluate(workflow, context):\n    import re, json\n    import pandas as pd\n    import numpy as np\n\n    weight = 1.5\n\n    out = context.get_primary_output()\n    if not out or not out.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n\n    def load_schedule():\n        try:\n            xls_path = context.files.get_path(out.id)\n            ef = pd.ExcelFile(xls_path)\n        except Exception:\n            return None, None\n        best = None\n        best_df = None\n        best_hits = -1\n        for s in ef.sheet_names:\n            try:\n                df = context.files.read_excel(out.id, sheet_name=s)\n                cols = [str(c).lower() for c in df.columns]\n                hits = sum(any(k in c for c in cols) for k in ['press','sku','start','end','changeover'])\n                if hits > best_hits:\n                    best_hits = hits\n                    best = s\n                    best_df = df\n            except Exception:\n                continue\n        return best, best_df\n\n    name, df = load_schedule()\n    if df is None or len(df)==0:\n        return 0.0, \"No schedule identified.\"\n\n    def pick(df, options):\n        cols = {c.lower(): c for c in df.columns}\n        for opt in options:\n            for k,v in cols.items():\n                if opt in k:\n                    return v\n        return None\n\n    press_col = pick(df, ['press','machine','line'])\n    sku_col = pick(df, ['sku','fg','part'])\n    co_col = pick(df, ['changeover','setup'])\n    start_col = pick(df, ['start time','start'])\n\n    if press_col is None or sku_col is None or co_col is None:\n        return weight*0.2, \"Insufficient columns for changeover check.\"\n\n    df2 = df.copy()\n    # Create a sort key\n    if start_col is not None:\n        df2['__sort'] = pd.to_datetime(df2[start_col], errors='coerce')\n    else:\n        df2['__sort'] = np.arange(len(df2))\n\n    checks = 0\n    ok = 0\n\n    for pv, grp in df2.groupby(press_col):\n        g = grp.sort_values('__sort')\n        prev_sku = None\n        for _, r in g.iterrows():\n            current_sku = str(r.get(sku_col, '')).strip()\n            co_val = r.get(co_col, np.nan)\n            try:\n                co_num = float(str(co_val)) if str(co_val).strip()!='' else 0.0\n            except Exception:\n                co_num = np.nan\n            if prev_sku is not None:\n                checks += 1\n                if current_sku != prev_sku:\n                    ok += 1 if (pd.notna(co_num) and co_num > 0) else 0\n                else:\n                    ok += 1 if (pd.isna(co_val) or co_num == 0) else 0\n            prev_sku = current_sku\n\n    if checks == 0:\n        return weight*0.3, \"Not enough adjacent jobs to evaluate changeovers.\"\n\n    ratio = ok / checks\n    return weight * ratio, f\"Changeover compliance: {ok}/{checks}\""}, {"type": "code", "name": "Material Requirements Tie-Out", "description": "Verify Total Required Qty approximately equals Per-FG Qty \u00d7 Planned FG Qty; if Planned FG Qty missing in Material Plan, cross-reference Planned Qty from Schedule by SKU.", "weight": 2.0, "code": "def evaluate(workflow, context):\n    import re, json\n    import pandas as pd\n    import numpy as np\n\n    weight = 2.0\n\n    out = context.get_primary_output()\n    if not out or not out.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n\n    def read_all_sheets():\n        try:\n            xls_path = context.files.get_path(out.id)\n            ef = pd.ExcelFile(xls_path)\n            return {s: context.files.read_excel(out.id, sheet_name=s) for s in ef.sheet_names}\n        except Exception:\n            return {}\n\n    sheets = read_all_sheets()\n    if not sheets:\n        return 0.0, \"Cannot read Excel sheets.\"\n\n    # Locate Material Plan\n    mat_name, mat_df = None, None\n    for s, df in sheets.items():\n        cols = ' | '.join([str(c).lower() for c in df.columns])\n        if all(k in cols for k in ['sku','qty']) and any(k in cols for k in ['raw','material','component']):\n            mat_name, mat_df = s, df\n            break\n    if mat_df is None:\n        return 0.0, \"No Material Plan-like sheet found.\"\n\n    # Locate Schedule for fallback planned qty by SKU\n    sch_name, sch_df = None, None\n    for s, df in sheets.items():\n        cols = ' | '.join([str(c).lower() for c in df.columns])\n        if all(k in cols for k in ['press','sku']) and any(k in cols for k in ['planned','run rate']):\n            sch_name, sch_df = s, df\n            break\n\n    def pick(df, options):\n        cols = {c.lower(): c for c in df.columns}\n        for opt in options:\n            for k,v in cols.items():\n                if opt in k:\n                    return v\n        return None\n\n    sku_m = pick(mat_df, ['sku','fg','part'])\n    per_fg = pick(mat_df, ['per-fg qty','per unit qty','qty per','per fg','unit req'])\n    plan_fg = pick(mat_df, ['planned fg qty','planned qty','fg qty'])\n    total_req = pick(mat_df, ['total required','total req','required qty','gross req'])\n\n    if sku_m is None or total_req is None or per_fg is None:\n        return weight*0.2, \"Material Plan missing key columns.\"\n\n    mat = mat_df.copy()\n    # Coerce numerics\n    for c in [per_fg, plan_fg, total_req]:\n        if c:\n            mat[c+'_num'] = pd.to_numeric(mat[c], errors='coerce')\n    # Build SKU-level planned qty if not present\n    sku_plan = None\n    if plan_fg is None and sch_df is not None:\n        sku_s = pick(sch_df, ['sku','fg','part'])\n        plan_s = pick(sch_df, ['planned qty','quantity','planned quantity','fg qty'])\n        if sku_s and plan_s:\n            t = sch_df[[sku_s, plan_s]].copy()\n            t[plan_s] = pd.to_numeric(t[plan_s], errors='coerce')\n            sku_plan = t.groupby(sku_s, dropna=False)[plan_s].sum(min_count=1)\n\n    checks = 0\n    ok = 0\n\n    # If Planned FG Qty present in mat\n    if plan_fg is not None:\n        for _, r in mat.iterrows():\n            try:\n                per = r.get(per_fg+'_num', np.nan)\n                pfg = r.get(plan_fg+'_num', np.nan)\n                tot = r.get(total_req+'_num', np.nan)\n                if pd.notna(per) and pd.notna(pfg) and pd.notna(tot):\n                    exp = per * pfg\n                    checks += 1\n                    if (abs(exp - tot) <= max(0.05*max(exp,1.0), 1e-6)):\n                        ok += 1\n            except Exception:\n                pass\n    elif sku_plan is not None:\n        # Aggregate material req by SKU and compare to per*plan\n        if sku_m in mat.columns:\n            grp = mat.copy()\n            grp['per_num'] = pd.to_numeric(grp[per_fg], errors='coerce')\n            grp['tot_num'] = pd.to_numeric(grp[total_req], errors='coerce')\n            for sku, gp in grp.groupby(sku_m):\n                plan_qty = sku_plan.get(sku, np.nan)\n                if pd.isna(plan_qty):\n                    continue\n                # Sum per-unit across components? Here we verify proportionality: total_req / per \u2248 plan_qty across rows\n                ratios = gp.apply(lambda r: (r['tot_num']/r['per_num']) if (pd.notna(r['tot_num']) and pd.notna(r['per_num']) and r['per_num']!=0) else np.nan, axis=1)\n                ratios = ratios.dropna()\n                if len(ratios)>0:\n                    checks += 1\n                    median_ratio = np.median(ratios)\n                    if abs(median_ratio - plan_qty) <= max(0.1*max(plan_qty,1.0), 1.0):\n                        ok += 1\n\n    if checks == 0:\n        return weight*0.3, \"Insufficient data to verify material tie-out.\"\n\n    ratio = ok / checks\n    return weight*ratio, f\"Material tie-out OK {ok}/{checks} cases.\""}, {"type": "code", "name": "Labor Coverage + No Double-Booking", "description": "For each scheduled press run, ensure an Operator is assigned overlapping the run window for that press; also check a person is not double-booked across overlapping times.", "weight": 1.5, "code": "def evaluate(workflow, context):\n    import re, json\n    import pandas as pd\n    import numpy as np\n\n    weight = 1.5\n\n    out = context.get_primary_output()\n    if not out or not out.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n\n    def read_all_sheets():\n        try:\n            xls_path = context.files.get_path(out.id)\n            ef = pd.ExcelFile(xls_path)\n            return {s: context.files.read_excel(out.id, sheet_name=s) for s in ef.sheet_names}\n        except Exception:\n            return {}\n\n    sheets = read_all_sheets()\n    if not sheets:\n        return 0.0, \"Cannot read Excel sheets.\"\n\n    # Identify schedule\n    sched = None\n    for s, df in sheets.items():\n        cols = ' | '.join([str(c).lower() for c in df.columns])\n        if all(k in cols for k in ['press','sku']) and any(k in cols for k in ['start','planned hours']):\n            sched = df\n            break\n\n    # Identify labor plan\n    labor = None\n    labor_name = None\n    for s, df in sheets.items():\n        cols = ' | '.join([str(c).lower() for c in df.columns])\n        if all(k in cols for k in ['press','role']) and any(k in cols for k in ['person','name','operator']):\n            labor = df\n            labor_name = s\n            break\n\n    if sched is None or labor is None:\n        return weight*0.3, \"Missing schedule or labor plan.\"\n\n    def pick(df, options):\n        cols = {c.lower(): c for c in df.columns}\n        for opt in options:\n            for k,v in cols.items():\n                if opt in k:\n                    return v\n        return None\n\n    s_press = pick(sched, ['press','machine','line'])\n    s_start = pick(sched, ['start time','start'])\n    s_end = pick(sched, ['end time','end','finish'])\n    s_date = pick(sched, ['date','day'])\n\n    l_press = pick(labor, ['press','machine','line'])\n    l_role = pick(labor, ['role','position'])\n    l_name = pick(labor, ['person','name','operator','team member'])\n    l_start = pick(labor, ['start time','start'])\n    l_end = pick(labor, ['end time','end','finish'])\n    l_date = pick(labor, ['date','day'])\n\n    if not all([s_press, l_press, l_role, l_name]):\n        return weight*0.3, \"Labor/schedule missing key columns.\"\n\n    def to_minutes(date_s, time_s):\n        d = pd.to_datetime(date_s, errors='coerce') if date_s is not None else None\n        t = pd.to_datetime(time_s, errors='coerce')\n        arr = []\n        for i in range(len(time_s)):\n            ti = t.iloc[i] if len(t)>i else pd.NaT\n            di = d.iloc[i] if (d is not None and len(d)>i) else pd.NaT\n            if pd.isna(ti) and not pd.isna(time_s.iloc[i]):\n                try:\n                    arr.append(float(str(time_s.iloc[i]))*60.0)\n                    continue\n                except Exception:\n                    arr.append(np.nan)\n                    continue\n            if pd.isna(ti):\n                arr.append(np.nan)\n            else:\n                minutes = ti.hour*60 + ti.minute + ti.second/60\n                if not pd.isna(di):\n                    minutes += (pd.Timestamp(di.date()) - pd.Timestamp('1970-01-01')).days*24*60\n                arr.append(float(minutes))\n        return pd.Series(arr)\n\n    sched = sched.copy()\n    labor = labor.copy()\n\n    if s_start and s_end:\n        sched['__s'] = to_minutes(sched[s_date] if s_date else None, sched[s_start])\n        sched['__e'] = to_minutes(sched[s_date] if s_date else None, sched[s_end])\n    else:\n        sched['__s'] = np.nan\n        sched['__e'] = np.nan\n\n    if l_start and l_end:\n        labor['__s'] = to_minutes(labor[l_date] if l_date else None, labor[l_start])\n        labor['__e'] = to_minutes(labor[l_date] if l_date else None, labor[l_end])\n    else:\n        labor['__s'] = np.nan\n        labor['__e'] = np.nan\n\n    # Coverage: Operator assigned per schedule row\n    sched_valid = sched.dropna(subset=['__s','__e']) if sched['__s'].notna().any() else sched\n    cover_checks = 0\n    cover_ok = 0\n\n    for idx, r in sched_valid.iterrows():\n        pr = str(r.get(s_press, '')).strip().lower()\n        rs, re = r.get('__s', np.nan), r.get('__e', np.nan)\n        if pd.isna(rs) or pd.isna(re):\n            continue\n        cover_checks += 1\n        cand = labor[labor[l_press].astype(str).str.lower().str.contains(pr.split()[-1] if 'press' in pr else pr, na=False)]\n        # Operator roles\n        roles = cand[l_role].astype(str).str.lower()\n        is_op = roles.str.contains('operator') | roles.str.contains('op')\n        cand = cand[is_op]\n        if len(cand)==0:\n            continue\n        # overlapping any portion\n        overlaps = cand[(cand['__s']<re) & (cand['__e']>rs)] if cand['__s'].notna().any() else cand\n        if len(overlaps)>0:\n            cover_ok += 1\n\n    cover_ratio = (cover_ok/cover_checks) if cover_checks>0 else 0.5\n\n    # Double-booking: per person overlapping times\n    dbl_checks = 0\n    dbl_viol = 0\n    if labor['__s'].notna().any() and labor['__e'].notna().any() and l_name:\n        for person, g in labor.dropna(subset=['__s','__e']).groupby(l_name):\n            g = g.sort_values('__s')\n            prev_e = -1e9\n            for _, rr in g.iterrows():\n                s = rr['__s']; e = rr['__e']\n                dbl_checks += 1\n                if s < prev_e - 1e-6:\n                    dbl_viol += 1\n                prev_e = max(prev_e, e)\n\n    dbl_ok = 1.0 if dbl_viol == 0 else max(0.0, 1.0 - (dbl_viol / max(dbl_checks,1)))\n\n    total = weight * (0.67*cover_ratio + 0.33*dbl_ok)\n    fb = f\"Coverage {cover_ok}/{cover_checks}; Double-book violations={dbl_viol} on {dbl_checks} checks\"\n    return total, fb"}, {"type": "code", "name": "Run-Rate vs Planned-Hours Consistency", "description": "Check that Planned Run Hours approximately equals Planned Qty / Run Rate for rows where all fields are present.", "weight": 0.5, "code": "def evaluate(workflow, context):\n    import re, json\n    import pandas as pd\n    import numpy as np\n\n    weight = 0.5\n\n    out = context.get_primary_output()\n    if not out or not out.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n\n    def find_schedule():\n        try:\n            xls_path = context.files.get_path(out.id)\n            ef = pd.ExcelFile(xls_path)\n        except Exception:\n            return None\n        best = None\n        best_df = None\n        best_hits = -1\n        for s in ef.sheet_names:\n            try:\n                df = context.files.read_excel(out.id, sheet_name=s)\n                cols = [str(c).lower() for c in df.columns]\n                hits = sum(any(k in c for c in cols) for k in ['planned qty','run rate','planned hours','press','sku'])\n                if hits > best_hits:\n                    best_hits = hits\n                    best_df = df\n            except Exception:\n                continue\n        return best_df\n\n    df = find_schedule()\n    if df is None:\n        return 0.0, \"No schedule sheet.\"\n\n    def pick(df, options):\n        cols = {c.lower(): c for c in df.columns}\n        for opt in options:\n            for k,v in cols.items():\n                if opt in k:\n                    return v\n        return None\n\n    q_col = pick(df, ['planned qty','quantity','planned quantity','fg qty'])\n    r_col = pick(df, ['run rate','units/hr','u/hr'])\n    h_col = pick(df, ['planned hours','run hours'])\n\n    if not all([q_col, r_col, h_col]):\n        return weight*0.2, \"Missing qty/rate/hours columns.\"\n\n    t = df[[q_col, r_col, h_col]].copy()\n    t['q'] = pd.to_numeric(t[q_col], errors='coerce')\n    t['r'] = pd.to_numeric(t[r_col], errors='coerce')\n    t['h'] = pd.to_numeric(t[h_col], errors='coerce')\n    valid = t.dropna()\n    if len(valid)==0:\n        return weight*0.2, \"No numeric rows.\"\n\n    valid = valid[valid['r']!=0]\n    if len(valid)==0:\n        return weight*0.2, \"Run rate zero/invalid.\"\n\n    valid['exp'] = valid['q']/valid['r']\n    ok = (abs(valid['exp']-valid['h']) <= (0.1*valid['exp'].abs().clip(lower=0.25))).sum()\n    ratio = ok / len(valid)\n\n    return weight*ratio, f\"Run-rate consistency {ok}/{len(valid)}\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Cross-Functional Readiness", "description": "LLM assessment of presentation quality, clarity, and readiness for maintenance/quality/engineering validation use. Not about numeric correctness; about usability and professionalism.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Readiness", "description": "Evaluate clarity, formatting, labeling, and whether the plan is directly usable by cross-functional teams for first validations.", "weight": 3.0, "judge_prompt": "Assess the overall professional quality and readiness of the Excel plan as a deliverable for first-run validations. Do NOT re-check Stage 1 structure, and do NOT judge numeric correctness. Focus on usability for maintenance, quality, and engineering.\n\nConsider:\n- Clarity and professional formatting: readable headers, frozen panes, units labeled (e.g., units/hour, minutes), visible legends/notes.\n- Clear indication of input fields vs. calculated fields, with the yellow-cell convention documented and followed.\n- Traceability: Press Schedule aligns with Labor Plan and Material Plan via consistent SKU and press labeling; Tooling IDs referenced consistently across sheets.\n- Practical readiness: includes assumptions/notes about validations (e.g., run targets, acceptance criteria placeholders, escalation contacts) and makes it easy to execute Day 1.\n- Appropriateness: audience-focused and free of extraneous narrative; concise but complete.\n\nScoring guidance:\n- 3: Highly professional, immediately usable by cross-functional teams; assumptions clear; inputs and units obvious; easy to navigate.\n- 2: Generally usable with minor formatting or clarity issues; assumptions mostly clear.\n- 1: Usable but with notable clarity/traceability gaps; teams would need to ask questions.\n- 0: Poorly formatted or confusing; not practically usable for Day 1 validation.\n", "expectation": "A clean, navigable workbook with clear labels, visible units, assumptions, and input conventions suitable for immediate shop-floor validation use."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "4d1a8410-e9c5-4be5-ab43-cc55563c594c", "rubric": {"category_name": "Healthcare Admin Scheduling: Interview Day Schedule + Applicant Itineraries", "rationale": "This rubric enforces a self-documenting, file-based workflow for creating a structured interview-day schedule and two one-page applicant itineraries. Stage 1 (LLM-only) mandates exact deliverable shapes so verification is trivial. Stage 2 mixes robust code checks (anchors, tours, names) with an LLM logic review now that structure exists. Stage 3 evaluates professional quality and presentation.", "max_total_score": 10.0, "stages": [{"name": "Stage 1: Structured Deliverables Gate", "description": "Validate that the candidate produced exactly the required document deliverables with the mandated structural elements that make verification possible.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Mandatory Files and Structure Present", "description": "Confirm presence and structure of 3 documents: (1) Interview Day Schedule document with a detailed table and timeline; (2) One-page Allen (Group A) itinerary; (3) One-page Isabelle (Group B) itinerary. Accept DOCX or PDF only. Do not score content correctness here\u2014only structural presence and formatting.", "weight": 4.0, "judge_prompt": "You are evaluating the SHAPE ONLY of the submitted deliverables. You can view all files (PDF/DOCX). Verify that there are THREE documents with the following structure. Be flexible with section names but strict about presence.\n\nRequired Deliverables (3 total):\n1) Interview Day Schedule (DOCX or PDF):\n   - A clearly titled schedule document for NAMC MTP Interview Day.\n   - Contains an agenda/timeline BEFORE interviews begin with these anchors:\n     \u2022 7:00 AM Breakfast in the Conference Room\n     \u2022 7:35 AM Welcome talks by Dr. Jones and Dr. Garrett in the Forge Auditorium (30 minutes)\n     \u2022 Followed by Dr. Meade 10-minute research talk\n     \u2022 Then a 5-minute break\n   - A structured TABLE for the interview schedule that shows, for each room/physician/resident:\n     \u2022 Columns including: Room Number, Physician/Resident Name, Timing of Interviews (20-minute blocks), 5-minute transitions, Breaks, Lunch, Applicant Name(s) with interview times\n     \u2022 The table must visibly allocate the 20-minute interview blocks and 5-minute transition buffers between rooms (the buffer should not be applied to lunch/breaks)\n     \u2022 Includes the two 15-minute breaks (after the first 5 interviews in the AM and in the PM)\n     \u2022 Includes a built-in break for each room; specifically: Dr. Jones has a 10-minute break at 8:50 AM; Dr. Garrett has the LAST break of the day so he can leave 20 minutes early\n   - Group flow is indicated:\n     \u2022 16 total applicants split into Group A (8) and Group B (8)\n     \u2022 Group A interviews first; Group B tours first\n     \u2022 Both groups return 10 minutes before lunch when on tours; and 10 minutes early from tours in the afternoon\n   - Tour plan lists the five sites to be visited: Main Hospital, Pediatric Center, Cancer Center, Rural Area Clinic, Simulation Learning Center\n   - The interview day runs from 7:00 AM to 4:40 PM; Lunch is 40 minutes and positioned mid-day\n\n2) Applicant Personal Itinerary \u2013 Allen (Group A) (DOCX or PDF):\n   - Exactly one page\n   - Includes: Applicant name (Allen) and Group A\n   - Includes the avatar image (avatar-764x1024)\n   - Includes the Floor Layout for Interviews image\n   - Includes site logos for the tour sites\n   - Shows Allen\u2019s interview times and which physician/residents he meets; indicates 20-min interview blocks with 5-min transitions; shows lunch (40 min); shows relevant pre-interview agenda items (breakfast, welcome talks)\n\n3) Applicant Personal Itinerary \u2013 Isabelle (Group B) (DOCX or PDF):\n   - Exactly one page\n   - Includes: Applicant name (Isabelle) and Group B\n   - Includes the avatar image (avatar-764x1024)\n   - Includes the Floor Layout for Interviews image\n   - Includes site logos for the tour sites\n   - Shows Isabelle\u2019s schedule with tours first and interviews after; includes 20-min interview blocks with 5-min transitions; shows lunch (40 min); shows relevant pre-interview agenda items\n\nScoring (STRUCTURE ONLY):\n- 1.00: All 3 documents present, valid PDF/DOCX, and each has the required visible sections/elements (including table in schedule and images/logos in itineraries). Both itineraries limited to one page each.\n- 0.70: All 3 present but one itinerary missing an image or logos OR minor omission in schedule table headings/labels while still clearly present as a structured table.\n- 0.40: Only two of the three documents present, or major missing structural elements (e.g., schedule lacks a proper table or itineraries not one page).\n- 0.00: Not PDF/DOCX, or only one/no document, or missing schedule document entirely.\n\nOnly evaluate presence/format/structure. Do NOT assess correctness of timing, logic, or quality.", "expectation": "Three properly structured documents: one schedule with a detailed table and agenda anchors; two one-page itineraries with images and logos."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Correctness and Constraints Verification", "description": "Now that the outputs are in verifiable shape, check compliance with timing anchors, content requirements, and minimal consistency across documents.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Files Present and Classified", "description": "Verify that there are at least three document outputs, and classify one as the schedule and two as itineraries for Allen and Isabelle.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0, \"No outputs.\"\n\n    # Filter to documents (docx/pdf)\n    docs = [r for r in outputs if getattr(r, 'is_document', False)]\n    if not docs:\n        return 0.0, \"No document outputs detected.\"\n\n    def read_text(res):\n        # Prefer based on file extension when possible\n        try:\n            path = context.files.get_path(res.id)\n            suf = path.suffix.lower()\n        except Exception:\n            suf = ''\n            path = None\n        text = ''\n        if suf == '.docx':\n            try:\n                text = context.files.read_docx_text(res.id)\n            except Exception:\n                text = ''\n        elif suf == '.pdf':\n            try:\n                text = context.files.read_pdf_text(res.id)\n            except Exception:\n                text = ''\n        else:\n            # Try both fallbacks\n            try:\n                text = context.files.read_docx_text(res.id)\n            except Exception:\n                try:\n                    text = context.files.read_pdf_text(res.id)\n                except Exception:\n                    text = ''\n        return text or ''\n\n    texts = {r.id: read_text(r) for r in docs}\n\n    # Classify documents\n    schedule_id = None\n    itinerary_allen_id = None\n    itinerary_isabelle_id = None\n\n    for r in docs:\n        t = texts.get(r.id, '').lower()\n        # Heuristics\n        is_itinerary = (('itinerary' in t) or ('personal schedule' in t))\n        has_table_cues = any(k in t for k in ['room', 'physician', 'break', 'lunch', 'applicant'])\n        has_agenda_cues = any(k in t for k in ['breakfast', 'welcome', 'forge auditorium', 'conference room'])\n\n        if 'allen' in t and 'group a' in t and is_itinerary:\n            itinerary_allen_id = itinerary_allen_id or r.id\n        if 'isabelle' in t and 'group b' in t and is_itinerary:\n            itinerary_isabelle_id = itinerary_isabelle_id or r.id\n        if (has_table_cues and has_agenda_cues) or ('interview schedule' in t):\n            # Avoid misclassifying itineraries as schedule\n            if not is_itinerary:\n                schedule_id = schedule_id or r.id\n\n    score = 0.0\n    feedback = []\n\n    if schedule_id:\n        score += 0.4\n    else:\n        feedback.append('Schedule document not confidently detected.')\n\n    if itinerary_allen_id:\n        score += 0.3\n    else:\n        feedback.append('Allen itinerary not confidently detected.')\n\n    if itinerary_isabelle_id:\n        score += 0.3\n    else:\n        feedback.append('Isabelle itinerary not confidently detected.')\n\n    # Cap to weight 1.0\n    score = min(score, 1.0)\n    return score, '; '.join(feedback) if feedback else 'All required docs detected.'"}, {"type": "code", "name": "Schedule Anchors and Constraints Present", "description": "Check schedule text for key anchors and policy language (times, breaks, buffers, rooms). Partial credit for partial coverage.", "weight": 1.2, "code": "import re\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    docs = [r for r in outputs if getattr(r, 'is_document', False)]\n    if not docs:\n        return 0.0\n\n    def read_text(res):\n        try:\n            path = context.files.get_path(res.id)\n            suf = path.suffix.lower()\n        except Exception:\n            suf = ''\n        try:\n            if suf == '.docx':\n                return context.files.read_docx_text(res.id) or ''\n            if suf == '.pdf':\n                return context.files.read_pdf_text(res.id) or ''\n        except Exception:\n            pass\n        # Fallback attempts\n        for fn in (context.files.read_docx_text, context.files.read_pdf_text):\n            try:\n                return fn(res.id) or ''\n            except Exception:\n                continue\n        return ''\n\n    # Pick schedule-like doc (most matches)\n    best = None\n    best_hits = -1\n    for r in docs:\n        t = read_text(r).lower()\n        cues = ['interview schedule','room','physician','break','lunch','applicant','forge auditorium','conference room']\n        hits = sum(1 for c in cues if c in t)\n        if hits > best_hits:\n            best_hits = hits\n            best = (r, t)\n\n    if not best or not best[1]:\n        return 0.0, 'Could not locate schedule text.'\n\n    text = best[1]\n\n    checks = []\n    def add(flag, label):\n        checks.append((flag, label))\n\n    # Time anchors and policies\n    add(('7:00' in text or '7:00 am' in text or '7am' in text), '7:00 breakfast')\n    add(('7:35' in text or '7:35 am' in text), '7:35 welcome')\n    # Either explicit 8:05 or textual 30-min then 10-min talk\n    add(('8:05' in text) or (('30' in text and 'welcome' in text and '10' in text and 'meade' in text)), 'Welcome 30 min then Meade 10 min')\n    add(('5-minute break' in text) or ('5 minute break' in text) or ('5-minute' in text and 'break' in text), '5-minute break after talks')\n\n    # Interview structure\n    add(('20-minute' in text) or ('20 minute' in text) or ('20 minutes' in text), '20-minute interviews')\n    add(('5-minute buffer' in text) or ('5 minute buffer' in text) or ('5-minute transition' in text) or ('5 minute transition' in text), '5-minute buffer between rooms')\n\n    # Specific breaks\n    add(('15-minute break' in text) or ('15 minute break' in text), '15-minute AM/PM breaks mentioned')\n    add(('8:50' in text or '8:50 am' in text) and ('jones' in text), 'Dr. Jones 8:50 break')\n    add(('garrett' in text) and ('last break' in text or 'leave early' in text or '20 minutes early' in text), 'Dr. Garrett last break to leave early')\n\n    # Lunch constraints\n    add(('lunch' in text) and (('40-minute' in text) or ('40 minute' in text) or ('40 minutes' in text)), 'Lunch 40 minutes')\n\n    # Groups and flows\n    add(('group a' in text) and ('group b' in text), 'Groups A and B referenced')\n    add(('tour' in text) and ('10 minute' in text or '10-minute' in text) and ('before lunch' in text), 'Tours return 10 minutes before lunch')\n\n    # Locations and day end\n    add(('forge auditorium' in text), 'Forge Auditorium present')\n    add(('conference room' in text), 'Conference Room present')\n    add(('4:40' in text or '4:40 pm' in text or '4:40pm' in text), 'Day end 4:40 PM')\n\n    total = len(checks)\n    matched = sum(1 for f, _ in checks if f)\n    score = (matched / total) * 1.2 if total else 0.0\n    missing = [label for f, label in checks if not f]\n    fb = f\"Matched {matched}/{total}. Missing: {', '.join(missing)}\" if missing else 'All anchors present.'\n    return score, fb"}, {"type": "code", "name": "Tours Include All Five Sites", "description": "Confirm that the five required tour sites are referenced across the deliverables.", "weight": 0.8, "code": "def evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    docs = [r for r in outputs if getattr(r, 'is_document', False)]\n    if not docs:\n        return 0.0\n\n    def read_text(res):\n        try:\n            path = context.files.get_path(res.id)\n            suf = path.suffix.lower()\n        except Exception:\n            suf = ''\n        try:\n            if suf == '.docx':\n                return context.files.read_docx_text(res.id) or ''\n            if suf == '.pdf':\n                return context.files.read_pdf_text(res.id) or ''\n        except Exception:\n            pass\n        for fn in (context.files.read_docx_text, context.files.read_pdf_text):\n            try:\n                return fn(res.id) or ''\n            except Exception:\n                continue\n        return ''\n\n    big = ' '.join((read_text(r) or '').lower() for r in docs)\n    sites = [\n        'main hospital',\n        'pediatric center',\n        'cancer center',\n        'rural area clinic',\n        'simulation learning center'\n    ]\n    hits = sum(1 for s in sites if s in big)\n    score = (hits / len(sites)) * 0.8\n    missing = [s for s in sites if s not in big]\n    fb = 'All tour sites found.' if not missing else ('Missing: ' + ', '.join(missing))\n    return score, fb"}, {"type": "code", "name": "Itineraries Name + Group Consistency", "description": "Verify each itinerary includes the proper name and group label: Allen\u2013Group A and Isabelle\u2013Group B.", "weight": 0.5, "code": "def evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    docs = [r for r in outputs if getattr(r, 'is_document', False)]\n    if not docs:\n        return 0.0\n\n    def read_text(res):\n        try:\n            path = context.files.get_path(res.id)\n            suf = path.suffix.lower()\n        except Exception:\n            suf = ''\n        try:\n            if suf == '.docx':\n                return context.files.read_docx_text(res.id) or ''\n            if suf == '.pdf':\n                return context.files.read_pdf_text(res.id) or ''\n        except Exception:\n            pass\n        for fn in (context.files.read_docx_text, context.files.read_pdf_text):\n            try:\n                return fn(res.id) or ''\n            except Exception:\n                continue\n        return ''\n\n    score = 0.0\n    feedback = []\n    # Identify likely itineraries and test constraints\n    for r in docs:\n        t = (read_text(r) or '').lower()\n        if 'itinerary' in t or 'personal schedule' in t:\n            if 'allen' in t and 'group a' in t:\n                score += 0.25\n            elif 'allen' in t:\n                feedback.append('Allen itinerary missing correct Group A label.')\n            if 'isabelle' in t and 'group b' in t:\n                score += 0.25\n            elif 'isabelle' in t:\n                feedback.append('Isabelle itinerary missing correct Group B label.')\n    # Cap at weight 0.5\n    score = min(score * 0.5 / 0.5, 0.5)\n    return score, '; '.join(feedback) if feedback else 'Both itineraries have correct name/group.'"}, {"type": "llm_judge", "name": "Timing Logic and Flow Compliance", "description": "Evaluate whether the schedule respects the 20-min interviews + 5-min transition (not applied to lunch/breaks), AM/PM 15-min breaks after 5 interviews, lunch 40 minutes mid-day, Group B tours first returning 10 min before lunch, Group A interviews first, Dr. Jones 8:50 break, Dr. Garrett last break to leave 20 min early.", "weight": 0.5, "judge_prompt": "Assess the correctness of timing logic across the schedule (not just structure):\n\nLook for these requirements to be satisfied in the schedule document:\n- Interviews are in 20-minute blocks with 5-minute transitions between rooms. The 5-minute buffer is NOT applied to lunch or breaks.\n- There is a 15-minute break after the first five interviews in the AM and again after the first five interviews in the PM.\n- Lunch lasts 40 minutes and occurs mid-day (not at the very beginning or very end of the day) within the 8:20 AM\u20134:40 PM window.\n- Group assignments and flows: Group A interviews first; Group B tours first; when on tours, both groups return 10 minutes before lunch; afternoon tours/interviews also include a 10-minute early return buffer.\n- Dr. Jones has a 10-minute break at 8:50 AM.\n- Dr. Garrett has the last break of the day so he can leave 20 minutes early.\n\nScoring:\n- 1.00: All conditions appear satisfied in the schedule.\n- 0.70: One minor discrepancy but overall consistent.\n- 0.40: Multiple discrepancies or unclear adherence to buffers/breaks.\n- 0.10: Only a couple of elements appear correct.\n- 0.00: Mostly non-compliant or not verifiable.\n\nUse only the schedule document for logic checks (not the itineraries). Provide brief justification.", "expectation": "Schedule implements all timing and flow constraints accurately."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3: Professional Quality and Usability", "description": "Assess presentation quality, clarity, and usefulness of the schedule and itineraries for real-world use.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Clarity", "description": "Evaluate formatting quality, readability of the schedule table, appropriateness for sharing with a CMO and applicants, visual inclusion of images/logos, and consistency across documents.", "weight": 2.0, "judge_prompt": "Evaluate overall professional quality and usability of the documents:\n- Schedule table: clear headers, legible time blocks, easy to follow per room and per group; professional formatting.\n- Itineraries: one-page each, visually balanced, include avatar image, Floor Layout image, and site logos placed clearly; timings easy to read and follow.\n- Consistency: terminology (e.g., physician/resident names), time formats (AM/PM), locations (Conference Room, Forge Auditorium) used consistently across documents.\n- Suitability: Would these be acceptable to share with the Chief Medical Officer and the applicants?\n\nScoring:\n- 1.00: Adequate quality with minor issues.\n- 1.50: Good professional quality.\n- 2.00: Excellent, polished, and highly usable.\n- 0.00: Poor formatting or difficult to use.\n\nBase the score on visual clarity and professionalism, not on correctness of timing which is covered elsewhere.", "expectation": "Polished, readable schedule and one-page itineraries with appropriate visuals and consistent formatting."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "a99d85fc-eff8-48d2-a7d4-42a75d62f18d", "rubric": {"category_name": "Real Estate Leasing Model: Annual and Monthly Rent Matrices (Dynamic Excel)", "rationale": "This rubric enforces a self-documenting, verifiable Excel deliverable for leasing scenarios. Stage 1 (LLM-only) mandates an exact workbook shape so later checks are trivial. Stage 2 mixes code and LLM to verify correctness: structure detection, bounds/logic checks, and cross-sheet total consistency. Stage 3 assesses presentation quality and usability for a professional audience.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Structured Output Format Requirement (GATE)", "description": "Gate the submission on exact Excel workbook structure so verification is possible. LLM-only, flexible with naming, but strict on presence and layout.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Workbook Shape and Required Sections", "description": "Check if the output is a single Excel workbook with the mandated sheets, sections, and tables. Be flexible with sheet/section names (e.g., 'Inputs' vs 'Assumptions'). Do NOT judge calculation correctness here\u2014only presence and structure.", "weight": 4.0, "judge_prompt": "You are evaluating whether the candidate output is a valid Excel workbook with the exact structural requirements below. Only check PRESENCE and STRUCTURE. Do not check math correctness.\n\nFormat requirements:\n- Must be an Excel file (.xlsx). Not PDF, not DOCX, not CSV.\n- Workbook must contain three logical sheets (names can vary slightly but content must match):\n  1) Inputs/Assumptions: Contains clearly labeled, light-blue editable variable cells for:\n     - Suite Number (e.g., 'Suite 330')\n     - Suite Size (in SF)\n     - Scenario parameters table with 3 rows (Scenarios 1\u20133) and these editable columns: [Scenario | Primary Term (years) | Rent/SF ($ per month) | Annual Escalator (%)]\n     - These variables should be referenced elsewhere (do not validate formulas here; just that inputs exist and are clearly designated as editable).\n  2) Annual Rent Matrix: Must contain a clear title and three distinct scenario sections (color-coded per scenario). For each scenario section:\n     - A table with columns: [Year # | Monthly Rent | $/SF | Annual Base Rent]\n     - Rows for each lease year up to the primary term (up to 10 years if applicable)\n     - A clearly labeled Total Gross Lease Value for the scenario at the bottom of the section\n     - A Notes section exists below the annual tables (at least a few lines/paragraph area)\n  3) Monthly Rent Matrix: Must contain a clear title with a displayed Total Lease Value directly below the title. For each scenario (three sections, each color-coded distinctly):\n     - A table with columns including: [Month #] and [Monthly Rent] (Year # and $/SF may also be present). Month # must be sequential starting at 1 and extend to the full term in months (e.g., up to 120 for 10 years)\n     - Rows beyond the lease term must be blank (or hidden) due to conditional logic (e.g., a 5-year lease shows up to 60 months and blanks thereafter)\n\nColor and editability requirements:\n- The three scenarios must be visually differentiated by distinct colors.\n- Cells intended as user-editable inputs must be visibly light blue.\n\nScoring guidance (STRUCTURE ONLY):\n- 4.0: Excel file present with all three sheets; each sheet contains the required sections/tables; scenario color-coding is present; editable input cells are clearly light blue; Annual sheet has Notes section; Monthly sheet shows Total below title and proper monthly breakdowns.\n- 3.0: Excel present with all three sheets and required tables, but missing one secondary element (e.g., color-coding OR editable cells not visibly light blue OR Annual Notes section missing OR Monthly top Total missing).\n- 2.0: Excel present but one required sheet or one scenario section missing, or critical table columns missing in a sheet.\n- 0.0: Not an Excel workbook, or multiple required sheets/sections missing.\n\nBe flexible with exact sheet/column titles but strict on the presence of the elements described. Do not assess formula correctness or the math; only that the structure exists to enable verification later.", "expectation": "A single .xlsx with: Inputs/Assumptions sheet (light-blue editable variables for Suite #, SF, and per-scenario parameters); Annual Rent Matrix sheet (3 scenario tables with required columns, totals at bottom, and Notes section below); Monthly Rent Matrix sheet (Total under title, per-scenario monthly tables with Month # starting at 1, appropriate lengths, and blanks beyond term). Scenarios color-coded; editable cells are light blue."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Consistency Verification (Mixed)", "description": "Now that the workbook shape is enforced, verify logic and numeric consistency. Code rules perform robust, fuzzy checks; an LLM rule cross-checks totals and monthly/annual alignment.", "is_required": false, "max_points": 4.5, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Spreadsheet and Fuzzy Sheet Discovery", "description": "Verify file is spreadsheet and identify likely Inputs, Annual, and Monthly sheets via fuzzy matching of sheet names. Partial credit if 2 of 3 found.", "weight": 0.5, "code": "import re\nimport json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    out = context.get_primary_output()\n    if not out or not out.is_spreadsheet:\n        return 0.0, 'No spreadsheet output.'\n    try:\n        path = context.files.get_path(out.id)\n        xls = pd.ExcelFile(path)\n        sheets = [s for s in xls.sheet_names]\n        found = {\n            'inputs': None,\n            'annual': None,\n            'monthly': None,\n        }\n        for s in sheets:\n            sl = s.lower()\n            if found['inputs'] is None and any(k in sl for k in ['input','assumption','variable','setup']):\n                found['inputs'] = s\n            if found['annual'] is None and any(k in sl for k in ['annual','year']):\n                found['annual'] = s\n            if found['monthly'] is None and any(k in sl for k in ['monthly','month','detail']):\n                found['monthly'] = s\n        score = sum(v is not None for v in found.values()) / 3.0\n        feedback = f\"Sheets detected: inputs={found['inputs']}, annual={found['annual']}, monthly={found['monthly']}\"\n        return max(0.0, min(1.0, score)), feedback\n    except Exception as e:\n        return 0.0, f'Error reading spreadsheet: {e}'\n"}, {"type": "code", "name": "Monthly Matrix Structure and Conditional Logic", "description": "Detect Month # column and at least 3 monthly rent columns. Validate that rows beyond the inferred lease term per scenario are blank (conditional logic working). If Inputs sheet provides terms, compare to inferred months.", "weight": 2.0, "code": "import re\nimport json\nimport pandas as pd\nimport numpy as np\n\ndef _get_sheet_by_hint(xls, path, hints):\n    for s in xls.sheet_names:\n        sl = s.lower()\n        if any(h in sl for h in hints):\n            try:\n                df = pd.read_excel(path, sheet_name=s, header=0)\n            except Exception:\n                df = pd.read_excel(path, sheet_name=s, header=None)\n            return s, df\n    # fallback: first sheet\n    s = xls.sheet_names[0]\n    df = pd.read_excel(path, sheet_name=s, header=0)\n    return s, df\n\ndef _find_month_col(df):\n    best_col = None\n    best_score = 0\n    for c in df.columns:\n        col = pd.to_numeric(df[c], errors='coerce')\n        if col.notna().sum() < 10:\n            continue\n        vals = col.dropna().astype(float).values\n        # Check if starts at 1 and mostly increasing by 1\n        ints = np.isclose(vals, np.round(vals))\n        if not ints.mean() > 0.8:\n            continue\n        # find longest increasing by 1 run\n        diffs = np.diff(vals)\n        run = (np.isclose(diffs, 1.0)).mean()\n        score = run * (len(vals) / (len(df) + 1))\n        if vals.min() <= 1.0 + 1e-6 and score > best_score:\n            best_score = score\n            best_col = c\n    return best_col\n\ndef _find_numeric_rent_cols(df, exclude_cols):\n    rent_cols = []\n    for c in df.columns:\n        if c in exclude_cols:\n            continue\n        series = pd.to_numeric(df[c], errors='coerce')\n        nonnull = series.notna().sum()\n        if nonnull >= 12 and (series.fillna(0) > 0).sum() >= 12:\n            rent_cols.append((c, nonnull))\n    # sort by non-null count descending\n    rent_cols.sort(key=lambda x: -x[1])\n    return [c for c, _ in rent_cols]\n\n\ndef evaluate(workflow, context):\n    out = context.get_primary_output()\n    if not out or not out.is_spreadsheet:\n        return 0.0, 'No spreadsheet output.'\n    try:\n        path = context.files.get_path(out.id)\n        xls = pd.ExcelFile(path)\n        # Monthly sheet\n        monthly_name, dfrm = _get_sheet_by_hint(xls, path, ['monthly','month','detail'])\n        # Try to ensure we have headers\n        if dfrm.columns.dtype == 'int64':\n            dfrm.columns = [str(c) for c in dfrm.columns]\n        # Find Month column\n        month_col = _find_month_col(dfrm)\n        if month_col is None:\n            return 0.0, f\"Could not find a Month # column in sheet '{monthly_name}'.\"\n        # Find rent columns\n        rent_cols = _find_numeric_rent_cols(dfrm, exclude_cols=[month_col])\n        if len(rent_cols) < 3:\n            # allow partial, but must have at least 2\n            if len(rent_cols) < 2:\n                return 0.2, f\"Found Month column '{month_col}' but only {len(rent_cols)} rent-like columns.\"\n        # Infer last non-null row per rent col and blanks beyond\n        mseries = pd.to_numeric(dfrm[month_col], errors='coerce')\n        max_month = int(np.nanmax(mseries.values)) if mseries.notna().any() else 0\n        # Cap analyses to 120 rows typical for 10-year\n        cap = min(120, len(dfrm))\n        def _blank_beyond_score(col):\n            s = pd.to_numeric(dfrm[col], errors='coerce')\n            # consider first cap rows\n            s_cap = s.iloc[:cap]\n            last_nonnull_idx = s_cap.last_valid_index()\n            if last_nonnull_idx is None:\n                return 0.0\n            last = int(last_nonnull_idx)\n            tail = s_cap.iloc[last+1:cap]\n            if len(tail) == 0:\n                return 1.0\n            blank_frac = tail.isna().mean() if len(tail) else 1.0\n            return float(blank_frac)\n        scores = []\n        for c in rent_cols[:3]:\n            scores.append(_blank_beyond_score(c))\n        # score composition: find month col (0.3), >=3 rent cols (0.3), blanks beyond for top 3 (0.4)\n        s_month = 1.0\n        s_rcols = min(1.0, len(rent_cols)/3.0)\n        s_blank = np.mean(scores) if scores else 0.0\n        score = 0.3*s_month + 0.3*s_rcols + 0.4*s_blank\n        feedback = f\"Monthly sheet '{monthly_name}': month_col={month_col}, rent_cols={rent_cols[:3]}, blank_beyond_scores={np.round(scores,3).tolist()}\"\n        return max(0.0, min(1.0, float(score))), feedback\n    except Exception as e:\n        return 0.0, f'Error analyzing monthly matrix: {e}'\n"}, {"type": "code", "name": "Escalation and Rent/SF Consistency", "description": "Use Inputs sheet to find SF and per-scenario parameters. Check month-13 vs month-1 rent stepped by escalator, and that month-1 rent approximates Rent/SF \u00d7 SF. Partial credit if only some scenarios found.", "weight": 1.5, "code": "import re\nimport json\nimport pandas as pd\nimport numpy as np\n\ndef _load_sheet(path, name, header=None):\n    try:\n        return pd.read_excel(path, sheet_name=name, header=header)\n    except Exception:\n        return pd.read_excel(path, sheet_name=name)\n\ndef _find_inputs_sheet(xls):\n    for s in xls.sheet_names:\n        sl = s.lower()\n        if any(k in sl for k in ['input','assumption','variable','setup']):\n            return s\n    return None\n\ndef _parse_sf(df):\n    # find a row with 'sf' or 'square feet' or 'size' and take adjacent numeric in the row\n    try:\n        txt = df.astype(str).applymap(lambda x: x.lower())\n        for i in range(len(df)):\n            row_txt = ' '.join(txt.iloc[i, :].tolist())\n            if any(k in row_txt for k in ['sf','square feet','sq ft','size']):\n                # search numeric in the row in original df\n                for v in df.iloc[i, :].values:\n                    try:\n                        num = float(str(v).replace(',',''))\n                        if num > 0:\n                            return num\n                    except Exception:\n                        continue\n        return None\n    except Exception:\n        return None\n\ndef _find_scenario_table(df):\n    # Try to find header row containing all of: scenario, term, rent, escalator\n    lower = df.astype(str).applymap(lambda x: x.lower())\n    header_idx = None\n    cols_map = {}\n    for i in range(min(20, len(df))):\n        row = lower.iloc[i, :].tolist()\n        # map columns by keyword\n        cands = {j: v for j, v in enumerate(row)}\n        has_scenario = any('scenario' in v for v in row)\n        has_term = any('term' in v or 'year' in v for v in row)\n        has_rent = any(('rent' in v and '/sf' in v) or '/sf' in v for v in row)\n        has_escal = any('escal' in v for v in row)\n        if has_scenario and has_term and has_rent and has_escal:\n            header_idx = i\n            break\n    if header_idx is None:\n        return None, None\n    header_row = lower.iloc[header_idx, :].tolist()\n    # determine column indices\n    for j, v in enumerate(header_row):\n        if 'scenario' in v and 'name' not in cols_map:\n            cols_map['scenario'] = j\n        if ('term' in v or 'year' in v) and 'term' not in cols_map:\n            cols_map['term'] = j\n        if (('rent' in v and '/sf' in v) or '/sf' in v) and 'rentsf' not in cols_map:\n            cols_map['rentsf'] = j\n        if 'escal' in v and 'escal' not in cols_map:\n            cols_map['escal'] = j\n    return header_idx, cols_map\n\ndef _extract_scenarios(df):\n    hdr_idx, cmap = _find_scenario_table(df)\n    scenarios = []\n    if hdr_idx is None or cmap is None:\n        return scenarios\n    i = hdr_idx + 1\n    # read next up to 5 rows looking for data\n    for r in range(i, min(i+6, len(df))):\n        row = df.iloc[r, :]\n        try:\n            scen = str(row[cmap['scenario']])\n        except Exception:\n            continue\n        # skip empty rows\n        if str(scen).strip() == '' or str(scen).lower() == 'nan':\n            continue\n        term = None\n        rentsf = None\n        escal = None\n        # parse term (years)\n        try:\n            term = float(str(row[cmap['term']]).replace('%','').replace(',',''))\n        except Exception:\n            pass\n        # parse rent/sf ($ per month)\n        try:\n            rentsf = float(str(row[cmap['rentsf']]).replace('$','').replace(',',''))\n        except Exception:\n            pass\n        # parse escalator %\n        try:\n            escal = float(str(row[cmap['escal']]).replace('%','').replace(',',''))/100.0\n        except Exception:\n            pass\n        scenarios.append({'label': scen, 'term_years': term, 'rentsf': rentsf, 'escal': escal})\n    # keep first 3 non-empty\n    return [s for s in scenarios if any(v is not None for v in s.values())][:3]\n\ndef _find_monthly_sheet(xls):\n    for s in xls.sheet_names:\n        if any(k in s.lower() for k in ['monthly','month','detail']):\n            return s\n    return xls.sheet_names[0]\n\n\ndef evaluate(workflow, context):\n    out = context.get_primary_output()\n    if not out or not out.is_spreadsheet:\n        return 0.0, 'No spreadsheet output.'\n    try:\n        path = context.files.get_path(out.id)\n        xls = pd.ExcelFile(path)\n        # Inputs\n        inputs_name = _find_inputs_sheet(xls)\n        df_inputs = None\n        if inputs_name:\n            df_inputs = _load_sheet(path, inputs_name, header=None)\n        sf = _parse_sf(df_inputs) if df_inputs is not None else None\n        scenarios = _extract_scenarios(df_inputs) if df_inputs is not None else []\n        # Monthly\n        monthly_name = _find_monthly_sheet(xls)\n        dfm = _load_sheet(path, monthly_name, header=0)\n        # Identify month column\n        month_col = None\n        for c in dfm.columns:\n            if 'month' in str(c).lower():\n                month_col = c\n                break\n        if month_col is None:\n            # fallback: first column\n            month_col = dfm.columns[0]\n        month_vals = pd.to_numeric(dfm[month_col], errors='coerce')\n        # Identify rent columns: numeric with many values\n        rent_cols = []\n        for c in dfm.columns:\n            if c == month_col:\n                continue\n            s = pd.to_numeric(dfm[c], errors='coerce')\n            if s.notna().sum() >= 12 and (s.fillna(0) > 0).sum() >= 12:\n                rent_cols.append(c)\n        rent_cols = rent_cols[:3]\n        if not rent_cols:\n            return 0.0, 'Could not identify monthly rent columns.'\n        # Build expected mapping by term (if available)\n        term_to_idx = {}\n        if scenarios and any(s.get('term_years') for s in scenarios):\n            exp_months = [int(s.get('term_years')*12) if s.get('term_years') else None for s in scenarios]\n            # infer observed non-null counts per rent col\n            obs_counts = []\n            for c in rent_cols:\n                s = pd.to_numeric(dfm[c], errors='coerce')\n                obs_counts.append(int(s.notna().sum()))\n            # map by nearest count\n            mapping = {}\n            used = set()\n            for i, em in enumerate(exp_months):\n                if em is None:\n                    continue\n                diffs = [(j, abs(obs_counts[j]-em)) for j in range(len(rent_cols)) if j not in used]\n                if diffs:\n                    j_best = sorted(diffs, key=lambda x: x[1])[0][0]\n                    mapping[i] = j_best\n                    used.add(j_best)\n        # Evaluate escalation and rent/sf\n        checks = []\n        feedback_parts = []\n        for idx, c in enumerate(rent_cols):\n            s = pd.to_numeric(dfm[c], errors='coerce')\n            s = s[s.notna()]\n            if len(s) < 13:\n                continue\n            m1 = float(s.iloc[0])\n            m13 = float(s.iloc[12])\n            # expected escalator\n            exp_escal = None\n            exp_rsf = None\n            if scenarios and idx < len(scenarios):\n                # try mapped by term first\n                if any(isinstance(k,int) for k in ([] if 'mapping' not in locals() else list(mapping.keys()))):\n                    # find if this rent col is mapped from any scenario\n                    rev = {v:k for k,v in mapping.items()} if 'mapping' in locals() else {}\n                    if idx in rev:\n                        sc = scenarios[rev[idx]]\n                        exp_escal = sc.get('escal')\n                        exp_rsf = sc.get('rentsf')\n                if exp_escal is None:\n                    sc = scenarios[idx]\n                    exp_escal = sc.get('escal')\n                    exp_rsf = sc.get('rentsf')\n            # Check escalation around anniversary\n            esc_ok = None\n            if exp_escal is not None:\n                try:\n                    ratio = (m13 / m1) - 1.0\n                    esc_ok = (abs(ratio - exp_escal) <= 0.01)\n                except Exception:\n                    esc_ok = False\n            # Check rent per SF for first month\n            rsf_ok = None\n            if sf is not None and exp_rsf is not None:\n                try:\n                    calc_rsf = m1 / float(sf)\n                    rsf_ok = (abs(calc_rsf - float(exp_rsf)) <= max(0.05, 0.02*float(exp_rsf)))\n                except Exception:\n                    rsf_ok = False\n            checks.append((esc_ok, rsf_ok))\n            feedback_parts.append(f\"col='{c}': esc_ok={esc_ok}, rsf_ok={rsf_ok}\")\n        # scoring: average of True checks across available\n        flat = []\n        for esc_ok, rsf_ok in checks:\n            if esc_ok is not None:\n                flat.append(1.0 if esc_ok else 0.0)\n            if rsf_ok is not None:\n                flat.append(1.0 if rsf_ok else 0.0)\n        score = np.mean(flat) if flat else 0.0\n        feedback = ' | '.join(feedback_parts) if feedback_parts else 'No verifiable checks (missing inputs or rents).'\n        return float(max(0.0, min(1.0, score))), feedback\n    except Exception as e:\n        return 0.0, f'Error verifying escalation and rent/sf: {e}'\n"}, {"type": "llm_judge", "name": "Cross-Sheet Totals and Anniversary Behavior", "description": "Visually confirm that Monthly Matrix totals equal Annual Matrix totals for each scenario, that Monthly Matrix shows the total directly under the title, and that rent increases occur on anniversary months only. Confirm blanks beyond term for shorter terms.", "weight": 0.5, "judge_prompt": "Open the Excel workbook. Evaluate the following (do not recompute in detail\u2014visually inspect and reason with the visible numbers):\n\n1) On the Monthly Rent Matrix sheet:\n   - Is there a clearly displayed Total Lease Value directly under the title? (Yes/No)\n   - For each scenario (1\u20133), does the monthly rent timeline show step increases on the yearly anniversaries only (e.g., months 13, 25, 37, etc.)? (Minor rounding is okay.)\n   - For 3-year and 5-year scenarios, are rows beyond 36 and 60 months respectively blank (or hidden)? For the 10-year scenario, does it reach 120 months?\n\n2) Cross-sheet consistency:\n   - For each scenario, does the Monthly Matrix total match the corresponding Total Gross Lease Value on the Annual Rent Matrix for that scenario?\n\nScoring:\n- 1.0: Monthly total shown under title; all scenarios exhibit anniversary-only steps; blanks beyond term are handled; monthly and annual totals match across all scenarios.\n- 0.6: One minor inconsistency (e.g., missing monthly top total OR one scenario has a slight mismatch/formatting issue), but overall consistent.\n- 0.3: Multiple inconsistencies, but at least one scenario matches and structure is mostly coherent.\n- 0.0: Monthly totals missing and/or totals do not match across scenarios, or clear anniversary behavior not visible.\n\nBe strict but fair; consider small rounding differences acceptable.", "expectation": "Monthly total shown under title; monthly-by-month patterns step at anniversary months; blanks beyond term for shorter scenarios; monthly totals equal annual totals for each scenario."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Presentation and Usability", "description": "Holistic quality assessment of clarity, usability, and professionalism for a leasing prospect and internal stakeholders.", "is_required": false, "max_points": 1.5, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Presentation Quality and User Experience", "description": "Assess whether the model is polished, readable, and easy for a leasing prospect to use. Verify color-coding, clear instructions, and consistent formatting.", "weight": 1.5, "judge_prompt": "Evaluate the Excel workbook\u2019s professional quality and usability for a property-leasing context:\n\nConsider:\n- Readability and organization: clear titles, section headers, and consistent fonts; currency and percentage formats correctly applied.\n- Color usage: three scenarios are clearly color-differentiated; editable variable cells are light blue and easy to identify.\n- Guidance: brief instructions or notes on how to use the model (where to edit Suite #, SF, and scenario parameters) and any modeling assumptions.\n- Clarity of matrices: Annual and Monthly matrices are easy to follow; columns are labeled; units ($, $/SF, %) are explicit; totals are highlighted.\n- Professional polish: alignment, spacing, borders, print/layout readiness.\n\nScoring:\n- 1.5: Highly professional; very clear guidance; excellent formatting and usability.\n- 1.0: Generally professional with minor issues (e.g., minor formatting inconsistencies) but easy to use.\n- 0.5: Adequate but somewhat rough; requires effort to interpret.\n- 0.0: Poorly formatted, confusing, or missing basic guidance.", "expectation": "A polished, color-coded, and well-labeled workbook with light-blue input cells, clear instructions/notes, and consistent currency/percent formats suitable for sharing with a leasing prospect."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "93b336f3-61f3-4287-86d2-87445e1e0f90", "rubric": {"category_name": "EV Battery Pack Localisation Partnership Proposal (CPO-Ready)", "rationale": "Mixed task: a 2\u20133 page executive document (DOCX/PDF) that embeds verifiable cost calculations in INR and a PMP-aligned localisation roadmap. Stage 1 uses an LLM gate to enforce exact document structure so later checks are trivial. Stage 2 mixes code rules for numeric correctness and factual inclusions with an LLM check for PMP roadmap alignment. Stage 3 assesses professional quality and CPO actionability.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (Document Structure)", "description": "LLM-only gate that verifies the output is a 2\u20133 page DOCX/PDF with exact sections and calculational artifacts enabling verification. No correctness checks here\u2014only presence and structure.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Document Format and Required Structure", "description": "Check the candidate produced a properly structured DOCX/PDF with all required sections and calculational tables enabling verification.", "weight": 4.0, "judge_prompt": "You are validating the SHAPE ONLY of the candidate\u2019s output. Do not judge correctness of numbers or quality of writing.\n\nAcceptable formats: DOCX or PDF only. Target length: 2\u20133 pages (be flexible: accept 2\u20134 pages with minor penalty). The document must be clearly formatted with headers and tables where specified.\n\nRequired sections and artifacts (flexible with near-equivalent titles):\n1) Executive Summary (must appear on page 1)\n   - Summarizes partnership and highlights cost-savings from local assembly in INR.\n2) Partnership Structure\n   - Must state 49:51 ownership split (EvTronics:EV Batteries Inc.)\n   - EV Batteries Inc. retains technical oversight; EvTronics leads assembly and local operations from Delhi.\n3) Sourcing Model\n   - Describes EV Batteries Inc. supplying child parts (cells, housing, thermal systems, BMS, connectors) to EvTronics; EvTronics assembles locally and supplies packs to the plant.\n4) Cost and Savings Analysis (INR) \u2014 must include:\n   - Assumptions list containing USD=83 INR and annual volume 110K units, 5-year horizon, and that all components except assembly and overhead remain unchanged.\n   - A per-unit comparison table labeled clearly (or equivalent) with columns like: [Cost Element | Baseline (Imported) INR | Local Assembly INR | Delta]. Rows include at minimum: Assembly, Overhead, Other Components (unchanged), Total Unit Cost, Per-Unit Savings.\n   - A multi-year summary table (or clear bullet list) showing: annual volume = 110,000 units, years = 5, total units = 550,000, and total 5-year savings in INR.\n5) Localisation Roadmap (PMP-aligned)\n   - Shows phased timeline referencing PMP phases. Minimum: immediate focus on assembly (Phase 1/2), plus later phases covering motors/power electronics/BMS/thermal and eventual cells/semiconductors.\n6) Benefits and Risks\n   - Benefits: regulatory compliance (FAME II/PMP), reduced forex exposure, long-term cost reductions, supply chain resilience.\n   - Risks: dependency on imported cells, coordination complexity, initial capex.\n7) Recommendations / Next Steps\n   - Clear, actionable next steps for CPO decision-making.\n\nScoring (STRUCTURE ONLY):\n- 4.0: Valid DOCX/PDF; 2\u20133 pages; ALL 7 sections present; both required cost tables/figures present; assumptions listed.\n- 3.5: Valid DOCX/PDF; 2\u20134 pages; 6/7 sections present OR minor omissions in tables (e.g., missing one row like Other Components but includes totals and savings).\n- 3.0: Valid DOCX/PDF; 2\u20134 pages; 5/7 sections present and at least one cost table present with totals and savings labeled.\n- 2.0: Valid DOCX/PDF, but only 3\u20134 sections present OR cost analysis lacks tables and is only narrative.\n- 1.0: Valid DOCX/PDF but only 1\u20132 sections present with no usable cost structure.\n- 0.0: Not a DOCX/PDF OR obviously <2 pages of substantive content.\n\nOnly evaluate presence/format. Do NOT check math correctness or writing quality.", "expectation": "A 2\u20133 page DOCX/PDF with all required sections, per-unit and multi-year INR cost tables, and PMP-aligned roadmap headings clearly visible."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Numeric and Factual Consistency)", "description": "Now that the structure is enforced, verify correctness and key factual inclusions. Combine code rules for numeric checks with an LLM check for PMP roadmap alignment and consistency.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "INR Calculations and Currency Assumption Presence", "description": "Checks the document text mentions INR calculations prominently and states or implies USD=83 INR conversion.", "weight": 0.6, "code": "import re\n\ndef _read_doc_text(context, output):\n    text = ''\n    # Try DOCX first\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        text = ''\n    if not text:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = ''\n    if not text:\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            text = ''\n    return text or ''\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = _read_doc_text(context, output)\n    if not text:\n        return 0.0\n    low = text.lower()\n\n    # Count INR and USD mentions\n    inr_count = len(re.findall(r'\\bINR\\b', text)) + len(re.findall(r'\\b\u20b9', text))\n    usd_count = len(re.findall(r'\\bUSD\\b', text)) + len(re.findall(r'\\$', text))\n\n    # Look for conversion statements like USD=83 INR or 1 USD = 83 INR\n    conv_patterns = [\n        r'usd\\s*[=:\\\\-]\\s*83\\s*inr',\n        r'1\\s*usd\\s*[=:]\\s*83\\s*inr',\n        r'83\\s*inr\\s*/\\s*usd',\n        r'usd\\s*to\\s*inr\\s*[:=]?\\s*83',\n        r'\\binr\\s*83\\s*per\\s*usd\\b'\n    ]\n    has_conv = any(re.search(p, low) for p in conv_patterns) or ('usd=83 inr' in low) or ('usd = 83 inr' in low)\n\n    score = 0.0\n    # Weighting within this rule: 0.35 for INR emphasis, 0.65 for explicit conversion mention\n    if inr_count >= max(3, usd_count):\n        score += 0.35\n    if has_conv:\n        score += 0.65\n\n    return min(1.0, score)\n"}, {"type": "code", "name": "Savings Math Consistency (Assembly/Overhead Delta)", "description": "Verifies key numeric facts are present and consistent with assumptions: per-unit savings (~103,910 INR), 5-year savings (~57.15B INR), total units (550,000), and localized unit total (~726,090 INR). Partial credit for partial matches.", "weight": 1.6, "code": "import re\n\nEXPECTED = {\n    'assembly_usd_inr': 1300 * 83,      # 107,900\n    'overhead_usd_inr': 200 * 83,       # 16,600\n    'localized_assembly': 20000,\n    'localized_overhead': 590,\n}\nEXPECTED['per_unit_savings'] = (EXPECTED['assembly_usd_inr'] + EXPECTED['overhead_usd_inr']) - (EXPECTED['localized_assembly'] + EXPECTED['localized_overhead'])  # 103,910\nEXPECTED['localized_total_unit'] = 830000 - (EXPECTED['assembly_usd_inr'] + EXPECTED['overhead_usd_inr']) + (EXPECTED['localized_assembly'] + EXPECTED['localized_overhead'])  # 726,090\nEXPECTED['annual_volume'] = 110000\nEXPECTED['total_units'] = EXPECTED['annual_volume'] * 5  # 550,000\nEXPECTED['five_year_savings'] = EXPECTED['per_unit_savings'] * EXPECTED['total_units']  # 57,150,500,000\n\n\ndef _read_doc_text(context, output):\n    text = ''\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        text = ''\n    if not text:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = ''\n    if not text:\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            text = ''\n    return text or ''\n\n\ndef _extract_numbers(text):\n    # Grab integers with commas and plain integers\n    nums = []\n    for m in re.findall(r'(?<![A-Za-z0-9])([0-9]{1,3}(?:,[0-9]{3})+|[0-9]{4,})', text):\n        try:\n            nums.append(float(m.replace(',', '')))\n        except Exception:\n            pass\n    # Also capture smaller numbers (like 590)\n    for m in re.findall(r'(?<![A-Za-z0-9\\.])([0-9]{1,3})(?![A-Za-z0-9])', text):\n        try:\n            nums.append(float(m))\n        except Exception:\n            pass\n    return nums\n\n\ndef _has_close(target, numbers, rel_tol=0.02, abs_tol=1000.0):\n    for n in numbers:\n        if abs(n - target) <= max(abs(target) * rel_tol, abs_tol):\n            return True\n    return False\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = _read_doc_text(context, output)\n    if not text:\n        return 0.0\n    numbers = _extract_numbers(text)\n\n    score = 0.0\n    # Scoring breakdown within this rule (sum to 1.0):\n    # 0.50 per-unit savings, 0.30 five-year savings, 0.10 total units, 0.10 localized unit total\n    if _has_close(EXPECTED['per_unit_savings'], numbers, rel_tol=0.02, abs_tol=2000):\n        score += 0.50\n    if _has_close(EXPECTED['five_year_savings'], numbers, rel_tol=0.02, abs_tol=2_000_000):\n        score += 0.30\n    if _has_close(EXPECTED['total_units'], numbers, rel_tol=0.0, abs_tol=100):\n        score += 0.10\n    if _has_close(EXPECTED['localized_total_unit'], numbers, rel_tol=0.02, abs_tol=2000):\n        score += 0.10\n\n    return min(1.0, score)\n"}, {"type": "code", "name": "Volume and Timeframe Mention", "description": "Checks that the document explicitly mentions annual volume (110K or 110,000) and a 5-year horizon for calculations.", "weight": 0.5, "code": "import re\n\ndef _read_doc_text(context, output):\n    text = ''\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        text = ''\n    if not text:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = ''\n    if not text:\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            text = ''\n    return text or ''\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = _read_doc_text(context, output)\n    low = text.lower()\n\n    vol_patterns = [r'110\\s*[, ]?000', r'110k', r'110\\s*k']\n    has_volume = any(re.search(p, low) for p in vol_patterns)\n\n    time_patterns = [r'5\\s*-?\\s*year', r'five\\s*year']\n    has_time = any(re.search(p, low) for p in time_patterns)\n\n    if has_volume and has_time:\n        return 1.0\n    if has_volume or has_time:\n        return 0.5\n    return 0.0\n"}, {"type": "code", "name": "Ownership Split and Governance Details", "description": "Detects the 49:51 ownership split (EvTronics:EV Batteries Inc.), technical oversight by EV Batteries Inc., EvTronics leading assembly/local ops, and Delhi location mention.", "weight": 0.8, "code": "import re\n\ndef _read_doc_text(context, output):\n    text = ''\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        text = ''\n    if not text:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = ''\n    if not text:\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            text = ''\n    return text or ''\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = _read_doc_text(context, output)\n    low = text.lower()\n\n    # Ownership split patterns\n    split_patterns = [r'49\\s*[:/\\-]\\s*51', r'49\\s*%\\s*(?:-|\\/|and|&)\\s*51\\s*%', r'49\\s*to\\s*51']\n    has_split = any(re.search(p, low) for p in split_patterns)\n\n    has_evtronics = 'evtronics' in low\n    has_evbatt = 'ev batteries inc' in low or 'ev batteries' in low\n\n    # Governance keywords\n    has_tech_oversight = ('technical oversight' in low) or ('technology oversight' in low) or ('technical lead' in low and 'ev batteries' in low)\n    has_evtronics_leads_assembly = ('evtronics' in low and ('leads assembly' in low or 'assembly lead' in low or 'lead assembly' in low or 'local operations' in low))\n    has_delhi = 'delhi' in low\n\n    score = 0.0\n    if has_split and has_evtronics and has_evbatt:\n        score += 0.4\n    if has_tech_oversight:\n        score += 0.2\n    if has_evtronics_leads_assembly:\n        score += 0.2\n    if has_delhi:\n        score += 0.2\n\n    return min(1.0, score)\n"}, {"type": "llm_judge", "name": "PMP Roadmap Alignment and Consistency", "description": "LLM checks that the localisation timeline aligns with PMP phases and is coherent with the immediate assembly-only scope, including later phases for deeper localisation.", "weight": 0.5, "judge_prompt": "Evaluate the document for correctness of the localisation roadmap against India\u2019s PMP phases. You are checking conceptual alignment, not prose style.\n\nRequired to award full credit:\n- Immediate focus on local assembly of battery packs consistent with Phase 1/2.\n- Medium-term phases (Years 3\u20135) show localisation of motors, VCUs/on-board chargers, and progression toward BMS/thermal/power electronics.\n- Longer-term (Years 5\u20139+) references deeper localisation and eventual cell/semiconductor localisation (Phase 4).\n- The roadmap should be coherent with the stated annual volumes and make sense operationally with the proposed partnership (EV Batteries Inc. supplying child parts initially, EvTronics assembling locally).\n\nScoring:\n- 1.0: Roadmap clearly maps to PMP phases, includes immediate assembly-only scope and subsequent phases with correct components; coherent with volumes and partnership structure.\n- 0.5: Mostly aligned but missing one element (e.g., omits VCUs or power electronics), or vague time bounds.\n- 0.0: Misaligned (e.g., jumps to cell localisation immediately) or roadmap absent.", "expectation": "A phased, PMP-aligned timeline: immediate assembly, then motors/VCU/chargers, then power electronics/BMS/thermal, then cells/semiconductors; coherent with the partnership and volume assumptions."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and CPO Readiness", "description": "Professionalism, clarity, and actionability for an executive audience.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Executive Communication Quality", "description": "Assesses clarity, structure, and professionalism for a CPO audience.", "weight": 1.5, "judge_prompt": "Assess the document\u2019s executive readability and professionalism for a CPO:\n- Clear, concise executive summary with headline savings and decision context\n- Logical flow with informative headings, bullet lists where appropriate, and readable tables\n- Proper use of units/currency (INR), consistent terminology, and minimal jargon\n- Visual cleanliness (spacing, alignment). Minor typos acceptable but should not impede comprehension\n\nScoring:\n- 1.5: Highly professional, clear, and well-structured; tables are easy to read\n- 1.0: Generally clear with minor issues (overlong sentences, small formatting inconsistencies)\n- 0.5: Understandable but cluttered or hard to scan; tables/labels confusing\n- 0.0: Disorganized and difficult for an executive to parse", "expectation": "A polished, executive-friendly proposal with clear headings, readable tables, and consistent INR presentation."}, {"type": "llm_judge", "name": "Actionability and Next-Step Readiness", "description": "Assesses whether the proposal gives the CPO immediately actionable next steps with owners and timelines.", "weight": 0.5, "judge_prompt": "Does the proposal end with concrete, prioritized next steps suitable for a CPO? Look for items like: partner MoU/term sheet drafting, pilot build with EvTronics, quality/PPAP plan, capex and facility readiness, commercial model/Incoterms, compliance filings (FAME II/PMP), governance cadence, and risk mitigations. Credit if owners and rough timelines/milestones are suggested.\n\nScoring:\n- 0.5: Clear, prioritized, and time-bound next steps with ownership indicated\n- 0.25: Next steps present but vague (no owners/timelines)\n- 0.0: No actionable next steps", "expectation": "A concise checklist of next steps with owners/timelines that enables an immediate CPO decision."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "f841ddcf-2a28-4f6d-bac3-61b607219d3e", "rubric": {"category_name": "Wholesale Trade - Order Clerks: June Shipment Recap (Excel)", "rationale": "Task Type: Analytical (Pattern A). Output: Excel workbook. The rubric enforces a self-documenting Excel shape that enables code verification of key calculations (percent shipped and short-shipped consistency) and presence of a slipped-orders summary. Stage 1 uses an LLM judge to confirm exact workbook structure and narrative presence. Stage 2 uses code rules to verify columns, internal math consistency, evidence of slipped-orders summary, and June 2025 date window references. Stage 3 uses an LLM judge to assess professional quality and usability (filters, clarity, totals, brief narrative).", "max_total_score": 10.0, "stages": [{"name": "Stage 1 GATE \u2014 Structured Excel Output Requirement", "description": "LLM-only gate that enforces the exact, verifiable Excel structure needed for downstream checks.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 2.8, "rules": [{"type": "llm_judge", "name": "Workbook Structure, Sections, and Tables", "description": "Check the candidate Excel workbook for the required sheets, sections, and tables to enable verification.", "weight": 4.0, "judge_prompt": "You are evaluating whether the delivered file is a properly structured Excel workbook for a June 2025 shipment recap. Only check presence/structure, not correctness of numbers.\n\nRequired format: Excel (.xlsx). Not PDF/DOCX/CSV.\n\nWorkbook requirements (be flexible with exact naming but strict on structure and semantics):\n\n1) Sheet for June shipment summary (acceptable names include: \"June Shipment Summary\", \"June Shipped Summary\", \"June 2025 Summary\", or similar). It must contain a clearly labeled summary table that is filterable (Excel Table with header filters) by Account. The table must include these columns (names can vary but meanings must match):\n   - Account (e.g., Account, Account Name, Customer)\n   - June Shipped $ at cost (e.g., Total Shipped at Cost, Actual Shipped $)\n   - % of Order Shipped (e.g., % Shipped, Percent of Order Shipped)\n   - Short-Shipped $ (e.g., Short Shipped $, Shortfall $)\n   This table should include a Grand Total row (sum) for Shipped $ and Short-Shipped $.\n\n2) Sheet for June POs that slipped to July (acceptable names include: \"June Slipped to July\", \"June Expected vs July Actual\", \"June->July Slipped Orders\"). It must include:\n   - A detail section/table listing the specific POs that had a June ship window (start ship date and cancel date within 6/1/25\u20136/30/25) but actually shipped in July, with columns: PO Number, Account, Start Ship Date, Cancel Date, Actual Ship Date, PO Value at Cost, Actual Shipped Value at Cost.\n   - A summary table by Account that quantifies the value at cost of those slipped orders (sum) and a Grand Total.\n   The summary table must be filterable by Account (Excel Table with header filters).\n\n3) Narrative text (2\u20135 sentences) somewhere in the workbook (e.g., on a Notes/Readme section or near the summary) that states: the June total shipped dollar value at cost and the impact of POs that were expected in June but slipped to July (mentioning the slipped value in dollars). The June fiscal window (6/1/25\u20136/30/25) should be referenced explicitly in text or headings.\n\nScoring (structure only):\n- 4.0: Excel format and all three elements present (June summary table with required columns and totals; slipped-to-July detail + summary tables; brief narrative mentioning June totals and impact with the June date window referenced). Both summary tables are filterable by Account.\n- 3.0: Excel format and two of the three elements present (e.g., missing narrative or missing slipped detail but has slipped summary), with both required columns present in the June summary. Tables are filterable by Account.\n- 2.0: Excel format and only one major element fully present (e.g., the June summary table) with filterable Account, or multiple elements present but clearly missing critical required columns/filters.\n- 0.0: Not an Excel workbook OR missing multiple required elements (e.g., no June summary table or no slipped-to-July content) OR not filterable by Account.", "expectation": "A cleanly structured .xlsx with: (1) a filterable June shipment summary table with Account, Shipped $, % Shipped, Short-Shipped $ and total row; (2) a filterable slipped-to-July summary (plus detail) with totals; (3) a brief narrative noting June total shipped and impact of slipped orders, referencing the June 2025 window."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Consistency Checks", "description": "Code-based verification of columns, internal math consistency, slipped summary presence, and June date window references.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "June Summary Table Columns Present", "description": "Find a June shipment summary-like sheet and verify the presence of Account, Shipped $, % Shipped, and Short-Shipped $ columns (flexible naming).", "weight": 1.2, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        candidate = None\n        feedback_bits = []\n\n        # Column synonym sets\n        acct_syns = [\"account\", \"account name\", \"customer\", \"customer name\", \"acct\"]\n        shipped_syns = [\"shipped\", \"total shipped\", \"actual shipped\", \"shipped at cost\", \"actual shipped value\", \"actual shipped value at cost\", \"total shipped at cost\", \"june shipped\"]\n        pct_syns = [\"% shipped\", \"percent shipped\", \"% of order shipped\", \"pct shipped\", \"ship %\", \"ship pct\", \"percent of order shipped\"]\n        short_syns = [\"short\", \"short-shipped\", \"short shipped\", \"shortfall\", \"unshipped\", \"under shipped\", \"unfulfilled\", \"backorder\", \"bo $\"]\n\n        def norm(s):\n            s = str(s).strip().lower()\n            s = re.sub(r\"[^a-z0-9%$ ]+\", \"\", s)\n            s = re.sub(r\"\\s+\", \" \", s)\n            return s\n\n        def has_any(colname, syns):\n            c = norm(colname)\n            return any(k in c for k in syns)\n\n        found_cols = None\n        found_sheet = None\n        for sheet in xls.sheet_names:\n            try:\n                df = pd.read_excel(path, sheet_name=sheet)\n            except Exception:\n                continue\n            if df is None or df.empty:\n                continue\n            cols = [c for c in df.columns if not str(c).startswith(\"Unnamed\")] or list(df.columns)\n            ncols = [norm(c) for c in cols]\n            # Identify best matches per type\n            acct_cols = [cols[i] for i,c in enumerate(ncols) if has_any(c, acct_syns)]\n            ship_cols = [cols[i] for i,c in enumerate(ncols) if has_any(c, shipped_syns)]\n            pct_cols  = [cols[i] for i,c in enumerate(ncols) if has_any(c, pct_syns)]\n            short_cols= [cols[i] for i,c in enumerate(ncols) if has_any(c, short_syns)]\n            if acct_cols and ship_cols and pct_cols and short_cols:\n                candidate = (sheet, df, acct_cols[0], ship_cols[0], pct_cols[0], short_cols[0])\n                found_cols = {\n                    \"account\": acct_cols[0],\n                    \"shipped\": ship_cols[0],\n                    \"percent\": pct_cols[0],\n                    \"short\": short_cols[0]\n                }\n                found_sheet = sheet\n                break\n        if candidate is None:\n            return 0.0, \"Did not find a June summary table with Account, Shipped, % Shipped, and Short-Shipped columns.\"\n        fb = f\"Found summary on sheet '{found_sheet}' with columns: {found_cols}.\"\n        return 1.2, fb\n    except Exception as e:\n        return 0.0, f\"Error scanning workbook: {e}\""}, {"type": "code", "name": "Percent and Short-Dollar Consistency", "description": "Check that % Shipped approximately equals Shipped / (Shipped + Short) across rows (tolerance 3 percentage points).", "weight": 1.6, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n\n        acct_syns = [\"account\", \"account name\", \"customer\", \"customer name\", \"acct\"]\n        shipped_syns = [\"shipped\", \"total shipped\", \"actual shipped\", \"shipped at cost\", \"actual shipped value\", \"actual shipped value at cost\", \"total shipped at cost\", \"june shipped\"]\n        pct_syns = [\"% shipped\", \"percent shipped\", \"% of order shipped\", \"pct shipped\", \"ship %\", \"ship pct\", \"percent of order shipped\"]\n        short_syns = [\"short\", \"short-shipped\", \"short shipped\", \"shortfall\", \"unshipped\", \"under shipped\", \"unfulfilled\", \"backorder\", \"bo $\"]\n\n        def norm(s):\n            s = str(s).strip().lower()\n            s = re.sub(r\"[^a-z0-9%$ ]+\", \"\", s)\n            s = re.sub(r\"\\s+\", \" \", s)\n            return s\n\n        def has_any(colname, syns):\n            c = norm(colname)\n            return any(k in c for k in syns)\n\n        # Locate the best candidate sheet/df\n        target = None\n        for sheet in xls.sheet_names:\n            try:\n                df = pd.read_excel(path, sheet_name=sheet)\n            except Exception:\n                continue\n            if df is None or df.empty:\n                continue\n            cols = [c for c in df.columns if not str(c).startswith(\"Unnamed\")] or list(df.columns)\n            ncols = [norm(c) for c in cols]\n            acct_cols = [cols[i] for i,c in enumerate(ncols) if has_any(c, acct_syns)]\n            ship_cols = [cols[i] for i,c in enumerate(ncols) if has_any(c, shipped_syns)]\n            pct_cols  = [cols[i] for i,c in enumerate(ncols) if has_any(c, pct_syns)]\n            short_cols= [cols[i] for i,c in enumerate(ncols) if has_any(c, short_syns)]\n            if acct_cols and ship_cols and pct_cols and short_cols:\n                target = (sheet, df, acct_cols[0], ship_cols[0], pct_cols[0], short_cols[0])\n                break\n        if target is None:\n            return 0.0, \"No suitable summary table with required columns.\"\n        sheet, df, acct_col, ship_col, pct_col, short_col = target\n\n        # Coerce to numeric\n        ship = pd.to_numeric(df[ship_col], errors='coerce')\n        short = pd.to_numeric(df[short_col], errors='coerce')\n        pct = pd.to_numeric(df[pct_col], errors='coerce')\n\n        # Handle % scale detection: if median > 1.5 assume 0-100\n        pct_clean = pct.copy()\n        med = np.nanmedian(pct_clean.values) if np.isfinite(np.nanmedian(pct_clean.values)) else np.nan\n        if pd.notna(med) and med > 1.5:\n            pct_clean = pct_clean / 100.0\n\n        denom = ship + short\n        valid = (denom > 0) & ship.notna() & short.notna() & pct_clean.notna()\n        if valid.sum() == 0:\n            return 0.0, f\"No valid rows to test consistency on sheet '{sheet}'.\"\n        pct_calc = ship[valid] / denom[valid]\n        pct_given = pct_clean[valid]\n        diff = (pct_calc - pct_given).abs()\n        consistent = (diff <= 0.03)  # 3 percentage points\n        ratio = consistent.mean()\n        score = float(ratio) * 1.6\n        fb = f\"Consistency on '{sheet}': {ratio:.0%} rows within 3pp tolerance.\"\n        return score, fb\n    except Exception as e:\n        return 0.0, f\"Error checking consistency: {e}\""}, {"type": "code", "name": "Slipped-to-July Summary Presence", "description": "Detect a slipped-orders summary table by Account with a numeric value-at-cost total (flexible naming); ensure at least one positive value.", "weight": 1.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n\n        acct_syns = [\"account\", \"account name\", \"customer\", \"customer name\", \"acct\"]\n        # Columns indicating slipped value; look for 'slip' or 'july' context or generic value columns in a slipped sheet\n        value_syns = [\"slip\", \"slipped\", \"july\", \"value\", \"at cost\", \"cost\", \"po value\", \"shipped value\"]\n\n        def norm(s):\n            s = str(s).strip().lower()\n            s = re.sub(r\"[^a-z0-9%$ ]+\", \"\", s)\n            s = re.sub(r\"\\s+\", \" \", s)\n            return s\n\n        found = False\n        found_sheet = None\n        found_cols = None\n\n        for sheet in xls.sheet_names:\n            sname = norm(sheet)\n            sheet_is_slip = any(k in sname for k in [\"slip\", \"july\", \"expected\", \"vs\", \"impact\"]) or (\"june\" in sname and (\"july\" in sname or \"slip\" in sname))\n            try:\n                df = pd.read_excel(path, sheet_name=sheet)\n            except Exception:\n                continue\n            if df is None or df.empty:\n                continue\n            cols = [c for c in df.columns if not str(c).startswith(\"Unnamed\")] or list(df.columns)\n            ncols = [norm(c) for c in cols]\n            acct_cols = [cols[i] for i,c in enumerate(ncols) if any(a in c for a in acct_syns)]\n            val_cols  = [cols[i] for i,c in enumerate(ncols) if any(v in c for v in value_syns)]\n            if acct_cols and val_cols:\n                # Check numeric and positive presence\n                for vc in val_cols:\n                    ser = pd.to_numeric(df[vc], errors='coerce')\n                    if np.nanmax(ser.values) if ser.notna().any() else np.nan > 0:\n                        found = True\n                        found_sheet = sheet\n                        found_cols = {\"account\": acct_cols[0], \"value\": vc}\n                        break\n            if found:\n                break\n        if not found:\n            return 0.0, \"No slipped-to-July summary by Account with positive value detected.\"\n        return 1.0, f\"Found slipped summary on '{found_sheet}' with columns {found_cols}.\"\n    except Exception as e:\n        return 0.0, f\"Error scanning slipped summary: {e}\""}, {"type": "code", "name": "June 2025 Window Referenced", "description": "Scan workbook text for references to the June 2025 window (e.g., '6/1/25', '6/30/25', 'June 2025').", "weight": 0.2, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        patterns = [r\"\\b6/1/25\\b\", r\"\\b6/30/25\\b\", r\"june\\s*2025\", r\"\\b6/2025\\b\"]\n        found = False\n        for sheet in xls.sheet_names:\n            try:\n                df = pd.read_excel(path, sheet_name=sheet, dtype=str)\n            except Exception:\n                continue\n            if df is None or df.empty:\n                continue\n            for col in df.columns:\n                ser = df[col].astype(str)\n                text = \" \\n \".join(ser.tolist()).lower()\n                if any(re.search(p, text) for p in patterns):\n                    found = True\n                    break\n            if found:\n                break\n        return (0.2 if found else 0.0), (\"Found June 2025 window reference.\" if found else \"No explicit June 2025 window reference found in sheet text.\")\n    except Exception as e:\n        return 0.0, f\"Error scanning dates: {e}\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Presentation and Usability", "description": "LLM quality assessment of organization, readability, filterability, and clarity of the brief narrative.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Quality and Usability", "description": "Assess whether the workbook is simple, organized, filterable, and includes a clear brief narrative that references the correct totals.", "weight": 2.0, "judge_prompt": "Evaluate the professional quality and usability of the Excel workbook. Only assess presentation and clarity, not recalculation accuracy. Consider:\n\n1) Organization and clarity:\n   - Are there clear sheet titles and section headers?\n   - Are the two required summary tables easy to find and read?\n   - Are totals (Grand Totals) clearly labeled and formatted?\n\n2) Filterability and usability:\n   - Are the summary tables formatted as Excel tables with header filters so the user can filter by Account?\n   - Are column headers concise and unambiguous (Shipped $, % Shipped, Short-Shipped $)?\n   - Are number formats appropriate (currency for dollars, percentage for % shipped)?\n\n3) Brief narrative:\n   - Is there a 2\u20135 sentence narrative stating the June total shipped at cost and describing the impact of POs that slipped to July?\n   - Does the narrative reference the same totals shown in the tables (at least consistent on face value)?\n\nScoring:\n- 2.0: Clean, well-organized workbook; both tables filterable by Account; readable headers; appropriate currency/percent formats; clear narrative that references the June total shipped and slipped impact, aligned with table totals.\n- 1.0: Generally organized; tables present but minor formatting or clarity issues (e.g., filters missing on one table or minor formatting lapses); narrative present but somewhat vague.\n- 0.0: Disorganized or hard to follow; missing filters on both tables; unclear headers/number formats; narrative missing or not referencing requested totals.", "expectation": "A simple, organized Excel with filterable tables, clear totals, appropriate number formats, and a concise narrative that references the June totals and the slipped-to-July impact."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "1b9ec237-bf9c-41f9-8fa9-0e685fcd93c6", "rubric": {"category_name": "Hypertension Lecture Presentation (Nurse Practitioners)", "rationale": "Pattern B (Document). The task is a structured teaching presentation. Stage 1 uses an LLM judge to strictly enforce a verifiable presentation shape (PDF/DOCX, required slides/sections, single MCQ pre-test, BP measurement illustration, speaker notes, final references, \u226420 slides). Stage 2 uses code rules to verify detectable content elements via text extraction (AHA staging plausibility signals, presence of pharmacologic/non-pharmacologic content, case study risk factors, references, topic coverage, pre-test structure). Stage 3 uses an LLM judge for holistic instructional quality and audience appropriateness.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 Gate \u2014 Presentation Format and Structural Completeness", "description": "LLM-only gate to enforce exact deliverable shape: PDF/DOCX slide deck-style document with all required sections, one pre-test MCQ near the beginning, BP measurement illustration, speaker notes, \u226420 slides/pages, and a final references slide. Only verify presence/structure, not medical correctness.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Presentation Requirement (PDF/DOCX)", "description": "Check if the output is a valid presentation document with all required structural elements and constraints.", "weight": 4.0, "judge_prompt": "You are validating the SHAPE (format and structural completeness) of the candidate output. Do NOT judge medical correctness; only verify the required structure exists so later checks are possible.\n\nFormat requirements (must all be met for top score):\n- File type: PDF or DOCX only (reject PPTX/others for this rubric). The document must represent a slide deck or slide-like handout (pages that clearly map to slides or sections per slide).\n- Length: No more than 20 slides/pages.\n- Professional layout with clear slide titles/sections. Speaker notes must be included (either as Notes pages in a PDF export or labeled sections like \"Notes:\" per slide in DOCX).\n\nRequired structural elements (be flexible with exact titles, but the content must be clearly present):\n1) Title/cover slide with lecture topic (Hypertension/HTN) and presenter (Nurse Practitioner or similar).\n2) A single pre-test multiple-choice question (one question only) placed at the beginning (within the first two slides/pages). Must show options (e.g., A/B/C/D).\n3) Definition of hypertension.\n4) Pathophysiology of hypertension.\n5) Risk factors.\n6) Clinical signs and symptoms.\n7) Diagnostic methods.\n8) Stages of hypertension per American Heart Association (AHA) guidelines (a slide or section that clearly presents the staging).\n9) An illustration that demonstrates how blood pressure is measured (e.g., diagram/image of cuff placement or measurement technique). The presence of a clear figure/illustration is required.\n10) Treatment options separated into pharmacologic and non-pharmacologic interventions (can be two slides or clearly separated sections on one slide).\n11) Patient education strategies.\n12) One case study applying learning to practice that includes risk factors such as smoking and a family history of cardiovascular disease.\n13) Final slide/page with properly formatted references.\n\nSpeaker notes requirement:\n- Speaker notes should be present for several content slides (e.g., Notes panes in PDF or explicit \"Notes:\" subsections). Aim for notes on at least 5 content slides/pages.\n\nScoring (0.0\u20134.0):\n- 4.0: PDF/DOCX; \u226420 slides/pages; all 13 required elements present; pre-test single MCQ within first 2 slides; BP measurement illustration present; speaker notes on \u22655 content slides; references as the final slide.\n- 3.0: PDF/DOCX; \u226420 slides/pages; pre-test present near the beginning; BP measurement illustration present; references at end; at most one missing required content element OR speaker notes present but on fewer than 5 slides.\n- 2.0: PDF/DOCX; \u226420 slides/pages; multiple (2\u20133) missing required elements OR pre-test not near beginning; limited or unclear speaker notes.\n- 1.0: PDF/DOCX but missing most required elements, or >20 slides/pages but otherwise resembles a slide deck.\n- 0.0: Not PDF/DOCX, or not a slide-like document, or cannot locate a majority of the required elements.\n\nOnly assess structure/format and presence. Do NOT verify clinical accuracy or completeness of content beyond structural presence.", "expectation": "A PDF or DOCX deck with the specified slides/sections, single MCQ pre-test near the start, BP measurement illustration, notes on several slides, \u226420 slides, and a final references slide."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Content Verification (Deterministic Checks)", "description": "Code-based checks against extracted text from PDF/DOCX to verify presence and plausibility of required content elements enforced by Stage 1.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Core Topic Coverage Checklist", "description": "Verify presence of core topic sections: definition, pathophysiology, risk factors, clinical signs/symptoms, diagnostics, and patient education.", "weight": 1.2, "code": "import re\\nimport pandas as pd\\nimport numpy as np\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0, \"No document output available.\"\\n\\n    text = \"\"\\n    try:\\n        if output.file_name.lower().endswith('.pdf'):\\n            text = context.files.read_pdf_text(output.id) or \"\"\\n        elif output.file_name.lower().endswith('.docx'):\\n            text = context.files.read_docx_text(output.id) or \"\"\\n        else:\\n            # Fallback for document types treated as text\\n            text = context.files.read_text(output.id) or \"\"\\n    except Exception:\\n        try:\\n            text = context.files.read_docx_text(output.id) or \"\"\\n        except Exception:\\n            try:\\n                text = context.files.read_pdf_text(output.id) or \"\"\\n            except Exception:\\n                text = \"\"\\n\\n    if not text:\\n        return 0.0, \"Unable to extract text from document.\"\\n\\n    t = re.sub(r\"\\s+\", \" \", text.lower())\\n\\n    checks = {\\n        'definition': any(k in t for k in ['definition of hypertension', 'definition', 'define hypertension', 'what is hypertension']),\\n        'pathophysiology': any(k in t for k in ['pathophysiology', 'patho ', 'pathogenesis', 'mechanism', 'renin-angiotensin', 'raas']),\\n        'risk_factors': 'risk factor' in t or 'risk factors' in t,\\n        'signs_symptoms': any(k in t for k in ['signs and symptoms', 'clinical signs', 'symptoms of hypertension', 'signs of hypertension']),\\n        'diagnostics': any(k in t for k in ['diagnostic', 'diagnosis', 'how diagnosed', 'bp measurement', 'ambulatory blood pressure monitoring', 'abpm', 'home blood pressure monitoring', 'hbpm']),\\n        'patient_education': any(k in t for k in ['patient education', 'education strategies', 'counseling', 'teach-back', 'lifestyle counseling', 'patient teaching'])\\n    }\\n\\n    present = sum(1 for v in checks.values() if v)\\n    total = len(checks)\\n    score = (present / total) * 1.2\\n    feedback = f\"Coverage present in {present}/{total} core areas: \" + \", \".join([k for k,v in checks.items() if v])\\n    return score, feedback\\n"}, {"type": "code", "name": "Pharmacologic and Non-Pharmacologic Sections Present", "description": "Verify both pharmacologic and non-pharmacologic (lifestyle) management are present.", "weight": 0.6, "code": "import re\\nimport pandas as pd\\nimport numpy as np\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n\\n    text = ''\\n    try:\\n        if output.file_name.lower().endswith('.pdf'):\\n            text = context.files.read_pdf_text(output.id) or ''\\n        elif output.file_name.lower().endswith('.docx'):\\n            text = context.files.read_docx_text(output.id) or ''\\n        else:\\n            text = context.files.read_text(output.id) or ''\\n    except Exception:\\n        text = ''\\n\\n    t = text.lower()\\n\\n    pharm_hits = any(k in t for k in [\\n        'pharmacologic', 'pharmacological', 'medication', 'antihypertensive',\\n        'ace inhibitor', 'arb', 'thiazide', 'calcium channel blocker', 'ccb', 'beta-blocker', 'diuretic'\\n    ])\\n    nonpharm_hits = any(k in t for k in [\\n        'non-pharmacologic', 'nonpharmacologic', 'lifestyle', 'dash diet', 'sodium', 'salt',\\n        'exercise', 'physical activity', 'weight loss', 'smoking cessation', 'alcohol moderation'\\n    ])\\n\\n    if pharm_hits and nonpharm_hits:\\n        return 0.6, 'Both pharmacologic and non-pharmacologic content detected.'\\n    if pharm_hits or nonpharm_hits:\\n        return 0.3, 'Only one of pharmacologic/non-pharmacologic content detected.'\\n    return 0.0, 'Neither pharmacologic nor non-pharmacologic content detected.'\\n"}, {"type": "code", "name": "AHA Staging Plausibility Signals", "description": "Check for AHA mention, staging category terms, and key threshold numbers (structure-level plausibility, not precise clinical validation).", "weight": 0.8, "code": "import re\\nimport numpy as np\\nimport pandas as pd\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n\\n    text = ''\\n    try:\\n        if output.file_name.lower().endswith('.pdf'):\\n            text = context.files.read_pdf_text(output.id) or ''\\n        elif output.file_name.lower().endswith('.docx'):\\n            text = context.files.read_docx_text(output.id) or ''\\n        else:\\n            text = context.files.read_text(output.id) or ''\\n    except Exception:\\n        text = ''\\n\\n    t = re.sub(r\"\\s+\", \" \", text.lower())\\n\\n    aha_present = ('american heart association' in t) or re.search(r'\\baha\\b', t) is not None\\n    categories = [\\n        'normal', 'elevated', 'stage 1', 'stage i', 'stage 2', 'stage ii', 'hypertensive crisis'\\n    ]\\n    cat_hits = len([c for c in categories if c in t])\\n\\n    # Key threshold numbers common in AHA staging\\n    thresholds = ['120', '129', '130', '139', '140', '180']  # 80/120 diastolic also common but 120 already included\\n    num_hits = len([n for n in thresholds if re.search(r'\\b' + re.escape(n) + r'\\b', t)])\\n\\n    score = 0.0\\n    fb_parts = []\\n\\n    # Allocate portions of the 0.8 weight\\n    if aha_present:\\n        score += 0.2\\n        fb_parts.append('AHA mentioned')\\n    score += min(cat_hits, 4) / 4 * 0.3  # up to 4 category matches\\n    fb_parts.append(f'{cat_hits} staging term hits')\\n    score += min(num_hits, 4) / 4 * 0.3  # up to 4 numeric threshold hits\\n    fb_parts.append(f'{num_hits} threshold number hits')\\n\\n    return score, \"; \".join(fb_parts)\\n"}, {"type": "code", "name": "Pre-test Single MCQ Near Beginning", "description": "Detect a single multiple-choice pre-test question near the beginning with options (A\u2013D).", "weight": 0.6, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n\\n    text = ''\\n    try:\\n        if output.file_name.lower().endswith('.pdf'):\\n            text = context.files.read_pdf_text(output.id) or ''\\n        elif output.file_name.lower().endswith('.docx'):\\n            text = context.files.read_docx_text(output.id) or ''\\n        else:\\n            text = context.files.read_text(output.id) or ''\\n    except Exception:\\n        text = ''\\n\\n    t = text\\n    tl = t.lower()\\n    if not t:\\n        return 0.0\\n\\n    # Find pre-test mention\\n    pre_idx = min([i for i in [tl.find('pre-test'), tl.find('pre test'), tl.find('pretest')] if i != -1] + [len(tl)])\\n    if pre_idx == len(tl):\\n        return 0.0, 'No pre-test marker found.'\\n\\n    # Heuristic: beginning if within first 25% of text\\n    early = pre_idx <= max(500, int(0.25 * len(tl)))\\n\\n    # Extract a window around the pre-test for option detection\\n    window = tl[max(0, pre_idx-200): pre_idx + 2000]\\n    options_present = all(opt in window for opt in ['a.', 'b.', 'c.', 'd.']) or \\\n                      all(opt in window for opt in ['a)', 'b)', 'c)', 'd)'])\\n\\n    q_marks = window.count('?')\\n    singleish = q_marks >= 1 and q_marks <= 2  # tolerate formatting variants\\n\\n    if early and options_present and singleish:\\n        return 0.6, 'Pre-test MCQ detected near beginning with options A\u2013D.'\\n    if options_present:\\n        return 0.3, 'MCQ-like structure detected but not clearly near beginning or multiple questions.'\\n    return 0.0, 'Pre-test not clearly structured as single MCQ with options.'\\n"}, {"type": "code", "name": "Case Study Includes Smoking and Family History", "description": "Verify a case study is present and includes smoking and family history of CVD risk factors.", "weight": 0.5, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n\\n    text = ''\\n    try:\\n        if output.file_name.lower().endswith('.pdf'):\\n            text = context.files.read_pdf_text(output.id) or ''\\n        elif output.file_name.lower().endswith('.docx'):\\n            text = context.files.read_docx_text(output.id) or ''\\n        else:\\n            text = context.files.read_text(output.id) or ''\\n    except Exception:\\n        text = ''\\n\\n    t = text.lower()\\n\\n    case_present = any(k in t for k in ['case study', 'case:', 'patient case', 'clinical vignette'])\\n    smoking = 'smok' in t  # smoke/smoker/smoking\\n    fam_hist = 'family history' in t or 'family hx' in t or re.search(r'\\bfh\\b', t) is not None\\n\\n    if case_present and smoking and fam_hist:\\n        return 0.5, 'Case study with smoking and family history detected.'\\n    if case_present and (smoking or fam_hist):\\n        return 0.25, 'Case study present but missing one specified risk factor.'\\n    if case_present:\\n        return 0.1, 'Case study present without specified risk factors.'\\n    return 0.0, 'No case study detected.'\\n"}, {"type": "code", "name": "References Presence and Citations", "description": "Verify a final references section exists with at least two recognizable citations (URL, DOI, or year).", "weight": 0.3, "code": "import re\\n\\nDOI_RE = re.compile(r\"10\\.\\d{4,9}/\\S+\", re.I)\\nURL_RE = re.compile(r\"https?://\\S+\", re.I)\\nYEAR_RE = re.compile(r\"\\((19|20)\\d{2}\\)\")\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n\\n    text = ''\\n    try:\\n        if output.file_name.lower().endswith('.pdf'):\\n            text = context.files.read_pdf_text(output.id) or ''\\n        elif output.file_name.lower().endswith('.docx'):\\n            text = context.files.read_docx_text(output.id) or ''\\n        else:\\n            text = context.files.read_text(output.id) or ''\\n    except Exception:\\n        text = ''\\n\\n    t = text\\n    tl = t.lower()\\n    has_refs_header = 'references' in tl or 'bibliography' in tl\\n\\n    dois = DOI_RE.findall(t)\\n    urls = URL_RE.findall(t)\\n    years = YEAR_RE.findall(t)\\n\\n    citation_count = len(set(dois)) + len(set(urls)) + (len(years) // 2)  # years alone are weaker; count two years as one citation\\n\\n    score = 0.0\\n    fb = []\\n    if has_refs_header:\\n        score += 0.1\\n        fb.append('References header detected')\\n    if citation_count >= 2:\\n        score += 0.2\\n        fb.append('\u22652 recognizable citations')\\n    elif citation_count == 1:\\n        score += 0.1\\n        fb.append('1 recognizable citation')\\n    else:\\n        fb.append('No recognizable citations')\\n\\n    return score, '; '.join(fb)\\n"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Instructional Quality and Professionalism", "description": "LLM judge evaluates pedagogical quality, clarity, organization, visual communication, and audience appropriateness for nursing students.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Pedagogical Quality and Appropriateness", "description": "Holistic assessment of clarity, flow, usefulness, and professional presentation for nursing students.", "weight": 2.0, "judge_prompt": "Evaluate the overall instructional quality of the presentation for nursing students. Do NOT re-check strict structural presence already assessed in Stage 1. Focus on: clarity of explanations, logical flow from definition \u2192 pathophysiology \u2192 risks \u2192 signs/symptoms \u2192 diagnostics \u2192 staging \u2192 treatment \u2192 patient education \u2192 case application; appropriateness of depth for pre-licensure nursing students; effective integration of the case study to reinforce decision-making; readability and visual design (titles, bullets, charts/figures legibility); usefulness of speaker notes for instructional delivery; and overall professionalism and engagement (e.g., prompts for discussion, interactive elements like the pre-test).\n\nScoring (0.0\u20132.0):\n- 2.0: Clear, well-structured, appropriately scoped for nursing students; visuals aid understanding; speaker notes enhance delivery; case study effectively applied; highly professional.\n- 1.0: Generally clear but with minor gaps (e.g., uneven depth, limited visuals/notes, or modest flow issues); still useful for the audience.\n- 0.5: Mixed quality; several issues with clarity/organization or design; limited usefulness without instructor augmentation.\n- 0.0: Poorly organized or confusing; not appropriate for the audience; visuals/notes unhelpful or absent.", "expectation": "A cohesive, student-appropriate, visually clear lecture with supportive notes and a useful case application."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "3600de06-3f71-4e48-9480-e4828c579924", "rubric": {"category_name": "Wealth Management Advisor Training Deck: CDs vs Variable Annuities (Fiduciary Guidance)", "rationale": "Pattern C (Mixed). Although the deliverable is a slide deck (document), it must embed justifications, structured comparisons, and regulatory references. Stage 1 uses an LLM gate to enforce an exact, verifiable slide structure in a PDF/DOCX export. Stage 2 uses code rules to check presence of required citations, slide numbering markers, topic coverage via robust keyword sets, and compliance statements\u2014leveraging the structure enforced in Stage 1. Stage 3 evaluates the professional quality and suitability for internal advisor training.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (LLM only)", "description": "MANDATORY slide-deck structure and format. The output must be a 10-slide PDF or DOCX export with explicit slide numbering and required topical coverage per slide.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Slide Deck Format and Structure Requirements", "description": "Check if the output is a valid 10-slide training deck (PDF or DOCX) with all required sections, explicit slide numbering, and references.", "weight": 4.0, "judge_prompt": "You are the Stage 1 gatekeeper. Check ONLY the presence of the required structure and format. Do not judge accuracy of claims or quality of writing.\n\nValid formats:\n- PDF or DOCX only (exported slide deck or slide-styled document). Reject plain text/Excel/images as the primary output.\n\nGlobal structure requirements:\n- Exactly 10 slides/pages intended as slides (a slide-styled PDF or a DOCX with slide-like sections).\n- Each slide MUST visibly show slide numbering markers: a header or footer like \"Slide X\" or \"Slide X of 10\" (any consistent visible numbering is acceptable).\n- Professional slide layout (titles + bullet points). Tables are allowed.\n\nRequired slides and topics (flexible naming, but topics must be clearly matched):\n1) Title slide: communicates purpose (e.g., \"CDs vs Variable Annuities \u2013 Fiduciary Guidance\").\n2) Executive summary/Overview: key takeaway\u2014why rolling CDs into variable annuities is generally imprudent for most clients.\n3) Product features comparison: CDs vs Variable Annuities\u2014must cite FINRA cautions to investors (table or bullets that contrast key features).\n4) Risk\u2013return and growth effects: explicitly compares risk/volatility and growth implications.\n5) Liquidity and penalties comparison: early withdrawal penalties vs surrender charges (contrast clearly).\n6) Suitability and risk tolerance: aligns with NAIC Best Interest framework (suitability focus).\n7) FINRA concerns/issues: explicitly labeled section.\n8) NAIC issues/regulations: explicitly labeled section.\n9) Advisor talking points/recommendations: practical guidance for fiduciary advisors (what to say/do).\n10) References & disclosures: must include both source links provided below.\n\nSources to be listed on the References slide:\n- https://content.naic.org/sites/default/files/government-affairs-brief-annuity-suitability-best-interest-model.pdf\n- https://www.finra.org/investors/insights/high-yield-cds\n\nScoring (apply to the 4-point weight):\n- 1.0 of weight (full): PDF/DOCX; clearly 10 slides; visible slide numbering; all 10 topical slides present; references slide includes BOTH URLs.\n- 0.7 of weight: PDF/DOCX; 10 slides; minor deviations (e.g., numbering visible but slightly inconsistent OR topics 3\u20138 covered but one is merged/renamed); references present but only one URL or URL truncated.\n- 0.4 of weight: PDF/DOCX; 7\u20139 slides OR 10 slides but missing 1\u20132 required topic sections OR numbering absent; references slide present without the specific URLs.\n- 0.0 of weight: Not PDF/DOCX; fewer than 7 slides; lacks multiple required sections; no evidence of a slide deck.\n\nOnly check presence/format/structure\u2014not correctness of content.", "expectation": "A 10-slide PDF/DOCX deck with clear slide numbering, exact coverage of the required topics, and a references slide containing both NAIC and FINRA URLs."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Code rules + deterministic checks)", "description": "Automated checks for citations, coverage of required regulatory topics, presence of recommendations and compliance language, and slide numbering markers.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Source Citations Present (FINRA and NAIC)", "description": "Verify the document text includes FINRA and NAIC mentions and, preferably, the specific URLs.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    if not (getattr(output, 'is_document', False) or getattr(output, 'is_text_format', False)):\n        return 0.0\n    text = ''\n    # Try PDF -> DOCX -> Text\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n    t = text.lower()\n    has_finra = ('finra' in t) or ('finra.org' in t)\n    has_naic = ('naic' in t) or ('naic.org' in t)\n    finra_url = 'finra.org/investors/insights/high-yield-cds' in t\n    naic_url = 'content.naic.org/sites/default/files/government-affairs-brief-annuity-suitability-best-interest-model.pdf' in t or 'annuity-suitability-best-interest-model' in t\n    if has_finra and has_naic:\n        if finra_url and naic_url:\n            base = 1.0\n        elif finra_url or naic_url:\n            base = 0.7\n        else:\n            base = 0.5\n    else:\n        base = 0.0\n    return base * 0.8"}, {"type": "code", "name": "Slide Numbering Markers Coverage", "description": "Check presence of slide/page numbering like \"Slide X\" or \"X of 10\" or \"X/10\" across slides 1\u201310.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    if not (getattr(output, 'is_document', False) or getattr(output, 'is_text_format', False)):\n        return 0.0\n    text = ''\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n    t = text.lower()\n    found = set()\n    for m in re.findall(r\"\\bslide\\s*(\\d{1,2})\\b\", t):\n        try:\n            n = int(m)\n            if 1 <= n <= 10:\n                found.add(n)\n        except Exception:\n            pass\n    for m in re.findall(r\"\\b(\\d{1,2})\\s*/\\s*10\\b\", t):\n        try:\n            n = int(m)\n            if 1 <= n <= 10:\n                found.add(n)\n        except Exception:\n            pass\n    for m in re.findall(r\"\\b(\\d{1,2})\\s+of\\s+10\\b\", t):\n        try:\n            n = int(m)\n            if 1 <= n <= 10:\n                found.add(n)\n        except Exception:\n            pass\n    coverage = min(len(found) / 10.0, 1.0)\n    return coverage * 0.6"}, {"type": "code", "name": "Coverage of Required Topics (Keyword Heuristics)", "description": "Heuristic check for presence of the six required topics: features comparison, risk-return/growth, penalties, suitability/NAIC Best Interest, FINRA concerns, NAIC regulations.", "weight": 1.2, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    if not (getattr(output, 'is_document', False) or getattr(output, 'is_text_format', False)):\n        return 0.0\n    text = ''\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n    t = text.lower()\n\n    def any_in(s, arr):\n        return any(a in s for a in arr)\n\n    # Topic 1: Features comparison (CDs vs Variable Annuities)\n    cd_terms = ['certificate of deposit', 'certificates of deposit', ' cd ', ' cds ']\n    va_terms = ['variable annuity', 'variable annuities', ' annuity', ' va ']\n    compare_terms = ['compare', 'comparison', 'vs', 'versus', 'contrast', 'difference', 'features']\n    topic1 = (any_in(t, cd_terms) and any_in(t, va_terms) and any_in(t, compare_terms))\n\n    # Topic 2: Risk\u2013return and growth\n    topic2 = ('risk' in t and 'return' in t and ('growth' in t or 'compounding' in t or 'accumulation' in t))\n\n    # Topic 3: Penalties (early withdrawal vs surrender charges)\n    topic3 = any_in(t, ['penalt', 'surrender', 'withdrawal charge', 'early withdrawal', 'cdsc'])\n\n    # Topic 4: Suitability & NAIC Best Interest\n    topic4 = ('naic' in t and (('suitability' in t) or ('best interest' in t)))\n\n    # Topic 5: FINRA concerns/issues\n    topic5 = ('finra' in t and any_in(t, ['concern', 'issue', 'caution', 'alert', 'warning']))\n\n    # Topic 6: NAIC issues/regulations\n    topic6 = ('naic' in t and any_in(t, ['regulation', 'model', ' 275', '#275']))\n\n    count = sum([topic1, topic2, topic3, topic4, topic5, topic6])\n    base = count / 6.0\n    return max(0.0, min(1.0, base)) * 1.2"}, {"type": "code", "name": "FDIC Insurance Statements (Heuristic Accuracy)", "description": "Check for presence of clear statements about FDIC insurance differences (CDs vs variable annuities).", "weight": 0.5, "code": "def evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    if not (getattr(output, 'is_document', False) or getattr(output, 'is_text_format', False)):\n        return 0.0\n    text = ''\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n    t = text.lower()\n    has_fdic_insured = ('fdic insured' in t) or ('insured by the fdic' in t) or ('fdic-insured' in t)\n    has_not_fdic = ('not fdic insured' in t) or ('not insured by the fdic' in t) or ('no fdic insurance' in t) or ('not bank guaranteed' in t)\n    if has_fdic_insured and has_not_fdic:\n        base = 1.0\n    elif has_fdic_insured or has_not_fdic:\n        base = 0.6\n    else:\n        base = 0.0\n    return base * 0.5"}, {"type": "code", "name": "Advisor Recommendations/Talking Points Presence", "description": "Check for a recommendations/talking-points section aligned to fiduciary guidance.", "weight": 0.5, "code": "def evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    if not (getattr(output, 'is_document', False) or getattr(output, 'is_text_format', False)):\n        return 0.0\n    text = ''\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n    t = text.lower()\n    rec_head = any(s in t for s in ['recommendations', 'advisor talking points', 'what to say', 'guidance', 'action steps'])\n    fiduciary = 'fiduciary' in t\n    if rec_head and fiduciary:\n        base = 1.0\n    elif rec_head or fiduciary:\n        base = 0.6\n    else:\n        base = 0.0\n    return base * 0.5"}, {"type": "code", "name": "Disclosures and Compliance Language", "description": "Check for internal-use/compliance disclaimers and non-advice language on references/disclosure slide.", "weight": 0.4, "code": "def evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    if not (getattr(output, 'is_document', False) or getattr(output, 'is_text_format', False)):\n        return 0.0\n    text = ''\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n    t = text.lower()\n    phrases = [\n        'for internal use', 'educational purposes', 'not investment advice', 'not a recommendation',\n        'consult your compliance', 'disclosure', 'for training purposes', 'does not constitute', 'past performance'\n    ]\n    count = sum(1 for p in phrases if p in t)\n    if count >= 2:\n        base = 1.0\n    elif count == 1:\n        base = 0.5\n    else:\n        base = 0.0\n    return base * 0.4"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism (LLM)", "description": "Holistic assessment of clarity, professionalism, and internal training suitability.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Quality and Fiduciary Appropriateness", "description": "Evaluate the deck\u2019s overall professionalism, clarity, and suitability for internal advisor training, ensuring fiduciary framing and actionable guidance.", "weight": 2.0, "judge_prompt": "Evaluate the overall quality of the slide deck for an internal wealth management advisor audience. Consider:\n- Professional slide craft: clear titles, scannable bullets, logical flow, consistent style.\n- Fiduciary framing: emphasizes client best interest, suitability, and risk tolerance.\n- Content balance: covers fees, liquidity, surrender charges, risk-return tradeoffs without promissory language.\n- Actionability: concrete advisor talking points that help conversations with clients considering rolling CDs into variable annuities.\n- Regulatory tone: compliant, cites FINRA/NAIC where appropriate, avoids guarantees.\n\nScoring (apply to the 2-point weight):\n- 2.0: Highly professional and persuasive; concise, compliant, and clearly actionable for advisors.\n- 1.2: Solid and serviceable; minor gaps in clarity, flow, or actionability.\n- 0.6: Basic; noticeable issues (clutter, verbosity, weak talking points) but still usable.\n- 0.0: Poor quality or inappropriate for internal professional training.\n\nFocus on presentation quality and appropriateness; do not re-check structural presence already validated in Stage 1.", "expectation": "A concise, well-structured internal training deck that is fiduciary-aligned, compliant in tone, and offers clear, practical talking points for advisors."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "e21cd746-404d-4602-b9d2-01d2812c5b87", "rubric": {"category_name": "Finance and Insurance \u2022 Financial and Investment Analysts \u2022 Last-Mile Logistics Targets and Public Comps Deck", "rationale": "This rubric enforces a self-documenting, verifiable slide deck. Stage 1 (LLM-only) strictly mandates a PDF slide structure (<=5 slides) with explicit tables for private last-mile targets and public comps with valuation multiples, enabling verification. Stage 2 (code) performs deterministic checks on headers, multiples presence, and plausible numeric bounds using PDF text extraction. Stage 3 (LLM) assesses professional quality and strategic value for a client-ready banking deliverable in April 2025.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (PDF Slide Deck Structure)", "description": "Output must be a client-ready PDF of up to 5 slides with required sections and tables. LLM checks structure only (not correctness).", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.0, "rules": [{"type": "llm_judge", "name": "PDF Slide Deck Structure Requirement", "description": "Verify the candidate produced a <=5 slide PDF deck with the mandated structure for private targets and public comps, including table-based content with specific columns and a footnote showing sources and an 'As of' date near April 2025.", "weight": 6.0, "judge_prompt": "You are evaluating whether the SUBMITTED OUTPUT is a valid client-ready PDF slide deck with the exact required structure. Only check PRESENCE and STRUCTURE (not content accuracy). Be flexible with synonyms for headers but strict about the existence of the specified elements.\n\nRequirements:\n1) File/Format\n- Must be a PDF (not PPTX/DOCX/Excel/text).\n- Slide count: 1\u20135 slides total.\n- Slides should visually resemble presentation slides (landscape layouts, clear headings, tables/bullets), not a plain memo.\n\n2) Slide 1 \u2014 Title/Overview\n- Contains a clear title referencing last-mile logistics or delivery, and the client\u2019s interest in logistics for a US e-commerce business.\n- Includes date or timeframe (ideally April 2025 or similar) and a firm/author identifier (bank name/logo, or author line).\n\n3) Private Last-Mile Players (1\u20132 slides)\n- At least one slide with a TABLE listing private companies in last-mile delivery/fulfillment.\n- Table has column headers clearly visible. Required fields (synonyms allowed):\n  \u2022 Company (Company/Name)\n  \u2022 Business Description (Business Description/Description/What they do)\n  \u2022 Latest Valuation (Latest Valuation/Valuation/Last Valuation/Post-money)\n  \u2022 Funding to Date (Funding to Date/Total Funding/Raised/Capital Raised)\n  \u2022 Key Investors (Key Investors/Investors/Backers/Lead Investors)\n  \u2022 Key Customers (Key Customers/Customers/Clients)\n- Target list size: ideally 5+ companies. Accept 3\u20134 for partial credit.\n\n4) Public Comparables & Valuation Multiples (1\u20132 slides)\n- A TABLE of public comps in delivery/logistics. Visible columns include:\n  \u2022 Company Name\n  \u2022 Ticker (Ticker/Exchange:Ticker)\n  \u2022 Market Cap (USD)\n  \u2022 Enterprise Value (USD)\n  \u2022 Revenue (TTM or FY)\n  \u2022 EBITDA (TTM or FY)\n  \u2022 Net Income (TTM or FY) \u2014 optional but preferred\n  \u2022 Valuation multiples: EV/Revenue (or EV/Sales), EV/EBITDA, and P/E\n- Multiples must be printed as values in the table (not just referenced in text).\n- Footnote or small text indicates data sources (e.g., FactSet/Bloomberg/filings) and an \u201cAs of\u201d date ideally around April 2025.\n\n5) Optional Slide \u2014 Summary/Next Steps (recommended but not mandatory)\n- Brief takeaways and how the bank can assist with M&A (sourcing, valuation, process).\n\nScoring (0 to 6):\n- 6.0: Valid PDF (<=5 slides) with Title slide, a private targets table including all required columns and ~5+ companies, a public comps table including all required financials and all three multiples, plus footnote with sources and an \u201cAs of\u201d date near April 2025. Summary/Next Steps present (optional but boosts to full if all else met).\n- 5.0: Same as above but missing the optional Summary/Next Steps OR one minor supporting column (e.g., Net Income) from comps while all multiples are present.\n- 3.5: Both main sections exist but one of the tables is thin (e.g., only 3\u20134 private names) or missing 2+ required columns, or comps table lacks one of the three required multiples.\n- 2.0: Only one of the two main sections (private targets or public comps) is present, or structure is not in tables.\n- 0.0: Not a PDF, more than 5 slides, or missing both main sections.\n\nImportant: Judge structure only. Do not verify data accuracy or evaluate quality.\n", "expectation": "A concise, client-ready PDF deck (<=5 slides) with a private targets table and a public comps table including EV/Revenue, EV/EBITDA, and P/E, and footnoted sources with an April 2025 as-of date."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Deterministic Checks)", "description": "Code-based checks on presence of required headers/terms and plausibility of multiples using PDF text extraction.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Private Targets Table Headers Present", "description": "Checks PDF text for presence of key private-table headers (synonym-flexible). Scores by fraction of required header concepts detected.", "weight": 1.0, "code": "import re\n\ndef _read_any_text(context, output):\n    text = ''\n    try:\n        if hasattr(output, 'mime_type') and isinstance(getattr(output, 'mime_type', ''), str) and 'pdf' in output.mime_type.lower():\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                text = ''\n        if not text:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                text = ''\n        if not text and getattr(output, 'is_text_format', False):\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = ''\n    except Exception:\n        text = ''\n    return text or ''\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, 'No primary output.'\n    if not getattr(output, 'is_document', False):\n        return 0.0, 'Primary output is not a document.'\n\n    text = _read_any_text(context, output)\n    if not text:\n        return 0.0, 'Could not extract text from document.'\n\n    lower = text.lower()\n\n    headers = {\n        'company': ['company', 'name'],\n        'business description': ['business description', 'description', 'what they do'],\n        'latest valuation': ['latest valuation', 'valuation', 'last valuation', 'post-money', 'post money'],\n        'funding to date': ['funding to date', 'total funding', 'raised', 'capital raised', 'funding'],\n        'key investors': ['key investors', 'investors', 'backers', 'lead investors'],\n        'key customers': ['key customers', 'customers', 'clients', 'notable customers', 'major customers']\n    }\n\n    found = {}\n    for concept, synonyms in headers.items():\n        found[concept] = any(s in lower for s in synonyms)\n\n    n_required = len(headers)\n    n_found = sum(1 for v in found.values() if v)\n    score = (n_found / n_required) if n_required else 0.0\n    score = max(0.0, min(1.0, score))\n\n    missing = [k for k, v in found.items() if not v]\n    fb = f\"Detected {n_found}/{n_required} private-table header concepts. Missing: {', '.join(missing) if missing else 'None'}.\"\n    return score, fb\n"}, {"type": "code", "name": "Public Comps Multiples and Identifiers Present", "description": "Checks for presence of the three required multiples (EV/Revenue or EV/Sales, EV/EBITDA, P/E) and for tickers/identifiers.", "weight": 1.0, "code": "import re\n\ndef _read_any_text(context, output):\n    text = ''\n    try:\n        if hasattr(output, 'mime_type') and isinstance(getattr(output, 'mime_type', ''), str) and 'pdf' in output.mime_type.lower():\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                text = ''\n        if not text:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                text = ''\n        if not text and getattr(output, 'is_text_format', False):\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = ''\n    except Exception:\n        text = ''\n    return text or ''\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, 'No primary output.'\n    if not getattr(output, 'is_document', False):\n        return 0.0, 'Primary output is not a document.'\n\n    text = _read_any_text(context, output)\n    if not text:\n        return 0.0, 'Could not extract text from document.'\n\n    lower = text.lower()\n\n    metrics = {\n        'ev/revenue': ['ev/revenue', 'ev / revenue', 'ev/rev', 'ev / rev', 'ev/sales', 'ev / sales'],\n        'ev/ebitda': ['ev/ebitda', 'ev / ebitda'],\n        'p/e': ['p/e', 'p / e', 'price/earnings', 'price / earnings', 'pe ratio', 'pe']\n    }\n\n    found_metrics = {}\n    for m, syns in metrics.items():\n        found_metrics[m] = any(s in lower for s in syns)\n\n    num_found = sum(1 for v in found_metrics.values() if v)\n\n    # Check tickers/identifiers\n    has_ticker_header = 'ticker' in lower\n    exch_pat = re.compile(r'(nyse|nasdaq|amex|tsx|lse|euronext|hkex|asx)\\s*[:\\-]\\s*[a-z0-9\\.\\-]{1,6}', re.I)\n    exch_hits = exch_pat.findall(lower)\n    identifiers_ok = bool(has_ticker_header or len(exch_hits) >= 2)\n\n    base = (num_found / 3.0)\n    if identifiers_ok:\n        base += 0.25\n    score = min(1.0, max(0.0, base))\n\n    missing = [k for k, v in found_metrics.items() if not v]\n    fb = f\"Found {num_found}/3 multiples; identifiers_ok={identifiers_ok}. Missing: {', '.join(missing) if missing else 'None'}.\"\n    return score, fb\n"}, {"type": "code", "name": "Valuation Multiples Within Plausible Bounds", "description": "Extracts numeric values for EV/Revenue (or Sales), EV/EBITDA, and P/E from the PDF text and checks plausible ranges.", "weight": 1.0, "code": "import re\n\ndef _read_any_text(context, output):\n    text = ''\n    try:\n        if hasattr(output, 'mime_type') and isinstance(getattr(output, 'mime_type', ''), str) and 'pdf' in output.mime_type.lower():\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                text = ''\n        if not text:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                text = ''\n        if not text and getattr(output, 'is_text_format', False):\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = ''\n    except Exception:\n        text = ''\n    return text or ''\n\n\ndef _extract_numbers(pattern, lower):\n    vals = []\n    for m in re.finditer(pattern, lower, flags=re.I|re.M):\n        try:\n            val = float(m.group(1))\n            vals.append(val)\n        except Exception:\n            pass\n    return vals\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, 'No primary output.'\n    if not getattr(output, 'is_document', False):\n        return 0.0, 'Primary output is not a document.'\n\n    text = _read_any_text(context, output)\n    if not text:\n        return 0.0, 'Could not extract text from document.'\n\n    lower = text.lower()\n\n    pat_ev_rev = r'ev\\s*(?:/|\\s+per\\s+)\\s*(?:revenue|sales)[^0-9\\-]{0,30}([0-9]{1,3}(?:\\.[0-9]+)?)\\s*x?'\n    pat_ev_ebitda = r'ev\\s*/\\s*ebitda[^0-9\\-]{0,30}([0-9]{1,3}(?:\\.[0-9]+)?)\\s*x?'\n    pat_pe = r'(?:p\\s*/\\s*e|price\\s*/\\s*earnings|p[\\s\\.-]?e)[^0-9\\-]{0,30}([0-9]{1,3}(?:\\.[0-9]+)?)\\s*x?'\n\n    vals = {\n        'ev/revenue': _extract_numbers(pat_ev_rev, lower),\n        'ev/ebitda': _extract_numbers(pat_ev_ebitda, lower),\n        'p/e': _extract_numbers(pat_pe, lower)\n    }\n\n    in_bounds = {}\n    # Plausible bounds\n    bounds = {\n        'ev/revenue': (0.1, 50.0),\n        'ev/ebitda': (1.0, 100.0),\n        'p/e': (1.0, 200.0)\n    }\n\n    for k, arr in vals.items():\n        lo, hi = bounds[k]\n        in_bounds[k] = any((lo <= v <= hi) for v in arr)\n\n    total = 3\n    ok = sum(1 for v in in_bounds.values() if v)\n    score = ok / total\n    fb = []\n    for k in ['ev/revenue','ev/ebitda','p/e']:\n        fb.append(f\"{k}: found {len(vals[k])} vals; in_bounds={in_bounds[k]}\")\n    return score, \"; \".join(fb)\n"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Strategic Fit", "description": "LLM assesses professional polish, clarity, and strategic usefulness for a Managing Director sending to a client in April 2025.", "is_required": false, "max_points": 1.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Quality and Strategic Value", "description": "Evaluate writing/design quality, clarity, strategic insight, and appropriateness for the stated client use-case. Do not re-check structure from Stage 1.", "weight": 1.0, "judge_prompt": "Assess overall quality and suitability of the PDF slide deck for a Managing Director to send to a client exploring last-mile logistics M&A in April 2025. Consider:\n- Clarity and brevity suitable for <=5 slides; logical flow from title \u2192 private targets \u2192 public comps \u2192 summary/next steps.\n- Professional formatting: readable tables, consistent fonts, clear labels/units (USD, TTM/FY), unobtrusive footnotes.\n- Strategic value: Does it help start a conversation? Clear rationale for selected private targets, relevance to US last-mile/logistics, and a crisp set of next steps where the bank can assist.\n- Multiples presentation: easy to compare; any brief note on drivers (growth, profitability) or segmentation that aids interpretation.\n- Temporal relevance: evident \u201cAs of\u201d date around April 2025; sources cited.\n\nScoring (0\u20131):\n- 1.0: Polished, banker-grade, concise, and strategically helpful; strong narrative and clean visuals.\n- 0.7: Generally solid and client-ready but with minor clarity/formatting gaps.\n- 0.4: Useful but noticeably rough or sparse; would need edits before sending.\n- 0.0: Not professional or not helpful for initiating M&A dialogue.\n", "expectation": "Concise, polished slides that a Managing Director can send without edits to initiate an M&A discussion."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "83d10b06-26d1-4636-a32c-23f92c57f30b", "rubric": {"category_name": "Audit Review of Anti-Financial Crime Risk Metrics", "rationale": "This rubric is designed to evaluate an audit review task for accountants and auditors in which they must verify the accuracy of financial crime risk metrics. The task requires analytical skills to process spreadsheet data, calculate sampling sizes, perform variance analysis, and select audit samples based on specified criteria. The structured approach ensures that the agent produces verifiable, well-organized outputs that can be checked for accuracy and quality.", "max_total_score": 10.0, "stages": [{"name": "Shape Enforcement Gate", "description": "Ensure the output Excel file has the required sheets and sections as per task description.", "is_required": true, "min_score_to_pass": 1.0, "rules": [{"type": "llm_judge", "name": "Excel File Structure and Format", "description": "Verify that the output is an Excel file with the specific required structure.", "weight": 1.0, "judge_prompt": "Check if the submission contains an Excel file structured as follows:\n\n1. Tab titled 'Population' containing original data with additional columns:\n   - Column J: Quarter-on-quarter variance.\n   - Column K: Sample indicator, marked with '1'.\n\n2. Tab titled 'Sample Size Calculation':\n   - Must show the sample size calculation based on a 90% confidence level and a 10% tolerable error rate.\n   - Include explanation and workings.\n\n3. Tab titled 'Sample':\n   - Contains only the selected sample rows from 'Population', with column K indicating selection.\n   - Must replicate the structure of the 'Population' tab for selected rows.\n\n**Scoring**:\n- 1.0: All required tabs and columns present with correct structure and titles.\n- 0.7: Minor omissions (e.g., missing optional explanation in 'Sample Size Calculation').\n- 0.4: Missing significant elements (e.g., a required tab or column).\n- 0.0: Incorrect format or multiple structural issues.", "expectation": "The output must include the specified sheets and columns to enable further verification of the calculations and sample selection process."}], "max_points": 1.0, "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Correctness Verification", "description": "Verify the correctness of calculations and sample selection based on specified criteria.", "is_required": false, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Sample Size Calculation Correctness", "description": "Check the correctness of the sample size calculation in 'Sample Size Calculation' tab.", "weight": 2.0, "code": "def evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n\n    try:\n        df = context.files.read_excel(output.id, sheet_name='Sample Size Calculation')\n        if 'Sample Size' in df.columns and df['Sample Size'].dtype in ['int64', 'float64']:\n            calculated_size = df['Sample Size'].iloc[0]\n            # Basic validation, assumes target size around expected theoretical value\n            if 25 <= calculated_size <= 100:  # Assuming expected size range\n                return 2.0\n        return 0.0  # If unexpected format or values\n    except Exception:\n        return 0.0"}], "max_points": 2.0, "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Quality and Comprehensiveness", "description": "Evaluate overall quality and thoroughness of the analysis and documentation.", "is_required": false, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Quality of Variance Analysis and Sampling", "description": "Assess the quality of variance calculation, the rationale for sample selection, and overall presentation.", "weight": 5.0, "judge_prompt": "Evaluate the quality of the variance analysis conducted on the 'Population' sheet. Ensure the following:\n\n- Correct calculation of variance in column J for both Q2 and Q3.\n- Logical and well-documented sample selection in column K based on provided criteria.\n- Clear, professional documentation of methods, assumptions, and criteria in the 'Sample Size Calculation' tab, including a rationale for the selection logic and choice of samples.\n\nConsider clarity, logical flow, and the professional standard of the presentation. \n\n**Scoring**:\n- 5.0: Exemplary analysis, perfectly meeting all criteria with outstanding clarity and insight.\n- 4.0: Very good analysis, minor inaccuracies or clarity issues.\n- 3.0: Satisfactory analysis, several issues with accuracy or clarity.\n- 2.0: Incomplete or unclear in several areas.\n- 1.0: Fundamentally flawed, with incorrect or unclear analysis and presentation.\n", "expectation": "The task output should clearly demonstrate a thorough understanding of variance analysis and sample selection, backed by logical and professional presentation."}], "max_points": 5.0, "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "116e791e-890c-42b1-ba90-1db02e8bfd45", "rubric": {"category_name": "Pediatric PACU Care Plan (RN)", "rationale": "This rubric forces a one-page PDF nursing care plan into a strictly verifiable shape (Stage 1 LLM Gate), then runs deterministic code checks for structural correctness and required clinical coverage tied to the case (Stage 2), and finally applies an LLM quality assessment for professionalism and appropriateness (Stage 3). Stage 1 is an LLM-only gate to confirm the exact document structure, section headers, counts per diagnosis, and one-page PDF format. Stage 2 leverages code rules to verify presence of case anchors (AB, age, ORIF, spica cast, right femur), mandatory content (pain with pharmacologic and nonpharmacologic methods, infection prevention with hand hygiene and aseptic technique, neurovascular/cast care), and shift update log. Stage 3 evaluates overall clarity, measurability, and pediatric appropriateness.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2013 Structural Gate (One-Page PDF Care Plan)", "description": "LLM-only gate verifying exact required document structure for a one-page pediatric PACU nursing care plan with three nursing diagnoses, each with specified subcomponents and counts. If this gate fails, the entire category scores 0.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "One-Page Pediatric PACU Nursing Care Plan \u2013 Structural Requirements", "description": "Check if the output is a one-page PDF care plan with the exact required structure and section counts to enable automated verification.", "weight": 4.0, "judge_prompt": "You are evaluating whether the candidate output is a ONE-PAGE PDF nursing care plan with the exact structure below. Only check PRESENCE, FORMAT, and COUNTS. Do not judge clinical quality.\n\nFormat Requirements:\n- Must be a PDF file (not DOCX, not plain text, not Excel).\n- Exactly one page.\n- Professional, readable formatting (clear headings, bullet lists for assessments and interventions).\n\nRequired Structure and Content (be flexible with exact header wording, but verify substance and counts):\n1) Title/Header:\n   - Includes patient identifier \u201cAB\u201d, pediatric context (3-year-old), and postoperative PACU care after right femur ORIF (spica cast).\n\n2) Brief Patient Status Summary (near the top):\n   - States: 3-year-old, ORIF of right femur, spica cast, current neurovascular status (capillary refill <2s, warm/pink distal skin, strong pedal pulse), pain 6/10 on FACES scale.\n\n3) Nursing Diagnoses Section:\n   - Exactly THREE diagnosis blocks labeled clearly as \u201cDiagnosis 1\u201d, \u201cDiagnosis 2\u201d, \u201cDiagnosis 3\u201d (or equivalent clear numbering). Each block MUST contain:\n     a) Outcome (exactly one per diagnosis)\n     b) Assessments: exactly 4 discrete items (bullets or numbered)\n     c) Interventions: exactly 4 discrete items (bullets or numbered)\n   - At least one diagnosis must address pain/comfort in PACU.\n   - At least one diagnosis must address infection risk/aseptic technique.\n   - At least one diagnosis must address mobility/neurovascular/cast care.\n   - Accept reasonable equivalents in naming (e.g., \u201cAcute Pain\u201d, \u201cRisk for Infection\u201d, \u201cImpaired Physical Mobility\u201d, \u201cRisk for Peripheral Neurovascular Dysfunction\u201d, etc.).\n\n4) Shift Review/Update Log (at bottom or end):\n   - Small table or lines with columns/fields such as: Date/Time, RN Initials/Name, Summary of Updates/Changes.\n\nScoring (structural presence only):\n- 4.0: PDF, exactly one page, all required sections present with correct counts (3 diagnoses; per diagnosis: 1 outcome, 4 assessments, 4 interventions), and includes patient status summary and shift review/update log. Coverage includes pain, infection/asepsis, and mobility/neurovascular/cast care.\n- 3.0: PDF and one page; only one minor structural miss (e.g., one diagnosis has 3 or 5 bullets instead of 4, or the title/status is present but missing one patient detail) with all core sections present and identifiable.\n- 2.0: PDF and one page; missing one required block or multiple count mismatches (e.g., fewer than 3 diagnoses, or more than one diagnosis missing required counts), or shift update log missing.\n- 1.0: PDF but wrong pagination (not exactly one page) OR multiple core sections missing (e.g., no clear diagnoses section or many counts missing).\n- 0.0: Not a PDF or no recognizable care plan structure.\n\nOnly judge the presence and structure, not correctness of content or writing quality.", "expectation": "A one-page PDF with title, patient status summary, three clearly numbered nursing diagnoses blocks (each with exactly 1 outcome, 4 assessments, 4 interventions), and a shift review/update log. Coverage spans pain, infection/asepsis, and mobility/neurovascular/cast care."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness and Coverage Verification", "description": "Deterministic code checks and a focused LLM check to verify structural correctness (as text), case anchoring, required clinical coverage (pain, infection prevention, neurovascular/cast care), and presence of shift update log.", "is_required": false, "max_points": 4.5, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Three Diagnoses Structure and Counts (as text)", "description": "Verify the document text includes three numbered diagnoses (1\u20133) and, per diagnosis, the presence of an Outcome section and at least 4 bullet-like items under both Assessments and Interventions.", "weight": 1.8, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = context.files.read_docx_text(output.id)\n    except Exception:\n        text = \"\"\n    if not text:\n        return 0.0\n    tl = text.lower()\n\n    # Helper to extract a block for a given diagnosis number\n    def get_diag_block(t, i):\n        # Match from \"diagnosis i\" (or \"nursing diagnosis i\") up to next diagnosis or end\n        pat = re.compile(r\"(?:^|\\n)\\s*(?:nursing\\s*)?diagnosis\\s*(?:#?\\s*)?%d\\b[\\s\\S]*?(?=(?:\\n\\s*(?:nursing\\s*)?diagnosis\\s*(?:#?\\s*)?(?:%d|%d)\\b|\\Z))\" % (i, i+1, i+2), re.IGNORECASE)\n        m = pat.search(t)\n        return m.group(0) if m else \"\"\n\n    def count_bullets(section_text):\n        count = 0\n        for line in section_text.splitlines():\n            if re.match(r\"^\\s*(?:[-*\\u2022\\u2013\\u2014]|\\d+\\.)\\s+\", line):\n                count += 1\n        return count\n\n    total_checks = 0\n    passed_checks = 0\n\n    for i in [1,2,3]:\n        block = get_diag_block(text, i)\n        if not block:\n            # Count as three failed checks for this diagnosis (Outcome, Assessments, Interventions)\n            total_checks += 3\n            continue\n        # Outcome present\n        total_checks += 1\n        if re.search(r\"\\boutcome\\b\\s*:?\", block, re.IGNORECASE):\n            passed_checks += 1\n        # Assessments bullet count (>=4)\n        total_checks += 1\n        assess_sec = \"\"\n        m = re.search(r\"assessments?\\b[\\s:]*([\\s\\S]*?)(?=\\n\\s*(interventions?|outcome|diagnosis)\\b|\\Z)\", block, re.IGNORECASE)\n        if m:\n            assess_sec = m.group(1)\n        if assess_sec and count_bullets(assess_sec) >= 4:\n            passed_checks += 1\n        # Interventions bullet count (>=4)\n        total_checks += 1\n        interv_sec = \"\"\n        m2 = re.search(r\"interventions?\\b[\\s:]*([\\s\\S]*?)(?=\\n\\s*(assessments?|outcome|diagnosis)\\b|\\Z)\", block, re.IGNORECASE)\n        if m2:\n            interv_sec = m2.group(1)\n        if interv_sec and count_bullets(interv_sec) >= 4:\n            passed_checks += 1\n\n    if total_checks == 0:\n        return 0.0\n    return passed_checks / total_checks"}, {"type": "code", "name": "Patient Context Anchoring (AB, age, ORIF, spica cast, right femur)", "description": "Checks that the plan anchors to the specific case: AB, 3-year-old, ORIF, spica cast, right femur.", "weight": 0.9, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = context.files.read_docx_text(output.id)\n    except Exception:\n        text = \"\"\n    if not text:\n        return 0.0\n    tl = text.lower()\n\n    checks = []\n    # AB identifier\n    checks.append('ab' in tl)\n    # Age 3-year-old variants\n    age_pat = re.compile(r\"\\b(3\\s*[- ]?year(?:s)?\\s*old|age\\s*3|3\\s*(?:y/?o|yo)|3\\s*yr\\b)\")\n    checks.append(bool(age_pat.search(tl)))\n    # ORIF\n    checks.append('orif' in tl or 'open reduction internal fixation' in tl)\n    # Spica cast\n    checks.append('spica' in tl and 'cast' in tl)\n    # Right femur\n    checks.append('right femur' in tl or ('right' in tl and 'femur' in tl))\n\n    score = sum(1 for c in checks if c) / len(checks)\n    return score"}, {"type": "code", "name": "Coverage: Pain (pharm + nonpharm), Infection Prevention, Neurovascular/Cast Care", "description": "Verifies presence of key content: pediatric pain scale mention and both pharmacologic and nonpharmacologic comfort measures; explicit infection prevention via hand hygiene and aseptic technique; neurovascular/cast care monitoring.", "weight": 1.2, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = context.files.read_docx_text(output.id)\n    except Exception:\n        text = \"\"\n    if not text:\n        return 0.0\n    tl = text.lower()\n\n    # Pain components\n    faces = ('faces' in tl) or ('face pain scale' in tl)\n    pharm_terms = ['analgesic','acetaminophen','paracetamol','ibuprofen','nsaid','opioid','morphine','hydromorphone','oxycodone','ketorolac']\n    pain_pharm = any(term in tl for term in pharm_terms)\n    nonpharm_terms = ['distraction','reposition','positioning','ice','cold','elevat','comfort','caregiver','parent presence','child life','music','guided imagery','deep breathing','bubble']\n    pain_nonpharm = any(term in tl for term in nonpharm_terms)\n\n    # Infection prevention\n    hand_hygiene = ('hand hygiene' in tl) or ('handwashing' in tl) or ('hand-washing' in tl) or ('hand sanit' in tl) or ('alcohol-based' in tl)\n    aseptic = ('aseptic' in tl) or ('asepsis' in tl) or ('sterile technique' in tl)\n\n    # Neurovascular/cast care\n    neuro_terms = ['neurovascular','capillary refill','cap refill','pulse','pulses','sensation','movement','motor','color','temperature']\n    neuro = any(term in tl for term in neuro_terms)\n    cast_care = ('cast' in tl) or ('spica' in tl)\n    neuro_cast = neuro and cast_care\n\n    flags = [faces, pain_pharm, pain_nonpharm, hand_hygiene, aseptic, neuro_cast]\n    score = sum(1 for f in flags if f) / len(flags)\n    return score"}, {"type": "code", "name": "Shift Review/Update Log Present", "description": "Checks for a shift review/update log with fields for Date/Time and RN initials/name and updates.", "weight": 0.3, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = context.files.read_docx_text(output.id)\n    except Exception:\n        text = \"\"\n    if not text:\n        return 0.0\n    tl = text.lower()\n    has_shift = bool(re.search(r\"shift\\s*(review|update|note|log)\", tl)) or ('shift' in tl and ('update' in tl or 'review' in tl))\n    has_dt = ('date' in tl) and ('time' in tl or 'datetime' in tl)\n    has_rn = ('rn' in tl) or ('nurse' in tl) or ('initials' in tl) or ('signature' in tl)\n    return 1.0 if (has_shift and has_dt and has_rn) else 0.0"}, {"type": "llm_judge", "name": "Clinical Coverage Alignment (Focused)", "description": "LLM check that the three diagnoses appropriately cover pain/comfort, infection/aseptic technique, and mobility/neurovascular/cast care; outcomes are measurable/time-bound; and interventions logically align with assessments for a 3-year-old PACU patient post-ORIF in a spica cast.", "weight": 0.3, "judge_prompt": "Evaluate the document for the following correctness aspects (not style):\n- Do the THREE nursing diagnoses collectively cover: (a) pain/comfort, (b) infection risk with explicit hand hygiene/aseptic technique, and (c) mobility/neurovascular/cast care in a pediatric PACU context?\n- Are the outcomes per diagnosis stated as measurable and time-bound (e.g., include a timeframe or clear target like within X hours/shift)?\n- Do listed interventions logically align with the assessments and diagnosis for a 3-year-old post-ORIF in a spica cast?\n\nScoring:\n- 1.0: All three coverage areas are clearly present; all outcomes are measurable/time-bound; and interventions align logically with assessments for this pediatric case.\n- 0.6: Two coverage areas clearly present; most outcomes measurable/time-bound; generally logical alignment.\n- 0.3: Only one coverage area clearly present OR outcomes not measurable/time-bound; alignment partially logical.\n- 0.0: None of the coverage areas clearly present OR interventions largely misaligned with case.\n\nReturn a score in [0,1] based on the above criteria (this will be scaled by weight).", "expectation": "Three appropriate diagnoses cover pain, infection prevention (hand hygiene/aseptic technique), and mobility/neurovascular/cast care; outcomes are measurable/time-bound; interventions fit the pediatric PACU case."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Professional Quality", "description": "Holistic LLM assessment of clarity, measurability, prioritization, and pediatric appropriateness for handoff and ongoing shift updates.", "is_required": false, "max_points": 1.5, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Practical Usefulness", "description": "Evaluate overall quality: clarity, organization, measurability, prioritization, and appropriateness for pediatric PACU nursing handoff and use throughout the stay.", "weight": 1.5, "judge_prompt": "Assess the overall professional quality of the one-page nursing care plan for a 3-year-old PACU patient post-right femur ORIF in a spica cast. Consider:\n- Clarity and organization for rapid nursing use (headers, bulleting, easy to scan)\n- Measurable outcomes (clear targets and timelines)\n- Practicality and prioritization appropriate to early postoperative pediatric care (pain control, infection prevention, neurovascular/cast monitoring, safety)\n- Appropriateness of language and specificity for nursing shift-to-shift updates\n\nScoring Guide:\n- 1.0\u20131.5: Highly professional, clear, and practical; outcomes measurable; excellent prioritization; ready for handoff/use.\n- 0.5\u20130.9: Generally clear and useful but with minor issues (some outcomes vague or minor organization flaws).\n- 0.1\u20130.4: Limited clarity/practicality; outcomes vague; important priorities underdeveloped.\n- 0.0: Unclear, disorganized, or not useful for nursing care.\n\nReturn a score in [0,1] (this will be scaled by weight).", "expectation": "A concise, clearly organized, measurable, and clinically prioritized one-page pediatric PACU care plan suitable for nurse handoff and shift updates."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "22c0809b-f8db-489e-93b3-b4da225e3e0e", "rubric": {"category_name": "BTAM Intake Form (Private Sector) - Homeland Security Unit", "rationale": "Pattern B (Document). The deliverable is a 2\u20134 page PDF intake checklist form. Stage 1 uses an LLM judge to strictly enforce document shape and section structure. Stage 2 uses code + LLM to verify presence and consistency of key elements (fields, indicators, background-check options). Stage 3 uses an LLM judge for overall professional quality and usability.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Structured Document Gate (PDF)", "description": "Gate: Verify the output is a 2\u20134 page PDF form with the exact required sections/fields and checklist structure to enable verification.", "is_required": true, "max_points": 5.0, "min_score_to_pass": 4.0, "rules": [{"type": "llm_judge", "name": "PDF Format and Required Structure Present", "description": "Check that the candidate output is a 2\u20134 page PDF and includes all mandated sections, fields, and checklist structure for BTAM intake.", "weight": 5.0, "judge_prompt": "You are validating the SHAPE ONLY of a BTAM intake checklist form. Do not judge content quality or correctness. Inspect the candidate output and answer strictly based on structure and presence.\n\nFormat Requirements:\n- Must be a PDF (not DOCX/Excel/plain text)\n- Length must be 2\u20134 pages (inclusive)\n- Professional form layout suitable for frontline supervisors (labels, checkboxes or lines/fields, tables where appropriate)\n\nRequired Sections and Elements (be flexible with exact headings, but all elements must be clearly present and labeled):\n1) Instructions for Supervisors: Clear, concise instructions on how to complete and submit the form. Should reference emergency/imminent danger actions (e.g., call 911) and that this is a screening/intake checklist.\n2) Required Identity/Context Fields (as labeled fields with space to fill):\n   - Individual\u2019s Name\n   - Date of Observation\n   - Supervisor\u2019s Name\n   - Workplace/School/Location\n3) Background Check Authorization Section:\n   - Explicit authorization/consent language for background checks\n   - Options (checkboxes or selectable list) for different types of background checks (e.g., criminal history, warrants/NCIC/local checks, social media/open source, employment/education verification, DMV/MVR, protection orders/RO, sex offender registry, etc.)\n   - A labeled field: Reason for background check\n   - A labeled field: Date of threatening behavior (incident date)\n4) Pathways to Violence (five subsections), each with 2\u20133 observable indicators that include a short guidance/example line AND space to capture details:\n   - Grievance\n   - Ideation\n   - Planning\n   - Preparation\n   - Action\n   Indicators should be checklist items or bullet points with a brief guidance hint and an adjacent space/line/box to record details.\n5) Additional Sections (as labeled fields/areas with space to fill and examples where appropriate):\n   - Dynamic Risk Factors\n   - Additional Red Flags\n   - Other Observations\n   - Action Taken\n   - Signature and Date Submitted\n\nScoring Guidance (STRUCTURE ONLY):\n- 5.0: PDF, 2\u20134 pages, and ALL required sections/elements above are clearly present with form-like structure (labels + fields/checkboxes/lines). Pathway subsections each show 2\u20133 indicators with guidance text and space for details.\n- 4.0: PDF and 2\u20134 pages; all core elements present but one minor sub-element is imperfect/ambiguous (e.g., one pathway shows only 2 indicators without explicit guidance text, or one of the Additional Sections is minimally labeled).\n- 2.0\u20133.0: PDF but missing multiple required sections/elements, or missing guidance/space for details in several places, or the pathways structure is incomplete (e.g., fewer than 5 subsections).\n- 0.0\u20131.0: Not a PDF, outside 2\u20134 pages, or the structure does not match a form/checklist (e.g., narrative only) or major required sections are absent.\n\nOnly evaluate presence/structure. Do not assess correctness, policy compliance, or writing quality.", "expectation": "A 2\u20134 page PDF form with instructions, labeled fields, background-check authorization with options, five Pathways subsections each with 2\u20133 observable indicators that include guidance and space for details, and the final sections (Dynamic Risk Factors, Additional Red Flags, Other Observations, Action Taken, Signature/Date)."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification Checks (Content Presence and Consistency)", "description": "Lightweight correctness/consistency checks using code and an auxiliary LLM rule. Ensure the mandated elements from Stage 1 appear in text, include observable indicators with guidance, and provide background-check option variety.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Required Fields Present (Text Scan)", "description": "Verify that the PDF text contains the required identity/context fields and the background-check reason/date references.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n\n    # Try to read text from PDF, fallback to DOCX\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0, \"Unable to read document text\"\n\n    if not text or len(text.strip()) == 0:\n        return 0.0, \"Empty or non-extractable text\"\n\n    t = text.lower()\n\n    def any_present(patterns):\n        return any(p in t for p in patterns)\n\n    checks = []\n\n    # Individual's Name (subject)\n    checks.append(any_present([\n        \"individual\u2019s name\", \"individual's name\", \"individual name\", \"subject name\", \"person\u2019s name\", \"person's name\", \"employee name\", \"name of individual\"\n    ]))\n\n    # Date of Observation\n    checks.append(any_present([\n        \"date of observation\", \"observation date\", \"date observed\", \"date of incident\", \"incident date\", \"date of behavior\"\n    ]))\n\n    # Supervisor's Name\n    checks.append(any_present([\n        \"supervisor\u2019s name\", \"supervisor's name\", \"supervisor name\", \"reporting supervisor\", \"manager name\", \"immediate supervisor\"\n    ]))\n\n    # Workplace/School/Location\n    checks.append(any_present([\n        \"workplace/school/location\", \"workplace / school / location\", \"workplace\", \"school\", \"campus\", \"location\", \"worksite\", \"facility\", \"site\"\n    ]))\n\n    # Background check authorization language\n    checks.append(any_present([\n        \"background check authorization\", \"authorization for background check\", \"i authorize\", \"consent to\", \"consent for background check\", \"release of information\", \"authorize the release\"\n    ]) and any_present([\"background check\", \"background checks\"]))\n\n    # Reason for background check + date of threatening behavior\n    reason_present = any_present([\n        \"reason for background check\", \"reason for the background check\", \"reason for check\", \"reason for request\"\n    ]) or (\"reason\" in t and \"background\" in t and \"check\" in t)\n\n    date_threat_present = any_present([\n        \"date of threatening behavior\", \"date of threatening incident\", \"date of threat\", \"threat date\", \"date of behavior\", \"incident date\"\n    ])\n\n    checks.append(reason_present and date_threat_present)\n\n    score = sum(1 for c in checks if c) / max(1, len(checks))\n    return score, f\"Fields present: {sum(1 for c in checks if c)}/{len(checks)}\""}, {"type": "code", "name": "Pathways Indicators Count and Guidance", "description": "For each Pathway (Grievance, Ideation, Planning, Preparation, Action), check that at least 2 indicators appear near that heading and that guidance/examples or details prompts are present.", "weight": 1.2, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n\n    # Read text\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0, \"Unable to read document text\"\n\n    if not text:\n        return 0.0, \"Empty text\"\n\n    t = text\n    tl = t.lower()\n\n    # Define headings and an ordered list to approximate section boundaries\n    headings = [\"grievance\", \"ideation\", \"planning\", \"preparation\", \"action\"]\n    # Potential boundary markers (other section headings)\n    boundaries = headings + [\n        \"dynamic risk factors\", \"additional red flags\", \"other observations\", \"action taken\", \"signature\", \"date submitted\", \"background check\", \"instructions\"\n    ]\n\n    def section_snippet(doc_text, heading):\n        hl = heading.lower()\n        start = tl.find(hl)\n        if start == -1:\n            return \"\"\n        # Find next boundary after this heading\n        next_pos = len(doc_text)\n        for b in boundaries:\n            if b == heading:\n                continue\n            idx = tl.find(b, start + len(hl))\n            if idx != -1:\n                next_pos = min(next_pos, idx)\n        return doc_text[start:next_pos]\n\n    def bullet_like_count(s):\n        # Count common bullet/checkbox markers and list dashes within the snippet\n        markers = [\"\\u2022\", \"\u2022\", \"- \", \"\u2013 \", \"\u2014 \", \"[]\", \"[ ]\", \"\u2610\", \"\u25a1\", \"\u25fb\", \"\u25cb \", \"o \", \"(1)\", \"1.\", \"2.\", \"3.\"]\n        count = 0\n        sl = s.lower()\n        for m in markers:\n            count += sl.count(m.lower())\n        # Also count line breaks that start with non-space chars as potential items\n        lines = [ln.strip() for ln in s.splitlines()]\n        itemish = sum(1 for ln in lines if len(ln) > 0 and (ln[0] in \"-\u2022\u2610\u25a1\"))\n        return count + itemish\n\n    def has_guidance_or_details(s):\n        sl = s.lower()\n        guidance_tokens = [\"e.g.\", \"for example\", \"examples\", \"such as\", \"including\", \"guidance\", \"look for\"]\n        detail_tokens = [\"details\", \"describe\", \"notes\", \"explain\", \"observed\", \"comments\"]\n        return any(tok in sl for tok in guidance_tokens) or any(tok in sl for tok in detail_tokens)\n\n    per_path_scores = []\n    feedback_bits = []\n\n    for h in headings:\n        snip = section_snippet(t, h)\n        if not snip:\n            per_path_scores.append(0)\n            feedback_bits.append(f\"Missing section: {h}\")\n            continue\n        bullets = bullet_like_count(snip)\n        has_help = has_guidance_or_details(snip)\n        ok = (bullets >= 2) and has_help\n        per_path_scores.append(1 if ok else 0.5 if (bullets >= 2 or has_help) else 0)\n        feedback_bits.append(f\"{h}: bullets={bullets}, help={has_help}\")\n\n    avg = sum(per_path_scores) / max(1, len(per_path_scores))\n    return avg, \"; \".join(feedback_bits)[:500]"}, {"type": "code", "name": "Background Check Options Variety", "description": "Check that multiple types of background checks are offered (variety) and that explicit authorization/consent language exists.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n\n    # Read text\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0, \"Unable to read document text\"\n\n    if not text:\n        return 0.0, \"Empty text\"\n\n    tl = text.lower()\n\n    # Authorization/consent presence\n    auth = any(p in tl for p in [\n        \"background check authorization\", \"authorization for background check\", \"i authorize\", \"consent to\", \"consent for background check\", \"release of information\", \"authorize the release\"\n    ]) and (\"background check\" in tl or \"background checks\" in tl)\n\n    # Variety of background check options (flexible matching)\n    options = {\n        \"criminal_history\": [\"criminal history\", \"criminal record\", \"criminal background\"],\n        \"warrants_ncic\": [\"ncic\", \"warrants\", \"wanted check\"],\n        \"local_state_checks\": [\"local records\", \"state records\", \"county records\"],\n        \"social_media_osint\": [\"social media\", \"open source\", \"osint\"],\n        \"employment_verification\": [\"employment verification\", \"employment history\"],\n        \"education_verification\": [\"education verification\", \"degree verification\"],\n        \"dmv_mvr\": [\"dmv\", \"driver record\", \"mvr\"],\n        \"protective_orders\": [\"protective order\", \"restraining order\", \"order of protection\"],\n        \"sex_offender\": [\"sex offender registry\", \"sorb\", \"megans law\"],\n    }\n\n    matched_categories = 0\n    matched_list = []\n    for cat, pats in options.items():\n        if any(p in tl for p in pats):\n            matched_categories += 1\n            matched_list.append(cat)\n\n    # Checkbox/option style markers (optional but supportive)\n    has_markers = any(m in text for m in [\"\u2022\", \"\\u2022\", \"\u2610\", \"\u25a1\", \"[ ]\", \"- \"])\n\n    # Scoring: need authorization + at least 3 categories for full credit\n    if auth and matched_categories >= 3:\n        score = 1.0\n    else:\n        # partial credit: proportion of categories (up to 3) and credit if auth present\n        base = min(matched_categories / 3.0, 1.0)\n        score = 0.5 * base + (0.5 if auth else 0.0)\n        score = min(score, 1.0)\n\n    fb = f\"auth={auth}, categories={matched_categories} ({', '.join(matched_list)}), markers={has_markers}\"\n    return score, fb"}, {"type": "llm_judge", "name": "Operational Instructions and Submission Workflow", "description": "Check presence of clear instructions for supervisors: what to do in imminent danger, how to submit the form, and any brief privacy/limitations notes appropriate to an intake (e.g., do not delay emergency response; high-level privacy caution).", "weight": 0.2, "judge_prompt": "Evaluate only whether the following operational instruction elements are present (not their quality):\n- Clear instructions to frontline supervisors on how to complete the form\n- Explicit immediate danger guidance (e.g., call 911 / do not delay emergency response)\n- How/where to submit the completed form (e.g., email/portal/contact)\n- A brief privacy/limitations note that this is an intake screen (e.g., not a full investigation, share minimal necessary information, or similar caution)\n\nScoring:\n- 0.2: All four elements present\n- 0.1: Two\u2013three elements present\n- 0.0: One or zero elements present\n\nOnly check presence, not depth or correctness.", "expectation": "A concise instruction area that covers completion steps, emergency guidance, submission method, and a brief privacy/limitations note."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Usability", "description": "Holistic LLM assessment of professionalism, readability, and suitability for law enforcement supervisors and private-sector contexts.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Fit for Purpose", "description": "Assess professional layout, readability, and practical usability in the LE/private-sector context.", "weight": 2.0, "judge_prompt": "Assess the overall professional quality and usability of the BTAM intake form for frontline supervisors in a Midwest municipal police Homeland Security Unit handling private-sector threat intakes. Consider:\n- Clarity, readability, and logical flow\n- Form usability: sufficient space for details, well-labeled fields, consistent terminology, and recognizable checkboxes/lines\n- Appropriateness for law enforcement/private-sector BTAM screening (neutral tone, non-stigmatizing, operationally useful)\n- Visual organization: headers, spacing, alignment, and ease of scanning\n\nScoring:\n- 2.0: Professional, clean, highly usable, and well-organized; immediately deployable with minimal edits\n- 1.0: Generally usable but needs moderate formatting/wording refinements\n- 0.0: Poorly organized, confusing, or impractical for frontline use\n\nDo not re-check structure beyond what affects overall readability/usability.", "expectation": "A professional, easy-to-use intake checklist with clear labels, adequate space, and an operationally appropriate tone."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "650adcb1-ed19-4f88-8117-77640f7b94b6", "rubric": {"category_name": "Government \u2022 Recreation Workers \u2014 Winter Intern Schedule (Excel)", "rationale": "Self-documenting rubric that forces a verifiable Excel structure (calendar tables per month + time-off tab + legend), then uses code checks to validate coverage, policy constraints, and cross-sheet consistency. Stage 1 is an LLM-only gate to enforce shape. Stage 2 mixes code rules to verify correctness using the mandated structure. Stage 3 evaluates professional quality for internal stakeholder use.", "max_total_score": 16.0, "stages": [{"name": "Stage 1 \u2014 Structured Output Gate (MANDATORY)", "description": "Ensures the candidate delivers an Excel workbook in a strictly verifiable structure so downstream code checks are possible.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Workbook Structure and Required Elements", "description": "Output must be an Excel workbook with specific sheets, tables, and a visible legend to enable verification. Only check presence/structure, not correctness of content.", "weight": 4.0, "judge_prompt": "You are verifying STRUCTURE ONLY (not content correctness). Check the candidate's primary output for these exact requirements:\n\nFormat and Tabs\n- Must be a valid Excel workbook (.xlsx). Not PDF, DOCX, or plain text.\n- Exactly six tabs total:\n  1) December 2025\n  2) January 2026\n  3) February 2026\n  4) March 2026\n  5) April 2026\n  6) Time Off Requests\n- Be flexible with month sheet names (e.g., \"Dec 2025\", \"December\", \"December 2025\"). Each month Dec 2025 through Apr 2026 must be present exactly once. The time-off sheet name must clearly indicate it is for time off (e.g., contains both words \"time\" and \"off\").\n\nLegend (Key)\n- On the FIRST month tab (December 2025), include a clear Key/Legend that explains:\n  (A) Working = green cell background + an \"X\"\n  (B) Scheduled Day Off = orange cell background + the word \"off\"\n  (C) Requested Day Off = red cell background + the words \"Requested Day Off\"\n- The legend must be visible and appropriately labeled so an internal user can understand the color-coding quickly.\n\nPer-Month Schedule Table Structure (for each month tab)\n- A clearly labeled table with columns:\n  \u2022 Date (one row per calendar date in that month)\n  \u2022 Day (optional, e.g., Mon/Tue)\n  \u2022 Adam Blake\n  \u2022 Dustin Herman\n  \u2022 Katie Montgomery\n  \u2022 Coverage Notes (or similar name such as \"Coverage Status\"; used to mark days with fewer than two interns working)\n- In the three intern columns, each cell should contain exactly one of these text entries:\n  \u2022 \"X\" when scheduled to work (cell should be green per legend)\n  \u2022 \"off\" when scheduled day off (cell should be orange per legend)\n  \u2022 \"Requested Day Off\" when an approved/requested day off (cell should be red per legend)\n- The color usage must be consistent with the Key.\n\nTime Off Requests Tab\n- A tab labeled clearly for time off requests containing a simple table with columns such as:\n  \u2022 Intern Name\n  \u2022 Date\n  \u2022 Notes/Reason\n- It must list each intern\u2019s requested dates as provided in the task description (structure only; do NOT validate correctness here).\n\nCoverage Notation\n- The Coverage Notes/Status column must be present on each month tab and intended to mark dates where fewer than two interns are scheduled to work (e.g., entries like \"Needs Coverage\"). Do not check correctness\u2014just that the column exists.\n\nScoring (STRUCTURE ONLY)\n- 4.0: All six tabs present; first tab has the Key with correct mapping; each month tab has the required table columns (with flexible naming for coverage column); Time Off Requests tab present with a structured table; intern cells use the specified tokens (\"X\", \"off\", \"Requested Day Off\") and color-coding is consistent with the Key.\n- 3.0: Minor issues only (e.g., slight column header naming variations, or Key on a different month tab but clearly present and correct), but overall the required structure exists on all tabs.\n- 2.0: Several structural elements missing (e.g., Coverage Notes column missing on some tabs, or Time Off Requests tab present but not in a clear tabular form), yet an Excel file with recognizable monthly schedule tables still exists.\n- 1.0: Excel file present but monthly schedule tables are not in the required tabular form or multiple required tabs/legend are missing.\n- 0.0: Not an Excel file OR missing multiple month tabs OR no recognizable schedule structure.\n\nOnly check presence/format/structure. Do NOT judge correctness of dates, assignments, or policies.", "expectation": "A single .xlsx containing five monthly tabs (Dec 2025\u2013Apr 2026) each with a Date-based schedule table for Adam, Dustin, Katie, plus Coverage Notes; the first tab includes a Key/Legend mapping color + text; and a Time Off Requests tab with a simple table of intern/date/notes."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification of Policy, Dates, and Coverage", "description": "Now that the workbook shape is enforced, verify correctness: date coverage, status tokens, time-off mapping to requirements, staffing sufficiency, and internal consistency.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Date Range Coverage (Dec 1, 2025 \u2014 Apr 30, 2026)", "description": "Checks that the combined monthly tabs cover every date in the required range with no gaps.", "weight": 1.5, "code": "import re\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        # Load sheet names\n        xlf = pd.ExcelFile(context.files.get_path(output.id))\n        sheet_names = xlf.sheet_names\n        # Map month names flexibly\n        month_aliases = {\n            '2025-12': ['december', 'dec'],\n            '2026-01': ['january', 'jan'],\n            '2026-02': ['february', 'feb'],\n            '2026-03': ['march', 'mar'],\n            '2026-04': ['april', 'apr'],\n        }\n        month_to_sheet = {}\n        for s in sheet_names:\n            sl = s.strip().lower()\n            for key, aliases in month_aliases.items():\n                y, m = key.split('-')\n                if any(a in sl for a in aliases) and (y in sl or any(y in sl for y in [y, y[-2:]])):\n                    # Prefer exact year match if present\n                    month_to_sheet[key] = s\n        # If year not in name, fallback by unique alias match\n        for key, aliases in month_aliases.items():\n            if key not in month_to_sheet:\n                candidates = [s for s in sheet_names if any(a in s.lower() for a in aliases)]\n                if len(candidates) == 1:\n                    month_to_sheet[key] = candidates[0]\n        required_keys = list(month_aliases.keys())\n        # Build combined dates from month sheets\n        all_dates = set()\n        missing_months = [k for k in required_keys if k not in month_to_sheet]\n        for key in required_keys:\n            sname = month_to_sheet.get(key)\n            if not sname:\n                continue\n            try:\n                df = context.files.read_excel(output.id, sheet_name=sname)\n            except Exception:\n                df = pd.read_excel(context.files.get_path(output.id), sheet_name=sname)\n            # Find a date column\n            date_col = None\n            for c in df.columns:\n                if isinstance(c, str) and 'date' in c.strip().lower():\n                    date_col = c\n                    break\n            if date_col is None:\n                # Try detect first datetime-like column\n                for c in df.columns:\n                    try:\n                        temp = pd.to_datetime(df[c], errors='coerce')\n                        if temp.notna().sum() > len(df) * 0.5:\n                            date_col = c\n                            break\n                    except Exception:\n                        pass\n            if date_col is None:\n                continue\n            dates = pd.to_datetime(df[date_col], errors='coerce')\n            for d in dates.dropna().unique():\n                all_dates.add(pd.Timestamp(d).date())\n        # Expected date range\n        start = datetime(2025, 12, 1).date()\n        end = datetime(2026, 4, 30).date()\n        expected = []\n        cur = start\n        while cur <= end:\n            expected.append(cur)\n            cur += timedelta(days=1)\n        expected_set = set(expected)\n        present = len(all_dates & expected_set)\n        coverage_ratio = present / len(expected) if expected else 0.0\n        # Penalize missing months more strongly\n        if missing_months:\n            coverage_ratio *= max(0.0, 1.0 - 0.2*len(missing_months))\n        feedback = f\"Dates present {present}/{len(expected)}; missing month tabs: {len(missing_months)}.\"\n        return max(0.0, min(1.0, coverage_ratio)), feedback\n    except Exception as e:\n        return 0.0, f\"Error checking date coverage: {e}\""}, {"type": "code", "name": "Structure: Columns + Allowed Status Tokens", "description": "Confirms each monthly sheet has intern columns and that their cells use only the allowed tokens: X, off, Requested Day Off (case-insensitive).", "weight": 1.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        xlf = pd.ExcelFile(context.files.get_path(output.id))\n        months = [s for s in xlf.sheet_names if any(m in s.lower() for m in ['dec','jan','feb','mar','apr'])]\n        if not months:\n            return 0.0, \"No month tabs found.\"\n        def norm_col(c):\n            return re.sub(r'[^a-z]', '', str(c).lower())\n        intern_keys = {\n            'adamblake': None,\n            'dustinherman': None,\n            'katiemontgomery': None\n        }\n        allowed = {'x':'work', 'off':'off', 'requesteddayoff':'rdo'}\n        total_cells, ok_cells = 0, 0\n        missing_intern_cols = set()\n        for s in months:\n            try:\n                df = context.files.read_excel(output.id, sheet_name=s)\n            except Exception:\n                df = pd.read_excel(context.files.get_path(output.id), sheet_name=s)\n            cols = {norm_col(c): c for c in df.columns}\n            # Find intern columns\n            found = {}\n            for key in intern_keys.keys():\n                if key in cols:\n                    found[key] = cols[key]\n                else:\n                    # Try fuzzy (first+last names separated)\n                    if key.endswith('adamblake') or key.endswith('dustinherman') or key.endswith('katiemontgomery'):\n                        pass\n            for key, col in found.items():\n                intern_keys[key] = col\n            # If any missing on this sheet, note\n            for key in intern_keys.keys():\n                if key not in found:\n                    # Try partial match: e.g., 'adam' and 'blake'\n                    lower_cols = [str(c).lower() for c in df.columns]\n                    if any(('adam' in lc and 'blake' in lc) for lc in lower_cols) and intern_keys['adamblake'] is None:\n                        intern_keys['adamblake'] = df.columns[ [ ('adam' in lc and 'blake' in lc) for lc in lower_cols ].index(True) ]\n                    if any(('dustin' in lc and 'herman' in lc) for lc in lower_cols) and intern_keys['dustinherman'] is None:\n                        intern_keys['dustinherman'] = df.columns[ [ ('dustin' in lc and 'herman' in lc) for lc in lower_cols ].index(True) ]\n                    if any(('katie' in lc and 'montgomery' in lc) for lc in lower_cols) and intern_keys['katiemontgomery'] is None:\n                        intern_keys['katiemontgomery'] = df.columns[ [ ('katie' in lc and 'montgomery' in lc) for lc in lower_cols ].index(True) ]\n            for key, col in intern_keys.items():\n                if col is None or col not in df.columns:\n                    missing_intern_cols.add((s, key))\n            # Validate token usage where columns exist\n            present_cols = [col for col in intern_keys.values() if col in df.columns]\n            if present_cols:\n                sub = df[present_cols]\n                for val in sub.replace(np.nan, '').astype(str).values.flatten():\n                    v = re.sub(r'\\s+', '', val.strip().lower())\n                    if v == '':\n                        continue\n                    total_cells += 1\n                    if v in allowed:\n                        ok_cells += 1\n        if total_cells == 0:\n            base = 0.0\n        else:\n            base = ok_cells / total_cells\n        # Penalize missing intern columns\n        penalty = 0.0\n        if missing_intern_cols:\n            # up to 50% penalty if many sheets miss columns\n            sheets_affected = len({m[0] for m in missing_intern_cols})\n            penalty = min(0.5, 0.1 * sheets_affected)\n        score = max(0.0, min(1.0, base * (1.0 - penalty)))\n        fb = f\"Token OK ratio: {ok_cells}/{max(1,total_cells)}; missing intern columns on {len({m[0] for m in missing_intern_cols})} sheet(s).\"\n        return score, fb\n    except Exception as e:\n        return 0.0, f\"Error in structure/tokens check: {e}\""}, {"type": "code", "name": "Requested Days Off Marked on Correct Dates", "description": "Validates that each intern\u2019s specified requested days are marked as \"Requested Day Off\" on the appropriate dates across the monthly sheets.", "weight": 2.0, "code": "import re\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        xlf = pd.ExcelFile(context.files.get_path(output.id))\n        sheets = xlf.sheet_names\n        # Helper: unify month sheets\n        month_sheets = [s for s in sheets if any(m in s.lower() for m in ['dec','jan','feb','mar','apr'])]\n        if not month_sheets:\n            return 0.0, \"No month tabs found.\"\n        def find_cols(df):\n            cols = {str(c).lower(): c for c in df.columns}\n            # Date col\n            date_col = None\n            for c in df.columns:\n                if isinstance(c, str) and 'date' in c.lower():\n                    date_col = c\n                    break\n            if date_col is None:\n                for c in df.columns:\n                    try:\n                        dt = pd.to_datetime(df[c], errors='coerce')\n                        if dt.notna().sum() > len(df)*0.5:\n                            date_col = c\n                            break\n                    except Exception:\n                        pass\n            # Intern cols\n            def pick(name_parts):\n                for c in df.columns:\n                    lc = str(c).lower()\n                    if all(p in lc for p in name_parts):\n                        return c\n                return None\n            return date_col, pick(['adam','blake']), pick(['dustin','herman']), pick(['katie','montgomery'])\n        # Build combined frame\n        frames = []\n        for s in month_sheets:\n            try:\n                df = context.files.read_excel(output.id, sheet_name=s)\n            except Exception:\n                df = pd.read_excel(context.files.get_path(output.id), sheet_name=s)\n            date_col, a_col, d_col, k_col = find_cols(df)\n            if date_col is None or a_col is None or d_col is None or k_col is None:\n                continue\n            sub = df[[date_col, a_col, d_col, k_col]].copy()\n            sub.columns = ['date','adam','dustin','katie']\n            sub['date'] = pd.to_datetime(sub['date'], errors='coerce')\n            frames.append(sub.dropna(subset=['date']))\n        if not frames:\n            return 0.0, \"Could not assemble monthly data with required columns.\"\n        all_df = pd.concat(frames, ignore_index=True)\n        all_df['date'] = all_df['date'].dt.date\n        def norm(v):\n            v = str(v).strip().lower()\n            v = re.sub(r'\\s+','', v)\n            if v in ['requesteddayoff']:\n                return 'rdo'\n            return v\n        for c in ['adam','dustin','katie']:\n            all_df[c] = all_df[c].apply(norm)\n        # Required RDO dates\n        req = {\n            'adam': [datetime(2025,12,25).date(), datetime(2026,3,16).date(), datetime(2026,3,17).date(), datetime(2026,4,1).date()],\n            'dustin': [datetime(2026,3,10).date(), datetime(2026,3,11).date(), datetime(2026,3,12).date(), datetime(2026,3,13).date()],\n            'katie': [datetime(2025,12,31).date(), datetime(2026,1,1).date(), datetime(2026,4,4).date(), datetime(2026,4,5).date()],\n        }\n        total_needed = sum(len(v) for v in req.values())\n        correct = 0\n        missing = []\n        for name, dates in req.items():\n            for d in dates:\n                row = all_df.loc[all_df['date'] == d]\n                if row.empty:\n                    missing.append((name, d, 'date missing'))\n                    continue\n                val = str(row.iloc[0][name])\n                if val == 'rdo':\n                    correct += 1\n                else:\n                    missing.append((name, d, f\"found '{val}'\"))\n        ratio = correct / total_needed if total_needed else 0\n        fb = f\"RDO correct {correct}/{total_needed}. Issues: {len(missing)}\"\n        return max(0.0, min(1.0, ratio)), fb\n    except Exception as e:\n        return 0.0, f\"Error validating requested days: {e}\""}, {"type": "code", "name": "Exactly Four Requested Days Per Intern + Time-Off Tab Cross-Check", "description": "Each intern should have exactly 4 requested days flagged in the schedule, and the Time Off Requests tab should list those dates for each intern.", "weight": 1.0, "code": "import re\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        xlf = pd.ExcelFile(context.files.get_path(output.id))\n        sheets = xlf.sheet_names\n        # Find time off sheet\n        timeoff_sheet = None\n        for s in sheets:\n            sl = s.lower()\n            if ('time' in sl and 'off' in sl):\n                timeoff_sheet = s\n                break\n        # Build combined intern status from month tabs\n        month_sheets = [s for s in sheets if any(m in s.lower() for m in ['dec','jan','feb','mar','apr'])]\n        def collect():\n            frames = []\n            for s in month_sheets:\n                try:\n                    df = context.files.read_excel(output.id, sheet_name=s)\n                except Exception:\n                    df = pd.read_excel(context.files.get_path(output.id), sheet_name=s)\n                cols = [str(c).lower() for c in df.columns]\n                # Find columns\n                date_col = None\n                for c in df.columns:\n                    if isinstance(c, str) and 'date' in c.lower():\n                        date_col = c; break\n                if date_col is None:\n                    for c in df.columns:\n                        try:\n                            dt = pd.to_datetime(df[c], errors='coerce')\n                            if dt.notna().sum() > len(df)*0.5:\n                                date_col = c; break\n                        except Exception:\n                            pass\n                def pick(parts):\n                    for c in df.columns:\n                        lc = str(c).lower()\n                        if all(p in lc for p in parts):\n                            return c\n                    return None\n                a = pick(['adam','blake']); d = pick(['dustin','herman']); k = pick(['katie','montgomery'])\n                if date_col is None or any(v is None for v in [a,d,k]):\n                    continue\n                sub = df[[date_col,a,d,k]].copy(); sub.columns=['date','adam','dustin','katie']\n                sub['date'] = pd.to_datetime(sub['date'], errors='coerce')\n                frames.append(sub.dropna(subset=['date']))\n            if not frames:\n                return None\n            res = pd.concat(frames, ignore_index=True)\n            for c in ['adam','dustin','katie']:\n                res[c] = res[c].astype(str).str.strip().str.lower().str.replace(r'\\s+','',regex=True)\n            res['date'] = res['date'].dt.date\n            return res\n        sched = collect()\n        if sched is None:\n            return 0.0, \"Could not collect schedule data.\"\n        counts = {}\n        for c in ['adam','dustin','katie']:\n            counts[c] = int((sched[c] == 'requesteddayoff').sum())\n        # Cross-check time off tab if available\n        cross_ok = 0\n        cross_total = 3\n        listed = { 'adam': set(), 'dustin': set(), 'katie': set() }\n        if timeoff_sheet:\n            try:\n                tf = context.files.read_excel(output.id, sheet_name=timeoff_sheet)\n            except Exception:\n                tf = pd.read_excel(context.files.get_path(output.id), sheet_name=timeoff_sheet)\n            # Attempt to infer columns: Intern Name, Date\n            intern_col = None; date_col = None\n            for c in tf.columns:\n                lc = str(c).lower()\n                if intern_col is None and ('intern' in lc or 'name' in lc): intern_col = c\n                if date_col is None and 'date' in lc: date_col = c\n            if intern_col is None or date_col is None:\n                # Try best-effort: use first two columns\n                if len(tf.columns) >= 2:\n                    intern_col = tf.columns[0]; date_col = tf.columns[1]\n            if intern_col is not None and date_col is not None:\n                tf2 = tf[[intern_col, date_col]].copy()\n                tf2.columns = ['intern','date']\n                tf2['intern'] = tf2['intern'].astype(str).str.lower()\n                tf2['date'] = pd.to_datetime(tf2['date'], errors='coerce').dt.date\n                for _, r in tf2.dropna(subset=['date']).iterrows():\n                    name = r['intern']\n                    if 'adam' in name and 'blake' in name:\n                        listed['adam'].add(r['date'])\n                    elif 'dustin' in name and 'herman' in name:\n                        listed['dustin'].add(r['date'])\n                    elif 'katie' in name and 'montgomery' in name:\n                        listed['katie'].add(r['date'])\n                # Check that counts match 4 each\n                cross_ok = sum(1 for k in ['adam','dustin','katie'] if len(listed[k]) == 4)\n        # Score: proportion interns with exactly 4 in schedule AND (if time-off tab present) with 4 listed there\n        ok_sched = sum(1 for k in ['adam','dustin','katie'] if counts[k] == 4)\n        if timeoff_sheet:\n            ratio = ( (ok_sched + cross_ok) / 6 )\n            fb = f\"RDO count in schedule: {counts}; time-off tab listed: {{'adam':{len(listed['adam'])}, 'dustin':{len(listed['dustin'])}, 'katie':{len(listed['katie'])}}}\"\n        else:\n            ratio = ok_sched / 3\n            fb = f\"RDO count in schedule: {counts}; time-off tab not found for cross-check.\"\n        return max(0.0, min(1.0, ratio)), fb\n    except Exception as e:\n        return 0.0, f\"Error checking RDO counts/cross-check: {e}\""}, {"type": "code", "name": "Daily Coverage: At Least Two Interns Working", "description": "Checks what fraction of days have at least two interns marked as working (\"X\").", "weight": 2.0, "code": "import re\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        xlf = pd.ExcelFile(context.files.get_path(output.id))\n        sheets = xlf.sheet_names\n        month_sheets = [s for s in sheets if any(m in s.lower() for m in ['dec','jan','feb','mar','apr'])]\n        frames = []\n        for s in month_sheets:\n            try:\n                df = context.files.read_excel(output.id, sheet_name=s)\n            except Exception:\n                df = pd.read_excel(context.files.get_path(output.id), sheet_name=s)\n            # find columns\n            date_col = None\n            for c in df.columns:\n                if isinstance(c, str) and 'date' in c.lower():\n                    date_col = c; break\n            if date_col is None:\n                for c in df.columns:\n                    try:\n                        dt = pd.to_datetime(df[c], errors='coerce')\n                        if dt.notna().sum() > len(df)*0.5:\n                            date_col = c; break\n                    except Exception:\n                        pass\n            def pick(parts):\n                for c in df.columns:\n                    lc = str(c).lower()\n                    if all(p in lc for p in parts): return c\n                return None\n            a = pick(['adam','blake']); d = pick(['dustin','herman']); k = pick(['katie','montgomery'])\n            if date_col is None or any(v is None for v in [a,d,k]):\n                continue\n            sub = df[[date_col,a,d,k]].copy(); sub.columns=['date','adam','dustin','katie']\n            sub['date'] = pd.to_datetime(sub['date'], errors='coerce')\n            frames.append(sub.dropna(subset=['date']))\n        if not frames:\n            return 0.0, \"Could not assemble schedule.\"\n        all_df = pd.concat(frames, ignore_index=True)\n        all_df['date'] = all_df['date'].dt.date\n        def is_work(v):\n            v = str(v).strip().lower().replace(' ', '')\n            return v == 'x'\n        grouped = all_df.groupby('date')[['adam','dustin','katie']].agg(lambda s: sum(is_work(x) for x in s))\n        grouped['ok'] = (grouped[['adam','dustin','katie']].sum(axis=1) >= 2).astype(int)\n        ratio = grouped['ok'].mean() if len(grouped) else 0.0\n        return max(0.0, min(1.0, ratio)), f\"Coverage OK on {int(grouped['ok'].sum())}/{len(grouped)} days.\"\n    except Exception as e:\n        return 0.0, f\"Error computing coverage: {e}\""}, {"type": "code", "name": "5-On / 2-Off Cadence Reasonableness", "description": "Checks whether most work runs approximate a 5-days-on, 2-days-off cadence (allowing some disruption due to requested days).", "weight": 1.5, "code": "import re\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        xlf = pd.ExcelFile(context.files.get_path(output.id))\n        sheets = xlf.sheet_names\n        # Assemble daily matrix\n        month_sheets = [s for s in sheets if any(m in s.lower() for m in ['dec','jan','feb','mar','apr'])]\n        frames = []\n        for s in month_sheets:\n            try:\n                df = context.files.read_excel(output.id, sheet_name=s)\n            except Exception:\n                df = pd.read_excel(context.files.get_path(output.id), sheet_name=s)\n            # Columns\n            date_col = None\n            for c in df.columns:\n                if isinstance(c, str) and 'date' in c.lower():\n                    date_col = c; break\n            if date_col is None:\n                for c in df.columns:\n                    try:\n                        dt = pd.to_datetime(df[c], errors='coerce')\n                        if dt.notna().sum() > len(df)*0.5:\n                            date_col = c; break\n                    except Exception:\n                        pass\n            def pick(parts):\n                for c in df.columns:\n                    lc = str(c).lower()\n                    if all(p in lc for p in parts): return c\n                return None\n            a = pick(['adam','blake']); d = pick(['dustin','herman']); k = pick(['katie','montgomery'])\n            if date_col is None or any(v is None for v in [a,d,k]):\n                continue\n            sub = df[[date_col,a,d,k]].copy(); sub.columns=['date','adam','dustin','katie']\n            sub['date'] = pd.to_datetime(sub['date'], errors='coerce')\n            frames.append(sub.dropna(subset=['date']))\n        if not frames:\n            return 0.0, \"Could not assemble schedule.\"\n        all_df = pd.concat(frames, ignore_index=True).sort_values('date')\n        all_df['date'] = all_df['date'].dt.date\n        for c in ['adam','dustin','katie']:\n            all_df[c] = all_df[c].astype(str).str.strip().str.lower().str.replace(r'\\s+','', regex=True)\n        # Convert to daily series per intern: 1=work (x), 0=off/rdo/other\n        def to_binary(s):\n            return s.apply(lambda v: 1 if v=='x' else 0).astype(int).values\n        scores = []\n        for intern in ['adam','dustin','katie']:\n            ser = all_df[[ 'date', intern ]].dropna()\n            ser = ser.drop_duplicates(subset=['date'])\n            vals = to_binary(ser[intern])\n            if len(vals) == 0:\n                scores.append(0.0); continue\n            # Run-length encode workdays\n            runs = []\n            curr = vals[0]; length = 1\n            for x in vals[1:]:\n                if x == curr:\n                    length += 1\n                else:\n                    runs.append((curr,length))\n                    curr = x; length = 1\n            runs.append((curr,length))\n            if not runs:\n                scores.append(0.0); continue\n            work_runs = [l for v,l in runs if v==1]\n            off_runs = [l for v,l in runs if v==0]\n            if not work_runs or not off_runs:\n                scores.append(0.0); continue\n            # Fraction of work days that are part of runs with length 4-6 (approx 5-on)\n            total_work_days = sum(work_runs)\n            good_work_days = sum(l for l in work_runs if 4 <= l <= 6)\n            work_score = good_work_days / total_work_days if total_work_days else 0.0\n            # Fraction of off days in runs with length >=2 (approx 2-off) \u2014 allow >2 due to holiday stretches\n            total_off_days = sum(off_runs)\n            good_off_days = sum(l for l in off_runs if l >= 2)\n            off_score = good_off_days / total_off_days if total_off_days else 0.0\n            scores.append(0.6*work_score + 0.4*off_score)\n        ratio = float(np.mean(scores)) if scores else 0.0\n        return max(0.0, min(1.0, ratio)), f\"Cadence score avg: {ratio:.2f} (per-intern: {', '.join(f'{s:.2f}' for s in scores)})\"\n    except Exception as e:\n        return 0.0, f\"Error checking cadence: {e}\""}, {"type": "code", "name": "Coverage Notes Accuracy on Exception Days", "description": "Verifies that days with fewer than two interns working are annotated in the Coverage Notes/Status column (e.g., \"Needs Coverage\").", "weight": 1.0, "code": "import re\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        xlf = pd.ExcelFile(context.files.get_path(output.id))\n        sheets = xlf.sheet_names\n        month_sheets = [s for s in sheets if any(m in s.lower() for m in ['dec','jan','feb','mar','apr'])]\n        def pick(parts, cols):\n            for c in cols:\n                lc = str(c).lower()\n                if all(p in lc for p in parts): return c\n            return None\n        frames = []\n        for s in month_sheets:\n            try:\n                df = context.files.read_excel(output.id, sheet_name=s)\n            except Exception:\n                df = pd.read_excel(context.files.get_path(output.id), sheet_name=s)\n            # Identify columns\n            date_col = None\n            for c in df.columns:\n                if isinstance(c, str) and 'date' in c.lower():\n                    date_col = c; break\n            if date_col is None:\n                for c in df.columns:\n                    try:\n                        dt = pd.to_datetime(df[c], errors='coerce')\n                        if dt.notna().sum() > len(df)*0.5:\n                            date_col = c; break\n                    except Exception:\n                        pass\n            a = pick(['adam','blake'], df.columns)\n            d = pick(['dustin','herman'], df.columns)\n            k = pick(['katie','montgomery'], df.columns)\n            cov = pick(['coverage','note'], df.columns) or pick(['coverage','status'], df.columns) or pick(['coverage'], df.columns)\n            if date_col is None or any(v is None for v in [a,d,k]) or cov is None:\n                continue\n            sub = df[[date_col,a,d,k,cov]].copy(); sub.columns=['date','adam','dustin','katie','cov']\n            sub['date'] = pd.to_datetime(sub['date'], errors='coerce')\n            frames.append(sub.dropna(subset=['date']))\n        if not frames:\n            return 0.0, \"Insufficient data to verify coverage notes.\"\n        all_df = pd.concat(frames, ignore_index=True)\n        all_df['date'] = all_df['date'].dt.date\n        def is_work(v):\n            v = str(v).strip().lower().replace(' ','')\n            return v == 'x'\n        def is_flagged(t):\n            t = str(t).strip().lower()\n            return any(k in t for k in ['need','coverage','insufficient','less than 2','<2','exception','short'])\n        grouped = all_df.groupby('date').agg({\n            'adam': lambda s: sum(is_work(x) for x in s),\n            'dustin': lambda s: sum(is_work(x) for x in s),\n            'katie': lambda s: sum(is_work(x) for x in s),\n            'cov': lambda s: any(is_flagged(x) for x in s)\n        })\n        grouped['workers'] = grouped[['adam','dustin','katie']].sum(axis=1)\n        exceptions = grouped['workers'] < 2\n        if exceptions.sum() == 0:\n            # If there are no exception days, consider this fully satisfied\n            return 1.0, \"No exception days; coverage notes not required.\"\n        noted = grouped.loc[exceptions, 'cov'].sum()\n        ratio = noted / exceptions.sum() if exceptions.sum() else 1.0\n        return max(0.0, min(1.0, ratio)), f\"Coverage exceptions noted {noted}/{exceptions.sum()} days.\"\n    except Exception as e:\n        return 0.0, f\"Error validating coverage notes: {e}\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Presentation and Usability (Quality)", "description": "Professionalism and stakeholder usability for an internal schedule that interns and program staff will rely on.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Formatting and Usability", "description": "LLM evaluates visual clarity, consistent color-coding matching the legend, easy scanning by Program Director/interns, and general professionalism.", "weight": 2.0, "judge_prompt": "Evaluate the final Excel workbook\u2019s presentation quality and usability for internal stakeholders (interns, Program Director):\n\nCriteria (holistic):\n- Readability: Date column is clear and sorted; weekly separation or consistent spacing; fonts and alignment are consistent; freeze panes or header rows make scanning easy.\n- Color-coding consistency: The cells for statuses visually match the legend on the first tab (green + X for working, orange + off for scheduled off, red + Requested Day Off for requests) with no confusing deviations.\n- Stakeholder usability: It\u2019s quick to answer \u201cIs intern X working on date Y?\u201d and \u201cAre there at least two interns on a given day?\u201d. Coverage Notes/Status is plainly visible when exceptions occur.\n- Professional polish: Clear tab names, sensible print area or margins, and a concise legend. No clutter, typos, or cryptic abbreviations.\n\nScoring\n- 2.0: Highly professional, easy to scan, consistent formatting and colors; coverage exceptions are visible and the file is obviously ready to share internally.\n- 1.0: Generally usable with minor formatting or clarity issues that slightly slow scanning, but acceptable for internal use.\n- 0.0: Difficult to read or inconsistent color usage; confusing layout or missing basic formatting for practical use.\n\nJudge only presentation/clarity here, not the factual correctness of dates or assignments (already covered in Stage 2).", "expectation": "A clean, consistent calendar workbook that an internal audience can scan quickly, with color-coding matching the legend and visible notes for exception days."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "f5d428fd-b38e-41f0-8783-35423dab80f6", "rubric": {"category_name": "Luxury Yacht Itinerary (Bahamas) \u2014 Concierge Deliverable", "rationale": "This rubric enforces a self-documenting, verifiable two-page PDF itinerary suitable for an ultra-high-net-worth family. Stage 1 (LLM judge) strictly mandates the output shape (PDF, two pages, 7 clearly delimited days, image per day, credits, and references). Stage 2 (code rules) verifies correctness elements enabled by the shape: presence of all 7 days, image credit compliance with royalty-free platforms, references to credible sources, and that each day mentions both activities and dining. Stage 3 (LLM judge) assesses professional quality, luxury suitability, clarity, and presentation for the target audience.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Format and Structural Gate", "description": "MANDATORY shape enforcement for a two-page, image-rich PDF itinerary with 7 daily sections, image credits, and references.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.2, "rules": [{"type": "llm_judge", "name": "Two-Page PDF with Required Sections and Per-Day Elements", "description": "Check that the candidate output is a professionally formatted, exactly two-page PDF travel itinerary with the required sections and elements per day.", "weight": 4.0, "judge_prompt": "You are validating ONLY the presence and structure of the deliverable, not the quality of the content. Inspect the candidate output as rendered.\n\nOutput must be a concise, exactly two-page PDF itinerary for a 7-day family yacht trip in the Bahamas. Check for the following STRUCTURAL elements:\n\nFormat Requirements:\n- File is a PDF (not Word, not Excel, not plain text)\n- Exactly 2 pages (no more, no fewer)\n- Professional formatting with clear headers and readable layout\n\nRequired Global Sections:\n1) Title/Header on page 1 with something like \u201c7-Day Bahamas Yacht Itinerary\u201d and family context (e.g., ages/interests or \u201cfirst-time visit\u201d)\n2) Trip Overview section (may be titled \u201cOverview\u201d or \u201cSummary\u201d), with brief bullets or short paragraph (e.g., yacht base or route overview, relaxation focus)\n3) Seven-Day Itinerary section with clearly labeled daily subsections (e.g., \u201cDay 1 \u2013 Nassau\u201d). Days must be labeled 1 through 7 and be visually separate.\n4) Image Credits (or Photo Credits) section mapping each day\u2019s image to a legitimate royalty-free source (platform name + photographer or URL). There must be 7 entries.\n5) References (or Sources) section listing online sources used (at least 3), preferably including sources like Lonely Planet, Nassau Paradise Island, Bahamas.com, Travel + Leisure, or similar public references.\n\nPer-Day Required Elements (Days 1\u20137):\n- A subtitle naming the destination for that day (e.g., Nassau, Harbour Island, Eleuthera, Staniel Cay, Highbourne Cay, Rose Island/Paradise Island, or other plausible Bahamas destinations)\n- A concise 3\u20134 sentence description that includes: (a) at least one family-friendly water/relaxation activity (e.g., swimming, snorkeling, jet skiing, paddleboarding, fishing, beaches/bathing pools/ocean views), and (b) at least one recommended dining venue by name.\n- One embedded royalty-free photo associated with the day\u2019s destination (not just a placeholder). A visible caption or footnote attribution per image is acceptable if Image Credits section fully maps images to sources.\n- A brief logistics note for that day (e.g., marina/anchorage, short travel time/movement between islands, or similar).\n\nScoring (STRUCTURE ONLY \u2014 do not judge content quality or factual accuracy):\n- 4.0: PDF with exactly 2 pages; all 5 global sections present; all 7 days present and clearly labeled; each day has a 3\u20134 sentence description with at least one activity and one dining venue; each day has an embedded image; Image Credits lists 7 images from legitimate royalty-free platforms with photographer/URL; References lists \u22653 sources.\n- 3.2\u20133.9: Correct format and page count; 7-day structure present; at most 1\u20132 minor misses (e.g., one day with 2 or 5 sentences instead of 3\u20134, one day\u2019s image lacks caption but is mapped in Image Credits, or one day\u2019s logistics line is thin). Image Credits and References sections exist and are mostly complete.\n- 2.0\u20133.1: Two-page PDF is present but several structural requirements are missing or inconsistent (e.g., only 5\u20136 days clearly labeled; images missing for multiple days; Image Credits section incomplete or missing several entries; References fewer than 3). Still recognizable as a 7-day itinerary attempt.\n- 0.0\u20131.9: Not a PDF; wrong page count (not exactly 2 pages); missing the 7-day structure; or largely missing images/credits/references.\n\nReturn a score within the rule weight based on the above. Only check presence/structure, not calculation accuracy or content quality.", "expectation": "A polished, exactly two-page PDF with a clear 7-day structure, per-day images and dining/activity mentions, plus complete Image Credits and References sections."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Code)", "description": "Deterministic checks enabled by Stage 1 shape: days, image credit compliance, credible references, activities and dining mentions.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Seven-Day Structure and Core Sections (Text Parse)", "description": "Verify seven distinct day labels, reasonable order, and presence of itinerary/overview terms and recommended destinations.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document output or wrong type.\"\n        try:\n            text = context.files.read_pdf_text(output.id) if output.file_extension.lower().endswith('.pdf') else context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0, \"Failed to read document text.\"\n        if not text or len(text.strip()) == 0:\n            return 0.0, \"Empty document text.\"\n        t = text\n        # Find day markers in order\n        pattern = re.compile(r\"(?i)\\bday\\s*(?:0*([1-9]))\\b\")\n        matches = list(pattern.finditer(t))\n        day_nums = [int(m.group(1)) for m in matches if int(m.group(1)) <= 7]\n        # Count unique days 1..7 present\n        unique_days = sorted(set(day_nums))\n        days_present = sum(1 for d in range(1,8) if d in unique_days)\n        # Sequential order check for 1..7\n        seq_ok = False\n        try:\n            # Extract the first occurrence for each day 1..7\n            first_pos = []\n            for d in range(1,8):\n                pos = None\n                for m in matches:\n                    if m.group(1) and int(m.group(1)) == d:\n                        pos = m.start()\n                        break\n                if pos is None:\n                    first_pos = []\n                    break\n                first_pos.append(pos)\n            if first_pos and all(first_pos[i] < first_pos[i+1] for i in range(len(first_pos)-1)):\n                seq_ok = True\n        except Exception:\n            seq_ok = False\n        # Presence of itinerary and overview terms\n        has_itinerary = bool(re.search(r\"(?i)\\bitinerary\\b\", t))\n        has_overview = bool(re.search(r\"(?i)\\boverview\\b|\\bsummary\\b\", t))\n        # Recommended destinations coverage\n        dest_terms = [\n            'nassau','harbour island','eleuthera','staniel cay','highbourne cay','rose island','paradise island','exuma','exumas','pig beach'\n        ]\n        found_dests = set()\n        tl = t.lower()\n        for d in dest_terms:\n            if d in tl:\n                found_dests.add(d)\n        # Scoring breakdown\n        score = 0.0\n        # up to 1.1 for days present\n        score += 1.1 * (days_present / 7.0)\n        # 0.2 for sequential order\n        score += 0.2 if seq_ok else 0.0\n        # 0.1 itinerary term\n        score += 0.1 if has_itinerary else 0.0\n        # 0.1 overview term\n        score += 0.1 if has_overview else 0.0\n        # 0.1 for at least 4 recommended destinations mentioned\n        score += 0.1 if len(found_dests) >= 4 else 0.0\n        # Cap at weight\n        score = min(score, 1.5)\n        feedback = f\"Days present: {days_present}/7; Sequential: {seq_ok}; Itinerary: {has_itinerary}; Overview: {has_overview}; Destinations found: {len(found_dests)}\"\n        return score, feedback\n    except Exception as e:\n        return 0.0, f\"Exception: {e}\""}, {"type": "code", "name": "Image Credits Compliance (Royalty-Free Platforms)", "description": "Check for an Image/Photo Credits section with at least 7 entries referencing legitimate royalty-free platforms and/or URLs, ideally mapped to days.", "weight": 1.0, "code": "import re\n\nPLATFORMS = [\n    'unsplash','pexels','pixabay','wikimedia','flickr','stocksnap','burst','freeimages','rawpixel','publicdomainpictures','commons'\n]\n\nSECTION_PAT = re.compile(r\"(?is)(image|photo)\\s+credits?.*?(?:(?:references?|sources?)\\b|\\Z)\")\nURL_PAT = re.compile(r\"https?://\\S+\")\nDAY_PAT = re.compile(r\"(?i)\\bday\\s*[1-7]\\b\")\n\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document output or wrong type.\"\n        try:\n            text = context.files.read_pdf_text(output.id) if output.file_extension.lower().endswith('.pdf') else context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0, \"Failed to read document text.\"\n        if not text:\n            return 0.0, \"No text extracted.\"\n        t = text\n        m = SECTION_PAT.search(t)\n        if not m:\n            return 0.0, \"No Image/Photo Credits section detected.\"\n        sec = m.group(0)\n        # Count entries that include either a known platform mention or a URL\n        tl = sec.lower()\n        platform_hits = 0\n        entries = 0\n        # Heuristic: split by line breaks and bullets\n        lines = [ln.strip() for ln in re.split(r\"[\\n\u2022\\-]+\", sec) if ln.strip()]\n        for ln in lines:\n            has_platform = any(p in ln.lower() for p in PLATFORMS)\n            has_url = bool(URL_PAT.search(ln))\n            if has_platform or has_url:\n                platform_hits += 1\n            # count an entry if line mentions Day or has a URL/platform\n            if DAY_PAT.search(ln) or has_platform or has_url:\n                entries += 1\n        # Score components\n        base = min(platform_hits, 7) / 7.0  # 0..1\n        day_map_bonus = 1.0 if sum(1 for ln in lines if DAY_PAT.search(ln)) >= 5 else 0.0\n        score = (0.8 * base) + (0.2 * day_map_bonus)\n        score = max(0.0, min(score, 1.0))\n        feedback = f\"Credits entries: {entries}; Platform/URL hits: {platform_hits}; Day labels present: {day_map_bonus>=1.0}\"\n        return score, feedback\n    except Exception as e:\n        return 0.0, f\"Exception: {e}\""}, {"type": "code", "name": "References and Source Coverage", "description": "Check for a References/Sources section with \u22653 total references and at least 2 from the specified credible domains.", "weight": 0.8, "code": "import re\n\nREF_SECTION = re.compile(r\"(?is)(references?|sources?)\\b.*?\\Z\")\nURL_PAT = re.compile(r\"https?://\\S+\", re.I)\nDOMAINS = [\n    'lonelyplanet.com','nassauparadiseisland.com','bahamas.com','travelandleisure.com',\n    'cntraveler.com','afar.com','travel.usnews.com','tripadvisor.com','visitthebahamas.com'\n]\n\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document output or wrong type.\"\n        try:\n            text = context.files.read_pdf_text(output.id) if output.file_extension.lower().endswith('.pdf') else context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0, \"Failed to read document text.\"\n        if not text:\n            return 0.0, \"No text extracted.\"\n        t = text\n        m = REF_SECTION.search(t)\n        if not m:\n            return 0.0, \"No References/Sources section detected.\"\n        sec = m.group(0)\n        urls = URL_PAT.findall(sec)\n        total_refs = len(urls)\n        tl = sec.lower()\n        known_hits = 0\n        for d in DOMAINS:\n            if d in tl:\n                known_hits += 1\n        # Score: 0.4 for total >=3; 0.4 for at least 2 known domains\n        total_part = min(total_refs, 3) / 3.0\n        known_part = min(known_hits, 2) / 2.0\n        score = 0.4 * total_part + 0.4 * known_part\n        score = max(0.0, min(score, 0.8))\n        feedback = f\"Total references (URLs): {total_refs}; Known domain hits: {known_hits}\"\n        return score, feedback\n    except Exception as e:\n        return 0.0, f\"Exception: {e}\""}, {"type": "code", "name": "Activities and Dining Presence by Day", "description": "For each day segment, verify at least one activity keyword (swimming/snorkeling/jet ski/paddleboard/fishing/beach/pool/ocean views) and at least one dining mention (dining/restaurant/etc.).", "weight": 0.7, "code": "import re\n\nDAY_PAT = re.compile(r\"(?i)\\bday\\s*([1-7])\\b\")\nACTIVITY_TERMS = [\n    'swim','swimming','snorkel','snorkeling','jet ski','jetski','paddleboard','paddle boarding','paddle-boarding','fishing','beach','pink sand','white sand','bathing pool','ocean view','ocean-view','ocean views','ocean-viewing'\n]\nDINING_TERMS = [\n    'dining','restaurant','restaurants','bistro','cafe','caf\u0000e9','grill','lounge','fine dining','seafood','steakhouse','chef','reservations'\n]\n\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document output or wrong type.\"\n        try:\n            text = context.files.read_pdf_text(output.id) if output.file_extension.lower().endswith('.pdf') else context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0, \"Failed to read document text.\"\n        if not text:\n            return 0.0, \"No text extracted.\"\n        t = text\n        # Locate day segments by positions\n        matches = list(DAY_PAT.finditer(t))\n        if not matches:\n            return 0.0, \"No day labels found.\"\n        segments = []\n        for i, m in enumerate(matches):\n            start = m.end()\n            end = matches[i+1].start() if i+1 < len(matches) else len(t)\n            segments.append((int(m.group(1)), t[start:end]))\n        # Evaluate each day segment for activity and dining\n        ok_days = 0\n        for d, seg in segments:\n            segl = seg.lower()\n            has_act = any(term in segl for term in ACTIVITY_TERMS)\n            has_dine = any(term in segl for term in DINING_TERMS)\n            if has_act and has_dine and 1 <= d <= 7:\n                ok_days += 1\n        score = (ok_days / 7.0) * 0.7\n        feedback = f\"Days with both activity and dining mentions: {ok_days}/7\"\n        return score, feedback\n    except Exception as e:\n        return 0.0, f\"Exception: {e}\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Presentation and Suitability (LLM)", "description": "Holistic professional quality and audience fit for a UHNW family traveling with children.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Quality, Luxury Fit, and Clarity", "description": "Assess whether the itinerary is polished, concise, and appropriate for a UHNW family with children, including readability, logistics clarity, and tasteful image use.", "weight": 2.0, "judge_prompt": "Assess overall presentation quality and audience fit. You may consider content quality here, but focus on professionalism and suitability rather than verifying facts.\n\nEvaluate:\n- Readability and concision: Is it genuinely two pages yet easy to scan, with clear headers, balanced text-to-image ratio, and 3\u20134 sentence day descriptions?\n- Luxury suitability: Does it feel tailored to an UHNW family (e.g., premium dining choices, private experiences, reservations language, concierge tone) without being ostentatious?\n- Family-appropriate balance: Are activities safe and engaging for ages 7 and 9, with gentle/snorkeling-friendly waters, beach time, and rest pacing?\n- Logistics clarity: Are movements between islands plausible and clearly indicated in short notes (e.g., marina/anchorage, short hops), avoiding over-scheduling?\n- Visual cohesion: Do images appear high-quality, on-topic, and well-placed relative to each day\u2019s content?\n\nScoring guidance (0\u20132 within weight):\n- 2.0: Highly polished, concise, and audience-appropriate; clear logistics; excellent visual/layout balance; strong luxury but family-friendly tone.\n- 1.0\u20131.9: Generally strong but with some issues (e.g., uneven pacing, minor layout or tone inconsistencies, thin logistics notes, or average image curation).\n- 0.0\u20130.9: Weak presentation, confusing layout, poor tone fit for UHNW family, or inadequate clarity for family travel.\n\nReturn a score within the rule weight.", "expectation": "A polished, concise, family-appropriate luxury itinerary with clear logistics and tasteful imagery."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "2d06bc0a-89c6-4e89-9417-5ffe725c1bc6", "rubric": {"category_name": "Real Estate LOI (CRE Broker) - Annocium Investors Acquisition", "rationale": "This rubric enforces a self-documenting workflow for drafting a professional Letter of Intent (LOI) for a commercial real estate acquisition. Stage 1 (LLM Gate) mandates a strict, verifiable LOI structure in a document format, enabling automated verification. Stage 2 (Code + LLM) checks correctness of key business terms (price at 6.5% cap, deposits, timelines, escrow/title, PSA/assignment, deliverables, 1031, closing costs, expiration, non-binding, addressing/date). Stage 3 holistically evaluates professional tone, clarity, and suitability for a CRE LOI.", "max_total_score": 10.0, "stages": [{"name": "Stage 1: LOI Format and Structural Completeness Gate", "description": "Gate: Document must be a professional LOI (DOCX or PDF), \u22645 pages, addressed and dated, with required CRE LOI sections and all specified deal terms present at a structural level. Only structure/presence is checked here, not correctness of calculations or legal enforceability.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 2.5, "rules": [{"type": "llm_judge", "name": "Structured LOI Requirements (Gate)", "description": "Check that the output is a professionally formatted LOI document with the required sections and elements so that verification is possible.", "weight": 4.0, "judge_prompt": "You are evaluating whether the candidate output is a properly structured commercial real estate LOI. Review the provided file (rendered as images if necessary) and score ONLY on structure/format and presence of required sections/elements. Do not judge numerical correctness or quality here.\n\nFormat Requirements:\n- Must be a document file (DOCX preferred; PDF acceptable). Not plain text or spreadsheet.\n- LOI-style business letter format with clear section headings and signature blocks.\n- Dated as of July 13, 2025.\n- Addressed to the selling broker: Bob Crobens, HPTR (Downtown Denver office, 457 89th Street, Denver, CO 80202).\n- Total length should be no more than 5 pages.\n\nRequired Parties/Property Elements (presence):\n- Buyer: Annocium Investors.\n- Seller: Denver Services Bank.\n- Property: 48,000-sf multi-tenant office on 4 acres at 536-41 Fraanklyn Ave, Denver, Colorado (be flexible on minor address formatting variations).\n\nRequired Section Headings/Content (be flexible on exact header names but the content must be present):\n1) Purchase Price and Cap Rate (reflecting a 6.5% cap rate; price stated explicitly).\n2) Deposits/Escrow: initial deposit; additional deposit upon feasibility approval; escrow with First American Title; timing for deposits.\n3) Feasibility/Due Diligence: 90-day feasibility period after PSA execution.\n4) Closing/Timing: Closing 90 days after feasibility approval; one-month extension option for an additional $20,000 deposit.\n5) PSA/Assignment: Buyer drafts PSA; Buyer reserves right to assign prior to closing.\n6) Seller Deliverables: customary information (P&L/operating statements, leases, surveys, etc.).\n7) 1031 Exchange Cooperation: seller to cooperate at no cost or burden to seller.\n8) Closing Costs: split as customary in Denver.\n9) Brokerage: acknowledges broker involvement.\n10) Expiration: explicit expiration window (typically 7\u201310 days from delivery).\n11) Non-Binding: states LOI is non-binding except customary binding provisions (e.g., confidentiality, exclusivity if included; do not require those to be present, just non-binding statement).\n12) Signatures: signature blocks for Buyer (and optionally Seller acknowledgment).\n\nScoring (structure/presence only):\n- 4.0: Valid DOCX/PDF; \u22645 pages; addressed and dated correctly; all 12 required content areas present with clear headings/sections; parties and property present.\n- 3.0: Valid DOCX/PDF; \u22645 pages; addressed and dated; parties/property present; missing 1\u20132 of the required content areas OR minor structural omissions.\n- 2.0: Valid DOCX/PDF; \u22645 pages; addressed and dated; parties/property present; missing 3\u20134 required content areas.\n- 1.0: Valid DOCX/PDF but missing several core sections (5+), unclear structure, or not clearly addressed/dated.\n- 0.0: Not DOCX/PDF, or clearly exceeds 5 pages, or not an LOI/letter format at all.\n\nOnly check presence/format, not correctness of numbers or legal sufficiency.", "expectation": "A DOCX (or PDF) LOI, \u22645 pages, addressed to Bob Crobens at HPTR, dated July 13, 2025, with clear sections covering price/cap, deposits/escrow, feasibility, closing/extension, PSA/assignment, seller deliverables, 1031, closing costs, brokerage, expiration, non-binding, and signatures; parties/property explicitly identified."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Term Correctness and Consistency Checks", "description": "Automated verification of the key business terms using code-based rules on extracted document text. Flexible matching and partial credit where appropriate.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Price reflects 6.5% cap and rounding", "description": "Verify LOI states a 6.5% cap rate and a purchase price consistent with a 6.5% cap on the implied NOI from the advertised $9,000,000 at 6% cap (NOI=$540,000), rounded to nearest $100,000 ($8,300,000).", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = None\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n    if not text:\n        return 0.0\n    t = text.lower()\n    # Expected values\n    expected_price_num = 8300000  # $8,300,000\n    # Helpers\n    def has_cap_rate_65(s):\n        return re.search(r\"6\\s*\\.\\s*5\\s*%|6\\s*\\.\\s*5\\s*percent|six\\s*and\\s*one\\s*half\\s*percent\", s) is not None\n    def has_price_8_3m(s):\n        s_nocomma = s.replace(\",\", \"\")\n        patterns = [\n            r\"\\$?\\s*8\\s*300\\s*000\\b\",\n            r\"\\$?\\s*8300000\\b\",\n            r\"\\$?\\s*8\\.?3\\s*(m|mm|million)\\b\",\n        ]\n        for p in patterns:\n            if re.search(p, s_nocomma):\n                return True\n        return False\n    score = 0.0\n    if has_cap_rate_65(t):\n        score += 0.5\n    if has_price_8_3m(t):\n        score += 0.5\n    return min(score, 1.0)\n"}, {"type": "code", "name": "Deposits and Escrow Timing", "description": "Check for initial $100,000 deposit within 5 days of PSA execution, additional $150,000 upon feasibility approval, and escrow with First American Title.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n    if not text:\n        return 0.0\n    t = text.lower()\n    t_nocomma = t.replace(\",\", \"\")\n    # Patterns\n    has_initial_100k = (re.search(r\"(initial|earnest)([^\\n]{0,80})\\$?\\s*100000\\b\", t_nocomma) is not None) or (\"$100,000\" in text or \"100,000\" in text)\n    has_5_days = (re.search(r\"5\\s*(business\\s+)?days?\", t) is not None) or (re.search(r\"five\\s*\\(\\s*5\\s*\\)\\s*days?\", t) is not None)\n    has_additional_150k = (re.search(r\"additional([^\\n]{0,80})\\$?\\s*150000\\b\", t_nocomma) is not None) or (\"150,000\" in text or \"$150,000\" in text)\n    has_feas_approval = (\"feasibility\" in t and (\"approval\" in t or \"approved\" in t))\n    has_first_american = (\"first american\" in t and \"title\" in t)\n\n    score = 0.0\n    if has_initial_100k and has_5_days:\n        score += 0.35\n    if has_additional_150k and has_feas_approval:\n        score += 0.35\n    if has_first_american:\n        score += 0.30\n    return min(score, 1.0)\n"}, {"type": "code", "name": "Feasibility and Closing Timelines", "description": "Verify 90-day feasibility after PSA execution; closing 90 days after feasibility approval; one-month extension option for additional $20,000 deposit.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n    if not text:\n        return 0.0\n    t = text.lower()\n    t_nocomma = t.replace(\",\", \"\")\n\n    # Feasibility 90 days after PSA execution\n    feas_90 = (\"feasibility\" in t and re.search(r\"90\\s*day\", t) is not None and (\"psa\" in t or \"purchase and sale\" in t) and (\"execution\" in t or \"executed\" in t))\n\n    # Closing 90 days after feasibility approval\n    closing_90 = (\"closing\" in t and re.search(r\"90\\s*day\", t) is not None and (\"after\" in t or \"following\" in t) and (\"feasibility\" in t) and (\"approval\" in t or \"approved\" in t))\n\n    # Extension one month for $20,000 deposit\n    has_extension = (re.search(r\"(extend|extension)\", t) is not None and (re.search(r\"one\\s*month|30\\s*day\", t) is not None) and (re.search(r\"\\$?\\s*20\\s*000\\b\", t_nocomma) is not None or \"$20,000\" in text or \"20,000\" in text))\n\n    score = 0.0\n    if feas_90:\n        score += 0.34\n    if closing_90:\n        score += 0.33\n    if has_extension:\n        score += 0.33\n    return min(score, 1.0)\n"}, {"type": "code", "name": "Parties, Property, Brokerage Addressee, and Date", "description": "Verify key identifiers: Buyer (Annocium Investors), Seller (Denver Services Bank), property/address, broker addressee (Bob Crobens, HPTR) and date (July 13, 2025). Partial credit per element.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n    if not text:\n        return 0.0\n    t = text.lower()\n\n    has_buyer = \"annocium investors\" in t\n    has_seller = \"denver services bank\" in t\n\n    # Property/address: be flexible on formatting; look for pieces\n    has_addr_number = \"536-41\" in text or \"536\u201341\" in text or \"536 41\" in text\n    has_street_hint = (\"fraanklyn\" in t) or (\"franklyn\" in t) or (\"franklin\" in t) or (\"ave\" in t and \"denver\" in t)\n    has_city_state = (\"denver\" in t and (\"co\" in t or \"colorado\" in t))\n    has_property = has_addr_number and has_street_hint and has_city_state\n\n    # Broker addressee\n    has_broker_name = (\"bob crobens\" in t)\n    has_hptr = (\"hptr\" in t)\n\n    # Date July 13, 2025 (allow variations like July 13, 2025 or 13 July 2025)\n    has_date = bool(re.search(r\"(july\\s*13,?\\s*2025|13\\s*july\\s*2025)\", t))\n\n    # Score components (5 items): buyer, seller, property, broker addressee (name+HPTR), date\n    score = 0.0\n    score += 0.2 if has_buyer else 0.0\n    score += 0.2 if has_seller else 0.0\n    score += 0.2 if has_property else 0.0\n    score += 0.2 if (has_broker_name and has_hptr) else 0.0\n    score += 0.2 if has_date else 0.0\n    return min(score, 1.0)\n"}, {"type": "code", "name": "Deliverables, Assignment, 1031, Costs, Expiration, Non-Binding", "description": "Verify presence of: Seller deliverables (P&L, leases, surveys, etc.), assignment right prior to closing, 1031 cooperation at no cost/burden, closing costs customary in Denver, LOI expiration within 7\u201310 days, and non-binding statement.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n    if not text:\n        return 0.0\n    t = text.lower()\n\n    # Seller deliverables: look for any of these items together with seller providing\n    has_deliverables = (\"seller\" in t and (\"provide\" in t or \"deliver\" in t) and (\"p&l\" in t or \"profit and loss\" in t or \"operating statements\" in t or \"leases\" in t or \"surveys\" in t))\n\n    # Assignment right prior to closing\n    has_assignment = (\"assign\" in t and (\"psa\" in t or \"purchase and sale\" in t) and (\"prior to closing\" in t or \"before closing\" in t))\n\n    # 1031 cooperation at no cost/burden to seller\n    has_1031 = (\"1031\" in t and (\"cooperate\" in t or \"accommodate\" in t) and (\"no cost\" in t or \"no expense\" in t or \"no burden\" in t))\n\n    # Closing costs split customary in Denver\n    has_costs = (\"closing costs\" in t and (\"split\" in t or \"shared\" in t or \"allocated\" in t) and (\"customary\" in t) and (\"denver\" in t or \"colorado\" in t))\n\n    # Expiration within 7\u201310 days\n    has_expire_word = (\"expire\" in t or \"expiration\" in t or \"valid until\" in t)\n    digit_days = re.search(r\"\\b(7|8|9|10)\\b\\s*(business\\s+)?days?\", t)\n    word_days = re.search(r\"\\b(seven|eight|nine|ten)\\b\\s*(business\\s+)?days?\", t)\n    has_expiration = has_expire_word and (digit_days or word_days)\n\n    # Non-binding\n    has_nonbinding = (\"non-binding\" in t or \"nonbinding\" in t)\n\n    # 6 components -> up to 1.0\n    score = 0.0\n    score += (1/6) if has_deliverables else 0.0\n    score += (1/6) if has_assignment else 0.0\n    score += (1/6) if has_1031 else 0.0\n    score += (1/6) if has_costs else 0.0\n    score += (1/6) if has_expiration else 0.0\n    score += (1/6) if has_nonbinding else 0.0\n    return min(score, 1.0)\n"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3: Professional Quality Assessment", "description": "Holistic evaluation of professionalism, clarity, organization, and appropriateness for a CRE LOI.", "is_required": false, "max_points": 1.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Quality and Tone", "description": "Assess presentation quality, clarity, organization, tone, and appropriateness for a commercial real estate LOI.", "weight": 1.0, "judge_prompt": "Evaluate the LOI strictly for professional quality and appropriateness (not structure or numerical correctness already checked elsewhere). Consider:\n- Professional tone consistent with a commercial real estate broker.\n- Clear organization with logical headings and readable layout.\n- Concise, unambiguous statements of business terms suitable for a non-binding LOI.\n- Proper address block, salutations, and signature blocks consistent with business letter conventions.\n- Grammar, spelling, and formatting consistency.\nScoring:\n- 1.0: Highly professional, polished, and ready to send.\n- 0.7: Generally professional with minor issues.\n- 0.4: Adequate but several issues with clarity/formatting.\n- 0.2: Noticeable problems in tone/structure/grammar.\n- 0.0: Poorly written or inappropriate for a CRE LOI.", "expectation": "A concise, professionally written LOI with clear headings, clean formatting, and appropriate business tone suitable for a brokered CRE acquisition."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "46bc7238-3501-4839-b989-e2bd47853676", "rubric": {"category_name": "QSR Tenant Outreach Playbook (Real Estate Brokers)", "rationale": "Pattern B (Document). The task requires a 5\u20138 page PDF/DOCX playbook with specific sections, visuals, and a one-page flyer template. Stage 1 uses an LLM judge to strictly enforce the required document shape and structure (pages, sections, presence of images per page). Stage 2 mixes code and LLM checks to verify grounding to the property, completeness of target categories and channels, and presence of contact/CTA, plus QSR-tailored scripts. Stage 3 assesses professional quality, repeatability, and strategic value for the Miami QSR market.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Format and Structural Gate (PDF/DOCX)", "description": "LLM-only gate validating file format, page count (5\u20138), presence of free stock photo on every page, and all required sections including a one-page flyer template.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Document Format and Structure Requirements", "description": "Check the output is a 5\u20138 page professional PDF/DOCX playbook with all required sections and images on each page.", "weight": 4.0, "judge_prompt": "You are evaluating whether the submitted output satisfies strict STRUCTURE requirements for a professional playbook. Only assess presence/format, not content quality or correctness.\n\nCheck the following:\n\nFormat Requirements:\n- The output must be a PDF or DOCX document (not Excel/CSV/plain text).\n- Page count must be between 5 and 8 pages (inclusive).\n- Each page should include at least one free stock photo or clear placeholder image (e.g., a shopping center, QSR, map/photo placeholders). Be flexible with image recognition; an image, graphic, or photo placeholder per page qualifies.\n\nRequired Sections (headers can vary but must clearly cover these topics):\n1) Cover Page with stock photo of a shopping center.\n2) Executive Summary & Property Highlights (include property name/address and key selling points such as 5,000 SF end cap, visibility, Publix shadow-anchor).\n3) Overview of target QSR tenant categories (e.g., fast casual, coffee/breakfast, pizza, subs, chicken/wings, smoothies/health). A bulleted or tabular breakdown counts.\n4) Sample cold call and email scripts tailored to QSR prospects (both modalities visible).\n5) Outreach cadence and follow-up strategy using multiple channels (email, call, LinkedIn, site visit) with an explicit sequence.\n6) One-page flyer template example for prospective tenants (containing property overview, highlights, and contact info) \u2014 this should clearly appear as a single-page template.\n7) Next Steps section with clear actions for the leasing team.\n\nScoring (STRUCTURE ONLY):\n- 1.0: Valid PDF/DOCX; 5\u20138 pages; image on every page; all 7 required sections clearly present, including a one-page flyer template.\n- 0.85: Valid PDF/DOCX; 5\u20138 pages; image on most pages (allow 1 page miss) AND all required sections present.\n- 0.70: Valid format; 5\u20138 pages; images mostly present; missing exactly one required section OR flyer template not clearly isolated as one page.\n- 0.40: Valid format but page count outside 5\u20138 OR multiple missing images; otherwise shows at least 3 core sections (Cover, Exec Summary/Highlights, Scripts, Cadence) recognizable.\n- 0.20: Valid format but only 1\u20132 relevant sections OR clearly under-structured (e.g., a short memo, no images).\n- 0.00: Not PDF/DOCX OR no readable playbook structure.\n\nImportant:\n- Be flexible with exact header names; judge by intent and layout.\n- Do not judge the accuracy of claims or market specifics here.\n- Return a single score in [0, 1].", "expectation": "A 5\u20138 page PDF/DOCX playbook with images on every page and all required sections, including a clearly isolated one-page flyer template."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification: Grounding and Completeness", "description": "Code and LLM checks to verify property grounding, target category coverage, outreach channels/cadence, contact/CTA presence, and QSR-specific tailoring of scripts.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Property-Specific Grounding", "description": "Verify the document text references the address/location, 5,000 SF size, Publix shadow-anchor, and QSR focus.", "weight": 1.0, "code": "import re\n\ndef _read_doc_text(context, output):\n    text = ''\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = ''\n    return text or ''\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = _read_doc_text(context, output)\n    if not text:\n        return 0.0\n    tl = text.lower()\n\n    score = 0.0\n    # Address (flexible Rd/Road)\n    addr_match = re.search(r\"123\\s+dade\\s+county\\s+r(?:d|oad)\\b\", tl)\n    if addr_match:\n        score += 0.3\n    # City\n    if 'miami' in tl:\n        score += 0.2\n    # State\n    if ' florida' in tl or ' miami, fl' in tl or ' fl ' in tl or ' fl\\n' in tl:\n        score += 0.1\n    # Size 5,000 SF (various formats)\n    size_patterns = [r\"5,?000\\s*sf\", r\"5\\s*\\.\\s*0\\s*k\\s*sf\", r\"5,?000\\s*sq\\.?\\s*ft\", r\"5000\\s*sf\"]\n    if any(re.search(p, tl) for p in size_patterns):\n        score += 0.2\n    # Publix\n    if 'publix' in tl:\n        score += 0.1\n    # QSR focus\n    if 'qsr' in tl or 'quick service restaurant' in tl:\n        score += 0.1\n\n    # Cap at 1.0\n    return min(score, 1.0)\n"}, {"type": "code", "name": "Target QSR Categories Coverage", "description": "Check the playbook mentions the requested QSR categories: fast casual; coffee/breakfast; pizza; subs; chicken/wings; smoothies/health.", "weight": 1.0, "code": "import re\n\ndef _read_doc_text(context, output):\n    text = ''\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = ''\n    return text or ''\n\nCATS = [\n    r\"fast\\s*casual\",\n    r\"coffee|breakfast\",\n    r\"pizza\",\n    r\"subs?|sandwich(?:es)?\",\n    r\"chicken|wings\",\n    r\"smoothies?|health\"\n]\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = _read_doc_text(context, output)\n    if not text:\n        return 0.0\n    tl = text.lower()\n\n    hits = 0\n    for pat in CATS:\n        if re.search(pat, tl):\n            hits += 1\n    coverage = hits / len(CATS)\n    return max(0.0, min(1.0, coverage))\n"}, {"type": "code", "name": "Outreach Cadence and Channels Mentioned", "description": "Verify multi-channel outreach (email, call/phone, LinkedIn, site visit/tour) and signs of a sequenced cadence (follow-ups, day/week steps).", "weight": 0.8, "code": "import re\n\ndef _read_doc_text(context, output):\n    text = ''\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = ''\n    return text or ''\n\nCHANNEL_PATTERNS = [\n    r\"email\",\n    r\"call|phone\",\n    r\"linkedin\",\n    r\"site\\s*visit|tour|on\\s*site|onsite\"\n]\nCADENCE_SIGNS = [\n    r\"follow[-\\s]?up\",\n    r\"cadence|sequence\",\n    r\"day\\s*\\d\",\n    r\"week\\s*\\d\",\n    r\"touch(es)?\",\n    r\"step\\s*\\d\"\n]\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = _read_doc_text(context, output)\n    if not text:\n        return 0.0\n    tl = text.lower()\n\n    channel_hits = 0\n    for pat in CHANNEL_PATTERNS:\n        if re.search(pat, tl):\n            channel_hits += 1\n    channel_score = channel_hits / len(CHANNEL_PATTERNS)\n\n    cadence_hit = any(re.search(p, tl) for p in CADENCE_SIGNS)\n    cadence_score = 1.0 if cadence_hit else 0.0\n\n    # Average: equal weight channels and cadence\n    score = 0.5 * channel_score + 0.5 * cadence_score\n    return max(0.0, min(1.0, score))\n"}, {"type": "code", "name": "CTA and Contact Information Presence", "description": "Check for clear Next Steps/CTA and discoverable contact info (email or phone) likely to appear on the flyer/template.", "weight": 0.6, "code": "import re\n\ndef _read_doc_text(context, output):\n    text = ''\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = ''\n    return text or ''\n\nEMAIL_RE = re.compile(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\")\nPHONE_RE = re.compile(r\"(\\+?1[\\s\\-.]?)?(\\(?\\d{3}\\)?[\\s\\-.]?\\d{3}[\\s\\-.]?\\d{4})\")\nCTA_WORDS = [\"next steps\", \"schedule\", \"book\", \"call us\", \"contact\", \"tour\", \"site visit\", \"submit loi\", \"reach out\"]\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = _read_doc_text(context, output)\n    if not text:\n        return 0.0\n    tl = text.lower()\n\n    cta_present = any(w in tl for w in CTA_WORDS)\n    contact_present = bool(EMAIL_RE.search(text) or PHONE_RE.search(text))\n\n    score = 0.0\n    if cta_present:\n        score += 0.5\n    if contact_present:\n        score += 0.5\n    return score\n"}, {"type": "llm_judge", "name": "QSR-Tailored Scripts and Objection Handling", "description": "Check that cold call and email scripts are clearly tailored to QSR prospects, reflect property highlights, and handle common QSR objections.", "weight": 0.6, "judge_prompt": "Evaluate the scripts ONLY for QSR specificity and practical utility (not prose quality). Look for:\n- Clearly labeled cold call AND email scripts for QSR.\n- Use of property highlights relevant to QSR: 5,000 SF end cap, high visibility, Publix shadow-anchor, Miami location.\n- QSR operational considerations: drive-thru potential, hood/grease trap/venting, power/gas, parking/stacking, pickup/delivery flow, patio seating, TI timeline.\n- Handling of common QSR objections (timing, rent, buildout, drive-thru feasibility, exclusives) with succinct counters and next-step asks (tour/LOI/info request).\n\nScoring:\n- 1.0: Both scripts present, clearly QSR-tailored, reference property strengths, include 2+ operational considerations, and include objection handling with a clear CTA.\n- 0.7: Both scripts present and QSR-specific but missing either operational details or objection handling.\n- 0.4: Scripts present but largely generic (little QSR specificity) or only one modality present.\n- 0.0: Scripts absent or entirely generic.\n\nReturn a single score in [0,1].", "expectation": "Scripts explicitly oriented to QSR decision drivers, with objections and CTAs."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Strategic Value", "description": "LLM assessment of professional presentation, repeatability/systemization, and strategic localization for Miami QSR leasing.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Repeatable System", "description": "Assess professional polish and whether the playbook is a reusable system for this and future vacancies.", "weight": 1.2, "judge_prompt": "Assess overall professionalism and systemization (not raw structure already checked in Stage 1). Consider:\n- Clear, skimmable layout with consistent headers, bullets, and readable visuals.\n- Reusable checklists/templates (e.g., prospecting checklist, CRM fields, qualification criteria, call/email templates) usable across properties.\n- Clear ownership and workflow (who does what, when) and metrics to track (touches per week, response rates, meetings booked, LOIs). \n- Practicality: instructions a junior agent could follow without further guidance.\n\nScoring:\n- 1.0: Highly professional and clearly a repeatable, metric-driven system with checklists/templates and role clarity.\n- 0.7: Professional and mostly reusable; some system elements present but less explicit on metrics or ownership.\n- 0.4: Basic guidance; limited reusability; missing checklists/templates or process clarity.\n- 0.0: Unprofessional or ad hoc; not a usable system.\n\nReturn a single score in [0,1].", "expectation": "A polished, reusable playbook with checklists, templates, roles, and metrics."}, {"type": "llm_judge", "name": "Strategic Localization for Miami QSR Market", "description": "Evaluate strategic value and localization to the Miami, FL QSR context while remaining landlord-focused.", "weight": 0.8, "judge_prompt": "Evaluate whether the playbook demonstrates strategic insight for attracting QSR tenants to a Miami neighborhood center shadow-anchored by Publix. Consider:\n- Localization: references to trade area attributes (traffic, daytime population), Hispanic/Latin food trends or bilingual outreach considerations where appropriate, and competitive set thinking without making unfounded claims.\n- Positioning: how the end-cap visibility, Publix co-tenancy, and drive-by counts translate into value props for different QSR categories.\n- Ethical/appropriate: no legal advice; keeps to marketing/outreach best practices.\n\nScoring:\n- 1.0: Strong strategic localization to Miami QSR dynamics and landlord positioning.\n- 0.7: Some localization and positioning; generally applicable but could be sharper.\n- 0.4: Minimal localization; generic national playbook feel.\n- 0.0: No evident localization or misaligned strategy.\n\nReturn a single score in [0,1].", "expectation": "Miami-aware strategy leveraging Publix shadow-anchor and end-cap visibility for QSR tenants."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "41f6ef59-88c9-4b2c-bcc7-9ceb88422f48", "rubric": {"category_name": "Healthcare Admin: Declined Payments Outreach Kit", "rationale": "Mixed-output task requiring a structured Excel tracker and a DOCX email template. Stage 1 uses an LLM gate to enforce exact file shapes enabling trivial verification. Stage 2 uses code rules to verify correctness and internal consistency given the enforced shape. Stage 3 uses an LLM for holistic quality and usability assessment.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 Gate: Structured Outputs Present and Properly Shaped", "description": "LLM-only gate ensuring BOTH required artifacts exist with the exact structures needed for verification: a 1-tab Excel tracker with specified sheet name/columns and UX aids, plus a DOCX email template containing all required content elements.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Two-Artifact Structured Output Requirement (Excel + DOCX)", "description": "Check that the candidate produced BOTH: (1) a one-tab Excel tracker named exactly 'June 2025 Declined Payments Outreach' with required columns and at least two UX aids (dropdowns, checkboxes, date validation, filters/frozen header), including a 'test user' example row; and (2) a Word (DOCX) email template containing all mandated elements.", "weight": 4.0, "judge_prompt": "You are the Stage 1 gate checking STRUCTURE ONLY (not calculation/content quality). Review all outputs. Score from 0.0 to 4.0 based on whether BOTH artifacts exist and meet the exact structural requirements.\n\nArtifacts to find:\nA) Excel tracker (must be an Excel file):\n- Exactly ONE sheet/tab, named exactly: \"June 2025 Declined Payments Outreach\".\n- Must include a header row with the following columns (flexible to minor naming variations that clearly match the intent):\n  1) Third Decline Date (date field)\n  2) Patient Name\n  3) Date of Birth\n  4) Email\n  5) Subscription Type\n  6) Emailed? (boolean/checkbox)\n  7) Responded? (boolean/checkbox)\n  8) Updated Payment Method? (boolean/checkbox)\n- Data-entry UX aids: At least TWO of the following are visible/obvious:\n  \u2022 Dropdown list for Subscription Type with values Plan A, Plan B, Plan C\n  \u2022 Checkboxes for the three boolean columns (or clear Yes/No data validation)\n  \u2022 Date picker or date validation for Third Decline Date\n  \u2022 Frozen header row or filter toggles for faster entry\n- Includes at least one example \"test user\" row (e.g., name/email indicating test/example or clearly labeled as sample).\n\nB) Email template (must be a Word DOCX document):\n- A copy-paste-ready email text (professional, readable formatting). Subject line present or clearly indicated.\n- Must explicitly state: payment method on file has declined for a third time.\n- Must warn: if it declines a fourth time, the subscription will be canceled AND they will not receive their medication refill.\n- Must provide clear step-by-step instructions to update payment method in portal, in this order: log in \u2192 go to settings \u2192 go to billing \u2192 click update payment method \u2192 click \"save\" after entering the new card.\n- Must request the patient to reply to confirm they updated their payment method.\n\nScoring (STRUCTURE ONLY):\n- 4.0: Both artifacts present in correct formats (Excel + DOCX). Excel has exactly one tab named exactly as required, all 8 required columns, at least two UX aids, and a visible test user row. Email DOCX contains all required elements (third decline notice, fourth-decline cancellation + no refill warning, step-by-step instructions in the specified order, and a request to reply; subject indicated).\n- 3.0: Both artifacts present and in correct formats. Minor structural deviations only (e.g., one column header slightly renamed but clearly equivalent; only one UX aid visible; subject implied but not labeled). Test user row still present.\n- 2.0: One artifact has significant structural deficiencies (e.g., Excel has multiple tabs or wrong tab name, missing 2+ required columns, no UX aids, or no test row; OR the email misses one major required element like the fourth-decline cancellation warning or the step list order).\n- 0.0: Missing one or both artifacts, wrong formats (e.g., not Excel or not DOCX), or grossly non-conforming structure.\n\nImportant: Do NOT judge content quality or correctness; only presence and structure that makes verification possible.", "expectation": "Enforce that both the Excel tracker and DOCX email template exist and are structurally ready for verification."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Correctness and Internal Consistency Checks", "description": "Deterministic code checks leveraging the mandated shapes: Excel structure and values, and required email content elements present in DOCX.", "is_required": true, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Excel Structure + Test Row Verification", "description": "Verify Excel file exists with a single required sheet name, required columns (fuzzy-matched), and a detectable test user example row.", "weight": 2.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        resources = context.get_all_outputs()\n        excel_res = None\n        for r in resources:\n            if getattr(r, 'is_spreadsheet', False):\n                excel_res = r\n                break\n        if not excel_res:\n            return 0.0, 'No spreadsheet output found.'\n\n        # Open workbook and check sheet names\n        path = context.files.get_path(excel_res.id)\n        try:\n            xls = pd.ExcelFile(path)\n            sheet_names = [str(s) for s in xls.sheet_names]\n        except Exception as e:\n            return 0.0, f'Could not open Excel: {e}'\n\n        required_sheet = 'June 2025 Declined Payments Outreach'\n        single_sheet_ok = len(sheet_names) == 1\n        sheet_name_ok = required_sheet in sheet_names\n\n        # Read required sheet (fallback to first if exact missing to continue fuzzy checks)\n        sheet_to_read = required_sheet if sheet_name_ok else sheet_names[0]\n        try:\n            df = pd.read_excel(path, sheet_name=sheet_to_read)\n        except Exception as e:\n            return 0.0, f'Could not read sheet: {e}'\n\n        def norm(s):\n            return re.sub(r'[^a-z0-9]+', ' ', str(s).lower()).strip()\n\n        cols_norm = [norm(c) for c in df.columns]\n\n        # Expected columns with fuzzy synonym sets\n        expected = {\n            'third_decline_date': ['third decline date','third payment decline date','third declined date','3rd decline date','third failed payment date','third payment declined date','3rd payment declined date','third decline'],\n            'patient_name': ['patient name','name','full name'],\n            'dob': ['date of birth','dob','birth date','birthdate'],\n            'email': ['email','email address','e mail'],\n            'subscription_type': ['subscription type','plan','plan type','plan name','subscription plan'],\n            'emailed': ['emailed','emailed?','email sent','notified','outreach sent','emailed status'],\n            'responded': ['responded','responded?','response received','replied','replied?','reply received'],\n            'updated_payment': ['updated payment method','updated payment method?','payment updated','payment method updated','updated on file','updated card','updated payment','payment updated?']\n        }\n\n        def find_match(syns):\n            for i, c in enumerate(cols_norm):\n                for s in syns:\n                    if norm(s) in c:\n                        return i\n            return None\n\n        matches = {k: find_match(v) for k, v in expected.items()}\n        present_count = sum(1 for v in matches.values() if v is not None)\n        cols_score = present_count / len(expected)\n\n        # Detect test user/example row (heuristic)\n        test_row_found = False\n        try:\n            name_idx = matches.get('patient_name')\n            email_idx = matches.get('email')\n            if name_idx is not None or email_idx is not None:\n                name_series = df.iloc[:, name_idx] if name_idx is not None else pd.Series([], dtype=str)\n                email_series = df.iloc[:, email_idx] if email_idx is not None else pd.Series([], dtype=str)\n                pattern = re.compile(r'\\b(test|example|demo|sample)\\b', re.IGNORECASE)\n                if name_idx is not None and name_series.astype(str).str.contains(pattern).any():\n                    test_row_found = True\n                if email_idx is not None and email_series.astype(str).str.contains(pattern).any():\n                    test_row_found = True\n        except Exception:\n            test_row_found = False\n\n        # Aggregate scoring within this rule\n        # 25% sheet checks, 50% columns presence, 25% test row presence\n        sheet_subscore = 0.0\n        if sheet_name_ok:\n            sheet_subscore += 0.6\n        if single_sheet_ok:\n            sheet_subscore += 0.4\n        sheet_subscore = min(sheet_subscore, 1.0)\n\n        test_subscore = 1.0 if test_row_found else 0.0\n\n        total_ratio = 0.25 * sheet_subscore + 0.50 * cols_score + 0.25 * test_subscore\n        total_ratio = max(0.0, min(1.0, total_ratio))\n        return 2 * total_ratio, f'Sheet name ok={sheet_name_ok}, single sheet={single_sheet_ok}, cols_present={present_count}/8, test_row={test_row_found}'\n    except Exception as e:\n        return 0.0, f'Exception in rule: {e}'"}, {"type": "code", "name": "Excel Values + Validation Proxies", "description": "Check allowed values for Subscription Type, boolean fields normalization, and parsable dates for Third Decline Date. Uses heuristics as proxies for dropdowns/checkboxes/date validation.", "weight": 1.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        # Locate Excel resource\n        resources = context.get_all_outputs()\n        excel_res = None\n        for r in resources:\n            if getattr(r, 'is_spreadsheet', False):\n                excel_res = r\n                break\n        if not excel_res:\n            return 0.0, 'No spreadsheet output found.'\n\n        path = context.files.get_path(excel_res.id)\n        xls = pd.ExcelFile(path)\n        sheet_names = xls.sheet_names\n        required_sheet = 'June 2025 Declined Payments Outreach'\n        sheet_to_read = required_sheet if required_sheet in sheet_names else sheet_names[0]\n        df = pd.read_excel(path, sheet_name=sheet_to_read)\n\n        def norm(s):\n            return re.sub(r'[^a-z0-9]+', ' ', str(s).lower()).strip()\n\n        cols_norm = [norm(c) for c in df.columns]\n        def find_col(syns):\n            for i, c in enumerate(cols_norm):\n                for s in syns:\n                    if norm(s) in c:\n                        return i\n            return None\n\n        idx_date = find_col(['third decline date','third payment decline date','3rd decline date','third payment declined date'])\n        idx_plan = find_col(['subscription type','plan','plan type','plan name'])\n        idx_em = find_col(['emailed','emailed?','email sent','notified','outreach sent'])\n        idx_resp = find_col(['responded','responded?','response received','replied','replied?'])\n        idx_upd = find_col(['updated payment method','payment updated','payment method updated','updated card','updated payment'])\n\n        checks = []\n        feedback_parts = []\n\n        # Subscription Type values\n        if idx_plan is not None:\n            plans = df.iloc[:, idx_plan].dropna().astype(str).str.lower().str.strip()\n            allowed_tokens = {'plan a','plan b','plan c','a','b','c'}\n            # consider entries like 'plan a - $150'\n            def plan_ok(v):\n                v = norm(v)\n                return any(tok in v for tok in ['plan a','plan b','plan c']) or v in {'a','b','c'}\n            if len(plans) == 0:\n                plan_score = 0.5  # empty but column exists\n            else:\n                valid = plans.apply(plan_ok)\n                plan_score = valid.mean()\n            checks.append(plan_score)\n            feedback_parts.append(f'Plan validity={plan_score:.2f}')\n        else:\n            checks.append(0.0)\n            feedback_parts.append('Plan column missing')\n\n        # Boolean columns: accept yes/no/true/false/y/n/1/0/checked/unchecked/blank\n        def bool_ok(v):\n            if pd.isna(v):\n                return True  # allow blank as not yet set\n            s = str(v).strip().lower()\n            return s in {'yes','no','true','false','y','n','1','0','checked','unchecked'} or s in {'',}\n\n        for label, idx in [('Emailed', idx_em), ('Responded', idx_resp), ('Updated', idx_upd)]:\n            if idx is not None:\n                series = df.iloc[:, idx]\n                if len(series) == 0:\n                    ok_ratio = 0.5\n                else:\n                    ok_ratio = series.apply(bool_ok).mean()\n                checks.append(ok_ratio)\n                feedback_parts.append(f'{label} validity={ok_ratio:.2f}')\n            else:\n                checks.append(0.0)\n                feedback_parts.append(f'{label} column missing')\n\n        # Date parse check for Third Decline Date\n        if idx_date is not None:\n            s = df.iloc[:, idx_date]\n            if len(s) == 0:\n                date_ratio = 0.5\n            else:\n                parsed = pd.to_datetime(s, errors='coerce', dayfirst=False)\n                # Only count non-empty as required to be parseable\n                mask_nonempty = ~s.isna() & (s.astype(str).str.strip() != '')\n                if mask_nonempty.any():\n                    date_ratio = parsed[mask_nonempty].notna().mean()\n                else:\n                    date_ratio = 0.5\n            checks.append(date_ratio)\n            feedback_parts.append(f'Date parse={date_ratio:.2f}')\n        else:\n            checks.append(0.0)\n            feedback_parts.append('Third Decline Date column missing')\n\n        # Aggregate\n        if len(checks) == 0:\n            return 0.0, 'No checks performed.'\n        ratio = float(np.mean(checks))\n        ratio = max(0.0, min(1.0, ratio))\n        return 1.5 * ratio, '; '.join(feedback_parts)\n    except Exception as e:\n        return 0.0, f'Exception in rule: {e}'"}, {"type": "code", "name": "Email Template: Required Content Elements", "description": "Verify the DOCX email template exists and contains the mandated notices, the ordered step-by-step instructions, and a request to reply.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        resources = context.get_all_outputs()\n        doc_res = None\n        for r in resources:\n            if getattr(r, 'is_document', False):\n                # Prefer DOCX over PDF if multiple\n                name = getattr(r, 'name', '') or ''\n                if str(name).lower().endswith('.docx'):\n                    doc_res = r\n                    break\n                if doc_res is None:\n                    doc_res = r\n        if not doc_res:\n            return 0.0, 'No document output found.'\n\n        # Read text (DOCX preferred; fallback to PDF text if needed)\n        text = ''\n        try:\n            text = context.files.read_docx_text(doc_res.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(doc_res.id)\n            except Exception:\n                return 0.0, 'Could not read document text.'\n        t = text.lower()\n\n        # Required elements\n        # 1) Third decline notice\n        third_decline_ok = ('third' in t and 'declin' in t and ('payment' in t or 'payment method' in t))\n\n        # 2) Fourth decline cancellation AND no refill\n        fourth_cancel_ok = ('fourth' in t and 'cancel' in t and ('subscription' in t))\n        no_refill_ok = (('no refill' in t) or ('medication' in t and 'refill' in t) or ('will not receive' in t and 'refill' in t))\n        cancel_block_ok = fourth_cancel_ok and no_refill_ok\n\n        # 3) Step-by-step instructions in order: login -> settings -> billing -> update payment method -> save\n        def find_idx(words):\n            idx = []\n            for w in words:\n                m = re.search(w, t)\n                idx.append(m.start() if m else -1)\n            return idx\n        order_words = [r'log\\s*in|login', r'settings', r'billing', r'update\\s+payment\\s+method', r'\\bsave\\b']\n        idxs = find_idx(order_words)\n        steps_present = all(i >= 0 for i in idxs)\n        steps_in_order = steps_present and all(idxs[i] < idxs[i+1] for i in range(len(idxs)-1))\n\n        # 4) Request to reply/confirm update\n        reply_ok = ('reply' in t and ('confirm' in t or 'let us know' in t or 'updated' in t))\n\n        # 5) Subject line presence (heuristic)\n        subject_ok = ('subject:' in t) or re.search(r'^subject', text, flags=re.IGNORECASE|re.MULTILINE) is not None\n\n        checks = [third_decline_ok, cancel_block_ok, steps_in_order, reply_ok, subject_ok]\n        ratio = sum(1 for c in checks if c) / len(checks)\n        return 1.5 * ratio, f\"third_decline={third_decline_ok}, cancel_no_refill={cancel_block_ok}, steps_in_order={steps_in_order}, reply_request={reply_ok}, subject={subject_ok}\"\n    except Exception as e:\n        return 0.0, f'Exception in rule: {e}'"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3: Professional Quality and Usability", "description": "LLM assesses tone, clarity, and usability of both artifacts for real-world clinic workflows and Zendesk macro usage.", "is_required": false, "max_points": 1.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Holistic Quality and Fitness for Purpose", "description": "Professionalism, clarity, empathy, and ease-of-use for the clinic workflow and Zendesk macro creation.", "weight": 1.0, "judge_prompt": "Assess overall quality and usability of BOTH artifacts (Excel tracker + DOCX email). Do not re-check structure already gated in Stage 1. Focus on whether a clinic could immediately use these with minimal edits.\n\nConsider:\n- Email tone and professionalism: clear, empathetic, firm about policy; grammar/spelling; copy/paste ready; includes a concise subject and optional placeholders (e.g., {{Patient First Name}}) suitable for a Zendesk macro.\n- Clarity of instructions: steps numbered/bulleted, easy to follow, minimal ambiguity.\n- Operational usefulness: Excel sheet is easy to use (helpful data validation, checkboxes, filters/frozen header), headers are readable, and the sample row illustrates usage.\n- Consistency with clinical/administrative context: avoids unnecessary PHI, avoids medical advice, focuses on payment and portal steps.\n\nScoring:\n- 1.0: Highly professional and immediately usable; email is macro-ready and empathetic; spreadsheet UX clearly streamlines data entry.\n- 0.7: Generally strong with minor polish issues (e.g., small tone or formatting nitpicks) but usable as-is.\n- 0.4: Usable with moderate revisions needed (tone awkward, steps not optimally formatted, or spreadsheet slightly clunky).\n- 0.0: Poor quality or not usable without major rewrite.", "expectation": "A polished, empathetic email and a streamlined tracker suitable for direct use in Zendesk workflow."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "40a99a31-42d6-4f23-b3ec-8f591afe25b6", "rubric": {"category_name": "Manufacturing \u2014 Industrial Engineering: Robotic CNC Work Cell Safety/Automation Design", "rationale": "Mixed-deliverable task (Excel + PNG + PDF). Stage 1 enforces exact file-and-structure shape so verification is trivial. Stage 2 uses code checks for quantitative/structural correctness leveraging the mandated shape, plus LLM checks for semantic alignment and integration logic. Stage 3 judges overall professional quality and strategic rigor.", "max_total_score": 25.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate", "description": "LLM-only gate verifying presence and precise structure of the three required deliverables: Excel hardware table + IO mapping, PNG layout diagram, and PDF report with required sections. If this gate fails, the entire category receives 0.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Excel Workbook Shape \u2014 Hardware Selection + IO Mapping", "description": "Verify there is an Excel workbook containing a hardware selection table and IO mapping sheet with the specified structure. Only check presence/shape, not correctness.", "weight": 2.0, "judge_prompt": "You are evaluating the candidate outputs for required file presence and STRUCTURE ONLY (no content correctness). Look across all outputs. Confirm there is a valid spreadsheet (Excel .xlsx preferred; .csv acceptable only for Sheet1 substitute) that meets the following structural requirements.\n\nWorkbook Requirements:\n- Must include a sheet clearly dedicated to hardware selections. Accept sheet names like: \"Hardware Selection\", \"Hardware List\", \"BOM\", or \"Device Catalog\" (flexible matching).\n- Must include a separate sheet dedicated to IO mapping. Accept sheet names like: \"IO Mapping\", \"I/O Map\", \"Signal Map\", or similar.\n\nSheet: Hardware Selection (examples: \"Hardware Selection\", \"BOM\") \u2014 must contain a clearly tabular layout with columns equivalent to:\n- Hardware Type (e.g., LIDAR, Camera, AMR, Pressure Mat, Gateway/PLC)\n- Make/Model\n- Interface(s) and/or Protocol(s) (e.g., Ethernet/IP, Modbus TCP, IO-Link)\n- Compatibility Notes (free text about robot/CNC/drive IO compatibility)\n- Quantity\n- Zone/Location (e.g., Zone 1\u20136 between CNCs, West Front, Robot-mounted, per-CNC cameras, etc.)\n- Estimated Cost (USD)\n- Safety Standard/Compliance (e.g., ISO 13849, ISO 10218, IEC 61496)\n\nNotes:\n- Be flexible on exact column names but the meanings must be clearly present.\n- The table must include entries for: LIDAR devices, cameras, an AMR, and pressure-sensitive mats.\n\nSheet: IO Mapping (examples: \"IO Mapping\", \"Signal Map\") \u2014 must include a tabular layout with columns equivalent to:\n- Device ID/Name\n- Device Type\n- Protocol (e.g., Ethernet/IP, Modbus TCP, IO-Link)\n- Network Address/Port (or similar addressing)\n- I/O Points Used (or Signal List)\n- Robot IO Mapping (how it ties to robot controller/IO layer)\n- CNC IO Mapping (how it ties to CNC machine IO)\n- Gateway/PLC (if used)\n- Interlocks/Events or Safety Category/PL\n\nScoring (STRUCTURE ONLY):\n- 2.0: Excel workbook present with BOTH sheets, each having clearly labeled columns covering the items above (flexible naming OK). Hardware sheet visibly includes rows for LIDAR, Cameras, AMR, and Pressure Mats.\n- 1.0: Excel workbook present with ONE of the two sheets complete and the other partially present (e.g., IO Mapping exists but missing many key columns), or Hardware sheet present but missing one of the required device categories.\n- 0.0: No valid Excel workbook OR missing both required sheets OR hardware sheet not recognizable.\n\nDo not evaluate math, counts, model choices, or protocol correctness here \u2014 only the presence of the required structure.", "expectation": "One Excel file with two sheets: Hardware Selection and IO Mapping, each with the specified tabular columns; hardware sheet includes LIDAR, Cameras, AMR, Pressure Mats."}, {"type": "llm_judge", "name": "PNG System Layout Diagram Shape", "description": "Verify there is a PNG diagram with required labeled elements and simple, readable layout. Only check presence/shape, not engineering correctness.", "weight": 2.0, "judge_prompt": "Check all outputs for a PNG diagram image. Evaluate STRUCTURE ONLY. The diagram must be simple, labeled, and depict the CNC work cell layout with the following elements (flexible labeling allowed):\n- A robot on a rail (clearly indicated the rail path)\n- Six CNC machines (labelled or numbered)\n- LIDAR zones: six static zones (between CNCs and one at front west end) identified/labelled, plus a robot-mounted LIDAR indicated\n- Pressure-sensitive mats in front of each CNC\n- Cameras positioned to monitor each CNC (at least six placements)\n- Barriers/screens shown on the South and East sides (or annotated as protected sides)\n- Clear labeling (legends or callouts) for key elements\n\nScoring:\n- 2.0: PNG present and all listed elements are clearly labeled/indicated.\n- 1.0: PNG present and most elements are labeled, but 1\u20132 minor elements missing or unlabeled (e.g., barrier sides not labeled, or robot-mounted LIDAR missing label).\n- 0.0: No PNG diagram OR diagram lacks multiple required elements such that it\u2019s not verifiably complete.\n\nDo not judge exact dimensions, scaling, or engineering accuracy \u2014 only that a labeled, complete diagram exists.", "expectation": "A single PNG with rail, six CNCs, seven LIDARs (six zones + robot-mounted), six pressure mats, six+ cameras, and barrier sides labeled."}, {"type": "llm_judge", "name": "PDF Report Structure", "description": "Verify there is a PDF report with the required sections and basic formatting. Only check presence/shape, not content quality.", "weight": 2.0, "judge_prompt": "Check all outputs for a PDF report. Assess FORMAT and SECTION PRESENCE ONLY.\n\nRequirements:\n- File is a PDF (not DOCX/MD)\n- At least 2 pages\n- Professional formatting with visible section headers\n- Required sections (flexibly named but recognizable):\n  1) Overview\n  2) Hardware Selection Summary\n  3) Integration Strategy (must discuss software separation and IO/protocol approach at a high level)\n  4) Installation & Layout\n  5) Conclusion\n\nScoring:\n- 2.0: Valid PDF, >=2 pages, and all 5 sections present as clear headers.\n- 1.0: Valid PDF with 4 of 5 sections present or sections present but poorly demarcated.\n- 0.0: Not a PDF, or fewer than 2 pages, or less than 4 recognizable sections.\n\nDo not judge technical correctness here \u2014 only the presence of the required sections and formatting.", "expectation": "A 2+ page PDF with clearly labeled sections: Overview, Hardware Selection Summary, Integration Strategy, Installation & Layout, Conclusion."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification of Correctness and Consistency", "description": "Code rules verify quantitative/structural correctness from the Excel workbook. LLM rules verify semantic alignment across outputs and integration logic soundness. Not a gate; partial credit allowed.", "is_required": false, "max_points": 15.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Hardware Table Coverage and Cost Fill Rate", "description": "Checks that the Hardware sheet has key columns and that most rows have valid numeric Estimated Cost. Flexible column-name matching.", "weight": 2.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef _find_spreadsheet(context):\n    for r in context.get_all_outputs():\n        if getattr(r, 'is_spreadsheet', False):\n            return r\n    return None\n\ndef _read_excel_any(context, res):\n    try:\n        path = context.files.get_path(res.id)\n        xls = pd.ExcelFile(path)\n        return xls\n    except Exception:\n        return None\n\ndef _find_sheet(xls, candidates):\n    # return best-matching sheet name or None\n    names = [s for s in xls.sheet_names]\n    lc = [s.lower() for s in names]\n    idx = None\n    best_score = -1\n    for i, name in enumerate(lc):\n        score = 0\n        for c in candidates:\n            if c in name:\n                score += 1\n        if score > best_score:\n            best_score = score\n            idx = i\n    return None if best_score <= 0 else names[idx]\n\ndef _normalize_columns(df):\n    mapping = {}\n    for c in df.columns:\n        cl = str(c).strip().lower()\n        key = cl\n        if cl in ['type','device type','category','hardware category']:\n            key = 'hardware type'\n        elif 'hardware' in cl and 'type' in cl:\n            key = 'hardware type'\n        elif 'make/model' in cl or ('model' in cl and 'make' in cl) or cl in ['model','make','manufacturer','vendor','part number','part #','pn']:\n            key = 'make/model'\n        elif 'interface' in cl or 'port' in cl or 'protocol' in cl or 'communication' in cl:\n            key = 'interfaces/protocols'\n        elif 'compat' in cl or 'note' in cl or 'remarks' in cl:\n            key = 'compatibility notes'\n        elif 'qty' in cl or 'quantity' in cl or 'count' in cl:\n            key = 'quantity'\n        elif 'zone' in cl or 'location' in cl or 'position' in cl or 'placement' in cl:\n            key = 'zone/location'\n        elif 'cost' in cl or 'price' in cl or 'usd' in cl or 'estimate' in cl:\n            key = 'estimated cost'\n        elif 'safety' in cl or 'compliance' in cl or 'standard' in cl:\n            key = 'safety standard'\n        elif 'payload' in cl and 'kg' in cl:\n            key = 'payload (kg)'\n        mapping[c] = key\n    df2 = df.copy()\n    df2.columns = [mapping[c] for c in df.columns]\n    return df2\n\ndef _to_number(x):\n    if pd.isna(x):\n        return np.nan\n    s = str(x)\n    s = re.sub(r'[^0-9.\\-]', '', s)\n    try:\n        return float(s)\n    except Exception:\n        return np.nan\n\ndef evaluate(workflow, context):\n    res = _find_spreadsheet(context)\n    if not res:\n        return 0.0\n    xls = _read_excel_any(context, res)\n    if xls is None:\n        return 0.0\n\n    # Find hardware sheet\n    hardware_sheet = _find_sheet(xls, ['hardware','selection','bom','device','catalog'])\n    if not hardware_sheet:\n        return 0.0\n\n    try:\n        df = pd.read_excel(xls, sheet_name=hardware_sheet)\n    except Exception:\n        return 0.0\n    if df.empty:\n        return 0.0\n\n    df = _normalize_columns(df)\n    expected_cols = ['hardware type','make/model','interfaces/protocols','compatibility notes','quantity','zone/location','estimated cost','safety standard']\n    present = set(df.columns)\n    col_hits = sum(1 for c in expected_cols if c in present)\n    col_score = col_hits / max(1, len(expected_cols))  # 0..1\n\n    # Cost fill rate\n    if 'estimated cost' in df.columns:\n        costs = df['estimated cost'].map(_to_number)\n        nonnull = costs.notna().sum()\n        positive = (costs > 0).sum()\n        fill_rate = 0.5 * (nonnull / max(1, len(costs))) + 0.5 * (positive / max(1, len(costs)))\n    else:\n        fill_rate = 0.0\n\n    score = 0.6 * col_score + 0.4 * fill_rate\n    score = max(0.0, min(1.0, float(score)))\n    return score"}, {"type": "code", "name": "Device Quantities and Constraints", "description": "Verifies minimum quantities and constraints from the Hardware sheet: LIDAR >=7 total (6 static + 1 robot-mounted), Cameras >=6, Pressure Mats >=6, AMR present with payload capacity >= 220 kg (if payload column available).", "weight": 2.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef _find_spreadsheet(context):\n    for r in context.get_all_outputs():\n        if getattr(r, 'is_spreadsheet', False):\n            return r\n    return None\n\ndef _read_excel_any(context, res):\n    try:\n        path = context.files.get_path(res.id)\n        return pd.ExcelFile(path)\n    except Exception:\n        return None\n\ndef _find_sheet(xls, candidates):\n    names = xls.sheet_names\n    best = None\n    best_score = -1\n    for n in names:\n        score = sum(1 for c in candidates if c in n.lower())\n        if score > best_score:\n            best_score = score\n            best = n\n    return best if best_score > 0 else None\n\ndef _norm(df):\n    mapping = {}\n    for c in df.columns:\n        cl = str(c).strip().lower()\n        key = cl\n        if cl in ['type','device type','category'] or ('hardware' in cl and 'type' in cl):\n            key = 'hardware type'\n        elif 'make/model' in cl or 'model' in cl or 'make' in cl or 'manufacturer' in cl:\n            key = 'make/model'\n        elif 'protocol' in cl or 'interface' in cl or 'communication' in cl:\n            key = 'interfaces/protocols'\n        elif 'qty' in cl or 'quantity' in cl or 'count' in cl:\n            key = 'quantity'\n        elif 'zone' in cl or 'location' in cl or 'position' in cl:\n            key = 'zone/location'\n        elif 'payload' in cl:\n            key = 'payload'\n        mapping[c] = key\n    out = df.copy()\n    out.columns = [mapping[c] for c in df.columns]\n    return out\n\ndef _to_num(x):\n    try:\n        s = re.sub(r'[^0-9.\\-]','', str(x))\n        return float(s) if s not in ['', None] else np.nan\n    except Exception:\n        return np.nan\n\n\ndef evaluate(workflow, context):\n    res = _find_spreadsheet(context)\n    if not res:\n        return 0.0\n    xls = _read_excel_any(context, res)\n    if xls is None:\n        return 0.0\n\n    sheet = _find_sheet(xls, ['hardware','selection','bom','device'])\n    if not sheet:\n        return 0.0\n\n    try:\n        df = pd.read_excel(xls, sheet_name=sheet)\n    except Exception:\n        return 0.0\n    if df.empty:\n        return 0.0\n\n    df = _norm(df)\n\n    # quantity helper\n    qty = df['quantity'].map(_to_num) if 'quantity' in df.columns else pd.Series([np.nan]*len(df))\n\n    def has_type(mask_terms):\n        if 'hardware type' in df.columns:\n            ht = df['hardware type'].astype(str).str.lower()\n        else:\n            ht = pd.Series(['']*len(df))\n        # fallback: use make/model or interfaces to detect\n        mm = df['make/model'].astype(str).str.lower() if 'make/model' in df.columns else pd.Series(['']*len(df))\n        ip = df['interfaces/protocols'].astype(str).str.lower() if 'interfaces/protocols' in df.columns else pd.Series(['']*len(df))\n        mask = ht.str.contains('|'.join(mask_terms)) | mm.str.contains('|'.join(mask_terms)) | ip.str.contains('|'.join(mask_terms))\n        return df[mask].copy()\n\n    # LIDAR check\n    lidar_df = has_type(['lidar','safety scanner','scanner'])\n    lidar_qty = qty.loc[lidar_df.index].fillna(0).sum()\n    if 'zone/location' in df.columns and lidar_df.shape[0] > 0 and pd.isna(lidar_qty) or lidar_qty == 0:\n        # fallback: count unique zone/location entries for lidar rows\n        zones = df.loc[lidar_df.index, 'zone/location'].dropna().astype(str).unique()\n        lidar_qty = max(lidar_qty, float(len(zones)))\n    lidar_ok = lidar_qty >= 7\n\n    # Cameras\n    cam_df = has_type(['camera'])\n    cam_qty = qty.loc[cam_df.index].fillna(0).sum()\n    if (pd.isna(cam_qty) or cam_qty == 0) and cam_df.shape[0] > 0:\n        cam_qty = max(cam_qty, float(cam_df.shape[0]))\n    cam_ok = cam_qty >= 6\n\n    # Pressure mats\n    mat_df = has_type(['pressure','mat','safety mat'])\n    mat_qty = qty.loc[mat_df.index].fillna(0).sum()\n    if (pd.isna(mat_qty) or mat_qty == 0) and mat_df.shape[0] > 0:\n        mat_qty = max(mat_qty, float(mat_df.shape[0]))\n    mat_ok = mat_qty >= 6\n\n    # AMR presence and payload\n    amr_df = has_type(['amr','autonomous mobile robot','mobile robot','agv'])\n    amr_present = amr_df.shape[0] > 0\n    payload_ok = False\n    if amr_present:\n        if 'payload' in df.columns:\n            payload_vals = amr_df['payload'].map(_to_num)\n            payload_ok = (payload_vals >= 220).any()\n        else:\n            # if payload column missing, partial credit will reflect this in other rules\n            payload_ok = False\n\n    parts = [1.0 if lidar_ok else 0.0,\n             1.0 if cam_ok else 0.0,\n             1.0 if mat_ok else 0.0,\n             1.0 if (amr_present and payload_ok) else (0.5 if amr_present else 0.0)]\n\n    score = sum(parts) / 4.0\n    return float(max(0.0, min(1.0, score)))"}, {"type": "code", "name": "Protocol Coverage (EtherNet/IP, Modbus TCP, IO-Link)", "description": "Checks that across the Hardware sheet there is visible coverage of the three target industrial protocols in the Interface/Protocol columns.", "weight": 1.5, "code": "import re\nimport pandas as pd\n\ndef _find_spreadsheet(context):\n    for r in context.get_all_outputs():\n        if getattr(r, 'is_spreadsheet', False):\n            return r\n    return None\n\ndef evaluate(workflow, context):\n    res = _find_spreadsheet(context)\n    if not res:\n        return 0.0\n    try:\n        xls = pd.ExcelFile(context.files.get_path(res.id))\n    except Exception:\n        return 0.0\n\n    # try to find hardware sheet\n    sheet = None\n    for s in xls.sheet_names:\n        sl = s.lower()\n        if any(k in sl for k in ['hardware','selection','bom','device','catalog']):\n            sheet = s\n            break\n    if not sheet:\n        return 0.0\n\n    try:\n        df = pd.read_excel(xls, sheet_name=sheet)\n    except Exception:\n        return 0.0\n    if df.empty:\n        return 0.0\n\n    cols = [c for c in df.columns if any(k in str(c).lower() for k in ['protocol','interface','communication','port'])]\n    if not cols:\n        return 0.0\n\n    text = ' '.join(df[cols].astype(str).fillna('').values.ravel()).lower()\n    # normalize variants\n    text = text.replace('ethernet/ip','ethernetip').replace('modbus-tcp','modbus tcp').replace('io link','io-link')\n    hits = 0\n    if 'ethernetip' in text or 'ethernet/ip' in text or 'cip' in text:\n        hits += 1\n    if 'modbus tcp' in text:\n        hits += 1\n    if 'io-link' in text or 'iolink' in text:\n        hits += 1\n\n    return hits/3.0"}, {"type": "code", "name": "IO Mapping Completeness", "description": "Verifies IO Mapping sheet exists with key columns populated and sufficient device rows.", "weight": 2.0, "code": "import pandas as pd\n\ndef _find_spreadsheet(context):\n    for r in context.get_all_outputs():\n        if getattr(r, 'is_spreadsheet', False):\n            return r\n    return None\n\ndef evaluate(workflow, context):\n    res = _find_spreadsheet(context)\n    if not res:\n        return 0.0\n    try:\n        xls = pd.ExcelFile(context.files.get_path(res.id))\n    except Exception:\n        return 0.0\n\n    # find IO mapping sheet\n    sheet = None\n    for s in xls.sheet_names:\n        sl = s.lower()\n        if ('io' in sl or 'i/o' in sl) and ('map' in sl or 'mapping' in sl or 'signals' in sl):\n            sheet = s\n            break\n    if not sheet:\n        return 0.0\n\n    try:\n        df = pd.read_excel(xls, sheet_name=sheet)\n    except Exception:\n        return 0.0\n\n    if df is None or df.empty:\n        return 0.0\n\n    # Column presence score\n    cols = [c.lower() for c in df.columns]\n    def has_any(keys):\n        return any(any(k in c for k in keys) for c in cols)\n\n    col_expect = [\n        has_any(['device id','device','name']),\n        has_any(['type']),\n        has_any(['protocol']),\n        has_any(['address','ip','port']),\n        has_any(['i/o points','signals','points used','channels']),\n        has_any(['robot io']),\n        has_any(['cnc io']),\n        has_any(['gateway','plc']),\n        has_any(['interlock','events','safety','pl '])\n    ]\n    col_score = sum(1 for b in col_expect if b) / len(col_expect)\n\n    # Row count score (expect at least 8 devices: 6 CNC-related, robot, AMR, plus sensors/hubs)\n    rows = len(df)\n    row_score = min(1.0, rows / 8.0)\n\n    score = 0.6 * col_score + 0.4 * row_score\n    return float(max(0.0, min(1.0, score)))"}, {"type": "code", "name": "Safety Standards Evidence", "description": "Checks that at least several recognized industrial safety standards are referenced across the workbook (Hardware or a dedicated Standards sheet).", "weight": 1.5, "code": "import re\nimport pandas as pd\n\ndef _find_spreadsheet(context):\n    for r in context.get_all_outputs():\n        if getattr(r, 'is_spreadsheet', False):\n            return r\n    return None\n\ndef _collect_text(df):\n    try:\n        return ' '.join(df.astype(str).fillna('').values.ravel()).lower()\n    except Exception:\n        return ''\n\ndef evaluate(workflow, context):\n    res = _find_spreadsheet(context)\n    if not res:\n        return 0.0\n    try:\n        xls = pd.ExcelFile(context.files.get_path(res.id))\n    except Exception:\n        return 0.0\n\n    all_text = ''\n    for s in xls.sheet_names:\n        try:\n            df = pd.read_excel(xls, sheet_name=s)\n        except Exception:\n            continue\n        all_text += ' ' + _collect_text(df)\n\n    tokens = [\n        'iso 13849', 'pl d', 'pl e', 'category 3', 'category 4',\n        'iso 10218', 'ria r15.06', 'ansi/ria r15.06',\n        'iec 61496', 'iec 61508', 'iec 60204', 'iec 62061',\n        'sil 2', 'sil 3'\n    ]\n    hits = set()\n    for t in tokens:\n        if t in all_text:\n            hits.add(t)\n    # Require at least 3 distinct references for full credit\n    score = min(1.0, len(hits)/3.0)\n    return float(score)"}, {"type": "llm_judge", "name": "Diagram Semantics & Labeling Completeness", "description": "LLM validation that the PNG diagram not only exists (Stage 1) but also meaningfully depicts the required zones/placements and labels are unambiguous.", "weight": 2.0, "judge_prompt": "Re-open the PNG diagram. Assess whether the diagram\u2019s labeling and placement are semantically adequate:\n- Six CNCs are distinctly identifiable.\n- LIDAR zones are clearly delineated and labeled (six static zones between CNCs and a front west zone). A robot-mounted LIDAR is indicated.\n- Cameras are placed to monitor each CNC (>=6) and labeled or shown with icons.\n- Pressure mats appear at each CNC front.\n- Barriers/screens indicated along South and East sides.\n- Rail path and robot shown with clear relation to CNCs and zones.\n\nScoring:\n- 2.0: All bullets clearly satisfied with unambiguous labels/legends.\n- 1.0: Minor ambiguity or 1 missing element, but overall understandable.\n- 0.0: Ambiguous/incomplete; multiple required elements missing or mislabeled.", "expectation": "Diagram conveys placements/labels that would allow a technician to implement the physical layout without confusion."}, {"type": "llm_judge", "name": "Report \u2014 Integration Strategy Adequacy", "description": "Checks that the PDF\u2019s Integration Strategy section addresses software separation, protocol/IO integration, and minimal rewiring approach.", "weight": 2.0, "judge_prompt": "Open the PDF report and focus on the Integration Strategy section. Confirm it addresses:\n- Clean software separation: robot\u2019s proprietary controller remains independent; external hardware managed via interoperable interfaces/gateways (e.g., PLC/edge controller, IO-Link master, safety relays/scanners) without embedding proprietary runtime.\n- Protocol strategy: how Ethernet/IP, Modbus TCP, and/or IO-Link are used (or bridged) to integrate LIDAR, cameras, pressure mats, AMR, and CNC/robot IO. Mention of safety I/O (OSSD), safety PLC/safety relay for mats and LIDAR is appropriate.\n- Event-based camera triggering (snapshots/feeds) tied to IO/events.\n- Minimal physical rewiring: use of available IO in robot cabinet/CNCs and modular hubs rather than large-scale rewiring.\n\nScoring:\n- 2.0: Clearly and explicitly covers all bullets with actionable details.\n- 1.0: Covers most points but lacks clarity on one area (e.g., protocol bridging or safety I/O handling).\n- 0.0: Vague or missing on multiple key items.", "expectation": "A precise, modular integration plan with explicit mention of protocol use/bridging, safety I/O handling, event triggers, and software separation."}, {"type": "llm_judge", "name": "Cross-Reference Consistency (Report \u2194 Excel)", "description": "Validates that specific makes/models referenced in the PDF match those listed in the Excel hardware sheet.", "weight": 1.0, "judge_prompt": "Compare the PDF report with the Excel Hardware Selection sheet. Do the specific device makes/models cited in the report match corresponding entries in the spreadsheet? Minor naming variations are acceptable (e.g., vendor prefixes/suffixes), but the models should be clearly the same devices.\n\nScoring:\n- 1.0: Clear cross-consistency for all major categories (LIDAR, Cameras, AMR, Pressure Mats, any gateways/PLCs).\n- 0.5: Mostly consistent with 1 mismatch/omission.\n- 0.0: Multiple mismatches or the report lists devices not present in the spreadsheet (or vice versa).", "expectation": "Report names align with the Excel table for all major device categories."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality", "description": "Holistic assessment of presentation quality, clarity, and strategic value for an industrial engineering audience.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Strategic Value", "description": "Judges overall clarity, professionalism, and suitability for engineering stakeholders.", "weight": 4.0, "judge_prompt": "Evaluate the overall quality of the deliverables together (Excel, PNG, PDF):\n- Clarity and professionalism: Are tables clean, units clear, and diagrams legible? Does the PDF read like a professional engineering document?\n- Actionability: Could a controls/safety engineer proceed with procurement and implementation based on these artifacts?\n- Strategic value: Emphasis on modularity, software independence, IO mapping clarity, and maintainability.\n- Appropriateness: Tone and depth suitable for industrial engineering/manufacturing stakeholders.\n\nScoring:\n- 4.0: Highly professional, immediately actionable, strategically sound.\n- 3.0: Strong overall with minor polish issues.\n- 2.0: Adequate but missing polish/clarity in multiple places.\n- 1.0: Rough/unclear; limited actionability.\n- 0.0: Unprofessional or not practically usable.", "expectation": "A polished, implementation-ready package suitable for internal review and vendor engagement."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "a69be28f-9a84-47c9-992e-b90446cdca9d", "rubric": {"category_name": "Regional Fit Performance Recap (Wholesale Trade \u2022 Sales Managers)", "rationale": "This is a mixed task (Pattern C): a presentation (document) that must embed verifiable analytical results from an Excel source. Stage 1 uses an LLM gate to enforce a specific, checkable slide structure that enables verification. Stage 2 uses code rules to validate internal consistency and basic numerical plausibility by parsing a required consolidated summary table, plus an LLM cross-reference. Stage 3 assesses professional quality and usefulness for merchandising/planning decisions.", "max_total_score": 10.0, "stages": [{"name": "Stage 1: Presentation Structure Gate", "description": "Enforce an exact, verifiable PDF slide structure so that downstream checks are possible.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Presentation Format Requirement (PDF with mandated slide set)", "description": "Check if the output is a PDF presentation with the exact sections and slides needed for verification: labeled region-by-gender slides with charts/tables and a consolidated Top-Fit Summary table.", "weight": 4.0, "judge_prompt": "You are evaluating whether the candidate output presentation has the required STRUCTURE. Only check presence/format, not correctness of numbers.\n\nOutput must be a PDF exported from presentation software (e.g., PowerPoint/Keynote/Google Slides). Verify all of the following:\n\nFormat Requirements:\n- File is a PDF (not Excel/Word/plain text)\n- Professionally formatted slides with clear titles\n- Minimum 10 slides\n\nRequired Slides and Structure (be flexible with exact titles but the content must be clearly present):\n1) Title/Cover slide: includes brand (Best Jeans) and topic (Regional Fit Performance Recap) and date context (around July 9, 2025)\n2) Executive Summary slide(s): aggregate view and topline insights; must show total units and total revenue summaries at the overall level or by region\n3) Methodology & Data Notes slide: references the attached Excel sell-in data, defines measures (units, revenue), period/timeframe, any filters/assumptions\n4) Top-Fit Summary slide (MANDATORY consolidated table): a single table summarizing top-selling fits by region and gender, with columns that clearly correspond to: [Region | Gender | Top Fit | Units | Revenue]. Must cover all regions: Midwest, South, Northeast, West Coast AND both genders (8 rows total expected)\n5) Regional Detail slides: For each region (Midwest, South, Northeast, West Coast), there are TWO separate slides:\n   - One slide for Men's performance in that region\n   - One slide for Women's performance in that region\n   Each Regional Detail slide must include at least one chart OR table that breaks down sales by fit for that gender in that region, with units and revenue visible or clearly labeled. The region and gender must be obvious in the slide title (e.g., \"Midwest \u2013 Men\" or similar). The top fit should be apparent (e.g., first in a sorted table, highlighted, or clearly labeled as Top Fit).\n6) Optional Appendix slides (not required for full credit): additional tables/pivots or definitions\n\nScoring (STRUCTURE ONLY):\n- 4.0: All required slides present with clear labeling; Top-Fit Summary table present with 8 rows (all 4 regions \u00d7 2 genders); every region has separate Men's and Women's slides with a chart/table each; Executive Summary shows total units and total revenue; Methodology present.\n- 3.0: One missing supporting element (e.g., Methodology slide missing OR one Regional Detail slide uses text bullets instead of a chart/table) but all regions \u00d7 genders are still separately covered and the consolidated Top-Fit Summary table exists.\n- 2.0: Missing up to two required elements (e.g., Top-Fit Summary present but 1\u20132 region\u00d7gender slides missing or combined; OR Executive Summary missing totals labeling), still a PDF with at least 10 slides.\n- 1.0: PDF present but multiple structural requirements missing (e.g., no Top-Fit Summary table; or regions not separated by gender; or fewer than 10 slides).\n- 0.0: Not a PDF OR grossly non-compliant structure (e.g., no regional detail slides, no summary, or not separated by gender).\n\nOnly evaluate structure/presence. Do NOT check if the numbers are correct.", "expectation": "A PDF deck with Title, Executive Summary (totals), Methodology, a consolidated Top-Fit Summary table (Region, Gender, Top Fit, Units, Revenue; 8 rows), and 8 Regional Detail slides (Men and Women for each of Midwest, South, Northeast, West Coast), each with a chart/table of fits by sales."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Verification and Internal Consistency", "description": "Now that the structure is enforced, verify internal consistency and numerical plausibility using the consolidated Top-Fit Summary and slide text.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Top-Fit Summary Table Parse and Sanity Checks", "description": "Parse the consolidated Top-Fit Summary table from the PDF text. Validate: 8 rows present, positive units/revenue, and plausible revenue per unit (e.g., $20\u2013$500).", "weight": 2.0, "code": "import re\n\ndef evaluate(workflow, context):\n    # Returns a score in [0,1] with feedback\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected or wrong type.\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception as e:\n        return 0.0, f\"Failed to read PDF text: {e}\"\n    if not text or not isinstance(text, str):\n        return 0.0, \"Empty or unreadable PDF text.\"\n\n    # Normalize text\n    tl = text.lower()\n\n    # Heuristic: find lines likely from the consolidated Top-Fit Summary with Region | Gender | Top Fit | Units | Revenue\n    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n\n    # Compile regex to capture summary rows regardless of delimiter (| , : - tab)\n    pattern = re.compile(\n        r\"\\b(?P<region>midwest|south|northeast|west\\s*coast)\\b\\s*[\\|,;:\\-\\t]*\\s*\"\n        r\"(?P<gender>men|male|mens|women|female|womens)\\b.*?[\\|,;:\\-\\t]+\\s*\"\n        r\"(?P<fit>[A-Za-z0-9][A-Za-z0-9 \\-/&]+?)\\s*[\\|,;:\\-\\t]+\\s*\"\n        r\"(?P<units>[0-9]{1,3}(?:,[0-9]{3})*)\\s*[\\|,;:\\-\\t]+\\s*\\$?\\s*\"\n        r\"(?P<revenue>[0-9]{1,3}(?:,[0,][0-9]{3})*(?:\\.[0-9]{2})?|[0-9]{1,3}(?:,[0-9]{3})*(?:\\.[0-9]{2})?)\",\n        re.IGNORECASE,\n    )\n\n    entries = []\n    for ln in lines:\n        m = pattern.search(ln)\n        if m:\n            region = m.group('region').strip().lower().replace('  ', ' ')\n            gender = m.group('gender').strip().lower()\n            fit = m.group('fit').strip()\n            units_s = m.group('units').replace(',', '')\n            revenue_s = m.group('revenue').replace(',', '')\n            try:\n                units = int(units_s)\n                revenue = float(revenue_s)\n            except Exception:\n                continue\n            entries.append({\n                'region': region,\n                'gender': gender,\n                'fit': fit,\n                'units': units,\n                'revenue': revenue,\n            })\n\n    if not entries:\n        return 0.0, \"No parsable Top-Fit Summary rows found. Ensure a consolidated table with Region, Gender, Top Fit, Units, Revenue exists.\"\n\n    # Basic validations\n    positives = sum(1 for e in entries if e['units'] > 0 and e['revenue'] > 0)\n    # Revenue per unit plausibility for premium denim\n    plausible = 0\n    plausible_details = []\n    for e in entries:\n        if e['units'] > 0:\n            rpu = e['revenue'] / max(e['units'], 1)\n            if 20 <= rpu <= 500:\n                plausible += 1\n            plausible_details.append((e['region'], e['gender'], e['fit'], round(rpu, 2)))\n\n    # Expect 8 unique region\u00d7gender combos\n    combos = {(e['region'], 'men' if 'men' in e['gender'] else 'women') for e in entries}\n    unique_combo_count = len(combos)\n\n    # Score components\n    n = len(entries)\n    frac_positive = positives / n if n else 0\n    frac_plausible = plausible / n if n else 0\n\n    # Weighted average: 50% positive values, 30% plausible RPU, 20% coverage fraction vs 8 expected\n    coverage_frac = min(unique_combo_count / 8, 1.0)\n    score = 0.5 * frac_positive + 0.3 * frac_plausible + 0.2 * coverage_frac\n\n    feedback = (\n        f\"Parsed {n} summary rows; positive vals: {positives}; plausible RPU: {plausible}; \"\n        f\"unique region\u00d7gender: {unique_combo_count}. Sample RPU entries: {plausible_details[:4]}\"\n    )\n    # Return 0..1 (framework will scale by weight)\n    return max(0.0, min(1.0, score)), feedback"}, {"type": "code", "name": "Region-Gender Coverage Check (8 combinations)", "description": "Ensure the consolidated summary covers all 4 regions \u00d7 2 genders.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected or wrong type.\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception as e:\n        return 0.0, f\"Failed to read PDF text: {e}\"\n    if not text:\n        return 0.0, \"Empty PDF text.\"\n\n    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n    pattern = re.compile(r\"\\b(midwest|south|northeast|west\\s*coast)\\b.*\\b(men|male|mens|women|female|womens)\\b\", re.IGNORECASE)\n    combos = set()\n    for ln in lines:\n        m = pattern.search(ln)\n        if m:\n            region = m.group(1).lower().replace('  ', ' ')\n            gender_raw = m.group(2).lower()\n            gender = 'men' if 'men' in gender_raw or 'male' in gender_raw else 'women'\n            combos.add((region, gender))\n    coverage = len(combos)\n    score = min(coverage / 8, 1.0)\n    return score, f\"Found {coverage}/8 region\u00d7gender combinations in summary-related text.\""}, {"type": "code", "name": "Executive Summary Totals Presence", "description": "Check that the PDF text includes an Executive Summary and both total units and total revenue mentions.", "weight": 0.5, "code": "def evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected or wrong type.\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception as e:\n        return 0.0, f\"Failed to read PDF text: {e}\"\n    if not text:\n        return 0.0, \"Empty PDF text.\"\n    tl = text.lower()\n    has_exec = ('executive summary' in tl) or ('summary' in tl and 'executive' in tl)\n    has_units = ('total units' in tl) or ('units total' in tl) or ('units: total' in tl)\n    has_revenue = ('total revenue' in tl) or ('revenue total' in tl) or ('revenue: total' in tl)\n    # Score: 0.2 for exec, 0.15 for units, 0.15 for revenue (caps at 1.0)\n    score = 0.0\n    if has_exec: score += 0.4\n    if has_units: score += 0.3\n    if has_revenue: score += 0.3\n    score = min(score, 1.0)\n    return score, f\"Executive Summary present: {has_exec}; totals (units/revenue): {has_units}/{has_revenue}.\""}, {"type": "llm_judge", "name": "Numerical Cross-Reference (Summary vs. Regional Slides)", "description": "Visually check that the Top-Fit Summary table values (units and revenue) match the corresponding values shown on each region\u2019s Men/Women detail slides.", "weight": 0.5, "judge_prompt": "Using the PDF, sample at least 2 regions (e.g., Midwest and West Coast), and for each, compare the Top-Fit Summary table row (Men and Women) against the numbers shown on the corresponding Regional Detail slides for that gender. Do the units and revenue match or plausibly align (allowing minor formatting differences like commas or $ signs)?\n\nScoring:\n- 0.5: Clear alignment for both sampled regions and genders\n- 0.3: Mostly aligned but a minor discrepancy or unclear formatting in one case\n- 0.1: Discrepancies or unclear for both regions, but the intent seems present\n- 0.0: No visible alignment or missing required elements\n\nOnly check alignment between stated numbers; do not judge the analytical correctness versus the original Excel data.", "expectation": "Numbers shown in the consolidated Top-Fit Summary table should match the same numbers on each region\u2019s Men/Women detail slides."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3: Presentation Quality and Strategic Usefulness", "description": "Assess professional quality, clarity, and usefulness for merchandising and planning decisions.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Quality and Actionability", "description": "Evaluate visual clarity, labeling, chart readability, and the usefulness of insights for future assortment decisions.", "weight": 2.0, "judge_prompt": "Assess the overall professional quality and strategic usefulness of the PDF presentation:\n- Visual clarity: clear slide titles, legible labels/axes, consistent formatting, brand-appropriate design\n- Data storytelling: clear callouts of the top fits per region and gender; charts/tables easy to read\n- Executive Summary quality: concise, accurate, and highlights the key regional differences by gender; clearly states total units and total revenue\n- Actionability: provides insights or implications that merchandising/planning teams can use (e.g., which fits to prioritize by region/gender)\n\nScore:\n- 2.0: Highly professional and actionable; excellent readability and clear, relevant insights\n- 1.4: Generally strong but minor issues (e.g., cluttered chart or vague insight)\n- 0.8: Adequate but several issues limit readability or usefulness\n- 0.3: Poor quality; difficult to read or lacks actionable guidance\n- 0.0: Not professional or not useful for decision-making", "expectation": "A polished, brand-appropriate deck with clear charts/tables, well-labeled slides, a crisp executive summary, and region/gender-specific insights that guide assortment planning."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "74ed1dc7-1468-48a8-9071-58775c0d667a", "rubric": {"category_name": "ERP Order Type Proposal (Wholesale Trade \u2022 Sales Managers)", "rationale": "This rubric enforces a self-documenting, verifiable Word proposal that enables automated checks. Stage 1 is a strict LLM structure gate to ensure the deliverable is a DOCX with explicit sections and per-type subsections, making later verification trivial. Stage 2 mixes code and LLM to validate correctness and consistency against the mandated structure (counts, required subsections, glossary, governance, reporting map). Stage 3 uses LLM to assess professional quality, strategic fit, and audience appropriateness for project managers and leadership.", "max_total_score": 12.0, "stages": [{"name": "Stage 1: Format and Structure Gate (LLM only)", "description": "Gate check that the output is a well-structured DOCX proposal with the exact sections and per-type subsections needed for verification.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Proposal Shape Requirement (DOCX)", "description": "Verify the deliverable is a Word document with the mandated structure enabling later verification.", "weight": 4.0, "judge_prompt": "You are evaluating whether the candidate output satisfies the REQUIRED SHAPE for a professional proposal in Word format. Do NOT assess quality or correctness\u2014only presence and structure.\n\nFormat Requirements:\n- Must be a Word document (DOCX). Not PDF, not plain text.\n- At least 4 pages of substantive content (excluding title page if present).\n- Clear, professional formatting with headings and subsections.\n\nRequired Sections (flexible with exact names but must be clearly identifiable):\n1) Executive Summary (first page or near the start)\n2) Background and Current State (must reference existing order types: Pre-Order, Re-Order/Reorder, Bulk, and reporting challenges)\n3) Objectives and Design Principles (goals for clear, unambiguous reporting; reduction of manual manipulation)\n4) Proposed Order Types (MUST introduce 3 or more NEW order types, each as its own sub-section headed like \u201cOrder Type: <Name>\u201d or similar)\n   Each new order type sub-section must include the following labeled subsections:\n   - Purpose / Rationale\n   - Eligibility & Triggers (or Use Cases)\n   - Lifecycle States (status model from creation to invoice/cancel)\n   - Reporting Fields & Flags (key attributes, dimensions, filters for BI)\n   - KPIs & Metrics Impact\n   - Edge Cases & Exclusions\n5) Cross-Functional Reporting Map (a table or clearly structured list mapping each proposed order type to reporting consumers\u2014e.g., Project Managers, Leadership, KAMs/Sales, Finance, Supply/Operations\u2014and key metrics each should use)\n6) Governance & Controls (owners, approvals, change control, data stewardship, auditing)\n7) Implementation Roadmap (timeline/milestones and responsibilities; RACI acceptable)\n8) Glossary & Abbreviations (must include KA, KAM, PO)\n9) Appendix: Order Type Catalog (a summary table of each new order type with key fields/flags and lifecycle)\n\nScoring (be flexible with exact header names but strict about the structure and presence):\n- 4.0: DOCX + 4+ pages + all 9 sections present + Proposed Order Types has 3+ new types, each with ALL six required subsections.\n- 3.5: DOCX + 4+ pages + all core sections present but missing the Appendix summary table ONLY.\n- 3.0: DOCX + 4+ pages + missing one supporting section (e.g., Governance or Roadmap) OR some new types missing 1-2 required subsections.\n- 2.0: DOCX but fewer than 4 pages OR fewer than 3 new order types OR multiple missing core sections.\n- 0.0: Not a DOCX OR no recognizable structure.\n\nOnly evaluate presence/structure, not the correctness of content.", "expectation": "A DOCX proposal with the exact structure above, enabling later automated checks."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Correctness and Consistency Verification", "description": "Now that the structure exists, verify internal consistency and correctness signals using code and LLM checks.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "References to Existing Order Types and Additions (not replacements)", "description": "Checks that the document references existing order types (Pre-Order, Re-Order/Reorder, Bulk) and explicitly frames proposals as additions, not replacements.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    W = 0.8\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        # Prefer DOCX text for precision\n        text = \"\"\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                return 0.0\n        t = text.lower()\n        # Check mentions of existing types\n        has_pre = 'pre-order' in t or 'preorder' in t\n        has_re = 're-order' in t or 'reorder' in t\n        has_bulk = 'bulk' in t\n        base = 0.0\n        if (has_pre and has_re) or (has_pre and has_bulk) or (has_re and has_bulk):\n            base = 0.5\n        # Check language about additions (not replacements)\n        additive_signals = [\n            'in addition to', 'additional order types', 'not replace', 'does not replace',\n            'supplement', 'alongside existing', 'retain existing', 'keep existing order types'\n        ]\n        add_lang = any(phrase in t for phrase in additive_signals) or ('existing order types' in t and 'add' in t)\n        bonus = 0.3 if add_lang else 0.0\n        score = min(W, base + bonus)\n        return score\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Count of Proposed New Order Types", "description": "Detects and scores the presence of 3 or more distinct proposed order types using heading markers like 'Order Type: <Name>' or similar.", "weight": 1.2, "code": "import re\n\ndef evaluate(workflow, context):\n    W = 1.2\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                return 0.0\n        t = text\n        # Find headings like 'Order Type: <Name>' or 'New Order Type: <Name>' or 'Order Type Name: <Name>'\n        patterns = [r'^\\s*(?:new\\s+)?order\\s+type\\s*:\\s*(.+)$', r'^\\s*order\\s+type\\s+name\\s*:\\s*(.+)$']\n        names = set()\n        for pat in patterns:\n            for m in re.finditer(pat, t, flags=re.IGNORECASE|re.MULTILINE):\n                name = m.group(1).strip()\n                if name:\n                    names.add(name.lower())\n        # Fallback: bullets mentioning Order Type:\n        if not names:\n            for m in re.finditer(r'^\\s*[-*\u2022]\\s*(?:new\\s+)?order\\s+type\\s*:\\s*(.+)$', t, flags=re.IGNORECASE|re.MULTILINE):\n                name = m.group(1).strip()\n                if name:\n                    names.add(name.lower())\n        n = len(names)\n        if n >= 3:\n            return W\n        elif n == 2:\n            return W * 0.66\n        elif n == 1:\n            return W * 0.33\n        else:\n            return 0.0\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Per-Type Required Subsections Completeness", "description": "For each detected order type section, checks presence of required subsections: Purpose/Rationale; Eligibility/Triggers/Use Cases; Lifecycle States; Reporting Fields/Flags; KPIs; Edge Cases/Exclusions. Scores by average completeness.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    W = 1.5\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                return 0.0\n        t = text\n        # Identify order type blocks by heading markers\n        heading_re = re.compile(r'^\\s*(?:new\\s+)?order\\s+type\\s*:\\s*(.+)$', flags=re.IGNORECASE|re.MULTILINE)\n        heads = list(heading_re.finditer(t))\n        # If none, try 'Order Type Name:'\n        if not heads:\n            heading_re = re.compile(r'^\\s*order\\s+type\\s+name\\s*:\\s*(.+)$', flags=re.IGNORECASE|re.MULTILINE)\n            heads = list(heading_re.finditer(t))\n        if not heads:\n            return 0.0\n        # Build segments per order type\n        segments = []\n        for i, m in enumerate(heads):\n            start = m.end()\n            end = heads[i+1].start() if i+1 < len(heads) else len(t)\n            segments.append(t[start:end])\n        # Define subsection keyword groups\n        groups = [\n            ('purpose/rationale', [r'purpose', r'rationale', r'why']),\n            ('eligibility/triggers/use cases', [r'eligibil', r'trigger', r'use case', r'when to use']),\n            ('lifecycle states', [r'lifecycle', r'state', r'status model']),\n            ('reporting fields/flags', [r'reporting field', r'field', r'flag', r'attribute', r'dimension']),\n            ('kpis/metrics', [r'kpi', r'metric', r'measure']),\n            ('edge cases/exclusions', [r'edge case', r'exclusion', r'out of scope'])\n        ]\n        totals = []\n        for seg in segments:\n            s = seg.lower()\n            hits = 0\n            for _, kws in groups:\n                if any(re.search(kw, s) for kw in kws):\n                    hits += 1\n            totals.append(hits/len(groups))\n        if not totals:\n            return 0.0\n        avg_ratio = sum(totals)/len(totals)\n        return min(W, W * avg_ratio)\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Glossary & Abbreviations Coverage (KA, KAM, PO)", "description": "Checks that a Glossary/Abbreviations section exists and includes KA, KAM, and PO entries (ideally with '=','\u2013', or ':' definitions).", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    W = 0.5\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                return 0.0\n        t = text\n        has_gloss = re.search(r'glossary|abbreviations', t, flags=re.IGNORECASE) is not None\n        # Look for definitions like 'KA =', 'KA \u2013', 'KA :' etc.\n        def_present = 0\n        for abbr in ['KA', 'KAM', 'PO']:\n            pattern = rf'\\b{abbr}\\b\\s*([=:\u2013-])'\n            if re.search(pattern, t):\n                def_present += 1\n            else:\n                # fallback: check at least the term appears near glossary section\n                def_present += 1 if (has_gloss and re.search(rf'\\b{abbr}\\b', t)) else 0\n        ratio = min(1.0, (def_present/3.0))\n        base = 0.25 if has_gloss else 0.0\n        score = min(W, W*(0.5*ratio) + base)\n        return score\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Governance & Controls Signals", "description": "Checks the Governance & Controls section for ownership, approvals, and audit/control language.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    W = 0.5\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                return 0.0\n        t = text.lower()\n        # Identify governance section window\n        gov_idx = t.find('governance')\n        window = t[gov_idx: gov_idx+2000] if gov_idx != -1 else t\n        keywords = ['owner', 'ownership', 'approval', 'approver', 'controls', 'control', 'audit', 'auditing', 'sox', 'sla', 'service level', 'quality gate', 'change control', 'data steward', 'data governance']\n        hits = sum(1 for k in keywords if k in window)\n        # At least two distinct governance concepts\n        if hits >= 5:\n            return W\n        elif hits >= 3:\n            return W * 0.7\n        elif hits >= 2:\n            return W * 0.5\n        elif hits >= 1:\n            return W * 0.3\n        else:\n            return 0.0\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Reporting Map: Consumers and Metrics", "description": "Checks for a cross-functional reporting map with at least 3 consumer roles and metric terms.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    W = 0.5\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                return 0.0\n        t = text.lower()\n        # Locate section\n        idx = -1\n        for term in ['reporting map', 'reporting consumers', 'stakeholder reporting', 'cross-functional reporting']:\n            idx = t.find(term)\n            if idx != -1:\n                break\n        window = t[idx: idx+3000] if idx != -1 else t\n        consumers = ['project manager', 'project managers', 'leadership', 'executive', 'finance', 'fp&a', 'supply', 'supply planning', 'operations', 'ops', 'kam', 'key account', 'sales', 'commercial']\n        metrics = ['fill rate', 'on-time', 'otif', 'cancellation', 'cancel', 'backorder', 'revenue', 'gm', 'gross margin', 'margin', 'aging', 'inventory', 'allocation', 'shipment', 'invoice', 'conversion']\n        cons_hits = len({c for c in consumers if c in window})\n        met_hits = len({m for m in metrics if m in window})\n        if cons_hits >= 4 and met_hits >= 3:\n            return W\n        elif cons_hits >= 3 and met_hits >= 2:\n            return W * 0.75\n        elif cons_hits >= 2 and met_hits >= 1:\n            return W * 0.5\n        elif cons_hits >= 1:\n            return W * 0.25\n        else:\n            return 0.0\n    except Exception:\n        return 0.0"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3: Professional Quality and Strategic Value", "description": "Holistic assessment of clarity, feasibility, and audience fit for PMs and leadership.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Strategic Fit and ERP Feasibility", "description": "Are the proposed order types realistic, non-overlapping with existing types, and likely to reduce manual manipulation while enabling unambiguous reporting?", "weight": 1.5, "judge_prompt": "Judge the proposal\u2019s strategic fit and ERP feasibility. Consider:\n- Realism: Proposed types are plausible in modern ERPs; do not duplicate existing Pre-Order/Re-Order/Bulk; clearly differentiated; include lifecycle states and fields that systems can implement.\n- Clarity: Definitions and eligibility rules reduce interpretation and manual manipulation.\n- Reporting enablement: Fields/flags and mapping make reports unambiguous for cross-functional consumers.\n- Risk-awareness: Notes governance/controls and change impacts.\nScoring:\n- 1.5: Strong across all dimensions; highly feasible; clearly minimizes manual work and ambiguity.\n- 1.0: Generally solid but minor gaps in feasibility or differentiation.\n- 0.5: Significant gaps; ambiguous or overlapping definitions; limited reporting enablement.\n- 0.0: Not feasible or fundamentally unclear.", "expectation": "Clear, differentiated, implementable order types that enable no-interpretation reporting and reduce manual manipulations."}, {"type": "llm_judge", "name": "Presentation and Audience Appropriateness", "description": "Evaluate executive readability, structure, and actionability for project managers and leadership.", "weight": 1.5, "judge_prompt": "Assess whether the document is professionally presented and suited for PMs and leadership:\n- Executive Summary: concise, outcomes-focused.\n- Organization: logical flow from background to roadmap; consistent headings and labeling.\n- Actionability: clear roadmap with milestones/RACI; measurable KPIs.\n- Communication: tone and clarity suitable for senior stakeholders; minimal jargon or well-defined terms.\nScoring:\n- 1.5: Excellent clarity and executive cadence; actionable and easy to implement.\n- 1.0: Good but with minor clarity or actionability issues.\n- 0.5: Hard to follow or weak on next steps/KPIs.\n- 0.0: Unprofessional or not audience-appropriate.", "expectation": "A polished, executive-ready proposal with clear next steps and measures of success."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "0e386e32-df20-4d1f-b536-7159bc409ad5", "rubric": {"category_name": "PrivateCrypMix Full-Stack Implementation (Software Deliverable)", "rationale": "Task Type = Mixed (Pattern C). This is a software codebase submission that must be proven correct via self-documenting artifacts. Stage 1 mandates a single, human-readable Submission Index document (PDF/DOCX) with precise sections enabling verification. Stage 2 uses code rules to validate machine-readable companion artifacts (MANIFEST.csv, TEST_MATRIX.xlsx, CONFIG.csv, ABI_INDEX.csv) and basic plausibility checks without unpacking the ZIP. Stage 3 uses an LLM for holistic quality. Stage 1 is an LLM-only gate per the V3 philosophy.", "max_total_score": 12.0, "stages": [{"name": "Stage 1 \u2014 Submission Index Document Gate (FORMAT & STRUCTURE)", "description": "Gate: Enforce exact structure for a self-documenting submission. The primary output must be a PDF/DOCX Submission Index that inventories the codebase, build/run instructions, privacy/UX requirements, and pointers to machine-readable artifacts.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.1, "rules": [{"type": "llm_judge", "name": "Submission Index Required Structure", "description": "Check that the PRIMARY OUTPUT is a professional PDF/DOCX Submission Index with required sections that make verification trivial.", "weight": 3.0, "judge_prompt": "You are validating the structure (not the correctness) of the submission's PRIMARY OUTPUT. Evaluate ONLY format and section presence. Do not judge code quality or calculations.\n\nFormat Requirements:\n- Must be a PDF or DOCX document (not ZIP, not plain text, not Excel)\n- Minimum length: 3 pages (flexible layout acceptable)\n- Professional formatting with clear, labeled sections and tables\n\nRequired Sections (headers may vary but meaning must be clear):\n1) Title/Cover Section (first page):\n   - Name similar to \u201cPrivateCrypMix Submission Index\u201d (or Submission Guide/Index)\n   - Version/date and brief description of the system\n2) System Overview:\n   - High-level architecture summary referencing: Frontend (React TS + ethers), Smart Contracts (Solidity), ZK proofs (commitments/nullifiers), Aave integration for yield, Connext for cross-chain, optional Relayer service\n3) Component Inventory (Table):\n   - Table with columns similar to: [Component | Language/Framework | Path | Build Command]\n   - Rows should include: Frontend dApp, Smart Contracts, ZK/Circuits, Cross-Chain/Connext integration, Optional Relayer, Infra/Deployment\n4) Build & Run Instructions:\n   - Step-by-step commands to install, build, run, and test each component\n   - Environment variables required (names and purpose)\n5) Privacy & UX Requirements:\n   - Clear statements covering: fixed-size deposits; anonymity delay/lock period; saving commitment hash; nullifier to prevent double-spend; yield accrual via Aave; cross-chain withdrawals via Connext; optional relayer for anonymity\n6) Contract Interfaces Summary:\n   - List of core contracts and top functions/events, with a note that detailed ABIs are provided separately (ABI_INDEX.csv)\n7) Test Plan Summary:\n   - Summary table of test suites (e.g., Unit, Integration, ZK Proofs, Cross-Chain) with counts of tests and pass/fail totals, and a note that detailed results are in TEST_MATRIX.xlsx\n8) Submission Manifest Reference:\n   - Explicit reference to MANIFEST.csv (file index with paths and checksums) and brief snippet/example of its columns\n9) Release Artifacts List:\n   - List naming the included files: source code ZIP, MANIFEST.csv, TEST_MATRIX.xlsx, CONFIG.csv, ABI_INDEX.csv (names may vary slightly but intent must be clear)\n\nScoring (structure-only):\n- 3.0: Valid PDF/DOCX, \u22653 pages, clearly labeled, and all 9 sections present with correct intent\n- 2.1: Valid PDF/DOCX, \u22653 pages, missing at most ONE section\n- 1.2: Valid PDF/DOCX but missing TWO sections or pages <3\n- 0.0: Wrong format (not PDF/DOCX) OR missing three or more required sections\n\nBe flexible with exact header names but strict about the presence of each required element. Do not judge code correctness.", "expectation": "A polished Submission Index PDF/DOCX that inventories components, explains privacy/UX requirements, includes build/test instructions, and references machine-readable artifacts: MANIFEST.csv, TEST_MATRIX.xlsx, CONFIG.csv, ABI_INDEX.csv, plus the source ZIP."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification & Correctness (Files, Structure, Sanity)", "description": "Now that the Submission Index exists, verify companion artifacts and basic plausibility using code rules + one LLM cross-check. Do not compile or unpack the ZIP; rely on manifest and matrices.", "is_required": false, "max_points": 7.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Required Submission Files Present", "description": "Check presence of core artifacts among outputs: source ZIP, MANIFEST.csv, TEST_MATRIX.xlsx, CONFIG.csv, ABI_INDEX.csv.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    import re\n    import pandas as pd\n    \n    outs = context.get_all_outputs() or []\n    def name_of(res):\n        return (getattr(res, 'name', None) or '').lower()\n    \n    present = {\n        'zip': any(name_of(r).endswith('.zip') for r in outs),\n        'manifest_csv': any(name_of(r).endswith('manifest.csv') or ('manifest' in name_of(r) and name_of(r).endswith('.csv')) for r in outs),\n        'test_matrix_xlsx': any((('test' in name_of(r) or 'matrix' in name_of(r) or 'result' in name_of(r)) and name_of(r).endswith('.xlsx')) for r in outs),\n        'config_csv': any(name_of(r).endswith('config.csv') or ('config' in name_of(r) and name_of(r).endswith('.csv')) for r in outs),\n        'abi_index_csv': any(name_of(r).endswith('abi_index.csv') or ('abi' in name_of(r) and 'index' in name_of(r) and name_of(r).endswith('.csv')) for r in outs),\n    }\n    keys = list(present.keys())\n    got = sum(1 for k in keys if present[k])\n    weight = 1.0\n    score = (got / len(keys)) * weight\n    missing = [k for k in keys if not present[k]]\n    return score, f\"Found {got}/{len(keys)} required files. Missing: {', '.join(missing) if missing else 'none'}.\""}, {"type": "code", "name": "MANIFEST.csv Schema & Coverage", "description": "Validate MANIFEST.csv has expected columns and at least minimal coverage of major components (frontend, contracts, zk, relayer). Also basic checksum sanity.", "weight": 1.5, "code": "def evaluate(workflow, context):\n    import re, pandas as pd\n    \n    outs = context.get_all_outputs() or []\n    manifest_res = None\n    for r in outs:\n        n = (getattr(r, 'name', '') or '').lower()\n        if n.endswith('manifest.csv') or ('manifest' in n and n.endswith('.csv')):\n            manifest_res = r\n            break\n    if not manifest_res:\n        return 0.0, 'MANIFEST.csv not found.'\n    try:\n        df = context.files.read_csv(manifest_res.id)\n    except Exception as e:\n        return 0.0, f'Failed to read MANIFEST.csv: {e}'\n    if df is None or df.empty:\n        return 0.0, 'MANIFEST.csv is empty.'\n    # Canonicalize columns\n    def canon(c):\n        return re.sub(r'[^a-z0-9]+','', str(c).strip().lower())\n    cols = {canon(c): c for c in df.columns}\n    required_aliases = {\n        'path': ['path','filepath','relpath'],\n        'type': ['type','kind','filetype'],\n        'size': ['size','sizebytes','bytes'],\n        'sha256': ['sha256','checksum','hash']\n    }\n    colmap = {}\n    for key, aliases in required_aliases.items():\n        for a in aliases:\n            if a in cols:\n                colmap[key] = cols[a]\n                break\n    missing_cols = [k for k in required_aliases.keys() if k not in colmap]\n    if missing_cols:\n        return 0.4, f\"MANIFEST.csv missing columns: {', '.join(missing_cols)}\"\n    # Basic coverage\n    path_col = colmap['path']\n    paths = df[path_col].astype(str).str.lower()\n    required_areas = ['frontend', 'contract', 'zk', 'relayer']\n    coverage = {area: paths.str.contains(area).any() for area in required_areas}\n    # Row count and checksum sanity\n    row_ok = len(df) >= 10\n    sha_col = colmap['sha256']\n    sample = df[sha_col].astype(str).head(10)\n    def is_sha(s):\n        return bool(re.fullmatch(r'[A-Fa-f0-9]{64}', s))\n    sha_ok = sample.apply(is_sha).mean() >= 0.7  # at least 70% of first 10 look like SHA-256\n    # Scoring\n    weight = 1.5\n    parts = []\n    score = 0.0\n    # Columns mostly present already -> base 0.6\n    if not missing_cols:\n        score += 0.6\n        parts.append('Columns OK')\n    # Coverage per area (0.15 each)\n    cov_points = 0.0\n    for area in required_areas:\n        if coverage.get(area, False):\n            cov_points += 0.15\n    score += cov_points\n    parts.append(f\"Coverage: {sum(1 for v in coverage.values() if v)}/{len(required_areas)}\")\n    # Row count (0.1) and checksum sanity (0.2)\n    if row_ok:\n        score += 0.1\n        parts.append('Rows>=10')\n    if sha_ok:\n        score += 0.2\n        parts.append('SHA256 sanity OK')\n    # Cap to weight\n    score = min(score, weight)\n    return score, '; '.join(parts)"}, {"type": "code", "name": "TEST_MATRIX.xlsx Structure & Pass Rates", "description": "Validate TEST_MATRIX.xlsx structure with required suites and reasonable pass rates. Do not judge test quality, only presence and plausibility.", "weight": 1.5, "code": "def evaluate(workflow, context):\n    import re\n    import pandas as pd\n    import numpy as np\n    \n    outs = context.get_all_outputs() or []\n    xlsx_res = None\n    for r in outs:\n        n = (getattr(r, 'name', '') or '').lower()\n        if n.endswith('.xlsx') and ('test' in n or 'matrix' in n or 'result' in n):\n            xlsx_res = r\n            break\n    if not xlsx_res:\n        return 0.0, 'TEST_MATRIX.xlsx not found.'\n    try:\n        xfp = context.files.get_path(xlsx_res.id)\n        xfile = pd.ExcelFile(xfp)\n        sheets = [s for s in xfile.sheet_names]\n    except Exception as e:\n        return 0.0, f'Failed to open TEST_MATRIX.xlsx: {e}'\n    required_suites = ['Unit Tests','Integration','ZK Proofs','Cross-Chain']\n    # Map expected names to actual sheet names via case-insensitive substring\n    def find_sheet(target):\n        t = target.lower()\n        for s in sheets:\n            if t in s.lower() or s.lower() in t or any(k in s.lower() for k in [t.replace(' ','')]):\n                return s\n        # fallback: nearest by tokens\n        for s in sheets:\n            if all(tok in s.lower() for tok in t.split()):\n                return s\n        return None\n    sheet_map = {t: find_sheet(t) for t in required_suites}\n    found = {k:v for k,v in sheet_map.items() if v}\n    # Column checks and pass rates\n    def canon(c):\n        return re.sub(r'[^a-z0-9]+','', str(c).strip().lower())\n    suite_info = {}\n    for t, s in found.items():\n        try:\n            df = pd.read_excel(xfp, sheet_name=s)\n        except Exception:\n            continue\n        if df is None or df.empty:\n            suite_info[t] = {'ok': False, 'pass_rate': 0.0, 'reason': 'empty'}\n            continue\n        ccols = {canon(c): c for c in df.columns}\n        has_id = any(k in ccols for k in ['testid','id'])\n        has_status = any(k in ccols for k in ['status','result'])\n        has_passed = 'passed' in ccols\n        # Determine pass rate\n        pr = None\n        if has_passed:\n            try:\n                vals = df[ccols['passed']].astype(str).str.lower().map(lambda x: x in ['true','1','pass','passed','ok','success'])\n                if vals.size > 0:\n                    pr = float(vals.mean())\n            except Exception:\n                pr = None\n        if pr is None and has_status:\n            try:\n                vals = df[ccols.get('status') or ccols.get('result')].astype(str).str.lower()\n                pr = float(vals.str.contains('pass|ok|success|succ', regex=True).mean()) if vals.size>0 else 0.0\n            except Exception:\n                pr = 0.0\n        if pr is None:\n            pr = 0.0\n        suite_info[t] = {'ok': has_id and (has_status or has_passed), 'pass_rate': pr}\n    # Scoring: presence (1.0 max) + pass rate plausibility (0.5 max)\n    weight = 1.5\n    presence_score = (len(found) / len(required_suites)) * 1.0\n    # Average pass rate across found sheets must be >= 0.5 for full 0.5\n    if found:\n        avg_pr = np.mean([suite_info[t]['pass_rate'] for t in found.keys()])\n        pr_score = 0.5 * max(0.0, min(1.0, (avg_pr - 0.3) / 0.7))  # 0.3->0, 1.0->full\n    else:\n        pr_score = 0.0\n    # Penalize if any found sheet lacks basic columns\n    penalties = 0.0\n    for t in found.keys():\n        if not suite_info[t]['ok']:\n            penalties += 0.1\n    score = max(0.0, min(weight, presence_score + pr_score - penalties))\n    feedback = f\"Found {len(found)}/{len(required_suites)} suites. Avg pass rate={(np.mean([suite_info[t]['pass_rate'] for t in found.keys()]) if found else 0):.2f}. Penalty={penalties:.2f}.\"\n    return score, feedback"}, {"type": "code", "name": "CONFIG.csv Sanity Checks", "description": "Validate configuration plausibility: Polygon chain id, Connext domain numeric, Aave pool address shape, non-empty supported assets, positive deposit sizes, reasonable lock period.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    import re, pandas as pd, numpy as np\n    outs = context.get_all_outputs() or []\n    cfg_res = None\n    for r in outs:\n        n = (getattr(r, 'name', '') or '').lower()\n        if n.endswith('config.csv') or ('config' in n and n.endswith('.csv')):\n            cfg_res = r\n            break\n    if not cfg_res:\n        return 0.0, 'CONFIG.csv not found.'\n    try:\n        df = context.files.read_csv(cfg_res.id)\n    except Exception as e:\n        return 0.0, f'Failed to read CONFIG.csv: {e}'\n    if df is None or df.empty:\n        return 0.0, 'CONFIG.csv is empty.'\n    # Try two shapes: key-value rows or single-row typed columns\n    kv = {}\n    cols = [str(c).strip().lower() for c in df.columns]\n    if set(['key','value']).issubset(cols) or set(['parameter','value']).issubset(cols):\n        key_col = 'key' if 'key' in cols else 'parameter'\n        key_col = df.columns[cols.index(key_col)]\n        val_col = df.columns[cols.index('value')]\n        for _, row in df.iterrows():\n            k = str(row[key_col]).strip().lower()\n            v = str(row[val_col]).strip()\n            kv[k] = v\n    else:\n        # Use first row as values\n        row = df.iloc[0].to_dict()\n        for k,v in row.items():\n            kv[str(k).strip().lower()] = '' if pd.isna(v) else str(v)\n    score = 0.0\n    weight = 1.0\n    fb = []\n    # Chain ID\n    chain_id = kv.get('chain_id') or kv.get('chainid')\n    try:\n        cid = int(float(chain_id)) if chain_id is not None and chain_id != '' else None\n    except Exception:\n        cid = None\n    if cid in (137, 80001):\n        score += 0.25; fb.append('chain_id OK')\n    else:\n        fb.append('chain_id missing/invalid (expect 137 or 80001)')\n    # Connext domain\n    connext = kv.get('connext_domain') or kv.get('connextdomain')\n    try:\n        cd = int(float(connext)) if connext else None\n    except Exception:\n        cd = None\n    if cd and cd > 0:\n        score += 0.15; fb.append('connext_domain OK')\n    else:\n        fb.append('connext_domain missing/invalid')\n    # Aave pool address shape\n    aave_addr = (kv.get('aave_pool_address') or kv.get('aavepooladdress') or '').strip()\n    if re.fullmatch(r'0x[a-fA-F0-9]{40}', aave_addr or ''):\n        score += 0.2; fb.append('aave_pool_address format OK')\n    else:\n        fb.append('aave_pool_address invalid')\n    # Supported assets\n    assets = (kv.get('supported_assets') or kv.get('assets') or '').strip()\n    assets_list = [a.strip() for a in re.split(r'[;,\\s]+', assets) if a.strip()]\n    if len(assets_list) >= 1:\n        score += 0.15; fb.append('supported_assets present')\n    else:\n        fb.append('supported_assets missing')\n    # Deposit sizes\n    dep = (kv.get('deposit_sizes') or kv.get('depositsizes') or '').strip()\n    dep_vals = []\n    for t in re.split(r'[;,\\s]+', dep):\n        try:\n            if t:\n                dep_vals.append(float(t))\n        except Exception:\n            pass\n    if dep_vals and all(d > 0 for d in dep_vals):\n        score += 0.15; fb.append('deposit_sizes positive')\n    else:\n        fb.append('deposit_sizes missing/invalid')\n    # Lock period\n    lockp = (kv.get('lock_period_seconds') or kv.get('lockperiodseconds') or '').strip()\n    try:\n        lp = int(float(lockp)) if lockp else None\n    except Exception:\n        lp = None\n    if lp and lp >= 3600:\n        score += 0.1; fb.append('lock_period_seconds >= 3600')\n    else:\n        fb.append('lock_period_seconds missing/too small')\n    # Cap and return\n    score = min(weight, score)\n    return score, '; '.join(fb)"}, {"type": "code", "name": "ABI_INDEX.csv Sanity", "description": "Validate ABI index lists multiple contracts with addresses and networks, indicating deploy artifacts are traceable.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    import re, pandas as pd\n    outs = context.get_all_outputs() or []\n    abi_res = None\n    for r in outs:\n        n = (getattr(r, 'name', '') or '').lower()\n        if n.endswith('abi_index.csv') or ('abi' in n and 'index' in n and n.endswith('.csv')):\n            abi_res = r\n            break\n    if not abi_res:\n        return 0.0, 'ABI_INDEX.csv not found.'\n    try:\n        df = context.files.read_csv(abi_res.id)\n    except Exception as e:\n        return 0.0, f'Failed to read ABI_INDEX.csv: {e}'\n    if df is None or df.empty:\n        return 0.0, 'ABI_INDEX.csv is empty.'\n    def canon(s):\n        import re\n        return re.sub(r'[^a-z0-9]+','', str(s).strip().lower())\n    cols = {canon(c): c for c in df.columns}\n    # Flexible columns\n    name_col = cols.get('contract') or cols.get('name')\n    addr_col = cols.get('address') or cols.get('deployedaddress')\n    net_col = cols.get('network') or cols.get('chain')\n    path_col = cols.get('abipath') or cols.get('artifactpath') or cols.get('path')\n    missing = [k for k,v in [('contract/name',name_col),('address',addr_col),('network',net_col),('path',path_col)] if v is None]\n    if missing:\n        return 0.4, f\"ABI_INDEX missing columns: {', '.join(missing)}\"\n    df2 = df[[name_col, addr_col, net_col, path_col]].dropna()\n    if df2.empty:\n        return 0.4, 'ABI_INDEX has no valid rows.'\n    # Validations\n    addr_ok = df2[addr_col].astype(str).str.fullmatch(r'0x[a-fA-F0-9]{40}').mean() if len(df2)>0 else 0\n    net_ok = df2[net_col].astype(str).str.lower().str.contains('polygon|mumbai|test|main').mean() if len(df2)>0 else 0\n    multi = len(df2) >= 2\n    score = 0.0\n    score += 0.4  # columns present baseline\n    score += 0.3 * float(addr_ok)\n    score += 0.2 * float(net_ok)\n    if multi:\n        score += 0.1\n    return min(1.0, score), f\"rows={len(df2)}, addr_ok={addr_ok:.2f}, net_ok={net_ok:.2f}\""}, {"type": "llm_judge", "name": "Privacy & Cross-Chain Spec Coverage (from Submission Index)", "description": "Cross-check that the Submission Index document explicitly covers key privacy and cross-chain requirements so testers know what to verify.", "weight": 1.0, "judge_prompt": "Review the Submission Index (the primary PDF/DOCX). Confirm that it explicitly mentions and briefly explains ALL of the following:\n- Fixed-size deposits to preserve anonymity sets\n- A mandatory anonymity delay/lock period before withdrawal\n- Commitment hash shown at deposit and required at withdrawal\n- Use of a nullifier to prevent double-spend\n- Yield accrual via Aave on Polygon\n- Cross-chain withdrawals via Connext\n- Optional relayer service to submit withdrawals privately\n\nScoring:\n- 1.0: All 7 items are explicitly described\n- 0.7: 5\u20136 items covered\n- 0.4: 3\u20134 items covered\n- 0.1: 1\u20132 items covered\n- 0.0: None of the items are covered\n\nOnly check presence of these points in the document; do not assess correctness or feasibility.", "expectation": "The document contains a concise but clear explanation of all privacy and cross-chain elements so that verifiers can trace them to code/tests."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality & Readiness", "description": "Holistic LLM assessment of clarity, professionalism, and readiness to build/test/deploy based on the Submission Index.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Overall Professionalism and Utility", "description": "Assess if the Submission Index would enable a competent engineer to build, test, and evaluate the system in a reasonable time.", "weight": 2.0, "judge_prompt": "Judge the overall professional quality and readiness of the Submission Index (primary PDF/DOCX). Consider:\n- Clarity and completeness of build/run/test instructions\n- Accuracy and usefulness of component inventory and paths\n- Presence of risks/assumptions/limitations and environment requirements\n- Consistency between sections (e.g., listed artifacts match references)\n- Formatting quality and ease of navigation\n\nScoring:\n- 2.0: Highly professional, precise, internally consistent, and immediately actionable\n- 1.4: Generally solid with minor gaps; still actionable with little friction\n- 0.8: Mixed quality; noticeable gaps or inconsistencies slow down validation\n- 0.3: Hard to follow; major omissions impede validation\n- 0.0: Not useful for building/testing (confusing, missing key info)\n\nEvaluate presentation and usefulness only; do not judge code correctness or execution.", "expectation": "A polished, internally consistent, and practically useful Submission Index enabling efficient validation."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "2ea2e5b5-257f-42e6-a7dc-93763f28b19d", "rubric": {"category_name": "Work-Time Study Presentation (CIO/IT Management)", "rationale": "This rubric enforces a self-documenting, verifiable deliverable for a Mixed task (slides with embedded data/analysis). Stage 1 is a strict LLM-only gate that mandates a five-slide PPTX/PDF with specific slide purposes and the presence of both tables and pie charts so that verification is possible. Stage 2 mixes LLM checks (for category-to-segment mapping correctness and table\u2194pie consistency) with a light, resilient code rule that attempts text extraction from PDF/DOCX to confirm all 12 categories are present. Stage 3 assesses professional presentation quality and audience fit. The shape-first approach ensures downstream checks are trivial and robust.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate", "description": "Enforce exact deliverable shape: a 5-slide PowerPoint or PDF with required tables and pie charts for the four analysis slides.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Presentation Structure and Elements (5-Slide Gate)", "description": "Output must be a PPTX or PDF with exactly 5 slides and specific required content on each slide. Only verify presence/structure, not correctness of numbers.", "weight": 4.0, "judge_prompt": "You are grading the SHAPE of the candidate\u2019s deliverable. Only assess format and the presence of required structural elements. Do NOT judge numeric correctness or the quality of insights.\n\nExpected deliverable format:\n- A PowerPoint (.pptx) OR a PDF export of slides (.pdf)\n- Exactly 5 slides\n\nRequired slide structure (names may vary slightly; be flexible with synonyms):\n1) Title Slide\n   - Must include a prominent title referencing a work-time or time study (e.g., \u201cWork Time Study\u201d, \u201cEmployee Time Analysis\u201d, \u201cWorkload Analysis\u201d).\n   - Must include at least one subtitle element (e.g., organization/department name, time period/week, or author/IT/PMO attribution).\n\n2) Activity Analysis\n   - Must include a table enumerating the 12 high-level activity categories (categories list below) with some quantitative field (e.g., hours, count, or percentage). Column names can vary but the intent must be clear.\n   - Must include a pie chart summarizing overall distribution across the 12 categories.\n\n3) Margin Impact by Activities\n   - Must include a table mapping each of the 12 categories to Margin Impact segment (Cost vs Investment). Column names can vary but mapping must be present.\n   - Must include a pie chart summarizing the distribution (e.g., share of time or count) by Cost vs Investment.\n\n4) Time Sensitivity by Activities\n   - Must include a table mapping each of the 12 categories to Time Sensitivity (Low/Medium/High).\n   - Must include a pie chart summarizing distribution by Low/Medium/High.\n\n5) Strategic Level by Activities\n   - Must include a table mapping each of the 12 categories to Strategic Level (Low/Medium/High).\n   - Must include a pie chart summarizing distribution by Low/Medium/High.\n\n12 high-level activity categories (naming variations acceptable if clearly the same concept):\n- Audit/Compliance (also written as \u201cAudit / Compliance\u201d)\n- Automation\n- Break/Fix\n- Change Management Meeting\n- Deployment of Upgrades\n- Develop/Integrate Tooling\n- Patching\n- Problem Management\n- Process Improvement\n- Service Request\n- Shift Handover Meeting\n- Training\n\nScoring for this gate (structure only):\n- 4.0: Correct format (PPTX or PDF), exactly 5 slides, and each required slide (2\u20135) includes BOTH a table AND a pie chart for the specified topic. Title slide includes a clear title and a subtitle.\n- 3.0: Correct format and exactly 5 slides; all four analysis slides exist and have the intended topic. One minor structural element is missing (e.g., a single table or a single pie chart missing from one slide, or the title slide lacks a subtitle) but overall the 5-slide structure and topics are clear.\n- 2.0: Correct format but multiple structural issues: wrong slide count (4 or 6) OR more than one required element missing (e.g., multiple slides missing tables or pies) yet the overall intent is still recognizable.\n- 0.0: Not PPTX/PDF OR fewer than 4 slides OR the required structure is largely absent.\n\nImportant: Only verify presence/structure, not the correctness of mappings or numbers.", "expectation": "A 5-slide PPTX/PDF deck: Title; Activity Analysis (table + pie); Margin Impact by Activities (table + pie); Time Sensitivity by Activities (table + pie); Strategic Level by Activities (table + pie)."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification of Mappings and Internal Consistency", "description": "Now that the shape is correct, verify that category-to-segment assignments match the specification and that tables and charts are mutually consistent.", "is_required": false, "max_points": 4.5, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Activity Analysis Integrity (Slide 2)", "description": "Checks that Slide 2 contains all 12 categories in a table and that the pie chart summarizes the same categories.", "weight": 0.6, "judge_prompt": "Evaluate only Slide 2 (Activity Analysis). Confirm:\n- A table enumerates the 12 categories (allowing reasonable naming variants): Audit/Compliance (Audit / Compliance), Automation, Break/Fix, Change Management Meeting, Deployment of Upgrades, Develop/Integrate Tooling, Patching, Problem Management, Process Improvement, Service Request, Shift Handover Meeting, Training.\n- The pie chart on Slide 2 summarizes the distribution across these same categories (labels align with the table list; proportions need only be directionally consistent, not numerically verified).\nScoring:\n- 0.6: All 12 categories are present in the table AND the pie references the same categories.\n- 0.3: Table lists at least 10 categories and the pie reflects the same set.\n- 0.0: Fewer than 10 categories in the table OR the pie does not correspond to the table categories.", "expectation": "Slide 2 table lists all 12 categories; pie chart reflects the same 12 categories."}, {"type": "llm_judge", "name": "Correct Mapping \u2014 Margin Impact (Cost vs Investment)", "description": "Check that each category is mapped to the correct Margin Impact segment per specification.", "weight": 1.2, "judge_prompt": "Evaluate the Margin Impact-by-Activities table (Slide 3). Check mapping against these rules:\n- Cost Activities: Audit/Compliance (Audit / Compliance), Break/Fix, Deployment of Upgrades, Patching, Service Request, Shift Handover Meeting.\n- Investment Activities: Automation, Change Management Meeting, Develop/Integrate Tooling, Problem Management, Process Improvement, Training.\nRequirements:\n- Each of the 12 categories must be assigned to exactly one of the two segments, matching the lists above.\nScoring:\n- 1.2: All 12 categories assigned and every assignment matches the specification.\n- 0.8: 1\u20132 mismatches or missing assignments.\n- 0.4: 3\u20134 mismatches or missing assignments.\n- 0.0: 5+ mismatches/missing OR mapping not present.", "expectation": "All categories correctly assigned: the six cost activities labeled Cost; the six investment activities labeled Investment."}, {"type": "llm_judge", "name": "Correct Mapping \u2014 Time Sensitivity (Low/Medium/High)", "description": "Check that each category is mapped to the correct Time Sensitivity per specification.", "weight": 1.2, "judge_prompt": "Evaluate the Time Sensitivity-by-Activities table (Slide 4). Check mappings against these rules:\n- High: Break/Fix\n- Medium: Audit/Compliance (Audit / Compliance), Automation, Change Management Meeting, Develop/Integrate Tooling, Problem Management, Process Improvement, Patching, Service Request\n- Low: Deployment of Upgrades, Shift Handover Meeting, Training\nRequirements:\n- Each of the 12 categories must be assigned exactly one of High/Medium/Low per the lists above.\nScoring:\n- 1.2: All 12 categories assigned and every assignment matches the specification.\n- 0.8: 1\u20132 mismatches or missing assignments.\n- 0.4: 3\u20134 mismatches or missing assignments.\n- 0.0: 5+ mismatches/missing OR mapping not present.", "expectation": "All categories correctly assigned to Low/Medium/High time sensitivity per the specification."}, {"type": "llm_judge", "name": "Correct Mapping \u2014 Strategic Level (Low/Medium/High)", "description": "Check that each category is mapped to the correct Strategic Level per specification.", "weight": 1.2, "judge_prompt": "Evaluate the Strategic Level-by-Activities table (Slide 5). Check mappings against these rules:\n- High: Automation, Problem Management, Process Improvement\n- Medium: Audit/Compliance (Audit / Compliance), Change Management Meeting, Develop/Integrate Tooling, Service Request, Shift Handover Meeting, Training\n- Low: Break/Fix, Deployment of Upgrades, Patching\nRequirements:\n- Each of the 12 categories must be assigned exactly one of High/Medium/Low per the lists above.\nScoring:\n- 1.2: All 12 categories assigned and every assignment matches the specification.\n- 0.8: 1\u20132 mismatches or missing assignments.\n- 0.4: 3\u20134 mismatches or missing assignments.\n- 0.0: 5+ mismatches/missing OR mapping not present.", "expectation": "All categories correctly assigned to Low/Medium/High strategic level per the specification."}, {"type": "llm_judge", "name": "Pie \u2194 Table Consistency on Slides 3\u20135", "description": "Check that the pie charts on Slides 3\u20135 reflect the same segment breakdowns as their tables (labels and overall distribution).", "weight": 0.2, "judge_prompt": "For each of Slides 3\u20135, check that the pie chart labels correspond to the segment labels shown in the adjacent table and that the slice grouping aligns with the table\u2019s grouping (exact percentages are not required; directional consistency and label alignment are sufficient).\nScoring:\n- 0.2: All relevant pies align with their tables in labeling and grouping.\n- 0.1: One pie has minor mislabeling or unclear correspondence.\n- 0.0: Multiple pies do not align or the relationship is unclear.", "expectation": "All pies on Slides 3\u20135 have labels and slice groupings consistent with their tables."}, {"type": "code", "name": "Text Coverage of All 12 Categories (PDF/DOCX only)", "description": "Attempts to extract text from PDF/DOCX to verify that all 12 categories are mentioned somewhere in the document. Skips PPTX (returns 0).", "weight": 0.1, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n\n    Returns:\n        float or (float, str) in [0, 1]\n    \"\"\"\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No primary output found.\"\n\n    # Only attempt text extraction for PDF/DOCX; PPTX not supported by helpers.\n    text = None\n    try:\n        if getattr(output, 'is_document', False):\n            # Try PDF first\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    text = None\n        elif getattr(output, 'is_text_format', False):\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = None\n    except Exception:\n        text = None\n\n    # Define categories with common variants\n    categories = [\n        [\"Audit/Compliance\", \"Audit / Compliance\", \"Audit-Compliance\"],\n        [\"Automation\"],\n        [\"Break/Fix\", \"Break - Fix\", \"Breakfix\"],\n        [\"Change Management Meeting\"],\n        [\"Deployment of Upgrades\", \"Deployments of Upgrades\"],\n        [\"Develop/Integrate Tooling\", \"Develop / Integrate Tooling\", \"Develop-Integrate Tooling\", \"Develop & Integrate Tooling\"],\n        [\"Patching\"],\n        [\"Problem Management\"],\n        [\"Process Improvement\"],\n        [\"Service Request\", \"Service Requests\"],\n        [\"Shift Handover Meeting\", \"Shift-HandOver Meeting\", \"Shift Handover\"],\n        [\"Training\"]\n    ]\n\n    if not text:\n        return 0.0, \"Could not extract text (likely PPTX). Rule weight is minimal; skipping.\"\n\n    t = text.lower()\n\n    def any_variant_present(variants):\n        for v in variants:\n            # loosen matching: collapse whitespace and slashes/hyphens\n            pattern = re.escape(v.lower())\n            if v.lower() in t:\n                return True\n            # Try without spaces around slashes\n            compact = v.lower().replace(\" / \", \"/\").replace(\" - \", \"-\")\n            if compact in t:\n                return True\n        return False\n\n    found = 0\n    missing_list = []\n    for variants in categories:\n        if any_variant_present(variants):\n            found += 1\n        else:\n            missing_list.append(variants[0])\n\n    score = found / 12.0\n    feedback = f\"Found {found}/12 categories in extracted text. Missing: {', '.join(missing_list)}\" if missing_list else \"All 12 categories found.\"\n    return score, feedback"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Presentation Quality and Professionalism", "description": "Assess overall clarity, design quality, and executive appropriateness for an IT management audience.", "is_required": false, "max_points": 1.5, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Visual Clarity, Consistency, and Executive Readability", "description": "Evaluate visual design quality and appropriateness for an enterprise IT leadership audience.", "weight": 1.5, "judge_prompt": "Evaluate the overall presentation quality for a CIO/IT management audience.\nConsider:\n- Clear, consistent slide titling aligned to the requested topics.\n- Legible tables (readable fonts, adequate contrast, uncluttered layouts) and charts with labels/legends that make categories/segments obvious.\n- Consistent color palette and formatting across slides; pie chart colors consistent within a slide.\n- Minimal chartjunk (no unnecessary 3D, shadows that obscure data, or excessive text).\n- Professional tone and suitable for enterprise stakeholders.\nScoring:\n- 1.5: Highly professional and consistent; tables and pies are easy to read; labeling and legends are clear; appropriate for execs.\n- 0.8: Generally clear but with minor readability or consistency issues (e.g., small fonts, color inconsistencies).\n- 0.3: Several readability/design issues that impede understanding.\n- 0.0: Poorly formatted; difficult to read; not suitable for professional audience.", "expectation": "A clean, consistent, executive-ready deck with readable tables and clearly labeled pie charts."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "0818571f-5ff7-4d39-9d2c-ced5ae44299e", "rubric": {"category_name": "Florida Retail Shopping Center Acquisition Sourcing", "rationale": "Mixed deliverable: a professional deal package (PDF/DOCX report) plus a normalized spreadsheet enables deterministic verification. Stage 1 forces exact structure (LLM-only gate). Stage 2 uses code to verify listing counts, dates, columns, and financial consistency leveraging the structured spreadsheet mandated by Stage 1, plus an LLM cross-reference. Stage 3 evaluates presentation quality and strategic fit for investor decisioning.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Deal Package Structure Gate", "description": "LLM-only check that the outputs include BOTH: (1) a professional report (PDF or DOCX) with required sections and per-property profiles, and (2) a companion spreadsheet/CSV with a normalized Property Summary sheet/table enabling verification.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Package Structure and Completeness (Report + Spreadsheet)", "description": "Verify the deliverables follow the mandated structure so downstream verification is trivial.", "weight": 4.0, "judge_prompt": "You are evaluating whether the candidate delivered a complete, verifiable deal package for retail shopping center acquisitions.\n\nCheck the following strictly as STRUCTURE/PRESENCE (not content correctness):\n\nA) Report Deliverable (PDF or DOCX; at least 3 pages):\n1) Cover page with title indicating shopping center acquisitions, date (June 2025 or later), and broker contact details.\n2) Investment Criteria Summary: a section that restates the investor\u2019s criteria used to source deals (may be titled \u201cInvestor Criteria\u201d, \u201cAcquisition Parameters\u201d, or similar).\n3) Property Shortlist Overview: states a shortlist of 5\u201310 active listings sourced from Crexi/LoopNet (or similar public platforms) dated June 2025 to present.\n4) Property Profiles (one per shortlisted property), each including:\n   - At least one property photo\n   - A map of the surrounding area or location\n   - Tenant mix/roster or anchor tenants\n   - Key facts: GLA, year built/renovated\n   - Financial key items: asking price, NOI, cap rate (if available)\n   - Source link(s) to the listing(s)\n   - Brief note on stabilized vs value-add rationale\n5) Comparative Summary Table aggregating key fields across all shortlisted properties (rows=properties).\n6) Next Steps / Process section describing evaluation-to-LOI workflow.\n\nB) Spreadsheet/CSV Deliverable (Excel .xlsx or .csv):\n- Contains a normalized table on a sheet named \u201cProperty Summary\u201d or similar (e.g., \u201cSummary\u201d, \u201cDeal Summary\u201d, \u201cProperty Index\u201d).\n- Table should include columns (names can be close variants):\n  Property Name; Address; City; State; Zip; MSA/Market; Source Platform; Listing URL; Date Accessed; Listing Status; Asking Price (USD); NOI (USD); Cap Rate (%); GLA (SF); Year Built; Year Renovated; Occupancy (%); Tenants Count; Anchor Tenants; Lease Type; WALT (yrs); Lot Size (acres); Price per SF; Value-Add Notes.\n- Include a second sheet/section like \u201cAssumptions & Notes\u201d explaining data sources and data currency (date pulled) and any assumptions.\n\nScoring (return a score from 0.0 to 1.0):\n- 1.0: Report (PDF/DOCX) meets items A1\u2013A6 AND a spreadsheet/CSV exists with a Property Summary sheet/table including most columns above (\u226518/24) AND an Assumptions/Notes sheet/section.\n- 0.8: Report fully present but spreadsheet is present with Property Summary table missing a few key columns (\u226514/24) OR Notes sheet/section missing.\n- 0.6: Report present with Property Profiles and Comparative Summary, but spreadsheet/CSV missing entirely OR present but lacks a recognizable Property Summary table.\n- 0.4: Report present but missing major structural elements (e.g., missing photos/maps or comparative table) and spreadsheet/CSV missing or structurally unusable.\n- 0.0: Not a PDF/DOCX report OR fewer than 3 pages OR no property profiles OR no spreadsheet/CSV.\n\nOnly evaluate presence/structure. Do not judge financial correctness or presentation quality here.", "expectation": "A professional PDF/DOCX report with all required sections plus a companion spreadsheet/CSV containing a normalized Property Summary table and a notes/assumptions sheet."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification: Data Validity and Consistency", "description": "Code- and LLM-based verification using the normalized spreadsheet mandated by Stage 1, plus cross-referencing with the report.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 2.0, "rules": [{"type": "code", "name": "Property Summary Table Found + Required Columns", "description": "Detect the primary spreadsheet/CSV and verify a recognizable Property Summary table exists with required columns (fuzzy names allowed).", "weight": 1.2, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        # Helper: find best spreadsheet among all outputs\n        all_outputs = context.get_all_outputs() or []\n        spreadsheets = [r for r in all_outputs if getattr(r, 'is_spreadsheet', False)]\n        if not spreadsheets:\n            return 0.0, 'No spreadsheet/CSV output found.'\n\n        def load_tables(res):\n            # returns list of (sheet_name, df)\n            path = context.files.get_path(res.id)\n            p = str(path).lower()\n            tables = []\n            try:\n                if p.endswith('.csv'):\n                    df = pd.read_csv(path)\n                    tables.append(('csv', df))\n                else:\n                    xls = pd.ExcelFile(path)\n                    for sn in xls.sheet_names:\n                        try:\n                            tables.append((sn, pd.read_excel(path, sheet_name=sn)))\n                        except Exception:\n                            pass\n            except Exception:\n                return []\n            return tables\n\n        # Synonyms map for fuzzy header matching\n        synonyms = {\n            'property_name': ['property name','name','center name','asset','asset name','shopping center'],\n            'address': ['address','street address','site address','property address'],\n            'city': ['city','municipality'],\n            'state': ['state','st'],\n            'zip': ['zip','zipcode','postal code'],\n            'msa': ['msa','market','metro','city/msa','market (msa)'],\n            'source': ['source','platform','listing source','marketplace'],\n            'url': ['listing url','url','link','listing link','web link'],\n            'date': ['date accessed','date','pulled date','as of date','data date','last updated'],\n            'status': ['listing status','status','availability','on-market status'],\n            'price': ['asking price','price','list price','asking price (usd)'],\n            'noi': ['noi','net operating income','noi (usd)'],\n            'cap_rate': ['cap rate','cap','capitalization rate','cap rate (%)'],\n            'gla': ['gla','gross leasable area','size (sf)','size','building area','rentable area'],\n            'year_built': ['year built','built','yr built'],\n            'year_renov': ['year renovated','renovated','yr renovated','renovation year'],\n            'occupancy': ['occupancy','occupancy %','occ %'],\n            'tenants_count': ['tenants count','number of tenants','# tenants','tenant count'],\n            'anchors': ['anchor tenants','anchors','major tenants','key tenants'],\n            'lease_type': ['lease type','leases','nnn/nn/fg'],\n            'walt': ['walt','weighted average lease term','walt (yrs)','wal t'],\n            'lot_size': ['lot size','site size','acres','site area (ac)'],\n            'pps': ['price per sf','$/sf','price/sf','ppsf'],\n            'notes': ['value-add notes','notes','business plan','remarks']\n        }\n        required_keys = ['property_name','address','city','state','source','url','date','price','gla','cap_rate','noi']\n        nice_to_have = ['year_built','year_renov','occupancy','tenants_count','anchors','lease_type','walt','lot_size','pps']\n\n        def normalize_cols(cols):\n            return [re.sub(r'\\s+', ' ', str(c)).strip().lower() for c in cols]\n\n        def find_matches(df):\n            cols = normalize_cols(df.columns)\n            mapping = {}\n            for k, syns in synonyms.items():\n                found = None\n                for s in syns:\n                    for i,c in enumerate(cols):\n                        if s in c:\n                            found = df.columns[i]\n                            break\n                    if found is not None:\n                        break\n                mapping[k] = found\n            return mapping\n\n        best = None\n        best_score = -1\n        best_info = None\n        for res in spreadsheets:\n            for sn, df in load_tables(res):\n                if df is None or df.shape[1] == 0:\n                    continue\n                mapping = find_matches(df)\n                have_req = sum(1 for k in required_keys if mapping.get(k) is not None)\n                have_opt = sum(1 for k in nice_to_have if mapping.get(k) is not None)\n                score = have_req * 10 + have_opt  # prioritize required\n                if score > best_score:\n                    best_score = score\n                    best = (res, sn, df)\n                    best_info = mapping\n        if best is None:\n            return 0.0, 'No recognizable Property Summary table found in spreadsheets.'\n\n        have_req = [k for k in required_keys if best_info.get(k) is not None]\n        missing_req = [k for k in required_keys if best_info.get(k) is None]\n        have_opt = [k for k in nice_to_have if best_info.get(k) is not None]\n\n        # Score: required columns drive majority; partial credit for optional\n        req_ratio = len(have_req) / max(1, len(required_keys))\n        opt_ratio = len(have_opt) / max(1, len(nice_to_have))\n        score = 0.8*req_ratio + 0.2*opt_ratio\n        feedback = f\"Found sheet with {len(have_req)}/{len(required_keys)} required and {len(have_opt)}/{len(nice_to_have)} optional columns. Missing required: {', '.join(missing_req) if missing_req else 'None'}.\"\n        return max(0.0, min(1.0, score)), feedback\n    except Exception as e:\n        return 0.0, f'Error verifying columns: {e}'"}, {"type": "code", "name": "Listing Count, Date Window, and Active Status", "description": "Validate 5\u201310 listings, dates from 2025-06-01 to today, and predominantly Active/Available status.", "weight": 1.2, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        all_outputs = context.get_all_outputs() or []\n        spreadsheets = [r for r in all_outputs if getattr(r, 'is_spreadsheet', False)]\n        if not spreadsheets:\n            return 0.0, 'No spreadsheet/CSV output found.'\n\n        def load_tables(res):\n            path = context.files.get_path(res.id)\n            p = str(path).lower()\n            tables = []\n            try:\n                if p.endswith('.csv'):\n                    df = pd.read_csv(path)\n                    tables.append(('csv', df))\n                else:\n                    xls = pd.ExcelFile(path)\n                    for sn in xls.sheet_names:\n                        try:\n                            tables.append((sn, pd.read_excel(path, sheet_name=sn)))\n                        except Exception:\n                            pass\n            except Exception:\n                return []\n            return tables\n\n        def normalize_cols(df):\n            return [re.sub(r'\\s+', ' ', str(c)).strip().lower() for c in df.columns]\n\n        def find_col(df, keys):\n            cols = normalize_cols(df)\n            for key in keys:\n                for i,c in enumerate(cols):\n                    if key in c:\n                        return df.columns[i]\n            return None\n\n        best_df = None\n        best_hits = -1\n        best_res_sn = None\n        for res in spreadsheets:\n            for sn, df in load_tables(res):\n                cols = normalize_cols(df)\n                hits = sum(any(k in c for k in ['property name','center name','asset']) for c in cols)\n                hits += sum(any(k in c for k in ['date accessed','date','pulled date','as of']) for c in cols)\n                if hits > best_hits:\n                    best_hits = hits\n                    best_df = df\n                    best_res_sn = (res, sn)\n        if best_df is None or best_df.shape[0] == 0:\n            return 0.0, 'No usable Property Summary table found.'\n\n        name_col = find_col(best_df, ['property name','center name','asset'])\n        date_col = find_col(best_df, ['date accessed','date','pulled date','as of'])\n        status_col = find_col(best_df, ['listing status','status','availability','on-market'])\n\n        # Listing count\n        if name_col is None:\n            return 0.0, 'Property name column not found.'\n        names = best_df[name_col].dropna().astype(str).str.strip()\n        n_props = (names != '').sum()\n        count_ok = 1.0 if 5 <= n_props <= 10 else 0.0\n\n        # Date window check\n        date_ok_ratio = 0.0\n        if date_col is not None:\n            dates_raw = best_df[date_col]\n            # Try parsing with pandas\n            dates = pd.to_datetime(dates_raw, errors='coerce', infer_datetime_format=True)\n            lo = pd.Timestamp('2025-06-01')\n            hi = pd.Timestamp.today() + pd.Timedelta(days=1)\n            valid = dates.between(lo, hi)\n            if valid.notna().sum() > 0:\n                date_ok_ratio = valid.mean()\n        # If no date column, 0 credit for dates\n\n        # Status check\n        status_ratio = 0.0\n        if status_col is not None:\n            st = best_df[status_col].astype(str).str.lower()\n            status_ratio = st.str.contains('active|available|on.?market').mean()\n\n        # Weighted combination\n        score = 0.4*count_ok + 0.4*date_ok_ratio + 0.2*status_ratio\n        feedback = f\"Listings: {n_props} (5-10 target). Date validity ratio: {date_ok_ratio:.2f}. Active/Available ratio: {status_ratio:.2f}.\"\n        return max(0.0, min(1.0, score)), feedback\n    except Exception as e:\n        return 0.0, f'Error in count/date/status checks: {e}'"}, {"type": "code", "name": "Financial Consistency and Plausibility", "description": "Verify that cap rate \u2248 NOI/Price, Price/SF \u2248 Price/GLA, and values fall in plausible ranges.", "weight": 1.2, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        all_outputs = context.get_all_outputs() or []\n        spreadsheets = [r for r in all_outputs if getattr(r, 'is_spreadsheet', False)]\n        if not spreadsheets:\n            return 0.0, 'No spreadsheet/CSV output found.'\n\n        def load_tables(res):\n            path = context.files.get_path(res.id)\n            p = str(path).lower()\n            tables = []\n            try:\n                if p.endswith('.csv'):\n                    df = pd.read_csv(path)\n                    tables.append(('csv', df))\n                else:\n                    xls = pd.ExcelFile(path)\n                    for sn in xls.sheet_names:\n                        try:\n                            tables.append((sn, pd.read_excel(path, sheet_name=sn)))\n                        except Exception:\n                            pass\n            except Exception:\n                return []\n            return tables\n\n        def norm_cols(df):\n            return [re.sub(r'\\s+', ' ', str(c)).strip().lower() for c in df.columns]\n\n        def find_col(df, keys):\n            cols = norm_cols(df)\n            for key in keys:\n                for i,c in enumerate(cols):\n                    if key in c:\n                        return df.columns[i]\n            return None\n\n        def to_float(x):\n            if pd.isna(x):\n                return np.nan\n            if isinstance(x, (int, float, np.number)):\n                return float(x)\n            s = str(x).strip().lower()\n            had_percent = '%' in s\n            s = s.replace('$','')\n            s = s.replace(',','')\n            s = re.sub(r'(sf|sq\\s*ft|sqft|ft\\^2|ft\u00b2)', '', s)\n            s = s.strip()\n            try:\n                v = float(s)\n            except Exception:\n                return np.nan\n            if had_percent:\n                v = v/100.0\n            return v\n\n        best_df = None\n        best_hits = -1\n        for res in spreadsheets:\n            for sn, df in load_tables(res):\n                cols = norm_cols(df)\n                hits = sum(any(k in c for k in ['price','asking price','list price']) for c in cols)\n                hits += sum(any(k in c for k in ['noi']) for c in cols)\n                hits += sum(any(k in c for k in ['cap rate','cap']) for c in cols)\n                hits += sum(any(k in c for k in ['gla','gross leasable']) for c in cols)\n                if hits > best_hits:\n                    best_hits = hits\n                    best_df = df\n        if best_df is None or best_df.shape[0] == 0:\n            return 0.0, 'No usable financial table found.'\n\n        price_col = find_col(best_df, ['asking price','price','list price'])\n        noi_col = find_col(best_df, ['noi','net operating income'])\n        cap_col = find_col(best_df, ['cap rate','cap'])\n        gla_col = find_col(best_df, ['gla','gross leasable area','size'])\n        ppsf_col = find_col(best_df, ['price per sf','ppsf','$/sf'])\n        occ_col = find_col(best_df, ['occupancy'])\n        yb_col = find_col(best_df, ['year built','built'])\n        yr_col = find_col(best_df, ['year renovated','renov'])\n        walt_col = find_col(best_df, ['walt'])\n\n        # Build series\n        price = best_df[price_col].apply(to_float) if price_col else pd.Series(dtype=float)\n        noi = best_df[noi_col].apply(to_float) if noi_col else pd.Series(dtype=float)\n        cap = best_df[cap_col].apply(to_float) if cap_col else pd.Series(dtype=float)\n        gla = best_df[gla_col].apply(to_float) if gla_col else pd.Series(dtype=float)\n        ppsf = best_df[ppsf_col].apply(to_float) if ppsf_col else pd.Series(dtype=float)\n        occ = best_df[occ_col].apply(to_float) if occ_col else pd.Series(dtype=float)\n        yb = best_df[yb_col].apply(to_float) if yb_col else pd.Series(dtype=float)\n        yr = best_df[yr_col].apply(to_float) if yr_col else pd.Series(dtype=float)\n        walt = best_df[walt_col].apply(to_float) if walt_col else pd.Series(dtype=float)\n\n        checks = []\n        # Cap rate consistency: abs(cap - NOI/Price) <= 1.0 percentage point\n        if not price.empty and not noi.empty and not cap.empty:\n            implied = noi / price\n            diff = (cap - implied).abs()\n            ok = diff <= 0.01\n            checks.append(ok.fillna(False))\n        # Price per SF consistency: within 5%\n        if not price.empty and not gla.empty and not ppsf.empty:\n            implied_ppsf = price / gla.replace(0, np.nan)\n            rel_err = (ppsf - implied_ppsf).abs() / implied_ppsf.replace(0, np.nan)\n            ok = rel_err <= 0.05\n            checks.append(ok.fillna(False))\n        # Plausibility bounds\n        if not occ.empty:\n            checks.append((occ.between(0,100)).fillna(False))\n        if not gla.empty:\n            checks.append((gla.between(1000, 2_000_000)).fillna(False))\n        if not cap.empty:\n            checks.append((cap.between(0.01, 0.20)).fillna(False))\n        if not yb.empty:\n            checks.append((yb.between(1950, 2026)).fillna(False))\n        if not yr.empty and not yb.empty:\n            # renovated year should be >= built and <= 2026\n            checks.append((yr >= yb) & (yr <= 2026))\n        if not walt.empty:\n            checks.append((walt.between(0, 30)).fillna(False))\n\n        if not checks:\n            return 0.0, 'Insufficient financial fields to verify consistency.'\n\n        # Compute overall ratio across available checks\n        ratios = [c.mean() for c in checks if hasattr(c, 'mean') and c.size > 0]\n        score = float(np.nanmean(ratios)) if ratios else 0.0\n        score = max(0.0, min(1.0, score))\n        feedback = ' | '.join([f'Check{i+1}:{r:.2f}' for i,r in enumerate(ratios)])\n        return score, feedback\n    except Exception as e:\n        return 0.0, f'Error in financial consistency checks: {e}'"}, {"type": "llm_judge", "name": "Cross-Reference: Report vs Spreadsheet", "description": "Spot-check that the properties listed in the report correspond to the spreadsheet rows and that key figures align at a glance.", "weight": 0.4, "judge_prompt": "Cross-check the report (PDF/DOCX) against the spreadsheet/CSV:\n- Do the property names in the report\u2019s Comparative Summary table match the rows in the spreadsheet\u2019s Property Summary (allow minor naming variations)?\n- Pick at least 3 properties and compare visually the key numbers (Asking Price, NOI, Cap Rate, GLA) shown in the report vs. the spreadsheet. Are they consistent at a glance (within rounding/formatting)?\n- Are photos and maps present for each profiled property in the report (presence only)?\n\nScoring (0.0\u20131.0):\n- 1.0: Names align and 3+ spot-checks are consistent; photos+maps present for all properties.\n- 0.7: Names align with minor omissions; 2\u20133 spot-checks consistent; most photos/maps present.\n- 0.4: Partial alignment; only 1\u20132 spot-checks consistent or many missing visuals.\n- 0.0: Mismatch between report and spreadsheet or visuals largely missing.", "expectation": "Strong alignment between report content and spreadsheet data with visuals present for each property."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Investor Readiness", "description": "Holistic appraisal of presentation quality, utility for underwriting and LOI, and strategic fit with investor criteria.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Underwriting Utility", "description": "Evaluate clarity, layout, and usefulness for swift investor decisioning and underwriting handoff.", "weight": 1.2, "judge_prompt": "Assess the PDF/DOCX report for professionalism and utility:\n- Is the document well-structured, branded, and easy to navigate (clear headers, tables, readable visuals)?\n- Does the Comparative Summary table allow quick comparison (prices, NOI, cap rate, GLA, occupancy, year built/renovated)?\n- Are listing links clickable and clearly labeled? Are sources cited?\n- Is there a concise summary of key risks/opportunities or value-add levers per asset that would help underwriting?\n\nScoring (0.0\u20131.0):\n- 1.0: Highly professional layout, clear comparisons, links/sources, and actionable notes for underwriting.\n- 0.7: Generally clear and useful with minor formatting gaps.\n- 0.4: Usable but cluttered/confusing; limited underwriting readiness.\n- 0.0: Poorly organized and hard to use.", "expectation": "A clean, professional report with clear comparisons and actionable notes that supports immediate underwriting."}, {"type": "llm_judge", "name": "Strategic Fit and Next Steps", "description": "Evaluate whether the shortlist aligns with stated investment criteria and outlines a clear path to LOI and diligence.", "weight": 0.8, "judge_prompt": "Judge the strategic fit and guidance:\n- Do the selected properties appear to match the investor\u2019s stated criteria (as restated in the report), including stabilized or value-add angles?\n- Is there a clear, practical next-steps section outlining evaluation, offer/LOI, and diligence workflow (including indicative timeline and roles)?\n- Are suggested follow-ups (e.g., document requests, rent rolls, estoppels, CAM reconciliations, lease abstracts) appropriate for shopping center acquisitions?\n\nScoring (0.0\u20131.0):\n- 1.0: Strong alignment with criteria and a crisp, credible next-steps plan to LOI and diligence.\n- 0.7: Generally aligned with most next steps covered.\n- 0.4: Partial alignment or vague next steps.\n- 0.0: Misaligned shortlist or no meaningful process guidance.", "expectation": "Clear alignment with criteria and concrete next steps guiding the investor toward LOI and diligence."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "6241e678-4ba3-4831-b3c7-78412697febc", "rubric": {"category_name": "B2B Live-Action Video Production Schedule (Producers & Directors)", "rationale": "This rubric enforces a self-documenting, file-based calendar deliverable for a 60-second B2B live-action video project. Stage 1 is an LLM-only gate that mandates a PDF visual schedule with explicit, color-coded structure and all required tasks, enabling verification. Stage 2 mixes code and LLM checks to validate dates, task coverage, revision cycles, dependencies, and business-day constraints. Stage 3 qualitatively assesses professional clarity and usability for cross-departmental planning.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Format & Structure Gate (LLM-only)", "description": "Output must be a visual, color-coded calendar/timeline PDF covering Jul 7\u2013Aug 29, 2025 with all required tasks and structural elements that make verification possible.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.2, "rules": [{"type": "llm_judge", "name": "Structured Calendar PDF Requirement", "description": "Verify the deliverable is a PDF visual calendar/timeline with required sections, color-coding, and explicit tasks and milestones.", "weight": 4.0, "judge_prompt": "You are evaluating a single candidate output file. Only assess the structural presence and format (not the quality of content or correctness of timing). Check the following:\n\nFORMAT & SPAN\n- File must be a PDF (not DOCX/Excel/images).\n- The schedule must visually span from Monday, July 7, 2025 through Friday, August 29, 2025.\n- The layout must be a visual calendar or Gantt/timeline view with day-level granularity.\n- Weekends should NOT be included as working columns/blocks. The schedule should be weekdays-only (Mon\u2013Fri). If the view shows weekends, they must be visibly empty and no tasks should be placed there. Preferably, weekends are removed/hidden.\n- US federal holidays within the window must have no work scheduled. (Between Jul 7 and Aug 29, 2025 there are no federal holidays except Independence Day on Jul 4, which is before the start date.)\n\nCOLOR-CODING & LEGEND\n- Color-coding must distinguish at minimum: Pre-Production, Graphics, Post-Production/Editing.\n- Client-involved tasks (any task containing the word \u201cClient\u201d) must be visually distinguishable from the team\u2019s tasks by color or a clearly labeled style.\n- A legend or key mapping colors/styles to the phases and to \u201cClient\u201d tasks must be present and visible.\n\nREQUIRED MILESTONES & TASKS (presence only; do not check durations):\n- Kickoff call (Jul 7, 2025)\n- Internal Creative Workshopping; Internal Creative Review\n- Client Pitch Meeting; Client Pitch Review; Client Pitch Approval\n- Budgeting; Lock Budget\n- Scriptwriting (two rounds indicated); Client Script Review; Client Script Approval\n- Storyboard; Client Storyboard Review; Client Storyboard Approval\n- Graphics (two rounds indicated); Client Graphics Review; Client Graphics Approval\n- Casting Call; Client Casting Review; Client Casting Approval\n- Location Scouting; Client Location Review; Client Location Approval\n- Crew Hire; Lock Cast; Lock Location; Lock Crew\n- Script to Cast; Reserve Gear Rental; Prep Call Sheet; Call Sheet to Crew; Final Preproduction Tweaks\n- Shoot Day (one day)\n- Footage Ingest + Project Set Up\n- Editing (three rounds indicated); Client Edit Reviews; Client Final Approval\n- Audio Mixing; Color Grading; Client review of audio and color (1 day)\n- Final Delivery (Aug 29, 2025)\n\nREVIEWS & ROUNDS VISIBILITY\n- Show two client review days scheduled after each asset delivery (script, storyboard, graphics, and each edit delivery/round as appropriate), and three rounds for editing.\n- Rounds should be explicitly labeled (e.g., \u201cRound 1/2/3\u201d) or clearly indicated via grouped bars/labels.\n\nREADABILITY & COMPLETENESS\n- No collapsed/hidden items like \u201c+2 more\u201d. All tasks for any given day must be directly visible.\n- Dates, task names, and color-coding must be legible.\n\nSCORING (structure only):\n- 1.0: PDF calendar/timeline present; weekday-only across Jul 7\u2013Aug 29; legend/key; color-coding for required phases and client tasks; all required tasks visible; review windows and rounds indicated; no collapsed items.\n- 0.7: PDF calendar present with minor structural gaps (e.g., legend missing but colors are clearly differentiated; or 1\u20132 non-critical tasks not labeled) while still covering the date range and showing client reviews and rounds.\n- 0.4: Valid PDF calendar but missing multiple key structural elements (e.g., no legend and unclear colors, several missing required task labels, or weekends mixed into workdays).\n- 0.0: Not a PDF, not a visual schedule, or large portions of the structure missing so verification isn\u2019t feasible.\n\nReturn a score in [0,1] based solely on structure/format compliance.", "expectation": "A PDF weekday-only visual schedule spanning Jul 7\u2013Aug 29, 2025 with a clear legend, consistent color-coding for Pre-Production, Graphics, Post-Production/Editing, distinct style for Client tasks, all required tasks labeled, explicit review windows and revision rounds, and no collapsed items."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Mixed Code + LLM)", "description": "Now verify correctness and logical compliance given the enforced structure: milestones/dates, task coverage, revision counts, dependencies, and business-day constraints.", "is_required": false, "max_points": 4.5, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Milestone Dates Present and Correct", "description": "Check that Kickoff (Jul 7, 2025) and Final Delivery (Aug 29, 2025) are present and correctly labeled with dates.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        # Expect PDF\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0\n        if not text:\n            return 0.0\n        t = text.lower()\n\n        def any_date_patterns(month_full, month_short, day, year):\n            mm = {'january':'01','february':'02','march':'03','april':'04','may':'05','june':'06','july':'07','august':'08','september':'09','october':'10','november':'11','december':'12'}[month_full]\n            dd = f\"{day:02d}\"\n            yyyy = str(year)\n            pats = [\n                rf\"{month_full}\\s*{day}\\s*,?\\s*{yyyy}\",\n                rf\"{month_short}\\.?\\s*{day}\\s*,?\\s*{yyyy}\",\n                rf\"{month_full}\\s*{dd}\\s*,?\\s*{yyyy}\",\n                rf\"{month_short}\\.?\\s*{dd}\\s*,?\\s*{yyyy}\",\n                rf\"{mm}/{dd}/{yyyy}\",\n                rf\"{mm}-{dd}-{yyyy}\",\n                rf\"{month_full}\\s*{day}\",\n                rf\"{month_short}\\.?\\s*{day}\"\n            ]\n            return pats\n\n        def near(term, month_full, month_short, day, year, window=80):\n            # find term and any of the date patterns within +/- window chars\n            for m in re.finditer(re.escape(term), t):\n                start = max(0, m.start()-window)\n                end = min(len(t), m.end()+window)\n                segment = t[start:end]\n                for p in any_date_patterns(month_full, month_short, day, year):\n                    if re.search(p, segment):\n                        return True\n            return False\n\n        kickoff_ok = near('kickoff', 'july', 'jul', 7, 2025) or near('kickoff call', 'july', 'jul', 7, 2025)\n        final_ok = near('final delivery', 'august', 'aug', 29, 2025)\n\n        score = 0.0\n        score += 0.5 if kickoff_ok else 0.0\n        score += 0.5 if final_ok else 0.0\n        return score\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Required Task Labels Coverage", "description": "Verify presence of the required task labels (flexible matching). Scores by coverage ratio.", "weight": 2.0, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0\n        if not text:\n            return 0.0\n        t = text.lower()\n\n        # Map of task name -> list of regex patterns (lowercase)\n        tasks = {\n            'kickoff call':[r'kickoff(\\s*call)?'],\n            'internal creative workshopping':[r'internal\\s+creative\\s+workshop'],\n            'internal creative review':[r'internal\\s+creative\\s+review'],\n            'client pitch meeting':[r'client.*pitch.*meeting', r'pitch\\s+meeting.*client'],\n            'client pitch review':[r'client.*pitch.*review'],\n            'client pitch approval':[r'client.*pitch.*approval'],\n            'budgeting':[r'budgeting'],\n            'lock budget':[r'lock\\s+budget'],\n            'scriptwriting':[r'script\\s*writing', r'scriptwriting', r'write\\s+script'],\n            'client script review':[r'client.*script.*review'],\n            'client script approval':[r'client.*script.*approval'],\n            'storyboard':[r'story\\s*board', r'storyboard'],\n            'client storyboard review':[r'client.*storyboard.*review'],\n            'client storyboard approval':[r'client.*storyboard.*approval'],\n            'graphics':[r'graphics?(\\s+design)?', r'ui\\s*graphics?'],\n            'client graphics review':[r'client.*graphics?.*review'],\n            'client graphics approval':[r'client.*graphics?.*approval'],\n            'casting call':[r'casting\\s+call'],\n            'client casting review':[r'client.*casting.*review'],\n            'client casting approval':[r'client.*casting.*approval'],\n            'location scouting':[r'location\\s+scout(ing)?'],\n            'client location review':[r'client.*location.*review'],\n            'client location approval':[r'client.*location.*approval'],\n            'crew hire':[r'crew\\s+hire', r'hire\\s+crew'],\n            'lock cast':[r'lock\\s+cast'],\n            'lock location':[r'lock\\s+location'],\n            'lock crew':[r'lock\\s+crew'],\n            'script to cast':[r'script\\s+to\\s+cast'],\n            'reserve gear rental':[r'(reserve|book)\\s+(gear|equipment).*(rental|rent)'],\n            'prep call sheet':[r'prep(are)?\\s+call\\s+sheet', r'prep\\s*call\\s*sheet'],\n            'call sheet to crew':[r'call\\s+sheet\\s+to\\s+crew', r'distribute\\s+call\\s+sheet'],\n            'final preproduction tweaks':[r'final\\s+pre\\s*production\\s+tweak', r'final\\s+pre[-\\s]?production\\s+tweaks?'],\n            'shoot day':[r'shoot\\s+day'],\n            'footage ingest + project set up':[r'footage\\s+ingest', r'project\\s+set\\s*up', r'project\\s+setup'],\n            'editing':[r'editing', r'editorial', r'edit\\b'],\n            'client edit reviews':[r'client.*edit.*review(s)?'],\n            'client final approval':[r'client.*final.*approval'],\n            'audio mixing':[r'audio\\s+mix(ing)?'],\n            'color grading':[r'color\\s+grading', r'colour\\s+grading', r'color\\s+grade'],\n            'final delivery':[r'final\\s+delivery'],\n            'client review of audio and color':[r'client.*review.*audio.*color', r'client.*audio.*color.*review']\n        }\n\n        found = 0\n        total = len(tasks)\n        for name, patterns in tasks.items():\n            ok = any(re.search(p, t) for p in patterns)\n            if ok:\n                found += 1\n        ratio = found / total if total else 0.0\n        # Return ratio (0..1); framework scales by weight\n        return ratio\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Revision Rounds Indicated", "description": "Check that script (2 rounds), graphics (2 rounds), and editing (3 rounds) are indicated via labels (e.g., Round 1/2/3) or text near the phase names.", "weight": 0.7, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0\n        if not text:\n            return 0.0\n        t = text.lower()\n\n        def has_two_rounds_near(term):\n            ok = False\n            for m in re.finditer(term, t):\n                seg = t[max(0, m.start()-120):min(len(t), m.end()+120)]\n                if re.search(r'round\\s*1', seg) and re.search(r'round\\s*2', seg):\n                    ok = True\n                    break\n                if re.search(r'(two|2)\\s+rounds?', seg):\n                    ok = True\n                    break\n            return ok\n\n        def has_three_rounds_near(term):\n            ok = False\n            for m in re.finditer(term, t):\n                seg = t[max(0, m.start()-160):min(len(t), m.end()+160)]\n                if re.search(r'round\\s*1', seg) and re.search(r'round\\s*2', seg) and re.search(r'round\\s*3', seg):\n                    ok = True\n                    break\n                if re.search(r'(three|3)\\s+rounds?', seg):\n                    ok = True\n                    break\n            return ok\n\n        script_ok = has_two_rounds_near('script')\n        graphics_ok = has_two_rounds_near('graphic')\n        edit_ok = has_three_rounds_near('edit') or has_three_rounds_near('editing')\n\n        score = 0.0\n        score += (1/3) if script_ok else 0.0\n        score += (1/3) if graphics_ok else 0.0\n        score += (1/3) if edit_ok else 0.0\n        return score\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "No Collapsed Items and Weekend Labels", "description": "Penalize if the PDF shows collapsed items like \"+2 more\" or if weekday-only constraint looks violated via explicit weekend labels.", "weight": 0.3, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0\n        if not text:\n            return 0.0\n        t = text.lower()\n        score = 1.0\n        # Collapsed indicators\n        if re.search(r\"\\+\\s*\\d+\\s*more|more\\s+events\", t):\n            score -= 0.5\n        # Weekend labels (may appear in headers; low-penalty if present)\n        if re.search(r\"\\b(sat|sun|saturday|sunday)\\b\", t):\n            score -= 0.3\n        if score < 0:\n            score = 0.0\n        if score > 1:\n            score = 1.0\n        return score\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Date Range Coverage Signals", "description": "Check that July 2025 and August 2025 and the specific endpoints (Jul 7 and Aug 29) are mentioned somewhere in the PDF text.", "weight": 0.3, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0\n        if not text:\n            return 0.0\n        t = text.lower()\n        has_july = 'july' in t or 'jul' in t\n        has_august = 'august' in t or 'aug' in t\n        has_2025 = '2025' in t\n        has_jul7 = bool(re.search(r'(july|jul)\\s*7(,?\\s*2025)?|07[/-]07[/-]2025', t))\n        has_aug29 = bool(re.search(r'(august|aug)\\s*29(,?\\s*2025)?|08[/-]29[/-]2025', t))\n        hits = sum([has_july, has_august, has_2025, has_jul7, has_aug29])\n        return hits / 5.0\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Legend/Key Presence Signal", "description": "Check for presence of a legend/key labeling color codes and client vs. internal tasks (text-only heuristic).", "weight": 0.2, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0\n        if not text:\n            return 0.0\n        t = text.lower()\n        # Heuristic terms that commonly label legends/keys\n        candidates = [r'legend', r'key\\b', r'color\\s*key', r'colour\\s*key', r'color\\s*coding', r'phase\\s*legend', r'phase\\s*key']\n        found = any(re.search(p, t) for p in candidates)\n        return 1.0 if found else 0.0\n    except Exception:\n        return 0.0"}, {"type": "llm_judge", "name": "Logical Sequencing & Review Windows", "description": "Use visual context to verify dependency logic and client review windows: overlap where appropriate, two-day client reviews after deliveries, three edit rounds, and business-day constraints.", "weight": 0.7, "judge_prompt": "Visually inspect the PDF schedule for logical correctness (not general quality):\n\nCheck the following and score proportionally:\n1) Dependency logic:\n   - Editing starts only after Shoot Day is completed.\n   - Footage Ingest/Project Setup occurs after Shoot Day and before Editing.\n   - Casting and Location Scouting overlap appropriately with Scriptwriting (can run in parallel).\n   - Final Approval occurs before Final Delivery.\n   - Audio Mixing and Color Grading occur after editing is locked.\n2) Review windows:\n   - After Script, Storyboard, Graphics deliveries, the schedule includes two business days for client review before approval.\n   - Editing shows three rounds (Round 1, 2, 3) with client reviews following each posted round (allowing two days), culminating in Client Final Approval.\n   - For Audio + Color, there is a 1-day client review (can be the same day if noted) before Final Delivery.\n3) Business-day adherence & timeline:\n   - No tasks placed on weekends.\n   - Entire plan fits within Jul 7\u2013Aug 29, 2025 and Final Delivery is on or before Aug 29, 2025.\n\nScoring guidance:\n- 1.0: All conditions are clearly satisfied.\n- 0.7: Minor deviations (e.g., one review window appears short) but overall compliant.\n- 0.4: Multiple issues (e.g., dependencies violated or missing several review windows).\n- 0.0: Major logical failures (editing before shoot, missing reviews entirely, or tasks on weekends).\n\nReturn a score in [0,1].", "expectation": "Dependencies and review windows are logically correct, with two-day client reviews after asset deliveries, three edit rounds with reviews, and no weekend tasks; final delivery by Aug 29, 2025."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality", "description": "Holistic assessment of presentation clarity and usefulness for cross-departmental planning and utilization forecasting.", "is_required": false, "max_points": 1.5, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation & Usability", "description": "Assess clarity, readability, and practical usefulness of the schedule as a circulated planning artifact.", "weight": 1.5, "judge_prompt": "Evaluate the PDF\u2019s professional quality and usability (not structure correctness):\n- Visual clarity: Are labels, dates, and color-coding easily readable when printed or viewed on screen? Is contrast sufficient?\n- Information design: Is the legend/key clear? Are phases and client tasks immediately distinguishable? Are tasks placed without clutter or overlaps? No truncated labels.\n- Fit for purpose: Would department heads be able to allocate staff and forecast availability from this schedule? Is the timeline density manageable (no overcrowding), and are overlapping tasks understandable?\n- Export quality: Is the PDF page layout clean (appropriate page size/orientation, margins, no pixelation)?\n\nScoring:\n- 1.0\u20131.5: Excellent professional presentation; highly usable for planning and forecasting.\n- 0.5\u20130.9: Adequate and readable with minor issues (e.g., slightly cramped areas).\n- 0.1\u20130.4: Hard to read or awkward layout but still somewhat usable.\n- 0.0: Poor readability/usability (illegible, cluttered, or confusing color usage).\n\nReturn a score in [0,1], where 1 corresponds to full points for this rule.", "expectation": "A clean, high-contrast, legible PDF with clear legend, distinct phase/client colors, readable task labels without truncation, and a layout that supports staffing and utilization planning."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "8314d1b1-5b0f-42a4-b5d5-91c0867b0913", "rubric": {"category_name": "Legal Memo: Delaware Standard of Review for Controller-Affiliated Acquisition", "rationale": "Three-stage, self-documenting rubric. Stage 1 is a strict LLM-format gate that forces a verifiable DOCX/PDF memo with required sections, citations, and topic coverage so that later checks are trivial. Stage 2 mixes deterministic code checks (word count, doctrine coverage, statutory references, application to facts, citations, risk-mitigation recommendations) with one focused LLM check on law-to-fact mapping. Stage 3 is a holistic LLM quality assessment for professionalism and client-friendliness.", "max_total_score": 10.0, "stages": [{"name": "Stage 1: Format and Structural Gate", "description": "LLM-only gate to enforce exact structure and format so verification is possible.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 2.8, "rules": [{"type": "llm_judge", "name": "Memo Format and Structural Requirements", "description": "Output must be a legal memo in DOCX (preferred) or PDF with required sections, addressing, citations, and topical coverage.", "weight": 4.0, "judge_prompt": "You are the Stage 1 format gate. Inspect the candidate output (rendered DOCX/PDF). Only check STRUCTURE and PRESENCE, not the legal accuracy of content.\n\nFormat requirements (strict but flexible on header wording):\n- File is a DOCX (preferred) or PDF document (not plain text, not Excel/code).\n- Includes a memo header block with at least: To (Elias Veynor), Date, and Re/Subject line referencing the proposed Clarivon Group\u2013Luminor Dynamics acquisition or standard of review.\n- Contains clearly labeled sections (exact titles or close variants acceptable):\n  1) Introduction\n  2) Executive Summary\n  3) Analysis (with visible subsections that cover ALL of the following topical areas):\n     - Delaware common law standard(s) of review for controller/conflict transactions\n     - March 2025 amendments to Delaware\u2019s corporate statute, including changes to DGCL \u00a7 144, and their impact\n     - Application of the framework to the facts presented (Clarivon Group acquiring Luminor Dynamics; Elias as controller; dual-class/super-voting)\n     - Recommendations to reduce risk (tied to the applicable framework)\n  4) Conclusion\n- Citations: At least 3 citations visible within the document body or footnotes/endnotes. These may include cases (e.g., case names with \u201cv.\u201d, Del. Ch./A.3d cites), statutes (e.g., \u201c8 Del. C. \u00a7 144\u201d), or reputable online sources (URLs).\n- Length: Does not appear to exceed 3,500 words (estimate is fine).\n- Professional formatting: headings clearly distinguish sections; paragraphs are readable; bulleting/numbering acceptable.\n\nScoring (return a numeric score between 0.0 and 4.0):\n- 4.0: DOCX or PDF; header addressing Elias; all required sections present; Analysis includes all four topical areas; \u22653 citations; length appears \u22643,500 words; formatting is professional.\n- 3.2: DOCX or PDF; minor deviation (e.g., header missing Date or mildly variant section titles) but all four Analysis topical areas are covered and citations present.\n- 2.0: Document format is valid but missing one core section OR missing either the DGCL \u00a7 144/2025-amendments topical coverage OR lacks citations.\n- 0.0: Not a DOCX/PDF; or missing multiple core sections; or no Analysis section; or clearly exceeds 3,500 words by a wide margin; or not addressed to Elias.\n\nOnly judge structure/presence/format. Do NOT judge legal correctness or quality.", "expectation": "A well-structured DOCX memo addressed to Elias, with Introduction, Executive Summary, Analysis (covering common law standard, 2025 DGCL amendments incl. \u00a7144, application to facts, recommendations), and Conclusion; includes \u22653 citations; \u22643,500 words; professional formatting."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Substantive Verification (Shape-enabled)", "description": "Deterministic and focused checks of legal coverage, statutory references, application to facts, citations, and recommendations, plus one LLM check on law-to-fact mapping coherence.", "is_required": false, "max_points": 4.5, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Word Count Within Limit", "description": "Verify memo does not exceed 3,500 words (estimated from extracted text).", "weight": 0.3, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output to evaluate.\"\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        pass\n    if not text:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            pass\n    if not text:\n        return 0.0, \"Unable to extract text from document.\"\n    words = re.findall(r\"\\b\\w+\\b\", text)\n    count = len(words)\n    if count == 0:\n        return 0.0, \"Document appears empty.\"\n    # Full credit if within 3,500; linear decay to 0 by 4,000.\n    if count <= 3500:\n        score = 1.0\n    elif count >= 4000:\n        score = 0.0\n    else:\n        # Between 3501 and 3999 -> scale from 1.0 down to 0.0\n        over = count - 3500\n        score = max(0.0, 1.0 - (over / 500.0))\n    return score, f\"Estimated word count: {count}.\""}, {"type": "code", "name": "Required Doctrines Mentioned", "description": "Check presence of core Delaware doctrines/cases: entire fairness (with fair dealing/fair price), business judgment, MFW, Corwin, and at least one hallmark case.", "weight": 0.9, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output.\"\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        pass\n    if not text:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            pass\n    if not text:\n        return 0.0, \"Text extraction failed.\"\n    tl = text.lower()\n    checks = {\n        'entire_fairness': 'entire fairness' in tl,\n        'fair_dealing': 'fair dealing' in tl,\n        'fair_price': 'fair price' in tl,\n        'business_judgment': 'business judgment' in tl or 'business-judgment' in tl,\n        'mfw': 'mfw' in tl or 'm&f world' in tl or 'm & f world' in tl or 'm\\u2019f' in tl,\n        'corwin': 'corwin' in tl,\n        'weinberger': 'weinberger v.' in tl or 'weinberger v ' in tl or 'weinberger' in tl,\n        'lynch': 'kahn v. lynch' in tl or 'kahn v lynch' in tl or 'lynch' in tl,\n    }\n    # Weighting within this rule (sums to 1.0)\n    parts = []\n    parts.append(0.3 if checks['entire_fairness'] else 0.0)\n    parts.append(0.1 if checks['fair_dealing'] else 0.0)\n    parts.append(0.1 if checks['fair_price'] else 0.0)\n    parts.append(0.15 if checks['business_judgment'] else 0.0)\n    parts.append(0.2 if checks['mfw'] else 0.0)\n    parts.append(0.1 if checks['corwin'] else 0.0)\n    hallmark = checks['weinberger'] or checks['lynch']\n    parts.append(0.05 if hallmark else 0.0)\n    score = sum(parts)\n    missing = [k for k,v in checks.items() if not v]\n    return min(1.0, score), f\"Missing elements: {', '.join(missing) if missing else 'None.'}\""}, {"type": "code", "name": "DGCL \u00a7144 and March 2025 Amendments Coverage", "description": "Verify mention of DGCL \u00a7 144 and discussion of March 2025 DGCL amendments and their impact.", "weight": 1.1, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output.\"\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        pass\n    if not text:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            pass\n    if not text:\n        return 0.0, \"Text extraction failed.\"\n    tl = text.lower()\n    dgcl_144 = ('\\u00a7 144' in tl) or ('section 144' in tl) or ('dgcl 144' in tl) or ('8 del. c. \\u00a7 144' in tl) or ('8 del.c. \\u00a7 144' in tl) or ('8 del. c.' in tl and '144' in tl)\n    has_2025 = '2025' in tl\n    has_march = 'march' in tl\n    has_amend = 'amend' in tl or 'amendment' in tl or 'amendments' in tl or 'amended' in tl or 'changes' in tl or 'revisions' in tl\n    # Bonus if 144 and amendment timing are discussed in proximity\n    proximity = 0\n    try:\n        # Check if \\\"144\\\" appears within 200 chars of \\\"2025\\\" or \\\"amend\\\"\n        for m in re.finditer(r\"144\", tl):\n            start = max(0, m.start()-200)\n            end = min(len(tl), m.end()+200)\n            window = tl[start:end]\n            if '2025' in window or 'amend' in window or 'amendment' in window or 'amendments' in window:\n                proximity = 1\n                break\n    except Exception:\n        pass\n    score = 0.0\n    score += 0.6 if dgcl_144 else 0.0\n    score += 0.4 if (has_2025 and (has_amend or has_march)) else 0.0\n    score += 0.1 if proximity else 0.0\n    return min(1.0, score), f\"DGCL \u00a7144: {'yes' if dgcl_144 else 'no'}; 2025 amendments: {'yes' if (has_2025 and (has_amend or has_march)) else 'no'}; proximity: {bool(proximity)}.\""}, {"type": "code", "name": "Application to Provided Facts", "description": "Check for explicit application to Clarivon/Luminor/Elias facts and controller/dual-class context.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output.\"\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        pass\n    if not text:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            pass\n    if not text:\n        return 0.0, \"Text extraction failed.\"\n    tl = text.lower()\n    checks = {\n        'clarivon': 'clarivon' in tl,\n        'luminor': 'luminor' in tl,\n        'elias': 'elias veynor' in tl or ('elias' in tl and 'veynor' in tl),\n        'dual_class': 'dual-class' in tl or 'dual class' in tl or 'class b' in tl,\n        'super_voting': 'super-voting' in tl or 'super voting' in tl or 'supervoting' in tl,\n        'controller': 'controlling stockholder' in tl or 'controller' in tl or 'control' in tl,\n        'conflict_both_sides': 'both sides' in tl or 'self-dealing' in tl or 'interested' in tl,\n    }\n    total = len(checks)\n    matched = sum(1 for v in checks.values() if v)\n    score = matched / float(total)\n    missing = [k for k,v in checks.items() if not v]\n    return score, f\"Application elements matched {matched}/{total}. Missing: {', '.join(missing) if missing else 'None.'}\""}, {"type": "code", "name": "Citations Present and Sufficient", "description": "Verify presence and sufficiency of legal/statutory/URL citations.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output.\"\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        pass\n    if not text:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            pass\n    if not text:\n        return 0.0, \"Text extraction failed.\"\n    # Count URLs\n    url_count = len(re.findall(r\"https?://\\S+\", text))\n    # Case citation heuristic: occurrences of \" v. \"\n    v_count = len(re.findall(r\"\\b[vV]\\.\\b\", text))\n    # Reporter/Delaware court indicators\n    reporter_count = 0\n    reporter_count += len(re.findall(r\"\\bA\\.\\d+d\\b\", text))  # A.2d / A.3d\n    reporter_count += len(re.findall(r\"Del\\.\\s*Ch\\.\", text))\n    reporter_count += len(re.findall(r\"Del\\.\\s*Supr\\.\", text))\n    reporter_count += len(re.findall(r\"\\bWL\\b\\s*\\d+\", text))\n    # Statute indicators\n    statute_count = len(re.findall(r\"8\\s*Del\\.\\s*C\\.\\s*\\\u00a7|8\\s*Del\\.\\s*C\\.\\s*\\u00a7\", text)) + len(re.findall(r\"\\u00a7\\s*\\d+\", text))\n    total_signals = url_count + v_count + reporter_count + statute_count\n    # Full credit at 5+ signals, linear up to that point\n    score = min(1.0, total_signals / 5.0)\n    return score, f\"Citations signals: URLs={url_count}, v.={v_count}, reporters={reporter_count}, statutes={statute_count}.\""}, {"type": "code", "name": "Risk-Mitigation Recommendations Present", "description": "Check for concrete risk-reduction steps tied to controller transactions (e.g., MFW protections, committee, disclosures).", "weight": 0.3, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output.\"\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        pass\n    if not text:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            pass\n    if not text:\n        return 0.0, \"Text extraction failed.\"\n    tl = text.lower()\n    terms = {\n        'special_committee': ('special committee' in tl) or ('independent committee' in tl),\n        'majority_of_minority': ('majority of the minority' in tl) or ('unaffiliated stockholders' in tl) or ('minority vote' in tl),\n        'ab_initio': 'ab initio' in tl,\n        'fairness_opinion': 'fairness opinion' in tl,\n        'disclosure': 'fully informed' in tl or 'adequate disclosure' in tl or 'disclosure' in tl,\n        'no_coercion': 'no coercion' in tl or 'uncoerced' in tl,\n        'separate_counsel': 'separate counsel' in tl or 'independent counsel' in tl,\n    }\n    total = len(terms)\n    matched = sum(1 for v in terms.values() if v)\n    score = matched / float(total)\n    missing = [k for k,v in terms.items() if not v]\n    return score, f\"Recommendations matched {matched}/{total}. Missing: {', '.join(missing) if missing else 'None.'}\""}, {"type": "llm_judge", "name": "Principle-to-Fact Mapping Check", "description": "LLM check: Does the memo connect the legal framework (entire fairness/MFW/\u00a7144/2025 changes) to the specific Clarivon\u2013Luminor facts and produce concrete, framework-grounded recommendations?", "weight": 0.5, "judge_prompt": "Review the memo\u2019s Analysis and Conclusion. Judge whether it coherently maps the legal framework to the provided facts and ties recommendations to that framework.\n\nWhat to look for:\n- Identifies that a controller/conflicted transaction typically triggers entire fairness unless MFW protections are satisfied from the outset; distinguishes business judgment and (if discussed) Corwin.\n- Explains DGCL \u00a7 144\u2019s role (interested director transaction safe harbors/ratification) and addresses the March 2025 DGCL amendments\u2019 impact on the common law standard (at least at a high level), even if cautiously.\n- Applies these principles to Clarivon/Luminor/Elias (dual-class, super-voting, controller on both sides) with specific, non-generic observations.\n- Provides concrete, framework-grounded recommendations (e.g., special committee, majority-of-the-minority, ab initio, disclosures, no coercion) that logically mitigate the identified risks.\n\nScoring (0.0\u20130.5):\n- 0.5: Clear, accurate mapping of law to facts and specific, grounded recommendations.\n- 0.3: Mostly correct mapping with minor gaps or generalities; recommendations somewhat tied to framework.\n- 0.1: Superficial or generic discussion; weak linkage between law and facts; recommendations not clearly grounded.\n- 0.0: No meaningful application or recommendations.", "expectation": "A coherent application showing entire fairness likely applies absent MFW protections; explains how \u00a7144/2025 changes intersect; gives concrete steps tied to the framework."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3: Professional Quality and Client-Friendliness", "description": "Holistic LLM assessment of communication quality, organization, tone, and usability for a sophisticated non-lawyer client.", "is_required": false, "max_points": 1.5, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Client Readability", "description": "Assesses clarity, organization, neutral tone, and client usability.", "weight": 1.5, "judge_prompt": "Assess the memo\u2019s professional quality and client-friendliness. Consider:\n- Clarity and organization: logical flow; headers/subheaders; signposting.\n- Neutral, objective tone suitable for professional legal advice.\n- Client readability: explains legal concepts (entire fairness, MFW, \u00a7144) in accessible language for a sophisticated non-lawyer; avoids jargon or explains it.\n- Actionability: recommendations are concrete, prioritized, and feasible.\n- Formatting polish: consistent headings, paragraph spacing, readable lists.\n\nScoring (0.0\u20131.5):\n- 1.5: Highly clear, well-structured, neutral, and accessible; actionable, prioritized recommendations; professional formatting.\n- 1.0: Generally clear and professional with minor issues; recommendations present but could be sharper.\n- 0.5: Mixed clarity/organization; noticeable jargon or vagueness; recommendations thin.\n- 0.0: Disorganized, hard to follow, or inappropriate tone.", "expectation": "A polished, neutral memo that a sophisticated non-lawyer can understand and use, with clear, actionable recommendations."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "87da214f-fd92-4c58-9854-f4d0d10adce0", "rubric": {"category_name": "Identity Theft Reimbursement Review Presentation", "rationale": "Mixed task: slide-style document with embedded financial analysis. Stage 1 (LLM) strictly enforces a verifiable slide/table structure. Stage 2 mixes code checks for numeric presence/consistency and LLM cross-referencing. Stage 3 uses LLM for professional quality and actionability.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2013 Shape Enforcement Gate (Slide Deck Structure)", "description": "LLM-only gate verifying the output is a PDF/DOCX slide-style deck with required slides and explicitly structured tables that enable verification.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Slide Deck Requirements", "description": "Check that the candidate output is a properly structured slide deck document with all required slides/sections and verifiable tables.", "weight": 4.0, "judge_prompt": "You are the Stage 1 gate. Inspect the candidate output. Only evaluate FORMAT/STRUCTURE/PRESENCE (not correctness). The output must be a slide-style deck delivered as a PDF or DOCX. Be flexible with exact slide names but follow requirements closely.\n\nFormat requirements:\n- Must be a PDF or DOCX file (not Excel/CSV/Markdown/images only).\n- Slide-style formatting with clear slide titles. Recommend at least 8 slides.\n- Includes at least one clearly structured table for financials.\n\nRequired slides/sections (titles can vary but intent must be clear):\n1) Title slide: includes company name (Gold Digital Insurance) or equivalent context.\n2) Agenda slide: an agenda or outline of the presentation.\n3) Purpose slide: states purpose of review (identity theft reimbursements vs policy).\n4) Summary of Results / Executive Summary slide: includes the financial impact to the company with at least one dollar amount AND one percentage related to funds involved.\n5) Financial Summary slide: must include a clearly labeled table with columns resembling [Metric | Value | Source/Notes]. The table must include rows for all of these metrics (naming can vary):\n   - Total Claims Reviewed\n   - Total Amount Claimed\n   - Total Amount Reimbursed\n   - Amount Outside Policy Scope (or similar)\n   - Percentage Outside Policy (explicitly state denominator; ideally \u201cof total reimbursed\u201d)\n6) Claims Sample slide: a table with at least 3 rows and columns similar to [Claim ID | Date | Amount | Category/Reason | Policy Section Cited | Within Policy? (Y/N) | Decision].\n7) Policy Mapping and Gaps slide: bullet points mapping observed claim issues to policy sections/clauses; at least 3 bullets or mappings.\n8) Recommendation slide: a clear remediation recommendation.\n9) Next Steps slide: at least 3 concrete, time-bound steps.\n10) Policy Language Update Options slide: at least one explicit proposed update option. Prefer a mini-table or paired text showing Current vs Proposed OR a clearly labeled \u201cProposed Language\u201d.\n11) Appendix / Methodology & Assumptions slide: brief methods, data sources, assumptions.\n\nScoring (STRUCTURE ONLY):\n- 4.0: Valid PDF/DOCX and all items 1\u201311 present. Financial Summary table clearly shows all required rows; Claims Sample table has \u22653 rows with the specified columns.\n- 3.5: Missing exactly one non-core slide among 7, 9, or 11 OR minor table labeling deviations but all required rows/columns are still clearly present.\n- 3.0: Missing up to two required slides/sections total OR Claims Sample table present but has only 2 rows.\n- 2.0: Valid PDF/DOCX but missing three required items OR Financial Summary lacks multiple required rows OR Claims Sample has \u22641 row.\n- 1.0: Valid file type but lacks slide structure or most required sections.\n- 0.0: Not PDF/DOCX, or completely wrong format.\n\nOnly evaluate presence/format/structure. Do not judge correctness of numbers or quality of writing.", "expectation": "A professional PDF/DOCX slide deck with the specific slides and tables listed, enabling downstream verification."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification and Consistency Checks", "description": "Code checks for numeric presence, plausibility, and simple internal consistency, plus an LLM cross-reference of narrative and tables.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Financial Metrics Presence and Plausibility", "description": "Detect key financial metrics (dollar amounts and percent) and check basic plausibility and internal consistency between percentage and amounts, if both are present.", "weight": 2.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output to analyze.\"\n\n    # Extract text from PDF or DOCX\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0, \"Unable to read document text.\"\n\n    if not text or len(text.strip()) == 0:\n        return 0.0, \"Empty document text.\"\n\n    # Normalize for matching labels\n    lower = text.lower()\n\n    # Look for Financial Summary context markers\n    labels = {\n        'total_claims_reviewed': ['total claims reviewed', 'claims reviewed total', 'number of claims reviewed'],\n        'total_amount_claimed': ['total amount claimed', 'sum claimed', 'total claimed'],\n        'total_amount_reimbursed': ['total amount reimbursed', 'sum reimbursed', 'total reimbursed'],\n        'amount_outside_policy': ['amount outside policy', 'outside policy amount', 'non-covered amount', 'amount outside policy scope'],\n        'percent_outside_policy': ['percent outside policy', 'percentage outside policy', 'outside policy %', 'outside-policy percentage']\n    }\n\n    # Helper to check if any variant appears\n    def has_label(keys):\n        return any(k in lower for k in keys)\n\n    # Dollar and percent regex\n    money_re = re.compile(r\"\\$\\s*([0-9]{1,3}(?:,[0-9]{3})*|[0-9]+)(?:\\.[0-9]{2})?\")\n    pct_re = re.compile(r\"(\\d{1,3}(?:\\.\\d+)?)\\s*%\")\n\n    # Score accumulation\n    score = 0.0\n    feedback = []\n\n    # Check presence of labels\n    present_labels = {\n        name: has_label(keys) for name, keys in labels.items()\n    }\n    label_count = sum(1 for v in present_labels.values() if v)\n\n    # Reward presence of a majority of required labels\n    if label_count >= 4:\n        score += 0.8\n    elif label_count >= 3:\n        score += 0.4\n    else:\n        score += 0.0\n    feedback.append(f\"Labels present: {label_count}/5\")\n\n    # Extract amounts near relevant labels (best-effort, tolerant)\n    def extract_near(label_phrases, window_chars=200):\n        vals = []\n        for phrase in label_phrases:\n            idx = lower.find(phrase)\n            if idx != -1:\n                start = max(0, idx - window_chars)\n                end = min(len(text), idx + window_chars)\n                chunk = text[start:end]\n                vals.extend(money_re.findall(chunk))\n                # For percent specifically\n                vals.extend(pct_re.findall(chunk))\n        return vals\n\n    # Attempt to get key values\n    near_reimbursed = extract_near(labels['total_amount_reimbursed'])\n    near_outside = extract_near(labels['amount_outside_policy'])\n    near_percent = extract_near(labels['percent_outside_policy'])\n\n    # Parse first plausible amounts/percent\n    def first_money(vals):\n        for v in vals:\n            if isinstance(v, tuple):\n                # from money regex: group 1 returned as str\n                v = v[0]\n            try:\n                # strip commas\n                return float(str(v).replace(',', ''))\n            except Exception:\n                continue\n        return None\n\n    def first_percent(vals):\n        for v in vals:\n            try:\n                p = float(v)\n                if 0 <= p <= 100:\n                    return p\n            except Exception:\n                continue\n        return None\n\n    amt_reimbursed = first_money(near_reimbursed)\n    amt_outside = first_money(near_outside)\n    pct_outside = first_percent(near_percent)\n\n    # Award for presence of at least one dollar amount and one percent\n    has_money = amt_reimbursed is not None or amt_outside is not None\n    has_pct = pct_outside is not None\n    if has_money and has_pct:\n        score += 0.8\n    elif has_money or has_pct:\n        score += 0.4\n    feedback.append(f\"Detected values - reimbursed: {amt_reimbursed}, outside: {amt_outside}, pct: {pct_outside}\")\n\n    # Consistency: if reimbursed and outside are present, check if pct ~ outside/reimbursed\n    if amt_reimbursed and amt_outside and amt_reimbursed > 0 and pct_outside is not None:\n        calc_pct = (amt_outside / amt_reimbursed) * 100.0\n        diff = abs(calc_pct - pct_outside)\n        if diff <= 3.0:\n            score += 0.4\n            feedback.append(f\"Percent consistent within 3pp (calc {calc_pct:.2f}% vs stated {pct_outside:.2f}%).\")\n        elif diff <= 8.0:\n            score += 0.2\n            feedback.append(f\"Percent loosely consistent within 8pp (calc {calc_pct:.2f}% vs stated {pct_outside:.2f}%).\")\n        else:\n            feedback.append(f\"Percent inconsistent (calc {calc_pct:.2f}% vs stated {pct_outside:.2f}%).\")\n\n    # Caps and return\n    score = min(score, 2.0)\n    return score, \"; \".join(feedback)"}, {"type": "code", "name": "Claims Sample Presence (\u22653 distinct claims)", "description": "Verify that a claims sample table is present by detecting at least three distinct Claim IDs and associated monetary amounts.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output to analyze.\"\n\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0, \"Unable to read document text.\"\n\n    if not text:\n        return 0.0, \"Empty document text.\"\n\n    lower = text.lower()\n\n    # Heuristics for Claim IDs: patterns like CLM12345, Claim ID: 12345, Claim # 2024-001, etc.\n    id_patterns = [\n        r\"claim\\s*id\\s*[:#]?\\s*([a-z0-9\\-]{4,})\",\n        r\"claim\\s*[#:]\\s*([a-z0-9\\-]{4,})\",\n        r\"\\bclm\\s*[-:]?\\s*([a-z0-9\\-]{3,})\",\n        r\"\\b([A-Z]{2,4}\\d{3,})\\b\"\n    ]\n\n    ids = set()\n    for pat in id_patterns:\n        for m in re.findall(pat, text, flags=re.IGNORECASE):\n            token = str(m).strip()\n            if len(token) >= 3:\n                ids.add(token.lower())\n\n    # Also require presence of monetary amounts near claim context\n    money_hits = re.findall(r\"\\$\\s*([0-9]{1,3}(?:,[0-9]{3})*|[0-9]+)(?:\\.[0-9]{2})?\", text)\n\n    # Scoring logic\n    n_ids = len(ids)\n    n_money = len(money_hits)\n\n    if n_ids >= 3 and n_money >= 3:\n        return 1.5, f\"Detected {n_ids} distinct claim IDs and {n_money} money amounts.\"\n    if n_ids >= 2 and n_money >= 2:\n        return 1.0, f\"Detected {n_ids} claim IDs and {n_money} money amounts (partial).\"\n    if n_ids >= 1 and n_money >= 1:\n        return 0.5, f\"Minimal detection: {n_ids} claim ID and {n_money} money amounts.\"\n    return 0.0, \"Could not detect sufficient claim IDs and amounts.\""}, {"type": "code", "name": "Policy Update Option Presence", "description": "Check that at least one proposed policy language update is present (e.g., labels like Current vs Proposed or explicit 'Proposed Language').", "weight": 0.5, "code": "def evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output to analyze.\"\n\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0, \"Unable to read document text.\"\n\n    lower = text.lower()\n    indicators = [\n        'proposed language', 'updated policy language', 'policy language update',\n        'current language', 'current wording', 'proposed wording', 'option', 'redline'\n    ]\n\n    hits = sum(1 for k in indicators if k in lower)\n\n    if ('proposed language' in lower) or (('current' in lower and 'proposed' in lower) or ('current language' in lower and 'proposed language' in lower)):\n        return 0.5, \"Detected explicit proposed policy language option.\"\n    if hits >= 2:\n        return 0.3, \"Detected partial indicators of policy language update.\"\n    if hits >= 1:\n        return 0.1, \"Weak indicator of policy language update.\"\n    return 0.0, \"No signs of policy language update option found.\""}, {"type": "llm_judge", "name": "Narrative\u2013Table Coherence (Financial and Policy)", "description": "Check that the Summary/Executive Summary statements align with the Financial Summary table and that recommendations map to identified policy gaps.", "weight": 1.0, "judge_prompt": "Read the document. Check high-level consistency between narrative and the structured sections:\n- Does the Summary/Executive Summary mention a financial impact that is also reflected in the Financial Summary table (similar amounts/percentages, not necessarily exact)?\n- Do the recommendations clearly address the policy gaps highlighted in the Policy Mapping and Gaps section?\n- Are Next Steps concrete and actionable (e.g., responsibilities, timelines, pilots, audits)?\n\nScoring:\n- 1.0: All three checks satisfied with clear alignment.\n- 0.5: Partial alignment (e.g., financials generally consistent but recommendations or next steps are generic).\n- 0.0: Major inconsistencies or missing alignment.\n\nFocus on coherence; do not penalize minor naming differences.", "expectation": "Narrative claims are consistent with the tabled financials and recommendations directly address identified gaps with actionable next steps."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Professional Quality and Actionability", "description": "LLM judge assesses presentation quality, clarity, and suitability for internal decision-making.", "is_required": false, "max_points": 1.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Presentation Quality and Fitness for Purpose", "description": "Evaluate professionalism, clarity, visual structure, and actionability for an internal colleague audience.", "weight": 1.0, "judge_prompt": "Assess the overall quality of the slide deck for internal review:\n- Professional formatting and readable slide titles\n- Clear organization and flow (agenda maps to content)\n- Tables are legible; key figures are easy to find\n- Recommendations are specific, feasible, and tied to findings\n- Tone appropriate for colleagues in Finance/Insurance\n\nScoring:\n- 1.0: Highly professional and actionable; easy to make decisions from this deck.\n- 0.5: Adequate but could be clearer/more professional or more actionable.\n- 0.0: Poorly formatted or unclear; not decision-useful.", "expectation": "A clean, well-structured deck that enables colleagues to quickly grasp the issue, quantify impact, and agree on next steps."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "fd6129bd-f095-429b-873c-dcc3137be2c3", "rubric": {"category_name": "Change Control SOP and Change Request Form (Biotech Nonclinical Operations)", "rationale": "This rubric enforces a self-documenting, audit-ready deliverable set: a formally structured SOP and a matching Change Request Form. Stage 1 (LLM-only) strictly gates the required file formats and structural elements. Stage 2 (code-heavy) verifies correctness via deterministic text checks and cross-references between the SOP and Form. Stage 3 (LLM) assesses overall professional quality and fitness for implementation.", "max_total_score": 12.0, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate (LLM ONLY)", "description": "Hard gate ensuring BOTH required deliverables are present as documents (PDF/DOCX) with specific structures enabling verification and audit-readiness.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "SOP + Form Structural Requirements", "description": "Verify that outputs include BOTH a formal SOP and a matching Change Request Form, each in document format (PDF or DOCX), with required sections/fields and professional structure. Do not check content quality or correctness; only presence and structural completeness.", "weight": 4.0, "judge_prompt": "You are evaluating whether the submission contains BOTH of the following, in verifiable, audit-ready structure:\n\nA) A formal SOP document (PDF or DOCX). B) A Change Request Form (PDF or DOCX) that matches the SOP process.\n\nAcceptable formats: PDF or DOCX only (not plain text, not Excel). Use flexible matching for section/field names but require clear headers/labels.\n\nSOP \u2014 Required Structural Elements (headers/sections visibly present):\n1) Title block on page 1: Document title (Change Control SOP), document ID/number, version, effective date, owning function/author, approval signatures block, and page numbers.\n2) Purpose.\n3) Scope (explicitly includes changes impacting scope, timeline/schedule, budget/cost, regulatory deliverables).\n4) Definitions/Abbreviations/Terms.\n5) Roles and Responsibilities (RACI or equivalent; clearly lists PM, QA, Technical Operations, Finance, Regulatory stakeholders).\n6) Change Types/Triggers (what requires formal review; thresholds/criteria).\n7) Change Request Submission (how to initiate; reference to the Change Request Form; intake channel).\n8) Review and Impact Assessment (dimensions: scope, schedule, cost/budget, risk/quality, regulatory impact; required evidence/attachments).\n9) Decision and Approval Workflow (routing and required approvers, e.g., PM, QA, Tech Ops, Finance, Regulatory; normal vs. expedited/emergency path).\n10) Implementation and Communication Plan.\n11) Documentation and Records Management (where to store artifacts; system/location; naming; retention).\n12) Audit Trail and Traceability (unique change ID, cross-links, change log).\n13) Change Log/Numbering Convention.\n14) Deviations/Emergency Changes (expedited handling and documentation requirements).\n15) Training/Change Effectiveness/Closure.\n16) References and Related Documents.\n17) Appendices (must include reference to the Change Request Form template).\n18) Revision History table.\n\nMinimum length: at least 3 pages. Professionally structured with visible section headers. Do not assess prose quality\u2014just presence.\n\nChange Request Form \u2014 Required Fields/Sections (labels visible; can be a template):\nA) Header: Form title (Change Request Form), form ID or code, version, date.\nB) Request Details: Project/Program Name, Project ID/Code, Requester name, Department/Function, Contact info, Request date.\nC) Change Basics: Change title/summary, Change category/type (scope, schedule/timeline, budget/cost, regulatory), Priority/urgency (normal vs expedited/emergency).\nD) Description and Justification: Problem/driver, background, objectives.\nE) Impact Assessment (separate labeled fields): scope, schedule (with dates/milestones), cost/budget (estimates), risk/quality, regulatory (deliverables/submissions), affected documents/deliverables.\nF) Proposed Plan and Alternatives Considered.\nG) Attachments/References section.\nH) Routing and Approvals: reviewers/approvers list with roles (PM, QA, Tech Ops, Finance, Regulatory), signature/approval blocks with name/date.\nI) Decision and Effective Date.\nJ) Implementation Owner, Target Completion, Communication plan (field or checkbox/notes area).\nK) Closure/Verification (evidence of completion), Post-implementation review.\nL) Traceability: Unique Change ID, linkage to SOP number/version, status (e.g., Submitted/In Review/Approved/Rejected/Closed), archive location/record ID.\n\nScoring Guidance (return a score from 0.0 to 4.0):\n- 4.0: Both files are PDF/DOCX; SOP has all required sections; Form has all required fields; SOP >= 3 pages; professional structure.\n- 3.0: Both files valid; SOP missing up to 3 non-critical sections OR Form missing up to 3 non-critical fields; overall structure is sound and traceable.\n- 2.0: Both files present and valid but multiple missing elements (4\u20136 across SOP/Form) limiting verification.\n- 1.0: Only one of the two (SOP or Form) is valid OR both are present but largely incomplete (<50% of required elements).\n- 0.0: Wrong formats (not PDF/DOCX) OR missing one of the two required deliverables entirely.\n\nOnly evaluate structure, presence, and format\u2014not calculation/content accuracy.", "expectation": "An SOP (PDF/DOCX) with the listed sections and a matching Change Request Form (PDF/DOCX) with labeled fields enabling end-to-end traceability."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Compliance Verification (Code + LLM)", "description": "Deterministic checks that the SOP and Form align, cover mandatory controls, and enable traceability and audit-readiness.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "SOP Mandatory Sections Coverage", "description": "Programmatically verify presence of core SOP sections using fuzzy keyword matching.", "weight": 1.6, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        resources = [r for r in context.get_all_outputs() if getattr(r, 'is_document', False)]\n        if not resources:\n            return 0.0, 'No document outputs found.'\n        # Read text for each resource\n        def read_text(r):\n            rid = r.id\n            name = getattr(r, 'name', '') or ''\n            try:\n                if str(name).lower().endswith('.pdf'):\n                    return context.files.read_pdf_text(rid)\n            except Exception:\n                pass\n            try:\n                return context.files.read_docx_text(rid)\n            except Exception:\n                pass\n            try:\n                return context.files.read_pdf_text(rid)\n            except Exception:\n                return ''\n        docs = [(r, read_text(r)) for r in resources]\n        # Classify SOP vs Form by content\n        sop_res, sop_text, form_res, form_text = None, '', None, ''\n        for r, t in docs:\n            lt = (t or '').lower()\n            if any(k in lt for k in ['change request form', 'request form', 'cr form', 'form id', 'submitter name']):\n                if not form_res:\n                    form_res, form_text = r, t\n            if any(k in lt for k in ['standard operating procedure', 'sop ', ' sop\\n', 'procedure', 'purpose', 'scope']):\n                if not sop_res:\n                    sop_res, sop_text = r, t\n        # Fallbacks if classification ambiguous\n        if sop_res is None and docs:\n            # pick the longer text as SOP\n            sop_res, sop_text = max(docs, key=lambda x: len(x[1] or ''))\n        if form_res is None and docs:\n            # pick the other as form\n            others = [d for d in docs if d[0] != sop_res]\n            if others:\n                form_res, form_text = max(others, key=lambda x: len(x[1] or ''))\n        if not sop_text:\n            return 0.0, 'Could not extract SOP text.'\n        lt = sop_text.lower()\n        # Define core sections (each with synonyms)\n        section_groups = {\n            'title_block': ['document id', 'doc id', 'document number', 'version', 'effective date', 'approved by', 'approvals'],\n            'purpose': ['purpose'],\n            'scope': ['scope'],\n            'definitions': ['definitions', 'abbreviations', 'glossary', 'terms'],\n            'roles_resp': ['roles and responsibilities', 'roles & responsibilities', 'raci', 'responsible', 'accountable', 'consulted', 'informed'],\n            'change_triggers': ['change types', 'triggers', 'thresholds', 'requires formal review'],\n            'submission': ['submission', 'submit a change', 'intake', 'initiate', 'requestor', 'requester'],\n            'review_assessment': ['impact assessment', 'assessment', 'review'],\n            'approval_workflow': ['approval workflow', 'approvals', 'approver', 'routing', 'sign-off'],\n            'implementation_comm': ['implementation', 'execute change', 'communication plan', 'communication'],\n            'records_mgmt': ['records management', 'records', 'document control', 'retention', 'repository', 'archive'],\n            'audit_trace': ['audit trail', 'traceability', 'traceable'],\n            'change_log': ['change log', 'log of changes', 'numbering convention', 'unique change id'],\n            'emergency': ['emergency', 'expedited', 'deviation'],\n            'training_closure': ['training', 'effectiveness', 'closure', 'closeout'],\n            'references': ['references', 'related documents'],\n            'appendices': ['appendix', 'appendices', 'template', 'form'],\n            'revision_history': ['revision history', 'change history', 'version history']\n        }\n        present = []\n        for key, synonyms in section_groups.items():\n            found = any(s in lt for s in synonyms)\n            present.append((key, found))\n        # Coverage ratio\n        coverage = sum(1 for _, f in present if f) / max(1, len(present))\n        score = coverage * 1.6\n        missing = [k for k, f in present if not f]\n        feedback = f\"SOP sections coverage: {coverage:.0%}. Missing groups: {', '.join(missing) if missing else 'None'}.\"\n        return score, feedback\n    except Exception as e:\n        return 0.0, f'Error in section coverage rule: {e}'\n"}, {"type": "code", "name": "Form Field Coverage", "description": "Verify that the Change Request Form includes labeled fields covering initiation, impact assessment (scope/schedule/cost/risk/quality/regulatory), routing/approvals, and closure/traceability.", "weight": 1.8, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        resources = [r for r in context.get_all_outputs() if getattr(r, 'is_document', False)]\n        if not resources:\n            return 0.0, 'No document outputs found.'\n        def read_text(r):\n            rid = r.id\n            name = getattr(r, 'name', '') or ''\n            try:\n                if str(name).lower().endswith('.pdf'):\n                    return context.files.read_pdf_text(rid)\n            except Exception:\n                pass\n            try:\n                return context.files.read_docx_text(rid)\n            except Exception:\n                pass\n            try:\n                return context.files.read_pdf_text(rid)\n            except Exception:\n                return ''\n        docs = [(r, read_text(r)) for r in resources]\n        # Identify form by content\n        form_text = ''\n        for r, t in docs:\n            lt = (t or '').lower()\n            if any(k in lt for k in ['change request form', 'request form', 'form id', 'submitter name', 'approvals']):\n                form_text = t or ''\n                break\n        if not form_text and docs:\n            # fallback: pick the shorter document as the form\n            form_text = min(docs, key=lambda x: len(x[1] or ''))[1] or ''\n        if not form_text:\n            return 0.0, 'Could not extract form text.'\n        lt = form_text.lower()\n        groups = {\n            'header': ['change request form', 'form id', 'form code', 'version', 'date'],\n            'request_details': ['project name', 'program name', 'project id', 'requester', 'requestor', 'department', 'contact'],\n            'change_basics': ['change title', 'summary', 'change category', 'type', 'scope', 'schedule', 'timeline', 'budget', 'cost', 'regulatory', 'priority', 'emergency', 'expedited'],\n            'description_justification': ['description of change', 'justification', 'reason'],\n            'impact_assessment': ['impact assessment', 'scope impact', 'schedule impact', 'timeline impact', 'cost impact', 'budget impact', 'risk', 'quality', 'regulatory impact', 'affected documents', 'affected deliverables'],\n            'plan_alternatives': ['proposed plan', 'implementation plan', 'alternatives considered'],\n            'attachments': ['attachments', 'references'],\n            'routing_approvals': ['routing', 'reviewers', 'approvals', 'approver', 'signature', 'qa', 'quality', 'technical operations', 'tech ops', 'finance', 'regulatory'],\n            'decision_effective': ['decision', 'approved', 'rejected', 'effective date'],\n            'implementation_owner': ['implementation owner', 'owner', 'target completion', 'communication plan', 'communication'],\n            'closure_verification': ['closure', 'closeout', 'verification', 'evidence', 'post-implementation review'],\n            'traceability': ['unique change id', 'change id', 'sop number', 'sop version', 'status', 'archive location', 'record id']\n        }\n        present = {k: any(s in lt for s in syns) for k, syns in groups.items()}\n        coverage = sum(1 for v in present.values() if v) / max(1, len(groups))\n        score = coverage * 1.8\n        missing = [k for k, v in present.items() if not v]\n        fb = f\"Form fields coverage: {coverage:.0%}. Missing groups: {', '.join(missing) if missing else 'None'}.\"\n        return score, fb\n    except Exception as e:\n        return 0.0, f'Error in form field coverage rule: {e}'\n"}, {"type": "code", "name": "Workflow Sequence Validity", "description": "Validate that SOP text describes the correct order: submission \u2192 review/impact assessment \u2192 decision/approval \u2192 implementation/communication \u2192 closure.", "weight": 0.8, "code": "def evaluate(workflow, context):\n    try:\n        resources = [r for r in context.get_all_outputs() if getattr(r, 'is_document', False)]\n        if not resources:\n            return 0.0, 'No documents found.'\n        def read_text(r):\n            rid = r.id\n            name = getattr(r, 'name', '') or ''\n            try:\n                if str(name).lower().endswith('.pdf'):\n                    return context.files.read_pdf_text(rid)\n            except Exception:\n                pass\n            try:\n                return context.files.read_docx_text(rid)\n            except Exception:\n                pass\n            try:\n                return context.files.read_pdf_text(rid)\n            except Exception:\n                return ''\n        docs = [(r, read_text(r)) for r in resources]\n        # Choose SOP as the longer document\n        sop_text = max(docs, key=lambda x: len(x[1] or ''))[1] if docs else ''\n        if not sop_text:\n            return 0.0, 'Could not extract SOP text.'\n        lt = sop_text.lower()\n        # Keywords for each step\n        steps = [\n            ('submission', ['submission', 'initiate', 'intake', 'submit a change', 'request submission']),\n            ('review', ['review', 'impact assessment', 'assessment']),\n            ('approval', ['approval', 'approver', 'decision', 'sign-off']),\n            ('implementation', ['implementation', 'execute change', 'rollout', 'communication']),\n            ('closure', ['closure', 'closeout', 'verification', 'training effectiveness'])\n        ]\n        positions = []\n        for name, synonyms in steps:\n            idxs = [lt.find(s) for s in synonyms if lt.find(s) != -1]\n            positions.append(min(idxs) if idxs else -1)\n        # Score: require presence of terms and increasing order\n        present = [p for p in positions if p != -1]\n        presence_ratio = len(present) / len(steps)\n        in_order = all(positions[i] < positions[i+1] for i in range(len(positions)-1) if positions[i] != -1 and positions[i+1] != -1)\n        score = presence_ratio * 0.6 + (0.2 if in_order and presence_ratio >= 0.6 else 0.0)\n        fb = f\"Workflow steps presence: {presence_ratio:.0%}. In-order: {in_order}.\"\n        return score, fb\n    except Exception as e:\n        return 0.0, f'Error in workflow sequence rule: {e}'\n"}, {"type": "code", "name": "Traceability and Audit Controls", "description": "Check for unique IDs, revision history, records management/retention, audit trail, and change log across SOP and Form.", "weight": 0.8, "code": "def evaluate(workflow, context):\n    try:\n        docs = [r for r in context.get_all_outputs() if getattr(r, 'is_document', False)]\n        if not docs:\n            return 0.0, 'No documents found.'\n        def read_text(r):\n            rid = r.id\n            name = getattr(r, 'name', '') or ''\n            try:\n                if str(name).lower().endswith('.pdf'):\n                    return context.files.read_pdf_text(rid)\n            except Exception:\n                pass\n            try:\n                return context.files.read_docx_text(rid)\n            except Exception:\n                pass\n            try:\n                return context.files.read_pdf_text(rid)\n            except Exception:\n                return ''\n        text = ' '.join([(read_text(r) or '') for r in docs]).lower()\n        checks = {\n            'unique_change_id': ['unique change id', 'change id'],\n            'revision_history': ['revision history', 'version history', 'change history'],\n            'records_retention': ['records management', 'record retention', 'retention', 'archive', 'repository'],\n            'audit_trail': ['audit trail', 'traceability', 'traceable'],\n            'change_log': ['change log', 'log of changes']\n        }\n        present = {k: any(s in text for s in v) for k, v in checks.items()}\n        coverage = sum(1 for v in present.values() if v) / max(1, len(checks))\n        score = coverage * 0.8\n        missing = [k for k, v in present.items() if not v]\n        fb = f\"Traceability controls coverage: {coverage:.0%}. Missing: {', '.join(missing) if missing else 'None'}.\"\n        return score, fb\n    except Exception as e:\n        return 0.0, f'Error in traceability rule: {e}'\n"}, {"type": "code", "name": "Cross-Reference Consistency (SOP \u2194 Form)", "description": "Ensure the SOP explicitly references the Change Request Form and required approver roles; ensure the Form references the SOP ID/version and includes those approver roles.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    try:\n        resources = [r for r in context.get_all_outputs() if getattr(r, 'is_document', False)]\n        if not resources:\n            return 0.0, 'No documents found.'\n        def read_text(r):\n            rid = r.id\n            name = getattr(r, 'name', '') or ''\n            try:\n                if str(name).lower().endswith('.pdf'):\n                    return context.files.read_pdf_text(rid)\n            except Exception:\n                pass\n            try:\n                return context.files.read_docx_text(rid)\n            except Exception:\n                pass\n            try:\n                return context.files.read_pdf_text(rid)\n            except Exception:\n                return ''\n        docs = [(r, (read_text(r) or '').lower()) for r in resources]\n        if not docs:\n            return 0.0, 'No text extracted.'\n        # Identify SOP vs Form by heuristics\n        sop = max(docs, key=lambda x: len(x[1]))[1]\n        form = min(docs, key=lambda x: len(x[1]))[1] if len(docs) > 1 else ''\n        # Swap if detection suggests otherwise\n        if 'change request form' in sop or 'form id' in sop:\n            sop, form = form, sop\n        # Checks\n        sop_refs_form = any(k in sop for k in ['change request form', 'request form', 'cr form'])\n        form_refs_sop = any(k in form for k in ['sop number', 'sop id', 'sop version', 'per sop', 'according to sop'])\n        approver_roles = ['qa', 'quality', 'technical operations', 'tech ops', 'finance', 'regulatory', 'project manager', 'pm']\n        sop_roles = any(a in sop for a in approver_roles)\n        form_roles = any(a in form for a in approver_roles)\n        # Score components\n        score = 0.0\n        details = []\n        if sop_refs_form:\n            score += 0.35; details.append('SOP references Form')\n        if form_refs_sop:\n            score += 0.35; details.append('Form references SOP')\n        if sop_roles:\n            score += 0.15; details.append('SOP lists approver roles')\n        if form_roles:\n            score += 0.15; details.append('Form includes approver roles')\n        return min(score, 1.0), ', '.join(details) if details else 'No cross-references detected.'\n    except Exception as e:\n        return 0.0, f'Error in cross-reference rule: {e}'\n"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Implementation Readiness (LLM)", "description": "Holistic LLM assessment of readability, professionalism, and practical implementability for a biotech context.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Quality and Fitness for Purpose", "description": "Evaluate whether the SOP and Form are professionally presented, internally consistent, and ready for implementation in a regulated biotech environment.", "weight": 2.0, "judge_prompt": "Assess the overall professional quality and implementability of BOTH documents together:\n\nConsider:\n- Clarity, concision, and consistent terminology (SOP \u2194 Form alignment).\n- Formatting quality: headers, numbering, tables, readable layout, page numbers, controlled document metadata (ID, version, effective date, approvals/signatures), and a complete revision history table.\n- Practicality: Can staff follow the SOP as written? Are decision points, roles, and inputs/outputs clear? Is the Form usable to route and decide changes?\n- Regulated context suitability: references to records management, retention, audit trail, traceability, and regulatory impacts are appropriate and precise.\n- Length sanity: SOP typically \u2265 4 pages; Form \u2265 1 page (template is okay) with adequate spacing for entries.\n\nScoring (0\u20132):\n- 2.0 Excellent: Professional, internally consistent, obviously implementable; formatting and metadata are complete.\n- 1.2 Good: Minor issues but ready to pilot/use.\n- 0.6 Fair: Usable but requires notable edits for clarity or consistency.\n- 0.0 Poor: Hard to follow, unprofessional formatting, or not practical.\n", "expectation": "A clean, controlled SOP and a usable Form that staff can immediately adopt in a biotech setting."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "f1be6436-ffff-4fee-9e66-d550291a1735", "rubric": {"category_name": "ACP-IMM 2026 Travel Cost Estimate (Medical Secretary)", "rationale": "This is a mixed task: a Word document that embeds screenshots and includes structured, itemized cost calculations. Stage 1 uses an LLM judge to strictly enforce file format and document structure so later verification is possible. Stage 2 mixes code checks (deterministic presence/consistency) with an LLM judge to verify calculation logic, policy compliance, and cross-references. Stage 3 assesses overall professional quality and usability for stakeholders.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Shape and Structure Gate", "description": "Mandatory structural and format requirements enabling verification. LLM-only per the self-documenting system: checks DOCX presence, required sections, screenshots embedded, and date tags on section headers.", "is_required": true, "max_points": 5.0, "min_score_to_pass": 3.5, "rules": [{"type": "llm_judge", "name": "Document Format and Required Structure", "description": "Check the candidate output is a DOCX with the required sections, each with a date tag, embedded screenshots, and itemized details/calculations as specified.", "weight": 5.0, "judge_prompt": "You are evaluating whether the submitted file is a valid, well-structured Word document (DOCX) that enables verification. Do not judge correctness of numbers here\u2014only presence/structure. Use the rendered document you see (including embedded images) to decide.\n\nFormat requirements:\n- File must be a Word document (DOCX). Prefer filename: \u201c2026 ACP-IMM Estimated Costs.docx\u201d (allow small variations).\n- Professional layout with clear section headers.\n- Each section header must include a date in parentheses indicating when info/screenshots were collected (e.g., \u201c(6/1/25)\u201d). If screenshots were captured on a different date than compilation, note both dates in that section.\n\nRequired sections and inclusions (be flexible with minor header variations):\n1) Registration (with date)\n   - Embedded screenshot of ACP-IMM registration page showing prices.\n   - Brief summary and itemized costs for Dr. Sarah Smith and Dr. Jane Doe, plus a total registration cost.\n2) Travel and Transportation (with date)\n   - FLIGHTS subsection or clearly labeled flight details for each physician: airline, dates, departure/arrival cities, departure times, arrival times, estimated cost per physician, and total flight cost.\n   - GROUND TRANSPORT subsection or clearly labeled ground transportation for airport\u2192hotel and hotel\u2192airport for each physician: embedded screenshots of route/pricing and itemized costs + total ground transportation cost.\n   - Each subsection must have embedded screenshots and a brief itemized summary with costs.\n3) Lodging (with date)\n   - Identify hotels within ~3-block radius of venue; select the cheapest 4- or 5-star option.\n   - Use July 17\u2013July 20 as a proxy window; include an embedded screenshot showing a 3-night stay cost with taxes/fees.\n   - Explain room-share split proportional to nights stayed: Dr. Smith 3 nights, Dr. Doe 2 nights; show per-physician per-day and total lodging costs.\n4) Total Costs (with date)\n   - Two columns labeled \u201cDr. Smith\u201d and \u201cDr. Doe\u201d.\n   - Show \u201c$2k Department Funding\u201d in each column.\n   - Show each physician\u2019s total cost, compute \u201cRemaining Cost\u201d = 2000 \u2212 total, and visually highlight: positive in green (covered) and negative in red (amount from discretionary fund).\n\nGeneral inclusions:\n- For all cost items (registration, flights, ground, lodging), ensure taxes/fees/surcharges are reflected in the totals.\n- Each section includes an embedded screenshot and a brief summary with itemized details and calculations.\n\nScoring (only structure/presence):\n- 5.0: DOCX present; all four required sections with date tags; each section includes embedded screenshots and itemized summaries; Travel and Transportation clearly separates flights and ground; Lodging uses July 17\u201320 proxy and shows proportional split; Total Costs has two columns with $2k funding and Remaining Cost with color cues.\n- 4.0: Minor omission (e.g., one section missing a date tag OR one section missing a screenshot) but overall structure intact.\n- 3.5: One required element missing or unclear (e.g., no flights/ground separation OR Total Costs lacks color but has values). Still verifiable.\n- 2.0: Multiple missing elements (e.g., missing screenshots in several sections, or a required section absent). Not reliably verifiable.\n- 0.0: Not a DOCX, or document lacks most required sections/structure.\n\nOnly assess structure and presence\u2014not numerical correctness or realism.", "expectation": "A DOCX containing four properly dated sections with embedded screenshots, itemized details, and totals; clear separation of flights vs ground transport; lodging proxy and proportional split; and a final two-column Total Costs section with $2k funding and Remaining Cost color cues."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification of Calculations and Policy Compliance", "description": "Now verify correctness/consistency: presence of key terms and dates in text, structural signals indicating calculations are done, and LLM cross-checks for arithmetic logic and policy conditions (e.g., $2k funding, Dr. Doe\u2019s time constraint).", "is_required": false, "max_points": 3.5, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "DOCX Naming and Type", "description": "Confirm the primary output is a DOCX and filename references ACP-IMM 2026 cost estimate.", "weight": 0.5, "code": "import re\nfrom pathlib import Path\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output:\n            return 0.0, \"No output found.\"\n        if not output.is_document:\n            return 0.0, \"Primary output is not a document.\"\n        path = context.files.get_path(output.id)\n        name = path.name.lower()\n        ok_suffix = path.suffix.lower() == \".docx\"\n        name_hint = (\"2026\" in name and \"acp\" in name and \"imm\" in name) or (\"estimated\" in name and \"cost\" in name)\n        score = 0.0\n        if ok_suffix:\n            score += 0.3\n        if name_hint:\n            score += 0.2\n        return min(score, 0.5), f\"Suffix OK: {ok_suffix}; Name hint: {name_hint}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Section Headers Include Dates", "description": "Check that Registration, Travel and Transportation, Lodging, and Total Costs headers appear with a date in parentheses near the header.", "weight": 0.7, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"Missing or wrong-type output.\"\n        text = context.files.read_docx_text(output.id)\n        t = text.lower()\n        # Patterns for headers with date like (6/1/25) or (06/01/2025)\n        date_pat = r\"\\(\\s*\\d{1,2}/\\d{1,2}/(\\d{2}|\\d{4})\\s*\\)\"\n        headers = [\n            (\"registration\", re.compile(r\"registration\\s*\" + date_pat)),\n            (\"travel and transportation\", re.compile(r\"travel\\s+and\\s+transportation\\s*\" + date_pat)),\n            (\"lodging\", re.compile(r\"lodging\\s*\" + date_pat)),\n            (\"total costs\", re.compile(r\"total\\s+costs\\s*\" + date_pat)),\n        ]\n        found = []\n        for label, pat in headers:\n            m = pat.search(t)\n            found.append(bool(m))\n        score = (sum(found)/len(found)) * 0.7\n        return score, f\"Headers with dates found: {found}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Totals Section Structure Signals", "description": "In the 'Total Costs' section text, find both physician names, the $2k funding reference, and the 'Remaining Cost' label.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"Missing or wrong-type output.\"\n        text = context.files.read_docx_text(output.id)\n        t = text\n        tl = t.lower()\n        # Locate Total Costs section window\n        idx = tl.find(\"total costs\")\n        window = t[idx: idx+1500] if idx != -1 else t\n        wl = window.lower()\n        checks = [\n            (\"dr. smith\" in wl),\n            (\"dr. doe\" in wl or \"jane doe\" in wl),\n            (\"$2k\" in wl or \"2000\" in wl and \"department\" in wl),\n            (\"remaining cost\" in wl)\n        ]\n        score = (sum(checks)/len(checks)) * 0.4\n        return score, f\"Totals checks: {checks}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Lodging Proxy Dates and Split Mention", "description": "Verify lodging section references July 17\u201320 proxy window and mentions the proportional split and the 3-night/2-night counts.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"Missing or wrong-type output.\"\n        text = context.files.read_docx_text(output.id)\n        tl = text.lower()\n        # Locate lodging section text window\n        idx = tl.find(\"lodging\")\n        window = tl[idx: idx+2000] if idx != -1 else tl\n        has_july = (\"july 17\" in window and \"july 20\" in window) or (\"july 17-20\" in window or \"jul 17-20\" in window)\n        has_nights = (\"3 night\" in window or \"3-night\" in window or \"three night\" in window) and (\"2 night\" in window or \"2-night\" in window or \"two night\" in window)\n        has_split_words = (\"split\" in window or \"allocation\" in window or \"proportion\" in window or \"proportionally\" in window)\n        checks = [has_july, has_nights, has_split_words]\n        score = (sum(checks)/len(checks)) * 0.4\n        return score, f\"Lodging checks: {checks}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "llm_judge", "name": "Calculation Consistency and Policy Compliance", "description": "Cross-check that sums and policy rules are applied: lodging split proportional to nights (3 vs 2), Dr. Doe\u2019s time constraint reflected in itinerary, taxes/fees included, and $2k funding subtraction is shown with correct color semantics.", "weight": 1.5, "judge_prompt": "Now verify basic correctness based on the structured document you see. Focus on the following:\n\n1) Lodging cost split: The document should allocate the total room cost proportionally by nights stayed (Dr. Smith 3 nights, Dr. Doe 2 nights). Check that per-physician lodging totals reflect this proportional split and that taxes/fees are included in lodging totals.\n2) Dr. Doe\u2019s time constraint: Verify the flight/itinerary shows Jane/Dr. Doe stays only the first two days and departs to make an obligation on April 18 at 3pm (e.g., arrival/time windows consistent, departure on/around Apr 18 with sufficient time).\n3) Taxes/fees/surcharges: Confirm totals for flights, ground transportation, and lodging explicitly include applicable taxes/fees where applicable (or the text clarifies that estimates include them).\n4) Total per-physician cost: Verify that each physician\u2019s total appears to equal the sum of Registration + Flights + Ground Transportation + Lodging allocation.\n5) Funding subtraction and highlighting: In the Total Costs section, verify that the Remaining Cost is computed from $2,000 Department Funding (Remaining = 2000 \u2212 total). Check color semantics: positive remaining highlighted green; negative remaining highlighted red.\n6) Lodging proxy dates: Confirm the document used July 17\u201320 as the pricing proxy for April 2026 and cites those dates in the Lodging section.\n\nScoring guide (allocate partial credit for minor arithmetic rounding differences or clearly explained assumptions):\n- 1.5: All items appear correct and consistent.\n- 1.0: One minor issue (e.g., small arithmetic mismatch < 2% or unclear fee handling) but overall correct.\n- 0.5: Multiple issues or unclear allocation/itinerary, but some elements correct.\n- 0.0: Major errors (e.g., lodging split not proportional as described, totals do not sum, $2k funding not applied, or Dr. Doe\u2019s time constraint ignored).", "expectation": "Arithmetic and policy logic are consistent: per-physician totals sum correctly from itemized components; lodging split uses 3 vs 2 nights proportionally; Dr. Doe departs in time for the 3pm Apr 18 obligation; taxes/fees included; Remaining Cost computed from $2,000 with correct color semantics."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Presentation Quality and Professionalism", "description": "Evaluate clarity, readability, and stakeholder readiness for departmental approval and booking actions.", "is_required": false, "max_points": 1.5, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation", "description": "Holistic quality assessment: formatting, clarity, and usability for physicians and administrators.", "weight": 1.5, "judge_prompt": "Assess the document\u2019s overall professional quality and usability for Sacred Health Hospital\u2019s Internal Medicine department:\n- Clarity and readability: Are sections well-organized with concise summaries, legible screenshots (cropped to relevant info), and clear labels?\n- Completeness signals: Are assumptions and constraints briefly stated (e.g., proxy dates, membership pricing, time-sensitive nature)? Are itemized lists/tables clean and consistent across sections?\n- Actionability: Could an administrator proceed to bookings from this doc? Are sources identifiable from screenshots? Are costs easy to find?\n- Polishing: Consistent terminology (physician names, ACP-IMM), correct spelling/grammar, page headers/footers optional, but overall professional tone.\n\nScoring:\n- 1.5: Highly professional, clear, and immediately actionable.\n- 1.0: Generally professional with minor issues.\n- 0.5: Adequate but some readability or organization problems.\n- 0.0: Poorly presented; hard to use for decision-making.", "expectation": "A clean, professional DOCX that an administrator can use to greenlight bookings: clear sections, legible screenshots, consistent itemization, and easy-to-spot totals and remaining amounts."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "1aecc095-4d76-4b89-b752-1a0f870502cd", "rubric": {"category_name": "Healthcare Ops: Telehealth Intake Workflow Package", "rationale": "Pattern B (Document). The task requires two Word documents (a 2\u20133 page step-by-step workflow and a 1-page Visio-like roadmap) plus a short announcement email. Stage 1 uses an LLM judge to enforce precise file shapes and structural elements so later verification is trivial. Stage 2 mixes code rules to verify required content signals and constraints (keywords, cross-references, word counts). Stage 3 uses an LLM judge for overall professional quality and audience fit.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate (LLM-only)", "description": "Gate that verifies the exact required deliverables exist in the correct formats with the mandated structure so verification is possible.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Required Word Docs: Presence + Structure", "description": "Verify two Word documents exist with the specified names and structural requirements.", "weight": 6.0, "judge_prompt": "You are validating the candidate\u2019s output files. Only check structure/format and presence, not content quality or correctness. Be flexible with minor naming variations (case/spacing), but the intent and structure must be clear.\n\nPASS CRITERIA (STRUCTURE ONLY):\nTwo separate Word documents (DOCX) with these names and characteristics:\n\nA) \u201cTelehealth Workflow\u201d (DOCX)\n- 2\u20133 pages total.\n- Purpose: step-by-step instructions from pre-telehealth setup through provider handoff.\n- Must include clearly labeled sections (flexible wording is OK) that cover:\n  1) Pre-telehealth setup (explicitly includes reviewing provider\u2019s schedule).\n  2) Pre-call preparation.\n  3) Patient call and identity verification (two identifiers) and consent for telehealth.\n  4) Doxy.me link/room workflow (joining, troubleshooting basics).\n  5) Self-reported vitals/intake questions.\n  6) Handoff to provider process (notify provider/transfer/notes).\n  7) Documentation steps in EMR.\n  8) Contingencies/exceptions (e.g., unreachable patient, no smartphone/internet, interpreter needed, urgent symptoms).\n- Must show at least one checklist or script (e.g., a pre-call checklist and/or call script).\n\nB) \u201cTelehealth Roadmap\u201d (DOCX)\n- 1 page total.\n- Visio-style visual workflow in Word (text boxes/arrows/ASCII flow OK) starting explicitly when the MA places a call to the patient.\n- Must include clear flow/decision points (e.g., Yes/No branches) and show endpoints (e.g., Proceed to Provider, Reschedule).\n- Should indicate swimlanes or labeled roles (e.g., MA, Patient, Provider) or otherwise clearly attribute steps to roles.\n\nSCORING (0\u20136):\n- 6.0: Both DOCX files present with correct/very close names and all required structural elements (A and B) satisfied.\n- 4.5: Both files present and DOCX; minor structural omission in one document (e.g., missing one section OR roadmap lacks explicit roles but still a clear visual flow starting from the MA call).\n- 3.0: Both files present but significant structural gaps (e.g., Workflow missing multiple required sections OR Roadmap not obviously visual/flow-like or not starting at MA call).\n- 1.5: Only one of the two required DOCX files present in roughly correct form.\n- 0.0: Missing both files, wrong formats, or not recognizable as the required documents.\n\nImportant: Do not assess calculation/content accuracy or writing quality. Only verify format, presence, and that required sections/flow elements exist.", "expectation": "Two DOCX files with the specified names and structures: a 2\u20133 page step-by-step workflow with specific sections and a 1-page Visio-style roadmap beginning at the MA call."}, {"type": "llm_judge", "name": "Announcement Email Presence & Basics", "description": "Verify there is a separate email message for MAs (file may be DOCX or Markdown) with length and intent requirements.", "weight": 2.0, "judge_prompt": "Check that a separate file exists containing a short email addressed to Medical Assistants. Format may be DOCX or Markdown. Only verify presence/length/intents, not prose quality.\n\nREQUIREMENTS:\n- Audience: Medical Assistants.\n- Length: 100\u2013150 words (approximate is fine).\n- Purpose: Announce the change, ask MAs to review the two documents (\u201cTelehealth Workflow\u201d and \u201cTelehealth Roadmap\u201d), and encourage feedback/questions.\n- Should reference the new Telehealth process and the provided documents.\n\nSCORING (0\u20132):\n- 2.0: Email present as separate file, clearly addressed to MAs, ~100\u2013150 words, references both documents and invites questions/feedback.\n- 1.0: Email present but missing one element (e.g., slightly outside length OR references only one doc OR audience not clearly MAs).\n- 0.0: No email found as a separate file or fundamentally wrong purpose/audience.", "expectation": "A concise, MA-targeted announcement email (~100\u2013150 words) that references both documents and invites review and feedback."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Consistency Checks (Code + LLM)", "description": "Now that the structure is enforced, verify key content signals, cross-references, and constraints via deterministic checks. Partial credit awarded proportionally.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Workflow Content Coverage", "description": "Check the \u2018Telehealth Workflow\u2019 DOCX includes key operational elements via keyword/section signals.", "weight": 3.0, "code": "import re\\n\\nDEF_REQUIRED = {\\n    'pre_setup': ['pre-telehealth', 'pre telehealth', 'pre-visit', 'previsit', 'setup'],\\n    'schedule': ['schedule review', 'review provider', 'review schedule'],\\n    'call_prep': ['pre-call', 'pre call', 'call preparation', 'prepare to call'],\\n    'identity_consent': ['identity', 'verify', 'two identifiers', 'consent'],\\n    'doxy': ['doxy', 'doxy.me'],\\n    'vitals': ['vitals', 'intake', 'questionnaire'],\\n    'handoff': ['handoff', 'hand-off', 'warm handoff', 'notify provider', 'transfer'],\\n    'documentation': ['document', 'documentation', 'chart', 'emr'],\\n    'contingencies': ['unreachable', 'no smartphone', 'no internet', 'interpreter', 'urgent', 'escalate', 'reschedule'],\\n    'checklist_or_script': ['checklist', 'script']\\n}\\n\\ndef read_doc_text(context, res):\\n    text = ''\\n    try:\\n        text = context.files.read_docx_text(res.id)\\n    except Exception:\\n        try:\\n            if res.is_document:\\n                # fallback for PDF if any\\n                text = context.files.read_pdf_text(res.id)\\n        except Exception:\\n            text = ''\\n    return text or ''\\n\\ndef find_workflow_resource(context):\\n    candidates = []\\n    for r in context.get_all_outputs():\\n        if not r.is_document:\\n            continue\\n        txt = read_doc_text(context, r).lower()\\n        name = getattr(r, 'name', '') or getattr(r, 'title', '') or ''\\n        score = 0\\n        if 'telehealth workflow' in name.lower():\\n            score += 5\\n        # heuristic content score\\n        for klist in DEF_REQUIRED.values():\\n            if any(k in txt for k in klist):\\n                score += 1\\n        if 'step' in txt:\\n            score += 1\\n        candidates.append((score, r, txt))\\n    if not candidates:\\n        return None, ''\\n    candidates.sort(key=lambda x: x[0], reverse=True)\\n    return candidates[0][1], candidates[0][2]\\n\\n\ndef evaluate(workflow, context):\\n    wf_res, wf_text = find_workflow_resource(context)\\n    if not wf_res:\\n        return 0.0, 'Workflow doc not found.'\\n    text = wf_text.lower()\\n\\n    # Check required elements\\n    checks = {}\\n    for key, variants in DEF_REQUIRED.items():\\n        checks[key] = any(v in text for v in variants)\\n\\n    # Score proportionally across 10 signals\\n    total = len(checks)\\n    hits = sum(1 for v in checks.values() if v)\\n    score = (hits / total) * 3.0\\n\\n    missing = [k for k, v in checks.items() if not v]\\n    feedback = f\"Found {hits}/{total} workflow elements. Missing: {', '.join(missing)}\"\\n    return score, feedback\\n"}, {"type": "code", "name": "Roadmap Flow Signals", "description": "Verify the \u2018Telehealth Roadmap\u2019 DOCX starts at MA call and includes decision/branching and role attribution signals.", "weight": 2.0, "code": "import re\\n\\nROADMAP_SIGNALS = {\\n    'start_call': ['ma places a call', 'ma calls', 'call patient', 'place a call'],\\n    'decisions': ['yes/no', 'yes / no', 'decision', 'if ', 'branch', 'no ->', 'yes ->'],\\n    'roles': ['ma', 'medical assistant', 'provider', 'patient', 'swimlane'],\\n    'endpoints': ['proceed to provider', 'handoff', 'reschedule', 'end'],\\n}\\n\\ndef read_doc_text(context, res):\\n    text = ''\\n    try:\\n        text = context.files.read_docx_text(res.id)\\n    except Exception:\\n        try:\\n            if res.is_document:\\n                text = context.files.read_pdf_text(res.id)\\n        except Exception:\\n            text = ''\\n    return text or ''\\n\\ndef find_roadmap_resource(context):\\n    candidates = []\\n    for r in context.get_all_outputs():\\n        if not r.is_document:\\n            continue\\n        txt = read_doc_text(context, r).lower()\\n        name = getattr(r, 'name', '') or getattr(r, 'title', '') or ''\\n        score = 0\\n        if 'telehealth roadmap' in name.lower():\\n            score += 5\\n        # roadmap-ish signals\\n        roadmap_hits = 0\\n        for k, variants in ROADMAP_SIGNALS.items():\\n            if any(v in txt for v in variants):\\n                roadmap_hits += 1\\n        score += roadmap_hits\\n        # presence of arrows\\n        if '->' in txt or '\u2192' in txt or '\u2794' in txt or 'flow' in txt:\\n            score += 1\\n        candidates.append((score, r, txt))\\n    if not candidates:\\n        return None, ''\\n    candidates.sort(key=lambda x: x[0], reverse=True)\\n    return candidates[0][1], candidates[0][2]\\n\\n\ndef evaluate(workflow, context):\\n    rm_res, rm_text = find_roadmap_resource(context)\\n    if not rm_res:\\n        return 0.0, 'Roadmap doc not found.'\\n    text = rm_text.lower()\\n\\n    checks = {}\\n    for key, variants in ROADMAP_SIGNALS.items():\\n        checks[key] = any(v in text for v in variants)\\n    # Arrow/flow hint\\n    arrows = any(s in text for s in ['->', '\u2192', '\u2794']) or 'flow' in text\\n\\n    total = len(checks) + 1\\n    hits = sum(1 for v in checks.values() if v) + (1 if arrows else 0)\\n    score = (hits / total) * 2.0\\n\\n    missing = [k for k, v in checks.items() if not v]\\n    if not arrows:\\n        missing.append('arrows/flow cues')\\n    feedback = f\"Roadmap signals {hits}/{total}. Missing: {', '.join(missing)}\"\\n    return score, feedback\\n"}, {"type": "code", "name": "Email Length and Content", "description": "Validate the email exists, is ~100\u2013150 words, references reviewing documents, and invites questions/feedback.", "weight": 2.0, "code": "import re\\n\\ndef read_text_any(context, res):\\n    try:\\n        if res.is_text_format:\\n            return context.files.read_text(res.id) or ''\\n    except Exception:\\n        pass\\n    try:\\n        if res.is_document:\\n            try:\\n                return context.files.read_docx_text(res.id) or ''\\n            except Exception:\\n                return context.files.read_pdf_text(res.id) or ''\\n    except Exception:\\n        return ''\\n    return ''\\n\\ndef find_email_resource(context, exclude_ids):\\n    best = None\\n    best_score = -1\\n    best_text = ''\\n    for r in context.get_all_outputs():\\n        if r.id in exclude_ids:\\n            continue\\n        txt = read_text_any(context, r)\\n        if not txt:\\n            continue\\n        low = txt.lower()\\n        score = 0\\n        if 'dear' in low or 'to:' in low or 'subject:' in low:\\n            score += 1\\n        if 'medical assistant' in low or 'mas' in low:\\n            score += 1\\n        if 'telehealth' in low:\\n            score += 1\\n        if 'workflow' in low or 'roadmap' in low:\\n            score += 1\\n        if score > best_score:\\n            best = r\\n            best_score = score\\n            best_text = txt\\n    return best, best_text\\n\\n\ndef evaluate(workflow, context):\\n    # Try to exclude obvious workflow/roadmap by name\\n    wf_ids = set()\\n    for r in context.get_all_outputs():\\n        name = (getattr(r, 'name', '') or getattr(r, 'title', '') or '').lower()\\n        if 'telehealth workflow' in name or 'telehealth roadmap' in name:\\n            wf_ids.add(r.id)\\n    email_res, email_text = find_email_resource(context, wf_ids)\\n    if not email_res:\\n        return 0.0, 'Email file not found.'\\n    text = re.sub(r\"\\s+\", ' ', email_text.strip())\\n    words = re.findall(r\"\\\\b\\\\w+\\\\b\", text)\\n    wc = len(words)\\n\\n    # Length scoring\\n    if 100 <= wc <= 150:\\n        length_score = 1.0\\n    elif 90 <= wc <= 170:\\n        length_score = 0.5\\n    else:\\n        length_score = 0.0\\n\\n    low = text.lower()\\n    content_hits = 0\\n    # Must encourage review of documents\\n    if 'review' in low and ('workflow' in low and 'roadmap' in low):\\n        content_hits += 1\\n    # Invite feedback/questions\\n    if 'feedback' in low or 'questions' in low:\\n        content_hits += 1\\n    # Telehealth context mention\\n    if 'telehealth' in low:\\n        content_hits += 1\\n\\n    content_score = (content_hits / 3) * 1.0\\n    total_score = length_score + content_score  # max 2.0\\n    feedback = f\"Email words={wc}; content hits={content_hits}/3; length_score={length_score:.2f}\"\\n    return total_score, feedback\\n"}, {"type": "code", "name": "Email File Name Cross-Reference", "description": "Check that the email references both document names explicitly (or very close variants).", "weight": 1.0, "code": "import re\\n\\nTARGETS = [\\n    ('telehealth workflow', ['telehealth workflow']),\\n    ('telehealth roadmap', ['telehealth roadmap'])\\n]\\n\\n\ndef read_all_texts(context):\\n    texts = []\\n    for r in context.get_all_outputs():\\n        try:\\n            if r.is_text_format:\\n                t = context.files.read_text(r.id)\\n            elif r.is_document:\\n                try:\\n                    t = context.files.read_docx_text(r.id)\\n                except Exception:\\n                    t = context.files.read_pdf_text(r.id)\\n            else:\\n                t = ''\\n        except Exception:\\n            t = ''\\n        if t:\\n            texts.append((r, t.lower()))\\n    return texts\\n\\n\ndef evaluate(workflow, context):\\n    texts = read_all_texts(context)\\n    # Heuristic: pick the shortest text that looks like an email\n    email_candidates = []\\n    for r, t in texts:\\n        if any(k in t for k in ['dear', 'subject:', 'to:']) and 'telehealth' in t:\\n            email_candidates.append((len(t.split()), r, t))\\n    if not email_candidates:\\n        return 0.0, 'No clear email candidate.'\\n    email_candidates.sort(key=lambda x: x[0])\\n    email_text = email_candidates[0][2]\\n\\n    hits = 0\\n    missing = []\\n    for label, variants in TARGETS:\\n        if any(v in email_text for v in variants):\\n            hits += 1\\n        else:\\n            missing.append(label)\\n\\n    score = (hits / len(TARGETS)) * 1.0\\n    feedback = f\"Email references {hits}/{len(TARGETS)} target names. Missing: {', '.join(missing)}\"\\n    return score, feedback\\n"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Fitness (LLM)", "description": "Holistic quality assessment for clarity, usability, and professionalism for the MA audience.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity, Usability, and Professionalism", "description": "Judge overall quality and audience fit of the documents and email.", "weight": 4.0, "judge_prompt": "Review the provided documents and email holistically for professional quality and audience fit. Focus on clarity, operational usability, and whether an MA could successfully execute the workflow with minimal confusion. Consider only quality\u2014not structural presence, which was checked earlier.\n\nEvaluate:\n1) Step-by-step Workflow (DOCX):\n   - Clear, concise steps; actionable checklists/scripts; logical sequencing; readability; practical troubleshooting tips.\n   - Handoff instructions easy to follow; documentation steps unambiguous.\n2) Roadmap (DOCX):\n   - Visually clear flow; decisions and endpoints obvious; role attribution understandable; 1-page readability.\n3) Email:\n   - Professional tone; concise and motivating; clearly directs MAs to review; invites questions; appropriate for the audience.\n\nSCORING (0\u20134):\n- 4.0: Highly clear, actionable, and professional; MAs can run intake reliably with minimal training.\n- 3.0: Generally clear and usable with minor gaps or awkward phrasing.\n- 2.0: Some clarity/usability issues; requires additional clarification for reliable execution.\n- 1.0: Hard to follow; significant gaps; likely confusion.\n- 0.0: Not professional or not usable for intended audience.", "expectation": "Polished, MA-ready materials that enable consistent Doxy.me intake and handoff via Telehealth."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "772e7524-174e-4c88-957e-6e510b61ea69", "rubric": {"category_name": "Healthcare: Nurse Practitioner SOAP Note for Suspected CAP", "rationale": "This rubric enforces a self-documenting SOAP note structure first (Stage 1), then verifies clinical correctness and internal consistency using code plus an LLM (Stage 2), and finally assesses professional quality and suitability (Stage 3). Stage 1 is a hard gate with explicit section and subsection requirements so later checks are trivial. Stage 2 uses deterministic code checks for antibiotics vs. allergies, presence of core clinical elements, diagnosis specificity, and safety net/follow-up, complemented by an LLM coherence check. Stage 3 evaluates professional presentation and documentation standards.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Structured SOAP Note Gate (Format and Sections)", "description": "LLM-only gate verifying the output is a professional SOAP note document with exact required sections and visible structure to enable verification.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "SOAP Structure and Format Requirements", "description": "Check the candidate output is a PDF or DOCX SOAP note with explicit required sections and subsections present and labeled.", "weight": 4.0, "judge_prompt": "You are evaluating whether the submitted output is a properly structured SOAP note document with all required sections. Only check presence/structure/format, not clinical quality.\n\nFormat requirements:\n- Must be a PDF or DOCX document (not plain text/markdown/Excel).\n- At least 1 full page of content, professionally formatted.\n- Clearly labeled SOAP headings: Subjective, Objective, Assessment, Plan (exact or close variants like \u201cS:\u201d, \u201cO:\u201d, etc.).\n\nRequired content under each heading (be flexible with exact wording, but headers must be visible):\n\nSubjective must include:\n- Chief Complaint (CC)\n- HPI covering current illness (fever/chills, cough with sputum, chest pain with cough, onset/timeline, modifiers, severity)\n- Review of Systems relevant to respiratory and constitutional at minimum\n- Past Medical History\n- Medications (must list Lexapro 10 mg daily or current meds)\n- Allergies (must list PCN and sulfa allergies)\n- Family History\n- Social History (occupation ER nurse, smoking/alcohol status)\n- Immunizations status\n\nObjective must include:\n- Vitals (specifically: Temp, Pulse/HR, Resp/RR, BP, and SpO2/O2 sat)\n- Physical Exam with lung and cardiac findings noted (e.g., right lower lobe crackles/bronchial breath sounds)\n- Diagnostics/Studies section noting chest X-ray (CXR) with RLL consolidation\n\nAssessment must include:\n- Primary diagnosis consistent with pneumonia (e.g., \u201cRight lower lobe pneumonia\u201d or \u201cCommunity-acquired pneumonia (CAP)\u201d) and a brief differential\n\nPlan must include (as labeled subsections or bullet points):\n- Diagnostics/monitoring (if any further tests)\n- Pharmacologic treatment (antibiotic choice must be explicitly documented; do not judge correctness yet, only presence)\n- Supportive care (e.g., antipyretic, hydration, cough management)\n- Patient education and return precautions\n- Follow-up interval/timing\n- Work/occupational or infection-control guidance\n- Optional: ICD-10 codes and provider signature/date\n\nScoring:\n- 4.0: PDF/DOCX with all SOAP headings + all required subsections present and clearly labeled under the correct section.\n- 3.5: All headings present; missing only one minor subsection OR headings present but one subsection is ambiguously placed.\n- 3.0: All headings present; missing two required subsections.\n- 2.0: SOAP format present but multiple subsections missing (>2) OR one major section missing content.\n- 1.0: Document format OK but SOAP structure largely incomplete (e.g., only 1\u20132 sections present).\n- 0.0: Not PDF/DOCX, or SOAP structure absent.\n\nOnly check presence and structure; do not judge clinical appropriateness or calculation correctness.", "expectation": "A professional PDF/DOCX SOAP note with complete Subjective, Objective, Assessment, and Plan sections, including the specified subsections that make subsequent verification trivial."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Clinical Correctness and Consistency", "description": "Code rules verify safety and internal consistency enabled by Stage 1\u2019s structure. An LLM judge checks clinical coherence with the presented case.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Antibiotic Selection and Allergy Safety", "description": "Checks that an appropriate CAP antibiotic is documented and that no penicillin or sulfonamide (sulfa) agents are prescribed given listed allergies.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not (output.is_document or output.is_text_format):\n        return 0.0, \"No readable document output.\"\n\n    text = \"\"\n    try:\n        # Try DOCX first\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            pass\n        if not text:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                pass\n        if not text:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0, \"Unable to read document text.\"\n    except Exception:\n        return 0.0, \"Error extracting text.\"\n\n    t = text.lower()\n\n    # Terms indicating allergy-conflicting agents\n    allergen_terms = [\n        'penicillin', 'pcn', 'amoxicillin', 'ampicillin', 'piperacillin', 'tazobactam',\n        'unasyn', 'augmentin', 'nafcillin', 'oxacillin', 'dicloxacillin',\n        'bactrim', 'trimethoprim-sulfamethoxazole', 'tmp-smx', 'sulfamethoxazole',\n        'sulfonamide', 'sulfa', 'sulfasalazine'\n    ]\n    if any(term in t for term in allergen_terms):\n        return 0.0, \"Allergy-conflicting antibiotic detected (penicillin or sulfonamide class).\"\n\n    # Reasonable outpatient CAP options (depending on local resistance/policies):\n    recommended_terms = [\n        'doxycycline', 'azithromycin', 'clarithromycin',\n        'levofloxacin', 'moxifloxacin', 'respiratory fluoroquinolone',\n        'macrolide', 'fluoroquinolone'\n    ]\n\n    if any(term in t for term in recommended_terms):\n        return 1.0, \"Appropriate CAP antibiotic class documented with no allergy conflict.\"\n\n    if ('antibiotic' in t) or ('abx' in t):\n        return 0.5, \"Non-specific antibiotic mention without explicit agent.\"\n\n    return 0.0, \"No antibiotic documented.\""}, {"type": "code", "name": "Presence of Core Clinical Elements", "description": "Checks for essential documentation elements: vitals, lung and cardiac exam, imaging mention (CXR), allergies, and immunization status.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not (output.is_document or output.is_text_format):\n        return 0.0, \"No readable document output.\"\n\n    text = \"\"\n    try:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            pass\n        if not text:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                pass\n        if not text:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0, \"Unable to read document text.\"\n    except Exception:\n        return 0.0, \"Error extracting text.\"\n\n    t = text.lower()\n\n    present = 0\n    total_checks = 9\n\n    # Vitals: BP, Temp, HR/Pulse, RR, O2/SpO2\n    if 'bp' in t or 'blood pressure' in t:\n        present += 1\n    if 'temp' in t or 'temperature' in t or 'fever' in t:\n        present += 1\n    if 'hr' in t or 'heart rate' in t or 'pulse' in t:\n        present += 1\n    if 'rr' in t or 'resp' in t or 'respiratory rate' in t:\n        present += 1\n    if 'spo2' in t or 'o2' in t or 'oxygen saturation' in t:\n        present += 1\n\n    # Exam elements\n    lungs_terms = ['crackles', 'rales', 'rhonchi', 'bronchial breath', 'right lower lobe', 'rll']\n    if any(term in t for term in lungs_terms):\n        present += 1\n\n    if 's1' in t and 's2' in t or 'regular rate and rhythm' in t or 'rrr' in t or 'no murmurs' in t:\n        present += 1\n\n    # Imaging mention\n    if 'cxr' in t or 'chest x-ray' in t or 'chest radiograph' in t or 'chest xray' in t:\n        present += 1\n\n    # Allergies documented\n    if 'allerg' in t:\n        present += 1\n\n    # Immunization status\n    if 'immuniz' in t or 'vaccine' in t or 'influenza' in t or 'flu shot' in t:\n        present += 1\n\n    # Cap score to total_checks for safety in case of overcount\n    present = min(present, total_checks)\n\n    score = (present / total_checks)\n    return score, f\"Core elements present: {present}/{total_checks}.\""}, {"type": "code", "name": "Diagnosis Documentation Specificity", "description": "Checks that pneumonia is documented with specificity (right lower lobe and/or CAP mention) and optionally an ICD-10 code.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not (output.is_document or output.is_text_format):\n        return 0.0, \"No readable document output.\"\n\n    text = \"\"\n    try:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            pass\n        if not text:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                pass\n        if not text:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0, \"Unable to read document text.\"\n    except Exception:\n        return 0.0, \"Error extracting text.\"\n\n    t = text.lower()\n\n    has_pna = 'pneumonia' in t\n    has_rll = ('right lower lobe' in t) or re.search(r'\\brll\\b', t) is not None\n    has_cap = 'community-acquired' in t or re.search(r'\\bcap\\b', t) is not None\n    has_icd = ('icd-10' in t or 'icd10' in t or 'icd' in t) and re.search(r'\\bj\\d{2}(?:\\.\\d+)?\\b', t) is not None\n\n    parts = [has_pna, has_rll, has_cap, has_icd]\n    score = sum(1 for x in parts if x) / 4.0\n    feedback_bits = []\n    if not has_pna:\n        feedback_bits.append('pneumonia term missing')\n    if not has_rll:\n        feedback_bits.append('laterality/location missing (RLL)')\n    if not has_cap:\n        feedback_bits.append('CAP label missing')\n    if not has_icd:\n        feedback_bits.append('ICD-10 code missing')\n    fb = 'All present' if not feedback_bits else ('Missing: ' + ', '.join(feedback_bits))\n    return score, fb"}, {"type": "code", "name": "Safety Net and Follow-Up Plan", "description": "Checks for return precautions, explicit follow-up interval, and work/infection-control guidance.", "weight": 0.7, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not (output.is_document or output.is_text_format):\n        return 0.0, \"No readable document output.\"\n\n    text = \"\"\n    try:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            pass\n        if not text:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                pass\n        if not text:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0, \"Unable to read document text.\"\n    except Exception:\n        return 0.0, \"Error extracting text.\"\n\n    t = text.lower()\n\n    # Return precautions: look for common red flags and ER guidance\n    red_flag_terms = [\n        'go to the er', 'emergency department', 'seek immediate care', 'call 911',\n        'worsening shortness of breath', 'increasing shortness of breath', 'severe dyspnea',\n        'chest pain', 'confusion', 'cyanosis', 'persistent fever', 'dehydration'\n    ]\n    has_precautions = any(term in t for term in red_flag_terms) or ('return precautions' in t) or ('red flags' in t)\n\n    # Follow-up interval: look for follow-up and timeframe\n    has_follow_word = re.search(r'follow[- ]?up|recheck|return visit', t) is not None\n    has_timeframe = re.search(r'\\b(24|48|72)\\b|\\b(1|2|3)\\s*(day|days)\\b|\\bweek\\b', t) is not None\n    has_followup = has_follow_word and has_timeframe\n\n    # Work/infection-control guidance\n    work_terms = ['work note', 'off work', 'return to work', 'occupational health', 'employee health', 'stay home', 'mask', 'isolation', 'infection control']\n    has_work = any(term in t for term in work_terms)\n\n    score = (int(has_precautions) + int(has_followup) + int(has_work)) / 3.0\n    fb = f\"Precautions: {'yes' if has_precautions else 'no'}, Follow-up: {'yes' if has_followup else 'no'}, Work/infection control: {'yes' if has_work else 'no'}.\"\n    return score, fb"}, {"type": "llm_judge", "name": "Clinical Coherence and Evidence Alignment", "description": "Judge whether the Assessment and Plan are clinically coherent for outpatient CAP given the presented case: aligns with RLL consolidation, vitals incl. SpO2 93% RA, allergy-aware antibiotic choice, supportive care, and appropriate follow-up/safety net.", "weight": 1.0, "judge_prompt": "Evaluate clinical coherence (not just structure). Consider the case context: 45-year-old female with fever to 104\u00b0F, cough with green sputum, pleuritic right-sided chest pain, RLL crackles/bronchial breath sounds, CXR with right lower lobe consolidation, SpO2 93% on room air; allergies to penicillin (PCN) and sulfa. She is an ER nurse.\n\nCriteria (score proportionally):\n- Diagnosis explicitly consistent with community-acquired pneumonia and RLL consolidation.\n- Plan includes a reasonable outpatient antibiotic that respects PCN and sulfa allergies (e.g., doxycycline or macrolide; or respiratory fluoroquinolone if justified). Do not penalize cephalosporins unless clearly contraindicated.\n- Supportive care addressed (antipyretic, hydration, cough control) and return precautions for worsening symptoms.\n- Follow-up specified within an appropriate timeframe (e.g., 24\u201372 hours or sooner if worsening) and consideration of work/infection control for an ER nurse.\n- No clearly unsafe or irrelevant interventions.\n\nScoring:\n- 1.0: All criteria addressed coherently with appropriate outpatient approach and allergy-aware antibiotic.\n- 0.7: Minor omissions (e.g., missing one element like work guidance or specific timeframe), otherwise coherent.\n- 0.4: Partially coherent but important elements missing or vague (e.g., antibiotic not specified; weak safety net).\n- 0.0: Incoherent plan or unsafe recommendations (e.g., penicillin or sulfa despite listed allergies).", "expectation": "A coherent CAP plan aligned to the case, with allergy-aware antibiotic selection, supportive care, follow-up, safety net, and work/infection-control guidance."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Documentation Standards", "description": "Assesses professionalism, clarity, readability, and completeness as a clinical note for healthcare use.", "is_required": false, "max_points": 1.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Clarity", "description": "Evaluates tone, organization, readability, and documentation professionalism appropriate for a nurse practitioner SOAP note.", "weight": 1.0, "judge_prompt": "Assess overall professional quality of the SOAP note:\n- Readability and organization (clear headings, logical flow, concise clinical language)\n- Appropriate medical terminology and standard abbreviations; minimal errors/typos\n- Professional tone; avoids extraneous or speculative content; privacy-appropriate\n- Clearly labeled sections/subsections and bulleting for plan items\n- Optional but beneficial: provider signature, date/time, ICD-10 codes\n\nScoring:\n- 1.0: Highly professional, clear, concise, minimal errors, well-formatted with helpful labels/bullets.\n- 0.7: Generally professional and clear; a few minor issues in formatting or wording.\n- 0.4: Understandable but several issues reduce clarity/professionalism.\n- 0.0: Disorganized, unprofessional, or difficult to follow.", "expectation": "A clean, well-organized, professional SOAP note suitable for the medical record."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "8384083a-c31b-4194-80ba-4d335a444918", "rubric": {"category_name": "Retail Pharmacy \u2014 Days\u2019 Supply Reference Guide (GLP-1s + Miebo)", "rationale": "This rubric enforces a self-documenting workflow for a 1\u20132 page PDF/DOCX quick-reference guide targeted at pharmacy technicians and interns. Stage 1 (LLM-only) mandates an explicit, verifiable document structure: a concise guide with a single consolidated table (or per-medication mini-tables) containing the NDC, strength, package size, standard SIG, formula used, and the correct days\u2019 supply per package for each of seven medications. Stage 2 mixes code and LLM checks to verify coverage, presence of NDC patterns, headers, SIG consistency (weekly vs daily), and plausibility of days\u2019 supply calculations. Stage 3 evaluates presentation quality and usability for frontline staff.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Structure and Format Gate (LLM-only)", "description": "Gate: verify the output is a 1\u20132 page PDF/DOCX guide with the exact structural elements needed for automated verification.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Document Requirement", "description": "Verify the candidate produced a 1\u20132 page PDF/DOCX with the required sections and tabular structure enabling verification.", "weight": 4.0, "judge_prompt": "You are the Stage 1 shape gate. Examine the candidate output (you can view PDFs/DOCX). Check ONLY structure/format, not calculation accuracy.\n\nRequired OUTPUT FORMAT:\n- File type: PDF or DOCX (not Excel/CSV, not plain text/markdown).\n- Length: 1\u20132 pages preferred (2 pages max for full credit; 3+ pages gets partial credit only if structure is otherwise correct).\n- Audience: technicians/interns (simple, quick-reference layout).\n\nRequired STRUCTURE (be flexible with near-synonyms for headers):\n1) A clear title on page 1 such as: \u201cDays\u2019 Supply Quick Reference,\u201d \u201cGLP\u20111 Days\u2019 Supply Guide,\u201d or similar.\n2) A brief \u201cHow to Use\u201d or \u201cPurpose\u201d blurb (1\u20133 sentences) stating the guide supports accurate days\u2019 supply billing for high\u2011cost meds to avoid audits.\n3) Tabular content in ONE of two acceptable shapes:\n   A. A single consolidated table with columns equivalent to:\n      - Medication\n      - NDC\n      - Strength\n      - Package Size\n      - Standard SIG (or SIG)\n      - Formula Used (for Days\u2019 Supply)\n      - Days\u2019 Supply per Package\n   OR\n   B. Seven clearly labeled mini\u2011tables (one per medication), each including the same fields above.\n4) Coverage: The following seven medications must be included as rows (in the consolidated table) OR as labeled subsections (for mini\u2011tables): Ozempic, Mounjaro, Wegovy, Zepbound, Saxenda, Victoza, Miebo.\n5) Footer or metadata: include a small \u201cPrepared by/Version/Date\u201d OR \u201cLast updated\u201d note anywhere in the document (footer or body).\n\nScoring (STRUCTURE ONLY):\n- 4.0: PDF/DOCX; 1\u20132 pages; has title; includes either consolidated table or seven mini\u2011tables; each of the 7 meds is present; all required fields (NDC, Strength, Package Size, Standard SIG, Formula Used, Days\u2019 Supply per Package) are present; includes a date/version/author note.\n- 3.2: Same as above but missing the footer/metadata OR slightly over 2 pages with otherwise perfect structure.\n- 2.4: Valid PDF/DOCX with a table layout, but missing 1\u20132 of: a required column, or one medication; OR unclear title/purpose.\n- 1.2: Valid PDF/DOCX but lacks a clearly structured table (e.g., prose only) OR missing multiple required columns/medications.\n- 0.0: Not PDF/DOCX, or no usable structure for verification.\n\nOnly assess presence/format of sections and tables, not content correctness.", "expectation": "A 1\u20132 page PDF/DOCX with a prominent title, a short purpose blurb, and a consolidated table (or seven mini\u2011tables) covering all 7 medications with the columns: Medication, NDC, Strength, Package Size, Standard SIG, Formula Used, Days\u2019 Supply per Package; plus a small date/version/author note."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Consistency Verification", "description": "Mixed code + LLM checks: coverage of all meds, presence of NDCs, header fields, SIG frequency alignment (weekly vs daily), and numeric plausibility of days\u2019 supply. Also an LLM cross-check for internal consistency of row calculations.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "All 7 Medications Mentioned + NDC Pattern Presence", "description": "Confirms all seven medication names are present and that at least seven NDC-like patterns appear.", "weight": 1.4, "code": "import re\n\ndef evaluate(workflow, context):\n    WEIGHT = 1.4\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    if not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        if hasattr(output, 'mime') and 'pdf' in (output.mime or '').lower():\n            text = context.files.read_pdf_text(output.id)\n        else:\n            # Try PDF first; fallback to DOCX\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n    if not text:\n        return 0.0\n    tl = text.lower()\n\n    meds = [\"ozempic\",\"mounjaro\",\"wegovy\",\"zepbound\",\"saxenda\",\"victoza\",\"miebo\"]\n    present = sum(1 for m in meds if m in tl)\n    med_score = present / len(meds)\n\n    # NDC patterns (hyphenated formats like 5-4-2, 4-4-2, etc.) and 11-digit contiguous as fallback\n    ndc_hyph = re.findall(r\"\\b\\d{4,5}-\\d{3,4}-\\d{1,2}\\b\", text)\n    ndc_11 = re.findall(r\"\\b\\d{11}\\b\", text)\n    ndc_count = len(set(ndc_hyph)) + max(0, len(set(ndc_11)) - len(set(ndc_hyph)))\n    ndc_score = min(1.0, ndc_count / 7.0)  # at least 7 NDC-like patterns expected\n\n    score = 0.5 * med_score + 0.5 * ndc_score\n    score = max(0.0, min(1.0, score))\n    return WEIGHT * score"}, {"type": "code", "name": "Header Fields Present (Table Readiness)", "description": "Checks presence of key header terms enabling verification: NDC, Strength, Package Size, Standard SIG/SIG, Formula, Days\u2019 Supply.", "weight": 0.8, "code": "def evaluate(workflow, context):\n    WEIGHT = 0.8\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    if not output.is_document:\n        return 0.0\n    # Extract text robustly\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n    tl = text.lower()\n    # Normalize apostrophes\n    tl = tl.replace(\"\u2019\", \"'\")\n\n    # Required semantic headers (flexible)\n    checks = {\n        'ndc': ['ndc'],\n        'strength': ['strength'],\n        'package': ['package size','pkg size','package','per package'],\n        'sig': ['standard sig','sig'],\n        'formula': ['formula','days\\' supply =','days supply =','calculation'],\n        'days': [\"days' supply\",\"days supply\",\"ds\"]\n    }\n\n    found = 0\n    total = len(checks)\n    for key, variants in checks.items():\n        if any(v in tl for v in variants):\n            found += 1\n    score = found / total\n    return WEIGHT * max(0.0, min(1.0, score))"}, {"type": "code", "name": "SIG Frequency Consistency (Weekly vs Daily)", "description": "Verifies that weekly meds are associated with weekly language and daily meds with daily language near each medication name.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    WEIGHT = 1.0\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    # Get text\n    text = ''\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n    tl = text.lower()\n\n    weekly_meds = ['ozempic','mounjaro','wegovy','zepbound']\n    daily_meds = ['victoza','saxenda','miebo']\n\n    def near_has(term, patterns, window=120):\n        hits = 0\n        positions = [m.start() for m in re.finditer(re.escape(term), tl)]\n        for pos in positions:\n            seg = tl[max(0, pos-window): pos+window]\n            if any(p in seg for p in patterns):\n                hits += 1\n        return hits, len(positions)\n\n    weekly_patterns = ['weekly','once weekly','q7d']\n    daily_patterns = ['daily','once daily','qd','qday','every day','q24h']\n\n    # Compute ratios only over meds that appear at least once\n    wk_found = 0\n    wk_total = 0\n    for m in weekly_meds:\n        h, n = near_has(m, weekly_patterns)\n        if n > 0:\n            wk_total += 1\n            if h > 0:\n                wk_found += 1\n\n    dy_found = 0\n    dy_total = 0\n    for m in daily_meds:\n        h, n = near_has(m, daily_patterns)\n        if n > 0:\n            dy_total += 1\n            if h > 0:\n                dy_found += 1\n\n    # If a subset appears, score by that subset; if none appear, neutral (1.0)\n    wk_ratio = (wk_found / wk_total) if wk_total > 0 else 1.0\n    dy_ratio = (dy_found / dy_total) if dy_total > 0 else 1.0\n\n    score = 0.5 * wk_ratio + 0.5 * dy_ratio\n    return WEIGHT * max(0.0, min(1.0, score))"}, {"type": "code", "name": "Days\u2019 Supply Numeric Plausibility", "description": "Extracts Days\u2019 Supply values and checks they are plausible (reasonable range). Also rewards presence of multiples of 7, which is expected for weekly packages.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    WEIGHT = 0.4\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0\n\n    t = text.replace('\\u2019', \"'\")\n    tl = t.lower()\n\n    # Capture numbers following days' supply or DS labels\n    nums = []\n    for m in re.finditer(r\"(days'?\\s*supply|days\\s*supply|\\bds\\b)[^\\n:=]*[:=]?\\s*(\\d{1,3})\", tl):\n        try:\n            nums.append(int(m.group(2)))\n        except Exception:\n            pass\n\n    if not nums:\n        return 0.0\n\n    in_bounds = [n for n in nums if 3 <= n <= 180]\n    bound_ratio = len(in_bounds)/len(nums) if nums else 0\n\n    multiples7 = [n for n in nums if n % 7 == 0]\n    mult_ratio = 1.0 if len(multiples7) >= 1 else 0.0\n\n    score = 0.6 * bound_ratio + 0.4 * mult_ratio\n    return WEIGHT * max(0.0, min(1.0, score))"}, {"type": "llm_judge", "name": "Row-Level Calculation Consistency (LLM Cross-Check)", "description": "Check that each medication\u2019s row shows a coherent formula and a days\u2019 supply value consistent with the listed SIG and package size (internal consistency, not perfect pharmacological accuracy).", "weight": 0.4, "judge_prompt": "Check the table(s) for internal consistency only.\n\nFor each of these medications: Ozempic, Mounjaro, Wegovy, Zepbound, Saxenda, Victoza, Miebo:\n- Does the row (or mini-table) include a stated standard SIG (e.g., once weekly for GLP-1 pens; once daily for Victoza/Saxenda/Miebo)?\n- Is there a formula that, in plain terms, could yield the stated Days\u2019 Supply per Package (e.g., pens per box \u00d7 7 days for weekly; unit-per-day \u00d7 days)? The arithmetic need not be fully shown, but the logic should align with SIG and package size.\n- Are the resulting Days\u2019 Supply values reasonable given the SIG (e.g., weekly entries typically multiples of 7; daily entries typically 28\u201390 range depending on package)?\n\nScoring:\n- 0.4: All 7 rows show internally coherent SIG\u2013formula\u2013days\u2019 supply alignment.\n- 0.3: 5\u20136 rows coherent; minor inconsistencies.\n- 0.2: 3\u20134 rows coherent or several unclear formulas.\n- 0.1: 1\u20132 rows coherent or most formulas missing/illogical.\n- 0.0: No coherent alignment or tables missing.\n\nDo not check brand-specific NDC correctness; only internal coherence across each row.", "expectation": "Each medication has SIG, formula logic, and a days\u2019 supply value that align (weekly \u2192 multiple of 7; daily \u2192 plausible range)."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Presentation Quality and Usability", "description": "LLM assessment of clarity, brevity, layout, and technician/intern usability for a California retail pharmacy context.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity, Usability, and Professionalism", "description": "Checks visual clarity, brevity, labeling, and technician-friendly design. Rewards simple language, clean table formatting, and presence of quick tips/notes.", "weight": 2.0, "judge_prompt": "Evaluate the document\u2019s usability for pharmacy technicians/interns doing data entry:\n\nConsider:\n- Readability: clear title, headers, consistent column labels, legible font size, minimal clutter.\n- Brevity: stays within 1\u20132 pages or is extremely concise if slightly over.\n- Technician-friendly: simple language, clear examples or notes; obvious location of Days\u2019 Supply per Package.\n- Practicality: includes a small \u201cPrepared by/Version/Date\u201d and any short caution like \u201cVerify plan-specific limits; actual SIG may vary.\u201d\n- Layout quality: alignment, spacing, and table borders are easy to follow when printed.\n\nScoring:\n- 2.0: Highly usable, polished, and concise; excellent table clarity and technician-oriented language.\n- 1.5: Generally clear and usable with minor issues (e.g., small font, cramped spacing).\n- 1.0: Adequate but some confusion (inconsistent headers, clutter, or missing small cues).\n- 0.5: Hard to use; poor formatting or dense language.\n- 0.0: Unprofessional or unusable layout.", "expectation": "A crisp, one-page (or two-page) quick reference with clean tables and simple, actionable phrasing appropriate for pharmacy technicians and interns."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "8c823e32-537c-42b2-84ba-635d63c2853a", "rubric": {"category_name": "UAS Drone Policy Development (Government - Police Supervisory)", "rationale": "This staged rubric enforces a self-documenting, verifiable PDF policy deliverable. Stage 1 (LLM gate) mandates a specific professional document structure aligned to police manual standards and FAA compliance topics. Stage 2 uses code rules to verify correctness signals by scanning the document text for required legal, operational, and governance elements that the structured format enables. Stage 3 applies an LLM quality assessment to judge tone, clarity, enforceability, and readiness for inclusion in the General Manual.", "max_total_score": 12.0, "stages": [{"name": "Stage 1 \u2014 Document Shape and Structure Gate", "description": "LLM-only gate to ensure the output is a professionally formatted policy document with the exact structure needed for verification. If this gate fails, the entire category is zeroed.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 2.8, "rules": [{"type": "llm_judge", "name": "Policy Format and Structural Completeness", "description": "Check that the candidate output is a professionally formatted PDF policy (or DOCX) with required header metadata and sections tailored to UAS operations in law enforcement, enabling subsequent automated verification.", "weight": 4.0, "judge_prompt": "You are verifying the structural FORMAT of a police policy document. Do NOT judge writing quality or correctness\u2014only check that the document is present, professionally formatted, and includes the required sections so it can be verified later.\n\nAcceptable formats: PDF (preferred) or DOCX. Reject plain text or spreadsheets.\n\nMinimum length: at least 3 pages.\n\nHeader block (visible near the beginning, typically the first page): must include all of the following fields with clear labels:\n- Title (e.g., \u201cUnmanned Aircraft Systems (UAS) Operations\u201d or similar)\n- Referenced Files (or References/Authority)\n- Responsible Office (e.g., Policy Development Unit, UAS Program, or Chief\u2019s Office)\n- Related Procedures (or Cross-References)\n\nRequired sections (headers may vary slightly; be flexible with exact wording):\n1) Purpose\n2) Policy (or Policy Statement)\n3) Definitions (e.g., UAS, RPIC, Visual Observer, LAANC, Remote ID, Part 107)\n4) Authorized Users/Personnel and Qualifications (or similar)\n5) Prohibited Uses (must be explicit)\n6) Operational Guidance/Procedures that explicitly includes the four operational use cases as subsections or clearly separated parts:\n   - High-Risk Emergency Deployment involving firearms\n   - Rapid Response / Pre-Staging / Airborne Readiness\n   - Vehicle Pursuit Support\n   - Tactical Team Integration (e.g., SWAT/SRT support)\n7) FAA Compliance / Airspace / Legal Alignment (Part 107 and related)\n8) Training (initial and recurrent)\n\nRecommended but optional sections (count toward completeness if present):\n- Privacy and Civil Liberties / First Amendment\n- Data Management / Retention / Evidence Handling\n- Reporting, Audits, and Accountability\n- Scope and Applicability\n- Appendices (e.g., checklists, forms)\n\nScoring (STRUCTURE ONLY):\n- 4.0: PDF format, 3+ pages, header block includes all four required fields, ALL required sections present (and the four operational use cases are clearly delineated). Professionally formatted.\n- 3.4: DOCX instead of PDF OR header missing one field OR one non-core required section missing (not the operational use cases). The four use-case subsections must still be present.\n- 2.8: Valid PDF/DOCX, 3+ pages, core policy sections present but missing up to three items among Definitions, FAA Compliance, Authorized Users, or Training; the four operational use cases must still be explicitly present. Professional formatting may be basic.\n- 1.6: Valid format but under-structured: fewer than half of the required sections OR the four use-case subsections are not clearly present.\n- 0.0: Not a PDF/DOCX, or fewer than 2 pages, or clearly not a formal policy document.\n\nReturn a score according to this scale. Only evaluate presence and structure\u2014not correctness or prose quality.", "expectation": "A PDF policy document, 3+ pages, with a proper header and all mandated sections including explicit subsections for the four operational use cases."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Compliance Verification (Code + Heuristics)", "description": "Deterministic text checks to verify legal alignment, operational specificity, and governance coverage based on the mandated structure.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Adequate Length and Density", "description": "Checks the document type and length to ensure sufficient policy depth for a General Manual entry.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document output detected.\"\n        # Extract text from PDF or DOCX\n        text = None\n        try:\n            if output.ext.lower().endswith('.pdf'):\n                text = context.files.read_pdf_text(output.id)\n            elif output.ext.lower().endswith('.docx'):\n                text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = None\n        if not text:\n            # As a fallback, try generic text (some systems may store markdown)\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0, \"Unable to read document text.\"\n        words = re.findall(r\"\\b\\w+\\b\", text)\n        wc = len(words)\n        # Smooth scaling up to 800 words\n        ratio = min(wc / 800.0, 1.0)\n        score = ratio * 0.5\n        return score, f\"Word count={wc}, score={score:.2f}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "FAA/Legal Alignment Signals", "description": "Detects presence of FAA Part 107 concepts and related operational constraints (VLOS, night ops, over people, Remote ID, LAANC/airspace authorization, 400 AGL, TFR).", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document output.\"\n        # Read text\n        text = None\n        try:\n            if output.ext.lower().endswith('.pdf'):\n                text = context.files.read_pdf_text(output.id)\n            elif output.ext.lower().endswith('.docx'):\n                text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = None\n        if not text:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0, \"Unable to read document text.\"\n        t = text.lower()\n        has_part107 = ('part 107' in t) or ('14 cfr 107' in t) or ('faa part 107' in t)\n        hits = 0\n        keywords = [\n            'visual line of sight', 'vlos',\n            'night operations', 'night ops', '107.29',\n            'operations over people', 'over people', '107.39',\n            'remote id', 'part 89',\n            'laanc', 'airspace authorization', 'atc authorization',\n            '400 agl', '400 feet',\n            'tfr', 'temporary flight restriction',\n            'yield right-of-way', 'right of way'\n        ]\n        hits = sum(1 for k in keywords if k in t)\n        # Scoring: require Part 107 plus at least 3 supporting hits for full credit\n        base = 0.5 if has_part107 else 0.0\n        extra = min(hits / 3.0, 1.0) * 0.5\n        score = (base + extra) * 1.0\n        # Cap at weight\n        score = min(score, 1.0)\n        return score, f\"part107={has_part107}, hits={hits}, score={score:.2f}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Prohibited Uses Coverage", "description": "Checks explicit prohibitions: weaponization, generalized or bias-based surveillance, First Amendment protections, and warrant/consent requirements for searches with REP.", "weight": 0.9, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document output.\"\n        # Read text\n        text = None\n        try:\n            if output.ext.lower().endswith('.pdf'):\n                text = context.files.read_pdf_text(output.id)\n            elif output.ext.lower().endswith('.docx'):\n                text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = None\n        if not text:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0, \"Unable to read document text.\"\n        t = text.lower()\n        # Weaponization prohibition\n        weap = any(p in t for p in [\n            'no weapons', 'not weapon', 'shall not be weapon', 'weaponized', 'weaponised', 'armed uas', 'arming the uas'\n        ])\n        # Generalized/bias surveillance + First Amendment protection\n        gen_surv = any(p in t for p in [\n            'general surveillance', 'mass surveillance', 'dragnet', 'without reasonable suspicion', 'without probable cause'\n        ])\n        first_amend = any(p in t for p in [\n            'first amendment', 'protected activity', 'constitutionally protected', 'lawful protest', 'political', 'religious', 'union activity'\n        ])\n        # Warrant/consent requirements\n        warrant = any(p in t for p in [\n            'warrant', 'consent', 'exigent', 'court order'\n        ])\n        satisfied = sum([weap, gen_surv and first_amend, warrant])\n        # Scale to weight: 3 checks\n        ratio = satisfied / 3.0\n        score = ratio * 0.9\n        return score, f\"prohibitions_met={satisfied}/3, score={score:.2f}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Operational Use Cases Explicitness", "description": "Verifies the four operational subsections are explicitly addressed: High-Risk Firearms, Rapid Response/Staging, Vehicle Pursuit Support, Tactical Team Integration.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document output.\"\n        # Read text\n        text = None\n        try:\n            if output.ext.lower().endswith('.pdf'):\n                text = context.files.read_pdf_text(output.id)\n            elif output.ext.lower().endswith('.docx'):\n                text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = None\n        if not text:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0, \"Unable to read document text.\"\n        t = text.lower()\n        high_risk = ('high-risk' in t or 'high risk' in t) and any(k in t for k in ['firearm', 'armed suspect', 'shots fired', 'active shooter', 'barricaded'])\n        rapid = any(k in t for k in ['rapid response', 'pre-staged', 'pre staged', 'staged', 'airborne', 'standby launch', 'forward staging'])\n        pursuit = any(k in t for k in ['vehicle pursuit', 'pursuit', 'fleeing vehicle', 'tracking vehicle'])\n        tactical = ('tactical' in t) and any(k in t for k in ['swat', 'srt', 'ert', 'crt', 'cnt', 'team integration'])\n        satisfied = sum([high_risk, rapid, pursuit, tactical])\n        ratio = satisfied / 4.0\n        score = ratio * 1.0\n        return score, f\"use_cases_met={satisfied}/4, score={score:.2f}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Roles, Certification, and Authorization", "description": "Checks for RPIC/VO roles, pilot certification requirements, and mission authorization by supervisory command (IC/Watch Commander).", "weight": 0.7, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document output.\"\n        # Read text\n        text = None\n        try:\n            if output.ext.lower().endswith('.pdf'):\n                text = context.files.read_pdf_text(output.id)\n            elif output.ext.lower().endswith('.docx'):\n                text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = None\n        if not text:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0, \"Unable to read document text.\"\n        t = text.lower()\n        rpic = ('remote pilot in command' in t) or re.search(r'\\brpic\\b', t) is not None\n        vo = ('visual observer' in t) or re.search(r'\\bvo\\b', t) is not None\n        cert = any(k in t for k in ['part 107 certificate', '107 certification', 'faa certificate', 'remote pilot certificate'])\n        auth = any(k in t for k in ['incident commander', 'ic ', 'ic/', 'watch commander', 'duty commander', 'on-duty commander', 'operational approval'])\n        satisfied = sum([rpic and vo, cert, auth])\n        ratio = satisfied / 3.0\n        score = ratio * 0.7\n        return score, f\"roles_cert_auth={satisfied}/3, score={score:.2f}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Data Management, Retention, and Accountability", "description": "Looks for data retention, evidence handling/chain-of-custody, audits/reviews, and public records/transparency references.", "weight": 0.9, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document output.\"\n        # Read text\n        text = None\n        try:\n            if output.ext.lower().endswith('.pdf'):\n                text = context.files.read_pdf_text(output.id)\n            elif output.ext.lower().endswith('.docx'):\n                text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = None\n        if not text:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0, \"Unable to read document text.\"\n        t = text.lower()\n        retention = 'retention' in t or 'retain' in t\n        evidence = any(k in t for k in ['evidence', 'chain of custody', 'digital evidence', 'evidence.com', 'cjis'])\n        audits = any(k in t for k in ['audit', 'review', 'quarterly review', 'annual audit', 'compliance review'])\n        public = any(k in t for k in ['public records', 'foia', 'open records', 'disclosure', 'sunshine'])\n        satisfied = sum([retention, evidence, audits, public])\n        ratio = satisfied / 4.0\n        score = ratio * 0.9\n        return score, f\"data_gov_met={satisfied}/4, score={score:.2f}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Readiness", "description": "LLM quality assessment of tone, clarity, enforceability, and readiness for the General Manual.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Policy Quality, Clarity, and Enforceability", "description": "Judges professional tone, internal consistency, enforceability (shall/must vs may), and readiness for inclusion in the General Manual. Do not re-check structure; focus on quality.", "weight": 3.0, "judge_prompt": "Evaluate the overall professional quality of the provided UAS police policy. Assume the document already passed the structure gate. Judge:\n- Tone and style: formal, directive, consistent with police manual conventions (e.g., shall/must for mandatory requirements; may for discretionary), numbering, and clean formatting.\n- Clarity and enforceability: clear roles and responsibilities, approval thresholds, and unambiguous requirements officers can follow; no contradictions.\n- Legal soundness: references to FAA rules are contextually appropriate; prohibitions and privacy protections are framed in enforceable policy terms.\n- Operational practicality: procedures are actionable for first-line supervisors and field units; integration with dispatch/IC workflows is plausible; reporting/audit processes are feasible.\n- Readiness for inclusion: overall polish, citations, cross-references, and references/authority are adequate for internal review.\n\nScoring:\n- 3.0: Exemplary\u2014clear, enforceable, consistent, and publication-ready.\n- 2.3: Strong\u2014minor edits would suffice.\n- 1.5: Adequate\u2014usable but needs moderate edits.\n- 0.7: Marginal\u2014significant editing required.\n- 0.0: Poor\u2014unclear or not professionally suitable.\n\nReturn a single score according to this scale. Do not penalize for minor stylistic variance if enforceability and clarity are strong.", "expectation": "A clear, directive, legally sound, and professionally formatted policy ready for inclusion in the General Manual."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "05389f78-589a-473c-a4ae-67c61050bfca", "rubric": {"category_name": "Supplier Escalation and Replacement Nomination Dossier", "rationale": "Task Type: Mixed (Pattern C). Two deliverables: (1) a one-page escalation email (DOCX/PDF) and (2) a 2\u20133 page CPO report (DOCX/PDF) with embedded cost/risk analysis in INR. Stage 1 mandates exact document shapes to make verification trivial. Stage 2 mixes code and LLM checks for correctness signals and internal consistency. Stage 3 assesses professional quality and strategic clarity for intended audiences.", "max_total_score": 8.0, "stages": [{"name": "Stage 1 \u2014 Deliverables Shape Gate", "description": "LLM-only gate to enforce exact structural shape of both deliverables so verification is possible. Must confirm presence and format of the escalation email and the CPO report with required sections and tables.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Deliverables Format Requirement", "description": "Check that BOTH deliverables exist in valid document format and follow the mandated structure. This is a shape/structure check only, not content quality or calculation correctness.", "weight": 4.0, "judge_prompt": "You are the Stage 1 shape gate. Review ALL candidate outputs provided. Approve only if the deliverables are in the correct formats and include the required structural elements. Be flexible on exact wording of headers, but strict on presence of sections and tables.\n\nACCEPTABLE SHAPES (any ONE of the following):\nA) Two separate documents: \n   - Escalation Email: a 1-page (single page) DOCX or PDF structured as a professional email.\n   - CPO Report: a 2\u20133 page DOCX or PDF with specified sections and at least one cost comparison table.\nB) One consolidated DOCX/PDF containing clearly separated parts:\n   - Part 1: The escalation email on its own page (or first page), clearly formatted as an email.\n   - Part 2: The 2\u20133 page CPO report starting on a new page with proper headers and required tables.\n\nESCALATION EMAIL \u2014 REQUIRED STRUCTURE:\n- Format: PDF or DOCX, 1 page only (not more than 1 page).\n- Addressed to: Mr. Colin Hartwell (CEO of Juvoxa), Juvoxa\u2019s design head, and Juvoxa\u2019s relationship manager (recipients visible via greeting, addressees, or To/CC list).\n- Must include all of the following elements in the body:\n  1) Statement of ongoing development issues (failed crash tests; repeated failures causing delay).\n  2) Breach of purchase contract and commercial impact.\n  3) Decision: termination of Juvoxa\u2019s nomination for Model A and all future programs.\n  4) Formal request for return of 30% of tooling and development costs paid upfront.\n  5) Tone: firm, professional, acknowledges partnership while noting erosion of confidence.\n- Must include a professional signature block (name, title, company) and a subject line or equivalent.\n\nCPO REPORT \u2014 REQUIRED STRUCTURE:\n- Format: PDF or DOCX, between 2 and 3 pages inclusive.\n- Must include clearly labeled sections (flexible on exact names) covering:\n  1) Executive Summary (or Overview) \u2014 top-level decision context and headline recommendation.\n  2) Background / Supplier Failure Summary \u2014 concise recap of Juvoxa\u2019s failure (crash tests, delays).\n  3) Vendor Options & Assumptions \u2014 identifies Autonexis Lighting (overseas; higher lead time and forex exposure) and Vendrax Components (domestic; shorter lead time; minimal currency risk); references that both are technically capable; cites use of figures from the \u201cModel A HL quotes\u201d.\n  4) Comparative Analysis (Costs & Risks) \u2014 must include at least one visible table in INR labeled or obviously a cost comparison. Table should compare Autonexis vs Vendrax (and optionally Juvoxa baseline) on key metrics (e.g., tooling, unit cost, volumes/total spend, lead time, forex exposure).\n  5) Financial Impact of Transition \u2014 explicit calculation narrative/section using the quotation figures (volumes and pricing), with totals presented in INR.\n  6) Timeline and Risk Considerations \u2014 addresses lead time recovery and forex risk at minimum.\n  7) Final Recommendation \u2014 selects exactly one replacement vendor (Autonexis or Vendrax) with a concise justification (cost, risk, timeline, strategic fit).\n- Must reference the \u201cModel A HL quotes\u201d as the source for pricing/volume figures (in a methodology, assumptions, footnote, or sources section).\n\nSCORING (STRUCTURE ONLY \u2014 do not judge content quality or numerical correctness):\n- 4.0: Correct format(s) + BOTH deliverables present with all required structural elements (email 1 page; report 2\u20133 pages; required sections; at least one INR cost comparison table; clear final recommendation).\n- 3.0: Minor deviations only (e.g., one small missing supporting element like an explicit sources line, or report is marginally off by 0.5 page based on layout) but both deliverables clearly present and structurally complete for verification.\n- 2.0: One deliverable is structurally incomplete (e.g., email missing explicit 30% refund request; report missing the cost comparison table) but the other is acceptable.\n- 1.0: Only one deliverable present OR both present but missing multiple core structural elements.\n- 0.0: Wrong format (not DOCX/PDF), or no acceptable deliverables.\n\nOnly check PRESENCE and STRUCTURE. Do not assess tone quality beyond verifying that the email is firm and professional in manner, and do not verify any calculation correctness.", "expectation": "Two DOCX/PDF deliverables (or one combined) with the specified sections and an INR cost comparison table, enabling downstream verification."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Consistency Checks", "description": "Mixed code + LLM checks for verifiable signals of correctness and internal consistency enabled by Stage 1 shape.", "is_required": false, "max_points": 2.5, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Dual-Deliverable Detection and Section Presence (Code)", "description": "Programmatically verify that an email artifact and a report artifact (or one file containing both) exist, and that key required keywords/sections are present (flexible matching).", "weight": 1.5, "code": "import re\n\ndef safe_read_text(context, res):\n    text = ''\n    # Try DOCX first\n    try:\n        text = context.files.read_docx_text(res.id)\n        if text:\n            return text\n    except Exception:\n        pass\n    # Try PDF\n    try:\n        t = context.files.read_pdf_text(res.id)\n        if t:\n            return t\n    except Exception:\n        pass\n    # Try generic text (unlikely for DOCX/PDF)\n    try:\n        t = context.files.read_text(res.id)\n        if t:\n            return t\n    except Exception:\n        pass\n    return text or ''\n\n\ndef evaluate(workflow, context):\n    docs = [r for r in context.get_all_outputs() if getattr(r, 'is_document', False)]\n    if not docs:\n        return 0.0, 'No document outputs found.'\n\n    email_strength = 0.0\n    report_strength = 0.0\n\n    email_feedback_parts = []\n    report_feedback_parts = []\n\n    for res in docs:\n        text_raw = safe_read_text(context, res)\n        t = (text_raw or '').lower()\n        if not t.strip():\n            continue\n\n        # Email signals\n        email_signals = [\n            bool(re.search(r'\\bdear\\b|\\bto:\\b', t)),\n            ('colin hartwell' in t) or ('mr. colin hartwell' in t) or ('hartwell' in t),\n            ('terminate' in t and 'nomination' in t) or ('termination' in t and 'nomination' in t),\n            ('30%' in t) or ('30 percent' in t) or ('thirty percent' in t),\n            ('tooling' in t and ('development cost' in t or 'development costs' in t)),\n            ('breach' in t and 'contract' in t) or ('breach of contract' in t),\n            ('juvoxa' in t),\n            ('model a' in t)\n        ]\n        email_score_local = sum(email_signals) / 8.0\n        email_strength = max(email_strength, email_score_local)\n        if email_score_local > 0:\n            email_feedback_parts.append(f\"Doc {res.name if hasattr(res,'name') else res.id}: email_signals={sum(email_signals)}/8\")\n\n        # Report signals\n        report_signals = [\n            ('executive summary' in t) or ('overview' in t),\n            ('recommendation' in t) or ('conclusion' in t),\n            ('autonexis' in t),\n            ('vendrax' in t),\n            ('lead time' in t) or ('lead-time' in t),\n            ('forex' in t) or ('foreign exchange' in t) or ('currency risk' in t),\n            ('inr' in t) or ('\u20b9' in t),\n            ('model a' in t),\n            ('financial impact' in t) or ('cost impact' in t) or ('transition impact' in t),\n            ('comparative analysis' in t) or ('comparison' in t) or ('cost comparison' in t)\n        ]\n        report_score_local = sum(report_signals) / 10.0\n        report_strength = max(report_strength, report_score_local)\n        if report_score_local > 0:\n            report_feedback_parts.append(f\"Doc {res.name if hasattr(res,'name') else res.id}: report_signals={sum(report_signals)}/10\")\n\n    # Weighted average: require both artifacts; each contributes half\n    overall_ratio = 0.5 * email_strength + 0.5 * report_strength\n    score = overall_ratio * 1.5\n\n    fb = []\n    if email_feedback_parts:\n        fb.append('Email detection: ' + ' | '.join(email_feedback_parts))\n    if report_feedback_parts:\n        fb.append('Report detection: ' + ' | '.join(report_feedback_parts))\n    if not fb:\n        fb.append('No recognizable email/report signals found in documents.')\n\n    return score, '\\n'.join(fb)"}, {"type": "code", "name": "Financial Analysis Signals and Recommendation Specificity (Code)", "description": "Check for INR currency usage, key financial terms (unit cost, tooling, volume), timeline risk mentions, and that exactly one supplier is unambiguously recommended.", "weight": 1.0, "code": "import re\n\ndef safe_texts(context):\n    texts = []\n    for r in context.get_all_outputs():\n        if not getattr(r, 'is_document', False):\n            continue\n        t = ''\n        try:\n            t = context.files.read_docx_text(r.id)\n        except Exception:\n            try:\n                t = context.files.read_pdf_text(r.id)\n            except Exception:\n                try:\n                    t = context.files.read_text(r.id)\n                except Exception:\n                    t = ''\n        if t:\n            texts.append((getattr(r, 'name', str(r.id)), t))\n    return texts\n\n\ndef evaluate(workflow, context):\n    texts = safe_texts(context)\n    if not texts:\n        return 0.0, 'No document text available.'\n\n    combined = '\\n'.join([t for _, t in texts]).lower()\n\n    # Currency signals (INR/\u20b9)\n    inr_count = len(re.findall(r'\\b(?:inr|\u20b9)\\b', combined))\n    inr_signal = min(inr_count / 3.0, 1.0)  # cap at 1 after 3+ mentions\n\n    # Financial terms\n    terms = [\n        ('unit cost' in combined) or ('per unit' in combined) or ('piece price' in combined),\n        ('tooling' in combined),\n        ('volume' in combined) or ('volumes' in combined) or ('units' in combined),\n        ('total' in combined) or ('grand total' in combined),\n        ('lead time' in combined) or ('lead-time' in combined)\n    ]\n    terms_ratio = sum(terms) / 5.0\n\n    # Recommendation specificity: recommend/nominate exactly one supplier\n    rec_spans = list(re.finditer(r'(recommend|nominate)[^\\n]{0,200}', combined))\n    pick_autonexis = any('autonexis' in m.group(0) for m in rec_spans)\n    pick_vendrax = any('vendrax' in m.group(0) for m in rec_spans)\n    if pick_autonexis ^ pick_vendrax:\n        rec_score = 1.0\n        rec_note = 'Single supplier recommendation detected.'\n    elif pick_autonexis and pick_vendrax:\n        rec_score = 0.4\n        rec_note = 'Ambiguous: both suppliers appear near recommendation.'\n    else:\n        rec_score = 0.0\n        rec_note = 'No clear recommendation phrase linked to a supplier.'\n\n    # Presence of Model A references\n    modela_score = 1.0 if ('model a' in combined) else 0.0\n\n    # Aggregate\n    ratio = (0.35 * inr_signal) + (0.35 * terms_ratio) + (0.2 * rec_score) + (0.1 * modela_score)\n    score = ratio * 1.0\n\n    feedback = f\"INR mentions={inr_count} (signal={inr_signal:.2f}); financial terms={sum(terms)}/5; {rec_note}; Model A ref={bool(modela_score)}\"\n    return score, feedback"}, {"type": "llm_judge", "name": "Cost Analysis Internal Consistency (LLM)", "description": "Judge whether the report\u2019s cost tables and totals are internally consistent and presented in INR, and whether the financial impact narrative aligns with the tabled figures (no contradictions within the document).", "weight": 0.5, "judge_prompt": "Review the document(s) containing the CPO report. Assess INTERNAL CONSISTENCY only (not correctness versus external data):\n- Are costs consistently shown in INR (e.g., INR or \u20b9 labels) across tables/text?\n- Do totals/subtotals in the financial impact section align with the component figures shown (no obvious contradictions on the same page/section)?\n- Does the narrative describing the financial impact match the table values (e.g., the described cheaper vendor corresponds to the lower total in the table)?\n\nScoring:\n- 0.5: Internally consistent, costs clearly labeled in INR, narrative matches tables.\n- 0.3: Mostly consistent with minor ambiguities or one unclear linkage.\n- 0.1: Several inconsistencies or INR labeling missing in places.\n- 0.0: Not consistent or no visible INR cost analysis present.\n\nFocus only on internal consistency of what is visible; do not verify against external sources or compute exact arithmetic.", "expectation": "Visible INR cost table(s) and narrative aligned with the numbers shown; no obvious contradictions."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Decision Rationale", "description": "LLM assessment of tone, professionalism, clarity, and strategic decision quality for intended audiences.", "is_required": false, "max_points": 1.5, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Escalation Email Professionalism and Clarity", "description": "Evaluate the email\u2019s tone, clarity, and managerial appropriateness for a CEO-level escalation while maintaining professionalism and partnership acknowledgement.", "weight": 0.7, "judge_prompt": "Evaluate ONLY the escalation email portion:\n- Tone: Firm, professional, respectful; acknowledges partnership while clearly conveying erosion of confidence.\n- Clarity: Issues, breach, commercial impact, termination decision, and 30% tooling/development cost refund request are clear and unambiguous.\n- Executive-readiness: Subject line, structure, and sign-off appropriate for CEO-level communication.\n\nScoring:\n- 0.7: Excellent tone and clarity; fully executive-ready.\n- 0.5: Solid with minor improvements needed.\n- 0.3: Understandable but significant tone/clarity issues.\n- 0.0: Unprofessional or missing key elements.\n\nDo not re-check structure already validated; focus on communication quality.", "expectation": "A concise, executive-ready email with firm yet respectful tone and crisp articulation of decision and refund request."}, {"type": "llm_judge", "name": "CPO Report Decision Quality and Strategic Fit", "description": "Evaluate the report\u2019s persuasiveness, alignment to procurement strategy, and clarity of the vendor recommendation considering cost, risk, and timeline recovery.", "weight": 0.8, "judge_prompt": "Evaluate ONLY the CPO report portion:\n- Decision quality: Clear, singular recommendation (Autonexis or Vendrax) with rationale tied to cost, risk, and timeline recovery.\n- Strategic alignment: Addresses forex exposure vs. lead time tradeoffs and aligns with Banyan Crest\u2019s procurement objectives.\n- Presentation: Logical flow, coherent argument, and appropriate use of tables/visuals to support the case.\n\nScoring:\n- 0.8: Highly persuasive, strategically grounded, and clearly argued.\n- 0.6: Persuasive with minor gaps or weaker linkage to one factor (cost/risk/timeline).\n- 0.4: Mixed or generic rationale; limited strategic linkage.\n- 0.0: No clear recommendation or rationale.\n\nDo not verify numbers; judge clarity and strategic reasoning.", "expectation": "A specific, well-justified recommendation that balances INR cost implications, lead-time recovery, and forex risk, presented in a clear, executive-ready format."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "6974adea-8326-43fa-8187-2724b15d9546", "rubric": {"category_name": "Technology Feature Article \u2013 RTLS Livestock Tracking (Journalism)", "rationale": "Pattern B (Document task). Stage 1 uses a strict LLM gate to enforce file type and visible structure so verification becomes trivial. Stage 2 mixes code checks (word count range, entity/quotes presence, UK spelling consistency, and technical coverage signals) with an LLM check for technical accuracy and impartial tone. Stage 3 is an LLM quality assessment for narrative strength, accessibility for a mainstream audience, and depth for expert readers.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2013 Format and Structural Gate (LLM ONLY)", "description": "Gate that enforces the exact required output shape for a professional feature article submitted as a Word document, enabling subsequent verification.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.5, "rules": [{"type": "llm_judge", "name": "Document Format and Structure Requirements", "description": "Verify the candidate produced a properly structured feature article as a Word document with all required elements present.", "weight": 6.0, "judge_prompt": "You are the Stage 1 STRUCTURE gate. Only check FORMAT and PRESENCE of required elements, not content quality. Evaluate the primary output the agent submitted.\n\nFormat Requirements (hard requirements):\n- Must be a Word document (DOCX). PDFs or plain text do NOT pass.\n- Length between 1,000 and 1,500 words (inclusive).\n- Professionally formatted with clear sections aimed at an international audience (UK-based outlet).\n\nStructural Requirements (be flexible with exact header wording):\n1) Headline: SEO-aware headline that clearly references the topic (livestock/cattle + RTLS/real-time tracking/tags/collars). It should be the document title or top line.\n2) Standfirst (deck/summary): 1\u20133 sentences directly under the headline summarising the piece.\n3) Subheadings: At least three descriptive subheadings that break up the article body.\n4) Interviewee voices: At least two direct quotes (quotation marks visible) attributed to named interviewees. Preferably from the provided list: Jim Dalton (FarmEx), Gaspar Olafsen (farmer), Anne Smith (Useful Technologies), Lars Andersen (Fair Farm Technologies). Names should be visible near the quotes.\n5) Company and product mentions: Mentions of both Fair Farm Technologies and Useful Technologies, and the device CattleWatch.\n6) Neutral news style: Impartial, non-opinionated tone (no overt first-person commentary).\n7) UK English: Use of UK spelling conventions is visible (e.g., colour/organise/centre rather than US variants), while being tolerant of proper nouns.\n\nScoring Guide (STRUCTURE ONLY):\n- 6.0: DOCX + word count in range + ALL structural requirements (1\u20137) clearly present.\n- 5.0: DOCX + word count in range + missing exactly one minor element (e.g., one company mention, or only two subheadings) but overall structure is clearly in place.\n- 4.5: DOCX + word count in range + minor omissions (up to two) yet article is recognisably complete (headline, standfirst, \u22652 subheadings, \u22652 quotes attributed).\n- 3.0: DOCX but word count or multiple structural elements missing (e.g., no standfirst, only one subheading, or lacks interviewee attribution).\n- 1.0: Wrong or unclear format (not DOCX) OR article is far below structural expectations (e.g., no headline/standfirst/subheadings/quotes).\n- 0.0: Not a Word document at all, or clearly not an article.\n\nImportant: Do NOT judge factual accuracy, calculations, or narrative quality here\u2014only whether the required structure and format exist to enable verification.", "expectation": "A DOCX feature article, 1,000\u20131,500 words, with SEO headline, standfirst, \u22653 subheadings, attributed quotes from interviewees, mentions of both companies and the device, neutral tone, and UK English spelling visible."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification (Mixed: Code + LLM)", "description": "Now that the structure exists, verify factual signals, constraints, and consistency. Use code for deterministic checks and an LLM for impartial tone and RTLS basics.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Word Count Within Range", "description": "Check the article length is between 1,000 and 1,500 words (inclusive), with soft partial credit near the boundary.", "weight": 2.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output.\"\n    if not output.is_document:\n        return 0.0, \"Primary output is not a document.\"\n    text = None\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0, \"Unable to read document text.\"\n    words = re.findall(r\"\\b\\w[\\w'-]*\\b\", text)\n    n = len(words)\n    if 1000 <= n <= 1500:\n        return 1.0, f\"Word count OK: {n}.\"\n    elif (900 <= n < 1000) or (1500 < n <= 1600):\n        return 0.5, f\"Word count near boundary: {n}.\"\n    else:\n        return 0.0, f\"Word count out of range: {n}.\""}, {"type": "code", "name": "Entities and Device Presence", "description": "Check mentions of both companies and the device name.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"Output missing or not a document.\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0, \"Unable to read document text.\"\n    low = text.lower()\n    targets = [\n        (\"fair farm technologies\", \"Fair Farm Technologies\"),\n        (\"useful technologies\", \"Useful Technologies\"),\n        (\"cattlewatch\", \"CattleWatch\"),\n    ]\n    present = []\n    for key, label in targets:\n        present.append(1 if key in low else 0)\n    score = sum(present) / 3.0\n    missing = [label for (key, label), p in zip(targets, present) if p == 0]\n    fb = \"All entities present.\" if not missing else (\"Missing: \" + \", \".join(missing))\n    return score, fb"}, {"type": "code", "name": "Interviewee Quotations Presence", "description": "Check for at least two direct quotes and attribution to at least three named interviewees.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"Output missing or not a document.\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0, \"Unable to read document text.\"\n    # Count approximate pairs of quotes (straight or curly)\n    dq = len(re.findall(r'[\\\"\\u201c\\u201d]', text))\n    quote_pairs = dq // 2\n    names = [\n        \"Jim Dalton\",\n        \"Gaspar Olafsen\",\n        \"Anne Smith\",\n        \"Lars Andersen\",\n    ]\n    low = text.lower()\n    names_present = set()\n    for n in names:\n        if n.lower() in low:\n            names_present.add(n)\n    # Scoring: 0.5 for >=2 quote pairs; 0.5 for >=3 distinct interviewee names (0.25 if exactly 2)\n    score = 0.0\n    if quote_pairs >= 2:\n        score += 0.5\n    if len(names_present) >= 3:\n        score += 0.5\n    elif len(names_present) == 2:\n        score += 0.25\n    fb = f\"Quote pairs: {quote_pairs}; Interviewees mentioned: {len(names_present)} ({', '.join(sorted(names_present))}).\"\n    return score, fb"}, {"type": "code", "name": "UK vs US Spelling Consistency", "description": "Reward consistent UK English spelling over US variants (tolerate proper nouns).", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"Output missing or not a document.\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0, \"Unable to read document text.\"\n    pairs = [\n        (r\"colour\", r\"color\"),\n        (r\"favour\", r\"favor\"),\n        (r\"organisation\", r\"organization\"),\n        (r\"organise\", r\"organize\"),\n        (r\"centre\", r\"center\"),\n        (r\"analys(e|ed|es|ing)\", r\"analyz(e|ed|es|ing)\"),\n        (r\"defence\", r\"defense\"),\n        (r\"licen[cs]e\", r\"license\"),\n        (r\"travelling\", r\"traveling\"),\n        (r\"programme\", r\"program\"),\n        (r\"catalogue\", r\"catalog\"),\n        (r\"metre\", r\"meter\"),\n        (r\"grey\", r\"gray\"),\n    ]\n    low = text.lower()\n    uk = 0\n    us = 0\n    for uk_pat, us_pat in pairs:\n        uk += len(re.findall(r\"\\b\" + uk_pat + r\"\\b\", low))\n        us += len(re.findall(r\"\\b\" + us_pat + r\"\\b\", low))\n    total = uk + us\n    if total == 0:\n        # No signals found; neutral partial credit\n        return 0.6, \"No clear UK/US spelling signals detected.\"\n    # Score proportional to UK dominance\n    ratio = (uk + 1) / (uk + us + 1)\n    score = min(1.0, max(0.0, ratio))\n    fb = f\"UK spellings: {uk}; US spellings: {us}; score ratio: {ratio:.2f}.\"\n    return score, fb"}, {"type": "code", "name": "RTLS Technical Coverage Signals", "description": "Check coverage breadth via presence of technical terms relevant to RTLS livestock tracking.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"Output missing or not a document.\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0, \"Unable to read document text.\"\n    low = text.lower()\n    terms = [\n        \"rtls\",\n        \"real-time location\",\n        \"real time location\",\n        \"bluetooth\",\n        \"ble\",\n        \"wi-fi\",\n        \"wifi\",\n        \"rfid\",\n        \"uwb\",\n        \"ultra-wideband\",\n        \"triangulation\",\n        \"trilateration\",\n        \"beacon\",\n        \"accelerometer\",\n        \"collar\",\n        \"ear tag\",\n        \"gateway\",\n        \"base station\",\n        \"battery\",\n        \"latency\",\n        \"range\",\n        \"line of sight\",\n    ]\n    found = set()\n    for t in terms:\n        if t in low:\n            found.add(t)\n    count = len(found)\n    # Expect at least ~6 distinct technical signals for full credit\n    score = min(1.0, count / 6.0)\n    fb = f\"Unique technical terms found: {count} ({', '.join(sorted(found))}).\"\n    return score, fb"}, {"type": "llm_judge", "name": "Impartial Tone and RTLS Basics Correctness", "description": "LLM check: RTLS explained accurately at a high level, claims attributed to sources, and tone is impartial/non-promotional.", "weight": 2.0, "judge_prompt": "Review the article for correctness of basics and tone. Do NOT judge structure (Stage 1 already did). Assess:\n1) RTLS basics accurately described (what it is, common signals like Wi-Fi/Bluetooth/RFID, real-time indoor/contained tracking). Minor omissions OK.\n2) Claims about benefits/impact are attributed (e.g., to interviewees or companies) rather than asserted as the author\u2019s opinion.\n3) Tone is impartial, avoiding hype words (e.g., revolutionary/game-changing) unless in attributed quotes.\n\nScoring (0\u20132):\n- 2.0: RTLS basics accurate; claims well-attributed; consistently impartial.\n- 1.0: Generally correct but with a minor inaccuracy OR occasional un-attributed generalisation/promotional phrasing.\n- 0.0: Significant inaccuracies about RTLS OR pervasive un-attributed claims/promotional tone.", "expectation": "Technically sound, attributed claims, neutral news style."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Quality and Audience Fit (LLM)", "description": "Holistic assessment of readability, depth, flow, and packaging for a global audience (UK-based outlet).", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Quality and Audience Appropriateness", "description": "Evaluate narrative quality, clarity, and depth for experts while remaining accessible to a mainstream audience.", "weight": 4.0, "judge_prompt": "Evaluate the overall quality of the article:\n- Clarity for mainstream readers: clear explanations, helpful context (e.g., why dairy/livestock sector cares, cost/benefit pressures), avoids jargon or explains it.\n- Depth for expert readers: includes meaningful technical/market detail (e.g., signals used, deployment considerations, data/health insights, limitations like battery life, connectivity, costs, privacy/animal welfare).\n- Narrative and structure: logical flow, effective subheadings, strong standfirst, good close. Quotes are woven in to tell the story.\n- Packaging and SEO: headline likely to rank for relevant queries (livestock/cattle + RTLS/real-time tracking/tags/collars), standfirst sets expectations, subheads guide scanning.\n- Style polish: UK English, consistent style, clean grammar, suitable for a respected impartial UK-based outlet with an international audience.\n\nScoring (0\u20134):\n- 4.0: Engaging, well-structured, accessible yet deep, excellent packaging, highly polished.\n- 3.0: Strong overall with minor rough edges (e.g., one weak subhead or slightly thin technical context).\n- 2.0: Adequate but uneven; readable with noticeable gaps in depth, flow, or packaging.\n- 1.0: Weak structure or clarity; superficial or confusing; packaging ineffective.\n- 0.0: Poorly written, confusing, or inappropriate for the outlet.\n", "expectation": "An engaging, impartial feature with strong quotes, clear explanations, and solid technical and market context, packaged with SEO-aware headline/standfirst/subheads."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "d7cfae6f-4a82-4289-955e-c799dfe1e0f4", "rubric": {"category_name": "Wholesale Trade \u2014 Sales Reps (Beutist Sets Recap)", "rationale": "Analytical task producing a structured Excel recap used for planning. Stage 1 uses an LLM gate to strictly enforce a verifiable Excel shape (sheets, sections, tables). Stage 2 uses code rules to verify correctness and internal consistency given the enforced shape (columns present, calculations recompute, totals, shipment month coverage). Stage 3 uses an LLM to assess professional quality and strategic usefulness for the national accounts audience.", "max_total_score": 20.0, "stages": [{"name": "Structured Excel Recap Gate", "description": "Stage 1 GATE \u2014 Verify the candidate output is a properly structured Excel recap enabling deterministic verification. LLM judge only.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.5, "rules": [{"type": "llm_judge", "name": "Excel Structure and Sections Presence", "description": "Output must be an Excel workbook with required sheets, sections, and tables to support verifiable analysis.", "weight": 6.0, "judge_prompt": "You are checking ONLY the presence and structure of the output. Do not judge calculation correctness or content quality.\n\nVerify the candidate output is an Excel workbook (.xlsx) that includes the following STRUCTURE. Be flexible with similar sheet/section names, but the required elements must be clearly present and readable in rendered Excel images.\n\nRequired Sheets and Structures:\n\n1) Main recap sheet: name like \"Recap\", \"Beutist Sets Recap\", \"Summary\", or similar.\n   Must contain a single clear tabular recap broken out by Axis (Skincare, Makeup, Fragrance) and Brand. The table must include columns equivalent to:\n   - Axis\n   - Brand\n   - YTD sales this year through 2023-09-22 (label may say \"YTD TY\", \"YTD 2023\", or similar)\n   - YTD sales last year through 2022-09-21 (label may say \"YTD LY\", \"YTD 2022\")\n   - Percent change vs last year (label may say \"% Change\", \"YoY %\", or similar)\n   - Total expected sales from now through end of Q1 (accept either Q1 2023 or Q1 2024 as a label, but the projection window must be stated somewhere in the file). Labels may include \"Expected Sales\", \"Forecast thru Q1\", etc.\n   - Inventory On-Hand (OH)\n   - Inventory On-Order (OO) including expected shipments in October 2023 and Q1 2024\n   - Total Supply (OH + OO) or equivalent\n   - Comparison vs expected sales expressed as a dollar difference and as a percentage (labels like \"$ Diff vs Exp\" and \"% vs Exp\" or \"Coverage %\")\n   - Comments placeholder column present and blank (cells empty)\n   Also required: subtotal rows by Axis and a Grand Total row across all Axes/Brands.\n\n2) Assumptions/Method sheet: name like \"Assumptions & Method\", \"Methodology\", or similar.\n   Must list, in text or a small table, the key assumptions:\n   - Dates used: YTD TY end date (2023-09-22), YTD LY end date (2022-09-21)\n   - Projection window (start at or after 2023-09-25; end at Q1 end \u2014 accept Q1 2023 or Q1 2024 if explicitly stated)\n   - Definition of what OH and OO include, and that OO includes expected Oct 2023 and Q1 2024 shipments\n   - Currency/units conventions\n   - Data source(s)\n\n3) Expected Shipments Detail sheet: name like \"Expected Shipments\", \"Inbound/Shipments Detail\", or similar.\n   Must show a table of expected shipments by month and at least by Axis or Brand. It must visibly include rows for October 2023 and at least one month in Q1 2024 (Jan/Feb/Mar 2024).\n\n4) Projection Basis sheet: name like \"Projection Basis\" or \"Historical Reference\".\n   Must show a table summarizing historical set sales from Q3 2022 through Q1 2023, broken out at least by Axis or Brand, indicating this is the basis for the projection.\n\nAdditional Requirements:\n- The recap must be clearly scoped to the Beutist product line sets.\n- Totals by Axis and a Grand Total must be present on the main recap.\n- The Comments column must exist and be blank (placeholders only).\n\nScoring:\n- 1.0: All required sheets present with the listed structures and fields visibly present; main recap has Axis/Brand breakdown, required columns, subtotals by Axis, and a Grand Total; Comments column exists and is blank.\n- 0.8: Minor omissions limited to secondary descriptive text on Assumptions or minor label variations; core tables and fields still present and clear.\n- 0.6: Missing exactly one required sheet OR the recap table lacks one required field (e.g., missing % vs expected or missing Total Supply), but most structure is present.\n- 0.3: Significantly incomplete structure (e.g., recap table present but missing multiple required fields; or only some supporting sheets exist).\n- 0.0: Not an Excel file OR main recap/structure largely missing.\n\nReturn a score in [0, 1] scaled by weight. Only evaluate presence/structure, not correctness or presentation quality.", "expectation": "A well-structured .xlsx with four sheets: a main recap table (Axis/Brand with YTD TY/LY, % change, expected sales thru Q1, OH, OO, Total Supply, $ and % vs expected, Comments blank, subtotals and grand total), an Assumptions & Method sheet (dates, definitions, projection window, currency/units, data sources), an Expected Shipments Detail sheet (including Oct 2023 and at least one month in Q1 2024), and a Projection Basis sheet (Q3'22\u2013Q1'23 sets history)."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Correctness and Consistency Verification", "description": "Stage 2 \u2014 Programmatic checks for internal consistency, calculations, dates, totals, and shipment coverage. Flexible matching and robust error handling.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Core Columns and Numeric Sanity", "description": "Find the main recap sheet. Verify presence of key columns, numeric types, and basic internal relationships (OH/OO/Expected/Supply fields present and numeric).", "weight": 4.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        out = context.get_primary_output()\n        if not out or not out.is_spreadsheet:\n            return 0.0, \"No spreadsheet output\"\n        # Identify recap-like sheet\n        path = context.files.get_path(out.id)\n        try:\n            xls = pd.ExcelFile(path)\n            sheet_names = [str(s) for s in xls.sheet_names]\n        except Exception as e:\n            return 0.0, f\"Failed to open Excel: {e}\"\n\n        def is_recap_name(name):\n            n = str(name).lower()\n            return any(k in n for k in [\"recap\",\"summary\",\"overview\",\"beutist\",\"sets recap\"]) \\\n                   and not any(k in n for k in [\"assumption\",\"method\",\"shipment\",\"basis\",\"reference\"]) \\\n                   or n.strip()==sheet_names[0].lower()\n\n        candidate_sheets = [s for s in sheet_names if is_recap_name(s)] or sheet_names\n        recap_df = None\n        recap_sheet = None\n        for s in candidate_sheets:\n            try:\n                df = pd.read_excel(path, sheet_name=s)\n                # Flatten possible multiindex columns\n                if isinstance(df.columns, pd.MultiIndex):\n                    df.columns = [' '.join([str(x) for x in tup if str(x)!='nan']).strip() for tup in df.columns]\n                df.columns = [str(c).strip() for c in df.columns]\n                cols_l = [c.lower() for c in df.columns]\n                if any('axis' in c for c in cols_l) and any('brand' in c for c in cols_l):\n                    recap_df = df\n                    recap_sheet = s\n                    break\n            except Exception:\n                continue\n        if recap_df is None:\n            return 0.8, \"Could not locate recap sheet with Axis and Brand columns\"\n\n        cols_l = [c.lower() for c in recap_df.columns]\n        def find_col(patterns):\n            for i,c in enumerate(cols_l):\n                for p in patterns:\n                    if re.search(p, c):\n                        return recap_df.columns[i]\n            return None\n\n        axis_col = find_col([r'\\baxis\\b'])\n        brand_col = find_col([r'\\bbrand\\b'])\n        ytd_ty_col = find_col([r'ytd.*(ty|2023|this|current)', r'(2023).*ytd'])\n        ytd_ly_col = find_col([r'ytd.*(ly|2022|last)', r'(2022).*ytd'])\n        pct_change_col = find_col([r'(percent|%|pct).*change', r'yoy', r'%.*vs'])\n        exp_sales_col = find_col([r'(expected|forecast|proj).*q1', r'(expected|forecast|proj).*(thru|through)', r'now.*q1'])\n        oh_col = find_col([r'\\boh\\b', r'on[- ]hand'])\n        oo_col = find_col([r'\\boo\\b', r'on[- ]order'])\n        supply_col = find_col([r'total.*supply', r'oh.*\\+.*oo', r'available'])\n        diff_col = find_col([r'(\\$|usd|dollar).*diff', r'diff.*expected', r'over(short)'])\n        pct_vs_col = find_col([r'(percent|%|pct).*(vs|coverage|of).*expected', r'coverage'])\n        comments_col = find_col([r'comment'])\n\n        required_cols = [axis_col, brand_col, ytd_ty_col, ytd_ly_col, pct_change_col, exp_sales_col, oh_col, oo_col]\n        present_count = sum([c is not None for c in required_cols])\n        score = 0.0\n        feedback = []\n\n        # Presence scoring\n        # 2.0 points for having at least 6/8 required columns, up to 2.5 for all 8.\n        if present_count >= 6:\n            score += 2.0 + 0.5 * ((present_count - 6) / 2)\n        else:\n            score += max(0.0, (present_count / 6.0) * 2.0)\n        if present_count < 8:\n            feedback.append(f\"Missing some required columns ({present_count}/8 present)\")\n\n        # Numeric sanity for key measures: expected sales, OH, OO should be numeric for majority rows\n        def numeric_share(series):\n            try:\n                s = pd.to_numeric(series, errors='coerce')\n                return (s.notna().sum()) / max(1, len(s))\n            except Exception:\n                return 0.0\n\n        numeric_ok = 0\n        total_checks = 0\n        for c in [exp_sales_col, oh_col, oo_col]:\n            if c is not None and c in recap_df.columns:\n                total_checks += 1\n                if numeric_share(recap_df[c]) >= 0.7:\n                    numeric_ok += 1\n        if total_checks > 0:\n            score += 0.8 * (numeric_ok / total_checks)\n        else:\n            feedback.append(\"Could not verify numeric fields (Expected/OH/OO)\")\n\n        # Supply-related columns presence bonus\n        supply_present = (supply_col is not None) and (diff_col is not None) and (pct_vs_col is not None)\n        score += 0.7 if supply_present else 0.2 if (supply_col or diff_col or pct_vs_col) else 0.0\n        if not supply_present:\n            feedback.append(\"One or more supply comparison columns (Total Supply, $ Diff, % vs Exp) missing\")\n\n        # Comments column presence\n        score += 0.5 if comments_col is not None else 0.0\n        if comments_col is None:\n            feedback.append(\"Comments column not found\")\n\n        # Cap score at weight\n        score = min(score, 4.0)\n        return score, \"; \".join(feedback) if feedback else \"OK\"\n    except Exception as e:\n        return 0.0, f\"Rule error: {e}\""}, {"type": "code", "name": "YTD Percent Change Recalculation", "description": "Recompute YoY % change = (YTD TY - YTD LY) / YTD LY for most rows; compare to provided % column within tolerance.", "weight": 2.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        out = context.get_primary_output()\n        if not out or not out.is_spreadsheet:\n            return 0.0\n        path = context.files.get_path(out.id)\n        xls = pd.ExcelFile(path)\n        # Find recap sheet\n        recap_df = None\n        for s in xls.sheet_names:\n            try:\n                df = pd.read_excel(path, sheet_name=s)\n                if isinstance(df.columns, pd.MultiIndex):\n                    df.columns = [' '.join([str(x) for x in tup if str(x)!='nan']).strip() for tup in df.columns]\n                df.columns = [str(c).strip() for c in df.columns]\n                cols_l = [c.lower() for c in df.columns]\n                if any('axis' in c for c in cols_l) and any('brand' in c for c in cols_l):\n                    recap_df = df\n                    break\n            except Exception:\n                continue\n        if recap_df is None:\n            return 0.0\n        cols_l = [c.lower() for c in recap_df.columns]\n        def find_col(patterns):\n            for i,c in enumerate(cols_l):\n                for p in patterns:\n                    if re.search(p, c):\n                        return recap_df.columns[i]\n            return None\n        ytd_ty = find_col([r'ytd.*(ty|2023|this|current)', r'(2023).*ytd'])\n        ytd_ly = find_col([r'ytd.*(ly|2022|last)', r'(2022).*ytd'])\n        pct_col = find_col([r'(percent|%|pct).*change', r'yoy', r'%.*vs'])\n        if not (ytd_ty and ytd_ly and pct_col):\n            return 0.2  # minimal credit if columns cannot be found\n        ty = pd.to_numeric(recap_df[ytd_ty], errors='coerce')\n        ly = pd.to_numeric(recap_df[ytd_ly], errors='coerce')\n        provided = pd.to_numeric(recap_df[pct_col].astype(str).str.replace('%','', regex=False), errors='coerce')/100.0\n        with np.errstate(divide='ignore', invalid='ignore'):\n            calc = (ty - ly) / ly\n        # Compare where ly != 0 and both sides present\n        mask = ly.ne(0) & calc.notna() & provided.notna()\n        if mask.sum() == 0:\n            return 0.4  # partial if no comparable rows\n        diff = (calc[mask] - provided[mask]).abs()\n        within_tol = (diff <= 0.02).mean()  # within 2 percentage points\n        # Scale: >=80% rows within tol -> full; else proportionally\n        score = 2.0 * max(0.0, min(1.0, (within_tol - 0.3) / 0.5)) if mask.sum()>=5 else 2.0 * within_tol\n        return max(0.0, min(2.0, float(score)))\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Axis Subtotals and Grand Total Checks", "description": "Verify subtotal rows by Axis and a Grand Total row exist, and that totals approximately match the sum of detail rows.", "weight": 2.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        out = context.get_primary_output()\n        if not out or not out.is_spreadsheet:\n            return 0.0, \"No spreadsheet\"\n        path = context.files.get_path(out.id)\n        xls = pd.ExcelFile(path)\n        recap_df = None\n        for s in xls.sheet_names:\n            try:\n                df = pd.read_excel(path, sheet_name=s)\n                if isinstance(df.columns, pd.MultiIndex):\n                    df.columns = [' '.join([str(x) for x in tup if str(x)!='nan']).strip() for tup in df.columns]\n                df.columns = [str(c).strip() for c in df.columns]\n                cols_l = [c.lower() for c in df.columns]\n                if any('axis' in c for c in cols_l) and any('brand' in c for c in cols_l):\n                    recap_df = df\n                    break\n            except Exception:\n                continue\n        if recap_df is None:\n            return 0.0, \"Recap sheet not found\"\n        cols_l = [c.lower() for c in recap_df.columns]\n        def find_col(patterns):\n            for i,c in enumerate(cols_l):\n                for p in patterns:\n                    if re.search(p, c):\n                        return recap_df.columns[i]\n            return None\n        axis_col = find_col([r'\\baxis\\b'])\n        brand_col = find_col([r'\\bbrand\\b'])\n        exp_col = find_col([r'(expected|forecast|proj)'])\n        oh_col = find_col([r'\\boh\\b', r'on[- ]hand'])\n        oo_col = find_col([r'\\boo\\b', r'on[- ]order'])\n        if not (axis_col and brand_col and exp_col):\n            return 0.3, \"Missing key columns for totals\"\n        # Identify subtotal rows heuristically: brand contains 'total' or is NA/blank\n        brand_vals = recap_df[brand_col].astype(str).str.strip().str.lower()\n        axis_vals = recap_df[axis_col].astype(str).str.strip().str.lower()\n        is_total_brand = brand_vals.str.contains('total') | brand_vals.isin(['', 'nan', 'none'])\n        # Grand total row: either axis or brand contains 'grand total'\n        has_grand = (axis_vals.str.contains('grand total') | brand_vals.str.contains('grand total')).any() or brand_vals.eq('total').any()\n        # Axis subtotals: for each axis, a row flagged as total\n        axes = [a for a in axis_vals.unique() if a and a not in ['nan','none','grand total','total']]\n        subtotal_ok = 0\n        for a in axes:\n            mask = axis_vals.eq(a) & is_total_brand\n            if mask.any():\n                subtotal_ok += 1\n        subtotal_share = (subtotal_ok / max(1,len(axes))) if len(axes)>0 else 0\n        score = 0.0\n        # Subtotals worth 1.2 points\n        score += 1.2 * subtotal_share\n        # Grand total worth 0.8 points\n        score += 0.8 if has_grand else 0.0\n        # Basic arithmetic check: sum(OH+OO) approx sum Total Supply or sum Expected columns if present\n        try:\n            oh = pd.to_numeric(recap_df[oh_col], errors='coerce') if oh_col else None\n            oo = pd.to_numeric(recap_df[oo_col], errors='coerce') if oo_col else None\n            expected = pd.to_numeric(recap_df[exp_col], errors='coerce')\n            if oh is not None and oo is not None:\n                calc_supply = (oh.fillna(0) + oo.fillna(0)).sum()\n                # compare to sum of any explicit supply column if exists\n                supply_col = None\n                for c in recap_df.columns:\n                    cl = str(c).lower()\n                    if ('total' in cl and 'supply' in cl) or ('oh' in cl and 'oo' in cl):\n                        supply_col = c\n                        break\n                if supply_col:\n                    supply_sum = pd.to_numeric(recap_df[supply_col], errors='coerce').sum()\n                    if np.isfinite(calc_supply) and np.isfinite(supply_sum) and supply_sum!=0:\n                        rel_err = abs(calc_supply - supply_sum)/max(1.0, abs(supply_sum))\n                        score += 0.5 if rel_err <= 0.05 else 0.2 if rel_err <= 0.15 else 0.0\n        except Exception:\n            pass\n        return min(score, 2.0), \"Subtotals checked\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Comments Placeholder Blank", "description": "Ensure Comments column exists and is blank/empty for all rows (placeholders only).", "weight": 0.5, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    try:\n        out = context.get_primary_output()\n        if not out or not out.is_spreadsheet:\n            return 0.0\n        path = context.files.get_path(out.id)\n        xls = pd.ExcelFile(path)\n        recap_df = None\n        for s in xls.sheet_names:\n            try:\n                df = pd.read_excel(path, sheet_name=s)\n                if isinstance(df.columns, pd.MultiIndex):\n                    df.columns = [' '.join([str(x) for x in tup if str(x)!='nan']).strip() for tup in df.columns]\n                df.columns = [str(c).strip() for c in df.columns]\n                if any('axis' in str(c).lower() for c in df.columns) and any('brand' in str(c).lower() for c in df.columns):\n                    recap_df = df\n                    break\n            except Exception:\n                continue\n        if recap_df is None:\n            return 0.0\n        # find comments column\n        comments_col = None\n        for c in recap_df.columns:\n            if re.search(r'comment', str(c).lower()):\n                comments_col = c\n                break\n        if comments_col is None:\n            return 0.0\n        vals = recap_df[comments_col]\n        # consider blank if NaN or only whitespace\n        blank_share = vals.apply(lambda x: (pd.isna(x)) or (str(x).strip()==\"\" )).mean()\n        if blank_share >= 0.95:\n            return 0.5\n        elif blank_share >= 0.75:\n            return 0.3\n        else:\n            return 0.0\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Expected Shipments Month Coverage", "description": "Check Expected Shipments Detail sheet includes October 2023 and at least one month in Q1 2024.", "weight": 1.5, "code": "import re\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\ndef evaluate(workflow, context):\n    try:\n        out = context.get_primary_output()\n        if not out or not out.is_spreadsheet:\n            return 0.0, \"No spreadsheet\"\n        path = context.files.get_path(out.id)\n        try:\n            xls = pd.ExcelFile(path)\n        except Exception as e:\n            return 0.0, f\"Open error: {e}\"\n        # find shipments sheet by name\n        ship_sheet = None\n        for s in xls.sheet_names:\n            n = str(s).lower()\n            if any(k in n for k in ['ship', 'inbound', 'expected', 'po', 'on-order', 'on order']):\n                ship_sheet = s\n                break\n        if not ship_sheet:\n            return 0.0, \"Shipments sheet not found\"\n        try:\n            df = pd.read_excel(path, sheet_name=ship_sheet)\n        except Exception as e:\n            return 0.0, f\"Read error: {e}\"\n        if isinstance(df.columns, pd.MultiIndex):\n            df.columns = [' '.join([str(x) for x in tup if str(x)!='nan']).strip() for tup in df.columns]\n        df.columns = [str(c).strip() for c in df.columns]\n        # find date/month column\n        date_col = None\n        for c in df.columns:\n            cl = str(c).lower()\n            if any(k in cl for k in ['month','date','period']):\n                date_col = c\n                break\n        if date_col is None:\n            # try to infer from first column if it looks like dates\n            date_col = df.columns[0]\n        # parse into datetime (coerce)\n        ser = df[date_col].astype(str).str.replace('\\n',' ', regex=False)\n        # Try to extract month-year like 'Oct 2023', '2023-10', etc.\n        def parse_any(x):\n            x = x.strip()\n            for fmt in ['%b %Y','%B %Y','%Y-%m','%m/%Y','%Y/%m','%Y-%m-%d','%m/%d/%Y']:\n                try:\n                    return pd.to_datetime(x, format=fmt)\n                except Exception:\n                    continue\n            try:\n                return pd.to_datetime(x, errors='coerce')\n            except Exception:\n                return pd.NaT\n        dates = ser.apply(parse_any)\n        months = dates.dt.to_period('M')\n        target_oct = pd.Period('2023-10')\n        q1_2024 = [pd.Period('2024-01'), pd.Period('2024-02'), pd.Period('2024-03')]\n        has_oct = months.eq(target_oct).any()\n        has_q1 = months.isin(q1_2024).any()\n        score = 0.0\n        if has_oct:\n            score += 0.75\n        if has_q1:\n            score += 0.75\n        return min(score, 1.5), \"Oct 2023 present: %s; Q1 2024 present: %s\" % (bool(has_oct), bool(has_q1))\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Supply vs Expected Calculations", "description": "Verify that $ difference and % vs Expected align with OH+OO and Expected Sales within reasonable tolerance.", "weight": 2.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        out = context.get_primary_output()\n        if not out or not out.is_spreadsheet:\n            return 0.0\n        path = context.files.get_path(out.id)\n        xls = pd.ExcelFile(path)\n        recap_df = None\n        for s in xls.sheet_names:\n            try:\n                df = pd.read_excel(path, sheet_name=s)\n                if isinstance(df.columns, pd.MultiIndex):\n                    df.columns = [' '.join([str(x) for x in tup if str(x)!='nan']).strip() for tup in df.columns]\n                df.columns = [str(c).strip() for c in df.columns]\n                cols_l = [c.lower() for c in df.columns]\n                if any('axis' in c for c in cols_l) and any('brand' in c for c in cols_l):\n                    recap_df = df\n                    break\n            except Exception:\n                continue\n        if recap_df is None:\n            return 0.0\n        cols_l = [c.lower() for c in recap_df.columns]\n        def find_col(patterns):\n            for i,c in enumerate(cols_l):\n                for p in patterns:\n                    if re.search(p, c):\n                        return recap_df.columns[i]\n            return None\n        oh_col = find_col([r'\\boh\\b', r'on[- ]hand'])\n        oo_col = find_col([r'\\boo\\b', r'on[- ]order'])\n        exp_col = find_col([r'(expected|forecast|proj)'])\n        supply_col = find_col([r'total.*supply', r'oh.*\\+.*oo', r'available'])\n        diff_col = find_col([r'(\\$|usd|dollar).*diff', r'diff.*expected', r'over(short)'])\n        pct_vs_col = find_col([r'(percent|%|pct).*(vs|coverage|of).*expected', r'coverage'])\n        if not (oh_col and oo_col and exp_col and (diff_col or pct_vs_col)):\n            return 0.4\n        oh = pd.to_numeric(recap_df[oh_col], errors='coerce')\n        oo = pd.to_numeric(recap_df[oo_col], errors='coerce')\n        exp = pd.to_numeric(recap_df[exp_col], errors='coerce')\n        calc_supply = oh.fillna(0) + oo.fillna(0)\n        if supply_col:\n            supply = pd.to_numeric(recap_df[supply_col], errors='coerce')\n        else:\n            supply = calc_supply\n        calc_diff = supply - exp\n        calc_pct = np.where(exp.fillna(0)!=0, calc_diff/exp, np.nan)\n        score = 0.0\n        # Compare to provided $ diff\n        if diff_col:\n            prov_diff = pd.to_numeric(recap_df[diff_col].astype(str).str.replace(',',''), errors='coerce')\n            mask = prov_diff.notna() & calc_diff.notna()\n            if mask.sum() > 0:\n                rel_err = ( (calc_diff[mask] - prov_diff[mask]).abs() / (prov_diff[mask].abs().replace(0, np.nan)) ).replace(np.nan, 0)\n                share = (rel_err <= 0.1).mean()  # within 10%\n                score += 1.0 * share\n        # Compare to provided % vs expected\n        if pct_vs_col:\n            prov_pct = pd.to_numeric(recap_df[pct_vs_col].astype(str).str.replace('%',''), errors='coerce')/100.0\n            mask2 = (~np.isnan(calc_pct)) & prov_pct.notna()\n            if mask2.sum() > 0:\n                diff = (calc_pct[mask2] - prov_pct[mask2]).abs()\n                share2 = (diff <= 0.03).mean()  # within 3pp\n                score += 1.0 * share2\n        return min(score, 2.0)\n    except Exception:\n        return 0.0"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Professional Quality and Strategic Usefulness", "description": "Stage 3 \u2014 LLM judge evaluates presentation quality, clarity, and strategic value for national accounts planning.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Presentation and Planning Utility", "description": "Assess whether the workbook is professionally formatted and directly useful for planning decisions (clear labeling, readability, emphasis on risks/opportunities).", "weight": 4.0, "judge_prompt": "Evaluate the FINAL Excel workbook for professional quality and practical usefulness for a Planning Manager and national accounts team. Do NOT re-check structural presence (Stage 1) or calculation accuracy (Stage 2). Focus on presentation and planning utility.\n\nConsider:\n- Clarity: Clear headers, consistent number formats (currency, percentages), readable column widths, frozen panes, filters.\n- Navigation: Sheet names are intuitive; a short intro on the Assumptions sheet explains how to read the recap.\n- Insightfulness: The recap highlights at-risk areas (e.g., coverage < 0% or < 0.0; negative or low coverage) and clear opportunities; subtotal and grand total lines are visually distinct.\n- Actionability: Comments column placed appropriately for future notes; expected shipment months are obvious and tie to the recap; projection window and dates are clearly labeled.\n- Appropriateness: Tailored to Beutist sets, broken out by Axis and Brand, with totals by Axis and a Grand Total.\n\nScoring:\n- 1.0: Basic formatting, understandable but rough\n- 2.5: Solid professional formatting, easy to read, suitable for sharing internally\n- 4.0: High polish, executive-ready, highlights risks/opportunities clearly, excellent usability\n\nReturn a score in [0, 1] scaled by weight.", "expectation": "A polished, easy-to-use workbook with clear labels, consistent formats, filters/frozen panes, visually distinct totals, and obvious planning takeaways (e.g., where supply < expected)."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "a079d38f-c529-436a-beca-3e291f9e62a3", "rubric": {"category_name": "Production Estimate (On-Set Only) \u2013 Producers and Directors", "rationale": "This rubric enforces a self-documenting, verifiable Excel workbook for an on-set educational video series estimate (no post-production). Stage 1 (LLM-only) mandates an exact workbook structure so verification is trivial. Stage 2 mixes code and LLM checks to validate assumptions, sums, and policy constraints (2 cameras, producer + audio tech present, no PA, 1\u20132 hr setup/day, 6\u20138 hr shooting/day). Stage 3 assesses professional quality and client readiness.", "max_total_score": 22.0, "stages": [{"name": "Stage 1 \u2013 Shape Enforcement Gate (LLM only)", "description": "Workbook must be an Excel file with the exact structural elements required to enable deterministic verification of costs and on-set time estimates (no post-production).", "is_required": true, "max_points": 8.0, "min_score_to_pass": 5.6, "rules": [{"type": "llm_judge", "name": "Structured Excel Workbook Requirements", "description": "Checks the presence of required sheets and tables with specific headers and mandatory assumptions. This is a pure structure/format gate, not a correctness check.", "weight": 8.0, "judge_prompt": "You are the Stage 1 gatekeeper. Evaluate ONLY the structure/format of the candidate output. Do not judge correctness of values, only that the required sheets, sections, and tables exist so later verification is possible.\n\nRequired format: Excel workbook (.xlsx). Not CSV, not PDF/Docx.\n\nWorkbook must include these sheets and structures (be flexible with similar sheet names in parentheses):\n\n1) Sheet: \"Assumptions\" (aka \"Input Assumptions\", \"Project Assumptions\")\n   - Must have a clearly labeled table with columns: [Parameter | Value | Source/Notes]\n   - The table must visibly include rows (flexible phrasing, but clearly identifiable) for ALL of the following:\n     \u2022 Cameras Used (must indicate 2)\n     \u2022 Producer On Site (Yes)\n     \u2022 Audio Technician On Site (Yes)\n     \u2022 Production Assistant Included (No)\n     \u2022 Post-Production Included (No)\n     \u2022 Hours per Shoot Day (6\u20138)\n     \u2022 Setup Hours per Day (1\u20132)\n     \u2022 Days of Shooting (integer)\n     \u2022 Number of Videos (from client list)\n\n2) Sheet: \"Daily Shoot Plan\" (aka \"Shoot Schedule\", \"Daily Plan\")\n   - Table with columns: [Day | Setup Hours | Shoot Hours | Total Hours | Crew/Roles]\n   - One row per shoot day. Total Hours should be visibly the sum of Setup + Shoot.\n\n3) Sheet: \"Cost Breakdown\" (aka \"Budget\", \"Detailed Costs\")\n   - Table with columns: [Category | Line Item | Unit | Unit Rate | Qty | Cost Subtotal]\n   - Must include, at minimum:\n     \u2022 Crew/Staff line items including Producer and Audio Technician\n     \u2022 Equipment line items reflecting use of 2 cameras (as equipment and/or operator roles)\n   - Must NOT include any PA (Production Assistant) line item\n   - Must NOT include any Post-Production category or line item\n\n4) Sheet: \"Summary\" (aka \"Totals\", \"Cost Summary\")\n   - Shows subtotals by category and a clear Grand Total\n   - Shows Total Shoot Hours across all days\n\nOptional (nice-to-have, not required to pass):\n- \"Rate Reference\" sheet with the organization\u2019s rates used (Role/Service | Unit | Rate)\n- \"Video Mapping\" sheet linking each requested video to shoot days/time (Video Title | Assigned Day | Est. Shoot Time | Notes)\n\nScoring (structure only):\n- 1.0: Valid Excel AND all 4 required sheets present with required tables/headers AND mandatory assumption rows present AND no PA or Post-Production sections appear.\n- 0.8: Valid Excel, all required sheets present, mandatory rows present, only optional sheets missing.\n- 0.6: Valid Excel, but missing ONE required sheet OR the required table/headers are incomplete in exactly one required sheet.\n- 0.3: Valid Excel, but multiple required sheets/tables are incomplete or missing.\n- 0.0: Not an Excel file OR structure is largely missing (e.g., multiple required sheets absent) OR PA/Post-Production items appear as cost items.\n\nOnly evaluate structure and presence. Do not check arithmetic or reasonableness here.", "expectation": "A clean Excel workbook with four core sheets\u2014Assumptions, Daily Shoot Plan, Cost Breakdown, and Summary\u2014each containing the specified tables/headers and mandatory rows, with no PA or post-production cost items."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification (Code + LLM)", "description": "Deterministic checks on assumptions, arithmetic consistency, and policy constraints (2 cameras; producer and audio tech present; no PA; no post-production).", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Assumptions Bounds Check", "description": "Verify core assumptions exist and fall within required bounds: 2 cameras; producer/audio yes; PA and post-production no; setup 1\u20132 hrs/day; shoot 6\u20138 hrs/day.", "weight": 3.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        fp = context.files.get_path(output.id)\n        xls = pd.ExcelFile(fp)\n        sheets = {name: pd.read_excel(fp, sheet_name=name, header=0) for name in xls.sheet_names}\n    except Exception:\n        return 0.0\n\n    def find_sheet_by_keywords(keys):\n        for n in sheets.keys():\n            nl = n.lower()\n            if any(k in nl for k in keys):\n                return n\n        return None\n\n    def norm_cols(df):\n        return [str(c).strip().lower() for c in df.columns]\n\n    def find_col(df, keywords):\n        cols = norm_cols(df)\n        for i,c in enumerate(cols):\n            for k in keywords:\n                if k in c:\n                    return df.columns[i]\n        return None\n\n    def to_scalar(x):\n        if isinstance(x, (int, float, np.integer, np.floating)):\n            return float(x)\n        s = str(x).strip().lower()\n        if s in [\"yes\",\"y\",\"true\",\"t\",\"1\"]:\n            return True\n        if s in [\"no\",\"n\",\"false\",\"f\",\"0\"]:\n            return False\n        try:\n            return float(s)\n        except Exception:\n            return s\n\n    checks = []  # list of booleans we can evaluate\n\n    # Find assumptions sheet\n    asum_name = find_sheet_by_keywords([\"assumption\"]) or find_sheet_by_keywords([\"input\", \"assumption\"]) or find_sheet_by_keywords([\"project\", \"assumption\"]) \n    if not asum_name:\n        return 0.0\n\n    df = sheets.get(asum_name)\n    if df is None or df.empty:\n        return 0.0\n\n    pcol = find_col(df, [\"parameter\", \"assumption\", \"item\"]) \n    vcol = find_col(df, [\"value\", \"val\"]) \n\n    if pcol is None or vcol is None:\n        # Cannot evaluate assumptions if table not found\n        return 0.0\n\n    # Build parameter dict (lowercased keys)\n    params = {}\n    for _, row in df[[pcol, vcol]].dropna(how='all').iterrows():\n        key = str(row[pcol]).strip().lower()\n        val = to_scalar(row[vcol])\n        if key:\n            params[key] = val\n\n    def get_param(keys):\n        for k in keys:\n            for pk, pv in params.items():\n                if k in pk:\n                    return pv\n        return None\n\n    # Cameras Used == 2\n    cam = get_param([\"camera\", \"cameras used\", \"camera count\"]) \n    if cam is not None:\n        try:\n            checks.append(float(cam) == 2.0)\n        except Exception:\n            checks.append(False)\n\n    # Producer On Site == True\n    prod = get_param([\"producer on site\", \"producer\", \"field producer\"]) \n    if prod is not None:\n        checks.append(bool(prod) is True)\n\n    # Audio Technician On Site == True\n    audio = get_param([\"audio technician\", \"audio tech\", \"sound engineer\", \"audio on site\"]) \n    if audio is not None:\n        checks.append(bool(audio) is True)\n\n    # PA Included == False\n    pa = get_param([\"pa included\", \"production assistant\", \"pa\"]) \n    if pa is not None:\n        checks.append(bool(pa) is False)\n\n    # Post-Production Included == False\n    post = get_param([\"post-production included\", \"post production included\", \"editing included\", \"post included\"]) \n    if post is not None:\n        checks.append(bool(post) is False)\n\n    # Hours per Shoot Day in [6,8]\n    shoot_hrs = get_param([\"hours per shoot day\", \"shoot hours per day\", \"daily shoot hours\", \"shooting hours\"]) \n    if shoot_hrs is not None and isinstance(shoot_hrs, (int,float,np.integer,np.floating)):\n        checks.append(6.0 <= float(shoot_hrs) <= 8.0)\n\n    # Setup Hours per Day in [1,2]\n    setup_hrs = get_param([\"setup hours per day\", \"setup time per day\", \"setup hours\"]) \n    if setup_hrs is not None and isinstance(setup_hrs, (int,float,np.integer,np.floating)):\n        checks.append(1.0 <= float(setup_hrs) <= 2.0)\n\n    if not checks:\n        return 0.0\n    return sum(1.0 for c in checks if c) / len(checks)"}, {"type": "code", "name": "Daily Plan Hours Consistency", "description": "Validate that each day\u2019s Total Hours equals Setup + Shoot, and per-day hour bounds (Setup 1\u20132, Shoot 6\u20138) are respected.", "weight": 2.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        fp = context.files.get_path(output.id)\n        xls = pd.ExcelFile(fp)\n        sheets = {name: pd.read_excel(fp, sheet_name=name, header=0) for name in xls.sheet_names}\n    except Exception:\n        return 0.0\n\n    def find_sheet_by_keywords(keys):\n        for n in sheets.keys():\n            nl = n.lower()\n            if any(k in nl for k in keys):\n                return n\n        return None\n\n    def find_col(df, keywords):\n        cols = [str(c).strip().lower() for c in df.columns]\n        for i,c in enumerate(cols):\n            for k in keywords:\n                if k in c:\n                    return df.columns[i]\n        return None\n\n    plan_name = find_sheet_by_keywords([\"daily\", \"plan\", \"schedule\", \"shoot\"]) \n    if not plan_name:\n        return 0.0\n    df = sheets.get(plan_name)\n    if df is None or df.empty:\n        return 0.0\n\n    scol = find_col(df, [\"setup\"]) \n    hcol = find_col(df, [\"shoot\"]) \n    tcol = find_col(df, [\"total\"]) \n    if scol is None or hcol is None or tcol is None:\n        return 0.0\n\n    checks = []\n    bounds_checks = []\n\n    for _, row in df[[scol, hcol, tcol]].dropna(how='all').iterrows():\n        try:\n            s = float(row[scol])\n            h = float(row[hcol])\n            t = float(row[tcol])\n            checks.append(abs((s + h) - t) <= 0.25)  # 15 min tolerance\n            bounds_checks.append(1.0 <= s <= 2.0)\n            bounds_checks.append(6.0 <= h <= 8.0)\n        except Exception:\n            continue\n\n    all_checks = checks + bounds_checks\n    if not all_checks:\n        return 0.0\n    return sum(1.0 for c in all_checks if c) / len(all_checks)"}, {"type": "code", "name": "Cost Breakdown Sum Consistency", "description": "Check row subtotals (Unit Rate \u00d7 Qty \u2248 Cost) and that grand total in Summary matches the sum of Cost Breakdown within tolerance.", "weight": 3.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        fp = context.files.get_path(output.id)\n        xls = pd.ExcelFile(fp)\n        sheets = {name: pd.read_excel(fp, sheet_name=name, header=0) for name in xls.sheet_names}\n    except Exception:\n        return 0.0\n\n    def find_sheet_by_keywords(keys):\n        candidates = []\n        for n in sheets.keys():\n            nl = n.lower()\n            if any(k in nl for k in keys):\n                candidates.append(n)\n        return candidates[0] if candidates else None\n\n    def find_col(df, keywords):\n        cols = [str(c).strip().lower() for c in df.columns]\n        for i,c in enumerate(cols):\n            for k in keywords:\n                if k in c:\n                    return df.columns[i]\n        return None\n\n    cost_name = find_sheet_by_keywords([\"cost\", \"budget\", \"breakdown\"]) \n    if not cost_name:\n        return 0.0\n    cdf = sheets.get(cost_name)\n    if cdf is None or cdf.empty:\n        return 0.0\n\n    unit_col = find_col(cdf, [\"unit rate\", \"rate\"]) \n    qty_col  = find_col(cdf, [\"qty\", \"quantity\"]) \n    sub_col  = find_col(cdf, [\"cost subtotal\", \"subtotal\", \"cost\"]) \n\n    if sub_col is None:\n        return 0.0\n\n    # Row-level subtotal checks\n    row_checks = []\n    if unit_col is not None and qty_col is not None:\n        for _, row in cdf[[unit_col, qty_col, sub_col]].dropna(how='all').iterrows():\n            try:\n                rate = float(row[unit_col])\n                qty  = float(row[qty_col])\n                sub  = float(row[sub_col])\n                ok = abs((rate * qty) - sub) <= max(1.0, 0.02 * sub)\n                row_checks.append(ok)\n            except Exception:\n                continue\n\n    # Sum of cost breakdown\n    try:\n        cb_total = pd.to_numeric(cdf[sub_col], errors='coerce').fillna(0).sum()\n    except Exception:\n        cb_total = None\n\n    # Compare with summary grand total (best-effort find max labeled total)\n    sum_name = find_sheet_by_keywords([\"summary\", \"total\"]) \n    sum_ok = None\n    if sum_name and cb_total is not None and cb_total > 0:\n        sdf = sheets.get(sum_name)\n        try:\n            # Look for a grand total candidate: the largest numeric in the sheet\n            nums = pd.to_numeric(sdf.select_dtypes(include=[np.number]).stack(), errors='coerce')\n            grand = float(nums.max()) if len(nums) else None\n        except Exception:\n            grand = None\n        if grand is not None and np.isfinite(grand):\n            tol = max(5.0, 0.02 * max(grand, cb_total))\n            sum_ok = abs(grand - cb_total) <= tol\n\n    parts = []\n    if row_checks:\n        parts.append(sum(1.0 for c in row_checks if c) / len(row_checks))\n    if sum_ok is not None:\n        parts.append(1.0 if sum_ok else 0.0)\n\n    if not parts:\n        return 0.0\n    return float(np.mean(parts))"}, {"type": "code", "name": "No Post-Production Cost Items", "description": "Ensure no post-production costs are included anywhere in the cost breakdown or summary.", "weight": 1.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        fp = context.files.get_path(output.id)\n        xls = pd.ExcelFile(fp)\n        sheets = {name: pd.read_excel(fp, sheet_name=name, header=0) for name in xls.sheet_names}\n    except Exception:\n        return 0.0\n\n    def find_sheet_by_keywords(keys):\n        for n in sheets.keys():\n            nl = n.lower()\n            if any(k in nl for k in keys):\n                return n\n        return None\n\n    cost_name = find_sheet_by_keywords([\"cost\", \"budget\", \"breakdown\"]) or \"\"\n    summary_name = find_sheet_by_keywords([\"summary\", \"total\"]) or \"\"\n\n    bad_terms = [\"post\", \"editing\", \"editor\", \"color\", \"grading\", \"vfx\", \"graphics\", \"post-production\"]\n\n    def has_bad_terms(df):\n        if df is None or df.empty:\n            return False\n        # Only scan likely item/description columns to reduce false positives\n        candidates = []\n        for c in df.columns:\n            cl = str(c).lower()\n            if any(k in cl for k in [\"category\", \"line\", \"item\", \"description\", \"service\"]):\n                candidates.append(c)\n        if not candidates:\n            # fallback: scan all text\n            candidates = list(df.columns)\n        for c in candidates:\n            col = df[c].astype(str).str.lower().fillna(\"\")\n            if col.str.contains('|'.join(bad_terms)).any():\n                return True\n        return False\n\n    bad = False\n    if cost_name:\n        bad = bad or has_bad_terms(sheets.get(cost_name))\n    if summary_name:\n        bad = bad or has_bad_terms(sheets.get(summary_name))\n\n    return 0.0 if bad else 1.0"}, {"type": "code", "name": "Staffing Presence in Costs (Producer + Audio, No PA)", "description": "Verify staffing lines include Producer and Audio roles but exclude PA in the cost breakdown.", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        fp = context.files.get_path(output.id)\n        xls = pd.ExcelFile(fp)\n        sheets = {name: pd.read_excel(fp, sheet_name=name, header=0) for name in xls.sheet_names}\n    except Exception:\n        return 0.0\n\n    def find_sheet_by_keywords(keys):\n        for n in sheets.keys():\n            nl = n.lower()\n            if any(k in nl for k in keys):\n                return n\n        return None\n\n    cost_name = find_sheet_by_keywords([\"cost\", \"budget\", \"breakdown\"]) \n    if not cost_name:\n        return 0.0\n    df = sheets.get(cost_name)\n    if df is None or df.empty:\n        return 0.0\n\n    # Identify a column likely to contain role/line item text\n    role_col = None\n    cols = [str(c).strip().lower() for c in df.columns]\n    for i,c in enumerate(cols):\n        if any(k in c for k in [\"line\", \"item\", \"role\", \"description\", \"service\"]):\n            role_col = df.columns[i]\n            break\n    if role_col is None:\n        # Fallback: use first column\n        role_col = df.columns[0]\n\n    text = df[role_col].astype(str).str.lower().str.strip().fillna(\"\")\n    has_producer = text.str.contains(\"producer\").any()\n    has_audio = text.str.contains(\"audio|sound engineer|sound tech\").any()\n    has_pa = text.str.contains(r\"\\bpa\\b|production assistant\").any()\n\n    score = 0.0\n    parts = []\n    parts.append(1.0 if has_producer else 0.0)\n    parts.append(1.0 if has_audio else 0.0)\n    parts.append(1.0 if not has_pa else 0.0)\n    return sum(parts)/len(parts) if parts else 0.0"}, {"type": "llm_judge", "name": "Requirement Alignment (LLM)", "description": "Cross-check that the workbook aligns with key requirements: 2 cameras, producer and audio tech present, no PA, setup 1\u20132 hrs/day, shoot 6\u20138 hrs/day, and no post-production.", "weight": 0.5, "judge_prompt": "Review the spreadsheet and answer: Does it align with the key constraints? Specifically:\n- Uses exactly 2 cameras for the shoot\n- Includes a Producer on site\n- Includes an Audio Technician on site\n- Excludes a Production Assistant (PA)\n- Per-day setup time is between 1\u20132 hours\n- Per-day shoot time is between 6\u20138 hours\n- No post-production items are included\n\nScoring:\n- 1.0: All 7 points clearly satisfied\n- 0.7: One minor element ambiguous but none violated\n- 0.4: One clear violation or multiple ambiguous items\n- 0.0: Multiple clear violations or fundamentally misaligned\n\nJudge only these alignment points; do not re-check arithmetic accuracy.", "expectation": "All constraints are clearly met and documented within the workbook."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Professional Quality (LLM)", "description": "Evaluate whether the workbook is client-ready: clarity, organization, labeling, transparent assumptions, and usability for decision-making.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Presentation and Professionalism", "description": "Holistic quality assessment of formatting, clarity, and client suitability.", "weight": 4.0, "judge_prompt": "Assess the overall quality of the Excel workbook (not just correctness):\n- Clarity and labeling: Are sheets, tables, and columns clearly titled and easy to navigate?\n- Professional formatting: Consistent fonts, readable number formats (currency, hours), visible totals, no clutter.\n- Transparency: Assumptions are documented with sources/notes; exclusions (post-production, PA, venue setup) are explicit.\n- Usability: Summary concisely presents category subtotals, grand total, and total shoot hours; Daily Shoot Plan is usable for scheduling.\n- Traceability: Costs in Summary are easy to map back to Cost Breakdown; rates (if referenced) are findable.\n\nScoring:\n- 1.0: Exemplary, fully client-ready, easy to understand and act upon\n- 0.7: Solid, minor improvements possible\n- 0.4: Adequate but noticeable issues in clarity or formatting\n- 0.2: Barely acceptable; difficult to use\n- 0.0: Poor quality, confusing or unprofessional\n\nFocus on presentation and practical usefulness; do not re-check numerical correctness.", "expectation": "A clean, well-organized, and client-ready workbook with clear labels, formatted numbers, transparent assumptions, and a concise, actionable summary."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "46fc494e-a24f-45ce-b099-851d5c181fd4", "rubric": {"category_name": "Thermal Screening Analysis: C/SiC Forward-Edge Heat Shield (Manufacturing \u2022 Mechanical Engineering)", "rationale": "This is a Mixed task (Pattern C): a concise engineering report with embedded plots plus a structured data workbook enabling verification. Stage 1 mandates a report + an Excel workbook with precise sheets and tables so verification is trivial. Stage 2 uses code rules to validate structure, bounds, and margin calculations against the 150 \u00b0C back-face limit. Stage 3 evaluates professional quality and decision usefulness, including mitigation recommendations when margin < 10 \u00b0C.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2013 SHAPE ENFORCEMENT (GATE)", "description": "LLM-only gate: Verify the candidate delivered BOTH a professional report (PDF/DOCX) and a companion Excel workbook with the exact structure needed for verification, plus required plots and tables.", "is_required": true, "max_points": 5.0, "min_score_to_pass": 4.0, "rules": [{"type": "llm_judge", "name": "Structured Output Requirement: Report + Data Workbook", "description": "Check presence and structure of BOTH the report (PDF/DOCX) and the Excel workbook with required sheets/tables enabling verification.", "weight": 5.0, "judge_prompt": "You are the Stage 1 Gate. Evaluate ONLY the STRUCTURE and PRESENCE of required deliverables. Do not judge correctness or quality.\n\nOutputs expected (both required):\nA) Report (PDF or DOCX): professionally formatted, at least 2 pages, with clear section headers.\nRequired report sections and artifacts:\n  1) Executive Summary / Findings: states whether back-face limit (150 \u00b0C) is exceeded or not, and reports the 20-minute back-face temperature and margin.\n  2) Model and Boundary Conditions: summarizes the 22-node in-plane conduction model and lists key parameters (k=5 W/m\u00b7K, \u03c1=2200 kg/m\u00b3, cp=800 J/kg\u00b7K, external gas 700 \u00b0C with h_ext=1200 W/m\u00b2\u00b7K, internal ambient 25 \u00b0C with h_int=15 W/m\u00b2\u00b7K, node spacing 0.05 m).\n  3) Results: must include all of the following plots:\n     - Node temperature profiles vs node index at each time: 0.5, 5, 10, 20 minutes (one composite figure with multiple curves or separate plots are acceptable).\n     - Contour/isotherm plot at 20 minutes (visual showing spatial temperature field or isotherms; screenshot/heatmap acceptable).\n     - Time-trace plots for representative nodes 1, 13, 22.\n  4) Recommendations (conditional): If margin at 20 minutes is under 10 \u00b0C, include mitigation suggestions (e.g., thicker panels, improved coatings, emissivity changes, surface catalycity control, increased internal convection, TPS layering).\n  5) Figures must show labeled axes and units (\u00b0C, minutes, node index) and have captions.\n\nB) Excel Workbook (required): one .xlsx file containing these sheets and tables:\n  Sheet: \"Inputs\"\n    - Table columns: [Parameter | Value | Units | Notes]\n    - Must include rows for: Thermal conductivity (5 W/m\u00b7K), Density (2200 kg/m\u00b3), Specific heat (800 J/kg\u00b7K), External gas temperature (700 \u00b0C), External convective coefficient (1200 W/m\u00b2\u00b7K), Internal ambient temperature (25 \u00b0C), Internal convective coefficient (15 W/m\u00b2\u00b7K), Node spacing (0.05 m), Time points (0.5, 5, 10, 20 min), Back Face Node Index (must be 1 or 22).\n  Sheet: \"NodeTemperatures\"\n    - Table with columns: [Node | T_0.5_min_C | T_5_min_C | T_10_min_C | T_20_min_C]\n    - Node rows must cover integers 1..22.\n  Sheet: \"TimeTraces\"\n    - Table with columns: [time_min | T_node1_C | T_node13_C | T_node22_C]\n    - Must include at least the time points 0.5, 5, 10, 20 minutes (additional times allowed).\n  Sheet: \"BackFaceSummary\"\n    - Table with columns: [time_min | back_face_node_index | back_face_temp_C | margin_to_150C_C]\n    - Must include at least a row for 20 minutes; may include other times and an overall maximum row. The margin must be shown as 150 \u2212 back_face_temp_C (in \u00b0C).\n  Sheet: \"Methodology\" (optional)\n    - Brief calculation/method notes (text acceptable).\n\nScoring (STRUCTURE ONLY; be flexible with minor naming variations but require the substantive elements):\n- 1.0: Report (PDF/DOCX) present with all required plots/sections + Excel present with all 4 required sheets and their tables correctly structured.\n- 0.7: All core items present but missing 1 minor element (e.g., missing optional Methodology sheet or small labeling omission in figures) \u2013 all 4 required Excel sheets must still exist with tables; all 3 plot types present.\n- 0.4: Missing one required Excel sheet OR missing one required plot type OR report not clearly including Executive Summary/Findings.\n- 0.2: Deliverables present but largely incomplete (e.g., only report without Excel, or Excel without report, or multiple required parts missing).\n- 0.0: Wrong formats or missing most required elements.\n\nReturn a score between 0 and 1 based on this rubric, plus a short rationale. Do NOT assess numerical correctness.", "expectation": "A professional report (PDF/DOCX) with required plots and a companion Excel workbook with Inputs, NodeTemperatures, TimeTraces, BackFaceSummary sheets exactly as specified."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 VERIFICATION (Correctness + Consistency)", "description": "Code checks for data presence, bounds, structure, and margin calculation consistency; plus LLM check that the stated conclusion matches the data.", "is_required": false, "max_points": 3.5, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Excel Structure and Required Data Presence", "description": "Verify presence of the Excel workbook and required sheets; confirm required columns/rows exist with flexible matching.", "weight": 1.2, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    # Find a spreadsheet among outputs\n    excel = None\n    for res in context.get_all_outputs() or []:\n        if getattr(res, 'is_spreadsheet', False):\n            excel = res\n            break\n    if excel is None:\n        return 0.0, 'No spreadsheet output found.'\n\n    # Helper: safe read sheet names\n    try:\n        xls_path = context.files.get_path(excel.id)\n        xls = pd.ExcelFile(xls_path)\n        sheet_names = [s.lower() for s in xls.sheet_names]\n    except Exception as e:\n        return 0.0, f'Failed to open Excel workbook: {e}'\n\n    required_sheets = ['inputs', 'nodetemperatures', 'timetraces', 'backfacesummary']\n    sheet_score = sum(1 for s in required_sheets if s in sheet_names) / len(required_sheets)\n\n    # Initialize checks\n    detail_checks = []\n\n    # Check Inputs sheet parameters\n    try:\n        df_in = pd.read_excel(xls_path, sheet_name=[s for s in xls.sheet_names if s.lower()=='inputs'][0])\n        df_in_str = df_in.astype(str).applymap(lambda x: x.lower())\n        def has_param(keyword):\n            return df_in_str.apply(lambda col: col.str.contains(keyword, na=False)).any(axis=None)\n        params_ok = all([\n            has_param('thermal') and has_param('conduct'),\n            has_param('density') or has_param('rho'),\n            has_param('specific') and has_param('heat'),\n            has_param('external') and (has_param('gas') or has_param('freestream')),\n            (has_param('convective') and has_param('external')) or has_param('h_ext'),\n            has_param('internal') and has_param('ambient'),\n            (has_param('convective') and has_param('internal')) or has_param('h_int'),\n            has_param('node') and has_param('spacing'),\n            has_param('back') and has_param('face') and has_param('node'),\n        ])\n        detail_checks.append(1.0 if params_ok else 0.0)\n    except Exception:\n        detail_checks.append(0.0)\n\n    # Check NodeTemperatures sheet has Node and the 4 time columns\n    def extract_times(cols):\n        times = {}\n        for c in cols:\n            cl = str(c).lower()\n            # look for minutes explicitly or plain numbers\n            m = re.findall(r\"(\\d+\\.?\\d*)\\s*(min|m|minute)?\", cl)\n            # Prefer entries with min markers; fallback to any number\n            if m:\n                try:\n                    val = float(m[0][0])\n                    # Heuristic: if seconds indicated with 's', skip (we only accept minutes here)\n                    if 's' in cl and 'min' not in cl:\n                        continue\n                    times[round(val, 3)] = c\n                except:\n                    pass\n        return times\n\n    try:\n        df_nt = pd.read_excel(xls_path, sheet_name=[s for s in xls.sheet_names if s.lower()=='nodetemperatures'][0])\n        cols = list(df_nt.columns)\n        has_node_col = any('node' in str(c).lower() for c in cols)\n        times_map = extract_times(cols)\n        required_times = [0.5, 5.0, 10.0, 20.0]\n        times_found = sum(1 for t in required_times if any(abs(t - k) < 1e-6 for k in times_map.keys()))\n        detail_checks.append((1.0 if has_node_col else 0.0))\n        detail_checks.append(times_found / 4.0)\n    except Exception:\n        detail_checks.append(0.0)\n        detail_checks.append(0.0)\n\n    # Check TimeTraces: time column + node1, node13, node22 columns\n    try:\n        df_tt = pd.read_excel(xls_path, sheet_name=[s for s in xls.sheet_names if s.lower()=='timetraces'][0])\n        cols = [str(c).lower() for c in df_tt.columns]\n        has_time = any('time' in c for c in cols)\n        def has_node_col(n):\n            return any((f\"node{n}\" in c.replace('_','') or f\"{n}\" in c and 'node' in c) for c in cols)\n        tt_ok = has_time and has_node_col(1) and has_node_col(13) and has_node_col(22)\n        # time coverage for required points\n        time_col = [c for c in df_tt.columns if 'time' in str(c).lower()]\n        time_cov = 0.0\n        if time_col:\n            tvals = pd.to_numeric(df_tt[time_col[0]], errors='coerce').dropna().values\n            req = np.array([0.5, 5.0, 10.0, 20.0])\n            time_cov = float(sum(any(abs(tv - r) < 1e-6 for tv in tvals) for r in req)) / 4.0\n        detail_checks.append(1.0 if tt_ok else 0.0)\n        detail_checks.append(time_cov)\n    except Exception:\n        detail_checks.append(0.0)\n        detail_checks.append(0.0)\n\n    # Check BackFaceSummary columns\n    try:\n        df_bf = pd.read_excel(xls_path, sheet_name=[s for s in xls.sheet_names if s.lower()=='backfacesummary'][0])\n        cols = [str(c).lower() for c in df_bf.columns]\n        has_req = (any('time' in c for c in cols) and\n                   any('back' in c and 'face' in c and 'node' in c for c in cols) and\n                   any('temp' in c for c in cols) and\n                   any('margin' in c for c in cols))\n        detail_checks.append(1.0 if has_req else 0.0)\n    except Exception:\n        detail_checks.append(0.0)\n\n    # Aggregate: 40% sheet presence + 60% detail checks\n    detail_score = np.mean(detail_checks) if detail_checks else 0.0\n    overall = 0.4 * sheet_score + 0.6 * detail_score\n    overall = float(max(0.0, min(1.0, overall)))\n    feedback = f\"Sheets present {sheet_score:.2f}, detail checks {detail_score:.2f}.\"\n    return overall, feedback"}, {"type": "code", "name": "Temperature Bounds and Node Coverage", "description": "Check node coverage (1..22), numeric temperatures, and plausible bounds relative to stated conditions (not strict correctness).", "weight": 0.8, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    excel = None\n    for res in context.get_all_outputs() or []:\n        if getattr(res, 'is_spreadsheet', False):\n            excel = res\n            break\n    if excel is None:\n        return 0.0, 'No spreadsheet output found.'\n\n    try:\n        xls_path = context.files.get_path(excel.id)\n        df_nt = pd.read_excel(xls_path, sheet_name='NodeTemperatures')\n    except Exception as e:\n        return 0.0, f'Cannot read NodeTemperatures: {e}'\n\n    # Identify node column\n    node_col = None\n    for c in df_nt.columns:\n        if 'node' in str(c).lower():\n            node_col = c\n            break\n    if node_col is None:\n        # fallback: first column if it looks like indices\n        node_col = df_nt.columns[0]\n\n    nodes = pd.to_numeric(df_nt[node_col], errors='coerce')\n    valid_nodes = nodes.dropna().astype(int).tolist()\n    coverage = len(set(valid_nodes) & set(range(1,23))) / 22.0\n\n    # Extract temperature columns (look for '_min' or 't_' columns)\n    temp_cols = [c for c in df_nt.columns if c != node_col]\n    temps = pd.DataFrame()\n    for c in temp_cols:\n        temps[c] = pd.to_numeric(df_nt[c], errors='coerce')\n    if temps.empty:\n        return 0.0, 'No temperature columns found.'\n\n    # Plausible bounds (broad): -50 to 900 \u00b0C; also suggest 0-800 as more typical\n    vals = temps.values.flatten()\n    finite = np.isfinite(vals)\n    if finite.sum() == 0:\n        return 0.0, 'No numeric temperature data.'\n    v = vals[finite]\n    low_ok = float(np.mean(v >= -50))\n    high_ok = float(np.mean(v <= 900))\n    bounds_ok = (low_ok + high_ok) / 2.0\n\n    # Encourage that extremes align with BCs: min near 25 \u00b0C, max <= 700 \u00b0C (soft check)\n    min_v = np.nanmin(v)\n    max_v = np.nanmax(v)\n    soft_ok = 0.0\n    soft_ok += 0.5 if min_v >= -10 and min_v <= 100 else 0.0  # near ambient range\n    soft_ok += 0.5 if max_v <= 750 else 0.0                   # not far beyond freestream\n\n    score = 0.4*coverage + 0.4*bounds_ok + 0.2*soft_ok\n    score = float(max(0.0, min(1.0, score)))\n    fb = f\"Node coverage {coverage:.2f}, bounds {bounds_ok:.2f}, soft {soft_ok:.2f}. Min={min_v:.1f}C Max={max_v:.1f}C\"\n    return score, fb"}, {"type": "code", "name": "Back-Face Margin Calculation Accuracy", "description": "Recompute margin_to_150C_C from NodeTemperatures and compare to BackFaceSummary values at 20 minutes (within 1 \u00b0C tolerance).", "weight": 0.8, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    excel = None\n    for res in context.get_all_outputs() or []:\n        if getattr(res, 'is_spreadsheet', False):\n            excel = res\n            break\n    if excel is None:\n        return 0.0, 'No spreadsheet output found.'\n\n    try:\n        xls_path = context.files.get_path(excel.id)\n        xls = pd.ExcelFile(xls_path)\n        df_nt = pd.read_excel(xls_path, sheet_name=[s for s in xls.sheet_names if s.lower()=='nodetemperatures'][0])\n        df_bf = pd.read_excel(xls_path, sheet_name=[s for s in xls.sheet_names if s.lower()=='backfacesummary'][0])\n        # Inputs (optional): attempt to read back-face node index\n        bf_idx = None\n        try:\n            df_in = pd.read_excel(xls_path, sheet_name=[s for s in xls.sheet_names if s.lower()=='inputs'][0])\n            mask = df_in.astype(str).applymap(lambda x: x.lower()).apply(\n                lambda col: col.str.contains('back') & col.str.contains('face') & col.str.contains('node'), axis=0\n            )\n            if mask.any().any():\n                r, c = np.where(mask.values)\n                if len(r):\n                    row = df_in.iloc[r[0]]\n                    # pick any numeric in the row\n                    nums = pd.to_numeric(row, errors='coerce')\n                    nums = nums[np.isfinite(nums)]\n                    if len(nums)>0:\n                        bf_idx = int(nums.iloc[0])\n        except Exception:\n            pass\n\n        # Identify node column and time columns in NodeTemperatures\n        node_col = None\n        for c in df_nt.columns:\n            if 'node' in str(c).lower():\n                node_col = c\n                break\n        if node_col is None:\n            node_col = df_nt.columns[0]\n        # Map required times to columns\n        def find_time_col(cols, target):\n            best = None\n            for c in cols:\n                cl = str(c).lower()\n                m = re.findall(r\"(\\d+\\.?\\d*)\\s*(min|m|minute)?\", cl)\n                if m:\n                    try:\n                        val = float(m[0][0])\n                        if abs(val - target) < 1e-6:\n                            best = c\n                            break\n                    except:\n                        pass\n            return best\n        col_20 = find_time_col(df_nt.columns, 20.0)\n        if col_20 is None:\n            return 0.0, 'Could not find 20-minute temperature column.'\n\n        # Determine back-face node index if absent by using BackFaceSummary column\n        if bf_idx is None:\n            # try from BackFaceSummary\n            cols = [c for c in df_bf.columns if 'back' in str(c).lower() and 'face' in str(c).lower() and 'node' in str(c).lower()]\n            if cols:\n                bf_col = cols[0]\n                # take the most frequent integer value\n                bf_vals = pd.to_numeric(df_bf[bf_col], errors='coerce').dropna().astype(int)\n                if not bf_vals.empty:\n                    bf_idx = int(bf_vals.mode().iloc[0])\n        if bf_idx is None:\n            return 0.0, 'Back-face node index not found.'\n\n        # Extract back-face temperature at 20 min from NodeTemperatures\n        # Find row with node = bf_idx\n        row = df_nt[pd.to_numeric(df_nt[node_col], errors='coerce')==bf_idx]\n        if row.empty:\n            return 0.0, f'Back-face node {bf_idx} not present in NodeTemperatures.'\n        t20_calc = pd.to_numeric(row[col_20], errors='coerce')\n        if t20_calc.isna().all():\n            return 0.0, 'Back-face 20-min temperature is non-numeric.'\n        t20_calc = float(t20_calc.iloc[0])\n        margin_calc = 150.0 - t20_calc\n\n        # Find matching 20-min row in BackFaceSummary\n        time_cols = [c for c in df_bf.columns if 'time' in str(c).lower()]\n        if not time_cols:\n            return 0.0, 'No time column in BackFaceSummary.'\n        tcol = time_cols[0]\n        times = pd.to_numeric(df_bf[tcol], errors='coerce')\n        if times.isna().all():\n            return 0.0, 'BackFaceSummary time column non-numeric.'\n        # choose row closest to 20 min\n        idx = (times - 20.0).abs().idxmin()\n        row_bf = df_bf.loc[idx]\n        temp_cols = [c for c in df_bf.columns if 'temp' in str(c).lower()]\n        margin_cols = [c for c in df_bf.columns if 'margin' in str(c).lower()]\n        if not temp_cols or not margin_cols:\n            return 0.0, 'BackFaceSummary missing temp or margin columns.'\n        t20_rep = pd.to_numeric([row_bf[temp_cols[0]]], errors='coerce')[0]\n        m_rep = pd.to_numeric([row_bf[margin_cols[0]]], errors='coerce')[0]\n        if not np.isfinite(t20_rep) or not np.isfinite(m_rep):\n            return 0.0, 'BackFaceSummary 20-min values non-numeric.'\n\n        # Compare within tolerance\n        t_ok = (abs(t20_rep - t20_calc) <= 1.0)\n        m_ok = (abs(m_rep - margin_calc) <= 1.0)\n        score = (1.0 if t_ok else 0.0) * 0.5 + (1.0 if m_ok else 0.0) * 0.5\n        fb = f\"t20_calc={t20_calc:.2f}C vs reported {t20_rep:.2f}C; margin_calc={margin_calc:.2f}C vs reported {m_rep:.2f}C.\"\n        return float(score), fb\n    except Exception as e:\n        return 0.0, f'Error verifying margins: {e}'"}, {"type": "code", "name": "Pass/Fail Determination vs 150 \u00b0C", "description": "Compute pass/fail at 20 min from NodeTemperatures and provide diagnostic feedback.", "weight": 0.2, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    excel = None\n    for res in context.get_all_outputs() or []:\n        if getattr(res, 'is_spreadsheet', False):\n            excel = res\n            break\n    if excel is None:\n        return 0.0, 'No spreadsheet output found.'\n\n    try:\n        xls_path = context.files.get_path(excel.id)\n        df_nt = pd.read_excel(xls_path, sheet_name='NodeTemperatures')\n        # Back-face node from Inputs or BackFaceSummary\n        bf_idx = None\n        try:\n            df_in = pd.read_excel(xls_path, sheet_name='Inputs')\n            df_in_l = df_in.astype(str).applymap(lambda x: x.lower())\n            mask = df_in_l.apply(lambda col: col.str.contains('back') & col.str.contains('face') & col.str.contains('node'), axis=0)\n            if mask.any().any():\n                r, c = np.where(mask.values)\n                row = df_in.iloc[r[0]]\n                nums = pd.to_numeric(row, errors='coerce')\n                nums = nums[np.isfinite(nums)]\n                if len(nums)>0:\n                    bf_idx = int(nums.iloc[0])\n        except Exception:\n            pass\n        if bf_idx is None:\n            # fallback: choose 1 (common for back-face) if present\n            bf_idx = 1\n\n        node_col = None\n        for c in df_nt.columns:\n            if 'node' in str(c).lower():\n                node_col = c\n                break\n        if node_col is None:\n            node_col = df_nt.columns[0]\n\n        def find_time_col(cols, target):\n            for c in cols:\n                cl = str(c).lower()\n                m = re.findall(r\"(\\d+\\.?\\d*)\\s*(min|m|minute)?\", cl)\n                if m:\n                    try:\n                        val = float(m[0][0])\n                        if abs(val - target) < 1e-6:\n                            return c\n                    except:\n                        pass\n            return None\n        col_20 = find_time_col(df_nt.columns, 20.0)\n        if col_20 is None:\n            return 0.0, 'No 20-min column found.'\n\n        row = df_nt[pd.to_numeric(df_nt[node_col], errors='coerce')==bf_idx]\n        if row.empty:\n            return 0.0, f'Back-face node {bf_idx} not found.'\n        t20 = pd.to_numeric(row[col_20], errors='coerce')\n        if t20.isna().all():\n            return 0.0, 'Back-face 20-min temp non-numeric.'\n        t20 = float(t20.iloc[0])\n        margin = 150.0 - t20\n        passed = t20 <= 150.0\n        fb = f\"Back-face node {bf_idx}: T20={t20:.2f}C, margin={margin:.2f}C, pass={passed}.\"\n        # award full score if computed successfully; this rule is informational\n        return 1.0, fb\n    except Exception as e:\n        return 0.0, f'Error computing pass/fail: {e}'"}, {"type": "llm_judge", "name": "Conclusion and Recommendation Consistency", "description": "Check the report states pass/fail for the 150 \u00b0C back-face limit at 20 min and reports the margin. If margin < 10 \u00b0C, confirm the report includes concrete mitigation recommendations.", "weight": 0.5, "judge_prompt": "Review the report (PDF/DOCX) content only for consistency and completeness of the conclusion and recommendations:\n\nRequirements:\n- Explicitly state whether the 150 \u00b0C back-face limit is exceeded at 20 minutes (PASS if \u2264 150 \u00b0C, FAIL if > 150 \u00b0C).\n- Report the 20-minute back-face temperature and the margin to 150 \u00b0C clearly in the Executive Summary/Findings.\n- If the stated margin is under 10 \u00b0C, include specific mitigation recommendations (e.g., thicker panels, improved coatings, emissivity/catalycity control, increased internal convection, TPS layering). Generic or missing recommendations should not receive full credit when margin < 10 \u00b0C is claimed.\n\nScoring:\n- 1.0: Clear pass/fail statement AND 20-min back-face temperature AND margin stated; when margin < 10 \u00b0C, specific mitigations are provided.\n- 0.6: Pass/fail with temperature OR margin stated; mitigations generic or slightly incomplete when margin < 10 \u00b0C.\n- 0.2: Vague conclusion or missing numerical support; no mitigations despite margin < 10 \u00b0C.\n- 0.0: No conclusion regarding the 150 \u00b0C criterion.\n\nReturn a score between 0 and 1 plus a short rationale. Do not re-evaluate numeric correctness; focus on presence/consistency in the text.", "expectation": "Report explicitly declares PASS/FAIL at 20 minutes with back-face temperature and margin; includes actionable mitigations if margin < 10 \u00b0C."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 QUALITY (Professionalism and Decision Usefulness)", "description": "Holistic LLM assessment of presentation quality, clarity, and decision usefulness for rapid screening.", "is_required": false, "max_points": 1.5, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation Quality", "description": "Evaluate clarity, formatting, figure labeling, and data-to-figure alignment.", "weight": 1.0, "judge_prompt": "Assess the professional quality of the report (PDF/DOCX):\n- Are sections logically organized and concise for a rapid screening memo?\n- Do all figures have captions, labeled axes, and units (\u00b0C, minutes, node index)?\n- Are plots legible and appropriately scaled (lines distinguishable, contour readable)?\n- Are the tables clear and consistent with the narrative?\n\nScoring:\n- 1.0: Highly professional; all figures/tables labeled with units; clear, concise structure.\n- 0.6: Generally good; minor labeling or clarity issues.\n- 0.3: Several presentation issues; readability suffers.\n- 0.0: Poorly formatted; figures/tables unclear.\n\nReturn a score between 0 and 1 and a short rationale.", "expectation": "A concise, professional report with well-labeled figures and clear tables suitable for engineering review."}, {"type": "llm_judge", "name": "Decision Usefulness and Mitigations", "description": "Evaluate whether insights support a go/no-go screening decision and whether mitigations are actionable when margin is tight.", "weight": 0.5, "judge_prompt": "Evaluate the decision usefulness of the report:\n- Does it clearly support a rapid screening decision (go/no-go) for this C/SiC heat-shield configuration under the stated event?\n- If margin is under ~10 \u00b0C, are mitigation options concrete and technically plausible (e.g., thickness increase with rationale, improved coating with expected effect, changes to internal convection)?\n- Are assumptions or limitations (e.g., 1-D/2-D simplifications, convective boundary idealization) acknowledged briefly?\n\nScoring:\n- 1.0: Clear decision guidance with actionable mitigations (if needed) and brief acknowledgement of assumptions/limits.\n- 0.6: Mostly useful; mitigations somewhat generic or assumptions not mentioned.\n- 0.3: Limited decision value; minimal mitigation discussion.\n- 0.0: Not decision-oriented.\n\nReturn a score between 0 and 1 and a short rationale.", "expectation": "Clear decision guidance with actionable mitigations when margin is tight; brief acknowledgement of modeling assumptions/limits."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "a1963a68-1bea-4bb1-b7e0-145c92a57449", "rubric": {"category_name": "Finance & Insurance \u2014 Financial Managers: Deep Dive Strategy Presentation (Korea Ride-hailing)", "rationale": "This rubric follows the self-documenting 3-stage approach. Stage 1 uses an LLM judge to strictly enforce the required PDF slide-deck structure so that verification is possible. Stage 2 mixes code rules and an LLM judge to verify textual presence of required pillars, timeline anchoring to H2 2024, evidence of data/citations, bullet structure density, regulatory/supply coverage, and localization signals, plus a coherence check. Stage 3 assesses executive readiness and strategic depth/priority as a holistic quality judgment.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (PDF Slide Deck Structure)", "description": "Gate: Validate the deliverable is a PDF slide deck with the mandated sections and verifiable structure for discussion-ready strategy content.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Structured Presentation Format Requirement (PDF, 5\u20136 core slides, mandated sections)", "description": "Check that the output is a properly structured slide deck in PDF with the specified sections and discussion-ready bullet formatting.", "weight": 8.0, "judge_prompt": "You are evaluating whether the candidate\u2019s output (a file) satisfies the SPECIFIC STRUCTURAL requirements below. Only check PRESENCE and STRUCTURE, not quality of content.\n\nRequired format and structure:\n1) File format: PDF (presentation-style slides). Reject if not PDF.\n2) Slide structure:\n   - A cover slide with a clear title indicating SuperK-Taxi (or SuperK-T) and Korea (flexible on exact naming like \u201cSuperK\u201d, \u201cSuperK-Taxi Korea\u201d, \u201cSuperK-T Korea\u201d).\n   - 5\u20136 core content slides (excluding cover, Q&A, and appendices). If there are 4 or 7 core content slides, allow partial credit. The deck may optionally include a Q&A slide and appendices.\n   - Each core content slide uses bullet points (\u2022, -, en dash, or numbered lists). Aim for 3\u20136 bullets per slide; be flexible but ensure bullet-style structure is visible across core slides.\n3) Mandated strategic pillars must be clearly represented with slide titles or section headers (flexible synonyms accepted):\n   A. \u201cMarket Reality & Strategic Imperatives\u201d (accept variants such as Market Overview/Context, Competitive Landscape, Strategic Imperatives, Challenges)\n   B. \u201cCore Growth & Operational Excellence Plan\u201d (accept variants such as Growth Plan/Back-to-Basics, Driver/Vehicle Supply Plan, Operational Excellence)\n   C. \u201cFuture-Proofing SuperK-Taxi\u201d (accept variants such as Innovation, Sustainability, Long-term Leadership, Roadmap)\n   These may be one slide each or split across multiple slides; the aggregate must clearly cover all three pillars.\n4) H2 2024 anchoring: At least one slide must visibly anchor the plan to H2 2024 (e.g., references to H2 2024, Q3/Q4 2024, August 2024 start).\n5) Data support and citations: Across the core slides, there should be visible data points (numbers/percentages) and at least 3 citations or source references (e.g., footnotes, \u201cSource:\u201d, URLs). Be flexible about exact formatting. References to Korean sources like the Korea Transportation Safety Authority or the Korea National Joint Conference of Taxi Associations count but are not strictly required as long as at least 3 sources are cited.\n\nScoring (return a numeric score from 0 to 8 and a brief justification):\n- 8.0: PDF; clearly a slide deck; 5\u20136 core content slides; all three pillars present; bullet formatting visible on core slides; H2 2024 timeline anchor present; \u22653 citations/sources visible; cover includes SuperK(-Taxi) and Korea.\n- 6.5: PDF; clearly a slide deck; 5\u20136 core slides; all three pillars present; bullet formatting visible; missing either the H2 2024 anchor OR has only 1\u20132 citations; or has 4 or 7 core slides with otherwise strong structure.\n- 4.0: PDF but only 3\u20134 core slides OR missing one of the three pillars OR bullets largely absent; or otherwise insufficient structure for verification.\n- 0.0: Not a PDF; or not a slide deck; or fewer than 3 core content slides; or structure grossly incompatible with the requirements.\n\nOnly judge structure and presence, not correctness, not writing quality. Output the score (0\u20138) and a concise rationale.", "expectation": "A PDF slide deck with a cover slide, 5\u20136 core content slides using bullets, the three strategic pillars clearly covered, an H2 2024 timeline anchor, and \u22653 citations/sources visible."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Content Presence and Consistency)", "description": "Now verify, using code and LLM, that the structured output contains the required elements: pillar coverage, timeline anchoring to H2 2024, data/sources evidence, bullet density, regulatory and supply coverage, and localization signals; plus an LLM consistency check.", "is_required": false, "max_points": 7.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Core Pillars Present (Text Match)", "description": "Check text for the three mandated pillars using flexible keyword matching.", "weight": 2.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output to evaluate.\"\n    text = \"\"\n    try:\n        if hasattr(context.files, 'read_pdf_text'):\n            try:\n                text = context.files.read_pdf_text(output.id) or \"\"\n            except Exception:\n                text = \"\"\n        if (not text) and hasattr(context.files, 'read_docx_text'):\n            try:\n                text = context.files.read_docx_text(output.id) or \"\"\n            except Exception:\n                text = \"\"\n    except Exception:\n        text = \"\"\n    if not text:\n        # If no extractable text (e.g., scanned PDF), give minimal but non-zero feedback if Stage 1 passed visually; here we must score 0 due to no verification possible.\n        return 0.0, \"No extractable text found.\"\n\n    lt = text.lower()\n    # Flexible synonym sets for each pillar\n    p1 = [\n        'market reality', 'market overview', 'market context', 'competitive landscape', 'competition',\n        'strategic imperative', 'imperatives', 'growth challenges', 'positioning', 'market reality & strategic'\n    ]\n    p2 = [\n        'core growth', 'growth plan', 'operational excellence', 'operations', 'driver supply', 'vehicle supply',\n        'supply plan', 'fleet', 'acquisition', 'customer segment', 'back-to-basics'\n    ]\n    p3 = [\n        'future-proof', 'future proof', 'innovation', 'sustainability', 'long-term', 'long term', 'roadmap',\n        'vision', 'ev', 'electric vehicle', 'autonomous', 'ai', 'loyalty'\n    ]\n\n    hits = 0\n    hits += 1 if any(k in lt for k in p1) else 0\n    hits += 1 if any(k in lt for k in p2) else 0\n    hits += 1 if any(k in lt for k in p3) else 0\n\n    weight = 2.0\n    score = (hits / 3.0) * weight\n    return score, f\"Pillars covered: {hits}/3.\""}, {"type": "code", "name": "Actionable Timeline Anchored to H2 2024", "description": "Detect references to H2 2024 timing (e.g., H2 2024, Q3/Q4 2024, August 2024).", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text:\n        return 0.0\n    lt = text.lower()\n    patterns = [\n        r\"\\bh2\\s*2024\\b\",\n        r\"\\bq[34]\\s*2024\\b\",\n        r\"\\baug(?:ust)?\\s*(?:2024)?\\b\",\n        r\"\\bsep(?:tember)?\\s*2024\\b\",\n        r\"\\boct(?:ober)?\\s*2024\\b\",\n        r\"\\bnov(?:ember)?\\s*2024\\b\",\n        r\"\\bdec(?:ember)?\\s*2024\\b\",\n    ]\n    found = any(re.search(p, lt) for p in patterns)\n    if found:\n        return 1.0, \"H2 2024 timeline references found.\"\n    # Partial credit if generic H2 and 2024 appear somewhere\n    partial = (\"h2\" in lt and \"2024\" in lt)\n    return (0.5 if partial else 0.0, \"Partial timeline signal.\" if partial else \"No clear H2 2024 anchor.\")"}, {"type": "code", "name": "Evidence of Data and Sources", "description": "Check for quantitative data and citations/URLs, currency markers, years, and references to Korean authorities/sources.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text:\n        return 0.0\n    lt = text.lower()\n    percent_count = len(re.findall(r\"\\b\\d{1,3}\\s?%\", lt))\n    url_count = lt.count('http') + lt.count('www.')\n    source_count = lt.count('source:') + lt.count('source')\n    currency = any(k in lt for k in ['krw', '\u20a9', 'won'])\n    year_hits = sum(1 for y in ['2023', '2024'] if y in lt)\n    korea_refs = any(k in lt for k in [\n        'korea transportation safety', 'ktsa', 'korea national joint conference of taxi associations',\n        'taxi association', 'molit', 'ministry of land, infrastructure and transport'\n    ])\n\n    # Components: percentages>=3, (url or source)>=1, currency present, years>=2, korea_refs present\n    components = 0\n    components += 1 if percent_count >= 3 else 0\n    components += 1 if (url_count + source_count) >= 1 else 0\n    components += 1 if currency else 0\n    components += 1 if year_hits >= 2 else 0\n    components += 1 if korea_refs else 0\n\n    weight = 1.5\n    score = (components / 5.0) * weight\n    return score, f\"Data/sourcing components: {components}/5 (perc={percent_count}, urls+sources={url_count+source_count}).\""}, {"type": "code", "name": "Bullet Structure Density", "description": "Estimate slide-like bullet organization by counting bullet markers.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text:\n        return 0.0\n    # Count common bullet characters at line starts\n    bullet_re = re.compile(r\"(?m)^\\s*[\\u2022\\-\\u2013\\u25E6\\u00B7\\u2219\\*\\d+\\.)]\\s+\")\n    # Fallback simple markers\n    simple_bullets = re.findall(r\"(?m)^\\s*[\\-\\u2022\\u2013]\\s+\", text)\n    count = len(simple_bullets)\n    # Heuristic scoring thresholds\n    if count >= 18:\n        return 1.0, f\"Bullet markers count ~{count}.\"\n    elif count >= 10:\n        return 0.7, f\"Bullet markers count ~{count}.\"\n    elif count >= 5:\n        return 0.4, f\"Bullet markers count ~{count}.\"\n    else:\n        return 0.0, f\"Low bullet marker count ~{count}.\""}, {"type": "code", "name": "Regulatory and Supply-side Coverage", "description": "Detect references to regulatory/compliance aspects and driver/vehicle supply levers.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text:\n        return 0.0\n    lt = text.lower()\n    regulatory_terms = ['regul', 'policy', 'license', 'licence', 'permit', 'compliance', 'molit', '\uad6d\ud1a0\uad50\ud1b5\ubd80', 'platform transport']\n    supply_terms = ['driver', 'vehicle', 'fleet', 'supply', 'onboarding', 'incentive', 'partnership', 'association']\n    has_reg = any(t in lt for t in regulatory_terms)\n    has_sup = any(t in lt for t in supply_terms)\n    if has_reg and has_sup:\n        return 0.5, \"Regulatory and supply-side both present.\"\n    if has_reg or has_sup:\n        return 0.25, \"Only one of regulatory/supply present.\"\n    return 0.0, \"No clear regulatory or supply references detected.\""}, {"type": "code", "name": "Localization Signals (Korea-specific)", "description": "Award small credit if Korean-language tokens or local entities/places are present.", "weight": 0.2, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text:\n        return 0.0\n    lt = text.lower()\n    # Hangul range\n    has_hangul = re.search(r\"[\\uAC00-\\uD7AF]\", text) is not None\n    local_terms = ['seoul', 'busan', 'gyeonggi', 'incheon', 'kakao', 't map', 'navor', 'naver', '\uce74\uce74\uc624', '\ud0dd\uc2dc', '\ubaa8\ube4c\ub9ac\ud2f0']\n    has_local = any(t in lt for t in local_terms)\n    return (0.2 if (has_hangul or has_local) else 0.0), (\"Localization signals present.\" if (has_hangul or has_local) else \"No localization signals detected.\")"}, {"type": "llm_judge", "name": "Coherence and Traceability Check", "description": "LLM verifies that challenges, initiatives, KPIs, and timeline are logically connected and that supply/regulatory constraints are reflected in recommendations.", "weight": 1.0, "judge_prompt": "Assess the document (already shape-validated) for internal coherence and traceability ONLY (not style):\n- Do identified Market Reality/Challenges link to Strategic Imperatives and corresponding initiatives in the Growth/Operational plan?\n- Are initiatives tied to H2 2024 timing (e.g., clear mapping of actions to Q3/Q4 or Aug 2024 start)?\n- Are there measurable KPIs/targets and do they align with the initiatives (e.g., driver supply KPIs, acquisition targets)?\n- Are regulatory constraints acknowledged with corresponding mitigation or engagement steps?\n- Is there a clear path from current state to Future-Proofing themes (innovation/sustainability) without contradicting resource or regulatory realities?\n\nScoring (0 to 1, then scaled by weight=1):\n- 1.0: Strong linkage across challenges \u2192 initiatives \u2192 KPIs/timeline; regulatory and supply interdependencies explicitly handled; future-proofing logically phased.\n- 0.6: Mostly linked; minor gaps or vague KPIs/timing; some nod to regulatory/supply.\n- 0.3: Weak linkage; timeline or KPIs largely missing; future-proofing disconnected.\n- 0.0: Disjointed/contradictory; no clear mapping between sections.", "expectation": "A logically connected strategy where challenges drive initiatives, which have KPIs and H2 2024 timing; regulatory/supply realities are reflected; future-proofing is phased sensibly."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality Assessment (Executive Readiness and Strategic Value)", "description": "Holistic professional quality evaluation suited for CEO/Regional Strategy discussion.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Executive Readiness and Clarity", "description": "Assess professionalism, clarity, and discussion-readiness of the slides.", "weight": 2.5, "judge_prompt": "Evaluate the deck\u2019s executive readiness and clarity (not structure, already validated):\n- Slide titles communicate the main point; bullets are concise and scannable; flow from context \u2192 plan \u2192 roadmap \u2192 wrap-up is clear.\n- Visual cleanliness and consistency (spacing, hierarchy, readable charts/tables if present). No text overload.\n- Suitable for a CEO/Regional Strategy discussion (decision-oriented framing, risks noted at high level).\n\nScoring (0\u20132.5):\n- 2.5: Highly professional, crisp, and executive-ready; easy to discuss and decide.\n- 1.8: Generally strong; minor clutter or occasional verbosity.\n- 1.0: Mixed clarity; requires refinement.\n- 0.0: Not discussion-ready; confusing or poorly presented.", "expectation": "A polished, concise, and well-structured executive presentation with clear slide titles and scannable bullets."}, {"type": "llm_judge", "name": "Strategic Depth, Prioritization, and Actionability", "description": "Assess whether the strategy demonstrates deep insight, prioritization, and actionable plans for Korea context.", "weight": 2.5, "judge_prompt": "Assess strategic depth and actionability for the Korea ride-hailing context:\n- Depth: Insights beyond obvious competitor statements; leverages local market realities (consumer behavior, urban geography, competitor strengths).\n- Prioritization: Clear sequencing and resource focus for H2 2024; trade-offs acknowledged; key bets highlighted.\n- Actionability: Specific initiatives with owners (or implied), KPIs, and decision checkpoints; risk/mitigation captured.\n- Future-proofing: Credible innovation/sustainability path tied to core growth (not disconnected moonshots).\n\nScoring (0\u20132.5):\n- 2.5: Deep, prioritized, and actionable with clear KPIs and risks; locally grounded.\n- 1.8: Solid but some generic or unprioritized elements.\n- 1.0: Shallow or generic; limited prioritization or missing KPIs/risks.\n- 0.0: No meaningful strategic depth or actionability.", "expectation": "A decisive, localized, and actionable plan with clear priorities, KPIs, and a credible future-proofing roadmap."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "3baa0009-5a60-4ae8-ae99-4955cb328ff3", "rubric": {"category_name": "Economic News Article + Chart (World Bank GEP June 2025)", "rationale": "Self-documenting rubric that forces a verifiable article-plus-chart package. Stage 1 (LLM-only) mandates exact file shape: a single article document with title and sources plus a separate JPG chart that visually covers 2024/2025/2027 World Bank global growth. Stage 2 mixes code and LLM to verify key correctness signals: word count bounds, presence of required source domains, plausible percentage values, article\u2013image cross-reference, and core-claim alignment. Stage 3 uses LLM for holistic quality: clarity, neutrality, and headline appropriateness for a US non-expert audience.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2013 Shape Enforcement Gate (LLM-only)", "description": "Verify the candidate produced the required structured outputs: a news article file (PDF/DOCX/Markdown) with title and sources, plus a separate JPG chart showing World Bank global growth for 2024, 2025, and 2027. This is a hard gate; failure zeros the category.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.0, "rules": [{"type": "llm_judge", "name": "Structured Article File Present (Title, Length, Sources)", "description": "Check that there is exactly one primary article document (PDF/DOCX/MD) with a clear headline, roughly 300\u2013500 words of body text, and a Sources section with live links to at least two of: worldbank.org, reuters.com, apnews.com.", "weight": 2.0, "judge_prompt": "You are verifying SHAPE ONLY. Inspect all candidate files. Pass if there is one article document and it is properly structured.\n\nRequirements (flexible with naming but strict on presence):\n- Format: One primary news article as a PDF, DOCX, or Markdown (.md). Not an image or spreadsheet.\n- Title/Headline: A clear headline at the top.\n- Length: Body text is approximately 300\u2013500 words (don\u2019t judge writing quality, just length range).\n- Sources section: A clearly labeled \u201cSources\u201d or \u201cReferences\u201d section at the end that includes live links (URLs) to at least two of these domains: worldbank.org, reuters.com, apnews.com. Exact link text can vary; hyperlinks or pasted URLs both count.\n- Optional but favorable: In-text mention of United States/US and China (do not score content, just presence as a structural cue).\n\nScoring (return a numeric score in [0, 2.0]):\n- 2.0: Valid article file + headline + ~300\u2013500 words + Sources section with \u22652 of the specified domains.\n- 1.5: Valid article file + headline + ~300\u2013500 words, but Sources section has only 1 specified domain or sources present but not clearly labeled.\n- 1.0: Valid article file + headline, but word count outside 300\u2013500 OR missing a Sources section entirely.\n- 0.0: No valid article document (PDF/DOCX/MD) or obviously wrong format (e.g., only an image).", "expectation": "A single well-formed article document with title, 300\u2013500 words, and a Sources section linking to at least two of worldbank.org, reuters.com, apnews.com."}, {"type": "llm_judge", "name": "JPG Chart Presence and Basic Labelling", "description": "Check that a separate JPG image file exists containing a chart that visually represents World Bank global growth for years 2024, 2025, and 2027, with visible year labels and a source credit to the World Bank (or equivalent).", "weight": 1.0, "judge_prompt": "You are verifying SHAPE ONLY for the chart. Inspect all candidate files for a separate JPG (or JPEG) image.\n\nRequirements:\n- A separate JPG/JPEG image file is included (not embedded only in the document).\n- The image is a chart/graph (bar/line/etc.).\n- The chart shows the years 2024, 2025, and 2027 explicitly labeled.\n- A source label/credit referencing the World Bank is visible on or near the chart (e.g., caption text).\n- The article text references the chart (by filename, caption, or phrases like \u201csee chart/figure below\u201d). Be flexible, but look for any clear reference.\n\nScoring (return a numeric score in [0, 1.0]):\n- 1.0: Separate JPG chart present; years 2024, 2025, 2027 are visibly labeled; World Bank source credit present; article references the chart.\n- 0.5: JPG chart present but missing one of: explicit year labels or visible source credit or article reference.\n- 0.0: No separate JPG chart present or the image is not a chart.", "expectation": "A separate JPG chart file with visible labels for 2024, 2025, 2027 and a World Bank source credit, referenced by the article."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness and Consistency Checks (Code + LLM)", "description": "Now that the outputs are in a verifiable shape, check mechanical and factual consistency: word count bounds, source domains present, chart cross-reference, plausible growth percentages, and core claim alignment with task brief.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Article\u2013Chart Cross-Reference Validity", "description": "Confirm there is at least one JPG image among outputs and the article references the chart (by filename or clear terms like \u201cchart\u201d/\u201cfigure\u201d).", "weight": 0.8, "code": "import re\nfrom pathlib import Path\n\ndef _read_article_text(context):\n    # Prefer the first document/text resource\n    article_res = None\n    for r in context.get_all_outputs():\n        if getattr(r, 'is_document', False) or getattr(r, 'is_text_format', False):\n            article_res = r\n            break\n    if not article_res:\n        pr = context.get_primary_output()\n        if pr and (getattr(pr, 'is_document', False) or getattr(pr, 'is_text_format', False)):\n            article_res = pr\n    if not article_res:\n        return None, None\n\n    text = \"\"\n    try:\n        if getattr(article_res, 'is_text_format', False):\n            text = context.files.read_text(article_res.id)\n        elif getattr(article_res, 'is_document', False):\n            # Try PDF first, then DOCX\n            try:\n                text = context.files.read_pdf_text(article_res.id)\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(article_res.id)\n                except Exception:\n                    text = \"\"\n    except Exception:\n        text = \"\"\n    return article_res, text or \"\"\n\ndef evaluate(workflow, context):\n    # Find JPG images\n    images = []\n    for r in context.get_all_outputs():\n        if getattr(r, 'is_image', False):\n            try:\n                p = context.files.get_path(r.id)\n                if p.suffix.lower() in ['.jpg', '.jpeg']:\n                    images.append((r, p))\n            except Exception:\n                continue\n\n    article_res, text = _read_article_text(context)\n    if not article_res or text is None:\n        return 0.0, \"No readable article document found.\"\n\n    text_l = text.lower()\n    score = 0.0\n\n    # Part A: Has at least one JPG\n    if images:\n        score += 0.5\n    \n    # Part B: Article references the chart\n    referenced = False\n    # Check filename mentions\n    for r, p in images:\n        if p.name.lower() in text_l:\n            referenced = True\n            break\n    # Fallback: generic reference\n    if not referenced:\n        if any(k in text_l for k in [\"chart\", \"figure\", \"exhibit\"]) and (\"world bank\" in text_l or \"global growth\" in text_l):\n            referenced = True\n    if referenced:\n        score += 0.5\n\n    return min(1.0, score), f\"images={len(images)}, referenced={referenced}\""}, {"type": "code", "name": "Word Count Compliance (300\u2013500)", "description": "Verify the article body contains approximately 300\u2013500 words.", "weight": 0.8, "code": "import re\n\ndef _read_article_text(context):\n    article_res = None\n    for r in context.get_all_outputs():\n        if getattr(r, 'is_document', False) or getattr(r, 'is_text_format', False):\n            article_res = r\n            break\n    if not article_res:\n        pr = context.get_primary_output()\n        if pr and (getattr(pr, 'is_document', False) or getattr(pr, 'is_text_format', False)):\n            article_res = pr\n    if not article_res:\n        return None\n    try:\n        if getattr(article_res, 'is_text_format', False):\n            return context.files.read_text(article_res.id)\n        elif getattr(article_res, 'is_document', False):\n            try:\n                return context.files.read_pdf_text(article_res.id)\n            except Exception:\n                try:\n                    return context.files.read_docx_text(article_res.id)\n                except Exception:\n                    return \"\"\n    except Exception:\n        return \"\"\n\ndef evaluate(workflow, context):\n    text = _read_article_text(context)\n    if not text:\n        return 0.0, \"No article text.\"\n    words = re.findall(r\"\\\\b\\\\w+\\\\b\", text)\n    wc = len(words)\n    if 300 <= wc <= 500:\n        return 1.0, f\"word_count={wc}\"\n    elif 250 <= wc <= 550:\n        return 0.6, f\"near_range word_count={wc}\"\n    else:\n        return 0.0, f\"out_of_range word_count={wc}\""}, {"type": "code", "name": "Sources Domain Presence (World Bank, Reuters, AP)", "description": "Check the article includes URLs or mentions of at least two of the specified domains: worldbank.org, reuters.com, apnews.com.", "weight": 1.2, "code": "import re\nfrom urllib.parse import urlparse\n\ndef _read_article_text(context):\n    article_res = None\n    for r in context.get_all_outputs():\n        if getattr(r, 'is_document', False) or getattr(r, 'is_text_format', False):\n            article_res = r\n            break\n    if not article_res:\n        pr = context.get_primary_output()\n        if pr and (getattr(pr, 'is_document', False) or getattr(pr, 'is_text_format', False)):\n            article_res = pr\n    if not article_res:\n        return None\n    try:\n        if getattr(article_res, 'is_text_format', False):\n            return context.files.read_text(article_res.id)\n        elif getattr(article_res, 'is_document', False):\n            try:\n                return context.files.read_pdf_text(article_res.id)\n            except Exception:\n                try:\n                    return context.files.read_docx_text(article_res.id)\n                except Exception:\n                    return \"\"\n    except Exception:\n        return \"\"\n\ndef evaluate(workflow, context):\n    text = _read_article_text(context)\n    if text is None:\n        return 0.0, \"No article text.\"\n    tl = text.lower()\n    targets = [\"worldbank.org\", \"reuters.com\", \"apnews.com\"]\n\n    # Gather domains from explicit URLs\n    found_domains = set()\n    for m in re.findall(r\"https?://[^\\s)]+\", text):\n        try:\n            net = urlparse(m).netloc.lower()\n            for t in targets:\n                if t in net:\n                    found_domains.add(t)\n        except Exception:\n            continue\n\n    # Also accept plain-text domain mentions\n    for t in targets:\n        if t in tl:\n            found_domains.add(t)\n\n    n = len(found_domains)\n    if n >= 3:\n        score = 1.0\n    elif n == 2:\n        score = 2/3\n    elif n == 1:\n        score = 1/3\n    else:\n        score = 0.0\n    return score, f\"domains_found={sorted(found_domains)}\""}, {"type": "code", "name": "Plausible Percentage Bounds", "description": "Extract percentage values in the article and ensure they are within a plausible range (-10% to +10%) to catch obvious typos (e.g., 260%).", "weight": 0.7, "code": "import re\n\ndef _read_article_text(context):\n    article_res = None\n    for r in context.get_all_outputs():\n        if getattr(r, 'is_document', False) or getattr(r, 'is_text_format', False):\n            article_res = r\n            break\n    if not article_res:\n        pr = context.get_primary_output()\n        if pr and (getattr(pr, 'is_document', False) or getattr(pr, 'is_text_format', False)):\n            article_res = pr\n    if not article_res:\n        return None\n    try:\n        if getattr(article_res, 'is_text_format', False):\n            return context.files.read_text(article_res.id)\n        elif getattr(article_res, 'is_document', False):\n            try:\n                return context.files.read_pdf_text(article_res.id)\n            except Exception:\n                try:\n                    return context.files.read_docx_text(article_res.id)\n                except Exception:\n                    return \"\"\n    except Exception:\n        return \"\"\n\ndef evaluate(workflow, context):\n    text = _read_article_text(context)\n    if text is None:\n        return 0.0, \"No article text.\"\n    nums = re.findall(r\"(-?\\d+(?:\\.\\d+)?)\\s?%\", text)\n    if not nums:\n        return 0.5, \"no percentages found\"\n    vals = []\n    for s in nums:\n        try:\n            vals.append(float(s))\n        except Exception:\n            pass\n    if not vals:\n        return 0.5, \"no parseable percentages\"\n    if any(abs(v) > 10 for v in vals):\n        return 0.0, f\"out_of_bounds={vals}\"\n    return 1.0, f\"percentages_ok={vals}\""}, {"type": "llm_judge", "name": "Core Claim and Attribution Alignment", "description": "Check the article states the World Bank (June 2025 GEP) downgraded or lowered global growth due to US\u2013China tariffs/trade tensions and that it briefly references the US and China forecasts/direction, with attribution to sources.", "weight": 1.0, "judge_prompt": "Evaluate the article content (not style) for the following factual/attribution elements. Do not browse the web; rely on the provided article and its sources list.\n\nRequirements:\n- Mentions the World Bank Global Economic Prospects (June 2025) and attributes the forecast to the World Bank.\n- States that global growth was lowered/downgraded or remains weak, and explicitly ties this to tariffs/trade tensions between the US and China.\n- Briefly references the US and China forecasts or at least the direction (e.g., slowing/softening), suitable for a short article.\n- Uses neutral, factual phrasing without unsupported claims; cites/credits World Bank, Reuters, and/or AP for the information.\n\nScoring (return a numeric score in [0, 1.0]):\n- 1.0: All elements present and clearly attributed.\n- 0.5: Missing one element (e.g., US/China mention or explicit trade-tension link) but otherwise aligned.\n- 0.0: Core claim missing or not attributed to the World Bank/June 2025 report.", "expectation": "A succinct, attributed summary that links downgraded global growth to US\u2013China tariffs, with brief US and China forecast mentions."}, {"type": "llm_judge", "name": "Chart Labels and Source Clarity (Deeper Check)", "description": "A second-pass visual check that the JPG chart clearly labels the years 2024, 2025, 2027 and includes a legible World Bank source note/caption.", "weight": 0.5, "judge_prompt": "Visually inspect the JPG chart. Confirm:\n- The chart clearly shows data or markers for the years 2024, 2025, and 2027.\n- Axes or data labels are readable enough to identify those years.\n- A source note or caption references the World Bank.\n\nScoring (return a numeric score in [0, 0.5]):\n- 0.5: All three items are clearly satisfied.\n- 0.25: Chart is present but labels or source are partially missing/unclear.\n- 0.0: Requirements not met.", "expectation": "A legible chart with explicit year labels and a World Bank source."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Quality and Audience Fit (LLM)", "description": "Professional presentation, clarity for a US non-expert audience, neutral tone, and effective headline.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity, Balance, and Readability", "description": "Assess whether the article is clear, balanced, and digestible for a US-based non-expert audience (no jargon without explanation, neutral tone, coherent flow).", "weight": 1.5, "judge_prompt": "Judge overall quality for a US non-expert audience:\n- Clarity: Plain language, minimal jargon or explained briefly.\n- Balance/Neutrality: No loaded or partisan language; even-handed presentation.\n- Structure: Logical flow with short paragraphs and transitions; reads like a professional news brief.\n\nScoring (return a numeric score in [0, 1.5]):\n- 1.5: Clear, balanced, and well-structured for non-experts.\n- 0.8: Generally understandable but with minor clarity/structure issues.\n- 0.0: Unclear, biased, or poorly structured.", "expectation": "A concise, neutral, well-structured news brief suitable for a general US audience."}, {"type": "llm_judge", "name": "Headline Accuracy and Usefulness", "description": "Evaluate whether the title/headline is specific, accurate, and non-clickbait, reflecting the World Bank downgrade/trade tensions theme.", "weight": 0.5, "judge_prompt": "Evaluate the headline:\n- Reflects the World Bank\u2019s June 2025 forecast theme (slower/downgraded global growth) and mentions or implies US\u2013China trade tensions/tariffs.\n- Accurate and non-clickbait; concise and useful.\n\nScoring (return a numeric score in [0, 0.5]):\n- 0.5: Accurate, concise, and informative headline.\n- 0.25: Partially informative but vague or missing key context.\n- 0.0: Misleading or irrelevant.", "expectation": "A precise, non-sensational headline that matches the article\u2019s core message."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "4122f866-01fa-400b-904d-fa171cdab7c7", "rubric": {"category_name": "Serverless Contact Form Backend (Terraform + Lambda + API Gateway)", "rationale": "This rubric enforces a self-documenting, file-based deliverable: a single ZIP archive containing Terraform IaC, a Node.js 18 Lambda handler, and a README. Stage 1 uses an LLM gate to verify the exact artifact shape and required files/sections, enabling deterministic Stage 2 code checks that scan the archive for key resources, integrations, and handler logic. Stage 3 assesses professional quality and security-mindedness.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Archive Shape and Required Contents (GATE)", "description": "LLM-only gate to ensure the deliverable is a single ZIP containing the exact files and structural elements needed for automated verification.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "ZIP + Required Files Present with Minimal Structure", "description": "Check that the output is a single ZIP containing required files at the root and minimal structural content to enable verification.", "weight": 4.0, "judge_prompt": "You are validating the SHAPE of a software deliverable for a serverless backend task. Only check the presence and structural placement of required files and clearly identifiable sections\u2014not correctness of code or Terraform.\n\nDeliverable must be a single ZIP archive containing these files (preferably at the root; if inside one top-level folder, that is acceptable):\n1) Terraform files: main.tf, variables.tf, outputs.tf\n2) Lambda handler: exports.js (Node.js 18)\n3) Documentation: README.md (Markdown)\n\nInside the ZIP, verify the following structural expectations (presence/finding-level checks only):\n- Terraform files appear to define AWS provider setup (a provider \"aws\" block visible somewhere), variables, IAM role/policies, SES-related resources, API Gateway REST API with a POST route at /contact-us, a deployed stage (e.g., v1), a CloudWatch log group, and outputs that include a fully qualified API URL.\n- README.md contains recognizable sections for prerequisites and setup steps. Look for mentions of: packaging the Lambda (e.g., zip exports.js.zip exports.js or similar), and Terraform commands (terraform init, fmt, validate, apply, destroy), and retrieving outputs (e.g., terraform output).\n- exports.js exists and looks like a Lambda handler file (e.g., contains a handler export and any reference to AWS SDK v3 like @aws-sdk/...). Do not judge code correctness\u2014just visible presence of these markers.\n\nScoring (map to a ratio 0.0\u20131.0, which will be multiplied by weight):\n- 1.0: ZIP present; all five files present; Terraform files, README, and exports.js each show expected high-level structures/markers.\n- 0.8: ZIP present; all five files present; one of Terraform/README/exports.js is missing some expected markers but still recognizably in place.\n- 0.6: ZIP present; at least four of the five required files present, and the Terraform files + README are plausibly structured.\n- 0.3: ZIP present; only two or three required files present, or files are present but obviously empty/minimal stubs.\n- 0.0: Not a ZIP, or ZIP missing most required files.\n\nDo not evaluate correctness, syntax, or runtime behavior\u2014only presence and structural plausibility.", "expectation": "A single ZIP archive at the root with main.tf, variables.tf, outputs.tf, exports.js, and README.md. README shows prerequisites and setup steps. Terraform shows provider block, IAM, SES, API Gateway, Lambda/logs, outputs. exports.js exists and looks like a Lambda handler using AWS SDK v3."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Automated Correctness and Integration Checks", "description": "Deterministic code rules unzip and scan files for required resources, API routes, SES configuration, and Lambda logic. Flexible substring/regex checks and partial credit.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Archive Structure: Required Filenames", "description": "Verify the ZIP contains the five required files (main.tf, variables.tf, outputs.tf, exports.js, README.md).", "weight": 1.0, "code": "def evaluate(workflow, context):\n    import re\n    from pathlib import Path\n    try:\n        from zipfile import ZipFile\n    except Exception:\n        return 0.0\n\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n\n    path = context.files.get_path(output.id)\n    p = Path(path)\n    if not str(p).lower().endswith('.zip'):\n        return 0.0\n\n    required = {'main.tf','variables.tf','outputs.tf','exports.js','readme.md'}\n    try:\n        with ZipFile(p, 'r') as z:\n            names = [n for n in z.namelist() if not n.endswith('/')]\n            basenames = set([Path(n).name.lower() for n in names])\n    except Exception:\n        return 0.0\n\n    score = 0\n    for fname in required:\n        if fname in basenames:\n            score += 1\n    ratio = score / len(required)\n    return ratio * 1.0"}, {"type": "code", "name": "README Completeness", "description": "Check README mentions prerequisites and clear setup steps including Lambda packaging and Terraform workflow.", "weight": 0.4, "code": "def evaluate(workflow, context):\n    import re\n    from pathlib import Path\n    try:\n        from zipfile import ZipFile\n    except Exception:\n        return 0.0\n\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    path = context.files.get_path(output.id)\n    if not str(path).lower().endswith('.zip'):\n        return 0.0\n\n    def read_file(z, member):\n        try:\n            with z.open(member) as f:\n                return f.read().decode('utf-8', errors='ignore')\n        except Exception:\n            return ''\n\n    try:\n        with ZipFile(path, 'r') as z:\n            readme_name = None\n            for n in z.namelist():\n                if n.lower().endswith('readme.md'):\n                    readme_name = n\n                    break\n            if not readme_name:\n                return 0.0\n            txt = read_file(z, readme_name).lower()\n    except Exception:\n        return 0.0\n\n    # Heuristics for presence\n    prereq_hits = sum(int(k in txt) for k in ['prerequisite','domain','route 53','hosted zone','email'])\n    lambda_pack_hits = sum(int(k in txt) for k in ['zip exports.js','zip exports.js.zip','zip function.zip','zip exports.js.zip exports.js','zip -r'])\n    tf_cmd_hits = sum(int(k in txt) for k in ['terraform init','terraform fmt','terraform validate','terraform apply','terraform destroy'])\n    output_hits = sum(int(k in txt) for k in ['terraform output','outputs'])\n\n    # Score components\n    score = 0.0\n    score += 0.15 if prereq_hits >= 2 else 0.0\n    score += 0.15 if lambda_pack_hits >= 1 else 0.0\n    score += 0.08 if tf_cmd_hits >= 3 else 0.0\n    score += 0.02 if output_hits >= 1 else 0.0\n\n    # Cap to weight\n    if score > 0.4:\n        score = 0.4\n    return score"}, {"type": "code", "name": "Terraform Core Resources and Variables", "description": "Static scan for provider, key variables, IAM, Lambda, CloudWatch logs, API Gateway REST with POST /contact-us and stage, and outputs for API URL.", "weight": 1.2, "code": "def evaluate(workflow, context):\n    import re\n    from pathlib import Path\n    try:\n        from zipfile import ZipFile\n    except Exception:\n        return 0.0\n\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    zpath = context.files.get_path(output.id)\n    if not str(zpath).lower().endswith('.zip'):\n        return 0.0\n\n    def read_all_tf(z):\n        txts = []\n        for n in z.namelist():\n            if n.lower().endswith('.tf'):\n                try:\n                    with z.open(n) as f:\n                        txts.append(f.read().decode('utf-8', errors='ignore'))\n                except Exception:\n                    pass\n        return '\\n'.join(txts)\n\n    try:\n        with ZipFile(zpath, 'r') as z:\n            tf = read_all_tf(z).lower()\n    except Exception:\n        return 0.0\n\n    score = 0.0\n    # Provider and variables (flexible names)\n    if 'provider \"aws\"' in tf or \"provider \\\"aws\\\"\" in tf:\n        score += 0.15\n    var_hits = sum(int(v in tf) for v in [\n        'variable \"region\"','variable \"aws_region\"','variable \"domain\"','variable \"domain_name\"',\n        'variable \"lambda_name\"','variable \"function_name\"','variable \"primary_recipient\"','variable \"admin_recipient\"',\n        'variable \"api_stage\"','variable \"stage\"','variable \"recaptcha_secret\"','variable \"tags\"'] )\n    if var_hits >= 5:\n        score += 0.15\n\n    # IAM role and policies\n    if 'resource \"aws_iam_role\"' in tf:\n        score += 0.15\n    if 'ses:sendemail' in tf or 'sesv2:sendemail' in tf:\n        score += 0.1\n    if 'logs:createloggroup' in tf or 'logs:createLogGroup'.lower() in tf:\n        score += 0.05\n    if 'logs:createlogstream' in tf or 'logs:putlogevents' in tf:\n        score += 0.05\n\n    # Lambda + log group\n    if 'resource \"aws_lambda_function\"' in tf:\n        score += 0.15\n    if 'resource \"aws_cloudwatch_log_group\"' in tf:\n        score += 0.05\n\n    # API Gateway REST with POST /contact-us and stage\n    if 'resource \"aws_api_gateway_rest_api\"' in tf:\n        score += 0.1\n    if 'resource \"aws_api_gateway_resource\"' in tf and 'contact-us' in tf:\n        score += 0.1\n    if 'resource \"aws_api_gateway_method\"' in tf and 'post' in tf:\n        score += 0.05\n    if 'resource \"aws_api_gateway_deployment\"' in tf or 'resource \"aws_api_gateway_stage\"' in tf:\n        score += 0.05\n\n    # Outputs for API URL (heuristic for execute-api or https)\n    if 'output' in tf and ('execute-api' in tf or 'https://' in tf or 'api_url' in tf):\n        score += 0.1\n\n    # Cap to weight\n    if score > 1.2:\n        score = 1.2\n    return score"}, {"type": "code", "name": "SES Configuration Presence", "description": "Static scan for SES domain identity + DKIM + MAIL FROM, recipient identities, and an SES template resource.", "weight": 0.8, "code": "def evaluate(workflow, context):\n    from pathlib import Path\n    try:\n        from zipfile import ZipFile\n    except Exception:\n        return 0.0\n\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    zpath = context.files.get_path(output.id)\n    if not str(zpath).lower().endswith('.zip'):\n        return 0.0\n\n    def read_all_tf(z):\n        txts = []\n        for n in z.namelist():\n            if n.lower().endswith('.tf'):\n                try:\n                    with z.open(n) as f:\n                        txts.append(f.read().decode('utf-8', errors='ignore'))\n                except Exception:\n                    pass\n        return '\\n'.join(txts).lower()\n\n    try:\n        with ZipFile(zpath, 'r') as z:\n            tf = read_all_tf(z)\n    except Exception:\n        return 0.0\n\n    score = 0.0\n    # Domain identity (v1) and DKIM + MAIL FROM or equivalent references\n    if 'aws_ses_domain_identity' in tf or 'ses domain identity' in tf:\n        score += 0.2\n    if 'aws_ses_domain_dkim' in tf or 'dkim' in tf:\n        score += 0.15\n    if 'aws_ses_domain_mail_from' in tf or 'mail from' in tf:\n        score += 0.15\n\n    # Recipient identities (v1 or v2) and template\n    if 'aws_ses_email_identity' in tf or 'aws_sesv2_email_identity' in tf:\n        score += 0.15\n    if 'aws_ses_template' in tf or 'aws_sesv2_template' in tf or 'ses template' in tf:\n        score += 0.15\n\n    if score > 0.8:\n        score = 0.8\n    return score"}, {"type": "code", "name": "Lambda Handler Logic (Node 18, reCAPTCHA, SES, Responses)", "description": "Scan exports.js for AWS SDK v3 usage, reCAPTCHA siteverify call, required fields, env vars, sending templated email to primary + admin, and API-style responses (200/400/500).", "weight": 1.0, "code": "def evaluate(workflow, context):\n    import re\n    from pathlib import Path\n    try:\n        from zipfile import ZipFile\n    except Exception:\n        return 0.0\n\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    zpath = context.files.get_path(output.id)\n    if not str(zpath).lower().endswith('.zip'):\n        return 0.0\n\n    def read_text(z, suffix):\n        for n in z.namelist():\n            if n.lower().endswith(suffix):\n                try:\n                    with z.open(n) as f:\n                        return f.read().decode('utf-8', errors='ignore')\n                except Exception:\n                    return ''\n        return ''\n\n    try:\n        with ZipFile(zpath, 'r') as z:\n            js = read_text(z, 'exports.js').lower()\n    except Exception:\n        return 0.0\n\n    if not js:\n        return 0.0\n\n    score = 0.0\n\n    # Handler export (cjs or esm)\n    if 'exports.handler' in js or 'module.exports' in js or 'export const handler' in js:\n        score += 0.2\n\n    # AWS SDK v3 usage\n    if \"@aws-sdk/\" in js:\n        score += 0.2\n\n    # reCAPTCHA siteverify\n    if 'www.google.com/recaptcha/api/siteverify' in js:\n        score += 0.2\n\n    # Required fields and env vars\n    required_fields = ['firstname','lastname','email','subject','message','captcha','recaptcha','token']\n    if sum(int(f in js) for f in required_fields) >= 4:\n        score += 0.15\n    env_hits = sum(int(k in js) for k in ['process.env','ses','template','recipient','admin','region','secret'])\n    if env_hits >= 3:\n        score += 0.1\n\n    # Responses with statusCode 200/400/500\n    if 'statuscode' in js and ('200' in js) and ('400' in js) and ('500' in js):\n        score += 0.15\n\n    if score > 1.0:\n        score = 1.0\n    return score"}, {"type": "code", "name": "API Gateway \u2194 Lambda Integration and Permission", "description": "Static check that API Gateway integrates with Lambda and Lambda has permission for API Gateway to invoke it; stage naming includes version (e.g., v1).", "weight": 0.6, "code": "def evaluate(workflow, context):\n    from pathlib import Path\n    try:\n        from zipfile import ZipFile\n    except Exception:\n        return 0.0\n\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    zpath = context.files.get_path(output.id)\n    if not str(zpath).lower().endswith('.zip'):\n        return 0.0\n\n    def read_all_tf(z):\n        txts = []\n        for n in z.namelist():\n            if n.lower().endswith('.tf'):\n                try:\n                    with z.open(n) as f:\n                        txts.append(f.read().decode('utf-8', errors='ignore'))\n                except Exception:\n                    pass\n        return '\\n'.join(txts).lower()\n\n    try:\n        with ZipFile(zpath, 'r') as z:\n            tf = read_all_tf(z)\n    except Exception:\n        return 0.0\n\n    score = 0.0\n    if 'resource \"aws_api_gateway_integration\"' in tf and 'lambda' in tf:\n        score += 0.25\n    if 'resource \"aws_lambda_permission\"' in tf and 'apigateway' in tf:\n        score += 0.2\n    if 'stage_name' in tf and ('v1' in tf or 'v2' in tf or 'version' in tf):\n        score += 0.15\n\n    if score > 0.6:\n        score = 0.6\n    return score"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Security Mindset", "description": "LLM judge assesses clarity, parameterization, security best practices, and production readiness based on the contents of README and code/IaC.", "is_required": false, "max_points": 1.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity, Parameterization, and Security Posture", "description": "Evaluate clarity of documentation, sensible parameterization, and security-minded choices (no hardcoded secrets, least-privilege IAM, input validation).", "weight": 1.0, "judge_prompt": "Review the ZIP\u2019s README.md, Terraform, and exports.js for overall professional quality and security-mindedness. Consider:\n- Documentation clarity: Are prerequisites and setup steps easy to follow? Are commands accurate and ordered? Are variables/placeholders clearly explained?\n- Parameterization: Are domain, recipients, stage, route, captcha secret, and tags parameterized via variables/env vars rather than hardcoded?\n- Security posture: Avoid hard-coded secrets; use environment variables and variables. IAM policies appear scoped to SES send and CloudWatch logs (not overly broad). reCAPTCHA token is validated. Inputs are checked for required fields. No obvious secrets in code.\n- Production readiness: API routes and stage naming are sensible; outputs surface the API URL; logging is configured; error handling returns clear 200/400/500 responses.\n\nScoring (map to 0.0\u20131.0):\n- 1.0: Clear, well-structured README; strong parameterization; no hardcoded secrets; least-privilege IAM; thoughtful validation and error handling.\n- 0.7: Generally solid with minor gaps (e.g., slightly broad IAM or sparse README details).\n- 0.4: Noticeable gaps (missing key steps, weak parameterization, or unclear error handling).\n- 0.1: Low clarity and limited security considerations.\n- 0.0: Lacks professional structure or contains hardcoded secrets.\n", "expectation": "Clean README with step-by-step setup, parameterized Terraform/Lambda, no hardcoded secrets, minimal IAM scope, validation and error handling consistent with production practices."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "61717508-4df7-41be-bf97-318dfb2475c0", "rubric": {"category_name": "Finance & Insurance \u2014 CSR Training Deck: Elder Financial Exploitation", "rationale": "Document-focused task producing two PDFs: a practical ~10-page training deck and a separate scenarios pack. The rubric enforces a strict, verifiable structure first (Stage 1 LLM-only gate), then verifies regulatory coverage and operational correctness with code and LLM (Stage 2), and finally assesses clarity, tone, and usability (Stage 3). The structure ensures trainees receive an immediately usable tool aligned with Senior Safe Act and FINRA Rule 2165.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (LLM ONLY)", "description": "Validate that two separate PDFs are delivered with the exact, verifiable structure needed for downstream checks. Only structure/format presence is judged here, not correctness of content.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Deck Structure: Required Sections and Format", "description": "Verify the primary training deck is a clean, practical PDF (~8\u201312 pages) with all required sections, visible headers, and quick-reference tools.", "weight": 2.5, "judge_prompt": "You are validating ONLY structure/format of the primary training deck (not the scenarios pack) based on the submitted files. Do not judge content quality or legal accuracy.\n\nConfirm the existence of ONE PDF that appears to be the training deck with ALL the following structural requirements:\n\nFormat and Length\n- PDF format (not DOCX, not PPTX). If multiple PDFs exist, identify the one that looks like the main training deck.\n- Approximately 8\u201312 pages (\u224810 pages). Accept 7\u201314 if clearly a concise deck.\n- Clear slide-style layout, readable headers, bullet lists, or visual structure.\n\nRequired Sections (flexible on titles, but headers must be visible):\n1) Quick Definition/Alignment: brief description of elder financial exploitation and abuse; emphasize subtlety/why vigilance matters.\n2) Spot the Signs (Red Flags): examples must include or closely match: unauthorized withdrawals, manipulation/coercion, sudden third-party involvement. Additional red flags are acceptable.\n3) Real-Time Call Handling: step-by-step actions usable on live calls (micro-scripts or prompts are fine): detect/verify, ask/probe, document, escalate.\n4) Legal Protections Overview (not a legal memo): Senior Safe Act (safe harbor concept) and FINRA Rule 2165 (temporary holds, trusted contact notification) summarized succinctly.\n5) Escalation & Documentation: when/how to escalate (e.g., to supervisor/compliance), what to capture in notes, when a temporary hold might apply.\n6) Quick-Reference Tooling: at least one of the following on its own slide/section: a checklist, a do/don\u2019t list, a decision flow/flowchart, or a one-page cheat sheet.\n7) Contacts/Pathways: where/how to escalate internally (team/queue/contact info or clear placeholders).\n\nScoring Guidance (structure only):\n- 2.5: PDF present; length reasonable; all 7 required sections present with visible headers; clear slide-like formatting.\n- 2.0: Missing exactly 1 required section OR length outside range but still a concise deck.\n- 1.5: Missing 2 required sections OR weak slide structure but still clearly a deck.\n- 1.0: Present but missing 3 required sections OR looks like an essay/long memo with little slide structure.\n- 0.0: No suitable PDF training deck found.\n\nReturn a score following the above. Do NOT evaluate correctness or legal quality here.", "expectation": "A single, concise PDF deck (~10 pages) with clear sections, bullets, and quick-reference tools covering definitions, red flags, live-call steps, legal overviews, and escalation."}, {"type": "llm_judge", "name": "Scenarios Pack Structure: Three Distinct Cases", "description": "Verify a separate PDF exists with three fictional mutual fund account scenarios featuring elder account holders and embedded red flags for role-play.", "weight": 1.5, "judge_prompt": "You are validating ONLY structure/format of the separate scenarios pack (not the main deck). Do not judge content quality or legal accuracy.\n\nConfirm the existence of a SECOND PDF that appears to be a scenarios/role-play packet with ALL the following structural requirements:\n\nFormat\n- PDF format.\n- Organized for role-play: distinct sections or slides per scenario.\n\nRequired Elements per Scenario (3 scenarios total, each must include these):\n- Elder account holder details: name (fictional), approximate age, and that the account involves mutual funds.\n- Red-flag storyline that could plausibly occur (e.g., niece/caregiver starts calling, sudden large redemptions, unusual wire requests, pressured tone, scripted answers).\n- Minimal guidance prompts: what to ask, when to escalate, or key decision points (not over-explained).\n\nCross-Reference\n- Each scenario should reflect at least one red-flag theme covered in the deck (e.g., unauthorized withdrawals, coercion, sudden third-party involvement). Only verify presence of such themes; do not judge quality.\n\nScoring Guidance (structure only):\n- 1.5: PDF present; clearly three distinct scenarios; each includes elder details, mutual fund context, realistic red flags, and brief prompts.\n- 1.0: PDF present; three scenarios present but one scenario is missing one element (e.g., missing age or MF context) OR only two scenarios fully formed with a partial third.\n- 0.5: PDF present but only 1\u20132 scenarios or missing multiple required elements.\n- 0.0: No scenarios PDF found.\n\nReturn a score following the above. Do NOT evaluate correctness or realism depth here.", "expectation": "A separate PDF with three clear, role-play-ready scenarios featuring elder MF account holders, realistic red flags, and brief prompts."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Coverage Verification (Mixed)", "description": "With structure enforced, verify regulatory coverage, operational readiness, and scenario completeness using code-based keyword checks plus an LLM sanity check.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Regulatory Coverage in Deck (SSA + FINRA 2165)", "description": "Checks the deck text for essential regulatory concepts and signals: Senior Safe Act, FINRA Rule 2165, safe harbor/immunity, temporary hold on disbursements/transactions, trusted contact, reasonable belief, older adult coverage.", "weight": 1.4, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef _read_doc_text(context, res):\n    text = ''\n    try:\n        if res.name and isinstance(res.name, str):\n            pass\n    except Exception:\n        pass\n    try:\n        if hasattr(res, 'is_document') and res.is_document:\n            # Prefer PDF text; fall back to DOCX\n            try:\n                text = context.files.read_pdf_text(res.id)\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(res.id)\n                except Exception:\n                    text = ''\n    except Exception:\n        text = ''\n    return text or ''\n\ndef _classify_docs(docs_texts):\n    # Return deck_idx, scenarios_idx based on keyword heuristics\n    deck_idx = None\n    scenarios_idx = None\n    # Score for deck: mentions SSA and 2165\n    deck_scores = []\n    for i, t in enumerate(docs_texts):\n        tl = t.lower()\n        score = 0\n        if 'senior safe act' in tl:\n            score += 2\n        if 'finra rule 2165' in tl or ('2165' in tl and 'finra' in tl):\n            score += 2\n        if 'temporary hold' in tl:\n            score += 1\n        if 'trusted contact' in tl:\n            score += 1\n        if 'safe harbor' in tl or 'immunity' in tl:\n            score += 1\n        deck_scores.append(score)\n    if deck_scores:\n        deck_idx = int(np.argmax(deck_scores)) if max(deck_scores) > 0 else None\n    # Scenarios likely include repeated 'scenario' or role-play cues\n    scen_scores = []\n    for i, t in enumerate(docs_texts):\n        tl = t.lower()\n        score = tl.count('scenario') + tl.count('role play') + tl.count('role-play')\n        score += tl.count('case')\n        score += tl.count('client')\n        scen_scores.append(score)\n    if scen_scores:\n        # Avoid choosing same index if possible\n        top_two = np.argsort(scen_scores)[-2:]\n        if len(top_two) == 2:\n            # Prefer the top that's not the deck\n            cand = int(top_two[-1])\n            if cand == deck_idx:\n                cand = int(top_two[-2])\n            scenarios_idx = cand\n        else:\n            scenarios_idx = int(np.argmax(scen_scores))\n    if deck_idx is not None and scenarios_idx == deck_idx:\n        scenarios_idx = None\n    return deck_idx, scenarios_idx\n\ndef evaluate(workflow, context):\n    # Identify the deck doc and check regulatory coverage keywords.\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0, 'No outputs.'\n    docs = [r for r in outputs if getattr(r, 'is_document', False)]\n    if len(docs) == 0:\n        return 0.0, 'No document outputs.'\n    texts = [_read_doc_text(context, r) for r in docs]\n    if not any(texts):\n        return 0.0, 'Unable to read document text.'\n    deck_idx, scen_idx = _classify_docs(texts)\n    if deck_idx is None:\n        # Choose the longest as fallback\n        deck_idx = int(np.argmax([len(t) for t in texts]))\n    deck_text = (texts[deck_idx] or '').lower()\n\n    checks = {\n        'senior_safe_act': ('senior safe act' in deck_text),\n        'finra_2165': ('finra rule 2165' in deck_text) or (('2165' in deck_text) and ('finra' in deck_text)),\n        'safe_harbor_immunity': ('safe harbor' in deck_text) or ('immunity' in deck_text),\n        'temporary_hold': ('temporary hold' in deck_text) or ('hold on disbursement' in deck_text) or ('hold on transactions' in deck_text) or ('disbursement hold' in deck_text),\n        'trusted_contact': ('trusted contact' in deck_text) or ('tcp' in deck_text),\n        'reasonable_belief': ('reasonable belief' in deck_text),\n        'older_adult_scope': ('65+' in deck_text) or ('age 65' in deck_text) or ('65 years' in deck_text) or ('older adult' in deck_text) or ('elder' in deck_text)\n    }\n    found = sum(1 for v in checks.values() if v)\n    total = len(checks)\n    score = (found / total) * 1.4\n    feedback = f\"Regulatory coverage hits {found}/{total}: \" + ', '.join([k for k,v in checks.items() if v])\n    return score, feedback"}, {"type": "code", "name": "Deck Operational Steps and Quick-Use Tools", "description": "Checks the deck for real-time call steps (detect/ask/document/escalate/hold), micro-scripts, and presence of at least one quick-reference tool (checklist, do/don\u2019t, flow).", "weight": 1.2, "code": "import re\nimport numpy as np\n\ndef _read_doc_text(context, res):\n    text = ''\n    try:\n        if hasattr(res, 'is_document') and res.is_document:\n            try:\n                text = context.files.read_pdf_text(res.id)\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(res.id)\n                except Exception:\n                    text = ''\n    except Exception:\n        text = ''\n    return text or ''\n\ndef _classify_deck(texts):\n    scores = []\n    for t in texts:\n        tl = t.lower()\n        score = 0\n        if 'senior safe act' in tl:\n            score += 2\n        if 'finra rule 2165' in tl or ('2165' in tl and 'finra' in tl):\n            score += 2\n        if 'temporary hold' in tl:\n            score += 1\n        scores.append(score)\n    return int(np.argmax(scores)) if scores else 0\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    docs = [r for r in outputs if getattr(r, 'is_document', False)]\n    if not docs:\n        return 0.0, 'No documents.'\n    texts = [_read_doc_text(context, r) for r in docs]\n    deck_idx = _classify_deck(texts)\n    deck = (texts[deck_idx] or '').lower()\n\n    step_checks = {\n        'detect_spot': any(k in deck for k in ['detect', 'spot the', 'recognize', 'watch for']),\n        'ask_probe': any(k in deck for k in ['ask', 'probe', 'open-ended', 'verify identity', 'verification questions']),\n        'document': any(k in deck for k in ['document', 'note in crm', 'capture notes', 'record details']),\n        'escalate': any(k in deck for k in ['escalate', 'supervisor', 'compliance', 'warm transfer', 'notify']),\n        'hold_guidance': any(k in deck for k in ['temporary hold', 'place a hold', 'disbursement hold'])\n    }\n\n    tool_checks = {\n        'checklist': 'checklist' in deck,\n        'do_dont': \"do\" in deck and \"don't\" in deck or 'do not' in deck,\n        'flowchart': 'flow' in deck or 'decision tree' in deck or 'pathway' in deck or 'escalation path' in deck\n    }\n\n    steps_found = sum(1 for v in step_checks.values() if v)\n    tools_found = sum(1 for v in tool_checks.values() if v)\n\n    step_score = (steps_found/5) * 0.9  # up to 0.9\n    tool_score = (min(tools_found,1)/1) * 0.3  # up to 0.3 for at least one tool\n    score = step_score + tool_score\n\n    feedback = f\"Steps {steps_found}/5; Tools present: {', '.join([k for k,v in tool_checks.items() if v]) or 'none'}\"\n    return score, feedback"}, {"type": "code", "name": "Scenarios Completeness and Red-Flag Variety", "description": "Verifies the scenarios PDF has 3+ cases and includes varied elder-focused red flags likely to spark role-play discussion.", "weight": 1.0, "code": "import re\nimport numpy as np\n\ndef _read_doc_text(context, res):\n    text = ''\n    try:\n        if hasattr(res, 'is_document') and res.is_document:\n            try:\n                text = context.files.read_pdf_text(res.id)\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(res.id)\n                except Exception:\n                    text = ''\n    except Exception:\n        text = ''\n    return text or ''\n\ndef _find_scenarios(text):\n    tl = text.lower()\n    # Count scenario-like headings\n    patterns = [r'\\bscenario\\s*\\d+\\b', r'\\bcase\\s*\\d+\\b', r'\\bscenario:\\b']\n    count = 0\n    for p in patterns:\n        count = max(count, len(re.findall(p, tl)))\n    # Fallback: count distinct headings with client-like cues\n    if count == 0:\n        # Look for multiple occurrences of words indicating distinct blocks\n        count = tl.count('scenario')\n    return count\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    docs = [r for r in outputs if getattr(r, 'is_document', False)]\n    if not docs:\n        return 0.0, 'No documents.'\n    # Identify scenarios doc as the one with most 'scenario/case' cues\n    texts = [_read_doc_text(context, r) for r in docs]\n    if not any(texts):\n        return 0.0, 'Cannot read documents.'\n    scen_idx = int(np.argmax([t.lower().count('scenario') + t.lower().count('case') for t in texts]))\n    scen_text = (texts[scen_idx] or '').lower()\n\n    scenario_count = _find_scenarios(scen_text)\n\n    # Red flag variety\n    red_flags = [\n        'niece', 'caregiver', 'third-party', 'third party', 'power of attorney', 'poa',\n        'beneficiary change', 'trusted contact', 'wire', 'redemption', 'withdrawal',\n        'large amount', 'unusual', 'coercion', 'pressured', 'scripted', 'confused', 'manipulation'\n    ]\n    flags_found = sum(1 for rf in red_flags if rf in scen_text)\n\n    # Elder + mutual fund cues\n    elder_cues = any(k in scen_text for k in ['age 7', 'age 8', '78', '80', '85', 'elder', 'older adult'])\n    mf_cues = 'mutual fund' in scen_text or 'fund account' in scen_text or 'mf account' in scen_text\n\n    score = 0.0\n    # Up to 0.5 for scenario count\n    if scenario_count >= 3:\n        score += 0.5\n    elif scenario_count == 2:\n        score += 0.3\n    elif scenario_count == 1:\n        score += 0.1\n\n    # Up to 0.3 for variety (>=5 unique flags)\n    if flags_found >= 5:\n        score += 0.3\n    elif flags_found >= 3:\n        score += 0.2\n    elif flags_found >= 1:\n        score += 0.1\n\n    # Up to 0.2 for context cues\n    if elder_cues:\n        score += 0.1\n    if mf_cues:\n        score += 0.1\n\n    feedback = f\"Scenarios detected: {scenario_count}; red-flag keywords found: {flags_found}; elder_cues={elder_cues}; mutual_fund_cues={mf_cues}\"\n    return min(score, 1.0), feedback"}, {"type": "llm_judge", "name": "Regulatory Sanity Check (Accuracy, Not Legal Advice)", "description": "LLM checks that the deck\u2019s brief summaries of the Senior Safe Act and FINRA Rule 2165 are broadly accurate and non-misleading at a high level (safe harbor concept; temporary holds; notification to trusted contact; reasonable belief; non-legal tone).", "weight": 0.4, "judge_prompt": "Review the training deck\u2019s sections on the Senior Safe Act and FINRA Rule 2165. Check for high-level accuracy and absence of misleading claims. This is NOT a legal review\u2014just a sanity check.\n\nConfirm:\n- Senior Safe Act: Properly conveys safe harbor/immunity for reporting suspected financial exploitation by trained employees to covered agencies; not framed as unlimited disclosure; suggests sharing only necessary info.\n- FINRA Rule 2165: Allows temporary holds on disbursements (and possibly transactions, if mentioned consistent with current guidance), for certain customers (e.g., 65+ or where impairment is suspected); mentions notifying the trusted contact and internal escalation/review; reflects time-bounded holds.\n- Tone: Practical summary, not a legal memo; no promises of guaranteed outcomes; doesn\u2019t instruct to give legal advice to customers.\n\nScoring:\n- 0.4: All items broadly correct and non-misleading.\n- 0.2: Minor omissions but not misleading.\n- 0.0: Material inaccuracies or misleading statements.", "expectation": "Brief, broadly accurate coverage of SSA safe harbor and FINRA 2165 temporary holds/notifications without legalistic overreach."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Fitness for Use (LLM)", "description": "Holistic assessment of clarity, tone, visual structure, and real-world usability for new CSR trainees on live calls.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity, Practicality, and New-Hire Readiness", "description": "Assesses whether a new CSR can use this immediately on live calls\u2014clarity of steps, simplicity, and pragmatic guidance.", "weight": 1.2, "judge_prompt": "Judge the combined materials (deck + scenarios) for practical usability by brand-new CSRs on live calls.\n\nConsider:\n- Clarity and brevity: no fluff; steps are plain-language and easy to follow under time pressure.\n- Immediate usability: micro-scripts, probing questions, what to document, how to escalate.\n- Alignment to the task context: financial services contact center, mutual fund accounts.\n- Avoids corporate/sterile tone; accessible and supportive.\n\nScoring:\n- 1.2: Exceptionally clear and immediately usable; a new hire could follow it today.\n- 0.8: Generally clear with minor gaps.\n- 0.4: Usable but cluttered or missing several practical cues.\n- 0.0: Hard to use in real time.", "expectation": "Down-to-earth, stepwise guidance with scripts/checklists geared to first live calls."}, {"type": "llm_judge", "name": "Visual Structure and Engagement", "description": "Assesses slide organization, use of color/visuals to guide attention, and overall professional polish without feeling corporate/cold.", "weight": 0.8, "judge_prompt": "Evaluate the visual presentation of the deck and scenarios packet.\n\nConsider:\n- Slide organization: clear headers, white space, readable bullets.\n- Visual cues: color, icons, or simple layouts that guide scanning.\n- Professional polish balanced with a warm, human tone.\n\nScoring:\n- 0.8: Strong visual structure and engaging design that aids scanning.\n- 0.4: Adequate but plain or slightly cluttered.\n- 0.0: Poor formatting, hard to scan or read.", "expectation": "Clean, scan-friendly slides with visual cues that help retention and engagement."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "a0552909-bc66-4a3a-8970-ee0d17b49718", "rubric": {"category_name": "Healthcare Admin \u2014 Bulk Lab Forms + Email Templates (Reach Oncology)", "rationale": "Mixed-output task: three Excel bulk forms (one per pathology lab) plus three Word email templates. Stage 1 uses an LLM gate to enforce exact deliverable shape and structural requirements so later checks are trivial. Stage 2 mixes code and LLM to verify correctness: file counts/types, required columns present, sorting by date, template content requirements, branding and dropdown validation, and filename uniqueness. Stage 3 assesses professional quality and appropriateness for a medical/PHI-adjacent workflow.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 SHAPE ENFORCEMENT GATE (Files + Structure)", "description": "MANDATORY gate. Verify the candidate produced exactly the expected artifacts with the mandated structure so verification is possible.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Output Format Requirement (3 XLSX + 3 DOCX)", "description": "Check that outputs include three separate Excel bulk forms (one per pathology lab) and three separate Word email templates (one per lab), with the exact structural elements needed for later verification.", "weight": 1.0, "judge_prompt": "You are the Stage 1 structure gate. Review ALL candidate outputs. Only evaluate structure/presence, not content quality or calculation correctness.\n\nRequired Deliverables:\nA) Three Excel workbooks (one per pathology lab) that will be emailed as bulk forms to labs. Each workbook must:\n  1) Be clearly labeled for a specific lab (in the file name and on the form header).\n  2) Display Reach Oncology branding: include the logo and the email address reach@oncologytesting.com somewhere visible on the form.\n  3) Contain a single, primary data table that includes ALL reference spreadsheet columns plus these additional columns (exact or very close naming accepted):\n     - Order Received\n     - Delayed At Another Facility\n     - Did Not Receive Request\n     - Date Shipped\n     - Additional Notes\n  4) Provide data validation/drop-downs for the three status columns: Order Received, Delayed At Another Facility, Did Not Receive Request, with allowed values Yes, No, N/A. (If the dropdown UI is not obviously visible, a clear, explicit in-sheet note stating validation is applied counts.)\n  5) Use the same color scheme/theme as the reference spreadsheet (be flexible: if the reference is not visible, accept an explicitly stated and consistently applied theme across the three forms).\n  6) Ensure the data table is sorted by the request sent date in ascending order (earliest first). The column may be named like Request Sent, Request Sent Date, or similar.\n\nB) Three Word email templates (one per pathology lab). Each template must:\n  1) Include an appropriate subject line.\n  2) Be addressed to the lab by name.\n  3) Request the current status of recent tissue requests for the patients listed in the attached bulk form.\n  4) Note that the team will follow up weekly.\n  5) Instruct the lab to return the completed form via email.\n\nScoring (STRUCTURE ONLY):\n- 1.0: All 6 files exist (3 XLSX + 3 DOCX). Each Excel form has branding (logo + email), added columns, visible/clearly-declared dropdowns for the 3 status columns, consistent theme, sorted-by-request-sent date, and lab labeling in both filename and header. Each email template has subject, addressee (lab), status request, weekly follow-up note, and return-by-email instruction.\n- 0.8: All 6 files exist and are lab-specific; missing exactly one minor structural item across the set (e.g., only implicit/declared dropdowns or minor theme uncertainty), with everything else present.\n- 0.6: All 6 files exist; missing one major Excel structural item OR one major email structural item (e.g., added columns missing in one form, or an email missing subject/addressee) but otherwise sound.\n- 0.3: Some but not all required files are present (e.g., only 2 Excel forms or 2 email templates) OR multiple major structural elements missing.\n- 0.0: Wrong formats (e.g., not XLSX/DOCX) OR forms/templates largely missing.\n\nDo not judge calculation accuracy or writing quality; only verify presence/visibility of the required structure and elements.", "expectation": "Three lab-specific Excel bulk forms with branding, added columns, dropdowns, consistent theme, and sorted-by-request-sent date; plus three lab-specific Word email templates with subject, addressee, status request, weekly follow-up note, and return-by-email instruction."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness + Consistency)", "description": "Now that the structure is present, verify correctness and consistency using code and LLM checks.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Files Present (3 XLSX + 3 DOCX)", "description": "Verify at least three Excel workbooks (.xlsx) and three Word docs (.docx) exist among outputs.", "weight": 0.5, "code": "import re, json, pandas as pd, numpy as np\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs() or []\n    excels, docs = [], []\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            suffix = p.suffix.lower()\n        except Exception:\n            suffix = ''\n        try:\n            if getattr(r, 'is_spreadsheet', False) and suffix in ['.xlsx', '.xlsm', '.xls']:\n                excels.append(r)\n            if getattr(r, 'is_document', False) and suffix in ['.docx', '.doc']:\n                docs.append(r)\n        except Exception:\n            continue\n    nx, nd = len(excels), len(docs)\n    fx = min(nx, 3) / 3.0\n    fd = min(nd, 3) / 3.0\n    frac = 0.5 * fx + 0.5 * fd\n    feedback = f\"Detected {nx} Excel workbooks and {nd} Word documents.\"\n    return frac, feedback"}, {"type": "code", "name": "Excel: Required Added Columns Present", "description": "Check each Excel bulk form contains the five added columns (case/spacing-flexible).", "weight": 0.9, "code": "import re, json, pandas as pd, numpy as np\n\nREQ_COLS = [\n    'order received',\n    'delayed at another facility',\n    'did not receive request',\n    'date shipped',\n    'additional notes'\n]\n\ndef _norm(s):\n    return re.sub(r'\\s+', ' ', str(s).strip().lower())\n\ndef _has_req_columns(cols):\n    cols_n = [_norm(c) for c in cols]\n    hits = {req: any(req in c for c in cols_n) for req in REQ_COLS}\n    return hits\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs() or []\n    excel_resources = []\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            if getattr(r, 'is_spreadsheet', False) and p.suffix.lower() in ['.xlsx', '.xlsm', '.xls']:\n                excel_resources.append((r, p))\n        except Exception:\n            continue\n\n    if not excel_resources:\n        return 0.0, 'No Excel workbooks detected.'\n\n    per_file_scores = []\n    details = []\n    for r, p in excel_resources[:5]:  # sample up to 5 in case of extras\n        try:\n            xl = pd.ExcelFile(p)\n            sheet_name = xl.sheet_names[0]\n            df = xl.parse(sheet_name=sheet_name)\n            hits = _has_req_columns(df.columns)\n            missing = [k for k, v in hits.items() if not v]\n            score = 1.0 if len(missing) == 0 else max(0.0, 1.0 - (len(missing) / len(REQ_COLS)))\n            per_file_scores.append(score)\n            if missing:\n                details.append(f\"{p.name}: missing {', '.join(missing)}\")\n            else:\n                details.append(f\"{p.name}: all required added columns present\")\n        except Exception as e:\n            per_file_scores.append(0.0)\n            details.append(f\"{p.name}: error reading ({e})\")\n\n    if not per_file_scores:\n        return 0.0, 'No readable Excel sheets.'\n\n    avg_score = float(np.mean(per_file_scores))\n    feedback = '; '.join(details)\n    return avg_score, feedback"}, {"type": "code", "name": "Excel: Sorted by Request Sent Date (Earliest First)", "description": "Detect a 'request sent' date column and verify rows are in ascending order by that column (ignoring blanks).", "weight": 0.6, "code": "import re, json, pandas as pd, numpy as np\n\ndef _norm(s):\n    return re.sub(r'\\s+', ' ', str(s).strip().lower())\n\ndef _find_request_sent_col(cols):\n    cols_n = [_norm(c) for c in cols]\n    # Prefer names containing 'request' and 'sent'\n    for i, c in enumerate(cols_n):\n        if 'request' in c and ('sent' in c or 'send' in c):\n            return i\n    # Fallbacks\n    for i, c in enumerate(cols_n):\n        if 'request sent date' in c or c.endswith('request sent') or ('request' in c and 'date' in c):\n            return i\n    return None\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs() or []\n    excel_resources = []\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            if getattr(r, 'is_spreadsheet', False) and p.suffix.lower() in ['.xlsx', '.xlsm', '.xls']:\n                excel_resources.append((r, p))\n        except Exception:\n            continue\n\n    if not excel_resources:\n        return 0.0, 'No Excel workbooks detected.'\n\n    results = []\n    msgs = []\n    for r, p in excel_resources[:5]:\n        try:\n            xl = pd.ExcelFile(p)\n            sheet_name = xl.sheet_names[0]\n            df = xl.parse(sheet_name=sheet_name)\n            idx = _find_request_sent_col(df.columns)\n            if idx is None:\n                results.append(0.0)\n                msgs.append(f\"{p.name}: request sent date column not found\")\n                continue\n            col = df.columns[idx]\n            dt = pd.to_datetime(df[col], errors='coerce')\n            # Consider only non-null rows in their original order\n            valid = dt.dropna()\n            if valid.empty:\n                # No dates to sort; give neutral partial credit\n                results.append(0.5)\n                msgs.append(f\"{p.name}: request sent date present but all/most values blank\")\n                continue\n            is_sorted = valid.is_monotonic_increasing\n            results.append(1.0 if is_sorted else 0.0)\n            msgs.append(f\"{p.name}: {'sorted' if is_sorted else 'not sorted'} by request sent date\")\n        except Exception as e:\n            results.append(0.0)\n            msgs.append(f\"{p.name}: error reading ({e})\")\n\n    avg = float(np.mean(results)) if results else 0.0\n    return avg, '; '.join(msgs)"}, {"type": "code", "name": "DOCX Templates: Required Content Present", "description": "Each email template includes subject line, addressee (lab), status request, weekly follow-up note, and return-by-email instruction.", "weight": 0.9, "code": "import re, json, pandas as pd, numpy as np\n\ndef _score_doc(text, fname):\n    t = ' '.join(text.split())\n    tl = t.lower()\n    checks = {}\n    checks['subject'] = bool(re.search(r'\\bsubject\\s*[:\\-]', tl)) or ('subject:' in tl)\n    checks['addressed'] = ('dear ' in tl) or bool(re.search(r'\\bto\\b.*lab', tl)) or ('attention' in tl)\n    checks['status_request'] = ('current status' in tl) or bool(re.search(r'status of .*request', tl))\n    checks['attachment_ref'] = ('attached' in tl) and (('bulk form' in tl) or ('form' in tl) or ('spreadsheet' in tl))\n    checks['weekly_followup'] = bool(re.search(r'\\bweekly\\b|\\beach week\\b|\\bevery week\\b', tl))\n    checks['return_by_email'] = bool(re.search(r'\\breturn\\b.*(completed|filled).*\\b(form|spreadsheet)\\b.*\\b(e[- ]?mail|email)\\b', tl)) or ('please email the completed form' in tl)\n    score = np.mean(list(checks.values())) if checks else 0.0\n    missing = [k for k, v in checks.items() if not v]\n    msg = f\"{fname}: {'ok' if score==1.0 else 'missing ' + ', '.join(missing) if missing else 'partial'}\"\n    return float(score), msg\n\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs() or []\n    docs = []\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            if getattr(r, 'is_document', False) and p.suffix.lower() in ['.docx', '.doc']:\n                docs.append((r, p))\n        except Exception:\n            continue\n\n    if not docs:\n        return 0.0, 'No Word templates detected.'\n\n    scores, msgs = [] , []\n    for r, p in docs[:5]:\n        try:\n            text = context.files.read_docx_text(r.id)\n            sc, msg = _score_doc(text, p.name)\n            scores.append(sc)\n            msgs.append(msg)\n        except Exception as e:\n            scores.append(0.0)\n            msgs.append(f\"{p.name}: error reading ({e})\")\n\n    avg = float(np.mean(scores)) if scores else 0.0\n    return avg, '; '.join(msgs)"}, {"type": "code", "name": "Filenames Are Lab-Labeled and Unique", "description": "Check that there are at least three uniquely named Excel files and three uniquely named DOCX files (suggesting per-lab labeling).", "weight": 0.2, "code": "import re, json, pandas as pd, numpy as np\nfrom pathlib import Path\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs() or []\n    excel_stems, docx_stems = [], []\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            if getattr(r, 'is_spreadsheet', False) and p.suffix.lower() in ['.xlsx', '.xlsm', '.xls']:\n                excel_stems.append(p.stem.lower())\n            if getattr(r, 'is_document', False) and p.suffix.lower() in ['.docx', '.doc']:\n                docx_stems.append(p.stem.lower())\n        except Exception:\n            continue\n\n    ex_unique = len(set(excel_stems))\n    doc_unique = len(set(docx_stems))\n    ex_score = min(ex_unique, 3) / 3.0 if excel_stems else 0.0\n    doc_score = min(doc_unique, 3) / 3.0 if docx_stems else 0.0\n    score = 0.5 * ex_score + 0.5 * doc_score\n    fb = f\"Unique Excel names: {ex_unique}; Unique DOCX names: {doc_unique}.\"\n    return score, fb"}, {"type": "llm_judge", "name": "Excel Visuals: Logo, Email, Dropdowns, Color Theme, Sorting (Sanity)", "description": "LLM visual verification: branding appears on each form, dropdowns exist for specified columns, consistent color scheme across forms, and tables appear sorted by request sent date.", "weight": 0.6, "judge_prompt": "Visually inspect the three Excel bulk forms.\nCheck:\n1) Branding present: Reach Oncology logo AND the email reach@oncologytesting.com visible on each form.\n2) Data validation/drop-downs for the three status columns (Order Received, Delayed At Another Facility, Did Not Receive Request) with values Yes/No/N/A. Accept clear, explicit in-sheet notes if dropdown arrows are not visible.\n3) Consistent color scheme/theme across the three forms (and, if the reference theme is not visible, treat a coherent, professional, consistent scheme as acceptable).\n4) Tables appear sorted by the request sent date ascending (earliest first). Use column names like Request Sent, Request Sent Date, etc.\n\nScoring:\n- 1.0: All four checks pass for all forms.\n- 0.7: Exactly one check is weak/uncertain across the set (e.g., dropdowns are declared but not visibly obvious, or minor theme uncertainty), others pass.\n- 0.4: Two checks weak/uncertain or one clearly missing.\n- 0.2: Multiple checks missing/uncertain on most forms.\n- 0.0: Branding or structure clearly absent across forms.\n\nOnly verify visible presence/consistency, not content quality.", "expectation": "Branding (logo + email) visible on each form; dropdowns present or explicitly declared; coherent theme; tables look sorted by request-sent date."}, {"type": "llm_judge", "name": "Cross-Consistency: Lab Names in Forms vs Emails", "description": "Confirm each email template references the same lab name as the corresponding Excel bulk form and that pairing is obvious via filenames or headers.", "weight": 0.3, "judge_prompt": "Match each Excel bulk form to a Word email template by lab name (use filenames and visible headers). Verify that each email addresses the same lab named on the corresponding form.\n\nScoring:\n- 1.0: Clear 1:1 pairing for all three labs; lab names match between each form and its email.\n- 0.5: Pairing is mostly clear but one mapping is ambiguous or slightly inconsistent.\n- 0.0: Pairing is unclear or mismatched for multiple labs.", "expectation": "One email per lab, lab names clearly consistent with corresponding Excel form filenames and headers."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality", "description": "Holistic assessment of clarity, professionalism, and appropriateness for a medical context.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Tone, Clarity, and Usability", "description": "Evaluate whether the emails and forms are professional, clear, and ready for colleagues to use with pathology labs.", "weight": 1.0, "judge_prompt": "Assess the overall professional quality of the deliverables:\n- Email templates: clear, concise, polite, and appropriate for pathology labs; effective subject lines; clear instructions; includes weekly follow-up note; avoids unnecessary patient details in the email body; contact info (reach@oncologytesting.com) visible.\n- Forms: clean layout, readable headers, consistent styling; easy for labs to fill; added columns are understandable; lab labeling is obvious.\n\nScoring:\n- 1.0: Highly professional and immediately usable by colleagues; excellent clarity and tone.\n- 0.7: Generally professional with minor phrasing or usability issues.\n- 0.4: Noticeable issues with clarity, tone, or usability but still serviceable.\n- 0.2: Poorly written or confusing; significant revisions needed.\n- 0.0: Unprofessional or unusable.", "expectation": "Polished, lab-ready forms and emails that colleagues can immediately use without edits."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "4d61a19a-8438-4d4c-9fc2-cf167e36dcd6", "rubric": {"category_name": "Promotion Projection Form & Training - Retail Meat Operations", "rationale": "Mixed-output task: a structured Excel template plus a concise training PowerPoint. Stage 1 uses LLM-only gates to enforce exact, verifiable shapes for both files. Stage 2 performs code-based checks on the Excel to validate columns, data types, and template behavior, plus an LLM cross-file consistency check. Stage 3 provides a light, holistic quality assessment for professional clarity and audience fit. This follows the self-documenting philosophy: mandate structure first, then verify correctness, then assess quality.", "max_total_score": 10.0, "stages": [{"name": "Shape Enforcement Gate \u2013 Excel Template", "description": "LLM-only gate to confirm the Excel workbook exists and has the required structure for store participation and regional control.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.5, "rules": [{"type": "llm_judge", "name": "Excel Template Structural Requirements", "description": "Verify the presence and structure of the 'Promotion Projection Form Template' Excel file with required sheets, fields, and clear store-editable areas.", "weight": 4.0, "judge_prompt": "You are evaluating whether the candidate outputs include a properly structured Excel template for promotions.\n\nCheck ALL outputs and verify the following for the Excel template:\n\nFORMAT AND IDENTITY\n- There is an Excel workbook (.xlsx) that serves as the template titled \u201cPromotion Projection Form Template\u201d or a very close title.\n\nREQUIRED SHEETS AND CONTENT\n1) Main form sheet (name can vary: 'Promotion Projections', 'Form', 'Promotion List', 'Promotions', 'Store Entry', etc.) with a single tabular area and a visible header row that includes at least these columns:\n   - Promotion Start Date\n   - Promotion End Date\n   - Product Name (or Item)\n   - SKU/PLU (or UPC/Item ID)\n   - Regular Price\n   - Promo Price (or Sale/Ad Price)\n   - Merchandising Notes (or Notes)\n   - Store Projected Units (this is store-editable)\n   - Store Sign-Off (Name and/or Date \u2013 store-editable)\n\n2) Instructions sheet (name can vary: 'Instructions', 'Read Me', 'How to Use') that clearly states:\n   - Purpose of the form and that stores ONLY edit the 'Store Projected Units' and 'Store Sign-Off' fields\n   - Where the form will be found (SharePoint folder named 'Promotions')\n   - Brief process for how Regional fills the basics, then stores review and submit\n\nOPTIONAL (nice-to-have):\n- A 'History' or 'Past Promotions' section/sheet with prior promo dates and units for context.\n\nVISUAL CUES\n- The store-editable fields are clearly indicated (e.g., shaded cells, a legend, \u201c[Store Entry]\u201d label, or explicit instruction callouts). Locking/protection cannot be verified here; visual indication suffices.\n\nSCORING (return a score 0\u20134 based on completeness of the above):\n- 4.0: Valid .xlsx with main form sheet having all required columns; an instructions sheet that mentions SharePoint 'Promotions' and clearly states ONLY the two store fields are editable; and store-editable fields are clearly marked.\n- 3.0: Valid .xlsx with main form sheet having all required columns and an instructions sheet, but visual marking is weak OR instructions lack one specificity (editable fields or SharePoint folder mention).\n- 2.0: Valid .xlsx but either missing the instructions sheet or missing 1\u20132 of the required columns on the main form.\n- 1.0: Excel workbook exists but strongly deviates from required structure (e.g., no clear main form table).\n- 0.0: No Excel workbook present or wrong format.\n\nOnly assess structure/format and presence, not calculation correctness.", "expectation": "A clean .xlsx with a main form table including required columns, a clear Instructions sheet, and visually indicated store-editable fields; SharePoint 'Promotions' location noted."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Shape Enforcement Gate \u2013 PowerPoint Training Deck", "description": "LLM-only gate to confirm the training deck exists, stays under 8 slides, and covers the required topics including a sample form.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.5, "rules": [{"type": "llm_judge", "name": "PowerPoint Deck Structural Requirements", "description": "Verify the presence and structure of the 'Promo Projection Form' training deck with required topical coverage and a sample of the form.", "weight": 3.0, "judge_prompt": "Evaluate whether the outputs include a PowerPoint (.pptx) training deck for the new Promo Projection Form.\n\nREQUIREMENTS\n- File is a PowerPoint (.pptx).\n- Total slides are under 8 (i.e., 7 or fewer).\n- Content coverage (must be present across slides, titles can vary):\n  1) What the tool is and why we\u2019re using it (ties to store feedback on promo quantities)\n  2) How stores get the form and where to find it (explicitly mentions SharePoint folder 'Promotions')\n  3) What sections stores must fill out (ONLY 'Store Projected Units' and 'Store Sign-Off')\n  4) Process flow overview (Regional fills basics \u2192 stores review and submit \u2192 timing/approvals)\n  5) A sample of the form with mock data (screenshot or embedded table reflecting the template\u2019s headers/fields)\n  6) Recap and a Q&A/Discussion slide\n\nSCORING (return a score 0\u20133):\n- 3.0: Valid .pptx, \u22647 slides, and all 6 content bullets are clearly covered, including a recognizable sample of the form with mock data.\n- 2.0: Valid .pptx, \u22647 slides, but missing one of the content bullets OR the sample is present but not clearly tied to the template.\n- 1.0: Valid .pptx but >7 slides OR missing multiple content bullets.\n- 0.0: No PowerPoint deck present.", "expectation": "A concise \u22647-slide deck that orients Meat Team Leaders to the tool, location (SharePoint 'Promotions'), what they edit, how the process works, and shows a realistic sample, ending with recap/Q&A."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Verification \u2013 Excel Content and Behavior Checks", "description": "Code-based and LLM checks leveraging the enforced shape to verify column presence, data plausibility, and cross-file consistency.", "is_required": false, "max_points": 2.5, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Excel Template \u2013 Required Columns & Sheets (Fuzzy Match)", "description": "Programmatically locate the Excel template, confirm an Instructions-like sheet, find the main table, and verify fuzzy presence of required columns plus optional history and SharePoint mentions.", "weight": 1.4, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    weight = 1.4\n\n    def clamp(x):\n        return max(0.0, min(weight, x))\n\n    # Find Excel resource\n    try:\n        outputs = context.get_all_outputs()\n    except Exception:\n        return 0.0\n    excels = [r for r in outputs if getattr(r, 'is_spreadsheet', False)]\n    if not excels:\n        return 0.0, 'No Excel file found.'\n\n    def name_score(p):\n        s = 0\n        n = p.name.lower()\n        for kw in ['promotion', 'projection', 'promo', 'template']:\n            if kw in n:\n                s += 1\n        return s\n\n    best = None\n    best_s = -1\n    for r in excels:\n        try:\n            path = context.files.get_path(r.id)\n            sc = name_score(path)\n        except Exception:\n            sc = 0\n        if sc > best_s:\n            best = r\n            best_s = sc\n    if best is None:\n        best = excels[0]\n\n    # Load workbook\n    try:\n        xpath = context.files.get_path(best.id)\n        xls = pd.ExcelFile(xpath)\n        sheet_names = [str(s) for s in xls.sheet_names]\n    except Exception as e:\n        return 0.0, f'Failed to open Excel: {e}'\n\n    sheet_names_l = [s.lower() for s in sheet_names]\n\n    # Detect Instructions-like sheet\n    instr_hits = {'instruction','instructions','read me','readme','how to','guide'}\n    has_instr_sheet = any(any(tok in s for tok in instr_hits) for s in sheet_names_l)\n\n    # Parse all sheets into DataFrames\n    sheets = {}\n    for sn in sheet_names:\n        try:\n            sheets[sn] = pd.read_excel(xpath, sheet_name=sn)\n        except Exception:\n            sheets[sn] = pd.DataFrame()\n\n    # Flatten text to search for SharePoint/Promotions mention\n    def sheet_text(df: pd.DataFrame):\n        try:\n            vals = df.astype(str).values.ravel().tolist()\n            txt = ' '.join([v for v in vals if v and v != 'nan'])\n            return txt.lower()\n        except Exception:\n            return ''\n\n    combined_text = ' '.join([sheet_text(df) for df in sheets.values()])\n    mentions_sharepoint = ('sharepoint' in combined_text)\n    mentions_promotions_folder = ('promotions' in combined_text)\n\n    # Find main table sheet by max header matches\n    req_groups = {\n        'start_date': ['promotion start date','start date','promo start'],\n        'end_date': ['promotion end date','end date','promo end'],\n        'product': ['product','product name','item','item name','cut'],\n        'sku': ['sku','plu','upc','item id','code'],\n        'reg_price': ['regular price','reg price','base price','list price'],\n        'promo_price': ['promo price','promotion price','sale price','ad price'],\n        'merch_notes': ['merchandising notes','merch notes','merchandising','display notes','notes'],\n        'projected_units': ['store projected units','projected units','store projection','units projected','store units'],\n        'signoff': ['store sign-off','sign off','sign-off','approval','approved by','manager signature','signature','sign'],\n    }\n    history_groups = {\n        'hist_units': ['last promo units','prior promo units','previous promo units','ly units','last units'],\n        'hist_dates': ['last promo start date','last promo end date','prior promo start','previous promo start','last promo start','last promo end'],\n    }\n\n    def norm_cols(df):\n        return [str(c).strip().lower() for c in df.columns]\n\n    def col_present(cols, keys):\n        return any(any(k in c for k in keys) for c in cols)\n\n    best_main = None\n    best_hits = -1\n    best_cols = []\n    for sn, df in sheets.items():\n        if df is None or df.empty:\n            continue\n        cols = norm_cols(df)\n        hits = 0\n        for g in req_groups.values():\n            hits += 1 if col_present(cols, g) else 0\n        if hits > best_hits:\n            best_hits = hits\n            best_main = (sn, df)\n            best_cols = cols\n\n    required_count = len(req_groups)\n    req_coverage = best_hits/required_count if required_count else 0.0\n\n    # History presence on any sheet\n    hist_present = False\n    for df in sheets.values():\n        if df is None or df.empty:\n            continue\n        cols = norm_cols(df)\n        if col_present(cols, history_groups['hist_units']) or col_present(cols, history_groups['hist_dates']):\n            hist_present = True\n            break\n\n    # Scoring\n    score = 0.0\n    details = []\n\n    # Instructions + SharePoint/Promotions mention (up to 0.4)\n    instr_score = 0.0\n    if has_instr_sheet:\n        instr_score += 0.25\n    if mentions_sharepoint and mentions_promotions_folder:\n        instr_score += 0.15\n    elif mentions_sharepoint or mentions_promotions_folder:\n        instr_score += 0.1\n    details.append(f\"Instructions sheet: {'yes' if has_instr_sheet else 'no'}; SharePoint/Promotions mention: {'yes' if (mentions_sharepoint or mentions_promotions_folder) else 'no'}\")\n    score += instr_score\n\n    # Main table coverage (up to 0.8)\n    main_cov = min(1.0, req_coverage)\n    score += 0.8 * main_cov\n    details.append(f\"Main table required column coverage: {best_hits}/{required_count}\")\n\n    # History presence (0.2)\n    if hist_present:\n        score += 0.2\n        details.append('History columns present: yes')\n    else:\n        details.append('History columns present: no')\n\n    return clamp(score), '; '.join(details)"}, {"type": "code", "name": "Excel Template \u2013 Data Plausibility & Editability Cues", "description": "Check date/price plausibility, ensure projected units are blank (as a template), and confirm instructions clarify edit-only fields and SharePoint location.", "weight": 0.8, "code": "import re\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\ndef evaluate(workflow, context):\n    weight = 0.8\n    def clamp(x):\n        return max(0.0, min(weight, x))\n\n    outputs = context.get_all_outputs()\n    excels = [r for r in outputs if getattr(r, 'is_spreadsheet', False)]\n    if not excels:\n        return 0.0, 'No Excel file found.'\n\n    def pick_excel(resources):\n        best = None\n        best_s = -1\n        for r in resources:\n            try:\n                p = context.files.get_path(r.id)\n                n = p.name.lower()\n            except Exception:\n                n = ''\n            s = 0\n            for kw in ['promotion','projection','promo','template']:\n                if kw in n:\n                    s += 1\n            if s > best_s:\n                best = r\n                best_s = s\n        return best or resources[0]\n\n    xres = pick_excel(excels)\n    path = context.files.get_path(xres.id)\n\n    try:\n        xls = pd.ExcelFile(path)\n        sheets = {sn: pd.read_excel(path, sheet_name=sn) for sn in xls.sheet_names}\n    except Exception as e:\n        return 0.0, f'Failed to open Excel: {e}'\n\n    # Helper functions\n    def ncols(df):\n        return [str(c).strip().lower() for c in df.columns]\n\n    def find_col(cols, options):\n        for i, c in enumerate(cols):\n            for o in options:\n                if o in c:\n                    return i\n        return None\n\n    # Locate main sheet by most matches\n    groups = {\n        'start': ['promotion start date','start date','promo start'],\n        'end': ['promotion end date','end date','promo end'],\n        'reg': ['regular price','reg price','base price','list price'],\n        'sale': ['promo price','promotion price','sale price','ad price'],\n        'proj': ['store projected units','projected units','store projection','units projected','store units'],\n        'sign': ['store sign-off','sign off','sign-off','approval','approved by','manager signature','signature','sign'],\n    }\n\n    best_sn = None\n    best_hits = -1\n    best_df = None\n    best_cols = []\n    for sn, df in sheets.items():\n        if df is None or df.empty:\n            continue\n        cols = ncols(df)\n        hits = sum(1 for g in groups.values() if any(opt in ' '.join(cols) for opt in g))\n        if hits > best_hits:\n            best_hits = hits\n            best_sn = sn\n            best_df = df\n            best_cols = cols\n\n    if best_df is None or best_df.empty:\n        return 0.0, 'No suitable main table found.'\n\n    # Date parseability score (0.2)\n    date_score = 0.0\n    si = find_col(best_cols, groups['start'])\n    ei = find_col(best_cols, groups['end'])\n    parsed_frac = 0.0\n    if si is not None and ei is not None:\n        sd = pd.to_datetime(best_df.iloc[:, si], errors='coerce')\n        ed = pd.to_datetime(best_df.iloc[:, ei], errors='coerce')\n        mask = (~sd.isna()) | (~ed.isna())\n        denom = max(1, int(mask.sum()))\n        ok = ((~sd.isna()) & (~ed.isna())).sum()\n        parsed_frac = ok/denom\n        date_score = 0.2 * parsed_frac\n    # Price plausibility (0.25)\n    price_score = 0.0\n    ri = find_col(best_cols, groups['reg'])\n    si2 = find_col(best_cols, groups['sale'])\n    if ri is not None and si2 is not None:\n        try:\n            reg = pd.to_numeric(best_df.iloc[:, ri], errors='coerce')\n            sale = pd.to_numeric(best_df.iloc[:, si2], errors='coerce')\n            mask = (~reg.isna()) & (~sale.isna())\n            denom = int(mask.sum())\n            if denom > 0:\n                ok = (sale <= reg).sum()\n                price_score = 0.25 * (ok/denom)\n        except Exception:\n            price_score = 0.0\n\n    # Projected units should be blank in template (0.25)\n    proj_score = 0.0\n    pi = find_col(best_cols, groups['proj'])\n    if pi is not None:\n        col = best_df.iloc[:, pi].astype(str)\n        # Consider blank if NaN or empty/whitespace\n        is_blank = col.isna() | (col.str.strip().replace({'nan':'', 'NaT':''}) == '')\n        frac_blank = is_blank.mean() if len(col) else 1.0\n        proj_score = 0.25 * frac_blank\n\n    # Instruction clarity and location mention (0.2 total)\n    # Search across all sheets' text\n    def flatten_text(dfs):\n        parts = []\n        for df in dfs.values():\n            try:\n                parts += [str(x) for x in df.values.ravel().tolist()]\n            except Exception:\n                continue\n        return ' '.join(parts).lower()\n    text = flatten_text(sheets)\n    edit_clarity = (('only' in text) and ('store' in text) and (('projected units' in text) or ('sign' in text)))\n    sharepoint_loc = (('sharepoint' in text) and ('promotions' in text))\n    info_score = 0.2 * ((0.5 if edit_clarity else 0.0) + (0.5 if sharepoint_loc else 0.0))\n\n    total = date_score + price_score + proj_score + info_score\n    feedback = f\"Date parse score={date_score:.2f}; Price plausibility score={price_score:.2f}; Projected units blank score={proj_score:.2f}; Info score={info_score:.2f} (edit clarity={edit_clarity}, sharepoint/promotions={sharepoint_loc}) on sheet '{best_sn}'.\"\n    return clamp(total), feedback"}, {"type": "llm_judge", "name": "Cross-File Consistency \u2013 Deck Sample vs. Template", "description": "The sample form shown in the deck should reflect the key columns/fields from the Excel template, especially the two store-editable fields.", "weight": 0.3, "judge_prompt": "Compare the PowerPoint deck\u2019s sample form (mock data slide) against the Excel template\u2019s main form headers. Judge if the sample visibly mirrors the template structure and includes the two store-editable fields.\n\nCHECKS\n- The sample shown in the deck includes column headers or a screenshot that closely matches the template\u2019s main headers (e.g., Product, SKU/PLU, Start/End Dates, Regular Price, Promo Price, Merchandising Notes).\n- It explicitly shows or calls out the two store-editable fields: 'Store Projected Units' and 'Store Sign-Off' (name/date or equivalent).\n\nSCORING (0\u20130.3):\n- 0.3: Sample clearly mirrors template headers AND clearly shows the two store-editable fields.\n- 0.2: Sample mirrors most template headers but the store-editable fields are not clearly visible or labeled.\n- 0.1: Sample is present but looks generic or mismatched to template.\n- 0.0: No recognizable sample of the form in the deck.", "expectation": "A realistic sample slide that looks like the provided Excel template and highlights the store-editable fields."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Quality Assessment \u2013 Professionalism and Audience Fit", "description": "LLM rates clarity, professionalism, and appropriateness for Meat Team Leaders; ensures brevity and process clarity.", "is_required": false, "max_points": 0.5, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Clarity and Fit", "description": "Holistic quality check for tone, clarity, process explanation, and usability for store leaders.", "weight": 0.5, "judge_prompt": "Assess overall professional quality and audience fit for the Excel template and the training deck:\n\nCONSIDER\n- Clarity and brevity (deck remains \u22647 slides; Excel is not cluttered)\n- Professional presentation (readable headers, consistent labeling, sensible ordering)\n- Process clarity (how Regional fills, how stores submit, timing/approvals)\n- Audience appropriateness (Meat Team Leaders in grocery retail; clear, actionable, not overly technical)\n\nSCORING (0\u20130.5):\n- 0.5: Clear, concise, professional; process is easy to follow; design is appropriate and helpful for Meat Team Leaders.\n- 0.3: Generally good but with minor clarity or design issues.\n- 0.1: Usable but with noticeable gaps in clarity or professionalism.\n- 0.0: Unclear or poorly presented.", "expectation": "A crisp, professional template and concise deck that Meat Team Leaders can use immediately with minimal confusion."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "ec2fccc9-b7f6-4c73-bf51-896fdb433cec", "rubric": {"category_name": "SEO Blog Copy: NFT Photography Intro Guide", "rationale": "This rubric mandates a verifiable, self-documenting DOCX artifact with explicit structural elements (title, subheading, H2/H3, hyperlinks, artist highlights with links, pull quote caption, secondary keyword list, and a closing \"what's next\" section). Stage 1 is an LLM-only gate that enforces the exact structure, enabling deterministic Stage 2 checks. Stage 2 mixes code rules (bounds/consistency) with an LLM coverage check. Stage 3 judges overall quality for tone, readability, SEO polish, and professional presentation.", "max_total_score": 15.0, "stages": [{"name": "Stage 1 \u2013 Shape Enforcement Gate (LLM)", "description": "Verify the candidate delivered a DOCX with the exact, verifiable structure required for downstream checks. LLM-only, flexible but strict on presence/placement.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Document Format and Structural Requirements", "description": "Check the final output is a single Word document (DOCX) with all required structural elements, headers, sections, formatting, and lists present.", "weight": 4.0, "judge_prompt": "You are validating STRUCTURE ONLY (not content quality). Inspect the candidate's primary output. Score strictly per these requirements:\n\nFormat Requirements:\n- Must be a DOCX file (not PDF/Markdown/Excel/Plain text).\n- Must have the exact title: \"What is NFT Photography? An Introductory Guide\" as the main document title.\n- Immediately below the title, include a clear subheading (one sentence or short paragraph).\n\nSection and Heading Structure:\n- Use H2 and H3 headers to break up the text.\n  - At least 4 H2 sections.\n  - At least 2 H3 subsections somewhere in the document.\n- Include a distinct section that highlights multiple travel photographers with NFT collections (e.g., a H2/H3 like \"Featured Travel NFT Photographers\" or similar) where each artist mention includes a hyperlink to a collection or social profile.\n\nFormatting and Links:\n- Bold and italic text must be used within paragraph text (not only in headers). Identify at least one instance of each.\n- Include hyperlinks throughout with SEO-friendly anchors (descriptive text, not \"click here\").\n  - At least 5 total hyperlinks.\n  - At least 3 links to reputable news/educational sources about NFTs/photography/crypto.\n  - At least 3 links to artist collections or social profiles (these can be in the artist highlight section). Overlap is allowed as long as both categories are satisfied.\n- Mention \"SuperRare\" at least once (a high-end NFT platform) somewhere in the document.\n\nLength and Placement:\n- Main article body should be approximately 1,500 words with 10% leeway (i.e., 1,350\u20131,650 words). Exclude the secondary keyword list and the pull quote caption from this count.\n- After the main article body, include a clearly labeled list of \"Secondary Keywords\" (or similar) with exactly four related keywords to also optimize for (not including the primary keyword \"NFT photography\"). A simple bullet list is acceptable.\n- At the very bottom, include a caption indicating the chosen pull quote (e.g., a labeled line: \"Pull Quote: \\\"...\\\"\").\n\nClosing Requirement:\n- Include a closing or final section that explicitly explains what's coming next: a deeper exploration of NFT photography with artist interviews and practical demonstrations around minting NFTs for photographers (use wording to this effect).\n\nScoring (STRUCTURE ONLY):\n- 4.0: DOCX and all required structural elements present (title + subheading; H2/H3 counts met; bold & italics in body; sufficient hyperlinks with both categories satisfied; artist highlight section with linked artists; SuperRare mentioned; body length within 1,350\u20131,650 excluding lists/captions; secondary keywords list with exactly 4 items; pull quote caption at bottom; explicit \"what\u2019s next\" closing).\n- 3.5: Only one minor structural element missing or slightly short (e.g., H3 count off by 1, or bold/italic used but only one identified, or links meet count but one category off by 1).\n- 3.0: Several minor issues or one moderate miss (e.g., missing artist highlight section OR secondary keyword list not exactly 4 items, but DOCX, title, H2/H3 usage, and closing are present).\n- 2.0: DOCX present but multiple core structural elements are missing (e.g., missing H2/H3 structure, no links/anchors, no pull quote caption, no secondary keywords list, or no closing \"what\u2019s next\").\n- 0.0: Not a DOCX or grossly fails structure (e.g., missing title, no sections, no links, no formatting).\n\nOnly check PRESENCE and STRUCTURE, not content correctness or writing quality.", "expectation": "A DOCX with the mandated title, subheading, H2/H3 structure, visible bold/italics in body text, sufficient links (news and artist profiles/collections) with SEO-friendly anchors, an artist highlight section, SuperRare mention, 1,350\u20131,650 words of body text, a four-item secondary keyword list after the article, a bottom pull quote caption, and a closing \"what\u2019s next\" section about interviews and minting demos."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification (Mixed Code + LLM)", "description": "Now that the shape is enforced, verify correctness and compliance details: bounds checks, keyword placement, lists, closing, platform mentions, and coverage of required topics/links.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "File Type + Word Count Bounds", "description": "Confirm document type and that the main body is within 1,350\u20131,650 words (attempt to exclude the secondary keyword list and pull quote caption).", "weight": 2.0, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output:\\n        return 0.0, 'No primary output.'\\n\\n    if not output.is_document:\\n        return 0.0, 'Output is not a document.'\\n\\n    # Read text from DOCX or PDF\\n    text = ''\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_text(output.id)\\n            except Exception:\\n                return 0.0, 'Unable to read document text.'\\n\\n    if not text or not text.strip():\\n        return 0.0, 'Empty document text.'\\n\\n    # Attempt to exclude secondary keywords section and pull quote caption from word count\\n    lower = text.lower()\\n    cutoff_idx = len(text)\\n    m = re.search(r\"(?im)^(secondary\\\\s+key\\\\w*).*\", text)\\n    if m:\\n        cutoff_idx = min(cutoff_idx, m.start())\\n    m2 = re.search(r\"(?i)pull\\\\s*-?\\\\s*quote\", text)\\n    if m2:\\n        cutoff_idx = min(cutoff_idx, m2.start())\\n    main_body = text[:cutoff_idx]\\n\\n    # Word count (basic heuristic)\\n    words = re.findall(r\"[A-Za-z0-9][A-Za-z0-9\\-'\u2019]*\", main_body)\\n    wc = len(words)\\n\\n    # Scoring: full if within 1350-1650; partial if within 1200-1800\\n    if 1350 <= wc <= 1650:\\n        return 1.0, f'Word count OK: {wc}.'\\n    elif 1200 <= wc <= 1800:\\n        return 0.5, f'Word count near bounds: {wc}.'\\n    else:\\n        return 0.0, f'Word count out of bounds: {wc}.'"}, {"type": "code", "name": "Primary Keyword Presence and Placement", "description": "Check that the primary keyword \"NFT photography\" appears enough times and near the beginning.", "weight": 1.0, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output:\\n        return 0.0, 'No primary output.'\\n    if not output.is_document:\\n        return 0.0, 'Output is not a document.'\\n\\n    text = ''\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_text(output.id)\\n            except Exception:\\n                return 0.0, 'Unable to read document text.'\\n\\n    if not text:\\n        return 0.0, 'Empty text.'\\n\\n    count = len(re.findall(r\"(?i)\\bnft photography\\b\", text))\\n    opening = text[:200].lower() if len(text) >= 200 else text.lower()\\n    in_opening = ('nft photography' in opening)\\n\\n    if count >= 5 and in_opening:\\n        return 1.0, f'Primary keyword count={count}, appears in opening.'\\n    elif count >= 3:\\n        return 0.6, f'Primary keyword count={count}, opening={in_opening}.'\\n    elif count >= 1:\\n        return 0.3, f'Primary keyword present {count} time(s).'\\n    else:\\n        return 0.0, 'Primary keyword not found.'"}, {"type": "code", "name": "Secondary Keywords Listing (Exactly Four)", "description": "Verify a clearly labeled list of four secondary keywords appears after the article body.", "weight": 1.0, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output:\\n        return 0.0, 'No primary output.'\\n    if not output.is_document:\\n        return 0.0, 'Output is not a document.'\\n\\n    # Read text\\n    text = ''\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_text(output.id)\\n            except Exception:\\n                return 0.0, 'Unable to read text.'\\n\\n    if not text:\\n        return 0.0, 'Empty text.'\\n\\n    # Find the header line containing Secondary Keywords\\n    m = re.search(r\"(?im)^(secondary\\s+key\\w*s?)\\s*:?.*$\", text)\\n    keywords = []\\n    if m:\\n        start = m.end()\\n        following = text[start:]\\n        lines = following.splitlines()\\n        # consume until blank line or up to 10 lines\\n        for line in lines[:10]:\\n            if not line.strip():\\n                break\\n            l = line.strip()\\n            # bullet/numbered list or comma-separated on same line\\n            if re.match(r\"^\\s*([\\-*\u2022\\u2022\\d]+[\\).\\-:]\\s+|[\\-*\u2022\\u2022]\\s+)\", l):\\n                # strip bullet\\n                item = re.sub(r\"^\\s*([\\-*\u2022\\u2022\\d]+[\\).\\-:]\\s+|[\\-*\u2022\\u2022]\\s+)\", '', l).strip()\\n                if item:\\n                    keywords.append(item)\\n            elif ',' in l and len(keywords) == 0:\\n                # possibly all on one line after header\\n                parts = [p.strip() for p in l.split(',') if p.strip()]\\n                keywords.extend(parts)\\n            else:\\n                # plain line keyword\\n                if l:\\n                    keywords.append(l)\\n        # clean and normalize\\n        cleaned = []\\n        for k in keywords:\\n            k2 = re.sub(r\"^[\\-\\*\u2022\\u2022\\s]+\", '', k).strip().strip('\"\\'')\\n            if k2:\\n                cleaned.append(k2)\n        # remove primary keyword if present\n        cleaned2 = []\n        for k in cleaned:\n            if re.search(r\"(?i)\\bnft photography\\b\", k):\n                continue\n            cleaned2.append(k)\n        # deduplicate preserving order\n        seen = set()\n        final = []\n        for k in cleaned2:\n            kl = k.lower()\n            if kl not in seen:\n                seen.add(kl)\n                final.append(k)\n        n = len(final)\n        if n == 4:\n            return 1.0, 'Found exactly four secondary keywords: ' + '; '.join(final)\n        elif n in (3,5):\n            return 0.6, f'Found {n} secondary keywords: ' + '; '.join(final)\n        else:\n            return 0.0, f'Found {n} secondary keywords: ' + '; '.join(final)\n    else:\n        return 0.0, 'No clearly labeled Secondary Keywords section found.'"}, {"type": "code", "name": "Pull Quote Caption Present Near Bottom", "description": "Detect a labeled pull quote caption near the bottom of the document.", "weight": 0.5, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output:\\n        return 0.0, 'No primary output.'\\n    if not output.is_document:\\n        return 0.0, 'Output is not a document.'\\n\\n    # Read text\\n    text = ''\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_text(output.id)\\n            except Exception:\\n                return 0.0, 'Unable to read text.'\\n\\n    if not text:\\n        return 0.0, 'Empty text.'\\n\\n    tail = text[-1200:] if len(text) > 1200 else text\\n    # Look for label and quoted content\n    m = re.search(r\"(?is)pull\\s*-?\\s*quote\\s*:?[\\s\\n]*([\\\"\\'\u201c\u2018].+?[\\\"\\'\u201d\u2019])\", tail)\n    if m:\n        quote = m.group(1)\n        # Require a minimum length to avoid trivial quotes\n        if len(quote) >= 20:\n            return 1.0, 'Pull quote found near bottom.'\n        else:\n            return 0.6, 'Pull quote label found but quote short.'\n    # Fallback: label present without captured quotes\n    if re.search(r\"(?i)pull\\s*-?\\s*quote\", tail):\n        return 0.6, 'Pull quote label found near bottom.'\n    return 0.0, 'No pull quote caption detected near bottom.'"}, {"type": "code", "name": "SuperRare Mention", "description": "Verify that SuperRare is mentioned at least once.", "weight": 0.5, "code": "def evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output:\\n        return 0.0, 'No primary output.'\\n    if not output.is_document:\\n        return 0.0, 'Output is not a document.'\\n\\n    text = ''\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_text(output.id)\\n            except Exception:\\n                return 0.0, 'Unable to read text.'\\n\\n    if not text:\\n        return 0.0, 'Empty text.'\\n\\n    if 'superrare' in text.lower():\\n        return 1.0, 'SuperRare mentioned.'\\n    return 0.0, 'No SuperRare mention found.'"}, {"type": "code", "name": "\"What\u2019s Next\" Closing with Interviews and Minting", "description": "Check for an explicit closing that previews next steps (artist interviews and minting demos), preferably near the end.", "weight": 1.0, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output:\\n        return 0.0, 'No primary output.'\\n    if not output.is_document:\\n        return 0.0, 'Output is not a document.'\\n\\n    text = ''\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_text(output.id)\\n            except Exception:\\n                return 0.0, 'Unable to read text.'\\n\\n    if not text:\\n        return 0.0, 'Empty text.'\\n\\n    L = len(text)\\n    tail = text[int(L*0.75):] if L > 0 else text\\n    has_intro = re.search(r\"(?i)(what'?s\\s+next|coming\\s+next|next\\s+up)\", tail) is not None\\n    has_interviews = re.search(r\"(?i)artist(s)?\\s+interview(s)?\", text) is not None\\n    has_mint = re.search(r\"(?i)mint(ing)?\", text) is not None\\n\\n    if has_intro and has_interviews and has_mint:\\n        return 1.0, 'Closing previews next steps (interviews + minting).'\n    elif has_intro and (has_interviews or has_mint):\n        return 0.6, 'Closing present but missing one of the required preview elements.'\n    elif has_interviews and has_mint:\n        return 0.3, 'Mentions interviews and minting, but no explicit \"what\\'s next\" phrasing near the end.'\n    else:\n        return 0.0, 'No adequate closing preview detected.'"}, {"type": "llm_judge", "name": "Content Coverage and Link Compliance", "description": "LLM verifies that the piece actually covers required concepts and includes links and artist highlights as specified.", "weight": 2.0, "judge_prompt": "Check the candidate DOCX for the following CONTENT coverage (not just structure):\\n\\n1) Beginner-friendly definition: Clearly explains what NFT photography is in simple, non-technical terms (avoid heavy jargon).\\n2) How NFT photographers make money: Mentions at least three mechanisms (e.g., primary sales/mints, secondary royalties, editions/open editions, licensing/CC0, token-gated perks, patronage, collaborations).\\n3) Why people buy photo NFTs: At least three reasons (e.g., provenance and ownership, collecting/supporting artists, community/access/utility, rarity/editions, curation/storytelling).\\n4) Travel NFT photographers highlighted: At least three travel photographers are named and linked (collection pages or social profiles) as examples.\\n5) News/educational links: At least three links to reputable news/education sources are included, using SEO-friendly anchor text (not generic anchors like \"click here\").\\n6) Subheading present under the title; H2 and H3 headings used to organize the content; bold and italics used within paragraph text.\\n\\nScoring:\n- 2.0: All 6 categories met.\\n- 1.4: 5/6 categories met.\\n- 0.8: 4/6 categories met.\\n- 0.3: 3/6 categories met or content is superficial/unclear.\\n- 0.0: Fewer than 3 categories satisfied.\n\nFocus on coverage completeness and link/anchor correctness; do not assess writing quality here.", "expectation": "A friendly, beginner-level explanation; clear monetization paths; reasons buyers collect; at least three highlighted travel photographers with links; at least three reputable news/education links with descriptive anchors; subheading, H2/H3, bold/italics present."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Quality and SEO Effectiveness", "description": "Holistic assessment of writing quality, tone, SEO polish, scannability, and professional presentation.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Quality, Tone, and SEO", "description": "Judge overall effectiveness for the intended non-web3-native audience and SEO execution.", "weight": 3.0, "judge_prompt": "Evaluate the article holistically for quality and suitability for a non-web3-native audience of a curated travel photography app moving into NFTs. Consider:\\n- Tone: Friendly, conversational, encouraging; avoids unnecessary jargon; explains terms briefly when used.\\n- Structure and scannability: Logical flow, clear H2/H3 hierarchy, short paragraphs, helpful subheads, and skimmable formatting.\\n- SEO execution: Natural integration of the primary keyword (\"NFT photography\") and secondary keywords (no stuffing), descriptive anchors for links, relevant internal/external linking, and a clear title/subheading.\\n- Brand/context fit: Aligns with a curated travel photography platform; highlights travel photographer examples appropriately; mentions SuperRare in a way consistent with a high-end gallery approach.\\n- Mechanics: Grammar, punctuation, clarity, and absence of contradictions.\n\nScoring:\n- 3.0: Exceptional polish and alignment; would publish as-is.\\n- 2.2: Strong overall; minor edits needed.\\n- 1.5: Adequate but uneven; several edits needed.\\n- 0.8: Weak; significant revision required.\\n- 0.0: Not publishable.\n\nProvide a brief rationale for your score.", "expectation": "Publication-ready, user-friendly, and SEO-aware article that aligns with a curated travel photography brand and effectively introduces NFT photography to beginners."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "9efbcd35-186d-49b6-ac24-28ee2bc9a263", "rubric": {"category_name": "EM Q1 2025 Outlook Document (Institutional Client Services)", "rationale": "This rubric enforces a self-documenting workflow for a short institutional outlook document. Stage 1 (LLM-only) strictly mandates a DOCX/PDF with specific sections, a performance table, and a sources/methodology section, capped at four pages. Stage 2 uses code rules to verify the presence of required sources, time-bounded references, and plausibility signals (Q1 2025 mentions with percentages, index-labeled performance). Stage 3 is an LLM quality review for professional tone, client relevance, and clarity. The structure ensures the agent proves correctness by producing verifiable artifacts.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (Document Structure and Format)", "description": "LLM-only structural validation of the delivered document. Confirms correct file type (DOCX or PDF), page-length cap (<= 4 pages), and presence of mandated sections and a performance table that enables verification in later stages. No calculation/content checking here.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Document Format and Structural Completeness", "description": "Check that the output is a properly formatted DOCX or PDF with required sections, within page limit, and includes a performance table for Q1 2025 enabling verification. Only evaluate structure/presence, not content quality or correctness.", "weight": 4.0, "judge_prompt": "You are evaluating ONLY the STRUCTURE and FORMAT of the candidate output. Do not judge content quality or calculation correctness.\n\nPass criteria and scoring are determined by the presence of a DOCX/PDF file no longer than 4 pages and the following sections and elements:\n\nFormat requirements:\n- File type: PDF or DOCX (not plain text, not Excel)\n- Length: No more than 4 pages\n- Professionally organized with clear headings\n\nRequired sections (flexible on exact names, but they must be clearly identifiable as headings):\n1) Executive Summary or Overview (brief summary of Q1 2025 EM performance and key drivers)\n2) Q1 2025 Performance Snapshot (must include a TABLE with columns similar to: [Index Name | Q1 2025 Return (%) | Currency/Base | Source])\n   - The table should reference MSCI indices (e.g., MSCI Emerging Markets, MSCI China, MSCI India, MSCI Brazil, CEEMEA, EM Tech/Information Technology). Naming can vary but must be clearly index-level.\n3) Macro Drivers or Drivers of Performance (umbrella section that introduces drivers)\n   Subsections under Macro Drivers (all required):\n   - China\n   - India\n   - Brazil\n   - Technology Sector (e.g., Technology/Information Technology/Semiconductors)\n   - CEEMEA (allow variants like CEEMEA/CEE+EMEA/Eastern Europe & Middle East & Africa)\n   - General Macro Landscape (overall EM macro environment)\n4) Methodology and Sources (must list MSCI as a performance source and news sources; flexible naming like \"Sources\", \"Methodology & Sources\", or \"Data & Sources\")\n\nOptional but encouraged (do NOT penalize if missing):\n- Outlook and Risks / Client Implications\n- Disclaimers / Not investment advice\n\nScoring (STRUCTURE ONLY):\n- 1.0: Valid DOCX/PDF, <= 4 pages, clear headings, includes the required sections above and a performance TABLE referencing MSCI indices with the indicated columns (or clear equivalents). Methodology/Sources section present.\n- 0.8: Valid format and page length; all required sections present but performance table is present with slightly different column labeling or missing one minor column (e.g., Currency/Base or Source), while still obviously a Q1 2025 index-level performance table.\n- 0.6: Valid format and page length; missing exactly one required subsection (e.g., one of China/India/Brazil/Technology/CEEMEA/Macro), or the performance table is unclear/partially structured but still present.\n- 0.4: Valid format and page length; missing two required subsections or missing a clearly structured performance table, but other core sections exist.\n- 0.0: Wrong format (not DOCX/PDF), exceeds 4 pages, or missing multiple required sections such that verification is not possible.\n\nBe flexible on section titles (e.g., \"Overview\" vs \"Executive Summary\"; \"Technology\" vs \"Information Technology\"). Only assess structure/presence and format, not accuracy or writing quality.", "expectation": "A <=4-page DOCX/PDF with an Executive Summary, a Q1 2025 Performance Snapshot table referencing MSCI indices, a Macro Drivers section with subsections for China, India, Brazil, Technology, CEEMEA, and General Macro Landscape, and a Methodology & Sources section explicitly listing MSCI and news sources."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Sources, Time-Bound References, and Plausibility)", "description": "Code-based checks leveraging the enforced structure: verify presence of MSCI and news sources, Q1 2025 time-bounded mentions with percentages, index-labeled performance signals, and explicit date cutoff reference. These checks do not validate numerical correctness, only consistency and plausibility.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Section Coverage Verification (China, India, Brazil, Technology, CEEMEA, Macro)", "description": "Verify that the text contains coverage for the six required subsections via flexible keyword matches. Returns fractional score based on how many are present.", "weight": 1.2, "code": "import re\\n\\n\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n    text = \"\"\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_text(output.id)\\n            except Exception:\\n                return 0.0\\n    if not text:\\n        return 0.0\\n    t = text.lower()\\n\\n    def has_ceemea(tt: str) -> bool:\\n        if 'ceemea' in tt or 'cee mea' in tt:\\n            return True\\n        if 'emea' in tt and ('cee' in tt or 'central and eastern europe' in tt):\\n            return True\\n        if ('eastern europe' in tt and ('middle east' in tt or 'africa' in tt)):\\n            return True\\n        # Accept EMEA mention as a fallback for CEEMEA if other EM context exists\\n        if 'emea' in tt and ('em ' in tt or 'emerging market' in tt or 'emerging-markets' in tt):\\n            return True\\n        return False\\n\\n    has_china = 'china' in t\\n    has_india = 'india' in t\\n    has_brazil = 'brazil' in t\\n    has_tech = any(k in t for k in ['technology', 'information technology', 'tech ', 'tech-', 'semiconductor', 'semiconductors'])\\n    has_macro = ('macro' in t) or any(k in t for k in ['macro environment', 'macro outlook', 'macro landscape'])\\n    has_ce = has_ceemea(t)\\n\\n    present = sum([has_china, has_india, has_brazil, has_tech, has_ce, has_macro])\\n    return present / 6.0\\n"}, {"type": "code", "name": "MSCI Source Presence", "description": "Check that MSCI is cited as a performance source (e.g., 'MSCI', 'msci.com', 'MSCI Indexes').", "weight": 0.6, "code": "import re\\n\\n\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n    text = ''\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_text(output.id)\\n            except Exception:\\n                return 0.0\\n    if not text:\\n        return 0.0\\n    t = text.lower()\\n    has_msci_domain = 'msci.com' in t\\n    has_msci_word = 'msci' in t\\n    has_index_context = bool(re.search(r'msci[^\\n\\r]{0,80}index', t)) or 'indexes' in t or 'indices' in t\\n    if has_msci_domain or (has_msci_word and has_index_context):\\n        return 1.0\\n    return 0.0\\n"}, {"type": "code", "name": "News Sources Presence (WSJ and FT)", "description": "Check for presence of at least one of Wall Street Journal/WSJ and Financial Times/FT. Partial credit if only one present; full if both are present.", "weight": 0.6, "code": "import re\\n\\n\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n    text = ''\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_text(output.id)\\n            except Exception:\\n                return 0.0\\n    if not text:\\n        return 0.0\\n    t = text.lower()\\n    has_wsj = ('wall street journal' in t) or bool(re.search(r'\\bwsj\\b', t))\\n    has_ft = ('financial times' in t) or ('ft.com' in t) or bool(re.search(r'\\bft\\b', t))\\n    if has_wsj and has_ft:\\n        return 1.0\\n    if has_wsj or has_ft:\\n        return 0.5\\n    return 0.0\\n"}, {"type": "code", "name": "Q1 2025 Performance Mentions with Percentages", "description": "Detect references to Q1 2025 alongside percentage returns. Looks for phrases like 'Q1 2025', '1Q25', 'first quarter 2025' within proximity to a percentage.", "weight": 0.8, "code": "import re\\n\\n\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n    text = ''\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_text(output.id)\\n            except Exception:\\n                return 0.0\\n    if not text:\\n        return 0.0\\n\\n    t = text.lower()\\n    patterns = [\\n        'q1 2025', 'q1-2025', '1q25', '1q 2025', 'first quarter 2025',\\n        'q1 of 2025', 'quarter ended march 31, 2025', 'quarter ending march 31, 2025'\\n    ]\\n    percent_re = re.compile(r'-?\\d{1,2}(?:\\.\\d+)?\\s*%')\\n    found = False\\n    for p in patterns:\\n        for m in re.finditer(re.escape(p), t):\\n            start = max(0, m.start() - 120)\\n            end = min(len(t), m.end() + 120)\\n            window = t[start:end]\\n            if percent_re.search(window):\\n                found = True\\n                break\\n        if found:\\n            break\\n    return 1.0 if found else 0.0\\n"}, {"type": "code", "name": "Explicit Date Cutoff Mention (through/as of March 31, 2025)", "description": "Check for an explicit statement indicating data is current through or as of March 31, 2025 (or equivalent formats).", "weight": 0.4, "code": "import re\\n\\n\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n    text = ''\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_text(output.id)\\n            except Exception:\\n                return 0.0\\n    if not text:\\n        return 0.0\\n    t = text.lower()\\n    date_patterns = [\\n        r'as of\\s+march\\s+31,\\s*2025',\\n        r'as at\\s+31\\s+march\\s+2025',\\n        r'through\\s+march\\s+31,\\s*2025',\\n        r'thru\\s+march\\s+31,\\s*2025',\\n        r'31\\s+march\\s+2025',\\n        r'03/31/2025', r'31/03/2025'\\n    ]\\n    for dp in date_patterns:\\n        if re.search(dp, t):\\n            return 1.0\\n    return 0.0\\n"}, {"type": "code", "name": "Index-Labeled Percentages (MSCI EM/China/India/Brazil/CEEMEA/Tech)", "description": "Heuristic check that at least three index/segment names (e.g., MSCI EM, MSCI China, MSCI India, MSCI Brazil, CEEMEA, Tech/Information Technology) appear near percentage figures, indicating a structured performance snapshot.", "weight": 0.4, "code": "import re\\n\\n\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n    text = ''\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_text(output.id)\\n            except Exception:\\n                return 0.0\\n    if not text:\\n        return 0.0\\n    t = text.lower()\\n    labels = [\\n        'msci emerging markets', 'msci em', 'msci china', 'msci india', 'msci brazil',\\n        'ceemea', 'emea', 'information technology', 'technology', 'tech'\\n    ]\\n    percent_re = re.compile(r'-?\\d{1,2}(?:\\.\\d+)?\\s*%')\\n    matched = set()\\n    for lbl in labels:\\n        for m in re.finditer(re.escape(lbl), t):\\n            start = max(0, m.start() - 100)\\n            end = min(len(t), m.end() + 100)\\n            window = t[start:end]\\n            if percent_re.search(window):\\n                matched.add(lbl)\\n                break\\n    count = len(matched)\\n    # Require at least three distinct label-percentage pairings for full credit\\n    if count >= 3:\\n        return 1.0\\n    if count == 2:\\n        return 0.67\\n    if count == 1:\\n        return 0.33\\n    return 0.0\\n"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Client Relevance", "description": "LLM qualitative assessment of clarity, professionalism, investor relevance, and utility for institutional client retention. Checks tone, coherence, actionable insights, and appropriateness for an institutional audience. Does not re-check structure or strict sourcing already handled in prior stages.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Strategic Value", "description": "Evaluate writing quality, clarity, client relevance, and strategic usefulness. Ensure the narrative is balanced, coherent, and suitable for institutional clients considering EM allocation decisions.", "weight": 2.0, "judge_prompt": "Evaluate the candidate document for overall professional quality and client relevance. Do NOT re-check format/structure already assessed in Stage 1, nor exact source mentions already checked in Stage 2. Focus on clarity, coherence, and usefulness for institutional clients.\n\nConsider:\n- Clarity and organization: Is the narrative easy to follow with concise summaries of Q1 2025 EM performance and macro drivers?\n- Investor relevance: Does it help institutional clients understand drivers (e.g., China, India, Brazil, Technology, CEEMEA, Macro) and implications for EM positioning and risk?\n- Balance and nuance: Are both headwinds and tailwinds discussed without overpromising? Are uncertainties and risks acknowledged?\n- Actionable insight and framing: Does it provide a practical outlook or scenarios that inform allocation conversations (without giving explicit investment advice)?\n- Professional tone and polish: Suitable for institutional readership; jargon used appropriately; no glaring typos; charts/tables (if any) referenced cleanly.\n\nScoring:\n- 1.0: Strong professional quality; highly readable; balanced and insightful; clearly helps client discussions and retention efforts.\n- 0.7: Generally solid; minor gaps in balance or specificity; still useful for clients.\n- 0.4: Adequate but generic; limited insight specific to EM Q1 2025; may be less persuasive.\n- 0.1: Poor clarity or tone; not suitable for institutional clients.\n- 0.0: Incoherent or severely unprofessional.\n\nOnly evaluate quality and usefulness, not strict structural compliance.", "expectation": "A polished, balanced, and client-relevant institutional brief that synthesizes Q1 2025 EM dynamics into a clear narrative with practical implications."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "60221cd0-686e-4a08-985e-d9bb2fa18501", "rubric": {"category_name": "Virginia Election Article (Journalism) \u2014 Self-Documenting Staged Rubric", "rationale": "Pattern B (Document). Deliverable is a single PDF article intended for the general public. Stage 1 uses an LLM-only structural gate to force a verifiable shape: PDF format, headline, distinct content for dates and how-to-vote, link placement, no images, target word count. Stage 2 mixes code and LLM to verify measurable correctness: word count bounds, URL placement, gubernatorial focus, presence of 2025 dates (June\u2013Nov), and voter participation methods; plus an LLM neutrality check. Stage 3 applies an LLM quality assessment for clarity, organization, and newsroom polish.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Format and Structural Gate (PDF News Article Template)", "description": "LLM-only gate ensuring the output has the exact verifiable structure for downstream checks. Do NOT assess factual correctness or writing quality here\u2014only structure/format presence.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Document Requirement (PDF Article)", "description": "Check the candidate output strictly for format and required structural elements.", "weight": 4.0, "judge_prompt": "You are evaluating a SINGLE deliverable produced by an agent. Only check FORMAT and STRUCTURE (not factual accuracy or writing quality).\n\nRequired output shape (be flexible with exact phrasing of headers, but strict about presence):\n1) File format: Must be a PDF (not DOCX, not plain text, not an image-only scan). The PDF should render selectable text (not only images of text). No pictures or embedded photos/graphics anywhere.\n2) Headline: A clear headline/title indicating Virginia\u2019s 2025 elections with focus on the governor\u2019s race (gubernatorial). A subheadline/dateline is optional.\n3) Article body: 300\u2013500 words of continuous prose, organized in paragraphs suitable for a local newspaper article.\n4) Two distinct content areas must be present in the body (section headers optional, but content must be clearly identifiable):\n   - Upcoming election dates: mentions at least one specific upcoming 2025 election date relevant to Virginia (between June and November 2025). Do not verify correctness\u2014only that explicit date(s) are listed/mentioned.\n   - Methods for voter participation: explains how voters can participate (e.g., early voting, absentee/mail, in-person, drop box, registration).\n5) Virginia Department of Elections link: The article must include the Virginia Department of Elections website (www.elections.virginia.gov or https://www.elections.virginia.gov). It must appear as the final line of the article.\n\nScoring (return a score from 0.0 to 4.0):\n- 4.0: PDF with selectable text; clear headline; 300\u2013500 words; both content areas present; no images; correct VA elections URL appears as the final line.\n- 3.0: One minor element missing or slightly ambiguous (e.g., headline present but less explicit; or link present but separated by trailing whitespace/extra blank line) while all core elements exist (PDF, word count in range, both content areas, no images).\n- 2.0: Missing one core element (either dates content OR how-to-vote content OR link is not last line) but still a PDF with 300\u2013500 words and a headline; no images.\n- 1.0: Multiple core elements missing but still a PDF with some article-like text.\n- 0.0: Not a PDF; contains images/graphics; or clearly not an article-like structure.\n\nImportant: Only evaluate presence/format/structure. Do NOT judge factual accuracy, tone, or quality in this stage.", "expectation": "A PDF news article with the mandated structure enabling automated verification in later stages."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Compliance Verification", "description": "Automated and targeted checks for measurable requirements: word count bounds, URL placement, gubernatorial focus, presence of 2025 dates (June\u2013Nov), and voter participation methods. Includes a brief LLM neutrality check.", "is_required": false, "max_points": 4.5, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Word Count Bounds (PDF)", "description": "Verify extracted text word count is within 300\u2013500; partial credit if modestly out of range.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document output.\"\n        path = context.files.get_path(output.id)\n        if path.suffix.lower() != '.pdf':\n            return 0.0, \"Not a PDF file.\"\n        text = context.files.read_pdf_text(output.id) or \"\"\n        words = re.findall(r\"\\b\\w+\\b\", text)\n        wc = len(words)\n        if 300 <= wc <= 500:\n            return 0.8, f\"Word count OK: {wc}.\"\n        elif 250 <= wc < 300 or 501 <= wc <= 550:\n            return 0.4, f\"Slightly out of range: {wc}.\"\n        else:\n            return 0.0, f\"Out of range: {wc}.\"\n    except Exception as e:\n        return 0.0, f\"Error reading PDF/word count: {e}\""}, {"type": "code", "name": "URL At End and Domain Correct", "description": "Verify the last non-empty line contains the Virginia Department of Elections URL.", "weight": 0.7, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output:\n            return 0.0, \"No output.\"\n        path = context.files.get_path(output.id)\n        if path.suffix.lower() != '.pdf':\n            return 0.0, \"Not a PDF file.\"\n        text = context.files.read_pdf_text(output.id) or \"\"\n        lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n        if not lines:\n            return 0.0, \"No text lines.\"\n        last = lines[-1]\n        end_pat = re.compile(r\"^(?:https?://)?(?:www\\.)?elections\\.virginia\\.gov/?$\", re.IGNORECASE)\n        domain_pat = re.compile(r\"elections\\.virginia\\.gov\", re.IGNORECASE)\n        if end_pat.search(last):\n            return 0.7, \"Correct URL at end.\"\n        elif domain_pat.search(text):\n            return 0.35, \"Domain present but not as final line.\"\n        else:\n            return 0.0, \"Required domain not found.\"\n    except Exception as e:\n        return 0.0, f\"Error verifying URL placement: {e}\""}, {"type": "code", "name": "Gubernatorial Focus Present", "description": "Check presence of gubernatorial/governor focus terms.", "weight": 0.7, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output:\n            return 0.0\n        text = (context.files.read_pdf_text(output.id) or \"\").lower()\n        # Count mentions of gubernatorial/governor\n        terms = [r\"gubernatorial\", r\"governor\", r\"governor's\", r\"governors?\"]\n        count = sum(len(re.findall(t, text)) for t in terms)\n        if count >= 2:\n            return 0.7, f\"Governor focus mentions: {count}.\"\n        elif count == 1:\n            return 0.35, \"One governor-related mention.\"\n        else:\n            return 0.0, \"No governor-related terms found.\"\n    except Exception as e:\n        return 0.0, f\"Error checking gubernatorial focus: {e}\""}, {"type": "code", "name": "Includes Upcoming 2025 Election Dates (June\u2013Nov)", "description": "Detect at least one explicit upcoming date in 2025 between June and November (month-name or numeric formats).", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output:\n            return 0.0\n        text = context.files.read_pdf_text(output.id) or \"\"\n        t = text\n        # Month-name formats like: June 18, 2025; Oct. 15, 2025; November 2025\n        month_pattern = r\"(?:June|July|August|September|October|November|Jun\\.?|Jul\\.?|Aug\\.?|Sept\\.?|Oct\\.?|Nov\\.?)\"\n        name_date = re.findall(rf\"{month_pattern}\\s+\\d{{1,2}}(?:st|nd|rd|th)?\\s*,?\\s*2025\", t, flags=re.IGNORECASE)\n        name_month_year = re.findall(rf\"{month_pattern}\\s+2025\", t, flags=re.IGNORECASE)\n        # Numeric MM/DD/YYYY with 2025 and month 06-11\n        num_date = re.findall(r\"\\b(0?[6-9]|1[01])/(\\d{1,2})/2025\\b\", t)\n        total_hits = len(name_date) + len(name_month_year) + len(num_date)\n        if total_hits >= 1:\n            return 1.0, f\"Found {total_hits} date reference(s) in target window.\"\n        # Partial credit if any 2025 date appears (not necessarily Jun\u2013Nov)\n        any_2025 = re.findall(r\"\\b\\d{1,2}/\\d{1,2}/2025\\b\", t) or re.findall(r\"\\b(?:Jan\\.?|Feb\\.?|Mar\\.?|Apr\\.?|May|December|Dec\\.?|January|February|March|April|May|December)\\s+\\d{1,2},?\\s*2025\\b\", t, flags=re.IGNORECASE)\n        if any_2025:\n            return 0.5, \"Has 2025 date(s) but not clearly Jun\u2013Nov.\"\n        return 0.0, \"No 2025 election date references detected.\"\n    except Exception as e:\n        return 0.0, f\"Error scanning dates: {e}\""}, {"type": "code", "name": "Voter Participation Methods Mentioned", "description": "Detect at least two distinct participation method categories (early voting, absentee/mail, in-person, drop box, registration/ID).", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output:\n            return 0.0\n        text = (context.files.read_pdf_text(output.id) or \"\").lower()\n        categories = {\n            'early_voting': [r'early voting'],\n            'absentee_mail': [r'absentee', r'mail[- ]?in', r'vote by mail'],\n            'in_person': [r'in[- ]?person', r'election day voting'],\n            'drop_box': [r'drop[- ]?box', r'ballot drop[- ]?off'],\n            'registration': [r'register', r'registration', r'same[- ]day'],\n            'id': [r'photo id', r'voter id', r'identification']\n        }\n        hits = 0\n        for cat, pats in categories.items():\n            if any(re.search(p, text) for p in pats):\n                hits += 1\n        if hits >= 3:\n            return 1.0, f\"Found {hits} participation method categories.\"\n        elif hits == 2:\n            return 0.7, \"Two participation method categories found.\"\n        elif hits == 1:\n            return 0.35, \"Only one participation method category found.\"\n        else:\n            return 0.0, \"No participation method categories detected.\"\n    except Exception as e:\n        return 0.0, f\"Error checking participation methods: {e}\""}, {"type": "llm_judge", "name": "Neutral Tone and Non-Advocacy", "description": "Check that the article remains neutral, does not endorse or oppose candidates or platforms, and avoids opinions.", "weight": 0.3, "judge_prompt": "Evaluate ONLY tone and neutrality (not structure or factual accuracy). The article should:\n- Use neutral, informative language suitable for a local newspaper.\n- Avoid endorsing or criticizing any candidate, party, or platform.\n- Avoid subjective adjectives and opinionated language.\n\nScoring (0.0\u20130.3):\n- 0.3: Entirely neutral; no advocacy or evaluative language.\n- 0.15: Mostly neutral with a minor lapse (one sentence/phrase borderline).\n- 0.0: Contains explicit advocacy, endorsements, or partisan language.", "expectation": "Neutral, informational tone throughout, with no endorsements or opinion statements."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Readability", "description": "Holistic LLM assessment of journalistic clarity, organization, and newsroom polish for a local Virginia audience.", "is_required": false, "max_points": 1.5, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity, Organization, and Readability", "description": "Evaluate headline effectiveness, paragraph flow, readability for the general public, and clarity of the how-to-vote guidance.", "weight": 0.9, "judge_prompt": "Assess overall clarity and organization for a local news article intended for the general public:\n- Headline is clear and informative.\n- Paragraphs are logically ordered; transitions are smooth.\n- How-to-vote guidance is easy to follow.\n- Sentences are concise and free of obvious grammatical errors.\n\nScoring (0.0\u20130.9):\n- 0.9: Clear, well-structured, and highly readable throughout.\n- 0.6: Generally clear with minor issues.\n- 0.3: Noticeable clarity/organization problems.\n- 0.0: Hard to follow, disorganized, or error-prone.", "expectation": "Professional clarity and flow with accessible, well-ordered information."}, {"type": "llm_judge", "name": "Local Relevance and Newsroom Polish", "description": "Evaluate professional presentation, VA-local relevance, concise style, and absence of distracting formatting issues.", "weight": 0.6, "judge_prompt": "Evaluate professional polish and local relevance:\n- Information is framed for Virginia voters; references and terminology are Virginia-appropriate.\n- Style matches a neutral newsroom tone; formatting is consistent and clean.\n- No extraneous visual elements or distracting layout quirks.\n\nScoring (0.0\u20130.6):\n- 0.6: Strong local focus and professional polish.\n- 0.4: Adequate with minor polish issues.\n- 0.2: Mixed; some relevance or style issues.\n- 0.0: Poorly polished or not locally relevant.", "expectation": "Polished newsroom style with clear Virginia focus and consistent formatting."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "a97369c7-e5cf-40ca-99e8-d06f81c57d53", "rubric": {"category_name": "Legal Memorandum \u2013 Delaware Corporate Law (Avalon Bancorp / Marcus Davenholt)", "rationale": "This staged rubric enforces a self-documenting workflow for a professional legal memorandum. Stage 1 (LLM gate) mandates a precise document structure and required authorities so verification is trivial. Stage 2 mixes code rules (keyword/structure/coverage checks) with LLM judges (legal correctness and application) now that the output shape is known. Stage 3 assesses professional quality and client usefulness.", "max_total_score": 16.0, "stages": [{"name": "Stage 1 \u2013 Format and Structural Gate (LLM only)", "description": "Gate to ensure the output is a properly structured legal memorandum document that is machine-verifiable and includes the mandated sections and authorities for downstream checks.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Legal Memo Requirements", "description": "Verify the candidate produced a properly formatted legal memorandum with all required sections, explicit assumptions, and mandatory authorities.", "weight": 4.0, "judge_prompt": "You are evaluating whether the candidate\u2019s output satisfies the REQUIRED structure for a professional legal memorandum. Only check presence/format/structure \u2014 do NOT judge quality or legal accuracy.\n\nFormat requirements:\n- File must be a PDF or DOCX (not plain text/markdown/Excel).\n- No more than 3,000 words (approximate check is fine).\n- Professional memo header on the first page that includes, clearly labeled: To: (Avalon Bancorp Inc. or Avalon), From:, Date:, and Re: (or Subject:) referencing Avalon and the Velridge offer.\n- A clear title or heading such as \u201cMemorandum,\u201d \u201cLegal Memorandum,\u201d or \u201cMemo.\u201d\n\nRequired sections (flexible on exact heading names; check presence of clearly delineated sections):\n1) Issues Presented (or Questions Presented)\n2) Facts or Background (restating the provided facts about Avalon, Marcus, the stockholders\u2019 agreement, and the Velridge offer)\n3) Assumptions (must explicitly state BOTH of the task assumptions: (i) Marcus vetoed purely for personal animus and not for business reasons; (ii) the demand requirement need not be addressed)\n4) Analysis (or Discussion) with three clearly labeled subparts that map to:\n   I. Board authority and enforceability of the stockholders\u2019 agreement with Marcus (DGCL \u00a7141, \u00a7109, \u00a7122, Moelis, SB 313)\n   II. Fiduciary duty implications for Avalon\u2019s board in deferring to the stockholders\u2019 agreement and Marcus\u2019s veto\n   III. Fiduciary duty implications for Marcus in blocking the Velridge deal for personal reasons\n5) Conclusion and Recommendations (or Next Steps)\n6) Authorities Cited (a references list or table) that lists the mandated authorities by name/citation.\n\nMandatory authorities that must be cited/discussed somewhere in the Analysis and also appear in the Authorities/Citations list:\n- DGCL \u00a7 141; DGCL \u00a7 109; DGCL \u00a7 122\n- West Palm Beach v. Moelis (Moelis)\n- Delaware Senate Bill 313 (2024 DGCL amendments affecting Moelis issues)\n- McMullin v. Beran; Kahn v. Lynch\n- In re Sears Hometown & Outlet Stores, Inc.; Voigt v. Metcalf; Basho Technologies v. Georgetown Basho Investors\n\nScoring (0\u20134):\n- 4.0: PDF/DOCX, \u22643,000 words, professional header, all required sections present (including explicit Assumptions and Authorities Cited), and all mandatory authorities listed and referenced in Analysis.\n- 3.2: Same as above but minor omission (e.g., Authorities Cited list present but missing 1\u20132 items while still discussed in text) OR one secondary section slightly mislabeled yet clearly present.\n- 2.0: Missing one major required section OR header elements incomplete, but document still a PDF/DOCX memo with most structure.\n- 0.0: Not PDF/DOCX, or missing multiple major sections, or no clear Analysis subparts.\n\nOnly evaluate structure and presence, not correctness or quality.", "expectation": "A DOCX/PDF memo with a professional header; Issues, Facts/Background, explicit Assumptions, three-part Analysis, Conclusion/Recommendations, and an Authorities Cited list including all required authorities."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Substantive Verification (Code + LLM)", "description": "Now that structure exists, verify coverage, assumptions, and basic legal framing using code checks and targeted LLM judgments.", "is_required": false, "max_points": 9.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "File Type and Word Count Bounds", "description": "Confirm document type and that memo length is within reasonable professional bounds.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output.\"\n    if not output.is_document:\n        return 0.0, \"Output is not a document file.\"\n\n    # Extract text robustly\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        pass\n    if not text:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            pass\n    if not text and output.is_text_format:\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            pass\n    if not text:\n        return 0.3, \"Could not extract text; awarding minimal partial credit.\"\n\n    words = re.findall(r\"\\b\\w+\\b\", text)\n    wc = len(words)\n    # Ideal: 800\u20133000 words; Acceptable: 400\u20133300 words\n    if 800 <= wc <= 3000:\n        return 1.0, f\"Word count OK: {wc}.\"\n    if 400 <= wc <= 3300:\n        return 0.6, f\"Word count marginal: {wc}.\"\n    return 0.2, f\"Word count out of bounds: {wc}.\""}, {"type": "code", "name": "Structural Headings Presence", "description": "Verify presence of core sections and three issue-specific analysis subparts using flexible keyword matching.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"Missing or wrong file type.\"\n\n    # Extract text\n    text = \"\"\n    for fn in [context.files.read_pdf_text, context.files.read_docx_text]:\n        try:\n            text = fn(output.id)\n            if text:\n                break\n        except Exception:\n            pass\n    if not text and output.is_text_format:\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            pass\n    if not text:\n        return 0.3, \"Could not extract text; minimal credit.\"\n\n    t = text.lower()\n    has_issues = bool(re.search(r\"\\b(issues?|questions?) presented\\b\", t))\n    has_facts = bool(re.search(r\"\\b(background|facts)\\b\", t))\n    has_analysis = bool(re.search(r\"\\b(analysis|discussion)\\b\", t))\n    has_conclusion = bool(re.search(r\"\\b(conclusion|recommendations?)\\b\", t))\n\n    # Subparts\n    issue1 = bool(re.search(r\"(board|director).{0,80}(authority|enforce|enforceability|stockholder.?s? agreement)\", t, flags=re.S))\n    issue2 = bool(re.search(r\"fiduciary.{0,80}(board|director)\", t, flags=re.S))\n    issue3 = bool(re.search(r\"fiduciary.{0,80}(marcus|stockholder|investor|controller)\", t, flags=re.S))\n\n    core = sum([has_issues, has_facts, has_analysis, has_conclusion]) / 4\n    subs = sum([issue1, issue2, issue3]) / 3\n\n    score = 0.5 * core + 0.5 * subs\n    return score, f\"Core sections: {has_issues,has_facts,has_analysis,has_conclusion}; Subparts: {issue1,issue2,issue3}.\""}, {"type": "code", "name": "Authorities Coverage (Mandated Sources Present)", "description": "Check that the memo text includes all required authorities (flexible matching).", "weight": 2.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"Missing or wrong file type.\"\n\n    # Extract text\n    text = \"\"\n    for fn in [context.files.read_pdf_text, context.files.read_docx_text]:\n        try:\n            text = fn(output.id)\n            if text:\n                break\n        except Exception:\n            pass\n    if not text and output.is_text_format:\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            pass\n    if not text:\n        return 0.2, \"Could not extract text; minimal credit.\"\n\n    t = text\n    checks = {\n        'DGCL \u00a7141': r'(DGCL|Del\\.?\\s*Gen\\.?\\s*Corp\\.?\\s*L\\.?|Delaware General Corporation Law)[^\\n\\r]{0,40}[\u00a7\\u00A7]\\s*141|8\\s*Del\\.?\\s*C\\.?\\s*\u00a7\\s*141',\n        'DGCL \u00a7109': r'(DGCL|Del\\.?\\s*Gen\\.?\\s*Corp\\.?\\s*L\\.?)[^\\n\\r]{0,40}[\u00a7\\u00A7]\\s*109|8\\s*Del\\.?\\s*C\\.?\\s*\u00a7\\s*109',\n        'DGCL \u00a7122': r'(DGCL|Del\\.?\\s*Gen\\.?\\s*Corp\\.?\\s*L\\.?)[^\\n\\r]{0,40}[\u00a7\\u00A7]\\s*122|8\\s*Del\\.?\\s*C\\.?\\s*\u00a7\\s*122',\n        'Senate Bill 313': r'(Senate\\s*Bill|SB)\\s*313|2024\\s+Delaware.*(amend|bill|legislation)',\n        'Moelis': r'(Moelis|West\\s+Palm\\s+Beach\\s+v\\.?\\s+Moelis)',\n        'McMullin v. Beran': r'McMullin\\s+v\\.?\\s+Beran',\n        'Kahn v. Lynch': r'Kahn\\s+v\\.?\\s+Lynch',\n        'In re Sears Hometown': r'(In\\s+re\\s+Sears\\s+Hometown|Sears\\s+Hometown\\s*&\\s*Outlet)',\n        'Voigt v. Metcalf': r'Voigt\\s+v\\.?\\s+Metcalf',\n        'Basho Technologies': r'Basho\\s+Tech|Basho\\s+Technologies|Georgetown\\s+Basho\\s+Investors'\n    }\n\n    missing = []\n    present = []\n    for label, pat in checks.items():\n        if re.search(pat, t, flags=re.I):\n            present.append(label)\n        else:\n            missing.append(label)\n\n    total = len(checks)\n    covered = len(present)\n    frac = covered / total if total else 0\n    score = frac * 2.0  # weight is 2.0\n    fb = f\"Covered {covered}/{total}. Missing: {', '.join(missing) if missing else 'None'}.\"\n    return score, fb"}, {"type": "code", "name": "Assumptions Reflected", "description": "Verify the memo explicitly states both task assumptions: personal animus (not business) and that demand requirement need not be addressed.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = \"\"\n    for fn in [context.files.read_pdf_text, context.files.read_docx_text]:\n        try:\n            text = fn(output.id)\n            if text:\n                break\n        except Exception:\n            pass\n    if not text and output.is_text_format:\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            pass\n    if not text:\n        return 0.1\n\n    t = text.lower()\n    animus = bool(re.search(r'(personal\\s+animus|personal\\s+(reason|vendetta|motive)|not\\s+for\\s+business\\s+reason)', t))\n    demand = bool(re.search(r'(demand\\s+requirement|demand\\s+futility|we\\s+do\\s+not\\s+address\\s+demand|assume\\s+no\\s+need\\s+to\\s+address\\s+demand)', t))\n\n    if animus and demand:\n        return 0.5, \"Both assumptions explicitly stated.\"\n    if animus or demand:\n        return 0.3, \"Only one assumption explicitly stated.\"\n    return 0.1, \"Assumptions not clearly stated.\""}, {"type": "code", "name": "Client Addressing and Header Elements", "description": "Check presence of To/From/Date/Re header and that it is addressed to Avalon.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = \"\"\n    for fn in [context.files.read_pdf_text, context.files.read_docx_text]:\n        try:\n            text = fn(output.id)\n            if text:\n                break\n        except Exception:\n            pass\n    if not text and output.is_text_format:\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            pass\n    if not text:\n        return 0.1\n\n    # Focus on the first ~1000 chars for header\n    head = text[:1000].lower()\n    has_to = 'to:' in head\n    has_from = 'from:' in head\n    has_date = 'date:' in head\n    has_re = ('re:' in head) or ('subject:' in head)\n    has_avalon = ('avalon' in head)\n\n    count = sum([has_to, has_from, has_date, has_re, has_avalon])\n    if count >= 4 and has_avalon:\n        return 0.5, \"Header complete and addressed to Avalon.\"\n    if count >= 3 and has_avalon:\n        return 0.3, \"Header partially complete but addressed to Avalon.\"\n    return 0.1, \"Header insufficient or not addressed to Avalon.\""}, {"type": "code", "name": "Citation Signals/Formats Present", "description": "Light-weight check for legal citation markers (e.g., section symbol, case citations).", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = \"\"\n    for fn in [context.files.read_pdf_text, context.files.read_docx_text]:\n        try:\n            text = fn(output.id)\n            if text:\n                break\n        except Exception:\n            pass\n    if not text and output.is_text_format:\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            pass\n    if not text:\n        return 0.1\n\n    t = text\n    has_section = '\u00a7' in t or '\\u00A7' in t or 'section' in t.lower()\n    has_case_v = bool(re.search(r\"\\bv\\.?\\b\", t))\n    has_reporter = bool(re.search(r\"\\bA\\.\\s?\\d+d?|A\\.\\d+\\s?d|A\\.3d|A\\.2d|A\\.\\s?\\d+d\\b\", t)) or ('Del.' in t)\n\n    score = sum([has_section, has_case_v, has_reporter]) / 3 * 0.5\n    return score, f\"Markers: section={has_section}, v.={has_case_v}, reporter={has_reporter}.\""}, {"type": "llm_judge", "name": "Substantive Accuracy \u2013 Board Authority, Moelis, and SB 313", "description": "Check whether the memo correctly frames DGCL \u00a7141(a) board authority, the Moelis decision\u2019s implications for stockholder agreements that restrict board action, and the potential impact of Delaware Senate Bill 313 (2024 amendments).", "weight": 1.5, "judge_prompt": "Evaluate the legal accuracy and completeness of the memo\u2019s treatment of: (a) DGCL \u00a7 141(a) board management authority; (b) West Palm Beach v. Moelis, including how certain stockholder agreement veto/approval rights can be invalid if they impermissibly restrict the board\u2019s statutory authority; and (c) Delaware Senate Bill 313 (2024 DGCL amendments) that address or modify the Moelis landscape (describe the gist and how it might apply here). Do not penalize minor citation format issues. Consider whether the memo links these principles to the enforceability of Marcus\u2019s appointment and veto rights in the stockholders\u2019 agreement.\n\nScoring (0\u20131.5):\n- 1.5: Accurate, coherent explanation that ties \u00a7141(a), Moelis, and SB 313 to enforceability analysis, including potential distinctions/limitations.\n- 1.0: Mostly accurate with small gaps or ambiguity, but overall correct framing.\n- 0.5: Partially accurate; misses key element (e.g., ignores SB 313) or muddles the Moelis rule.\n- 0.0: Inaccurate or no meaningful discussion.", "expectation": "A concise, accurate synthesis of \u00a7141(a), Moelis, and SB 313 applied to the Avalon\u2013Marcus stockholders\u2019 agreement."}, {"type": "llm_judge", "name": "Controller and Fiduciary Duties Analysis", "description": "Assess whether the memo correctly analyzes fiduciary duties for the Board and for Marcus as a potential controller/minority with outsized rights.", "weight": 1.5, "judge_prompt": "Evaluate whether the memo accurately addresses fiduciary duties in this context:\n- Board: duties of care and loyalty when deferring to a stockholder agreement/stockholder veto in a high-value opportunity; appropriate standards of review (business judgment, enhanced scrutiny, or entire fairness, as relevant to facts); use of independent process.\n- Marcus: whether his rights and conduct could render him a controlling stockholder despite minority ownership (consider Voigt, Basho, Sears Hometown), and if blocking for personal reasons could constitute a breach of fiduciary duty; whether Kahn v. Lynch and McMullin are applied appropriately to controller dynamics and standard of review.\n\nScoring (0\u20131.5):\n- 1.5: Clear, accurate application using the cited cases; recognizes controller analysis and connects to entire fairness/standards of review.\n- 1.0: Mostly correct but misses one relevant nuance.\n- 0.5: Touches on concepts but is shallow or partially incorrect.\n- 0.0: Incorrect or absent analysis.", "expectation": "Balanced, accurate fiduciary-duty analysis that uses Voigt, Basho, Sears, Kahn v. Lynch, and McMullin to assess both the Board and Marcus."}, {"type": "llm_judge", "name": "Remedies and Risk Assessment", "description": "Check whether the memo outlines plausible claims/defenses and practical risk assessment for potential shareholder litigation.", "weight": 0.5, "judge_prompt": "Does the memo briefly and accurately discuss plausible causes of action (e.g., breach of fiduciary duty, aiding and abetting, declaratory/injunctive relief), likely standards of review, and key defenses? Does it give a realistic risk assessment and identify practical options for Avalon (e.g., revisiting process, special committee, seeking declaratory relief on enforceability, negotiating waivers/modifications)? Keep this concise; focus on presence and plausibility.\n\nScoring (0\u20130.5):\n- 0.5: Concrete, realistic, and tailored to facts.\n- 0.3: Present but generic or missing one key element.\n- 0.0: Absent or materially off-base.", "expectation": "A short, grounded risk and remedies overview with actionable options for Avalon."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Professional Quality and Client Usefulness (LLM)", "description": "Holistic assessment of writing quality, organization, and practical value for the client audience.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Quality and Practical Value", "description": "Evaluate tone, clarity, and client usefulness, including recommendations tailored to Avalon.", "weight": 2.0, "judge_prompt": "Assess the memo\u2019s professional quality and client usefulness:\n- Tone: professional, neutral, objective.\n- Clarity: well-organized paragraphs, clear transitions, minimal redundancy.\n- Practicality: concrete, tailored recommendations and next steps for Avalon.\n\nScoring (0\u20132):\n- 2.0: Highly professional, crystal-clear, and practically useful with tailored recommendations.\n- 1.3: Generally strong but with minor clarity or tailoring gaps.\n- 0.7: Adequate but generic or somewhat hard to follow.\n- 0.0: Poorly written or unusable for a client.", "expectation": "A polished memo with clear, actionable guidance for Avalon."}, {"type": "llm_judge", "name": "Organization and Readability", "description": "Check logical flow, sectioning, and readability within the 3,000-word constraint.", "weight": 1.0, "judge_prompt": "Evaluate organization and readability:\n- Logical flow following the required sections.\n- Effective headings, signposting, and concise prose within the 3,000-word limit.\n\nScoring (0\u20131):\n- 1.0: Excellent structure and readability.\n- 0.7: Good structure with minor issues.\n- 0.4: Acceptable but uneven.\n- 0.0: Disorganized or hard to read.", "expectation": "Clear headings and tight prose that makes the analysis easy to navigate."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "4c4dc603-c21c-4284-8fb1-1b827c1fddf4", "rubric": {"category_name": "One-Page Product Summary (Project Kenonic)", "rationale": "This rubric enforces a self-documenting, verifiable one-page PDF product summary with clearly labeled sections and contact details. Stage 1 (LLM-only) gates strict format/structure. Stage 2 uses code rules to verify critical factual elements (contacts, numeric KPIs, token economics, section coverage) in extracted text. Stage 3 assesses professional quality for the intended accredited retail investor audience.", "max_total_score": 10.0, "stages": [{"name": "Stage 1: Shape Enforcement Gate", "description": "LLM-only gate to ensure the candidate output is a single-page PDF with all required sections and visibly labeled content enabling verification.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.2, "rules": [{"type": "llm_judge", "name": "Structured One-Page PDF Requirement", "description": "Check that the output is a one-page PDF with all required sections and contact details present as labeled content.", "weight": 4.0, "judge_prompt": "You are evaluating a single candidate output file. Only check structure/format and presence of required elements, not correctness of numbers or content quality.\n\nFormat Requirements:\n- Must be a PDF (not DOCX/PNG/HTML).\n- Exactly one page.\n- Professionally laid out with clear section headers or bolded labels.\n\nRequired Sections (flexible naming allowed; look for semantic equivalents):\n1) Fund Details (mission, objectives) \u2013 headers like: Fund Overview, Mission, Objectives, About the Fund.\n2) Problem \u2013 e.g., Problem, Market Pain, Challenge.\n3) Proposed Solution \u2013 e.g., Solution, How It Works, Approach.\n4) Salient Numbers \u2013 a compact area with key metrics; should include at least three numeric KPIs such as: market size, target raise, target IRR. Accept equivalents (AUM target, fund size, expected IRR, etc.).\n5) Key Economics (token economics) \u2013 include: token supply, valuation/pricing methodology, valuation/pricing frequency, and price per token (accept synonyms: tokenomics, valuation method, NAV frequency, pricing cadence).\n6) Investment Strategy \u2013 e.g., Strategy, Thesis, Deployment Plan.\n7) Dividend Distribution Strategy \u2013 e.g., Distributions, Yield Policy, Payouts.\n8) Team Profiles \u2013 at least two team members with names and roles/titles.\n9) LKK Capital Contact Details \u2013 MUST include all four: website (https://www.lkkacapital.com), email (letstalk@lkkcapital.com), phone ((+1) 000 000 111), and disclosures link (https://www.lkkcapital.com/disclosures).\n\nAdditional expectations:\n- Content should be concise and scannable (bullets, short paragraphs, or a small table for numbers is fine).\n- The page should reference Project Kenonic (by name) to confirm document relevance.\n\nScoring (structure only):\n- 4.0: PDF, exactly 1 page, all 9 sections present with clearly labeled content; Salient Numbers show >=3 KPIs; Key Economics explicitly includes supply, valuation methodology, valuation/pricing frequency, and price per token; all contact details present with the exact items/links.\n- 3.2\u20133.9: PDF, 1 page, at least 8/9 sections present; at most one minor omission (e.g., missing frequency OR price per token); contact block includes at least website, email, and disclosures link; phone may be missing or slightly formatted differently.\n- 2.4\u20133.1: PDF, 1 page, 6\u20137 sections present; contact block includes at least website and email; some numeric KPIs present but fewer than 3, or token economics missing one major element.\n- 1.6\u20132.3: PDF, 1 page, 4\u20135 sections present OR multiple required elements missing.\n- 0.8\u20131.5: PDF, but only 2\u20133 sections present.\n- 0.0: Not a PDF, more than one page, or the structure is fundamentally missing (cannot identify core sections).", "expectation": "A single-page PDF with all nine labeled sections and full LKK Capital contact details, enabling trivial verification of presence (not correctness)."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Correctness and Consistency Checks", "description": "Code-based verification of critical elements in extracted text: contact details, numeric KPIs, token economics, and section coverage references.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Contact Details Presence", "description": "Verify that website, email, phone, and disclosures link for LKK Capital are present in the document text.", "weight": 1.2, "code": "import re\n\ndef evaluate(workflow, context):\n    W = 1.2\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output to evaluate.\"\n\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text or len(text.strip()) < 30:\n        return 0.0, \"Text could not be extracted or is too short.\"\n\n    low = text.lower()\n\n    website_ok = re.search(r\"https?://(www\\.)?lkkacapital\\.com(?!/disclosures)\\b\", low) is not None\n    email_ok = \"letstalk@lkkcapital.com\" in low\n    # Accept variations of formatting for the phone number\n    phone_ok = re.search(r\"\\+?\\s*1\\D*0{3}\\D*0{3}\\D*111\", low) is not None or \"(+1) 000 000 111\" in text\n    disclosures_ok = re.search(r\"https?://(www\\.)?lkkacapital\\.com/disclosures\\b\", low) is not None\n\n    checks = [website_ok, email_ok, phone_ok, disclosures_ok]\n    score = (sum(1 for c in checks if c) / 4.0) * W\n\n    missing = []\n    if not website_ok: missing.append(\"website\")\n    if not email_ok: missing.append(\"email\")\n    if not phone_ok: missing.append(\"phone\")\n    if not disclosures_ok: missing.append(\"disclosures link\")\n\n    fb = \"All contact details present.\" if not missing else f\"Missing: {', '.join(missing)}.\"\n    return score, fb"}, {"type": "code", "name": "Numeric KPIs Coverage (IRR, Raise, Market Size)", "description": "Check for presence of at least two of the three numeric KPI categories: target IRR (with %), target raise ($), and market size ($).", "weight": 1.2, "code": "import re\n\ndef evaluate(workflow, context):\n    W = 1.2\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output to evaluate.\"\n\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text or len(text.strip()) < 30:\n        return 0.0, \"Text could not be extracted or is too short.\"\n\n    low = text.lower()\n\n    money = r\"(?:\\$|usd\\s*)?\\s?\\d{1,3}(?:[\\s,]\\d{3})*(?:\\.\\d+)?\\s?(?:k|m|mm|b|bn)?\"\n\n    irr_found = re.search(r\"(irr|internal rate of return)[^%]{0,60}\\d{1,2}(?:\\.\\d+)?\\s?%\", low) is not None\n\n    raise_found = re.search(r\"(target\\s*raise|raise|offering size|offering|fund size|target fund size)[^\\n\\r]{0,40}\" + money, low) is not None\n\n    market_found = re.search(r\"(tam|total addressable market|market size|market)[^\\n\\r]{0,40}\" + money, low) is not None\n\n    categories = {\"IRR\": irr_found, \"Raise\": raise_found, \"Market\": market_found}\n    n = sum(1 for v in categories.values() if v)\n\n    if n >= 3:\n        score = W\n    elif n == 2:\n        score = W * 0.8\n    elif n == 1:\n        score = W * 0.35\n    else:\n        score = 0.0\n\n    missing = [k for k, v in categories.items() if not v]\n    fb = f\"Found {n}/3 KPI categories. Missing: {', '.join(missing)}\" if missing else \"All KPI categories found.\"\n    return score, fb"}, {"type": "code", "name": "Token Economics Specifics", "description": "Verify presence of key token economics elements: token supply, valuation/pricing methodology, valuation/pricing frequency, and price per token.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    W = 0.8\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output to evaluate.\"\n\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text or len(text.strip()) < 30:\n        return 0.0, \"Text could not be extracted or is too short.\"\n\n    low = text.lower()\n\n    money = r\"(?:\\$|usd\\s*)?\\s?\\d{1,3}(?:[\\s,]\\d{3})*(?:\\.\\d+)?\\s?(?:k|m|mm|b|bn)?\"\n\n    supply_ok = re.search(r\"(token[^\\n\\r]{0,20}supply|total\\s*supply)[^\\n\\r]{0,60}\\d{1,3}(?:[\\s,]\\d{3})+\", low) is not None\n\n    valuation_ok = (\n        \"valuation methodology\" in low or\n        \"pricing methodology\" in low or\n        \"valuation method\" in low or\n        re.search(r\"\\bnav\\b\", low) is not None\n    )\n\n    frequency_ok = re.search(r\"(valuation|pricing|nav)[^\\n\\r]{0,40}(monthly|quarterly|weekly|daily|semi-annual|semiannual|annual)\", low) is not None\n\n    price_ok = re.search(r\"(price per token|token price)[^\\n\\r]{0,30}\" + money, low) is not None\n\n    checks = {\"Token Supply\": supply_ok, \"Valuation Methodology\": valuation_ok, \"Valuation/Pricing Frequency\": frequency_ok, \"Price per Token\": price_ok}\n    n = sum(1 for v in checks.values() if v)\n\n    # Graded: 4/4 -> 100%, 3/4 -> 75%, 2/4 -> 50%, 1/4 -> 25%\n    score = (n / 4.0) * W\n\n    missing = [k for k, v in checks.items() if not v]\n    fb = f\"Found {n}/4 token economics elements. Missing: {', '.join(missing)}\" if missing else \"All token economics elements found.\"\n    return score, fb"}, {"type": "code", "name": "Section Coverage and Project Reference", "description": "Fuzzy check for presence of most required sections and explicit reference to Project Kenonic.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    W = 0.8\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output to evaluate.\"\n\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text or len(text.strip()) < 30:\n        return 0.0, \"Text could not be extracted or is too short.\"\n\n    low = text.lower()\n\n    def any_in(keys):\n        return any(k in low for k in keys)\n\n    sections = {\n        \"Fund Details\": [\"fund details\", \"fund overview\", \"mission\", \"objective\", \"about the fund\"],\n        \"Problem\": [\"problem\", \"challenge\", \"market pain\"],\n        \"Solution\": [\"solution\", \"how it works\", \"approach\"],\n        \"Salient Numbers\": [\"salient numbers\", \"key metrics\", \"by the numbers\", \"at a glance\", \"highlights\"],\n        \"Key Economics\": [\"token economics\", \"tokenomics\", \"economics\", \"valuation methodology\", \"pricing methodology\"],\n        \"Investment Strategy\": [\"investment strategy\", \"strategy\", \"thesis\"],\n        \"Dividend Distribution\": [\"dividend\", \"distribution\", \"payout\", \"yield policy\", \"distribution policy\"],\n        \"Team\": [\"team\", \"management\", \"leadership\", \"key people\", \"bios\"]\n    }\n\n    found = {}\n    for k, vals in sections.items():\n        found[k] = any_in(vals)\n\n    kenonic_ok = (\"kenonic\" in low) or (\"project kenonic\" in low)\n\n    n_sections = sum(1 for v in found.values() if v)\n\n    # Score: require most sections; proportional with a bonus for Kenonic reference\n    base = (n_sections / len(sections))  # 0..1\n    # Bonus: +10% if Kenonic is referenced\n    bonus = 0.1 if kenonic_ok else 0.0\n    score = min(1.0, base + bonus) * W\n\n    missing = [k for k, v in found.items() if not v]\n    fb_parts = [f\"Sections found: {n_sections}/{len(sections)}\"]\n    if missing:\n        fb_parts.append(\"Missing: \" + \", \".join(missing))\n    if not kenonic_ok:\n        fb_parts.append(\"No explicit 'Project Kenonic' reference found\")\n    return score, \"; \".join(fp for fp in fb_parts if fp)}]},{"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3: Professional Quality Assessment", "description": "LLM-based holistic review of clarity, investor appropriateness, and presentation quality (not factual correctness).", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Investor-Ready Quality", "description": "Assess clarity, concision, visual hierarchy, and appropriateness for accredited retail investors.", "weight": 2.0, "judge_prompt": "Evaluate the one-page PDF product summary for professional quality. Ignore correctness of numbers or consistency with the source IM\u2014assess only presentation quality and suitability for accredited retail investors.\n\nConsider:\n- Clarity and concision: Does it succinctly explain the fund\u2019s mission, problem/solution, strategy, and economics without hype?\n- Visual hierarchy: Clear sectioning, readable typography, whitespace, and scannable bullets/tables for key numbers.\n- Investor appropriateness: Balanced tone (not promotional), minimal jargon or clearly defined, includes dividend/distribution explanation and strategy at a high level.\n- Coherence and flow: Logical ordering from overview \u2192 problem/solution \u2192 numbers/economics \u2192 strategy \u2192 team \u2192 contacts.\n- Professional polish: Consistent formatting, few/no typos, brand-consistent contact block.\n\nScoring:\n- 2.0: Excellent\u2014clear, concise, professional, highly scannable, strong visual hierarchy.\n- 1.2\u20131.8: Good\u2014minor issues with wording, layout, or density.\n- 0.6\u20131.1: Fair\u2014readable but cluttered, uneven, or somewhat promotional/jargony.\n- 0.0\u20130.5: Poor\u2014confusing, disorganized, or unprofessional.\n", "expectation": "A concise, visually well-structured one-page PDF suitable for accredited investors, with balanced tone and strong readability."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "57b2cdf2-ad62-4591-aa91-aad489740320", "rubric": {"category_name": "Private Investigation Surveillance Report Finalization", "rationale": "This rubric uses a self-documenting, three-stage design. Stage 1 is an LLM-only gate that enforces strict structural and format requirements (PDF, <= 2 pages, required sections, clear timeline with photo references) so the output is verifiable. Stage 2 mixes code and LLM rules to check correctness elements enabled by the mandated shape: fuzzy section detection, timestamp coverage/chronology within the specified surveillance window, length reasonableness, and alignment between photos and observations. Stage 3 uses an LLM judge to assess overall professional quality, readability, and adherence to the client-facing tone and requirements.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2013 Format and Structure Gate", "description": "LLM-only gate that verifies the final deliverable is a properly structured surveillance report PDF (1\u20132 pages), with the exact sections and timeline/photo reference structure needed for verification.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Document Format and Structural Requirements", "description": "Verify that the output is a PDF of at most two pages with the required sections and a verifiable surveillance timeline that references photos.", "weight": 4.0, "judge_prompt": "You are the Stage 1 gatekeeper. Inspect the primary output. Determine if it satisfies ALL of the following structure and format requirements. Only check presence/structure/format, not the quality of content.\n\nRequired Format:\n- Must be a PDF document (not Word, not plain text, not Excel).\n- Length must be 1\u20132 pages (must not exceed two pages).\n- Professionally formatted with clear, readable layout.\n\nRequired Sections (headers visible; allow case-insensitive and close variants in parentheses):\n1) \"Summary\" (acceptable variants: Overview, Case Summary)\n2) \"Surveillance\" (acceptable variants: Observations, Surveillance Log, Timeline)\n3) \"Assessment\" (acceptable variants: Conclusion, Findings, Evaluation)\n\nSurveillance Section Structure Requirements:\n- Must present a chronological timeline of events. This can be a table or bullet list where each entry begins with or prominently highlights a time (e.g., 7:30 p.m., 9:05 PM, 21:10). Accept 12-hour or 24-hour formats.\n- Must include at least three timestamped entries, with multiple entries covering the client\u2019s requested surveillance window (9:00 p.m. to 1:00 a.m.). It is acceptable if the timeline begins earlier (around 7:30 p.m.).\n- Photo references must be verifiable in at least one of the following ways:\n  a) Embedded photos with captions that include the time and brief description, or\n  b) Inline references such as [Photo 1], (Fig. 1), or similar, aligned to specific timeline entries, or\n  c) A small \"Photo References\" list/table mapping Photo ID \u2192 Time \u2192 Caption/Description.\n\nScoring (return a score from 0.0 to 1.0):\n- 1.0: PDF, 1\u20132 pages, all three sections present, timeline is clearly chronological with timestamped entries spanning the requested window, and photos are embedded or references are clearly mapped to timeline entries.\n- 0.8: PDF, 1\u20132 pages, sections present, timeline present and mostly chronological with timestamps, but photo mapping is less explicit (e.g., captions present but not clearly tied to entries) or one minor structural deviation.\n- 0.5: PDF and sections present but timeline structure is weak (e.g., few timestamps or not clearly chronological) or photo references are missing/ambiguous.\n- 0.2: PDF present but missing multiple required sections or timeline structure entirely unclear.\n- 0.0: Not a PDF or exceeds two pages.\n\nOnly verify format/structure here; do not judge content quality or calculation correctness.", "expectation": "A 1\u20132 page PDF with clear headers for Summary, Surveillance, and Assessment, a chronological timeline emphasizing times, and photos embedded or referenced in a way that maps to the timeline."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness and Consistency Verification", "description": "Checks enabled by the mandated shape: section presence (fuzzy), timestamp coverage and chronology against the requested window, reasonable length proxy, and alignment between photos and observations.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Section Headings Present (Fuzzy)", "description": "Verify required sections exist using fuzzy header matching in the PDF text.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n\n    Returns:\n        float in [0,1] or (float, str)\n    \"\"\"\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    if not output.is_document:\n        return 0.0\n\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n    if not text:\n        return 0.0\n\n    low = re.sub(r\"\\s+\", \" \", text).lower()\n\n    # Fuzzy groups for headers\n    summary_terms = [r\"\\bsummary\\b\", r\"\\boverview\\b\", r\"\\bcase summary\\b\"]\n    surveillance_terms = [r\"\\bsurveillance\\b\", r\"\\bobservations\\b\", r\"\\btimeline\\b\", r\"\\bsurveillance log\\b\"]\n    assessment_terms = [r\"\\bassessment\\b\", r\"\\bconclusion\\b\", r\"\\bfindings\\b\", r\"\\bevaluation\\b\"]\n\n    def has_any(patterns):\n        return any(re.search(p, low) for p in patterns)\n\n    count = 0\n    count += 1 if has_any(summary_terms) else 0\n    count += 1 if has_any(surveillance_terms) else 0\n    count += 1 if has_any(assessment_terms) else 0\n\n    score = count / 3.0\n    return max(0.0, min(1.0, score))"}, {"type": "code", "name": "Timeline Coverage and Chronology", "description": "Parse timestamps and check that entries cover the intended window (~7:30 p.m. start, requested 9:00 p.m. to 1:00 a.m.) and are in chronological order across midnight.", "weight": 1.5, "code": "import re\n\n# Helper to parse and convert time strings to minutes since day start, with after-midnight adjustment\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Checks for:\n    - Sufficient number of timestamps\n    - Chronological ordering across midnight\n    - Coverage: early event around/before 20:00 and late event around/after 00:50\n    - Ratio of timestamps within [19:00, 01:30]\n    Returns float in [0,1].\n    \"\"\"\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    if not output.is_document:\n        return 0.0\n\n    # Extract text\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n    if not text:\n        return 0.0\n\n    low = text.lower()\n\n    # Regexes for times: 12-hour (with/without minutes) and 24-hour\n    patterns = [\n        r\"\\b(?:1[0-2]|0?[1-9]):[0-5]\\d\\s?(?:a\\.?m?\\.?|p\\.?m?\\.?)\\b\",  # 9:05 pm\n        r\"\\b(?:1[0-2]|0?[1-9])\\s?(?:a\\.?m?\\.?|p\\.?m?\\.?)\\b\",          # 9 pm\n        r\"\\b(?:[01]\\d|2[0-3]):[0-5]\\d\\b\"                                 # 21:05\n    ]\n\n    def parse_time_to_minutes(t):\n        t = t.strip().lower().replace(\" \", \"\")\n        # 12-hour with minutes\n        m = re.match(r\"^(1[0-2]|0?[1-9]):([0-5]\\d)(am|a\\.m\\.|pm|p\\.m\\.)$\", t)\n        if m:\n            h = int(m.group(1))\n            mins = int(m.group(2))\n            ap = m.group(3)\n            if ap.startswith('p') and h != 12:\n                h += 12\n            if ap.startswith('a') and h == 12:\n                h = 0\n            return h*60 + mins\n        # 12-hour no minutes\n        m = re.match(r\"^(1[0-2]|0?[1-9])(am|a\\.m\\.|pm|p\\.m\\.)$\", t)\n        if m:\n            h = int(m.group(1))\n            ap = m.group(2)\n            if ap.startswith('p') and h != 12:\n                h += 12\n            if ap.startswith('a') and h == 12:\n                h = 0\n            return h*60\n        # 24-hour\n        m = re.match(r\"^([01]\\d|2[0-3]):([0-5]\\d)$\", t)\n        if m:\n            h = int(m.group(1))\n            mins = int(m.group(2))\n            return h*60 + mins\n        return None\n\n    # Find raw matches\n    raw = []\n    for p in patterns:\n        raw.extend(re.findall(p, low))\n\n    # Normalize to strings and parse\n    timestr_list = []\n    if raw:\n        for r in raw:\n            if isinstance(r, tuple):\n                # From grouped regex, rebuild approximate string\n                timestr_list.append(\"\".join(r) if len(r) > 1 else r[0])\n            else:\n                timestr_list.append(r)\n\n    minutes = []\n    for ts in timestr_list:\n        m = parse_time_to_minutes(ts)\n        if m is not None:\n            # Adjust across midnight: treat hours < 6 as next day\n            if m < 6*60:\n                m += 24*60\n            minutes.append(m)\n\n    if not minutes:\n        return 0.0\n\n    # Deduplicate slight repeats\n    uniq = sorted(set(minutes))\n\n    # Components:\n    # 1) Sufficient timestamps\n    comp1 = 1.0 if len(uniq) >= 4 else (0.5 if len(uniq) >= 3 else 0.0)\n\n    # 2) Chronological ordering (allow equality)\n    comp2 = 1.0 if minutes == sorted(minutes) else 0.0\n\n    # 3) Coverage: early and late reach\n    earliest = min(uniq)\n    latest = max(uniq)\n    # Early score: <= 20:00 (1200) best; <= 21:00 (1260) partial\n    comp3_early = 1.0 if earliest <= 1200 else (0.5 if earliest <= 1260 else 0.0)\n    # Late score: >= 00:50 (1490) best; >= 00:20 (1460) partial (times after midnight already shifted by +1440)\n    comp3_late = 1.0 if latest >= (24*60 + 50) else (0.5 if latest >= (24*60 + 20) else 0.0)\n    comp3 = (comp3_early + comp3_late) / 2.0\n\n    # 4) In-window ratio [19:00, 01:30]\n    lower = 19*60\n    upper = 24*60 + 90\n    in_window = [m for m in minutes if lower <= m <= upper]\n    comp4 = (len(in_window) / len(minutes)) if minutes else 0.0\n\n    score = (comp1 + comp2 + comp3 + comp4) / 4.0\n    return max(0.0, min(1.0, score))"}, {"type": "code", "name": "Length Reasonableness (Word Count Proxy)", "description": "Proxy check that the written content likely fits within two pages (not strict).", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Returns 1.0 if <=1200 words, 0.5 if <=1500, else 0.0.\n    This is a proxy only; Stage 1 enforces the real 2-page limit.\n    \"\"\"\n    output = context.get_primary_output()\n    if not output:\n        return 0.0\n    if not output.is_document:\n        return 0.0\n\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n    if not text:\n        return 0.0\n\n    words = re.findall(r\"\\b\\w+\\b\", text)\n    wc = len(words)\n    if wc <= 1200:\n        return 1.0\n    elif wc <= 1500:\n        return 0.5\n    else:\n        return 0.0"}, {"type": "llm_judge", "name": "Photo\u2013Observation Alignment and Internal Consistency", "description": "Check that photos embedded or referenced clearly align with the surveillance timeline entries and depict what the text claims at the indicated times.", "weight": 1.0, "judge_prompt": "Review the PDF. Focus ONLY on whether photos (embedded or referenced) align with the written observations:\n- Are photos visible in the PDF or clearly referenced (e.g., [Photo 1], Fig. 1) with matching times and descriptions?\n- Do captions or inline references map to specific timeline entries in the Surveillance section?\n- Do the images plausibly depict the subject/activity described at or near the stated times? (High-level visual check; reasonable tolerance for lighting/angle.)\n- Is the number of photo references consistent with the number of images (or a Photo References mini-list), without obvious mismatches?\n\nScoring (0.0\u20131.0):\n- 1.0: Clear mapping of each referenced photo to a timeline entry with consistent time and description; no mismatches observed.\n- 0.5: Photos present but mapping is incomplete/ambiguous (some entries not referenced or times not shown), or minor inconsistencies.\n- 0.0: No photos present and no references, or clear misalignment (e.g., captions/times contradict the timeline).", "expectation": "Each referenced photo should have a recognizable caption or inline tag that corresponds to a specific timeline entry, with no obvious inconsistencies."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Professional Quality", "description": "Holistic professional assessment: clarity, readability, emphasis of times, and appropriateness for a client-facing private investigation report.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Readability, Tone, and Presentation Quality", "description": "Assess the overall professionalism of the final report for a client audience.", "weight": 2.0, "judge_prompt": "Evaluate the PDF for professional quality. Consider:\n- Clarity and readability: concise, well-edited prose with correct grammar, punctuation, and sentence structure.\n- Emphasis on times of relevant activities: timestamps prominent and easy to scan.\n- Organization and flow: Summary, Surveillance, and Assessment sections guide the reader logically; formatting is clean.\n- Objective, neutral tone appropriate to a private investigation deliverable; avoids speculation.\n- Focus and length discipline: no unnecessary details; content fits comfortably within 1\u20132 pages.\n\nScoring (0.0\u20131.0):\n- 1.0: Highly professional, concise, clearly emphasizes times, excellent organization and tone.\n- 0.6: Generally professional with minor issues (occasional verbosity, small grammar/style lapses).\n- 0.3: Noticeable clarity/organization issues or inconsistent tone.\n- 0.0: Poorly written, confusing, or unprofessional presentation.", "expectation": "A polished, client-ready PDF report that is concise, objective, and easy to scan for key times and observations."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "8f9e8bcd-6102-40da-ab76-23f51d8b21fa", "rubric": {"category_name": "Retail Training Document: Overcoming Bridal Sales Objections", "rationale": "This rubric enforces a self-documenting, verifiable Word/PDF training document with clear sections and artifacts that make validation trivial. Stage 1 (LLM-only) strictly checks document shape and required sections. Stage 2 combines code-based text parsing to verify key contractual elements (coverage of objection types, practice set completeness, homework specifics) plus an LLM cross-check for a concrete strategy framework. Stage 3 applies an LLM holistic quality assessment for professionalism, clarity, and bridal retail relevance.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 Gate \u2014 Document Format and Structure", "description": "LLM-only gate to enforce exact document shape required for verification. Checks file type, page length, and required sections with specified structural elements.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Training Document Requirement (DOCX/PDF)", "description": "Verify the candidate output is a professionally formatted Word or PDF training document with all required sections and structural elements for a bridal sales objections training.", "weight": 4.0, "judge_prompt": "You are the Stage 1 gatekeeper. Check ONLY structure/format\u2014not content quality or correctness.\n\nAcceptable formats: DOCX or PDF only. Minimum length: 2 pages. The document must be a professional training handout for bridal sales associates.\n\nRequired structure and elements (be flexible with exact header wording, but the intent must be clear):\n1) Overview (first page):\n   - States why overcoming objections is critical in a bridal store context and mentions decline in closing conversion rate as the reason for the training.\n   - Mentions the most common objections in brief.\n2) Types of Objections:\n   - Contains the five types: price (cost/budget), need (necessity/relevance), urgency (time frame), trust (company/product uncertainty), authority (needs to check with partner/parent/friend).\n   - Each type has a short description and at least one example.\n3) Core Strategies to Overcoming the Objection:\n   - Presents a practical, step-by-step framework (e.g., LAER/LAARC, Feel\u2013Felt\u2013Found, or similar) with clearly labeled steps.\n4) Let\u2019s Practice:\n   - Provides practice items as either a table or a structured list mapping: Objection \u2192 Type \u2192 Suggested Response.\n   - Prefer a table with columns labeled like: Objection | Type | Suggested Response. At least 5 examples.\n5) Conclusion:\n   - Brief recap that ties back to purpose (improve individual sales and overall store performance).\n6) Homework:\n   - Instructs each salesperson to track at least 6 objections over one week, including the objection type, how they responded, and whether it resulted in a purchase.\n   - Includes a fill-in Due Date line and a fill-in line to print the salesperson\u2019s name (e.g., \u201cDue Date: ________\u201d, \u201cSalesperson Name (Print): ________\u201d).\n\nScoring (0.0\u20131.0):\n- 1.0: DOCX/PDF, 2+ pages, and all 6 sections present with required elements (including the table/list in Let\u2019s Practice with \u22655 examples and the Homework fill-in lines).\n- 0.85: All sections present but one minor structural element missing (e.g., Let\u2019s Practice is a list not a table, or one example missing).\n- 0.70: Missing one required section OR Homework lacks fill-in lines OR Let\u2019s Practice has <5 examples.\n- 0.40: Missing two required sections OR not clearly structured headings.\n- 0.0: Not DOCX/PDF OR <2 pages OR missing 3+ sections.\n\nReturn a score only for structural compliance. Do not judge writing quality or correctness.", "expectation": "A 2+ page DOCX/PDF with clearly labeled sections, the five objection types each with examples, a practical strategy framework, a practice table/list mapping Objection\u2192Type\u2192Suggested Response (\u22655 items), and a Homework section with the specified tracking instructions plus fill-in lines for due date and salesperson name."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification of Contracted Elements", "description": "Code and LLM checks to verify the presence and completeness of required content elements enabled by Stage 1\u2019s structure.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Five Objection Types Coverage with Examples", "description": "Verify the 'Types of Objections' section covers all five required types (price, need, urgency, trust, authority) and includes examples.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Returns 0.0\u20131.0 based on:\n    - All five types mentioned (price/cost/budget; need/necessity/relevance; urgency/timeframe/timeline; trust/credibility/confidence; authority/decision/partner/parent/friend)\n    - Examples indicated within the Types section (keywords like 'example', 'e.g.', 'for example', or presence of multiple bullets)\n    \"\"\"\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output resource found.\"\n    if not getattr(output, 'is_document', False):\n        return 0.0, \"Primary output is not a document (DOCX/PDF).\"\n\n    text = None\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = None\n    if not text:\n        return 0.0, \"Unable to extract text from document.\"\n\n    low = text.lower()\n\n    # Attempt to isolate the \"Types of Objections\" section\n    headers = [\n        'types of objections', 'objection types', 'types of customer objections',\n        'types of bridal objections', 'types of sales objections'\n    ]\n    sec_start = -1\n    for h in headers:\n        sec_start = low.find(h)\n        if sec_start != -1:\n            break\n    if sec_start == -1:\n        # fallback to full text\n        section = low\n    else:\n        # naive slice until next major header keyword\n        next_headers = ['core strategies', 'strategies', \"let's practice\", 'lets practice', 'practice', 'conclusion', 'homework']\n        # find nearest next header after sec_start\n        next_pos = len(low)\n        for nh in next_headers:\n            p = low.find(nh, sec_start + 1)\n            if p != -1 and p < next_pos:\n                next_pos = p\n        section = low[sec_start:next_pos]\n\n    # Synonym sets for each type\n    syn = {\n        'price': [r'price', r'cost', r'budget'],\n        'need': [r'need', r'necessit', r'relevan'],\n        'urgency': [r'urgenc', r'time\\s*frame', r'timeline', r'deadline', r'timing'],\n        'trust': [r'trust', r'credibil', r'confiden', r'review', r'reputation'],\n        'authority': [r'authorit', r'decision', r'partner', r'parent', r'friend', r'approval']\n    }\n\n    covered = 0\n    missing = []\n    for t, patterns in syn.items():\n        found = any(re.search(p, section) for p in patterns)\n        if found:\n            covered += 1\n        else:\n            missing.append(t)\n\n    coverage_ratio = covered / 5.0\n\n    # Detect presence of examples in the section\n    example_tokens = ['e.g.', 'example', 'for example', 'eg.']\n    bullets = len(re.findall(r'(^|\\n)[\\-\\*\\u2022]\\s', section))\n    has_examples = any(tok in section for tok in example_tokens) or bullets >= 5\n\n    score = 0.7 * coverage_ratio\n    if coverage_ratio >= 1.0 and has_examples:\n        score += 0.3\n    score = max(0.0, min(1.0, score))\n\n    feedback = f\"Types covered: {covered}/5. Missing: {', '.join(missing) if missing else 'None'}. Examples detected: {'yes' if has_examples else 'no'}.\"\n    return score, feedback"}, {"type": "code", "name": "Homework Requirements Present", "description": "Verify Homework section includes: track \u22656 objections over a week, include type, response, and purchase outcome; plus fill-in Due Date and Salesperson Name lines.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Returns 0.0\u20131.0 based on Homework section completeness:\n    Checks for: (1) number >=6, (2) week, (3) objection(s), (4) type, (5) respond*, (6) purchase/sale outcome, (7) Due Date with blank line, (8) Name with blank line.\n    \"\"\"\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output resource found.\"\n    if not getattr(output, 'is_document', False):\n        return 0.0, \"Primary output is not a document (DOCX/PDF).\"\n\n    text = None\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = None\n    if not text:\n        return 0.0, \"Unable to extract text from document.\"\n\n    low = text.lower()\n\n    # Extract Homework section\n    start_keys = ['homework', 'assignment', 'action items']\n    end_keys = [\"conclusion\", \"let's practice\", 'lets practice', 'practice', 'appendix']\n    s = -1\n    for k in start_keys:\n        s = low.find(k)\n        if s != -1:\n            break\n    if s == -1:\n        section = low\n    else:\n        epos = len(low)\n        for k in end_keys:\n            p = low.find(k, s + 1)\n            if p != -1 and p < epos:\n                epos = p\n        section = low[s:epos]\n\n    checks = {}\n    checks['six'] = bool(re.search(r'\\b(6|six)\\b', section))\n    checks['week'] = 'week' in section\n    checks['objection'] = 'objection' in section\n    checks['type'] = 'type' in section\n    checks['respond'] = 'respond' in section or 'response' in section\n    checks['purchase'] = ('purchase' in section) or ('sale' in section)\n\n    # Due date with blank line\n    due_line = bool(re.search(r'due\\s*date[^\\n]*(_{3,}|\\.{3,})', section)) or \\\n               bool(re.search(r'due\\s*date\\s*:\\s*$', section, re.MULTILINE))\n    checks['due_date_line'] = due_line\n\n    # Name with blank line\n    name_line = bool(re.search(r'(name|print name|salesperson name)[^\\n]*(_{3,}|\\.{3,})', section)) or \\\n                bool(re.search(r'(name|print name|salesperson name)\\s*:\\s*$', section, re.MULTILINE))\n    checks['name_line'] = name_line\n\n    passed = sum(1 for v in checks.values() if v)\n    score = passed / 8.0\n    feedback = ' | '.join([f\"{k}:{'\u2714' if v else '\u2718'}\" for k, v in checks.items()])\n    return score, f\"Homework checks passed {passed}/8. Details: {feedback}\""}, {"type": "code", "name": "Practice Section: Mapping and Volume", "description": "Verify the Let\u2019s Practice section has a recognizable mapping Objection\u2192Type\u2192Suggested Response and at least 5 practice items.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Returns 0.0\u20131.0 based on:\n    - Presence of mapping headers/labels (Objection, Type, Suggested Response/Response)\n    - Count of practice items (aim for >=5). Uses counts of 'Type:'/'Response:' labels or bullet lines as proxy.\n    \"\"\"\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output resource found.\"\n    if not getattr(output, 'is_document', False):\n        return 0.0, \"Primary output is not a document (DOCX/PDF).\"\n\n    text = None\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = None\n    if not text:\n        return 0.0, \"Unable to extract text from document.\"\n\n    low = text.lower()\n\n    # Extract Practice section\n    start_keys = [\"let's practice\", 'lets practice', 'practice']\n    end_keys = ['conclusion', 'homework']\n    s = -1\n    for k in start_keys:\n        s = low.find(k)\n        if s != -1:\n            break\n    if s == -1:\n        section = low\n    else:\n        epos = len(low)\n        for k in end_keys:\n            p = low.find(k, s + 1)\n            if p != -1 and p < epos:\n                epos = p\n        section = low[s:epos]\n\n    # Header/label presence\n    has_obj = 'objection' in section\n    has_type = 'type' in section\n    has_resp = ('suggested response' in section) or ('response' in section) or ('reply' in section)\n\n    # Count items via labels\n    type_labels = len(re.findall(r'\\btype\\b\\s*[:\\-]', section))\n    resp_labels = len(re.findall(r'\\b(suggested\\s*response|response|reply)\\b\\s*[:\\-]', section))\n    items = max(type_labels, resp_labels)\n\n    # Fallback: bullets\n    if items == 0:\n        bullets = re.findall(r'(^|\\n)[\\-\\*\\u2022]\\s', section)\n        items = len(bullets)\n\n    items_score = min(items / 5.0, 1.0)\n    header_score = 1.0 if (has_obj and has_type and has_resp) else 0.0\n\n    score = 0.7 * items_score + 0.3 * header_score\n    score = max(0.0, min(1.0, score))\n\n    feedback = f\"Detected items: {items}. Labels -> Objection:{'\u2714' if has_obj else '\u2718'}, Type:{'\u2714' if has_type else '\u2718'}, Response:{'\u2714' if has_resp else '\u2718'}.\"\n    return score, feedback"}, {"type": "llm_judge", "name": "Strategy Framework Completeness and Bridal Fit", "description": "Check that the Core Strategies section presents a concrete, step-by-step framework (e.g., LAER/LAARC, Feel\u2013Felt\u2013Found, or equivalent), includes sample phrasing, and maps strategies to each objection type in a bridal context.", "weight": 0.5, "judge_prompt": "Evaluate ONLY the Core Strategies section for framework completeness and bridal relevance.\n\nCriteria:\n- A named or clearly delineated multi-step framework is present (steps labeled or numbered).\n- Includes example phrasing/scripts for associates to use.\n- Explicitly maps or gives examples of how to apply the framework to each of the five objection types (price, need, urgency, trust, authority) in a bridal context (e.g., dress timelines, alterations, fittings, partner involvement).\n\nScoring (0.0\u20131.0):\n- 1.0: Clear multi-step framework + sample phrasing + mapping/examples across all 5 types and bridal-specific context.\n- 0.7: Solid framework and scripts but missing explicit mapping for 1\u20132 types or light on bridal specifics.\n- 0.4: Generic high-level tips, minimal steps or scripts, weak bridal tailoring.\n- 0.0: No coherent framework present.", "expectation": "A practical, labeled step-by-step objection-handling framework with sample scripts tailored to bridal scenarios and coverage of all five objection types."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Overall Quality and Usefulness", "description": "Holistic assessment of professionalism, clarity, and usefulness for a bridal sales team audience.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Training Value", "description": "Evaluate overall quality: clarity, organization, tone, readability, bridal specificity, and actionability for retail associates.", "weight": 2.0, "judge_prompt": "Assess the document holistically for a bridal store sales team training.\n\nConsider:\n- Clarity and organization of sections; headings, lists, and tables improve readability.\n- Professional, supportive training tone suitable for new and seasoned associates.\n- Bridal retail specificity (e.g., fittings, alterations, delivery timelines, deposit policies, coordination with partners/parents).\n- Actionability: concrete scripts, checklists, and examples that can be put into practice.\n- Conciseness and visual cleanliness appropriate for a brief training handout.\n\nScoring (0.0\u20131.0):\n- 1.0: Highly professional, clear, bridal-specific, and immediately actionable.\n- 0.7: Generally strong with minor gaps (some generic content or mild clutter).\n- 0.4: Mixed quality; somewhat generic or hard to use.\n- 0.0: Poorly organized or not useful for the intended audience.", "expectation": "A clear, polished, bridal-relevant training handout with actionable scripts and tools suitable for immediate use by sales associates."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "2c249e0f-4a8c-4f8e-b4f4-6508ba29b34f", "rubric": {"category_name": "Robot Fleet Mission Data Ingestion API (OpenAPI + Data Flow)", "rationale": "Self-documenting rubric that forces a verifiable artifact pair: an OpenAPI 3.0+ YAML specification and a companion data_flow.txt. Stage 1 (LLM-only) enforces exact structural shape so downstream verification is trivial. Stage 2 uses code rules to validate key correctness elements (resumable uploads, prioritization of insight vs payload, endpoint coverage, security, and data flow completeness). Stage 3 assesses professional quality and appropriateness for stakeholders.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Shape and Structure Gate (LLM-only)", "description": "Gate: Output must include an OpenAPI 3.0+ YAML spec and a data_flow.txt file with required structural elements enabling verification. Only structure, not correctness.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Output Format Requirement (OpenAPI + Data Flow)", "description": "Check presence and structural completeness of OpenAPI YAML and data_flow.txt with required sections and endpoints. Do not judge content quality or correctness beyond presence/structure.", "weight": 4.0, "judge_prompt": "You are evaluating whether the candidate delivered the required files in the required structure. Only check presence and structural completeness. Do NOT judge content quality or correctness.\n\nFiles expected:\n1) An OpenAPI 3.0+ specification in YAML format (file extension .yaml or .yml). It must not be JSON.\n2) A text file named exactly data_flow.txt describing data flow and how robots use the API.\n\nFor the OpenAPI YAML, verify the following structural elements exist (be flexible with exact names but require the essence):\n- Top-level fields: openapi (version 3.x), info (with title and version), servers, paths, components.\n- Security: components.securitySchemes (e.g., bearer or API key) and a top-level security requirement or per-operation security.\n- Tags: Include tags covering at least Missions, Uploads, and Insights (flexible naming is fine, e.g., \"Mission\", \"Data Uploads\", \"Insight Delivery\").\n- Components.schemas with at least these conceptual schemas (names can vary, but intent must be clear): Mission, SensorFile (or similar), UploadSession (or ChunkedUpload), ProcessingStageStatus (pipeline stage/state), InsightAsset (deliverable for customers).\n- Paths covering the core workflow (names can vary, methods can be POST/PUT/PATCH where appropriate):\n  1. Create/Register Mission: e.g., POST /missions\n  2. Update/Resume Mission: e.g., PATCH or POST /missions/{missionId}/resume (or similar ability to resume)\n  3. Initiate Upload Session for mission files: e.g., POST /missions/{missionId}/uploads (or pre-signed URL flow)\n  4. Chunked/resumable upload endpoint: e.g., POST/PUT /uploads/{uploadId}/chunks (must exist in some form)\n  5. Complete/finalize upload: e.g., POST /uploads/{uploadId}/complete\n  6. Customer insight retrieval/listing endpoint(s): e.g., GET /insights or GET /customers/{customerId}/insights\n  7. Optional: status/health endpoints (e.g., GET /health or GET /missions/{missionId}/status)\n- Resumability and priority present in operation descriptions or parameters (not correctness):\n  - Evidence of resumable/chunked upload (headers like Content-Range or fields like chunkNumber/totalChunks, checksum/ETag, or similar), and language indicating resume on connectivity loss.\n  - Clear differentiation between \"insight\" and \"payload\" data in tags, parameters, schema fields, or descriptions, including prioritization of insight data for faster availability.\n- Cloud services acknowledged in descriptions (not implementation): references to S3 for data storage and DynamoDB for mission metadata and upload status.\n\nFor data_flow.txt, verify it is a plain text file that:\n- Describes the end-to-end flow from robot finishing a mission to customer-accessible insights.\n- Explicitly mentions prioritization of insight data vs payload data (payload less frequent or via SSD shipping; insight urgent).\n- Mentions resumable transfers to handle connectivity loss.\n- Mentions base station high-speed internet (~1 Gbps via satellite or ethernet).\n- Mentions multi-stage cloud processing pipeline after upload completion.\n- Mentions DynamoDB (mission metadata/upload status) and S3 (object storage).\n- Mentions multiple missions per day and mission resume after recharge/battery limits.\n\nScoring guidance (STRUCTURE ONLY):\n- 1.0: Both files present; OpenAPI YAML includes all required top-level sections, schemas, security, and core paths; data_flow.txt covers all listed bullet points.\n- 0.7: Both files present; minor omissions (e.g., missing one schema or one path, or data_flow.txt missing one bullet).\n- 0.4: OpenAPI YAML present but missing multiple required sections or endpoints; or data_flow.txt very brief/missing several bullets.\n- 0.0: Missing OpenAPI YAML or missing data_flow.txt; or OpenAPI not in YAML format.\n\nOnly check presence/structure, not correctness or design quality.", "expectation": "A properly structured OpenAPI YAML with the listed sections and endpoints, plus a comprehensive data_flow.txt covering all enumerated bullet points."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification and Consistency Checks (Code + Light LLM)", "description": "Deterministic checks on the delivered files for key correctness cues: OpenAPI structure keywords, endpoint coverage, resumable/priority semantics, security, and data flow completeness and cross-file consistency.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "File Presence and Basic OpenAPI Structure", "description": "Ensure a .yaml/.yml OpenAPI file exists and contains key top-level keywords; ensure data_flow.txt exists.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs()\n        if not outputs:\n            return 0.0, \"No outputs.\"\n        # Locate files\n        yaml_res = None\n        data_flow_res = None\n        for r in outputs:\n            try:\n                p = context.files.get_path(r.id)\n                name = p.name.lower()\n                if name.endswith('.yaml') or name.endswith('.yml'):\n                    yaml_res = r if yaml_res is None else yaml_res\n                if name == 'data_flow.txt':\n                    data_flow_res = r\n            except Exception:\n                continue\n        if yaml_res is None:\n            return 0.0, \"Missing OpenAPI YAML (.yaml/.yml).\"\n        if data_flow_res is None:\n            return 0.25, \"Missing data_flow.txt but YAML present.\"\n        # Read YAML text\n        txt = context.files.read_text(yaml_res.id)\n        low = txt.lower()\n        checks = 0\n        total = 5\n        if 'openapi:' in low and re.search(r'openapi:\\s*3', low):\n            checks += 1\n        if 'info:' in low and 'title:' in low and 'version:' in low:\n            checks += 1\n        if 'servers:' in low:\n            checks += 1\n        if 'paths:' in low:\n            checks += 1\n        if 'components:' in low:\n            checks += 1\n        score = checks/total\n        return score, f\"Basic YAML structure checks passed {checks}/{total}.\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Endpoint Coverage for Mission Lifecycle and Uploads", "description": "Check that the OpenAPI YAML references key paths for missions, uploads (init, chunks, complete), and insights retrieval.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs()\n        yaml_text = ''\n        for r in outputs:\n            p = context.files.get_path(r.id)\n            if p.name.lower().endswith(('.yaml','.yml')):\n                yaml_text = context.files.read_text(r.id)\n                break\n        if not yaml_text:\n            return 0.0, \"No OpenAPI YAML found.\"\n        low = yaml_text.lower()\n        checks = []\n        # Use flexible checks for presence of path strings\n        checks.append('/missions' in low)  # create/list missions\n        checks.append('/missions/{' in low)  # parameterized mission path\n        # Upload session initiation under mission\n        checks.append(('/uploads' in low and '/missions/' in low) or '/missions/{' in low and '/uploads' in low)\n        # Chunked upload endpoint\n        checks.append('/chunks' in low)\n        # Upload complete endpoint\n        checks.append('/complete' in low)\n        # Insights retrieval endpoint(s)\n        checks.append('/insight' in low or '/customers/' in low)\n        passed = sum(1 for c in checks if c)\n        total = len(checks)\n        score = passed/total\n        return score, f\"Endpoint coverage passed {passed}/{total}.\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Resumable Uploads and Priority Semantics", "description": "Verify presence of resumable upload cues (Content-Range, chunk fields, checksum/ETag) and explicit insight vs payload prioritization.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs()\n        yaml_text = ''\n        for r in outputs:\n            p = context.files.get_path(r.id)\n            if p.name.lower().endswith(('.yaml','.yml')):\n                yaml_text = context.files.read_text(r.id)\n                break\n        if not yaml_text:\n            return 0.0, \"No OpenAPI YAML found.\"\n        low = yaml_text.lower()\n        checks = 0\n        total = 5\n        if 'content-range' in low or 'range' in low:\n            checks += 1\n        if 'etag' in low or 'checksum' in low or 'md5' in low or 'sha256' in low:\n            checks += 1\n        if 'chunk' in low or 'chunks' in low or 'chunknumber' in low or 'totalchunks' in low:\n            checks += 1\n        if 'insight' in low:\n            checks += 1\n        if 'payload' in low:\n            checks += 1\n        score = checks/total\n        return score, f\"Resumable/priority cues passed {checks}/{total}.\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Security and Servers", "description": "Confirm securitySchemes and some form of security requirement exist; and that servers use https; info.version present.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs()\n        yaml_text = ''\n        for r in outputs:\n            p = context.files.get_path(r.id)\n            if p.name.lower().endswith(('.yaml','.yml')):\n                yaml_text = context.files.read_text(r.id)\n                break\n        if not yaml_text:\n            return 0.0, \"No OpenAPI YAML found.\"\n        low = yaml_text.lower()\n        checks = 0\n        total = 4\n        if 'securityschemes:' in low:\n            checks += 1\n        if '\\nsecurity:' in low or '\\n  security:' in low:\n            checks += 1\n        if 'servers:' in low and 'https' in low:\n            checks += 1\n        if 'info:' in low and 'version:' in low:\n            checks += 1\n        return checks/total, f\"Security/servers checks passed {checks}/{total}.\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Data Flow Completeness and Cross-File Consistency", "description": "Validate data_flow.txt covers key bullets and aligns with OpenAPI on terms: insight, payload, and resumable behavior. Also confirm S3 and DynamoDB are mentioned across files.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs()\n        yaml_text = ''\n        flow_text = ''\n        yaml_name = ''\n        flow_name = ''\n        for r in outputs:\n            p = context.files.get_path(r.id)\n            name = p.name.lower()\n            if name.endswith(('.yaml','.yml')) and not yaml_text:\n                yaml_text = context.files.read_text(r.id)\n                yaml_name = name\n            if name == 'data_flow.txt' and not flow_text:\n                flow_text = context.files.read_text(r.id)\n                flow_name = name\n        if not flow_text:\n            return 0.0, \"Missing data_flow.txt.\"\n        lowf = flow_text.lower()\n        lowy = yaml_text.lower() if yaml_text else ''\n        checks = 0\n        total = 9\n        # Key bullet coverage in data_flow.txt\n        if 'insight' in lowf and 'payload' in lowf:\n            checks += 1\n        if '1gbps' in lowf or '1 gbps' in lowf or 'high-speed' in lowf or 'gigabit' in lowf:\n            checks += 1\n        if 'satellite' in lowf or 'ethernet' in lowf:\n            checks += 1\n        if 'resume' in lowf or 'resumable' in lowf or 'reconnect' in lowf:\n            checks += 1\n        if 'pipeline' in lowf or 'multi-stage' in lowf or 'multistage' in lowf:\n            checks += 1\n        if 's3' in lowf:\n            checks += 1\n        if 'dynamodb' in lowf:\n            checks += 1\n        if 'battery' in lowf or 'recharge' in lowf:\n            checks += 1\n        if 'customer' in lowf and ('available' in lowf or 'availability' in lowf or 'deliver' in lowf):\n            checks += 1\n        # Cross-file consistency: both files reference key terms\n        bonus_checks = 0\n        bonus_total = 3\n        if lowy:\n            if ('insight' in lowy and 'payload' in lowy):\n                bonus_checks += 1\n            if ('resume' in lowy or 'resumable' in lowy or 'content-range' in lowy or 'chunk' in lowy):\n                bonus_checks += 1\n            if ('s3' in lowy and 'dynamodb' in lowy):\n                bonus_checks += 1\n        score = (checks/total)\n        # Blend in cross-file consistency at 30% weight of this rule\n        if bonus_total > 0:\n            score = 0.7*score + 0.3*(bonus_checks/bonus_total)\n        score = max(0.0, min(1.0, score))\n        return score, f\"Data flow coverage {checks}/{total}; cross-file {bonus_checks}/{bonus_total}.\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism (LLM)", "description": "Holistic assessment of the API design clarity, professionalism, and suitability for robot fleet operations at scale.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Quality and Fitness for Purpose", "description": "Assess whether the API is professionally presented and well-suited to the constraints (robot fleet at scale, resumable uploads, prioritization, pipeline). Prefer concrete, implementable detail.", "weight": 2.0, "judge_prompt": "Evaluate the OpenAPI YAML and data_flow.txt for overall quality and fitness for the stated use case. Consider:\n- Clarity and completeness of the API: Are operations coherently grouped, tagged, and documented? Are request/response bodies and error codes described? Is pagination/rate limiting addressed where relevant (e.g., listing missions/insights)?\n- Practicality for robots: Are resumable uploads clearly specified with headers/fields, idempotency guidance (e.g., idempotency keys), chunk sizing, checksums, and retry semantics? Are pre-signed URL approaches described if applicable? Are timeouts and limits noted?\n- Prioritization of insight data: Does the design make it straightforward to prioritize insight uploads and surface them quickly to customers (e.g., priority flags, separate queues/paths, SLAs)?\n- Cloud processing pipeline: Does the spec and text describe a plausible multi-stage pipeline with statuses, callbacks/webhooks or polling endpoints to track processing and delivery readiness?\n- Security and multi-tenant concerns: Are security schemes appropriate (bearer/apiKey), is tenant isolation addressed (customerId scoping), and is PII/privacy considered if relevant?\n- Cross-file consistency and professional polish: Are terminology and flows consistent between YAML and data_flow.txt? Is formatting clean and professional, with examples where appropriate?\n\nScoring:\n- 1.0\u20131.5: Solid, mostly complete and professional; minor gaps.\n- 1.6\u20132.0: Excellent: production-ready clarity, strong operational detail, thoughtful edge cases.\n- 0.4\u20130.9: Some useful structure but notable gaps in clarity or practicality.\n- 0.0\u20130.3: Poor quality, unclear or not suitable for purpose.", "expectation": "A professional, production-minded API spec and data flow that would be credible to platform and robotics engineers managing large robot fleets."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "6074bba3-7e3a-4b1c-b8c6-a15bb6695c3b", "rubric": {"category_name": "CMA Report Evaluation - Real Estate Broker (Mixed: Document + Embedded Analysis)", "rationale": "This rubric enforces a self-documenting, verifiable CMA deliverable. Stage 1 (LLM-only gate) mandates a specific PDF structure with required sections, tables, and graphs. Stage 2 mixes code checks (text parsing from PDF) with an LLM consistency check to verify correctness signals enabled by Stage 1\u2019s structure. Stage 3 assesses overall professionalism and strategic value for the client\u2019s listing decision.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 - CMA Format and Structure Gate (LLM Only)", "description": "Verify the deliverable is a properly structured CMA PDF with all required sections, tables, and charts. Only check presence/format, not quality or numerical correctness.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "CMA PDF Structured Requirements", "description": "Check the output is a PDF CMA with required sections, tables, and graphs present and clearly labeled. Only enforce shape/presence; do not judge content quality or correctness.", "weight": 8.0, "judge_prompt": "You are evaluating a Comparative Market Analysis (CMA) deliverable. Only check FILE FORMAT and STRUCTURE (presence of required elements) \u2014 do NOT judge analysis quality or numerical correctness.\n\nDeliverable Requirements (STRICT SHAPE):\n- File format: PDF (not Word, not Excel).\n- Professionally structured CMA with clear section headers and tables/charts where required.\n\nRequired Sections and Elements (flexible with naming, but must be clearly present):\n1) Subject Property Summary (must be on the first 1\u20132 pages):\n   - Include the subject address: 112 Pine Crest Ln, Adairsville, GA 30103\n   - Property basics: location, size, use/type (duplex), occupancy/lease status\n2) Comparable Sales (5\u201310 recent comps, ideally past 6\u201312 months):\n   - A table is required (rows = comps). Columns should visibly cover most of: Address, Beds, Baths, Sq Ft (or GLA), Sale Date (or Closed), Sale Price, DOM, and ideally $/SF or Distance\n3) Active or Pending Listings (3\u20135):\n   - A table is required (rows = listings). Columns should visibly cover most of: Address, Status (Active/Pending), List Price, Beds, Baths, Sq Ft, DOM\n4) Valuation Summary & Recommendation:\n   - Explicit pricing tiers (Low / Mid or Most Likely / High) with a recommended list price and short rationale\n5) Charts/Graphs:\n   - A \u201cList Price vs Sales Price\u201d chart (or equivalent, e.g., Sold-to-List comparison)\n   - A \u201cDays on Market\u201d (DOM) chart\n\nOptional but acceptable if present:\n- Data sources/references section (e.g., Zillow, Redfin, Realtor.com, public records)\n- Methodology notes and assumptions\n\nScoring (STRUCTURE ONLY):\n- 8.0: PDF format AND all required sections (1\u20135) present with appropriate tables/charts clearly labeled\n- 7.0: All required sections present but one table/chart is ambiguously labeled or slightly incomplete\n- 6.0: Missing exactly one required section OR one required table is clearly missing\n- 3.0\u20135.0: Two required elements missing (e.g., one section and one chart) but still a multi-section PDF CMA\n- 1.0\u20132.0: Minimal CMA structure (e.g., subject summary only) with most required elements missing\n- 0.0: Not a PDF OR lacks recognizable CMA structure\n\nBe flexible on exact section names (e.g., \u201cSales Comparables\u201d for Comparable Sales, \u201cPricing Recommendation\u201d for Valuation Summary, \u201cSold-to-List\u201d for the LP vs SP chart). Only evaluate presence and format, not correctness or quality.", "expectation": "A multi-page PDF CMA that includes: subject summary with address and duplex details; a comps table (5\u201310); an active/pending table (3\u20135); a valuation summary with low/mid/high tiers and recommendation; and two charts (List vs Sale; Days on Market)."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 - Verification (Code + LLM)", "description": "Now that the PDF has a verifiable structure, check correctness signals: presence of the subject details, counts of comps and actives, presence of valuation tiers and charts references, locality and recency hints, plus a consistency check that the recommendation lies within the stated range.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Subject Property Mention and Key Attributes", "description": "Verify the PDF text mentions the subject address, duplex property type, and ideally key attributes (beds/baths) and occupancy/lease info.", "weight": 1.2, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Checks PDF text for:\n    - Address: \"112 Pine Crest\" and \"Adairsville\"/\"GA\"\n    - Property type: duplex (flexible synonyms)\n    - Beds/Baths counts: 4 bed, 2 bath (flexible patterns)\n    - Occupancy/lease terms: occupied/vacant/lease/tenant\n    Returns a score in [0,1].\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n\n    text = \"\"\n    try:\n        if output.file_extension.lower() == '.pdf' or output.mime_type == 'application/pdf':\n            text = context.files.read_pdf_text(output.id) or \"\"\n        elif output.file_extension.lower() in ('.docx',) or (output.mime_type and 'word' in output.mime_type.lower()):\n            text = context.files.read_docx_text(output.id) or \"\"\n        else:\n            text = context.files.read_text(output.id) or \"\"\n    except Exception:\n        text = \"\"\n\n    lt = text.lower()\n    score = 0.0\n    feedback = []\n\n    # Address\n    addr_ok = ('112 pine crest' in lt) and ('adairsville' in lt or 'ga' in lt or 'georgia' in lt)\n    score += 0.4 if addr_ok else 0.0\n    feedback.append(f\"Address found: {addr_ok}\")\n\n    # Duplex property type\n    duplex_ok = bool(re.search(r\"\\b(duplex|2[- ]unit|two[- ]unit|multi[- ]family)\\b\", lt))\n    score += 0.3 if duplex_ok else 0.0\n    feedback.append(f\"Duplex/type found: {duplex_ok}\")\n\n    # Beds/Baths (4/2) flexible patterns\n    beds_ok = bool(re.search(r\"\\b4\\s*(bed|bd|bedroom)s?\\b|\\b4br\\b\", lt))\n    baths_ok = bool(re.search(r\"\\b2\\s*(bath|ba|bathroom)s?\\b|\\b2ba\\b\", lt))\n    bb_score = 0.2 if (beds_ok and baths_ok) else 0.1 if (beds_ok or baths_ok) else 0.0\n    score += bb_score\n    feedback.append(f\"Beds/Baths detected: beds={beds_ok}, baths={baths_ok}\")\n\n    # Occupancy / lease details\n    occ_ok = bool(re.search(r\"\\b(occupied|vacant|lease|tenant|rented)\\b\", lt))\n    score += 0.1 if occ_ok else 0.0\n    feedback.append(f\"Occupancy/lease mention: {occ_ok}\")\n\n    return min(score, 1.0), \"; \".join(feedback)\n"}, {"type": "code", "name": "Comparable Sales Count Detected", "description": "Detect that 5\u201310 comparable sales are present by parsing the Comparable Sales section for address-like rows and pricing indicators.", "weight": 1.8, "code": "import re\n\ndef _extract_section(text, start_heads, end_heads):\n    lt = text.lower()\n    start_positions = [lt.find(h) for h in start_heads]\n    start_positions = [p for p in start_positions if p >= 0]\n    if not start_positions:\n        return \"\"\n    start = min(start_positions)\n    # Find end as next heading occurrence after start\n    end = len(lt)\n    for eh in end_heads:\n        p = lt.find(eh, start + 1)\n        if p >= 0:\n            end = min(end, p)\n    return text[start:end]\n\ndef _count_addresses(seg):\n    if not seg:\n        return 0\n    pattern = re.compile(r\"\\b\\d{2,5}\\s+[A-Za-z0-9\\.'\u2019\\- ]+\\s+(st|street|ave|avenue|rd|road|ln|lane|dr|drive|ct|court|way|blvd|boulevard|pkwy|parkway|ter|terrace|pl|place|cir|circle)\\b\", re.IGNORECASE)\n    addrs = set(m.group(0).lower() for m in pattern.finditer(seg))\n    return len(addrs)\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n    try:\n        if output.file_extension.lower() == '.pdf' or output.mime_type == 'application/pdf':\n            text = context.files.read_pdf_text(output.id) or \"\"\n        elif output.file_extension.lower() in ('.docx',):\n            text = context.files.read_docx_text(output.id) or \"\"\n        else:\n            text = context.files.read_text(output.id) or \"\"\n    except Exception:\n        return 0.0, \"Unable to read document text\"\n\n    start_heads = [\n        'comparable sales','sales comparables','sold comparables','sold comps',\n        'closed sales','sold listings','recent sales','comparables'\n    ]\n    end_heads = [\n        'active','pending','listings','valuation','summary','recommendation','pricing','charts','appendix'\n    ]\n    seg = _extract_section(text, start_heads, end_heads)\n    n = _count_addresses(seg)\n    has_price_indicator = ('$' in seg) or ('sale price' in seg.lower()) or ('sold price' in seg.lower())\n\n    # Scoring map\n    if n >= 5:\n        base = 1.0\n    elif n == 4:\n        base = 0.8\n    elif n == 3:\n        base = 0.6\n    elif n == 2:\n        base = 0.3\n    else:\n        base = 0.0\n\n    if base > 0 and not has_price_indicator:\n        base = max(0.0, base - 0.2)\n\n    feedback = f\"Detected {n} comparable addresses; price indicator: {has_price_indicator}\"\n    return base, feedback\n"}, {"type": "code", "name": "Active/Pending Listings Count Detected", "description": "Detect that 3\u20135 active/pending listings are present by parsing the Active/Pending section for address-like rows and list price indicators.", "weight": 1.2, "code": "import re\n\ndef _extract_section(text, start_heads, end_heads):\n    lt = text.lower()\n    start_positions = [lt.find(h) for h in start_heads]\n    start_positions = [p for p in start_positions if p >= 0]\n    if not start_positions:\n        return \"\"\n    start = min(start_positions)\n    end = len(lt)\n    for eh in end_heads:\n        p = lt.find(eh, start + 1)\n        if p >= 0:\n            end = min(end, p)\n    return text[start:end]\n\ndef _count_addresses(seg):\n    if not seg:\n        return 0\n    pattern = re.compile(r\"\\b\\d{2,5}\\s+[A-Za-z0-9\\.'\u2019\\- ]+\\s+(st|street|ave|avenue|rd|road|ln|lane|dr|drive|ct|court|way|blvd|boulevard|pkwy|parkway|ter|terrace|pl|place|cir|circle)\\b\", re.IGNORECASE)\n    addrs = set(m.group(0).lower() for m in pattern.finditer(seg))\n    return len(addrs)\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n    try:\n        if output.file_extension.lower() == '.pdf' or output.mime_type == 'application/pdf':\n            text = context.files.read_pdf_text(output.id) or \"\"\n        elif output.file_extension.lower() in ('.docx',):\n            text = context.files.read_docx_text(output.id) or \"\"\n        else:\n            text = context.files.read_text(output.id) or \"\"\n    except Exception:\n        return 0.0, \"Unable to read document text\"\n\n    start_heads = [\n        'active listings','active/pending','active & pending','active and pending','active','pending listings','pending'\n    ]\n    end_heads = ['valuation','summary','recommendation','pricing','charts','appendix']\n    seg = _extract_section(text, start_heads, end_heads)\n    n = _count_addresses(seg)\n    has_list_price = ('$' in seg) or ('list price' in seg.lower()) or ('asking' in seg.lower())\n\n    if n >= 3:\n        base = 1.0\n    elif n == 2:\n        base = 0.66\n    elif n == 1:\n        base = 0.33\n    else:\n        base = 0.0\n\n    if base > 0 and not has_list_price:\n        base = max(0.0, base - 0.16)\n\n    feedback = f\"Detected {n} active/pending addresses; list price indicator: {has_list_price}\"\n    return base, feedback\n"}, {"type": "code", "name": "Valuation Tiers and Price Range Present", "description": "Check that the valuation summary includes pricing tiers (low/mid/high) with at least two dollar amounts and a recommended list price mention.", "weight": 1.4, "code": "import re\n\ndef _extract_section(text, heads):\n    lt = text.lower()\n    starts = [lt.find(h) for h in heads]\n    starts = [s for s in starts if s >= 0]\n    if not starts:\n        return \"\"\n    start = min(starts)\n    end_markers = ['appendix','charts','data sources','methodology','supporting data']\n    end = len(lt)\n    for em in end_markers:\n        p = lt.find(em, start + 1)\n        if p >= 0:\n            end = min(end, p)\n    return text[start:end]\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n    try:\n        if output.file_extension.lower() == '.pdf' or output.mime_type == 'application/pdf':\n            text = context.files.read_pdf_text(output.id) or \"\"\n        elif output.file_extension.lower() in ('.docx',):\n            text = context.files.read_docx_text(output.id) or \"\"\n        else:\n            text = context.files.read_text(output.id) or \"\"\n    except Exception:\n        return 0.0, \"Unable to read document text\"\n\n    seg = _extract_section(text, ['valuation summary','pricing recommendation','summary valuation','pricing summary','recommendation'])\n    lt = seg.lower()\n    dollars = re.findall(r\"\\$\\s*[\\d,]+(?:\\.\\d{2})?\", seg)\n    has_low = 'low' in lt\n    has_high = 'high' in lt\n    has_mid = ('mid' in lt) or ('most likely' in lt) or ('base case' in lt)\n    has_list = ('list price' in lt) or ('recommended' in lt) or ('asking price' in lt)\n\n    score = 0.0\n    if len(set(dollars)) >= 2 and (has_low or has_high) and (has_mid or has_low or has_high):\n        score = 1.0\n    elif len(set(dollars)) >= 2 and has_list:\n        score = 0.7\n    elif has_list and len(dollars) >= 1:\n        score = 0.5\n    else:\n        score = 0.0\n\n    fb = f\"Valuation dollars={len(set(dollars))}; tiers: low={has_low}, mid/most={has_mid}, high={has_high}; list mention={has_list}\"\n    return score, fb\n"}, {"type": "code", "name": "Charts/Graphs References Present", "description": "Check that the PDF text references the required charts: List Price vs Sales Price and Days on Market.", "weight": 0.6, "code": "def evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n    try:\n        if output.file_extension.lower() == '.pdf' or output.mime_type == 'application/pdf':\n            text = context.files.read_pdf_text(output.id) or \"\"\n        elif output.file_extension.lower() in ('.docx',):\n            text = context.files.read_docx_text(output.id) or \"\"\n        else:\n            text = context.files.read_text(output.id) or \"\"\n    except Exception:\n        return 0.0, \"Unable to read document text\"\n\n    lt = text.lower()\n    lp_sp = ('list price vs sales price' in lt) or ('list vs sale' in lt) or ('sold-to-list' in lt) or ('lp vs sp' in lt)\n    dom = ('days on market' in lt) or ('dom' in lt)\n\n    if lp_sp and dom:\n        return 1.0, \"Both chart references found\"\n    elif lp_sp or dom:\n        return 0.5, f\"Single chart reference found (lp_vs_sp={lp_sp}, dom={dom})\"\n    else:\n        return 0.0, \"No required chart references found\"\n"}, {"type": "code", "name": "Locality and Recency Sanity Checks", "description": "Check for local market references (Adairsville/GA) and recent year mentions (2023+).", "weight": 0.8, "code": "def evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output\"\n    try:\n        if output.file_extension.lower() == '.pdf' or output.mime_type == 'application/pdf':\n            text = context.files.read_pdf_text(output.id) or \"\"\n        elif output.file_extension.lower() in ('.docx',):\n            text = context.files.read_docx_text(output.id) or \"\"\n        else:\n            text = context.files.read_text(output.id) or \"\"\n    except Exception:\n        return 0.0, \"Unable to read document text\"\n\n    lt = text.lower()\n    locality = any(k in lt for k in ['adairsville', 'bartow', 'ga', 'georgia'])\n    recency = any(y in lt for y in ['2025','2024','2023'])\n\n    score = 0.0\n    if locality:\n        score += 0.5\n    if recency:\n        score += 0.5\n\n    return min(score, 1.0), f\"Locality={locality}, RecencyYearMention={recency}\"\n"}, {"type": "llm_judge", "name": "Recommendation Within Valuation Range (Consistency)", "description": "Verify that the recommended list price lies within the stated valuation range (low/mid/high). Use visual reading of tables/text.", "weight": 1.0, "judge_prompt": "Read the PDF CMA and check for consistency:\n- Identify the Valuation Summary section.\n- Extract the pricing tiers (e.g., Low / Mid or Most Likely / High) with their amounts.\n- Identify the recommended list price.\n- Decide whether the recommendation lies within the stated low\u2013high range, or aligns with the described most likely/mid value.\n\nScoring:\n- 1.0: Clear low\u2013high range and the recommendation is inside the range, or explicitly justified at the mid/most-likely tier\n- 0.6: Range present but recommendation slightly outside; explanation provided\n- 0.3: Range present but recommendation substantially outside with no justification\n- 0.0: No clear range and no identifiable recommendation\n\nBe strict about structural presence and numeric alignment, but do not judge market accuracy or formatting aesthetics.", "expectation": "Recommended list price should be numerically consistent with the disclosed low\u2013high (or low/mid/high) valuation range."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 - Professional Quality and Strategic Value", "description": "Assess the professional presentation, clarity, and strategic usefulness of the CMA for the client\u2019s listing decision within 30\u201360 days.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Presentation, Clarity, and Strategic Usefulness", "description": "Holistic quality assessment of the CMA for a client-ready deliverable.", "weight": 4.0, "judge_prompt": "Evaluate the overall professional quality and usefulness of this CMA. Consider:\n- Professional presentation: clean layout, readable tables, well-labeled charts, clear sectioning\n- Clarity of rationale: the valuation narrative clearly explains how comps and actives inform low/mid/high tiers\n- Strategic value: the recommendation is actionable for listing in 30\u201360 days; notes on pricing strategy, DOM expectations, and market positioning\n- Transparency: data sources and assumptions referenced; limitations/disclaimers included\n\nScoring:\n- 4.0: Highly professional and persuasive; crystal-clear rationale; charts/tables enhance understanding; strong, actionable guidance\n- 3.0: Solid professional quality with minor issues; reasoning is clear; generally useful guidance\n- 2.0: Adequate but modest clarity/formatting; limited strategic insight\n- 1.0: Barely client-ready; unclear rationale; formatting weak\n- 0.0: Not professionally presented or lacks coherent guidance\n\nDo not re-check Stage 1 structure; focus on quality, clarity, and strategic value.", "expectation": "A client-ready CMA PDF that is professional, clearly reasoned, and offers actionable listing guidance aligned with the 30\u201360 day timeline."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "c357f0e2-963d-4eb7-a6fa-3078fe55b3ba", "rubric": {"category_name": "UAT Test Plan (ProjMGR Tool) - Self-Documenting Staged Evaluation", "rationale": "This rubric enforces a self-documenting, Excel-based UAT test plan that is structurally verifiable (Stage 1 gate), then programmatically validated for coverage and correctness (Stage 2), and finally judged for overall professional quality (Stage 3). Stage 1 uses an LLM judge to confirm the exact spreadsheet shape so that Stage 2 code rules can reliably parse and verify coverage across roles, modules, edge cases, and placeholders required for UAT circulation.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (LLM only)", "description": "Verify the candidate output is an Excel UAT plan in the mandated structure so subsequent verification is trivial.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.5, "rules": [{"type": "llm_judge", "name": "Structured UAT Excel Format Requirement", "description": "Check the output is a properly structured Excel UAT plan ready for programmatic verification.", "weight": 6.0, "judge_prompt": "You are evaluating whether the submission is a valid Excel (.xlsx) UAT Test Plan for the ProjMGR Tool with the SPECIFIC structure below. Only check PRESENCE/STRUCTURE, not content quality or calculation correctness.\n\nAcceptable file types: Excel spreadsheet (preferred: .xlsx). Reject if PDF/DOCX/MD/CSV/plain text.\n\nPrimary Sheet (flexible name, e.g., \"UAT Test Plan\", \"Test Cases\", \"UAT Plan\"): Must contain a clearly tabular list of test cases with the following REQUIRED columns (exact names may vary slightly; be flexible with synonyms):\n- Test ID (synonyms: ID, Case ID, TC ID)\n- Role (synonyms: User Role)\n- Module (synonyms: Area, Component)\n- User Action (synonyms: Steps, Action, User Steps)\n- Test Scenario (synonyms: Scenario, Description, Test Case Description)\n- Expected Result (synonyms: Expected, Expected Outcome)\n- Actual Result (keep as blank placeholder)\n- Test Date (keep as blank placeholder)\n\nOPTIONAL helpful columns (do NOT penalize if missing): Submodule/Feature, Preconditions/Prerequisites, Priority/Severity, Requirement/Ref ID, Browser/Environment, Notes.\n\nCoverage expectations (STRUCTURE ONLY):\n- Roles present across the plan: Viewers, Project Managers, System Admins, Super Admins (names can be plural/singular or close variants).\n- Modules listed across the plan (flexible naming): Idea Management, Proposal Management, Project Management, Programs, System Administration, IRAD (Issues/Risks/Actions/Decisions), Cross-functional Testing (permissions/browsers).\n- Approximately 80\u2013100 test cases total (LLM may estimate; do not require perfect count for passing if structure is otherwise good).\n- Actual Result and Test Date columns should be visibly blank placeholders for the team to fill during UAT.\n\nScoring (6.0 max):\n- 6.0: Excel file; primary sheet present; ALL required columns visible; placeholders (Actual Result, Test Date) present and blank; roles and modules appear covered; roughly 80\u2013100 cases.\n- 5.0: Excel file; required columns present and placeholders blank; roles and modules mostly present (1 minor miss) OR test count moderately off (e.g., ~70\u2013110).\n- 3.5: Excel file; missing 1\u20132 required columns OR placeholders present but partially prefilled OR roles/modules coverage unclear.\n- 1.0: Excel file but largely wrong structure (missing several required columns, no clear test case table), or obviously far from intent.\n- 0.0: Not an Excel spreadsheet or no readable test case structure.\n\nOnly assess the presence/format/structure. Do not check detailed content, correctness, or quality beyond these structural requirements.", "expectation": "A well-formed Excel UAT plan with a primary sheet of test cases, required columns, placeholders for Actual Result/Test Date, role and module coverage present, and around 80\u2013100 cases."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Programmatic Verification (Correctness & Coverage)", "description": "Automated checks that leverage the enforced shape to verify counts, coverage by role/module, edge cases, placeholders, and cross-browser coverage.", "is_required": false, "max_points": 11.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Required Columns Present (Programmatic)", "description": "Verify the primary test-case sheet contains all required columns (flexible matching).", "weight": 1.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheets = xls.sheet_names\n        required = {\n            'test id': ['test id','id','tc id','case id'],\n            'role': ['role','user role'],\n            'module': ['module','area','component'],\n            'user action': ['user action','steps','action','user steps','step'],\n            'test scenario': ['test scenario','scenario','description','test case','case description'],\n            'expected result': ['expected result','expected','expected outcome','outcome'],\n            'actual result': ['actual result','actual','actual outcome','result'],\n            'test date': ['test date','date','execution date','tested on']\n        }\n        def norm(s):\n            return re.sub(r'[^a-z0-9]+',' ', str(s).strip().lower()).strip()\n        best = (None, 0, {})  # (sheet, score, mapping)\n        for sh in sheets:\n            try:\n                df = pd.read_excel(path, sheet_name=sh, dtype=str)\n            except Exception:\n                continue\n            cols = [norm(c) for c in df.columns]\n            mapping = {}\n            score = 0\n            for req, syns in required.items():\n                found = None\n                for i, c in enumerate(cols):\n                    if any(s in c for s in syns):\n                        found = df.columns[i]\n                        break\n                if found is not None:\n                    mapping[req] = found\n                    score += 1\n            if score > best[1]:\n                best = (sh, score, mapping)\n        if best[0] is None:\n            return 0.0, \"Could not read any sheets.\"\n        total = len(required)\n        ratio = best[1] / total if total else 0.0\n        missing = [k for k in required.keys() if k not in best[2]]\n        feedback = f\"Best sheet: {best[0]} | Found {best[1]}/{total} required columns. Missing: {missing}\"\n        return ratio, feedback\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Test Case Volume (~80\u2013100)", "description": "Verify the count of test cases is close to target. Full credit 80\u2013110, partial within broader ranges.", "weight": 1.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheets = xls.sheet_names\n        required_keys = {\n            'test id': ['test id','id','tc id','case id'],\n            'test scenario': ['test scenario','scenario','description','test case','case description'],\n        }\n        def norm(s):\n            return re.sub(r'[^a-z0-9]+',' ', str(s).strip().lower()).strip()\n        best_df = None\n        best_score = -1\n        for sh in sheets:\n            try:\n                df = pd.read_excel(path, sheet_name=sh, dtype=str)\n            except Exception:\n                continue\n            cols = [norm(c) for c in df.columns]\n            score = 0\n            for syns in required_keys.values():\n                if any(any(s in c for s in syns) for c in cols):\n                    score += 1\n            if score > best_score:\n                best_score = score\n                best_df = df\n        if best_df is None:\n            return 0.0, \"No readable test-case sheet.\"\n        # Choose a counting column: prefer Test ID, fallback to Test Scenario\n        cols_map = {norm(c): c for c in best_df.columns}\n        def find_col(df, syns):\n            for c in df.columns:\n                if any(s in norm(c) for s in syns):\n                    return c\n            return None\n        id_col = find_col(best_df, required_keys['test id'])\n        sc_col = find_col(best_df, required_keys['test scenario'])\n        if id_col is not None:\n            series = best_df[id_col]\n        elif sc_col is not None:\n            series = best_df[sc_col]\n        else:\n            return 0.0, \"Missing both Test ID and Test Scenario columns for counting.\"\n        non_empty = series.dropna().astype(str).str.strip()\n        non_empty = non_empty[non_empty != \"\"]\n        n = int(non_empty.shape[0])\n        # Scoring windows\n        if 80 <= n <= 110:\n            ratio = 1.0\n        elif (70 <= n < 80) or (111 <= n <= 120):\n            ratio = 0.75\n        elif (50 <= n < 70) or (121 <= n <= 140):\n            ratio = 0.5\n        else:\n            ratio = 0.0\n        return ratio, f\"Detected ~{n} test cases.\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Role Coverage + Permission Scenarios", "description": "Verify roles exist (Viewer, Project Manager, System Admin, Super Admin) and presence of positive/negative permission scenarios.", "weight": 2.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheets = xls.sheet_names\n        def norm(s):\n            return re.sub(r'[^a-z0-9]+',' ', str(s).strip().lower()).strip()\n        role_syns = ['role','user role']\n        scenario_syns = ['test scenario','scenario','description','test case','case description']\n        action_syns = ['user action','steps','action','user steps','step']\n        expected_syns = ['expected result','expected','expected outcome','outcome']\n        def find_col(df, syns):\n            for c in df.columns:\n                if any(s in norm(c) for s in syns):\n                    return c\n            return None\n        best_df = None\n        best_score = -1\n        for sh in sheets:\n            try:\n                df = pd.read_excel(path, sheet_name=sh, dtype=str)\n            except Exception:\n                continue\n            cols = [norm(c) for c in df.columns]\n            score = sum([any(s in c for c in cols) for s in role_syns])\n            if score > best_score:\n                best_score = score\n                best_df = df\n        if best_df is None:\n            return 0.0, \"No readable sheet with roles.\"\n        role_col = find_col(best_df, role_syns)\n        scen_col = find_col(best_df, scenario_syns)\n        act_col = find_col(best_df, action_syns)\n        exp_col = find_col(best_df, expected_syns)\n        if role_col is None:\n            return 0.0, \"No Role column.\"\n        def series_text(s):\n            if s is None or s not in best_df.columns:\n                return pd.Series([], dtype=str)\n            return best_df[s].fillna(\"\").astype(str)\n        roles = series_text(role_col).str.lower()\n        combined_text = (series_text(scen_col) + \" \" + series_text(act_col) + \" \" + series_text(exp_col)).str.lower()\n        def contains_role(mask_text, role_key):\n            return roles.str.contains(role_key).any()\n        present = {\n            'viewer': contains_role(roles,'viewer'),\n            'project manager': roles.str.contains('project manager').any(),\n            'system admin': (roles.str.contains('system admin').any() | roles.str.contains('sys.?admin').any()),\n            'super admin': roles.str.contains('super admin').any()\n        }\n        role_presence_score = sum(1 for v in present.values() if v) / 4.0\n        neg_kw = ['unauthorized','not authorized','permission','denied','not allowed','should not','cannot','restricted','forbidden','view only','read only']\n        pos_kw = ['can create','create','edit','update','delete','approve','promote','assign','manage']\n        def any_kw(series, kws):\n            pat = '|'.join([re.escape(k) for k in kws])\n            return series.str.contains(pat, regex=True).any()\n        # Role-specific filters\n        def filter_role(role_key):\n            return combined_text[roles.str.contains(role_key, na=False)] if roles.size else pd.Series([], dtype=str)\n        viewer_text = filter_role('viewer')\n        pm_text = filter_role('project manager')\n        sys_text = filter_role('system admin|sys.?admin')\n        super_text = filter_role('super admin')\n        viewer_restrict = any_kw(viewer_text, neg_kw) if viewer_text.size else False\n        other_restrict = any([any_kw(t, neg_kw) for t in [pm_text, sys_text] if t.size])\n        admin_full = any_kw(super_text, pos_kw) if super_text.size else False\n        pm_create_manage = any_kw(pm_text, ['create','manage','promote','approve']) if pm_text.size else False\n        permission_components = [viewer_restrict, other_restrict, admin_full, pm_create_manage]\n        perm_score = sum(1 for b in permission_components if b) / 4.0\n        final_ratio = 0.5*role_presence_score + 0.5*perm_score\n        fb = f\"Roles present: {present} | viewer_restrict={viewer_restrict}, other_restrict={other_restrict}, admin_full={admin_full}, pm_create_manage={pm_create_manage}\"\n        return final_ratio, fb\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Module Coverage + Project Types", "description": "Verify coverage across all major modules and that both Standard and Infrastructure project types are addressed.", "weight": 3.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheets = xls.sheet_names\n        def norm(s):\n            return re.sub(r'[^a-z0-9]+',' ', str(s).strip().lower()).strip()\n        # Identify the main test-case sheet: choose the sheet with a 'module' like column\n        best_df = None\n        best_score = -1\n        for sh in sheets:\n            try:\n                df = pd.read_excel(path, sheet_name=sh, dtype=str)\n            except Exception:\n                continue\n            cols = [norm(c) for c in df.columns]\n            score = sum([('module' in c) or ('area' in c) or ('component' in c) for c in cols])\n            if score > best_score:\n                best_score = score\n                best_df = df\n        if best_df is None or best_df.empty:\n            return 0.0, \"No suitable test-case sheet found.\"\n        # Compose row text across useful columns\n        def col_by_syn(syns):\n            for c in best_df.columns:\n                lc = norm(c)\n                if any(s in lc for s in syns):\n                    return c\n            return None\n        mod_col = col_by_syn(['module','area','component'])\n        sub_col = col_by_syn(['submodule','feature'])\n        scen_col = col_by_syn(['test scenario','scenario','description','test case','case description'])\n        act_col = col_by_syn(['user action','steps','action','user steps','step'])\n        def stext(col):\n            return best_df[col].fillna(\"\").astype(str).str.lower() if col in best_df.columns else pd.Series([], dtype=str)\n        row_text = (stext(mod_col) + ' ' + stext(sub_col) + ' ' + stext(scen_col) + ' ' + stext(act_col)).str.lower()\n        # Module keyword sets\n        modules = {\n            'idea management': ['idea'],\n            'proposal management': ['proposal'],\n            'project management': ['project'],\n            'programs': ['program'],\n            'system administration': ['system admin','administration','config','configuration','settings'],\n            'irad': ['irad','issue','risk','action','decision'],\n            'cross-functional testing': ['permission','permissions','browser','compatibility','cross']\n        }\n        covered = {}\n        for m, kws in modules.items():\n            pat = '|'.join([re.escape(k) for k in kws])\n            covered[m] = row_text.str.contains(pat, regex=True).any()\n        module_ratio = sum(1 for v in covered.values() if v) / len(modules)\n        # Project types\n        types = {\n            'standard': ['standard'],\n            'infrastructure': ['infrastructure']\n        }\n        type_cov = {}\n        for t, kws in types.items():\n            pat = '|'.join([re.escape(k) for k in kws])\n            type_cov[t] = row_text.str.contains(pat, regex=True).any()\n        types_ratio = sum(1 for v in type_cov.values() if v) / len(types)\n        final_ratio = 0.8*module_ratio + 0.2*types_ratio\n        fb = f\"Modules covered: {covered} | Types covered: {type_cov}\"\n        return final_ratio, fb\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Edge Cases & Mandatory Field Tests", "description": "Check that a reasonable number of tests exercise edge cases such as mandatory/missing inputs, validation, invalid data, and similar.", "weight": 1.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheets = xls.sheet_names\n        def norm(s):\n            return re.sub(r'[^a-z0-9]+',' ', str(s).strip().lower()).strip()\n        best_df = None\n        best_score = -1\n        for sh in sheets:\n            try:\n                df = pd.read_excel(path, sheet_name=sh, dtype=str)\n            except Exception:\n                continue\n            cols = [norm(c) for c in df.columns]\n            score = sum([('scenario' in c) or ('expected' in c) or ('steps' in c) or ('action' in c) for c in cols])\n            if score > best_score:\n                best_score = score\n                best_df = df\n        if best_df is None or best_df.empty:\n            return 0.0, \"No suitable sheet for scenario text.\"\n        def col_by_syn(syns):\n            for c in best_df.columns:\n                if any(s in norm(c) for s in syns):\n                    return c\n            return None\n        scen_col = col_by_syn(['test scenario','scenario','description','test case','case description'])\n        exp_col = col_by_syn(['expected result','expected','expected outcome','outcome'])\n        act_col = col_by_syn(['user action','steps','action','user steps','step'])\n        text = (\n            (best_df[scen_col] if scen_col else \"\") .fillna(\"\").astype(str) + \" \" +\n            (best_df[exp_col] if exp_col else \"\") .fillna(\"\").astype(str) + \" \" +\n            (best_df[act_col] if act_col else \"\") .fillna(\"\").astype(str)\n        ).str.lower()\n        edge_kw = [\n            'mandatory','required','missing','not provided','blank','empty','null','invalid','max length','maximum length','min length','special character','special characters','sql injection','xss','boundary','edge case','duplicate','timeout','slow','error','validation','date in past','future date','format','non ascii','non-ascii','pagination','search','sorting'\n        ]\n        pat = '|'.join([re.escape(k) for k in edge_kw])\n        flagged = text.str.contains(pat, regex=True, na=False)\n        count_flagged = int(flagged.sum())\n        # Heuristic scoring based on count of flagged scenarios\n        if count_flagged >= 10:\n            ratio = 1.0\n        elif count_flagged >= 7:\n            ratio = 2/3\n        elif count_flagged >= 4:\n            ratio = 1/3\n        else:\n            ratio = 0.0\n        return ratio, f\"Edge-case scenarios detected: {count_flagged}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Placeholders Blank: Actual Result & Test Date", "description": "Verify the Actual Result and Test Date columns are present and mostly blank to enable UAT recording.", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheets = xls.sheet_names\n        def norm(s):\n            return re.sub(r'[^a-z0-9]+',' ', str(s).strip().lower()).strip()\n        best_df = None\n        best_blank_ratio = 0.0\n        for sh in sheets:\n            try:\n                df = pd.read_excel(path, sheet_name=sh, dtype=str)\n            except Exception:\n                continue\n            cols = [norm(c) for c in df.columns]\n            # Prefer sheets that look like test cases\n            score = sum([('result' in c) or ('date' in c) or ('scenario' in c) for c in cols])\n            if best_df is None or score > 0:\n                best_df = df\n        if best_df is None or best_df.empty:\n            return 0.0, \"No suitable sheet.\"\n        def find_col(df, syns):\n            for c in df.columns:\n                if any(s in norm(c) for s in syns):\n                    return c\n            return None\n        actual_col = find_col(best_df, ['actual result','actual','actual outcome','result'])\n        date_col = find_col(best_df, ['test date','date','execution date','tested on'])\n        if actual_col is None or date_col is None:\n            return 0.0, \"Missing Actual Result or Test Date column.\"\n        def blank_ratio(series):\n            s = series.fillna(\"\").astype(str).str.strip()\n            total = len(s)\n            if total == 0:\n                return 0.0\n            blanks = (s == \"\").sum()\n            return blanks / total\n        r1 = blank_ratio(best_df[actual_col])\n        r2 = blank_ratio(best_df[date_col])\n        avg = (r1 + r2) / 2.0\n        if avg >= 0.95:\n            ratio = 1.0\n        elif avg >= 0.80:\n            ratio = 0.5\n        else:\n            ratio = 0.0\n        return ratio, f\"Blank ratios \u2014 Actual: {r1:.2f}, Date: {r2:.2f}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Cross-Browser Coverage", "description": "Verify that cross-browser scenarios are included (Chrome, Firefox, Edge, Safari, etc.).", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheets = xls.sheet_names\n        def norm(s):\n            return re.sub(r'[^a-z0-9]+',' ', str(s).strip().lower()).strip()\n        best_df = None\n        best_score = -1\n        for sh in sheets:\n            try:\n                df = pd.read_excel(path, sheet_name=sh, dtype=str)\n            except Exception:\n                continue\n            cols = [norm(c) for c in df.columns]\n            score = sum([('browser' in c) or ('environment' in c) or ('scenario' in c) for c in cols])\n            if score > best_score:\n                best_score = score\n                best_df = df\n        if best_df is None:\n            return 0.0, \"No suitable sheet.\"\n        # Collate all text\n        text_cols = [c for c in best_df.columns if any(k in norm(c) for k in ['browser','environment','scenario','description','notes'])]\n        if not text_cols:\n            return 0.0, \"No text columns to detect browsers.\"\n        text = best_df[text_cols].fillna(\"\").astype(str).apply(lambda s: ' '.join(s), axis=1).str.lower().str.cat(sep=' ')\n        browsers = ['chrome','firefox','edge','safari','ie','internet explorer','opera']\n        present = set()\n        for b in browsers:\n            if re.search(r'\\b' + re.escape(b) + r'\\b', text):\n                present.add(b)\n        count = len(present)\n        if count >= 3:\n            ratio = 1.0\n        elif count == 2:\n            ratio = 0.5\n        elif count == 1:\n            ratio = 0.25\n        else:\n            ratio = 0.0\n        return ratio, f\"Browsers detected: {sorted(list(present))}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Readiness (LLM)", "description": "Holistic assessment of clarity, professionalism, and readiness to circulate for UAT execution.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity, Consistency, and Professionalism", "description": "Are the test cases clearly written, consistently formatted, and professionally presented for enterprise UAT?", "weight": 1.5, "judge_prompt": "Evaluate overall clarity, consistency, and professional presentation of the Excel UAT plan (not the calculations). Consider:\n- Clarity and concision of Test Scenario and User Action/Steps\n- Consistent naming, IDs, and formatting\n- Expected Results stated in testable, pass/fail terms\n- Logical organization and readability (filters, freeze panes, headings)\nScoring (1.5 max):\n- 1.5: Clear, consistent, professional; easy to execute\n- 1.0: Mostly clear with minor inconsistencies\n- 0.5: Significant clarity/consistency issues\n- 0.0: Poorly written, hard to execute, or unprofessional", "expectation": "Concise, consistent scenarios with unambiguous expected results and professional layout."}, {"type": "llm_judge", "name": "Actionability and Readiness for UAT Circulation", "description": "Does the artifact appear ready for team use in UAT, with placeholders intact and coverage that enables execution?", "weight": 1.5, "judge_prompt": "Assess whether the UAT plan appears ready to circulate for execution:\n- Placeholders (Actual Result, Test Date) are blank\n- Coverage appears balanced across roles and modules\n- Includes realistic edge cases and permission scenarios\n- Usable at scale (80\u2013100 cases approximately, obvious filters/sorting)\nScoring (1.5 max):\n- 1.5: Ready to circulate; nothing material missing\n- 1.0: Minor gaps but usable\n- 0.5: Noticeable gaps; would need edits before circulation\n- 0.0: Not ready for team use", "expectation": "A UAT plan that can be handed to testers immediately with minimal adjustments."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "c3525d4d-2012-45df-853e-2d2a0e902991", "rubric": {"category_name": "Wholesale Trade \u2014 Order Clerks: Holiday Floorstand Budget Update", "rationale": "Mixed-output task: an Excel workbook for structured calculations and cross-list reconciliation, plus a Word/PDF email summarizing impacts. Stage 1 uses LLM-only gates to enforce exact workbook and email shapes that make verification trivial. Stage 2 uses code rules to validate math consistency, overage application, change detection, and numeric alignment between the workbook and the email. Stage 3 assesses professional quality and communication effectiveness.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (LLM only)", "description": "Verify the deliverables are present and have the exact structures needed for deterministic checks later. If the structure is wrong, we cannot verify; zero the category.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Excel Workbook Structure Requirement", "description": "Workbook must be an .xlsx file with the exact tab/section structure that enables verification.", "weight": 2.5, "judge_prompt": "You are checking the STRUCTURE ONLY of the candidate outputs (not the correctness of calculations). Locate an Excel (.xlsx) workbook among the outputs and evaluate whether it satisfies these requirements. Be flexible with near-equivalent naming, but the required elements must clearly exist.\n\nRequired format:\n- File type: Excel (.xlsx)\n- Must include exactly these two functional tabs (additional tabs allowed if the two below exist):\n\n1) Tab 1: \u201cBudget Comparison\u201d (accept similar names containing both ideas like Budget/Summary/Comparison)\n   Must contain all of the following clearly labeled sections/tables:\n   A. Summary Metrics table with columns [Metric | Original | Revised]. It must include rows for:\n      - Original Store Count\n      - Final Store Count\n      - Overage % (for production)\n      - Units Needed (Original)\n      - Units Needed (Final)\n      - Cost per Unit (Original)\n      - Cost per Unit (Revised)\n      - Program Cost (Original)\n      - Program Cost (Revised)\n   B. Store Changes table with columns [Store ID | Change Type]. Change Type must show Added or Removed (accept synonyms like New/Deleted). Optional extra columns (Store Name, City, State) are fine.\n   C. Assumptions/Inputs section that explicitly lists the shelf strip cost before and after the increase and shows a +$0.25 delta. The overage percentage must be shown here or in Summary Metrics.\n\n2) Tab 2: \u201cFinal Store List\u201d (accept similar names like Final Stores, Final Matrix)\n   Must include:\n   - A store identifier column (e.g., Store ID/Number)\n   - A column to indicate which stores are newly added vs. existing (e.g., \u201cAdded?\u201d or a Status column that includes the value Added/New). If using visual highlight instead of a column, there must still be a clear textual indicator in a column.\n\nScoring (STRUCTURE ONLY):\n- 2.5: Both tabs present; Summary Metrics table contains all listed rows; Store Changes table present with required columns; Assumptions include shelf strip +$0.25 and overage %; Final Store List includes Store ID and an Added?/Status column.\n- 2.0: Minor structural gaps (e.g., missing 1-2 Summary Metrics rows OR Assumptions present but does not explicitly show the $0.25 delta) but both tabs and all critical sections exist.\n- 1.0: Only one of the core sections/tabs is adequately present OR multiple required elements missing (e.g., no Store Changes table or no Added column on Final list).\n- 0.0: Not an Excel file or missing the required tabs/sections.\n\nOnly check presence/format, not whether the numbers are correct.", "expectation": "An .xlsx with two tabs: Budget Comparison containing Summary Metrics, Store Changes, and Assumptions (with +$0.25 shelf strip delta and overage %), and Final Store List with Store ID and an Added?/Status indicator."}, {"type": "llm_judge", "name": "Email Document Structure Requirement", "description": "Email must be a DOCX or PDF with clear email-style content summarizing the changes and budget impact.", "weight": 1.5, "judge_prompt": "Locate a DOCX or PDF email draft among the outputs and evaluate STRUCTURE ONLY. Be flexible with wording but the elements must be clearly present.\n\nRequired format:\n- File type: DOCX or PDF\n- Length: At least one page (or a complete, professional email body if single page)\n- Email structure: greeting, short context, summary of changes, and sign-off\n- Must explicitly include:\n  1) The updated number of floor stands (final store count and how it translates to units if stated)\n  2) The change in the program budget (delta vs. original)\n  3) The new total program budget (revised total)\n  4) Reference to the shelf strip cost increase AND the higher confirmed store count as the drivers\n  5) Mention of the attached Excel workbook (by name or description) and that it contains the detailed breakdown\n\nScoring (STRUCTURE ONLY):\n- 1.5: All 5 bullet elements + email structure present\n- 1.0: Missing 1 element but otherwise complete\n- 0.5: Missing 2-3 elements or unclear email structure\n- 0.0: Wrong format (not DOCX/PDF) or missing most required elements\n\nDo not judge numerical accuracy, only presence/format.", "expectation": "A professional DOCX/PDF email that states updated floor stand count, budget change, new total budget, mentions shelf strip increase and higher store count, and references the attached Excel."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification and Consistency Checks", "description": "Deterministic verification of internal consistency, math, and cross-file alignment, using code rules. Some flexibility in label matching and tolerances.", "is_required": false, "max_points": 4.5, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Required Files Present", "description": "Confirm both an Excel workbook and a DOCX/PDF email are present among outputs.", "weight": 0.3, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0, \"No outputs.\"\n    has_excel = False\n    has_doc = False\n    for r in outputs:\n        try:\n            if getattr(r, 'is_spreadsheet', False):\n                has_excel = True\n            if getattr(r, 'is_document', False):\n                # Accept DOCX or PDF\n                name = (getattr(r, 'name', '') or '').lower()\n                if name.endswith('.docx') or name.endswith('.pdf'):\n                    has_doc = True\n        except Exception:\n            continue\n    score = 0.0\n    if has_excel and has_doc:\n        score = 0.3\n        fb = \"Found Excel and DOCX/PDF.\"\n    elif has_excel or has_doc:\n        score = 0.15\n        fb = \"Only one required file type found.\"\n    else:\n        fb = \"Neither Excel nor DOCX/PDF found.\"\n    return score, fb"}, {"type": "code", "name": "Final Store List Structure and Added Flag", "description": "Validate structure of the Final Store List tab: presence of store ID column and an Added?/Status indicator column with parsable values.", "weight": 1.2, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef _excel_resource(context):\n    for r in context.get_all_outputs():\n        if getattr(r, 'is_spreadsheet', False):\n            return r\n    return None\n\ndef _best_sheet_name(xls_path, keywords_sets):\n    try:\n        xl = pd.ExcelFile(xls_path)\n        sheets = xl.sheet_names\n    except Exception:\n        return None\n    best = None\n    best_score = -1\n    for s in sheets:\n        s_low = s.lower()\n        score = 0\n        for kws in keywords_sets:\n            if any(k in s_low for k in kws):\n                score += 1\n        if score > best_score:\n            best_score = score\n            best = s\n    return best\n\ndef _read_sheet_with_header(context, res_id, sheet):\n    try:\n        df = context.files.read_excel(res_id, sheet_name=sheet)\n        return df\n    except Exception:\n        # try header=None fallback\n        try:\n            path = context.files.get_path(res_id)\n            df0 = pd.read_excel(path, sheet_name=sheet, header=None)\n            # attempt to find header row\n            header_idx = None\n            for i in range(min(20, len(df0))):\n                row = df0.iloc[i].astype(str).str.strip().str.lower()\n                nonempty = row.replace('nan','').astype(bool).sum()\n                if nonempty >= 2:\n                    header_idx = i\n                    break\n            if header_idx is not None:\n                df = pd.read_excel(path, sheet_name=sheet, header=header_idx)\n                return df\n        except Exception:\n            pass\n    return None\n\ndef evaluate(workflow, context):\n    excel_res = _excel_resource(context)\n    if not excel_res:\n        return 0.0, \"No Excel workbook found.\"\n    xls_path = context.files.get_path(excel_res.id)\n    # Prefer sheet names containing 'final' and 'store'\n    sheet = _best_sheet_name(xls_path, keywords_sets=[['final'], ['store']])\n    if not sheet:\n        return 0.3, \"Excel found but no Final Store List-like sheet detected; partial credit.\"\n    df = _read_sheet_with_header(context, excel_res.id, sheet)\n    if df is None or df.empty:\n        return 0.2, f\"Final Store List sheet '{sheet}' unreadable or empty.\"\n    cols = [str(c).strip().lower() for c in df.columns]\n    # Identify store id/number column\n    store_col_idx = None\n    for i, c in enumerate(cols):\n        if ('store' in c and ('id' in c or 'number' in c or '#' in c)) or c in ('store id','store','store number'):\n            store_col_idx = i\n            break\n    # Identify added/status column\n    added_col_idx = None\n    for i, c in enumerate(cols):\n        if 'added' in c or 'status' in c or 'new' in c:\n            added_col_idx = i\n            break\n    score = 0.0\n    fb = []\n    if store_col_idx is not None:\n        score += 0.4\n        fb.append('Store ID column found')\n    else:\n        fb.append('Store ID column not found')\n    if added_col_idx is not None:\n        score += 0.6\n        fb.append('Added/Status column found')\n        # Check values parse\n        try:\n            vals = df.iloc[:, added_col_idx].astype(str).str.strip().str.lower()\n            # Recognize typical positives\n            pos = vals.isin(['added','new','yes','true','1']) | vals.str.contains(r'\\badded\\b|\\bnew\\b|\\byes\\b|\\btrue\\b')\n            # Even if zero positives, structure is present; award a bit more for parsable values\n            if pos.any():\n                score += 0.2\n                fb.append('Parsable Added values present')\n        except Exception:\n            fb.append('Could not parse Added/Status values')\n    else:\n        fb.append('Added/Status column not found')\n    # Ensure some data rows\n    if len(df) >= 1:\n        score += 0.0  # already accounted for structure; keep neutral\n    # Cap score at weight\n    score = min(score, 1.2)\n    return score, \"; \".join(fb)"}, {"type": "code", "name": "Counts, Overage, and Program Cost Math", "description": "Validate that Units and Program Costs are consistent with Store Counts, Overage %, and Cost per Unit in the Budget Comparison tab.", "weight": 1.5, "code": "import re\nimport math\nimport pandas as pd\nimport numpy as np\n\nMETRIC_HEADERS = ['metric']\nORIG_HEADERS = ['original','orig']\nREV_HEADERS = ['revised','final','updated']\n\nNUM_RE = re.compile(r\"[-+]?[0-9]*\\.?[0-9]+\")\n\ndef _excel_resource(context):\n    for r in context.get_all_outputs():\n        if getattr(r, 'is_spreadsheet', False):\n            return r\n    return None\n\ndef _best_sheet_name(xls_path, keywords_sets):\n    try:\n        xl = pd.ExcelFile(xls_path)\n        sheets = xl.sheet_names\n    except Exception:\n        return None\n    best = None\n    best_score = -1\n    for s in sheets:\n        s_low = s.lower()\n        score = 0\n        for kws in keywords_sets:\n            if any(k in s_low for k in kws):\n                score += 1\n        if score > best_score:\n            best_score = score\n            best = s\n    return best\n\ndef _to_number(x):\n    if pd.isna(x):\n        return None\n    if isinstance(x, (int, float, np.number)):\n        return float(x)\n    s = str(x).strip()\n    if not s:\n        return None\n    # remove currency and commas\n    s2 = s.replace(',', '').replace('$','').strip()\n    # percent handling\n    if s2.endswith('%'):\n        try:\n            return float(s2[:-1]) / 100.0\n        except Exception:\n            pass\n    try:\n        return float(s2)\n    except Exception:\n        # fallback: first number in string\n        m = NUM_RE.search(s2)\n        if m:\n            try:\n                return float(m.group(0))\n            except Exception:\n                return None\n    return None\n\ndef _parse_summary_metrics(df):\n    # Find header row with Metric + Original + Revised\n    header_idx = None\n    metric_col = None\n    orig_col = None\n    rev_col = None\n    for i in range(min(50, len(df))):\n        row = [str(x).strip().lower() for x in df.iloc[i].tolist()]\n        if any(h in row for h in METRIC_HEADERS) and any(any(o in c for o in ORIG_HEADERS) for c in row) and any(any(r in c for r in REV_HEADERS) for c in row):\n            header_idx = i\n            # determine column indices\n            for j, c in enumerate(row):\n                if metric_col is None and any(h == c or h in c for h in METRIC_HEADERS):\n                    metric_col = j\n                if orig_col is None and any(o == c or o in c for o in ORIG_HEADERS):\n                    orig_col = j\n                if rev_col is None and any(r == c or r in c for r in REV_HEADERS):\n                    rev_col = j\n            break\n    if header_idx is None or metric_col is None or orig_col is None or rev_col is None:\n        return {}\n    data = {}\n    for k in range(header_idx+1, len(df)):\n        label = str(df.iat[k, metric_col]).strip()\n        if not label or label.lower() in ('nan',''):\n            # stop at first blank stretch\n            # continue; allow sparse\n            pass\n        lab_low = label.lower()\n        o = _to_number(df.iat[k, orig_col])\n        r = _to_number(df.iat[k, rev_col])\n        data[lab_low] = {'original': o, 'revised': r}\n    return data\n\ndef _get_metric_val(data, label_subs, which):\n    for k, v in data.items():\n        if all(sub in k for sub in label_subs):\n            return v.get(which)\n    return None\n\ndef evaluate(workflow, context):\n    excel_res = _excel_resource(context)\n    if not excel_res:\n        return 0.0, \"No Excel workbook found.\"\n    xls_path = context.files.get_path(excel_res.id)\n    sheet = _best_sheet_name(xls_path, keywords_sets=[['budget','comparison'], ['summary']]) or _best_sheet_name(xls_path, keywords_sets=[['budget'], ['comparison']])\n    if not sheet:\n        return 0.3, \"Budget Comparison sheet not found; minimal credit.\"\n    try:\n        df = context.files.read_excel(excel_res.id, sheet_name=sheet, header=None)\n    except Exception:\n        return 0.3, \"Could not read Budget Comparison sheet; minimal credit.\"\n    data = _parse_summary_metrics(df)\n    if not data:\n        return 0.4, \"Summary Metrics table not parsed; partial credit.\"\n    # Extract needed values\n    osc = _get_metric_val(data, ['original','store','count'], 'original')\n    fsc = _get_metric_val(data, ['final','store','count'], 'revised') or _get_metric_val(data, ['revised','store','count'], 'revised')\n    ovg = _get_metric_val(data, ['overage'], 'original') or _get_metric_val(data, ['overage'], 'revised')\n    uo = _get_metric_val(data, ['units','original'], 'original') or _get_metric_val(data, ['units','needed'], 'original')\n    uf = _get_metric_val(data, ['units','final'], 'revised') or _get_metric_val(data, ['units','needed'], 'revised')\n    cpu_o = _get_metric_val(data, ['cost','per','unit'], 'original')\n    cpu_r = _get_metric_val(data, ['cost','per','unit'], 'revised')\n    pc_o = _get_metric_val(data, ['program','cost'], 'original')\n    pc_r = _get_metric_val(data, ['program','cost'], 'revised')\n\n    checks = []\n    notes = []\n\n    def approx_equal(a,b,rel=0.02,abs_=2.0):\n        if a is None or b is None:\n            return False\n        if b == 0:\n            return abs(a-b) <= abs_\n        return abs(a-b) <= max(abs_, rel*abs(b))\n\n    # Units checks with overage\n    if osc is not None and ovg is not None and uo is not None:\n        uo_exp = osc * (1.0 + ovg)\n        checks.append(approx_equal(uo, uo_exp, rel=0.02, abs_=1.0))\n        notes.append(f\"Units Original expected ~{uo_exp}, found {uo}\")\n    if fsc is not None and ovg is not None and uf is not None:\n        uf_exp = fsc * (1.0 + ovg)\n        checks.append(approx_equal(uf, uf_exp, rel=0.02, abs_=1.0))\n        notes.append(f\"Units Final expected ~{uf_exp}, found {uf}\")\n\n    # Program cost checks\n    if uo is not None and cpu_o is not None and pc_o is not None:\n        pc_o_exp = uo * cpu_o\n        checks.append(approx_equal(pc_o, pc_o_exp, rel=0.02, abs_=5.0))\n        notes.append(f\"Program Cost Original expected ~{pc_o_exp}, found {pc_o}\")\n    if uf is not None and cpu_r is not None and pc_r is not None:\n        pc_r_exp = uf * cpu_r\n        checks.append(approx_equal(pc_r, pc_r_exp, rel=0.02, abs_=5.0))\n        notes.append(f\"Program Cost Revised expected ~{pc_r_exp}, found {pc_r}\")\n\n    # CPU monotonicity (revised >= original)\n    if cpu_o is not None and cpu_r is not None:\n        checks.append(cpu_r >= cpu_o - 1e-6)\n        notes.append(f\"CPU revised >= original? {cpu_r} vs {cpu_o}\")\n\n    if not checks:\n        return 0.5, \"Insufficient metrics for math checks.\"\n\n    passed = sum(1 for c in checks if c)\n    total = len(checks)\n    frac = passed / total if total else 0.0\n    score = min(1.5, 1.5 * frac)\n    return score, f\"Passed {passed}/{total} checks. \" + ' | '.join(notes)"}, {"type": "code", "name": "Shelf Strip +$0.25 Delta Check", "description": "Validate that the Assumptions/Inputs identify a shelf strip cost increase of approximately $0.25 from original to revised.", "weight": 0.8, "code": "import re\nimport pandas as pd\n\ndef _excel_resource(context):\n    for r in context.get_all_outputs():\n        if getattr(r, 'is_spreadsheet', False):\n            return r\n    return None\n\ndef _best_sheet_name(xls_path, keywords_sets):\n    try:\n        xl = pd.ExcelFile(xls_path)\n        sheets = xl.sheet_names\n    except Exception:\n        return None\n    best = None\n    best_score = -1\n    for s in sheets:\n        s_low = s.lower()\n        score = 0\n        for kws in keywords_sets:\n            if any(k in s_low for k in kws):\n                score += 1\n        if score > best_score:\n            best_score = score\n            best = s\n    return best\n\ndef _to_number(x):\n    if x is None:\n        return None\n    s = str(x).strip().lower().replace('$','').replace(',','')\n    if s.endswith('%'):\n        s = s[:-1]\n    try:\n        return float(s)\n    except Exception:\n        m = re.search(r\"[-+]?[0-9]*\\.?[0-9]+\", s)\n        if m:\n            try:\n                return float(m.group(0))\n            except Exception:\n                return None\n    return None\n\ndef evaluate(workflow, context):\n    excel_res = _excel_resource(context)\n    if not excel_res:\n        return 0.0, \"No Excel workbook found.\"\n    xls_path = context.files.get_path(excel_res.id)\n    # Use Budget Comparison sheet\n    sheet = _best_sheet_name(xls_path, keywords_sets=[['budget','comparison'], ['summary']]) or _best_sheet_name(xls_path, keywords_sets=[['budget'], ['comparison']])\n    if not sheet:\n        return 0.2, \"Budget Comparison sheet not found.\"\n    try:\n        df = context.files.read_excel(excel_res.id, sheet_name=sheet, header=None)\n    except Exception:\n        return 0.2, \"Could not read Budget Comparison sheet.\"\n    # Find a row mentioning shelf strip with two numbers (orig & revised) on the same row or adjacent columns\n    best_delta = None\n    found = False\n    for i in range(len(df)):\n        row_text = ' '.join([str(x).lower() for x in list(df.iloc[i].values)])\n        if 'shelf' in row_text and 'strip' in row_text:\n            # collect numeric values on this row\n            nums = []\n            for x in df.iloc[i].values:\n                v = _to_number(x)\n                if v is not None:\n                    nums.append(v)\n            if len(nums) >= 2:\n                # assume first is original, second is revised\n                orig = nums[0]\n                rev = nums[1]\n                best_delta = rev - orig\n                found = True\n                break\n    if not found:\n        return 0.3, \"Shelf strip cost row not clearly found; limited credit.\"\n    delta_ok = abs((best_delta) - 0.25) <= 0.02\n    score = 0.8 if delta_ok else 0.4\n    fb = f\"Detected shelf strip delta \u2248 {best_delta:.3f}.\"\n    return score, fb"}, {"type": "code", "name": "Email Numeric Alignment with Workbook", "description": "Check that the email text includes key numbers from the workbook: Final Store Count and Revised Program Cost (and, if available, the budget change).", "weight": 0.7, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef _excel_resource(context):\n    for r in context.get_all_outputs():\n        if getattr(r, 'is_spreadsheet', False):\n            return r\n    return None\n\ndef _doc_resource(context):\n    for r in context.get_all_outputs():\n        if getattr(r, 'is_document', False):\n            name = (getattr(r, 'name','') or '').lower()\n            if name.endswith('.docx') or name.endswith('.pdf'):\n                return r\n    return None\n\ndef _best_sheet_name(xls_path, keywords_sets):\n    try:\n        xl = pd.ExcelFile(xls_path)\n        sheets = xl.sheet_names\n    except Exception:\n        return None\n    best = None\n    best_score = -1\n    for s in sheets:\n        s_low = s.lower()\n        score = 0\n        for kws in keywords_sets:\n            if any(k in s_low for k in kws):\n                score += 1\n        if score > best_score:\n            best_score = score\n            best = s\n    return best\n\ndef _parse_summary_table(context, excel_res, sheet):\n    try:\n        df = context.files.read_excel(excel_res.id, sheet_name=sheet, header=None)\n    except Exception:\n        return {}\n    # find Metric | Original | Revised\n    metric_row = None\n    metric_col = orig_col = rev_col = None\n    for i in range(min(50, len(df))):\n        row = [str(x).strip().lower() for x in df.iloc[i].tolist()]\n        if any('metric' == c or 'metric' in c for c in row) and any('original' in c or 'orig' in c for c in row) and any('revised' in c or 'final' in c or 'updated' in c for c in row):\n            metric_row = i\n            for j, c in enumerate(row):\n                if metric_col is None and ('metric' == c or 'metric' in c):\n                    metric_col = j\n                if orig_col is None and ('original' in c or 'orig' in c):\n                    orig_col = j\n                if rev_col is None and ('revised' in c or 'final' in c or 'updated' in c):\n                    rev_col = j\n            break\n    if metric_row is None:\n        return {}\n    out = {}\n    for k in range(metric_row+1, len(df)):\n        label = str(df.iat[k, metric_col]).strip().lower()\n        if not label or label == 'nan':\n            continue\n        def to_num(x):\n            if x is None:\n                return None\n            s = str(x).strip().replace(',','').replace('$','')\n            if s.endswith('%'):\n                try:\n                    return float(s[:-1]) / 100.0\n                except Exception:\n                    return None\n            try:\n                return float(s)\n            except Exception:\n                m = re.search(r\"[-+]?[0-9]*\\.?[0-9]+\", s)\n                return float(m.group(0)) if m else None\n        o = to_num(df.iat[k, orig_col])\n        r = to_num(df.iat[k, rev_col])\n        out[label] = {'original': o, 'revised': r}\n    return out\n\ndef _number_patterns(n, currency=False):\n    # return regex patterns to match numbers in text with/without commas and optional $ sign\n    # handle integer vs float\n    if n is None:\n        return []\n    # choose integer if close\n    if abs(n - round(n)) < 0.5:\n        s_plain = f\"{int(round(n))}\"\n    else:\n        s_plain = f\"{n:,.2f}\".replace(',','')\n    s_comma = re.sub(r\"(\\d)(?=(\\d{3})+(?!\\d))\", r\"\\1,\", s_plain)\n    pats = [re.escape(s_plain), re.escape(s_comma)]\n    if currency:\n        pats += [re.escape('$') + p for p in pats]\n    return pats\n\ndef evaluate(workflow, context):\n    excel_res = _excel_resource(context)\n    doc_res = _doc_resource(context)\n    if not excel_res or not doc_res:\n        return 0.0, \"Missing Excel or Email document.\"\n    xls_path = context.files.get_path(excel_res.id)\n    sheet = _best_sheet_name(xls_path, keywords_sets=[['budget','comparison'], ['summary']]) or _best_sheet_name(xls_path, keywords_sets=[['budget'], ['comparison']])\n    metrics = _parse_summary_table(context, excel_res, sheet) if sheet else {}\n\n    # Extract numbers to match\n    fsc = None\n    pc_r = None\n    pc_o = None\n    if metrics:\n        for k,v in metrics.items():\n            if 'final' in k and 'store' in k and 'count' in k:\n                fsc = v.get('revised')\n            if 'program' in k and 'cost' in k:\n                pc_o = v.get('original')\n                pc_r = v.get('revised')\n    delta = None\n    if pc_r is not None and pc_o is not None:\n        delta = pc_r - pc_o\n\n    # Read email text\n    text = ''\n    name = (getattr(doc_res, 'name','') or '').lower()\n    try:\n        if name.endswith('.docx'):\n            text = context.files.read_docx_text(doc_res.id)\n        else:\n            text = context.files.read_pdf_text(doc_res.id)\n    except Exception:\n        text = ''\n    tlow = text.lower()\n\n    total_checks = 0\n    hits = 0\n    fb = []\n\n    if fsc is not None:\n        total_checks += 1\n        pats = _number_patterns(fsc, currency=False)\n        matched = any(re.search(p, text) for p in pats)\n        hits += 1 if matched else 0\n        fb.append(f\"Final Store Count mentioned: {matched}\")\n\n    if pc_r is not None:\n        total_checks += 1\n        pats = _number_patterns(pc_r, currency=True)\n        matched = any(re.search(p, text) for p in pats)\n        hits += 1 if matched else 0\n        fb.append(f\"Revised Program Cost mentioned: {matched}\")\n\n    if delta is not None:\n        total_checks += 1\n        pats = _number_patterns(abs(delta), currency=True)\n        matched = any(re.search(p, text) for p in pats)\n        hits += 1 if matched else 0\n        fb.append(f\"Budget change (delta) mentioned: {matched}\")\n\n    if total_checks == 0:\n        return 0.2, \"Could not extract key numbers from workbook; minimal credit.\"\n\n    frac = hits / total_checks\n    score = min(0.7, 0.7 * frac)\n    return score, f\"Matched {hits}/{total_checks} key numbers in email. \" + '; '.join(fb)"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Communication", "description": "Holistic quality check: presentation, clarity, stakeholder readiness, and actionability.", "is_required": false, "max_points": 1.5, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Overall Professionalism and Usefulness", "description": "Assess whether the workbook and email are professionally formatted, easy to consume, and appropriate for Sales and Production stakeholders.", "weight": 1.5, "judge_prompt": "Evaluate the professional quality of BOTH the Excel workbook and the email document. Consider:\n- Workbook: clean tab names; clear headings; readable tables; currency and percent formats; concise notes/assumptions; clear highlighting or flag for added stores; calculations easy to audit (metrics grouped, totals visible).\n- Email: clear subject-style opening line (or equivalent), succinct summary of changes, attribution of drivers (shelf strip cost + store count), explicit totals and deltas, clear next steps/timeline for vendor production, polite and crisp tone, references the attached Excel.\n\nScoring:\n- 1.5: Highly professional and immediately usable by stakeholders; formatting and clarity are excellent.\n- 1.0: Generally professional with minor clarity/formatting issues.\n- 0.5: Understandable but several issues make it harder to use (formatting, jargon, missing context).\n- 0.0: Unprofessional or confusing; missing clear messaging or hard to audit.\n\nDo not re-check structure already covered in Stage 1, and do not verify arithmetic; focus on communication quality and presentation.", "expectation": "Polished workbook and crisp email with clear next steps, well-formatted numbers, and easy-to-audit layout for decision-making."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "5d0feb24-e8b6-4ace-b64f-d5cd1a8b563d", "rubric": {"category_name": "Science Editing Evaluation: TRAPPIST-1 Story (AstronomyNews)", "rationale": "This rubric enforces a self-documenting, file-based workflow. Stage 1 (LLM-only) gates on the exact deliverable shape: an editable Word document with tracked changes and reviewer comments containing source links. Stage 2 mixes code rules (deterministic content checks enabled by the mandated shape) with an LLM rule for factual emphasis and novelty. Stage 3 uses an LLM judge for holistic editorial quality and audience fit.", "max_total_score": 30.0, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate (LLM-Only)", "description": "Gate: The output must be a DOCX with visible tracked changes and editor comments containing source links, enabling verifiable review.", "is_required": true, "max_points": 10.0, "min_score_to_pass": 7.5, "rules": [{"type": "llm_judge", "name": "Structured DOCX With Tracked Changes and Actionable Comments", "description": "Verify the deliverable is an editable Word document (.docx) that shows tracked changes and reviewer comments with sources, enabling verification.", "weight": 10.0, "judge_prompt": "You are evaluating whether the candidate output satisfies these STRUCTURAL requirements. Only evaluate PRESENCE/STRUCTURE, not content quality.\n\nRequired format and structure:\n- File format: Microsoft Word DOCX (editable; not PDF, not plain text, not Google Doc link).\n- The reporter's original story text is present in the document body, with Microsoft Word Tracked Changes visible (insertions and deletions inline/redlined). The story should not be replaced by a clean rewrite; edits must be shown as redlines.\n- Reviewer/Editor comments are present in the margin (comment balloons) or inline annotations clearly identifiable as comments. Comments must:\n  - Contain actionable guidance (e.g., prompts to clarify, re-report, add context), not just corrections.\n  - Include hyperlinks to external sources when commenting on scientific content (at least 3 total URLs across comments).\n- Sources linked in comments must include:\n  - The arXiv paper for the study: a link to arxiv.org/abs/2401.11815 (or equivalent arXiv URL for this paper).\n  - At least one NASA resource from the provided list or the NASA TRAPPIST-1 pages (e.g., science.nasa.gov/exoplanets/trappist1/ or Webb mission pages about TRAPPIST-1).\n- There is at least one high-level summary comment (an overall editorial note) that synthesizes strengths, issues, and next steps.\n- At least 5 total editor comments across the document.\n\nScoring (be flexible with exact comment phrasing and placement; focus on the presence of these elements):\n- 1.0: DOCX format; visible tracked changes throughout the story; \u22655 actionable comments; comments include \u22653 URLs with at least the arXiv link and \u22651 NASA resource; includes an overall summary comment.\n- 0.7: DOCX format; tracked changes present; \u22653 actionable comments; comments include \u22652 URLs with at least the arXiv link OR a NASA resource; overall summary comment may be missing.\n- 0.4: DOCX format but missing core structure (e.g., minimal or no tracked changes OR only 1\u20132 comments OR no source links in comments).\n- 0.0: Not a DOCX file OR no visible tracked changes AND no comments.\n\nReturn a score in [0, 1] mapped by the rule weight. Do not judge correctness or writing quality here\u2014only the presence of structure that enables verification.", "expectation": "An editable DOCX with redlined edits to the reporter\u2019s story and numerous margin comments that include source links (arXiv + NASA), plus at least one overall summary comment."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification and Content Checks (Mixed)", "description": "Now that the document shape is correct, verify factual anchors, sourcing presence, core concept coverage, and approximate length. Includes one LLM rule for factual emphasis/novelty.", "is_required": false, "max_points": 16.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "DOCX File Detected (Basic)", "description": "Verify the primary output is a DOCX document (defensive check).", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output:\n            return 0.0, \"No output found.\"\n        if not output.is_document:\n            return 0.0, \"Primary output is not a document.\"\n        path = context.files.get_path(output.id)\n        suffix = path.suffix.lower()\n        if suffix == '.docx':\n            return 1.0, \"DOCX detected.\"\n        # Allow legacy .doc partial credit? Requirements prefer DOCX; here we give 0.\n        return 0.0, f\"Not DOCX (found {suffix}).\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Core Concepts Mentioned", "description": "Check presence of key TRAPPIST-1 concepts: TRAPPIST-1, ultracool/M dwarf, habitable zone, JWST/Webb.", "weight": 3.0, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document to evaluate.\"\n        text = context.files.read_docx_text(output.id)\n        if not text:\n            return 0.0, \"Empty document text.\"\n        t = text.lower()\n        score = 0.0\n        max_score = 3.0\n        # TRAPPIST-1 frequency\n        trappist_count = len(re.findall(r\"trappist[-\\s]?1\", t))\n        cond_trappist = trappist_count >= 3\n        # Star type terms\n        cond_star = any(k in t for k in [\"ultracool dwarf\", \"m dwarf\", \"red dwarf\"]) \n        # Habitable zone\n        cond_hz = \"habitable zone\" in t\n        # JWST/Webb\n        cond_webb = (\"jwst\" in t) or (\"webb\" in t)\n        conds = [cond_trappist, cond_star, cond_hz, cond_webb]\n        met = sum(1 for c in conds if c)\n        score = (met / 4.0) * max_score\n        feedback = []\n        if not cond_trappist:\n            feedback.append(\"TRAPPIST-1 mentioned fewer than 3 times.\")\n        if not cond_star:\n            feedback.append(\"Missing star type term (ultracool/M dwarf/red dwarf).\")\n        if not cond_hz:\n            feedback.append(\"Missing 'habitable zone'.\")\n        if not cond_webb:\n            feedback.append(\"Missing JWST/Webb mention.\")\n        return score, \"; \".join(feedback) if feedback else \"All core concepts present.\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Citations to Required Sources Present", "description": "Check for presence of arXiv 2401.11815 and at least one authoritative NASA/related source (science.nasa.gov, astrobiology.nasa.gov, aanda.org).", "weight": 3.0, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document.\"\n        text = context.files.read_docx_text(output.id)\n        if not text:\n            return 0.0, \"Empty text.\"\n        t = text.lower()\n        # Look for arXiv paper 2401.11815\n        has_arxiv = (\"arxiv.org/abs/2401.11815\" in t) or (\"arxiv.org/pdf/2401.11815\" in t) or bool(re.search(r\"arxiv\\.org/(abs|pdf)/2401\\.11815\", t))\n        # Look for at least one of the authoritative domains relevant to the refs\n        has_nasa_science = (\"science.nasa.gov\" in t) and (\"trappist\" in t or \"webb\" in t)\n        has_astrobio = \"astrobiology.nasa.gov\" in t\n        has_aanda = \"aanda.org\" in t\n        has_any_authority = has_nasa_science or has_astrobio or has_aanda\n        score = 0.0\n        feedback = []\n        if has_arxiv:\n            score += 1.5\n        else:\n            feedback.append(\"Missing arXiv 2401.11815 link.\")\n        if has_any_authority:\n            score += 1.5\n        else:\n            feedback.append(\"Missing NASA/Astrobiology/A&A authoritative link.\")\n        return score, \"; \".join(feedback) if feedback else \"Has arXiv and authoritative links.\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Hyperlink Density and Domain Diversity", "description": "Check that there are enough URLs and from multiple domains (\u22655 URLs overall, \u22653 unique domains).", "weight": 2.0, "code": "import re\nfrom urllib.parse import urlparse\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document.\"\n        text = context.files.read_docx_text(output.id) or \"\"\n        urls = re.findall(r\"https?://[^\\s)\\]]+\", text)\n        domains = set()\n        for u in urls:\n            try:\n                d = urlparse(u).netloc.lower()\n                if d:\n                    domains.add(d)\n            except:\n                pass\n        url_count = len(urls)\n        domain_count = len(domains)\n        # Scoring: half for count, half for diversity\n        count_score = 1.0 if url_count >= 5 else min(url_count / 5.0, 1.0)\n        diversity_score = 1.0 if domain_count >= 3 else min(domain_count / 3.0, 1.0)\n        total = (count_score * 1.0 + diversity_score * 1.0)  # max 2.0\n        feedback = f\"URLs: {url_count}, domains: {domain_count}.\"\n        return total, feedback\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Approximate Story Length (Word Count)", "description": "Estimate total words in the document text as a proxy for story length. Target ~800 words; acceptable 600\u20131200.", "weight": 3.0, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No document.\"\n        text = context.files.read_docx_text(output.id)\n        if not text:\n            return 0.0, \"Empty text.\"\n        # Remove URLs from count\n        text_wo_urls = re.sub(r\"https?://\\S+\", \" \", text)\n        words = re.findall(r\"\\b\\w[\\w'\\-]*\\b\", text_wo_urls)\n        wc = len(words)\n        # Full credit if 600 <= wc <= 1200; partial if 450-1400 linearly tapered; else 0\n        if 600 <= wc <= 1200:\n            score = 3.0\n        elif 450 <= wc < 600:\n            # scale 0 to 3 across 450-600\n            score = 3.0 * (wc - 450) / (600 - 450)\n        elif 1200 < wc <= 1400:\n            score = 3.0 * (1400 - wc) / (1400 - 1200)\n        else:\n            score = 0.0\n        return max(0.0, min(3.0, score)), f\"Word count ~{wc}.\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "llm_judge", "name": "Factual Emphasis and Novelty (LLM)", "description": "Check that the edited story emphasizes the novelty of the research process (studying an ultracool/M-dwarf unlike the Sun) and avoids common inaccuracies about TRAPPIST-1.", "weight": 4.0, "judge_prompt": "Evaluate the edited document for factual emphasis and avoidance of common inaccuracies. Do not grade writing style here\u2014focus on factual framing and emphasis.\n\nCheck for the following (use your general scientific knowledge of TRAPPIST-1; do not browse):\n1) The story correctly identifies TRAPPIST-1 as an ultracool/red/M dwarf star system about 40 light-years away with seven roughly Earth-sized rocky planets.\n2) The story appropriately frames habitability uncertainty (e.g., atmospheres not confirmed; JWST has placed limits) and avoids overclaiming life or atmospheres.\n3) The story highlights the novelty described in the task: that this research advances study of worlds around non-Sun-like stars (ultracool/M dwarfs), and why that matters for future searches (e.g., methods, observing strategies, star activity considerations), with concrete takeaways.\n4) Where scientific edits were made, the editor provided source links in or near the relevant comments.\n\nScoring guidelines:\n- 1.0: All four conditions satisfied; facts are accurate and novelty is clearly foregrounded with correct nuance.\n- 0.7: Minor factual or emphasis issues (e.g., slight overstatement or thin novelty explanation) but overall correct.\n- 0.4: Multiple issues (missing novelty emphasis or notable factual hedging missing).\n- 0.0: Major inaccuracies or misleading framing (e.g., claiming confirmed biosignatures or atmospheres; misidentifying the star type or planet count).\n\nReturn a score in [0, 1] mapped by the rule weight.", "expectation": "Accurate, nuanced emphasis on studying ultracool/M-dwarf systems (non-Sun-like) and implications for future habitability research; avoids hype and overclaims; includes source links near scientific edits."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Editorial Quality and Audience Fit", "description": "Holistic quality assessment: clarity, flow, tone, and usefulness of edits for a scientifically curious non-expert audience.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Editorial Craft, Clarity, and Actionability", "description": "Assess overall editorial quality, clarity for non-experts, coherence after edits, and the actionability/constructiveness of comments.", "weight": 4.0, "judge_prompt": "Holistically assess the edited document as an editorial deliverable for a scientifically curious non-expert audience.\n\nConsider:\n- Clarity and flow: Do the edits improve structure and readability? Are transitions and ledes tightened without adding jargon?\n- Audience fit: Are complex concepts (e.g., star type, habitability caveats, JWST methods) explained accessibly with minimal jargon and accurate simplifications?\n- Comment quality: Are comments polite, specific, constructive, and actionable (e.g., pointing to missing context, suggesting re-reporting, data checks, or added explanations)? Do they acknowledge what\u2019s strong when appropriate?\n- Professionalism: Clean presentation, judicious use of tracked changes (no gratuitous rewriting), and consistent editorial voice.\n\nScoring guidelines:\n- 1.0: Strong improvements to clarity and flow; excellent, actionable comments; well-calibrated for non-experts; professional tone.\n- 0.7: Good overall, with a few minor issues (spotty explanations or occasional heavy jargon).\n- 0.4: Mixed; several unclear sections remain and/or comments are generic or minimally helpful.\n- 0.0: Poor; edits reduce clarity or comments are unhelpful/unprofessional.\n\nReturn a score in [0, 1] mapped by the rule weight.", "expectation": "A professional, reader-centered edit with constructive, specific comments and clear explanations suitable for non-experts."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "11e1b169-5fb6-4d79-8a83-82ddf4987a85", "rubric": {"category_name": "Government \u2014 First-Line Supervisors of Police and Detectives | Roll-Call Legal Quick Reference (KY)", "rationale": "Pattern B (Document). The deliverable is a two-page PDF quick-reference guide for patrol officers. Stage 1 (LLM-only) enforces exact document structure and verifiable sections, enabling straightforward verification. Stage 2 mixes code and LLM checks for presence of critical legal concepts, KY-specific citations, reasonable length, and practical bulleting. Stage 3 assesses field usability and professional tone for a roll-call context.", "max_total_score": 11.7, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate (LLM)", "description": "Gate: Enforce exact file type, page count, and required quick-reference structure so verification is trivial.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.2, "rules": [{"type": "llm_judge", "name": "Two-Page PDF Quick-Reference Structure", "description": "Verify the candidate is a two-page PDF with a clear title and eight required sections, each with minimal quick-reference elements.", "weight": 4.0, "judge_prompt": "You are validating the SHAPE ONLY of a training quick-reference. Do not judge correctness of legal content. Review the submitted output (rendered PDF) and score based on format and structure.\n\nFormat requirements (must check all):\n- File type: PDF (not Word, not plain text, not Excel)\n- Page count: exactly 2 pages\n- Professional quick-reference layout suitable for roll call (e.g., concise sections, bullets, short sentences)\n\nRequired sections (headers may vary slightly; be flexible, but content must clearly map to each):\n1) Fourth Amendment (Search and Seizure)\n2) Reasonable Suspicion\n3) Probable Cause\n4) Exigent Circumstances\n5) Terry Stops (Investigatory stops)\n6) Pat Downs (Frisks)\n7) Protective Sweeps\n8) KRS 503.090 (Use of Physical Force in Law Enforcement)\n\nMicro-structure required per section (flexible with naming, but each section should visibly include):\n- A brief definition/summary (1\u20132 sentences)\n- A labeled quick standard cue (e.g., \"Key Standard\", \"Rule\", or similar)\n- Practical bullets (at least 2 bullet points) providing field cues or considerations\n- A short officer guidance cue such as \"Do/Don\u2019t\" or \"Officer Tip\" (naming flexible; concise practical guidance is acceptable)\n\nOptional but encouraged: A small references/sources box or footer listing sources.\n\nScoring (STRUCTURE ONLY):\n- 1.0: PDF, exactly 2 pages, clear title, all 8 sections present with the required micro-structure elements in each.\n- 0.8: PDF, exactly 2 pages, all 8 sections present but 1\u20132 sections missing one micro-element OR references section absent.\n- 0.6: PDF, exactly 2 pages, 6\u20137 sections present with most micro-elements OR all 8 present but multiple sections are missing micro-elements.\n- 0.3: PDF, exactly 2 pages, only 3\u20135 sections present and/or minimal quick-reference structure.\n- 0.0: Not a PDF, not exactly 2 pages, or fewer than 3 required sections.\n\nReturn a score according to the rubric, and succinct feedback stating any missing structural elements.", "expectation": "A 2-page PDF quick-reference with 8 clearly labeled sections, each containing a short definition, a key standard cue, practical bullets, and an officer tip; professional and concise layout; optional references box."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Content Presence and Basic Correctness)", "description": "Now that the shape is correct, verify presence of key legal concepts, KY-specific citations, reasonable length for 2 pages, and practical bulleting. Mix code checks with an LLM accuracy scan.", "is_required": false, "max_points": 5.5, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Section Keywords Present", "description": "Checks presence of essential topic keywords across the PDF text to confirm each required concept appears.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No primary document output.\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception as e:\n        return 0.0, f\"Failed to read PDF text: {e}\"\n    if not text:\n        return 0.0, \"Empty PDF text.\"\n\n    low = text.lower()\n\n    topics = {\n        'fourth amendment/search and seizure': any(k in low for k in ['fourth amendment', '4th amendment', 'search and seizure']),\n        'reasonable suspicion': 'reasonable suspicion' in low,\n        'probable cause': 'probable cause' in low,\n        'exigent circumstances': 'exigent' in low,\n        'terry stops': any(k in low for k in ['terry stop', 'terry v. ohio', 'stop and frisk', 'investigatory stop']),\n        'pat downs/frisks': any(k in low for k in ['pat down', 'pat-down', 'frisk']),\n        'protective sweeps': 'protective sweep' in low,\n        'use of force (krs 503.090)': any(k in low for k in ['krs 503.090', '503.090', 'use of physical force'])\n    }\n\n    found = sum(1 for v in topics.values() if v)\n    total = len(topics)\n    score = found / total\n\n    missing = [k for k, v in topics.items() if not v]\n    fb = f\"Found {found}/{total} key topic signals. Missing: {', '.join(missing) if missing else 'None'}.\"\n    return max(0.0, min(1.0, score)), fb"}, {"type": "code", "name": "KRS 503.090 Citation Specificity", "description": "Confirm explicit reference to KRS 503.090 (KY Use of Physical Force statute).", "weight": 0.7, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output.\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception as e:\n        return 0.0, f\"Failed to read PDF text: {e}\"\n    low = (text or '').lower()\n\n    has_exact = 'krs 503.090' in low\n    has_number_only = '503.090' in low\n\n    if has_exact:\n        return 1.0, \"Explicit 'KRS 503.090' found.\"\n    if has_number_only:\n        return 0.6, \"Statute number found without 'KRS' prefix.\"\n    return 0.0, \"No KRS 503.090 reference detected.\""}, {"type": "code", "name": "Reasonable Length for Two Pages", "description": "Check that total word count is plausible for a concise, two-page quick reference.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output.\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception as e:\n        return 0.0, f\"Failed to read PDF text: {e}\"\n    words = re.findall(r\"\\b\\w+\\b\", text or '')\n    wc = len(words)\n    # Ideal concise 2-page quick reference: ~500\u20131100 words\n    if 500 <= wc <= 1100:\n        return 1.0, f\"Word count {wc} within ideal range.\"\n    if 400 <= wc <= 1300:\n        return 0.6, f\"Word count {wc} acceptable but not ideal.\"\n    return 0.0, f\"Word count {wc} implausible for a 2-page quick reference.\""}, {"type": "code", "name": "Practical Bullets Density", "description": "Count bullet/numbered list lines to ensure actionable quick-reference cues are present.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output.\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception as e:\n        return 0.0, f\"Failed to read PDF text: {e}\"\n    lines = (text or '').splitlines()\n    bullet_re = re.compile(r\"^\\s*(?:[-\u2022*]|\\d+\\.|[a-zA-Z]\\))\\s+\")\n    count = sum(1 for ln in lines if bullet_re.match(ln))\n    # Expect at least ~16 bullets across 8 sections; scale up to 20\n    if count <= 0:\n        return 0.0, \"No bullet-like lines detected.\"\n    score = min(count / 20.0, 1.0)\n    return score, f\"Detected {count} bullet-like lines.\""}, {"type": "code", "name": "Kentucky Context Signals", "description": "Verify Kentucky-specific context is present beyond the statute number (e.g., 'Kentucky', 'KRS', 'Commonwealth', 'Ky.').", "weight": 0.5, "code": "def evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output.\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception as e:\n        return 0.0, f\"Failed to read PDF text: {e}\"\n    low = (text or '').lower()\n    tokens = {\n        'kentucky': 'kentucky' in low,\n        'krs': 'krs' in low,\n        'commonwealth': 'commonwealth' in low,\n        'ky.': 'ky.' in low or ' ky ' in low or '(ky' in low\n    }\n    hits = sum(1 for v in tokens.values() if v)\n    # At least 2 distinct KY signals for full credit; 1 signal = partial\n    if hits >= 2:\n        return 1.0, f\"KY signals present: {', '.join([k for k,v in tokens.items() if v])}\"\n    if hits == 1:\n        return 0.5, f\"Only one KY signal present: {', '.join([k for k,v in tokens.items() if v])}\"\n    return 0.0, \"No KY context signals detected.\""}, {"type": "llm_judge", "name": "High-Level Legal Accuracy and Distinctions", "description": "LLM verifies that brief definitions and constraints are broadly accurate and distinguish key standards (RS vs PC; Terry scope; weapons-only frisk; exigency limits; protective sweep scope; use-of-force factors).", "weight": 1.5, "judge_prompt": "Evaluate whether the content is broadly accurate at a high level (not legal advice). Focus on whether the guide communicates the following distinctions and constraints correctly and non-misleadingly:\n\n- Reasonable Suspicion vs Probable Cause: RS is a specific, articulable suspicion; PC is a fair probability standard. RS < PC; RS supports a stop; PC supports arrest/search with warrant exception.\n- Terry Stops: Brief, investigatory detention; limited duration; scope tied to confirming/dispelling suspicion; requires RS; not a de facto arrest.\n- Pat Downs/Frisks: Weapons-only frisk based on RS that the person is armed and dangerous; not a general evidence search.\n- Exigent Circumstances: Narrow exceptions permitting warrantless entry/search; examples: hot pursuit, imminent destruction of evidence, immediate danger; must be objectively reasonable and not police-created exigency.\n- Protective Sweeps: Quick, limited search of spaces where a person may be found; tied to arrest/entry; aimed at safety; not a full search.\n- Fourth Amendment baseline: Reasonableness; warrant preference; recognized exceptions.\n- KRS 503.090 (Use of Physical Force in Law Enforcement): Describes when force is authorized under KY law; should align with objective reasonableness and proportionality; deadly force constraints (e.g., consistent with constitutional standards like Graham/ Garner if referenced).\n\nScoring guide:\n- 1.0\u20131.5: Definitions and constraints are accurate and clearly distinguish standards with no material inaccuracies.\n- 0.6\u20130.9: Mostly accurate; minor omissions or slightly unclear distinctions but not misleading.\n- 0.1\u20130.5: Some inaccuracies or missing key constraints likely to mislead.\n- 0.0: Major inaccuracies that misstate legal thresholds or scope.\n\nReturn a score and brief rationale focusing on correctness of distinctions, not formatting.", "expectation": "Clear, non-misleading distinctions: RS vs PC; Terry scope; weapons-only frisk; exigency and protective sweep limits; KY force statute aligned with constitutional standards."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Field Usability", "description": "Holistic assessment of usefulness in a roll-call context: clarity, actionability, tone, and professionalism.", "is_required": false, "max_points": 2.2, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Field Usability and Clarity", "description": "Is this truly a pocketable, roll-call quick reference? Concise, scannable, with actionable do/don\u2019t and cues.", "weight": 1.2, "judge_prompt": "Assess field usability for mid-watch patrol officers: Is the guide concise, scannable (bullets, short phrases), and immediately actionable? Look for:\n- Clear, plain-language phrasing suitable for officers with varying legal background\n- Actionable bullets and brief do/don\u2019t or tips per section\n- Logical ordering and white space aiding fast lookup under time pressure\n- Avoidance of dense legalese and footnote-heavy prose\n\nScoring:\n- 1.0\u20131.2: Highly usable quick-reference with clear, actionable bullets and excellent scannability\n- 0.5\u20130.9: Generally usable with some density or minor clarity issues\n- 0.1\u20130.4: Limited usability; dense text or lacking actionable cues\n- 0.0: Not usable in the field context\n\nReturn score and short feedback.", "expectation": "Concise, scannable bullets and tips tailored for quick field reference during roll call and patrol."}, {"type": "llm_judge", "name": "Professional Tone and Presentation", "description": "Tone, organization, and professional presentation appropriate for a large police agency.", "weight": 0.6, "judge_prompt": "Evaluate tone and presentation: professional, neutral, and appropriate for a large police agency\u2019s roll-call training. Check for:\n- Neutral, objective language; avoids slang or sensationalism\n- Consistent headings and formatting, readable typography\n- No distracting errors (typos, glaring grammar issues)\n\nScoring:\n- 0.5\u20130.6: Professional and polished\n- 0.2\u20130.4: Generally professional with minor issues\n- 0.0\u20130.1: Unprofessional or error-prone\n\nReturn score and brief feedback.", "expectation": "Professional, neutral tone with clean, consistent formatting."}, {"type": "llm_judge", "name": "References and Source Support", "description": "Are credible references or links provided to authoritative sources (optional but valuable)?", "weight": 0.4, "judge_prompt": "Check whether the guide includes a brief references box or source list (optional in Stage 1 but valuable), preferably authoritative (e.g., statutes, DOJ/OJP, FLETC, court cases, official KY legislative site). Scoring:\n- 0.3\u20130.4: Includes credible, relevant references or links\n- 0.1\u20130.2: References present but limited or of mixed credibility\n- 0.0: No references provided\n\nReturn score and short note.", "expectation": "A compact references box citing authoritative sources (e.g., KRS site, FLETC, OJP, key cases)."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "69a8ef86-4e69-4fe2-9168-080f1e978e67", "rubric": {"category_name": "Wholesale Trade \u2013 Sales Managers \u2013 RA Process Documentation", "rationale": "This rubric enforces a self-documenting, two-document deliverable: an internal RA process (structured, step-by-step with SLAs, roles, and systems) and an external key-account guideline (clear instructions, labeling/EDI, timelines). Stage 1 (LLM-only) mandates exact structure and format so verification is trivial. Stage 2 mixes code checks for SLA presence, role coverage, external requirements, and cross-document consistency. Stage 3 evaluates professional quality for both audiences.", "max_total_score": 14.0, "stages": [{"name": "Stage 1 \u2013 Shape Enforcement Gate (STRUCTURE ONLY)", "description": "MANDATORY format and structure check. Two separate documents in verifiable formats with required sections and visible SLAs. No content-quality judgment here.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Two-Document Structure and Section Completeness", "description": "Confirm there are TWO separate documents (DOCX or PDF), each at least 2 pages, with required sections and the five SLA milestones present in the internal process. External doc must include retailer-facing guidance sections.", "weight": 4.0, "judge_prompt": "You are evaluating whether the candidate provided TWO separate documents with the exact structural requirements below. Only check PRESENCE and STRUCTURE, not the correctness of content or calculations. Be flexible with section names if intent is clear.\n\nDeliverables Required:\n- Two separate files (not one combined file):\n  \u2022 Document A: Internal RA Process (internal audience)\n  \u2022 Document B: External RA Guidelines (key account/retailer audience)\n- Valid formats: DOCX or PDF only.\n- Each document must be at least 2 pages.\n\nDocument A \u2013 Internal RA Process (must include all):\n1) Title and metadata: company name, date/effective date, and version/owner (any visible metadata block is acceptable).\n2) Purpose/Scope: intent of the document and which key accounts/processes it covers.\n3) Roles and Responsibilities: includes at minimum these roles/systems (exact acronyms may vary, but intent must be visible):\n   - KAM or KAR (Key Account Manager/Representative)\n   - CS (Customer Service)\n   - Warehouse/DC (Distribution Center)\n   - Finance/AR (Accounts Receivable)\n   - EDI/SPS (SPS Commerce or EDI team)\n   - D365 (Dynamics 365)\n4) Step-by-step RA workflow presented clearly (preferably tabular) with columns or clearly labeled fields showing at least: Action(s), Responsible Role/Team, and Timeline/SLA for each step.\n5) The following five SLA milestones explicitly present (wording may vary but intent and durations must be visible):\n   a) 3 days from KAM/KAR approval of RTV to RA# issued\n   b) Returns received at warehouse/DC within 60 days of RA issuance\n   c) 14 days for warehouse/DC to provide report of items to CS\n   d) Return credit issued within 45 days of warehouse receiving shipment\n   e) RA closed internally after 90 days of creation; if not received/credited, manually close and notify account\n6) Exception handling/escalation and basic controls/reporting (e.g., logs, D365 entries).\n\nDocument B \u2013 External RA Guidelines (must include all):\n1) Title and metadata: company name and effective date/version or similar.\n2) Overview/Effective Date of policy and who it applies to (key accounts/retailers).\n3) RA Request Requirements (what the retailer must submit): purchase order/order number, SKU/style/UPC and quantities, reason codes/return reason, photos for defects (if applicable), contact info.\n4) Return Window and Timelines: explicitly states the 60-day return window from RA issuance for the shipment to be received at DC/warehouse.\n5) Packaging/Labeling/Documentation: RA# must be on outer carton and packing slip; packing list included; carton count; EDI/ASN via SPS if applicable (or equivalent EDI instructions).\n6) Shipping/Routing Instructions: destination/address or reference to routing guide; carrier/billing terms; appointments if palletized.\n7) Credit and Timing: communicates that credit is issued within 45 days of warehouse receipt; how credit memos are communicated/tracked; dispute process.\n8) Non-compliance/Exceptions: mention of potential chargebacks, refusal, or delays for non-compliance.\n9) Contact/Support: CS and/or EDI contact details.\n\nScoring (0\u20134):\n- 4.0: Two separate DOCX/PDF files; each >= 2 pages; all required sections for both docs present; internal doc clearly shows all 5 SLA milestones; external doc includes required retailer-facing sections.\n- 3.0: Two files and valid formats/pages; minor omissions (e.g., one supporting section missing or one SLA phrased but slightly unclear), overall structure is present.\n- 2.0: Two files exist but multiple required sections missing across docs or internal SLA coverage incomplete (e.g., only 2\u20133 of 5).\n- 1.0: Only one document provided OR wrong format (not PDF/DOCX) OR both docs < 2 pages, even if some sections exist.\n- 0.0: Not documents, or structure so incomplete that verification is impossible.\n\nImportant: Judge only the presence/structure and section visibility. Do not evaluate accuracy of timelines, feasibility, or writing quality.", "expectation": "Two clearly separate, well-structured DOCX/PDF documents with internal step-by-step process (including all 5 SLAs) and external retailer guidelines (with request, labeling/EDI, timelines, credit, and contacts)."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification (Content Presence, Consistency)", "description": "Deterministic checks over extracted text to verify SLAs, role coverage, external requirements, and cross-document consistency. Flexible matching is used to handle wording variants.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 3.0, "rules": [{"type": "code", "name": "Two Document Files Present (Basic)", "description": "Verify at least two document files (PDF/DOCX) were produced.", "weight": 0.5, "code": "def evaluate(workflow, context):\n    try:\n        docs = [r for r in context.get_all_outputs() if getattr(r, 'is_document', False)]\n        score = 0.5 if len(docs) >= 2 else 0.0\n        feedback = f\"Found {len(docs)} document(s).\" if docs else \"No documents found.\"\n        return score, feedback\n    except Exception as e:\n        return 0.0, f\"Error checking documents: {e}\""}, {"type": "code", "name": "Internal SLA Milestones Presence", "description": "Check internal document text contains the five SLA milestones: 3/60/14/45/90-day items with contextual keywords.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    def read_text(res):\n        try:\n            return context.files.read_docx_text(res.id)\n        except Exception:\n            try:\n                return context.files.read_pdf_text(res.id)\n            except Exception:\n                return ''\n\n    def pick_internal(docs_text):\n        # Heuristic: internal cues vs external\n        scores = []\n        for i, t in enumerate(docs_text):\n            tl = t.lower()\n            cues = 0\n            for kw in [\n                'internal', 'warehouse', 'distribution center', 'dc', 'd365',\n                'sps', 'edi', 'roles', 'responsible', 'owner', 'process', 'sla',\n                'raci', 'approval', 'close ra', 'close the ra'\n            ]:\n                if kw in tl:\n                    cues += 1\n            scores.append((cues, i))\n        scores.sort(reverse=True)\n        return scores[0][1] if scores else 0\n\n    def contains_sla(text, number, context_terms):\n        tl = ' '.join(text.lower().split())\n        # Match number as digits or words for 3/14/45/60/90\n        words_map = {'3':'three','14':'fourteen','45':'forty five','60':'sixty','90':'ninety'}\n        num_word = words_map.get(str(number), '')\n        num_pat = rf\"(\\b{number}\\b|\\b{number}\\s*day|\\b{num_word}\\b)\\s*(business\\s*)?day\"\n        if not re.search(num_pat, tl):\n            return False\n        # All context terms should appear within a reasonable window of the number mention\n        # Simplify: require all context terms exist anywhere in doc, plus at least one within 120 chars\n        idx = re.search(num_pat, tl)\n        if not idx:\n            return False\n        start = max(0, idx.start()-120)\n        end = min(len(tl), idx.end()+120)\n        window = tl[start:end]\n        near_ok = any(ct in window for ct in context_terms)\n        far_ok = all(ct in tl for ct in context_terms if ct in ['ra','credit','warehouse','dc','cs','close'])\n        return near_ok or far_ok\n\n    try:\n        docs = [r for r in context.get_all_outputs() if getattr(r, 'is_document', False)]\n        if len(docs) == 0:\n            return 0.0, 'No documents to check.'\n        texts = [read_text(d) for d in docs]\n        if not any(texts):\n            return 0.0, 'Could not read any document text.'\n        internal_idx = pick_internal(texts)\n        itext = texts[internal_idx]\n        if not itext:\n            return 0.0, 'Internal document text not readable.'\n\n        checks = []\n        # 3 days -> RA issued\n        checks.append(contains_sla(itext, 3, ['ra', 'issue']))\n        # 60 days -> received at warehouse/DC\n        checks.append(contains_sla(itext, 60, ['receive', 'warehouse']) or contains_sla(itext, 60, ['receive', 'dc']))\n        # 14 days -> warehouse report to CS\n        ok14 = ('14' in itext.lower() or 'fourteen' in itext.lower()) and ('report' in itext.lower() or 'reconciliation' in itext.lower()) and (('warehouse' in itext.lower()) and ('cs' in itext.lower() or 'customer service' in itext.lower()))\n        checks.append(ok14)\n        # 45 days -> credit issued after receipt\n        checks.append(contains_sla(itext, 45, ['credit', 'issue']))\n        # 90 days -> RA closed\n        ok90 = ('90' in itext.lower() or 'ninety' in itext.lower()) and ('close' in itext.lower()) and ('ra' in itext.lower())\n        checks.append(ok90)\n\n        passed = sum(1 for c in checks if c)\n        score = (passed / 5.0) * 1.5\n        return score, f\"Internal SLA checks passed: {passed}/5\"\n    except Exception as e:\n        return 0.0, f\"Error verifying SLAs: {e}\""}, {"type": "code", "name": "Internal Roles/Systems Coverage", "description": "Verify internal doc mentions key roles/systems: KAM/KAR, CS, Warehouse/DC, Finance/AR, EDI/SPS, and D365.", "weight": 0.9, "code": "def evaluate(workflow, context):\n    def read_text(res):\n        try:\n            return context.files.read_docx_text(res.id)\n        except Exception:\n            try:\n                return context.files.read_pdf_text(res.id)\n            except Exception:\n                return ''\n\n    def pick_internal(texts):\n        best_i, best_s = 0, -1\n        for i, t in enumerate(texts):\n            tl = t.lower()\n            cues = sum(kw in tl for kw in ['internal','warehouse','dc','d365','sps','edi','roles','responsible','owner','process','sla'])\n            if cues > best_s:\n                best_s, best_i = cues, i\n        return best_i\n\n    try:\n        docs = [r for r in context.get_all_outputs() if getattr(r, 'is_document', False)]\n        texts = [read_text(d) for d in docs]\n        if not texts:\n            return 0.0, 'No text available.'\n        idx = pick_internal(texts)\n        tl = texts[idx].lower() if texts[idx] else ''\n        if not tl:\n            return 0.0, 'Internal doc not readable.'\n\n        checks = {\n            'kam_or_kar': any(k in tl for k in ['kam','kar','key account manager','key account representative']),\n            'cs': ('customer service' in tl) or (' cs ' in f' {tl} '),\n            'wh_dc': any(k in tl for k in ['warehouse','distribution center',' dc ']),\n            'finance_ar': any(k in tl for k in ['finance','accounts receivable',' ar ']),\n            'edi_sps': any(k in tl for k in ['edi','sps']),\n            'd365': any(k in tl for k in ['d365','dynamics 365'])\n        }\n        total = sum(1 for v in checks.values() if v)\n        raw = total / 6.0  # 0..1\n        score = raw * 0.9\n        return score, f\"Roles/systems present: {total}/6\"\n    except Exception as e:\n        return 0.0, f\"Error verifying roles: {e}\""}, {"type": "code", "name": "External Retailer Requirements Presence", "description": "Check the external doc includes request info, labeling/packaging/EDI, timelines, credit timing, non-compliance, and contact info.", "weight": 1.5, "code": "def evaluate(workflow, context):\n    import re\n    def read_text(res):\n        try:\n            return context.files.read_docx_text(res.id)\n        except Exception:\n            try:\n                return context.files.read_pdf_text(res.id)\n            except Exception:\n                return ''\n\n    def pick_external(texts):\n        best_i, best_s = 0, -1\n        for i, t in enumerate(texts):\n            tl = t.lower()\n            cues = 0\n            for kw in ['guidelines','retailer','key account','label','packing','outer carton','non-compliance','chargeback','contact','routing','appointment','asn','sps','edi']:\n                if kw in tl:\n                    cues += 1\n            if cues > best_s:\n                best_s, best_i = cues, i\n        return best_i\n\n    try:\n        docs = [r for r in context.get_all_outputs() if getattr(r, 'is_document', False)]\n        texts = [read_text(d) for d in docs]\n        if not texts:\n            return 0.0, 'No documents to evaluate.'\n        eidx = pick_external(texts)\n        tl = texts[eidx].lower() if texts[eidx] else ''\n        if not tl:\n            return 0.0, 'External doc not readable.'\n\n        score = 0.0\n        # RA request info elements (0.5 total)\n        req_elems = [\n            any(k in tl for k in ['po', 'purchase order', 'order number']),\n            any(k in tl for k in ['sku', 'style', 'upc']),\n            'quantity' in tl,\n            any(k in tl for k in ['reason code', 'return reason', 'defect']),\n            any(k in tl for k in ['photo', 'image', 'picture']),\n            'contact' in tl\n        ]\n        score += (sum(1 for x in req_elems if x) / 6.0) * 0.5\n\n        # Packaging/Labeling/Documentation (0.6 total)\n        pack_checks = [\n            ('ra#' in tl) or ('ra number' in tl) or ('rma' in tl),\n            any(k in tl for k in ['outer carton','box exterior','carton label']),\n            any(k in tl for k in ['packing slip','packing list']),\n            any(k in tl for k in ['carton count','number of cartons']),\n            any(k in tl for k in ['asn','advance ship notice','sps','edi']),\n        ]\n        score += (sum(1 for x in pack_checks if x) / 5.0) * 0.6\n\n        # Timelines: 60-day window (0.2)\n        if ('60' in tl or 'sixty' in tl) and any(k in tl for k in ['day','days']) and any(k in tl for k in ['window','received','receive','arrival','warehouse','dc']):\n            score += 0.2\n        # Credit 45 days (0.1)\n        if ('45' in tl or 'forty five' in tl) and ('credit' in tl):\n            score += 0.1\n        # Non-compliance (0.1)\n        if any(k in tl for k in ['non-compliance','noncompliance','chargeback','refusal','refused']):\n            score += 0.1\n        # Contact (0.1)\n        if any(k in tl for k in ['contact','email','support']):\n            score += 0.1\n\n        # Cap at weight\n        if score > 1.5:\n            score = 1.5\n        return score, 'External requirements evaluated.'\n    except Exception as e:\n        return 0.0, f\"Error verifying external guidelines: {e}\""}, {"type": "code", "name": "Cross-Document Consistency on Timelines", "description": "Check both docs consistently mention the 60-day return window and 45-day credit timing; award bonus if internal also mentions 3/14/90 steps.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    def read_text(res):\n        try:\n            return context.files.read_docx_text(res.id)\n        except Exception:\n            try:\n                return context.files.read_pdf_text(res.id)\n            except Exception:\n                return ''\n\n    def classify(texts):\n        # Return (internal_idx, external_idx)\n        internal_scores, external_scores = [], []\n        for i, t in enumerate(texts):\n            tl = t.lower()\n            int_cues = sum(kw in tl for kw in ['internal','warehouse','dc','d365','sps','edi','sla','process'])\n            ext_cues = sum(kw in tl for kw in ['guidelines','retailer','label','packing','non-compliance','contact','routing','asn'])\n            internal_scores.append((int_cues, i))\n            external_scores.append((ext_cues, i))\n        internal_idx = sorted(internal_scores, reverse=True)[0][1] if texts else 0\n        external_idx = sorted(external_scores, reverse=True)[0][1] if texts else (1 if len(texts) > 1 else 0)\n        if internal_idx == external_idx and len(texts) > 1:\n            external_idx = 1 if internal_idx == 0 else 0\n        return internal_idx, external_idx\n\n    try:\n        docs = [r for r in context.get_all_outputs() if getattr(r, 'is_document', False)]\n        texts = [read_text(d) for d in docs]\n        if len(texts) < 2:\n            return 0.0, 'Need two documents for consistency check.'\n        i_idx, e_idx = classify(texts)\n        itl = texts[i_idx].lower() if texts[i_idx] else ''\n        etl = texts[e_idx].lower() if texts[e_idx] else ''\n        score = 0.0\n        # 60-day window alignment\n        if (('60' in itl or 'sixty' in itl) and ('60' in etl or 'sixty' in etl)):\n            score += 0.4\n        # 45-day credit alignment\n        if ('45' in itl or 'forty five' in itl) and ('45' in etl or 'forty five' in etl) and ('credit' in itl) and ('credit' in etl):\n            score += 0.4\n        # Bonus for internal completeness (3/14/90 present) \u2013 not required in external\n        extra = 0\n        for n in ['3','14','90']:\n            if n in itl:\n                extra += 1\n        score += min(extra, 3) * (0.2/3)\n        return score, 'Consistency on timelines evaluated.'\n    except Exception as e:\n        return 0.0, f\"Error in consistency check: {e}\""}, {"type": "code", "name": "Step-by-Step Structure Cues (Internal)", "description": "Detect whether internal doc appears to use a step-by-step structure with labeled action/owner/timeline fields.", "weight": 0.5, "code": "def evaluate(workflow, context):\n    def read_text(res):\n        try:\n            return context.files.read_docx_text(res.id)\n        except Exception:\n            try:\n                return context.files.read_pdf_text(res.id)\n            except Exception:\n                return ''\n\n    def pick_internal(texts):\n        best_i, best_s = 0, -1\n        for i, t in enumerate(texts):\n            tl = t.lower()\n            cues = sum(kw in tl for kw in ['internal','roles','responsible','owner','timeline','deadline','sla','warehouse','dc','process'])\n            if cues > best_s:\n                best_s, best_i = cues, i\n        return best_i\n\n    try:\n        docs = [r for r in context.get_all_outputs() if getattr(r, 'is_document', False)]\n        texts = [read_text(d) for d in docs]\n        if not texts:\n            return 0.0, 'No documents to evaluate.'\n        idx = pick_internal(texts)\n        tl = texts[idx].lower() if texts[idx] else ''\n        if not tl:\n            return 0.0, 'Internal doc not readable.'\n        step_cues = tl.count('step ') + tl.count('step-') + tl.count('step:')\n        has_action = 'action' in tl\n        has_owner = ('responsible' in tl) or ('owner' in tl)\n        has_timeline = any(k in tl for k in ['timeline','deadline','sla'])\n        score = 0.0\n        if step_cues >= 2 and has_action and has_owner and has_timeline:\n            score = 0.5\n        elif (step_cues >= 1 and ((has_action and has_owner) or (has_action and has_timeline) or (has_owner and has_timeline))):\n            score = 0.3\n        elif has_action or has_owner or has_timeline:\n            score = 0.1\n        return score, 'Step-by-step cues evaluated.'\n    except Exception as e:\n        return 0.0, f\"Error evaluating step-by-step cues: {e}\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Quality and Professionalism", "description": "Holistic assessment of clarity, operational viability, and audience appropriateness for both documents.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Quality and Fitness for Purpose", "description": "Assess clarity, completeness, cohesion, and professionalism for internal and external audiences.", "weight": 4.0, "judge_prompt": "Evaluate the overall professional quality of both documents. Consider clarity, organization, cohesion, and whether each document suits its intended audience and purpose. The internal process should be operationally actionable (clear steps, roles, controls, escalations). The external guidelines should be retailer-friendly (concise, unambiguous, actionable, minimizes back-and-forth) and align to the stated timelines.\n\nCriteria:\n- Clarity and structure: headings, logical flow, minimal ambiguity.\n- Actionability: internal doc reads like a runbook; external doc tells retailers exactly what to do.\n- Alignment and completeness: internal and external timelines align (60-day receipt, 45-day credit), and internal doc includes controls/escalations.\n- Professional tone and formatting: consistent terminology, avoids jargon where unclear, visually readable.\n\nScoring (0\u20134):\n- 4.0: Highly professional, clearly actionable, minimal ambiguity; strong alignment between docs; excellent audience fit.\n- 3.0: Generally strong with minor gaps or mild ambiguity; still very usable.\n- 2.0: Usable but with noticeable gaps, inconsistencies, or clarity issues that would require edits.\n- 1.0: Poorly organized or confusing; significant rewrites required.\n- 0.0: Not professional or not fit for purpose.\n\nJudge quality only; do not re-check Stage 1 structure.", "expectation": "Two polished, audience-appropriate documents: an actionable internal runbook and a concise, unambiguous external guideline aligned to the specified SLAs."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "c7d83f01-2874-4876-b7fd-52582ec99e1a", "rubric": {"category_name": "American Option Pricing Framework (Finance and Insurance > Financial and Investment Analysts)", "rationale": "This rubric enforces a self-documenting, verifiable submission for an American option pricing framework. Stage 1 (LLM-only) mandates a professional PDF/DOCX report with an explicit Artifact Manifest listing the accompanying notebook, benchmark data, and figures. This shape makes Stage 2 code checks straightforward: we locate the .ipynb, parse code cells for required methodologies, validate benchmarks tables, run intrinsic-value bounds checks, confirm presence of figures, and cross-reference the manifest to actual files. Stage 3 uses LLM judgment to assess analytical depth and the practicality of recommendations for a high-performance trading context.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (LLM ONLY)", "description": "Primary output must be a professional PDF/DOCX report with specific sections and an explicit Artifact Manifest referencing accompanying files (notebook, benchmarks, figures). This is a hard gate.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Report + Artifact Manifest Requirement", "description": "Verify the primary output is a PDF or DOCX report (not a notebook or spreadsheet) with required structure and explicit manifest linking to the code and data artifacts.", "weight": 4.0, "judge_prompt": "You are the Stage 1 GATE. Examine the candidate's primary output only.\n\nGoal: Confirm that the primary output is a professional PDF/DOCX report with the exact structure below. Do NOT assess correctness or quality beyond structure. Be flexible with section titles if meaning is equivalent.\n\nFORMAT REQUIREMENTS:\n- Must be a PDF or DOCX file (not plain text, not Excel, not a notebook).\n- At least 3 pages.\n- Professionally formatted with clear section headers.\n\nREQUIRED SECTIONS (headers visible; equivalent wording allowed):\n1) Executive Summary or Overview\n2) Method Implementations (must list implemented methods: Binomial/Tree, Finite Difference, Monte Carlo with early-exercise handling like Longstaff\u2013Schwartz)\n3) Experimental Protocol (parameters, instruments, grids/paths/steps, hardware; at least 3 sentences)\n4) Results and Comparisons (tables and figures for pricing accuracy and convergence)\n5) Performance Benchmarks (runtime comparisons and discussion)\n6) Recommendations (production recommendation and rationale)\n7) Artifact Manifest (must list actual filenames of: a .ipynb notebook, at least one benchmarks file (CSV or XLSX), and at least 3 figure images)\n\nREQUIRED TABLES/FIGURES IN THE REPORT:\n- At least one convergence plot.\n- At least one pricing comparison figure or table.\n- At least one runtime benchmark chart.\n\nSCORING (return a numeric score between 0.0 and 4.0):\n- 4.0: PDF/DOCX with all 7 sections present, includes the three required plot types, and the Artifact Manifest lists a .ipynb file, a benchmarks CSV/XLSX, and \u22653 figure filenames.\n- 3.2: Missing exactly one required section OR exactly one required plot type, but otherwise compliant including manifest items.\n- 2.0: Valid PDF/DOCX but missing 2\u20133 required sections/plot types or lacks a usable manifest linking to artifacts.\n- 0.5: Valid PDF/DOCX but minimal structure (\u22642 sections) and no clear manifest.\n- 0.0: Not PDF/DOCX, or clearly fails format (e.g., <3 pages).\n\nOnly check presence and structure. Do not verify correctness of methods or results. Return just the numeric score.", "expectation": "A 3+ page PDF/DOCX report that includes all required sections and an Artifact Manifest listing the notebook (.ipynb), benchmarks data (CSV/XLSX), and at least three figure files (e.g., convergence, pricing comparison, runtime)."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification and Consistency Checks (Code + Light LLM if needed)", "description": "Deterministic code checks leveraging the enforced shape. Validate presence of methods in the notebook, benchmark data structure, fundamental financial bounds, expected figures, and manifest cross-references.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Notebook Methods Implemented (Binomial, FDM, MC-LSM)", "description": "Detect a .ipynb notebook among outputs and check code cells for presence of three core methods: Binomial/Tree with early exercise handling, Finite Difference (e.g., Implicit/Crank\u2013Nicolson), and Monte Carlo Longstaff\u2013Schwartz or equivalent. Partial credit per method found.", "weight": 1.2, "code": "import json, re\n\ndef evaluate(workflow, context):\n    weight = 1.2\n    # Find .ipynb in all outputs\n    ipynb_paths = []\n    for r in context.get_all_outputs():\n        try:\n            p = context.files.get_path(r.id)\n            if p.suffix.lower() == '.ipynb':\n                ipynb_paths.append(p)\n        except Exception:\n            continue\n    if not ipynb_paths:\n        return 0.0, 'No .ipynb notebook found among outputs.'\n\n    # Read first notebook and concatenate code cells\n    nb_path = ipynb_paths[0]\n    try:\n        with open(nb_path, 'r', encoding='utf-8') as f:\n            nb = json.load(f)\n    except Exception as e:\n        return 0.0, f'Failed to read notebook: {e}'\n\n    code = []\n    for cell in nb.get('cells', []):\n        if cell.get('cell_type') == 'code':\n            src = cell.get('source', [])\n            if isinstance(src, list):\n                code.append(''.join(src))\n            else:\n                code.append(str(src))\n    code_text = ('\\n'.join(code)).lower()\n\n    found = []\n    # Binomial/Tree with early exercise/backward induction\n    has_tree = any(k in code_text for k in ['binomial', 'trinomial', 'lattice'])\n    early_keywords = ['early exercise', 'early_exercise', 'american', 'snell', 'backward induction', 'backward_induction']\n    has_early = any(k in code_text for k in early_keywords)\n    if has_tree and has_early:\n        found.append('tree')\n\n    # Finite Difference (implicit / Crank\u2013Nicolson / tridiagonal solve)\n    fdm_keywords = ['finite difference', 'crank', 'crank-nicolson', 'implicit scheme', 'implicit method', 'theta method', 'tridiagonal', 'thomas algorithm']\n    has_fdm = any(k in code_text for k in fdm_keywords)\n    if has_fdm:\n        found.append('fdm')\n\n    # Monte Carlo with Longstaff\u2013Schwartz or equivalent regression continuation\n    mc_keywords = ['longstaff', 'lsm', 'least squares', 'least-squares', 'continuation value', 'basis functions', 'polynomial basis']\n    has_mc_lsm = ('monte carlo' in code_text or 'monte-carlo' in code_text or 'mc' in code_text) and any(k in code_text for k in mc_keywords)\n    if has_mc_lsm:\n        found.append('mc_lsm')\n\n    methods_found = len(set(found))\n    score = min(methods_found, 3) / 3.0 * weight\n    feedback = f\"Methods detected: {', '.join(found) if found else 'none'}\"\n    return score, feedback"}, {"type": "code", "name": "Benchmarks Data Presence and Columns", "description": "Locate a benchmarks CSV/XLSX and validate key columns exist with a reasonable number of rows. Flexible matching of column names.", "weight": 1.0, "code": "import pandas as pd\nimport re\n\n\ndef evaluate(workflow, context):\n    weight = 1.0\n\n    # Helper: choose likely benchmarks file\n    bench_res = None\n    bench_path = None\n    for r in context.get_all_outputs():\n        try:\n            p = context.files.get_path(r.id)\n            name = p.name.lower()\n            if p.suffix.lower() in ['.xlsx', '.csv'] and any(k in name for k in ['bench', 'result', 'compare', 'metric', 'performance', 'pricing']):\n                bench_res = r\n                bench_path = p\n                break\n        except Exception:\n            continue\n    # Fallback: first spreadsheet/csv\n    if bench_res is None:\n        for r in context.get_all_outputs():\n            try:\n                p = context.files.get_path(r.id)\n                if p.suffix.lower() in ['.xlsx', '.csv']:\n                    bench_res = r\n                    bench_path = p\n                    break\n            except Exception:\n                continue\n\n    if bench_res is None:\n        return 0.0, 'No benchmarks CSV/XLSX found.'\n\n    # Read\n    try:\n        if bench_path.suffix.lower() == '.xlsx':\n            df = context.files.read_excel(bench_res.id)\n        else:\n            df = context.files.read_csv(bench_res.id)\n    except Exception as e:\n        return 0.0, f'Failed to read benchmarks file: {e}'\n\n    if df is None or len(df) == 0:\n        return 0.0, 'Benchmarks file empty.'\n\n    # Normalize columns\n    cols = [str(c).strip().lower().replace(' ', '_') for c in df.columns]\n    colset = set(cols)\n\n    # Synonym sets\n    req_groups = {\n        'method': {'method', 'algorithm', 'model'},\n        'price': {'price', 'value', 'premium'},\n        'runtime': {'runtime', 'run_time', 'time_ms', 'milliseconds', 'ms', 'time', 'elapsed', 'seconds', 'secs', 'time_s'},\n        'steps': {'steps', 'nsteps', 'time_steps', 'timesteps', 'tree_steps', 'grid_points', 'm', 'n'},\n        'paths': {'paths', 'npaths', 'mc_paths', 'simulations'},\n        'spot': {'spot', 's', 's0', 'underlying', 'spot_price'},\n        'strike': {'strike', 'k', 'strike_price'},\n        'rate': {'rate', 'r', 'risk_free', 'risk_free_rate'},\n        'vol': {'vol', 'volatility', 'sigma'},\n        'maturity': {'maturity', 't', 'tenor', 'expiry', 'expiration'},\n        'option_type': {'option_type', 'type', 'opt_type'}\n    }\n\n    found_keys = set()\n    for key, synonyms in req_groups.items():\n        if any(s in colset for s in synonyms):\n            found_keys.add(key)\n\n    # Require a core subset\n    core = {'method', 'price', 'runtime', 'spot', 'strike', 'rate', 'vol', 'maturity'}\n    core_found = core.intersection(found_keys)\n    core_ratio = len(core_found) / len(core)\n\n    rows_ok = 1.0 if len(df) >= 5 else (len(df) / 5.0)\n\n    # Score combines core coverage and overall coverage, weighted with rows count\n    overall_ratio = len(found_keys) / max(len(req_groups), 1)\n    base_score = 0.6 * core_ratio + 0.4 * overall_ratio\n    score = weight * max(0.0, min(1.0, base_score * rows_ok))\n\n    fb = f\"Cols found: {sorted(list(found_keys))}; rows={len(df)}\"\n    return score, fb"}, {"type": "code", "name": "Intrinsic Value Bounds Check", "description": "Using benchmarks data (if available), verify American option prices are not below intrinsic value for calls/puts. Flexible column matching; partial credit by pass ratio.", "weight": 0.6, "code": "import pandas as pd\n\n\ndef _find_bench_df(context):\n    # Try to reuse logic: first spreadsheet or csv\n    candidates = []\n    for r in context.get_all_outputs():\n        try:\n            p = context.files.get_path(r.id)\n            if p.suffix.lower() in ['.xlsx', '.csv']:\n                candidates.append((r, p))\n        except Exception:\n            continue\n    for r, p in candidates:\n        try:\n            if p.suffix.lower() == '.xlsx':\n                df = context.files.read_excel(r.id)\n            else:\n                df = context.files.read_csv(r.id)\n            if df is not None and len(df) > 0:\n                return df\n        except Exception:\n            continue\n    return None\n\n\ndef evaluate(workflow, context):\n    weight = 0.6\n    df = _find_bench_df(context)\n    if df is None or len(df) == 0:\n        return 0.0, 'No benchmarks data available for bounds check.'\n\n    cols = {str(c).strip().lower().replace(' ', '_'): c for c in df.columns}\n\n    def colname(options):\n        for o in options:\n            if o in cols:\n                return cols[o]\n        return None\n\n    c_price = colname(['price', 'value', 'premium'])\n    c_spot = colname(['spot', 's', 's0', 'underlying', 'spot_price'])\n    c_strike = colname(['strike', 'k', 'strike_price'])\n    c_type = colname(['option_type', 'type', 'opt_type'])\n\n    if not all([c_price, c_spot, c_strike, c_type]):\n        return 0.0, 'Required columns for bounds check not found.'\n\n    sub = df[[c_price, c_spot, c_strike, c_type]].dropna()\n    if len(sub) == 0:\n        return 0.0, 'No usable rows for bounds check.'\n\n    def is_call(v):\n        v = str(v).strip().lower()\n        return v in ['c', 'call', 'calls']\n\n    def intrinsic(row):\n        S = float(row[c_spot])\n        K = float(row[c_strike])\n        if is_call(row[c_type]):\n            return max(0.0, S - K)\n        else:\n            return max(0.0, K - S)\n\n    passes = 0\n    total = 0\n    for _, row in sub.iterrows():\n        try:\n            iv = intrinsic(row)\n            price = float(row[c_price])\n            if price + 1e-8 >= iv:  # allow tiny numerical tolerance\n                passes += 1\n            total += 1\n        except Exception:\n            continue\n\n    if total == 0:\n        return 0.0, 'No valid rows computed.'\n\n    ratio = passes / total\n    score = weight * ratio\n    return score, f'Bounds pass ratio: {ratio:.2%} ({passes}/{total})'"}, {"type": "code", "name": "Figures Presence (Convergence/Runtime/Comparison)", "description": "Verify that at least two figures exist among outputs with names indicating convergence, runtime, or comparison/accuracy.", "weight": 0.6, "code": "def evaluate(workflow, context):\n    weight = 0.6\n    count = 0\n    names = []\n    for r in context.get_all_outputs():\n        try:\n            p = context.files.get_path(r.id)\n            if p.suffix.lower() in ['.png', '.jpg', '.jpeg']:\n                name = p.name.lower()\n                if any(k in name for k in ['convergence', 'runtime', 'bench', 'benchmark', 'compare', 'comparison', 'accuracy']):\n                    count += 1\n                    names.append(p.name)\n        except Exception:\n            continue\n\n    # Score up to 3 figures => full credit\n    score = weight * min(count, 3) / 3.0\n    if count == 0:\n        return score, 'No matching figures found.'\n    return score, f'Figures found ({count}): ' + ', '.join(names)"}, {"type": "code", "name": "Report \u2194 Artifact Manifest Cross-Reference", "description": "Check that the primary report text mentions the actual notebook filename and at least one benchmarks file name.", "weight": 0.6, "code": "def evaluate(workflow, context):\n    weight = 0.6\n    primary = context.get_primary_output()\n    if not primary or not primary.is_document:\n        return 0.0, 'Primary output is not a document.'\n\n    # Read report text\n    text = ''\n    try:\n        # Try PDF first\n        text = context.files.read_pdf_text(primary.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(primary.id)\n        except Exception:\n            text = ''\n    if not text:\n        return 0.0, 'Unable to read report text.'\n    t = text.lower()\n\n    # Find notebook and benchmarks among outputs\n    nb_name = None\n    bench_name = None\n    for r in context.get_all_outputs():\n        try:\n            p = context.files.get_path(r.id)\n            sfx = p.suffix.lower()\n            if sfx == '.ipynb' and nb_name is None:\n                nb_name = p.name.lower()\n            if sfx in ['.xlsx', '.csv'] and bench_name is None:\n                bench_name = p.name.lower()\n        except Exception:\n            continue\n\n    hits = 0\n    details = []\n    if nb_name and nb_name in t:\n        hits += 1\n        details.append(f'Notebook referenced: {nb_name}')\n    elif nb_name:\n        details.append(f'Notebook NOT referenced: {nb_name}')\n\n    if bench_name and bench_name in t:\n        hits += 1\n        details.append(f'Benchmarks referenced: {bench_name}')\n    elif bench_name:\n        details.append(f'Benchmarks NOT referenced: {bench_name}')\n\n    score = weight * (hits / 2.0)\n    return score, '; '.join(details) if details else 'No artifacts detected for cross-reference.'"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Strategic Value (LLM)", "description": "Holistic assessment of clarity, methodological insight, and actionable recommendations for high-performance trading deployment.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Analytical Depth and Comparative Insight", "description": "Evaluate the clarity and depth of the methodological comparison and the evidence provided (convergence, accuracy vs. runtime).", "weight": 1.0, "judge_prompt": "Assess the report for analytical depth and comparative insight about American option pricing methods. Consider:\n- Clear explanation of Binomial/Tree (with early exercise), Finite Difference scheme specifics (e.g., implicit/Crank\u2013Nicolson, stability), and Monte Carlo with Longstaff\u2013Schwartz.\n- Evidence-backed comparisons: convergence behavior, pricing accuracy versus a reference (e.g., high-resolution method), and runtime trade-offs.\n- Discussion of limitations (bias, variance, grid/step sensitivity, boundary conditions, regression basis risk).\n\nScoring (0.0\u20131.0):\n- 1.0: Thorough, technically sound comparison with multiple figures/tables, clear convergence and runtime trade-offs, and explicit limitations.\n- 0.7: Generally good coverage with some evidence, minor gaps in technical rigor or limitations.\n- 0.4: Superficial comparison; mentions methods but limited evidence or unclear trade-offs.\n- 0.0: Lacks meaningful analysis of methods.\nReturn just the numeric score.", "expectation": "A rigorous, well-evidenced comparison of Tree, FDM, and MC-LSM with explicit convergence and runtime insights."}, {"type": "llm_judge", "name": "Actionable Production Recommendation", "description": "Evaluate how actionable and context-appropriate the recommendations are for a high-performance trading environment.", "weight": 1.0, "judge_prompt": "Evaluate the recommendations for production use in a high-performance trading setting. Consider:\n- Specificity of the chosen method(s) for American options across regimes (e.g., dividends, moneyness, maturities).\n- Practical parameter guidance (steps/grid sizing, early-exercise handling, basis functions, variance reduction) and expected error/runtime.\n- Implementation details for speed and reliability (vectorization, parallelism/GPU, JIT, caching/calibration, fallback methods, monitoring).\n- Risk control and model governance (sanity checks, limits, regression tests, reproducibility seeds).\n\nScoring (0.0\u20131.0):\n- 1.0: Clear, specific, and actionable plan tailored to HFT/production with parameters, performance expectations, and safeguards.\n- 0.7: Good but somewhat generic; some actionable elements, missing a few deployment details.\n- 0.4: Vague or academic; limited operational guidance.\n- 0.0: No real recommendations.\nReturn just the numeric score.", "expectation": "A concrete, deployable recommendation with parameters and safeguards suitable for a trading desk."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "7ed932dd-244f-4d61-bf02-1bc3bab1af14", "rubric": {"category_name": "Wholesale Trade \u2014 Inventory Coverage Plan (Alcoholic Beverages)", "rationale": "This rubric enforces a self-documenting, verifiable Excel model that projects July coverage, identifies out-of-stock dates, and produces a shipment recommendation by SKU. Stage 1 (LLM gate) mandates an exact workbook structure (sheets, sections, tables, and required columns) with highlight conventions to make verification trivial. Stage 2 uses code rules to test arithmetic consistency, pallet conversion and rounding integrity, and coherence between the model and the recommendation sheet, plus an LLM check for visual highlighting the code cannot read. Stage 3 assesses professional presentation and stakeholder usability.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate", "description": "LLM-only gate ensuring the candidate produced an Excel workbook with the exact verifiable structure required for deterministic checks in Stage 2.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Output Format: Inventory Coverage Model and Recommended Shipments", "description": "Verify the output is a single Excel workbook with the precise structure below. Only check PRESENCE/STRUCTURE, not correctness of calculations.", "weight": 4.0, "judge_prompt": "You are checking ONLY the structure and formatting of the submitted file, not calculation correctness.\n\nFile/type requirements:\n- The primary output must be an Excel workbook (.xlsx). Not CSV, not PDF/DOCX.\n\nWorkbook structure (required):\n1) Sheet: \"Inventory Model\" (flexible naming accepted: e.g., Stock Projection, Coverage Model, Inventory Coverage) with TWO visible parts:\n   A. Inputs section (a clearly labeled block) that includes ALL of the following (flexible labels OK):\n      - Report/As-Of Date\n      - Period End Date (end of July)\n      - A note of units (cases) and a visible pallet conversion assumption (either a global note or per-SKU/cases-per-pallet column)\n   B. SKU Projections table with CLEAR column headers containing ALL of the following (flexible names accepted, but the meaning must be clear):\n      - SKU (or Item Code)\n      - Current Inventory (cases)\n      - Daily Rate of Sale (cases/day)\n      - Days of Inventory (Current)\n      - Projected Out-of-Stock Date (Current)\n      - Scheduled Inbound Cases through the Period End\n      - Next Scheduled Delivery Date (from current schedule)\n      - Delivered Days of Inventory (Post Scheduled)\n      - Additional Cases Needed (to cover through end of July)\n      - Cases per Pallet (either here OR a separate Pallet Conversion sheet \u2014 one of these must exist visibly)\n      - Additional Pallets Needed (rounded up)\n      - Required Delivery Date (latest delivery date to prevent stockout)\n      - Flags/Indicators for: Rounded Up and Earlier than Schedule (can be boolean columns or clearly labeled indicators)\n\n2) Sheet: \"Recommended Shipments\" (flexible naming accepted: e.g., Recommendation, Orders, Proposed Shipments) with a clean table that includes ALL of the following columns:\n   - SKU\n   - Pallets Required (rounded up)\n   - Required Delivery Date\n   And formatting requirements:\n   - Rows requiring earlier delivery than the current schedule are visually highlighted (e.g., fill color or clear visual indicator on those rows)\n   - Rows where pallets were rounded up are visually highlighted (may be the same or a different highlight style; must be clearly indicated)\n\n3) Pallet Conversion reference (choose ONE of the following is required):\n   - A separate sheet (e.g., \"Pallet Conversion\") with columns: [SKU | Cases per Pallet], OR\n   - A visible \"Cases per Pallet\" column in the SKU Projections table on the Inventory Model sheet.\n\nScoring (structure only):\n- 4.0: Valid Excel + both sheets present with all required columns/sections + required highlight indicators + pallet conversion visible (sheet or per-SKU column)\n- 3.0: Valid Excel + both sheets present and structurally complete but one supporting element is missing or unclear (e.g., Next Scheduled Delivery Date, or flags not explicitly labeled, or highlight applied but not clearly distinguishable)\n- 2.0: Valid Excel but missing one major element (e.g., the Recommended Shipments sheet, or the Inputs block on Inventory Model, or the pallet conversion reference)\n- 1.0: Valid Excel but table structures are too incomplete or unlabeled to enable verification\n- 0.0: Not an Excel file or structure does not match the requirements\n\nBe flexible with exact header text, but strict about the presence/visibility of the required sections and columns. Do NOT judge math correctness here.", "expectation": "A two-sheet model with Inputs + SKU Projections and a Recommended Shipments table, plus pallet conversion visible and clear highlight conventions for rounded pallets and earlier-than-schedule deliveries."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness Verification", "description": "Deterministic checks of internal consistency, math, rounding, and cross-sheet coherence. Includes one small LLM rule for visual highlighting that code cannot inspect.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Parsable Structure and Column Detection", "description": "Confirm the workbook is a spreadsheet and the key sheets/columns can be located with flexible matching.", "weight": 0.8, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output found.\"\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheet_names = [s for s in xls.sheet_names]\n        # Heuristics to find sheets\n        def score_name(name, keys):\n            n = name.lower()\n            return sum(1 for k in keys if k in n)\n        model_sheet = None\n        rec_sheet = None\n        conv_sheet = None\n        for s in sheet_names:\n            if score_name(s, [\"model\",\"projection\",\"coverage\",\"inventory\"]) >= 1:\n                model_sheet = s if model_sheet is None else model_sheet\n            if score_name(s, [\"recommend\",\"shipment\",\"order\",\"proposed\"]) >= 1:\n                rec_sheet = s if rec_sheet is None else rec_sheet\n            if score_name(s, [\"pallet\",\"conversion\",\"convert\"]) >= 1:\n                conv_sheet = s if conv_sheet is None else conv_sheet\n        # If heuristic misses, fall back to first sheet as model\n        if model_sheet is None and len(sheet_names) > 0:\n            model_sheet = sheet_names[0]\n        # Read candidate sheets\n        dfm = pd.read_excel(path, sheet_name=model_sheet)\n        # Column mapping (flexible)\n        cols = [str(c) for c in dfm.columns]\n        ncols = [c.lower().strip() for c in cols]\n        def find_col(cands):\n            for i, c in enumerate(ncols):\n                for cand in cands:\n                    if cand in c:\n                        return cols[i]\n            return None\n        mapping = {}\n        mapping['sku'] = find_col([\"sku\",\"item code\",\"product code\",\"item\",\"material\"])    \n        mapping['inv'] = find_col([\"current inventory\",\"on hand\",\"inventory\",\"cases on hand\",\"boh\",\"soh\"]) \n        mapping['ros'] = find_col([\"rate of sale\",\"ros\",\"daily rate\",\"cases/day\",\"avg daily\",\"sell-thru\",\"sell thru\",\"sell through\"]) \n        mapping['days_cur'] = find_col([\"days of inventory\",\"days on hand\",\"doh\",\"days of supply\",\"current days\"]) \n        mapping['oos'] = find_col([\"out-of-stock\",\"oos\",\"stockout\",\"runout\"]) \n        mapping['inbound'] = find_col([\"scheduled inbound\",\"incoming\",\"inbound\",\"receipts\",\"arrivals\"]) \n        mapping['next_sched'] = find_col([\"next scheduled\",\"earliest scheduled\",\"next arrival\"]) \n        mapping['days_deliv'] = find_col([\"delivered days\",\"post scheduled\",\"after arrival\",\"post-arrival\"]) \n        mapping['add_cases'] = find_col([\"additional cases\",\"cases needed\",\"shortfall cases\",\"delta cases\"]) \n        mapping['cpp'] = find_col([\"cases per pallet\",\"cs/pallet\",\"pallet size\",\"conversion\"]) \n        mapping['add_pallets'] = find_col([\"additional pallets\",\"pallets required\",\"pallets needed\",\"to ship\"]) \n        mapping['req_date'] = find_col([\"required delivery\",\"deliver by\",\"need by\",\"required date\"]) \n        # Check recommended sheet presence\n        has_rec = rec_sheet is not None\n        # Check for conversion via separate sheet if not in model\n        has_conv = mapping['cpp'] is not None or (conv_sheet is not None)\n        required_keys = ['sku','inv','ros','days_cur','oos','inbound','days_deliv','add_cases','add_pallets','req_date']\n        found = sum(1 for k in required_keys if mapping.get(k) is not None)\n        # Score assembly\n        parts = 0\n        score = 0\n        # Columns presence\n        parts += 1\n        score += (found/len(required_keys)) if len(required_keys)>0 else 0\n        # Conversion availability\n        parts += 1\n        score += 1.0 if has_conv else 0.0\n        # Recommended sheet presence\n        parts += 1\n        score += 1.0 if has_rec else 0.0\n        # Normalize to 0..1\n        final = max(0.0, min(1.0, score/parts))\n        return final, f\"Found {found}/{len(required_keys)} key columns; rec_sheet={has_rec}; conversion_present={has_conv}.\"\n    except Exception as e:\n        return 0.0, f\"Error parsing workbook: {e}\""}, {"type": "code", "name": "Arithmetic Consistency: Days of Inventory and Delivered Days", "description": "Check that Days of Inventory \u2248 Current Inventory / Rate of Sale and Delivered Days \u2248 (Current + Inbound) / Rate of Sale within reasonable tolerance.", "weight": 1.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        # Find likely model sheet\n        def pick_model(sheet_names):\n            for s in sheet_names:\n                n=s.lower()\n                if any(k in n for k in [\"model\",\"projection\",\"coverage\",\"inventory\"]):\n                    return s\n            return sheet_names[0] if sheet_names else None\n        model_sheet = pick_model(xls.sheet_names)\n        if not model_sheet:\n            return 0.0, \"No sheets found.\"\n        df = pd.read_excel(path, sheet_name=model_sheet)\n        cols = [str(c) for c in df.columns]\n        ncols = [c.lower().strip() for c in cols]\n        def find_col(cands):\n            for i,c in enumerate(ncols):\n                for cand in cands:\n                    if cand in c:\n                        return cols[i]\n            return None\n        c_inv = find_col([\"current inventory\",\"on hand\",\"inventory\",\"cases on hand\",\"boh\",\"soh\"]) \n        c_ros = find_col([\"rate of sale\",\"ros\",\"daily rate\",\"cases/day\",\"avg daily\",\"sell-thru\",\"sell thru\",\"sell through\"]) \n        c_days = find_col([\"days of inventory\",\"days on hand\",\"doh\",\"days of supply\",\"current days\"]) \n        c_inb = find_col([\"scheduled inbound\",\"incoming\",\"inbound\",\"receipts\",\"arrivals\"]) \n        c_days_del = find_col([\"delivered days\",\"post scheduled\",\"after arrival\",\"post-arrival\"]) \n        if not (c_inv and c_ros and c_days):\n            return 0.0, \"Missing core columns for days check.\"\n        inv = pd.to_numeric(df[c_inv], errors='coerce')\n        ros = pd.to_numeric(df[c_ros], errors='coerce')\n        days = pd.to_numeric(df[c_days], errors='coerce')\n        inb = pd.to_numeric(df[c_inb], errors='coerce') if c_inb else pd.Series([np.nan]*len(df))\n        days_del = pd.to_numeric(df[c_days_del], errors='coerce') if c_days_del else pd.Series([np.nan]*len(df))\n        # Compute expected\n        with np.errstate(divide='ignore', invalid='ignore'):\n            exp_days = inv/ros\n            exp_days_del = (inv.fillna(0)+inb.fillna(0))/ros\n        # Tolerance settings\n        def ok(a,b):\n            if pd.isna(a) or pd.isna(b):\n                return False\n            if b == 0:\n                return abs(a-b) <= 0.5\n            return abs(a-b) <= max(0.5, 0.15*abs(b))\n        mask_valid = (~inv.isna()) & (~ros.isna()) & (ros>0) & (~days.isna())\n        n1 = int(mask_valid.sum())\n        m1 = 0\n        if n1>0:\n            m1 = int(np.sum([ok(days.iloc[i], exp_days.iloc[i]) for i in np.where(mask_valid)[0]]))\n        frac1 = m1/n1 if n1>0 else 0.0\n        frac2 = 0.0\n        if c_inb and c_days_del:\n            mask2 = (~inv.isna()) & (~ros.isna()) & (ros>0) & (~inb.isna()) & (~days_del.isna())\n            n2 = int(mask2.sum())\n            if n2>0:\n                m2 = int(np.sum([ok(days_del.iloc[i], exp_days_del.iloc[i]) for i in np.where(mask2)[0]]))\n                frac2 = m2/n2\n        # Weighted average: current days (60%), delivered days (40% if available)\n        if c_inb and c_days_del:\n            score = 0.6*frac1 + 0.4*frac2\n        else:\n            score = frac1\n        return max(0.0, min(1.0, score)), f\"Days match: current={frac1:.2f}, delivered={frac2:.2f}.\"\n    except Exception as e:\n        return 0.0, f\"Error in arithmetic consistency: {e}\""}, {"type": "code", "name": "Pallet Conversion and Rounding Integrity", "description": "Verify Additional Pallets equals CEILING(Additional Cases / Cases per Pallet), non-negative and integer.", "weight": 1.2, "code": "import re\nimport pandas as pd\nimport numpy as np\nimport math\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet.\"\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        # Identify likely sheets\n        def pick_model(sheet_names):\n            for s in sheet_names:\n                n=s.lower()\n                if any(k in n for k in [\"model\",\"projection\",\"coverage\",\"inventory\"]):\n                    return s\n            return sheet_names[0] if sheet_names else None\n        model_sheet = pick_model(xls.sheet_names)\n        df = pd.read_excel(path, sheet_name=model_sheet)\n        cols = [str(c) for c in df.columns]\n        ncols = [c.lower().strip() for c in cols]\n        def find_col(cands):\n            for i,c in enumerate(ncols):\n                for cand in cands:\n                    if cand in c:\n                        return cols[i]\n            return None\n        c_sku = find_col([\"sku\",\"item code\",\"product code\",\"item\",\"material\"])    \n        c_add_cases = find_col([\"additional cases\",\"cases needed\",\"shortfall cases\",\"delta cases\"]) \n        c_cpp = find_col([\"cases per pallet\",\"cs/pallet\",\"pallet size\",\"conversion\"]) \n        c_add_pallets = find_col([\"additional pallets\",\"pallets required\",\"pallets needed\",\"to ship\"]) \n        if not (c_sku and c_add_cases and c_add_pallets):\n            return 0.0, \"Missing SKU/additional columns.\"\n        # Try to get conversion from separate sheet if not in model\n        conv_map = None\n        if c_cpp is None:\n            conv_sheet = None\n            for s in xls.sheet_names:\n                n=s.lower()\n                if any(k in n for k in [\"pallet\",\"conversion\",\"convert\"]):\n                    conv_sheet = s\n                    break\n            if conv_sheet is not None:\n                dconv = pd.read_excel(path, sheet_name=conv_sheet)\n                c_sku2 = None\n                c_cpp2 = None\n                if len(dconv.columns)>=2:\n                    cols2 = [str(c) for c in dconv.columns]\n                    n2 = [c.lower().strip() for c in cols2]\n                    for i,c in enumerate(n2):\n                        if any(k in c for k in [\"sku\",\"item code\",\"product code\",\"item\",\"material\"]):\n                            c_sku2 = cols2[i]\n                        if any(k in c for k in [\"cases per pallet\",\"cs/pallet\",\"pallet size\",\"conversion\"]):\n                            c_cpp2 = cols2[i]\n                if c_sku2 and c_cpp2:\n                    conv_map = dconv[[c_sku2, c_cpp2]].dropna()\n                    conv_map[c_cpp2] = pd.to_numeric(conv_map[c_cpp2], errors='coerce')\n                    conv_map = conv_map.dropna()\n                    conv_map = dict(zip(conv_map[c_sku2].astype(str).str.strip(), conv_map[c_cpp2]))\n        # Prepare series\n        add_cases = pd.to_numeric(df[c_add_cases], errors='coerce').fillna(0)\n        add_pallets = pd.to_numeric(df[c_add_pallets], errors='coerce')\n        if c_cpp is not None:\n            cpp = pd.to_numeric(df[c_cpp], errors='coerce')\n        else:\n            # Map by SKU\n            if conv_map is not None and c_sku is not None:\n                sku_series = df[c_sku].astype(str).str.strip()\n                cpp = sku_series.map(conv_map)\n            else:\n                # No conversion available\n                return 0.2, \"No pallet conversion found; minimal score given.\"\n        # Compute expected pallets\n        valid = (~add_cases.isna()) & (~cpp.isna()) & (cpp>0) & (~add_pallets.isna())\n        n = int(valid.sum())\n        if n == 0:\n            return 0.0, \"No valid rows to check rounding.\"\n        exp = np.ceil(add_cases[valid]/cpp[valid])\n        given = add_pallets[valid]\n        # Check integer and non-negative\n        is_int = (abs(given - np.round(given)) < 1e-6)\n        nonneg = (given >= 0)\n        match = (abs(given - exp) < 1e-6)\n        frac_int = float(np.mean(is_int))\n        frac_nonneg = float(np.mean(nonneg))\n        frac_match = float(np.mean(match))\n        score = 0.4*frac_match + 0.3*frac_int + 0.3*frac_nonneg\n        return max(0.0, min(1.0, score)), f\"match={frac_match:.2f}, int={frac_int:.2f}, nonneg={frac_nonneg:.2f} on {n} rows.\"\n    except Exception as e:\n        return 0.0, f\"Error in pallet rounding check: {e}\""}, {"type": "code", "name": "Required Delivery Date and Recommendation Consistency", "description": "Check (a) if Additional Pallets > 0 then Required Delivery Date is on/before Projected OOS; and (b) Recommended Shipments sheet agrees with the model for SKU, pallets, and date.", "weight": 0.6, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet.\"\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        # Pick sheets\n        def pick_model(sheet_names):\n            for s in sheet_names:\n                n=s.lower()\n                if any(k in n for k in [\"model\",\"projection\",\"coverage\",\"inventory\"]):\n                    return s\n            return sheet_names[0] if sheet_names else None\n        model_sheet = pick_model(xls.sheet_names)\n        rec_sheet = None\n        for s in xls.sheet_names:\n            if any(k in s.lower() for k in [\"recommend\",\"shipment\",\"order\",\"proposed\"]):\n                rec_sheet = s\n                break\n        dfm = pd.read_excel(path, sheet_name=model_sheet)\n        cols = [str(c) for c in dfm.columns]\n        ncols = [c.lower().strip() for c in cols]\n        def find_col(cands):\n            for i,c in enumerate(ncols):\n                for cand in cands:\n                    if cand in c:\n                        return cols[i]\n            return None\n        c_sku = find_col([\"sku\",\"item code\",\"product code\",\"item\",\"material\"])    \n        c_add_pallets = find_col([\"additional pallets\",\"pallets required\",\"pallets needed\",\"to ship\"]) \n        c_req_date = find_col([\"required delivery\",\"deliver by\",\"need by\",\"required date\"]) \n        c_oos = find_col([\"out-of-stock\",\"oos\",\"stockout\",\"runout\"]) \n        # Part A: Required <= OOS when pallets > 0\n        if not (c_sku and c_add_pallets and c_req_date and c_oos):\n            partA = 0.0\n        else:\n            m = dfm\n            addp = pd.to_numeric(m[c_add_pallets], errors='coerce')\n            reqd = pd.to_datetime(m[c_req_date], errors='coerce')\n            oosd = pd.to_datetime(m[c_oos], errors='coerce')\n            mask = (addp>0) & (~reqd.isna()) & (~oosd.isna())\n            n = int(mask.sum())\n            ok = 0\n            if n>0:\n                ok = int(((reqd[mask] <= oosd[mask]).astype(int)).sum())\n            partA = (ok/n) if n>0 else 0.0\n        # Part B: Recommended Shipments matches model rows needing pallets\n        partB = 0.0\n        if rec_sheet is not None and c_sku and c_add_pallets and c_req_date:\n            dfr = pd.read_excel(path, sheet_name=rec_sheet)\n            # Map rec cols\n            rcols = [str(c) for c in dfr.columns]\n            rn = [c.lower().strip() for c in rcols]\n            def rfind(cands):\n                for i,c in enumerate(rn):\n                    for cand in cands:\n                        if cand in c:\n                            return rcols[i]\n                return None\n            r_sku = rfind([\"sku\",\"item code\",\"product code\",\"item\",\"material\"]) \n            r_pal = rfind([\"pallets\",\"pallets required\",\"pallets needed\"]) \n            r_date = rfind([\"required delivery\",\"deliver by\",\"need by\",\"required date\"]) \n            if r_sku and r_pal and r_date:\n                m_tbl = dfm[[c_sku, c_add_pallets, c_req_date]].copy()\n                m_tbl[c_add_pallets] = pd.to_numeric(m_tbl[c_add_pallets], errors='coerce')\n                m_tbl[c_req_date] = pd.to_datetime(m_tbl[c_req_date], errors='coerce')\n                need = m_tbl[(m_tbl[c_add_pallets] > 0) & (~m_tbl[c_req_date].isna())].copy()\n                # normalize keys\n                need[c_sku] = need[c_sku].astype(str).str.strip()\n                dfr[r_sku] = dfr[r_sku].astype(str).str.strip()\n                dfr[r_pal] = pd.to_numeric(dfr[r_pal], errors='coerce')\n                dfr[r_date] = pd.to_datetime(dfr[r_date], errors='coerce')\n                if len(need) > 0:\n                    # Build lookup from rec\n                    rec_map = {(row[r_sku], row[r_date].date() if not pd.isna(row[r_date]) else None): row[r_pal] for _,row in dfr.iterrows()}\n                    hits = 0\n                    for _, row in need.iterrows():\n                        key = (row[c_sku], row[c_req_date].date())\n                        pal = rec_map.get(key, np.nan)\n                        if not pd.isna(pal) and abs(pal - row[c_add_pallets]) < 1e-6:\n                            hits += 1\n                    partB = hits/len(need)\n        # Combine: A (60%), B (40%)\n        score = 0.6*partA + 0.4*partB\n        return max(0.0, min(1.0, score)), f\"Req<=OOS={partA:.2f}, Rec-match={partB:.2f}.\"\n    except Exception as e:\n        return 0.0, f\"Error in delivery date/recommendation check: {e}\""}, {"type": "llm_judge", "name": "Visual Highlighting Presence and Clarity", "description": "The model requires rows to be highlighted for (a) rounded-up pallets and (b) earlier-than-schedule deliveries. Code cannot read cell formats; confirm visually that these highlight cues are present and clearly labeled/legended.", "weight": 0.4, "judge_prompt": "Inspect the Excel file visually. In the Recommended Shipments sheet (and/or the Inventory Model table):\n- Confirm that there is a clear visual highlight (e.g., fill color or icon) on rows where pallets were rounded up (e.g., a row with Pallets Required reflecting a ceiling/round-up). This can be indicated by a boolean column AND a visual highlight, or a legend explaining conditional formatting.\n- Confirm that rows requiring earlier delivery than the current schedule are visually highlighted and/or explicitly flagged, and that the legend/labels make this meaning clear.\n\nScoring:\n- 1.0: Both highlight types are clearly present and distinguishable (or very clearly labeled even if same style), with a legend or unambiguous headers.\n- 0.6: Both highlight types present but not well distinguished (e.g., unclear legend), or only one area (Model vs. Recommended sheet) shows the cues.\n- 0.3: Only one of the two highlight types is present, or highlighting is very faint/ambiguous.\n- 0.0: No visible highlighting/labels for either condition.\n\nJudge only the presence/clarity of highlights, not whether the underlying logic is correct.", "expectation": "Distinct and clearly explained highlights for rounded pallets and earlier-than-schedule deliveries."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism", "description": "Assess professionalism, clarity, and stakeholder usability for a distributor audience.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Usability", "description": "Evaluate formatting quality, readability, and usability for the distributor.", "weight": 1.0, "judge_prompt": "Evaluate the workbook for professional presentation and usability for a distributor:\n- Clear, consistent headers; units (cases, pallets) labeled; date formats consistent (MM/DD/YYYY or local standard)\n- Readability: freeze panes, filters enabled, reasonable column widths, no clipped text\n- Visual hierarchy: Inputs clearly separated; tables clean; legends for highlights; optional notes for assumptions\n- Minimal errors: no #DIV/0! or obvious placeholder text\n- Export-readiness: someone at the distributor could act on this without extra clarification\n\nScoring:\n- 1.0: Highly professional and immediately usable, with clear labels, formatting, and minor/no issues\n- 0.6: Generally professional with a few minor usability issues\n- 0.3: Usable but several formatting or clarity issues\n- 0.0: Poorly formatted or confusing", "expectation": "A clean, professional workbook with clear labels, consistent formatting, and ready for distributor use."}, {"type": "llm_judge", "name": "Decision Support and Reasonableness", "description": "Judge whether the model offers practical decision support and reasonable planning for July coverage.", "weight": 1.0, "judge_prompt": "Assess whether the model provides practical decision support:\n- Inputs show the coverage horizon (end of July) and as-of date\n- Projection logic appears reasonable, with coherent out-of-stock dates and delivered days of inventory\n- The Recommended Shipments sheet is actionable (SKUs, pallets, required dates) and aligns with the model\u2019s logic at a glance\n- Optional: brief notes/assumptions or buffer (e.g., safety days) \n\nScoring:\n- 1.0: Strong decision support; coherent and actionable\n- 0.6: Adequate but could use clearer assumptions or minor refinements\n- 0.3: Weak or hard to use for decisions\n- 0.0: Misaligned or not actionable", "expectation": "An actionable recommendation aligned with the model, showing coverage through end of July."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "c9bf9801-9640-45fa-8166-1ab01f2d98e4", "rubric": {"category_name": "DGHT Formal Mentorship Program Guide (Medical and Health Services Managers)", "rationale": "This rubric enforces a self-documenting, file-based deliverable: a structured DOCX guide with clearly labeled sections and linked templates. Stage 1 (LLM-only) is a strict shape gate that verifies the document format, section headers, appendix with documents and templates, visible links to separate Word files, and credits. Stage 2 uses code rules to verify cross-references, timeline coverage, presence of separate template files, and key program elements. Stage 3 applies an LLM quality assessment for clarity, inclusive tone, branding alignment, and professional polish.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate: DGHT Mentorship Guide Structure", "description": "LLM-only gate to ensure the primary output is a DOCX guide with the exact structural components needed for verification and use.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.2, "rules": [{"type": "llm_judge", "name": "Structured DOCX Guide with Required Sections and Linked Templates", "description": "Verify the candidate produced a Microsoft Word (DOCX) mentorship guide with all required sections, an appendix listing Documents and Templates, and visible links to separate DOCX files for Mentor/Mentee applications and the Mentorship Roadmap template.", "weight": 4.0, "judge_prompt": "You are evaluating Stage 1 (Shape Enforcement) for the DGHT Formal Mentorship Program Guide. Only assess presence/structure and file format, not content quality.\n\nCheck the PRIMARY OUTPUT (first output of the last task) for these requirements:\n\nFORMAT REQUIREMENTS\n- Primary output MUST be a Microsoft Word DOCX file (not PDF, not plain text).\n- Length: at least 6 pages.\n- Professional formatting: clear title/cover, consistent heading hierarchy, readable spacing, page numbers.\n- CDC/DGHT branding: presence of CDC and/or DGHT logo on cover or header/footer (flexible if a clearly branded mark is shown).\n\nREQUIRED STRUCTURE (headers or clear section labels; be flexible on exact phrasing):\n1) Introduction and Program Overview\n2) Program Purpose and Goals\n3) Definitions and Value of Mentoring\n4) Eligibility Criteria and Application Process\n5) Matching Procedures\n6) Roles and Responsibilities of Mentors and Mentees (both groups addressed)\n7) Key Program Requirements (e.g., mentorship roadmap, monthly meetings, IDPs)\n8) Training Components (skills-building sessions and networking events)\n9) Supporting Documentation Expectations (progress reports, evaluation)\n10) Detailed Program Timeline with monthly milestones and deliverables\n11) Appendix titled \"Documents and Templates\" that lists and clearly labels:\n    - Mentor application (DOCX) and Mentee application (DOCX), or a combined Mentor/Mentee application (DOCX)\n    - Mentorship roadmap template (DOCX)\n    - 4-month and 8-month evaluation forms (can be listed even if not separate files)\n12) Credits/Acknowledgments including explicit reference to NCIPC\u2019s Mentoring Program as inspiration.\n\nLINKING REQUIREMENTS\n- In the Appendix, the items Mentor/Mentee applications and Mentorship roadmap template should appear as visible links to separate DOCX files. You do not need to click links\u2014just confirm they look like hyperlinks and are labeled appropriately.\n\nSCORING (only structure/format, do not judge content quality):\n- 4.0: DOCX format; 6+ pages; branding present; all 12 structural elements present; Appendix shows visibly linked DOCX for applications and roadmap template; credits include NCIPC.\n- 3.2: DOCX format; 6+ pages; branding present; minor omissions (e.g., 1 missing non-core detail such as page numbers or a small labeling inconsistency) but core sections, Appendix, and visible links are present.\n- 2.4: DOCX format; multiple omissions (e.g., missing 1-2 required sections OR Appendix present but links not visibly shown). Still broadly resembles the requested structure.\n- 1.2: DOCX format but missing several core sections OR no Appendix/links; structure is clearly incomplete.\n- 0.0: Not a DOCX file OR largely missing required structure.\n\nImportant: Do not assess calculation correctness or writing quality here\u2014only the presence and shape of the required elements.", "expectation": "A DOCX guide with all required sections, a properly labeled Appendix that includes visibly linked DOCX templates (Mentor/Mentee applications and Mentorship roadmap), and a credits section acknowledging NCIPC."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification: Cross-References and Structural Correctness", "description": "Deterministic checks using code to validate presence of linked templates as separate files, timeline coverage, role coverage, required program elements, and branding/acknowledgment mentions.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Linked Templates Present as Separate Files", "description": "Verify that separate DOCX files exist among outputs for Mentor/Mentee applications (either two separate files or one combined file) and for the Mentorship Roadmap template.", "weight": 2.0, "code": "import re\nfrom pathlib import Path\n\ndef evaluate(workflow, context):\n    try:\n        primary = context.get_primary_output()\n        if not primary:\n            return 0.0, \"No primary output.\"\n        # Collect other outputs\n        all_outputs = context.get_all_outputs() or []\n        other_docs = []\n        for r in all_outputs:\n            if r.id == primary.id:\n                continue\n            try:\n                p = context.files.get_path(r.id)\n                name = p.name.lower()\n            except Exception:\n                continue\n            if getattr(r, 'is_document', False) and name.endswith('.docx'):\n                other_docs.append(name)\n        # Heuristics for applications and roadmap template\n        def has_mentor_app(names):\n            return any(('application' in n and 'mentor' in n) for n in names)\n        def has_mentee_app(names):\n            return any(('application' in n and 'mentee' in n) for n in names)\n        def has_combined_app(names):\n            return any(('application' in n and 'mentor' in n and 'mentee' in n) for n in names)\n        def has_roadmap(names):\n            return any((('roadmap' in n or 'mentorship' in n) and 'template' in n) for n in names)\n        names = other_docs\n        apps_ok = (has_mentor_app(names) and has_mentee_app(names)) or has_combined_app(names)\n        roadmap_ok = has_roadmap(names)\n        # Scoring: full if both apps_ok and roadmap_ok; partial if only one satisfied\n        if apps_ok and roadmap_ok:\n            return 2.0, \"Found DOCX files for applications and roadmap template.\"\n        elif apps_ok or roadmap_ok:\n            return 1.0, \"Found partial set of DOCX templates (either applications or roadmap).\"\n        else:\n            return 0.0, \"No separate DOCX templates detected for applications/roadmap.\"\n    except Exception as e:\n        return 0.0, f\"Error in template verification: {e}\""}, {"type": "code", "name": "Timeline Covers 8 Months", "description": "Check the main document text for an 8-month timeline (e.g., explicit Month 1..Month 8 labels or mentions of an 8-month cycle).", "weight": 1.2, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        primary = context.get_primary_output()\n        if not primary or not getattr(primary, 'is_document', False):\n            return 0.0, \"No primary document.\"\n        text = ''\n        try:\n            text = context.files.read_docx_text(primary.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(primary.id)\n            except Exception:\n                pass\n        if not text:\n            return 0.0, \"Unable to read document text.\"\n        t = text.lower()\n        # Count explicit month markers\n        month_hits = 0\n        for i in range(1, 9):\n            if re.search(rf\"\\bmonth\\s*{i}\\b\", t):\n                month_hits += 1\n        eight_month_mention = bool(re.search(r\"\\b(8|eight)[\\-\\s]?month\\b\", t))\n        # Scoring logic\n        if month_hits >= 6 or (month_hits >= 4 and eight_month_mention):\n            return 1.2, f\"Timeline shows {month_hits}/8 explicit months; 8-month cycle noted: {eight_month_mention}.\"\n        if month_hits >= 3 or eight_month_mention:\n            return 0.6, f\"Partial timeline evidence: {month_hits} months; 8-month mention: {eight_month_mention}.\"\n        return 0.0, \"No clear 8-month timeline detected.\"\n    except Exception as e:\n        return 0.0, f\"Error checking timeline: {e}\""}, {"type": "code", "name": "Roles and Responsibilities Coverage", "description": "Verify that both mentors and mentees are explicitly addressed in a roles/responsibilities context.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        primary = context.get_primary_output()\n        if not primary or not getattr(primary, 'is_document', False):\n            return 0.0, \"No primary document.\"\n        text = ''\n        try:\n            text = context.files.read_docx_text(primary.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(primary.id)\n            except Exception:\n                pass\n        if not text:\n            return 0.0, \"Unable to read text.\"\n        t = text.lower()\n        has_roles = 'role' in t or 'responsibilit' in t\n        has_mentor = 'mentor' in t\n        has_mentee = 'mentee' in t\n        # Bonus for proximity (roles near mentor/mentee)\n        roles_context = 0\n        if has_roles and has_mentor:\n            roles_context += 0.4\n        if has_roles and has_mentee:\n            roles_context += 0.4\n        if has_roles and has_mentor and has_mentee:\n            # Check at least one instance of mentor/mentee near 'responsibilities' or 'roles'\n            snippet_hit = bool(re.search(r\"(roles?|responsibilit\\w*).{0,200}(mentor|mentee)|(mentor|mentee).{0,200}(roles?|responsibilit\\w*)\", t, flags=re.DOTALL))\n            if snippet_hit:\n                roles_context = 0.8\n        return roles_context, f\"Roles coverage score: {roles_context:.2f} (roles: {has_roles}, mentor: {has_mentor}, mentee: {has_mentee}).\"\n    except Exception as e:\n        return 0.0, f\"Error checking roles: {e}\""}, {"type": "code", "name": "Core Program Elements Coverage", "description": "Check text for presence of key program elements: eligibility, application, matching, meetings, IDP, training, skills-building, networking, progress reports, evaluation, roadmap, appendix.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        primary = context.get_primary_output()\n        if not primary or not getattr(primary, 'is_document', False):\n            return 0.0, \"No primary document.\"\n        text = ''\n        try:\n            text = context.files.read_docx_text(primary.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(primary.id)\n            except Exception:\n                pass\n        if not text:\n            return 0.0, \"Unable to read text.\"\n        t = text.lower()\n        keywords = [\n            'eligibility', 'application', 'matching', 'meeting', 'idp', 'individual development plan',\n            'training', 'skills-building', 'networking', 'progress report', 'evaluation', 'roadmap', 'appendix'\n        ]\n        hits = 0\n        for kw in keywords:\n            if kw in t:\n                hits += 1\n        ratio = hits / len(keywords)\n        score = max(0.0, min(0.6, 0.6 * ratio))\n        return score, f\"Found {hits}/{len(keywords)} core element keywords.\"\n    except Exception as e:\n        return 0.0, f\"Error checking program elements: {e}\""}, {"type": "code", "name": "Branding and Acknowledgment Mentions", "description": "Verify text mentions CDC/DGHT branding and acknowledges NCIPC.", "weight": 0.4, "code": "def evaluate(workflow, context):\n    try:\n        primary = context.get_primary_output()\n        if not primary or not getattr(primary, 'is_document', False):\n            return 0.0, \"No primary document.\"\n        text = ''\n        try:\n            text = context.files.read_docx_text(primary.id)\n        except Exception:\n            try:\n                text = context.files.read_pdf_text(primary.id)\n            except Exception:\n                pass\n        if not text:\n            return 0.0, \"Unable to read text.\"\n        t = text.lower()\n        has_cdc = ('cdc' in t) or ('centers for disease control' in t)\n        has_dght = 'dght' in t or 'division of global hiv' in t\n        has_ncipc = 'ncipc' in t or 'national center for injury prevention and control' in t\n        score = 0.0\n        if has_cdc:\n            score += 0.15\n        if has_dght:\n            score += 0.15\n        if has_ncipc:\n            score += 0.10\n        score = min(score, 0.4)\n        return score, f\"Branding/acknowledgment: CDC={has_cdc}, DGHT={has_dght}, NCIPC={has_ncipc}.\"\n    except Exception as e:\n        return 0.0, f\"Error checking branding: {e}\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism (LLM)", "description": "Holistic assessment of clarity, inclusivity, branding alignment, and presentation quality for internal DGHT distribution.", "is_required": false, "max_points": 1.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Quality, Clarity, and Inclusivity", "description": "Assess whether the guide is polished, readable, inclusive, and aligned with CDC/DGHT tone and internal use needs.", "weight": 1.0, "judge_prompt": "Evaluate overall professional quality for internal DGHT distribution. Assume Stage 1 structure exists; focus on communication and polish rather than structure.\n\nConsider:\n- Clarity and coherence of writing, with a concise introduction and clear overview of program logic.\n- Inclusive, person-centered language (avoids biased/gendered assumptions; supportive tone for diverse mentoring styles).\n- Alignment with CDC/DGHT internal style and tone (professional, plain language where appropriate).\n- Readability and navigation (consistent headings, spacing, page numbers, lists/tables for requirements; optional callouts/icons used effectively if present).\n- Practical usability: key timelines, responsibilities, and requirements are easy to find; links are clearly labeled.\n\nScoring:\n- 1.0: Highly professional, clear, and inclusive; easy to navigate; branding tone feels aligned; visuals (if any) aid understanding.\n- 0.7: Generally strong but with minor clarity/flow issues or minor inconsistencies.\n- 0.4: Mixed quality; several issues with readability or tone; still serviceable.\n- 0.1: Hard to use; disorganized or confusing.\n- 0.0: Unusable or clearly unprofessional for internal distribution.", "expectation": "A polished, accessible, and inclusive guide that is easy to navigate and appropriate for internal DGHT use."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "4b98ccce-9e42-44e9-9115-6fc3e79de288", "rubric": {"category_name": "Health Care Admin \u2014 EMR Transition Packaging (Medical Administrative Assistant)", "rationale": "Task Type: Mixed (Pattern C). The deliverable set combines a structured Excel workbook plus two formal correspondence documents. Stage 1 uses an LLM judge to strictly enforce artifact presence and structure. Stage 2 uses code rules to verify internal consistency and content-level requirements made possible by Stage 1\u2019s shape. Stage 3 assesses professional quality and appropriateness for healthcare/HIPAA context.", "max_total_score": 18.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement GATE", "description": "Mandatory structure and files present so downstream verification is possible. LLM-only per philosophy.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.5, "rules": [{"type": "llm_judge", "name": "Structured Deliverable Set Present (Workbook + Letters)", "description": "Check that the submission includes the exact required files and structures with flexible matching on labels and headers.", "weight": 6.0, "judge_prompt": "You are the Stage 1 SHAPE GATE. Only verify PRESENCE and STRUCTURE, not correctness of values.\n\nDeliverable set MUST include ALL of the following:\n\nA) One Excel workbook named \"PATIENT INCIDENT 007.xlsx\" (case-insensitive OK, must be an Excel file):\n  - Contains BOTH tabs within the SAME workbook:\n    1) Tab exactly named or very close to: \"EMR TRANSFER PATIENTS\"\n    2) Tab exactly named or very close to: \"Golden Valley EMS DEC\"\n  - Each tab includes a clearly structured table with visible column headers. Accept minor header variations but the table on the EMR TRANSFER PATIENTS tab should cover these data fields:\n      \u2022 Patient Name\n      \u2022 Medical Record Number (MRN)\n      \u2022 Date of Birth (DOB)\n      \u2022 Address\n      \u2022 Telephone Number\n      \u2022 Aliases\n      \u2022 Known Relatives\n      \u2022 Deceased indicator (e.g., Deceased/Status/Is Deceased Yes-No)\n    The \"Golden Valley EMS DEC\" tab should list deceased patients (subset) with at least Patient Name, MRN, DOB, and a deceased/status indicator.\n  - Beneath the data in EACH tab, there is a SIGN-OFF area that includes:\n      \u2022 The assistant's name AND employee ID\n      \u2022 A visible label like \"Signed:\" / \"Signature\" / \"Signed by\"\n\nB) Two correspondence letters saved as DOCX or PDF (not plain text):\n  1) File named exactly or very close to: \"DECEASED CORRESPONDENCE 2025\" (.docx or .pdf)\n     - Professional letter structure present: letterhead/org name, date, recipient block, subject line, body, HIPAA clause section (explicitly labeled or clearly containing the word \u201cHIPAA\u201d), closing, signature block with name AND employee ID.\n     - Body states the patient is deceased and that Golden Valley EMS must provide authorized documentation to obtain medical records. (Check presence of these statements, not their legal sufficiency.)\n  2) File named exactly or very close to: \"GENERAL CORRESPONDENCE 2025\" (.docx or .pdf)\n     - Professional letter structure present: letterhead/org name, date, recipient block, subject line, body, HIPAA clause section (explicitly labeled or clearly containing the word \u201cHIPAA\u201d), closing, signature block with name AND employee ID.\n     - Body states the request is declined because the patient is not yet in the new EMR system. (Check presence of this statement only.)\n\nScoring (STRUCTURE ONLY):\n- 6.0: Excel workbook (correct name and format) contains BOTH required tabs with the described tables and sign-offs; BOTH letters present in DOCX/PDF with the required sections and body statements; each letter includes HIPAA clause and signature with name+ID.\n- 5.0: Minor naming/label deviations but clearly the same artifacts; small header variations OK; all required sections present across files.\n- 4.5: All core artifacts present, but missing one minor structural element (e.g., one letter missing an explicit HIPAA label while still referencing HIPAA, or a sign-off label present without explicit \u201cID\u201d text but name+ID appear).\n- 3.0: Workbook present but missing one required tab OR one of the two letters missing; or letters present but missing key structural sections (e.g., no signature block or no HIPAA mention) \u2014 still recognizably the required deliverables.\n- 1.0: Some artifacts present but mostly incomplete (e.g., workbook but wrong file type/naming and both letters missing; or letters present but no workbook).\n- 0.0: Not an Excel workbook and no valid letters; structure not followable.\n\nOnly check structure and presence, not the correctness of data, legality of HIPAA text, or writing quality.", "expectation": "A single Excel file named PATIENT INCIDENT 007.xlsx with the two specified tabs and sign-offs, plus two professional letters (DOCX/PDF) named DECEASED CORRESPONDENCE 2025 and GENERAL CORRESPONDENCE 2025 with HIPAA clauses and signature blocks."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Code + LLM)", "description": "Now that structure exists, verify internal consistency and key correctness using code rules. Flexible, robust checks with fallbacks.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "File Set Completeness and Naming", "description": "Verify presence of Excel workbook named with 'PATIENT INCIDENT 007' and both correspondence files with required base names and acceptable extensions.", "weight": 1.5, "code": "import re\nfrom pathlib import Path\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0, 'No outputs found.'\n\n    # Locate Excel workbook (prefer name match)\n    spreadsheets = [r for r in outputs if getattr(r, 'is_spreadsheet', False)]\n    excel_res = None\n    excel_path = None\n    for r in spreadsheets:\n        p = context.files.get_path(r.id)\n        if 'patient incident 007' in p.name.lower():\n            excel_res, excel_path = r, p\n            break\n    if excel_res is None and spreadsheets:\n        excel_res = spreadsheets[0]\n        excel_path = context.files.get_path(excel_res.id)\n\n    # Score Excel naming/extension\n    excel_score = 0.0\n    if excel_res is not None:\n        name_ok = 'patient incident 007' in excel_path.stem.lower()\n        ext_ok = excel_path.suffix.lower() in ['.xlsx']\n        if name_ok and ext_ok:\n            excel_score = 1.0\n        elif name_ok or ext_ok:\n            excel_score = 0.7\n        else:\n            excel_score = 0.4\n    else:\n        excel_score = 0.0\n\n    # Find letters\n    def find_doc(target):\n        target = target.lower()\n        best = None\n        for r in outputs:\n            if not getattr(r, 'is_document', False):\n                continue\n            p = context.files.get_path(r.id)\n            if any(p.suffix.lower().endswith(ext) for ext in ['.pdf', '.docx']):\n                stem = p.stem.lower()\n                if stem == target or target in stem:\n                    best = r\n                    break\n        return best\n\n    deceased_doc = find_doc('deceased correspondence 2025')\n    general_doc = find_doc('general correspondence 2025')\n\n    docs_present = (1 if deceased_doc else 0) + (1 if general_doc else 0)\n    if docs_present == 2:\n        doc_score = 1.0\n    elif docs_present == 1:\n        doc_score = 0.5\n    else:\n        doc_score = 0.0\n\n    score = (excel_score + doc_score) / 2.0\n    return score, f'Excel score={excel_score:.2f}, docs score={doc_score:.2f}'"}, {"type": "code", "name": "Excel Structural Verification (Tabs + Columns)", "description": "Verify both required tabs exist in the same workbook and that key columns are present with flexible header matching.", "weight": 2.5, "code": "import re\nimport pandas as pd\nfrom pathlib import Path\n\nREQUIRED_MAIN = {\n    'name': ['name','patient name','full name'],\n    'mrn': ['mrn','medical record number','record #','med rec #'],\n    'dob': ['dob','date of birth','birthdate','d.o.b'],\n    'address': ['address','street','mailing address'],\n    'phone': ['phone','telephone','tel','contact number','phone number'],\n    'aliases': ['alias','aliases','aka','also known as'],\n    'relatives': ['relative','relatives','known relative','next of kin','nok','emergency contact'],\n    'deceased': ['deceased','status','is deceased','dec','death','decd']\n}\n\nALT_MAIN_TAB = ['emr transfer patients','emr transfer','transfer patients']\nALT_DEC_TAB = ['golden valley ems dec','ems dec','golden valley dec','ems deceased']\n\n\ndef normalize(s):\n    s = str(s or '').strip().lower()\n    s = re.sub(r'[^a-z0-9\\s#]+','',s)\n    s = re.sub(r'\\s+',' ',s)\n    return s\n\n\ndef find_tab(sheet_names, targets):\n    sn = [normalize(x) for x in sheet_names]\n    for i, s in enumerate(sn):\n        for t in targets:\n            if t in s:\n                return sheet_names[i]\n    return None\n\n\ndef map_columns(cols, required_map):\n    ncols = [normalize(c) for c in cols]\n    mapping = {}\n    for key, variants in required_map.items():\n        found = None\n        for i, c in enumerate(ncols):\n            for v in variants:\n                if v in c:\n                    found = i\n                    break\n            if found is not None:\n                break\n        if found is not None:\n            mapping[key] = cols[found]\n    return mapping\n\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    spreadsheets = [r for r in outputs if getattr(r, 'is_spreadsheet', False)]\n    if not spreadsheets:\n        return 0.0, 'No spreadsheet found.'\n\n    # Choose workbook (prefer name match)\n    excel_res = None\n    for r in spreadsheets:\n        if 'patient incident 007' in context.files.get_path(r.id).name.lower():\n            excel_res = r\n            break\n    if excel_res is None:\n        excel_res = spreadsheets[0]\n\n    # Load sheet names\n    try:\n        xpath = context.files.get_path(excel_res.id)\n        xfile = pd.ExcelFile(xpath)\n        sheets = xfile.sheet_names\n    except Exception as e:\n        return 0.0, f'Failed to open Excel: {e}'\n\n    main_tab = find_tab(sheets, ALT_MAIN_TAB)\n    dec_tab = find_tab(sheets, ALT_DEC_TAB)\n\n    if not main_tab and not dec_tab:\n        return 0.2, 'Excel opened but required tabs not found.'\n\n    score_bits = []\n    feedback = []\n\n    # Main tab columns\n    if main_tab:\n        try:\n            df_main = context.files.read_excel(excel_res.id, sheet_name=main_tab)\n            main_map = map_columns(list(df_main.columns), REQUIRED_MAIN)\n            present = len(main_map)\n            need = len(REQUIRED_MAIN)\n            score_main = present / max(need,1)\n            score_bits.append(score_main)\n            feedback.append(f'Main tab columns mapped: {present}/{need}')\n        except Exception as e:\n            score_bits.append(0.0)\n            feedback.append(f'Failed reading main tab: {e}')\n    else:\n        score_bits.append(0.0)\n        feedback.append('Main tab not found.')\n\n    # Deceased tab columns (subset acceptable: require at least name, mrn, dob, deceased/status)\n    required_dec = {\n        'name': REQUIRED_MAIN['name'],\n        'mrn': REQUIRED_MAIN['mrn'],\n        'dob': REQUIRED_MAIN['dob'],\n        'deceased': REQUIRED_MAIN['deceased'],\n    }\n\n    if dec_tab:\n        try:\n            df_dec = context.files.read_excel(excel_res.id, sheet_name=dec_tab)\n            dec_map = map_columns(list(df_dec.columns), required_dec)\n            present = len(dec_map)\n            need = len(required_dec)\n            score_dec = present / max(need,1)\n            score_bits.append(score_dec)\n            feedback.append(f'Deceased tab columns mapped: {present}/{need}')\n        except Exception as e:\n            score_bits.append(0.0)\n            feedback.append(f'Failed reading deceased tab: {e}')\n    else:\n        score_bits.append(0.0)\n        feedback.append('Deceased tab not found.')\n\n    # Presence of both tabs bonus\n    tabs_bonus = 0.0\n    if main_tab and dec_tab:\n        tabs_bonus = 0.2\n\n    # Average structural mapping plus bonus, clipped to 1\n    base = sum(score_bits) / max(len(score_bits),1)\n    total = min(1.0, max(0.0, base + tabs_bonus))\n\n    return total, '; '.join(feedback)"}, {"type": "code", "name": "Deceased Subset Consistency", "description": "If a deceased/status indicator exists on the main tab, verify that rows on the deceased tab correspond to deceased rows in the main tab by MRN (or by Name if MRN missing).", "weight": 1.5, "code": "import re\nimport pandas as pd\n\nDECEASED_TRUE = {'y','yes','true','1','deceased','dec','death','decd'}\n\n\ndef normalize(s):\n    if pd.isna(s):\n        return ''\n    s = str(s).strip().lower()\n    s = re.sub(r'\\s+',' ',s)\n    return s\n\n\ndef looks_true(val):\n    s = normalize(val)\n    return s in DECEASED_TRUE\n\n\ndef best_col(cols, candidates):\n    cols_n = [normalize(c) for c in cols]\n    for i, c in enumerate(cols_n):\n        for cand in candidates:\n            if cand in c:\n                return i\n    return None\n\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    spreadsheets = [r for r in outputs if getattr(r, 'is_spreadsheet', False)]\n    if not spreadsheets:\n        return 0.0, 'No spreadsheet found.'\n\n    # Choose workbook\n    excel_res = None\n    for r in spreadsheets:\n        if 'patient incident 007' in context.files.get_path(r.id).name.lower():\n            excel_res = r\n            break\n    if excel_res is None:\n        excel_res = spreadsheets[0]\n\n    # Load both tabs if possible\n    import pandas as pd\n    try:\n        xfile = pd.ExcelFile(context.files.get_path(excel_res.id))\n        sheets = xfile.sheet_names\n    except Exception as e:\n        return 0.0, f'Failed to open Excel: {e}'\n\n    def find_tab(sheets, keys):\n        keys = [k.lower() for k in keys]\n        for s in sheets:\n            sl = s.lower()\n            for k in keys:\n                if k in sl:\n                    return s\n        return None\n\n    main_tab = find_tab(sheets, ['emr transfer patients','emr transfer','transfer patients'])\n    dec_tab = find_tab(sheets, ['golden valley ems dec','ems dec','golden valley dec','ems deceased'])\n\n    if not main_tab or not dec_tab:\n        return 0.4, 'Required tabs not both present; limited consistency check.'\n\n    try:\n        dfm = context.files.read_excel(excel_res.id, sheet_name=main_tab)\n        dfd = context.files.read_excel(excel_res.id, sheet_name=dec_tab)\n    except Exception as e:\n        return 0.0, f'Failed to read tabs: {e}'\n\n    # Identify key columns\n    mrn_i_m = best_col(dfm.columns, ['mrn','medical record number','record'])\n    name_i_m = best_col(dfm.columns, ['name','patient name','full name'])\n    dec_i_m = best_col(dfm.columns, ['deceased','status','is deceased','dec','death','decd'])\n\n    mrn_i_d = best_col(dfd.columns, ['mrn','medical record number','record'])\n    name_i_d = best_col(dfd.columns, ['name','patient name','full name'])\n\n    if dec_i_m is None:\n        # Cannot evaluate strictly; partial credit for presence\n        return 0.6, 'Deceased indicator not found on main tab; partial credit granted.'\n\n    # Build sets of deceased identifiers from main\n    deceased_mrn = set()\n    deceased_name = set()\n    for idx, row in dfm.iterrows():\n        try:\n            decv = row.iloc[dec_i_m]\n        except Exception:\n            continue\n        if looks_true(decv) or 'deceas' in normalize(decv):\n            if mrn_i_m is not None:\n                deceased_mrn.add(normalize(row.iloc[mrn_i_m]))\n            if name_i_m is not None:\n                deceased_name.add(normalize(row.iloc[name_i_m]))\n\n    if len(dfd) == 0:\n        return 0.8, 'Deceased tab empty; assuming no deceased records. Partial credit.'\n\n    # Check each row in dec tab aligns with deceased in main (prefer MRN)\n    total = len(dfd)\n    matched = 0\n    for idx, row in dfd.iterrows():\n        ok = False\n        if mrn_i_d is not None:\n            mrn = normalize(row.iloc[mrn_i_d])\n            if mrn and (mrn in deceased_mrn):\n                ok = True\n        if not ok and name_i_d is not None:\n            nm = normalize(row.iloc[name_i_d])\n            if nm and (nm in deceased_name):\n                ok = True\n        if ok:\n            matched += 1\n\n    ratio = matched / total if total else 1.0\n    # Reward strict match; allow some mismatch\n    score = 0.4 + 0.6*ratio\n    return min(1.0, score), f'Matched {matched}/{total} deceased rows to main tab.'"}, {"type": "code", "name": "Sign-off Detection on Each Tab", "description": "Detect a sign-off area containing a signal like 'Signed'/'Signature' and any mention of 'ID' or 'Employee' along with a name-like token on BOTH tabs.", "weight": 1.0, "code": "import re\nimport pandas as pd\n\ndef textify(df):\n    try:\n        vals = df.astype(str).values.flatten().tolist()\n    except Exception:\n        vals = []\n    text = ' \\n '.join([v for v in vals if v and v != 'nan'])\n    return text.lower()\n\n\ndef has_signoff(text):\n    if not text:\n        return False\n    sign_hit = any(k in text for k in ['signed','signature','signed by'])\n    id_hit = any(k in text for k in ['employee id','emp id','id:','id ','id#','employee'])\n    name_hint = bool(re.search(r'\\b[a-z]+\\s+[a-z]+\\b', text))  # crude full name pattern\n    return sign_hit and id_hit and name_hint\n\n\ndef find_tab(sheets, keys):\n    for s in sheets:\n        sl = s.lower()\n        for k in keys:\n            if k in sl:\n                return s\n    return None\n\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    spreadsheets = [r for r in outputs if getattr(r, 'is_spreadsheet', False)]\n    if not spreadsheets:\n        return 0.0, 'No spreadsheet found.'\n\n    # Choose workbook (prefer name match)\n    excel_res = None\n    for r in spreadsheets:\n        if 'patient incident 007' in context.files.get_path(r.id).name.lower():\n            excel_res = r\n            break\n    if excel_res is None:\n        excel_res = spreadsheets[0]\n\n    # Read sheets\n    import pandas as pd\n    try:\n        xfile = pd.ExcelFile(context.files.get_path(excel_res.id))\n        sheets = xfile.sheet_names\n    except Exception as e:\n        return 0.0, f'Failed to open Excel: {e}'\n\n    main_tab = find_tab(sheets, ['emr transfer patients','emr transfer','transfer patients'])\n    dec_tab = find_tab(sheets, ['golden valley ems dec','ems dec','golden valley dec','ems deceased'])\n\n    found = 0\n    total = 0\n    fb = []\n\n    if main_tab:\n        total += 1\n        try:\n            dfm = context.files.read_excel(excel_res.id, sheet_name=main_tab)\n            if has_signoff(textify(dfm)):\n                found += 1\n                fb.append('Main tab sign-off detected.')\n            else:\n                fb.append('Main tab sign-off not detected.')\n        except Exception as e:\n            fb.append(f'Main tab read error: {e}')\n\n    if dec_tab:\n        total += 1\n        try:\n            dfd = context.files.read_excel(excel_res.id, sheet_name=dec_tab)\n            if has_signoff(textify(dfd)):\n                found += 1\n                fb.append('Deceased tab sign-off detected.')\n            else:\n                fb.append('Deceased tab sign-off not detected.')\n        except Exception as e:\n            fb.append(f'Deceased tab read error: {e}')\n\n    if total == 0:\n        return 0.0, 'No required tabs found.'\n\n    score = found / total\n    return score, '; '.join(fb)"}, {"type": "code", "name": "Deceased Letter Content (Keywords)", "description": "Deceased correspondence includes deceased statement, Golden Valley EMS authorization requirement, HIPAA mention, and a signature block with name/ID.", "weight": 0.75, "code": "import re\n\ndef read_text_any(res, context):\n    p = context.files.get_path(res.id)\n    if p.suffix.lower() == '.pdf':\n        try:\n            return context.files.read_pdf_text(res.id)\n        except Exception:\n            return ''\n    elif p.suffix.lower() == '.docx':\n        try:\n            return context.files.read_docx_text(res.id)\n        except Exception:\n            return ''\n    else:\n        try:\n            return context.files.read_text(res.id)\n        except Exception:\n            return ''\n\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    deceased = None\n    for r in outputs:\n        if getattr(r, 'is_document', False):\n            p = context.files.get_path(r.id)\n            if p.suffix.lower() in ['.pdf','.docx'] and 'deceased correspondence 2025' in p.stem.lower():\n                deceased = r\n                break\n    if not deceased:\n        return 0.0, 'Deceased correspondence file not found.'\n\n    text = read_text_any(deceased, context).lower()\n    if not text:\n        return 0.2, 'Could not extract text; minimal credit.'\n\n    checks = []\n    checks.append('deceased' in text)\n    checks.append('golden valley ems' in text)\n    checks.append(('authorized' in text) and ('document' in text))\n    checks.append(('medical record' in text) or ('records' in text))\n    checks.append('hipaa' in text or 'health insurance portability and accountability act' in text)\n    # signature signals\n    has_sig = any(k in text for k in ['signed','signature','sincerely','respectfully']) and ('id' in text or 'employee' in text)\n    checks.append(has_sig)\n\n    score = sum(1 for c in checks if c) / len(checks)\n    return score, f'Deceased letter checks passed: {sum(checks)}/{len(checks)}'"}, {"type": "code", "name": "General Letter Content (Keywords)", "description": "General correspondence includes decline statement tied to EMR unavailability, HIPAA mention, and a signature block with name/ID.", "weight": 0.75, "code": "import re\n\ndef read_text_any(res, context):\n    p = context.files.get_path(res.id)\n    if p.suffix.lower() == '.pdf':\n        try:\n            return context.files.read_pdf_text(res.id)\n        except Exception:\n            return ''\n    elif p.suffix.lower() == '.docx':\n        try:\n            return context.files.read_docx_text(res.id)\n        except Exception:\n            return ''\n    else:\n        try:\n            return context.files.read_text(res.id)\n        except Exception:\n            return ''\n\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    general = None\n    for r in outputs:\n        if getattr(r, 'is_document', False):\n            p = context.files.get_path(r.id)\n            if p.suffix.lower() in ['.pdf','.docx'] and 'general correspondence 2025' in p.stem.lower():\n                general = r\n                break\n    if not general:\n        return 0.0, 'General correspondence file not found.'\n\n    text = read_text_any(general, context).lower()\n    if not text:\n        return 0.2, 'Could not extract text; minimal credit.'\n\n    checks = []\n    # Decline of request for not-in-EMR\n    decline_hit = any(k in text for k in ['decline','unable to','cannot','can\\'t']) and ('request' in text or 'record request' in text)\n    checks.append(decline_hit)\n    emr_hit = ('emr' in text or 'electronic medical record' in text) and ('not' in text or 'not yet' in text or 'not currently' in text)\n    checks.append(emr_hit)\n    checks.append('hipaa' in text or 'health insurance portability and accountability act' in text)\n    has_sig = any(k in text for k in ['signed','signature','sincerely','respectfully']) and ('id' in text or 'employee' in text)\n    checks.append(has_sig)\n\n    score = sum(1 for c in checks if c) / len(checks)\n    return score, f'General letter checks passed: {sum(checks)}/{len(checks)}'"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality", "description": "Evaluate professionalism, clarity, and appropriateness for healthcare correspondence and EMR transition support.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Appropriateness", "description": "Holistic quality assessment across the Excel workbook and letters. Do not re-check structure already handled by Stage 1.", "weight": 4.0, "judge_prompt": "Assess professional quality (not structure) of the provided workbook and letters:\n\nExcel workbook (PATIENT INCIDENT 007.xlsx):\n- Readability: clear headers, consistent formatting, reasonable date/phone formats, alignment, no obvious typos in headings.\n- Usability: filters, frozen header row, sensible ordering, no mixed data in header rows, deceased tab clearly labeled as such.\n- Compliance awareness: no apparent PHI exposed inappropriately in letter files (e.g., avoid unnecessary details), minimal visible errors.\n\nLetters (DECEASED CORRESPONDENCE 2025, GENERAL CORRESPONDENCE 2025):\n- Tone and professionalism: respectful, concise, suitable for external recipients in healthcare.\n- Clarity: purpose and next steps obvious; for deceased letter, the authorization requirement is clearly communicated; for general letter, EMR limitation and rationale for decline are clear.\n- Formatting: letterhead, spacing, subject clarity, closing and signature block look professional.\n\nScoring guidelines:\n- 4.0: Highly professional presentation in both workbook and letters; formatting aids usability; tone and clarity exemplary.\n- 3.0: Generally professional with minor issues (small formatting inconsistencies or minor wording awkwardness).\n- 2.0: Noticeable issues that reduce professionalism or clarity; still acceptable.\n- 1.0: Barely professional; multiple issues; might require revision before sending.\n- 0.0: Poorly presented or clearly unprofessional.\n\nReturn a score only based on quality; do not penalize for missing structure already enforced in Stage 1.", "expectation": "Polished, readable Excel and professional letters suitable for healthcare compliance communication."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "11593a50-734d-4449-b5b4-f8986a133fd8", "rubric": {"category_name": "Real Estate Buyer Tour Selection Rubric", "rationale": "This rubric enforces a self-documenting, verifiable workflow for a buyer-side real estate selection task. Stage 1 (LLM-only) mandates precise deliverable shapes: a 2-page brochure PDF with a photo and required data columns for each property (fewer than 15), and a 1-page map PDF with pins. Stage 2 mixes code and LLM rules to validate constraints (location, beds/baths, price ceiling, count, and status) and map/list alignment. Stage 3 assesses professional quality and usefulness to buyers.", "max_total_score": 10.0, "stages": [{"name": "Stage 1: Shape Enforcement Gate", "description": "Gate that ensures the outputs exist in the exact, verifiable shape: a 2-page brochure PDF listing <15 qualifying homes with required columns and photos, plus a 1-page map PDF with pins.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Brochure PDF Structural Requirements", "description": "Verify there is a brochure-style PDF with the mandated structure for property selection.", "weight": 3.0, "judge_prompt": "You are verifying STRUCTURE ONLY for the brochure deliverable. Use the provided outputs (PDFs). Do NOT judge content correctness beyond the structural presence of elements. Be flexible with near-equivalent header names (e.g., \"Beds\" vs \"Bedrooms\").\n\nRequirements for the Brochure PDF:\n- File format: PDF (not Word/Excel/plain text)\n- Page count: exactly 2 pages\n- Contains a clear property list: fewer than 15 homes total (i.e., 1\u201314)\n- Each property displays a photo (thumbnail or larger) of the home\n- A tabular layout (or clearly columnar grid) is present and includes the following columns/fields (exact names may vary slightly):\n  \u2022 Status (must be present as a field/column header)\n  \u2022 Type (e.g., Single Family)\n  \u2022 Price (a dedicated Price column/field)\n  \u2022 List Date\n  \u2022 Address (with city and ZIP visible in the address field)\n  \u2022 Beds (Bedrooms)\n  \u2022 Baths (Bathrooms)\n  \u2022 Square Footage (Sq Ft / Living Area)\n  \u2022 Lot Size (Lot / Acres)\n  \u2022 Year Built\n  \u2022 $/SqFt (price per square foot)\n- Properties are formatted in a consistent, professional layout (rows or cards) where the above fields are visible for each property.\n\nScoring (STRUCTURE only):\n- 3.0: PDF, exactly 2 pages, <15 properties, each property has a photo, and ALL required columns/fields are clearly present.\n- 2.3: Meets PDF + 2 pages + <15 + photos, but missing 1\u20132 non-critical fields (e.g., Year Built or $/SqFt) OR the table layout is present but one field is ambiguously labeled.\n- 1.5: PDF present but page count incorrect OR lacks photos for several properties OR missing 3\u20134 required fields/columns.\n- 0.5: PDF present but not a structured brochure list (e.g., mostly narrative text, no consistent table/columns).\n- 0.0: Not a PDF OR no brochure-like structure.\n\nOnly evaluate the presence/structure of elements, not whether values meet the buyer criteria.", "expectation": "A clean, 2-page PDF brochure listing fewer than 15 homes with a photo and all required columns clearly visible."}, {"type": "llm_judge", "name": "Map PDF Structural Requirements", "description": "Verify a dedicated 1-page map PDF exists with visual pins for the properties.", "weight": 1.0, "judge_prompt": "You are verifying STRUCTURE ONLY for the map deliverable. Use the provided outputs (PDFs). Do NOT judge content correctness beyond the structural presence of elements.\n\nRequirements for the Map PDF:\n- File format: PDF (not Word/Excel/plain text)\n- Page count: exactly 1 page\n- A map is visible with multiple location pins/markers corresponding to the listed properties (numbers/labels/legend acceptable but not required)\n- Map appears to cover Massapequa Park, NY area (exact boundary precision not required for Stage 1)\n\nScoring (STRUCTURE only):\n- 1.0: PDF, exactly 1 page, and a map image with clear pins/markers representing multiple properties.\n- 0.6: PDF and a map image present but pins/markers are unclear/faint OR only 1\u20132 pins visible.\n- 0.2: PDF present but map not clearly visible (e.g., low quality) or no pins.\n- 0.0: Not a PDF or no map at all.\n\nOnly evaluate presence/structure, not correctness of geographic placement.", "expectation": "A one-page PDF map with visible pins/markers for the selected properties."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Correctness Verification", "description": "Automated checks and targeted LLM verification against buyer criteria and geographic alignment.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Location Compliance (Massapequa Park, NY 11762)", "description": "From the brochure PDF text, detect property addresses and verify they indicate Massapequa Park and ZIP 11762.", "weight": 1.0, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    try:\\n        # Collect brochure-like PDF text (the one with table headers)\\n        texts = []\\n        brochure_texts = []\\n        for res in context.get_all_outputs():\\n            if getattr(res, 'is_document', False):\\n                try:\\n                    t = context.files.read_pdf_text(res.id) or ''\\n                except Exception:\\n                    t = ''\\n                texts.append(t)\\n        # Heuristic: brochure contains many of these headers\\n        header_keys = ['address', 'price', 'bed', 'bath', 'status', 'list date', 'sq', '$/sqft', 'year built', 'lot']\\n        for t in texts:\\n            lt = t.lower()\\n            hits = sum(1 for k in header_keys if k in lt)\\n            if hits >= 4:\\n                brochure_texts.append(lt)\\n        if not brochure_texts:\\n            # Fall back to all text\\n            brochure_texts = [t.lower() for t in texts if t]\\n        if not brochure_texts:\\n            return 0.0, 'No readable PDF text found.'\\n\\n        text = '\\n'.join(brochure_texts)\\n\\n        # Extract addresses strongly matching city+state+zip\\n        strong_addr_re = re.compile(r\"(\\d{1,6}[^\\n,]*?),\\s*massapequa\\s*park,\\s*ny\\s*11762\", re.I)\\n        strong = set(m.group(0).strip() for m in strong_addr_re.finditer(text))\\n\\n        # Weak matches: any line with 11762 and Massapequa (Park optional)\\n        weak_addr_re = re.compile(r\"(\\d{1,6}[^\\n]*?)(massapequa(?:\\s*park)?),\\s*ny[^\\n]*?(11762)\", re.I)\\n        weak = set(m.group(0).strip() for m in weak_addr_re.finditer(text))\\n\\n        total_candidates = len(strong | weak)\\n        if total_candidates == 0:\\n            # As a fallback, count any lines with 11762\\n            zip_hits = re.findall(r\"11762\", text)\\n            if not zip_hits:\\n                return 0.0, 'No addresses found referencing Massapequa Park or 11762.'\\n            # If zip appears, give partial (0.4) for probable location presence\\n            return 0.4, 'Found ZIP 11762 references but could not parse full addresses.'\\n\\n        # Score: fraction that are strong matches (with Massapequa Park + 11762)\\n        strong_count = len(strong)\\n        frac = strong_count / max(total_candidates, 1)\\n        # Map to score 0..1 (at least half strong -> good)\\n        score = 0.5 + 0.5 * min(1.0, frac / 0.5) if strong_count else 0.5 * min(1.0, total_candidates / 10)\\n        return max(0.0, min(1.0, score)), f'Strong addresses: {strong_count}, Total candidates: {total_candidates}'\\n    except Exception as e:\\n        return 0.0, f'Error: {e}'"}, {"type": "code", "name": "Bedrooms and Bathrooms Constraints", "description": "Verify bedrooms are between 4 and 6 inclusive and bathrooms are at least 2.", "weight": 1.0, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    try:\\n        texts = []\\n        for res in context.get_all_outputs():\\n            if getattr(res, 'is_document', False):\\n                try:\\n                    t = context.files.read_pdf_text(res.id) or ''\\n                except Exception:\\n                    t = ''\\n                texts.append(t.lower())\\n        if not texts:\\n            return 0.0, 'No PDF text available.'\\n        text = '\\n'.join(texts)\\n\\n        bed_matches = re.findall(r\"(\\d+)\\s*(?:bed|beds|br)\\b\", text)\\n        bath_matches = re.findall(r\"(\\d+(?:\\.\\d+)?)\\s*(?:bath|baths|ba)\\b\", text)\\n\\n        if not bed_matches and not bath_matches:\\n            return 0.0, 'No bed/bath patterns found.'\\n\\n        bed_vals = [int(b) for b in bed_matches if b.isdigit()]\\n        bath_vals = []\\n        for b in bath_matches:\\n            try:\\n                bath_vals.append(float(b))\\n            except:\\n                pass\\n\\n        bed_score = 0.0\\n        if bed_vals:\\n            valid_beds = sum(1 for v in bed_vals if 4 <= v <= 6)\\n            bed_score = valid_beds / len(bed_vals)\\n        bath_score = 0.0\\n        if bath_vals:\\n            valid_baths = sum(1 for v in bath_vals if v >= 2.0)\\n            bath_score = valid_baths / len(bath_vals)\\n\\n        if bed_vals and bath_vals:\\n            score = (bed_score + bath_score) / 2\\n        elif bed_vals:\\n            score = bed_score * 0.8  # partial since only beds confirmed\\n        else:\\n            score = bath_score * 0.8  # partial since only baths confirmed\\n\\n        return max(0.0, min(1.0, score)), f'Beds valid ratio: {bed_score:.2f}, Baths valid ratio: {bath_score:.2f}'\\n    except Exception as e:\\n        return 0.0, f'Error: {e}'"}, {"type": "code", "name": "Price Ceiling Under $1,500,000", "description": "Verify all detected property prices are at or below $1,500,000.", "weight": 1.0, "code": "import re\\n\\ndef parse_amount(s):\\n    try:\\n        return int(float(s.replace(',', '')))\\n    except:\\n        return None\\n\\ndef evaluate(workflow, context):\\n    try:\\n        texts = []\\n        for res in context.get_all_outputs():\\n            if getattr(res, 'is_document', False):\\n                try:\\n                    t = context.files.read_pdf_text(res.id) or ''\\n                except Exception:\\n                    t = ''\\n                texts.append(t.lower())\\n        if not texts:\\n            return 0.0, 'No PDF text found.'\\n        text = '\\n'.join(texts)\\n\\n        # Prefer amounts near the word Price\\n        near_price = re.findall(r\"price[^\\n$]*\\$\\s*([0-9][0-9,]{2,})\", text, re.I)\\n        amounts = [a for a in near_price]\\n        # Fallback: any dollar amounts\\n        if not amounts:\\n            amounts = re.findall(r\"\\$\\s*([0-9][0-9,]{2,})\", text)\\n\\n        nums = [parse_amount(a) for a in amounts]\\n        nums = [n for n in nums if n is not None]\\n        # Filter to likely listing prices (>= $100k and <= $5M to exclude $/sqft)\\n        prices = [n for n in nums if 100_000 <= n <= 5_000_000]\\n        if not prices:\\n            return 0.4, 'Could not confidently extract listing prices; giving minimal partial credit.'\\n\\n        max_price = max(prices)\\n        within = [p for p in prices if p <= 1_500_000]\\n        score = len(within) / len(prices)\\n        return max(0.0, min(1.0, score)), f'Max detected price: ${max_price:,}; {len(within)}/{len(prices)} prices <= $1.5M'\\n    except Exception as e:\\n        return 0.0, f'Error: {e}'"}, {"type": "code", "name": "Property Count Under 15", "description": "Count distinct addresses to ensure total properties listed are fewer than 15.", "weight": 1.0, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    try:\\n        texts = []\\n        for res in context.get_all_outputs():\\n            if getattr(res, 'is_document', False):\\n                try:\\n                    t = context.files.read_pdf_text(res.id) or ''\\n                except Exception:\\n                    t = ''\\n                texts.append(t)\\n        if not texts:\\n            return 0.0, 'No PDF text.'\\n        text = '\\n'.join(texts)\\n        lt = text.lower()\\n        # Extract likely addresses (lines with a street number and a comma)\\n        addr_re = re.compile(r\"\\b\\d{1,6}[^\\n,]*,\\s*(?:massapequa(?:\\s*park)?|amityville|seaford|wantagh|farmingdale)[^\\n]*\\b\", re.I)\\n        addrs = set(m.group(0).strip() for m in addr_re.finditer(lt))\\n        # Fallback: lines containing 11762 with a street number\\n        if not addrs:\\n            addr_re2 = re.compile(r\"\\b\\d{1,6}[^\\n]*11762\\b\", re.I)\\n            addrs = set(m.group(0).strip() for m in addr_re2.finditer(lt))\\n        n = len(addrs)\\n        if n == 0:\\n            return 0.3, 'No distinct addresses parsed; awarding minimal credit.'\\n        if 1 <= n <= 14:\\n            return 1.0, f'Parsed {n} distinct addresses (<15).'\\n        if 15 <= n <= 20:\\n            return 0.4, f'Parsed {n} addresses; exceeds limit but close.'\\n        return 0.0, f'Parsed {n} addresses; far above limit.'\\n    except Exception as e:\\n        return 0.0, f'Error: {e}'"}, {"type": "code", "name": "Status Check: Active Only", "description": "Ensure listings are Active and not Pending or Coming Soon.", "weight": 1.0, "code": "def evaluate(workflow, context):\\n    try:\\n        texts = []\\n        for res in context.get_all_outputs():\\n            if getattr(res, 'is_document', False):\\n                try:\\n                    t = context.files.read_pdf_text(res.id) or ''\\n                except Exception:\\n                    t = ''\\n                texts.append(t.lower())\\n        if not texts:\\n            return 0.0, 'No PDF text.'\\n        text = '\\n'.join(texts)\\n        has_active = 'active' in text\\n        bad_tokens = ['pending', 'coming soon', 'contingent', 'offer accepted']\\n        has_bad = any(bt in text for bt in bad_tokens)\\n        if has_active and not has_bad:\\n            return 1.0, 'Active present; no disqualifying statuses found.'\\n        if has_active and has_bad:\\n            return 0.5, 'Active present but also found disqualifying statuses.'\\n        return 0.0, 'No evidence of Active status.'\\n    except Exception as e:\\n        return 0.0, f'Error: {e}'"}, {"type": "llm_judge", "name": "Map/List Alignment and Geography Reasonableness", "description": "LLM cross-check that the map pins correspond to the listed properties and are within/near Massapequa Park, NY 11762.", "weight": 1.0, "judge_prompt": "Use all provided outputs. Cross-check the brochure property list with the map. You may rely on recognizable address text, labels, or pin numbering. You are judging reasonableness and alignment, not pixel-perfect accuracy.\n\nChecks:\n1) The map is centered on or near Massapequa Park, NY (Long Island, south shore area).  \n2) The distribution of pins plausibly corresponds to the listed properties (counts are similar: within 0\u20132 difference is acceptable).  \n3) Pins appear within or immediately adjacent to Massapequa Park boundaries (not far-away towns).  \n\nScoring:\n- 1.0: Clear alignment: pin count matches or is off by \u22642; geography matches Massapequa Park area.  \n- 0.6: Mostly aligned but a few pins seem out of area OR count off by 3\u20134.  \n- 0.2: Weak alignment: map seems unrelated to the listed area OR pins far from Massapequa Park.  \n- 0.0: No usable map or cannot relate to the list at all.", "expectation": "Pins align with the listed properties and sit in/near Massapequa Park, NY 11762."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3: Presentation Quality", "description": "Holistic quality assessment for client-ready materials and usefulness for a 2-day visit.", "is_required": false, "max_points": 1.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professionalism and Buyer Usefulness", "description": "Evaluate clarity, visual quality, and usefulness for planning a weekend tour.", "weight": 1.0, "judge_prompt": "Assess the brochure and map for professional polish and usefulness to buyers planning a 2-day tour. Consider:\n- Visual clarity and consistency (clean layout, readable fonts, aligned columns)\n- Photo quality and relevance (each property has a clear exterior photo)\n- Data completeness (most fields filled; $/SqFt derived and visible)\n- Practicality for a weekend tour (addresses readable, easy to compare, concise)\n\nScoring:\n- 1.0: Highly professional, clear, complete, and buyer-friendly for a quick tour.\n- 0.7: Generally professional with minor issues (some missing fields or cluttered sections).\n- 0.4: Adequate but several presentation issues reduce usability.\n- 0.1: Poorly formatted; hard to use.\n- 0.0: Not evaluable in these terms.", "expectation": "A clean, client-ready set of PDFs that are easy to skim and compare for a short house-hunting trip."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "3940b7e7-ec4f-4cea-8097-3ab4cfdcaaa6", "rubric": {"category_name": "CFD Flow-Simulation Report (Aerospace - Mechanical Engineering)", "rationale": "This rubric enforces a self-documenting, verifiable PDF report for a CFD flow-simulation of an X-Wing assembly. Stage 1 (LLM-only) mandates the exact document structure, sections, and tables, making validation possible. Stage 2 mixes code and LLM to check presence of key metrics, min/max pairs, and boundary condition details using robust text extraction and fuzzy matching. Stage 3 evaluates professional quality and actionability for design review purposes.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Format and Structural Gate (LLM-only)", "description": "Gate: Enforce exact structure for a verifiable CFD flow-simulation report exported as PDF.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "PDF Structure and Required Sections/Tables", "description": "Verify the output is a multi-page PDF report with required sections and tables, enabling further verification.", "weight": 4.0, "judge_prompt": "You are the Stage 1 gatekeeper. Inspect the candidate output. Only evaluate STRUCTURE and PRESENCE, not technical correctness.\n\nFormat requirements (hard gate):\n- Must be a PDF file (not DOCX, not plain text, not Excel).\n- At least 2 pages.\n- Professional formatting with visible section headers.\n\nRequired section headers (exact or close synonyms acceptable):\n1) \"Objective\"\n2) \"Simulation environment\"\n3) \"Boundary conditions\"\n4) \"Results\"\n5) \"Discussion\"\n6) \"Conclusion\"\n\nWithin sections, verify presence of the following content (structure only):\n- In Simulation environment: description of computational domain and mesh.\n- In Boundary conditions: inlet and outlet boundary conditions, plus material properties.\n- A mention of engineering goals or monitors used to drive convergence/residuals.\n\nRequired tables (must be visible as tables):\nA) Global goals table: a table titled/captioned like \"Global Goals\" or similar. Columns should include at least Goal (or Name), Unit, and Value. At least 3 rows.\nB) Field variable min/max table: a table titled/captioned like \"Field Variable Min/Max\", \"Extrema\", or similar. Columns should include Variable, Min (or Minimum), Max (or Maximum), and Unit. Include at least 6 variables (e.g., density, pressure, temperature, velocity components or magnitude, Mach number, relative pressure, TKE, turbulence intensity, etc.).\n\nRequired key metrics listed in Results (as text items or within tables):\n- Peak axial velocity (or equivalent wording for the axial component maximum)\n- Maximum turbulence intensity\n- Turbulent kinetic energy (TKE)\n- Forces acting on the wing (mention of lift and/or drag)\n\nScoring (0.0\u20131.0; do not judge correctness):\n- 1.0 (maps to full weight): PDF, \u22652 pages, all six sections present, both required tables present and identifiable, and all four key metrics are explicitly listed.\n- 0.75: PDF, \u22652 pages, missing exactly one required element (one section OR one required table is incomplete/missing OR one key metric not explicitly listed).\n- 0.5: PDF, \u22652 pages, missing two\u2013three required elements total.\n- 0.25: PDF but severely incomplete (e.g., fewer than 4 sections or both tables missing).\n- 0.0: Not a PDF or fewer than 2 pages.\n\nReturn a score between 0.0 and 1.0 based on the above structural criteria. Provide brief feedback on any missing structural elements.", "expectation": "A 2+ page PDF with the six specified sections, a Global Goals table, a Field Variable Min/Max table, and explicit listing of the four key metrics, with clear structure enabling verification."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Consistency Verification", "description": "Code and LLM checks for presence and plausible structure of metrics, variable extrema, and simulation setup details (not numerical correctness).", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Required Metrics Presence and Numerics", "description": "Check that peak axial velocity, maximum turbulence intensity, TKE, and lift/drag forces are mentioned with nearby numeric values.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n\n    text = \"\"\n    try:\n        if hasattr(context.files, 'read_pdf_text'):\n            try:\n                text = context.files.read_pdf_text(output.id) or \"\"\n            except Exception:\n                text = \"\"\n        if (not text) and hasattr(context.files, 'read_docx_text'):\n            try:\n                text = context.files.read_docx_text(output.id) or \"\"\n            except Exception:\n                text = text or \"\"\n    except Exception:\n        text = text or \"\"\n\n    if not text:\n        return 0.0, \"Unable to extract text from document.\"\n\n    tl = text.lower()\n\n    def has_numeric_near(patterns, qualifiers=None):\n        if qualifiers is None:\n            qualifiers = []\n        found = False\n        for pat in patterns:\n            for m in re.finditer(pat, tl, flags=re.IGNORECASE):\n                start = max(0, m.start()-60)\n                end = min(len(tl), m.end()+120)\n                window = tl[start:end]\n                qual_ok = True\n                if qualifiers:\n                    qual_ok = any(q in window for q in qualifiers)\n                num_ok = re.search(r\"[-+]?\\d+(?:\\.\\d+)?(?:e[+-]?\\d+)?\", window) is not None\n                if qual_ok and num_ok:\n                    found = True\n                    break\n            if found:\n                break\n        return found\n\n    # 1) Peak axial velocity\n    axial_ok = has_numeric_near([\n        r\"axial\\s+velocity\",\n        r\"\\bu[_\\-\\s]?x\\b\",\n        r\"\\bU[xX]\\b\",\n    ], qualifiers=[\"peak\", \"max\", \"maximum\"]) or has_numeric_near([r\"velocity\\s*\\(x\\)\"], qualifiers=[\"max\", \"maximum\"]) \n\n    # 2) Maximum turbulence intensity\n    ti_ok = has_numeric_near([r\"turbulence\\s+intensity\", r\"\\bTI\\b\"], qualifiers=[\"max\", \"maximum\", \"peak\"]) or has_numeric_near([r\"turbulence\\s+intensity\\s*\\(max\\)\"])\n\n    # 3) Turbulent kinetic energy (TKE)\n    tke_ok = has_numeric_near([r\"turbulent\\s+kinetic\\s+energy\", r\"\\bTKE\\b\"]) \n\n    # 4) Forces acting on the wing (lift/drag)\n    lift_ok = has_numeric_near([r\"\\blift\\b\", r\"lift\\s+force\", r\"c[lL]\"], qualifiers=None)\n    drag_ok = has_numeric_near([r\"\\bdrag\\b\", r\"drag\\s+force\", r\"c[dD]\"], qualifiers=None)\n    forces_score = 1.0 if (lift_ok and drag_ok) else (0.5 if (lift_ok or drag_ok) else 0.0)\n\n    present = [axial_ok, ti_ok, tke_ok]\n    base = sum(1.0 if x else 0.0 for x in present) / 3.0\n    overall = (0.75 * base) + (0.25 * forces_score)  # weight forces slightly lower individually\n\n    overall = max(0.0, min(1.0, overall))\n    feedback = []\n    feedback.append(f\"Axial velocity: {'OK' if axial_ok else 'MISSING'}\")\n    feedback.append(f\"Turbulence intensity: {'OK' if ti_ok else 'MISSING'}\")\n    feedback.append(f\"TKE: {'OK' if tke_ok else 'MISSING'}\")\n    feedback.append(f\"Lift/Drag: {'both' if (lift_ok and drag_ok) else ('one' if (lift_ok or drag_ok) else 'none')}\")\n\n    return overall * 1.0, \"; \".join(feedback)\n"}, {"type": "code", "name": "Field Variable Min/Max Pairs", "description": "Detect min/max pairs for important field variables (density, pressure, temperature, velocity, Mach, relative pressure, TKE, turbulence intensity).", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n\n    text = \"\"\n    try:\n        try:\n            text = context.files.read_pdf_text(output.id) or \"\"\n        except Exception:\n            text = context.files.read_docx_text(output.id) or \"\"\n    except Exception:\n        text = text or \"\"\n\n    if not text:\n        return 0.0, \"Unable to extract text from document.\"\n\n    tl = text.lower()\n\n    variables = {\n        'density': [r\"density\", r\"\\brho\\b\"],\n        'pressure': [r\"pressure\", r\"static\\s+pressure\"],\n        'temperature': [r\"temperature\", r\"\\btemp\\b\"],\n        'velocity': [r\"velocity\\s+magnitude\", r\"velocity\", r\"speed\"],\n        'mach': [r\"\\bmach\\b\", r\"mach\\s+number\"],\n        'relative pressure': [r\"relative\\s+pressure\", r\"gauge\\s+pressure\"],\n        'tke': [r\"turbulent\\s+kinetic\\s+energy\", r\"\\bTKE\\b\"],\n        'turbulence intensity': [r\"turbulence\\s+intensity\", r\"\\bTI\\b\"],\n    }\n\n    def has_min_max_for(patterns):\n        for pat in patterns:\n            for m in re.finditer(pat, tl, flags=re.IGNORECASE):\n                start = m.start()\n                window = tl[start:start+400]\n                min_hit = re.search(r\"min(?:imum)?[^0-9\\-+]*([-+]?\\d+(?:\\.\\d+)?(?:e[+-]?\\d+)?)\", window)\n                max_hit = re.search(r\"max(?:imum)?[^0-9\\-+]*([-+]?\\d+(?:\\.\\d+)?(?:e[+-]?\\d+)?)\", window)\n                if min_hit and max_hit:\n                    # Optionally check min <= max if parseable\n                    try:\n                        vmin = float(min_hit.group(1))\n                        vmax = float(max_hit.group(1))\n                        if vmin <= vmax or True:\n                            return True\n                    except Exception:\n                        return True\n        return False\n\n    found = {}\n    count = 0\n    for name, pats in variables.items():\n        ok = has_min_max_for(pats)\n        found[name] = ok\n        if ok:\n            count += 1\n\n    # Require at least 5 variables with min/max for full credit; otherwise scale.\n    fraction = min(1.0, count / 5.0)\n    feedback = \", \".join([f\"{k}: {'OK' if v else 'MISSING'}\" for k, v in found.items()])\n    return fraction * 1.0, f\"Min/Max pairs detected for {count} variables. Details: {feedback}\"\n"}, {"type": "code", "name": "Simulation Environment and BCs Completeness", "description": "Check presence of computational domain/mesh, material properties, inlet/outlet BCs, and convergence goals/monitors.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n\n    text = \"\"\n    try:\n        try:\n            text = context.files.read_pdf_text(output.id) or \"\"\n        except Exception:\n            text = context.files.read_docx_text(output.id) or \"\"\n    except Exception:\n        text = text or \"\"\n\n    if not text:\n        return 0.0, \"Unable to extract text from document.\"\n\n    tl = text.lower()\n\n    def any_in(words):\n        return any(w in tl for w in words)\n\n    # 1) Computational domain and mesh\n    domain_mesh = any_in([\"computational domain\", \"domain extent\", \"periodic boundary\", \"symmetry plane\", \"grid\", \"mesh\", \"cells\", \"elements\", \"polyhedral\", \"tetra\", \"hexa\", \"refinement\", \"y+\"])\n\n    # 2) Material properties\n    materials = any_in([\"material properties\", \"air\", \"aluminum\", \"steel\", \"density\", \"viscosity\", \"ideal gas\", \"sutherland\", \"compressible\", \"gamma\"])\n\n    # 3) Inlet/Outlet boundary conditions\n    bcs = any_in([\"inlet\", \"outlet\", \"velocity inlet\", \"mass flow\", \"pressure outlet\", \"total pressure\", \"static pressure\", \"stagnation\", \"mach\", \"turbulence intensity\", \"tke\"])\n\n    # 4) Convergence goals/monitors\n    goals = any_in([\"goal\", \"monitor\", \"residual\", \"converge\", \"convergence\", \"stopping\", \"iterations\", \"criterion\", \"target\"])\n\n    hits = [domain_mesh, materials, bcs, goals]\n    fraction = sum(1.0 for h in hits if h) / 4.0\n\n    feedback = f\"Domain/Mesh: {'OK' if domain_mesh else 'MISSING'}; Materials: {'OK' if materials else 'MISSING'}; BCs: {'OK' if bcs else 'MISSING'}; Goals: {'OK' if goals else 'MISSING'}\"\n    return fraction * 1.0, feedback\n"}, {"type": "llm_judge", "name": "Narrative Consistency and Aerodynamic Implications", "description": "Check that Discussion/Conclusion reference reported values and provide implications on lift/drag, shocks, separation, turbulence, with preliminary recommendations.", "weight": 1.0, "judge_prompt": "Evaluate the document for narrative-to-data consistency and aerodynamic reasoning. Do NOT re-check Stage 1 structure. Focus on whether the Discussion and Conclusion:\n\nChecks:\n- Reference specific reported values from Results/tables (e.g., citing magnitudes or ranges, not just generic statements).\n- Discuss aerodynamic implications relevant to an X-Wing: lift vs drag trade-offs, potential shock formation, flow separation regions, turbulence behavior.\n- Provide preliminary, actionable recommendations to improve the design (e.g., geometry adjustments, boundary layer control, mesh refinement targets, operating point changes), tied to observed results.\n- Avoid contradictions (e.g., claiming low drag while reporting high drag values) and maintain internal consistency.\n\nScoring (0.0\u20131.0):\n- 1.0: Clear references to specific values, sound aerodynamic implications covering at least 3 of the listed topics, and actionable, results-driven recommendations; no contradictions.\n- 0.5: Some references to values but limited reasoning or only 1\u20132 topics covered; recommendations vague or partially tied to results; no major contradictions.\n- 0.0: No concrete references to values and/or aerodynamic implications missing or clearly inconsistent; recommendations absent or irrelevant.\n\nReturn a score between 0.0 and 1.0 and brief feedback highlighting strengths/weaknesses.", "expectation": "Discussion/Conclusion clearly tie numeric results to aerodynamic implications and provide actionable next steps without contradictions."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Actionability", "description": "Holistic LLM assessment of presentation quality and usefulness for design review.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation", "description": "Assess clarity, organization, and suitability for internal design review.", "weight": 1.0, "judge_prompt": "Assess the report\u2019s professional quality for internal aerospace design review:\n- Clarity and conciseness of writing\n- Logical flow matching the specified section order\n- Proper figures/tables captions and readable units\n- Terminology appropriate for mechanical/aerospace engineers\n\nScoring (0.0\u20131.0):\n- 1.0: Clear, concise, well-organized, professional tone; tables/figures labeled; units consistently presented.\n- 0.5: Generally readable but with organizational or clarity issues; minor labeling/unit inconsistencies.\n- 0.0: Disorganized, unclear writing; poor labeling; units missing or inconsistent.\n\nReturn a score between 0.0 and 1.0 with brief feedback.", "expectation": "Clean, concise, engineer-ready presentation with coherent flow and labeled tables."}, {"type": "llm_judge", "name": "Actionability of Recommendations", "description": "Assess whether recommendations are specific and immediately useful to guide next optimization steps.", "weight": 1.0, "judge_prompt": "Evaluate the actionability of recommendations for improving the X-Wing design:\n- Are recommendations specific and prioritized?\n- Do they include concrete next steps (e.g., adjust angle of attack range, refine mesh near hinge/LE, modify airfoil thickness, adjust boundary layer transition model, evaluate alternative turbulence model)?\n- Are they feasible within typical aerospace CFD workflows?\n\nScoring (0.0\u20131.0):\n- 1.0: Specific, prioritized, feasible next steps clearly linked to results.\n- 0.5: Somewhat specific but lacking prioritization or weak linkage to results.\n- 0.0: Vague or generic; not clearly tied to results or not feasible.\n\nReturn a score between 0.0 and 1.0 with brief feedback.", "expectation": "Concrete, feasible next steps that clearly follow from reported findings."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "3f821c2d-ab97-46ec-a0fb-b8f73c2682bc", "rubric": {"category_name": "Omnichannel Stock & Sales Flow (Wholesale Trade)", "rationale": "This rubric enforces a self-documenting Excel model that proves correctness through a clear, machine-checkable structure. Stage 1 is a strict LLM-gated shape requirement for the workbook (sheets, named sections, tables). Stage 2 uses code rules to verify arithmetic identities, budget/constraints, and cross-sheet consistency using normalized tables mandated in Stage 1. Stage 3 holistically assesses professional quality and strategic soundness.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Structured Output Format Requirement (LLM Gate)", "description": "Gate: Verify the output is a single Excel workbook with required sheets, tables, and side-by-side flows enabling machine checks. DO NOT check math correctness here\u2014only structure and presence.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.5, "rules": [{"type": "llm_judge", "name": "Omnichannel Flow Workbook Shape", "description": "Check presence and structure of Excel workbook, sheets, and tables needed for verification.", "weight": 6.0, "judge_prompt": "You are evaluating whether the candidate output is a single Excel workbook (.xlsx) with the exact, verifiable structure below. Only check structure and presence, not math correctness.\n\nFormat Requirements:\n- Must be a single Excel file (.xlsx). Not PDF/DOCX/CSV.\n- Months covered: Aug, Sep, Oct, Nov, Dec, Jan (Fall Season = Aug\u2013Jan).\n- Channels: Stores, E-commerce, and Omni (sum of both). Also include LY (Last Year) tables for comparison.\n\nRequired Sheets and Sections (be flexible with very similar names):\n\n1) Sheet: 'Assumptions & Targets' (or very similar)\n   - A two-column key/value table listing at least these items (keys visible):\n     \u2022 Gross Receipt Budget (Omni Season) = 675,000\n     \u2022 Season Turn Target (Omni) = 4.0\n     \u2022 EOM January Target (Omni) = 200,000 (or stated as \u2264 $200k)\n     \u2022 Min Monthly Receipts - Stores = 10,000\n     \u2022 Min Monthly Receipts - E-commerce = 6,000\n     \u2022 July 2025 Projected EOM (Stores)\n     \u2022 July 2025 Projected EOM (E-commerce)\n   - Keys must be readable as text next to numeric values.\n\n2) Sheet: 'Sales Plan' (or similar)\n   - Normalized table with columns: [Channel | Month | Retail Sales $]\n   - Rows: Stores and E-commerce for each month Aug\u2013Jan.\n\n3) Sheet: 'Receipts Plan' (or similar)\n   - Normalized table with columns: [Channel | Month | Receipts $]\n   - Rows: Stores and E-commerce for each month Aug\u2013Jan (Omni optional).\n\n4) Sheet: 'Flow (TY) - Side-by-Side' (or similar)\n   - Clearly formatted, side-by-side tables for TY with Months as columns (Aug\u2013Jan) and rows:\n     \u2022 BOM Inventory $  \u2022 Retail Sales $  \u2022 Receipts $  \u2022 EOM Inventory $  \u2022 Turn (Monthly)\n   - Two channel tables (Stores and E-commerce) placed side-by-side.\n   - An Omni (TY) table (same structure) either beneath or adjacent.\n   - A parallel section for LY tables in the same side-by-side layout and row structure.\n\n5) Sheet: 'Flow_Long_TY' and 'Flow_Long_LY' (or similar names)\n   - Normalized tables with columns: [Channel | Month | Metric | Value]\n   - Metrics include at least: BOM, Sales, Receipts, EOM, Turn (Monthly)\n   - Channels include: Stores, E-commerce, Omni; Months are Aug\u2013Jan.\n\n6) Sheet: 'Validation Checks' (or similar)\n   - Two visible sections:\n     a) 'Arithmetic Audit' table with columns (or very similar):\n        [Channel | Month | BOM | Sales | Receipts | EOM | Check_EOM=BOM+Receipts-Sales | Diff | Pass]\n        Rows should cover Stores, E-commerce, and Omni for Aug\u2013Jan.\n     b) 'KPI Summary' key/value table listing at least:\n        \u2022 Omni_Total_Receipts\n        \u2022 Omni_Budget\n        \u2022 Omni_Receipts_Delta\n        \u2022 Omni_Seasonal_Turn\n        \u2022 Turn_Target\n        \u2022 Turn_Pass\n        \u2022 Omni_Jan_EOM\n        \u2022 Jan_EOM_Target\n        \u2022 Jan_EOM_Pass\n        \u2022 Min_Receipts_Pass\n\n7) Sheet: 'Methodology' (or similar)\n   - A short text section (3+ sentences) describing formulas used including:\n     \u2022 EOM = BOM + Receipts \u2212 Sales\n     \u2022 Turn (Monthly) = Sales / Average Inventory, where Average Inventory = (BOM + EOM)/2\n     \u2022 Seasonal Turn (Omni) = Sales / (Sum of Monthly EOM / 6)\n\nScoring (structure only):\n- 6.0: All required sheets present with the specified tables/sections clearly visible and labeled. Months/channels and required rows match. LY present and in same structure. Side-by-side layout visible on the TY sheet.\n- 4.5: Minor omissions only (e.g., KPI Summary missing 1-2 keys OR Arithmetic Audit missing 1-2 columns), but core sheets and normalized tables exist.\n- 3.0: Several required components missing (e.g., no Flow_Long sheets OR missing Sales Plan/Receipts Plan), but a clear attempt at the specified structure exists.\n- 0.0: Not an Excel file, or the structure is largely missing (no side-by-side channel flows, no normalized tables, or months/channels not identifiable).\n\nOnly check presence/format/structure. Do NOT check numerical correctness or if formulas work.", "expectation": "A cleanly structured .xlsx workbook with the mandated sheets, normalized tables, side-by-side TY/LY flows, and a Validation Checks sheet enabling trivial verification."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Numerical and Consistency Verification (Code + LLM)", "description": "Now verify correctness using the structured shapes from Stage 1. Code rules run deterministic checks on identities, budgets, constraints, and cross-sheet consistency. Tolerances: $1 for dollar math, 0.05 for turn ratios.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Inventory Identity and Non-Negative Checks", "description": "Verify EOM = BOM + Receipts \u2212 Sales for each channel/month (Stores, E-commerce, Omni) in Flow_Long_TY. Penalize negatives in BOM/EOM. Tolerance: $1.", "weight": 3.0, "code": "def evaluate(workflow, context):\n    import pandas as pd, numpy as np, re\n    def norm_channel(s):\n        if s is None: return None\n        t = str(s).strip().lower()\n        if t in ['store','stores','st']:\n            return 'Stores'\n        if t in ['e-commerce','ecommerce','e-comm','ecomm','ecom','e commerce']:\n            return 'E-commerce'\n        if 'omni' in t:\n            return 'Omni'\n        return None\n    def norm_month(s):\n        if s is None: return None\n        t = str(s).strip().lower()\n        m = {\n            'aug':'Aug','august':'Aug',\n            'sep':'Sep','sept':'Sep','september':'Sep',\n            'oct':'Oct','october':'Oct',\n            'nov':'Nov','november':'Nov',\n            'dec':'Dec','december':'Dec',\n            'jan':'Jan','january':'Jan'\n        }\n        return m.get(t, None)\n    def norm_metric(s):\n        if s is None: return None\n        t = str(s).strip().lower()\n        if 'bom' in t: return 'bom'\n        if 'eom' in t: return 'eom'\n        if 'receipt' in t: return 'receipts'\n        if 'sale' in t: return 'sales'\n        if 'turn' in t: return 'turn'\n        return None\n    def get_sheet_df(xls_path, candidates):\n        xl = pd.ExcelFile(xls_path)\n        sheets = xl.sheet_names\n        picked = None\n        for cand in candidates:\n            for sh in sheets:\n                if cand.lower() in sh.lower():\n                    picked = sh\n                    break\n            if picked: break\n        if not picked:\n            return None\n        return pd.read_excel(xls_path, sheet_name=picked)\n    out = context.get_primary_output()\n    if not out or not out.is_spreadsheet:\n        return 0.0, 'No spreadsheet output.'\n    xls_path = context.files.get_path(out.id)\n    df = get_sheet_df(xls_path, ['Flow_Long_TY','Flow Long TY','Normalized Flow (TY)','Flow Long'])\n    if df is None or df.empty:\n        return 0.0, 'Missing Flow_Long_TY normalized table.'\n    # Normalize columns\n    cols = {c.lower().strip(): c for c in df.columns}\n    # try to map columns\n    def pick(colnames, options):\n        for o in options:\n            for c in colnames:\n                if o == c.lower().strip():\n                    return c\n        return None\n    ch_col = pick(df.columns, ['channel'])\n    mo_col = pick(df.columns, ['month'])\n    met_col = pick(df.columns, ['metric'])\n    val_col = None\n    for cand in ['value','amount','val']:\n        v = pick(df.columns, [cand])\n        if v:\n            val_col = v\n            break\n    if not (ch_col and mo_col and met_col and val_col):\n        return 0.0, 'Flow_Long_TY missing required columns [Channel, Month, Metric, Value].'\n    # Build dict\n    vals = {}\n    missing_rows = 0\n    for _, r in df.iterrows():\n        ch = norm_channel(r.get(ch_col))\n        mo = norm_month(r.get(mo_col))\n        mt = norm_metric(r.get(met_col))\n        try:\n            v = float(r.get(val_col))\n        except Exception:\n            v = np.nan\n        if ch and mo and mt and pd.notna(v):\n            vals[(ch, mo, mt)] = v\n        else:\n            missing_rows += 1\n    channels = ['Stores','E-commerce','Omni']\n    months = ['Aug','Sep','Oct','Nov','Dec','Jan']\n    total = 0\n    passes = 0\n    negatives = 0\n    diffs = []\n    for ch in channels:\n        for mo in months:\n            bom = vals.get((ch,mo,'bom'))\n            sal = vals.get((ch,mo,'sales'))\n            rec = vals.get((ch,mo,'receipts'))\n            eom = vals.get((ch,mo,'eom'))\n            if bom is None or sal is None or rec is None or eom is None:\n                continue\n            total += 1\n            diff = (bom + rec - sal) - eom\n            diffs.append(diff)\n            if abs(diff) <= 1.0:\n                passes += 1\n            if (bom is not None and bom < -1.0) or (eom is not None and eom < -1.0):\n                negatives += 1\n    if total == 0:\n        return 0.0, 'No complete rows to audit in Flow_Long_TY.'\n    # Score: proportion correct minus penalty for negatives\n    frac = passes / total\n    penalty = min(negatives, total) / total * 0.3  # up to 0.3 reduction\n    score = max(0.0, (frac - penalty)) * 3.0\n    fb = f'Identity checks passed {passes}/{total}; negatives: {negatives}; avg diff=${np.mean(diffs) if diffs else 0:.2f}.'\n    return score, fb"}, {"type": "code", "name": "BOM Roll-Forward Consistency", "description": "Verify that for each channel, BOM for next month equals prior EOM (Aug\u2192Sep\u2192Oct\u2192Nov\u2192Dec\u2192Jan). Tolerance: $1.", "weight": 1.5, "code": "def evaluate(workflow, context):\n    import pandas as pd, numpy as np\n    def norm_channel(s):\n        if s is None: return None\n        t=str(s).strip().lower()\n        if t in ['store','stores','st']: return 'Stores'\n        if t in ['e-commerce','ecommerce','e-comm','ecomm','ecom','e commerce']: return 'E-commerce'\n        if 'omni' in t: return 'Omni'\n        return None\n    def norm_month(s):\n        if s is None: return None\n        t=str(s).strip().lower()\n        m={'aug':'Aug','august':'Aug','sep':'Sep','sept':'Sep','september':'Sep','oct':'Oct','october':'Oct','nov':'Nov','november':'Nov','dec':'Dec','december':'Dec','jan':'Jan','january':'Jan'}\n        return m.get(t)\n    out=context.get_primary_output()\n    if not out or not out.is_spreadsheet:\n        return 0.0, 'No spreadsheet.'\n    x=context.files.get_path(out.id)\n    try:\n        df=pd.read_excel(x, sheet_name=[s for s in pd.ExcelFile(x).sheet_names if 'flow' in s.lower() and 'long' in s.lower()][0])\n    except Exception:\n        return 0.0, 'Missing Flow_Long_TY sheet.'\n    # columns\n    def pick(colnames, name):\n        for c in colnames:\n            if name==c.lower().strip(): return c\n        return None\n    ch=pick(df.columns,'channel'); mo=pick(df.columns,'month'); me=pick(df.columns,'metric'); va=None\n    for cand in ['value','amount','val']:\n        v=pick(df.columns,cand)\n        if v: va=v; break\n    if not (ch and mo and me and va):\n        return 0.0, 'Flow_Long_TY missing required columns.'\n    vals={}\n    for _,r in df.iterrows():\n        C=norm_channel(r.get(ch)); M=norm_month(r.get(mo)); Mt=str(r.get(me)).strip().lower() if r.get(me) is not None else None\n        if Mt:\n            if 'bom' in Mt: Mt='bom'\n            elif 'eom' in Mt: Mt='eom'\n            else: continue\n        try:\n            V=float(r.get(va))\n        except Exception:\n            V=np.nan\n        if C and M and Mt and pd.notna(V):\n            vals[(C,M,Mt)]=V\n    channels=['Stores','E-commerce','Omni']\n    months=['Aug','Sep','Oct','Nov','Dec','Jan']\n    total=0; ok=0\n    for C in channels:\n        for i in range(len(months)-1):\n            m0=months[i]; m1=months[i+1]\n            e=vals.get((C,m0,'eom')); b=vals.get((C,m1,'bom'))\n            if e is None or b is None:\n                continue\n            total+=1\n            if abs((e)-(b))<=1.0:\n                ok+=1\n    if total==0:\n        return 0.0, 'No BOM/EOM pairs to compare.'\n    frac=ok/total\n    return frac*1.5, f'BOM roll-forward passed {ok}/{total}.'"}, {"type": "code", "name": "Receipts Budget Alignment and Minimum Floors", "description": "Check that Receipts Plan totals equal the omni budget (675,000) and per-month receipts meet channel floors (Stores >=10k, E-comm >=6k). Also ensure Flow_Long_TY Receipts match Receipts Plan per channel/month.", "weight": 2.0, "code": "def evaluate(workflow, context):\n    import pandas as pd, numpy as np\n    def find_sheet(x, names):\n        xl=pd.ExcelFile(x); sh=xl.sheet_names\n        for n in names:\n            for s in sh:\n                if n.lower() in s.lower():\n                    return s\n        return None\n    def norm_channel(s):\n        if s is None: return None\n        t=str(s).strip().lower()\n        if t in ['store','stores','st']: return 'Stores'\n        if t in ['e-commerce','ecommerce','e-comm','ecomm','ecom','e commerce']: return 'E-commerce'\n        if 'omni' in t: return 'Omni'\n        return None\n    def norm_month(s):\n        if s is None: return None\n        t=str(s).strip().lower()\n        m={'aug':'Aug','august':'Aug','sep':'Sep','sept':'Sep','september':'Sep','oct':'Oct','october':'Oct','nov':'Nov','november':'Nov','dec':'Dec','december':'Dec','jan':'Jan','january':'Jan'}\n        return m.get(t)\n    def get_kv_assumptions(x):\n        sh=find_sheet(x,['Assumptions & Targets','Assumptions','Targets'])\n        if not sh: return {}\n        df=pd.read_excel(x, sheet_name=sh)\n        if df.shape[1]<2: return {}\n        kv={}\n        for _,r in df.iterrows():\n            k=r.iloc[0]; v=r.iloc[1] if df.shape[1]>1 else None\n            if pd.isna(k) or pd.isna(v):\n                continue\n            key=str(k).strip().lower()\n            kv[key]=v\n        # extract essentials with fuzzy keys\n        def find(keys):\n            for kk in kv:\n                if all(sub in kk for sub in keys):\n                    return kv[kk]\n            return None\n        return {\n            'budget': find(['gross','budget']),\n            'turn_target': find(['turn','target']),\n            'jan_target': find(['eom','jan']),\n            'min_store': find(['min','store','receipt']),\n            'min_ecom': find(['min','e-','receipt']) or find(['min','ecom','receipt'])\n        }\n    out=context.get_primary_output()\n    if not out or not out.is_spreadsheet:\n        return 0.0, 'No spreadsheet.'\n    x=context.files.get_path(out.id)\n    # Assumptions\n    ass=get_kv_assumptions(x)\n    budget=ass.get('budget')\n    min_store=ass.get('min_store')\n    min_ecom=ass.get('min_ecom')\n    # defaults if missing\n    if pd.isna(budget) or budget is None:\n        budget=675000.0\n    if pd.isna(min_store) or min_store is None:\n        min_store=10000.0\n    if pd.isna(min_ecom) or min_ecom is None:\n        min_ecom=6000.0\n    # Receipts Plan\n    sh_r=find_sheet(x,['Receipts Plan','Receipts Allocation','Receipts'])\n    if not sh_r:\n        return 0.0, 'Missing Receipts Plan.'\n    rp=pd.read_excel(x, sheet_name=sh_r)\n    # map columns\n    def pick(cols, names):\n        for n in names:\n            for c in cols:\n                if n==c.lower().strip(): return c\n        return None\n    cols=[c.lower().strip() for c in rp.columns]\n    ch=pick(rp.columns, ['channel']); mo=pick(rp.columns, ['month'])\n    rc=None\n    for nm in ['receipts $','receipts','amount','value']:\n        rc=pick(rp.columns,[nm])\n        if rc: break\n    if not (ch and mo and rc):\n        return 0.0, 'Receipts Plan missing required columns.'\n    rp2=[]\n    for _,r in rp.iterrows():\n        C=norm_channel(r.get(ch)); M=norm_month(r.get(mo))\n        try:\n            V=float(r.get(rc))\n        except Exception:\n            V=np.nan\n        if C in ['Stores','E-commerce'] and M and pd.notna(V):\n            rp2.append((C,M,V))\n    if not rp2:\n        return 0.0, 'Receipts Plan has no usable rows.'\n    total=sum(v for _,_,v in rp2)\n    # Floors\n    floors_ok=0; floors_total=0\n    for C,M,V in rp2:\n        floors_total+=1\n        thr=min_store if C=='Stores' else min_ecom\n        if V+1e-9>=thr: floors_ok+=1\n    # Flow_Long_TY receipts match\n    sh_f=find_sheet(x,['Flow_Long_TY','Flow Long TY','Normalized Flow (TY)','Flow Long'])\n    if not sh_f:\n        return 0.0, 'Missing Flow_Long_TY.'\n    fl=pd.read_excel(x, sheet_name=sh_f)\n    chf=pick(fl.columns,['channel']); mof=pick(fl.columns,['month']); mef=pick(fl.columns,['metric']); vf=None\n    for nm in ['value','amount','val']:\n        vf=pick(fl.columns,[nm])\n        if vf: break\n    if not (chf and mof and mef and vf):\n        return 0.0, 'Flow_Long_TY missing required columns.'\n    rec_map={}\n    for _,r in fl.iterrows():\n        C=norm_channel(r.get(chf)); M=norm_month(r.get(mof)); Mt=str(r.get(mef)).strip().lower() if r.get(mef) is not None else ''\n        if 'receipt' in Mt:\n            try:\n                V=float(r.get(vf))\n            except Exception:\n                continue\n            if C in ['Stores','E-commerce'] and M:\n                rec_map[(C,M)]=V\n    match_ok=0; match_total=0\n    for C,M,V in rp2:\n        match_total+=1\n        V2=rec_map.get((C,M))\n        if V2 is not None and abs(V2-V)<=1.0:\n            match_ok+=1\n    # Score components\n    score=0.0; fb=[]\n    # Budget alignment (50%)\n    budget_ok=abs(total - float(budget))<=1.0\n    score+= (1.0 if budget_ok else 0.0) * (2*0.5)\n    fb.append(f'Total receipts={total:.2f}, budget={float(budget):.2f}, budget_ok={budget_ok}')\n    # Floors (25%)\n    if floors_total>0:\n        score+= (floors_ok/floors_total) * (2*0.25)\n        fb.append(f'Floors pass {floors_ok}/{floors_total}')\n    else:\n        fb.append('No floors rows to test')\n    # Flow match (25%)\n    if match_total>0:\n        score+= (match_ok/match_total) * (2*0.25)\n        fb.append(f'Receipts match Flow_Long_TY {match_ok}/{match_total}')\n    else:\n        fb.append('No matching rows to compare')\n    return score, '; '.join(fb)"}, {"type": "code", "name": "Targets: Omni Seasonal Turn and Jan EOM", "description": "Compute Omni seasonal turn from Flow_Long_TY and check >= target (default 4.0). Check Omni EOM in January <= target (default $200,000).", "weight": 2.0, "code": "def evaluate(workflow, context):\n    import pandas as pd, numpy as np\n    def find_sheet(x, names):\n        xl=pd.ExcelFile(x); sh=xl.sheet_names\n        for n in names:\n            for s in sh:\n                if n.lower() in s.lower():\n                    return s\n        return None\n    def norm_channel(s):\n        if s is None: return None\n        t=str(s).strip().lower()\n        if 'omni' in t: return 'Omni'\n        if t in ['store','stores','st']: return 'Stores'\n        if t in ['e-commerce','ecommerce','e-comm','ecomm','ecom','e commerce']: return 'E-commerce'\n        return None\n    def norm_month(s):\n        if s is None: return None\n        t=str(s).strip().lower()\n        m={'aug':'Aug','august':'Aug','sep':'Sep','sept':'Sep','september':'Sep','oct':'Oct','october':'Oct','nov':'Nov','november':'Nov','dec':'Dec','december':'Dec','jan':'Jan','january':'Jan'}\n        return m.get(t)\n    def get_assumption(x, keys):\n        sh=find_sheet(x,['Assumptions & Targets','Assumptions','Targets'])\n        if not sh: return None\n        df=pd.read_excel(x, sheet_name=sh)\n        if df.shape[1]<2: return None\n        for _,r in df.iterrows():\n            k=r.iloc[0]; v=r.iloc[1]\n            if pd.isna(k) or pd.isna(v):\n                continue\n            kk=str(k).strip().lower()\n            if all(s in kk for s in keys):\n                try:\n                    return float(v)\n                except Exception:\n                    return None\n        return None\n    out=context.get_primary_output()\n    if not out or not out.is_spreadsheet:\n        return 0.0, 'No spreadsheet.'\n    x=context.files.get_path(out.id)\n    sh=find_sheet(x,['Flow_Long_TY','Flow Long TY','Normalized Flow (TY)','Flow Long'])\n    if not sh:\n        return 0.0, 'Missing Flow_Long_TY.'\n    df=pd.read_excel(x, sheet_name=sh)\n    # columns\n    def pick(cols, name):\n        for c in cols:\n            if name==c.lower().strip(): return c\n        return None\n    ch=pick(df.columns,'channel'); mo=pick(df.columns,'month'); me=pick(df.columns,'metric'); va=None\n    for nm in ['value','amount','val']:\n        v=pick(df.columns,nm)\n        if v: va=v; break\n    if not (ch and mo and me and va):\n        return 0.0, 'Flow_Long_TY missing required columns.'\n    sales=0.0; eom_sum=0.0; got=[False,False]\n    jan_eom=None\n    for _,r in df.iterrows():\n        C=norm_channel(r.get(ch)); M=norm_month(r.get(mo)); Mt=str(r.get(me)).strip().lower() if r.get(me) is not None else ''\n        if C!='Omni' or not M:\n            continue\n        try:\n            V=float(r.get(va))\n        except Exception:\n            continue\n        if 'sale' in Mt:\n            sales+=V\n            got[0]=True\n        if 'eom' in Mt:\n            eom_sum+=V\n            if M=='Jan':\n                jan_eom=V\n            got[1]=True\n    if not all(got):\n        return 0.0, 'Missing Omni sales/EOM data.'\n    seasonal_turn = sales / (eom_sum/6.0) if eom_sum!=0 else 0.0\n    turn_target = get_assumption(x, ['turn','target'])\n    if turn_target is None:\n        turn_target=4.0\n    jan_target = get_assumption(x, ['eom','jan'])\n    if jan_target is None:\n        jan_target=200000.0\n    pass_turn = seasonal_turn + 1e-12 >= float(turn_target)\n    pass_jan = (jan_eom is not None) and (jan_eom <= float(jan_target) + 1e-9)\n    # score: 1 point each\n    score = (1.0 if pass_turn else 0.0) + (1.0 if pass_jan else 0.0)\n    fb = f'Seasonal turn={seasonal_turn:.3f} (target {float(turn_target):.2f}) pass={pass_turn}; Jan EOM={jan_eom if jan_eom is not None else \"NA\":.2f} (target {float(jan_target):.2f}) pass={pass_jan}.'\n    return score, fb"}, {"type": "code", "name": "Sales Plan Consistency", "description": "Ensure Flow_Long_TY Sales match Sales Plan per channel/month exactly (Stores, E-commerce).", "weight": 1.0, "code": "def evaluate(workflow, context):\n    import pandas as pd, numpy as np\n    def find_sheet(x, names):\n        xl=pd.ExcelFile(x); sh=xl.sheet_names\n        for n in names:\n            for s in sh:\n                if n.lower() in s.lower():\n                    return s\n        return None\n    def norm_channel(s):\n        if s is None: return None\n        t=str(s).strip().lower()\n        if t in ['store','stores','st']: return 'Stores'\n        if t in ['e-commerce','ecommerce','e-comm','ecomm','ecom','e commerce']: return 'E-commerce'\n        return None\n    def norm_month(s):\n        if s is None: return None\n        t=str(s).strip().lower()\n        m={'aug':'Aug','august':'Aug','sep':'Sep','sept':'Sep','september':'Sep','oct':'Oct','october':'Oct','nov':'Nov','november':'Nov','dec':'Dec','december':'Dec','jan':'Jan','january':'Jan'}\n        return m.get(t)\n    out=context.get_primary_output()\n    if not out or not out.is_spreadsheet:\n        return 0.0, 'No spreadsheet.'\n    x=context.files.get_path(out.id)\n    sh_sp=find_sheet(x,['Sales Plan'])\n    if not sh_sp:\n        return 0.0, 'Missing Sales Plan.'\n    sp=pd.read_excel(x, sheet_name=sh_sp)\n    def pick(cols,name):\n        for c in cols:\n            if name==c.lower().strip(): return c\n        return None\n    ch=pick(sp.columns,'channel'); mo=pick(sp.columns,'month'); sv=None\n    for nm in ['retail sales $','sales $','sales','amount','value']:\n        sv=pick(sp.columns,nm)\n        if sv: break\n    if not (ch and mo and sv):\n        return 0.0, 'Sales Plan missing required columns.'\n    plan={}\n    for _,r in sp.iterrows():\n        C=norm_channel(r.get(ch)); M=norm_month(r.get(mo))\n        try:\n            V=float(r.get(sv))\n        except Exception:\n            V=np.nan\n        if C and M and pd.notna(V):\n            plan[(C,M)]=V\n    sh_fl=find_sheet(x,['Flow_Long_TY','Flow Long TY','Normalized Flow (TY)','Flow Long'])\n    if not sh_fl:\n        return 0.0, 'Missing Flow_Long_TY.'\n    fl=pd.read_excel(x, sheet_name=sh_fl)\n    chf=pick(fl.columns,'channel'); mof=pick(fl.columns,'month'); mef=pick(fl.columns,'metric'); vf=None\n    for nm in ['value','amount','val']:\n        v=pick(fl.columns,nm)\n        if v: vf=v; break\n    if not (chf and mof and mef and vf):\n        return 0.0, 'Flow_Long_TY missing required columns.'\n    total=0; ok=0\n    for _,r in fl.iterrows():\n        C=norm_channel(r.get(chf)); M=norm_month(r.get(mof)); Mt=str(r.get(mef)).strip().lower() if r.get(mef) is not None else ''\n        if C in ['Stores','E-commerce'] and M and 'sale' in Mt:\n            try:\n                V=float(r.get(vf))\n            except Exception:\n                V=np.nan\n            if pd.notna(V):\n                total+=1\n                P=plan.get((C,M))\n                if P is not None and abs(P-V)<=1.0:\n                    ok+=1\n    if total==0:\n        return 0.0, 'No comparable sales rows.'\n    return (ok/total)*1.0, f'Sales match {ok}/{total} rows.'"}, {"type": "code", "name": "Monthly Turn Formula Consistency", "description": "Check Turn (Monthly) = Sales / ((BOM+EOM)/2) per channel/month in Flow_Long_TY. Tolerance: 0.05.", "weight": 0.5, "code": "def evaluate(workflow, context):\n    import pandas as pd, numpy as np\n    def find_sheet(x, names):\n        xl=pd.ExcelFile(x); sh=xl.sheet_names\n        for n in names:\n            for s in sh:\n                if n.lower() in s.lower():\n                    return s\n        return None\n    def norm_channel(s):\n        if s is None: return None\n        t=str(s).strip().lower()\n        if t in ['store','stores','st']: return 'Stores'\n        if t in ['e-commerce','ecommerce','e-comm','ecomm','ecom','e commerce']: return 'E-commerce'\n        if 'omni' in t: return 'Omni'\n        return None\n    def norm_month(s):\n        if s is None: return None\n        t=str(s).strip().lower()\n        m={'aug':'Aug','august':'Aug','sep':'Sep','sept':'Sep','september':'Sep','oct':'Oct','october':'Oct','nov':'Nov','november':'Nov','dec':'Dec','december':'Dec','jan':'Jan','january':'Jan'}\n        return m.get(t)\n    out=context.get_primary_output()\n    if not out or not out.is_spreadsheet:\n        return 0.0, 'No spreadsheet.'\n    x=context.files.get_path(out.id)\n    sh=find_sheet(x,['Flow_Long_TY','Flow Long TY','Normalized Flow (TY)','Flow Long'])\n    if not sh:\n        return 0.0, 'Missing Flow_Long_TY.'\n    df=pd.read_excel(x, sheet_name=sh)\n    def pick(cols,name):\n        for c in cols:\n            if name==c.lower().strip(): return c\n        return None\n    ch=pick(df.columns,'channel'); mo=pick(df.columns,'month'); me=pick(df.columns,'metric'); va=None\n    for nm in ['value','amount','val']:\n        v=pick(df.columns,nm)\n        if v: va=v; break\n    if not (ch and mo and me and va):\n        return 0.0, 'Flow_Long_TY missing required columns.'\n    data={}\n    for _,r in df.iterrows():\n        C=norm_channel(r.get(ch)); M=norm_month(r.get(mo)); Mt=str(r.get(me)).strip().lower() if r.get(me) is not None else ''\n        try:\n            V=float(r.get(va))\n        except Exception:\n            V=np.nan\n        if C and M and pd.notna(V):\n            key=(C,M)\n            if key not in data: data[key]={}\n            if 'bom' in Mt: data[key]['bom']=V\n            elif 'eom' in Mt: data[key]['eom']=V\n            elif 'sale' in Mt: data[key]['sales']=V\n            elif 'turn' in Mt: data[key]['turn']=V\n    total=0; ok=0\n    for k,v in data.items():\n        if all(x in v for x in ['bom','eom','sales','turn']):\n            total+=1\n            avg=(v['bom']+v['eom'])/2.0\n            calc= (v['sales']/avg) if avg!=0 else 0.0\n            if abs(calc - v['turn'])<=0.05:\n                ok+=1\n    if total==0:\n        return 0.0, 'No complete rows for turn check.'\n    return (ok/total)*0.5, f'Monthly Turn matches {ok}/{total} rows.'"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Strategic Soundness (LLM)", "description": "Holistic assessment of clarity, usability, and strategic appropriateness for a Divisional Merchandise Manager audience.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Presentation, Usability, and Strategic Value", "description": "Evaluate professional formatting, clarity of side-by-side views, LY vs TY comparability, methodology clarity, and whether receipts flow choices are strategically reasonable given goals.", "weight": 4.0, "judge_prompt": "Assess the workbook qualitatively on presentation and strategic clarity. Consider:\n\n1) Professional Formatting and Usability\n   - Are the TY Stores and E-commerce flows clearly side-by-side with consistent months, row labels, currency formatting, and readable headings?\n   - Is the Omni roll-up clearly labeled and easy to find?\n   - Are charts or conditional formats (if present) helpful and non-distracting?\n\n2) Comparability\n   - Are LY flows presented with the same layout/rows and adjacent positioning to enable quick comparison?\n   - Are differences vs LY (e.g., turns, inventories) easy to interpret?\n\n3) Methodology and Transparency\n   - Does the 'Methodology' sheet clearly describe the formulas and assumptions (3+ sentences)?\n   - Are the Validation Checks intelligible to a business user (boolean checks, diffs, clear pass/fail)?\n\n4) Strategic Soundness\n   - Do receipts appear phased to support fixed monthly sales while avoiding excess EOM (e.g., heavier before peak months, tapering into January)?\n   - Does the plan appear aligned to the targets (turn >= 4.0 omni, Jan EOM <= $200k) without obviously unrealistic receipts spikes or starving months?\n\nScoring Guidelines:\n- 4.0: Highly professional, easy to navigate; LY/TY aligned for comparison; methodology and checks are clear; receipt phasing appears sensible and aligned to constraints.\n- 3.0: Generally professional with minor usability gaps; LY/TY mostly aligned; methodology adequate; phasing mostly sensible.\n- 2.0: Several formatting or clarity issues; comparisons are hard; methodology thin; phasing plausibility unclear.\n- 0.0\u20131.0: Poorly formatted, confusing, or strategically dubious.\n\nJudge only on quality/usability/strategy impression; do not recheck math correctness.", "expectation": "A business-ready workbook: clean layout, clear TY/LY comparisons, transparent methodology, and strategically phased receipts aligned to goals."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "f9f82549-fdde-4462-aff8-e70fba5b8c66", "rubric": {"category_name": "Retail LP: Employee Theft Investigation Package (Flowchart PDF + Incident PPT)", "rationale": "Pattern B (Document). Deliverables are two files: a PDF flowchart titled/covering \u201cMissing Bank Deposits Investigation\u201d and a PPT deck mapping incident details to each flowchart header. Stage 1 uses LLM-only to enforce the exact structural contract so later verification is trivial. Stage 2 mixes code (deterministic text checks on the PDF) with LLM (PPT mapping and redaction compliance). Stage 3 assesses professional quality and strategic value for LP investigators.", "max_total_score": 15.0, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate (LLM-only)", "description": "Gate: Verify BOTH required deliverables exist with the mandated structure so verification is possible.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.5, "rules": [{"type": "llm_judge", "name": "PDF Flowchart Structural Requirements", "description": "Check presence and structure of the PDF flowchart document.", "weight": 3.0, "judge_prompt": "You are evaluating whether the candidate outputs include a valid PDF flowchart that satisfies these STRUCTURAL requirements (shape only; do not judge content quality):\n\nRequired Deliverable (PDF):\n- Must include a single PDF document that contains a legible flowchart (diagram with nodes and connectors). \n- The first page should clearly present the document title: \u201cMissing Bank Deposits Investigation\u201d (exact or very close variant).\n- The flowchart should be labeled or clearly identified as \u201cLoss Prevention Incident Flowchart\u201d (exact phrase or close variant). This can be a heading near the diagram.\n\nFlowchart structural elements (visual/structural, not correctness):\n- Distinct shapes for start/end and process steps; decision diamonds with yes/no or similar branching; arrows/connectors.\n- Clear major-stage headers/labels aligned to an LP investigation lifecycle. Accept synonyms but require coverage of these concepts:\n  1) Incident Detection/Report\n  2) Initial Containment and Notification\n  3) Evidence Preservation and Data Collection\n  4) Interview Planning/Employee Interviews\n  5) Bank Deposit Irregularities Review (bank drop discrepancies)\n  6) Decision Points (e.g., Suspend Employee, Contact Law Enforcement, HR Involvement)\n  7) Case Documentation and Reporting\n  8) Remediation and Controls (e.g., armored car, dual control, daily reconciliation)\n  9) Communication and Awareness (share learnings to Regions/Districts)\n- A visible note or callout indicating that each flowchart header maps to a corresponding slide in a separate PowerPoint deck (e.g., \u201cSee PPT: one slide per header\u201d).\n\nScoring (0\u20133):\n- 3.0: A PDF is present AND shows a recognizable flowchart with the title on page 1 (or cover) \u2248\u201cMissing Bank Deposits Investigation,\u201d a visible label \u2248\u201cLoss Prevention Incident Flowchart,\u201d AND covers most of the 9 lifecycle concepts with decision branching and connectors AND includes a note that each header maps to a PPT slide.\n- 2.0: PDF flowchart present with title and most structure; minor omissions (e.g., mapping note missing OR 1\u20132 lifecycle concepts missing OR decision diamonds unclear) but overall the shape is sufficient.\n- 1.0: PDF exists but is not clearly a flowchart OR missing multiple core structural elements (no decision branching, no lifecycle labels) OR titles missing.\n- 0.0: No PDF, or not a flowchart at all.\n\nOnly check presence/structure. Do not judge calculation or narrative quality.", "expectation": "A properly titled PDF flowchart with visual nodes, decision diamonds, connectors, and lifecycle headers, plus a note indicating mapping to a PPT deck."}, {"type": "llm_judge", "name": "PPT Incident Details Structural Requirements", "description": "Check presence and structure of the PowerPoint deliverable and its mapping to the flowchart headers.", "weight": 3.0, "judge_prompt": "Check if a PowerPoint deck (PPTX/PPT) is present among candidate outputs and if it structurally maps incident details to the flowchart headers.\n\nRequired Deliverable (PPT):\n- A PowerPoint file exists (PPTX/PPT). \n- For each major header in the PDF flowchart, there is a corresponding slide titled with that header (or a very close variant). A 1:1 mapping is expected.\n\nPer-slide structural expectations (shape, not prose quality):\n- Slide title = the flowchart header (or very close).\n- 3\u20136 concise bullets covering:\n  \u2022 Incident tie-back to the prior bank deposit cash theft case (high-level; no names/store #s).\n  \u2022 What happened/observed relevant to that step.\n  \u2022 Control failure or gap observed (if applicable).\n  \u2022 Corrective action(s) taken in the incident.\n  \u2022 Preventive/standard control recommended going forward.\n- Clear redaction: no personal names or store numbers; roles/titles only.\n\nScoring (0\u20133):\n- 3.0: PPT present; every flowchart header has a slide; each slide has a proper title and 3\u20136 bullets covering the listed elements; redactions observed.\n- 2.0: PPT present; mapping mostly complete (1\u20132 headers missing or merged); bullets largely present; redactions observed.\n- 1.0: PPT present but weak mapping (many headers missing) OR slides lack the expected bullet structure OR redaction unclear.\n- 0.0: No PPT found.\n\nOnly check structural mapping and presence of expected bullet categories; do not assess narrative quality.", "expectation": "A PPT deck with one slide per flowchart header, each slide containing concise bullets tying the prior incident to controls and actions; properly redacted."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Compliance Verification", "description": "Verify titles/keywords in PDF, presence of control remediation themes, dual-file presence, mapping completeness, and redaction compliance.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "PDF Title and Key Phrases Found", "description": "Deterministically verify the PDF includes the specified title and key investigation lifecycle terms.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        # Locate a PDF output\n        pdf_res = None\n        for r in context.get_all_outputs():\n            try:\n                p = str(context.files.get_path(r.id)).lower()\n            except Exception:\n                continue\n            if p.endswith('.pdf'):\n                pdf_res = r\n                break\n        if not pdf_res:\n            return 0.0, 'No PDF output found.'\n\n        try:\n            text = context.files.read_pdf_text(pdf_res.id)\n        except Exception:\n            return 0.0, 'Unable to read PDF text.'\n\n        t = (text or '').lower()\n        total = 0.0\n        notes = []\n\n        # Component A: exact/near title phrase\n        title_ok = 'missing bank deposits investigation' in t\n        total += 0.4 if title_ok else 0.0\n        if not title_ok:\n            notes.append('Missing or altered document title.')\n\n        # Component B: flowchart label presence (flexible)\n        flowchart_ok = ('loss prevention incident flowchart' in t) or (\n            ('incident flowchart' in t and 'loss prevention' in t)\n        )\n        total += 0.2 if flowchart_ok else 0.0\n        if not flowchart_ok:\n            notes.append('Flowchart label not clearly detected.')\n\n        # Component C: lifecycle concept coverage (keyword presence)\n        concepts = [\n            'incident detection', 'detection', 'report', 'containment', 'notification',\n            'evidence', 'data collection', 'interview', 'bank deposit', 'irregularit',\n            'decision', 'law enforcement', 'hr', 'documentation', 'reporting',\n            'remediation', 'controls', 'awareness', 'communication'\n        ]\n        matches = 0\n        for c in concepts:\n            if c in t:\n                matches += 1\n        # Require at least 5 distinct concepts for full 0.4\n        concept_score = min(matches / 5.0, 1.0) * 0.4\n        total += concept_score\n        if concept_score < 0.4:\n            notes.append(f'Lifecycle concepts matched: {matches}.')\n\n        score = max(0.0, min(1.0, total))\n        feedback = f\"Title OK: {title_ok}; Flowchart label OK: {flowchart_ok}; Concept matches: {matches}.\"\n        if notes:\n            feedback += ' Notes: ' + ' '.join(notes)\n        return score, feedback\n    except Exception as e:\n        return 0.0, f'Error: {e}'"}, {"type": "code", "name": "Remediation Controls Present (PDF)", "description": "Check that standard LP control remedies are explicitly mentioned (e.g., armored car, dual control, daily reconciliation).", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        pdf_res = None\n        for r in context.get_all_outputs():\n            try:\n                p = str(context.files.get_path(r.id)).lower()\n            except Exception:\n                continue\n            if p.endswith('.pdf'):\n                pdf_res = r\n                break\n        if not pdf_res:\n            return 0.0, 'No PDF output found.'\n        try:\n            text = context.files.read_pdf_text(pdf_res.id)\n        except Exception:\n            return 0.0, 'Unable to read PDF text.'\n        t = (text or '').lower()\n\n        controls = [\n            'armored car', 'armoured car', 'dual control', 'two-person', 'two person',\n            'segregation of duties', 'separation of duties', 'daily reconciliation',\n            'deposit log', 'tamper-evident', 'timely deposit', 'same-day deposit',\n            'bank drop', 'night drop', 'surprise audit', 'camera', 'cctv',\n            'manager approval', 'transport policy', 'chain of custody'\n        ]\n        found = []\n        for c in controls:\n            if c in t:\n                found.append(c)\n        # Expect at least 4 distinct remedies for full credit\n        score = min(len(set(found)) / 4.0, 1.0)\n        feedback = f\"Found controls: {', '.join(sorted(set(found)))}\" if found else 'No remediation controls detected.'\n        return score, feedback\n    except Exception as e:\n        return 0.0, f'Error: {e}'"}, {"type": "code", "name": "Dual-File Presence Sanity Check", "description": "Confirm at least one PDF and one PPTX/PPT are present among outputs.", "weight": 0.5, "code": "def evaluate(workflow, context):\n    try:\n        pdf_count = 0\n        ppt_count = 0\n        for r in context.get_all_outputs():\n            try:\n                p = str(context.files.get_path(r.id)).lower()\n            except Exception:\n                continue\n            if p.endswith('.pdf'):\n                pdf_count += 1\n            if p.endswith('.pptx') or p.endswith('.ppt'):\n                ppt_count += 1\n        if pdf_count >= 1 and ppt_count >= 1:\n            return 1.0, f'PDFs: {pdf_count}, PPTs: {ppt_count}.'\n        if (pdf_count >= 1) ^ (ppt_count >= 1):\n            return 0.5, f'Only one deliverable type present (PDFs: {pdf_count}, PPTs: {ppt_count}).'\n        return 0.0, 'Neither required deliverable type present.'\n    except Exception as e:\n        return 0.0, f'Error: {e}'"}, {"type": "llm_judge", "name": "PPT\u2194Flowchart Mapping Completeness", "description": "Judge whether PPT slides map 1:1 to the flowchart headers and cover the expected bullet categories.", "weight": 1.5, "judge_prompt": "Using all available outputs (PDF and PPT), check mapping completeness:\n- For each major header in the PDF flowchart, is there a corresponding PPT slide with the same or very close title?\n- Do slides have 3\u20136 bullets covering: incident tie-back, what happened/observed, control failure, corrective actions, preventive/standard control?\n- If headers are merged/split, ensure net coverage is still clear and complete.\n\nScoring (0\u20131.5):\n- 1.5: Complete 1:1 mapping with expected bullets on each slide.\n- 1.0: Minor gaps (1\u20132 headers merged/missing) but overall coverage intact.\n- 0.5: Multiple gaps; partial coverage only.\n- 0.0: No meaningful mapping.\n\nFocus on mapping and presence of elements, not prose quality.", "expectation": "Slides correspond to each flowchart header with expected bullet categories present."}, {"type": "llm_judge", "name": "Redaction and Privacy Compliance", "description": "Verify exclusion of names and store numbers while preserving informative role-based references.", "weight": 1.0, "judge_prompt": "Check both the PDF and PPT for redaction compliance:\n- Prohibited: personal names (first/last), store numbers/IDs (e.g., \u201cStore #1234\u201d), specific employee IDs, addresses identifying a store.\n- Allowed: generic roles/titles (e.g., Store Manager, LP Investigator), district/region level references without unique identifiers.\n\nScoring (0\u20131):\n- 1.0: No names/IDs present; clean redaction throughout.\n- 0.5: Minor lapse (e.g., one stray identifier) but largely compliant.\n- 0.0: Multiple identifiers or clear privacy breaches.\n\nJudge presence/absence only; do not over-penalize for generic descriptors.", "expectation": "No PII or store identifiers appear; only generic roles/titles are used."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Strategic Value", "description": "Holistic assessment of presentation quality, practical usefulness, and audience fit for LP investigators.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation", "description": "Visual clarity and professional formatting of the flowchart and slides.", "weight": 1.5, "judge_prompt": "Evaluate professional presentation:\n- Flowchart: legibility, clear connectors/decision paths, consistent shapes/colors, readable labels.\n- Slides: consistent template, readable fonts, concise bullets, logical sequence.\n- Overall: coherent visual hierarchy, clean spacing/margins.\n\nScoring (0\u20131.5):\n- 1.5: Highly professional and clear.\n- 1.0: Generally professional with minor issues.\n- 0.5: Adequate but with noticeable clarity/formatting issues.\n- 0.0: Poorly formatted/unreadable.", "expectation": "Clean, consistent, and readable flowchart and slides suitable for corporate distribution."}, {"type": "llm_judge", "name": "Strategic Value and Preventive Insight", "description": "Actionability of procedures and preventive guidance to avoid future incidents.", "weight": 1.5, "judge_prompt": "Assess strategic value:\n- Are decision points well-defined (e.g., when to contact HR, Legal, Law Enforcement, and the bank)?\n- Are preventive controls clearly articulated (e.g., armored car service, dual control, daily reconciliation, deposit timeliness, chain-of-custody)?\n- Are follow-up actions and awareness plans specified for Regions/Districts?\n\nScoring (0\u20131.5):\n- 1.5: Clear decision criteria, robust controls, and actionable plans.\n- 1.0: Mostly actionable with minor gaps.\n- 0.5: Limited actionable guidance.\n- 0.0: Not useful strategically.", "expectation": "Concrete decision criteria and preventive controls that can be operationalized by LP teams."}, {"type": "llm_judge", "name": "Audience Appropriateness (LP Investigators)", "description": "Tone, specificity, and practicality for LP investigators across Regions/Districts.", "weight": 1.0, "judge_prompt": "Evaluate audience alignment:\n- Tone: professional, neutral, investigative, avoids blame.\n- Specificity: sufficiently detailed to guide LP investigators without local store identifiers; includes timelines (e.g., same-day deposits), documentation standards, and escalation protocols.\n- Shareability: suitable as an awareness tool across Regions/Districts.\n\nScoring (0\u20131):\n- 1.0: Well-targeted and practical for LP investigators.\n- 0.5: Partially aligned; some gaps in specificity or tone.\n- 0.0: Poorly aligned with the intended audience.", "expectation": "Appropriate tone and detail for LP investigators; readily usable as an awareness and prevention tool."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "2696757c-1f8a-4959-8f0d-f5597b9e70fc", "rubric": {"category_name": "VA Servicing Purchase \u2013 Bankruptcy Testing Template (Regulatory Testing Items)", "rationale": "This rubric enforces a self-documenting deliverable for a compliance document. Stage 1 is a strict LLM-judged shape gate requiring a single PDF with the mandated header and two test items (each with a test question, immediate citation, and an exception statement). Stage 2 uses code rules to verify key textual elements, citations, proximity, and regulatory tone now that the structure exists. Stage 3 provides a light LLM quality assessment for professional tone and usability for operational risk testing.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (PDF and Required Sections)", "description": "Confirm the candidate produced a single PDF document with the exact required structure, including header, two test items tied to the specified VA Handbook paragraphs, and exception statements.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Document Format and Structural Completeness (MANDATORY)", "description": "Verify the output is a single PDF with the exact required structure for two test items.", "weight": 4.0, "judge_prompt": "You are evaluating whether the submitted deliverable meets the REQUIRED structure. Only check structure and presence, not quality or legal correctness.\n\nRequirements:\n1) File/Format:\n   - Output must be a single PDF document (not DOCX, not plain text, not Excel).\n\n2) Header (must be visible near the top of the first page):\n   - Exact header text (flexible to minor dash variants): \"VA Servicing Purchase \u2013 Bankruptcy Testing Template\".\n   - Be flexible with the dash (en/em/ascii hyphen), but the words must match in order.\n\n3) Two Required Test Items (one for each specified paragraph):\n   For EACH of the following paragraphs, the document must contain ALL of these elements:\n   - A clearly phrased Test Question related to the paragraph.\n   - The citation for that paragraph immediately following the question text (e.g., on the same line or right after the question), referencing the exact paragraph identifier.\n   - An Exception Statement (a narrative, regulatory tone) corresponding to that question.\n\n   Required paragraphs:\n   - 9.07(a)(2)(a)\n   - 9.08(c)(3)\n\n4) Layout flexibility:\n   - It is acceptable if the content is in paragraph form without tables.\n   - Section labels like \"Test Question\" and \"Exception Statement\" are preferred but not mandatory if the structure is unambiguous (i.e., you can clearly identify the question, the immediate citation, and the exception statement for each paragraph).\n\nScoring (structure only):\n- 4.0: Single PDF present AND header present near top AND BOTH paragraph items have (a) a question, (b) the citation immediately after the question, and (c) an exception statement.\n- 3.0: Single PDF present AND header present AND BOTH paragraph items are present but one item is missing a minor structural element (e.g., label missing yet still unambiguously present), OR citation placement is slightly unclear but present close to the question.\n- 2.0: Single PDF present AND header present BUT ONLY ONE paragraph item is fully present (question + immediate citation + exception statement) OR both items are present but both are missing one structural element each.\n- 1.0: Single PDF present but header missing OR both paragraph items are incomplete with major structural gaps.\n- 0.0: Not a PDF, or content missing/insufficient to identify the two required items.\n\nOnly evaluate the structural presence and arrangement. Do NOT judge the correctness of legal content or the quality of writing.", "expectation": "A single PDF that begins with the specified header and contains two test items, one for paragraph 9.07(a)(2)(a) and one for paragraph 9.08(c)(3). Each item includes a test question with the citation immediately after, plus a corresponding exception statement."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Content Checks via Code)", "description": "Deterministic checks for presence of key identifiers, proximity of citations to questions, exception statement count and tone, and context alignment. Assumes Stage 1 structure exists.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Single PDF Output and Header Presence", "description": "Confirm the primary output is a single PDF and the mandated header appears near the top.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs()\n        if not outputs:\n            return 0.0, \"No outputs found.\"\n        # Single primary output check\n        primary = context.get_primary_output()\n        if primary is None:\n            return 0.0, \"No primary output.\"\n        # Check one output only (soft check)\n        single_ok = 1 if len(outputs) == 1 else 0\n        # Must be a PDF document\n        try:\n            path = context.files.get_path(primary.id)\n            is_pdf = 1 if str(path).lower().endswith('.pdf') else 0\n        except Exception:\n            is_pdf = 0\n        # Read text\n        try:\n            text = context.files.read_pdf_text(primary.id)\n        except Exception:\n            text = \"\"\n        # Normalize dashes and case\n        norm = text.lower()\n        norm = norm.replace('\\u2013', '-').replace('\\u2014', '-').replace('\\u2212', '-').replace('\u2013', '-').replace('\u2014', '-')\n        header = \"va servicing purchase - bankruptcy testing template\"\n        idx = norm.find(header)\n        header_ok = 1 if (idx != -1 and idx < 600) else 0\n        # Score equally among checks\n        checks = [single_ok, is_pdf, header_ok]\n        score = (sum(checks) / 3) * 1.0\n        feedback = f\"Single:{single_ok} PDF:{is_pdf} Header:{header_ok}\"\n        return score, feedback\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Required Paragraph Identifiers Present", "description": "Verify that both 9.07(a)(2)(a) and 9.08(c)(3) identifiers appear in the PDF text (allowing flexible whitespace).", "weight": 1.2, "code": "import re\n\ndef _has_pattern(text, pattern):\n    return re.search(pattern, text, flags=re.IGNORECASE) is not None\n\ndef evaluate(workflow, context):\n    primary = context.get_primary_output()\n    if not primary:\n        return 0.0, \"No output.\"\n    try:\n        text = context.files.read_pdf_text(primary.id)\n    except Exception:\n        return 0.0, \"Cannot read PDF text.\"\n    # Flexible regex for paragraph IDs\n    p1 = r\"9\\.07\\s*\\(\\s*a\\s*\\)\\s*\\(\\s*2\\s*\\)\\s*\\(\\s*a\\s*\\)\"\n    p2 = r\"9\\.08\\s*\\(\\s*c\\s*\\)\\s*\\(\\s*3\\s*\\)\"\n    h1 = 1 if _has_pattern(text, p1) else 0\n    h2 = 1 if _has_pattern(text, p2) else 0\n    score = (h1 + h2) / 2 * 1.2\n    return score, f\"9.07(a)(2)(a):{h1} 9.08(c)(3):{h2}\""}, {"type": "code", "name": "Exception Statements Present with Regulatory Tone", "description": "Check that there are at least two 'Exception Statement' occurrences and that regulatory tone keywords are present.", "weight": 1.4, "code": "import re\n\ndef evaluate(workflow, context):\n    primary = context.get_primary_output()\n    if not primary:\n        return 0.0, \"No output.\"\n    try:\n        text = context.files.read_pdf_text(primary.id)\n    except Exception:\n        return 0.0, \"Cannot read PDF text.\"\n    low = text.lower()\n    count_ex = len(re.findall(r\"exception\\s*statement\", low))\n    ex_ok = 1 if count_ex >= 2 else 0\n    tone_keywords = [\n        \"failed to\", \"did not\", \"non-compliance\", \"noncompliance\", \"not in compliance\",\n        \"in violation\", \"deficient\", \"breach\", \"non-compliant\", \"non compliant\"\n    ]\n    tone_hits = sum(1 for k in tone_keywords if k in low)\n    tone_ok = 1 if tone_hits >= 2 else 0\n    # Half credit for presence of two exception statements, half for tone\n    base = (ex_ok + tone_ok) / 2\n    score = base * 1.4\n    return score, f\"ExceptionStatements>=2:{ex_ok} ToneKeywords>=2:{tone_ok}\""}, {"type": "code", "name": "Citation Immediately Follows Question (Proximity Check)", "description": "Ensure each paragraph citation appears shortly after a question mark, indicating it follows the question text.", "weight": 0.8, "code": "import re\n\ndef _prox_after_question(text, id_pattern, window=200):\n    # Find all question marks and check for id pattern within window after\n    for m in re.finditer(r\"\\?\", text):\n        start = m.end()\n        end = min(len(text), start + window)\n        if re.search(id_pattern, text[start:end], flags=re.IGNORECASE):\n            return True\n    return False\n\ndef evaluate(workflow, context):\n    primary = context.get_primary_output()\n    if not primary:\n        return 0.0, \"No output.\"\n    try:\n        text = context.files.read_pdf_text(primary.id)\n    except Exception:\n        return 0.0, \"Cannot read PDF text.\"\n    p1 = r\"9\\.07\\s*\\(\\s*a\\s*\\)\\s*\\(\\s*2\\s*\\)\\s*\\(\\s*a\\s*\\)\"\n    p2 = r\"9\\.08\\s*\\(\\s*c\\s*\\)\\s*\\(\\s*3\\s*\\)\"\n    ok1 = 1 if _prox_after_question(text, p1, 250) else 0\n    ok2 = 1 if _prox_after_question(text, p2, 250) else 0\n    score = ((ok1 + ok2) / 2) * 0.8\n    return score, f\"Proximity 9.07(a)(2)(a):{ok1} 9.08(c)(3):{ok2}\""}, {"type": "code", "name": "Context Alignment: Bankruptcy, VASP/VA Servicing Purchase, and Handbook/Chapter Mention", "description": "Check the document mentions bankruptcy context, the VA Servicing Purchase program (or VASP), and the VA Servicer Handbook/Chapter 9.", "weight": 0.6, "code": "def evaluate(workflow, context):\n    primary = context.get_primary_output()\n    if not primary:\n        return 0.0, \"No output.\"\n    try:\n        text = context.files.read_pdf_text(primary.id)\n    except Exception:\n        return 0.0, \"Cannot read PDF text.\"\n    low = text.lower()\n    bk = 1 if (\"bankruptcy\" in low) else 0\n    vasp = 1 if (\"va servicing purchase\" in low or \"vasp\" in low) else 0\n    handbook = 1 if (\"m26-4\" in low or \"va servicer handbook\" in low) else 0\n    chap9 = 1 if (\"chapter 9\" in low) else 0\n    # award for any handbook mention + chapter 9\n    handbook_ok = 1 if (handbook and chap9) else 0\n    total_checks = bk + vasp + handbook_ok\n    score = (total_checks / 3) * 0.6\n    return score, f\"Bankruptcy:{bk} VASP:{vasp} Handbook+Ch9:{handbook_ok}\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality Assessment (LLM)", "description": "Holistic evaluation of professional quality, clarity, and suitability for operational risk testing use.", "is_required": false, "max_points": 1.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Quality and Usability", "description": "Assess whether the document is professionally written, clear, and suitable for compliance testing use, with appropriately crafted exception statements.", "weight": 1.0, "judge_prompt": "Evaluate the overall quality of the PDF deliverable (quality only; structure was already assessed in Stage 1). Consider:\n- Clarity and precision of the two test questions for regulatory testing purposes.\n- Whether each Exception Statement uses clear regulatory tone and explicitly articulates how non-compliance would occur with reference to the cited paragraph (without judging legal correctness in depth).\n- Readability and professional presentation suitable for compliance officers.\n- Citations placed immediately after questions (do not re-check structure rigidly; consider clarity and readability of citation placement).\n\nScoring:\n- 1.0: Clear, professional, actionable for testing; exception statements read as precise regulatory narratives.\n- 0.7: Generally clear and usable with minor issues (wordiness, minor ambiguity, tone slightly uneven).\n- 0.4: Understandable but not ready for use without edits; exception statements vague or generic.\n- 0.0: Poor quality or confusing; would not be usable in operational testing.\n", "expectation": "A concise, professional PDF with two clearly phrased test questions and strong, regulatory-tone exception statements that compliance officers can use directly."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "fe0d3941-e32c-4bf1-a643-b566d2b4cb3c", "rubric": {"category_name": "Workflows Slides and Market Survey (Non-invasive Blood Analysis)", "rationale": "Pattern C (Mixed) simplified to document-centric outputs: a slide deck (PowerPoint/PDF) plus a survey PDF. Stage 1 uses an LLM gate to enforce strict structural shape for whichever file is the primary output (slides or survey). Stage 2 uses code rules to deterministically verify both deliverables exist across all outputs and meet key structural constraints (counts, titles, sections). Stage 3 uses LLM judging for professional quality and strategic clarity of the primary deliverable.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate (LLM-only)", "description": "Gate ensures the primary output is a valid, well-structured deliverable. It must be EITHER the slide deck titled \u201cWorkflows\u201d with required sections OR the survey PDF titled \u201cInstant non-invasive blood analysis\u201d with the required two-section layout. This enables downstream verification.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Primary Output Structured Deliverable Requirement", "description": "Verify that the primary output file is a properly structured document. Acceptable cases: (A) Slide deck (PPTX or PDF of slides) titled \u201cWorkflows\u201d with specific slides; OR (B) Survey PDF titled \u201cInstant non-invasive blood analysis\u201d with two separate sections/pages for physicians and non-physicians.", "weight": 6.0, "judge_prompt": "You are evaluating the PRIMARY output file only. Determine which of the two cases it is and score accordingly. Be flexible with exact phrasing but strict about structure.\n\nACCEPTABLE CASE A \u2014 Slide Deck (PowerPoint .pptx or PDF of slides):\nFormat and Structure Requirements:\n- File appears to be a slide presentation (PPTX or a PDF exported from slides)\n- Must include a visible title slide with the word \u201cWorkflows\u201d in the title\n- Must include ALL of these required slides/sections (headers can vary slightly):\n  1) Current Lab Workflow (steps of today\u2019s process)\n  2) Proposed Non-Invasive Workflow (future process)\n  3) Legend (brief legend for workflow symbols/notation)\n  4) Benefits in Diagnosis (how the new tech improves diagnosis)\n- Optional: a picture/visual on the title slide\n- Minimum 4 distinct slides covering the required sections (title may be combined with a required section if clearly visible)\n\nScoring for Case A:\n- 6.0: Valid slide deck + all 4 required slides/sections clearly labeled/present\n- 5.0: All required present but minor labeling/placement issues OR missing optional picture only\n- 3.0: Missing exactly one required slide/section\n- 0.0: Not a slide deck format OR missing multiple required sections\n\nACCEPTABLE CASE B \u2014 Survey PDF:\nFormat and Structure Requirements:\n- Must be a PDF document\n- Title on the first page: \u201cInstant non-invasive blood analysis\u201d (exact phrase or trivially close)\n- Two separate sections/pages with clear headings:\n  1) \u201cQuestions for physicians\u201d (5\u20137 questions)\n  2) \u201cQuestions for non-physicians\u201d (3\u20135 questions)\n- All questions phrased to be answerable with Yes/No (e.g., start with Do/Is/Are/Should/Would/Will/Can/Have, etc.)\n\nScoring for Case B:\n- 6.0: Valid PDF + correct title + both sections present with correct question counts and clear Yes/No phrasing\n- 4.0: Valid PDF + correct title + both sections present but one section off by 1 question OR a few questions not clearly Yes/No\n- 2.0: Valid PDF but only one section present or title missing\n- 0.0: Not PDF OR structurally not a two-section Yes/No survey\n\nIF THE FILE DOES NOT MATCH EITHER CASE A OR B: Score 0.0.\n\nOnly assess presence/format/structure, not content quality or correctness.", "expectation": "Primary output must be a well-structured slide deck titled \u201cWorkflows\u201d with required sections OR a properly structured two-section Yes/No survey PDF titled \u201cInstant non-invasive blood analysis.\u201d"}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Deterministic checks across all outputs)", "description": "Programmatic verification that BOTH deliverables exist across the workflow outputs and satisfy key structural constraints made possible by Stage 1\u2019s shape. Uses flexible heuristics to handle variations.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Both Deliverables Present Across Outputs", "description": "Checks that the workflow produced BOTH: (1) a slide deck (PPTX or a PDF of slides) and (2) a survey PDF with both physician and non-physician sections.", "weight": 2.5, "code": "import re\nfrom pathlib import Path\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    outputs = context.get_all_outputs() or []\n    if not outputs:\n        return 0.0, \"No outputs found.\"\n\n    def get_path(res):\n        try:\n            return context.files.get_path(res.id)\n        except Exception:\n            return None\n\n    def read_pdf_text_safe(res):\n        try:\n            return (context.files.read_pdf_text(res.id) or \"\").lower()\n        except Exception:\n            return \"\"\n\n    has_survey = False\n    has_deck = False\n    survey_hits = []\n    deck_hits = []\n\n    # Patterns\n    pat_phys = re.compile(r\"questions?\\s+for\\s+physicians?\", re.I)\n    pat_nonphys = re.compile(r\"questions?\\s+for\\s+non-?physicians?\", re.I)\n    pat_title = re.compile(r\"instant\\s+non\\s*-?\\s*invasive\\s+blood\\s+analysis\", re.I)\n\n    for res in outputs:\n        p = get_path(res)\n        if not p:\n            continue\n        suffix = p.suffix.lower()\n\n        # Survey PDF detection\n        if suffix == \".pdf\":\n            txt = read_pdf_text_safe(res)\n            if txt:\n                if pat_title.search(txt) and pat_phys.search(txt) and pat_nonphys.search(txt):\n                    has_survey = True\n                    survey_hits.append(p.name)\n\n        # Slide deck detection (PPTX or Slide-PDF)\n        if suffix == \".pptx\":\n            has_deck = True\n            deck_hits.append(p.name)\n        elif suffix == \".pdf\":\n            txt = read_pdf_text_safe(res)\n            if txt:\n                # Heuristic: slide-like if mentions workflows and 2+ key sections\n                keys = 0\n                if \"workflows\" in txt: keys += 1\n                if \"current\" in txt: keys += 1\n                if \"proposed\" in txt: keys += 1\n                if \"legend\" in txt: keys += 1\n                if \"benefit\" in txt: keys += 1\n                if \"diagnos\" in txt: keys += 1\n                if keys >= 3:\n                    has_deck = True\n                    deck_hits.append(p.name)\n\n    score = 0.0\n    if has_survey and has_deck:\n        score = 2.5\n        fb = f\"Found survey in: {survey_hits}; deck in: {deck_hits}.\"\n    elif has_survey or has_deck:\n        score = 1.25\n        which = \"survey only\" if has_survey else \"deck only\"\n        fb = f\"Only {which} found. Survey: {survey_hits}; Deck: {deck_hits}.\"\n    else:\n        fb = \"Neither survey PDF nor slide deck detected.\"\n\n    return score, fb"}, {"type": "code", "name": "Survey Structure: Title, Sections, Counts, Yes/No Suitability", "description": "If a survey PDF exists, verify it has the correct title, two sections (physicians and non-physicians), appropriate question counts, and predominantly Yes/No style phrasing.", "weight": 3.0, "code": "import re\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs() or []\n\n    def read_pdf_text_safe(res):\n        try:\n            return (context.files.read_pdf_text(res.id) or \"\")\n        except Exception:\n            return \"\"\n\n    pat_title = re.compile(r\"instant\\s+non\\s*-?\\s*invasive\\s+blood\\s+analysis\", re.I)\n    pat_phys = re.compile(r\"questions?\\s+for\\s+physicians?\", re.I)\n    pat_nonphys = re.compile(r\"questions?\\s+for\\s+non-?physicians?\", re.I)\n\n    survey_text = None\n    for res in outputs:\n        try:\n            path = context.files.get_path(res.id)\n        except Exception:\n            continue\n        if path.suffix.lower() != \".pdf\":\n            continue\n        txt = read_pdf_text_safe(res)\n        if not txt:\n            continue\n        if pat_title.search(txt) and pat_phys.search(txt) and pat_nonphys.search(txt):\n            survey_text = txt\n            break\n\n    if not survey_text:\n        return 0.0, \"Survey PDF with expected title/sections not found.\"\n\n    # Normalize and locate sections\n    text = survey_text\n    lower = text.lower()\n    m_phys = re.search(pat_phys, text)\n    m_non = re.search(pat_nonphys, text)\n\n    # Define section slices (robust to ordering)\n    phys_section = \"\"\n    non_section = \"\"\n    if m_phys and m_non:\n        if m_phys.start() < m_non.start():\n            phys_section = text[m_phys.end():m_non.start()]\n            non_section = text[m_non.end():]\n        else:\n            non_section = text[m_non.end():m_phys.start()]\n            phys_section = text[m_phys.end():]\n    elif m_phys:\n        phys_section = text[m_phys.end():]\n    elif m_non:\n        non_section = text[m_non.end():]\n\n    def count_questions(seg):\n        lines = [ln.strip() for ln in seg.splitlines() if ln.strip()]\n        q_lines = [ln for ln in lines if \"?\" in ln]\n        return q_lines, len(q_lines)\n\n    phys_q_lines, phys_n = count_questions(phys_section)\n    non_q_lines, non_n = count_questions(non_section)\n\n    # Yes/No suitability heuristic\n    yn_starts = re.compile(r\"^(do|does|did|is|are|was|were|should|would|will|can|could|have|has|may|might)\\b\", re.I)\n    banned = re.compile(r\"(rate|scale|why|how\\s+much|describe|select|choose|multiple|open\\s*ended)\", re.I)\n\n    def yn_ratio(lines):\n        if not lines:\n            return 0.0\n        good = 0\n        total = 0\n        for ln in lines:\n            total += 1\n            if yn_starts.search(ln) and not banned.search(ln):\n                good += 1\n        return good / max(total, 1)\n\n    phys_ratio = yn_ratio(phys_q_lines)\n    non_ratio = yn_ratio(non_q_lines)\n\n    # Scoring components\n    title_ok = 1.0 if pat_title.search(text) else 0.0\n\n    def count_score(n, lo, hi):\n        if n == 0:\n            return 0.0\n        if lo <= n <= hi:\n            return 1.0\n        if (n == lo - 1) or (n == hi + 1):\n            return 0.5\n        return 0.2\n\n    phys_count_score = count_score(phys_n, 5, 7)\n    non_count_score = count_score(non_n, 3, 5)\n\n    yn_score = (phys_ratio + non_ratio) / 2.0  # 0..1\n\n    # Weighted aggregation: title 0.3, counts 0.4 (0.2 each), yes/no 0.3\n    total_ratio = 0.3 * title_ok + 0.2 * phys_count_score + 0.2 * non_count_score + 0.3 * yn_score\n    score = 3.0 * max(0.0, min(1.0, total_ratio))\n\n    fb = (\n        f\"Title: {'OK' if title_ok else 'Missing'}; \"\n        f\"Phys Qs: {phys_n} (count_score={phys_count_score:.2f}, yn={phys_ratio:.2f}); \"\n        f\"Non-phys Qs: {non_n} (count_score={non_count_score:.2f}, yn={non_ratio:.2f}).\"\n    )\n    return score, fb"}, {"type": "code", "name": "Workflow Deck Structure Hints (Title/Sections)", "description": "If a slide deck exists (PDF or PPTX), verify presence of key sections. Full parsing only if PDF; PPTX presence yields partial credit.", "weight": 2.5, "code": "def evaluate(workflow, context):\n    outputs = context.get_all_outputs() or []\n\n    def read_pdf_text_safe(res):\n        try:\n            return (context.files.read_pdf_text(res.id) or \"\").lower()\n        except Exception:\n            return \"\"\n\n    deck_pdf_text = None\n    has_pptx = False\n    pdf_names = []\n    pptx_names = []\n\n    for res in outputs:\n        try:\n            p = context.files.get_path(res.id)\n        except Exception:\n            continue\n        suf = (p.suffix or \"\").lower()\n        if suf == \".pptx\":\n            has_pptx = True\n            pptx_names.append(p.name)\n        elif suf == \".pdf\":\n            txt = read_pdf_text_safe(res)\n            if txt and (\"workflows\" in txt) and (\"current\" in txt or \"proposed\" in txt or \"legend\" in txt or \"benefit\" in txt):\n                deck_pdf_text = txt\n                pdf_names.append(p.name)\n\n    if deck_pdf_text:\n        txt = deck_pdf_text\n        checks = {\n            \"title_workflows\": (\"workflows\" in txt),\n            \"current\": (\"current\" in txt),\n            \"proposed\": (\"proposed\" in txt),\n            \"legend\": (\"legend\" in txt),\n            \"benefits\": (\"benefit\" in txt),\n            \"diagnosis\": (\"diagnos\" in txt),\n        }\n        n_ok = sum(1 for v in checks.values() if v)\n        ratio = min(1.0, n_ok / 5.0)  # require ~5 hits for full credit\n        score = 2.5 * ratio\n        fb = f\"Deck PDF found ({pdf_names}). Section hits: {checks}.\"\n        return score, fb\n\n    if has_pptx:\n        # PPTX present but not parsed -> partial credit acknowledging presence\n        return 1.5, f\"PPTX deck detected ({pptx_names}). PDF text not available for deeper checks.\"\n\n    return 0.0, \"No slide deck detected (PPTX or slide-like PDF).\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality Assessment (LLM)", "description": "Holistic evaluation of professional presentation and strategic clarity for the PRIMARY output (slide deck or survey).", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Readability", "description": "Assess visual/structural polish for the primary file only (slide deck or survey PDF).", "weight": 3.0, "judge_prompt": "Evaluate the PRIMARY output\u2019s professional quality. Apply the relevant criteria based on the file type you see.\n\nIF SLIDE DECK (PPTX or slide-like PDF):\n- Visual clarity: clean layout, readable fonts, consistent styles/colors\n- Workflow communication: steps are legible and ordered; arrows/flow are understandable\n- Benefits and legend slides: easy to read; legend clearly explains symbols\n- Overall slide craft: spacing, alignment, minimal clutter\nScoring:\n- 3.0: Highly professional and clear across all aspects\n- 2.0: Generally professional; minor issues (e.g., clutter, inconsistent styles)\n- 1.0: Understandable but rough formatting or confusing layout\n- 0.0: Poorly formatted or unreadable\n\nIF SURVEY PDF:\n- Structure: clear title, two distinct sections/pages with correct headings\n- Readability: numbered/bulleted questions, spacing for responses, consistent formatting\n- Clarity: questions are concise and unambiguous (appear answerable Yes/No)\nScoring:\n- 3.0: Clean, professional, highly readable\n- 2.0: Generally clear with minor formatting or clarity issues\n- 1.0: Understandable but messy or inconsistent\n- 0.0: Hard to read or disorganized", "expectation": "Polished, readable deliverable appropriate for professional stakeholders."}, {"type": "llm_judge", "name": "Strategic Clarity and Audience Alignment", "description": "Assess whether the primary file clearly communicates value and suitability for intended audiences (physicians and consumers).", "weight": 3.0, "judge_prompt": "Evaluate the PRIMARY output for strategic clarity and audience alignment. Use the relevant criteria below based on what the file is.\n\nIF SLIDE DECK:\n- Do the workflows clearly illustrate time-to-result improvements vs. current lab process?\n- Do the benefits link to clinical relevance (diagnostic speed, patient comfort) and market relevance (adoption potential)?\n- Is the legend adequate for a non-technical stakeholder to follow the workflows?\n- Overall: Would this help decision-makers understand the opportunity and rationale?\nScoring:\n- 3.0: Strong, clear strategic narrative for stakeholders\n- 2.0: Adequate narrative with some gaps\n- 1.0: Minimal/unclear strategic messaging\n- 0.0: Not strategic or confusing\n\nIF SURVEY PDF:\n- Physicians\u2019 questions cover usefulness, need, reliability, acceptance, instant results\n- Non-physicians\u2019 questions cover usefulness and willingness to pay\n- Questions avoid leading/biasing phrasing\n- Overall: Would these responses inform clinical acceptance and market interest?\nScoring:\n- 3.0: Strong coverage and appropriateness\n- 2.0: Mostly covered with some gaps\n- 1.0: Limited relevance or bias issues\n- 0.0: Not aligned with objectives", "expectation": "Clear linkage between problem, solution, and stakeholder value; survey questions map to clinical acceptance and willingness to pay."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "a45bc83b-22f9-4def-8d89-9c5661b2b86f", "rubric": {"category_name": "GCP Migration & Modernization Architecture Proposal and POC", "rationale": "Three-stage, self-documenting rubric. Stage 1 is a strict LLM-only gate enforcing exact deliverable shapes (DOCX + PDF with specific structures) to make verification trivial. Stage 2 mixes code and LLM checks to validate correctness and cross-document consistency, leveraging text extraction for DOCX/PDF and multimodal reasoning for the diagram. Stage 3 uses LLM for holistic quality and stakeholder readiness.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 - Shape Enforcement Gate (Deliverables & Structure)", "description": "LLM-only gate verifying deliverables exist with the exact required formats and structural elements to enable verification.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured DOCX: Proposed Architecture Summary", "description": "Check for a DOCX containing a bulleted architecture summary that mirrors the style of a bulleted current-architecture summary. Must contain named sections and bullet structure.", "weight": 1.0, "judge_prompt": "You are checking ONLY structure/format. Do not assess technical accuracy.\n\nTarget deliverable: A Word document (DOCX) titled like \"Proposed GCP Architecture Summary\" or similar, that mirrors a bulleted summary style.\n\nVerify the following:\n1) File format: It is a DOCX (not PDF/Excel/plain text).\n2) Bulleted structure: The content is primarily bullet points (1\u20132 nesting levels). No large paragraphs.\n3) Required section headers are visible (flexible naming allowed):\n   - Data Flow\n   - Core GCP Services/Components\n   - Networking (VPC/Subnets/DNS)\n   - Security (Cloud Armor/IAM/Encryption)\n   - Availability & Scalability (MIG/GKE autoscaling, multi-zone/regional LB)\n   - Static Content Hosting (Cloud Storage + CDN)\n4) Minimum density: At least ~12 total bullet points across sections.\n5) Professional formatting: Clear title/header, consistent bullet style.\n\nScoring:\n- 1.0: Valid DOCX + clear bullet structure + all required sections present + >=12 bullets total.\n- 0.7: Valid DOCX + bulleted, but missing 1\u20132 required sections OR only ~8\u201311 bullets.\n- 0.4: Valid DOCX but mostly paragraphs or sparse bullets (<8) OR missing 3+ sections.\n- 0.0: Not a DOCX or missing entirely.\n\nReturn a score in [0,1] based only on structure/presence.", "expectation": "A bulleted DOCX with clearly labeled sections covering data flow, core services, networking, security, availability/scalability, and static hosting."}, {"type": "llm_judge", "name": "Structured DOCX: POC Plan with Step-by-Step Instructions", "description": "Check for a DOCX containing a POC plan with numbered steps, sections, and purpose per step where needed.", "weight": 1.0, "judge_prompt": "You are checking ONLY structure/format. Do not assess technical accuracy.\n\nTarget deliverable: A Word document (DOCX) titled like \"Proof of Concept (POC) Plan\" or similar.\n\nVerify the following:\n1) File format: It is a DOCX.\n2) Section structure (flexible names allowed):\n   - Objective & Scope\n   - Prerequisites\n   - Environment Setup\n   - Networking & Security\n   - Data/Database Setup\n   - Application Deployment\n   - Static Content Hosting\n   - Load Balancing/CDN/DDoS\n   - Monitoring & Logging\n   - Validation & Success Criteria\n   - Rollback\n   - Cleanup\n3) Numbered step-by-step instructions: At least 10 discrete, numbered steps (may be spread across sections). Steps should use imperative verbs.\n4) Purpose/clarification: Each step includes a brief \u201cPurpose\u201d or equivalent note if not obvious (can be inline notes or sub-bullets).\n5) Professional formatting: Clear headings, numbered lists, and consistent styling.\n\nScoring:\n- 1.0: Valid DOCX + sections present (allow up to 2 minor variations) + >=10 numbered steps + most steps include purpose/clarification.\n- 0.7: Valid DOCX + numbered steps but missing 3\u20134 sections or only ~7\u20139 steps or purpose/notes sporadic.\n- 0.4: Valid DOCX but minimal structure (<=6 steps) and/or largely missing sections.\n- 0.0: Not a DOCX or missing entirely.\n\nReturn a score in [0,1] based only on structure/presence.", "expectation": "A DOCX POC plan with clear sections and at least 10 numbered steps, each with purpose/notes when not obvious."}, {"type": "llm_judge", "name": "Structured PDF: Proposed Architecture Diagram (GCP Icons)", "description": "Check for a PDF diagram using official GCP icons with a clear end-to-end data flow, consistent with the requested style.", "weight": 1.0, "judge_prompt": "You are checking ONLY structure/format. Do not assess technical accuracy of choices beyond presence.\n\nTarget deliverable: A PDF with a proposed architecture diagram using official GCP icons and a style similar to a current-architecture diagram.\n\nVerify the following:\n1) File format: It is a PDF.\n2) Visual diagram present (not just text), using recognizable official GCP icons (e.g., Cloud Load Balancer, Cloud CDN, Cloud Armor, Cloud Storage, GKE/Compute Engine, Cloud SQL/Firestore, VPC/Subnets, Cloud DNS).\n3) Clear end-to-end data flow: From DNS/clients through LB/CDN/Armor to web/app tier and data tier; static content served via Cloud Storage (+ CDN/backend bucket). Arrows/labels present.\n4) Professional layout: Legible labels, consistent iconography, optional legend/title.\n\nScoring:\n- 1.0: Valid PDF + GCP-icon diagram + all major layers/components present and labeled.\n- 0.7: Valid PDF + diagram with GCP icons but missing 1\u20132 major components or labels.\n- 0.4: Valid PDF but generic blocks/no recognizable GCP icons.\n- 0.0: Not a PDF or missing diagram.\n\nReturn a score in [0,1] based only on structure/presence.", "expectation": "A PDF diagram with official GCP icons showing DNS\u2192LB/CDN/Armor\u2192web/app\u2192DB, plus static content via Cloud Storage + CDN."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 - Correctness and Consistency Verification", "description": "Validate that the deliverables collectively satisfy core requirements and are internally consistent. Mix of code-based text checks and LLM cross-referencing.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Coverage of Required GCP Capabilities", "description": "Checks that the documents reference key services/capabilities: Load Balancing, DDoS/Armor, CDN, DNS, Static hosting via Cloud Storage, Compute tier (GKE/CE/MIG/Run/App Engine), Data tier (Cloud SQL/Firestore/Spanner), VPC/Subnets, HA/Autoscaling, and security basics (IAM/encryption/private access). Scores by fraction covered.", "weight": 2.5, "code": "def evaluate(workflow, context):\n    import re\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0, \"No outputs.\"\n    texts = []\n    for res in outputs:\n        try:\n            rid = getattr(res, 'id', None)\n            name = str(rid).lower() if rid else ''\n            if getattr(res, 'is_document', False):\n                if name.endswith('.docx'):\n                    t = context.files.read_docx_text(rid)\n                    if t:\n                        texts.append(t)\n                elif name.endswith('.pdf'):\n                    t = context.files.read_pdf_text(rid)\n                    if t:\n                        texts.append(t)\n        except Exception:\n            pass\n    if not texts:\n        return 0.0, \"No readable document text.\"\n    corpus = (\"\\n\".join(texts)).lower()\n\n    categories = [\n        (\"load_balancing\", [r\"cloud [a-z/() ]*load balanc\", r\"\\bload[- ]balanc\"]),\n        (\"ddos_protection\", [r\"cloud armor\", r\"\\bddos\\b\", r\"\\bwaf\\b\"]),\n        (\"cdn\", [r\"cloud cdn\"]),\n        (\"dns\", [r\"cloud dns\"]),\n        (\"static_content\", [r\"cloud storage\", r\"backend bucket\", r\"static (site|content|assets|files)\", r\"bucket (website|hosting)\"]),\n        (\"compute_tier\", [r\"\\b(gke|kubernetes engine)\\b\", r\"compute engine\", r\"managed instance group\", r\"\\bmig\\b\", r\"cloud run\", r\"app engine\"]),\n        (\"data_tier\", [r\"cloud sql\", r\"firestore\", r\"spanner\"]),\n        (\"networking\", [r\"\\bvpc\\b\", r\"subnet\"]),\n        (\"availability_scaling\", [r\"multi[- ](zone|regional|region)\", r\"\\bregional\\b\", r\"autoscal\", r\"replica\", r\"health check\"]),\n        (\"security_basics\", [r\"\\biam\\b\", r\"least privilege\", r\"encrypt\", r\"\\bkms\\b\", r\"private (ip|service connect|service access)\"]),\n    ]\n\n    found = []\n    missing = []\n    for label, pats in categories:\n        hit = any(re.search(p, corpus) for p in pats)\n        if hit:\n            found.append(label)\n        else:\n            missing.append(label)\n    score = len(found) / len(categories)\n    feedback = f\"Found: {found}. Missing: {missing}.\"\n    return max(0.0, min(1.0, score)), feedback\n"}, {"type": "code", "name": "POC Step-by-Step Structure and Purpose Checks", "description": "Verifies the POC DOCX includes \u226510 numbered steps and that steps include purpose/clarifications, plus presence of common POC sections.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    import re\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0, \"No outputs.\"\n    poc_texts = []\n    docs = []\n    for res in outputs:\n        try:\n            rid = getattr(res, 'id', None)\n            name = str(rid).lower() if rid else ''\n            if getattr(res, 'is_document', False) and name.endswith('.docx'):\n                t = context.files.read_docx_text(rid) or ''\n                docs.append((rid, t))\n        except Exception:\n            pass\n    if not docs:\n        return 0.0, \"No DOCX found.\"\n\n    def step_count(txt):\n        # Count lines like: 1. / 1) / Step 1\n        c1 = len(re.findall(r\"(?mi)^[ \\t]*\\d+[\\).\\-: ]\", txt))\n        c2 = len(re.findall(r\"(?mi)\\bstep\\s*\\d+\\b\", txt))\n        return max(c1, c2)\n\n    # Choose the most likely POC doc: contains 'poc' or 'proof of concept' or most steps\n    best = None\n    best_score = -1\n    for rid, t in docs:\n        base = 0\n        if re.search(r\"\\bpoc\\b|proof of concept\", t, re.I):\n            base += 5\n        sc = step_count(t)\n        score = base + sc\n        if score > best_score:\n            best_score = score\n            best = (rid, t, sc)\n\n    if not best:\n        return 0.0, \"No POC-like DOCX identified.\"\n\n    rid, text, steps = best\n    purpose_hits = len(re.findall(r\"(?i)\\bpurpose\\b|\\bwhy\\b\", text))\n    sections = [\n        r\"objective\", r\"scope\", r\"prerequisite\", r\"network\", r\"security\",\n        r\"database|data\", r\"deployment\", r\"static\", r\"load balanc|cdn|armor\",\n        r\"monitor|logging\", r\"validation|success criteria\", r\"rollback\", r\"cleanup\"\n    ]\n    sec_hits = sum(1 for s in sections if re.search(s, text, re.I))\n\n    # Scoring: steps coverage is primary; then ensure purposes and section coverage\n    base = min(1.0, steps / 10.0)  # full at 10 steps\n    if purpose_hits == 0:\n        base -= 0.2\n    if sec_hits < 8:\n        base -= 0.2\n    score = max(0.0, min(1.0, base))\n    fb = f\"Detected steps={steps}, purpose_mentions={purpose_hits}, section_hits={sec_hits}.\"\n    return score, fb\n"}, {"type": "llm_judge", "name": "Diagram \u2194 Documents Consistency (Components & Data Flow)", "description": "The PDF diagram should align with the two DOCX documents: same major components and data flow pathways; static content hosting and security elements appear in both.", "weight": 1.0, "judge_prompt": "Evaluate cross-artifact consistency. You have: (a) an architecture summary DOCX, (b) a POC DOCX, and (c) a PDF diagram. Check alignment on:\n- Major components: Cloud DNS, External HTTP(S) Load Balancing, Cloud CDN, Cloud Armor, web/app tier (GKE, Compute Engine MIG, Cloud Run, or App Engine), data tier (Cloud SQL/Firestore/Spanner), VPC/Subnets, Cloud Storage for static content.\n- Data flow: client\u2192DNS\u2192LB/CDN/Armor\u2192web/app\u2192DB; static path via Storage/CDN.\n- Terminology: names of services and tiers are consistent across all three deliverables.\n- Any critical item in text (e.g., Cloud Armor, Static Hosting) should be visible on the diagram.\n\nScoring:\n- 1.0: Diagram and documents clearly reflect the same services and flows with consistent naming.\n- 0.5: Mostly consistent; minor mismatches or 1 missing element.\n- 0.0: Significant inconsistencies or diagram does not reflect core elements described in text.\n\nOnly evaluate consistency/presence, not deep technical merit.", "expectation": "All three artifacts tell the same story with matching GCP components and flows."}, {"type": "llm_judge", "name": "Security & Availability Sufficiency (Plausibility)", "description": "Evaluate if the solution plausibly meets enterprise security and availability: segmentation, IAM, encryption, private access, logging/monitoring, backups/DR, multi-zone/regional HA, health checks, autoscaling.", "weight": 0.5, "judge_prompt": "Judge whether the proposed architecture and POC plausibly meet enterprise-grade security and availability requirements based on presence and completeness (not perfect correctness):\n\nLook for:\n- Security: IAM/least privilege, VPC segmentation (subnets/firewalls), Cloud Armor/WAF, TLS/SSL, encryption at rest (CMEK/KMS optional), secrets management, private IP/private service access for databases, audit logging.\n- Availability: Regional external HTTP(S) load balancer, multi-zone replicas or MIG/GKE across zones, health checks, autoscaling, backups/replicas, rollout/rollback strategy.\n- DDoS: Layer 3/4 protection via Google edge + LB; Cloud Armor for L7; mention of DDoS mitigations acceptable.\n\nScoring:\n- 1.0: Most items above are explicitly addressed in the docs and diagram.\n- 0.5: Some are addressed but notable gaps remain.\n- 0.0: Few/no enterprise security/HA considerations are evident.\n\nOnly evaluate plausibility and presence, not exact configurations.", "expectation": "Solution references IAM, VPC segmentation, Armor/TLS/encryption, private access, logging, backups; HA across zones with health checks and autoscaling."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 - Professional Quality and Customer Readiness", "description": "Holistic assessment of writing quality, presentation polish, and stakeholder readiness.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Clarity", "description": "Assess clarity, organization, and professionalism across both DOCX files and the diagram.", "weight": 1.0, "judge_prompt": "Evaluate overall communication quality across the two DOCX documents and the PDF diagram:\n- Clarity: Concise, unambiguous wording; well-structured bullets and steps; clear labels on diagram.\n- Professional formatting: Consistent headings, numbering, styles; legible diagram layout.\n- References: Links or mentions of relevant official docs where appropriate.\n\nScoring:\n- 1.0: Highly professional, clear, and well-structured across all artifacts.\n- 0.5: Generally clear but with several formatting/clarity issues.\n- 0.0: Poorly formatted or hard to follow.\n\nDo not re-check structure from Stage 1; focus on polish and clarity.", "expectation": "Customer-ready, clean formatting, easy to follow structure, and clear visuals."}, {"type": "llm_judge", "name": "Stakeholder Readiness and Strategic Value", "description": "Evaluate whether the proposal and POC equip stakeholders to proceed: feasibility, risks/assumptions, validation, and alignment to requirements.", "weight": 1.0, "judge_prompt": "Assess if the artifacts prepare customer stakeholders for next steps:\n- Feasibility and alignment: Choices reflect scalability, security, availability, static hosting, and DDoS requirements.\n- Practical readiness: POC includes validation and success criteria, rollback/cleanup, and prerequisites.\n- Risk and assumptions: Mentions risks, constraints, or assumptions where relevant; suggests mitigations.\n\nScoring:\n- 1.0: Strong alignment and readiness with clear success criteria and practical next steps.\n- 0.5: Partial readiness; some gaps in validation/assumptions or unclear next steps.\n- 0.0: Not actionable or misaligned with requirements.", "expectation": "A feasible path with clear validation, risks/assumptions, and next steps aligned to the stated requirements."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "afe56d05-dac8-47d7-a233-ad1d035ca5bd", "rubric": {"category_name": "WorldCast Special Reporting Guidance (Editors)", "rationale": "This rubric enforces a self-documenting, verifiable Word-document deliverable with explicit section headers and checklists so that verification is trivial. Stage 1 (LLM-only) strictly gates structural format and required sections. Stage 2 (code rules) runs deterministic checks enabled by the mandated shape (word count, title/intro note, section coverage, external links). Stage 3 (LLM) evaluates professional quality and usability, not correctness.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Format and Structural Gate (LLM only)", "description": "Gate: Verify the candidate produced a properly structured Word document with all required sections, explicit Do/Don't guidance, an opening note, and working hyperlinks.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.2, "rules": [{"type": "llm_judge", "name": "Document Format and Structure Requirements", "description": "Check presence of required structure in a DOCX (preferred) or PDF. Do NOT judge content quality or factual correctness\u2014only the presence of the mandated shape.", "weight": 4.0, "judge_prompt": "You are verifying ONLY structure/format (not content quality). Inspect the candidate\u2019s primary output.\n\nAcceptable format: Word DOCX (preferred) or PDF. Be flexible with minor naming variations and British/American spelling. Do not penalize for small phrasing differences as long as intent is clear.\n\nRequired STRUCTURE (all must be visible in the rendered document):\n1) File type: DOCX (preferred) or PDF. Award full credit for DOCX; allow partial credit for PDF.\n2) Title: On page 1, a prominent title closely matching \u201cWorldCast GUIDANCE ON SPECIAL REPORTING SITUATIONS.\u201d (Minor variations like capitalization/plurals acceptable.)\n3) Opening note: The first section must include a short note encouraging journalists to raise any issues not covered with their supervisors and to continue discussing journalistic concerns with colleagues and supervisors. This should appear near the start (page 1).\n4) Required section headers (as top-level sections or clear subheadings). Flexible on exact wording, but each must be clearly identifiable:\n   - Conflicts and combat activities\n   - Terrorism\n   - Hostage and barricade situations (variants like \u201chostage situations\u201d and \u201cbarricade incidents\u201d acceptable)\n   - Protests/demonstrations (variants like \u201cprotests and demonstrations\u201d acceptable)\n   - Criminal activity (variants like \u201ccrime reporting\u201d acceptable)\n   - Victims\n   - Mass shootings (variants like \u201cmass violence/shootings\u201d acceptable)\n   - Suicides (variants like \u201csuicide reporting\u201d acceptable)\n   - Identifying and interviewing minors (variants like \u201cchildren/minors\u201d acceptable)\n5) Each of the nine sections contains explicit, scannable Do and Don\u2019t guidance. Accept labels such as \u201cDo,\u201d \u201cRecommended,\u201d \u201cShould,\u201d \u201cBest practices\u201d for Do; and \u201cDon\u2019t,\u201d \u201cAvoid,\u201d \u201cRefrain,\u201d \u201cNever,\u201d \u201cMust not\u201d for Don\u2019t. Bullet lists or numbered lists are acceptable.\n6) Hyperlinks: At least 3 total hyperlinks, including:\n   - One link to the SPJ Code of Ethics (spj.org)\n   - At least two links from among: thenewsmanual.net, icfj.org, poynter.org, rcfp.org, unicef.org\n7) Length: Approximately 2,200\u20132,300 words. Be flexible and accept roughly 2,000\u20132,600 as structurally compliant. Do not nitpick exact count here.\n\nScoring (STRUCTURE ONLY):\n- 1.0: DOCX present + title + opening note + all 9 section headers + each section has Do/Don\u2019t lists + hyperlinks requirement met + length roughly 2,000\u20132,600.\n- 0.8: PDF instead of DOCX OR missing one minor structural element (e.g., opening note slightly buried but present, or one section\u2019s Do/Don\u2019t formatting is unclear) AND the rest of structure is present.\n- 0.5: Missing 1\u20132 sections OR Do/Don\u2019t lists absent/unclear in multiple sections OR hyperlinks partially missing.\n- 0.0: Not a document (neither DOCX nor PDF) OR fewer than 7 sections OR no visible checklists OR essentially empty.\n\nIMPORTANT: Only check presence/format, not the correctness of advice or tone. Return a score in [0,1] per the rubric above.", "expectation": "A properly structured DOCX with title, opening note, 9 clearly labeled sections with Do/Don\u2019t checklists, and compliant hyperlinks; roughly target length."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Deterministic Checks)", "description": "Code-based checks that leverage the mandated structure to verify key requirements: word count band, title and opening note presence, section coverage, and external links to specified domains.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Word Count Band", "description": "Reward closeness to the 2,200\u20132,300 target, with partial credit for 2,100\u20132,399 and 1,900\u20132,500. Robust to DOCX/PDF.", "weight": 1.2, "code": "import re\\n\\n\\ndef evaluate(workflow, context):\\n    \\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0, \\\"No document output found.\\\"\\n\\n    text = None\\n    err = []\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception as e:\\n        err.append(f\\\"DOCX read failed: {e}\\\")\\n    if not text:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception as e:\\n            err.append(f\\\"PDF read failed: {e}\\\")\\n    if not text:\\n        return 0.0, \\\"Unable to extract text from document. \\\" + \\\" | \".join(err)\\n\\n    words = re.findall(r\\\"\\\\b\\w+\\\\b\\\", text)\\n    wc = len(words)\\n\\n    # Scoring bands: exact target best, nearby good, broader range some, far range tiny\\n    if 2200 <= wc <= 2300:\\n        ratio = 1.0\\n    elif 2100 <= wc <= 2399:\\n        ratio = 0.7\\n    elif 1900 <= wc <= 2500:\\n        ratio = 0.4\\n    elif 1700 <= wc <= 2700:\\n        ratio = 0.2\\n    else:\\n        ratio = 0.0\\n\\n    feedback = f\\\"Word count: {wc}.\\\"\\n    return max(0.0, min(1.0, ratio)), feedback\\n"}, {"type": "code", "name": "Title and Opening Note Presence", "description": "Check title match near top and an opening note encouraging escalation to supervisors and ongoing discussion with colleagues/supervisors.", "weight": 0.8, "code": "import re\\n\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0, \\\"No document output found.\\\"\\n\\n    text = None\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            return 0.0, \\\"Unable to extract text from document.\\\"\\n\\n    # Focus on the beginning for title and opening note\\n    head = \\\" \\\".join(text.split()[:350])\\n    head_l = head.lower()\\n\\n    # Title detection: look for 'worldcast' and 'guidance on special reporting situations' near the start\\n    title_ok = (\\\"worldcast\\\" in head_l) and (\\\"guidance on special reporting situations\\\" in head_l or (\\\"guidance\\\" in head_l and \\\"special reporting\\\" in head_l))\\n\\n    # Opening note detection: look for patterns about raising issues, supervisors, and continued discussion\\n    cues_any_issue = any(k in head_l for k in [\\\"raise any issues\\\", \\\"issues not covered\\\", \\\"not covered in this guide\\\"])\\n    cues_supervisor = any(k in head_l for k in [\\\"supervisor\\\", \\\"editor\\\", \\\"managing editor\\\"])\\n    cues_discuss = any(k in head_l for k in [\\\"continue discussing\\\", \\\"keep discussing\\\", \\\"ongoing discussion\\\", \\\"discuss with colleagues\\\", \\\"colleagues\\\"])\\n    opening_ok = (cues_supervisor and cues_discuss) or (cues_any_issue and (cues_supervisor or cues_discuss))\\n\\n    score = 0.0\\n    if title_ok:\\n        score += 0.5\\n    if opening_ok:\\n        score += 0.5\\n\\n    feedback = f\\\"Title_ok={title_ok}, Opening_note_ok={opening_ok}.\\\"\\n    return max(0.0, min(1.0, score)), feedback\\n"}, {"type": "code", "name": "Required Sections Coverage", "description": "Fuzzy match presence of the 9 required sections using synonyms/variants.", "weight": 1.2, "code": "import re\\n\\nSECTIONS = {\\n    \\\"conflicts and combat activities\\\": [\\\"conflicts and combat\\\", \\\"conflict reporting\\\", \\\"war reporting\\\", \\\"combat\\\"],\\n    \\\"terrorism\\\": [\\\"terrorism\\\", \\\"terrorist\\\"],\\n    \\\"hostage and barricade situations\\\": [\\\"hostage\\\", \\\"barricade\\\", \\\"hostage situation\\\", \\\"barricade situation\\\"],\\n    \\\"protests/demonstrations\\\": [\\\"protests\\\", \\\"demonstrations\\\", \\\"protest\\\", \\\"demonstration\\\"],\\n    \\\"criminal activity\\\": [\\\"criminal activity\\\", \\\"crime reporting\\\", \\\"crime\\\"],\\n    \\\"victims\\\": [\\\"victims\\\", \\\"victim care\\\"],\\n    \\\"mass shootings\\\": [\\\"mass shooting\\\", \\\"mass shootings\\\", \\\"mass violence\\\"],\\n    \\\"suicides\\\": [\\\"suicides\\\", \\\"suicide reporting\\\", \\\"suicide\\\"],\\n    \\\"identifying and interviewing minors\\\": [\\\"minors\\\", \\\"children\\\", \\\"youth\\\", \\\"interviewing minors\\\", \\\"interviewing children\\\"]\\n}\\n\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0, \\\"No document output found.\\\"\\n\\n    text = None\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            return 0.0, \\\"Unable to extract text from document.\\\"\\n\\n    tl = text.lower()\\n    found = 0\\n    missing = []\\n    for canon, variants in SECTIONS.items():\\n        present = any(v in tl for v in variants)\\n        if present:\\n            found += 1\\n        else:\\n            missing.append(canon)\n\\n    ratio = found / 9.0\\n    feedback = f\\\"Sections found: {found}/9. Missing: {', '.join(missing)}\\\"\\n    return max(0.0, min(1.0, ratio)), feedback\\n"}, {"type": "code", "name": "External Links to SPJ and Reputable Sources", "description": "Detect hyperlinks and reward presence of SPJ plus at least two other specified domains (thenewsmanual.net, icfj.org, poynter.org, rcfp.org, unicef.org).", "weight": 0.8, "code": "import re\\n\\nOTHER_DOMAINS = {\\\"thenewsmanual.net\\\", \\\"icfj.org\\\", \\\"poynter.org\\\", \\\"rcfp.org\\\", \\\"unicef.org\\\"}\\n\\n\\ndef _get_domain(url: str) -> str:\\n    u = url.strip()\\n    # Remove trailing punctuation\\n    u = re.sub(r\\\"[)\\\"'.,;:]+$\\\", \\\"\\\", u)\\n    # Strip protocol\\n    u = re.sub(r\\\"^https?://\\\", \\\"\\\", u, flags=re.I)\\n    # Strip www.\\n    if u.startswith(\\\"www.\\\"):\\n        u = u[4:]\\n    return u.split('/')[0].lower()\\n\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0, \\\"No document output found.\\\"\\n\\n    text = None\\n    try:\\n        text = context.files.read_docx_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            return 0.0, \\\"Unable to extract text from document.\\\"\\n\\n    urls = re.findall(r\\\"https?://[^\\s)]+\\\", text)\\n    domains = {_get_domain(u) for u in urls}\\n\\n    has_spj = any(d.endswith(\\\"spj.org\\\") for d in domains)\\n    other_count = sum(1 for d in domains if any(d.endswith(x) for x in OTHER_DOMAINS))\\n\\n    # Scoring: 0.5 for SPJ presence, 0.5 for >=2 other specified domains (0.25 if only 1)\n    score = 0.0\\n    if has_spj:\\n        score += 0.5\\n    if other_count >= 2:\\n        score += 0.5\\n    elif other_count == 1:\\n        score += 0.25\\n\\n    feedback = f\\\"Links found: {len(urls)}; Domains: {sorted(list(domains))}; SPJ={has_spj}; Other_count={other_count}.\\\"\\n    return max(0.0, min(1.0, score)), feedback\\n"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Editorial Fitness (LLM)", "description": "Holistic editorial assessment: clarity, neutrality, professionalism, and practical usability for WorldCast journalists.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Editorial Quality and Neutrality", "description": "Assess tone, neutrality, and professionalism for an internal newsroom guide. Avoids sensationalism/loaded language; advice is clear and responsible.", "weight": 1.0, "judge_prompt": "Evaluate the guide\u2019s editorial quality for internal newsroom use. Consider:\n- Neutral, non-sensational tone aligned with a \u201cLeast Biased, High Factual\u201d outlet\n- Clear, practical, safety-conscious guidance (concise bullets/checklists)\n- Avoids graphic detail, speculation, and stigmatizing language (esp. for suicides, victims, minors)\n- Consistent terminology and formatting that support quick reference under pressure\n\nScoring:\n- 1.0: Professional, neutral, clearly actionable, safety- and harm-minimizing throughout\n- 0.6: Generally professional with minor lapses (e.g., a few loaded phrases or vague items)\n- 0.3: Noticeable issues (several vague or potentially biased/sensational passages)\n- 0.0: Unprofessional or clearly at odds with neutral, harm-minimizing standards", "expectation": "A professional, neutral, actionable guide suitable for quick reference by reporters."}, {"type": "llm_judge", "name": "Usability and Referencing", "description": "Assess whether structure aids rapid use and references are helpful and properly linked.", "weight": 1.0, "judge_prompt": "Evaluate how usable and well-referenced the document is.\n- Headers and lists enable quick scanning under deadline pressure\n- Do/Don\u2019t items are specific and operational, not generic platitudes\n- Hyperlinks are meaningful (SPJ and other reputable sources) and placed where relevant\n- Cross-references (if any) are sensible (e.g., minors + victims overlap)\n\nScoring:\n- 1.0: Highly scannable, specific, with meaningful links\n- 0.6: Mostly usable; some items generic or links minimally helpful\n- 0.3: Hard to scan; guidance vague; links perfunctory\n- 0.0: Poor usability; references absent or unhelpful", "expectation": "A scannable, checklist-oriented layout with purposeful references that improve practice."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "7151c60a-d4cb-4fc4-8169-3d4cb446e6b9", "rubric": {"category_name": "Healthcare Admin Artifacts \u2014 Dialysis Transfer Packet (Fax Cover + Pre\u2011Screen Checklist)", "rationale": "This rubric follows the self-documenting, staged approach. Stage 1 is an LLM-only format/shape gate that mandates two separate documents (fax cover sheet and pre-screening checklist) with explicit structural elements that make verification trivial. Stage 2 uses code rules to verify key required text elements and compliance statements based on flexible string matching across the DOCX/PDF outputs. Stage 3 is an LLM quality assessment judging professional presentation and appropriateness for administrative and regulatory use.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Structured Document Requirements Gate (LLM-only)", "description": "Gate: Verify the candidate produced TWO separate documents with the exact required structure so subsequent verification is possible. Do NOT assess calculation/content quality\u2014only presence/format/layout/sections.", "is_required": true, "max_points": 5.0, "min_score_to_pass": 4.0, "rules": [{"type": "llm_judge", "name": "Two-Document Shape, File Type, and Structural Completeness", "description": "Check both required documents exist as separate files, are Word/PDF, obey page limits, include required sections/fields, logos, and checklist/table/footers so that code checks in Stage 2 can succeed.", "weight": 5.0, "judge_prompt": "You are the Stage 1 gatekeeper. Inspect ALL outputs. You must verify the candidate produced TWO separate documents with the exact structure below. Be flexible with wording but strict about presence and layout. Only evaluate structure/format/presence\u2014NOT the substantive quality of content.\n\nRequired Deliverables (two separate files):\n1) Fax Cover Sheet\n   - Format: DOCX (preferred) or PDF\n   - Length: Exactly 1 page\n   - Clear title indicating it is a \u201cFax Cover Sheet\u201d (or close variant)\n   - Company logo visible (an actual image; allow reasonable placeholder like company wordmark if clearly used as a logo)\n   - Sender section with fields/room for: Sender Name, Sender Fax Number, Sender Telephone Number\n   - Recipient section with fields/room for: Recipient Name, Recipient Fax Number, Recipient Telephone Number\n   - Fields/room for: Date, Subject, and Number of pages (including cover sheet)\n   - Prominent selectable options for: Urgent, For Review, Please Comment, Please Reply (checkboxes or similar selection indicators acceptable)\n   - Confidentiality Statement included as a distinct section/paragraph (do not judge exact wording; only presence and clear labeling/placement)\n\n2) Facility Admission: Pre\u2011Screening Checklist\n   - Format: DOCX (preferred) or PDF\n   - Length: No more than 2 pages total\n   - Page numbers present in the footer\n   - Clear title: \u201cFacility Admission: Pre\u2011Screening Checklist\u201d (or very close variant)\n   - Company logo visible on the document\n   - Above the main checklist table on EACH page: space/fields to enter Patient\u2019s Name and Date of Birth\n   - Main content in TABLE format (grid of items/columns). The table must allow documentation of: Date Sent, Date Received, and Initials (columns or clearly labeled fields)\n   - A clear note near/above the table indicating: \u201cDate Received and Initials are completed by Internal Dialysis Facility Staff Only\u201d (flexible wording, but meaning must be unambiguous)\n   - The following instruction block included somewhere clearly visible (flexible formatting, but all elements must be present):\n     \u2022 \u201cPlease fax or send the information requested to Fax #: (000) 111-1234 or Email: Sunny@Sunnydialysisclinic.com.\u201d\n     \u2022 \u201cPlease include your preferred method of contact with the requested documents.\u201d\n     \u2022 \u201cOur clinical team will review within 48 hours of receiving ALL required documents and notification of the facility\u2019s decision will be sent to the preferred method of contact provided.\u201d\n   - Checklist content should be comprehensive for dialysis transfer. Since the source list is not provided, ensure the table includes multiple typical items such as: patient demographics/ID, dialysis prescription & schedule, vascular access type/status, nephrologist orders, medication list, allergies, recent labs (including Hepatitis B status/dates), vaccinations, care plan, problem list, advance directives, insurance/authorization, emergency contacts, previous facility info, recent treatment records (e.g., pre/post weights), dialysis modality. Be flexible with exact names but look for coverage across many of these categories.\n\nScoring (Structure/Format Only):\n- 5.0: Two separate files; correct formats; page limits met; logo present in both; all required sections/fields/options present in the fax cover sheet; checklist has title, patient name/DOB on each page above table, table structure with Date Sent/Date Received/Initials, note limiting Date Received & Initials to internal staff only, instruction block with fax/email/preferred contact/48 hours lines; comprehensive set of checklist items present.\n- 4.0\u20134.9: Two separate files and correct formats; page limits met; only a minor omission (e.g., one checkbox label slightly different; minor wording variance in the internal-only note; small number of checklist categories missing) but structure clearly supports verification.\n- 2.0\u20133.9: Major structural gaps (e.g., missing logo on one document; missing required field groups; no table in the checklist; missing the instruction block; patient info not above table on each page), but still recognizably attempted both documents.\n- 0.0\u20131.9: Not two separate files; wrong file type(s); egregious omissions; page limits violated; overall structure not suitable for verification.\n\nImportant: Be flexible on exact phrasing, but strict on presence and layout. Do not judge quality or correctness beyond structural presence.", "expectation": "Both documents exist separately, are in DOCX/PDF, meet page limits, display logo, and contain all structurally required fields/sections so later verification is trivial."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Code + Light LLM where needed)", "description": "Deterministic checks for core required text elements and compliance language using flexible string matching across the produced DOCX/PDF files.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Fax Cover Sheet \u2014 Core Fields Present", "description": "Verify the fax cover sheet contains labels/space for sender/recipient details, date, subject, and number of pages.", "weight": 1.2, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs()\n    except Exception:\n        outputs = []\n    if not outputs:\n        return 0.0, \"No outputs.\"\n\n    docs = [r for r in outputs if getattr(r, 'is_document', False) or getattr(r, 'is_text_format', False)]\n    if not docs:\n        return 0.0, \"No document outputs.\"\n\n    def read_text(res):\n        for reader in (context.files.read_docx_text, context.files.read_pdf_text, context.files.read_text):\n            try:\n                t = reader(res.id)\n                if t:\n                    return t\n            except Exception:\n                continue\n        return \"\"\n\n    fax_text = \"\"\n    for r in docs:\n        t = (read_text(r) or \"\").lower()\n        if not t:\n            continue\n        if \"fax cover\" in t or re.search(r'\\bfax\\b', t):\n            # prefer document that clearly looks like a cover sheet\n            if \"cover\" in t or \"cover sheet\" in t:\n                fax_text = t\n                break\n            fax_text = t\n    if not fax_text:\n        return 0.0, \"Could not locate fax cover sheet text.\"\n\n    checks = {\n        'sender': bool(re.search(r'\\b(sender|from)\\b', fax_text)),\n        'recipient': bool(re.search(r'\\b(recipient|to)\\b', fax_text)),\n        'fax': bool(re.search(r'\\bfax\\b', fax_text)),\n        'telephone': bool(re.search(r'\\b(telephone|phone|tel)\\b', fax_text)),\n        'subject': bool(re.search(r'\\bsubject\\b', fax_text)),\n        'date': bool(re.search(r'\\bdate\\b', fax_text)),\n        'pages': bool(re.search(r'(number of pages|total pages|pages including cover|pages including cover sheet|no\\.\\s*of\\s*pages|pages\\s*:\\s*)', fax_text)),\n    }\n\n    score = sum(checks.values()) / len(checks) * 1.2\n    missing = [k for k, v in checks.items() if not v]\n    fb = \"Missing: \" + \", \".join(missing) if missing else \"All core fields found.\"\n    return score, fb"}, {"type": "code", "name": "Fax Cover Sheet \u2014 Action Options Present", "description": "Verify presence of the four selectable labels: Urgent, For Review, Please Comment, Please Reply.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs()\n    except Exception:\n        outputs = []\n    if not outputs:\n        return 0.0\n\n    docs = [r for r in outputs if getattr(r,'is_document', False) or getattr(r,'is_text_format', False)]\n    if not docs:\n        return 0.0\n\n    def read_text(res):\n        for reader in (context.files.read_docx_text, context.files.read_pdf_text, context.files.read_text):\n            try:\n                t = reader(res.id)\n                if t:\n                    return t\n            except Exception:\n                continue\n        return \"\"\n\n    fax_text = \"\"\n    for r in docs:\n        t = (read_text(r) or \"\").lower()\n        if not t:\n            continue\n        if \"fax cover\" in t or re.search(r'\\bfax\\b', t):\n            if \"cover\" in t or \"cover sheet\" in t:\n                fax_text = t\n                break\n            fax_text = t\n    if not fax_text:\n        return 0.0\n\n    options = [\"urgent\", \"for review\", \"please comment\", \"please reply\"]\n    found = sum(1 for o in options if o in fax_text)\n    return (found/len(options)) * 0.4"}, {"type": "code", "name": "Fax Cover Sheet \u2014 Confidentiality Statement Present", "description": "Verify that a confidentiality statement/section exists on the fax cover sheet.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs()\n    except Exception:\n        outputs = []\n    if not outputs:\n        return 0.0\n\n    docs = [r for r in outputs if getattr(r,'is_document', False) or getattr(r,'is_text_format', False)]\n    if not docs:\n        return 0.0\n\n    def read_text(res):\n        for reader in (context.files.read_docx_text, context.files.read_pdf_text, context.files.read_text):\n            try:\n                t = reader(res.id)\n                if t:\n                    return t\n            except Exception:\n                continue\n        return \"\"\n\n    fax_text = \"\"\n    for r in docs:\n        t = (read_text(r) or \"\").lower()\n        if not t:\n            continue\n        if (\"fax cover\" in t or re.search(r'\\bfax\\b', t)) and (\"confidential\" in t or \"confidentiality\" in t):\n            fax_text = t\n            break\n    if not fax_text:\n        # Try second pass in any fax-like doc\n        for r in docs:\n            t = (read_text(r) or \"\").lower()\n            if (\"fax cover\" in t or re.search(r'\\bfax\\b', t)):\n                fax_text = t\n                break\n    if not fax_text:\n        return 0.0\n\n    has_conf = bool(re.search(r'\\b(confidential|confidentiality)\\b', fax_text))\n    return 0.4 if has_conf else 0.0"}, {"type": "code", "name": "Checklist \u2014 Title, Patient Info, and Internal-Only Note", "description": "Verify the checklist has proper title, patient name/DOB fields, and the internal-staff-only note regarding Date Received and Initials.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs()\n    except Exception:\n        outputs = []\n    if not outputs:\n        return 0.0, \"No outputs.\"\n\n    docs = [r for r in outputs if getattr(r, 'is_document', False) or getattr(r, 'is_text_format', False)]\n    if not docs:\n        return 0.0, \"No document outputs.\"\n\n    def read_text(res):\n        for reader in (context.files.read_docx_text, context.files.read_pdf_text, context.files.read_text):\n            try:\n                t = reader(res.id)\n                if t:\n                    return t\n            except Exception:\n                continue\n        return \"\"\n\n    chk_text = \"\"\n    for r in docs:\n        t = (read_text(r) or \"\").lower()\n        if not t:\n            continue\n        if (\"pre-screen\" in t or \"prescreen\" in t or \"checklist\" in t) and (\"facility admission\" in t or \"admission\" in t):\n            chk_text = t\n            break\n    if not chk_text:\n        return 0.0, \"Checklist document not found.\"\n\n    title_ok = (\"checklist\" in chk_text) and (\"pre\" in chk_text) and (\"admission\" in chk_text)\n    name_ok = bool(re.search(r'(patient\\s*name|patient[\u2019\\']s\\s*name)', chk_text))\n    dob_ok = bool(re.search(r'(date\\s*of\\s*birth|\\bdob\\b)', chk_text))\n    internal_only_ok = bool(re.search(r'internal\\s+dialysis\\s+facility\\s+staff\\s+only', chk_text))\n\n    found = [title_ok, name_ok, dob_ok, internal_only_ok]\n    score = sum(found)/4 * 0.6\n    missing = []\n    if not title_ok: missing.append('title')\n    if not name_ok: missing.append('patient name')\n    if not dob_ok: missing.append('DOB')\n    if not internal_only_ok: missing.append('internal-only note')\n    fb = \"Missing: \" + \", \".join(missing) if missing else \"All checklist header elements found.\"\n    return score, fb"}, {"type": "code", "name": "Checklist \u2014 Column Labels (Date Sent, Date Received, Initials)", "description": "Verify the checklist includes the three required column cues.", "weight": 0.2, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs()\n    except Exception:\n        outputs = []\n    if not outputs:\n        return 0.0\n\n    docs = [r for r in outputs if getattr(r,'is_document', False) or getattr(r,'is_text_format', False)]\n    if not docs:\n        return 0.0\n\n    def read_text(res):\n        for reader in (context.files.read_docx_text, context.files.read_pdf_text, context.files.read_text):\n            try:\n                t = reader(res.id)\n                if t:\n                    return t\n            except Exception:\n                continue\n        return \"\"\n\n    chk_text = \"\"\n    for r in docs:\n        t = (read_text(r) or \"\").lower()\n        if not t:\n            continue\n        if (\"pre-screen\" in t or \"prescreen\" in t or \"checklist\" in t) and (\"admission\" in t or \"facility admission\" in t):\n            chk_text = t\n            break\n    if not chk_text:\n        return 0.0\n\n    checks = [\n        bool(re.search(r'date\\s*sent', chk_text)),\n        bool(re.search(r'date\\s*received', chk_text)),\n        bool(re.search(r'\\binitials\\b', chk_text)),\n    ]\n    return (sum(checks)/3) * 0.2"}, {"type": "code", "name": "Checklist \u2014 Contact + SLA Text Present", "description": "Verify the instruction block with fax number, email, preferred contact method, and 48-hour review SLA is present somewhere in the checklist.", "weight": 0.2, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs()\n    except Exception:\n        outputs = []\n    if not outputs:\n        return 0.0, \"No outputs.\"\n\n    docs = [r for r in outputs if getattr(r,'is_document', False) or getattr(r,'is_text_format', False)]\n    if not docs:\n        return 0.0, \"No documents.\"\n\n    def read_text(res):\n        for reader in (context.files.read_docx_text, context.files.read_pdf_text, context.files.read_text):\n            try:\n                t = reader(res.id)\n                if t:\n                    return t\n            except Exception:\n                continue\n        return \"\"\n\n    chk_text = \"\"\n    for r in docs:\n        t = (read_text(r) or \"\").lower()\n        if not t:\n            continue\n        if (\"pre-screen\" in t or \"prescreen\" in t or \"checklist\" in t) and (\"admission\" in t or \"facility admission\" in t):\n            chk_text = t\n            break\n    if not chk_text:\n        return 0.0, \"Checklist not found.\"\n\n    fax_ok = bool(re.search(r'fax\\s*#?:?\\s*\\(?000\\)?\\s*111-?1234', chk_text))\n    email_ok = 'sunny@sunnydialysisclinic.com' in chk_text\n    pref_ok = 'preferred method of contact' in chk_text\n    hours_ok = bool(re.search(r'48\\s*hours', chk_text))\n\n    checks = [fax_ok, email_ok, pref_ok, hours_ok]\n    score = sum(checks)/4 * 0.2\n    missing = []\n    if not fax_ok: missing.append('fax number')\n    if not email_ok: missing.append('email')\n    if not pref_ok: missing.append('preferred contact mention')\n    if not hours_ok: missing.append('48 hours')\n    fb = \"Missing: \" + \", \".join(missing) if missing else \"All contact/SLA lines present.\"\n    return score, fb"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Suitability (LLM)", "description": "Holistic quality assessment: professional formatting, clarity for admin staff, regulatory sensibility, and usability in real workflows.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Practical Usability", "description": "Assess final documents for professional appearance, clarity, and readiness for use by administrative staff while supporting compliance.", "weight": 2.0, "judge_prompt": "Evaluate only after the structure is in place. Judge the overall quality and usability of BOTH documents:\n\nConsider:\n- Professional formatting: consistent fonts, spacing, alignment; logical layout; clear headers; readable tables; checkboxes visually aligned.\n- Clarity and completeness of labels/fields so staff can quickly fill them out without ambiguity.\n- Practicality for fax workflows: sufficient whitespace for handwriting; sender/recipient fields not cramped; page numbers in checklist footer well formatted (e.g., Page X of Y).\n- Compliance sensibility: presence and prominence of confidentiality statement on fax cover; inclusion of Hepatitis B / vaccination/lab cues and other dialysis-relevant items in the checklist (do not require exact phrasing, focus on whether the selection appears comprehensive and appropriate to dialysis transfer).\n- Branding and professionalism: logo placement, consistent header/footer, and document titles.\n\nScoring:\n- 2.0: Highly professional and immediately usable; excellent visual clarity and organization; minor or no issues.\n- 1.0\u20131.9: Generally professional and usable with some minor layout/clarity improvements possible.\n- 0.0\u20130.9: Noticeable formatting or clarity issues that would hinder use or present compliance risk.\n\nOnly assess quality/usability, not structural presence already verified in Stage 1.", "expectation": "Clean, professional documents ready for administrative use with clear labels, checkboxes, tables, and good readability."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "c6269101-fdc8-4602-b345-eac7597c0c81", "rubric": {"category_name": "Manufacturing \u2022 Industrial Engineering \u2022 Process Capability Study (Brightland Processing Center)", "rationale": "This rubric enforces a self-documenting, mixed-output workflow: an analysis workbook (Excel) containing verifiable calculations and diagnostics, and a leadership-ready presentation (PDF export of PowerPoint). Stage 1 (LLM-only) mandates exact, verifiable structure for both files so Stage 2 code rules can run deterministic checks without needing the original raw data. Stage 3 assesses presentation quality and strategic value for leadership. No thresholds are assumed; capability is evidenced via descriptive statistics, control-chart diagnostics, and comparative variability analysis.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (STRUCTURE ONLY)", "description": "LLM-only gate that verifies the exact, verifiable structure of both deliverables: an analysis Excel workbook and a presentation deck (PDF). If the required shape is not present, the entire category is scored as 0.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.5, "rules": [{"type": "llm_judge", "name": "Excel Analysis Workbook \u2014 Required Structure Present", "description": "Checks that an Excel workbook exists and contains the mandated sheets, sections, tables, and charts to enable verification.", "weight": 3.0, "judge_prompt": "You are verifying the SHAPE ONLY of the candidate\u2019s analysis workbook. Use flexible judgment on naming, but enforce the required structure. Do not judge correctness of numbers.\n\nVerify there is an Excel workbook among the outputs with the following structure (sheet names may vary slightly; accept close synonyms in parentheses):\n\n1) Sheet: \"Overview\" (or \"Executive Summary\" / \"Summary\")\n   - Contains a table with one row per process: [Process Name | Key Metrics | Stability Status | Variability Metric | Risk Summary | Most Variable?]\n   - Explicit cell/line stating: \"Most Variable Process: <name>\"\n\n2) Sheet: \"Task Duration\" (or \"Duration\" / \"Cycle Time\")\n   - Section: \"Descriptive Statistics\" with: Count, Mean, Median, Std Dev, MAD, Min, Max, CV\n   - Section: \"Stability Diagnostics (I-MR)\" with a summary table of rule violations (Test, Count/Result)\n   - Section: \"Time Trend\" including slope and p-value or equivalent trend test\n   - Section: \"Capability Summary\" narrative (at least 3 sentences)\n   - At least one control chart image and one time-series plot on the sheet\n\n3) Sheet: \"Failure Rate\" (or \"Defect Rate\" / \"Failures\")\n   - \"Data Summary\" with: Total Units, Total Failures, Overall Failure Rate\n   - Control chart type indicated (p-chart or np-chart)\n   - A tabular log with columns resembling: [Date/Period | Units | Failures | Failure Rate (p) | UCL | LCL | Signal?]\n   - \"Stability Diagnostics\" summary and a narrative paragraph\n\n4) Sheet: \"System Errors\" (or \"Errors\" / \"Rework\")\n   - \"Data Summary\" with: Total Transactions, Total Errors, Error Rate (or Errors/Unit)\n   - Control chart type indicated (u-chart preferred when denominators vary, or c-chart if constant)\n   - A tabular log with columns resembling: [Date/Period | Transactions | Errors | Error Rate or u | UCL | LCL | Signal?]\n   - \"Stability Diagnostics\" summary and a narrative paragraph\n\n5) Sheet: \"Comparative Variability\" (or \"Variability Ranking\" / \"Comparison\")\n   - Table with: [Process | Variability Metric Name | Value | Rank (1 = most variable)]\n   - Visible label: \"Most Variable Process:\" matching the top-ranked row\n\n6) Sheet: \"Consolidated Analysis\" (or \"Extended/Deep Dive/Focus Analysis\")\n   - Single-page consolidated report for the identified most variable process\n   - Includes capability evaluation, stability summary, time-trend analysis, at least one chart, and a \"Next Steps / Recommendations\" list\n\n7) Sheet: \"Methodology\" (or \"Methods\" / \"Calculation Log\")\n   - At least 5 bullet points describing methods used (e.g., chart selection, formulas, no assumed thresholds, data preparation)\n\nScoring:\n- 3.0: Workbook present with all 7 sheets and specified sections/tables; charts/plots present where required\n- 2.5: Minor omissions (e.g., 1 missing subsection or 1 missing chart), core sheets present\n- 2.0: Missing up to 2 required sheets or multiple subsections, but core process sheets (Duration, Failure, Errors) + Overview are present\n- 1.0: Only some core sheets present with minimal structure\n- 0.0: No Excel workbook or structure far from requirements\n\nOnly assess structure/presence, not numerical correctness.", "expectation": "An Excel workbook with the specified sheets and sections so verification is trivial."}, {"type": "llm_judge", "name": "Leadership Deck (PDF) \u2014 Required Structure Present", "description": "Checks that a leadership-ready presentation is provided as a PDF export with the required sections and slide structure. Shape only.", "weight": 3.0, "judge_prompt": "Verify there is a leadership deck provided as a PDF (export of PowerPoint is acceptable). Check only the structure and presence of sections; do not assess numerical correctness.\n\nRequired structure:\n- Format: PDF (not DOCX/Excel/plain text). Minimum 10 slides.\n- Slide 1: Title slide (project name, site: Brightland Processing Center, date, author/org)\n- Slide: Executive Summary \u2014 concise stability and capability findings across the three processes\n- 3 slides (minimum): One per process (Task Duration, Failure Rate, System Errors) including: capability summary, stability assessment, at least one chart/table per process\n- Slide: Comparative Variability & Risk Prioritization \u2014 clearly states which process is most variable and why\n- Slide: Consolidated Analysis for the most variable process \u2014 combined capability, stability, and time-trend view (+ at least one visual)\n- Slide: Recommendations / Next Steps \u2014 prioritized, practical actions\n- Slide(s): Appendix \u2014 methods, definitions, notes on control charts and analysis approach\n\nScoring:\n- 3.0: Valid PDF with all sections present and minimum slide counts\n- 2.5: One minor section missing or thin (e.g., small appendix) but core storyline intact\n- 2.0: Missing up to two required sections but still a coherent executive deliverable\n- 1.0: Barebones deck with significant omissions\n- 0.0: Not a PDF or fewer than 10 slides\n\nCheck presence and structure only; do not judge content quality.", "expectation": "A PDF deck fit for leadership with the required slides and sections."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness and Internal Consistency)", "description": "Now that the output shape is enforced, verify basic numerical consistency and cross-artifact agreement using code rules and a focused LLM cross-check.", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Workbook Soft Presence Sanity", "description": "Locate an analysis Excel workbook and softly verify the presence of key sheets using fuzzy matching. This is a light check complementing Stage 1.", "weight": 0.5, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    weight = 0.5\n    # Find a spreadsheet among outputs\n    spreadsheets = [r for r in context.get_all_outputs() if getattr(r, 'is_spreadsheet', False)]\n    if not spreadsheets:\n        return 0.0\n    wb = spreadsheets[-1]\n    try:\n        xfile = pd.ExcelFile(context.files.get_path(wb.id))\n        sheet_names = [s.lower() for s in xfile.sheet_names]\n    except Exception:\n        return 0.0\n\n    def has_sheet(synonyms):\n        return any(any(k in s for k in synonyms) for s in sheet_names)\n\n    required = [\n        ['overview','executive summary','summary'],\n        ['task duration','duration','cycle time'],\n        ['failure rate','failures','defect rate'],\n        ['system errors','errors','rework'],\n        ['comparative variability','variability ranking','comparison'],\n        ['consolidated analysis','extended analysis','deep dive','focus analysis'],\n        ['methodology','methods','calculation log']\n    ]\n\n    hits = sum(1 for syn in required if has_sheet(syn))\n    score = (hits/len(required)) * weight\n    return score"}, {"type": "code", "name": "Task Duration \u2014 CV consistency check", "description": "Verify that CV \u2248 Std Dev / Mean in the Task Duration sheet (tolerant matching).", "weight": 1.5, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    weight = 1.5\n    spreadsheets = [r for r in context.get_all_outputs() if getattr(r, 'is_spreadsheet', False)]\n    if not spreadsheets:\n        return 0.0\n    wb = spreadsheets[-1]\n    try:\n        xfile = pd.ExcelFile(context.files.get_path(wb.id))\n        # Pick duration-like sheet\n        dur_sheet = None\n        for s in xfile.sheet_names:\n            sl = s.lower()\n            if any(k in sl for k in ['task duration','duration','cycle time']):\n                dur_sheet = s\n                break\n        if not dur_sheet:\n            return 0.0\n        df = pd.read_excel(context.files.get_path(wb.id), sheet_name=dur_sheet, header=None)\n        # search rows for labels and pick first numeric in the row\n        def find_value(keys):\n            for i in range(min(len(df), 200)):\n                row = df.iloc[i]\n                row_str = ' '.join(str(x).lower() for x in row.values if pd.notna(x))\n                if any(k in row_str for k in keys):\n                    # return first numeric in the row (excluding text)\n                    for v in row.values:\n                        try:\n                            val = float(v)\n                            if np.isfinite(val):\n                                return val\n                        except Exception:\n                            continue\n            return None\n        mean = find_value(['mean','average'])\n        std = find_value(['std','stdev','standard deviation'])\n        cv = find_value(['cv','coefficient of variation'])\n        if mean is None or std is None or cv is None:\n            return 0.0\n        if mean == 0:\n            return 0.0\n        calc = abs(std/mean)\n        # allow 8% relative tolerance\n        rel_err = abs(calc - cv) / max(1e-9, abs(calc))\n        if rel_err <= 0.08:\n            return weight\n        elif rel_err <= 0.20:\n            return weight * 0.5\n        else:\n            return 0.0\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Failure Rate \u2014 Overall rate equals Failures/Units", "description": "Verify Overall Failure Rate \u2248 Total Failures / Total Units, and within [0,1].", "weight": 1.5, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    weight = 1.5\n    spreadsheets = [r for r in context.get_all_outputs() if getattr(r, 'is_spreadsheet', False)]\n    if not spreadsheets:\n        return 0.0\n    wb = spreadsheets[-1]\n    path = context.files.get_path(wb.id)\n    try:\n        xfile = pd.ExcelFile(path)\n        fr_sheet = None\n        for s in xfile.sheet_names:\n            sl = s.lower()\n            if any(k in sl for k in ['failure rate','failures','defect rate']):\n                fr_sheet = s\n                break\n        if not fr_sheet:\n            return 0.0\n        df = pd.read_excel(path, sheet_name=fr_sheet, header=None)\n        def find_value(keys):\n            for i in range(min(len(df), 200)):\n                row = df.iloc[i]\n                row_str = ' '.join(str(x).lower() for x in row.values if pd.notna(x))\n                if any(k in row_str for k in keys):\n                    for v in row.values:\n                        try:\n                            val = float(v)\n                            if np.isfinite(val):\n                                return val\n                        except Exception:\n                            continue\n            return None\n        units = find_value(['total units','units total'])\n        fails = find_value(['total failures','failures total'])\n        rate = find_value(['overall failure rate','failure rate overall','overall p'])\n        if units is None or fails is None or rate is None:\n            return 0.0\n        if units <= 0 or fails < 0:\n            return 0.0\n        calc = fails/units\n        # allow 1% absolute tolerance\n        abs_err = abs(calc - rate)\n        if 0 <= rate <= 1 and abs_err <= 0.01:\n            return weight\n        elif 0 <= rate <= 1 and abs_err <= 0.03:\n            return weight * 0.5\n        else:\n            return 0.0\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "System Errors \u2014 Overall rate equals Errors/Transactions", "description": "Verify Error Rate \u2248 Total Errors / Total Transactions (or u = Errors/Units), and non-negativity.", "weight": 1.5, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    weight = 1.5\n    spreadsheets = [r for r in context.get_all_outputs() if getattr(r, 'is_spreadsheet', False)]\n    if not spreadsheets:\n        return 0.0\n    wb = spreadsheets[-1]\n    path = context.files.get_path(wb.id)\n    try:\n        xfile = pd.ExcelFile(path)\n        se_sheet = None\n        for s in xfile.sheet_names:\n            sl = s.lower()\n            if any(k in sl for k in ['system errors','errors','rework']):\n                se_sheet = s\n                break\n        if not se_sheet:\n            return 0.0\n        df = pd.read_excel(path, sheet_name=se_sheet, header=None)\n        def find_value(keys):\n            for i in range(min(len(df), 200)):\n                row = df.iloc[i]\n                row_str = ' '.join(str(x).lower() for x in row.values if pd.notna(x))\n                if any(k in row_str for k in keys):\n                    for v in row.values:\n                        try:\n                            val = float(v)\n                            if np.isfinite(val):\n                                return val\n                        except Exception:\n                            continue\n            return None\n        tx = find_value(['total transactions','transactions total','total units'])\n        errs = find_value(['total errors','errors total'])\n        rate = find_value(['error rate','overall error rate','overall u','errors per unit'])\n        if tx is None or errs is None or rate is None:\n            return 0.0\n        if tx <= 0 or errs < 0:\n            return 0.0\n        calc = errs/tx\n        abs_err = abs(calc - rate)\n        # For error rate, ensure reasonable bounds [0,1]\n        if 0 <= rate <= 1 and abs_err <= 0.01:\n            return weight\n        elif 0 <= rate <= 1 and abs_err <= 0.03:\n            return weight * 0.5\n        else:\n            return 0.0\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Comparative Variability \u2014 Ranking matches declared Most Variable", "description": "Check that the top-ranked process in Comparative Variability matches the declared Most Variable in Overview and is the focus of the Consolidated Analysis sheet.", "weight": 2.0, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    weight = 2.0\n    spreadsheets = [r for r in context.get_all_outputs() if getattr(r, 'is_spreadsheet', False)]\n    if not spreadsheets:\n        return 0.0\n    wb = spreadsheets[-1]\n    path = context.files.get_path(wb.id)\n    try:\n        xfile = pd.ExcelFile(path)\n        # Identify sheets\n        sheets = xfile.sheet_names\n        cmp_sheet = None\n        ov_sheet = None\n        cons_sheet = None\n        for s in sheets:\n            sl = s.lower()\n            if any(k in sl for k in ['comparative variability','variability ranking','comparison']):\n                cmp_sheet = s\n            if any(k in sl for k in ['overview','executive summary','summary']):\n                ov_sheet = s\n            if any(k in sl for k in ['consolidated analysis','extended analysis','deep dive','focus analysis']):\n                cons_sheet = s\n        if not cmp_sheet:\n            return 0.0\n        # Read comparative variability\n        dfc = pd.read_excel(path, sheet_name=cmp_sheet)\n        cols = [c.lower() for c in dfc.columns]\n        # attempt to find process and rank columns\n        proc_col = None\n        rank_col = None\n        for i,c in enumerate(cols):\n            if 'process' in c:\n                proc_col = dfc.columns[i]\n            if 'rank' in c:\n                rank_col = dfc.columns[i]\n        if proc_col is None:\n            # fallback: first column\n            proc_col = dfc.columns[0]\n        if rank_col is None and dfc.shape[1] >= 2:\n            # try to infer rank by sorting a variability metric descending\n            # but keep None to avoid false positives\n            pass\n        top_proc = None\n        if rank_col is not None:\n            try:\n                dfc_sorted = dfc.sort_values(rank_col, ascending=True)\n                top_proc = str(dfc_sorted.iloc[0][proc_col]).strip()\n            except Exception:\n                pass\n        if not top_proc:\n            # fallback: pick the first non-empty process name\n            for v in dfc[proc_col].astype(str).values:\n                if isinstance(v, str) and v.strip():\n                    top_proc = v.strip()\n                    break\n        if not top_proc:\n            return 0.0\n        top_proc_l = top_proc.lower()\n        score_parts = 0\n        parts_total = 2\n        # Check Overview declared Most Variable\n        if ov_sheet:\n            dfo = pd.read_excel(path, sheet_name=ov_sheet, header=None)\n            text = ' '.join(str(x) for x in dfo.values.flatten() if pd.notna(x))\n            tl = text.lower()\n            # try to capture explicit label\n            if ('most variable' in tl) and (top_proc_l in tl):\n                score_parts += 1\n        # Check Consolidated Analysis references the same process\n        if cons_sheet:\n            dfa = pd.read_excel(path, sheet_name=cons_sheet, header=None)\n            text = ' '.join(str(x) for x in dfa.values.flatten() if pd.notna(x))\n            if top_proc_l in text.lower():\n                score_parts += 1\n        if score_parts == 2:\n            return weight\n        elif score_parts == 1:\n            return weight * 0.5\n        else:\n            return 0.0\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Sanity checks \u2014 Nonnegative counts and bounded rates in summaries", "description": "Basic plausibility checks on summary fields for Failure Rate and System Errors sheets.", "weight": 1.0, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    weight = 1.0\n    spreadsheets = [r for r in context.get_all_outputs() if getattr(r, 'is_spreadsheet', False)]\n    if not spreadsheets:\n        return 0.0\n    wb = spreadsheets[-1]\n    path = context.files.get_path(wb.id)\n    try:\n        xfile = pd.ExcelFile(path)\n        def check_sheet(keywords, units_key, count_key, rate_key):\n            target = None\n            for s in xfile.sheet_names:\n                sl = s.lower()\n                if any(k in sl for k in keywords):\n                    target = s\n                    break\n            if not target:\n                return 0\n            df = pd.read_excel(path, sheet_name=target, header=None)\n            def find_value(keys):\n                for i in range(min(len(df), 200)):\n                    row = df.iloc[i]\n                    row_str = ' '.join(str(x).lower() for x in row.values if pd.notna(x))\n                    if any(k in row_str for k in keys):\n                        for v in row.values:\n                            try:\n                                val = float(v)\n                                if np.isfinite(val):\n                                    return val\n                            except Exception:\n                                continue\n                return None\n            units = find_value(units_key)\n            count = find_value(count_key)\n            rate = find_value(rate_key)\n            if units is None or count is None or rate is None:\n                return 0\n            if units < 0 or count < 0:\n                return 0\n            if not (0 <= rate <= 1):\n                return 0\n            return 1\n        ok1 = check_sheet(['failure rate','failures','defect rate'], ['total units','units total'], ['total failures','failures total'], ['overall failure rate','failure rate overall','overall p'])\n        ok2 = check_sheet(['system errors','errors','rework'], ['total transactions','transactions total','total units'], ['total errors','errors total'], ['error rate','overall error rate','overall u','errors per unit'])\n        ok = ok1 + ok2\n        if ok == 2:\n            return weight\n        elif ok == 1:\n            return weight * 0.5\n        else:\n            return 0.0\n    except Exception:\n        return 0.0"}, {"type": "llm_judge", "name": "Cross-artifact consistency \u2014 Deck vs Workbook", "description": "LLM cross-check that the PDF deck\u2019s stated \u201cmost variable process\u201d and risk prioritization align with the Excel workbook\u2019s Comparative Variability and Overview.", "weight": 2.0, "judge_prompt": "Using all available outputs (the PDF leadership deck and the Excel analysis workbook), check for cross-consistency:\n\n- Identify the process named as \u201cMost Variable\u201d (or equivalent) in the workbook (Overview and Comparative Variability sheets) and confirm the same process is identified/prioritized in the deck (e.g., Variability & Risk slide, Consolidated Analysis slide).\n- Ensure the deck\u2019s Consolidated Analysis slide focuses on that same process and includes capability, stability, and time-trend elements also present in the workbook\u2019s Consolidated Analysis sheet.\n\nScoring:\n- 2.0: Clear, consistent alignment across both artifacts (naming and focus match). Consolidated Analysis matches the same process.\n- 1.0: Mostly consistent but with minor discrepancies (e.g., naming variations) or one artifact is slightly vague.\n- 0.0: Inconsistent or unable to locate required elements in one/both artifacts.\n\nJudge consistency only; do not assess numerical correctness.", "expectation": "Deck and workbook agree on which process is most variable and the focus of the deep-dive analysis."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality Assessment (Leadership Readout)", "description": "Holistic LLM assessment of presentation quality, clarity of insights, and actionability for a leadership audience.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Presentation Quality and Clarity for Leadership", "description": "Assesses whether the PDF deck is clear, well-structured, and visually effective for executive stakeholders.", "weight": 4.0, "judge_prompt": "Evaluate the PDF leadership deck for professional quality and clarity.\n\nConsider:\n- Clear storyline from problem to findings to actions.\n- Visuals: readable charts/tables (axes labeled, legible text), minimal clutter.\n- Executive-ready phrasing (concise, avoids jargon or explains it).\n- Slide hygiene: consistent formatting, headers, and takeaways on each slide.\n\nScoring:\n- 4.0: Excellent clarity and polish; compelling executive narrative with clean visuals.\n- 3.0: Strong overall with minor issues.\n- 2.0: Adequate but some clutter or gaps in clarity.\n- 1.0: Weak clarity or formatting issues.\n- 0.0: Not a professional deck or unreadable.", "expectation": "A polished, executive-ready deck with clear narrative and readable visuals."}, {"type": "llm_judge", "name": "Analytical Insight and Actionability", "description": "Assesses whether insights and recommendations are grounded in the analysis (without inventing thresholds), prioritize risk, and suggest practical actions.", "weight": 2.0, "judge_prompt": "Evaluate whether the insights and recommendations are well-grounded in the presented analysis:\n\n- Do findings about capability and stability reflect what the data shows (trends, variation, control-chart diagnostics) without assuming unprovided thresholds?\n- Are the most significant risks clearly identified and prioritized (e.g., the most variable process and its drivers)?\n- Are next steps practical and appropriate (e.g., root-cause investigation, error-proofing, process controls, data quality improvements)?\n\nScoring:\n- 2.0: Strong, data-grounded insights and actionable, prioritized recommendations.\n- 1.0: Partially grounded or generic recommendations.\n- 0.0: Weak linkage to analysis or unrealistic actions.", "expectation": "Insights aligned to evidence with clear, prioritized, and practical next steps."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "ee09d943-5a11-430a-b7a2-971b4e9b01b5", "rubric": {"category_name": "Financial Package Preparation for April Month-End", "rationale": "This evaluation rubric is designed to ensure the preparation of the April month-end financial package adheres to the required structure, accuracy, and professional quality standards. The task requires updating an existing March financial workbook with data from April source files, ensuring all necessary financial schedules are included, accurately processed, and professionally presented. The staged evaluation focuses on shape enforcement through an LLM judge, correctness verification via code rules and LLM judgments, and quality assessment focusing on presentation and clarity.", "max_total_score": 10.0, "stages": [{"name": "Format and Structure Validation", "description": "Verify the structural requirements of the financial package using an LLM judge.", "is_required": true, "min_score_to_pass": 1.0, "rules": [{"type": "llm_judge", "name": "Excel Structure Verification", "description": "Ensure the Excel workbook meets all specified structural requirements for the April month-end financial package.", "weight": 1.0, "judge_prompt": "Check if the submitted Excel file 'Aurisic_Financials_4-25-1.xlsx' adheres to the following structural requirements:\n\n1. **Tab 0: 'Table of Contents'**\n   - Must list all included tabs with descriptions and status/comments.\n\n2. **Tabs 3a and beyond:**\n   - All tabs should be updated with April data.\n   - New schedules/tabs must be included at the end and clearly labeled.\n\n3. **Tab Exclusions and Additions:**\n   - Ensure Tabs 1, 2, 2a, and 3 are excluded.\n   - Any new April tabs must match or extend beyond the tab order from the March file, starting from Tab 3a.\n\n4. **File Naming and Format:**\n   - The file must be named 'Aurisic_Financials_4-25-1.xlsx' and be a valid Excel workbook.\n\nScoring:\n- 1.0: All structural requirements met perfectly.\n- 0.5: Minor issues with tab labeling or table of contents.\n- 0.0: Major structural issues or incorrect format.", "expectation": "The agent should produce a single, consolidated Excel workbook following the exact structural guidelines provided."}], "max_points": 1.0, "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Correctness Verification", "description": "Evaluate the accuracy and consistency of financial calculations and updates in the workbook.", "is_required": false, "min_score_to_pass": 0.7, "rules": [{"type": "code", "name": "Cross-tab Consistency Check", "description": "Verify that data consistency is maintained across related tabs in the workbook.", "weight": 0.4, "code": "def evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        # Load the spreadsheet\n        xls = pd.ExcelFile(context.files.get_path(output.id))\n        # Read specific sheets to check data matching\n        df_main = pd.read_excel(xls, sheet_name='Main Financials')  # Example name\n        df_accrual = pd.read_excel(xls, sheet_name='Accruals')  # Example name\n\n        # Example check: Total in Main Financials should match Total of Accruals tab\n        if df_main['Total'].sum() != df_accrual['Accrued Amount'].sum():\n            return 0.0, \"Mismatch between Main Financials and Accruals tab\"\n        return 0.4\n    except Exception as e:\n        return 0.0, str(e)"}, {"type": "llm_judge", "name": "Data Appropriateness and Integration", "description": "Ensure that the updated data is correctly integrated into the financial package and matches the April source files.", "weight": 0.6, "judge_prompt": "Review the Excel workbook 'Aurisic_Financials_4-25-1.xlsx' to ensure:\n\n- Data from April source files is correctly incorporated.\n- Values match those in the corresponding source files listed.\n- Any missing information or discrepancies are clearly flagged within the workbook.\n\nScoring:\n- 0.6: Data accurately reflects source files and any issues are flagged appropriately.\n- 0.3: Minor data integration issues or corrections not clearly documented.\n- 0.0: Major data discrepancies or lack of documentation for changes.", "expectation": "Precise incorporation of April data and clear documentation of any adjustments or anomalies."}], "max_points": 1.0, "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Professional Quality Assessment", "description": "Evaluate the overall quality, professional presentation, and clarity of the financial package.", "is_required": false, "min_score_to_pass": 0.8, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Clarity", "description": "Assess the professional quality, clarity, and readability of the finalized financial package.", "weight": 0.8, "judge_prompt": "Evaluate the Excel workbook 'Aurisic_Financials_4-25-1.xlsx' on:\n\n- Overall professional appearance and formatting.\n- Clarity in the presentation of financial data.\n- Use of clear labels, comments, and enhancements for user readability.\n\nScoring:\n- 0.8: High standard of clarity, professional formatting, and readability.\n- 0.4: Acceptable clarity but lacks some enhancements or polish.\n- 0.0: Poor formatting, difficult to understand or follow.", "expectation": "The financial package should be professionally formatted, with clear data presentation and any enhancements well justified."}], "max_points": 1.0, "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "cd9efc18-d14a-4f69-8531-5d178a08084d", "rubric": {"category_name": "Texas Estate Planning \u2013 Last Will and Testament (Grace J. Parsons)", "rationale": "This rubric enforces a self-documenting, verifiable structure for a Texas-compliant Last Will and Testament delivered as an 8\u201311 page PDF/DOCX. Stage 1 (LLM-only) strictly gates format and section structure so later verification is trivial. Stage 2 mixes code checks (names, roles, dates, Texas formalities, trust parameters) with an LLM consistency review of beneficiary flow and fiduciary roles. Stage 3 assesses professional drafting quality and presentation.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2013 Format and Structural Gate (Texas Will)", "description": "LLM-only gate to ensure the output is a properly structured Texas Last Will and Testament with all sections needed for deterministic verification in Stage 2.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Will Format Requirement (Texas)", "description": "Check that the deliverable is a Texas-compliant Will document with all required sections, roles, and execution elements present in a professional structure.", "weight": 4.0, "judge_prompt": "You are evaluating whether the candidate output satisfies the REQUIRED STRUCTURE for a Texas Last Will and Testament for Grace J. Parsons. Only check PRESENCE/STRUCTURE, not legal sufficiency or content quality.\n\nFormat requirements:\n- Must be a PDF or DOCX (not plain text, not spreadsheet/code).\n- Length: approximately 8\u201311 pages total.\n- Professional formatting with clear section headers and article numbering.\n\nRequired document identity (first page):\n- Title prominently identifying: \u201cLast Will and Testament of Grace J. Parsons.\u201d\n- Opening clause confirming domicile/capacity under Texas law (e.g., Austin/Travis County, Texas), revocation of prior wills/codicils.\n\nCore sections (flexible header names ok if clearly equivalent):\n1) Family and Definitions: identify spouse Thomas A. Parsons (Client Spouse), children Timothy S. Parsons and Joshua J. Parsons; define terms such as \u201cdescendants,\u201d \u201cissue,\u201d or similar if used.\n2) Appointment of Executor: name Client Spouse as Executor; name Sarah R. Roberts as alternate; indicate customary fiduciary powers, and independence (e.g., without bond/independent administration) if included; state Executor has sole discretion to distribute tangible personal property.\n3) Dispositive Provisions:\n   - Primary: entire estate to Client Spouse if surviving the Client.\n   - Contingent: if spouse predeceases Client, estate to Client\u2019s children in equal shares.\n   - Remote Contingent: if not survived by spouse or any descendants, distribute equally to Sarah R. Roberts and Howard C. Long.\n   - Include a residuary clause ensuring the rest of the estate passes per the above.\n   - Survivorship/simultaneous death language (e.g., 120-hour rule or similar survivorship clause).\n4) Testamentary Trust for minors:\n   - Trust arises for minor beneficiaries; minimum distribution age 25; trust maximum duration 21 years.\n   - Trustee/Guardian designations: Sarah R. Roberts as primary trustee and guardian; Howard C. Long as alternate trustee/guardian; Michael T. Fisher as temporary local guardian until permanent guardian takes possession.\n   - Include spendthrift provision and customary trustee discretion (including authority to distribute/sell estate property).\n5) Taxes, Debts, and Administrative Provisions: payment of debts/expenses/taxes and fiduciary powers.\n6) Execution and Self-Proving:\n   - Signature block for Testator, dated May 13, 2025.\n   - Two witness attestation blocks naming Jose P. Harris and Geraldine R. Watson, executed same date.\n   - Texas self-proving affidavit with notary block (e.g., \u201cState of Texas, County of [___]\u201d), notary acknowledgment, and notary signature/seal, same date.\n\nScoring guidance (return a fractional score 0.0\u20131.0 that will be scaled by weight):\n- 1.0: Valid PDF/DOCX, ~8\u201311 pages, and ALL required sections/elements above are clearly present.\n- 0.85: Valid format, ~8\u201311 pages, and only one minor omission (e.g., explicit 21-year maximum or explicit survivorship clause missing) while the rest is present.\n- 0.7: Valid format with multiple minor omissions or one significant omission (e.g., missing self-proving affidavit or missing trust section header) but overall structure still verifiable.\n- 0.4: Valid format but major structural gaps (e.g., missing executor/beneficiary framework or missing execution/witness/notary section).\n- 0.0: Not PDF/DOCX, not ~8\u201311 pages, or missing multiple core sections making verification impossible.", "expectation": "A professionally structured Texas Will in PDF/DOCX of 8\u201311 pages with all named parties, roles, dispositive scheme, testamentary trust terms, and full execution (witness and self-proving affidavit) blocks present."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness Verification (Names, Roles, Logic, Texas Formalities)", "description": "Code checks verify presence of required names, dates, roles, and Texas-specific elements. LLM checks internal consistency of dispositions/roles with instructions.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Parties and Date Presence", "description": "Verify all named individuals and the execution date appear in the document text.", "weight": 1.2, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n\n    # Try to read text from PDF, then DOCX, then plain text\n    text = None\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0, \"Unable to read document text.\"\n\n    if not text:\n        return 0.0, \"Empty or unreadable document text.\"\n\n    t = text.lower()\n\n    required_names = [\n        \"grace j. parsons\",\n        \"thomas a. parsons\",\n        \"timothy s. parsons\",\n        \"joshua j. parsons\",\n        \"sarah r. roberts\",\n        \"howard c. long\",\n        \"michael t. fisher\",\n        \"jose p. harris\",\n        \"geraldine r. watson\",\n    ]\n\n    present = 0\n    missing = []\n    for name in required_names:\n        if name.lower() in t:\n            present += 1\n        else:\n            missing.append(name)\n\n    # Date check: May 13, 2025 (allow extra spaces)\n    date_patterns = [r\"may\\s+13,\\s*2025\"]\n    date_present = any(re.search(pat, t) for pat in date_patterns)\n\n    total_checks = len(required_names) + 1\n    score = (present + (1 if date_present else 0)) / total_checks\n\n    feedback = f\"Names present: {present}/{len(required_names)}; Date present: {date_present}. Missing: {', '.join(missing)}\" if missing else f\"All names found; Date present: {date_present}.\"\n    return max(0.0, min(1.0, score)), feedback"}, {"type": "code", "name": "Executor, Trustee, and Guardianship Structure", "description": "Verify executor/alternate, trustee/guardian designations, and related role keywords appear.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n\n    # Read text\n    text = None\n    for reader in (context.files.read_pdf_text, context.files.read_docx_text, context.files.read_text):\n        try:\n            text = reader(output.id)\n            if text:\n                break\n        except Exception:\n            continue\n    if not text:\n        return 0.0, \"Unable to read document text.\"\n\n    t = text.lower()\n\n    checks = []\n    # Executor and alternate\n    checks.append(\"executor\" in t)\n    checks.append(\"thomas a. parsons\" in t)  # named executor\n    # Alternate executor Sarah\n    checks.append(\"sarah r. roberts\" in t and (\"alternate\" in t or \"successor\" in t or \"if the executor\" in t))\n\n    # Trustee/Guardian roles\n    # Trustee keyword and names\n    checks.append(\"trustee\" in t)\n    checks.append(\"sarah r. roberts\" in t)\n    checks.append(\"howard c. long\" in t)\n\n    # Guardian keyword and names (including temporary guardian Michael)\n    checks.append(\"guardian\" in t)\n    checks.append(\"michael t. fisher\" in t)\n\n    # Optional but common in Texas: independent administration / without bond\n    optional_positive = 0\n    if \"independent\" in t and \"executor\" in t:\n        optional_positive += 1\n    if \"without bond\" in t or \"no bond\" in t:\n        optional_positive += 1\n\n    base_score = sum(1 for c in checks if c)\n    max_base = len(checks)\n    score = base_score / max_base\n\n    # small boost (capped at 1.0) for optional Texas features\n    score = min(1.0, score + 0.1 * optional_positive)\n\n    missing_notes = []\n    labels = [\n        \"executor keyword\",\n        \"executor name: Thomas A. Parsons\",\n        \"alternate executor (Sarah named and alt/successor context)\",\n        \"trustee keyword\",\n        \"trustee name: Sarah R. Roberts\",\n        \"alternate trustee name: Howard C. Long\",\n        \"guardian keyword\",\n        \"temporary guardian: Michael T. Fisher\",\n    ]\n    for ok, label in zip(checks, labels):\n        if not ok:\n            missing_notes.append(label)\n\n    feedback = f\"Role checks passed: {base_score}/{max_base}. Optional TX boosts: {optional_positive}. Missing: {', '.join(missing_notes)}\" if missing_notes else f\"All core role checks present. Optional TX boosts: {optional_positive}.\"\n    return max(0.0, min(1.0, score)), feedback"}, {"type": "code", "name": "Trust Parameters and Spendthrift Controls", "description": "Verify testamentary trust triggers for minors, age 25 distribution, max duration 21 years, spendthrift, and trustee discretion to distribute/sell.", "weight": 1.0, "code": "import re\n\ndef near(text, kw1, kw2, window=60):\n    idx = text.find(kw1)\n    if idx == -1:\n        return False\n    start = max(0, idx - window)\n    end = min(len(text), idx + len(kw1) + window)\n    return kw2 in text[start:end]\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n\n    # Read text\n    text = None\n    for reader in (context.files.read_pdf_text, context.files.read_docx_text, context.files.read_text):\n        try:\n            text = reader(output.id)\n            if text:\n                break\n        except Exception:\n            continue\n    if not text:\n        return 0.0, \"Unable to read document text.\"\n\n    t = text.lower()\n\n    checks = []\n    # Trust presence\n    checks.append(\"trust\" in t or \"trustee\" in t)\n\n    # Age 25 distribution (look for 'age' near '25')\n    age25 = bool(re.search(r\"age\\s*(of\\s*)?\\s*25|25\\s*(years)?\\s*of\\s*age\", t)) or (near(t, \"age\", \"25\") or near(t, \"25\", \"age\"))\n    checks.append(age25)\n\n    # Maximum duration 21 years\n    max21 = bool(re.search(r\"21\\s*year\", t)) or (near(t, \"21\", \"year\") or near(t, \"year\", \"21\"))\n    checks.append(max21)\n\n    # Spendthrift\n    checks.append(\"spendthrift\" in t)\n\n    # Trustee discretion and authority to sell\n    discretion = (\"discretion\" in t or \"sole discretion\" in t) and (\"distribute\" in t or \"distribution\" in t)\n    checks.append(discretion)\n    sell_power = (\"sell\" in t or \"sale\" in t) and (\"property\" in t or \"assets\" in t or \"estate\" in t)\n    checks.append(sell_power)\n\n    score = sum(1 for c in checks if c) / len(checks)\n\n    labels = [\n        \"trust present\",\n        \"age 25 distribution\",\n        \"max duration 21 years\",\n        \"spendthrift provision\",\n        \"trustee distribution discretion\",\n        \"authority to sell property\",\n    ]\n    missing = [lbl for ok, lbl in zip(checks, labels) if not ok]\n    feedback = \"All trust parameters detected.\" if not missing else f\"Missing/unclear: {', '.join(missing)}\"\n    return max(0.0, min(1.0, score)), feedback"}, {"type": "code", "name": "Disposition Logic and Residuary Scheme", "description": "Verify the presence of primary-to-spouse, contingent-to-children (equal shares), remote-contingent to Sarah/Howard, survivorship language, and residuary clause.", "weight": 1.1, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n\n    # Read text\n    text = None\n    for reader in (context.files.read_pdf_text, context.files.read_docx_text, context.files.read_text):\n        try:\n            text = reader(output.id)\n            if text:\n                break\n        except Exception:\n            continue\n    if not text:\n        return 0.0, \"Unable to read document text.\"\n\n    t = text.lower()\n\n    checks = []\n    # Primary to spouse\n    primary_spouse = (\"thomas a. parsons\" in t) and (\"spouse\" in t) and (\"entire\" in t or \"all\" in t or \"residuary\" in t) and (\"if\" in t and (\"survive\" in t or \"survives\" in t))\n    checks.append(primary_spouse)\n\n    # Contingent to children equal shares\n    children_ok = (\"timothy s. parsons\" in t and \"joshua j. parsons\" in t) and (\"equal\" in t and \"share\" in t)\n    checks.append(children_ok)\n\n    # Remote contingent to Sarah and Howard\n    remote_ok = (\"sarah r. roberts\" in t and \"howard c. long\" in t) and (\"equal\" in t and \"share\" in t)\n    checks.append(remote_ok)\n\n    # Survivorship/simultaneous death language (e.g., 120-hour rule)\n    survivorship = (\"surviv\" in t and (\"120\" in t and \"hour\" in t)) or (\"simultaneous\" in t and \"death\" in t)\n    checks.append(survivorship)\n\n    # Residuary clause\n    residuary = \"residuary\" in t or \"rest, residue\" in t or \"rest and residue\" in t\n    checks.append(residuary)\n\n    score = sum(1 for c in checks if c) / len(checks)\n\n    labels = [\n        \"primary to spouse present\",\n        \"contingent to children equal shares\",\n        \"remote contingent to Sarah/Howard equal shares\",\n        \"survivorship/120-hour language\",\n        \"residuary clause present\",\n    ]\n    missing = [lbl for ok, lbl in zip(checks, labels) if not ok]\n    feedback = \"Disposition logic detected.\" if not missing else f\"Missing/unclear: {', '.join(missing)}\"\n    return max(0.0, min(1.0, score)), feedback"}, {"type": "code", "name": "Texas Execution Formalities and Self-Proving Affidavit", "description": "Verify presence of Texas self-proving affidavit elements, county/state, witnesses, and notary indicators.", "weight": 0.7, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n\n    # Read text\n    text = None\n    for reader in (context.files.read_pdf_text, context.files.read_docx_text, context.files.read_text):\n        try:\n            text = reader(output.id)\n            if text:\n                break\n        except Exception:\n            continue\n    if not text:\n        return 0.0, \"Unable to read document text.\"\n\n    t = text.lower()\n\n    checks = []\n    # Texas indicators\n    checks.append(\"state of texas\" in t or \"texas\" in t)\n    checks.append(\"county of\" in t)\n\n    # Self-proving affidavit keywords\n    spa = (\"self-proving\" in t and \"affidavit\" in t) or (\"self proving\" in t and \"affidavit\" in t)\n    checks.append(spa)\n\n    # Notary indicators\n    notary = (\"notary public\" in t) or (\"before me\" in t and \"subscribed\" in t)\n    checks.append(notary)\n\n    # Witnesses explicitly named\n    witnesses = (\"jose p. harris\" in t) and (\"geraldine r. watson\" in t)\n    checks.append(witnesses)\n\n    score = sum(1 for c in checks if c) / len(checks)\n\n    labels = [\n        \"Texas (state) indicated\",\n        \"County indicated\",\n        \"Self-proving affidavit keyword(s)\",\n        \"Notary indicator\",\n        \"Witness names present\",\n    ]\n    missing = [lbl for ok, lbl in zip(checks, labels) if not ok]\n    feedback = \"Texas execution and affidavit elements detected.\" if not missing else f\"Missing/unclear: {', '.join(missing)}\"\n    return max(0.0, min(1.0, score)), feedback"}, {"type": "llm_judge", "name": "Disposition and Roles Consistency Review", "description": "LLM checks that the flow of property and fiduciary/guardian roles matches the instructions without contradictions.", "weight": 1.0, "judge_prompt": "Review the document text for consistency with the specified instructions. Only assess internal consistency and alignment, not drafting quality.\n\nConfirm ALL of the following are true and non-contradictory (score proportionally if partially true):\n- Primary beneficiary scheme: Entire estate to Client Spouse (Thomas A. Parsons) if he survives the Client.\n- Contingent scheme: If spouse predeceases, estate passes to Client\u2019s children (Timothy S. Parsons and Joshua J. Parsons) in equal shares.\n- Remote contingent: If not survived by spouse or any descendants, the entire estate is distributed in equal shares to Sarah R. Roberts and Howard C. Long.\n- Executor is Client Spouse; alternate executor is Sarah R. Roberts; executor has sole discretion to distribute tangible personal property.\n- Testamentary trust for minor beneficiaries includes: distribution age 25 and max duration 21 years; trustee/guardian roles align (Sarah primary; Howard alternate; Michael T. Fisher temporary local guardian until permanent guardian can take possession); includes a spendthrift provision and trustee discretion (including authority to distribute/sell estate property).\n- Execution section shows signatures for Testator, two witnesses (Jose P. Harris and Geraldine R. Watson), and a notary, all dated May 13, 2025.\n\nScoring guidance (return 0.0\u20131.0):\n- 1.0: All items present and consistent; no conflicting clauses.\n- 0.7: Minor omissions/ambiguity but overall flow matches.\n- 0.4: Multiple gaps or inconsistencies; core flow partially unclear.\n- 0.0: Major contradictions or missing core schemes/roles.", "expectation": "The will\u2019s dispositive hierarchy, fiduciary/guardian appointments, and execution details align exactly with the instructions without contradictions."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Professional Quality and Presentation", "description": "LLM holistic assessment of drafting professionalism, clarity, and presentation appropriate for a Texas estate planning practice.", "is_required": false, "max_points": 1.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Drafting Quality", "description": "Assess overall professionalism, organization, clarity, and readiness for client signature.", "weight": 1.0, "judge_prompt": "Evaluate the document\u2019s professional quality as a Texas Last Will and Testament:\n- Organization and readability: clear article headings, numbering, and logical flow.\n- Defined terms are capitalized and used consistently (e.g., Executor, Trustee, Residuary Estate).\n- Cross-references and internal references (articles/sections) are accurate.\n- No placeholders or bracketed template remnants; names/dates/roles are consistently spelled.\n- Tone and language are professional, precise, and plain-English where possible.\n- Presentation is signature-ready for a Texas estate planning practice.\n\nScoring (0.0\u20131.0):\n- 1.0: Highly professional, clean, consistent, signature-ready.\n- 0.7: Professional with minor editorial issues.\n- 0.4: Noticeable drafting/consistency issues.\n- 0.0: Poorly drafted/unprofessional or template placeholders remain.", "expectation": "A clean, consistent, signature-ready Texas will drafted at a professional standard."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "e4f664ea-0e5c-4e4e-a0d3-a87a33da947a", "rubric": {"category_name": "Screenplay Evaluation: SAINTLINESS (Production-Ready PDF)", "rationale": "Three-stage self-documenting rubric. Stage 1 is an LLM-only gate enforcing screenplay shape (PDF, pages, formatting, scene headings, dialogue blocks). Stage 2 mixes code and LLM to verify structural correctness via regex and heuristic checks enabled by Stage 1\u2019s shape: scene count, title presence, dialogue structure, transitions, and basic adherence to show-don\u2019t-tell. Stage 3 is an LLM quality assessment of cinematic writing, pacing, and production readiness.", "max_total_score": 20.0, "stages": [{"name": "Format & Structure Gate (Screenplay Compliance)", "description": "Gate to ensure the output is a properly formatted screenplay PDF for SAINTLINESS with industry-standard structure that enables verification.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Screenplay Shape and Format Gate", "description": "Verify the candidate output is a production-ready screenplay PDF following industry-standard screenplay formatting.", "weight": 8.0, "judge_prompt": "You are evaluating whether the submitted file is a properly formatted screenplay for the short film titled \u201cSAINTLINESS.\u201d Use only STRUCTURE and FORMAT checks (do not judge story quality here). You can view rendered pages. Apply flexibility on minor naming variations but enforce mandatory elements.\n\nRequired format and structure:\n1) File format: PDF (not Word, not plain text, not images-only). Text should be selectable/copyable.\n2) Length: 8\u201312 pages of script content (excluding a separate title page if present). Minor variance of \u00b11 page is acceptable only as a minor deviation.\n3) Typeface: Courier or a clear Courier-equivalent monospaced font at approximately 12 pt throughout script pages.\n4) Title: \u201cSAINTLINESS\u201d appears on the title page or top of the first page; a simple credit line like \u201cWritten by\u201d or equivalent is acceptable but optional in this gate.\n5) Screenplay elements visibly present and correctly placed:\n   - Scene headings (sluglines) in ALL CAPS, beginning with INT., EXT., INT/EXT., or I/E., followed by location, a hyphen, and time of day (DAY/NIGHT/DAWN/SUNSET/SUNRISE/DUSK/etc.). Target 10\u201315 short scenes.\n   - Action lines describing only what is seen/heard (no prose exposition blocks or character inner thoughts unless expressed via V.O.).\n   - Character cues: character names in ALL CAPS above dialogue blocks; dialogue lines beneath, parentheticals when used are on their own line in parentheses.\n   - Occasional transitions (e.g., FADE IN:, CUT TO:, DISSOLVE TO:, FADE OUT.) used sparingly and in the correct position/alignment.\n6) Professional screenplay page layout: margins and placements that look like standard script pages (e.g., character names centered-ish/narrow column, dialogue narrower than action, consistent spacing). \n7) The script contains only what an audience would see/hear; no analysis sections, outlines, or prose paragraphs outside screenplay form.\n\nScoring for this gate (0 to 8):\n- 8.0: PDF; 8\u201312 pages; Courier-like 12 pt monospaced; clear screenplay layout; 10\u201315 properly formatted scene headings; correct dialogue blocks; title present. Structure is industry-typical and clean.\n- 6.0\u20137.5: Minor deviations only (e.g., 7 or 13 pages; slight but clearly monospaced font variance; 9 or 16 scenes; a few minor placement inconsistencies). Still unmistakably a properly formatted screenplay PDF.\n- 3.0\u20135.5: Multiple formatting issues but still a recognizable screenplay (e.g., wrong font yet monospaced; page count 6 or 14\u201315; inconsistent sluglines/dialogue alignment). Usable but not reliably verifiable.\n- 0.0\u20132.5: Not a PDF; largely wrong format (no sluglines/dialogue form); fewer than ~6 pages; or a prose document/outline rather than a screenplay.\n\nOnly assess format/structure presence and basic counts. Do not judge story quality or calculation-like correctness here.", "expectation": "A selectable-text PDF screenplay, 8\u201312 pages, in Courier 12 pt (or equivalent), with title SAINTLINESS, 10\u201315 properly formatted scene headings, dialogue blocks with character names in caps, and standard screenplay layout."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Verification: Structural and Formal Correctness", "description": "Now that the structure is in place, verify key correctness aspects using regex/heuristics and focused LLM checks.", "is_required": false, "max_points": 7.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Scene Count Verification (10\u201315)", "description": "Count properly formatted scene headings (sluglines) using regex. Award based on proximity to 10\u201315 scenes.", "weight": 2.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    txt = \"\"\n    try:\n        txt = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            txt = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n    if not txt:\n        return 0.0\n    # Regex for scene headings: INT., EXT., INT/EXT., I/E.  Location - TIME\n    pattern = re.compile(r\"^(INT\\.|EXT\\.|INT/EXT\\.|I/E\\.)\\s+[^\\n\\-]+\\s-\\s*(DAY|NIGHT|DAWN|DUSK|SUNSET|SUNRISE|EVENING|MORNING|CONTINUOUS|LATER|SAME TIME)\\b\", re.IGNORECASE | re.MULTILINE)\n    matches = pattern.findall(txt)\n    n = len(matches)\n    # Scoring bands\n    if 10 <= n <= 15:\n        ratio = 1.0\n    elif (8 <= n < 10) or (16 <= n <= 18):\n        ratio = 0.6\n    elif (6 <= n < 8) or (18 < n <= 22):\n        ratio = 0.3\n    else:\n        ratio = 0.0\n    return 2.5 * ratio"}, {"type": "code", "name": "Title and Credit Presence", "description": "Check that the title SAINTLINESS appears and that a simple credit line (e.g., \u201cWritten by\u201d) is present near the start.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        txt = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            txt = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n    if not txt:\n        return 0.0\n    t = txt.upper()\n    has_title = 'SAINTLINESS' in t\n    head = t[:1200]\n    has_credit = ('WRITTEN BY' in head) or ('SCREENPLAY BY' in head) or re.search(r\"\\bBY\\b\", head) is not None or ('A FILM BY' in head)\n    if has_title and has_credit:\n        return 1.0\n    if has_title:\n        return 0.7\n    return 0.0"}, {"type": "code", "name": "Dialogue Block Structure Heuristic", "description": "Heuristically detect uppercase character cues and following dialogue lines to ensure substantive dialogue is present.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        txt = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            txt = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n    if not txt:\n        return 0.0\n    lines = [l.rstrip() for l in txt.splitlines()]\n    def is_transition(l):\n        L = l.strip().upper()\n        return bool(re.search(r\"(FADE IN:|FADE OUT\\.?|FADE TO BLACK\\.?|CUT TO:|SMASH CUT TO:|MATCH CUT TO:|DISSOLVE TO:|WIPE TO:)$\", L))\n    def is_scene(l):\n        return bool(re.match(r\"^(INT\\.|EXT\\.|INT/EXT\\.|I/E\\.)\\b\", l.strip(), re.IGNORECASE))\n    def is_char_name(l):\n        s = l.strip()\n        if len(s) < 2 or len(s) > 40:\n            return False\n        if s.endswith(':'):\n            return False\n        if is_scene(s) or is_transition(s):\n            return False\n        # Mostly uppercase letters/spaces/punct\n        if re.fullmatch(r\"[A-Z0-9 .'()\\-&]+\", s) is None:\n            return False\n        # Require at least one letter and not all punctuation\n        return re.search(r\"[A-Z]\", s) is not None\n    def is_dialogue(l):\n        s = l.strip()\n        if not s:\n            return False\n        if is_scene(s) or is_transition(s):\n            return False\n        # Dialogue is not all caps typically and is sentence-like\n        upper_ratio = sum(ch.isupper() for ch in s if ch.isalpha()) / max(1, sum(ch.isalpha() for ch in s))\n        return upper_ratio < 0.9 and len(s) <= 200\n    dialogue_blocks = 0\n    i = 0\n    n = len(lines)\n    while i < n - 1:\n        if is_char_name(lines[i]):\n            # Look ahead for the next non-empty line as dialogue\n            j = i + 1\n            while j < n and not lines[j].strip():\n                j += 1\n            if j < n and is_dialogue(lines[j]):\n                dialogue_blocks += 1\n                i = j\n            else:\n                i += 1\n        else:\n            i += 1\n    # Scoring bands\n    if dialogue_blocks >= 20:\n        ratio = 1.0\n    elif 10 <= dialogue_blocks < 20:\n        ratio = 0.8\n    elif 5 <= dialogue_blocks < 10:\n        ratio = 0.4\n    else:\n        ratio = 0.0\n    return 1.5 * ratio"}, {"type": "code", "name": "Transitions Presence", "description": "Check for presence of at least one standard on-screen transition.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        txt = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            txt = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n    if not txt:\n        return 0.0\n    T = txt.upper()\n    has = any(k in T for k in [\n        'FADE IN:', 'FADE OUT', 'FADE TO BLACK', 'CUT TO:', 'SMASH CUT TO:', 'MATCH CUT TO:', 'DISSOLVE TO:', 'WIPE TO:'\n    ])\n    return 0.5 if has else 0.0"}, {"type": "code", "name": "Parentheticals Usage (Optional)", "description": "Detect at least one parenthetical line placed within dialogue blocks. Optional\u2014partial credit if absent.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        txt = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            txt = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n    if not txt:\n        return 0.0\n    has_parenthetical = False\n    for line in txt.splitlines():\n        s = line.strip()\n        if re.fullmatch(r\"\\([^\\n]{1,80}\\)\", s):\n            has_parenthetical = True\n            break\n    # Optional: award full if present, small partial if absent\n    return 0.5 if has_parenthetical else 0.25"}, {"type": "llm_judge", "name": "Show-Don\u2019t-Tell and First-Mention Capitalization", "description": "LLM checks that action lines show only what is seen/heard, avoid internal thoughts unless V.O., and that character first mentions are capitalized.", "weight": 1.0, "judge_prompt": "Review the screenplay PDF text. Verify the following:\n1) Show, not tell: Action lines should describe observable visuals/sounds. Penalize obvious interior thoughts, backstory exposition, or author commentary not conveyed on screen. V.O. (voiceover) is acceptable when correctly indicated.\n2) First-mention capitalization: When characters are first introduced in action, their names should be in ALL CAPS at first mention (flexibly judge given extraction noise). \n\nScoring (0 to 1.0):\n- 1.0: Strong adherence to show-don\u2019t-tell; first-mention capitalization is consistently applied with at most minor lapses.\n- 0.6: Generally adherent with a few clear violations or missed capitalizations.\n- 0.3: Recurrent tell-y prose or frequent capitalization issues, but screenplay intent remains.\n- 0.0: Predominantly tell-y prose and/or no sign of first-mention capitalization practice.", "expectation": "Action is visual/audio-focused; minimal internal exposition; first character introductions are ALL CAPS in action."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Quality: Cinematic Writing and Production Readiness", "description": "Holistic LLM assessment of writing quality, pacing, readability, and whether the script is production-ready as a short film.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Quality Assessment", "description": "Assess cinematic clarity, pacing, scene economy, dialogue authenticity, tonal cohesion, and overall production readiness.", "weight": 5.0, "judge_prompt": "Evaluate the screenplay as a production-ready short film script titled \u201cSAINTLINESS.\u201d Focus on quality, not format (already checked). Consider:\n- Cinematic clarity: Are scenes visually clear and easy to stage? Are locations/time clear per scene heading?\n- Pacing and scene economy: 10\u201315 concise scenes targeting ~8\u201312 pages overall. Does the story move efficiently with purposeful beats?\n- Dialogue authenticity: Sounds natural, concise, and actable; uses parentheticals sparingly and effectively.\n- Tonal cohesion and theme: Consistent mood and a coherent throughline appropriate for the title and concept.\n- Readability and professionalism: Clean, consistent, minimal typos; transitions and formatting support the read; feels ready for table read/pre-production.\n\nScoring (0\u20135):\n- 5: Exceptional\u2014compelling, polished, highly producible.\n- 4: Strong\u2014minor opportunities but clearly production-ready.\n- 3: Adequate\u2014readable with noticeable issues; would benefit from revisions.\n- 2: Weak\u2014significant issues in pacing/dialogue/cohesion hinder producibility.\n- 0\u20131: Poor\u2014confusing, unpolished, or not viable in current form.", "expectation": "A polished, readable short-film script with clear cinematic storytelling, efficient pacing, authentic dialogue, and production-ready professionalism."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "4b894ae3-1f23-4560-b13d-07ed1132074e", "rubric": {"category_name": "Audio Bass Edit and Final Mix Delivery (Audio/Video Technicians)", "rationale": "This rubric enforces a self-documenting delivery for a bass-edit-and-mix task. Stage 1 (LLM-only) mandates a strict, verifiable file/package structure so later checks are trivial. Stage 2 uses code rules to verify technical specs, timing integrity, and edit-log validity derived from the mandated artifacts. Stage 3 qualitatively assesses professional fit for the artist\u2019s needs and the 70's-era aesthetic intent.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Delivery Shape Enforcement (GATE)", "description": "LLM-only gate that verifies the package contains all required artifacts in the exact structures needed for verification. No calculation/content correctness\u2014only presence/format/structure.", "is_required": true, "max_points": 5.0, "min_score_to_pass": 4.0, "rules": [{"type": "llm_judge", "name": "Structured Delivery Package Present", "description": "Check that the candidate delivers a complete, self-documenting package with the required files and structures to enable automated verification.", "weight": 5.0, "judge_prompt": "You are evaluating the STRUCTURE ONLY (not the correctness) of a delivery for an audio mixing/editing task.\n\nConfirm the package includes ALL of the following, with flexible naming where noted. Score based on presence and structural completeness, not content accuracy:\n\nA) Final Stereo Mix Audio File\n- A single stereo WAV file named exactly (case-insensitive base name ok) \u201cState of Affairs_FULL_EDIT_MIX.wav\u201d.\n- It must be a .wav file (48k/24b target; do NOT verify those values here\u2014just confirm the WAV file exists and name matches).\n\nB) Technical Specs (machine-readable)\n- A machine-readable file documenting the final mix\u2019s technical properties (prefer CSV; spreadsheet acceptable): \u201cTechnical Specs.csv\u201d (or similar name like \u201cTech Specs.csv\u201d, \u201cTechnical_Specs.xlsx\u201d).\n- Must have a row for the final stereo mix and include columns (flexible naming accepted, e.g., underscores/spaces):\n  \u2022 file_name\n  \u2022 sample_rate_hz\n  \u2022 bit_depth_bits\n  \u2022 channels\n  \u2022 duration_sec\n\nC) Edit Log Spreadsheet (Bass Edits)\n- An Excel workbook \u201cEdit Log.xlsx\u201d (or similar, e.g., \u201cBass Edit Log.xlsx\u201d).\n- Must contain at least these sheets (flexible sheet names accepted if clearly equivalent):\n  1) \u201cBass Edits\u201d \u2014 a tabular log with columns (flexible names allowed):\n     \u2022 issue_type (e.g., wrong_note, dissonant_note, string_noise, click, pop)\n     \u2022 timecode_start (format mm:ss.mmm, e.g., 01:44.375)\n     \u2022 timecode_end (required for noise removals; optional for note replacements)\n     \u2022 action (e.g., replace_note, mute_noise/silence)\n     \u2022 source_timecode (required for replace_note)\n     \u2022 musical_context or chord/section (the surrounding harmony/section reference)\n     \u2022 notes (freeform)\n  2) \u201cTiming Integrity\u201d \u2014 a table reporting:\n     \u2022 pre_length_sec\n     \u2022 post_length_sec\n     \u2022 delta_ms (difference; should be ~0)\n  3) \u201cMix Level Matching\u201d \u2014 a table reporting:\n     \u2022 rough_mix_rms_dbfs (or LUFS/RMS equivalent)\n     \u2022 final_mix_rms_dbfs (or LUFS/RMS equivalent)\n     \u2022 diff_db (final - rough)\n     \u2022 final_mix_peak_dbfs (peak of the delivered mix)\n\nD) Session Report (Document)\n- A PDF or DOCX report with professional formatting and clear headers. At least these sections (exact names flexible):\n  1) Executive Summary / Project Overview\n  2) Methodology and Editing Approach (must reference copying in-key notes from repeated sections and muting offensive noises to silence; mention preserving a 70\u2019s-era feel/natural performance)\n  3) Edit Details (should reference the Bass Edits log and how issues were addressed)\n  4) Mix and Level Matching (should reference how bass level was matched to the Rough Mix without altering other stems)\n  5) Appendix / Version History\n\nScoring (STRUCTURE ONLY):\n- 5.0: All A\u2013D present; each sub-item has the required structure.\n- 4.0: One minor omission (e.g., a single column missing in one table) but overall structure enables verification.\n- 3.0: One major item (A, B, C, or D) missing or substantially incomplete.\n- 1.0\u20132.0: Multiple major items missing/incomplete.\n- 0.0: Not a structured package; critical items missing (e.g., no final WAV or no Edit Log spreadsheet).\n\nBe flexible with section/tab/column names if clearly equivalent in meaning. Do NOT judge math/audio quality or metadata correctness here\u2014only structure/presence.", "expectation": "A complete delivery with: the correctly named stereo WAV, a machine-readable technical specs file including key columns, an Edit Log spreadsheet with the specified sheets and columns, and a professional Session Report with the required sections."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Code + LLM)", "description": "Now verify the correctness and internal consistency using the artifacts mandated in Stage 1. Perform deterministic checks via code and light cross-reference via LLM.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Technical Specs and Deliverable Consistency", "description": "Verify the final WAV naming and the technical specs (sample rate 48000 Hz, bit depth 24-bit, stereo=2 channels) from the machine-readable specs; ensure duration is positive and the row references the delivered file.", "weight": 1.5, "code": "import re\nimport json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0, \"No outputs found.\"\n\n    feedback = []\n    score_parts = []\n\n    # 1) Find final WAV (by name and extension)\n    final_wav = None\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            if p.suffix.lower() == '.wav':\n                if p.stem.lower() == 'state of affairs_full_edit_mix':\n                    final_wav = r\n                    break\n        except Exception:\n            continue\n    if not final_wav:\n        # fallback: any wav for partial credit context\n        for r in outputs:\n            try:\n                p = context.files.get_path(r.id)\n                if p.suffix.lower() == '.wav':\n                    final_wav = r\n                    break\n            except Exception:\n                continue\n\n    name_ok = 0.0\n    if final_wav:\n        p = context.files.get_path(final_wav.id)\n        if p.stem.lower() == 'state of affairs_full_edit_mix':\n            name_ok = 1.0\n            feedback.append(\"Final WAV found with correct name.\")\n        else:\n            feedback.append(f\"WAV found but filename mismatch: {p.name}\")\n    else:\n        feedback.append(\"No WAV file found.\")\n    score_parts.append(name_ok)\n\n    # 2) Locate Technical Specs (CSV preferred, spreadsheet acceptable)\n    specs_df = None\n    specs_src = None\n    def normalize_cols(df):\n        df = df.copy()\n        df.columns = [re.sub(r'[^a-z0-9]+', '_', str(c).strip().lower()) for c in df.columns]\n        return df\n\n    # Find by CSV name\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            if p.suffix.lower() in ['.csv'] and re.search(r'tech|spec', p.name, re.I):\n                df = context.files.read_csv(r.id)\n                specs_df = normalize_cols(df)\n                specs_src = ('csv', r)\n                break\n        except Exception:\n            continue\n\n    # If not found, try spreadsheet with a likely sheet\n    if specs_df is None:\n        for r in outputs:\n            try:\n                p = context.files.get_path(r.id)\n                if p.suffix.lower() in ['.xlsx', '.xls']:\n                    # try obvious sheet names\n                    for sheet in ['Technical Specs', 'Tech Specs', 'Specs', 'Technical', 'Delivery Specs']:\n                        try:\n                            df = context.files.read_excel(r.id, sheet_name=sheet)\n                            specs_df = normalize_cols(df)\n                            specs_src = ('xlsx', r)\n                            break\n                        except Exception:\n                            continue\n                    if specs_df is not None:\n                        break\n            except Exception:\n                continue\n\n    # If still not found, accept first CSV as last resort\n    if specs_df is None:\n        for r in outputs:\n            try:\n                p = context.files.get_path(r.id)\n                if p.suffix.lower() == '.csv':\n                    df = context.files.read_csv(r.id)\n                    specs_df = normalize_cols(df)\n                    specs_src = ('csv_any', r)\n                    break\n            except Exception:\n                continue\n\n    if specs_df is None or specs_df.empty:\n        feedback.append(\"Technical specs file not found or empty.\")\n        # scoring limited by missing info\n        return np.mean(score_parts), \"; \".join(feedback)\n\n    # 3) Identify row for final mix\n    cols = set(specs_df.columns)\n    # Flexible synonyms\n    fname_col = next((c for c in specs_df.columns if c in ['file_name','filename','name']), None)\n    sr_col = next((c for c in specs_df.columns if c in ['sample_rate_hz','sample_rate','samplerate_hz']), None)\n    bd_col = next((c for c in specs_df.columns if c in ['bit_depth_bits','bit_depth','bitdepth_bits']), None)\n    ch_col = next((c for c in specs_df.columns if c in ['channels','num_channels','channel_count']), None)\n    dur_col = next((c for c in specs_df.columns if c in ['duration_sec','duration_seconds','length_sec']), None)\n\n    have_cols = [x is not None for x in [fname_col, sr_col, bd_col, ch_col, dur_col]]\n    if not all(have_cols):\n        feedback.append(f\"Technical specs missing required columns (have: {list(specs_df.columns)}).\")\n        return np.mean(score_parts), \"; \".join(feedback)\n\n    # match file row\n    row = None\n    wav_name = None\n    if final_wav:\n        wav_path = context.files.get_path(final_wav.id)\n        wav_name = wav_path.name\n        m = specs_df[fname_col].astype(str).str.strip()\n        # match exact or case-insensitive\n        match_idx = np.where(m.str.lower() == wav_name.lower())[0]\n        if len(match_idx) > 0:\n            row = specs_df.iloc[match_idx[0]]\n    if row is None and len(specs_df) == 1:\n        row = specs_df.iloc[0]\n        wav_name = str(row[fname_col])\n\n    if row is None:\n        feedback.append(\"Could not find a specs row referencing the delivered WAV filename.\")\n        # still proceed to partial checks using medians if possible\n        row = specs_df.iloc[0]\n\n    # 4) Validate specs values\n    checks = []\n    # sample rate\n    try:\n        sr = float(row[sr_col])\n        checks.append(1.0 if abs(sr - 48000) < 1e-6 else 0.0)\n        if abs(sr - 48000) < 1e-6:\n            feedback.append(\"Sample rate reported as 48000 Hz.\")\n        else:\n            feedback.append(f\"Sample rate not 48000 Hz (reported {sr}).\")\n    except Exception:\n        checks.append(0.0)\n        feedback.append(\"Sample rate missing/invalid.\")\n\n    # bit depth\n    try:\n        bd = int(float(row[bd_col]))\n        checks.append(1.0 if bd == 24 else 0.0)\n        if bd == 24:\n            feedback.append(\"Bit depth reported as 24-bit.\")\n        else:\n            feedback.append(f\"Bit depth not 24-bit (reported {bd}).\")\n    except Exception:\n        checks.append(0.0)\n        feedback.append(\"Bit depth missing/invalid.\")\n\n    # channels\n    try:\n        ch = int(float(row[ch_col]))\n        checks.append(1.0 if ch == 2 else 0.0)\n        if ch == 2:\n            feedback.append(\"Channels reported as stereo (2).\")\n        else:\n            feedback.append(f\"Channels not 2 (reported {ch}).\")\n    except Exception:\n        checks.append(0.0)\n        feedback.append(\"Channels missing/invalid.\")\n\n    # duration positive\n    try:\n        dur = float(row[dur_col])\n        checks.append(1.0 if dur > 0 else 0.0)\n        if dur > 0:\n            feedback.append(f\"Duration reported as {dur:.2f}s (positive).\")\n        else:\n            feedback.append(\"Duration not positive.\")\n    except Exception:\n        checks.append(0.0)\n        feedback.append(\"Duration missing/invalid.\")\n\n    # combine: name_ok plus mean of checks\n    if checks:\n        tech_ok = float(np.mean(checks))\n    else:\n        tech_ok = 0.0\n    score_parts.append(tech_ok)\n\n    final_score = float(np.mean(score_parts)) if score_parts else 0.0\n    return final_score, \"; \".join(feedback)\n"}, {"type": "code", "name": "Edit Log Structure and Timecodes Validity", "description": "Validate the Bass Edits sheet structure and values: timecode formats, required fields per action type, and presence of at least one edit.", "weight": 1.5, "code": "import re\nimport json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0, \"No outputs.\"\n\n    feedback = []\n\n    # Find an Excel file likely to be the Edit Log\n    edit_book = None\n    edit_path = None\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            if p.suffix.lower() in ['.xlsx', '.xls'] and re.search(r'edit|bass', p.name, re.I):\n                edit_book = r\n                edit_path = p\n                break\n        except Exception:\n            continue\n\n    if not edit_book:\n        # Fallback: any spreadsheet\n        for r in outputs:\n            try:\n                p = context.files.get_path(r.id)\n                if p.suffix.lower() in ['.xlsx', '.xls']:\n                    edit_book = r\n                    edit_path = p\n                    break\n            except Exception:\n                continue\n\n    if not edit_book:\n        return 0.0, \"No Edit Log spreadsheet found.\"\n\n    # Try likely sheet names for 'Bass Edits'\n    bass_df = None\n    for sheet in ['Bass Edits','Edits','Bass_Edit_Log','Bass','Edit Log']:\n        try:\n            df = context.files.read_excel(edit_book.id, sheet_name=sheet)\n            bass_df = df\n            break\n        except Exception:\n            continue\n\n    if bass_df is None:\n        try:\n            # last resort: first sheet\n            df = context.files.read_excel(edit_book.id, sheet_name=0)\n            bass_df = df\n        except Exception:\n            return 0.0, \"Could not read any sheet from Edit Log.\"\n\n    # normalize columns\n    def norm(s):\n        return re.sub(r'[^a-z0-9]+','_', str(s).strip().lower())\n    bass_df = bass_df.copy()\n    bass_df.columns = [norm(c) for c in bass_df.columns]\n\n    # required columns (flexible matching)\n    def col_any(options):\n        for c in bass_df.columns:\n            if c in options:\n                return c\n        return None\n\n    issue_col = col_any(['issue_type','issue','type'])\n    start_col = col_any(['timecode_start','start','in'])\n    end_col   = col_any(['timecode_end','end','out'])\n    action_col= col_any(['action','edit_action'])\n    src_col   = col_any(['source_timecode','src_timecode','source','copy_from'])\n    notes_col = col_any(['notes','comment','remarks'])\n\n    req_ok = all([issue_col, start_col, action_col])\n    if not req_ok:\n        return 0.2, f\"Missing required columns. Have: {list(bass_df.columns)}\"\n\n    if len(bass_df) == 0:\n        return 0.2, \"Bass Edits sheet is present but empty.\"\n\n    # helpers\n    tc_re = re.compile(r'^\\d{2}:\\d{2}\\.\\d{3}$')\n    def parse_tc(tc):\n        if not isinstance(tc, str):\n            tc = str(tc)\n        if not tc_re.match(tc.strip()):\n            return None\n        mm, rest = tc.split(':',1)\n        ss, ms = rest.split('.')\n        try:\n            return int(mm)*60 + int(ss) + int(ms)/1000.0\n        except Exception:\n            return None\n\n    # Evaluate rows\n    n = len(bass_df)\n    valid_rows = 0\n    replace_ok = 0\n    mute_ok = 0\n\n    for _, row in bass_df.iterrows():\n        action = str(row[action_col]).strip().lower()\n        start_tc = str(row[start_col]).strip()\n        start_ok = parse_tc(start_tc) is not None\n        end_ok = True\n        if action in ['mute_noise','mute','silence','remove_noise','cut_noise']:\n            # needs end time\n            end_val = row[end_col] if end_col in bass_df.columns else ''\n            end_ok = parse_tc(str(end_val).strip()) is not None\n        src_ok = True\n        if action in ['replace_note','replace','copy_note','copy_paste']:\n            src_val = row[src_col] if src_col in bass_df.columns else ''\n            src_ok = parse_tc(str(src_val).strip()) is not None\n        if start_ok and end_ok and src_ok:\n            valid_rows += 1\n            if action in ['replace_note','replace','copy_note','copy_paste']:\n                replace_ok += 1\n            if action in ['mute_noise','mute','silence','remove_noise','cut_noise']:\n                mute_ok += 1\n\n    frac_valid = valid_rows / max(1,n)\n    # score components: structure present (0.3), row validity (0.5), diversity of actions (0.2)\n    structure_score = 1.0  # we already returned early if missing\n    row_score = frac_valid\n    diversity = 1.0 if (replace_ok > 0 or mute_ok > 0) else 0.5  # at least one actionable row\n\n    score = 0.3*structure_score + 0.5*row_score + 0.2*diversity\n    feedback_msg = f\"Rows={n}, valid_rows={valid_rows}, replace_ok={replace_ok}, mute_ok={mute_ok}.\"\n    return float(score), feedback_msg\n"}, {"type": "code", "name": "Timing Integrity and Length Equality", "description": "Check the Timing Integrity sheet to ensure pre/post lengths match (delta \u2248 0) and cross-check with Technical Specs duration where possible.", "weight": 0.6, "code": "import re\nimport json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0, \"No outputs.\"\n\n    # find edit log sheet with timing integrity\n    timing_df = None\n    timing_book = None\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            if p.suffix.lower() in ['.xlsx', '.xls'] and re.search(r'edit|bass|log|mix', p.name, re.I):\n                for sheet in ['Timing Integrity','Length Check','Timing','Integrity']:\n                    try:\n                        df = context.files.read_excel(r.id, sheet_name=sheet)\n                        timing_df = df\n                        timing_book = r\n                        break\n                    except Exception:\n                        continue\n                if timing_df is not None:\n                    break\n        except Exception:\n            continue\n\n    if timing_df is None:\n        return 0.0, \"No Timing Integrity sheet found.\"\n\n    # normalize columns\n    def norm(s):\n        return re.sub(r'[^a-z0-9]+','_', str(s).strip().lower())\n    timing_df = timing_df.copy()\n    timing_df.columns = [norm(c) for c in timing_df.columns]\n\n    pre_col = next((c for c in timing_df.columns if c in ['pre_length_sec','pre_duration_sec','pre_sec']), None)\n    post_col = next((c for c in timing_df.columns if c in ['post_length_sec','post_duration_sec','post_sec']), None)\n    delta_ms_col = next((c for c in timing_df.columns if c in ['delta_ms','diff_ms','delta_milliseconds']), None)\n\n    if not pre_col or not post_col:\n        return 0.2, f\"Timing sheet present but missing pre/post columns (have: {list(timing_df.columns)}).\"\n\n    # pick the first row\n    row = timing_df.iloc[0]\n    try:\n        pre = float(row[pre_col])\n        post = float(row[post_col])\n    except Exception:\n        return 0.2, \"Invalid numeric values for pre/post length.\"\n\n    delta_s = abs(post - pre)\n    delta_ms = delta_s * 1000.0\n\n    # tolerance\n    tol_ms = 50.0  # <= 50 ms difference\n    base_score = 1.0 if delta_ms <= tol_ms else max(0.0, 1.0 - (delta_ms - tol_ms)/500.0)\n\n    fb = [f\"Pre={pre:.3f}s, Post={post:.3f}s, \u0394={delta_ms:.1f}ms\"]\n\n    # cross-check with Technical Specs duration if available\n    # find specs\n    specs_df = None\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            if p.suffix.lower() == '.csv' and re.search(r'tech|spec', p.name, re.I):\n                specs_df = context.files.read_csv(r.id)\n                break\n        except Exception:\n            continue\n    if specs_df is not None and not specs_df.empty:\n        def ncol(cands):\n            cols = [re.sub(r'[^a-z0-9]+','_', str(c).strip().lower()) for c in specs_df.columns]\n            m = dict(zip(cols, specs_df.columns))\n            for c in cands:\n                if c in m:\n                    return m[c]\n            return None\n        dcol = ncol(['duration_sec','duration_seconds','length_sec'])\n        if dcol:\n            try:\n                spec_dur = float(specs_df.iloc[0][dcol])\n                # any of pre or post should roughly equal spec_dur\n                if abs(spec_dur - post) <= 0.1 or abs(spec_dur - pre) <= 0.1:\n                    fb.append(\"Specs duration matches timing within 0.1s.\")\n                else:\n                    fb.append(f\"Specs duration {spec_dur:.3f}s does not match pre/post within 0.1s.\")\n            except Exception:\n                pass\n\n    return float(base_score), \"; \".join(fb)\n"}, {"type": "code", "name": "Level Match Tolerance and Headroom", "description": "Verify that level matching claims are internally consistent: final vs rough RMS/LUFS difference within \u00b11.0 dB and final peak is reasonable (e.g., below 0 dBFS and not excessively low).", "weight": 0.4, "code": "import re\nimport json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    outputs = context.get_all_outputs()\n    if not outputs:\n        return 0.0, \"No outputs.\"\n\n    # Find Mix Level Matching (in Edit Log workbook or CSV)\n    df = None\n    for r in outputs:\n        try:\n            p = context.files.get_path(r.id)\n            if p.suffix.lower() in ['.xlsx', '.xls'] and re.search(r'edit|log|mix|level', p.name, re.I):\n                for sheet in ['Mix Level Matching','Level Match','Levels','Mix Levels']:\n                    try:\n                        df = context.files.read_excel(r.id, sheet_name=sheet)\n                        break\n                    except Exception:\n                        continue\n                if df is not None:\n                    break\n        except Exception:\n            continue\n\n    if df is None:\n        # try CSV\n        for r in outputs:\n            try:\n                p = context.files.get_path(r.id)\n                if p.suffix.lower() == '.csv' and re.search(r'level|mix', p.name, re.I):\n                    df = context.files.read_csv(r.id)\n                    break\n            except Exception:\n                continue\n\n    if df is None or df.empty:\n        return 0.0, \"No Mix Level Matching data found.\"\n\n    # normalize columns\n    def normcols(d):\n        d = d.copy()\n        d.columns = [re.sub(r'[^a-z0-9]+','_', str(c).strip().lower()) for c in d.columns]\n        return d\n    df = normcols(df)\n\n    rough_col = next((c for c in df.columns if c in ['rough_mix_rms_dbfs','rough_mix_lufs','rough_rms_dbfs','rough_lufs']), None)\n    final_col = next((c for c in df.columns if c in ['final_mix_rms_dbfs','final_mix_lufs','final_rms_dbfs','final_lufs']), None)\n    peak_col  = next((c for c in df.columns if c in ['final_mix_peak_dbfs','final_peak_dbfs','peak_dbfs']), None)\n    diff_col  = next((c for c in df.columns if c in ['diff_db','difference_db','delta_db']), None)\n\n    if not rough_col or not final_col:\n        return 0.2, f\"Missing rough/final level columns (have: {list(df.columns)}).\"\n\n    row = df.iloc[0]\n    feedback = []\n    score = 1.0\n\n    try:\n        rough = float(row[rough_col])\n        final = float(row[final_col])\n        diff = float(row[diff_col]) if diff_col else (final - rough)\n        if abs(diff) <= 1.0:\n            feedback.append(f\"Level diff within \u00b11 dB (diff={diff:.2f} dB).\")\n        else:\n            feedback.append(f\"Level diff exceeds \u00b11 dB (diff={diff:.2f} dB).\")\n            score -= 0.4\n    except Exception:\n        feedback.append(\"Invalid numeric levels.\")\n        score -= 0.6\n\n    try:\n        if peak_col:\n            peak = float(row[peak_col])\n            # Reasonable headroom: between -12 dBFS and -0.1 dBFS\n            if -12.0 <= peak <= -0.1:\n                feedback.append(f\"Peak in reasonable range ({peak:.2f} dBFS).\")\n            else:\n                feedback.append(f\"Peak out of reasonable range ({peak:.2f} dBFS).\")\n                score -= 0.2\n        else:\n            feedback.append(\"No peak column; skipping headroom check.\")\n            score -= 0.1\n    except Exception:\n        feedback.append(\"Invalid peak value.\")\n        score -= 0.2\n\n    score = max(0.0, min(1.0, score))\n    return float(score), \"; \".join(feedback)\n"}, {"type": "llm_judge", "name": "Cross-Reference and Aesthetic Alignment", "description": "LLM checks that the Session Report\u2019s narrative is consistent with the Edit Log (e.g., mentions replacing wrong notes with in-key copies from repeated sections and muting noises) and explicitly references preserving a 70\u2019s-era, natural feel without altering other stems\u2019 levels.", "weight": 0.4, "judge_prompt": "Review the Session Report (PDF/DOCX) and cross-reference it against the Edit Log structure at a high level (do not compute or verify math). Assess the following:\n- The report clearly describes: replacing wrong/dissonant bass notes by copying in-key notes from repeated sections; muting/removing offensive string noise/clicks/pops by inserting silence; and keeping track length unchanged.\n- The report explicitly states preserving a 70\u2019s-era, natural performance feel, and keeping other stems\u2019 levels unchanged while matching the bass level to the Rough Mix.\n- The narrative references the existence of the Edit Log sheets (Bass Edits, Timing Integrity, Mix Level Matching) and explains how they map to the edits/mix choices.\n\nScoring:\n- 1.0: Clear, specific, and consistent with the required artifacts and aesthetic constraints.\n- 0.6: Generally consistent but missing one element (e.g., no explicit 70\u2019s-era reference or no mention of length preservation).\n- 0.3: Vague or partially inconsistent; mentions only some elements.\n- 0.0: No relevant cross-reference or contradictions (e.g., suggests altering other stems\u2019 levels or heavy quantization contrary to brief).", "expectation": "A coherent, internally consistent narrative that references the Edit Log artifacts and affirms the 70\u2019s-era aesthetic and mix constraints."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professional Readiness", "description": "Holistic LLM assessment of professional presentation and readiness for the artist to track vocals over the delivered mix.", "is_required": false, "max_points": 1.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professionalism and Usefulness", "description": "Evaluate whether the documentation and delivery would be considered professional and directly useful for the artist to record vocals.", "weight": 1.0, "judge_prompt": "Assess the overall professionalism and readiness of the delivery:\n- Documentation clarity: Are sections clear, well-formatted, and free of obvious errors? Are tables legible and labeled?\n- Practical usefulness: Would this package allow the artist to confidently track vocals now (cleaned bass, timing integrity verified, bass level matched to the Rough Mix, stems otherwise unchanged)?\n- Appropriateness: Does the described approach respect the 70\u2019s-era, natural performance aesthetic (e.g., minimal over-editing/quantization, tasteful noise cleanup)?\n\nScoring:\n- 1.0: Professional, clear, and immediately useful for the stated purpose.\n- 0.6: Generally good but with minor clarity issues or small gaps.\n- 0.3: Marginal quality; would need revisions before use.\n- 0.0: Unprofessional or unsuitable for the intended recording session.", "expectation": "A clean, professional package that clearly enables the next step (vocal tracking) while honoring the artistic brief."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "a95a5829-34bb-40f3-993b-558aed6dcdef", "rubric": {"category_name": "General Order: Training Request Policy (Police Department)", "rationale": "This rubric enforces a self-documenting policy artifact. Stage 1 uses an LLM to strictly validate the document\u2019s shape (DOCX format, required policy sections, mandated signature roles, and Excel logging instructions/template). Stage 2 mixes code and LLM checks to verify presence of key elements (roles, timelines, approval authority, Excel logging specifics) and cross-referenced procedural coverage. Stage 3 assesses professional quality and suitability for a government law-enforcement audience.", "max_total_score": 10.5, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate (LLM ONLY)", "description": "Gate: Enforce exact artifact shape for a formal general order in DOCX with required sections, mandated approvers/signatures, and Excel logging instructions/template.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Policy Format Requirement", "description": "Verify the delivered artifact is a DOCX general order with the exact structure needed for subsequent verification.", "weight": 4.0, "judge_prompt": "You are validating the SHAPE (format and structural completeness) of a submitted policy document. Do NOT judge content quality or correctness beyond structural presence. Review the candidate output using the following checklist. Be flexible on exact header phrasing, but the structure must be clearly present.\n\nFORMAT REQUIREMENTS (Gate):\n- Must be a Word document (DOCX). Not PDF, not plain text, not Excel.\n- General order/policy format with a clear title for training request procedures.\n- Professionally structured with section headers. Preferably 2+ pages.\n\nREQUIRED SECTIONS (flexible naming allowed, but meaning must match):\n1) Title: Clearly indicates General Order/Policy on Training Requests.\n2) Purpose\n3) Scope or Applicability\n4) Definitions\n5) Responsibilities (aka Roles and Responsibilities)\n6) Procedures (must outline submission, review, evaluation, approval sequence)\n7) Approval and Signatures section/block that includes signature lines or sign-off entries for ALL of the following roles:\n   - Ethics Liaison Officer\n   - Chief, Division of Parole\n   - Chief, Fiscal Services Unit\n   - Chairman (aka Chair/Chairperson)\n8) Records Management and Training Tracking (must explicitly mention Excel spreadsheet logging instructions)\n9) Effective Date and Review Cycle (or Review/Revision schedule)\n10) Revision History (aka Amendments/Version Control)\n11) References/Authority (e.g., state mandates, internal policies)\n\nEXCEL LOGGING INSTRUCTIONS/TEMPLATE REQUIREMENT:\n- The policy must include either an Appendix or an embedded table labeled or clearly identified as a training log template for use in Excel. The template should list columns. Flexible naming is OK, but aim to see at least 8 of the following columns present or clearly described: Date, Employee Name, Employee ID/Badge, Training Title/Course Title, Provider/Vendor, Hours, Cost, Request ID/Number, Pre-Approval Reference/Authorization, Completion Status, Certificate File Name/Link, Location, Notes.\n\nSCORING (STRUCTURE ONLY):\n- 4.0: DOCX + all 11 required sections present with clear headers + signature block includes all 4 named roles + Excel logging instructions AND a table/template with at least 8 suggested columns.\n- 3.0: DOCX + all core policy sections (Purpose, Scope, Definitions, Responsibilities, Procedures) + Approval and Signatures section includes at least 3 of 4 required roles + Records/Tracking section mentions Excel logging but lacks a clear template/table.\n- 2.0: DOCX + partial sections (at least Purpose and Procedures) but missing either the signature block or Excel logging instructions.\n- 0.0: Not a DOCX, or lacks core structural elements (no clear sections), or missing multiple critical sections.\n\nReturn a numeric score per the rubric above. Only evaluate structure/format, not content correctness.", "expectation": "A well-structured DOCX policy with all required sections, signature roles, and Excel logging template present."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Content Presence and Consistency)", "description": "Now that the document shape is valid, verify with code and LLM that essential elements are present: required roles, timelines, approval authority, Excel logging details, and procedural coverage/cross-references.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Required Roles Present in Signatures/Body", "description": "Check that all mandated approver roles appear: Ethics Liaison Officer; Chief, Division of Parole; Chief, Fiscal Services Unit; Chairman/Chair/Chairperson.", "weight": 1.2, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Verifies presence of required approver roles using flexible pattern matching.\n    Returns up to 1.2 points.\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0\n    if not text:\n        return 0.0\n    t = text.lower()\n\n    # Role patterns (flexible)\n    patterns = []\n\n    # Ethics Liaison Officer variants\n    patterns.append(r\"ethics\\s+(liaison\\s+officer|officer|liaison)\")\n\n    # Chief, Division of Parole: look for 'division of parole' with 'chief' nearby\n    patterns.append(r\"chief[^\\n\\r]{0,50}division\\s+of\\s+parole|division\\s+of\\s+parole[^\\n\\r]{0,50}chief\")\n\n    # Chief, Fiscal Services Unit: look for 'fiscal services unit' with 'chief' nearby\n    patterns.append(r\"chief[^\\n\\r]{0,50}fiscal\\s+services\\s+unit|fiscal\\s+services\\s+unit[^\\n\\r]{0,50}chief\")\n\n    # Chairman variants\n    patterns.append(r\"\\b(chairman|chairperson|chair)\\b\")\n\n    hits = 0\n    for pat in patterns:\n        if re.search(pat, t):\n            hits += 1\n    score = (hits / len(patterns)) * 1.2\n    return max(0.0, min(1.2, score))"}, {"type": "code", "name": "Core Sections Present (Text Check)", "description": "Confirm presence of key headers: Purpose, Scope/Applicability, Definitions, Responsibilities, Procedures.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Checks for core policy section headers via flexible matching.\n    Returns up to 0.8 points.\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0\n    if not text:\n        return 0.0\n    t = text.lower()\n\n    checks = {\n        'purpose': r\"\\bpurpose\\b\",\n        'scope': r\"\\b(scope|applicability)\\b\",\n        'definitions': r\"\\bdefinitions?\\b\",\n        'responsibilities': r\"\\b(responsibilit(y|ies)|roles\\s+and\\s+responsibilities)\\b\",\n        'procedures': r\"\\bprocedures?\\b\"\n    }\n    found = 0\n    for k, pat in checks.items():\n        if re.search(pat, t):\n            found += 1\n    score = (found / len(checks)) * 0.8\n    return max(0.0, min(0.8, score))"}, {"type": "code", "name": "Timelines Mentioned", "description": "Detect whether submission/review timelines are specified (e.g., X days/weeks, within/no later than).", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Awards up to 0.6 based on presence of timeline expressions.\n    0.0 = none, 0.3 = at least one, 0.6 = two or more distinct mentions.\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0\n    if not text:\n        return 0.0\n    t = text.lower()\n\n    patterns = [\n        r\"\\b\\d{1,3}\\s*(business|calendar)?\\s*day(s)?\\b\",\n        r\"\\b\\d{1,2}\\s*week(s)?\\b\",\n        r\"\\bwithin\\s+\\d{1,3}\\s*(day|days|week|weeks)\\b\",\n        r\"\\bno\\s+later\\s+than\\s+\\d{1,3}\\s*(day|days|week|weeks)\\b\",\n        r\"\\bat\\s+least\\s+\\d{1,3}\\s*(day|days|week|weeks)\\b\",\n    ]\n    hits = 0\n    for pat in patterns:\n        if re.search(pat, t):\n            hits += 1\n    if hits == 0:\n        return 0.0\n    elif hits == 1:\n        return 0.3\n    else:\n        return 0.6"}, {"type": "code", "name": "Excel Logging Instructions and Columns", "description": "Verify the policy mentions Excel/spreadsheet logging and lists multiple expected columns in a training log template.", "weight": 0.9, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Checks for Excel/spreadsheet logging instructions and column names.\n    Scoring:\n    - 0.0 if no mention of excel/spreadsheet.\n    - 0.45 if excel is mentioned AND at least 3 expected columns present.\n    - 0.9 if excel is mentioned AND at least 6 expected columns present.\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0\n    if not text:\n        return 0.0\n    t = text.lower()\n\n    mentions_excel = bool(re.search(r\"\\b(excel|spreadsheet|workbook|.xlsx)\\b\", t))\n    if not mentions_excel:\n        return 0.0\n\n    # Expected columns (flexible wording)\n    cols = [\n        r\"date\",\n        r\"employee\\s*(name|id|badge)\",\n        r\"(training|course)\\s*(title|name)\",\n        r\"provider|vendor\",\n        r\"hour(s)?\",\n        r\"cost|tuition|fee(s)?\",\n        r\"request\\s*(id|number|no\\.)\",\n        r\"(pre-)?approval\\s*(ref(erence)?|number|id)\",\n        r\"completion\\s*status|status\",\n        r\"certificate|credential\",\n        r\"location\",\n        r\"notes|remarks|comments\"\n    ]\n    hits = 0\n    for c in cols:\n        if re.search(r\"\\b\" + c + r\"\\b\", t):\n            hits += 1\n    if hits >= 6:\n        return 0.9\n    elif hits >= 3:\n        return 0.45\n    else:\n        return 0.2  # minimal template signal"}, {"type": "code", "name": "Final Approval Authority Identified", "description": "Verify the policy identifies a final approval authority and ties it to a role (e.g., Chief, Chairman).", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Looks for 'final approval' or 'approval authority' and associates with a role.\n    Scoring: 0.5 if both phrase and role appear near each other; 0.25 if only phrase; else 0.\n    \"\"\"\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            return 0.0\n    if not text:\n        return 0.0\n    t = text.lower()\n\n    # Find occurrences of final approval/authority\n    final_spans = []\n    for m in re.finditer(r\"final\\s+(approval|approv(al)?\\s+authority)|approving\\s+authority|approval\\s+authority\", t):\n        final_spans.append((m.start(), m.end()))\n\n    if not final_spans:\n        return 0.0\n\n    # Roles to associate\n    role_patterns = [\n        r\"chief(\\s+of\\s+police)?\",\n        r\"chief[^\\n\\r]{0,60}division\\s+of\\s+parole|division\\s+of\\s+parole[^\\n\\r]{0,60}chief\",\n        r\"chief[^\\n\\r]{0,60}fiscal\\s+services\\s+unit|fiscal\\s+services\\s+unit[^\\n\\r]{0,60}chief\",\n        r\"chair(man|person)?|\\bchair\\b\",\n    ]\n\n    # Check proximity: role within 120 characters of final approval phrase\n    for (s, e) in final_spans:\n        window = t[max(0, s-120):min(len(t), e+120)]\n        if any(re.search(pat, window) for pat in role_patterns):\n            return 0.5\n    return 0.25"}, {"type": "llm_judge", "name": "Procedural Coverage and Cross-References", "description": "LLM verifies the policy covers who can submit, required request contents, evaluation criteria, review sequence with required approving roles, timelines, and record-keeping (tracking participation and maintaining records), with internal consistency.", "weight": 1.0, "judge_prompt": "Evaluate whether the policy text substantively covers the following items and that references are consistent across sections:\n\nCoverage checklist:\n1) Eligibility to submit training requests (who may submit: sworn, civilian, probationary, supervisors).\n2) Required contents of a training request (course info, justification, dates, costs, provider, hours, location, funding, etc.).\n3) Evaluation criteria (alignment to role/mandate, compliance requirements, budget, scheduling, prerequisites).\n4) Review and approval sequence explicitly mentioning the required roles (Ethics Liaison Officer, Chief\u2014Division of Parole, Chief\u2014Fiscal Services Unit, Chairman) in a logical order; signatures required.\n5) Timelines (submission lead times, review durations, notification timelines).\n6) Instructions for logging approved trainings via Excel and how participation/completion is tracked and how records are maintained/retained.\n7) References to state training mandates or internal authority (if stated in References/Authority section).\n\nScoring (0 to 1.0):\n- 1.0: All 7 items clearly present and internally consistent (roles named in procedures align with signature block).\n- 0.7: 5\u20136 items present with generally consistent references.\n- 0.4: 3\u20134 items present or notable inconsistencies.\n- 0.1: 1\u20132 items only.\n- 0.0: None evident.\n\nDo not penalize writing quality here\u2014focus on coverage and consistency.", "expectation": "Complete and consistent coverage of submitter eligibility, request contents, evaluation criteria, approval sequence with required roles, timelines, and record-keeping via Excel."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism", "description": "Holistic quality assessment of the policy document\u2019s clarity, professionalism, and suitability for a government law-enforcement audience.", "is_required": false, "max_points": 1.5, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Quality", "description": "Assess writing clarity, organization, tone, consistency of terminology, and suitability for official departmental use.", "weight": 1.5, "judge_prompt": "Assess the overall professional quality of this DOCX policy:\n- Clarity and concision suitable for a police department general order.\n- Logical organization and readable formatting (consistent headers, numbering, lists).\n- Consistent terminology (e.g., same names for roles/units throughout).\n- Neutral, directive tone; avoids ambiguity.\n- Length appropriate for a comprehensive policy (typically 2+ pages) without unnecessary filler.\n\nScoring:\n- 1.5: Excellent professional standard; ready for official use.\n- 1.0: Good quality with minor edits needed.\n- 0.5: Fair; noticeable issues in clarity/formatting.\n- 0.0: Poorly organized or unprofessional tone.", "expectation": "A polished, clearly written, well-structured general order suitable for official issuance."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "f2986c1f-2bbf-4b83-bc93-624a9d617f45", "rubric": {"category_name": "Retail Trade \u2013 Pharmacists: Emergency Medication Identification Spreadsheet", "rationale": "Task Type: Analytical (Pattern A). The expected deliverable is a structured Excel file that must enable straightforward verification. Stage 1 uses an LLM judge to enforce an exact spreadsheet shape with required columns so later checks are trivial. Stage 2 uses code rules to verify correctness and consistency now that the structure is guaranteed, focusing on NA policy, controlled/legend/OTC/unknown typing, MedlinePlus link validity, strength plausibility, and identification detail coverage. Stage 3 uses an LLM judge to assess professional usability for clinical workflow in the ER setting.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2013 Shape Enforcement (Gate)", "description": "LLM-only gate to ensure the output is an Excel spreadsheet with the exact, verifiable structure required for downstream checks.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Excel Medication List Requirement", "description": "Check that the output is a well-structured Excel file with the required sheet and columns so verification is possible.", "weight": 4.0, "judge_prompt": "You are checking ONLY the structure/format of the candidate output. Do not judge data quality. Confirm the following:\n\nFormat requirements:\n- Output is an Excel spreadsheet (.xlsx). Not PDF/DOCX/CSV/MD.\n- Contains at least one sheet with a tabular list of medications, starting with a clear header row.\n- Has at least 1 data row.\n\nRequired columns (flexible naming allowed; accept clear synonyms in parentheses):\n- Markings (aka Imprint, Imprint Code, Pill Imprint)\n- Color (Colour)\n- Shape (Pill Shape)\n- Dose form (Dosage Form, Form, Formulation)\n- Name of medication (Medication Name, Drug Name, Generic/Brand Name)\n- Strength of medication (Strength, Dose, Dosage)\n- Type of medication (Medication Type, Regulatory Status). Allowed values expected later: Controlled substance, Legend drug, Over the counter/OTC, Unknown.\n- MedlinePlus counseling link (MedlinePlus Link/URL, Patient Counseling Link)\n\nOptional but encouraged (do not penalize if absent in Stage 1):\n- Drugs.com source link (e.g., the specific pill ID page used for identification), or a separate sheet such as \"Sources\"/\"Audit Trail\" with source URLs per item.\n\nScoring (structure only):\n- 1.0: .xlsx with a medication table; header row includes all 8 required columns (or clear synonyms), and at least 1 data row.\n- 0.8: .xlsx with the medication table; exactly 1 required column missing or ambiguously labeled; at least 1 data row.\n- 0.5: .xlsx present but 2+ required columns missing/unclear OR table structure is unclear (no distinct header) OR no data rows.\n- 0.0: Not an Excel file OR no medication table identifiable.\n\nBe flexible with column names but strict about the presence of all required fields. Do NOT evaluate calculation/data correctness; ONLY structure and format.", "expectation": "A single .xlsx file with a clear medication table including the eight required fields and at least one row of data. Optional sources/audit sheet is a plus but not required for full credit."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness and Consistency Verification", "description": "Code-based checks leveraging the enforced shape to verify field validity, NA policy, link correctness, and minimal plausibility across fields.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "NA Policy on Required Fields", "description": "Penalize empty cells: required fields must be filled with a value or 'NA' if unavailable.", "weight": 1.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n    except Exception:\n        return 0.0\n\n    # Synonym map for required fields\n    syns = {\n        'markings': ['markings','imprint','imprint code','pill imprint','imprint/markings','pill id'],\n        'color': ['color','colour','pill color'],\n        'shape': ['shape','pill shape'],\n        'dose_form': ['dose form','dosage form','form','formulation'],\n        'name': ['name','medication','medication name','drug name','generic name','brand name'],\n        'strength': ['strength','dose','dosage','strength/concentration','concentration'],\n        'type': ['type','medication type','regulatory status','status'],\n        'medlineplus': ['medlineplus','medlineplus url','medlineplus link','patient counseling','counseling link','counseling info']\n    }\n\n    def clean(s):\n        return str(s).strip().lower()\n\n    def map_columns(df_cols):\n        cols_clean = [clean(c) for c in df_cols]\n        mapping = {}\n        for key, options in syns.items():\n            found = None\n            for i, c in enumerate(cols_clean):\n                for opt in options:\n                    if opt in c:\n                        found = i\n                        break\n                if found is not None:\n                    break\n            mapping[key] = df_cols[found] if found is not None else None\n        return mapping\n\n    best = None\n    best_map = None\n    best_hits = -1\n    for sheet in xls.sheet_names:\n        try:\n            df = pd.read_excel(file_path, sheet_name=sheet, header=0)\n        except Exception:\n            continue\n        m = map_columns(df.columns)\n        hits = sum(1 for k in syns if m.get(k) is not None)\n        if hits > best_hits and len(df) >= 1:\n            best, best_map, best_hits = df, m, hits\n\n    if best is None:\n        return 0.0\n\n    req_keys = ['markings','color','shape','dose_form','name','strength','type','medlineplus']\n    present_cols = [best_map[k] for k in req_keys if best_map.get(k) is not None]\n    if len(present_cols) == 0:\n        return 0.0\n\n    # Evaluate NA policy: each cell must be non-empty or NA/N/A\n    def is_ok(v):\n        if pd.isna(v):\n            return False  # must be explicit 'NA' if unknown\n        s = str(v).strip()\n        if s == '':\n            return False\n        s_low = s.lower()\n        if s_low in {'na','n/a','n\\u2215a','n.a.'}:\n            return True\n        return True  # any non-empty string is acceptable\n\n    sub = best[present_cols]\n    total = sub.shape[0] * sub.shape[1]\n    if total == 0:\n        return 0.0\n\n    ok = 0\n    for col in present_cols:\n        for v in sub[col].values:\n            if is_ok(v):\n                ok += 1\n    score = ok / total\n    return max(0.0, min(1.0, float(score)))"}, {"type": "code", "name": "Medication Type Validity", "description": "Type values should be one of: Controlled substance, Legend drug, Over the counter (OTC), or Unknown (case-insensitive; allow common variants like 'otc', 'over-the-counter').", "weight": 0.8, "code": "import re\nimport pandas as pd\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n    except Exception:\n        return 0.0\n\n    syn_options = ['type','medication type','regulatory status','status']\n\n    def clean(s):\n        return str(s).strip().lower()\n\n    def find_type_col(df):\n        cols = list(df.columns)\n        low = [clean(c) for c in cols]\n        for i, c in enumerate(low):\n            for opt in syn_options:\n                if opt in c:\n                    return cols[i]\n        return None\n\n    best_df = None\n    best_col = None\n    for sheet in xls.sheet_names:\n        try:\n            df = pd.read_excel(file_path, sheet_name=sheet)\n        except Exception:\n            continue\n        col = find_type_col(df)\n        if col is not None and len(df) >= 1:\n            best_df = df\n            best_col = col\n            break\n\n    if best_df is None or best_col is None:\n        return 0.0\n\n    allowed = {\n        'controlled substance': {'controlled','controlled substance','schedule','cs'},\n        'legend drug': {'legend','legend drug','rx only','prescription','rx'},\n        'over the counter': {'over the counter','otc','over-the-counter','over\u2013the\u2013counter','over the-counter'},\n        'unknown': {'unknown','unsure','n/a','na'}\n    }\n\n    def normalize(v):\n        s = clean(v)\n        s = re.sub(r'[^a-z0-9 ]+',' ', s)\n        s = re.sub(r'\\s+',' ', s).strip()\n        return s\n\n    def is_valid(v):\n        if v is None:\n            return False\n        s = normalize(v)\n        for _, variants in allowed.items():\n            for token in variants:\n                if token in s:\n                    return True\n        return False\n\n    values = list(best_df[best_col].astype(str).values)\n    if len(values) == 0:\n        return 0.0\n    valid = sum(1 for v in values if is_valid(v))\n    return max(0.0, min(1.0, valid / len(values)))"}, {"type": "code", "name": "MedlinePlus Link Validity", "description": "Each row should include a MedlinePlus counseling link that points to medlineplus.gov and looks like a URL. 'NA' is acceptable if not available.", "weight": 0.8, "code": "import re\nimport pandas as pd\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n    except Exception:\n        return 0.0\n\n    syns = ['medlineplus','medlineplus url','medlineplus link','patient counseling','counseling link','counseling info']\n\n    def clean(s):\n        return str(s).strip().lower()\n\n    def find_col(df):\n        cols = list(df.columns)\n        low = [clean(c) for c in cols]\n        for i, c in enumerate(low):\n            for opt in syns:\n                if opt in c:\n                    return cols[i]\n        return None\n\n    best_df = None\n    best_col = None\n    for sheet in xls.sheet_names:\n        try:\n            df = pd.read_excel(file_path, sheet_name=sheet)\n        except Exception:\n            continue\n        col = find_col(df)\n        if col is not None and len(df) >= 1:\n            best_df = df\n            best_col = col\n            break\n\n    if best_df is None:\n        return 0.0\n\n    values = list(best_df[best_col].astype(str).values)\n    if len(values) == 0:\n        return 0.0\n\n    def is_ok(v):\n        s = v.strip()\n        if s == '' or s.lower() in {'na','n/a'}:\n            return True  # treat explicit NA as acceptable\n        s_low = s.lower()\n        if 'medlineplus.gov' in s_low and re.search(r'https?://', s_low):\n            return True\n        return False\n\n    ok = sum(1 for v in values if is_ok(v))\n    return max(0.0, min(1.0, ok / len(values)))"}, {"type": "code", "name": "Strength Field Plausibility", "description": "Strength should look like a dose with units (e.g., 10 mg, 10 mg/5 mL, 0.1%, 500 mg-325 mg). 'NA' permitted when not available.", "weight": 0.6, "code": "import re\nimport pandas as pd\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n    except Exception:\n        return 0.0\n\n    syns = ['strength','dose','dosage','strength/concentration','concentration']\n\n    def clean(s):\n        return str(s).strip().lower()\n\n    def find_col(df):\n        cols = list(df.columns)\n        low = [clean(c) for c in cols]\n        for i, c in enumerate(low):\n            for opt in syns:\n                if opt in c:\n                    return cols[i]\n        return None\n\n    def is_strength_like(s):\n        t = str(s).strip()\n        if t == '' or t.lower() in {'na','n/a'}:\n            return True  # explicit NA is acceptable\n        # Accept common patterns: number + unit, combos with / or -, percentages\n        patterns = [\n            r\"\\b\\d+(?:\\.\\d+)?\\s*(mcg|mg|g|gram|ml|mL|l|iu|units|meq)\\b\",\n            r\"\\b\\d+(?:\\.\\d+)?\\s*%\\b\",\n            r\"\\b\\d+(?:\\.\\d+)?\\s*(mcg|mg|g|ml|mL|iu|units|meq)\\s*[/\\-]\\s*\\d+(?:\\.\\d+)?\\s*(mcg|mg|g|ml|mL|iu|units|meq|%)\\b\"\n        ]\n        tl = t.lower()\n        for p in patterns:\n            if re.search(p, tl):\n                return True\n        return False\n\n    best_df = None\n    best_col = None\n    for sheet in xls.sheet_names:\n        try:\n            df = pd.read_excel(file_path, sheet_name=sheet)\n        except Exception:\n            continue\n        col = find_col(df)\n        if col is not None and len(df) >= 1:\n            best_df = df\n            best_col = col\n            break\n\n    if best_df is None:\n        return 0.0\n\n    vals = list(best_df[best_col].astype(str).values)\n    if len(vals) == 0:\n        return 0.0\n    ok = sum(1 for v in vals if is_strength_like(v))\n    return max(0.0, min(1.0, ok / len(vals)))"}, {"type": "code", "name": "Markings/Color/Shape Coverage", "description": "For pill identification, at least two of [Markings, Color, Shape] should be present per row (or 'NA' when unknown).", "weight": 0.6, "code": "import pandas as pd\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n    except Exception:\n        return 0.0\n\n    syns = {\n        'markings': ['markings','imprint','imprint code','pill imprint','imprint/markings','pill id'],\n        'color': ['color','colour','pill color'],\n        'shape': ['shape','pill shape']\n    }\n\n    def clean(s):\n        return str(s).strip().lower()\n\n    def find_cols(df):\n        cols = list(df.columns)\n        low = [clean(c) for c in cols]\n        mapping = {}\n        for key, opts in syns.items():\n            found = None\n            for i, lc in enumerate(low):\n                if any(opt in lc for opt in opts):\n                    found = cols[i]\n                    break\n            mapping[key] = found\n        return mapping\n\n    best_df = None\n    best_map = None\n    best_hits = -1\n    for sheet in xls.sheet_names:\n        try:\n            df = pd.read_excel(file_path, sheet_name=sheet)\n        except Exception:\n            continue\n        m = find_cols(df)\n        hits = sum(1 for v in m.values() if v is not None)\n        if hits > best_hits and len(df) >= 1:\n            best_df, best_map, best_hits = df, m, hits\n\n    if best_df is None or best_hits == 0:\n        return 0.0\n\n    cols = [c for c in best_map.values() if c is not None]\n    if len(cols) < 2:\n        return 0.0\n\n    def present(v):\n        if v is None:\n            return False\n        s = str(v).strip()\n        if s == '' or s.lower() in {'na','n/a'}:\n            return False\n        return True\n\n    ok_rows = 0\n    total_rows = len(best_df)\n    for _, row in best_df.iterrows():\n        count = 0\n        for c in cols:\n            if present(row.get(c)):\n                count += 1\n        if count >= 2:\n            ok_rows += 1\n    if total_rows == 0:\n        return 0.0\n    return max(0.0, min(1.0, ok_rows / total_rows))"}, {"type": "code", "name": "Drugs.com Source Link Presence (Optional Bonus)", "description": "Reward inclusion of a Drugs.com source link per row or as a column. Scores the fraction of rows with a drugs.com URL if such a column exists; otherwise returns 0.", "weight": 0.2, "code": "import re\nimport pandas as pd\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n    except Exception:\n        return 0.0\n\n    syns = ['drugs.com','source','source url','identification source','drugs.com link']\n\n    def clean(s):\n        return str(s).strip().lower()\n\n    def find_col(df):\n        cols = list(df.columns)\n        low = [clean(c) for c in cols]\n        # Prefer columns explicitly mentioning drugs.com\n        for i, c in enumerate(low):\n            if 'drugs.com' in c:\n                return cols[i]\n        # Otherwise look for a generic source column\n        for i, c in enumerate(low):\n            if 'source' in c:\n                return cols[i]\n        return None\n\n    best_df = None\n    best_col = None\n    for sheet in xls.sheet_names:\n        try:\n            df = pd.read_excel(file_path, sheet_name=sheet)\n        except Exception:\n            continue\n        col = find_col(df)\n        if col is not None and len(df) >= 1:\n            best_df = df\n            best_col = col\n            break\n\n    if best_df is None or best_col is None:\n        return 0.0\n\n    vals = list(best_df[best_col].astype(str).values)\n    if len(vals) == 0:\n        return 0.0\n\n    def is_drugs_url(s):\n        sl = s.strip().lower()\n        if sl in {'na','n/a',''}:\n            return False\n        return ('drugs.com' in sl) and (('http://' in sl) or ('https://' in sl))\n\n    ok = sum(1 for v in vals if is_drugs_url(v))\n    return max(0.0, min(1.0, ok / len(vals)))"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Presentation and Clinical Usability", "description": "LLM judge assesses professional formatting and usability in ER workflow (filters, freeze panes, clarity, link usability).", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Usability", "description": "Evaluate whether the workbook is easy to use in a clinical setting.", "weight": 2.0, "judge_prompt": "Assess presentation and usability (not the correctness of drug identification):\n- Is there a clear title and/or context (e.g., patient medication identification, date/time, image reference such as \"what are these.jpg\")?\n- Are column headers readable and consistent (capitalization, no cryptic abbreviations)?\n- Are hyperlinks (MedlinePlus, optional Drugs.com) clickable and clearly formatted?\n- Are filters enabled or is the sheet otherwise easy to scan (reasonable column widths, wrap text where needed, frozen header row)?\n- Is 'NA' used consistently rather than leaving blanks?\n- Is the layout suitable for quick ER use (no clutter, clear separation of fields)?\n- Optional plus: Data validation (drop-down) for Medication Type values; a sources/audit sheet.\n\nScoring guidance:\n- 1.0: Adequate clinical usability (clean headers, readable, links present and usable, minimal formatting).\n- 2.0: Highly professional (title/context, good formatting such as frozen header/filters, consistent NA policy, clickable links, optional validation or sources sheet).\n- 0.0: Sloppy or hard to use (inconsistent headers, cluttered/illegible, links missing/unusable).", "expectation": "A clean, readable workbook with clickable links, consistent formatting, and basic usability features fit for ER workflow."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "4520f882-715a-482d-8e87-1cb3cbdfe975", "rubric": {"category_name": "Finance & Insurance \u2014 Financial Managers: Musician Payroll Model (CBA)", "rationale": "This rubric enforces a self-documenting, auditable Excel payroll model for a theatre\u2019s musician hiring under a CBA. Stage 1 is an LLM-only gate that mandates an exact, verification-friendly workbook shape (sheets, sections, and table headers). Stage 2 uses code + LLM to verify correctness via referential integrity, plausibility bounds, category coverage, reconciliation, and explicit compliance mappings. Stage 3 assesses professional usability, robustness, and maintainability for real-world contractor use.", "max_total_score": 15.0, "stages": [{"name": "Stage 1 \u2014 Structured Output Format Requirement (GATE)", "description": "LLM-only gate that checks the output is an Excel workbook with clearly named sheets and specific sections/tables enabling verification of CBA-driven payroll calculations and validations.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Workbook Shape and Section Presence", "description": "Verify the candidate output is an Excel workbook with required sheets and tables structured for contract-driven payroll. Only check presence/structure, not calculation correctness.", "weight": 4.0, "judge_prompt": "You are the Stage 1 gate. Inspect the submitted file and score ONLY on structure/format. Do not judge calculation correctness or presentation quality.\n\nFormat requirement:\n- Must be a valid Excel workbook (.xlsx). Not a PDF, DOCX, CSV, or plain text.\n\nRequired sheets (accept flexible names indicated in parentheses):\n1) Inputs \u2013 Production (aka: \"Setup\", \"Production Inputs\")\n   Sections (must be visible with headers):\n   - Production Metadata: a 2+ column table with columns similar to [Field | Value] and rows including at least: Show/Production Title, Week Start Date, CBA Version/Year or Rate Version/Effective Date.\n   - CBA Parameters: a 3+ column table with columns like [Parameter | Value | Unit or Clause/Source]. Must include rows for at least: Rehearsal Hourly Rate, Performance Base Rate, Overtime Trigger, Overtime Multiplier, Doubling Premium(s), Cartage rules/categories, Pension %, Vacation %, Health & Welfare amount. Values need not be correct, but rows/labels must be present.\n\n2) Rates (aka: \"CBA Rates\", \"Rate Tables\")\n   Sections (tables) clearly labeled within the sheet:\n   - Base Rates: columns like [Parameter | Subtype | Value | Unit | Effective Start | Effective End | Clause].\n   - Premiums & Allowances: columns like [Item | Trigger/Rule | Rate | Unit | Clause].\n   - Benefits & Contributions: columns like [Benefit | Rate/Amount | Basis | Clause].\n\n3) Roster (aka: \"Musicians\")\n   A single table with headers including at least: [Musician ID | Name | Instrument | Role/Chair | Union ID (or similar) | Cartage Category | Benefit Eligible].\n\n4) Calls (aka: \"Schedule\", \"Services\")\n   A table with headers including at least: [Call ID | Date | Call Type (Rehearsal/Performance) | Start Time | End Time | Duration (hrs) | Location].\n\n5) Timecards (aka: \"Attendance\", \"Timesheets\", \"Work Logs\")\n   A table with headers including at least: [Call ID | Musician ID | Doubling Count or Instruments Played | Cartage Applied (Y/N) | Notes].\n\n6) Payroll \u2013 By Person (aka: \"Payroll Detail by Musician\")\n   A summary table with headers including at least: [Musician ID | Name | Rehearsal Wages | Performance Wages | Overtime Wages | Doubling Premiums | Cartage/Porterage | Vacation | Pension | Health & Welfare | Total Gross | Total Benefits | Total Cost].\n\n7) Validation Log (aka: \"Compliance Checks\", \"Contract Flags\")\n   A log table with headers like: [Issue ID | Severity | Affected Musician/Call | Category | Clause/Rule | Description | Suggested Correction | Resolved (Y/N)]. It should be clear this sheet is intended to flag inputs that conflict with the CBA.\n\nOptional (do not penalize if missing):\n- Read Me / Instructions (overview of usage and assumptions)\n- Payroll \u2013 Summary (roll-up totals by category/week)\n- Lookups (supporting lists)\n\nScoring (be flexible with sheet/section names but strict about presence of the structures):\n- 4.0: All 7 required sheets present and each has the specified tables/columns.\n- 3.5: Missing only optional sheets OR very minor header naming variations but clear equivalents for all required fields.\n- 3.0: Missing one required sheet OR one required section/table on a sheet.\n- 1.0: Missing two required sheets/sections but workbook is Excel and at least has Roster, Calls, Timecards, and Payroll \u2013 By Person.\n- 0.0: Not an Excel file OR missing multiple core sheets/sections making verification impossible.\n\nOnly verify presence/structure, not the accuracy of values or formulas.", "expectation": "A multi-sheet Excel model with clearly labeled tables for inputs, rates, roster, calls, timecards, payroll by person, and a validation log referencing CBA clauses."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Contract Logic Verification", "description": "Deterministic checks using code + targeted LLM review now that the workbook shape is enforced. Focus on referential integrity, plausibility, category coverage, reconciliation, and explicit compliance mappings.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Referential Integrity: Timecards link to Roster and Calls", "description": "Verify Timecards has valid Musician IDs and Call IDs that exist in Roster and Calls sheets. Score by error rate.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    import pandas as pd, numpy as np, re\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"Primary output missing or not an Excel workbook.\"\n    try:\n        path = context.files.get_path(output.id)\n        xl = pd.ExcelFile(path)\n        sheets = xl.sheet_names\n    except Exception as e:\n        return 0.0, f\"Unable to open Excel: {e}\"\n\n    def find_sheet(cands):\n        for s in sheets:\n            sl = s.lower()\n            for c in cands:\n                if c in sl:\n                    return s\n        return None\n\n    roster_sheet = find_sheet([\"roster\",\"musician\"])  # musicians\n    calls_sheet = find_sheet([\"call\",\"schedule\",\"service\"])  # calls/services\n    time_sheet = find_sheet([\"timecard\",\"attendance\",\"timesheet\",\"work log\",\"worklog\",\"time\"])  # timecards\n\n    if not (roster_sheet and calls_sheet and time_sheet):\n        return 0.0, \"Required sheets (Roster/Calls/Timecards) not found.\"\n\n    try:\n        df_r = context.files.read_excel(output.id, sheet_name=roster_sheet)\n        df_c = context.files.read_excel(output.id, sheet_name=calls_sheet)\n        df_t = context.files.read_excel(output.id, sheet_name=time_sheet)\n    except Exception as e:\n        return 0.0, f\"Failed reading sheets: {e}\"\n\n    if df_t is None or len(df_t) == 0:\n        return 1.0, \"No timecard rows present; referential integrity passes vacuously.\"\n\n    def norm_map(df):\n        return {col: str(col).strip().lower() for col in df.columns}\n\n    rmap, cmap, tmap = norm_map(df_r), norm_map(df_c), norm_map(df_t)\n\n    def get_col(colmap, alts):\n        for k, v in colmap.items():\n            for alt in alts:\n                if alt in v:\n                    return k\n        return None\n\n    mus_id_r = get_col(rmap, [\"musician id\",\"employee id\",\"id\"])  # Roster key\n    call_id_c = get_col(cmap, [\"call id\",\"service id\",\"id\"])      # Calls key\n    mus_id_t = get_col(tmap, [\"musician id\",\"employee id\",\"id\"])   # Timecards link to roster\n    call_id_t = get_col(tmap, [\"call id\",\"service id\"])             # Timecards link to calls\n\n    if not (mus_id_r and call_id_c and mus_id_t and call_id_t):\n        return 0.0, \"Missing key ID columns in one or more sheets (Musician ID / Call ID).\"\n\n    r_ids = set(df_r[mus_id_r].dropna().astype(str).str.strip())\n    c_ids = set(df_c[call_id_c].dropna().astype(str).str.strip())\n    t_m_ids = df_t[mus_id_t].dropna().astype(str).str.strip()\n    t_c_ids = df_t[call_id_t].dropna().astype(str).str.strip()\n\n    if len(t_m_ids) == 0 or len(t_c_ids) == 0:\n        return 1.0, \"Timecards present but no link IDs; treating as empty payload.\"\n\n    missing_m = (~t_m_ids.isin(r_ids)).sum()\n    missing_c = (~t_c_ids.isin(c_ids)).sum()\n\n    total_links = max(len(df_t), 1)\n    bad_rate = (missing_m + missing_c) / (2 * total_links)\n\n    if bad_rate <= 0.01:\n        score = 1.0\n    elif bad_rate <= 0.05:\n        score = 0.7\n    elif bad_rate <= 0.15:\n        score = 0.4\n    else:\n        score = 0.0\n\n    feedback = f\"Missing musician links: {missing_m}; missing call links: {missing_c}; total rows: {len(df_t)}; error rate: {bad_rate:.2%}.\"\n    return score, feedback"}, {"type": "code", "name": "Call Duration Plausibility", "description": "Check Calls durations are within plausible bounds. If Call Type is present, apply type-specific bounds; otherwise use generic bounds.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    import pandas as pd, numpy as np\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"Primary output missing or not a spreadsheet.\"\n    try:\n        path = context.files.get_path(output.id)\n        xl = pd.ExcelFile(path)\n        sheets = xl.sheet_names\n    except Exception as e:\n        return 0.0, f\"Unable to open Excel: {e}\"\n\n    def find_sheet(cands):\n        for s in sheets:\n            sl = s.lower()\n            for c in cands:\n                if c in sl:\n                    return s\n        return None\n\n    calls_sheet = find_sheet([\"call\",\"schedule\",\"service\"])  # Calls sheet\n    if not calls_sheet:\n        return 0.0, \"Calls sheet not found.\"\n\n    try:\n        df = context.files.read_excel(output.id, sheet_name=calls_sheet)\n    except Exception as e:\n        return 0.0, f\"Failed reading Calls: {e}\"\n\n    if df is None or len(df) == 0:\n        return 1.0, \"No calls present; plausibility passes vacuously.\"\n\n    cols = {c: str(c).strip().lower() for c in df.columns}\n\n    def get_col(alts):\n        for c, v in cols.items():\n            for a in alts:\n                if a in v:\n                    return c\n        return None\n\n    dur_col = get_col([\"duration\"])  # hours\n    type_col = get_col([\"call type\",\"type\"])  # rehearsal/performance\n\n    # If duration missing, try computing from start/end\n    if dur_col is None:\n        start_col = get_col([\"start time\",\"start\"])   \n        end_col = get_col([\"end time\",\"end\"])       \n        if start_col and end_col:\n            try:\n                start = pd.to_datetime(df[start_col], errors='coerce')\n                end = pd.to_datetime(df[end_col], errors='coerce')\n                dur = (end - start).dt.total_seconds() / 3600.0\n                df['_dur_calc_'] = dur\n                dur_col = '_dur_calc_'\n            except Exception:\n                pass\n\n    if dur_col is None:\n        return 0.0, \"No Duration column and unable to compute from Start/End.\"\n\n    durations = pd.to_numeric(df[dur_col], errors='coerce')\n    valid = durations.notna() & (durations > 0.0) & (durations <= 12.0)\n\n    if type_col is not None:\n        types = df[type_col].astype(str).str.lower()\n        is_reh = types.str.contains('rehe')\n        is_perf = types.str.contains('perf')\n        # Apply type-specific bounds where type is known\n        valid_reh = (~is_reh) | ((durations >= 0.25) & (durations <= 8.0))\n        valid_perf = (~is_perf) | ((durations >= 0.5) & (durations <= 6.0))\n        valid = valid & valid_reh & valid_perf\n\n    if len(df) == 0:\n        return 1.0, \"Empty Calls sheet.\"\n\n    pct_valid = valid.mean()\n    if pct_valid >= 0.98:\n        score = 1.0\n    elif pct_valid >= 0.90:\n        score = 0.7\n    elif pct_valid >= 0.75:\n        score = 0.4\n    else:\n        score = 0.0\n    return score, f\"Valid durations: {pct_valid:.1%} of rows.\""}, {"type": "code", "name": "Payroll Categories Present (By Person)", "description": "Ensure the Payroll \u2013 By Person sheet includes core payroll categories and totals. Score by coverage fraction.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    import pandas as pd\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"Not a spreadsheet.\"\n    try:\n        path = context.files.get_path(output.id)\n        xl = pd.ExcelFile(path)\n        sheets = xl.sheet_names\n    except Exception as e:\n        return 0.0, f\"Unable to open Excel: {e}\"\n\n    def find_sheet(cands):\n        for s in sheets:\n            sl = s.lower()\n            for c in cands:\n                if c in sl:\n                    return s\n        return None\n\n    pay_sheet = find_sheet([\"payroll\", \"by person\", \"detail by musician\", \"person\"])\n    if not pay_sheet:\n        return 0.0, \"Payroll \u2013 By Person sheet not found.\"\n\n    df = context.files.read_excel(output.id, sheet_name=pay_sheet)\n    cols = {str(c): str(c).strip().lower() for c in df.columns}\n\n    required_keywords = {\n        'rehearsal': ['rehearsal'],\n        'performance': ['performance'],\n        'overtime': ['overtime','ot'],\n        'doubling': ['double','doubl'],\n        'cartage': ['cartage','porter'],\n        'vacation': ['vacation','vac'],\n        'pension': ['pension'],\n        'health': ['health','welfare','h&w'],\n        'total_gross': ['total gross','gross total','gross'],\n        'total_cost': ['total cost','grand total','all-in']\n    }\n\n    present = []\n    for key, kws in required_keywords.items():\n        found = False\n        for c, v in cols.items():\n            if any(k in v for k in kws):\n                found = True\n                break\n        present.append(found)\n    coverage = sum(present) / len(present)\n\n    if coverage >= 0.95:\n        score = 1.0\n    elif coverage >= 0.75:\n        score = 0.7\n    elif coverage >= 0.50:\n        score = 0.4\n    else:\n        score = 0.0\n\n    return score, f\"Category coverage: {coverage:.0%}.\""}, {"type": "code", "name": "Benefits/Premiums Plausibility (Rates)", "description": "Scan Rates for Pension %, Vacation %, and Health & Welfare amounts and check plausible bounds. Flexible extraction by row text and adjacent numeric.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    import pandas as pd, numpy as np, re\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"Not a spreadsheet.\"\n    try:\n        path = context.files.get_path(output.id)\n        xl = pd.ExcelFile(path)\n        sheets = xl.sheet_names\n    except Exception as e:\n        return 0.0, f\"Unable to open Excel: {e}\"\n\n    def find_sheet(cands):\n        for s in sheets:\n            sl = s.lower()\n            for c in cands:\n                if c in sl:\n                    return s\n        return None\n\n    rates_sheet = find_sheet([\"rate\",\"cba\"]) or find_sheet([\"rates\"]) or find_sheet([\"setup\",\"inputs\"])  # fallback\n    if not rates_sheet:\n        return 0.0, \"Rates sheet not found.\"\n\n    try:\n        df = context.files.read_excel(output.id, sheet_name=rates_sheet)\n    except Exception as e:\n        return 0.0, f\"Failed reading Rates: {e}\"\n\n    if df is None or df.empty:\n        return 0.0, \"Rates sheet empty.\"\n\n    # Normalize text view of the sheet for fuzzy search\n    sdf = df.copy()\n    for c in sdf.columns:\n        sdf[c] = sdf[c].astype(str).str.lower()\n\n    targets = {\n        'pension': {'ok_range': (0.01, 0.20)},   # 1% to 20%\n        'vacation': {'ok_range': (0.02, 0.12)},  # 2% to 12%\n        'health': {'ok_abs': (5.0, 1000.0)}      # flat per week; very wide bounds\n    }\n\n    found = {}\n\n    def extract_numeric(row):\n        nums = []\n        for val in row:\n            m = re.findall(r\"[-+]?[0-9]*\\.?[0-9]+%?\", str(val))\n            for x in m:\n                try:\n                    if x.endswith('%'):\n                        nums.append(float(x[:-1]) / 100.0)\n                    else:\n                        nums.append(float(x))\n                except:\n                    pass\n        return nums\n\n    for label in targets:\n        hit_rows = sdf.apply(lambda r: any(label in str(v) for v in r), axis=1)\n        if hit_rows.any():\n            idx = list(sdf[hit_rows].index)\n            ok = False\n            val_used = None\n            for i in idx:\n                nums = extract_numeric(list(df.loc[i]))\n                for n in nums:\n                    if label in ['pension','vacation']:\n                        lo, hi = targets[label]['ok_range']\n                        if lo <= n <= hi:\n                            ok = True; val_used = n; break\n                    else:\n                        lo, hi = targets[label]['ok_abs']\n                        if lo <= n <= hi:\n                            ok = True; val_used = n; break\n                if ok:\n                    break\n            found[label] = ok\n        else:\n            found[label] = False\n\n    coverage = sum(1 for k,v in found.items() if v) / len(targets)\n\n    if coverage >= 0.99:\n        score = 1.0\n    elif coverage >= 0.66:\n        score = 0.7\n    elif coverage >= 0.33:\n        score = 0.4\n    else:\n        score = 0.0\n\n    return score, f\"Benefits plausibility found: {found}.\""}, {"type": "code", "name": "Reconciliation: Wage Categories sum to Total Gross", "description": "If a Total Gross column exists on Payroll \u2013 By Person, verify it equals the sum of wage categories (rehearsal, performance, overtime, doubling, cartage, penalties if present) within a small tolerance.", "weight": 1.0, "code": "def evaluate(workflow, context):\n    import pandas as pd, numpy as np\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"Not a spreadsheet.\"\n    try:\n        path = context.files.get_path(output.id)\n        xl = pd.ExcelFile(path)\n        sheets = xl.sheet_names\n    except Exception as e:\n        return 0.0, f\"Unable to open Excel: {e}\"\n\n    def find_sheet(cands):\n        for s in sheets:\n            sl = s.lower()\n            for c in cands:\n                if c in sl:\n                    return s\n        return None\n\n    pay_sheet = find_sheet([\"payroll\", \"by person\", \"detail by musician\", \"person\"])\n    if not pay_sheet:\n        return 0.0, \"Payroll \u2013 By Person sheet not found.\"\n\n    df = context.files.read_excel(output.id, sheet_name=pay_sheet)\n    if df is None or df.empty:\n        return 0.0, \"Payroll \u2013 By Person is empty.\"\n\n    cols_l = {c: str(c).strip().lower() for c in df.columns}\n\n    def find_cols(kws):\n        res = []\n        for c, v in cols_l.items():\n            if any(k in v for k in kws):\n                res.append(c)\n        return res\n\n    wage_cols = set(find_cols([\"rehearsal\"])) | set(find_cols([\"performance\"])) | set(find_cols([\"overtime\",\"ot\"])) | set(find_cols([\"double\",\"doubl\"])) | set(find_cols([\"cartage\",\"porter\"])) | set(find_cols([\"penalty\"]))\n\n    total_gross_cols = find_cols([\"total gross\",\"gross total\",\"gross\"])\n\n    if not wage_cols or not total_gross_cols:\n        return 0.0, \"Missing wage category columns or Total Gross column.\"\n\n    # Use first total gross-like column\n    tg = pd.to_numeric(df[total_gross_cols[0]], errors='coerce').fillna(0.0)\n    wages_sum = pd.Series(0.0, index=df.index)\n    for c in wage_cols:\n        wages_sum = wages_sum + pd.to_numeric(df[c], errors='coerce').fillna(0.0)\n\n    diff = (tg - wages_sum).abs()\n    ok = (diff <= 1.00)  # $1 tolerance\n    if len(ok) == 0:\n        return 0.0, \"No rows to reconcile.\"\n\n    pct_ok = ok.mean()\n    if pct_ok >= 0.98:\n        score = 1.0\n    elif pct_ok >= 0.90:\n        score = 0.7\n    elif pct_ok >= 0.75:\n        score = 0.4\n    else:\n        score = 0.0\n\n    return score, f\"Rows reconciled within $1: {pct_ok:.1%}.\""}, {"type": "llm_judge", "name": "Compliance Mapping and Validation Coverage", "description": "Check that the model explicitly maps validations to CBA clauses and covers typical conflict categories (overtime triggers, minimum call lengths, weekly caps, doubling rules, cartage eligibility, turnaround/rest violations).", "weight": 1.0, "judge_prompt": "Review the workbook for explicit compliance logic and coverage. Look for a sheet like \"Validation Log\" (or similar) that:\n- Includes a Clause/Rule reference and a Description of the violation.\n- Categorizes issues (e.g., Overtime Trigger, Minimum Call, Doubling Premium eligibility/count, Cartage eligibility, Weekly services cap, Turnaround/rest).\n- Provides Suggested Correction/Resolution status.\nAlso accept an Instructions/Read Me sheet that clearly documents these checks and where they are surfaced (but the Validation Log sheet should still exist per Stage 1).\n\nScoring:\n- 1.0: Validation Log present with clause references and categories covering at least: overtime/minimum call, doubling, cartage, weekly caps/turnaround; fields for suggested correction and resolution are present.\n- 0.7: Validation Log present with clause references and 3-4 of the categories covered.\n- 0.4: Validation Log present but only 1-2 categories or missing clause references; or checks are described only in Read Me without clear logging structure.\n- 0.0: No clear compliance mapping or log of validation issues.", "expectation": "A Validation Log that ties detected conflicts to specific CBA clauses and covers common musician payroll compliance scenarios."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Maintainability", "description": "Holistic assessment of usability for contractors and maintainability as CBA rates change over time.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Usability and Contractor Experience", "description": "Assess clarity for weekly use by a music contractor: instructions, input highlighting, and easy workflow from Roster/Calls/Timecards to Payroll.", "weight": 3.0, "judge_prompt": "Evaluate practical usability for a third-party music contractor submitting weekly payroll:\n- Is there a clear entry point (Read Me / Instructions) with steps to follow?\n- Are input cells visibly distinguished (color, data validation lists) and protected areas separated from outputs?\n- Is the weekly workflow logical: Roster -> Calls -> Timecards -> Payroll \u2013 By Person -> Summary/export?\n- Are errors and flags easy to find (e.g., conditional formatting, Validation Log linked from key sheets)?\n- Are print/export views clean for submission?\n\nScoring:\n- 3.0: Highly usable with clear instructions, input highlighting, and an intuitive weekly flow.\n- 2.0: Generally usable; minor friction points.\n- 1.0: Usable but confusing without guidance.\n- 0.0: Disorganized or unclear workflow.", "expectation": "A contractor can follow instructions, enter data safely, see flags, and produce final totals without confusion."}, {"type": "llm_judge", "name": "Robustness and Maintainability", "description": "Assess whether the model is robust to different orchestra configurations and easy to update annually as rates change.", "weight": 2.0, "judge_prompt": "Assess the model\u2019s robustness and maintainability:\n- Are rates centralized (e.g., a single Rates sheet) with Effective Dates or Versions to support year-over-year updates?\n- Minimal hard-coded numbers in calculation areas (prefer references/lookups).\n- Structure scales to any orchestra size (tables with dynamic ranges rather than fixed cell blocks).\n- Clear separation of inputs, calculations, and outputs. Named tables/ranges or obvious headers.\n- Notes on how to update for a new CBA year.\n\nScoring:\n- 2.0: Strong: centralized rates, dynamic tables, clear separation, and update notes.\n- 1.0: Adequate but with some hard-coded items or limited guidance.\n- 0.0: Brittle structure or scattered hard-coding makes updates risky.", "expectation": "Centralized rates with effective periods, dynamic tables, and clear guidance to update the model for new CBA years."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "ce864f41-8584-49ba-b24f-9c9104b47bf0", "rubric": {"category_name": "Workload Distribution Tracker (Project Management Specialists)", "rationale": "This rubric uses a strict Stage 1 LLM gate to enforce an Excel-first, self-documenting structure that makes verification trivial. Stage 2 mixes code rules (deterministic checks on registry size, utilization math, department rollups, and budget exceed flags) with a lightweight LLM consistency check for the brief responses. Stage 3 evaluates professional quality and usefulness for executives. The structure ensures the agent proves correctness via verifiable artifacts rather than narrative claims.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Structured Output Gate (Excel + Required Sheets)", "description": "LLM-only gate verifying the required Excel workbook structure and presence of supplemental brief responses (either as a Summary sheet or a separate document). If this stage fails, the entire category is zeroed.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Workbook Structure and Supporting Brief Responses", "description": "Check that the candidate provides: (1) an Excel workbook for the Workload Distribution Tracker with required sheets and tables; and (2) brief responses to the 3 questions (either in a separate document or a 'Summary' sheet). Only verify structure and presence, not correctness of calculations.", "weight": 4.0, "judge_prompt": "You are evaluating whether the submission follows the exact structural requirements below. Consider all submitted files (Excel, PDF/DOCX/MD). Do NOT judge correctness of calculations or content quality\u2014only verify presence and structure. Be flexible with sheet naming if clearly equivalent (e.g., 'Dept Utilization' for 'Department Utilization').\n\nFormat Requirements:\n- Primary deliverable MUST be an Excel workbook (.xlsx) containing structured analysis.\n- It must include these sheets (names may vary slightly if obviously equivalent):\n  1) Stakeholder Registry \u2014 table columns including: [Employee Name | Department | Role/Position | Employment Status (FT/PT) | Estimated Hours/Month (Full Capacity)]. The table must list 23 employees.\n  2) Individual Allocation \u2014 contains:\n     - An Assumptions section showing a 15% admin/overhead reserve, the standard 40-hour FT workweek and 20-hour PT workweek, and that the period is March 2025.\n     - A main table with columns including: [Employee Name | Department | Employment Status | Capacity Hours/Month | Adjusted Capacity (85%) | Actual Hours (Mar 2025) | Utilization % | Risk Flag].\n  3) Department Utilization \u2014 table columns including: [Department | Headcount | Capacity Hours/Month | Adjusted Capacity (85%) | Actual Hours (Mar 2025) | Utilization % | Risk Flag]. Should show exactly FIVE departments.\n  4) Project Budget vs Actual \u2014 table columns including: [Project | Budgeted Hours (Mar) | Actual Hours (Mar 2025) | Variance (Actual - Budget) | Exceeded?].\n- Brief Responses to the 3 questions MUST be provided EITHER:\n  - As a separate document (PDF/DOCX/MD) with three numbered answers; OR\n  - As a dedicated 'Summary' (or similarly named) sheet inside the workbook with three numbered/bulleted answers.\n\nScoring (map to 0\u20134.0):\n- 4.0: Valid Excel + all 4 required sheets present with appropriately structured tables; Stakeholder Registry lists 23 employees; Individual Allocation has Assumptions and Risk Flag; Department Utilization shows 5 departments; Project Budget vs Actual includes Variance and Exceeded?; AND brief responses are present (either Summary sheet or separate document with 3 answers).\n- 3.2: Valid Excel + all 4 sheets present with tables, but missing either the Assumptions section OR the brief responses document/summary; minor column naming variations acceptable if clearly equivalent.\n- 2.2: Valid Excel + at least 3 of the 4 required sheets with tables (and Stakeholder Registry present), but missing others; brief responses may be missing.\n- 1.0: Valid Excel present but lacks most required structure (\u22642 required sheets or tables not identifiable).\n- 0.0: Not an Excel workbook or no identifiable required structure.\n\nOnly check presence/structure, not numerical correctness. Provide a brief rationale for the score.", "expectation": "An .xlsx with the four specified sheets/tables and either a Summary sheet or separate brief responses file. Stakeholder Registry lists 23 employees; Department Utilization shows five departments; Individual Allocation contains assumptions and risk flags; Project Budget vs Actual includes variance and an exceeded flag."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness Verification (Data and Logic)", "description": "Deterministic checks on registry size/columns, application of admin overhead, individual risk flags against thresholds, department rollups, and budget exceed flags; plus an LLM check that brief answers are consistent with the workbook.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Stakeholder Registry Integrity", "description": "Verify the Stakeholder Registry sheet exists, has key columns, and lists 23 unique employees. Flexible matching for column names.", "weight": 1.1, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"Primary output is not a spreadsheet.\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheets = xls.sheet_names\n    except Exception as e:\n        return 0.0, f\"Failed to open Excel: {e}\"\n\n    def norm(s):\n        return re.sub(r\"[^a-z0-9]\", \"\", str(s).lower())\n\n    # Find Stakeholder Registry sheet (flexible)\n    target_patterns = [\"stakeholderregistry\", \"stakeholders\", \"registry\"]\n    stake_sheet = None\n    for s in sheets:\n        sn = norm(s)\n        if any(p in sn for p in target_patterns):\n            stake_sheet = s\n            break\n    if not stake_sheet:\n        return 0.0, \"Stakeholder Registry sheet not found.\"\n\n    try:\n        df = context.files.read_excel(output.id, sheet_name=stake_sheet)\n    except Exception as e:\n        return 0.0, f\"Failed reading Stakeholder sheet: {e}\"\n\n    if df is None or df.empty:\n        return 0.0, \"Stakeholder sheet empty.\"\n\n    cols = {c: norm(c) for c in df.columns}\n\n    def find_col(key_tokens):\n        for c, cn in cols.items():\n            if all(tok in cn for tok in key_tokens):\n                return c\n        return None\n\n    name_col = find_col([\"name\"]) or find_col([\"employee\"]) or find_col([\"staff\"]) or find_col([\"person\"])\n    dept_col = find_col([\"dept\"]) or find_col([\"department\"]) or find_col([\"org\"])\n    role_col = find_col([\"role\"]) or find_col([\"position\"]) or find_col([\"title\"]) or find_col([\"job\"])\n    status_col = find_col([\"status\"]) or find_col([\"employment\"]) or find_col([\"fte\"]) or find_col([\"full\"]) or find_col([\"part\"]) or find_col([\"pt\"]) or find_col([\"ft\"]) \n    est_col = find_col([\"estimated\",\"hour\"]) or find_col([\"est\",\"hour\"]) or find_col([\"capacity\"]) or find_col([\"monthly\",\"hour\"]) or find_col([\"hours\",\"month\"]) \n\n    required_cols = [name_col, dept_col, role_col, status_col, est_col]\n    col_score = sum([1 for c in required_cols if c is not None]) / 5.0  # 0..1\n\n    # Count unique employees\n    count_score = 0.0\n    count_msg = \"\"\n    if name_col is not None:\n        names = df[name_col].dropna().astype(str).str.strip()\n        uniq = names[names != \"\"].nunique()\n        if uniq == 23:\n            count_score = 1.0\n            count_msg = \"23 unique employees found.\"\n        elif 20 <= uniq <= 26:\n            count_score = 0.55\n            count_msg = f\"Near 23 employees: found {uniq}.\"\n        else:\n            count_score = 0.0\n            count_msg = f\"Employee count off: found {uniq}.\"\n    else:\n        count_msg = \"Name column not found; cannot count employees.\"\n\n    # Weighting: columns 50%, count 50%\n    total = (col_score * 0.55 + count_score * 0.55) * 1.1\n    total = max(0.0, min(1.1, total))\n    fb = f\"Columns present={col_score*5:.0f}/5; {count_msg}\"\n    return total, fb"}, {"type": "code", "name": "Admin/Overhead Handling Evidenced (15%)", "description": "Check workbook evidence that 15% admin/overhead is considered (e.g., assumptions section, 'Adjusted Capacity (85%)', or mentions of 15%/85%/overhead/admin).", "weight": 0.5, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"Not a spreadsheet.\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheets = xls.sheet_names\n    except Exception as e:\n        return 0.0, f\"Excel open error: {e}\"\n\n    patterns = [r\"15\\s*%\", r\"85\\s*%\", r\"\\b0\\.85\\b\", r\"overhead\", r\"admin(istrative)?\", r\"adjust(ed)?\\s*capacity\"]\n\n    found = False\n    found_where = []\n    for s in sheets:\n        try:\n            df = context.files.read_excel(output.id, sheet_name=s)\n        except Exception:\n            continue\n        # Check column names and some sample cell values (first 2000 cells to avoid huge cost)\n        texts = [str(c) for c in df.columns]\n        # sample up to first 50 non-null values\n        sample_vals = df.head(50).astype(str).values.flatten().tolist()\n        texts.extend(sample_vals)\n        big = \"\\n\".join([t.lower() for t in texts if t and t != 'nan'])\n        if any(re.search(p, big) for p in patterns):\n            found = True\n            found_where.append(s)\n    if found:\n        return 0.5, f\"Overhead evidence found in sheets: {', '.join(found_where[:3])}\"\n    return 0.0, \"No clear evidence of 15% overhead/adjusted capacity.\""}, {"type": "code", "name": "Individual Risk Flags vs Thresholds", "description": "Recalculate utilization from Individual Allocation sheet and verify Risk Flag classifications: Underutilized <60%, Balanced 60\u201390%, Overutilized >90%. Tolerate minor rounding differences.", "weight": 1.2, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"Not a spreadsheet.\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheets = xls.sheet_names\n    except Exception as e:\n        return 0.0, f\"Excel open error: {e}\"\n\n    def norm(s):\n        return re.sub(r\"[^a-z0-9]\", \"\", str(s).lower())\n\n    # Find Individual Allocation sheet\n    target_patterns = [\"individualallocation\", \"allocation\", \"staffallocation\", \"resourceallocation\"]\n    alloc_sheet = None\n    for s in sheets:\n        sn = norm(s)\n        if any(p in sn for p in target_patterns):\n            alloc_sheet = s\n            break\n    if not alloc_sheet:\n        return 0.0, \"Individual Allocation sheet not found.\"\n\n    try:\n        df = context.files.read_excel(output.id, sheet_name=alloc_sheet)\n    except Exception as e:\n        return 0.0, f\"Read error: {e}\"\n    if df is None or df.empty:\n        return 0.0, \"Allocation sheet empty.\"\n\n    # Find columns\n    lc = {c: norm(c) for c in df.columns}\n    def fcol(tokens):\n        for c, cn in lc.items():\n            if all(tok in cn for tok in tokens):\n                return c\n        return None\n    adj_col = fcol([\"adjust\", \"cap\"]) or fcol([\"85\", \"cap\"]) or fcol([\"adj\", \"cap\"]) or fcol([\"net\", \"cap\"]) \n    act_col = fcol([\"actual\"]) or fcol([\"hours\", \"mar\"]) or fcol([\"actual\", \"mar\"]) or fcol([\"logged\"]) \n    util_col = fcol([\"util\"]) or fcol([\"utilization\"]) or fcol([\"%\"]) \n    flag_col = fcol([\"risk\"]) or fcol([\"flag\"]) or fcol([\"status\"]) or fcol([\"category\"]) \n\n    if adj_col is None or act_col is None:\n        return 0.0, \"Adjusted capacity or actual hours column not found.\"\n\n    work = df[[adj_col, act_col] + ([util_col] if util_col else []) + ([flag_col] if flag_col else [])].copy()\n\n    def to_num(x):\n        try:\n            if isinstance(x, str):\n                x = x.replace('%','')\n            return float(x)\n        except Exception:\n            return np.nan\n\n    work['adj'] = work[adj_col].apply(to_num)\n    work['act'] = work[act_col].apply(to_num)\n    work = work.replace([np.inf, -np.inf], np.nan).dropna(subset=['adj','act'])\n    work = work[work['adj'] > 0]\n    if work.empty or len(work) < 5:\n        return 0.0, \"Insufficient rows to validate risk flags.\"\n\n    util_calc = work['act'] / work['adj']\n    # If provided util_col exists, compare within tolerance (2 percentage points)\n    util_ok = 1.0\n    tol = 0.02\n    if util_col:\n        util_provided = work[util_col].apply(to_num)\n        # Normalize if provided appears 0-100\n        if util_provided.dropna().median() > 1.5:\n            util_provided = util_provided / 100.0\n        diff = (util_provided - util_calc).abs()\n        util_ok = float((diff <= tol).mean())  # fraction within tolerance\n\n    # Check risk flags\n    flag_ok = 1.0\n    if flag_col:\n        flags = work[flag_col].astype(str).str.lower()\n        def expected_flag(u):\n            if pd.isna(u):\n                return None\n            if u < 0.60 - 1e-6:\n                return 'under'\n            if u > 0.90 + 1e-6:\n                return 'over'\n            return 'bal'\n        exp = util_calc.apply(expected_flag)\n        def match(f, e):\n            if e is None:\n                return False\n            if e == 'under':\n                return 'under' in f or 'low' in f\n            if e == 'over':\n                return 'over' in f or 'burn' in f or 'high' in f\n            if e == 'bal':\n                return 'bal' in f or 'ok' in f or 'within' in f or 'normal' in f\n            return False\n        m = [match(f, e) for f, e in zip(flags, exp)]\n        flag_ok = float(np.mean(m)) if len(m) else 0.0\n    else:\n        flag_ok = 0.0  # require explicit risk flags per Stage 1 structure\n\n    # Combine: give 40% weight to utilization tolerance match, 60% to correct risk flags\n    score = (util_ok * 0.4 + flag_ok * 0.6) * 1.2\n    score = max(0.0, min(1.2, score))\n    fb = f\"Util within tol: {util_ok:.2f}; Flag match: {flag_ok:.2f}\"\n    return score, fb"}, {"type": "code", "name": "Department Rollup Consistency", "description": "Aggregate individual adjusted capacity and actuals by department and compare to Department Utilization sheet utilization within tolerance. Also verify there are 5 departments.", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"Not a spreadsheet.\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheets = xls.sheet_names\n    except Exception as e:\n        return 0.0, f\"Excel open error: {e}\"\n\n    def norm(s):\n        return re.sub(r\"[^a-z0-9]\", \"\", str(s).lower())\n\n    # Find sheets\n    alloc_sheet = None\n    dept_sheet = None\n    for s in sheets:\n        sn = norm(s)\n        if alloc_sheet is None and any(p in sn for p in [\"individualallocation\",\"allocation\",\"staffallocation\",\"resourceallocation\"]):\n            alloc_sheet = s\n        if dept_sheet is None and any(p in sn for p in [\"departmentutilization\",\"deptutil\",\"deptutilization\",\"department\"]):\n            dept_sheet = s\n    if not alloc_sheet or not dept_sheet:\n        return 0.0, \"Required sheets not found.\"\n\n    try:\n        da = context.files.read_excel(output.id, sheet_name=alloc_sheet)\n        dd = context.files.read_excel(output.id, sheet_name=dept_sheet)\n    except Exception as e:\n        return 0.0, f\"Read error: {e}\"\n    if da is None or da.empty or dd is None or dd.empty:\n        return 0.0, \"Empty sheets.\"\n\n    def fcol(df, tokens):\n        lc = {c: norm(c) for c in df.columns}\n        for c, cn in lc.items():\n            if all(t in cn for t in tokens):\n                return c\n        return None\n\n    dept_a = fcol(da, [\"dept\"]) or fcol(da, [\"department\"]) or fcol(da, [\"org\"]) \n    adj_a = fcol(da, [\"adjust\",\"cap\"]) or fcol(da, [\"85\",\"cap\"]) or fcol(da, [\"adj\",\"cap\"]) \n    act_a = fcol(da, [\"actual\"]) or fcol(da, [\"hours\",\"mar\"]) or fcol(da, [\"actual\",\"mar\"]) \n\n    dept_d = fcol(dd, [\"dept\"]) or fcol(dd, [\"department\"]) \n    util_d = fcol(dd, [\"util\"]) or fcol(dd, [\"utilization\"]) or fcol(dd, [\"%\"]) \n\n    if any(v is None for v in [dept_a, adj_a, act_a, dept_d, util_d]):\n        return 0.0, \"Missing required columns for rollup.\"\n\n    def to_num(x):\n        try:\n            if isinstance(x, str):\n                x = x.replace('%','')\n            return float(x)\n        except:\n            return np.nan\n\n    da2 = da[[dept_a, adj_a, act_a]].copy()\n    da2['adj'] = da2[adj_a].apply(to_num)\n    da2['act'] = da2[act_a].apply(to_num)\n    da2 = da2.replace([np.inf, -np.inf], np.nan).dropna(subset=['adj','act'])\n    grp = da2.groupby(da2[dept_a].astype(str).str.strip()).sum(numeric_only=True)\n    grp = grp[grp['adj'] > 0]\n    util_from_ind = (grp['act'] / grp['adj']).to_dict()  # 0..1\n\n    dd2 = dd[[dept_d, util_d]].copy()\n    dd2['dept'] = dd2[dept_d].astype(str).str.strip()\n    dd2['util'] = dd2[util_d].apply(to_num)\n    # Normalize if 0..100\n    if dd2['util'].dropna().median() > 1.5:\n        dd2['util'] = dd2['util'] / 100.0\n\n    merged = dd2.dropna(subset=['dept','util']).copy()\n    # Compute matches\n    tol = 0.05\n    matches = 0\n    total = 0\n    for _, r in merged.iterrows():\n        d = r['dept']\n        if d in util_from_ind:\n            total += 1\n            if abs(r['util'] - util_from_ind[d]) <= tol:\n                matches += 1\n    match_frac = (matches / total) if total > 0 else 0.0\n\n    # Department count check (expect 5)\n    depts_count = merged['dept'].nunique()\n    count_score = 1.0 if depts_count == 5 else (0.6 if 4 <= depts_count <= 6 else 0.0)\n\n    score = (match_frac * 0.7 + count_score * 0.3) * 0.5\n    score = max(0.0, min(0.5, score))\n    fb = f\"Dept util match: {match_frac:.2f}; Dept count: {depts_count}\"\n    return score, fb"}, {"type": "code", "name": "Project Budget Exceeded Flags", "description": "Validate that in Project Budget vs Actual sheet, rows with Actual > Budget are flagged as Exceeded (Yes/True), and others are not. Tolerate minor rounding.", "weight": 0.2, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"Not a spreadsheet.\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheets = xls.sheet_names\n    except Exception as e:\n        return 0.0, f\"Excel open error: {e}\"\n\n    def norm(s):\n        return re.sub(r\"[^a-z0-9]\", \"\", str(s).lower())\n\n    proj_sheet = None\n    for s in sheets:\n        sn = norm(s)\n        if any(p in sn for p in [\"projectbudgetvsactual\",\"budgetvsactual\",\"budget\",\"projects\"]):\n            proj_sheet = s\n            break\n    if not proj_sheet:\n        return 0.0, \"Project Budget vs Actual sheet not found.\"\n\n    try:\n        df = context.files.read_excel(output.id, sheet_name=proj_sheet)\n    except Exception as e:\n        return 0.0, f\"Read error: {e}\"\n    if df is None or df.empty:\n        return 0.0, \"Sheet empty.\"\n\n    lc = {c: norm(c) for c in df.columns}\n    def fcol(tokens):\n        for c, cn in lc.items():\n            if all(tok in cn for tok in tokens):\n                return c\n        return None\n    bud_col = fcol([\"budget\"]) or fcol([\"planned\"]) or fcol([\"budgeted\"]) \n    act_col = fcol([\"actual\"]) or fcol([\"logged\"]) \n    exc_col = fcol([\"exceed\"]) or fcol([\"over\"]) or fcol([\"flag\"]) \n\n    if bud_col is None or act_col is None:\n        return 0.0, \"Budget or Actual column missing.\"\n\n    def to_num(x):\n        try:\n            return float(str(x).replace('%',''))\n        except:\n            return np.nan\n\n    work = df[[bud_col, act_col] + ([exc_col] if exc_col else [])].copy()\n    work['b'] = work[bud_col].apply(to_num)\n    work['a'] = work[act_col].apply(to_num)\n    work = work.dropna(subset=['b','a'])\n    if work.empty:\n        return 0.0, \"No comparable rows.\"\n    tol = 0.5\n    correct = 0\n    total = 0\n    if exc_col:\n        for _, r in work.iterrows():\n            total += 1\n            should = (r['a'] - r['b']) > tol\n            val = str(r[exc_col]).strip().lower()\n            is_exc = val in ['yes','true','y','1','over','exceeded','flag','x'] or val == '\u2713'\n            if should == is_exc:\n                correct += 1\n        frac = correct / total if total else 0.0\n        return 0.2 * frac, f\"Exceeded flag accuracy: {frac:.2f} (n={total})\"\n    else:\n        # No exceeded flag; partial credit if a Variance column exists and positive/negative values appear\n        var_col = None\n        for c, cn in lc.items():\n            if \"variance\" in cn or (\"actual\" in cn and \"budget\" in cn):\n                var_col = c\n                break\n        if var_col is not None:\n            return 0.05, \"No 'Exceeded?' column, but variance present.\"\n        return 0.0, \"No 'Exceeded?' flag or variance column.\""}, {"type": "llm_judge", "name": "Brief Answers Fact Consistency", "description": "Check that the supplemental brief responses (separate doc OR 'Summary' sheet) correctly reflect the workbook\u2019s findings: departments within \u00b15% of 100% utilization are called balanced; any departments outside are identified; individuals flagged under <60% or over >90% are mentioned; and projects exceeding budget are noted.", "weight": 0.5, "judge_prompt": "Review the entire submission. Locate the brief responses (either a separate PDF/DOCX/MD or a 'Summary' sheet in the workbook). Compare their statements to the actual tables in the Excel workbook:\n- Department Utilization: Do the answers accurately state which departments are within \u00b15 percentage points of 100% utilization and which are not? (Use the Department Utilization sheet.)\n- Individual Allocation: Do the answers reference individuals at risk (<60% underutilized or >90% overutilized) consistent with the Risk Flag column? A short list or count is acceptable.\n- Project Budget vs Actual: Do the answers correctly state whether any projects exceeded budget (Actual > Budget)?\n\nScoring (0\u20130.5):\n- 0.5: All three answers align with the workbook\u2019s tables (no material contradictions). Any counts or names cited are consistent.\n- 0.3: Mostly aligns; at most one minor inconsistency or omission.\n- 0.1: Significant inconsistencies (2+ areas) or very vague statements that do not match tables.\n- 0.0: No brief responses found or they contradict the workbook in most areas.\n\nOnly check consistency with the workbook, not writing quality.", "expectation": "Three brief answers that accurately summarize which departments/people/projects meet thresholds, consistent with the workbook\u2019s flags and figures."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Presentation and Executive Usefulness", "description": "Holistic LLM assessment of formatting, clarity, and usefulness for leadership decision-making.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Quality and Insightfulness", "description": "Assess the workbook\u2019s presentation and the brief responses\u2019 clarity for an executive audience.", "weight": 2.0, "judge_prompt": "Evaluate the professionalism and usefulness of the deliverable for an executive audience. Consider:\n- Workbook formatting: clear sheet names, frozen headers, readable column labels, consistent number formats (hours, percentages), and helpful visuals (conditional formatting or simple charts) where appropriate.\n- Traceability: presence of an Assumptions section (overhead %, FT/PT hours), and clear formulas/columns for capacity, adjusted capacity, actuals, utilization, and risk flags.\n- Clarity of brief responses: concise, direct answers to the 3 questions with specific references (counts, names, or thresholds) without unnecessary verbosity.\n- Actionability: do outputs help leadership spot over/underutilization and budget overruns at a glance?\n\nScoring (0\u20132.0):\n- 2.0: Highly professional and clear; easy to navigate; formatting and labels are consistent; brief responses are succinct and actionable.\n- 1.2: Generally good; minor formatting or clarity issues; still useful to leadership.\n- 0.6: Adequate but cluttered or confusing; limited usefulness without additional cleanup.\n- 0.0: Poorly formatted or unclear; not suitable for executive use.", "expectation": "A clean, well-labeled workbook with clear utilization/risk indicators and concise, useful brief responses fit for the CEO."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "61e7b9c6-0051-429f-a341-fda9b6578a84", "rubric": {"category_name": "Clinical Formulary Curation (Menopause Service)", "rationale": "This rubric enforces a self-documenting, verifiable Excel formulary for a U.S. online Women's Health menopause service. Stage 1 (LLM-only) mandates an exact workbook shape that makes verification trivial. Stage 2 uses code and LLM checks to verify correctness and clinical coverage, leveraging Stage 1\u2019s structure. Stage 3 assesses professional quality and strategic value for provider use. All outputs must be file-based, with the primary deliverable as an Excel workbook.", "max_total_score": 13.0, "stages": [{"name": "Stage 1 \u2013 Structure Gate: Excel Formulary Shape Enforcement", "description": "LLM-only gate that checks the workbook is an Excel file with required sheets, headers, and structural elements to enable automated verification. If this gate fails, the entire category is zeroed.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Excel Formulary Requirements", "description": "Check that the candidate output is a well-structured Excel workbook for a menopause formulary with the required sheets and tables. Only verify presence/structure, not content correctness.", "weight": 4.0, "judge_prompt": "You are verifying that the output is an Excel workbook (XLSX or XLS) with the exact structural elements needed to evaluate a menopause formulary. Do not judge correctness of clinical content or prices; only confirm presence and clear structure. Be flexible with sheet names if obviously equivalent.\n\nREQUIRED WORKBOOK STRUCTURE\nFormat:\n- Must be an Excel spreadsheet (.xlsx or .xls). Not PDF/DOCX/CSV.\n\nSheet A: \"Formulary\" (or clearly equivalent like \"Menopause Formulary\" or \"Medication List\")\n- A single tabular sheet with one row per medication/formulation.\n- Must include visible column headers (flexible wording allowed) covering ALL of the following fields:\n  1) Therapy Category/Class (e.g., systemic estrogen, progestin, SERM, local vaginal estrogen, nonhormonal VMS, etc.)\n  2) Generic Name\n  3) Selected Brand (only one brand per identical formulation; if multiple exist, the clinic\u2019s chosen brand should be identified here)\n  4) Dosage Form (e.g., tablet, capsule, transdermal patch, gel, spray, vaginal cream, ring, etc.)\n  5) Route (e.g., oral, transdermal, vaginal)\n  6) Strength(s) (and units)\n  7) FDA Status (values like: MHT FDA-approved, Non-hormonal FDA-approved, Off-label for menopause symptoms)\n  8) Indication/Symptoms Targeted\n  9) Typical Starting Dose\n  10) Titration/Notes\n  11) Contraindications/Warnings (black box warning if applicable)\n  12) Monitoring/Key Safety Notes\n  13) 30-day Cash Price (USD) \u2013 numeric or currency formatted\n  14) Price Source (URL or vendor name, e.g., GoodRx)\n  15) Price Date (date of quote)\n  (Optional) NDC/SKU\n\nSheet B: \"Price Snapshots\" or \"Quotes\" (raw price evidence)\n- A table capturing the quoted prices used to derive the Formulary price column.\n- Columns should include: Drug/Formulation, Quantity (30-day), Pharmacy/Vendor, Price, Coupon/Notes, Source URL, Date.\n\nSheet C: \"Equivalence Map\" or \"Brand Selection\" (deduplication rationale)\n- A table that maps identical formulations with multiple brands and shows the single brand the clinic selected.\n- Include columns like: Formulation Key (generic + route + dosage form + strength), Brands Available, Selected Brand, Rationale/Notes.\n\nSheet D: \"Methodology & Assumptions\" (narrative)\n- A clearly written text section (at least 5 sentences) explaining:\n  - Sources for prices (e.g., GoodRx) and whether coupon/cash prices were used\n  - The reference quantity/assumption for a 30-day supply\n  - How brands were selected when multiple equivalent brands exist\n  - How FDA-approved vs off-label use was identified\n  - Date range when prices were collected\n- Also include a brief \"Version/Change Log\" section on this sheet (even a small dated note is acceptable).\n\nSheet E: \"Summary\" (computed counts)\n- A small table with summary metrics, including all of the following:\n  - Count of MHT FDA-approved items\n  - Count of Non-hormonal FDA-approved items\n  - Count of Off-label items\n  - Number of unique formulations\n  - Number of items missing price\n\nSCORING (STRUCTURE ONLY)\n- 4.0: All five sheets present with clearly labeled tables/sections, and all required columns/sections exist on each sheet.\n- 3.5: One non-core supporting element missing or thin (e.g., Summary present but missing one metric; Methodology text slightly under 5 sentences).\n- 3.0: Core sheets present (Formulary + Methodology + either Price Snapshots or Equivalence Map), but one of the other required sheets/columns missing.\n- 2.0: Only the Formulary sheet present with some required columns, but key supporting sheets missing.\n- 0.0: Not an Excel workbook OR structure insufficient to verify (missing Formulary or virtually all required elements).\n\nOnly evaluate presence and structural completeness. Do not check accuracy of drug lists, clinical statements, or prices.", "expectation": "An Excel workbook with 5 structured sheets: Formulary, Price Snapshots, Equivalence Map, Methodology & Assumptions (with a brief Change Log), and Summary. The Formulary sheet includes all key columns enabling verification and pricing review."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification: Data Validity and Clinical Coverage", "description": "Code and LLM checks that leverage the enforced shape to verify fields, bounds, deduplication, FDA-status labeling, price evidence, and minimum coverage of key menopause therapies.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Sheet/Column Presence and Readability", "description": "Confirm the output is a spreadsheet and that a Formulary-like sheet is readable with core columns present (fuzzy-matched). Penalize if many required columns are missing.", "weight": 1.2, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        # Find a likely formulary sheet\n        file_path = context.files.get_path(output.id)\n        xl = pd.ExcelFile(file_path)\n        sheet_names = [s for s in xl.sheet_names]\n        # Flexible match for formulary sheet name\n        target_sheet = None\n        for s in sheet_names:\n            sl = s.lower()\n            if any(k in sl for k in [\"formulary\",\"medication\",\"drug list\",\"menopause\"]):\n                target_sheet = s\n                break\n        if target_sheet is None:\n            # fallback to first sheet\n            target_sheet = sheet_names[0]\n        df = pd.read_excel(file_path, sheet_name=target_sheet)\n        # Normalize columns\n        cols = [str(c).strip().lower() for c in df.columns]\n        def has_any(keys):\n            return any(any(k in c for k in keys) for c in cols)\n        # Core required columns (fuzzy)\n        reqs = {\n            \"generic\": [\"generic\"],\n            \"brand\": [\"brand\", \"selected brand\"],\n            \"dosage_form\": [\"dosage form\", \"form\"],\n            \"route\": [\"route\"],\n            \"strength\": [\"strength\"],\n            \"fda_status\": [\"fda\", \"status\"],\n            \"price\": [\"price\", \"cash\"],\n            \"price_source\": [\"source\", \"goodrx\", \"url\"],\n            \"price_date\": [\"date\"],\n        }\n        score_bits = []\n        missing = []\n        for k, keys in reqs.items():\n            ok = has_any(keys)\n            score_bits.append(1.0 if ok else 0.0)\n            if not ok:\n                missing.append(k)\n        score = np.mean(score_bits) if score_bits else 0.0\n        feedback = f\"Formulary sheet: found={target_sheet}; missing core fields: {', '.join(missing) if missing else 'none'}.\"\n        return score, feedback\n    except Exception as e:\n        return 0.0, f\"Error reading spreadsheet: {e}\""}, {"type": "code", "name": "Price Validity and Evidence Links", "description": "Verify 30-day price values are numeric and plausible, sources look like URLs or known vendors, and Price Date parses. Partial credit by proportion of valid rows.", "weight": 1.2, "code": "import re\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        file_path = context.files.get_path(output.id)\n        xl = pd.ExcelFile(file_path)\n        sheet_names = [s for s in xl.sheet_names]\n        # Find likely formulary sheet\n        target_sheet = None\n        for s in sheet_names:\n            sl = s.lower()\n            if any(k in sl for k in [\"formulary\",\"medication\",\"drug list\",\"menopause\"]):\n                target_sheet = s\n                break\n        if target_sheet is None:\n            target_sheet = sheet_names[0]\n        df = pd.read_excel(file_path, sheet_name=target_sheet)\n        df.columns = [str(c).strip().lower() for c in df.columns]\n        # Identify columns\n        def pick_col(cands):\n            for c in df.columns:\n                for k in cands:\n                    if k in c:\n                        return c\n            return None\n        price_col = pick_col([\"30\", \"price\", \"cash\"]) or pick_col([\"price\"]) \n        source_col = pick_col([\"source\", \"goodrx\", \"url\", \"vendor\"]) \n        date_col = pick_col([\"price date\", \"date\"]) \n        if price_col is None:\n            return 0.0, \"Price column not found.\"\n        vals = df[price_col].copy()\n        # Coerce currency to numeric\n        def to_num(x):\n            if pd.isna(x):\n                return np.nan\n            s = str(x)\n            s = re.sub(r\"[^0-9.]+\", \"\", s)\n            try:\n                return float(s) if s else np.nan\n            except:\n                return np.nan\n        nums = vals.apply(to_num)\n        # Plausible monthly price bounds\n        valid_price = nums.apply(lambda v: 0 < v <= 5000 if pd.notna(v) else False)\n        pct_valid_price = valid_price.mean() if len(valid_price) else 0.0\n        # Source check\n        known_vendors = [\"goodrx\",\"costplus\",\"singlecare\",\"walgreens\",\"cvs\",\"walmart\",\"amazon\"]\n        src_ok = 0.0\n        if source_col is not None:\n            src_series = df[source_col].astype(str).str.lower()\n            src_ok = src_series.apply(lambda s: (\"http\" in s) or any(v in s for v in known_vendors)).mean()\n        # Date parse check (allow wide range)\n        date_ok = 0.0\n        if date_col is not None:\n            def parse_ok(s):\n                try:\n                    _ = pd.to_datetime(s, errors='coerce')\n                    return pd.notna(_)\n                except:\n                    return False\n            date_ok = df[date_col].apply(parse_ok).mean()\n        # Aggregate\n        parts = [pct_valid_price, src_ok, date_ok]\n        # If source/date columns missing, count as 0 in average\n        denom = sum(1 for x in parts if x is not None)\n        score = float(np.nanmean(parts)) if denom > 0 else pct_valid_price\n        feedback = f\"Valid price pct={pct_valid_price:.2f}; source_ok={src_ok:.2f}; date_ok={date_ok:.2f}.\"\n        return max(0.0, min(1.0, score)), feedback\n    except Exception as e:\n        return 0.0, f\"Error verifying prices: {e}\""}, {"type": "code", "name": "One-Brand-Per-Identical-Formulation Check", "description": "Identify identical formulations (generic + route + dosage form + strength) carrying multiple brands. Partial credit if a Selected Brand indicator exists and duplicate clusters are resolved.", "weight": 1.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        file_path = context.files.get_path(output.id)\n        xl = pd.ExcelFile(file_path)\n        # Find formulary-like sheet\n        target_sheet = None\n        for s in xl.sheet_names:\n            sl = s.lower()\n            if any(k in sl for k in [\"formulary\",\"medication\",\"drug list\",\"menopause\"]):\n                target_sheet = s\n                break\n        if target_sheet is None:\n            target_sheet = xl.sheet_names[0]\n        df = pd.read_excel(file_path, sheet_name=target_sheet)\n        df.columns = [str(c).strip().lower() for c in df.columns]\n        def pick(cands):\n            for c in df.columns:\n                if any(k in c for k in cands):\n                    return c\n            return None\n        gcol = pick([\"generic\"]) or pick([\"drug\", \"name\"]) \n        bcol = pick([\"selected brand\"]) or pick([\"brand\"]) \n        formcol = pick([\"dosage form\",\"form\"]) \n        routecol = pick([\"route\"]) \n        strcol = pick([\"strength\"]) \n        # If key columns missing, bail with 0\n        if not all([gcol, bcol, formcol, routecol, strcol]):\n            return 0.0, \"Missing key columns for deduplication.\"\n        # Build formulation key\n        def norm(s):\n            return re.sub(r\"\\s+\", \" \", str(s).strip().lower())\n        key = (\n            df[gcol].map(norm).fillna(\"\") + \"|\" +\n            df[routecol].map(norm).fillna(\"\") + \"|\" +\n            df[formcol].map(norm).fillna(\"\") + \"|\" +\n            df[strcol].map(norm).fillna(\"\")\n        )\n        df['__key'] = key\n        # For each key, count distinct brands\n        grp = df.groupby('__key')[bcol].nunique(dropna=True)\n        # clusters with >1 brands are potential violations\n        multi = grp[grp > 1]\n        total_keys = grp.shape[0]\n        if total_keys == 0:\n            return 0.0, \"No rows to evaluate.\"\n        violation_rate = (multi.shape[0] / total_keys)\n        score = 1.0 - min(1.0, violation_rate)\n        feedback = f\"Identical formulation clusters: {total_keys}, multi-brand clusters: {multi.shape[0]} (violation_rate={violation_rate:.2f}).\"\n        return max(0.0, min(1.0, score)), feedback\n    except Exception as e:\n        return 0.0, f\"Error in deduplication check: {e}\""}, {"type": "code", "name": "FDA Status Label Validity and Exclusions", "description": "Check FDA status values are constrained and flag likely non-FDA items (e.g., compounded hormones, estriol).", "weight": 0.8, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        file_path = context.files.get_path(output.id)\n        xl = pd.ExcelFile(file_path)\n        # Find formulary sheet\n        target_sheet = None\n        for s in xl.sheet_names:\n            sl = s.lower()\n            if any(k in sl for k in [\"formulary\",\"medication\",\"drug list\",\"menopause\"]):\n                target_sheet = s\n                break\n        if target_sheet is None:\n            target_sheet = xl.sheet_names[0]\n        df = pd.read_excel(file_path, sheet_name=target_sheet)\n        df.columns = [str(c).strip().lower() for c in df.columns]\n        # FDA status column\n        fcol = None\n        for c in df.columns:\n            cl = c.lower()\n            if (\"fda\" in cl) or (\"status\" in cl):\n                fcol = c\n                break\n        if fcol is None:\n            return 0.0, \"No FDA status column found.\"\n        vals = df[fcol].astype(str).str.lower()\n        allowed = [\n            \"mht fda-approved\",\"mht fda approved\",\"mht approved\",\"fda-approved mht\",\n            \"non-hormonal fda-approved\",\"non hormonal fda-approved\",\"non-hormonal fda approved\",\n            \"off-label\",\"off label\"\n        ]\n        def ok_val(s):\n            s = s.strip()\n            # Flexible contains checks\n            if (\"off\" in s and \"label\" in s):\n                return True\n            if (\"fda\" in s and \"approved\" in s and (\"mht\" in s or \"hormone\" in s or \"non\" in s)):\n                return True\n            # Accept simple forms\n            if s in [\"fda-approved\",\"fda approved\",\"approved\"]:\n                return True\n            return False\n        validity = vals.apply(ok_val).mean()\n        # Exclusion terms suggest non-FDA products\n        bad_terms = [\"compounded\",\"compound\",\"estriol\",\"bioidentical cream\",\"otc supplement\"]\n        bad_hit = vals.copy()*0  # placeholder to keep code simple\n        # search across entire row text\n        row_text = df.astype(str).apply(lambda r: \" \".join(r.values), axis=1).str.lower()\n        has_bad = row_text.apply(lambda s: any(bt in s for bt in bad_terms)).mean()\n        # Score: reward valid labels, penalize presence of bad terms\n        score = max(0.0, min(1.0, validity * (1.0 - has_bad)))\n        feedback = f\"FDA status validity={validity:.2f}; non-FDA red-flag prevalence={has_bad:.2f}.\"\n        return score, feedback\n    except Exception as e:\n        return 0.0, f\"Error in FDA status validation: {e}\""}, {"type": "code", "name": "Minimum Clinical Coverage Set", "description": "Check that the formulary covers key menopause therapy categories: systemic estrogen, progestin for endometrial protection, local vaginal estrogen, SERM/combination, dyspareunia agents, nonhormonal FDA-approved VMS, and at least one common off-label option.", "weight": 1.2, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        file_path = context.files.get_path(output.id)\n        xl = pd.ExcelFile(file_path)\n        target_sheet = None\n        for s in xl.sheet_names:\n            sl = s.lower()\n            if any(k in sl for k in [\"formulary\",\"medication\",\"drug list\",\"menopause\"]):\n                target_sheet = s\n                break\n        if target_sheet is None:\n            target_sheet = xl.sheet_names[0]\n        df = pd.read_excel(file_path, sheet_name=target_sheet)\n        df.columns = [str(c).strip().lower() for c in df.columns]\n        # Build searchable text per row\n        row_text = df.astype(str).apply(lambda r: \" \".join(r.values), axis=1).str.lower()\n        def has_any(subs):\n            return row_text.apply(lambda s: any(x in s for x in subs)).any()\n        # Categories\n        systemic_estrogen = has_any([\"estradiol\", \"conjugated estrogens\", \"cee \", \"premarin\", \"climara\", \"vivelle\", \"minivelle\", \"alora\", \"evamist\", \"estrogel\"])\n        progestin_protection = has_any([\"progesterone\", \"micronized progesterone\", \"prometrium\", \"medroxyprogesterone\", \"mpa \"])\n        local_vaginal_estrogen = has_any([\"vaginal estradiol\", \"estradiol vaginal\", \"vagifem\", \"estrace vaginal\", \"estring\", \"yuvafem\", \"imvexxy\", \"premarin vaginal\"])\n        serm_combo = has_any([\"bazedoxifene\", \"duavee\"])  # CE/BZA\n        dyspareunia_agents = has_any([\"ospemifene\", \"prasterone\", \"intrarosa\"])\n        nonhormonal_fda_vms = has_any([\"fezolinetant\", \"veozah\", \"paroxetine 7.5\", \"brisdelle\"])  # FDA-approved for VMS\n        offlabel_common = has_any([\"venlafaxine\", \"desvenlafaxine\", \"escitalopram\", \"sertraline\", \"gabapentin\", \"clonidine\"])\n        hits = [systemic_estrogen, progestin_protection, local_vaginal_estrogen, serm_combo, dyspareunia_agents, nonhormonal_fda_vms, offlabel_common]\n        score = sum(1 for h in hits if h) / len(hits)\n        feedback = \"Coverage: \" + \", \".join([\n            f\"systemic_estrogen={systemic_estrogen}\",\n            f\"progestin={progestin_protection}\",\n            f\"local_vaginal_estrogen={local_vaginal_estrogen}\",\n            f\"serm_combo={serm_combo}\",\n            f\"dyspareunia={dyspareunia_agents}\",\n            f\"nonhormonal_fda_vms={nonhormonal_fda_vms}\",\n            f\"offlabel_common={offlabel_common}\"\n        ])\n        return score, feedback\n    except Exception as e:\n        return 0.0, f\"Error in coverage check: {e}\""}, {"type": "code", "name": "Summary Consistency Check", "description": "If a Summary sheet exists, cross-check counts (FDA categories, missing prices, unique formulations). Partial credit based on agreement.", "weight": 0.6, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        file_path = context.files.get_path(output.id)\n        xl = pd.ExcelFile(file_path)\n        sheets = [s for s in xl.sheet_names]\n        # Read formulary\n        formulary_sheet = None\n        for s in sheets:\n            sl = s.lower()\n            if any(k in sl for k in [\"formulary\",\"medication\",\"drug list\",\"menopause\"]):\n                formulary_sheet = s\n                break\n        if formulary_sheet is None:\n            return 0.0, \"No formulary sheet found.\"\n        df = pd.read_excel(file_path, sheet_name=formulary_sheet)\n        df.columns = [str(c).strip().lower() for c in df.columns]\n        # Identify columns\n        def pick(cands):\n            for c in df.columns:\n                if any(k in c for k in cands):\n                    return c\n            return None\n        fda_col = pick([\"fda\",\"status\"]) \n        price_col = pick([\"price\"]) \n        # Compute our counts\n        our_missing_price = 0\n        if price_col:\n            our_missing_price = df[price_col].isna().sum()\n        our_unique = df.shape[0]\n        our_mht = our_nonh = our_off = 0\n        if fda_col:\n            vals = df[fda_col].astype(str).str.lower()\n            our_mht = vals.str.contains(\"mht|hormone\").sum()\n            our_off = vals.str.contains(\"off\\s*-?label\").sum()\n            # non-hormonal FDA approved\n            our_nonh = vals.str.contains(\"non.*fda.*approved\").sum()\n        # Try to find Summary sheet\n        summary_sheet = None\n        for s in sheets:\n            sl = s.lower()\n            if \"summary\" in sl:\n                summary_sheet = s\n                break\n        if not summary_sheet:\n            return 0.3, \"No Summary sheet; partial credit.\"\n        sm = pd.read_excel(file_path, sheet_name=summary_sheet)\n        # Convert to key-value by scanning text\n        sm_str = sm.astype(str).applymap(lambda x: x.strip().lower())\n        text = \" \".join([\" \".join(row) for row in sm_str.values.tolist()])\n        # Heuristics: look for numbers near keywords\n        def find_near(keyword):\n            m = re.search(rf\"{keyword}[^0-9]*([0-9]+)\", text)\n            return int(m.group(1)) if m else None\n        sm_mht = find_near(\"mht\") or find_near(\"hormone\")\n        sm_nonh = find_near(\"non\\s*-?hormonal\")\n        sm_off = find_near(\"off\\s*-?label\")\n        sm_missing = find_near(\"missing price\")\n        sm_unique = find_near(\"unique formulation\") or find_near(\"unique items\") or find_near(\"total items\")\n        # Compare with tolerances\n        matches = 0\n        total_checks = 0\n        def cmp(a,b):\n            if a is None or b is None:\n                return None\n            return abs(a-b) <= 2  # tolerance\n        for a,b in [(our_mht, sm_mht),(our_nonh, sm_nonh),(our_off, sm_off),(our_missing_price, sm_missing),(our_unique, sm_unique)]:\n            res = cmp(a,b)\n            if res is None:\n                continue\n            total_checks += 1\n            matches += 1 if res else 0\n        score = (matches/total_checks) if total_checks>0 else 0.3\n        feedback = f\"Summary agreement: {matches}/{total_checks} within tolerance.\"\n        return score, feedback\n    except Exception as e:\n        return 0.0, f\"Error in summary consistency: {e}\""}, {"type": "llm_judge", "name": "Methodology Coherence and Sourcing Adequacy", "description": "LLM evaluates whether the Methodology & Assumptions sheet clearly explains price sources, 30-day assumptions, brand selection logic, FDA vs off-label identification, and date range of collection.", "weight": 0.9, "judge_prompt": "Check the \"Methodology & Assumptions\" sheet for a clear, coherent narrative (\u22655 sentences) that explicitly states: (1) price sources (e.g., GoodRx) and whether coupon/cash were used, (2) 30-day quantity assumptions, (3) rules for choosing a single brand among identical formulations, (4) how off-label items were identified while ensuring only FDA-approved products are listed, and (5) the date range when prices were captured. Score based on completeness and clarity of these five items. Do not judge clinical accuracy or price correctness\u2014just whether the explanation is present and understandable.", "expectation": "A brief, well-structured methodology paragraph covering all five items; includes a small Version/Change Log note with date."}, {"type": "llm_judge", "name": "Drug Naming and Dosage Clarity", "description": "LLM checks that drug names, dosage forms, strengths, and routes are clearly specified on the Formulary sheet with minimal ambiguity.", "weight": 0.6, "judge_prompt": "Scan the Formulary sheet and judge whether each row clearly specifies the generic name, selected brand, dosage form, route, and strength(s) without ambiguity. Occasional minor omissions are acceptable, but frequent ambiguity, missing strengths, or unclear routes should reduce the score. Do not evaluate correctness of choices\u2014only clarity and completeness of naming and dosing fields.", "expectation": "Clear drug naming, explicit dosage forms/routes, and strength units visible for most rows with few or no ambiguous entries."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Quality and Fitness for Clinical Use", "description": "LLM judges the professional presentation, practicality for prescribers, and strategic fit for the clinic\u2019s policies.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Usability", "description": "Evaluate formatting, readability, filtering/sorting readiness, and inclusion of key fields that make this usable for clinicians during prescribing.", "weight": 2.0, "judge_prompt": "Assess the Formulary workbook for professional presentation and ease of use by clinicians: consistent formatting, readable headers, filters or sort-friendly tables, frozen header rows if visible, and helpful columns (indications, contraindications/black box, monitoring). Higher scores for clean layout, consistent styling, and clear separation of categories. Do not judge the accuracy of content, only presentation and usability.", "expectation": "Clean, consistent tables with clinician-friendly formatting that support quick lookup and decision-making."}, {"type": "llm_judge", "name": "Strategic Fit and Policy Alignment", "description": "Evaluate whether the deliverable aligns with clinic policy: only FDA-approved medications (while allowing off-label use of FDA-approved drugs), single-brand selection per identical formulation, and inclusion of price context for shared decision making.", "weight": 1.0, "judge_prompt": "Evaluate whether the formulary appears aligned with the clinic\u2019s strategy: (1) Only FDA-approved medications are listed (no compounded hormones, estriol-only products, or supplements); (2) Off-label items are clearly marked as such; (3) For identical formulations with multiple brands, only one brand is chosen; (4) Prices are present and framed for shared decision-making. Score higher if these policy elements are consistently reflected in the workbook\u2019s structure and annotations, without assessing clinical correctness.", "expectation": "Workbook reflects policy constraints and supports shared decision-making via transparent pricing and clear off-label labeling."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "ab81b076-e5d8-473a-9bdb-7ea7c38f6ebc", "rubric": {"category_name": "Automotive Parts Check-In Procedure (Wholesale Trade)", "rationale": "This rubric enforces a self-documenting, verifiable PDF procedure for dealership parts check-in. Stage 1 (LLM-only) mandates a specific PDF structure with distinct stock vs. critical workflows, exception handling, visual guidance, and a final checklist. Stage 2 mixes code and LLM checks for correctness and consistency that are machine-verifiable (keywords, field coverage, ordering). Stage 3 evaluates professional quality, usability, and visual effectiveness for the target audience.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Format and Structural Gate (PDF Required)", "description": "Gate: The candidate must provide a 1\u20133 page PDF with the exact structural elements to enable verification. No code checks here; LLM judge only.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 5.6, "rules": [{"type": "llm_judge", "name": "Structured PDF with Required Sections", "description": "Verify the output is a 1\u20133 page PDF that contains all required sections and structural elements enabling trivial verification.", "weight": 8.0, "judge_prompt": "You are evaluating whether the submitted document satisfies the REQUIRED STRUCTURE for a dealership parts check-in procedure. Only check presence and structure, not content quality.\n\nFormat Requirements:\n- File must be a PDF (not DOCX, not text, not Excel)\n- Length: 1\u20133 pages\n- Professionally organized with clear section headers and numbered/bulleted steps\n\nRequired Sections and Elements (flexible on exact naming, but structure must be clear):\n1) Title and Purpose/Scope (on page 1)\n   - Title indicating this is a parts check-in procedure\n   - A short Purpose/Scope statement\n2) Distinct Procedures: Stock vs. Critical Orders\n   - Separate, clearly labeled subsections for Stock Orders and Critical Orders\n   - Each subsection must include a step-by-step numbered process from delivery arrival through system confirmation\n3) Paperwork & Verification\n   - A section that references the Bill of Lading (BOL) or packing slip and matching/verification steps\n4) Exceptions/Issues Handling\n   - Instructions for damaged parts, missing items/shortages, and BOL discrepancies (each must be addressed)\n5) Visual Guidance\n   - At least two figures/images or clearly described representative visuals with captions showing:\n     a) How to document damage (photo/annotation guidance)\n     b) How to mark items for visibility (e.g., red tag/label) or highlight discrepancies on paperwork\n   - Real photos or representative/stock images or clearly described placeholders are acceptable\n6) Communication with Manufacturer\u2019s Parts Distribution Center (PDC)\n   - A subsection that explains how to contact PDC and what information to include (e.g., order/PO, part number, quantity, condition, photos, requested action)\n   - Template or bullet list format acceptable\n7) Final System Confirmation Checklist\n   - A short checklist at the end summarizing confirmation steps (e.g., receipt posted, bin/stow complete, discrepancy ticket opened, PDC notified, system updated)\n\nScoring (assign a single score in [0, 8]):\n- 8.0: PDF, 1\u20133 pages, all 7 elements present with clear headers; Stock vs. Critical each with numbered steps; Visual Guidance has 2+ figures/descriptions with captions\n- 6.0: PDF, 1\u20133 pages, missing ONE minor structural element (e.g., only one visual) but Stock vs. Critical steps, Exceptions, PDC comms, and Final Checklist are present\n- 4.0: PDF, 1\u20133 pages, missing ONE major element (e.g., no Visual Guidance section or no PDC comms) but still has distinct Stock vs. Critical procedures and Exceptions\n- 1.0\u20133.0: PDF present but multiple core sections missing or sections are not clearly delineated; does not enable verification\n- 0.0: Not a PDF, or length outside 1\u20133 pages, or missing most core sections\n\nOnly check structural presence and format. Do not evaluate correctness or quality.", "expectation": "A 1\u20133 page PDF with clearly separated Stock and Critical procedures, numbered steps, BOL verification, exceptions handling, visual guidance, PDC communication instructions, and a final checklist."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Consistency Verification", "description": "Now that the PDF structure is enforced, verify correctness and consistency using code and LLM checks (keywords, field coverage, and flow logic).", "is_required": false, "max_points": 8.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Distinct Stock vs. Critical Procedures with Steps", "description": "Verify the document distinguishes Stock and Critical orders and includes sufficient numbered steps overall.", "weight": 2.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = \"\"\n    # Try PDF then DOCX then plaintext\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    if not text:\n        return 0.0\n\n    t = text.lower()\n\n    # Presence of stock and critical terminology\n    stock_terms = [r\"stock\", r\"replenish\", r\"regular\"]\n    critical_terms = [r\"critical\", r\"vor\", r\"emergency\", r\"rush\", r\"hot\"]\n\n    has_stock = any(re.search(rf\"\\b{w}\\b\", t) for w in stock_terms)\n    has_critical = any(re.search(rf\"\\b{w}\\b\", t) for w in critical_terms)\n\n    # Count numbered steps overall (approximation)\n    step_count = len(re.findall(r\"(?:^|\\n)\\s*\\d+\\.\\s+\", t))\n\n    score = 0.0\n    if has_stock and has_critical:\n        if step_count >= 6:\n            score = 1.0\n        elif step_count >= 4:\n            score = 0.7\n        else:\n            score = 0.5\n    else:\n        score = 0.0\n\n    return score"}, {"type": "code", "name": "Workflow Coverage: Delivery-to-Confirmation Keywords", "description": "Check coverage of key workflow milestones: arrival, paperwork verification, inspection, receiving, put-away, and final system confirmation.", "weight": 2.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    # Read text from PDF/DOCX/MD as fallback\n    text = \"\"\n    for reader in (context.files.read_pdf_text, context.files.read_docx_text, context.files.read_text):\n        try:\n            text = reader(output.id)\n            if text:\n                break\n        except Exception:\n            continue\n    if not text:\n        return 0.0\n\n    t = text.lower()\n\n    groups = [\n        [\"delivery\", \"truck\", \"carrier\", \"driver\", \"arrival\", \"receiving dock\", \"drop-off\"],\n        [\"bill of lading\", \"bol\", \"packing slip\", \"waybill\"],\n        [\"inspect\", \"inspection\", \"damage\", \"verify\", \"count\", \"match\", \"compare\"],\n        [\"scan\", \"receive\", \"receipt\", \"post\", \"check in\", \"check-in\", \"book in\"],\n        [\"bin\", \"shelf\", \"put-away\", \"stow\", \"location\", \"stocking\", \"bin label\"],\n        [\"confirm\", \"confirmation\", \"close\", \"complete\", \"finalize\", \"dms\", \"system\", \"post invoice\", \"acknowledge\"],\n    ]\n\n    covered = 0\n    for grp in groups:\n        if any(kw in t for kw in grp):\n            covered += 1\n    return covered / len(groups)"}, {"type": "code", "name": "Exceptions Handling Coverage", "description": "Verify presence of instructions for damaged parts, missing items/shortages, overages/discrepancies, and returns/claims.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = \"\"\n    for reader in (context.files.read_pdf_text, context.files.read_docx_text, context.files.read_text):\n        try:\n            text = reader(output.id)\n            if text:\n                break\n        except Exception:\n            continue\n    if not text:\n        return 0.0\n\n    t = text.lower()\n\n    sets = [\n        [\"damaged\", \"damage\", \"defect\", \"broken\", \"concealed damage\"],\n        [\"missing\", \"short\", \"shortage\", \"not received\"],\n        [\"overage\", \"over-shipped\", \"excess\", \"extra\"],\n        [\"discrepancy\", \"mismatch\", \"variance\", \"does not match\", \"bol discrepancy\"],\n        [\"rma\", \"return authorization\", \"return\", \"claim\", \"case\"],\n    ]\n\n    found = 0\n    for s in sets:\n        if any(kw in t for kw in s):\n            found += 1\n\n    # Expect at least 4 of 5 topics present\n    return min(1.0, found / 4.0)"}, {"type": "code", "name": "PDC Communication Fields Present", "description": "Verify guidance includes contacting the Parts Distribution Center (PDC) and includes key fields to send.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = \"\"\n    for reader in (context.files.read_pdf_text, context.files.read_docx_text, context.files.read_text):\n        try:\n            text = reader(output.id)\n            if text:\n                break\n        except Exception:\n            continue\n    if not text:\n        return 0.0\n\n    t = text.lower()\n\n    pdc_present = any(kw in t for kw in [\"parts distribution center\", \"pdc\", \"distribution center\"]) \n\n    field_groups = [\n        [\"po\", \"purchase order\", \"order number\", \"reference\"],\n        [\"part number\", \"p/n\", \"sku\"],\n        [\"quantity\", \"qty\"],\n        [\"photo\", \"image\", \"attachment\", \"evidence\"],\n        [\"requested action\", \"replace\", \"reship\", \"credit\", \"refund\"],\n        [\"ticket\", \"case\", \"incident\", \"rma\"],\n        [\"email\", \"portal\", \"phone\", \"contact\"],\n    ]\n\n    fields_found = 0\n    for grp in field_groups:\n        if any(kw in t for kw in grp):\n            fields_found += 1\n\n    if not pdc_present:\n        return 0.0\n\n    # Expect at least 5 distinct field groups\n    return min(1.0, fields_found / 5.0)"}, {"type": "llm_judge", "name": "Logical Flow and Differentiation Check", "description": "LLM validates that steps flow from delivery to system confirmation and that Critical vs. Stock procedures are meaningfully differentiated (priority, staging, communication timing).", "weight": 1.0, "judge_prompt": "Evaluate the PDF for logical correctness and differentiation (not stylistic quality).\n\nChecks:\n- The procedure flows in order: delivery arrival -> paperwork match -> inspection -> receiving/scan -> put-away/staging -> system confirmation/closeout.\n- Critical orders are handled with higher urgency than stock (e.g., prioritized receiving, immediate staging, expedited communication) and clear differences are stated.\n- Exceptions handling steps reference or link to the communication section when escalation is needed.\n\nScoring (0\u20131):\n- 1.0: Clear, ordered flow covering all phases; explicit, meaningful distinctions between Stock and Critical handling; exceptions steps connect to comms/escalation\n- 0.7: Mostly ordered with minor gaps; distinctions present but subtle; exceptions mostly reference comms\n- 0.3: Several ordering gaps; minimal differentiation; weak linkage for exceptions\n- 0.0: Disordered, no meaningful differentiation, exceptions disconnected from comms", "expectation": "A logically ordered, end-to-end flow with explicit differences for Critical orders and clear escalation linkages."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Usability", "description": "Holistic assessment of presentation quality, clarity, and practical usability for dealership parts staff.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity, Actionability, and Audience Fit", "description": "Assesses clarity of language, actionability of steps and checklists, and fit for dealership parts personnel.", "weight": 2.0, "judge_prompt": "Assess the document\u2019s professional clarity and actionability for dealership parts staff.\n\nConsider:\n- Clear, concise language and consistent terminology (e.g., BOL, DMS, PDC)\n- Numbered steps and checklists that are easy to follow on the floor\n- Roles/responsibilities cues (who does what) and safety/handling notes when relevant\n\nScoring (0\u20132):\n- 2.0: Clear, concise, highly actionable; consistent terms; easy to adopt\n- 1.0: Generally clear with minor ambiguity or jargon\n- 0.0: Hard to follow or not actionable", "expectation": "A concise, easy-to-use procedure with clear steps and terminology appropriate for parts personnel."}, {"type": "llm_judge", "name": "Visual Effectiveness and Accessibility", "description": "Assesses how well visuals support damage documentation and visibility marking; captions/labels and accessibility cues.", "weight": 1.0, "judge_prompt": "Evaluate the effectiveness of visuals.\n\nConsider:\n- Do images/figures clearly show damage documentation and visibility marking (e.g., red tag/label)?\n- Are captions or annotations present so staff know what to do?\n- If representative images are used, are they described well enough to convey the intended action?\n\nScoring (0\u20131):\n- 1.0: Visuals are clear, captioned/annotated, and directly support actions\n- 0.5: Visuals present but partially explained or generic\n- 0.0: Visuals missing or unhelpful", "expectation": "Clear figures with captions that demonstrate documenting damage and marking for visibility."}, {"type": "llm_judge", "name": "Length and Focus", "description": "Confirms the document stays within 1\u20133 pages and remains focused on the operational procedure.", "weight": 1.0, "judge_prompt": "Check the document\u2019s length and focus.\n\n- Stays within 1\u20133 pages\n- Focused on the operational process without unnecessary filler\n\nScoring (0\u20131):\n- 1.0: Within length and tightly focused\n- 0.5: Slightly long or some fluff, but mostly focused\n- 0.0: Outside 1\u20133 pages or largely unfocused", "expectation": "Concise, focused 1\u20133 page procedure document."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "76418a2c-a3c0-4894-b89d-2493369135d9", "rubric": {"category_name": "Shipping Method Selection and Cost Savings Manifest (Manufacturing \u2013 Shipping/Receiving Clerks)", "rationale": "Task Type: Analytical (Pattern A). Output should be a structured Excel workbook enabling deterministic verification of weight-based method selection and savings calculations using provided parameters and pick tickets. Stage 1 (LLM-only) enforces an exact, verifiable workbook shape. Stage 2 uses code rules to re-compute savings, validate method selection against rules, reconcile summaries, and sanity-check data, plus an LLM cross-reference check. Stage 3 assesses professional presentation and Sales usefulness.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2013 Structured Output Format Requirement (GATE)", "description": "Output must be an Excel workbook with required sheets and clearly labeled tables to enable verification of weight-based shipping method selection and savings calculations.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.2, "rules": [{"type": "llm_judge", "name": "Excel Workbook Shape and Sections", "description": "Validate that the candidate produced an Excel file with the exact structural elements required for later verification.", "weight": 4.0, "judge_prompt": "You are checking ONLY the FORMAT/STRUCTURE of the submitted output. Do not judge correctness of numbers. Be flexible with minor naming differences.\n\nOutput must be a valid Excel workbook (.xlsx). Verify the following structure:\n\nRequired Sheet A: \"Daily Shipment Manifest\" (or similar like \"Shipment Manifest\", \"Daily Manifest\")\n- Single tabular area with these columns (headers visible):\n  - Order ID (or Pick Ticket #)\n  - Customer (or Ship-to Name)\n  - Destination (City/State or City/State/ZIP)\n  - Shipment Date\n  - Weight (lb) [units must be indicated as lb or lbs]\n  - Shipment Method Selected (e.g., Parcel/Small Package, LTL, FTL)\n  - Carrier (optional)\n  - Actual Cost (USD)\n  - Industry Avg Cost (USD)\n  - Savings (USD) [defined as Industry Avg Cost \u2212 Actual Cost]\n  - Savings (%) [defined as Savings USD / Industry Avg Cost]\n  - Rationale/Notes (must indicate reason for method choice referencing weight rule)\n\nRequired Sheet B: \"Shipping Parameters\" (or similar: \"Parameters\", \"Method Rules\")\n- Must contain TWO clearly labeled sections:\n  1) Method Selection Rules: a table mapping weight ranges to shipping method.\n     - Columns should indicate range boundaries (e.g., Min Weight, Max Weight) and the Method name.\n  2) Cost Benchmarks by Method: a table listing per shipping method the Actual cost basis and the Industry Average cost basis used (flat, per-lb, or other) with a Source/Reference.\n\nRequired Sheet C: \"Calculation Log\" (or similar: \"Computation Details\", \"Audit Log\")\n- A row-by-row log explaining how values were derived with columns:\n  - Order ID\n  - Weight (lb)\n  - Rule Applied (weight range \u2192 method)\n  - Selected Method\n  - Actual Cost Calculation (text or components) and resulting value\n  - Industry Avg Calculation (text or components) and resulting value\n  - Savings (USD)\n  - Savings (%)\n\nRequired Sheet D: \"Summary\" (or similar: \"Totals\", \"Executive Summary\")\n- Must include:\n  - Overall Totals table with labeled metrics and values: Total Shipments, Total Actual Cost, Total Industry Avg Cost, Total Savings (USD), Average Savings (%).\n  - By Method table with columns: Method, Shipments, Actual Cost, Industry Avg Cost, Savings (USD), Savings (%).\n\nRequired Sheet E (short): \"Data Sources\" (or \"Read Me\", \"Sources\")\n- Lists the source files used, explicitly naming:\n  - \"Pick Tickets 062525\"\n  - \"Shipping parameters\"\n- Brief note on usage (1\u20133 sentences).\n\nOptional (for partial credit if all Required sheets exist):\n- \"Pivot/Charts\" sheet with a pivot or chart visualizing savings by method or customer.\n\nScoring:\n- 4.0: Valid Excel AND all Required sheets present with the specified table structures and columns. Units for weight and currency visible.\n- 3.5: All Required sheets present but one Required table missing 1\u20132 minor columns OR units missing in one place.\n- 3.2: All Required sheets present but one Required sheet is thin (e.g., Calculation Log minimal) OR Data Sources missing brief note.\n- 2.5: Missing exactly one Required sheet OR multiple key columns absent across sheets.\n- 1.0: Wrong format (not Excel) but appears to contain some of the required structure in another format.\n- 0.0: Not Excel OR missing multiple Required sheets/tables.\n\nOnly check presence/structure. Do NOT validate math or quality.", "expectation": "An .xlsx with five clearly structured sheets enabling trivial verification: Manifest, Shipping Parameters (rules + cost benchmarks), Calculation Log, Summary (overall + by method), and Data Sources. Weight and currency units are explicit."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness and Consistency Verification", "description": "Deterministic checks leveraging the enforced structure: recompute savings, validate method selection against weight rules, reconcile summary totals, perform data sanity checks, plus an LLM cross-reference check.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Recompute Savings Math (Row-Level)", "description": "Recalculate Savings USD and Savings % from Actual and Industry Avg costs on the Manifest and compare row-by-row.", "weight": 1.4, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    WEIGHT = 1.4\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n        # Prefer a sheet with 'manifest' or 'shipment'\n        sheet_candidates = [s for s in xls.sheet_names if re.search(r\"manifest|shipment\", s, re.I)]\n        sheet_name = sheet_candidates[0] if sheet_candidates else xls.sheet_names[0]\n        df = pd.read_excel(file_path, sheet_name=sheet_name)\n        # Normalize columns\n        cols = {str(c).strip(): c for c in df.columns}\n        def find_col(keywords):\n            for k in cols:\n                lk = k.lower()\n                if any(kw in lk for kw in keywords):\n                    return cols[k]\n            return None\n        col_actual = find_col([\"actual\", \"act cost\", \"actual cost\"])\n        col_ind = find_col([\"industry\", \"benchmark\", \"avg cost\", \"average cost\"])\n        col_save = find_col([\"savings (usd)\", \"savings usd\", \"savings$\", \"savings \"])\n        col_pct = find_col([\"savings (%)\", \"savings%\", \"% savings\", \"savings percent\"])        \n        # Weight and method columns not needed here but keep flexible\n        if any(c is None for c in [col_actual, col_ind]):\n            return (0.0, \"Missing Actual or Industry Avg cost columns on Manifest\")\n        a = pd.to_numeric(df[col_actual], errors='coerce')\n        b = pd.to_numeric(df[col_ind], errors='coerce')\n        calc_save = b - a\n        with np.errstate(divide='ignore', invalid='ignore'):\n            calc_pct = np.where(b.abs() > 0, calc_save / b, np.nan)\n        n = len(df)\n        if n == 0:\n            return (0.0, \"No rows to evaluate\")\n        ok_rows = np.full(n, True)\n        # Compare against provided columns if exist\n        details = []\n        tol_abs = 0.05\n        tol_pct = 0.01\n        score_frac = 0.0\n        checks = 0\n        if col_save is not None:\n            s_prov = pd.to_numeric(df[col_save], errors='coerce')\n            ok_save = (calc_save - s_prov).abs() <= np.maximum(tol_abs, (b.abs()*0.005))\n            ok_rows &= ok_save.fillna(False)\n            score_frac += ok_save.fillna(False).mean()\n            checks += 1\n        if col_pct is not None:\n            p_prov = pd.to_numeric(df[col_pct], errors='coerce')\n            ok_pct = (calc_pct - p_prov).abs() <= 0.01\n            ok_rows &= ok_pct.fillna(False)\n            score_frac += ok_pct.fillna(False).mean()\n            checks += 1\n        # If provided columns absent, grade by ability to compute\n        if checks == 0:\n            # Basic sanity: all non-null and finite\n            valid = (~pd.isna(calc_save)) & (~pd.isna(calc_pct)) & np.isfinite(calc_pct)\n            score_frac = valid.mean()\n            feedback = \"Savings columns missing; scored on computability only\"\n        else:\n            score_frac = score_frac / checks\n            feedback = f\"Row match rate across savings checks: {score_frac:.2%}\"\n        return (max(0.0, min(1.0, float(score_frac))) * WEIGHT, feedback)\n    except Exception as e:\n        return (0.0, f\"Exception in Savings Math check: {e}\")"}, {"type": "code", "name": "Method Selection vs Weight Rules", "description": "Validate selected shipping method for each order against the weight-based rules in Shipping Parameters.", "weight": 1.4, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    WEIGHT = 1.4\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n        # Identify manifest sheet\n        man_sheet = None\n        for s in xls.sheet_names:\n            if re.search(r\"manifest|shipment|daily\", s, re.I):\n                man_sheet = s\n                break\n        if man_sheet is None:\n            man_sheet = xls.sheet_names[0]\n        dfm = pd.read_excel(file_path, sheet_name=man_sheet)\n        # Identify parameters/rules sheet\n        rule_sheet = None\n        for s in xls.sheet_names:\n            if re.search(r\"parameter|rule|method\", s, re.I):\n                rule_sheet = s\n                break\n        if rule_sheet is None:\n            return (0.0, \"No rules/parameters sheet found\")\n        dfr = pd.read_excel(file_path, sheet_name=rule_sheet)\n        # Try to locate the Method Selection Rules section by presence of columns\n        def norm_cols(df):\n            return [str(c).strip() for c in df.columns]\n        rc = norm_cols(dfr)\n        # Heuristic: pick columns for min, max, method\n        def find_col(df, patterns):\n            for c in df.columns:\n                lc = str(c).strip().lower()\n                if any(p in lc for p in patterns):\n                    return c\n            return None\n        min_c = find_col(dfr, [\"min\", \"lower\", \">=\", \"from\", \"start\"])\n        max_c = find_col(dfr, [\"max\", \"upper\", \"<=\", \"to\", \"end\"])\n        meth_c = find_col(dfr, [\"method\", \"service\", \"mode\"])\n        # If dfr has multiple tables, drop empty columns/rows\n        dfr2 = dfr.copy()\n        dfr2 = dfr2.dropna(how='all').dropna(axis=1, how='all')\n        if meth_c is None and len(dfr2.columns) >= 2:\n            # Try second column as method if its values look textual\n            textiness = dfr2.iloc[:,1].astype(str).str.len().mean()\n            if textiness > 2:\n                meth_c = dfr2.columns[1]\n        if meth_c is None:\n            return (0.0, \"No method column found in rules\")\n        # Build rules\n        rules = []\n        if min_c is not None or max_c is not None:\n            mins = pd.to_numeric(dfr2.get(min_c, np.nan), errors='coerce') if min_c is not None else pd.Series([np.nan]*len(dfr2))\n            maxs = pd.to_numeric(dfr2.get(max_c, np.nan), errors='coerce') if max_c is not None else pd.Series([np.nan]*len(dfr2))\n            for i, row in dfr2.iterrows():\n                mname = str(row[meth_c]).strip()\n                mn = pd.to_numeric(row[min_c], errors='coerce') if min_c in dfr2.columns else np.nan\n                mx = pd.to_numeric(row[max_c], errors='coerce') if max_c in dfr2.columns else np.nan\n                if not (pd.isna(mn) and pd.isna(mx)) and mname:\n                    rules.append({\"min\": mn if not pd.isna(mn) else -np.inf, \"max\": mx if not pd.isna(mx) else np.inf, \"method\": mname})\n        # Fallback: if only an ordered list with an upper bound exists\n        if not rules and dfr2.shape[1] >= 2:\n            # Attempt interpret first numeric column as max bound\n            num_col = None\n            for c in dfr2.columns:\n                if pd.to_numeric(dfr2[c], errors='coerce').notna().mean() > 0.5:\n                    num_col = c\n                    break\n            if num_col is not None:\n                prev = -np.inf\n                for i, row in dfr2.iterrows():\n                    mx = pd.to_numeric(row[num_col], errors='coerce')\n                    mname = str(row[meth_c]).strip()\n                    if pd.notna(mx) and mname:\n                        rules.append({\"min\": prev, \"max\": float(mx), \"method\": mname})\n                        prev = float(mx)\n                # Add open-ended last band if possible\n                if rules:\n                    rules[-1]['max'] = np.inf\n        if not rules:\n            return (0.0, \"Could not construct weight bands from rules sheet\")\n        # Prepare manifest columns\n        def find_col(df, keywords):\n            for c in df.columns:\n                lc = str(c).strip().lower()\n                if any(k in lc for k in keywords):\n                    return c\n            return None\n        w_col = find_col(dfm, [\"weight\", \"wt\"])    \n        m_col = find_col(dfm, [\"shipment method\", \"ship method\", \"method\", \"service\"]) \n        if w_col is None or m_col is None:\n            return (0.0, \"Missing Weight or Selected Method column on Manifest\")\n        weights = pd.to_numeric(dfm[w_col], errors='coerce')\n        meth_sel = dfm[m_col].astype(str).str.strip()\n        # Simple method name normalization\n        def norm_method(s):\n            ls = str(s).lower()\n            if re.search(r\"parcel|small pack|ups|fedex ground\", ls): return \"parcel\"\n            if re.search(r\"ltl|less than truck\", ls): return \"ltl\"\n            if re.search(r\"ftl|full truck|truckload\", ls): return \"ftl\"\n            return ls\n        def method_for_weight(w):\n            for r in rules:\n                mn, mx = r['min'], r['max']\n                if pd.notna(w) and (w >= mn) and (w <= mx):\n                    return norm_method(r['method'])\n            return None\n        matches = []\n        for w, m in zip(weights, meth_sel):\n            expected = method_for_weight(w)\n            m_norm = norm_method(m)\n            if expected is None or pd.isna(w):\n                matches.append(False)\n            else:\n                # Match if one contains the other or exact after normalization\n                ok = (expected == m_norm) or (expected in m_norm) or (m_norm in expected)\n                matches.append(ok)\n        if len(matches) == 0:\n            return (0.0, \"No rows in Manifest to compare against rules\")\n        frac = float(pd.Series(matches).mean())\n        feedback = f\"Method selection match rate: {frac:.2%} across {len(matches)} rows\"\n        return (max(0.0, min(1.0, frac)) * WEIGHT, feedback)\n    except Exception as e:\n        return (0.0, f\"Exception in Method Rules check: {e}\")"}, {"type": "code", "name": "Summary Totals Reconcile", "description": "Compare Overall Totals in Summary sheet to sums from the Manifest.", "weight": 0.6, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    WEIGHT = 0.6\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n        # Manifest\n        man_sheet = None\n        for s in xls.sheet_names:\n            if re.search(r\"manifest|shipment|daily\", s, re.I):\n                man_sheet = s\n                break\n        if man_sheet is None:\n            return (0.0, \"Manifest sheet not found\")\n        dfm = pd.read_excel(file_path, sheet_name=man_sheet)\n        def find_col(df, kws):\n            for c in df.columns:\n                lc = str(c).strip().lower()\n                if any(k in lc for k in kws):\n                    return c\n            return None\n        col_actual = find_col(dfm, [\"actual\", \"actual cost\"]) \n        col_ind = find_col(dfm, [\"industry\", \"avg\", \"benchmark\"]) \n        col_save = find_col(dfm, [\"savings (usd)\", \"savings usd\", \"savings \"]) \n        if col_actual is None or col_ind is None:\n            return (0.0, \"Missing Actual/Industry columns on Manifest for totals calc\")\n        a = pd.to_numeric(dfm[col_actual], errors='coerce').fillna(0)\n        b = pd.to_numeric(dfm[col_ind], errors='coerce').fillna(0)\n        s_calc = b - a\n        if col_save is not None:\n            s_prov = pd.to_numeric(dfm[col_save], errors='coerce').fillna(0)\n        else:\n            s_prov = s_calc\n        tot_actual = float(a.sum())\n        tot_ind = float(b.sum())\n        tot_save = float(s_prov.sum())\n        # Summary sheet\n        sum_sheet = None\n        for s in xls.sheet_names:\n            if re.search(r\"summary|total|executive\", s, re.I):\n                sum_sheet = s\n                break\n        if sum_sheet is None:\n            return (0.0, \"Summary sheet not found\")\n        dfs = pd.read_excel(file_path, sheet_name=sum_sheet, header=None)\n        # Search rows for labels and numeric neighbors\n        def find_metric(label):\n            for i in range(dfs.shape[0]):\n                row = dfs.iloc[i, :]\n                row_str = row.astype(str).str.lower()\n                if row_str.str.contains(label).any():\n                    # pick numeric from same row\n                    nums = pd.to_numeric(row, errors='coerce').dropna()\n                    if len(nums) > 0:\n                        # choose the largest magnitude as total\n                        return float(nums.iloc[np.argmax(nums.abs())])\n            return None\n        s_tot_actual = find_metric(\"total actual\")\n        s_tot_ind = find_metric(\"total industry\") or find_metric(\"industry avg\") or find_metric(\"industry average\")\n        s_tot_save = find_metric(\"total savings\") or find_metric(\"savings (usd)\")\n        found = 0\n        agree = 0\n        tol_rel = 0.02  # 2%\n        def agrees(x, y):\n            if x is None or y is None:\n                return False\n            denom = max(1.0, abs(y))\n            return abs(x - y) / denom <= tol_rel\n        if s_tot_actual is not None:\n            found += 1\n            if agrees(s_tot_actual, tot_actual):\n                agree += 1\n        if s_tot_ind is not None:\n            found += 1\n            if agrees(s_tot_ind, tot_ind):\n                agree += 1\n        if s_tot_save is not None:\n            found += 1\n            if agrees(s_tot_save, tot_save):\n                agree += 1\n        if found == 0:\n            return (0.0, \"No recognizable totals found on Summary\")\n        frac = agree / found\n        return (frac * WEIGHT, f\"Summary agreement on {agree}/{found} totals ({frac:.0%})\")\n    except Exception as e:\n        return (0.0, f\"Exception in Summary Reconcile: {e}\")"}, {"type": "code", "name": "Data Sanity and Bounds", "description": "Check basic plausibility: positive weights and costs, plausible weight bounds, and non-extreme savings percentages.", "weight": 0.2, "code": "import re, json\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    WEIGHT = 0.2\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n        # Manifest sheet\n        man_sheet = None\n        for s in xls.sheet_names:\n            if re.search(r\"manifest|shipment|daily\", s, re.I):\n                man_sheet = s\n                break\n        if man_sheet is None:\n            man_sheet = xls.sheet_names[0]\n        dfm = pd.read_excel(file_path, sheet_name=man_sheet)\n        def find_col(df, kws):\n            for c in df.columns:\n                lc = str(c).strip().lower()\n                if any(k in lc for k in kws):\n                    return c\n            return None\n        w_col = find_col(dfm, [\"weight\", \"wt\"]) \n        a_col = find_col(dfm, [\"actual\", \"actual cost\"]) \n        i_col = find_col(dfm, [\"industry\", \"avg\", \"benchmark\"]) \n        p_col = find_col(dfm, [\"savings (%)\", \"savings%\", \"% savings\"]) \n        if w_col is None or a_col is None or i_col is None:\n            return (0.0, \"Missing essential columns (Weight/Actual/Industry)\")\n        w = pd.to_numeric(dfm[w_col], errors='coerce')\n        a = pd.to_numeric(dfm[a_col], errors='coerce')\n        i = pd.to_numeric(dfm[i_col], errors='coerce')\n        with np.errstate(divide='ignore', invalid='ignore'):\n            pct = (i - a) / i\n        conds = []\n        conds.append((w > 0) & (w <= 50000))\n        conds.append(a > 0)\n        conds.append(i > 0)\n        # savings % between -100% and +100% for plausibility\n        conds.append((pct > -1.0) & (pct < 1.0))\n        valid = np.logical_and.reduce([c.fillna(False) for c in conds])\n        if len(valid) == 0:\n            return (0.0, \"No rows\")\n        frac = float(valid.mean())\n        return (frac * WEIGHT, f\"Data sanity pass rate: {frac:.2%}\")\n    except Exception as e:\n        return (0.0, f\"Exception in Data Sanity check: {e}\")"}, {"type": "llm_judge", "name": "Cross-References and Notes Consistency", "description": "Check that Data Sources sheet cites the required files and that Manifest notes reference weight-based rules for rationale.", "weight": 0.4, "judge_prompt": "Review the Excel file. Verify the following:\n- Data Sources sheet (or similarly named) is present and explicitly lists both of these:\n  1) \"Pick Tickets 062525\"\n  2) \"Shipping parameters\"\n- In the Manifest sheet, the Rationale/Notes column contains references to weight-based rule application for at least two orders (e.g., mentions of a weight threshold or band and the resulting method like Parcel/LTL/FTL).\n\nScoring:\n- 0.4: Both sources listed AND at least two rows include rationale referencing weight-based rules.\n- 0.2: Only one source listed OR only one row includes a weight-based rationale reference.\n- 0.0: Neither source listed AND no weight-based rationale references.\n\nBe tolerant of minor naming variations but ensure unambiguous presence.", "expectation": "Clear Data Sources citing both files and meaningful weight-based rationale notes in the Manifest."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Professional Quality and Sales Usefulness", "description": "Holistic assessment of presentation, usability, and value to Sales for communicating cost savings to customers.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Presentation, Clarity, and Sales Value", "description": "Evaluate formatting, readability, and usefulness to Sales.", "weight": 2.0, "judge_prompt": "Assess the Excel workbook for professional quality and usefulness to Sales:\n- Formatting: readable headers, number formats (currency with $ and two decimals, percentages with %), units for weight (lb), filters and/or freeze panes on Manifest.\n- Clarity: concise sheet names, clear sections in Parameters and Calculation Log, no clutter.\n- Communication value: Summary includes Overall Totals and By Method breakdown that clearly demonstrates savings; optional charts/pivots are a plus; brief narrative note for Sales (if present) improves score.\n\nScoring:\n- 2.0: Professional formatting throughout, clear and concise, numbers well-formatted, Summary convincingly communicates savings; optional visuals or notes present.\n- 1.2: Generally clear with minor formatting or labeling issues; Summary adequate but could be clearer; no visuals.\n- 0.6: Barely formatted, hard to read, but information present; Summary minimal.\n- 0.0: Disorganized or unreadable; fails to communicate savings effectively.\n\nFocus on presentation and usefulness, not recalculating math.", "expectation": "A clean, well-formatted workbook that Sales can share to evidence shipping method choices and realized savings."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "47ef842d-8eac-4b90-bda8-dd934c228c96", "rubric": {"category_name": "Wholesale Trade \u00b7 Order Clerks \u2014 Inventory Health Summary (WOS & OOS)", "rationale": "This rubric applies the self-documenting, 3-stage approach. Stage 1 (LLM-only) strictly enforces a verifiable Excel workbook structure: a Summary table for the 5 specified UPCs, a Calculations/Methodology sheet that shows the math and definitions, and a chart highlighting percent of stores out-of-stock by UPC. Stage 2 mixes code and an LLM judge to verify numerical coherence and methodology correctness, leveraging the mandated shape. Stage 3 assesses professional presentation and usefulness for account management stakeholders.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Structured Output Gate (LLM Only)", "description": "Gate: Verify the candidate produced a properly structured Excel workbook enabling verification.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 2.5, "rules": [{"type": "llm_judge", "name": "Excel Structure and Required Elements Present", "description": "Verify the output is an Excel workbook (.xlsx) with the exact structure needed for verification and decision-making.", "weight": 4.0, "judge_prompt": "You are checking ONLY the structure/format of the submitted file (not the correctness of calculations). The task requires an Excel workbook summarizing inventory health for 5 specific UPCs with clear calculations and a chart.\n\nPass criteria: The submission must be an Excel workbook (.xlsx) with these elements:\n\nA) Sheet: \"Summary\" (or similar name like \"Inventory Summary\") containing a single, clearly labeled table with exactly these columns (flexible on minor naming/capitalization):\n   - UPC\n   - Weekly Unit Rate of Sale (computed as daily inventory sold in the last 4 weeks \u00d7 7)\n   - Weeks of Supply (WOS)\n   - Active Stores (number of active stores)\n   - Stores OOS (count of stores out of stock)\n   - OOS % of Active Stores (percent of active stores out of stock)\n   And rows for each of the 5 UPCs (these must appear as rows):\n   901153373247, 567219040266, 217313054556, 875218534223, 375301052429\n\nB) Sheet: \"Calculations\" (or similar name like \"Data Prep\" or \"Methodology & Calculations\") that SHOWS THE WORK, including:\n   - A description of how Weekly Unit Rate of Sale is derived (daily inventory sold in last 4 weeks \u00d7 7)\n   - The definition of Active Stores used\n   - How Stores OOS and OOS % are computed\n   - The WOS definition (e.g., On Hand \u00f7 Weekly Unit Rate of Sale)\n   This can be done via text blocks and/or calculation tables. The key is that it is explicit and traceable.\n\nC) Chart: A bar/column chart visualizing Percent of Stores OOS by UPC (title and axis labels should clearly communicate the metric; it may reside on the Summary sheet or a dedicated Charts sheet).\n\nScoring (0\u20134 points):\n- 4.0: Valid Excel workbook with all three elements A, B, C present; Summary table includes the 5 UPCs with the required columns; chart clearly shows OOS % by UPC.\n- 3.5: All required elements present but with minor deviations (e.g., slightly different but equivalent column names; chart in a different sheet).\n- 2.5: Summary table and chart present for the 5 UPCs, but the Calculations/Methodology content is minimal or partially missing.\n- 1.5: Some structure is present (e.g., Summary table) but missing a major element (no chart or no calculations/methodology) or missing multiple UPCs.\n- 0.0: Not an Excel workbook or structure is largely missing.\n\nOnly assess presence/structure, not correctness of any numbers.", "expectation": "A verifiable Excel with: Summary table for the 5 UPCs, a Calculations/Methodology sheet showing how metrics were derived, and a chart highlighting OOS % by UPC."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification: Numerical Coherence and Methodology", "description": "Verify the Summary table numerics are consistent and that the methodology is explicitly documented.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Summary Data Coherence Checks", "description": "Code-verify that the Summary sheet contains the 5 UPCs and that OOS % is consistent with Stores OOS / Active Stores, with numeric values in plausible bounds.", "weight": 3.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float or (float, str)\n    \"\"\"\n    # Weight for this rule\n    WEIGHT = 3.0\n\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output detected.\"\n\n    # Helper functions\n    def normalize(s):\n        return re.sub(r\"[^a-z0-9%]+\", \" \", str(s).lower()).strip()\n\n    def to_number(v):\n        if pd.isna(v):\n            return np.nan\n        try:\n            if isinstance(v, str):\n                s = v.strip().replace(\",\", \"\")\n                if s.endswith('%'):\n                    return float(s[:-1]) / 100.0\n                return float(s)\n            return float(v)\n        except Exception:\n            return np.nan\n\n    target_upcs = {\"901153373247\",\"567219040266\",\"217313054556\",\"875218534223\",\"375301052429\"}\n\n    # Open Excel and locate a plausible Summary sheet\n    try:\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n        sheet_names = xls.sheet_names\n    except Exception as e:\n        return 0.0, f\"Failed to read Excel file: {e}\"\n\n    summary_df = None\n    summary_sheet = None\n\n    # Try preferred names first, then any sheet that contains a UPC-like column\n    preferred_order = [s for s in sheet_names if re.search(r\"summary|overview|results\", s, re.I)] + sheet_names\n\n    for s in preferred_order:\n        try:\n            df = pd.read_excel(file_path, sheet_name=s)\n        except Exception:\n            continue\n        if df is None or df.empty:\n            continue\n        norm_cols = [normalize(c) for c in df.columns]\n        if any('upc' in c for c in norm_cols):\n            summary_df = df.copy()\n            summary_sheet = s\n            break\n\n    if summary_df is None:\n        return 0.0, \"Could not find a Summary-like sheet with a UPC column.\"\n\n    # Map required columns flexibly\n    cols = {normalize(c): c for c in summary_df.columns}\n\n    def find_col(patterns):\n        for nc, orig in cols.items():\n            for pat in patterns:\n                if isinstance(pat, str):\n                    if pat in nc:\n                        return orig\n                else:\n                    try:\n                        if pat(nc):\n                            return orig\n                    except Exception:\n                        pass\n        return None\n\n    upc_col = find_col(['upc'])\n    weekly_ros_col = find_col([\n        'weekly unit rate of sale', 'weekly rate of sale', 'weekly units', 'units week', 'ros',\n        lambda n: ('weekly' in n and ('rate' in n or 'units' in n))\n    ])\n    wos_col = find_col(['wos', 'weeks of supply', 'weeks supply'])\n    active_col = find_col([\n        'active stores', 'stores active', 'store count', 'number of stores',\n        lambda n: ('store' in n and ('count' in n or 'number' in n))\n    ])\n    oos_count_col = find_col([\n        'oos count', 'stores oos', 'out of stock count',\n        lambda n: ('oos' in n and 'count' in n) or ('out of stock' in n and ('count' in n or 'stores' in n))\n    ])\n    oos_pct_col = find_col([\n        'oos %', 'oos percent', 'percent stores oos', 'out of stock %', 'oos pct',\n        lambda n: (('oos' in n) or ('out of stock' in n)) and (('%' in n) or ('percent' in n) or ('pct' in n))\n    ])\n\n    required_cols = [upc_col, weekly_ros_col, wos_col, active_col, oos_count_col, oos_pct_col]\n    if any(c is None for c in required_cols):\n        missing = []\n        names = ['UPC','Weekly Rate of Sale','WOS','Active Stores','Stores OOS','OOS %']\n        for name, col in zip(names, required_cols):\n            if col is None:\n                missing.append(name)\n        return 0.5 * WEIGHT, f\"Summary sheet found but missing expected columns: {', '.join(missing)}\"\n\n    # Clean UPCs to digits for matching\n    def clean_upc_series(s):\n        return s.astype(str).str.replace(r\"[^0-9]\", \"\", regex=True).str.strip()\n\n    df = summary_df.copy()\n    df['_upc_clean'] = clean_upc_series(df[upc_col])\n    present_upcs = set(u for u in df['_upc_clean'] if u)\n\n    # Subscore A: UPC coverage\n    covered = len(target_upcs & present_upcs)\n    coverage_score = (covered / 5.0) * 0.8 * WEIGHT  # up to 0.8*WEIGHT fraction\n\n    # Focus rows for the target UPCs\n    rows = df[df['_upc_clean'].isin(target_upcs)].copy()\n    if rows.empty:\n        return coverage_score, \"None of the target UPCs found in Summary table.\"\n\n    # Coherence checks per row\n    oos_consistency_points = 0.0\n    bounds_points = 0.0\n    max_oos_points = 0.4 * WEIGHT  # OOS% arithmetic consistency\n    max_bounds_points = 0.4 * WEIGHT  # numeric bounds and relationships\n    n = len(rows)\n\n    # Avoid division by zero\n    def safe_div(a, b):\n        try:\n            return np.nan if (b is None or b == 0 or pd.isna(b)) else a / b\n        except Exception:\n            return np.nan\n\n    consistent_count = 0\n    bounds_ok_count = 0\n\n    for _, r in rows.iterrows():\n        active = to_number(r[active_col])\n        oos_ct = to_number(r[oos_count_col])\n        oos_pct = to_number(r[oos_pct_col])\n        weekly_ros = to_number(r[weekly_ros_col])\n        wos = to_number(r[wos_col])\n\n        # Normalize percentage if represented as 0-100\n        if pd.notna(oos_pct) and oos_pct > 1.5:\n            oos_pct = oos_pct / 100.0\n\n        # Arithmetic consistency: oos_pct \u2248 oos_ct / active (within 2 percentage points)\n        pred = safe_div(oos_ct, active)\n        if pd.notna(pred) and pd.notna(oos_pct):\n            if abs(pred - oos_pct) <= 0.02:  # within 2 p.p.\n                consistent_count += 1\n\n        # Bounds checks\n        bounds_ok = True\n        # Counts should be non-negative; oos_ct <= active\n        if (pd.isna(active) or active < 0) or (pd.isna(oos_ct) or oos_ct < 0) or (pd.notna(active) and pd.notna(oos_ct) and oos_ct > active):\n            bounds_ok = False\n        # Weekly rate of sale and WOS should be non-negative if present\n        if (pd.notna(weekly_ros) and weekly_ros < 0) or (pd.notna(wos) and wos < 0):\n            bounds_ok = False\n        # If active is zero, we tolerate oos metrics at zero or NaN\n        bounds_ok_count += 1 if bounds_ok else 0\n\n    if n > 0:\n        oos_consistency_points = (consistent_count / n) * max_oos_points\n        bounds_points = (bounds_ok_count / n) * max_bounds_points\n\n    total = coverage_score + oos_consistency_points + bounds_points\n    total = max(0.0, min(WEIGHT, total))\n\n    feedback = (\n        f\"UPC coverage: {covered}/5; OOS% consistency rows: {consistent_count}/{n}; \"\n        f\"Bounds OK rows: {bounds_ok_count}/{n}.\"\n    )\n    return total, feedback\n"}, {"type": "llm_judge", "name": "Methodology Clarity and Correctness (Textual)", "description": "LLM-verify that the workbook explicitly documents how metrics were derived and aligns with the task definitions.", "weight": 1.0, "judge_prompt": "Check the workbook for a Calculations/Methodology sheet (e.g., named \"Calculations\", \"Data Prep\", or \"Methodology & Assumptions\"). Confirm that it explicitly states:\n- Weekly Unit Rate of Sale is computed as: daily inventory sold in the last 4 weeks \u00d7 7.\n- How Active Stores are defined (a store appears in the dataset for that UPC and has an out-of-stock percentage).\n- How Stores OOS and OOS % are calculated.\n- The Weeks of Supply (WOS) formula (e.g., On Hand \u00f7 Weekly Unit Rate of Sale; note: do not check On Hand values themselves).\nAlso confirm that the sheet shows the work via either a step-by-step calculation table, intermediate aggregations, or clear text-backed tables linking inputs to Summary.\n\nScoring (0\u20131 points):\n- 1.0: All items above explicitly documented; shows work clearly.\n- 0.7: Most items documented; one element is vague or missing.\n- 0.4: General explanation present but key formulas/definitions are missing.\n- 0.0: No methodology/calculations documentation found.\n\nJudge only the presence and clarity of documentation, not the numeric correctness.", "expectation": "A clear, explicit methodology sheet that documents formulas and definitions per the task and shows how the Summary was derived."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Communication", "description": "Assess professional presentation, clarity, and usefulness for Account Management stakeholders.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Insightfulness", "description": "LLM-assess formatting quality, clarity of the visualization, and practical usefulness of the Summary for decision-making.", "weight": 2.0, "judge_prompt": "Evaluate the professional quality and usefulness of the workbook:\n- Is the Summary table clean, readable, and clearly labeled (units, % formatting, sensible number formats)?\n- Does the chart highlighting OOS % by UPC have a clear title, labeled axes, and legible data labels or scale; is it easy to compare UPCs at a glance?\n- Are sheets named and organized logically (Summary first, Calculations/Methodology easy to find)?\n- Optional but positive: brief notes or labels that help interpret risks (e.g., high OOS % despite seemingly healthy aggregate WOS). Do not require a narrative essay; assess clarity and practical readiness.\n\nScoring (0\u20132 points):\n- 2.0: Polished, clear labeling, well-formatted, and immediately useful to stakeholders.\n- 1.0: Generally clear but has minor formatting or clarity issues.\n- 0.0: Poorly formatted, confusing, or hard to use for decisions.\n", "expectation": "A polished Summary with a clear chart and sensible formatting that a sales/account manager could use immediately."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "476db143-163a-4537-9e21-fe46adad703b", "rubric": {"category_name": "Move-Out Inspection Communication & Schedule (Real Estate - Counter and Rental Clerks)", "rationale": "This rubric uses a 3-stage self-documenting approach. Stage 1 (LLM-only) enforces exact deliverable shapes: two separate PDFs with mandated sections and table structure, enabling later verification. Stage 2 uses code rules to verify correctness signals made possible by the enforced shape: presence of default inspection date, headers, and contact/rescheduling cues. Stage 3 uses LLM to assess professional quality and usefulness for residents and the manager.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate", "description": "MANDATORY structure check of both required PDFs: (1) Resident notification email PDF and (2) Inspection schedule PDF with a tabular roster.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Deliverables Presence and Format (PDF)", "description": "Verify both required PDFs exist and follow the exact structural requirements to enable verification.", "weight": 4.0, "judge_prompt": "You are the Stage 1 Shape Enforcement judge. Evaluate ONLY the presence, file format, and structural completeness of the outputs. Do not judge content quality or calculation correctness.\n\nRequired Outputs (TWO separate files):\n1) Resident Notification Email (PDF)\n   Format and Structure Requirements:\n   - Must be a PDF document (not DOCX, not Excel, not plaintext)\n   - 1 page is acceptable; professional formatting is expected\n   - Must include a visible Subject line (e.g., \"Subject: Move-Out Inspection\" or similar)\n   - Must include a greeting suitable for residents (e.g., \"Dear [Resident Name]\" or \"Dear Resident\")\n   - Must mention the default inspection date: 9/23/25 (acceptable variations: 09/23/2025, September 23, 2025, Sept 23, 2025)\n   - Must mention that residents may request a different date or reschedule\n   - Must include Qyrevia Property Management (company name) in header, body, or signature\n   - Must include contact information section (e.g., email and/or phone) and a signature block\n\n2) Inspection Schedule for Manager (PDF)\n   Format and Structure Requirements:\n   - Must be a separate PDF document (not combined with the email)\n   - Must have a clear title (e.g., \"September Move-Out Inspection Schedule\")\n   - Must present a tabular roster with columns equivalent to the following (flexible on exact wording):\n     \u2022 Unit # (e.g., \"Unit #\", \"Unit\", \"Apartment\", \"Apt #\", \"Unit Number\" )\n     \u2022 Resident Name (e.g., \"Resident Name\", \"Tenant Name\")\n     \u2022 Move-Out Date (e.g., \"Move-Out Date\", \"Move Out\", \"Vacate Date\")\n     \u2022 Inspection Date (e.g., \"Inspection Date\", \"Final Inspection\")\n   - The table must have at least one data row (not just headers)\n\nScoring (0\u20134):\n- 4.0: Both PDFs present and satisfy all structural requirements listed above.\n- 3.0: Both PDFs present, each in PDF format, with only 1\u20132 minor structural omissions across both files (e.g., slight column header naming deviation still mapping clearly; or missing small element like signature block if contact info and company name are present).\n- 2.0: Both PDFs present but multiple structural requirements missing OR one document is not clearly structured (e.g., schedule lacks recognizable table headers or no Subject line in email).\n- 1.0: Only one of the required PDFs present in correct format and structure.\n- 0.0: Required outputs missing or wrong formats (e.g., not PDFs), or structure too vague to verify.\n\nImportant: Be flexible on exact phrases (e.g., \"Unit Number\" counts as \"Unit #\"). Focus on presence of required elements enabling later verification. Do not assess tone, grammar, or factual accuracy here.", "expectation": "Two separate, well-structured PDFs: a resident-facing email PDF with subject, greeting, default date (9/23/25), rescheduling option, company name, and contact info; and a manager-facing schedule PDF with a clear title and a table containing columns for Unit #, Resident Name, Move-Out Date, and Inspection Date, with at least one row."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness Verification", "description": "Code-based checks leveraging the enforced shape to verify key facts and internal consistency.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Two Separate PDFs Detected", "description": "Verify there are at least two distinct PDF documents among outputs.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs() or []\n        if not outputs:\n            return 0.0\n        # Count documents and PDFs\n        pdf_count = 0\n        doc_ids = []\n        for r in outputs:\n            if getattr(r, 'is_document', False):\n                doc_ids.append(r.id)\n                try:\n                    p = context.files.get_path(r.id)\n                    if str(p).lower().endswith('.pdf'):\n                        pdf_count += 1\n                except Exception:\n                    # If path not available, assume document could be PDF\n                    pass\n        if len(doc_ids) >= 2 and pdf_count >= 2:\n            return 1.0\n        if len(doc_ids) >= 2 and pdf_count >= 1:\n            return 0.7\n        if len(doc_ids) >= 1:\n            return 0.3\n        return 0.0\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Email Mentions Default Date, Rescheduling, Company, and Contact", "description": "Find the resident email PDF and check it mentions default date (9/23/25 variants), rescheduling option, Qyrevia, and contact info (email or phone).", "weight": 1.0, "code": "import re\n\nDEFAULT_DATE_PATTERNS = [\n    r\"\\b9/23/25\\b\",\n    r\"\\b09/23/2025\\b\",\n    r\"\\b9/23/2025\\b\",\n    r\"\\b09/23/25\\b\",\n    r\"\\bSeptember\\s+23,\\s+2025\\b\",\n    r\"\\bSept(?:ember)?\\s+23,\\s+2025\\b\",\n]\n\ndef contains_default_date(text: str) -> bool:\n    t = text or \"\"\n    for pat in DEFAULT_DATE_PATTERNS:\n        if re.search(pat, t, flags=re.IGNORECASE):\n            return True\n    return False\n\nEMAIL_RE = re.compile(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\")\nPHONE_RE = re.compile(r\"(\\+?1[-.\\s]?)?(\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4})\")\n\nKEY_EMAIL_CUES = [\"subject\", \"dear\", \"regards\", \"sincerely\", \"inspection\", \"move-out\", \"move out\"]\n\n\ndef pick_email_doc_text(context, outputs):\n    candidates = []\n    for r in outputs:\n        if getattr(r, 'is_document', False):\n            try:\n                p = context.files.get_path(r.id)\n                if not str(p).lower().endswith('.pdf'):\n                    continue\n            except Exception:\n                # If path not accessible, still try reading text\n                pass\n            try:\n                txt = context.files.read_pdf_text(r.id)\n            except Exception:\n                txt = None\n            if not txt:\n                continue\n            # score likelihood it's the email\n            score = sum(1 for k in KEY_EMAIL_CUES if k in txt.lower())\n            candidates.append((score, len(txt), r.id, txt))\n    if not candidates:\n        return None\n    # pick by highest score; tie-breaker: longer text\n    candidates.sort(key=lambda x: (x[0], x[1]), reverse=True)\n    return candidates[0][3]\n\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs() or []\n        email_text = pick_email_doc_text(context, outputs)\n        if not email_text:\n            return 0.0\n        text = email_text\n        hits = 0\n        total = 4\n        # Default date present\n        if contains_default_date(text):\n            hits += 1\n        # Rescheduling option cues\n        if re.search(r\"reschedul|different\\s+date|alternate\\s+date|another\\s+date|change\\s+the\\s+date\", text, flags=re.IGNORECASE):\n            hits += 1\n        # Company name\n        if re.search(r\"qyrevia\", text, flags=re.IGNORECASE):\n            hits += 1\n        # Contact info: email or phone present\n        if EMAIL_RE.search(text) or PHONE_RE.search(text):\n            hits += 1\n        return hits / total\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Schedule Has Required Headers and Dates", "description": "Find the schedule PDF and verify it includes headers mapping to Unit, Resident Name, Move-Out Date, Inspection Date, and includes at least one date (preferably default date).", "weight": 1.5, "code": "import re\n\nHEADER_SYNONYMS = {\n    'unit': [r\"unit\\s*#?\", r\"apartment\", r\"apt\\s*#?\", r\"unit\\s*number\"],\n    'resident': [r\"resident\\s*name\", r\"tenant\\s*name\", r\"resident\", r\"tenant\"],\n    'moveout': [r\"move[-\\s]?out\\s*date\", r\"move\\s*out\", r\"vacate\\s*date\"],\n    'inspection': [r\"inspection\\s*date\", r\"final\\s*inspection\", r\"inspection\"]\n}\n\nDATE_RE1 = re.compile(r\"\\b(?:0?[1-9]|1[0-2])/[0-3]?\\d/(?:\\d{2}|\\d{4})\\b\")\nDATE_RE2 = re.compile(r\"\\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Sept|Oct|Nov|Dec)[a-z]*\\s+\\d{1,2},\\s+\\d{4}\\b\", re.IGNORECASE)\nDEFAULT_DATE_PATTERNS = [\n    r\"\\b9/23/25\\b\",\n    r\"\\b09/23/2025\\b\",\n    r\"\\b9/23/2025\\b\",\n    r\"\\b09/23/25\\b\",\n    r\"\\bSeptember\\s+23,\\s+2025\\b\",\n    r\"\\bSept(?:ember)?\\s+23,\\s+2025\\b\",\n]\n\ndef contains_default_date(text: str) -> bool:\n    for pat in DEFAULT_DATE_PATTERNS:\n        if re.search(pat, text or '', flags=re.IGNORECASE):\n            return True\n    return False\n\n\ndef pick_schedule_doc_text(context, outputs):\n    # Prefer doc containing many schedule cues and table-like content\n    candidates = []\n    for r in outputs:\n        if getattr(r, 'is_document', False):\n            try:\n                p = context.files.get_path(r.id)\n                if not str(p).lower().endswith('.pdf'):\n                    continue\n            except Exception:\n                pass\n            try:\n                txt = context.files.read_pdf_text(r.id)\n            except Exception:\n                txt = None\n            if not txt:\n                continue\n            cues = 0\n            for syns in HEADER_SYNONYMS.values():\n                for s in syns:\n                    if re.search(s, txt, flags=re.IGNORECASE):\n                        cues += 1\n                        break\n            # prefer doc with 'schedule' or 'roster' in title/body\n            if re.search(r\"schedule|roster|list\", txt, flags=re.IGNORECASE):\n                cues += 1\n            candidates.append((cues, len(txt), r.id, txt))\n    if not candidates:\n        return None\n    candidates.sort(key=lambda x: (x[0], x[1]), reverse=True)\n    return candidates[0][3]\n\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs() or []\n        sched_text = pick_schedule_doc_text(context, outputs)\n        if not sched_text:\n            return 0.0\n        text = sched_text\n        # Check header coverage\n        header_hits = 0\n        for key, syns in HEADER_SYNONYMS.items():\n            found = any(re.search(s, text, flags=re.IGNORECASE) for s in syns)\n            if found:\n                header_hits += 1\n        header_score = min(header_hits / 4.0, 1.0)\n        # Dates present\n        any_date = bool(DATE_RE1.search(text) or DATE_RE2.search(text))\n        default_present = contains_default_date(text)\n        date_score = (1 if any_date else 0) + (1 if default_present else 0)\n        date_score = date_score / 2.0\n        # Combine: emphasize headers slightly more\n        score = 0.6 * header_score + 0.4 * date_score\n        return max(0.0, min(1.0, score))\n    except Exception:\n        return 0.0"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism", "description": "LLM assessment of tone, clarity, formatting, and managerial usefulness.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Resident Email Professional Quality", "description": "Assess if the email PDF is professional, clear, actionable, and appropriate for residents.", "weight": 1.5, "judge_prompt": "Evaluate ONLY the resident-facing email PDF for professional quality. You may reference all outputs to locate it. Consider:\n- Clear and informative Subject line (mentions move-out/final inspection)\n- Greeting and respectful, clear tone; concise paragraphs\n- States default inspection date (9/23/25) and explains how to request a different date\n- Provides clear contact method (email/phone) and expected response window\n- Identifies Qyrevia Property Management in signature or header\n- Free of obvious grammar/spelling errors; formatted for easy reading\n\nScoring (0\u20131.5):\n- 1.5: Excellent professional email with all elements above, polished formatting, and unambiguous instructions.\n- 1.0: Generally professional; minor omissions (e.g., weaker subject or missing response window) but still clear and actionable.\n- 0.5: Understandable but several issues (tone/clarity/formatting) or missing key element like contact method.\n- 0.0: Not professional, confusing, or not clearly an email to residents.", "expectation": "A crisp, courteous, well-formatted email with subject, default date, rescheduling instructions, and contact info, branded as Qyrevia."}, {"type": "llm_judge", "name": "Schedule Readability and Manager Usefulness", "description": "Assess the schedule PDF for readability, organization, and usefulness to the manager.", "weight": 1.5, "judge_prompt": "Evaluate the inspection schedule PDF for managerial usefulness. Consider:\n- Clear title and date context (e.g., September move-outs)\n- Well-structured table with visible headers for Unit #, Resident Name, Move-Out Date, Inspection Date\n- Legible formatting: alignment, consistent date formats, reasonable sorting (e.g., by date or unit)\n- Optional helpful additions: notes column (e.g., special requests), pagination, footer with preparer/date\n\nScoring (0\u20131.5):\n- 1.5: Highly readable, neatly formatted table with clear headers and consistent layout; obviously useful for tracking.\n- 1.0: Usable but could be cleaner (minor alignment or formatting inconsistencies).\n- 0.5: Messy or hard to scan; headers unclear but table somewhat usable.\n- 0.0: Not helpful as a tracking schedule (e.g., lacks clear table structure or headers).", "expectation": "A clean, clearly titled PDF roster with a well-formatted table containing the four required columns and readable entries."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "24d1e93f-9018-45d4-b522-ad89dfd78079", "rubric": {"category_name": "Manufacturing \u2014 Buyers and Purchasing Agents: Vendor NPV Analysis (Headlamps)", "rationale": "This rubric enforces a self-documenting Excel workbook for a 3-vendor NPV analysis with explicit assumptions and cash-flow build-up. Stage 1 (LLM-only) mandates a very specific workbook shape to make verification trivial. Stage 2 (code) performs plausibility and consistency checks enabled by the enforced shape (discount rate usage, 70:30 split, tooling amortization cap, R&D timing/allocation, and summary ranking logic). Stage 3 (LLM) assesses professional quality and suitability for an executive audience.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Structured Output Gate (LLM-only)", "description": "Validates that the candidate produced the exact Excel workbook structure needed for verification.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.0, "rules": [{"type": "llm_judge", "name": "Structured NPV Workbook Requirement (GATE)", "description": "The output must be an Excel workbook with one vendor sheet per supplier and a final summary sheet. Each vendor sheet must contain clearly labeled sections and tables with specified columns. This is a structure-only check; do not assess calculation correctness.", "weight": 6.0, "judge_prompt": "You are checking ONLY the format/structure of the submitted output (not the correctness of numbers). The expected output is an Excel workbook with the following required structure for a headlamp sourcing NPV analysis across three vendors.\n\nFORMAT REQUIREMENTS:\n- Must be a valid Excel spreadsheet (.xlsx preferred).\n- Must include FOUR sheets minimum:\n  1) One sheet for each vendor (3 total): Autolantic, Vendocrat, Solimoto. Flexible naming allowed (e.g., \"NPV - Autolantic\", \"Autolantic NPV\").\n  2) One final comparison sheet named like \"Summary\", \"Supplier Comparison\", \"NPV Summary\", or similar.\n\nREQUIRED STRUCTURE IN EACH VENDOR SHEET (flexible section titles allowed, but the content must be clearly present and visible, not hidden):\n1) Header identifying the vendor by name (Autolantic, Vendocrat, or Solimoto).\n2) An \"Input Assumptions\" section containing a 3-column table with columns similar to [Parameter | Value | Notes/Source]. It MUST include explicit rows for:\n   - Discount Rate for Years 2\u20134 = 10%\n   - Inflation Rate = 0%\n   - Volume Split (Base:Premium) = 70% : 30%\n   - Tooling amortization cap (sets) = 100,000 (one set = 2 headlamps)\n   - R&D payment timing = Year 1 upfront\n   - R&D allocation across variants = 50/50 between Base and Premium\n3) A \"Volume Plan\" section with a table having columns similar to:\n   - [Year | Vehicle Sales | Headlamp Sets | Base Sets (70%) | Premium Sets (30%)]\n   - Must cover a 4-year horizon (Years 1\u20134) and reflect the 70:30 split.\n4) A vendor-specific \"Cost Build-up\" or \"Cash Outflow Detail\" section that rolls up costs with columns similar to:\n   - [Year | R&D Upfront | Tooling Amortization | Piece Cost - Base | Piece Cost - Premium | Total Cash Outflow]\n   - It can include additional columns, but these components must be clearly visible.\n5) An \"NPV Calculation\" section showing the step-by-step calculation log with a table like:\n   - [Year | Total Cash Outflow | Discount Factor | Present Value]\n   - A clearly labeled final line: \"Total NPV (Outflows, USD)\" or equivalent.\n\nREQUIRED SUMMARY SHEET (final comparison):\n- A table comparing all three suppliers side-by-side with columns similar to:\n  - [Supplier Name | Total NPV (Outflows, USD) | Rank (1 = Best/Lowest NPV) | Recommendation | Rationale/Comments]\n- The recommendation must be clearly stated on this sheet.\n\nOPTIONAL (not required for full credit, but good to see):\n- A simple sensitivity mini-table on the Summary sheet showing NPV at alternate discount rates (e.g., 8% and 12%).\n\nSCORING (structure only):\n- 6.0: Valid Excel + All 3 vendor sheets present + Summary sheet present. Each vendor sheet has all 5 required sections with appropriate tables/columns, and the summary sheet has the comparison table as specified. Labels can vary but structure must be unambiguous.\n- 5.0: Everything above except missing only the optional sensitivity table or very minor labeling deviations where structure is still clear.\n- 4.0: Exactly one vendor sheet is missing one required section OR the Summary sheet is missing one column (but still clearly compares NPVs for all three suppliers).\n- 2.5: Multiple required sections missing across vendor sheets OR the Summary sheet lacks clear NPV comparison for all three vendors.\n- 0.0: Not an Excel file OR fewer than 4 sheets (3 vendor + 1 summary) OR the structure is too disorganized to verify the elements above.\n\nImportant: Do NOT judge numerical correctness. Only confirm the presence and clarity of the required sections/tables so that verification is possible.", "expectation": "An .xlsx with 4+ sheets: one per vendor (Autolantic, Vendocrat, Solimoto) and a Summary sheet. Each vendor sheet must contain Input Assumptions, Volume Plan, Cost Build-up, and NPV Calculation sections, with a clearly labeled Total NPV. The Summary sheet must compare NPVs and state the recommendation."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Compliance Checks (Code + light heuristics)", "description": "Now that the structure exists, verify key requirements with deterministic and heuristic checks enabled by the shape: discount rate usage, 70:30 split, amortization cap, R&D timing/allocation, and summary ranking/recommendation consistency.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Discount Rate (10%) Stated for Years 2\u20134", "description": "Checks each vendor sheet for presence of a 10% discount rate tied to Years 2\u20134 in assumptions or calculation areas (heuristic text scan).", "weight": 0.6, "code": "import re\\nimport pandas as pd\\nimport numpy as np\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_spreadsheet:\\n        return 0.0\\n    try:\\n        path = context.files.get_path(output.id)\\n        xls = pd.ExcelFile(path)\\n    except Exception:\\n        return 0.0\\n\\n    sheet_names = [s for s in xls.sheet_names]\\n    vendors = ['autolantic','vendocrat','solimoto']\\n    vendor_sheets = [s for s in sheet_names if any(v in s.lower() for v in vendors)]\\n    if not vendor_sheets:\\n        return 0.0\\n\\n    def sheet_has_10pct(sn):\\n        try:\\n            df = pd.read_excel(path, sheet_name=sn, header=None)\\n        except Exception:\\n            return False\\n        hits = 0\\n        for r in range(min(df.shape[0], 500)):\\n            row = ' '.join([str(x) for x in df.iloc[r].tolist()]).lower()\\n            if 'discount' in row and (('10%' in row) or (' 10 %' in row) or re.search(r'\\b0?\\.1\\b', row)):\\n                hits += 1\\n        return hits > 0\\n\\n    score = sum(1.0 if sheet_has_10pct(s) else 0.0 for s in vendor_sheets) / len(vendor_sheets)\\n    return float(score * 0.6)\\n"}, {"type": "code", "name": "70:30 Base:Premium Volume Split Implemented", "description": "Verifies the 70/30 split is present. First tries to detect Base and Premium volume columns and test ratios across rows; falls back to scanning for explicit 70:30 language if table parsing fails.", "weight": 0.6, "code": "import re\\nimport pandas as pd\\nimport numpy as np\\n\\nTOL = 0.02\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_spreadsheet:\\n        return 0.0\\n    try:\\n        path = context.files.get_path(output.id)\\n        xls = pd.ExcelFile(path)\\n    except Exception:\\n        return 0.0\\n\\n    sheet_names = [s for s in xls.sheet_names]\\n    vendors = ['autolantic','vendocrat','solimoto']\\n    vendor_sheets = [s for s in sheet_names if any(v in s.lower() for v in vendors)]\\n    if not vendor_sheets:\\n        return 0.0\\n\\n    def try_ratio_check(sn):\\n        try:\\n            df = pd.read_excel(path, sheet_name=sn)\\n        except Exception:\\n            return 0.0\\n        # Find likely base and premium columns\\n        cols = [str(c).lower() for c in df.columns]\\n        base_idx = [i for i,c in enumerate(cols) if ('base' in c) and ('cost' not in c)]\\n        prem_idx = [i for i,c in enumerate(cols) if ('premium' in c) and ('cost' not in c)]\\n        if not base_idx or not prem_idx:\\n            return 0.0\\n        ok_rows = 0\\n        total_rows = 0\\n        for bi in base_idx:\\n            for pi in prem_idx:\\n                # Iterate numeric rows\\n                for _, row in df.iloc[:, [bi, pi]].iterrows():\\n                    try:\\n                        b = float(row.iloc[0])\\n                        p = float(row.iloc[1])\\n                    except Exception:\\n                        continue\\n                    if not np.isfinite(b) or not np.isfinite(p):\\n                        continue\\n                    if b <= 0 and p <= 0:\\n                        continue\\n                    total = b + p\\n                    if total <= 0:\\n                        continue\\n                    total_rows += 1\\n                    ratio = b / total\\n                    if abs(ratio - 0.70) <= TOL:\\n                        ok_rows += 1\\n        if total_rows >= 2 and ok_rows >= max(1, int(0.5 * total_rows)):\\n            return 1.0\\n        return 0.0\\n\\n    def fallback_text(sn):\\n        try:\\n            df = pd.read_excel(path, sheet_name=sn, header=None)\\n        except Exception:\\n            return 0.0\\n        text = ' \\n'.join(' '.join([str(x) for x in df.iloc[r].tolist()]) for r in range(min(df.shape[0], 500))).lower()\\n        # Look for common notations of 70/30 split\\n        if ('70:30' in text) or ('70 / 30' in text) or ('70-30' in text) or ('70% ' in text and '30% ' in text and 'split' in text):\\n            return 1.0\\n        return 0.0\\n\\n    per_vendor = []\\n    for s in vendor_sheets:\\n        r = try_ratio_check(s)\\n        if r < 1.0:\\n            r = max(r, fallback_text(s))\\n        per_vendor.append(r)\\n\\n    score = sum(per_vendor) / len(vendor_sheets)\\n    return float(score * 0.6)\\n"}, {"type": "code", "name": "Tooling Amortization Cap = 100,000 Sets Noted", "description": "Checks each vendor sheet for presence of a 100,000 set amortization cap tied to tooling/amortization in assumptions or notes (regex tolerant of commas/spaces).", "weight": 0.4, "code": "import re\\nimport pandas as pd\\n\\nPAT = re.compile(r\"\\b100\\s*,?\\s*000\\b\")\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_spreadsheet:\\n        return 0.0\\n    try:\\n        path = context.files.get_path(output.id)\\n        xls = pd.ExcelFile(path)\\n    except Exception:\\n        return 0.0\\n\\n    sheet_names = [s for s in xls.sheet_names]\\n    vendors = ['autolantic','vendocrat','solimoto']\\n    vendor_sheets = [s for s in sheet_names if any(v in s.lower() for v in vendors)]\\n    if not vendor_sheets:\\n        return 0.0\\n\\n    def has_cap(sn):\\n        try:\\n            df = pd.read_excel(path, sheet_name=sn, header=None)\\n        except Exception:\\n            return False\\n        for r in range(min(df.shape[0], 500)):\\n            row = ' '.join([str(x) for x in df.iloc[r].tolist()]).lower()\\n            if ('tool' in row or 'amort' in row) and PAT.search(row):\\n                return True\\n        return False\\n\\n    score = sum(1.0 if has_cap(s) else 0.0 for s in vendor_sheets) / len(vendor_sheets)\\n    return float(score * 0.4)\\n"}, {"type": "code", "name": "R&D Costs Upfront in Year 1 and Split Equally 50/50", "description": "Checks each vendor sheet text for R&D being paid in Year 1 (upfront) and mentions of equal split 50/50 across variants. Heuristic text scan with partial credit.", "weight": 0.6, "code": "import re\\nimport pandas as pd\\n\\nUPFRONT_PATTERNS = [r\"year\\s*1\", r\"y\\s*1\", r\"upfront\", r\"paid\\s*in\\s*year\\s*1\"]\\nSPLIT_PATTERNS = [r\"50\\s*/\\s*50\", r\"50-50\", r\"50:50\", r\"split\\s*equally\", r\"equal\\s*split\"]\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_spreadsheet:\\n        return 0.0\\n    try:\\n        path = context.files.get_path(output.id)\\n        xls = pd.ExcelFile(path)\\n    except Exception:\\n        return 0.0\\n\\n    sheet_names = [s for s in xls.sheet_names]\\n    vendors = ['autolantic','vendocrat','solimoto']\\n    vendor_sheets = [s for s in sheet_names if any(v in s.lower() for v in vendors)]\\n    if not vendor_sheets:\\n        return 0.0\\n\\n    def scan(sn):\\n        try:\\n            df = pd.read_excel(path, sheet_name=sn, header=None)\\n        except Exception:\\n            return 0.0\\n        blob = ' \\n'.join(' '.join([str(x) for x in df.iloc[r].tolist()]) for r in range(min(df.shape[0], 600))).lower()\\n        has_rd = ('r&d' in blob) or ('research' in blob and 'development' in blob) or ('engineering' in blob)\\n        if not has_rd:\\n            return 0.0\\n        upfront = any(re.search(p, blob) for p in UPFRONT_PATTERNS)\\n        split = any(re.search(p, blob) for p in SPLIT_PATTERNS)\\n        # 0.5 for upfront signal, 0.5 for equal split signal\\n        return (0.5 if upfront else 0.0) + (0.5 if split else 0.0)\\n\\n    per_vendor = [scan(s) for s in vendor_sheets]\\n    score = sum(per_vendor) / (len(vendor_sheets) * 1.0)\\n    return float(score * 0.6)\\n"}, {"type": "code", "name": "Summary Sheet: NPVs Numeric, Ranking and Recommendation Consistent", "description": "Verifies the summary/comparison sheet lists NPVs for all three vendors, values are numeric, ranking matches ascending NPVs, and the recommendation aligns with the best (lowest NPV) within tie tolerance.", "weight": 0.6, "code": "import re\\nimport pandas as pd\\nimport numpy as np\\n\\nVENDORS = ['autolantic','vendocrat','solimoto']\\n\\ndef parse_number(x):\\n    if isinstance(x, (int, float)) and np.isfinite(x):\\n        return float(x)\\n    if isinstance(x, str):\\n        s = x.strip().lower().replace('$','').replace(',','')\\n        # Keep signs, digits, decimal, exponent\\n        s = re.sub(r'[^0-9eE+\\-\\.]+', '', s)\\n        if s in ('', '-', '+'):\\n            return None\\n        try:\\n            return float(s)\\n        except Exception:\\n            return None\\n    return None\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_spreadsheet:\\n        return 0.0\\n    try:\\n        path = context.files.get_path(output.id)\\n        xls = pd.ExcelFile(path)\\n    except Exception:\\n        return 0.0\\n\\n    # Identify summary sheet by name\\n    summary_candidates = [s for s in xls.sheet_names if any(k in s.lower() for k in ['summary','comparison'])]\\n    if not summary_candidates:\\n        return 0.0\\n    sname = summary_candidates[0]\\n    try:\\n        df = pd.read_excel(path, sheet_name=sname)\\n    except Exception:\\n        return 0.0\\n\\n    # Normalize columns\\n    cols = {str(c).strip().lower(): i for i,c in enumerate(df.columns)}\\n    def find_col(keys):\\n        for k in cols:\\n            if any(key in k for key in keys):\\n                return cols[k]\\n        return None\\n\\n    supplier_col = find_col(['supplier','vendor'])\\n    npv_col = find_col(['npv','present value'])\\n    rank_col = find_col(['rank'])\\n    rec_col = find_col(['recommend']) or find_col(['nomina'])\\n\\n    if supplier_col is None or npv_col is None:\\n        return 0.0\\n\\n    # Build records\\n    rows = []\\n    for _, row in df.iterrows():\\n        name = str(row.iloc[supplier_col]).strip().lower() if supplier_col is not None else ''\\n        if not name:\\n            continue\\n        npv = parse_number(row.iloc[npv_col]) if npv_col is not None else None\\n        rank = None\\n        if rank_col is not None:\\n            rank = parse_number(row.iloc[rank_col])\\n        rec = None\\n        if rec_col is not None:\\n            rec = str(row.iloc[rec_col]).strip().lower()\\n        rows.append({'name': name, 'npv': npv, 'rank': rank, 'rec': rec})\\n\\n    # Filter to vendors if present, otherwise use all\\n    have_vendor_rows = [r for r in rows if any(v in r['name'] for v in VENDORS)]\\n    data = have_vendor_rows if len(have_vendor_rows) >= 3 else rows\\n\\n    # Require at least 3 entries with numeric NPV\\n    numeric_ok = [r for r in data if r['npv'] is not None]\\n    if len(numeric_ok) < 3:\\n        return 0.0\\n\\n    # 0.3 for numeric NPVs present for all three vendors\\n    vendors_present = set()\\n    for r in numeric_ok:\\n        for v in VENDORS:\\n            if v in r['name']:\\n                vendors_present.add(v)\\n    part_numeric = 0.3 if len(vendors_present) == 3 else 0.15\\n\\n    # Determine best (lowest NPV) and check ranks\\n    sorted_rows = sorted(numeric_ok, key=lambda r: r['npv'])\\n    best_name = sorted_rows[0]['name']\\n\\n    part_rank = 0.0\\n    if any(r['rank'] is not None for r in numeric_ok):\\n        # derive expected ranks from ascending NPV\\n        expected = {sorted_rows[i]['name']: i+1 for i in range(len(sorted_rows))}\\n        mismatches = 0\\n        checked = 0\\n        for r in numeric_ok:\\n            if r['rank'] is not None and r['name'] in expected:\\n                checked += 1\\n                if int(round(r['rank'])) != expected[r['name']]:\\n                    mismatches += 1\\n        if checked > 0:\\n            part_rank = 0.2 * (1.0 - (mismatches/checked))\\n\\n    part_rec = 0.0\\n    if any(r['rec'] for r in numeric_ok):\\n        # Accept 'yes' on the best row or the best vendor name stated in rec column cell\\n        best_row = next((r for r in numeric_ok if r['name'] == best_name), None)\\n        if best_row and best_row['rec']:\\n            val = best_row['rec']\\n            if any(tok in val for tok in ['yes','1','recommended','select']):\\n                part_rec = 0.1\\n        else:\\n            # If rec column carries vendor names, check if any rec cell contains best vendor\\n            if any(best_name in (r['rec'] or '') for r in numeric_ok):\\n                part_rec = 0.1\\n\\n    return float(part_numeric + part_rank + part_rec)\\n"}, {"type": "code", "name": "Discount Factors Plausible for Years 2\u20134", "description": "Heuristically checks for discount factor values near 0.909, 0.826, 0.751 (10% for years 2\u20134) in each vendor sheet.", "weight": 0.2, "code": "import re\\nimport pandas as pd\\nimport numpy as np\\n\\nTARGETS = [0.909, 0.826, 0.751]\\nTOL = 0.03\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_spreadsheet:\\n        return 0.0\\n    try:\\n        path = context.files.get_path(output.id)\\n        xls = pd.ExcelFile(path)\\n    except Exception:\\n        return 0.0\\n\\n    sheet_names = [s for s in xls.sheet_names]\\n    vendors = ['autolantic','vendocrat','solimoto']\\n    vendor_sheets = [s for s in sheet_names if any(v in s.lower() for v in vendors)]\\n    if not vendor_sheets:\\n        return 0.0\\n\\n    def has_factors(sn):\\n        try:\\n            df = pd.read_excel(path, sheet_name=sn)\\n        except Exception:\\n            return False\\n        # Check columns first for something like 'discount factor'\\n        cols = [str(c).lower() for c in df.columns]\\n        candidate_cols = [i for i,c in enumerate(cols) if ('discount' in c and 'factor' in c) or ('disc factor' in c)]\\n        values = []\\n        if candidate_cols:\\n            for ci in candidate_cols:\\n                for v in df.iloc[:, ci].tolist():\\n                    try:\\n                        fv = float(v)\\n                        if np.isfinite(fv):\\n                            values.append(fv)\\n                    except Exception:\\n                        continue\\n        else:\\n            # Fallback: scan whole frame for numeric values\\n            for col in df.columns:\\n                for v in df[col].tolist():\\n                    try:\\n                        fv = float(v)\\n                        if np.isfinite(fv):\\n                            values.append(fv)\\n                    except Exception:\\n                        continue\\n        hits = 0\\n        for t in TARGETS:\\n            if any(abs(x - t) <= TOL for x in values):\\n                hits += 1\\n        return hits >= 2\\n\\n    score = sum(1.0 if has_factors(s) else 0.0 for s in vendor_sheets) / len(vendor_sheets)\\n    return float(score * 0.2)\\n"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Presentation and Decision Quality", "description": "Assesses executive readiness, clarity, and decision usefulness for Finance/Program stakeholders.", "is_required": false, "max_points": 1.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Quality and Decision Usefulness", "description": "Evaluates if the workbook is professionally presented, clearly explains methodology/assumptions, and provides a defensible recommendation appropriate for finance/controller review.", "weight": 1.0, "judge_prompt": "Evaluate the overall professionalism and decision usefulness of the workbook. Consider: \\n- Clarity and readability of sections, consistent formatting, labeled tables, and units/currencies.\\n- Presence of a succinct methodology/assumptions narrative (beyond just tables) that explains how R&D, tooling amortization (100,000 sets), the 70:30 split, and the 10% discount rate (Years 2\u20134) are applied.\\n- Whether the Summary sheet communicates the recommendation clearly with brief, finance-relevant rationale (e.g., cost drivers, trade-offs, risk notes).\\n- Suitability for a finance/controller audience (concise, auditable, and free of ambiguity).\\n\\nScoring:\\n- 1.0: Highly professional: clean, consistent formats; clear methodology notes; unambiguous recommendation with strong rationale.\\n- 0.7: Generally professional: minor formatting issues; rationale present but could be sharper.\\n- 0.4: Mixed quality: readable but lacks some clarity in methodology or rationale.\\n- 0.2: Barely adequate: significant formatting issues or unclear recommendation.\\n- 0.0: Not appropriate for executive review or impossible to follow.\\n\\nDo not re-check structural presence already graded in Stage 1 or numeric correctness tested in Stage 2; focus on presentation and decision usefulness.", "expectation": "A clean, well-formatted workbook with a concise methodology note, finance-ready summary, and a clear, defensible recommendation."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "b78fd844-db76-448e-a783-5e9877cb74c2", "rubric": {"category_name": "Finance and Insurance \u2014 Financial Managers: Investment Recommendation Report (Tiny-Rod Hit Inc.)", "rationale": "Mixed task: a Board-ready document with embedded directional financial analysis. Stage 1 uses an LLM judge to strictly enforce a verifiable, document-first structure (PDF/DOCX, <=15 pages, required sections, tables). Stage 2 combines code checks (text parsing for key financial signals, allocation math coherence, risk-mitigation-contingency presence) with an LLM consistency check for the recommendation. Stage 3 assesses overall professional quality and strategic insight for a Board audience.", "max_total_score": 10.5, "stages": [{"name": "Stage 1 \u2014 Format and Structural Gate", "description": "Gate: The output must be a Board-addressed PDF/DOCX report (<=15 pages) with specific sections and tables enabling verification. Only structure/shape checked here, not correctness.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Document Format and Structure Requirements (GATE)", "description": "Verify the candidate output is a PDF or DOCX report to the Board, within 15 pages, containing all required sections and prescribed tables to enable verification.", "weight": 4.0, "judge_prompt": "You are evaluating whether the submitted output conforms to a strict structural specification. Only check PRESENCE and STRUCTURE, not content correctness. Be flexible with exact phrasing of headers, but the required elements must be clearly present. If the shape is wrong or incomplete, score lower. If not a PDF/DOCX document or longer than 15 pages, score very low.\n\nRequired format and layout:\n- Must be a PDF or DOCX document (not Excel/CSV/plain text/code). The assignment asked for a Word-drafted report converted to PDF; accept PDF or DOCX.\n- Length: 15 pages or fewer.\n- Professionally formatted, addressed to the Board of Directors.\n\nRequired sections (flexible with naming but must be clearly identifiable):\n1) Executive Summary (must be on the first page or start very early). Should summarize the two opportunities, directional NPV/IRR insights, the initial recommendation, and the allocation scenario.\n2) Project Overviews (two subsections, e.g., Project A and Project B): brief description and strategic fit for each project.\n3) Assumptions and Methodology: explicitly state the WACC of 9% and that directional (not exact) estimates are used; outline key simplifying assumptions.\n4) Directional Financial Analysis: include at least one summary table comparing both projects side-by-side. The summary table must contain columns for each project and rows that include at minimum: Initial Investment (estimate), NPV (directional), IRR (directional). Optional additional rows (e.g., Payback, Sensitivity ranges) are acceptable.\n5) Recommendation to the Board: select one project and provide both quantitative and qualitative justification.\n6) Risks, Mitigations, and Contingencies for the recommended project: present the top three risks spanning financial and operational exposure (at least 3 total); for each risk, include both a specific mitigation strategy and a contingency plan. A table format is preferred with columns like [Risk | Type (Financial/Operational) | Mitigation | Contingency], but structured bullets are acceptable if all four elements are present per risk.\n7) Dual-Project Capital Allocation Plan: assume both projects are viable; propose how to allocate the $100 million across the two projects. Include amounts and/or percentages (they should conceptually sum to 100% or ~$100M) and a brief rationale referencing long-term value creation, diversification, and strategic alignment.\n8) Strategic Considerations Beyond Returns: a short section explicitly addressing long-term value creation, diversification, and strategic alignment (may be integrated into Recommendation or Allocation rationale if clearly labeled or obviously addressed).\nOptional: Appendices for calculation notes or sensitivity ranges.\n\nScoring guidance (structure only):\n- 1.0: PDF/DOCX, <=15 pages, addressed to Board, and all 8 required sections are clearly present with the specified table(s) enabling verification.\n- 0.8: PDF/DOCX, <=15 pages, addressed to Board, core sections present but missing one supporting element (e.g., Strategic Considerations explicitly labeled, or Risks table uses bullets but includes all fields). Summary comparison table exists.\n- 0.5: Valid PDF/DOCX but missing one core section (Executive Summary, Directional Financial Analysis with the summary table, Recommendation, or Risks with mitigation and contingency) OR lacks the comparison table.\n- 0.2: Valid PDF/DOCX but missing multiple core sections and/or exceeds 15 pages or not addressed to Board.\n- 0.0: Not a PDF/DOCX document at all.\nOnly judge the presence and structure. Do not assess numerical correctness or writing quality.", "expectation": "A <=15-page PDF/DOCX, addressed to the Board, with all required sections and a clear side-by-side financial summary table enabling verification."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification and Consistency Checks", "description": "Now that the structure exists, verify key correctness signals with code and a focused LLM cross-check: presence of WACC=9%, directional NPV/IRR for both projects, coherent $100M allocation, and that risks include mitigation and contingency for each.", "is_required": false, "max_points": 4.5, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Core Financial Signals Present (WACC, NPV, IRR for both projects)", "description": "Checks document text for WACC=9%, and at least two NPV mentions with numeric context and at least two IRR mentions with percent values (covering both projects). Also checks reference to $100 million cash.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        text = \"\"\n    if not text:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text:\n        return 0.0, \"Unable to extract text from document.\"\n\n    t = text.lower()\n\n    # WACC 9% check (flexible)\n    wacc_patterns = [\n        r\"wacc[^%]{0,50}9\\s*%\",\n        r\"9\\s*%[^%]{0,50}wacc\",\n        r\"weighted\\s+average\\s+cost\\s+of\\s+capital[^%]{0,80}9\\s*%\"\n    ]\n    has_wacc = any(re.search(p, t) for p in wacc_patterns)\n\n    # $100 million available cash reference (flexible)\n    cash_patterns = [r\"\\$?\\s*100\\s*(million|m)\\b\", r\"one\\s*hundred\\s*million\"]\n    has_cash = any(re.search(p, t) for p in cash_patterns)\n\n    # NPV mentions with numeric context\n    npv_matches = re.findall(r\"npv[^\\$%\\d]{0,60}(\\$?\\s?-?\\d[\\d,\\.]*\\s*(million|m)?)\", t)\n    has_two_npvs = len(npv_matches) >= 2\n\n    # IRR mentions with % values\n    irr_matches = re.findall(r\"irr[^%\\n]{0,60}(-?\\d{1,3}(?:\\.\\d+)?\\s*%)\", t)\n    has_two_irrs = len(irr_matches) >= 2\n\n    checks = [has_wacc, has_cash, has_two_npvs, has_two_irrs]\n    score = sum(1 for c in checks if c)\n    weight = 1.5\n    frac = score / 4.0\n    points = frac * weight\n\n    feedback_bits = []\n    if not has_wacc:\n        feedback_bits.append(\"WACC 9% not clearly found.\")\n    if not has_cash:\n        feedback_bits.append(\"Reference to $100 million not clearly found.\")\n    if not has_two_npvs:\n        feedback_bits.append(\"At least two NPV values (one per project) not clearly found.\")\n    if not has_two_irrs:\n        feedback_bits.append(\"At least two IRR values (one per project) not clearly found.\")\n    feedback = \" | \".join(feedback_bits) if feedback_bits else \"All core financial signals detected.\"\n\n    return points, feedback"}, {"type": "code", "name": "Allocation Sums Coherent (~$100M or ~100%)", "description": "Detects a dual-project allocation plan. Attempts to find two amounts summing to roughly $100M (+/-10%) or two percentages summing to ~100% (+/-5%). Awards partial credit if only one allocation form is coherent or if at least one valid amount/percent is present.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        text = \"\"\n    if not text:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text:\n        return 0.0, \"Unable to extract text from document.\"\n\n    t = text.lower()\n\n    # Heuristic: focus on sentences/lines mentioning allocation keywords\n    alloc_blocks = []\n    for line in t.splitlines():\n        if any(k in line for k in [\"allocate\", \"allocation\", \"capital plan\", \"split\", \"distribution\"]):\n            alloc_blocks.append(line)\n    block_text = \"\\n\".join(alloc_blocks) if alloc_blocks else t\n\n    # Extract monetary amounts with M/million\n    amt_matches = re.findall(r\"\\$?\\s*([0-9]{1,3}(?:[,\\s][0-9]{3})*(?:\\.[0-9]+)?|[0-9]+\\.?[0-9]*)\\s*(million|m)\\b\", block_text)\n    amts_millions = []\n    for val, unit in amt_matches:\n        try:\n            v = float(val.replace(',', '').strip())\n            amts_millions.append(v)\n        except Exception:\n            pass\n\n    # Extract percentages\n    pct_matches = re.findall(r\"(\\d{1,3}(?:\\.\\d+)?)\\s*%\", block_text)\n    pcts = []\n    for p in pct_matches:\n        try:\n            v = float(p)\n            if 0 <= v <= 100:\n                pcts.append(v)\n        except Exception:\n            pass\n\n    # Evaluate sums using best two values if many found\n    def best_two_sum_close(values, target, tol):\n        if len(values) < 2:\n            return None\n        best = None\n        n = len(values)\n        for i in range(n):\n            for j in range(i+1, n):\n                s = values[i] + values[j]\n                if abs(s - target) <= tol:\n                    # exact hit within tol\n                    return s\n                if best is None or abs(s - target) < abs(best - target):\n                    best = s\n        return best\n\n    # Amounts in millions should sum near 100 (+/-10)\n    amount_sum = best_two_sum_close(amts_millions, 100.0, 10.0)\n    amount_ok = amount_sum is not None and abs(amount_sum - 100.0) <= 10.0\n\n    # Percentages should sum near 100% (+/-5)\n    pct_sum = best_two_sum_close(pcts, 100.0, 5.0)\n    pct_ok = pct_sum is not None and abs(pct_sum - 100.0) <= 5.0\n\n    weight = 1.5\n\n    # Scoring logic\n    if amount_ok and pct_ok:\n        return weight, \"Amounts and percentages both coherent (near $100M and 100%).\"\n    if amount_ok or pct_ok:\n        return weight * 0.8, \"One allocation form coherent (near target).\"\n\n    # Partial: evidence of allocation values present but sums off\n    if len(amts_millions) >= 2 or len(pcts) >= 2:\n        return weight * 0.4, \"Allocation values found but sums not near targets.\"\n\n    return 0.0, \"No coherent allocation detected (two clear amounts or percents not found).\""}, {"type": "code", "name": "Risks Include Mitigations and Contingencies", "description": "Checks that the recommended-project risk section includes at least three risks and includes both mitigation and contingency language.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        text = \"\"\n    if not text:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text:\n        return 0.0, \"Unable to extract text from document.\"\n\n    t = text.lower()\n\n    # Count occurrences of the word 'risk' near bullet-like structures or headings\n    risk_count = len(re.findall(r\"\\brisk\\b\", t))\n    has_mitig = bool(re.search(r\"mitigat\", t))\n    has_cont = bool(re.search(r\"contingenc\", t))\n\n    weight = 1.0\n\n    if risk_count >= 3 and has_mitig and has_cont:\n        return weight, \"Found >=3 risks with mitigation and contingency language.\"\n    if risk_count >= 3 and (has_mitig or has_cont):\n        return weight * 0.6, \"Found >=3 risks; only one of mitigation/contingency clearly present.\"\n    if risk_count >= 1 and (has_mitig or has_cont):\n        return weight * 0.3, \"Some risk content with partial mitigation/contingency.\"\n    return 0.0, \"Risk, mitigation, and contingency content not sufficiently present.\""}, {"type": "llm_judge", "name": "Recommendation Coherence vs. Directional Analysis", "description": "LLM checks whether the chosen recommendation is clearly named and supported by the directional financial analysis and/or strategic factors. If recommending the lower-NPV/IRR project, the rationale should credibly lean on qualitative/strategic arguments. Also check that the risks discussed correspond to the recommended project and that the allocation plan is consistent with earlier sections.", "weight": 0.5, "judge_prompt": "Review the document (content only; structure was checked in Stage 1). Evaluate:\n- Is exactly one project clearly recommended to the Board?\n- Does the recommendation refer back to the directional financial analysis (NPV/IRR) and/or strategic factors (long-term value, diversification, strategic alignment)?\n- If the recommended project does not appear to have the stronger directional metrics, does the narrative provide credible strategic justification?\n- Do the risks, mitigations, and contingencies clearly correspond to the recommended project?\n- Is the dual-project allocation plan logically consistent with the earlier rationale and totals ~100% or ~$100M?\nScoring guidance:\n- 1.0: Clear recommendation, supported by both directional metrics and strategic factors; risks align; allocation consistent with rationale.\n- 0.6: Clear recommendation with partial linkage to metrics/strategy; minor inconsistencies.\n- 0.3: Recommendation present but weakly supported or inconsistently linked to analysis.\n- 0.0: No clear recommendation or justification.", "expectation": "A single, clear recommendation that is coherently justified by directional metrics and strategy; consistent risks and allocation."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Executive Readability", "description": "Holistic assessment of professional polish, clarity, and strategic depth for a Board audience.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Clarity", "description": "Evaluate overall professionalism, structure flow, clarity, conciseness, and suitability for Board readers (executive tone, headings, tables, visuals).", "weight": 1.0, "judge_prompt": "Assess the document\u2019s professional presentation and clarity for a Board audience. Consider: executive tone, logical flow, clear headings/subheadings, readable tables/visuals, concise language, and absence of fluff. Also consider whether the document is easy to skim (Executive Summary clarity) and whether key takeaways are prominent.\nScoring guidance:\n- 1.0: Highly professional, clear, concise, and skimmable with strong visual/structural aids.\n- 0.6: Generally professional with minor clarity or flow issues.\n- 0.3: Adequate but somewhat verbose or poorly organized.\n- 0.0: Unprofessional or hard to follow.", "expectation": "A concise, well-structured, executive-ready document with clear tables and headings."}, {"type": "llm_judge", "name": "Strategic Depth and Practicality", "description": "Evaluate the depth of strategic insight and practicality of recommendations, including trade-offs and implementation realism.", "weight": 1.0, "judge_prompt": "Evaluate the strategic depth and practical value: Does the document go beyond project-specific returns to address long-term value creation, diversification, and strategic alignment? Are trade-offs acknowledged? Are the risk mitigations and contingency plans realistic and actionable? Are capital allocation and sequencing practical given the firm\u2019s strong balance sheet?\nScoring guidance:\n- 1.0: Strong strategic insight with realistic, actionable recommendations and explicit trade-offs.\n- 0.6: Some strategic insight; mostly practical but missing depth in one area.\n- 0.3: Superficial strategic comments; limited practicality.\n- 0.0: Lacks strategic perspective or practicality.", "expectation": "Recommendations grounded in strategy and practicality, acknowledging trade-offs and execution considerations."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "3a4c347c-4aec-43c7-9a54-eb1f816ab1f9", "rubric": {"category_name": "Asia Season Coverage Proposal (Editors)", "rationale": "Pattern B (Document). We enforce a strict DOCX structure up front (Stage 1 LLM-only gate) so verification becomes trivial. Stage 2 mixes code and LLM to validate required KPIs, schedule logic, Asian country coverage, and budget plausibility against the brief. Stage 3 uses an LLM judge to assess editorial quality, feasibility, and audience fit.", "max_total_score": 11.5, "stages": [{"name": "Stage 1 \u2014 Format and Mandatory Structure Gate", "description": "LLM-only gate that verifies the output is a Word document (DOCX) no longer than 6 pages and includes all required sections and structural elements to enable downstream verification.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Document Format and Structure Gate", "description": "Checks that the submission is a DOCX with required sections, page length, and a verifiable schedule layout.", "weight": 4.0, "judge_prompt": "You are validating the structure of a proposal document for an editorial season. Only assess FORMAT and PRESENCE of required elements (not quality or correctness). Scoring must reflect how completely the required structure is present.\n\nRequirements to check:\n1) File format and length\n- Must be a Microsoft Word document (DOCX). Not PDF, not plain text, not Excel.\n- Length: 2\u20136 pages (inclusive). If fewer than 2 or more than 6 pages, fail.\n\n2) Required sections (headers can vary slightly but must be clearly present):\n- Suggested season title (on page 1)\n- Introduction\n- Aims of the season\n- Potential news hooks (for scheduling)\n- Suggested budget (a dedicated section)\n- Story ideas (with proposed contributors) AND clear suitability flags for VT/radio/podcast\n- Proposed CTO interviewees\n- Draft broadcast and publication schedule covering a 4-week period (weeks clearly delineated)\n- KPIs section that includes: page views, time on page, bounce rate, CTR, social engagement (likes/shares/comments), and sales sponsorship success as an added measure\n\n3) Schedule structure (presence-only, not correctness):\n- Evident Monday, Wednesday, Friday rhythm\n- Each of the 4 weeks has two online features and one CTO interview identified in the plan (either listed explicitly or clearly summarized)\n- VT requirement: either one VT per week OR at minimum one clearly-flagged VT in the season accompanied by radio/podcast re-versioning notes (accept either as compliant)\n\n4) International context and Asia coverage intent\n- Document signals international audience and explicitly mentions Asia focus with intent to cover multiple Asian countries (don\u2019t verify exact countries here; just scan for clear intent)\n\nScoring (return a decimal from 0.0 to 1.0 then it will be weighted):\n- 1.0: DOCX; 2\u20136 pages; ALL required sections present; schedule shows 4 weeks with Mon/Wed/Fri cadence; VT requirement noted (weekly or season-level); KPIs include all listed metrics including sponsorship; international + Asia focus stated.\n- 0.85: DOCX; 2\u20136 pages; all core sections present but one minor item thin/missing (e.g., VT re-versioning note missing but VT present; or KPIs missing one social sub-metric while sponsorship present).\n- 0.7: DOCX; 2\u20136 pages; missing up to two required components (e.g., schedule is present but not clearly 4 weeks; or KPIs missing two metrics; or contributors not clearly tied to stories).\n- 0.4: DOCX present but structural completeness is weak (3\u20134+ missing elements), or schedule missing entirely, or KPIs largely missing.\n- 0.0: Not a DOCX OR outside 2\u20136 pages OR missing multiple core sections making verification impossible.\n\nOnly judge structure and presence, not quality or correctness.", "expectation": "A professional DOCX (2\u20136 pages) containing all requested sections, a clearly structured 4-week Mon/Wed/Fri schedule, explicit VT with radio/podcast re-versioning, and a KPIs section including sponsorship success."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification: KPIs, Budget, Coverage, and Schedule", "description": "Code and LLM verification of correctness and plausibility, leveraging the mandated structure from Stage 1.", "is_required": false, "max_points": 5.5, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "KPI Coverage Check", "description": "Verify presence of required KPIs and sponsorship success metric in the DOCX text.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output resource found.\"\n\n    text = \"\"\n    try:\n        if output.is_document:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_pdf_text(output.id)\n                except Exception:\n                    text = \"\"\n        elif output.is_text_format:\n            text = context.files.read_text(output.id)\n        else:\n            text = \"\"\n    except Exception:\n        text = \"\"\n\n    if not text:\n        return 0.0, \"Could not extract text from document.\"\n\n    t = text.lower()\n\n    # Required KPI groups\n    kpis = {\n        'page_views': any(kw in t for kw in [\"page views\", \"pageviews\", \"pages viewed\"]),\n        'time_on_page': any(kw in t for kw in [\"time on page\", \"avg time on page\", \"average time on page\", \"dwell time\"]),\n        'bounce_rate': \"bounce rate\" in t,\n        'ctr': any(kw in t for kw in [\"click-through rate\", \"click through rate\", \"ctr\"]),\n        'social_engagement': (\n            (\"likes\" in t) + (\"shares\" in t) + (\"comments\" in t)\n        ) >= 2,  # allow at least 2 of 3 explicitly mentioned\n        'sponsorship': any(kw in t for kw in [\"sponsorship\", \"sponsor revenue\", \"sponsor secured\", \"sales team\", \"sold sponsorship\"]) \n    }\n\n    total = len(kpis)\n    hits = sum(1 for v in kpis.values() if v)\n    score = hits / total if total else 0.0\n\n    missing = [k for k,v in kpis.items() if not v]\n    feedback = f\"KPI hits: {hits}/{total}. Missing: {', '.join(missing) if missing else 'None'}.\"\n    return score, feedback"}, {"type": "code", "name": "Budget Plausibility Check", "description": "Validate travel budget is ~\u00a320k\u2013\u00a325k and freelancer costs (~\u00a31\u20131.5k each for two features) are acknowledged. Partial credit if ranges are implied without exact numbers.", "weight": 1.5, "code": "import re\n\ndef _extract_text(output, context):\n    txt = \"\"\n    if output.is_document:\n        try:\n            txt = context.files.read_docx_text(output.id)\n        except Exception:\n            try:\n                txt = context.files.read_pdf_text(output.id)\n            except Exception:\n                txt = \"\"\n    elif output.is_text_format:\n        try:\n            txt = context.files.read_text(output.id)\n        except Exception:\n            txt = \"\"\n    return txt\n\ndef _money_spans(text):\n    # Returns list of tuples (value_int, start_idx)\n    results = []\n    for m in re.finditer(r\"(?:\u00a3|gbp)?\\s*([0-9]{1,3}(?:,[0-9]{3})+|[0-9]+(?:\\.[0-9]+)?)\\s*(k|\\bk\\b)?\", text, flags=re.I):\n        num = m.group(1)\n        if num is None:\n            continue\n        val = float(num.replace(\",\", \"\"))\n        if m.group(2):\n            val = val * 1000.0\n        results.append((int(round(val)), m.start()))\n    return results\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output resource.\"\n\n    text = _extract_text(output, context)\n    if not text:\n        return 0.0, \"No readable text to evaluate budget.\"\n\n    t = text.lower()\n    spans = _money_spans(text)\n    if not spans:\n        return 0.0, \"No monetary amounts found.\"\n\n    # Travel indicators window\n    travel_terms = [\"travel\", \"flights\", \"accommodation\", \"local transport\", \"transport\", \"crew\", \"on-the-ground\"]\n\n    # Check for travel budget within 20k-25k near travel terms\n    travel_ok = False\n    for val, pos in spans:\n        if 20000 <= val <= 25000:\n            window = t[max(0, pos-120): pos+120]\n            if any(term in window for term in travel_terms):\n                travel_ok = True\n                break\n    # Fallback: overall mention in range without proximity\n    travel_weak = any(20000 <= val <= 25000 for val, _ in spans)\n\n    # Freelancer costs ~\u00a31k\u2013\u00a31.5k each (two features)\n    freelancer_terms = [\"freelancer\", \"stringer\", \"external\", \"commissioned\"]\n    freelancer_vals = [val for val, pos in spans if 900 <= val <= 1600]\n    freelancer_near = 0\n    for val, pos in spans:\n        if 900 <= val <= 1600:\n            window = t[max(0, pos-100): pos+100]\n            if any(term in window for term in freelancer_terms):\n                freelancer_near += 1\n    # Scoring logic\n    score = 0.0\n    details = []\n\n    # Travel contributes 0.7 of this rule\n    if travel_ok:\n        score += 0.7\n        details.append(\"Travel budget 20\u201325k near travel terms: OK\")\n    elif travel_weak:\n        score += 0.4\n        details.append(\"Travel budget 20\u201325k mentioned but not tied to travel terms\")\n    else:\n        details.append(\"Travel budget 20\u201325k not found\")\n\n    # Freelancer contributes 0.3 of this rule\n    if freelancer_near >= 2:\n        score += 0.3\n        details.append(\"Freelancer costs ~\u00a31\u20131.5k each (>=2 mentions) near freelancer terms: OK\")\n    elif len(freelancer_vals) >= 2:\n        score += 0.15\n        details.append(\"Two ~\u00a31\u20131.5k amounts present but not tied to freelancer terms\")\n    elif len(freelancer_vals) == 1:\n        score += 0.1\n        details.append(\"Single ~\u00a31\u20131.5k amount found\")\n    else:\n        details.append(\"Freelancer costs ~\u00a31\u20131.5k not found\")\n\n    # score is normalized [0,1] for this rule (0.7 + 0.3 max = 1.0)\n    return score, \"; \".join(details)"}, {"type": "code", "name": "Asia Coverage Diversity Check", "description": "Count distinct Asian countries mentioned in story ideas/schedule to ensure a good spread.", "weight": 1.0, "code": "import re\n\naisan_countries = {\n    'china','india','japan','south korea','korea','north korea','singapore','malaysia','indonesia','vietnam','thailand',\n    'philippines','taiwan','hong kong','pakistan','bangladesh','sri lanka','nepal','myanmar','burma','cambodia','laos',\n    'mongolia','kazakhstan','uzbekistan','turkmenistan','kyrgyzstan','tajikistan','afghanistan','iran','iraq','saudi arabia',\n    'united arab emirates','uae','qatar','kuwait','bahrain','oman','israel','jordan','lebanon','syria','yemen','turkiye','turkey',\n    'armenia','azerbaijan','georgia','brunei','timor-leste','east timor','bhutan','maldives'\n}\n\n# Also consider city-states/regions as hints\naliases = {\n    'hk': 'hong kong',\n}\n\ndef evaluate(workflow, context):\n    out = context.get_primary_output()\n    if not out:\n        return 0.0, \"No output.\"\n    text = \"\"\n    try:\n        if out.is_document:\n            try:\n                text = context.files.read_docx_text(out.id)\n            except Exception:\n                text = context.files.read_pdf_text(out.id)\n        elif out.is_text_format:\n            text = context.files.read_text(out.id)\n    except Exception:\n        text = \"\"\n    if not text:\n        return 0.0, \"No text to analyze.\"\n\n    t = text.lower()\n    for k,v in aliases.items():\n        t = re.sub(r\"\\b\"+re.escape(k)+r\"\\b\", v, t)\n\n    found = set()\n    for name in aisan_countries:\n        if re.search(r\"\\b\"+re.escape(name)+r\"\\b\", t):\n            # normalize 'korea' to 'south korea' if ambiguous; count as one\n            norm = 'south korea' if name in ['korea'] else name\n            found.add(norm)\n\n    n = len(found)\n    if n >= 6:\n        score = 1.0\n    elif n >= 4:\n        score = 0.7\n    elif n >= 2:\n        score = 0.4\n    else:\n        score = 0.0\n\n    return score, f\"Distinct Asian countries detected: {n} ({', '.join(sorted(found))})\""}, {"type": "code", "name": "Schedule Structure and Volume Check", "description": "Verify Monday/Wednesday/Friday cadence across 4 weeks and sufficient volume: ~8 features, 4 CTO interviews, and at least one VT.", "weight": 1.0, "code": "import re\n\ndef read_any_text(output, context):\n    try:\n        if output.is_document:\n            try:\n                return context.files.read_docx_text(output.id)\n            except Exception:\n                return context.files.read_pdf_text(output.id)\n        elif output.is_text_format:\n            return context.files.read_text(output.id)\n    except Exception:\n        return \"\"\n    return \"\"\n\ndef evaluate(workflow, context):\n    out = context.get_primary_output()\n    if not out:\n        return 0.0, \"No output.\"\n    text = read_any_text(out, context)\n    if not text:\n        return 0.0, \"No text to analyze.\"\n\n    t = text.lower()\n\n    # Check days\n    days_ok = all(d in t for d in [\"monday\",\"wednesday\",\"friday\"]) \n\n    # Check 4 weeks presence\n    week_markers = len(re.findall(r\"week\\s*[1-4]\", t))\n    generic_weeks = len(re.findall(r\"\\bfour-?week\\b|\\b4-?week\\b|\\bfor\\s+four\\s+weeks\\b\", t))\n    weeks_ok = week_markers >= 3 or generic_weeks >= 1\n\n    # Volume heuristics\n    features_count = len(re.findall(r\"\\bfeature(s)?\\b\", t))\n    cto_count = len(re.findall(r\"\\bcto\\b|chief technology officer\", t))\n    vt_count = len(re.findall(r\"\\bvt\\b|video package|video tape|video piece|tv package\", t))\n\n    # Accept either explicit \"two features per week\" phrasing or total counts\n    two_per_week_phrase = \"two\" in t and \"features per week\" in t\n\n    features_ok = two_per_week_phrase or features_count >= 8\n    cto_ok = cto_count >= 4\n    vt_ok = vt_count >= 1  # accept at least one VT season-level\n\n    # Scoring: days (0.3), weeks (0.2), features (0.25), cto (0.15), vt (0.1)\n    score = 0.0\n    details = []\n\n    if days_ok:\n        score += 0.3\n    else:\n        details.append(\"Missing Mon/Wed/Fri cadence\")\n\n    if weeks_ok:\n        score += 0.2\n    else:\n        details.append(\"4-week structure unclear\")\n\n    if features_ok:\n        score += 0.25\n    else:\n        details.append(f\"Insufficient features (found ~{features_count})\")\n\n    if cto_ok:\n        score += 0.15\n    else:\n        details.append(f\"Insufficient CTO interviews (found ~{cto_count})\")\n\n    if vt_ok:\n        score += 0.1\n    else:\n        details.append(\"No VT found\")\n\n    return score, (\"; \".join(details) if details else \"Schedule structure OK\")"}, {"type": "code", "name": "VT Re-versioning and Broadcast Alignment", "description": "Check that VT is flagged for radio and podcast re-versioning and acknowledges Friday broadcast on the rolling international news service.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    out = context.get_primary_output()\n    if not out:\n        return 0.0\n    try:\n        if out.is_document:\n            try:\n                text = context.files.read_docx_text(out.id)\n            except Exception:\n                text = context.files.read_pdf_text(out.id)\n        else:\n            text = context.files.read_text(out.id)\n    except Exception:\n        text = \"\"\n    if not text:\n        return 0.0\n\n    t = text.lower()\n    radio = \"radio\" in t\n    podcast = \"podcast\" in t\n    reversion = any(k in t for k in [\"re-version\", \"reversion\", \"reversioned\", \"repurpose\", \"adapted\"])\n    friday = \"friday\" in t\n    broadcast = any(k in t for k in [\"broadcast\", \"tv programme\", \"rolling international news service\", \"on air\"])\n\n    score = 0.0\n    if radio and podcast and reversion:\n        score += 0.65  # of the rule's normalized 1.0\n    elif (radio and podcast) or (reversion and (radio or podcast)):\n        score += 0.4\n    # Broadcast alignment contributes rest\n    if friday and broadcast:\n        score += 0.35\n    elif broadcast or friday:\n        score += 0.2\n\n    if score > 1.0:\n        score = 1.0\n\n    return score, \"Re-versioning/broadcast checks completed.\""}, {"type": "code", "name": "Contributors and Expert Sourcing Check", "description": "Confirm plan references multiple contributors/analysts/experts to match in-depth feature requirement.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    out = context.get_primary_output()\n    if not out:\n        return 0.0\n    text = \"\"\n    try:\n        if out.is_document:\n            try:\n                text = context.files.read_docx_text(out.id)\n            except Exception:\n                text = context.files.read_pdf_text(out.id)\n        else:\n            text = context.files.read_text(out.id)\n    except Exception:\n        text = \"\"\n\n    if not text:\n        return 0.0\n\n    t = text.lower()\n    # Look for sourcing language\n    terms = [\"analyst\", \"experts\", \"expert\", \"contributors\", \"contributor\", \"spokesperson\", \"industry analyst\", \"research firm\", \"think tank\", \"advisor\"]\n    count = sum(1 for term in terms if term in t)\n\n    # Also ensure interviews are mentioned for features\n    interviews = len(re.findall(r\"\\binterview(s)?\\b\", t))\n\n    if count >= 4 and interviews >= 3:\n        score = 1.0\n    elif count >= 2 and interviews >= 1:\n        score = 0.6\n    elif count >= 1:\n        score = 0.4\n    else:\n        score = 0.0\n\n    return score, f\"Contributor signal terms: {count}, interview mentions: {interviews}\""}, {"type": "llm_judge", "name": "Coherence and Feasibility Cross-Check", "description": "LLM verifies that budget lines, scheduling, and role allocations are consistent and feasible for a month-long, tri-weekly cadence with travel.", "weight": 0.5, "judge_prompt": "Assess internal consistency and feasibility (not prose quality):\n- Does the stated travel plan and budget plausibly fit 4 weeks with multi-country Asia coverage for a small crew (reporter + camera/producer) and align with the ~\u00a320\u201325k travel guidance?\n- Are two features per week and one CTO interview scheduled or evidently feasible alongside Friday TV broadcast obligations?\n- Are VT selections appropriate for video and explicitly tied to radio/podcast re-versioning?\n- Do proposed CTO interviewees logically match the enterprise technology innovation theme and the target geographies?\n\nScoring (0.0\u20131.0):\n- 1.0: Travel/budget, cadence, VT/re-versioning, and CTO selections are all coherent and feasible.\n- 0.7: Mostly coherent; one moderate inconsistency (e.g., tight travel but still plausible, or VT choice weakly justified).\n- 0.4: Multiple inconsistencies; plan would struggle without substantial revision.\n- 0.0: Infeasible or contradictory (e.g., unrealistic travel or scheduling, CTO choices irrelevant).", "expectation": "A realistic plan where time, travel, and production obligations can be met with the stated budget and staffing."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Editorial Quality and Strategic Fit", "description": "LLM assessment of professionalism, editorial strength, and audience suitability.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professionalism, Clarity, and Strategic Value", "description": "Judge overall editorial quality, clarity, and international audience fit.", "weight": 2.0, "judge_prompt": "Evaluate the proposal\u2019s editorial quality and strategic fit. Consider:\n- Clarity and brevity (should fit within 2\u20136 pages while remaining comprehensive)\n- Professional formatting and logical flow of sections\n- Strength and originality of story ideas for an international audience\n- Appropriateness of tone for a respected UK-based but global news outlet\n- Strategic value: clear aims, measurable KPIs, and a schedule that supports impactful coverage\n\nScoring (0.0\u20131.0):\n- 1.0: Highly professional, concise, well-structured, strong globally-relevant ideas and clear strategy.\n- 0.7: Generally professional with minor clarity or structure issues; ideas are solid.\n- 0.4: Adequate but generic/unclear in places; limited strategic depth.\n- 0.0: Poorly structured, unclear, or not appropriate to the outlet/audience.", "expectation": "A polished, strategically sound proposal tailored to an international audience with clear, compelling ideas and a coherent plan."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "bd72994f-5659-4084-9fab-fc547d1efe3b", "rubric": {"category_name": "Retail Client Outreach \u2014 Luxury Resort 2025 Looks + Booking Templates", "rationale": "Pattern C (Mixed): A visual document (PDF slides) plus templated outreach text. Stage 1 strictly enforces deliverable shape so later checks are trivial. Stage 2 mixes code and LLM for structural correctness and cross-reference consistency (season, look count, templates, CTA). Stage 3 evaluates professional quality for a luxury retail context.", "max_total_score": 16.0, "stages": [{"name": "Stage 1 \u2014 Format & Structure Gate (LLM-only)", "description": "Gate: Verify the output has the required self-documenting structure so verification is possible. Core requirement is a single PDF presentation containing 4\u20136 look slides from one brand\u2019s Resort/Cruise 2025 collection. Outreach templates preferably included as a final slide/section in the same PDF. Extra cover/appendix slides are allowed. This gate focuses on shape only, not content quality.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 2.5, "rules": [{"type": "llm_judge", "name": "PDF Presentation \u2014 4\u20136 Resort 2025 Look Slides (Structure Only)", "description": "Check the candidate\u2019s primary output for a valid PDF presentation with 4\u20136 distinct look slides from a single brand\u2019s Resort/Cruise 2025 collection. Each look slide should contain: a Look title/number, at least one image, and a brief list of items (3+ pieces: garment/accessory/shoes, etc.). The presentation should reference the brand and the season (e.g., \u201cResort 2025\u201d or \u201cCruise 2025\u201d). A cover slide is allowed but not required. Outreach templates may be included as an ending slide.", "weight": 3.0, "judge_prompt": "You are validating the SHAPE ONLY of the submission. Review the candidate output as rendered. Decide if it is a PDF presentation with the following STRUCTURE:\n\nRequired presentation shape (flexible on exact wording of headers):\n- Format: PDF file (not Word, not Excel, not plain text)\n- Contains 4 to 6 distinct look slides (these are slides showing individual outfits/looks)\n- Each look slide includes:\n  \u2022 A clear look title/number (e.g., \u201cLook 1\u201d, \u201cEvening Look\u201d, similar)\n  \u2022 At least one image (photo of outfit/products)\n  \u2022 A short item list of at least 3 pieces (e.g., dress/top, outerwear/bottom, bag/shoes/accessories)\n- The presentation references a single luxury brand and the 2025 resort season somewhere (use synonyms like \u201cResort 2025\u201d or \u201cCruise 2025\u201d). The brand/season can be on a cover slide or on look slides.\n- Extra slides (cover, appendix, outreach template slide) are allowed; the count of look slides must still be between 4 and 6.\n\nScoring (structure only; do not judge content quality or accuracy):\n- 3.0: PDF present AND 4\u20136 distinct look slides AND each look slide shows title/number, image, and 3+ item list; brand + \u201cResort 2025\u201d/\u201cCruise 2025\u201d mentioned.\n- 2.0: PDF present AND 4\u20136 look slides, but one structural element is inconsistent or missing on 1\u20132 slides (e.g., one slide lacks item list or brand/season mention appears only once).\n- 1.0: PDF present but fewer than 4 look slides OR look slides lack multiple required elements.\n- 0.0: Not a PDF OR no identifiable look slides.\n\nOnly evaluate PRESENCE/STRUCTURE of these elements, not correctness of brand selection or image authenticity.", "expectation": "A PDF deck with 4\u20136 look slides, each with title, image(s), and a short item list, and a visible brand + Resort/Cruise 2025 reference."}, {"type": "llm_judge", "name": "Outreach Templates Present in Deck (Email + SMS) \u2014 Structure Only", "description": "Check whether the PDF includes a clearly labeled outreach template section/slide with both Email and SMS templates containing personalization placeholders. If not present in the PDF, this rule scores lower; Stage 2 may still detect templates in a separate file.", "weight": 1.0, "judge_prompt": "Check if the same PDF presentation includes a clearly labeled outreach section/slide (e.g., \u201cOutreach Template\u201d, \u201cClient Outreach\u201d, \u201cBooking Message Templates\u201d) containing BOTH:\n- An Email template\n- A Text/SMS template\n\nEach template should include personalization placeholders such as {ClientName}, {StylistName}, {AppointmentLink}, {StoreName} (exact placeholder names can vary but must serve these roles).\n\nScoring:\n- 1.0: Both email and SMS templates are present in the PDF and include at least three placeholders (e.g., client name, stylist name, booking link, store location/date).\n- 0.5: Only one of the two (email or SMS) is present OR both are present but placeholders are sparse (fewer than three clear placeholders).\n- 0.0: No outreach templates found in the PDF.\n\nOnly check presence/structure, not writing quality.", "expectation": "A final slide/section labeled for outreach, with both Email and SMS templates and placeholders."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness & Consistency Checks (Code + LLM)", "description": "With the shape established, verify internal consistency and required functional elements: Resort/Cruise 2025 mention, 4\u20136 look indications, presence of outreach templates (in the deck or a companion file), and a clear CTA/URL or placeholder link.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Season + Look Count Heuristics (from PDF text)", "description": "Confirm the PDF text mentions Resort/Cruise 2025 and reflects 4\u20136 look headings. Heuristic based on text extraction only.", "weight": 2.0, "code": "import re\\nimport pandas as pd\\nimport numpy as np\\n\\ndef evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_document:\\n            return 0.0, \"Primary output missing or not a document.\"\\n\\n        text = \"\"\\n        # Read text from PDF/DOCX\\n        try:\\n            text = context.files.read_pdf_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_docx_text(output.id)\\n            except Exception:\\n                try:\\n                    text = context.files.read_text(output.id)\\n                except Exception:\\n                    text = \"\"\\n        if not text:\\n            return 0.0, \"No extractable text from primary document.\"\\n\\n        lt = text.lower()\\n\\n        # Check season mention\\n        season_ok = (\"resort 2025\" in lt) or (\"cruise 2025\" in lt)\\n\\n        # Count look headings (\"look 1\", \"look one\", etc.)\\n        spelled = {\\n            'one':1,'two':2,'three':3,'four':4,'five':5,'six':6,'1':1,'2':2,'3':3,'4':4,'5':5,'6':6\\n        }\\n        # Find numeric look headings\\n        nums = re.findall(r\"\\\\blook\\\\s*(\\n|:|#|no\\\\.?|)\\\\s*(\\d{1,2})\\\\b\", lt)\\n        look_nums = [int(n[1]) for n in nums if n and n[1].isdigit()]\\n        # Find spelled look headings\\n        words = re.findall(r\"\\\\blook\\\\s*(?:-|:|#|no\\\\.?)?\\\\s*(one|two|three|four|five|six)\\\\b\", lt)\\n        look_nums += [spelled[w] for w in words]\\n        # Fallback: general occurrences of 'look' lines\\n        if not look_nums:\\n            generic = re.findall(r\"^\\\\s*look\\\\b\", lt, flags=re.M)\n            look_count = len(generic)\n        else:\n            look_count = len(set(look_nums))\n\n        # Scoring: 1 point for valid look count, 1 point for season mention\n        score = 0.0\n        feedback_parts = []\n        if 4 <= look_count <= 6:\n            score += 1.0\n            feedback_parts.append(f\"Look headings detected: {look_count} (OK)\")\n        else:\n            # partial credit if at least some looks present\n            if look_count > 0:\n                score += min(look_count / 4.0, 1.0) * 0.6  # partial up to 0.6\n            feedback_parts.append(f\"Look headings detected: {look_count} (needs 4\u20136)\")\n\n        if season_ok:\n            score += 1.0\n            feedback_parts.append(\"Season mention found (Resort/Cruise 2025).\")\n        else:\n            feedback_parts.append(\"No clear 'Resort 2025' or 'Cruise 2025' mention in text.\")\n\n        # Cap score at weight=2\n        score = max(0.0, min(2.0, score))\n        return score, \"; \".join(feedback_parts)\n    except Exception as e:\n        return 0.0, f\"Error in rule: {e}\""}, {"type": "code", "name": "Outreach Templates Present (Any Output) + Placeholders", "description": "Search across all outputs for outreach templates (Email + SMS/Text) and placeholders. Accepts templates either inside the PDF or in a companion PDF/DOCX/MD file. Rewards both channels and presence of at least 3 placeholders (e.g., {ClientName}, {StylistName}, {AppointmentLink}, {StoreName}).", "weight": 2.0, "code": "import re\\nimport pandas as pd\\nimport numpy as np\\n\\nPLACEHOLDERS = [\\n    'clientname','stylistname','appointmentlink','storename','storelocation','preferreddate','preferredtime','collectionname'\\n]\\n\\nEMAIL_KEYS = [\"email\", \"subject\"]\\nSMS_KEYS = [\"sms\", \"text message\", \"text\", \"sms template\"]\\n\\nURL_RE = re.compile(r\"https?://[^\\s)>\\]}]+\", re.I)\\n\\n\ndef _read_text_for_resource(context, res):\\n    # Try appropriate readers per type\n    if getattr(res, 'is_document', False):\n        # try pdf then docx\n        try:\n            return context.files.read_pdf_text(res.id)\n        except Exception:\n            try:\n                return context.files.read_docx_text(res.id)\n            except Exception:\n                try:\n                    return context.files.read_text(res.id)\n                except Exception:\n                    return \"\"\n    elif getattr(res, 'is_text_format', False):\n        try:\n            return context.files.read_text(res.id)\n        except Exception:\n            return \"\"\n    else:\n        return \"\"\n\n\ndef evaluate(workflow, context):\n    try:\n        resources = context.get_all_outputs() or []\n        if not resources:\n            return 0.0, \"No outputs to scan.\"\n\n        texts = []\n        for r in resources:\n            try:\n                t = _read_text_for_resource(context, r) or \"\"\n            except Exception:\n                t = \"\"\n            if t:\n                texts.append((r, t))\n\n        if not texts:\n            return 0.0, \"Could not extract text from any output.\"\n\n        # Aggregate findings\n        email_found = False\n        sms_found = False\n        placeholders_found = set()\n        email_wordcount = 0\n        any_text = \"\\n\\n\".join(t for _, t in texts)\n        lt_all = any_text.lower()\n\n        # Heuristic: presence of sections\n        if any(k in lt_all for k in EMAIL_KEYS):\n            email_found = True\n        if any(k in lt_all for k in SMS_KEYS):\n            sms_found = True\n\n        # Placeholder detection (case-insensitive, allow braces or angle/curly variations)\n        for ph in PLACEHOLDERS:\n            # accept {ClientName} or {{ClientName}} or <ClientName>\n            if re.search(r\"[\\{<\\[]\\s*\" + re.escape(ph) + r\"\\s*[\\}>\\]]\", lt_all, flags=re.I):\n                placeholders_found.add(ph)\n\n        # Estimate email body length if possible\n        email_body_wc = 0\n        try:\n            m = re.search(r\"email.*?(?:template|:|\\n)\\s*(.+?)\\n\\s*(?:sms|text message|text\\b|$)\", any_text, flags=re.I|re.S)\n            if m:\n                email_body_wc = len(re.findall(r\"\\w+\", m.group(1)))\n            else:\n                # fallback to overall text if sectioning not found\n                email_body_wc = len(re.findall(r\"\\w+\", any_text)) // 3  # conservative estimate\n        except Exception:\n            email_body_wc = 0\n\n        # Scoring components (max 2.0)\n        score = 0.0\n        fb = []\n        # Channel presence (both better)\n        if email_found and sms_found:\n            score += 0.8\n            fb.append(\"Email and SMS sections detected.\")\n        elif email_found or sms_found:\n            score += 0.4\n            fb.append(\"Only one channel detected.\")\n        else:\n            fb.append(\"No clear Email/SMS sections detected.\")\n\n        # Placeholder breadth\n        ph_count = len(placeholders_found)\n        if ph_count >= 3:\n            score += 0.8\n            fb.append(f\"Placeholders found: {ph_count} (>=3).\")\n        elif ph_count == 2:\n            score += 0.5\n            fb.append(\"Two placeholders found.\")\n        elif ph_count == 1:\n            score += 0.25\n            fb.append(\"One placeholder found.\")\n        else:\n            fb.append(\"No placeholders found.\")\n\n        # Email body length adequacy\n        if email_found:\n            if email_body_wc >= 60:\n                score += 0.4\n                fb.append(f\"Email length OK (~{email_body_wc} words).\")\n            elif email_body_wc >= 30:\n                score += 0.2\n                fb.append(f\"Email length moderate (~{email_body_wc} words).\")\n            else:\n                fb.append(f\"Email appears short (~{email_body_wc} words).\")\n\n        score = max(0.0, min(2.0, score))\n        return score, \"; \".join(fb)\n    except Exception as e:\n        return 0.0, f\"Error in rule: {e}\""}, {"type": "code", "name": "CTA Link or Booking Placeholder Present", "description": "Ensure there is a clear call-to-action link: either a real URL (http/https) in the outreach template(s) or a booking placeholder like {AppointmentLink}.", "weight": 1.0, "code": "import re\\n\\nURL_RE = re.compile(r\"https?://[^\\s)>\\]}]+\", re.I)\\n\\nPLACEHOLDER_RE = re.compile(r\"\\{\\s*appointment\\s*link\\s*\\}|\\{\\s*appointmentlink\\s*\\}\", re.I)\n\n\ndef evaluate(workflow, context):\n    try:\n        texts = []\n        for r in context.get_all_outputs() or []:\n            try:\n                if getattr(r, 'is_document', False):\n                    try:\n                        texts.append(context.files.read_pdf_text(r.id))\n                    except Exception:\n                        try:\n                            texts.append(context.files.read_docx_text(r.id))\n                        except Exception:\n                            pass\n                if getattr(r, 'is_text_format', False):\n                    try:\n                        texts.append(context.files.read_text(r.id))\n                    except Exception:\n                        pass\n            except Exception:\n                continue\n        blob = \"\\n\\n\".join([t for t in texts if t])\n        lt = blob.lower()\n        if not lt:\n            return 0.0, \"No extractable text across outputs.\"\n\n        has_url = bool(URL_RE.search(blob))\n        has_ph = bool(PLACEHOLDER_RE.search(blob))\n\n        if has_url and has_ph:\n            return 1.0, \"Found live URL and appointment placeholder.\"\n        if has_url:\n            return 1.0, \"Found live URL.\"\n        if has_ph:\n            return 0.7, \"Found appointment placeholder only.\"\n        if (\"book\" in lt and \"appointment\" in lt) or (\"schedule\" in lt and \"appointment\" in lt):\n            return 0.3, \"CTA language present without link/placeholder.\"\n        return 0.0, \"No CTA link or placeholder detected.\"\n    except Exception as e:\n        return 0.0, f\"Error in rule: {e}\""}, {"type": "llm_judge", "name": "Same-Brand, Same-Collection Cohesion (Reasonableness)", "description": "LLM cross-check that all look slides appear to belong to one luxury brand and the same Resort/Cruise 2025 collection; looks should be thematically cohesive. Do not verify authenticity; just consistency and reasonableness based on visible cues and labeling.", "weight": 1.0, "judge_prompt": "Check the rendered presentation for consistency:\n- Do all look slides appear to reference a single brand and the same collection season (Resort/Cruise 2025)?\n- Do the looks feel cohesive (consistent styling, palette, or narrative) as one collection rather than a mix of unrelated brands/years?\n\nScoring:\n- 1.0: Clearly one brand and one Resort/Cruise 2025 collection; cohesive styling across looks.\n- 0.5: Mostly consistent, with minor indications of mixed sources or a slide that seems off-theme.\n- 0.0: Multiple brands/years mixed or no visible coherence.\n\nDo not judge writing/visual quality here\u2014just consistency across slides.", "expectation": "Visually and textually consistent set of 4\u20136 looks from one brand\u2019s Resort/Cruise 2025 collection."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality & Audience Fit (LLM)", "description": "Holistic quality assessment for luxury retail: presentation polish and outreach messaging effectiveness for client appointment booking.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Presentation Design & Retail Usability", "description": "Evaluate the deck\u2019s professionalism and luxury aesthetic: visual polish, layout clarity, readable typography, high-quality imagery, per-look item clarity, and overall client-facing readiness.", "weight": 3.0, "judge_prompt": "Evaluate the presentation\u2019s professional quality for a luxury retail client context. Consider:\n- Visual polish and cohesive luxury aesthetic (layout, spacing, typography)\n- Image quality and appropriateness for a luxury audience\n- Per-look clarity (each look is understandable, with item names/descriptions readable)\n- Practical usability in a client appointment outreach scenario (clear sequence, minimal clutter)\n\nScoring:\n- 3: Excellent polish and luxury aesthetic; images and captions are clear and compelling; immediately client-ready.\n- 2: Good overall with minor issues (e.g., some text sizing/cropping inconsistencies) but still client-usable.\n- 1: Fair; noticeable issues reduce impact or clarity, would need edits.\n- 0: Poor; confusing structure or low-quality visuals for a luxury context.\n\nDo not re-check structure; focus on quality and usability.", "expectation": "A polished, cohesive, client-ready luxury presentation with clear, readable look slides."}, {"type": "llm_judge", "name": "Outreach Messaging Quality (Email + SMS)", "description": "Evaluate tone, clarity, and effectiveness of the outreach templates for booking appointments with luxury clientele.", "weight": 3.0, "judge_prompt": "Assess the outreach templates (Email and SMS/Text) for effectiveness in booking appointments in a luxury retail setting. Consider:\n- Tone: luxury-appropriate, warm, and consultative (not pushy or discount-focused)\n- Personalization: use of placeholders (client name, stylist name, store/location), optional reference to selected collection/looks\n- Clarity of CTA: clear request to book with link or instructions; subject line present for email\n- Brevity and fit: email is concise but substantive; SMS is short and compliant for text norms\n\nScoring:\n- 3: Excellent tone and clarity; strong personalization; clear CTA with link/instructions; email subject present; SMS concise and on-brand.\n- 2: Good overall with minor issues (e.g., slightly long/short, mildly generic phrasing) but usable.\n- 1: Basic/weak; generic or unclear CTA; placeholders minimal or awkward.\n- 0: Ineffective; inappropriate tone, missing CTA, or unusable for luxury clientele.\n\nIgnore whether templates are in-PDF vs separate file; judge whatever is provided.", "expectation": "Refined, on-brand templates with clear CTA, smart personalization, and concise SMS."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "cecac8f9-8203-4ebd-ad49-54436a8c4171", "rubric": {"category_name": "Retail: Black Friday 2024 Store Readiness (UK) \u2014 8-Week Plan + Team Launch Deck", "rationale": "Task Type = Pattern B (Document). Outputs are two PDFs: (1) an 8-week preparation plan with an upfront Strategic Objectives section; (2) a Team Launch deck for Black Friday weekend operations. The rubric enforces a self-documenting structure in Stage 1 via LLM judges (gate), then verifies correctness and cross-document alignment with code and an LLM check in Stage 2, and finally assesses professional quality in Stage 3. Stage 1 mandates exact structural elements (sections/slides) so Stage 2 can programmatically verify presence of weeks, targets, promotions, and operational coverage. The UK context (currency, terminology) is considered.", "max_total_score": 33.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (Structure Only, LLM)", "description": "Checks that BOTH deliverables are present as PDFs with the exact, verifiable structure needed for automated verification. This is a strict gate \u2014 if structure is missing, the category is zeroed.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Preparation Plan PDF \u2014 Required Structure", "description": "Verify the 8-week preparation plan is a PDF with the mandated sections and structure enabling verification.", "weight": 4.5, "judge_prompt": "You are verifying the SHAPE (structure/format) only. Do not judge content quality or correctness.\n\nTarget deliverable: An 8-week Black Friday preparation plan as a PDF, intended for UK retail store leadership.\n\nCheck the following STRICT structure requirements (be flexible with synonyms, but enforce presence):\n\nFormat requirements:\n- Must be a PDF (not Word/Excel/plain text)\n- At least 3 pages\n- Clear headings and professional page layout\n\nRequired sections and elements:\n1) Upfront section: \"Strategic Objectives\" or \"Success Objectives\" or similar\n   - A short paragraph and/or bullet list explaining what success looks like\n   - A visible summary table of performance goals (flexible naming), e.g., a table with columns similar to: [Metric | 2023 (or Baseline) | 2024 Target | Notes/Owner]. Column names can vary, but intent must match (baseline vs 2024 target side-by-side).\n\n2) \"8-Week Plan\" or \"Roadmap\" section covering eight weeks leading up to Black Friday\n   - Subsections or clear labels for each week (e.g., Week 1 \u2026 Week 8, or Week -8 \u2026 Week -1)\n   - Each week must list high-level operational action bullets (aim for 3\u20137 bullets per week)\n   - Weeks should appear in chronological order\n\n3) Optional but encouraged sections (do not penalize if missing):\n   - Risks & contingencies\n   - Staffing & training\n   - Inventory & merchandising readiness\n   - Customer communications / local marketing\n\nScoring (STRUCTURE ONLY):\n- 1.0: PDF, 3+ pages, has Strategic Objectives with a goals summary table, and a clearly labeled 8-week plan with 8 distinct weeks each having bullets\n- 0.7: PDF, 3+ pages, has Strategic Objectives and 8-week plan, but missing the goals summary table OR some weeks have unclear bulleting/labels\n- 0.4: PDF format but missing either the Strategic Objectives section OR an 8-week sequence\n- 0.0: Not a PDF OR less than 3 pages OR missing multiple core elements", "expectation": "A multi-page PDF with a clear Strategic Objectives section including a goals summary table and a chronologically labeled 8-week action plan, each week with bullets."}, {"type": "llm_judge", "name": "Team Launch Deck PDF \u2014 Required Slides", "description": "Verify the Team Launch deck is a PDF deck with the mandated slide types enabling operational use across the Black Friday weekend.", "weight": 3.5, "judge_prompt": "You are verifying the SHAPE (structure/format) only. Do not judge content quality or correctness.\n\nTarget deliverable: A Team Launch deck as a PDF to brief staff on Black Friday morning and throughout the weekend.\n\nCheck the following STRICT structure requirements (be flexible with synonyms, but enforce presence):\n\nFormat requirements:\n- Must be a PDF (deck-style, multiple pages/slides)\n- At least 10 slides/pages\n- Clear slide headings and readable layout; visuals/icons acceptable\n\nRequired slide types (headers can vary, but intent must be present):\n1) Title/Welcome slide (e.g., \"Black Friday 2024 Team Launch\" or similar)\n2) Performance Goals/Targets slide(s) (should mirror the plan\u2019s goals at a high level)\n3) Promotional Offers overview slide(s) (what\u2019s on promo and how to communicate it)\n4) Execution Priorities / Today\u2019s Focus slide(s)\n5) Customer Experience standards (greeting, queue management, recovery)\n6) Safety & Loss Prevention (H&S reminders, LP/security)\n7) Staffing / Breaks / Handover or Shift Briefing guidance\n8) Live KPIs / Tracking & cadence (how the team will monitor through the day)\n\nOptional slides (do not penalize if missing): FAQs, Store map/zone assignments, Visual merchandising guidance.\n\nScoring (STRUCTURE ONLY):\n- 1.0: PDF deck with 10+ slides and at least items 1\u20138 clearly present\n- 0.7: PDF deck with 10+ slides; 6\u20137 of the 8 required slide types present\n- 0.4: PDF deck but fewer than 10 slides OR only 3\u20135 required slide types present\n- 0.0: Not a PDF deck or fewer than 5 slides", "expectation": "A 10+ slide PDF deck including title, performance goals, promotions, execution priorities, CX, safety/LP, staffing/handovers, and live KPI tracking."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness, Cross-Checks)", "description": "Now that the structure is enforced, verify correctness and cross-document alignment using code rules and one LLM cross-check. These rules focus on plausibility, internal consistency, and operational completeness for a UK retail context.", "is_required": false, "max_points": 15.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Two Deliverables Present (Docs)", "description": "Confirm there are at least two document outputs (expected: plan PDF and deck PDF).", "weight": 1.0, "code": "import re\\n\\ndef evaluate(workflow, context):\\n    try:\\n        outputs = context.get_all_outputs() or []\\n        docs = [r for r in outputs if getattr(r, 'is_document', False)]\\n        n = len(docs)\\n        if n >= 2:\\n            return 1.0\\n        elif n == 1:\\n            return 0.5\\n        else:\\n            return 0.0\\n    except Exception:\\n        return 0.0\\n"}, {"type": "code", "name": "8 Weeks Sequenced + Bulleted Actions (Plan)", "description": "Detect an 8-week sequence and sufficient bullets for weekly actions in the plan.", "weight": 3.0, "code": "import re\\n\\nWEEK_WORDS = {\\n    'one':1,'two':2,'three':3,'four':4,'five':5,'six':6,'seven':7,'eight':8,\\n    '1':1,'2':2,'3':3,'4':4,'5':5,'6':6,'7':7,'8':8,\\n    '-1':1,'-2':2,'-3':3,'-4':4,'-5':5,'-6':6,'-7':7,'-8':8\\n}\\n\\ndef read_doc_text(context, res):\\n    name = getattr(res, 'name', '') or ''\\n    try:\\n        if str(name).lower().endswith('.pdf'):\\n            return context.files.read_pdf_text(res.id) or ''\\n        if str(name).lower().endswith('.docx'):\\n            return context.files.read_docx_text(res.id) or ''\\n    except Exception:\\n        pass\\n    try:\\n        return context.files.read_text(res.id) or ''\\n    except Exception:\\n        return ''\\n\\ndef pick_plan_doc_text(context, docs_texts):\\n    # Heuristic: the plan tends to have many mentions of weeks and strategic objectives\\n    best_res, best_score = None, -1\\n    for res, txt in docs_texts:\\n        t = txt.lower()\\n        score = t.count('week') + t.count('wk') + t.count('strategic objective') + t.count('roadmap')\\n        if score > best_score:\\n            best_res, best_score = (res, txt), score\\n    return best_res[1] if best_res else ''\\n\\ndef bullet_count(text):\\n    cnt = 0\\n    for line in text.splitlines():\\n        l = line.strip()\\n        if re.match(r'^[\\\\u2022\\\\-\\\\*\\\\u2013\\\\u2014]+\\\\s+.+', l):\\n            cnt += 1\\n    return cnt\\n\\ndef week_labels(text):\\n    t = text.lower()\\n    labels = set()\\n    # match patterns like Week 1, Week-1, Wk 1, Wk-8, etc.\\n    for m in re.finditer(r'\\\\b(wk|week)\\\\s*[-\\\\u2013\\\\u2014]?\\\\s*([a-z0-9]+)\\\\b', t):\\n        token = m.group(2)\\n        if token in WEEK_WORDS:\\n            v = WEEK_WORDS[token]\\n            if 1 <= v <= 8:\\n                labels.add(v)\\n    return labels\\n\\ndef evaluate(workflow, context):\\n    try:\\n        outputs = context.get_all_outputs() or []\\n        docs = [r for r in outputs if getattr(r, 'is_document', False)]\\n        if not docs:\\n            return 0.0\\n        docs_texts = []\\n        for r in docs:\\n            txt = read_doc_text(context, r)\\n            if txt:\\n                docs_texts.append((r, txt))\\n        if not docs_texts:\\n            return 0.0\\n        plan_text = pick_plan_doc_text(context, docs_texts)\\n        if not plan_text:\\n            return 0.0\\n        # Weeks score\\n        weeks = week_labels(plan_text)\\n        week_score = min(1.0, len(weeks) / 8.0)\\n        # Bullets score (lenient)\\n        bcount = bullet_count(plan_text)\\n        bullet_score = 1.0 if bcount >= 24 else (0.75 if bcount >= 16 else (0.5 if bcount >= 8 else 0.0))\\n        return (week_score * 0.6) + (bullet_score * 0.4)\\n    except Exception:\\n        return 0.0\\n"}, {"type": "code", "name": "Strategic Objectives Include Targets (Plan)", "description": "Verify the plan\u2019s Strategic Objectives section includes measurable targets (percentages, currency, or explicit numeric goals).", "weight": 2.0, "code": "import re\\n\\nNUM_PATTERNS = [\\n    r'\\\\b\\\\d{1,3}\\\\s?%\\\\b',\\n    r'\u00a3\\\\s?\\\\d{1,3}(?:,\\\\d{3})*(?:\\\\.\\\\d+)?',\\n    r'\\\\b\\\\d{1,4}\\\\s?(k|K)\\\\b',\\n    r'\\\\btarget\\\\s*[:=]\\\\s*\\\\d+',\\n]\\n\\ndef read_doc_text(context, res):\\n    name = getattr(res, 'name', '') or ''\\n    try:\\n        if str(name).lower().endswith('.pdf'):\\n            return context.files.read_pdf_text(res.id) or ''\\n        if str(name).lower().endswith('.docx'):\\n            return context.files.read_docx_text(res.id) or ''\\n    except Exception:\\n        pass\\n    try:\\n        return context.files.read_text(res.id) or ''\\n    except Exception:\\n        return ''\\n\\ndef pick_plan_doc_text(context, docs_texts):\\n    best, best_score = None, -1\\n    for res, txt in docs_texts:\\n        t = txt.lower()\\n        score = t.count('strategic objectives') + t.count('success') + t.count('goals') + t.count('targets')\\n        if score > best_score:\\n            best, best_score = (res, txt), score\\n    return best[1] if best else ''\\n\\ndef count_numeric_targets(text):\\n    cnt = 0\\n    for p in NUM_PATTERNS:\\n        cnt += len(re.findall(p, text))\\n    return cnt\\n\\ndef has_objectives_header(text):\\n    t = text.lower()\\n    return any(k in t for k in ['strategic objectives','success objectives','strategic goals','what success looks like'])\\n\\ndef evaluate(workflow, context):\\n    try:\\n        outputs = context.get_all_outputs() or []\\n        docs = [r for r in outputs if getattr(r, 'is_document', False)]\\n        if not docs:\\n            return 0.0\\n        docs_texts = [(r, read_doc_text(context, r)) for r in docs]\\n        docs_texts = [(r, t) for r, t in docs_texts if t]\\n        if not docs_texts:\\n            return 0.0\\n        plan_text = pick_plan_doc_text(context, docs_texts)\\n        if not plan_text:\\n            return 0.0\\n        header_ok = has_objectives_header(plan_text)\\n        num_targets = count_numeric_targets(plan_text)\\n        score = 0.0\\n        if header_ok:\\n            score += 0.5\\n        if num_targets >= 3:\\n            score += 0.5\\n        elif num_targets >= 1:\\n            score += 0.25\\n        return score\\n    except Exception:\\n        return 0.0\\n"}, {"type": "code", "name": "KPI/Target Consistency Between Plan and Deck", "description": "Check that key KPI terms used in the plan also appear in the deck, indicating alignment of goals communicated to the team.", "weight": 2.5, "code": "import re\\n\\nKPI_TERMS = [\\n    'conversion', 'aov', 'average order value', 'atv', 'basket', 'upt', 'units per transaction',\\n    'nps', 'csat', 'revenue', 'sales', 'traffic', 'footfall', 'attachment rate', 'attachment',\\n    'shrink', 'loss', 'returns', 'refunds'\\n]\\n\\ndef read_doc_text(context, res):\\n    name = getattr(res, 'name', '') or ''\\n    try:\\n        if str(name).lower().endswith('.pdf'):\\n            return context.files.read_pdf_text(res.id) or ''\\n        if str(name).lower().endswith('.docx'):\\n            return context.files.read_docx_text(res.id) or ''\\n    except Exception:\\n        pass\\n    try:\\n        return context.files.read_text(res.id) or ''\\n    except Exception:\\n        return ''\\n\\ndef identify_plan_and_deck(docs_texts):\\n    # Heuristics: plan is week-heavy; deck is launch/briefing-heavy\\n    plan_idx, deck_idx = -1, -1\\n    best_plan_score, best_deck_score = -1, -1\\n    for i, (res, txt) in enumerate(docs_texts):\\n        t = txt.lower()\\n        pscore = t.count('week') + t.count('strategic') + t.count('roadmap')\\n        dscore = t.count('launch') + t.count('deck') + t.count('briefing') + t.count('huddle')\\n        if pscore > best_plan_score:\\n            best_plan_score, plan_idx = pscore, i\\n        if dscore > best_deck_score:\\n            best_deck_score, deck_idx = dscore, i\\n    if plan_idx == deck_idx and len(docs_texts) >= 2:\\n        # pick second best as deck\\n        deck_idx = 1 if plan_idx == 0 else 0\\n    plan_text = docs_texts[plan_idx][1] if plan_idx >= 0 else ''\\n    deck_text = docs_texts[deck_idx][1] if deck_idx >= 0 else ''\\n    return plan_text, deck_text\\n\\ndef evaluate(workflow, context):\\n    try:\\n        outputs = context.get_all_outputs() or []\\n        docs = [r for r in outputs if getattr(r, 'is_document', False)]\\n        if len(docs) < 2:\\n            return 0.0\\n        docs_texts = [(r, read_doc_text(context, r)) for r in docs]\\n        docs_texts = [(r, t) for r, t in docs_texts if t]\\n        if len(docs_texts) < 2:\\n            return 0.0\\n        plan_text, deck_text = identify_plan_and_deck(docs_texts)\\n        pt, dt = plan_text.lower(), deck_text.lower()\\n        plan_terms = [k for k in KPI_TERMS if k in pt]\\n        if not plan_terms:\\n            return 0.0\\n        # Focus on up to 4 unique KPIs used in plan\\n        plan_terms = list(dict.fromkeys(plan_terms))[:4]\\n        matched = sum(1 for k in plan_terms if k in dt)\n        # Require at least 3 matches for full credit, scale otherwise\n        return min(1.0, matched / max(1, min(3, len(plan_terms))))\n    except Exception:\n        return 0.0\n"}, {"type": "code", "name": "Deck Covers Promotions and Execution Priorities", "description": "Detect promotional content and execution priorities in the deck.", "weight": 1.5, "code": "import re\\n\\nPROMO_TERMS = ['offer','offers','promotion','promotions','discount','deal','bundle','bundles']\\nPRIORITY_TERMS = ['priority','priorities','focus','execute','execution priorities','today\\'s focus']\\n\\ndef read_doc_text(context, res):\\n    name = getattr(res, 'name', '') or ''\\n    try:\\n        if str(name).lower().endswith('.pdf'):\\n            return context.files.read_pdf_text(res.id) or ''\\n        if str(name).lower().endswith('.docx'):\\n            return context.files.read_docx_text(res.id) or ''\\n    except Exception:\\n        pass\\n    try:\\n        return context.files.read_text(res.id) or ''\\n    except Exception:\\n        return ''\\n\\ndef pick_deck_text(docs_texts):\\n    best, best_score = None, -1\\n    for res, txt in docs_texts:\\n        t = txt.lower()\\n        score = t.count('launch') + t.count('briefing') + t.count('huddle') + t.count('today\\'s focus') + t.count('promotions')\\n        if score > best_score:\\n            best, best_score = (res, txt), score\\n    return best[1] if best else ''\\n\\ndef evaluate(workflow, context):\\n    try:\\n        outputs = context.get_all_outputs() or []\\n        docs = [r for r in outputs if getattr(r, 'is_document', False)]\\n        docs_texts = [(r, read_doc_text(context, r)) for r in docs]\\n        docs_texts = [(r, t) for r, t in docs_texts if t]\\n        if not docs_texts:\\n            return 0.0\\n        deck_text = pick_deck_text(docs_texts).lower()\\n        if not deck_text:\\n            return 0.0\\n        promo = any(k in deck_text for k in PROMO_TERMS)\\n        prio = any(k in deck_text for k in PRIORITY_TERMS)\\n        if promo and prio:\\n            return 1.0\\n        if promo or prio:\\n            return 0.5\\n        return 0.0\\n    except Exception:\\n        return 0.0\\n"}, {"type": "code", "name": "Weekend Coverage and Shift Mechanics (Deck)", "description": "Deck should address Black Friday weekend coverage (Fri-Sun) and shift/handovers.", "weight": 1.0, "code": "import re\\n\\nWEEKEND_TERMS = ['black friday','saturday','sunday','weekend']\\nSHIFT_TERMS = ['shift','rota','roster','break','handover','relief','briefing','opening huddle','close of day']\\n\\ndef read_doc_text(context, res):\\n    name = getattr(res, 'name', '') or ''\\n    try:\\n        if str(name).lower().endswith('.pdf'):\\n            return context.files.read_pdf_text(res.id) or ''\\n        if str(name).lower().endswith('.docx'):\\n            return context.files.read_docx_text(res.id) or ''\\n    except Exception:\\n        pass\\n    try:\\n        return context.files.read_text(res.id) or ''\\n    except Exception:\\n        return ''\\n\\ndef pick_deck_text(docs_texts):\\n    best, best_score = None, -1\\n    for res, txt in docs_texts:\\n        t = txt.lower()\\n        score = t.count('launch') + t.count('briefing') + t.count('huddle') + t.count('weekend')\\n        if score > best_score:\\n            best, best_score = (res, txt), score\\n    return best[1] if best else ''\\n\\ndef evaluate(workflow, context):\\n    try:\\n        outputs = context.get_all_outputs() or []\\n        docs = [r for r in outputs if getattr(r, 'is_document', False)]\\n        docs_texts = [(r, read_doc_text(context, r)) for r in docs]\\n        docs_texts = [(r, t) for r, t in docs_texts if t]\\n        if not docs_texts:\\n            return 0.0\\n        deck_text = pick_deck_text(docs_texts).lower()\\n        has_weekend = any(k in deck_text for k in WEEKEND_TERMS)\\n        has_shift = any(k in deck_text for k in SHIFT_TERMS)\\n        if has_weekend and has_shift:\\n            return 1.0\\n        if has_weekend or has_shift:\\n            return 0.5\\n        return 0.0\\n    except Exception:\\n        return 0.0\\n"}, {"type": "code", "name": "Safety and Loss Prevention Coverage (Deck)", "description": "Deck mentions safety and LP/security expectations.", "weight": 1.0, "code": "def read_doc_text(context, res):\\n    name = getattr(res, 'name', '') or ''\\n    try:\\n        if str(name).lower().endswith('.pdf'):\\n            return context.files.read_pdf_text(res.id) or ''\\n        if str(name).lower().endswith('.docx'):\\n            return context.files.read_docx_text(res.id) or ''\\n    except Exception:\\n        pass\\n    try:\\n        return context.files.read_text(res.id) or ''\\n    except Exception:\\n        return ''\\n\\nSAFETY_TERMS = ['safety','health and safety','first aid','incident']\\nLP_TERMS = ['loss prevention','lp','security','till variance','bag check','shrink']\\n\\ndef evaluate(workflow, context):\\n    try:\\n        outputs = context.get_all_outputs() or []\\n        docs = [r for r in outputs if getattr(r, 'is_document', False)]\\n        texts = [read_doc_text(context, r).lower() for r in docs]\\n        deck_text = ''\\n        # pick the doc more likely to be the deck\\n        best, score = '', -1\\n        for t in texts:\\n            s = t.count('launch') + t.count('briefing') + t.count('huddle')\\n            if s > score:\\n                best, score = t, s\\n        deck_text = best\\n        if not deck_text:\\n            return 0.0\\n        has_safety = any(k in deck_text for k in SAFETY_TERMS)\\n        has_lp = any(k in deck_text for k in LP_TERMS)\\n        if has_safety and has_lp:\\n            return 1.0\\n        if has_safety or has_lp:\\n            return 0.5\\n        return 0.0\\n    except Exception:\\n        return 0.0\\n"}, {"type": "code", "name": "UK Retail Context Present", "description": "Confirm UK context appears (currency/terminology).", "weight": 0.5, "code": "def read_doc_text(context, res):\\n    name = getattr(res, 'name', '') or ''\\n    try:\\n        if str(name).lower().endswith('.pdf'):\\n            return context.files.read_pdf_text(res.id) or ''\\n        if str(name).lower().endswith('.docx'):\\n            return context.files.read_docx_text(res.id) or ''\\n    except Exception:\\n        pass\\n    try:\\n        return context.files.read_text(res.id) or ''\\n    except Exception:\\n        return ''\\n\\nUK_TERMS = ['\u00a3','gbp','vat','till','tills','queue','queuing','high street','boxing day']\\n\\ndef evaluate(workflow, context):\\n    try:\\n        outputs = context.get_all_outputs() or []\\n        docs = [r for r in outputs if getattr(r, 'is_document', False)]\\n        txt = ('\\n'.join(read_doc_text(context, r) for r in docs)).lower()\\n        hit = any(k in txt for k in UK_TERMS)\\n        return 1.0 if hit else 0.0\\n    except Exception:\\n        return 0.0\\n"}, {"type": "code", "name": "Live KPI Tracking / Cadence (Deck)", "description": "Deck includes how performance will be monitored during the day (e.g., KPIs, tracker, cadence).", "weight": 0.5, "code": "def read_doc_text(context, res):\\n    name = getattr(res, 'name', '') or ''\\n    try:\\n        if str(name).lower().endswith('.pdf'):\\n            return context.files.read_pdf_text(res.id) or ''\\n        if str(name).lower().endswith('.docx'):\\n            return context.files.read_docx_text(res.id) or ''\\n    except Exception:\\n        pass\\n    try:\\n        return context.files.read_text(res.id) or ''\\n    except Exception:\\n        return ''\\n\\nKPI_TERMS = ['kpi','dashboard','tracker','tracking','scoreboard','cadence','hourly','half-hourly','reporting','stand-up']\\n\\ndef evaluate(workflow, context):\\n    try:\\n        outputs = context.get_all_outputs() or []\\n        docs = [r for r in outputs if getattr(r, 'is_document', False)]\\n        texts = [read_doc_text(context, r).lower() for r in docs]\\n        # assume deck likely mentions launch/briefing terms\\n        deck_text = max(texts, key=lambda t: t.count('launch') + t.count('briefing') + t.count('huddle')) if texts else ''\\n        if not deck_text:\\n            return 0.0\\n        hits = sum(1 for k in KPI_TERMS if k in deck_text)\\n        return 1.0 if hits >= 2 else (0.5 if hits == 1 else 0.0)\\n    except Exception:\\n        return 0.0\\n"}, {"type": "llm_judge", "name": "Cross-Document Alignment (Goals, Offers, Priorities)", "description": "LLM check that the deck\u2019s goals and weekend priorities align with the plan\u2019s Strategic Objectives and that promotions are consistently represented.", "weight": 2.0, "judge_prompt": "Review BOTH documents (the plan PDF and the deck PDF) together. Assess alignment and internal consistency. Do not judge graphic design here \u2014 focus on content alignment.\n\nChecks:\n1) Goals alignment: Deck reiterates the same top-line performance goals from the plan\u2019s Strategic Objectives (allow concise/abbreviated phrasing, but intent should match). If numeric targets are present in the plan, the deck should reflect the same directionality and priority (exact numbers may be summarized).\n2) Offers consistency: Promotions presented in the deck are consistent within the deck and not in conflict with any goals messaging from the plan (e.g., upsell/attachment priorities match what the plan emphasizes). If a separate marketing email reference is not available, ignore external validation \u2014 just check internal coherence.\n3) Operational focus: Deck execution priorities are logically derived from the plan\u2019s roadmap focus areas (e.g., staffing, queue management, merchandising readiness) and cover the Black Friday weekend.\n\nScoring:\n- 1.0: Clear alignment on goals and priorities; offers are presented consistently; no contradictions\n- 0.7: Mostly aligned; minor omissions or slight mismatch (e.g., one KPI not echoed) without contradictions\n- 0.4: Partial alignment; several goals/priorities missing in deck or unclear connections\n- 0.0: Misaligned or contradictory content between plan and deck", "expectation": "Deck cleanly mirrors plan goals and priorities with no contradictions; promotions are coherent and not at odds with the plan."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality Assessment (LLM)", "description": "Holistic assessment of professional quality, actionability, and leadership value for UK store teams across the Black Friday weekend.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Polish and Visual Communication", "description": "Assess whether both PDFs are professionally presented and easy to use on the day (legibility, hierarchy, readable slides, sensible layout).", "weight": 4.0, "judge_prompt": "Evaluate professional presentation quality across both PDFs. Consider:\n- Clear hierarchy of headings, consistent typography, readable contrast\n- Logical flow; scannable bullets; minimal clutter\n- For deck: slide readability for quick briefings; use of visuals/icons that aid comprehension\n- No obvious formatting errors (orphans, broken tables, unreadable charts)\nScoring: 1.0 exceptional polish; 0.7 good with minor flaws; 0.4 usable but rough; 0.0 poor/unusable.", "expectation": "Professional, readable, and consistent formatting that supports quick on-floor comprehension."}, {"type": "llm_judge", "name": "Clarity and Actionability for Store Teams", "description": "Assess whether weekly actions and deck guidance are clear, sequenced, and actionable for frontline teams and leaders.", "weight": 3.0, "judge_prompt": "Assess clarity and actionability:\n- Weekly plan bullets are specific and sequenced; each week advances readiness\n- Deck gives concrete guidance (what to do at open, during peak, handovers, close)\n- Instructions are unambiguous and time-bound where appropriate (e.g., hourly checks)\nScoring: 1.0 highly actionable and clear; 0.7 mostly actionable; 0.4 somewhat vague; 0.0 unclear or confusing.", "expectation": "Clear, specific, sequenced instructions that frontline staff can follow without ambiguity."}, {"type": "llm_judge", "name": "Strategic Insight and Leadership Communication", "description": "Evaluate whether the documents reinforce leadership focus, inspire performance, and reflect UK retail context.", "weight": 3.0, "judge_prompt": "Evaluate leadership and strategic value:\n- Strategic Objectives focus the team on the right KPIs and customer experience\n- Messaging energizes the team and reinforces standards (service, safety, LP)\n- UK context is respected (terminology, currency); tone is appropriate\nScoring: 1.0 strong leadership emphasis and strategic clarity; 0.7 good; 0.4 limited; 0.0 weak.", "expectation": "Strong leadership framing tied to KPIs, customer experience, and UK retail realities."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "b7a5912e-0e63-41f5-8c22-9cdb8f46ab01", "rubric": {"category_name": "Daily Closed Operational Report \u2014 Counter and Rental Clerks (Real Estate and Rental and Leasing)", "rationale": "Three-stage, self-documenting rubric. Stage 1 is a strict LLM-only shape gate that mandates a verifiable Excel workbook structure with named sheets and tables, enabling trivial verification. Stage 2 mixes code rules (deterministic numeric/consistency checks) and an LLM rule (grounded observations) that leverage the enforced structure. Stage 3 assesses professionalism and strategic value for management audiences.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement (GATE)", "description": "LLM-only gate to verify the workbook exists and follows the exact, auditable structure so downstream verification is trivial.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.25, "rules": [{"type": "llm_judge", "name": "Structured Workbook Requirements (Excel only)", "description": "Checks that the output is an Excel file with the required sheets, sections, and table structures for verifiable analysis.", "weight": 3.0, "judge_prompt": "You are evaluating the primary output. Only assess STRUCTURE and PRESENCE, not correctness of numbers.\n\nRequirement: Output must be a single Excel workbook (XLSX). Be flexible with sheet names but ensure the required content exists and is clearly labeled.\n\nWorkbook Required Structure:\n\n1) Sheet: \"Daily Activity & Key Trends\" (or similar: \"Daily Summary\", \"Daily Activity and Key Trends\"). Must visibly include:\n   - A key metrics section showing all of the following (as labels with values):\n     \u2022 Total number of closed rentals\n     \u2022 Total number of rental days\n     \u2022 Average Length of Rental (LOR)\n     \u2022 Total revenue\n     \u2022 Average revenue per rental\n     \u2022 Average daily rate\n   - A Category Utilization subsection (on this sheet is preferred) that shows % of rentals per vehicle category. This can be a small table with Category and % of Rentals (or equivalent) or a clearly labeled chart plus a table.\n\n2) Sheet: \"Category Breakdown\" (or similar: \"By Category\", \"Category Details\"). Must contain a single table with columns (flexible naming, but all must be present):\n   - Vehicle Category\n   - Total number of rentals\n   - Total rental days\n   - Total revenue\n   - Average revenue per rental\n   - Average length of rental\n   - Average revenue per day (or Average daily rate)\n\n3) Sheet: \"Booking Source Summary\". Must contain a table summarizing by source (e.g., Website, Expedia, Call Center). Table must include:\n   - Booking Source (name)\n   - Total revenue (by source)\n   - Rentals count by source is preferred but optional for Stage 1\n\n4) Sheet: \"Payment Method Summary\". Must contain a table summarizing total revenue collected by payment method (e.g., Credit Card, Debit Card). Table must include:\n   - Payment Method (name)\n   - Total revenue collected (by method)\n\n5) Sheet: \"Observations\" (or similar: \"Notes\", \"Insights\"). Must contain a short section of written observations tailored to management/sales, referring to rental trends, payment methods, and booking sources. Require at least 5 bullet points or 120+ words, whichever is easier to verify.\n\nScoring (STRUCTURE ONLY):\n- 3.0: Excel workbook present AND all 5 sheets exist with clearly labeled sections/tables containing the required fields. Category utilization is present (on Daily Summary or as its own clearly labeled table) and uses percentages by category.\n- 2.5: Excel workbook present, sheets exist, and nearly all required fields are present; only minor omissions (e.g., booking source rentals count missing; utilization shown but on separate sheet; or one label slightly ambiguous but clearly the right metric).\n- 2.0: Excel workbook present but missing exactly one required sheet or one required metrics subsection on the Daily Summary.\n- 1.0: Excel workbook present but only 1\u20132 of the required sheets/sections exist, or the Daily Summary is missing most key metrics.\n- 0.0: Not an Excel workbook OR workbook lacks core structure (e.g., no Daily Summary and no Category Breakdown).\n\nDo not judge numerical correctness or formatting polish here\u2014only presence and structural completeness.", "expectation": "A well-structured Excel file with 5 sheets: Daily Summary with key metrics and category utilization; Category Breakdown table with required columns; Booking Source Summary; Payment Method Summary; and an Observations sheet with substantive notes."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness and Consistency)", "description": "Deterministic code checks on numeric consistency across sheets and an LLM grounding check for observations.", "is_required": false, "max_points": 6.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Daily Metrics Internal Consistency", "description": "Verify Daily Summary metrics are numerically coherent: LOR \u2248 total rental days / total rentals; Avg revenue per rental \u2248 total revenue / total rentals; Avg daily rate \u2248 total revenue / total rental days.", "weight": 2.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output found.\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n    except Exception as e:\n        return 0.0, f\"Failed to open Excel: {e}\"\n\n    def to_num(x):\n        if pd.isna(x):\n            return None\n        if isinstance(x, (int, float, np.number)):\n            return float(x)\n        s = str(x).strip()\n        if s == \"\":\n            return None\n        neg = False\n        if s.startswith(\"(\") and s.endswith(\")\"):\n            neg = True\n            s = s[1:-1]\n        s = s.replace(\",\", \"\").replace(\"$\", \"\").replace(\"%\", \"\")\n        try:\n            v = float(s)\n            return -v if neg else v\n        except:\n            return None\n\n    # Find a likely Daily Summary sheet\n    summary_candidates = []\n    for name in xls.sheet_names:\n        low = name.lower()\n        if any(k in low for k in [\"daily\", \"summary\", \"activity\", \"key trend\"]):\n            summary_candidates.append(name)\n    # If none matched, just try the first sheet\n    target_sheet = summary_candidates[0] if summary_candidates else xls.sheet_names[0]\n\n    try:\n        df = pd.read_excel(path, sheet_name=target_sheet, header=None)\n    except Exception as e:\n        return 0.0, f\"Failed to read summary sheet: {e}\"\n\n    labels = {\n        'total_rentals': [\"total number of closed rentals\",\"total closed rentals\",\"total rentals\",\"closed rentals\"],\n        'total_days': [\"total number of rental days\",\"total rental days\",\"rental days total\",\"total days\"],\n        'avg_lor': [\"average length of rental\",\"average lor\",\"avg length of rental\",\"avg lor\"],\n        'total_revenue': [\"total revenue\",\"revenue total\",\"gross revenue\",\"total $\"],\n        'avg_rev_per_rental': [\"average revenue per rental\",\"avg revenue per rental\",\"revenue per rental\"],\n        'avg_daily_rate': [\"average daily rate\",\"avg daily rate\",\"adr\",\"average revenue per day\",\"revenue per day\"],\n    }\n\n    def find_value(df, keywords):\n        nrows, ncols = df.shape\n        for r in range(nrows):\n            for c in range(ncols):\n                cell = df.iat[r, c]\n                if isinstance(cell, str):\n                    s = cell.strip().lower()\n                    if any(k in s for k in keywords):\n                        # search rightward in same row\n                        for c2 in range(c+1, min(c+4, ncols)):\n                            v = to_num(df.iat[r, c2])\n                            if v is not None:\n                                return v\n                        # search below in same column\n                        for r2 in range(r+1, min(r+4, nrows)):\n                            v = to_num(df.iat[r2, c])\n                            if v is not None:\n                                return v\n        return None\n\n    vals = {k: find_value(df, v) for k, v in labels.items()}\n\n    # Determine which checks we can run\n    checks = []\n\n    tr = vals.get('total_rentals')\n    td = vals.get('total_days')\n    tlr = vals.get('avg_lor')\n    trev = vals.get('total_revenue')\n    arv = vals.get('avg_rev_per_rental')\n    adr = vals.get('avg_daily_rate')\n\n    def close(a, b, tol=0.02, atol=0.01):\n        if a is None or b is None:\n            return None\n        if a == 0 and b == 0:\n            return True\n        if abs(b) < atol:\n            return abs(a-b) <= atol\n        return abs(a-b)/max(atol, abs(b)) <= tol\n\n    # LOR \u2248 total_days / total_rentals\n    if td is not None and tr is not None and tr not in [0]:\n        est_lor = td / tr\n        res = close(tlr, est_lor)\n        if res is not None:\n            checks.append(bool(res))\n\n    # Avg revenue per rental \u2248 total_revenue / total_rentals\n    if trev is not None and tr is not None and tr not in [0]:\n        est_arv = trev / tr\n        res = close(arv, est_arv)\n        if res is not None:\n            checks.append(bool(res))\n\n    # Avg daily rate \u2248 total_revenue / total_days\n    if trev is not None and td is not None and td not in [0]:\n        est_adr = trev / td\n        res = close(adr, est_adr)\n        if res is not None:\n            checks.append(bool(res))\n\n    if not checks:\n        return 0.0, \"Could not locate enough metrics to verify internal consistency.\"\n\n    score_ratio = sum(checks) / len(checks)\n    return 2.0*score_ratio, f\"Passed {sum(checks)}/{len(checks)} internal consistency checks.\"\n"}, {"type": "code", "name": "Category Breakdown Totals and Utilization", "description": "Verify category-level totals sum to daily totals and utilization percentages are coherent.", "weight": 1.8, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output found.\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n    except Exception as e:\n        return 0.0, f\"Failed to open Excel: {e}\"\n\n    def to_num(x):\n        if pd.isna(x):\n            return None\n        if isinstance(x, (int, float, np.number)):\n            return float(x)\n        s = str(x).strip()\n        if s == \"\":\n            return None\n        neg = False\n        if s.startswith(\"(\") and s.endswith(\")\"):\n            neg = True\n            s = s[1:-1]\n        s = s.replace(\",\", \"\").replace(\"$\", \"\").replace(\"%\", \"\")\n        try:\n            v = float(s)\n            return -v if neg else v\n        except:\n            return None\n\n    # Helper: extract daily totals again (robust if Stage 1 naming varied)\n    def get_daily_totals():\n        tr = td = trev = None\n        for name in xls.sheet_names:\n            try:\n                df = pd.read_excel(path, sheet_name=name, header=None)\n            except:\n                continue\n            lowname = name.lower()\n            if not any(k in lowname for k in [\"daily\", \"summary\", \"activity\", \"key trend\"]):\n                # still scan in case metrics are elsewhere\n                pass\n            labels = {\n                'total_rentals': [\"total number of closed rentals\",\"total closed rentals\",\"total rentals\",\"closed rentals\"],\n                'total_days': [\"total number of rental days\",\"total rental days\",\"rental days total\",\"total days\"],\n                'total_revenue': [\"total revenue\",\"revenue total\",\"gross revenue\",\"total $\"]\n            }\n            def find_value(df, keywords):\n                nrows, ncols = df.shape\n                for r in range(nrows):\n                    for c in range(ncols):\n                        cell = df.iat[r, c]\n                        if isinstance(cell, str):\n                            s = cell.strip().lower()\n                            if any(k in s for k in keywords):\n                                for c2 in range(c+1, min(c+4, ncols)):\n                                    v = to_num(df.iat[r, c2])\n                                    if v is not None:\n                                        return v\n                                for r2 in range(r+1, min(r+4, nrows)):\n                                    v = to_num(df.iat[r2, c])\n                                    if v is not None:\n                                        return v\n                return None\n            tr = tr or find_value(df, labels['total_rentals'])\n            td = td or find_value(df, labels['total_days'])\n            trev = trev or find_value(df, labels['total_revenue'])\n        return tr, td, trev\n\n    total_rentals, total_days, total_revenue = get_daily_totals()\n\n    # Find a category breakdown table (header row containing 'category' and another metric)\n    cat_df = None\n    for name in xls.sheet_names:\n        try:\n            raw = pd.read_excel(path, sheet_name=name, header=None)\n        except:\n            continue\n        nrows, ncols = raw.shape\n        for r in range(min(nrows, 30)):\n            row = [str(x).strip().lower() if isinstance(x, str) else \"\" for x in list(raw.iloc[r, :])]\n            if any(\"category\" in x for x in row) and (any(\"revenue\" in x for x in row) or any(\"rental\" in x for x in row) or any(\"days\" in x for x in row)):\n                try:\n                    df = pd.read_excel(path, sheet_name=name, header=r)\n                    # Clean columns\n                    df.columns = [str(c).strip().lower() for c in df.columns]\n                    # Heuristic: drop fully empty rows\n                    df2 = df.dropna(how='all')\n                    # Identify columns\n                    def pick(colkeys, exclude=None):\n                        for c in df2.columns:\n                            cl = str(c).lower()\n                            if any(k in cl for k in colkeys):\n                                if exclude and any(k in cl for k in exclude):\n                                    continue\n                                return c\n                        return None\n                    cat_col = pick([\"category\"])\n                    rentals_col = pick([\"rentals\"], exclude=[\"per\"]) or pick([\"total number of rentals\",\"# rentals\"]) \n                    days_col = pick([\"rental days\",\"days\"], exclude=[\"avg\"]) \n                    revenue_col = pick([\"total revenue\",\"revenue\"], exclude=[\"avg\",\"rate\",\"per day\",\"per rental\"]) \n                    if cat_col and (rentals_col or days_col or revenue_col):\n                        # keep necessary columns\n                        use_cols = [cat_col, rentals_col, days_col, revenue_col]\n                        use_cols = [c for c in use_cols if c is not None]\n                        cat_df = df2[use_cols].copy()\n                        cat_df.rename(columns={cat_col:\"category\"}, inplace=True)\n                        break\n                except:\n                    pass\n        if cat_df is not None:\n            break\n\n    if cat_df is None or cat_df.empty:\n        return 0.0, \"Could not locate a usable Category Breakdown table.\"\n\n    # Convert numerics\n    for col in cat_df.columns:\n        if col != \"category\":\n            cat_df[col] = cat_df[col].apply(to_num)\n    cat_df = cat_df.dropna(subset=[\"category\"]) \n\n    rentals_sum = cat_df[[c for c in cat_df.columns if c != 'category']].sum(numeric_only=True)\n    cat_rentals = rentals_sum.get([c for c in cat_df.columns if c != 'category' and 'rent' in c.lower() and 'per' not in c.lower()][0], np.nan) if any('rent' in c.lower() and 'per' not in c.lower() for c in cat_df.columns if c != 'category') else np.nan\n    cat_days = rentals_sum.get([c for c in cat_df.columns if c != 'category' and 'day' in c.lower()][0], np.nan) if any('day' in c.lower() for c in cat_df.columns if c != 'category') else np.nan\n    cat_rev = rentals_sum.get([c for c in cat_df.columns if c != 'category' and 'revenue' in c.lower()][0], np.nan) if any('revenue' in c.lower() for c in cat_df.columns if c != 'category') else np.nan\n\n    checks = []\n    msgs = []\n\n    def close(a, b, tol=0.02, atol=0.01):\n        if a is None or b is None or np.isnan(a) or np.isnan(b):\n            return None\n        if a == 0 and b == 0:\n            return True\n        if abs(b) < atol:\n            return abs(a-b) <= atol\n        return abs(a-b)/max(atol, abs(b)) <= tol\n\n    # Compare sums to daily totals if available\n    if total_rentals is not None and not np.isnan(cat_rentals):\n        res = close(cat_rentals, total_rentals, tol=0.02, atol=1.0)\n        if res is not None:\n            checks.append(bool(res)); msgs.append(f\"Rentals sum match: {res}\")\n    if total_days is not None and not np.isnan(cat_days):\n        res = close(cat_days, total_days, tol=0.02, atol=1.0)\n        if res is not None:\n            checks.append(bool(res)); msgs.append(f\"Days sum match: {res}\")\n    if total_revenue is not None and not np.isnan(cat_rev):\n        res = close(cat_rev, total_revenue, tol=0.02, atol=0.5)\n        if res is not None:\n            checks.append(bool(res)); msgs.append(f\"Revenue sum match: {res}\")\n\n    # Utilization coherence: category share sums to ~100% based on rentals if rentals available\n    if not np.isnan(cat_rentals) and cat_rentals and cat_rentals > 0 and 'rent' in ' '.join(cat_df.columns).lower():\n        rent_col = [c for c in cat_df.columns if c != 'category' and 'rent' in c.lower() and 'per' not in c.lower()][0]\n        shares = cat_df[rent_col].dropna().astype(float) / float(cat_rentals)\n        total_share = shares.sum()\n        res = abs(total_share - 1.0) <= 0.02\n        checks.append(res); msgs.append(f\"Utilization shares sum\u22481: {res} ({total_share:.3f})\")\n\n    if not checks:\n        return 0.0, \"Insufficient signals to verify category breakdown totals.\"\n\n    score_ratio = sum(1 for x in checks if x) / len(checks)\n    return 1.8*score_ratio, \"; \".join(msgs)\n"}, {"type": "code", "name": "Booking Source and Payment Method Totals", "description": "Verify booking source revenue (and rentals if present) and payment method revenue sums align with daily totals.", "weight": 1.2, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output found.\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n    except Exception as e:\n        return 0.0, f\"Failed to open Excel: {e}\"\n\n    def to_num(x):\n        if pd.isna(x):\n            return None\n        if isinstance(x, (int, float, np.number)):\n            return float(x)\n        s = str(x).strip()\n        if s == \"\":\n            return None\n        neg = False\n        if s.startswith(\"(\") and s.endswith(\")\"):\n            neg = True\n            s = s[1:-1]\n        s = s.replace(\",\", \"\").replace(\"$\", \"\").replace(\"%\", \"\")\n        try:\n            v = float(s)\n            return -v if neg else v\n        except:\n            return None\n\n    def find_summary_values():\n        tr = trev = None\n        for name in xls.sheet_names:\n            try:\n                df = pd.read_excel(path, sheet_name=name, header=None)\n            except:\n                continue\n            labels = {\n                'total_rentals': [\"total number of closed rentals\",\"total closed rentals\",\"total rentals\",\"closed rentals\"],\n                'total_revenue': [\"total revenue\",\"revenue total\",\"gross revenue\",\"total $\"]\n            }\n            def find_value(df, keywords):\n                nrows, ncols = df.shape\n                for r in range(nrows):\n                    for c in range(ncols):\n                        cell = df.iat[r, c]\n                        if isinstance(cell, str):\n                            s = cell.strip().lower()\n                            if any(k in s for k in keywords):\n                                for c2 in range(c+1, min(c+4, ncols)):\n                                    v = to_num(df.iat[r, c2])\n                                    if v is not None:\n                                        return v\n                                for r2 in range(r+1, min(r+4, nrows)):\n                                    v = to_num(df.iat[r2, c])\n                                    if v is not None:\n                                        return v\n                return None\n            tr = tr or find_value(df, labels['total_rentals'])\n            trev = trev or find_value(df, labels['total_revenue'])\n        return tr, trev\n\n    total_rentals, total_revenue = find_summary_values()\n\n    def find_table(col_name_keys, alt_sheet_keys):\n        # Find a table whose header row contains the key column(s)\n        for name in xls.sheet_names:\n            low = name.lower()\n            if any(k in low for k in alt_sheet_keys):\n                try:\n                    raw = pd.read_excel(path, sheet_name=name, header=None)\n                except:\n                    continue\n                nrows, ncols = raw.shape\n                for r in range(min(nrows, 30)):\n                    row = [str(x).strip().lower() if isinstance(x, str) else \"\" for x in list(raw.iloc[r, :])]\n                    if all(any(k in cell for cell in row) for k in col_name_keys):\n                        try:\n                            df = pd.read_excel(path, sheet_name=name, header=r)\n                            df.columns = [str(c).strip().lower() for c in df.columns]\n                            df = df.dropna(how='all')\n                            return df\n                        except:\n                            pass\n        # Fallback: scan any sheet\n        for name in xls.sheet_names:\n            try:\n                raw = pd.read_excel(path, sheet_name=name, header=None)\n            except:\n                continue\n            nrows, ncols = raw.shape\n            for r in range(min(nrows, 30)):\n                row = [str(x).strip().lower() if isinstance(x, str) else \"\" for x in list(raw.iloc[r, :])]\n                if all(any(k in cell for cell in row) for k in col_name_keys):\n                    try:\n                        df = pd.read_excel(path, sheet_name=name, header=r)\n                        df.columns = [str(c).strip().lower() for c in df.columns]\n                        df = df.dropna(how='all')\n                        return df\n                    except:\n                        pass\n        return None\n\n    # Booking Source Summary\n    bs_df = find_table([\"source\"], [\"booking\", \"source\"])\n    # Payment Method Summary\n    pm_df = find_table([\"payment\"], [\"payment\", \"method\", \"tender\"])\n\n    checks = []\n    msgs = []\n\n    def sum_column(df, include_keys, exclude_keys=None):\n        if df is None:\n            return None\n        for c in df.columns:\n            cl = str(c).lower()\n            if any(k in cl for k in include_keys) and not (exclude_keys and any(k in cl for k in exclude_keys)):\n                try:\n                    vals = df[c].apply(to_num)\n                    return float(pd.Series(vals).dropna().sum())\n                except:\n                    continue\n        return None\n\n    # Booking source: revenue and rentals (rentals optional)\n    bs_revenue = sum_column(bs_df, [\"revenue\", \"total $\"], exclude_keys=[\"avg\",\"rate\",\"per\"]) if bs_df is not None else None\n    bs_rentals = sum_column(bs_df, [\"rental\", \"rentals\", \"count\"]) if bs_df is not None else None\n\n    # Payment method: revenue collected\n    pm_revenue = sum_column(pm_df, [\"revenue\", \"collected\", \"amount\", \"total $\"], exclude_keys=[\"avg\",\"rate\",\"per\"]) if pm_df is not None else None\n\n    def close(a, b, tol=0.03, atol=0.5):\n        if a is None or b is None or (isinstance(a, float) and np.isnan(a)) or (isinstance(b, float) and np.isnan(b)):\n            return None\n        if a == 0 and b == 0:\n            return True\n        if abs(b) < atol:\n            return abs(a-b) <= atol\n        return abs(a-b)/max(atol, abs(b)) <= tol\n\n    # Compare sums\n    if total_revenue is not None and bs_revenue is not None:\n        res = close(bs_revenue, total_revenue, tol=0.03)\n        if res is not None:\n            checks.append(bool(res)); msgs.append(f\"Booking source revenue match: {res}\")\n    if total_rentals is not None and bs_rentals is not None:\n        res = close(bs_rentals, total_rentals, tol=0.03, atol=1.0)\n        if res is not None:\n            checks.append(bool(res)); msgs.append(f\"Booking source rentals match: {res}\")\n    if total_revenue is not None and pm_revenue is not None:\n        # Slightly wider tolerance for collected vs billed\n        res = close(pm_revenue, total_revenue, tol=0.05)\n        if res is not None:\n            checks.append(bool(res)); msgs.append(f\"Payment method revenue match: {res}\")\n\n    if not checks:\n        return 0.0, \"Could not verify booking/payment totals; missing usable tables or totals.\"\n\n    score_ratio = sum(1 for x in checks if x) / len(checks)\n    return 1.2*score_ratio, \"; \".join(msgs)\n"}, {"type": "llm_judge", "name": "Observations Grounded in Workbook Data", "description": "Check that the Observations sheet includes specific, data-grounded insights referencing categories, sources, or payment methods and aligns with visible figures.", "weight": 1.0, "judge_prompt": "Inspect the Excel workbook output. Open the Observations (or Notes/Insights) sheet. Assess whether the observations are grounded in the data shown elsewhere in the workbook (Daily Summary, Category Breakdown, Booking Source Summary, Payment Method Summary).\n\nAward full credit if:\n- There are at least 5 bullets (or 120+ words) AND\n- The observations cite at least 3 specific quantitative facts (e.g., percentages, counts, revenues) that are clearly visible in the workbook tables/charts AND\n- No evident contradictions with the figures in the workbook.\n\nPartial credit (0.5) if:\n- There are at least 3 solid, data-grounded points but the rest are generic, OR\n- Only 1\u20132 specific quantitative references are used, OR\n- Minor inconsistency or unclear phrasing that does not materially conflict with the data.\n\n0 points if:\n- Observations are generic platitudes without data references, OR\n- They contain claims that clearly contradict the workbook\u2019s numbers.\n\nFocus on whether the insights clearly reference the actual numbers in the Excel, not stylistic quality.", "expectation": "Observations reference concrete numbers from the workbook (e.g., category shares, ADR, source revenue mix) without contradicting the data."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality Assessment (Presentation and Managerial Use)", "description": "LLM evaluation of professional presentation and the strategic value of insights for management/sales.", "is_required": false, "max_points": 1.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Organization", "description": "Evaluate formatting clarity, legibility, labeling, and executive readability.", "weight": 0.5, "judge_prompt": "Evaluate the workbook\u2019s presentation quality. Consider: clear sheet names; readable tables; labeled columns with units/currency; consistent number formatting; presence of titles/subtitles; and basic visual aids (optional). Scoring:\n- 0.5: Professional and easy to navigate; tables are clearly labeled and readable; units/currency are evident; layout is appropriate for a daily close report.\n- 0.3: Generally readable but with minor labeling/formatting issues (e.g., inconsistent currency formats, unclear titles).\n- 0.1: Messy or confusing organization; difficult to find key metrics; poor labeling.\n- 0.0: Disorganized to the point that content is hard to read or interpret.\nOnly assess presentation/organization, not numeric correctness.", "expectation": "Clean, labeled, and readable workbook suitable for quick managerial review."}, {"type": "llm_judge", "name": "Strategic Value of Insights", "description": "Assess whether observations deliver actionable takeaways for management/sales.", "weight": 0.5, "judge_prompt": "Review the Observations sheet. Judge whether the insights are actionable for management and sales. Look for: highlighted trends (e.g., ADR movements, LOR differences), category-demand shifts, mix of booking sources, payment mix implications, and any concrete suggestions (e.g., inventory allocation, promo focus, staffing). Scoring:\n- 0.5: Clear, concise, and actionable insights tied to the data, with at least one concrete managerial implication.\n- 0.3: Some useful observations but limited actionability or too general.\n- 0.1: Vague or obvious statements with little strategic value.\n- 0.0: No meaningful insights for management.\nDo not grade on style beyond clarity; focus on usefulness and relevance.", "expectation": "Concise, actionable insights that would inform daily operational and sales decisions."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "45c6237b-f9c9-4526-9a8d-6a5c404624ec", "rubric": {"category_name": "Retail Presentation \u2013 Purchase Assortment Proposal (PDF)", "rationale": "This rubric enforces a self-documenting workflow: Stage 1 (LLM-only) mandates an explicit PDF slide structure so verification is trivial; Stage 2 uses code + LLM to verify correctness signals like size-split logic, hats sizing, and numeric sanity of the summary; Stage 3 evaluates professional quality and fitness for executive review.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2013 Structure and Format Gate", "description": "LLM-only gate that enforces exact presentation shape so later verification is trivial.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "PDF Slide Structure Requirement", "description": "Verify the delivered file is a PDF presentation with the exact structure required by the task.", "weight": 4.0, "judge_prompt": "You are evaluating whether the candidate output is a properly structured PDF presentation for a retail assortment proposal. Only check PRESENCE/STRUCTURE, not calculation accuracy or content quality.\n\nRequired FORMAT:\n- Must be a PDF file (not PPTX, DOCX, or plain text).\n- Fewer than 10 slides/pages (i.e., maximum of 9 pages). Assume each PDF page corresponds to one slide.\n\nRequired STRUCTURE (be flexible with minor wording/casing but check the intent):\n1) Slide 1 (Title Slide):\n   - Title: \"Crescent Pines Lodge & Spa\" (exact or very close)\n   - Subtitle: \"Purchase Assortment Spring 2022\" (exact or very close)\n\n2) Subsequent slides (2..N):\n   - Each slide shows the presentation title \"Crescent Pines Lodge & Spa\" somewhere as a heading/banner.\n   - Must include distinct sections for:\n     a) Custom Hats \u2013 explicitly marked for Gift Shop\n     b) Custom Shirts \u2013 explicitly marked for Apparel Store\n   - For Custom Hats: clearly indicates One-Size/OS only (can be noted as OS, One Size, or One-Size).\n   - For Custom Shirts: includes a clear Size Breakdown element per SKU or per style (e.g., a table or list) that shows the five sizes S, M, L, XL, XXL with allocated quantities.\n\n3) \"Next Season Assortment\" section:\n   - A slide or slides with a visible subtitle \"Next Season Assortment\"\n   - Shows multiple product images (styles/colors). You do not need to validate image provenance; just confirm actual images are present.\n\n4) Final slide (Summary Table):\n   - A tabular summary of purchase order details aggregated from the Purchase Order Excel source, with columns that clearly include at minimum: Item/SKU, Style/Description, Wholesale Price, Proposed Purchase Quantity. (Column names can vary slightly; judge by intent.)\n\nScoring (0.0\u20134.0):\n- 4.0: PDF format; <=9 slides; Title slide correct; both Hats (OS noted) and Shirts sections present; Shirts include size breakdowns (S, M, L, XL, XXL); \"Next Season Assortment\" section present with images; final summary table present with the required columns.\n- 3.0: PDF format and <=9 slides; title slide correct; most required sections present but one minor structural element missing (e.g., Shirts have sizes listed but no explicit breakdown per SKU/style; or title banner missing on some slides; or summary columns slightly incomplete).\n- 2.0: PDF format, but missing multiple required structural elements (e.g., no Next Season Assortment, or no final summary table, or missing entire Hats/Shirts section). Title slide may be present.\n- 1.0: PDF format but largely non-compliant with required structure (e.g., wrong title/ subtitle, many sections missing, >9 slides, or looks like a generic document not a slide deck).\n- 0.0: Not a PDF OR clearly fewer than 1 slide OR no recognizable required sections.\n\nOnly evaluate structure and presence. Do not judge the correctness of numbers or design polish.", "expectation": "Well-formed PDF slide deck under 10 pages with exact title/sections, explicit Hats OS-only note, Shirt size breakdowns, Next Season Assortment with images, and a final summary table including SKU, wholesale price, and proposed purchase quantity."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification of Key Requirements", "description": "Now that the structure exists, check correctness signals and internal consistency using code + LLM.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Document Type & Extractable Text", "description": "Confirm primary output is a document (PDF) and text can be extracted (non-empty).", "weight": 0.3, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"Primary output missing or not a document.\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception as e:\n        return 0.0, f\"Failed to read PDF text: {e}\"\n    if not text or len(text.strip()) < 30:\n        return 0.0, \"PDF text appears empty or too short to verify.\"\n    return 1.0, \"PDF text extracted successfully.\""}, {"type": "code", "name": "Hats Are OS (One-Size) Only \u2013 Evidence in Text", "description": "Look for evidence that Custom Hats are one-size/OS only.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document.\"\n    try:\n        text = context.files.read_pdf_text(output.id) or \"\"\n    except Exception:\n        return 0.0, \"Could not extract PDF text.\"\n    t = text.lower()\n    hats_present = ('custom hat' in t) or ('custom hats' in t) or ('hats' in t)\n    os_terms = any(term in t for term in [' one-size', ' one size', '(os)', ' os ', ' os-only', 'os only'])\n    if hats_present and os_terms:\n        return 1.0, \"Found Custom Hats with OS/One-Size notation.\"\n    if ('hat' in t or 'hats' in t) and os_terms:\n        return 0.6, \"Found hats and OS mention, but not clearly under Custom Hats section.\"\n    return 0.0, \"No clear evidence that hats are OS only.\""}, {"type": "code", "name": "Summary Table \u2013 Numeric Sanity", "description": "Check the final summary includes SKU/Item and shows wholesale prices and quantities (regex text check).", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document.\"\n    try:\n        text = context.files.read_pdf_text(output.id) or \"\"\n    except Exception:\n        return 0.0, \"Could not extract PDF text.\"\n    t = text.lower()\n    price_found = re.search(r\"\\$\\s?\\d{1,3}(?:,\\d{3})*(?:\\.\\d{2})?\", t) is not None\n    sku_found = ('sku' in t) or ('item' in t and 'style' in t)\n    qty_found = ('qty' in t) or ('quantity' in t) or re.search(r\"\\bqty\\b\", t) is not None\n    wholesale_found = 'wholesale' in t\n    score = (1 if price_found else 0) + (1 if sku_found else 0) + (1 if qty_found else 0) + (1 if wholesale_found else 0)\n    return score/4.0, f\"Signals \u2013 price:{price_found}, sku/item:{sku_found}, qty:{qty_found}, wholesale:{wholesale_found}\""}, {"type": "code", "name": "Shirt Size Split Mention (72%/28% with sizes)", "description": "Detect mention of the required shirt size split: 72% across M/L/XL and 28% across S/XXL (evenly split).", "weight": 0.7, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document.\"\n    try:\n        text = context.files.read_pdf_text(output.id) or \"\"\n    except Exception:\n        return 0.0, \"Could not extract PDF text.\"\n    t = text.lower()\n    has_72 = ('72%' in t) or ('0.72' in t) or ('72 percent' in t)\n    has_28 = ('28%' in t) or ('0.28' in t) or ('28 percent' in t)\n    # size tokens as standalone words\n    has_s  = re.search(r\"\\bS\\b\", text, flags=re.IGNORECASE) is not None\n    has_m  = re.search(r\"\\bM\\b\", text, flags=re.IGNORECASE) is not None\n    has_l  = re.search(r\"\\bL\\b\", text, flags=re.IGNORECASE) is not None\n    has_xl = re.search(r\"\\bXL\\b\", text, flags=re.IGNORECASE) is not None\n    has_xxl= re.search(r\"\\bXXL\\b\", text, flags=re.IGNORECASE) is not None\n    sizes_present = has_s and has_m and has_l and has_xl and has_xxl\n    evenly_lang = any(term in t for term in ['evenly split', 'split evenly', 'equal split'])\n    popular_lang = 'popular' in t\n\n    if has_72 and has_28 and sizes_present:\n        return 1.0, \"Found 72%/28% with all shirt sizes present.\"\n    if sizes_present and (evenly_lang or popular_lang):\n        return 0.6, \"Found sizes with language hinting at the required split.\"\n    return 0.0, \"Did not find clear evidence of required size split.\""}, {"type": "llm_judge", "name": "Shirt Size Allocation Calculation Correctness", "description": "Check whether size allocations for shirts follow the stated rule: ~72% of total to M/L/XL (evenly split) and ~28% to S/XXL (evenly split). Also, per-SKU size quantities should sum to the SKU total.", "weight": 0.8, "judge_prompt": "Evaluate the correctness of shirt size allocations in the PDF:\n- For at least two shirt SKUs (or representative examples if many), verify that the size allocation aligns with the rule: about 72% of total across M, L, XL split evenly, and about 28% across S and XXL split evenly. Allow small rounding deviations (\u00b11 unit or \u00b11%).\n- Confirm that, per SKU, the sum of the listed size quantities equals the SKU purchase total shown elsewhere on the slide/deck.\n\nScoring (0.0\u20130.8):\n- 0.8: At least two SKUs checked; both adhere to 72/28 split (evenly within rounding) AND sizes sum to totals.\n- 0.5: Evidence shows mostly correct application with minor inconsistencies or only one SKU clearly validated; sums mostly match.\n- 0.2: Size breakdowns exist but do not follow the 72/28 rule or sums are inconsistent.\n- 0.0: No verifiable size allocations or clearly incorrect distributions.", "expectation": "Per-SKU size tables that sum correctly and reflect roughly 72/28 allocation with even splits among M/L/XL and S/XXL."}, {"type": "llm_judge", "name": "Assortment Coverage \u2013 Images and Store Mapping", "description": "Confirm that both Hats and Shirts sections include multiple items with images and that store destinations are indicated (Gift Shop for Hats; Apparel Store for Shirts). Also verify the 'Next Season Assortment' shows multiple styles/colors with images.", "weight": 0.4, "judge_prompt": "Check the PDF for visual assortment coverage:\n- Custom Hats section: at least two items with images; explicitly tagged for Gift Shop.\n- Custom Shirts section: at least two items with images; explicitly tagged for Apparel Store.\n- 'Next Season Assortment' section: includes multiple styles/colors with images (not just text).\n\nScoring (0.0\u20130.4):\n- 0.4: All three bullet points satisfied.\n- 0.2: Two of the three satisfied.\n- 0.1: Only one satisfied.\n- 0.0: None satisfied.", "expectation": "Both Hats and Shirts visually represented with multiple items and store mapping; Next Season Assortment shows multiple images."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Presentation Quality", "description": "Holistic assessment of professional quality and executive readiness.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Executive Readiness and Professionalism", "description": "Assess design clarity, consistency, and suitability for Director-level approval.", "weight": 2.0, "judge_prompt": "Assess the overall professionalism and clarity of the PDF presentation:\n- Visual consistency: recurring title banner, consistent fonts/colors/branding suitable for a lodge & spa retail context.\n- Slide economy: under 10 slides, concise messaging, clear headings.\n- Readability: clean layout, legible tables, appropriate whitespace, clear labels for prices/quantities/SKUs.\n- Decision readiness: final slide summarizes selections, pricing, and purchase volumes clearly enough for quick approval.\n\nScoring (0.0\u20132.0):\n- 2.0: Highly professional, consistent, and immediately decision-ready.\n- 1.5: Professional with minor polish issues; easy to understand.\n- 1.0: Adequate but somewhat cluttered/inconsistent; still usable.\n- 0.5: Weak formatting; hard to read tables or confusing flow.\n- 0.0: Poorly formatted or incoherent.", "expectation": "A polished, concise, and executive-ready PDF with clear summaries and consistent visual design."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "81db15ff-ceea-4f63-a1cd-06dc88114709", "rubric": {"category_name": "Telehealth Provider Strategy (NP vs PA) \u2014 Regulatory Matrix and Recommendation", "rationale": "This rubric enforces a self-documenting, verifiable Excel deliverable. Stage 1 (LLM-only) mandates an explicit workbook structure that makes downstream verification trivial. Stage 2 uses code rules to verify correctness and internal consistency leveraging the structured shape. Stage 3 assesses professional quality and strategic utility for leadership.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate", "description": "LLM-only verification that the candidate output is an Excel workbook with the mandated structure enabling automated checks.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Workbook Requirements", "description": "Check if the output is an Excel workbook with a regulatory matrix and a recommendation sheet, formatted to enable verification.", "weight": 4.0, "judge_prompt": "You are validating ONLY the presence and structure of the output (not content correctness). Examine the candidate output as-rendered.\n\nFormat requirement:\n- Must be a single Excel workbook (.xlsx). Not PDF, not DOCX, not plain text.\n\nRequired workbook structure (be flexible with sheet names, but all elements must exist):\n\nSheet A: \u201cRegulatory Matrix\u201d (or similar, e.g., State Matrix, Provider Scope Matrix)\n- A single table with exactly one row per provider-role per state (i.e., 10 total rows for 5 states \u00d7 2 roles [NP and PA]). Minor extras like a header row are OK; but the table should clearly contain those 10 provider rows.\n- Required columns (flexible naming allowed; check semantic meaning):\n  1) State (Arizona, Pennsylvania, Washington, West Virginia, Virginia; abbreviations like AZ, PA, WA, WV, VA acceptable)\n  2) Provider Role (NP/Nurse Practitioner and PA/Physician Assistant)\n  3) Independent Practice (values like Yes/No/Conditional/Depends permitted)\n  4) Physician Signature Required (Yes/No/Conditional permitted)\n  5) Supervision Limit (number of providers a physician may supervise; N/A or similar allowed where not applicable)\n  6) Notes (telehealth-relevant notes accepted; any short text)\n  7) Source URL (at least one citation per row; a URL string)\n\nSheet B: \u201cRecommendation\u201d (or similar, e.g., Overall Recommendation, Strategic Summary)\n- Must include a clearly labeled section titled \u201cOverall Recommendation\u201d (or similarly named) with:\n  - An explicit overall choice between Nurse Practitioners (NPs) or Physician Assistants (PAs) as the stronger strategic choice across the five states.\n  - A rationale of at least 5 sentences referencing the matrix factors (independent practice, physician signature, supervision limits) and telehealth context.\n- Must include a small table titled \u201cRecommended Role by State\u201d (or similar), listing each of the five states with a chosen role (NP or PA) for that state.\n\nOptional (do not penalize if missing):\n- A \u201cMethodology & Definitions\u201d sheet with definitions (Independent practice, physician signature, supervision) and any assumptions.\n\nScoring (STRUCTURE ONLY):\n- 4.0: Excel format AND Regulatory Matrix sheet with all required columns AND 10 provider rows present AND Recommendation sheet with both the explicit overall choice + \u22655-sentence rationale AND per-state recommendation table.\n- 3.0: Excel format AND Matrix complete, AND Recommendation sheet present but missing either the per-state recommendation table OR the rationale is under 5 sentences.\n- 2.0: Excel format AND Matrix present but incomplete (e.g., missing required columns or <10 provider rows), OR Recommendation sheet missing.\n- 0.0: Not an Excel workbook OR missing core structural pieces (no matrix or no recommendation section).\n\nOnly assess presence/structure. Do NOT assess correctness of values or quality of argumentation.", "expectation": "A well-structured Excel workbook with a complete regulatory matrix (10 rows, required columns) and a recommendation sheet (explicit overall choice, \u22655-sentence rationale, and per-state table)."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification and Consistency Checks", "description": "Code-based checks leveraging the enforced shape to validate completeness, internal consistency, and basic reasonableness.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Matrix Sheet and Columns Detected", "description": "Detect a matrix-like sheet and required columns (flexible naming).", "weight": 1.2, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output detected.\"\n        path = context.files.get_path(output.id)\n        xl = pd.ExcelFile(path)\n        sheet_names = [s for s in xl.sheet_names]\n        # Find matrix sheet by name hints\n        def pick_matrix_sheet(names):\n            hints = ['matrix', 'state', 'scope', 'regulation', 'comparison', 'rules']\n            for s in names:\n                low = s.lower()\n                if any(h in low for h in hints):\n                    return s\n            return names[0] if names else None\n        matrix_sheet = pick_matrix_sheet(sheet_names)\n        if not matrix_sheet:\n            return 0.0, \"No sheets found.\"\n        df = pd.read_excel(path, sheet_name=matrix_sheet)\n        # Normalize column names\n        cols = {str(c).strip().lower(): c for c in df.columns}\n        def find_col(candidates):\n            for k in cols:\n                for cand in candidates:\n                    if re.search(cand, k):\n                        return cols[k]\n            return None\n        state_col = find_col([r'\\bstate\\b'])\n        role_col = find_col([r'role', r'profession', r'provider\\s*type'])\n        indep_col = find_col([r'independent', r'full\\s*practice', r'practice\\s*authority'])\n        sign_col = find_col([r'signature', r'co-?sign', r'chart.*sign'])\n        superv_col = find_col([r'supervis', r'ratio', r'limit', r'max.*supervis'])\n        notes_col = find_col([r'note', r'telehealth', r'comment'])\n        source_col = find_col([r'source', r'citation', r'url', r'link'])\n        score = 0.0\n        feedback = []\n        # 0.4 for sheet detected\n        score += 0.4\n        feedback.append(f\"Matrix sheet detected: {matrix_sheet}\")\n        # 0.4 for state + role columns\n        if state_col and role_col:\n            score += 0.4\n        else:\n            missing = []\n            if not state_col: missing.append('State')\n            if not role_col: missing.append('Role')\n            feedback.append(f\"Missing columns: {', '.join(missing)}\")\n        # 0.4 for at least 2 of the 3 key columns\n        key_have = sum([1 for c in [indep_col, sign_col, superv_col] if c is not None])\n        if key_have >= 2:\n            score += 0.4\n        else:\n            feedback.append(\"Fewer than 2 key columns (Independent, Signature, Supervision) detected.\")\n        return min(score, 1.2), \"; \".join(feedback)\n    except Exception as e:\n        return 0.0, f\"Exception: {e}\""}, {"type": "code", "name": "Coverage: 5 States \u00d7 2 Roles", "description": "Ensure the matrix contains NP and PA entries for each of the five target states.", "weight": 1.4, "code": "import re\nimport pandas as pd\nimport numpy as np\n\nSTATE_MAP = {\n    'az': 'AZ', 'arizona': 'AZ',\n    'pa': 'PA', 'pennsylvania': 'PA',\n    'wa': 'WA', 'washington': 'WA',\n    'wv': 'WV', 'west virginia': 'WV',\n    'va': 'VA', 'virginia': 'VA'\n}\nTARGET_STATES = ['AZ','PA','WA','WV','VA']\n\nROLE_MAP = {\n    'np': 'NP', 'nurse practitioner': 'NP',\n    'pa': 'PA', 'physician assistant': 'PA'\n}\n\ndef normalize_state(x):\n    if pd.isna(x):\n        return None\n    s = str(x).strip().lower()\n    return STATE_MAP.get(s) or STATE_MAP.get(re.sub(r'[^a-z ]','',s))\n\ndef normalize_role(x):\n    if pd.isna(x):\n        return None\n    s = str(x).strip().lower()\n    # exact tokens first\n    for k,v in ROLE_MAP.items():\n        if k == s:\n            return v\n    # substring contains\n    for k,v in ROLE_MAP.items():\n        if k in s:\n            return v\n    return None\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output detected.\"\n        path = context.files.get_path(output.id)\n        xl = pd.ExcelFile(path)\n        # pick matrix sheet\n        def pick_matrix_sheet(names):\n            hints = ['matrix', 'state', 'scope', 'regulation', 'comparison', 'rules']\n            for s in names:\n                if any(h in s.lower() for h in hints):\n                    return s\n            return names[0] if names else None\n        sheet = pick_matrix_sheet(xl.sheet_names)\n        if not sheet:\n            return 0.0, \"No matrix-like sheet found.\"\n        df = pd.read_excel(path, sheet_name=sheet)\n        # find columns\n        cols = {str(c).strip().lower(): c for c in df.columns}\n        def find_col(cands):\n            for k in cols:\n                for c in cands:\n                    if re.search(c, k):\n                        return cols[k]\n            return None\n        state_col = find_col([r'\\bstate\\b'])\n        role_col = find_col([r'role', r'profession', r'provider\\s*type'])\n        if not state_col or not role_col:\n            return 0.0, \"Missing State or Role column.\"\n        # normalize\n        states = df[state_col].map(normalize_state)\n        roles = df[role_col].map(normalize_role)\n        # count coverage\n        score = 0.0\n        fb = []\n        # 0.4 for having all 5 target states present at least once\n        present_states = sorted(list(set([s for s in states if s in TARGET_STATES])))\n        if set(TARGET_STATES).issubset(set(present_states)):\n            score += 0.4\n        else:\n            missing = [s for s in TARGET_STATES if s not in present_states]\n            fb.append(f\"Missing states: {', '.join(missing)}\")\n        # 0.5 for both roles per state \u2014 award proportionally\n        both_roles_count = 0\n        for st in TARGET_STATES:\n            subset = df[states == st]\n            rset = set([normalize_role(x) for x in subset[role_col]])\n            if 'NP' in rset and 'PA' in rset:\n                both_roles_count += 1\n        score += 0.5 * (both_roles_count / 5.0)\n        if both_roles_count < 5:\n            fb.append(f\"States with both roles: {both_roles_count}/5\")\n        # 0.5 for having at least 10 provider rows (allow >=10)\n        provider_rows = df[[state_col, role_col]].dropna().shape[0]\n        if provider_rows >= 10:\n            score += 0.5\n        else:\n            fb.append(f\"Only {provider_rows} provider rows; expected at least 10.\")\n        return min(score, 1.4), \"; \".join(fb)\n    except Exception as e:\n        return 0.0, f\"Exception: {e}\""}, {"type": "code", "name": "Value Validity: Practice, Signature, Supervision", "description": "Check whether key fields contain parsable values and plausible supervision entries.", "weight": 0.6, "code": "import re\nimport pandas as pd\nimport numpy as np\n\nYESNO = {\n    'yes': 'yes', 'y': 'yes', 'true': 'yes',\n    'no': 'no', 'n': 'no', 'false': 'no',\n    'conditional': 'conditional', 'depends': 'conditional', 'partial': 'conditional'\n}\nNAISH = {'na','n/a','none','not applicable','n a',''}\n\ndef norm_yesno(x):\n    if pd.isna(x):\n        return None\n    s = str(x).strip().lower()\n    s = re.sub(r'[^a-z ]','', s)\n    if s in YESNO:\n        return YESNO[s]\n    # contains keywords\n    if 'conditional' in s or 'depend' in s or 'partial' in s:\n        return 'conditional'\n    if s in NAISH:\n        return None\n    if 'yes' in s:\n        return 'yes'\n    if 'no' in s:\n        return 'no'\n    return None\n\ndef parse_supervision(x):\n    if pd.isna(x):\n        return np.nan\n    s = str(x).strip().lower()\n    if s in NAISH or 'not required' in s:\n        return np.nan\n    # try number in text\n    m = re.search(r'(\\d+\\.?\\d*)', s)\n    if m:\n        try:\n            return float(m.group(1))\n        except:\n            return np.nan\n    # words like unlimited\n    if 'unlimited' in s or 'no limit' in s:\n        return 9999.0\n    return np.nan\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        path = context.files.get_path(output.id)\n        xl = pd.ExcelFile(path)\n        # pick matrix sheet\n        def pick_matrix_sheet(names):\n            hints = ['matrix', 'state', 'scope', 'regulation', 'comparison', 'rules']\n            for s in names:\n                if any(h in s.lower() for h in hints):\n                    return s\n            return names[0] if names else None\n        sheet = pick_matrix_sheet(xl.sheet_names)\n        if not sheet:\n            return 0.0, \"Matrix sheet not found.\"\n        df = pd.read_excel(path, sheet_name=sheet)\n        cols = {str(c).strip().lower(): c for c in df.columns}\n        def find_col(cands):\n            for k,v in cols.items():\n                for c in cands:\n                    if re.search(c, k):\n                        return v\n            return None\n        indep_col = find_col([r'independent', r'full\\s*practice', r'practice\\s*authority'])\n        sign_col = find_col([r'signature', r'co-?sign', r'chart.*sign'])\n        superv_col = find_col([r'supervis', r'ratio', r'limit', r'max.*supervis'])\n        score = 0.0\n        fb = []\n        if indep_col is None and sign_col is None and superv_col is None:\n            return 0.0, \"No key columns found.\"\n        # 0.3: proportion of rows with valid yes/no/conditional in independent + signature (avg)\n        n = max(len(df), 1)\n        valid_indep = 0\n        valid_sign = 0\n        if indep_col is not None:\n            valid_indep = df[indep_col].apply(norm_yesno).notna().sum()\n        if sign_col is not None:\n            valid_sign = df[sign_col].apply(norm_yesno).notna().sum()\n        parts = []\n        if indep_col is not None:\n            parts.append(valid_indep / n)\n        if sign_col is not None:\n            parts.append(valid_sign / n)\n        if parts:\n            score += 0.3 * (sum(parts) / len(parts))\n        else:\n            fb.append(\"Independent/Signature columns missing for validation.\")\n        # 0.3: proportion of rows with plausible supervision parse (number or NAish accepted)\n        if superv_col is not None:\n            parsed = df[superv_col].apply(parse_supervision)\n            ok = parsed.notna().sum()  # number or NA accepted as NaN; treat NaN as acceptable? We'll accept numeric OR explicit NA words\n            # Adjust: consider empty/NA as acceptable only if corresponding independent is 'yes'\n            # To keep robust, we count as valid if numeric or NAish string present\n            # Recompute using raw values\n            raw = df[superv_col]\n            def is_valid_super(x):\n                if pd.isna(x):\n                    return True\n                s = str(x).strip().lower()\n                if s in NAISH or 'not applicable' in s or 'n/a' in s:\n                    return True\n                if re.search(r'(\\d+\\.?\\d*)', s) or 'unlimited' in s or 'no limit' in s:\n                    return True\n                return False\n            ok = raw.apply(is_valid_super).sum()\n            score += 0.3 * (ok / n)\n        else:\n            fb.append(\"Supervision column missing for validation.\")\n        return min(score, 0.6), \"; \".join(fb)\n    except Exception as e:\n        return 0.0, f\"Exception: {e}\""}, {"type": "code", "name": "Recommendation Presence and Explicit Choice", "description": "Detect a recommendation sheet with \u22655-sentence rationale and an explicit overall NP or PA choice.", "weight": 0.5, "code": "import re\nimport pandas as pd\n\nCHOICE_TERMS = {\n    'np': ['np', 'nurse practitioner', 'nurse practitioners'],\n    'pa': ['pa', 'physician assistant', 'physician assistants']\n}\n\ndef find_reco_sheet(names):\n    hints = ['recom', 'summary', 'decision', 'conclusion']\n    for s in names:\n        if any(h in s.lower() for h in hints):\n            return s\n    return None\n\ndef extract_sheet_text(df):\n    # Flatten all cell values to text\n    texts = []\n    for col in df.columns:\n        texts.extend([str(x) for x in df[col].dropna().astype(str).tolist()])\n    return \"\\n\".join(texts)\n\ndef count_sentences(text):\n    # naive sentence count\n    parts = re.split(r'[\\.!?]+', text)\n    return sum(1 for p in parts if len(p.strip()) > 0)\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        path = context.files.get_path(output.id)\n        xl = pd.ExcelFile(path)\n        sheet = find_reco_sheet(xl.sheet_names)\n        score = 0.0\n        fb = []\n        if sheet is None:\n            return 0.0, \"No recommendation-like sheet found.\"\n        score += 0.2  # sheet detected\n        df = pd.read_excel(path, sheet_name=sheet, header=0)\n        text = extract_sheet_text(df).lower()\n        # look for explicit overall choice\n        has_np = any(t in text for t in CHOICE_TERMS['np'])\n        has_pa = any(t in text for t in CHOICE_TERMS['pa'])\n        if has_np or has_pa:\n            score += 0.1\n        else:\n            fb.append(\"No explicit NP or PA mention detected.\")\n        # sentence length >= 5\n        sent_count = count_sentences(text)\n        if sent_count >= 5:\n            score += 0.2\n        else:\n            fb.append(f\"Rationale too short: {sent_count} sentences.\")\n        return min(score, 0.5), \"; \".join(fb)\n    except Exception as e:\n        return 0.0, f\"Exception: {e}\""}, {"type": "code", "name": "Overall Choice Aligns With Majority Advantage", "description": "Compute a simple per-state advantage from matrix and check if the stated overall choice matches the majority winner.", "weight": 0.3, "code": "import re\nimport pandas as pd\nimport numpy as np\n\nSTATE_MAP = {\n    'az': 'AZ', 'arizona': 'AZ',\n    'pa': 'PA', 'pennsylvania': 'PA',\n    'wa': 'WA', 'washington': 'WA',\n    'wv': 'WV', 'west virginia': 'WV',\n    'va': 'VA', 'virginia': 'VA'\n}\nTARGET_STATES = ['AZ','PA','WA','WV','VA']\nROLE_MAP = {\n    'np': 'NP', 'nurse practitioner': 'NP',\n    'pa': 'PA', 'physician assistant': 'PA'\n}\n\ndef normalize_state(x):\n    if pd.isna(x):\n        return None\n    s = str(x).strip().lower()\n    s2 = re.sub(r'[^a-z ]','', s)\n    return STATE_MAP.get(s) or STATE_MAP.get(s2)\n\ndef normalize_role(x):\n    if pd.isna(x):\n        return None\n    s = str(x).strip().lower()\n    for k,v in ROLE_MAP.items():\n        if k == s:\n            return v\n    for k,v in ROLE_MAP.items():\n        if k in s:\n            return v\n    return None\n\ndef norm_yesno(x):\n    if pd.isna(x):\n        return None\n    s = str(x).strip().lower()\n    s = re.sub(r'[^a-z ]','', s)\n    if 'yes' in s:\n        return 'yes'\n    if 'no' in s:\n        return 'no'\n    if 'conditional' in s or 'depend' in s or 'partial' in s:\n        return 'conditional'\n    return None\n\ndef parse_supervision(x):\n    if pd.isna(x):\n        return np.nan\n    s = str(x).strip().lower()\n    if 'n/a' in s or 'na' == s or 'not applicable' in s or 'none' == s:\n        return np.nan\n    if 'unlimited' in s or 'no limit' in s:\n        return 9999.0\n    m = re.search(r'(\\d+\\.?\\d*)', s)\n    if m:\n        try:\n            return float(m.group(1))\n        except:\n            return np.nan\n    return np.nan\n\ndef pick_matrix_sheet(names):\n    hints = ['matrix', 'state', 'scope', 'regulation', 'comparison', 'rules']\n    for s in names:\n        if any(h in s.lower() for h in hints):\n            return s\n    return names[0] if names else None\n\ndef pick_reco_sheet(names):\n    hints = ['recom', 'summary', 'decision', 'conclusion']\n    for s in names:\n        if any(h in s.lower() for h in hints):\n            return s\n    return None\n\ndef extract_text(df):\n    texts = []\n    for col in df.columns:\n        texts.extend([str(x) for x in df[col].dropna().astype(str).tolist()])\n    return \"\\n\".join(texts).lower()\n\ndef detect_overall_choice(text):\n    if any(t in text for t in ['nurse practitioner', 'nurse practitioners', ' np ']):\n        # Prefer explicit formulations like \"Overall, NPs are recommended\"\n        return 'NP'\n    if any(t in text for t in ['physician assistant', 'physician assistants', ' pa ']):\n        return 'PA'\n    return None\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output.\"\n        path = context.files.get_path(output.id)\n        xl = pd.ExcelFile(path)\n        m_sheet = pick_matrix_sheet(xl.sheet_names)\n        if not m_sheet:\n            return 0.0, \"Matrix sheet not found.\"\n        df = pd.read_excel(path, sheet_name=m_sheet)\n        cols = {str(c).strip().lower(): c for c in df.columns}\n        def find_col(cands):\n            for k,v in cols.items():\n                for c in cands:\n                    if re.search(c, k):\n                        return v\n            return None\n        state_col = find_col([r'\\bstate\\b'])\n        role_col = find_col([r'role', r'profession', r'provider\\s*type'])\n        indep_col = find_col([r'independent', r'full\\s*practice', r'practice\\s*authority'])\n        sign_col = find_col([r'signature', r'co-?sign', r'chart.*sign'])\n        superv_col = find_col([r'supervis', r'ratio', r'limit', r'max.*supervis'])\n        if not state_col or not role_col:\n            return 0.0, \"Missing State/Role columns.\"\n        df['_state'] = df[state_col].map(normalize_state)\n        df['_role'] = df[role_col].map(normalize_role)\n        if indep_col is not None:\n            df['_indep'] = df[indep_col].map(norm_yesno)\n        else:\n            df['_indep'] = None\n        if sign_col is not None:\n            df['_sign'] = df[sign_col].map(norm_yesno)\n        else:\n            df['_sign'] = None\n        if superv_col is not None:\n            df['_sup'] = df[superv_col].map(parse_supervision)\n        else:\n            df['_sup'] = np.nan\n        # Compute per-state winner by simple hierarchy: Independent yes > no/conditional; if tie, Signature no > yes; if tie, higher supervision limit wins; if tie, no winner\n        winners = []\n        for st in TARGET_STATES:\n            d = df[df['_state'] == st]\n            np_row = d[d['_role'] == 'NP'].head(1)\n            pa_row = d[d['_role'] == 'PA'].head(1)\n            if np_row.empty or pa_row.empty:\n                continue\n            def score_row(r):\n                indep = r.get('_indep', pd.Series([None])).iloc[0]\n                sign = r.get('_sign', pd.Series([None])).iloc[0]\n                sup = r.get('_sup', pd.Series([np.nan])).iloc[0]\n                s = 0\n                if indep == 'yes':\n                    s += 2\n                elif indep == 'conditional':\n                    s += 1\n                # signature: no is better\n                if sign == 'no':\n                    s += 1\n                elif sign == 'conditional':\n                    s += 0.5\n                # supervision: higher allowed number is better; NaN interpreted as not applicable (treat as high)\n                if pd.isna(sup):\n                    s += 0.5\n                else:\n                    try:\n                        s += min(1.0, float(sup)/10.0)  # cap contribution\n                    except:\n                        s += 0\n                return s\n            np_s = score_row(np_row)\n            pa_s = score_row(pa_row)\n            if np_s > pa_s:\n                winners.append('NP')\n            elif pa_s > np_s:\n                winners.append('PA')\n            else:\n                winners.append('TIE')\n        # majority winner\n        np_count = winners.count('NP')\n        pa_count = winners.count('PA')\n        majority = 'NP' if np_count > pa_count else ('PA' if pa_count > np_count else 'TIE')\n        # detect stated overall choice\n        r_sheet = pick_reco_sheet(xl.sheet_names)\n        if not r_sheet:\n            return 0.0, \"Recommendation sheet not found.\"\n        rdf = pd.read_excel(path, sheet_name=r_sheet)\n        rtext = extract_text(rdf)\n        stated = detect_overall_choice(rtext)\n        if stated is None:\n            return 0.0, \"No explicit overall choice detected.\"\n        return (0.3 if (majority == 'TIE' or stated == majority) else 0.0), f\"Majority: {majority}, Stated: {stated}, per-state winners: NP={np_count}, PA={pa_count}\"\n    except Exception as e:\n        return 0.0, f\"Exception: {e}\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Strategic Value", "description": "LLM assessment of clarity, professionalism, and strategic usefulness for leadership.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Executive Readiness and Strategic Insight", "description": "Assess presentation quality, clarity, and strategic value of the recommendation for leadership decision-making.", "weight": 2.0, "judge_prompt": "Evaluate the workbook for professional communication quality and strategic usefulness. Assume structural gating already passed.\n\nConsider:\n- Clarity and readability: Clear headers, legible tables, consistent formatting, and concise notes.\n- Executive readiness: A succinct, defensible overall recommendation that a leadership team could act on quickly.\n- Strategic reasoning: Does the rationale tie directly to matrix evidence (independent practice, physician signature, supervision limits) and the telehealth context? Are trade-offs and uncertainties noted?\n- Actionability: Per-state recommendations are present and unambiguous.\n- Risk awareness: Mentions limitations, suggests verifying statutes/board rules, and includes citations.\n\nScoring Guide:\n- 2.0: Highly professional, clear, actionable, with strong evidence linkage and risk awareness.\n- 1.0: Generally clear and useful, but with minor issues (e.g., thin rationale, formatting inconsistencies, or light risk notes).\n- 0.5: Understandable but not executive-ready; weak linkage to evidence or ambiguous asks.\n- 0.0: Hard to follow, lacks coherent recommendation, or not useful for leadership.\n\nFocus on communication quality and strategic value, not raw factual accuracy (already checked elsewhere).", "expectation": "A polished, executive-ready deliverable: clean matrix, crisp per-state choices, a clear overall choice with evidence-based rationale and citations."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "dd724c67-8118-4b99-ab50-4761af705c3b", "rubric": {"category_name": "ACO Discharge Coordination: Facility Contact List + CMS TFU Reference (Registered Nurse Case Manager)", "rationale": "Pattern C (Mixed): The deliverable is a single Excel workbook combining a structured data table (facility contacts) and a document-like reference guide (TFU overview, rationale, and condition-specific timeframes) on a separate sheet. Stage 1 uses an LLM gate to enforce the exact spreadsheet structure so verification becomes trivial. Stage 2 uses code rules to validate plausibility and structural correctness (phones, NY locality signals, TFU table presence and columns, condition coverage, CMS/ACO REACH references). Stage 3 uses an LLM to assess professional usability and presentation for busy case managers.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Excel Shape and Structure Gate (LLM ONLY)", "description": "Gate: Verify the candidate produced a single Excel workbook with two required sheets: a Facilities contact list and a TFU reference guide, each with specific, verifiable structure. Only check structure and presence; do not judge correctness of content.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Workbook Structure and Required Sections Present", "description": "LLM verifies the output is an Excel workbook with both required sheets and specified structural elements.", "weight": 4.0, "judge_prompt": "You are validating ONLY the SHAPE/STRUCTURE of the output. Do not check content accuracy.\n\nRequirement: The output must be a single Excel workbook (.xlsx) with at least two sheets:\n\nSheet A (Facilities Contact List): Name should clearly imply facilities/contacts (acceptable examples: \"Facilities\", \"Facility Contacts\", \"Hospitals & Rehab\", \"Long Island Facilities\", similar variants). Must contain a tabular contact list with:\n- Required columns (flexible naming, obvious intent):\n  1) Facility Name (e.g., \"Facility Name\", \"Name\")\n  2) Address (either a single column like \"Address\" OR separate columns for Street/City/State/ZIP)\n  3) Telephone (e.g., \"Phone\", \"Telephone\")\n- Optional but allowed: Type (Hospital/Rehab), City, County, Fax, Website, Notes, etc.\n- At least several rows of facilities (aim for \u226510; minor deviations acceptable for partial credit). Rows should appear as a single, regular table (not scattered text).\n\nSheet B (TFU Reference Guide): Name should clearly imply Timely Follow-Up (acceptable examples: \"TFU Reference\", \"Timely Follow-Up\", \"CMS TFU\", similar variants). Must include clearly labeled sections:\n- Overview: a short paragraph (\u22653 sentences) describing the TFU measure.\n- Rationale: a short paragraph (\u22652 sentences) explaining why the measure matters.\n- Follow-up Timeframes by Condition: a visible table whose columns indicate:\n  \u2022 Condition (e.g., CAD, diabetes, COPD, etc.)\n  \u2022 Recommended Follow-up Timeframe (days)\n  \u2022 Notes/Rationale (optional but encouraged)\n  \u2022 Source/Reference (optional but encouraged)\n  Accept flexible column names that obviously convey these ideas (e.g., \"Condition\", \"Diagnosis\"; \"Follow-up\", \"Timeframe\", \"Days\"; \"Notes\"; \"Source\").\n- Source mention: The sheet should reference the \"ACO REACH Model PY 2025 Quality Measurement Methodology Report\" and/or cms.gov as the source (flexible phrasing acceptable).\n\nFormat constraints:\n- File type must be Excel (.xlsx). Not CSV/Google Sheets links/Word/PDF.\n- Both sheets should be present with clear headers/labels that make each section easy to identify.\n\nScoring (STRUCTURE ONLY):\n- 4.0: Valid Excel. Both sheets present with correct intent in sheet names. Facilities sheet shows a clear table with the three required column intents (name, address or components, phone) and multiple rows (\u224810+). TFU sheet has visible Overview and Rationale text sections AND a condition/timeframe table with recognizable columns. Source to CMS ACO REACH/PY 2025 is mentioned.\n- 3.0: Valid Excel. Both sheets present and mostly correct, but one minor structural piece is weak/missing (e.g., TFU source mention absent OR Rationale/Overview too thin OR facility rows fewer than ~10 but still multiple rows). Still clearly usable for verification.\n- 1.5: Valid Excel but only one task is properly structured (e.g., Facilities sheet OK but TFU sheet lacks the table/sections, or vice versa). Or both sheets exist but structure is too incomplete to verify.\n- 0.0: Not an Excel file OR missing one of the two required sheets OR Facilities sheet lacks any of the three core column intents OR TFU sheet lacks both prose sections and the condition/timeframe table.\n\nOnly evaluate presence and structural shape. Do NOT judge whether the content is correct, complete, or clinically accurate.", "expectation": "A single Excel file with two sheets: a Facilities table (with Name, Address, Phone) and a TFU Reference sheet containing Overview, Rationale, and a Condition vs. Follow-up Timeframe table, plus a CMS ACO REACH PY 2025 source mention."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Consistency Verification (Code + LLM-friendly)", "description": "Now that the structure is enforced, programmatically verify plausibility and internal consistency of data and references using flexible, robust checks.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Facilities Data Validity (Columns, Phones, NY locality, Row count)", "description": "Check facilities sheet for required columns, minimum plausible row count, valid US phone formats, and NY locality signals in address/state.", "weight": 1.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n        sheet_names = xls.sheet_names\n        # Find facilities-like sheet\n        fac_tokens = [\"facil\", \"contact\", \"hospital\", \"rehab\", \"long island\"]\n        fac_sheet = None\n        for s in sheet_names:\n            sl = s.lower()\n            if any(t in sl for t in fac_tokens):\n                fac_sheet = s\n                break\n        if fac_sheet is None:\n            # fallback: first sheet if looks tabular\n            fac_sheet = sheet_names[0] if sheet_names else None\n        if fac_sheet is None:\n            return 0.0\n        df = pd.read_excel(file_path, sheet_name=fac_sheet)\n        if df is None or df.shape[1] == 0:\n            return 0.0\n        # Normalize columns\n        col_lower = [str(c).strip().lower() for c in df.columns]\n        # Identify name/phone/address-related columns\n        name_idx = None\n        phone_idx = None\n        address_idxs = []\n        for i, c in enumerate(col_lower):\n            if name_idx is None and (\"name\" in c):\n                name_idx = i\n            if phone_idx is None and (\"phone\" in c or \"tel\" in c):\n                phone_idx = i\n            if any(k in c for k in [\"address\", \"street\", \"city\", \"state\", \"zip\", \"postal\"]):\n                address_idxs.append(i)\n        # If phone column not found, try to infer a phone-like column\n        if phone_idx is None:\n            for i in range(df.shape[1]):\n                series = df.iloc[:, i].astype(str)\n                m = series.str.contains(r\"\\(?\\d{3}\\)?[\\s\\-\\.]?\\d{3}[\\s\\-\\.]?\\d{4}\", case=False, regex=True, na=False)\n                if m.mean() >= 0.4:  # at least 40% look like phones\n                    phone_idx = i\n                    break\n        # Address considered present if an explicit address column exists OR multiple location columns exist\n        has_address = False\n        if any(\"address\" in c for c in col_lower):\n            has_address = True\n        else:\n            # Accept combination of city/state or city/zip\n            has_city = any(\"city\" in c for c in col_lower)\n            has_state = any(\"state\" in c for c in col_lower)\n            has_zip = any(\"zip\" in c or \"postal\" in c for c in col_lower)\n            has_address = (has_city and (has_state or has_zip))\n        # Compute basic row counts using name column if available, otherwise non-empty across any column\n        if name_idx is not None:\n            name_series = df.iloc[:, name_idx].astype(str).str.strip()\n            row_count = (name_series.replace({\"nan\":\"\", \"None\":\"\"}) != \"\").sum()\n        else:\n            any_nonempty = df.apply(lambda r: any(str(v).strip() not in [\"\", \"nan\", \"None\"] for v in r), axis=1)\n            row_count = any_nonempty.sum()\n        # Phone validity proportion\n        phone_valid_prop = 0.0\n        if phone_idx is not None:\n            phones = df.iloc[:, phone_idx].astype(str)\n            valid = phones.str.contains(r\"\\(?\\d{3}\\)?[\\s\\-\\.]?\\d{3}[\\s\\-\\.]?\\d{4}\", case=False, regex=True, na=False)\n            phone_valid_prop = float(valid.mean()) if len(phones) > 0 else 0.0\n        # NY locality signal: check state==NY if present, else look in address text for NY/New York or 1xxxx zip\n        ny_signal = 0.0\n        try:\n            state_cols = [i for i, c in enumerate(col_lower) if \"state\" in c]\n            if state_cols:\n                st = df.iloc[:, state_cols[0]].astype(str).str.upper().str.strip()\n                ny_signal = (st == \"NY\").mean()\n            else:\n                addr_text = pd.Series(dtype=str)\n                if address_idxs:\n                    pieces = []\n                    for idx in address_idxs:\n                        pieces.append(df.iloc[:, idx].astype(str))\n                    addr_text = pd.concat(pieces, axis=1).astype(str).fillna(\"\").agg(\" \".join, axis=1)\n                else:\n                    addr_text = df.astype(str).agg(\" \".join, axis=1)\n                addr_lower = addr_text.str.lower()\n                ny_hits = addr_lower.str.contains(r\"\\bny\\b|new york\", na=False)\n                zip_hits = addr_lower.str.contains(r\"\\b1\\d{4}\\b\", na=False)\n                ny_signal = float((ny_hits | zip_hits).mean())\n        except Exception:\n            ny_signal = 0.0\n        # Scoring breakdown (max 1.5):\n        score = 0.0\n        # Columns presence\n        if name_idx is not None and phone_idx is not None and has_address:\n            score += 0.5\n        # Row count adequacy\n        if row_count >= 10:\n            score += 0.5\n        elif row_count >= 5:\n            score += 0.25\n        # Phone validity quality\n        score += 0.3 * min(1.0, phone_valid_prop / 0.7)  # full credit if >=70% valid\n        # NY locality signal (at least half of entries signal NY)\n        if ny_signal >= 0.5:\n            score += 0.2\n        elif ny_signal > 0.0:\n            score += 0.1\n        return float(min(score, 1.5))\n    except Exception:\n        return 0.0\n"}, {"type": "code", "name": "TFU Sheet \u2014 Sections and Condition/Timeframe Table Presence", "description": "Check TFU-like sheet for Overview and Rationale text presence and a table with columns indicating Condition and Follow-up/Timeframe; verify several conditions are listed (e.g., CAD, diabetes, COPD).", "weight": 1.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n        sheet_names = xls.sheet_names\n        # Find TFU-like sheet\n        tfu_tokens = [\"tfu\", \"timely\", \"follow\", \"cms\"]\n        tfu_sheet = None\n        for s in sheet_names:\n            sl = s.lower()\n            if any(t in sl for t in tfu_tokens):\n                tfu_sheet = s\n                break\n        if tfu_sheet is None:\n            return 0.0\n        # Read without headers to scan content flexibly\n        raw = pd.read_excel(file_path, sheet_name=tfu_sheet, header=None, dtype=str)\n        raw = raw.fillna(\"\")\n        all_text = \" \".join(raw.astype(str).values.ravel()).lower()\n        # Check for mentions of overview and rationale\n        has_overview = \"overview\" in all_text\n        has_rationale = \"rationale\" in all_text\n        # Find a header row that contains condition and follow/time\n        header_row_idx = None\n        for i in range(min(25, raw.shape[0])):\n            row_vals = [str(v).strip().lower() for v in list(raw.iloc[i, :].values)]\n            row_text = \"|\".join(row_vals)\n            if (\"condition\" in row_text or \"diagnosis\" in row_text) and (\"follow\" in row_text or \"time\" in row_text or \"days\" in row_text):\n                header_row_idx = i\n                break\n        has_table = False\n        cond_coverage = 0\n        if header_row_idx is not None:\n            df = pd.read_excel(file_path, sheet_name=tfu_sheet, header=header_row_idx, dtype=str)\n            df = df.dropna(how='all').fillna(\"\")\n            cols = [str(c).strip().lower() for c in df.columns]\n            # Identify likely columns\n            cond_cols = [c for c in cols if (\"condition\" in c or \"diagnosis\" in c)]\n            time_cols = [c for c in cols if (\"follow\" in c or \"time\" in c or \"days\" in c)]\n            if cond_cols and time_cols:\n                has_table = True\n                cond_series = None\n                for c in cond_cols:\n                    s = df[c].astype(str).str.lower()\n                    # choose the first non-empty majority column\n                    if (s != \"\").mean() > 0.3:\n                        cond_series = s\n                        break\n                if cond_series is None:\n                    cond_series = df[cond_cols[0]].astype(str).str.lower()\n                # Check coverage of exemplar conditions\n                text_blob = \" \".join(list(cond_series.values))\n                exemplars = {\"cad\": [\"cad\", \"coronary\"], \"diabetes\": [\"diabetes\", \"dm\"], \"copd\": [\"copd\", \"chronic obstructive\"]}\n                for key, patterns in exemplars.items():\n                    if any(p in text_blob for p in patterns):\n                        cond_coverage += 1\n        # Score\n        score = 0.0\n        if has_overview:\n            score += 0.25\n        if has_rationale:\n            score += 0.25\n        if has_table:\n            score += 0.7\n        # coverage: up to 0.3 for 2-3 exemplars present\n        score += 0.3 * min(1.0, cond_coverage / 2.0)\n        return float(min(score, 1.5))\n    except Exception:\n        return 0.0\n"}, {"type": "code", "name": "CMS/ACO REACH Source Referencing", "description": "Verify the TFU sheet references CMS and the ACO REACH PY 2025 Quality Measurement Methodology Report (flexible phrasing).", "weight": 0.5, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n        # Find TFU-like sheet\n        tfu_tokens = [\"tfu\", \"timely\", \"follow\", \"cms\"]\n        tfu_sheet = None\n        for s in xls.sheet_names:\n            sl = s.lower()\n            if any(t in sl for t in tfu_tokens):\n                tfu_sheet = s\n                break\n        if tfu_sheet is None:\n            return 0.0\n        raw = pd.read_excel(file_path, sheet_name=tfu_sheet, header=None, dtype=str).fillna(\"\")\n        text = \" \".join(raw.astype(str).values.ravel()).lower()\n        has_cms = (\"cms.gov\" in text) or (\"cms\" in text)\n        has_aco_reach = (\"aco reach\" in text)\n        has_py2025 = (\"py 2025\" in text) or (\"2025\" in text)\n        # Scoring\n        if has_cms and has_aco_reach and has_py2025:\n            return 0.5\n        if (has_cms and has_aco_reach) or (has_cms and has_py2025):\n            return 0.3\n        if has_cms:\n            return 0.2\n        return 0.0\n    except Exception:\n        return 0.0\n"}, {"type": "code", "name": "Facilities Data Hygiene (Duplicates and Required Field Completeness)", "description": "Penalize heavy duplication of facility names and excessive missing values in required fields (name, address, phone).", "weight": 0.5, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0\n        file_path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(file_path)\n        fac_tokens = [\"facil\", \"contact\", \"hospital\", \"rehab\", \"long island\"]\n        fac_sheet = None\n        for s in xls.sheet_names:\n            sl = s.lower()\n            if any(t in sl for t in fac_tokens):\n                fac_sheet = s\n                break\n        if fac_sheet is None:\n            return 0.0\n        df = pd.read_excel(file_path, sheet_name=fac_sheet)\n        if df is None or df.shape[1] == 0:\n            return 0.0\n        cols = [str(c).strip().lower() for c in df.columns]\n        # Identify columns\n        name_idx = None\n        phone_idx = None\n        addr_idx = None\n        for i, c in enumerate(cols):\n            if name_idx is None and (\"name\" in c):\n                name_idx = i\n            if phone_idx is None and (\"phone\" in c or \"tel\" in c):\n                phone_idx = i\n            if addr_idx is None and (\"address\" in c):\n                addr_idx = i\n        # Fallbacks\n        if addr_idx is None:\n            # If no single address, consider presence of city/state/zip as address-proxy\n            addr_present = any(\"city\" in c for c in cols) and (any(\"state\" in c for c in cols) or any(\"zip\" in c or \"postal\" in c for c in cols))\n        else:\n            addr_present = True\n        # Required fields availability\n        required_ok = (name_idx is not None) and (phone_idx is not None) and addr_present\n        if not required_ok:\n            return 0.0\n        name_series = df.iloc[:, name_idx].astype(str).str.strip().str.lower()\n        phone_series = df.iloc[:, phone_idx].astype(str).str.strip()\n        if addr_idx is not None:\n            addr_series = df.iloc[:, addr_idx].astype(str).str.strip()\n        else:\n            # Compose pseudo-address from available pieces\n            parts = []\n            for c in df.columns:\n                cl = str(c).lower()\n                if any(k in cl for k in [\"address\", \"street\", \"city\", \"state\", \"zip\", \"postal\"]):\n                    parts.append(df[c].astype(str))\n            if parts:\n                addr_series = pd.concat(parts, axis=1).agg(\" \".join, axis=1)\n            else:\n                addr_series = pd.Series([\"\"] * len(df))\n        # Duplicates (by normalized name)\n        name_norm = name_series.str.replace(r\"\\s+\", \" \", regex=True)\n        dup_frac = 0.0\n        if len(name_norm) > 0:\n            dup_frac = 1.0 - name_norm.drop_duplicates().shape[0] / max(1, name_norm.shape[0])\n        # Missing required fields\n        phone_missing = phone_series.replace({\"nan\":\"\", \"None\":\"\"}).str.strip() == \"\"\n        addr_missing = addr_series.replace({\"nan\":\"\", \"None\":\"\"}).str.strip() == \"\"\n        name_missing = name_series.replace({\"nan\":\"\", \"none\":\"\"}).str.strip() == \"\"\n        any_missing = (phone_missing | addr_missing | name_missing)\n        miss_frac = float(any_missing.mean()) if len(any_missing) else 1.0\n        # Scoring (max 0.5)\n        score = 0.0\n        # Reward low duplication\n        if dup_frac <= 0.05:\n            score += 0.25\n        elif dup_frac <= 0.10:\n            score += 0.15\n        # Reward low missingness\n        if miss_frac <= 0.10:\n            score += 0.25\n        elif miss_frac <= 0.20:\n            score += 0.15\n        return float(min(score, 0.5))\n    except Exception:\n        return 0.0\n"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Usability and Presentation (LLM)", "description": "Holistic assessment of presentation quality and practical usability for case managers scheduling post-discharge follow-ups.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity, Formatting, and Practical Utility", "description": "LLM evaluates whether the workbook is professionally formatted, easy to use, and tailored to case managers\u2019 workflows.", "weight": 2.0, "judge_prompt": "Evaluate the overall professional quality and usability of the Excel workbook for a nurse case manager working in an ACO on Long Island. Consider only presentation and usefulness, not the correctness of clinical content.\n\nFacilities sheet:\n- Is the list reasonably complete-looking for Long Island and clearly organized (e.g., alphabetized, grouped, or filterable)?\n- Are columns clearly labeled and consistently formatted (e.g., phone formats consistent)?\n- Is it easy to scan for facility name, address, and phone? Are helpful extras included (Type, City/County, Website, Notes)?\n- Does it appear clean (minimal duplicates, blank rows, or clutter)?\n\nTFU Reference sheet:\n- Are sections (Overview, Rationale) concise and readable for quick reference?\n- Is the condition/timeframe table easy to scan (clear headers, consistent time units such as days, notes/source visible)?\n- Is the source citation to CMS/ACO REACH visible and clearly attributed?\n\nGeneral presentation:\n- Professional, readable formatting (headers, bolding, spacing, column widths). Logical sheet names. No obvious formatting issues.\n\nScoring:\n- 2.0: Highly professional and immediately usable; clear structure, consistent formatting, and helpful details beyond minimum requirements.\n- 1.3: Good quality; minor issues but overall usable and clear.\n- 0.7: Adequate but rough; noticeable inconsistencies or missing helpful details hinder usability.\n- 0.0: Poor presentation; hard to read/use or obviously unprofessional.\n\nDo not judge the factual correctness of clinical guidance\u2014only the usability and presentation.", "expectation": "A clean, well-formatted workbook: organized facility list with clear labels and consistent phone formatting; a readable TFU reference with scannable table and visible CMS citation."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "90f37ff3-e4ed-4a0b-94bb-bed0f7def1ef", "rubric": {"category_name": "Lease Rate Analysis Report (Commercial Retail) \u2014 Real Estate Sales Agents", "rationale": "This rubric enforces a self-documenting, verifiable 4-page Lease Rate Analysis Report in PDF/DOCX with a comp table and a recommendation range. Stage 1 (LLM-only) gates on strict structure and format so Stage 2 code rules can verify plausibility and consistency without brittle parsing. Stage 3 judges professional quality and strategic value for a landlord in Miami Gardens, FL.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate (LLM-only)", "description": "Gate: The output must be a 4-page, presentation-ready PDF/DOCX with the exact sections and comp-table structure required to enable verification.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 2.8, "rules": [{"type": "llm_judge", "name": "Format and Pagination", "description": "Validates file type and pagination; checks presentation-ready formatting suitable for a client-facing report.", "weight": 1.5, "judge_prompt": "You are the Stage 1 shape gate. Inspect the primary output.\n\nRequired format:\n- Must be a PDF or DOCX (not Excel, not plain text)\n- Target length: exactly 4 pages. Acceptable range: 3\u20135 pages if otherwise strong\n- Presentation-ready formatting: readable typography, clear headings, page structure\n\nScoring:\n- 1.5 = PDF/DOCX, exactly 4 pages, presentation-ready (clear sections and layout)\n- 1.0 = PDF/DOCX, 3\u20135 pages, presentation-ready overall\n- 0.5 = PDF/DOCX, poor formatting OR 2 or >5 pages\n- 0.0 = Not PDF/DOCX OR 1 page only\n\nReturn a score only based on format and pagination. Do not assess content quality.", "expectation": "A 4-page PDF with clear headings and professional layout."}, {"type": "llm_judge", "name": "Required Sections Present", "description": "Checks presence of mandated sections that enable verification of comps and recommendation.", "weight": 2.0, "judge_prompt": "Check the document for these REQUIRED sections with visible headers (flexible naming allowed):\n\n1) Title/Cover section (page 1) including all:\n   - Subject property identification and location (Miami Gardens, FL)\n   - Suite size: 2,225 SF\n2) Market Rent Survey section:\n   - A clearly tabular comp table with 3\u20136 comparable retail spaces\n   - Each comp row shows at minimum: property/address and asking rent in $/SF/yr (rate type like NNN or Gross if available)\n3) Lease Rate Recommendation section:\n   - A recommended rent range in $/SF/yr (e.g., $X\u2013$Y PSF/yr) and stated rate type if available\n4) Methodology/Assumptions & Sources section:\n   - Notes on data sources (e.g., LoopNet, Crexi) and selection criteria (\u22643-mile radius, retail, timeframe up to 3 years)\n\nHelpful/optional (do not penalize if missing when other requirements are met):\n- Distances or a small map/descriptor showing comps are within 3 miles\n- Dates or recency indicators for comps\n\nScoring:\n- 2.0 = All four required sections present as listed (flexible header names OK)\n- 1.5 = Missing one secondary element in the Market Rent Survey (e.g., rate type) but core requirements present\n- 1.0 = Missing one required section OR comp table exists but is not clearly 3\u20136 comps\n- 0.0 = Missing multiple required sections OR no comp table\n\nOnly judge presence/structure, not correctness.", "expectation": "All four required sections with clear headers and comp table present."}, {"type": "llm_judge", "name": "Comparable Table Integrity", "description": "Validates that the comp information is presented in a recognizable table with minimum required fields, enabling code verification later.", "weight": 0.5, "judge_prompt": "Focus only on the Market Rent Survey table.\n\nCheck:\n- It is presented as a table (grid or clearly tabular layout)\n- Contains at least these columns/fields: Property/Address and Asking Rent ($/SF/yr)\n- Preferably includes Size (SF), Distance from Subject, and Source/Date (acceptable if some of these are missing but core two are present)\n- Has 3\u20136 rows of unique comps\n\nScoring:\n- 0.5 = Clear table with 3\u20136 comps; each row has Property/Address and Asking Rent\n- 0.3 = Table present but weak (e.g., only 2 comps, or multiple rows lack either address or rent)\n- 0.0 = No recognizable comp table\n\nDo not assess accuracy of values\u2014just the presence of a usable table.", "expectation": "A comp table of 3\u20136 entries with address and asking rent fields present."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Code + LLM)", "description": "Verifies plausibility and internal consistency now that the structure exists.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Recommendation Range Plausibility", "description": "Extracts the recommended rent range ($/SF/yr) and checks for sane bounds and proper ordering.", "weight": 1.3, "code": "import re\nfrom datetime import datetime\n\ndef evaluate(workflow, context):\n    weight = 1.3\n    out = context.get_primary_output()\n    if not out or not out.is_document:\n        return 0.0\n    try:\n        text = \"\"\n        try:\n            if out.file_extension.lower().endswith('.pdf'):\n                text = context.files.read_pdf_text(out.id)\n            else:\n                text = context.files.read_docx_text(out.id)\n        except Exception:\n            text = context.files.read_docx_text(out.id)\n    except Exception:\n        return 0.0\n    if not text:\n        return 0.0\n    low_text = text.lower()\n    # Focus near the recommendation section if present\n    idx = low_text.find('recommend')\n    segment = text[max(idx-200:idx+1000, 0):] if idx != -1 else text\n    # Range pattern like $20 - $30 /SF/yr or $20 to $30 PSF\n    rng_pattern = re.compile(r\"\\$?\\s*([0-9]{1,3}(?:,\\d{3})?(?:\\.\\d{1,2})?)\\s*(?:-|to|\u2013|\u2014)\\s*\\$?\\s*([0-9]{1,3}(?:,\\d{3})?(?:\\.\\d{1,2})?)\\s*(?:/|per)?\\s*(?:sf|psf|square\\s*foot)\\s*(?:/|per)?\\s*(?:yr|year|ann?u?m)?\", re.IGNORECASE)\n    m = rng_pattern.search(segment)\n    if not m:\n        # fallback: search whole document\n        m = rng_pattern.search(text)\n    if not m:\n        return 0.0\n    try:\n        lo = float(m.group(1).replace(',', ''))\n        hi = float(m.group(2).replace(',', ''))\n    except Exception:\n        return 0.0\n    if hi <= lo:\n        return weight * 0.3\n    # Plausibility bounds for retail PSF/yr\n    plausible = (5 <= lo <= 150) and (10 <= hi <= 200)\n    if plausible:\n        return weight\n    # somewhat plausible\n    if (3 <= lo <= 250) and (lo < hi <= 300):\n        return weight * 0.6\n    return weight * 0.2"}, {"type": "code", "name": "Comparable Count Evidence", "description": "Heuristically checks for 3\u20136 comparable $/SF entries, prioritizing the Market Rent Survey section.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 1.0\n    out = context.get_primary_output()\n    if not out or not out.is_document:\n        return 0.0\n    try:\n        text = ''\n        if out.file_extension.lower().endswith('.pdf'):\n            text = context.files.read_pdf_text(out.id)\n        else:\n            text = context.files.read_docx_text(out.id)\n    except Exception:\n        return 0.0\n    lt = text.lower()\n    # Try to isolate Market Rent Survey section\n    start_idx = max(lt.find('market rent survey'), lt.find('comparable'))\n    if start_idx == -1:\n        segment = text\n    else:\n        segment = text[start_idx:start_idx+5000]\n    # Count per-SF mentions\n    psf_pattern = re.compile(r\"\\$\\s*\\d{1,3}(?:,\\d{3})?(?:\\.\\d{1,2})?\\s*(?:/|per)?\\s*(?:sf|psf)(?:\\s*/?\\s*(?:yr|year))?\", re.IGNORECASE)\n    matches = psf_pattern.findall(segment)\n    count = len(matches)\n    if 3 <= count <= 6:\n        return weight\n    if count == 2:\n        return weight * 0.7\n    if count == 1:\n        return weight * 0.3\n    if count > 6:\n        return weight * 0.6\n    return 0.0"}, {"type": "code", "name": "Distance and Recency Evidence", "description": "Checks for signals that comps are within 3 miles and within the last 3 years.", "weight": 0.8, "code": "import re\nfrom datetime import datetime\n\ndef evaluate(workflow, context):\n    weight = 0.8\n    out = context.get_primary_output()\n    if not out or not out.is_document:\n        return 0.0\n    try:\n        text = ''\n        if out.file_extension.lower().endswith('.pdf'):\n            text = context.files.read_pdf_text(out.id)\n        else:\n            text = context.files.read_docx_text(out.id)\n    except Exception:\n        return 0.0\n    lt = text.lower()\n    # Distance evidence\n    within_phrase = ('within 3 miles' in lt) or ('3-mile' in lt) or ('3 mile' in lt)\n    dist_vals = []\n    for m in re.finditer(r\"(\\d+(?:\\.\\d+)?)\\s*(?:mi|miles?)\", lt):\n        try:\n            dist_vals.append(float(m.group(1)))\n        except Exception:\n            pass\n    dist_ok = within_phrase or any(d <= 3.0 for d in dist_vals)\n    # Recency evidence\n    cur_year = datetime.utcnow().year\n    years = [int(y) for y in re.findall(r\"\\b(20\\d{2})\\b\", text)]\n    recent_ok = any(y >= cur_year - 3 for y in years) if years else False\n    if dist_ok and recent_ok:\n        return weight\n    if dist_ok or recent_ok:\n        return weight * 0.5\n    return 0.0"}, {"type": "code", "name": "Subject Suite Size Mention", "description": "Verifies that the 2,225 SF suite size is explicitly mentioned.", "weight": 0.2, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 0.2\n    out = context.get_primary_output()\n    if not out or not out.is_document:\n        return 0.0\n    try:\n        text = ''\n        if out.file_extension.lower().endswith('.pdf'):\n            text = context.files.read_pdf_text(out.id)\n        else:\n            text = context.files.read_docx_text(out.id)\n    except Exception:\n        return 0.0\n    lt = text.lower()\n    patterns = [r\"\\b2,225\\b\\s*(sf|sq\\s*ft|square\\s*feet)\", r\"\\b2225\\b\\s*(sf|sq\\s*ft|square\\s*feet)\"]\n    for p in patterns:\n        if re.search(p, lt):\n            return weight\n    return 0.0"}, {"type": "code", "name": "Recommendation vs. Comp Spread", "description": "Checks whether the recommended range sits within a reasonable band of the comp spread.", "weight": 0.4, "code": "import re\n\ndef _extract_text(context, out):\n    try:\n        if out.file_extension.lower().endswith('.pdf'):\n            return context.files.read_pdf_text(out.id)\n        return context.files.read_docx_text(out.id)\n    except Exception:\n        return ''\n\ndef _extract_rec_range(text):\n    lt = text.lower()\n    idx = lt.find('recommend')\n    segment = text[max(idx-200:idx+1200, 0):] if idx != -1 else text\n    rng_pattern = re.compile(r\"\\$?\\s*([0-9]{1,3}(?:,\\d{3})?(?:\\.\\d{1,2})?)\\s*(?:-|to|\u2013|\u2014)\\s*\\$?\\s*([0-9]{1,3}(?:,\\d{3})?(?:\\.\\d{1,2})?)\\s*(?:/|per)?\\s*(?:sf|psf|square\\s*foot)\\s*(?:/|per)?\\s*(?:yr|year|ann?u?m)?\", re.IGNORECASE)\n    m = rng_pattern.search(segment) or rng_pattern.search(text)\n    if not m:\n        return None\n    try:\n        lo = float(m.group(1).replace(',', ''))\n        hi = float(m.group(2).replace(',', ''))\n        if hi <= lo:\n            return None\n        return (lo, hi)\n    except Exception:\n        return None\n\ndef _extract_comp_values(text):\n    lt = text.lower()\n    # Try to focus on comps section\n    start_idx = lt.find('market rent survey')\n    if start_idx == -1:\n        start_idx = lt.find('comparable')\n    segment = text[start_idx:start_idx+6000] if start_idx != -1 else text\n    # Extract PSF single values and both sides of ranges\n    vals = []\n    # Ranges\n    for m in re.finditer(r\"\\$\\s*([0-9]{1,3}(?:,\\d{3})?(?:\\.\\d{1,2})?)\\s*(?:-|to|\u2013|\u2014)\\s*\\$\\s*([0-9]{1,3}(?:,\\d{3})?(?:\\.\\d{1,2})?)\\s*(?:/|per)?\\s*(?:sf|psf)\", segment, re.IGNORECASE):\n        try:\n            vals.append(float(m.group(1).replace(',', '')))\n            vals.append(float(m.group(2).replace(',', '')))\n        except Exception:\n            pass\n    # Singles\n    for m in re.finditer(r\"\\$\\s*([0-9]{1,3}(?:,\\d{3})?(?:\\.\\d{1,2})?)\\s*(?:/|per)?\\s*(?:sf|psf)(?:\\s*/?\\s*(?:yr|year))?\", segment, re.IGNORECASE):\n        try:\n            vals.append(float(m.group(1).replace(',', '')))\n        except Exception:\n            pass\n    # Deduplicate and sanitize\n    vals = [v for v in vals if 1 <= v <= 500]\n    if not vals:\n        return None\n    return (min(vals), max(vals))\n\ndef evaluate(workflow, context):\n    weight = 0.4\n    out = context.get_primary_output()\n    if not out or not out.is_document:\n        return 0.0\n    text = _extract_text(context, out)\n    if not text:\n        return 0.0\n    rec = _extract_rec_range(text)\n    comp_span = _extract_comp_values(text)\n    if not rec or not comp_span:\n        return 0.0\n    rec_lo, rec_hi = rec\n    comp_lo, comp_hi = comp_span\n    # Allow modest buffer around comps (+/- $5 PSF)\n    lo_ok = rec_lo >= (comp_lo - 5)\n    hi_ok = rec_hi <= (comp_hi + 5)\n    if lo_ok and hi_ok:\n        return weight\n    if lo_ok or hi_ok:\n        return weight * 0.5\n    return 0.0"}, {"type": "llm_judge", "name": "Cross-reference Consistency (LLM)", "description": "LLM checks if the stated recommendation clearly ties back to the comps and uses consistent units/rate type.", "weight": 0.3, "judge_prompt": "Review the document holistically:\n- Does the Lease Rate Recommendation clearly derive from the Market Rent Survey (e.g., stated rationale tied to the comp figures)?\n- Are units consistent (e.g., $/SF/yr or PSF/yr) and is a rate type (NNN/Gross) indicated consistently across comps and the recommendation where available?\n\nScoring:\n- 0.3 = Clear linkage: recommendation is justified by the comp table; units and rate type are consistent\n- 0.2 = Mostly linked but minor inconsistencies\n- 0.1 = Weak linkage or unclear units\n- 0.0 = No apparent linkage; units inconsistent\n\nOnly judge consistency and linkage, not presentation quality.", "expectation": "Recommendation aligns with comps; units and rate type are consistent."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Strategic Value (LLM)", "description": "Assesses presentation polish and strategic usefulness for the landlord\u2019s pricing decision.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Executive Quality and Landlord-Facing Value", "description": "Evaluates clarity, visuals, and actionable recommendations tailored to leasing strategy in Miami Gardens, FL.", "weight": 2.0, "judge_prompt": "Evaluate overall professional quality and strategic usefulness:\n- Clarity: concise, well-structured narrative; clean tables; readable figures\n- Visuals: any helpful map/schematic or distances; consistent typography\n- Consistency: coherent units ($/SF/yr), NNN vs Gross labeling, no contradictions\n- Strategic value: actionable pricing guidance (recommended range with rationale), suggested listing vs target, notes on concessions/TI if relevant, positioning to attract qualified tenants in Miami Gardens, FL\n\nScoring:\n- 2.0 = Polished, client-ready, highly actionable strategy\n- 1.5 = Strong overall with minor polish issues\n- 1.0 = Adequate but limited strategic depth or polish\n- 0.5 = Weak clarity or minimal landlord guidance\n- 0.0 = Unprofessional or not useful\n\nFocus on presentation quality and strategic value, not re-verifying structure.", "expectation": "Concise, polished, and actionable landlord-facing recommendations with clear rationale."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "36d567ba-e205-4313-9756-931c6e4691fe", "rubric": {"category_name": "Government - Compliance Officers: Pre-Award Risk Assessment Tool (Pre-Award Risk Screening)", "rationale": "This rubric enforces a self-documenting, file-based workflow for a federally appropriate, pre-award risk assessment question set. Stage 1 (LLM-only) strictly enforces the required DOCX format, 1\u20132 page length, title, topic coverage, two-part structure, and Uniform Guidance citations for topics 6\u201310. Stage 2 uses code rules to verify structural and content signals programmatically (title match, topic coverage, two-part structure heuristics, Uniform Guidance citation presence). Stage 3 applies an LLM quality assessment on clarity, usability, professionalism, and federal compliance orientation.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (LLM Only)", "description": "Gate: The output must be a Word document (DOCX) titled \"Federal Applicant - Risk Assessment Tool\", professionally structured into the 11 required topics with the requested two-part question pattern (except topic 7) and Uniform Guidance references embedded in topics 6\u201310. Length must be 1\u20132 pages.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Document Requirements (DOCX, Title, Topics, Two-Part Pattern, UG Citations)", "description": "Verify the candidate output is a DOCX with correct title, length, sections, two-part questions, and 2 CFR Part 200 citations for topics 6\u201310.", "weight": 4.0, "judge_prompt": "You are validating a single candidate output file produced by an agent. Evaluate ONLY the visible file content and format.\n\nTask: The supervisor requested a Word document titled \u201cFederal Applicant - Risk Assessment Tool\u201d that is 1\u20132 pages, with specific topics and question structure for a pre-award risk assessment.\n\nShape/Format requirements to check (be flexible in headings/wording but strict about presence):\n1) File format: Must be a Word DOCX (not PDF, not plain text, not Excel).  \n2) Title: Top of document includes the title or a near-exact variant: \u201cFederal Applicant - Risk Assessment Tool\u201d (minor punctuation or dash variations OK).  \n3) Length: 1\u20132 pages total.  \n4) Intro/instructions: Brief introductory instruction(s) (1\u20133 sentences) explaining how to answer.  \n5) Topics: Eleven numbered topics appear as headings or clearly delineated sections:  \n   1. Tracking multiple sources of revenue/funding separately  \n   2. Written accounting policies and procedures  \n   3. Financial Management System \u2013 tracking expenditures  \n   4. Timing of federal payments and disbursement of funds  \n   5. Internal controls  \n   6. Records retention  \n   7. Conflicts of interest  \n   8. Applicant point person\u2019s knowledge of federal requirements  \n   9. Subaward management and monitoring  \n   10. Timekeeping  \n   11. High-risk status with federal agencies  \n   - Exact phrasing may vary, but each topic must be clearly and unambiguously present.  \n6) Two-part question pattern: For topics 1\u20136 and 8\u201311, each topic includes a two-part prompt in one or more sentences:  \n   - Part A: A Yes/No\u2013style lead (the applicant can start the response with \u201cYes\u201d or \u201cNo\u201d).  \n   - Part B: An open-ended follow-up requesting details (e.g., \u201cdescribe,\u201d \u201cexplain,\u201d \u201cprovide details\u201d).  \n   - Topic 7 (Conflicts of interest) may be single-part but must still solicit disclosure; two-part is acceptable but not required.  \n7) Uniform Guidance references: Topics 6\u201310 each contain a citation to the Uniform Guidance (2 CFR Part 200) relevant section(s). Accept formats like \u201c2 CFR 200.xxx\u201d or \u201c2 CFR \u00a7 200.xxx.\u201d  \n\nScoring (return a single numeric score from 0 to 4):\n- 4.0: DOCX; proper title at top; 1\u20132 pages; all 11 topics present; topics 1\u20136 and 8\u201311 use a clear Yes/No lead plus an open-ended follow-up; topic 7 present (one- or two-part acceptable); topics 6\u201310 each include a visible 2 CFR Part 200 citation.\n- 3.0\u20133.5: Minor deviations only (e.g., one minor phrasing or formatting issue), but all core structural requirements are met (DOCX, title, 1\u20132 pages, all topics, two-part pattern except topic 7, and UG citations in 6\u201310).\n- 2.0\u20132.9: Missing one core element (e.g., one topic missing, or two-part pattern missing in one applicable topic, or a missing UG citation in one of topics 6\u201310) but otherwise in correct DOCX format with title and near-correct structure.\n- 0.5\u20131.9: Multiple structural gaps (e.g., two or three topics missing; several two-part omissions; multiple missing UG citations), but still a DOCX with the intended title and an attempt at the required structure.\n- 0.0: Not a DOCX; or lacks the correct title; or not 1\u20132 pages; or missing many topics/structure such that verification is not possible.\n\nOnly assess structure and format\u2014not the substantive quality of the writing or legal correctness of citations.\n", "expectation": "A one- to two-page DOCX titled as specified with 11 clearly labeled topics, two-part prompts for all but topic 7, and 2 CFR Part 200 citations embedded in topics 6\u201310."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Code + Heuristics)", "description": "Programmatic checks leveraging the enforced shape: title presence, topic coverage, two-part Yes/No + detail structure detection, and embedded Uniform Guidance citations for topics 6\u201310. Flexible matching and robust error handling are used.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "DOCX File Confirmation", "description": "Light verification that the primary output is a DOCX file.", "weight": 0.3, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output provided.\"\n    if not output.is_document:\n        return 0.0, \"Output is not a document.\"\n    try:\n        path = context.files.get_path(output.id)\n        is_docx = str(path).lower().endswith('.docx')\n        return (0.3 if is_docx else 0.0, \"DOCX detected.\" if is_docx else \"Not a DOCX file.\")\n    except Exception as e:\n        return 0.0, f\"Error checking file extension: {e}\""}, {"type": "code", "name": "Title Presence and Match", "description": "Checks for the presence of \u201cFederal Applicant\u201d and \u201cRisk Assessment Tool\u201d in the title/body.", "weight": 0.3, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = \"\"\n    try:\n        path = context.files.get_path(output.id)\n        if str(path).lower().endswith('.docx'):\n            text = context.files.read_docx_text(output.id)\n        else:\n            text = context.files.read_pdf_text(output.id)\n    except Exception:\n        text = \"\"\n    t = (text or \"\").lower()\n    ok = (\"federal applicant\" in t) and (\"risk assessment tool\" in t)\n    return (0.3 if ok else 0.0, \"Title signal found.\" if ok else \"Expected title phrase(s) not found.\")"}, {"type": "code", "name": "Topic Coverage (11 required topics)", "description": "Detects whether each of the 11 required topic areas is present via flexible keyword grouping. Awards proportional credit.", "weight": 2.0, "code": "import re\n\ndef match_groups(text, groups):\n    # Each group must have at least one term present (substring match)\n    for group in groups:\n        if not any(term in text for term in group):\n            return False\n    return True\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document to evaluate.\"\n    try:\n        path = context.files.get_path(output.id)\n        if str(path).lower().endswith('.docx'):\n            raw = context.files.read_docx_text(output.id)\n        else:\n            raw = context.files.read_pdf_text(output.id)\n    except Exception as e:\n        return 0.0, f\"Error reading document: {e}\"\n\n    text = (raw or \"\").lower()\n\n    topics = {\n        1: [[\"multiple\",\"multi\"],[\"source\",\"sources\",\"funding\",\"revenue\"],[\"separate\",\"separately\",\"segregat\",\"distinct\"]],\n        2: [[\"written\",\"documented\",\"formal\"],[\"accounting\"],[\"policies\",\"policy\",\"procedures\",\"procedure\"]],\n        3: [[\"financial management system\",\"fms\",\"erp\",\"accounting system\",\" system\"],[\"track\",\"tracking\",\"record\",\"recording\"],[\"expenditure\",\"expenses\",\"costs\",\"outlays\",\"disburse\"]],\n        4: [[\"timing\",\"schedule\",\"promptness\"],[\"federal\",\"grant\"],[\"payment\",\"payments\",\"drawdown\",\"draw down\",\"disbursement\",\"disbursements\"]],\n        5: [[\"internal control\",\"internal controls\",\"control environment\",\"segregation of duties\",\"segregation\"]],\n        6: [[\"record\",\"records\"],[\"retention\",\"retain\",\"retaining\",\"maintenance\"]],\n        7: [[\"conflict of interest\",\"conflicts of interest\",\" coi \"]],\n        8: [[\"point person\",\"project lead\",\"program manager\",\"designated contact\",\"responsible official\",\"authorized representative\"],[\"knowledge\",\"understanding\",\"familiarity\",\"training\"],[\"federal requirement\",\"uniform guidance\",\"2 cfr\",\"grant regulation\"]],\n        9: [[\"subaward\",\"sub-recipient\",\"subrecipient\",\"sub recipient\"],[\"management\",\"monitoring\",\"oversight\",\"pass-through\",\"passthrough\"]],\n        10: [[\"timekeeping\",\"time and effort\",\"timesheet\",\"personnel activity report\",\" par \"]],\n        11: [[\"high-risk\",\"high risk\",\"special conditions\",\"poor performance\"],[\"federal\",\"agency\",\"agencies\",\"awarding agency\",\"grantor\"]]\n    }\n\n    found = []\n    for i, groups in topics.items():\n        found.append(1 if match_groups(text, groups) else 0)\n\n    count = sum(found)\n    score = (count / 11.0) * 2.0\n    return score, f\"Topics detected: {count}/11\""}, {"type": "code", "name": "Two-Part Question Pattern Detection (Yes/No + details)", "description": "Checks that topics 1\u20136 and 8\u201311 are likely two-part: a Yes/No lead plus an open-ended request for details near the topic reference.", "weight": 0.8, "code": "import re\n\ndef find_index(text, terms):\n    idxs = [text.find(t) for t in terms if t in text]\n    idxs = [i for i in idxs if i >= 0]\n    return min(idxs) if idxs else -1\n\ndef window(text, idx, pad_before=300, pad_after=600):\n    if idx < 0:\n        return \"\"\n    start = max(0, idx - pad_before)\n    end = min(len(text), idx + pad_after)\n    return text[start:end]\n\ndef has_two_part(win):\n    if not win:\n        return False\n    yesno = bool(re.search(r\"\\b(yes\\s*/\\s*no|yes\\s+or\\s+no)\\b\", win)) or bool(re.search(r\"\\byes\\b.{0,60}\\bno\\b|\\bno\\b.{0,60}\\byes\\b\", win))\n    detail = bool(re.search(r\"\\b(describe|explain|provide|detail|elaborate|outline|identify)\\b\", win))\n    return yesno and detail\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document to evaluate.\"\n    try:\n        path = context.files.get_path(output.id)\n        if str(path).lower().endswith('.docx'):\n            raw = context.files.read_docx_text(output.id)\n        else:\n            raw = context.files.read_pdf_text(output.id)\n    except Exception as e:\n        return 0.0, f\"Error reading document: {e}\"\n\n    text = (raw or \"\").lower()\n\n    topic_terms = {\n        1: [\"multiple\",\"multi\",\"sources\",\"funding\",\"revenue\",\"separate\",\"segregat\"],\n        2: [\"written\",\"documented\",\"formal\",\"accounting\",\"policies\",\"procedures\"],\n        3: [\"financial management system\",\"fms\",\"erp\",\"accounting system\",\"track\",\"expenditure\",\"disburse\"],\n        4: [\"timing\",\"federal\",\"payment\",\"drawdown\",\"disbursement\"],\n        5: [\"internal control\",\"control environment\",\"segregation of duties\"],\n        6: [\"records\",\"record\",\"retention\",\"retain\"],\n        8: [\"point person\",\"project lead\",\"program manager\",\"designated contact\",\"responsible official\",\"authorized representative\",\"federal requirement\",\"uniform guidance\",\"2 cfr\"],\n        9: [\"subaward\",\"subrecipient\",\"sub-recipient\",\"sub recipient\",\"monitoring\",\"oversight\"],\n        10: [\"timekeeping\",\"time and effort\",\"timesheet\",\"personnel activity report\"],\n        11: [\"high-risk\",\"high risk\",\"special conditions\",\"federal agency\",\"agencies\",\"grantor\"]\n    }\n\n    required_topics = [1,2,3,4,5,6,8,9,10,11]\n    checks = []\n    for t in required_topics:\n        idx = find_index(text, topic_terms.get(t, []))\n        win = window(text, idx)\n        checks.append(1 if has_two_part(win) else 0)\n\n    ok = sum(checks)\n    score = (ok / len(required_topics)) * 0.8\n    return score, f\"Two-part pattern detected for {ok}/{len(required_topics)} required topics.\""}, {"type": "code", "name": "Uniform Guidance Citations Present for Topics 6\u201310", "description": "Checks that each of topics 6\u201310 includes a nearby Uniform Guidance reference (e.g., \u201c2 CFR 200.xxx\u201d). Proportional credit.", "weight": 0.5, "code": "import re\n\ndef find_index(text, terms):\n    idxs = [text.find(t) for t in terms if t in text]\n    idxs = [i for i in idxs if i >= 0]\n    return min(idxs) if idxs else -1\n\ndef window(text, idx, pad_before=300, pad_after=600):\n    if idx < 0:\n        return \"\"\n    start = max(0, idx - pad_before)\n    end = min(len(text), idx + pad_after)\n    return text[start:end]\n\ndef has_ug_citation(win):\n    if not win:\n        return False\n    # Accept common variants like: 2 CFR 200.334, 2 CFR \u00a7 200.318, 2 CFR Part 200\n    return bool(re.search(r\"2\\s*cfr[^\\n\\r]{0,30}200(\\.|\\s|\u00a7|part)[^\\n\\r]{0,15}\\d*\", win, flags=re.IGNORECASE))\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document to evaluate.\"\n    try:\n        path = context.files.get_path(output.id)\n        if str(path).lower().endswith('.docx'):\n            raw = context.files.read_docx_text(output.id)\n        else:\n            raw = context.files.read_pdf_text(output.id)\n    except Exception as e:\n        return 0.0, f\"Error reading document: {e}\"\n\n    text = (raw or \"\").lower()\n\n    topic_terms = {\n        6: [\"records retention\",\"record retention\",\"retain records\",\"records\"],\n        7: [\"conflict of interest\",\"conflicts of interest\",\" coi \"],\n        8: [\"point person\",\"project lead\",\"program manager\",\"responsible official\",\"authorized representative\"],\n        9: [\"subaward\",\"subrecipient\",\"sub-recipient\",\"sub recipient\"],\n        10: [\"timekeeping\",\"time and effort\",\"timesheet\",\"personnel activity report\"]\n    }\n\n    targets = [6,7,8,9,10]\n    hits = 0\n    for t in targets:\n        idx = find_index(text, topic_terms.get(t, []))\n        win = window(text, idx)\n        if has_ug_citation(win):\n            hits += 1\n    score = (hits / len(targets)) * 0.5\n    return score, f\"UG citations detected for {hits}/5 topics (6\u201310).\""}, {"type": "code", "name": "Conflict of Interest Presence (Topic 7)", "description": "Ensures a conflict-of-interest prompt is present and appears interrogative or directive (single- or two-part acceptable).", "weight": 0.1, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        if str(path).lower().endswith('.docx'):\n            raw = context.files.read_docx_text(output.id)\n        else:\n            raw = context.files.read_pdf_text(output.id)\n    except Exception:\n        raw = \"\"\n    text = (raw or \"\").lower()\n    present = (\"conflict of interest\" in text) or (\"conflicts of interest\" in text) or (\" coi \" in text)\n    if not present:\n        return 0.0, \"Conflict of interest topic not found.\"\n    # Basic question/command check near the phrase\n    idx = -1\n    for term in [\"conflict of interest\",\"conflicts of interest\",\" coi \"]:\n        if term in text:\n            idx = text.find(term)\n            break\n    start = max(0, idx - 200)\n    end = min(len(text), idx + 500)\n    win = text[start:end]\n    looks_question = (\"?\" in win) or bool(re.search(r\"\\b(disclose|describe|explain|provide|identify|certify)\\b\", win))\n    return (0.1 if looks_question else 0.0, \"Conflict of interest prompt detected.\" if looks_question else \"COI present but not clearly prompting a response.\")"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Professionalism (LLM)", "description": "Holistic LLM assessment of clarity, usability, and professional presentation for a federal grants pre-award risk assessment context.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity, Usability, and Professional Tone", "description": "Evaluate whether the tool is concise, clearly worded for applicants across entity types, and professionally formatted for federal use; confirm Yes/No leads are unambiguous and follow-ups solicit actionable detail; verify that Uniform Guidance citations are legible and not misleading.", "weight": 2.0, "judge_prompt": "Judge the professional quality of the provided DOCX tool for pre-award risk assessment. Focus on clarity, usability, and suitability for federal grants administration. Do NOT re-check Stage 1 shape; assume it passed. Assess:\n- Clarity and plain language: Questions are clear for varied applicants (IHEs, nonprofits, local govts). Minimal jargon or jargon is explained.\n- Usability: Yes/No part is unambiguous; follow-up prompts elicit actionable, specific detail. Flow is scannable.\n- Professional presentation: Formatting is consistent; numbering/headers are clean; spacing is readable within 1\u20132 pages; references to 2 CFR Part 200 are legible and placed near relevant topics.\n- Compliance orientation: Prompts reflect federal expectations (internal controls, subrecipient monitoring, etc.) without overreach or misleading statements.\n\nScoring (0\u20132):\n- 2.0: Highly clear, concise, and professional; usable without edits; references are helpful and properly placed.\n- 1.0\u20131.5: Generally clear and professional with minor edits suggested for usability or presentation.\n- 0.0\u20130.5: Hard to use or unclear; formatting distracts; follow-ups not actionable or references confusing.\n", "expectation": "A concise, professionally formatted, applicant-friendly tool with explicit Yes/No leads, strong follow-up prompts, and clearly placed 2 CFR references."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "105f8ad0-8dd2-422f-9e88-2be5fbd2b215", "rubric": {"category_name": "Excel Pricing Model: Luxury Men's Fragrance MSRPs (Wholesale Trade)", "rationale": "This rubric enforces a self-documenting, verification-friendly Excel model for benchmarking competitor MSRPs and recommending new SKU prices. Stage 1 (LLM gate) mandates a precise workbook structure so verification is trivial. Stage 2 uses code rules to check correctness: inclusion criteria adherence, proper aggregations, \u00b16% compliance vs competitor averages, and COGS-aligned pricing with concentration premiums. Stage 3 assesses professional quality and strategic clarity for a premium rebranding context.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Structured Output Format Requirement (GATE)", "description": "LLM-only shape enforcement. Output must be a single Excel workbook with specified sheets and sections enabling deterministic verification of pricing logic and compliance with inclusion criteria.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Workbook Shape and Section Presence", "description": "Check that the candidate produced an Excel workbook with all required sheets, tables, and fields to enable verification of competitor benchmarking and recommended MSRPs.", "weight": 4.0, "judge_prompt": "You are verifying ONLY the structure and format (not the correctness of numbers) of the delivered file. Be flexible with exact names, but the content must clearly match these requirements.\n\nFormat Requirements:\n- Must be a valid Excel workbook (.xlsx). Not PDF/DOCX/CSV.\n- Professionally organized with tab names and labeled tables.\n\nRequired Sheets and Minimum Contents:\n1) SKU List (or similar: \"SKU Input\", \"Products\")\n   - Table columns (or clear equivalents): [SKU | Product Name | Size (oz) | Concentration (EDP/EDT/Elixir) | Current MSRP (USD) | COGS (USD)]\n   - Sizes recorded in ounces (oz) or convertible (ml noted) and clearly labeled per SKU.\n\n2) Competitor Data (or similar: \"Benchmark\", \"Market Scan\")\n   - Only fragrances sold at Macy\u2019s, Ulta, or Sephora (brand-site MSRPs prioritized if present).\n   - Excludes gift sets, refills, limited editions, and multi-packs.\n   - Table with columns (or clear equivalents): [Brand | Product Name | Concentration | Size (oz) | Distribution Channel (Macy\u2019s/Ulta/Sephora/Brand) | MSRP (USD) | Price per oz (USD/oz) | As-of Date (must reference September 2025) | Source URL | Eligibility/Exclusion Notes]\n\n3) Competitive Averages (or similar: \"Averages\", \"Segment Benchmarks\")\n   - Grouped averages by Size Range bucket AND Concentration with at least these four size buckets:\n     \u2022 Travel 0.30\u20131.4 oz\n     \u2022 1.5\u20132.9 oz\n     \u2022 3.0\u20134.2 oz\n     \u2022 4.3\u20136.8 oz\n   - Table columns (or equivalents): [Size Range | Concentration | Avg Price/oz | Count] (Min/Max/Std Dev optional but helpful).\n\n4) Recommendations (or similar: \"Pricing Model\", \"MSRP Recommendations\")\n   - One row per SKU with columns (or equivalents): [SKU | Size (oz) | Concentration | Current MSRP | COGS | Competitor Avg Price/oz (matched segment) | Recommended Price/oz | Recommended MSRP | Deviation vs Avg (%) | MSRP/COGS multiple | Gross Margin % | Rationale (brief sentence) | Within \u00b16%?]\n\n5) Assumptions & Methodology (or similar)\n   - A readable text section (at least 5 bullet points or 5+ sentences) covering:\n     \u2022 Inclusion criteria (channels, formats EDP/EDT/Elixir)\n     \u2022 Size bucketing logic (the four ranges above)\n     \u2022 Exclusions (gift sets, refills, limited editions, multipacks)\n     \u2022 Source prioritization (brand-site MSRPs; otherwise retailer pages)\n     \u2022 As-of date explicitly referencing September 2025 and non-sale/regular prices\n\nOptional (scores not penalized if missing):\n6) Checks/QA (or similar) with summary flags of compliance (e.g., % within \u00b16%).\n\nScoring (STRUCTURE ONLY):\n- 4.0: Excel workbook present with all 5 required sheets and the required fields/sections clearly visible; optional QA/Checks may or may not be present.\n- 3.5: Excel present; all required sheets found but one sheet is missing 1-2 minor fields; overall structure still enables verification.\n- 3.0: Excel present; exactly one required sheet missing OR multiple sheets exist but key fields are missing such that verification is constrained.\n- 1.5: Excel present but only 2 of the required sheets with minimal fields.\n- 0.0: Not Excel, or grossly incorrect structure (missing multiple required sheets), or the content does not reflect the task at all.\n\nOnly evaluate presence/format/structure. Do NOT assess numeric correctness, logic quality, or business judgment here.", "expectation": "A single .xlsx with the five required sheets (SKU List, Competitor Data, Competitive Averages, Recommendations, Assumptions & Methodology) and clearly labeled tables enabling verification."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification of Calculations and Compliance", "description": "Deterministic checks using code. Verifies inclusion criteria, segment averages, \u00b16% rule, and COGS-consistent, concentration-aware pricing.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Structural Data Integrity + Price-per-oz Calculation", "description": "Verify key sheets/columns exist with flexible matching and that competitor price-per-oz aligns with MSRP/Size.", "weight": 1.2, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    weight = 1.2\n    out = context.get_primary_output()\n    if not out or not out.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(out.id)\n        xls = pd.ExcelFile(path)\n        sheet_names = xls.sheet_names\n        s_lower = [s.lower() for s in sheet_names]\n\n        def find_sheet(cands):\n            for i, s in enumerate(s_lower):\n                for c in cands:\n                    if c in s:\n                        return sheet_names[i]\n            return None\n\n        sku_sh = find_sheet([\"sku\", \"product\", \"input\"])\n        comp_sh = find_sheet([\"compet\", \"benchmark\", \"market\", \"scan\"])\n        avg_sh  = find_sheet([\"average\", \"benchmark\", \"segment\", \"pivot\"])\n        rec_sh  = find_sheet([\"recommend\", \"pricing\", \"msrp\"])\n        asm_sh  = find_sheet([\"assumption\", \"method\"])\n\n        present = [sku_sh is not None, comp_sh is not None, avg_sh is not None, rec_sh is not None, asm_sh is not None]\n        base_cov = sum(present)/5.0\n\n        # Column fuzzy finders\n        def norm_cols(df):\n            return [str(c).strip() for c in df.columns]\n        def find_col(df, cand_list):\n            cols = norm_cols(df)\n            low = [c.lower() for c in cols]\n            for i,c in enumerate(low):\n                for cand in cand_list:\n                    if cand in c:\n                        return cols[i]\n            return None\n\n        cpz_ok = 0.0\n        if comp_sh is not None:\n            comp = xls.parse(comp_sh)\n            msrp_c = find_col(comp, [\"msrp\", \"price\"])\n            size_c = find_col(comp, [\"size\", \"oz\", \"ounce\"])\n            cpz_c  = find_col(comp, [\"price per oz\", \"price/oz\", \"ppoz\", \"$/oz\"]) \n            if msrp_c and size_c and cpz_c and len(comp) > 0:\n                def parse_size(v):\n                    if pd.isna(v):\n                        return np.nan\n                    s = str(v).lower()\n                    # extract number and unit\n                    num = None\n                    m = re.search(r\"([0-9]*\\.?[0-9]+)\", s)\n                    if m:\n                        num = float(m.group(1))\n                    if num is None:\n                        return np.nan\n                    if \"ml\" in s:\n                        return num/29.5735\n                    else:\n                        return num\n                tmp = comp[[msrp_c, size_c, cpz_c]].copy()\n                tmp[\"_msrp\"] = pd.to_numeric(tmp[msrp_c], errors=\"coerce\")\n                tmp[\"_size\"] = tmp[size_c].apply(parse_size)\n                tmp[\"_cpz\"]  = pd.to_numeric(tmp[cpz_c], errors=\"coerce\")\n                tmp = tmp.replace([np.inf, -np.inf], np.nan).dropna(subset=[\"_msrp\", \"_size\", \"_cpz\"])\n                tmp = tmp[tmp[\"_msrp\"] > 0]\n                tmp = tmp[tmp[\"_size\"] > 0]\n                if len(tmp) > 3:\n                    tmp[\"_calc\"] = tmp[\"_msrp\"] / tmp[\"_size\"]\n                    # relative error\n                    rel_err = (tmp[\"_calc\"] - tmp[\"_cpz\"]).abs() / tmp[\"_calc\"].replace(0, np.nan)\n                    med_err = float(rel_err.median()) if len(rel_err) else 1.0\n                    if med_err <= 0.03:\n                        cpz_ok = 1.0\n                    elif med_err <= 0.10:\n                        cpz_ok = 0.5\n                    else:\n                        cpz_ok = 0.0\n            else:\n                # If CPZ column is missing but structure present, partial credit\n                if msrp_c and size_c:\n                    cpz_ok = 0.4\n        # Combine: structure (60%), cpz check (40%)\n        score01 = 0.6*base_cov + 0.4*cpz_ok\n        score = max(0.0, min(1.0, score01)) * weight\n        feedback = f\"Sheets present={sum(present)}/5; CPZ check={cpz_ok:.2f}\"\n        return score, feedback\n    except Exception as e:\n        return 0.0, f\"Error reading workbook: {e}\""}, {"type": "code", "name": "Inclusion Criteria Compliance (Channels, Concentrations, Exclusions)", "description": "Check competitor rows are from allowed channels (Macy\u2019s/Ulta/Sephora/Brand site), concentrations are EDP/EDT/Elixir, and excluded items (gift sets, refills, limited, multipacks) are minimal.", "weight": 0.8, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    weight = 0.8\n    out = context.get_primary_output()\n    if not out or not out.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(out.id)\n        xls = pd.ExcelFile(path)\n        sheet_names = xls.sheet_names\n        s_lower = [s.lower() for s in sheet_names]\n        def find_sheet(cands):\n            for i, s in enumerate(s_lower):\n                for c in cands:\n                    if c in s:\n                        return sheet_names[i]\n            return None\n        comp_sh = find_sheet([\"compet\", \"benchmark\", \"market\", \"scan\"])\n        if comp_sh is None:\n            return 0.0, \"Missing competitor sheet\"\n        comp = xls.parse(comp_sh)\n        def find_col(df, cand_list):\n            cols = [str(c) for c in df.columns]\n            low = [c.lower().strip() for c in cols]\n            for i,c in enumerate(low):\n                for cand in cand_list:\n                    if cand in c:\n                        return cols[i]\n            return None\n        ch_c = find_col(comp, [\"channel\", \"retail\", \"source\", \"site\"])\n        conc_c = find_col(comp, [\"concentration\", \"format\", \"conc\", \"type\"])\n        name_c = find_col(comp, [\"product\", \"name\", \"title\"])\n        # Channel compliance\n        ch_ok = 0.0\n        if ch_c:\n            allowed = {\"macys\", \"macy\", \"macy\u2019s\", \"macy's\", \"ulta\", \"sephora\", \"brand\", \"brand site\", \"direct\"}\n            vals = comp[ch_c].dropna().astype(str).str.lower()\n            def norm(v):\n                return re.sub(r\"[^a-z]\", \"\", v)\n            allowed_norm = {norm(v) for v in allowed}\n            good = vals.apply(lambda v: norm(v) in allowed_norm)\n            if len(vals) > 0:\n                frac = good.mean()\n                ch_ok = 1.0 if frac >= 0.95 else (0.5 if frac >= 0.80 else 0.0)\n        # Concentration compliance\n        conc_ok = 0.0\n        if conc_c:\n            vals = comp[conc_c].dropna().astype(str).str.lower()\n            def normc(v):\n                v = v.replace(\"eau de parfum\", \"edp\").replace(\"eau de toilette\", \"edt\")\n                v = v.replace(\"parfum\", \"edp\")\n                return re.sub(r\"[^a-z]\", \"\", v)\n            norm_vals = vals.apply(normc)\n            allowed = {\"edp\", \"edt\", \"elixir\"}\n            frac = norm_vals.apply(lambda v: any(a in v for a in allowed)).mean() if len(norm_vals) else 0\n            conc_ok = 1.0 if frac >= 0.95 else (0.5 if frac >= 0.80 else 0.0)\n        # Exclusions\n        excl_ok = 0.0\n        if name_c:\n            vals = comp[name_c].dropna().astype(str).str.lower()\n            bad_terms = [\"set\", \"gift\", \"refill\", \"limited\", \"mini set\", \"bundle\", \"duo\", \"trio\", \"value set\", \"multi-pack\", \"multipack\", \"coffret\"]\n            bad = vals.apply(lambda v: any(bt in v for bt in bad_terms))\n            frac_bad = bad.mean() if len(bad) else 0\n            excl_ok = 1.0 if frac_bad <= 0.02 else (0.5 if frac_bad <= 0.10 else 0.0)\n        parts = [p for p in [ch_ok, conc_ok, excl_ok] if p is not None]\n        if not parts:\n            return 0.0\n        score = (sum(parts)/len(parts)) * weight\n        fb = f\"Channel={ch_ok:.2f}, Concentration={conc_ok:.2f}, Exclusions={excl_ok:.2f}\"\n        return score, fb\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Competitive Averages Coverage by Size Buckets", "description": "Verify the averages table covers the four size buckets across concentrations with numeric Avg Price/oz.", "weight": 0.7, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    weight = 0.7\n    out = context.get_primary_output()\n    if not out or not out.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(out.id)\n        xls = pd.ExcelFile(path)\n        sheet_names = xls.sheet_names\n        s_lower = [s.lower() for s in sheet_names]\n        def find_sheet(cands):\n            for i, s in enumerate(s_lower):\n                for c in cands:\n                    if c in s:\n                        return sheet_names[i]\n            return None\n        avg_sh = find_sheet([\"average\", \"segment\", \"pivot\", \"benchmark\"])\n        if avg_sh is None:\n            return 0.0, \"Missing averages sheet\"\n        df = xls.parse(avg_sh)\n        cols = [str(c) for c in df.columns]\n        low = [c.lower() for c in cols]\n        def find_col(cands):\n            for i,c in enumerate(low):\n                for cand in cands:\n                    if cand in c:\n                        return cols[i]\n            return None\n        sr_c = find_col([\"size range\", \"bucket\", \"range\"])\n        conc_c = find_col([\"concentration\", \"format\", \"conc\", \"type\"])\n        avg_c = find_col([\"avg price/oz\", \"avg price per oz\", \"average price per oz\", \"avg$/oz\", \"avg\", \"mean\"])\n        if not sr_c or not avg_c or len(df) == 0:\n            return 0.0, \"Missing key columns for averages\"\n        test = df[[sr_c]].astype(str).applymap(lambda x: x.lower())\n        # detect bucket presence\n        buckets = {\n            \"0.30\u20131.4\": False,\n            \"1.5\u20132.9\": False,\n            \"3.0\u20134.2\": False,\n            \"4.3\u20136.8\": False,\n        }\n        def mark_bucket(txt):\n            t = txt.replace(\" \", \"\").replace(\"oz\", \"\")\n            patterns = {\n                \"0.30\u20131.4\": [\"0.3-1.4\", \"0.30-1.4\", \"0.3\u20131.4\", \"0.30\u20131.4\", \"travel\"],\n                \"1.5\u20132.9\": [\"1.5-2.9\", \"1.5\u20132.9\"],\n                \"3.0\u20134.2\": [\"3.0-4.2\", \"3-4.2\", \"3.0\u20134.2\"],\n                \"4.3\u20136.8\": [\"4.3-6.8\", \"4.3\u20136.8\", \"jumbo\"],\n            }\n            for k, pats in patterns.items():\n                if any(p in t for p in pats):\n                    buckets[k] = True\n        for v in test[sr_c].fillna(\"\").astype(str):\n            mark_bucket(v.lower())\n        bucket_cov = sum(buckets.values())/4.0\n        # avg numeric check\n        avg_vals = pd.to_numeric(df[avg_c], errors=\"coerce\")\n        avg_ok = 1.0 if (avg_vals.dropna()>0).any() else 0.0\n        # concentration coverage (optional bonus)\n        conc_cov = 1.0\n        if conc_c:\n            cc = df[[sr_c, conc_c]].astype(str)\n            grp = cc.groupby(cc[sr_c].str.lower())[conc_c].nunique()\n            if not grp.empty:\n                conc_cov = np.clip((grp >= 2).mean(), 0, 1)\n        score = (0.6*bucket_cov + 0.3*avg_ok + 0.1*conc_cov) * weight\n        fb = f\"Buckets={bucket_cov:.2f}, AvgNumeric={avg_ok:.2f}, ConcCoverage={conc_cov:.2f}\"\n        return score, fb\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "\u00b16% Compliance vs Competitor Avg Price/oz", "description": "Check that each SKU\u2019s recommended price/oz is within \u00b16% of the competitor average for the matched size range and concentration.", "weight": 1.6, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    weight = 1.6\n    out = context.get_primary_output()\n    if not out or not out.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(out.id)\n        xls = pd.ExcelFile(path)\n        sheet_names = xls.sheet_names\n        s_lower = [s.lower() for s in sheet_names]\n        def find_sheet(cands):\n            for i, s in enumerate(s_lower):\n                for c in cands:\n                    if c in s:\n                        return sheet_names[i]\n            return None\n        avg_sh = find_sheet([\"average\", \"segment\", \"pivot\", \"benchmark\"])\n        rec_sh = find_sheet([\"recommend\", \"pricing\", \"msrp\"])\n        if not avg_sh or not rec_sh:\n            return 0.0, \"Missing averages or recommendations\"\n        avg = xls.parse(avg_sh)\n        rec = xls.parse(rec_sh)\n        def find_col(df, cand_list):\n            cols = [str(c) for c in df.columns]\n            low = [c.lower().strip() for c in cols]\n            for i,c in enumerate(low):\n                for cand in cand_list:\n                    if cand in c:\n                        return cols[i]\n            return None\n        # columns\n        sr_c = find_col(avg, [\"size range\", \"bucket\", \"range\"])\n        concA_c = find_col(avg, [\"concentration\", \"format\", \"conc\", \"type\"])\n        avg_c = find_col(avg, [\"avg price/oz\", \"avg price per oz\", \"average price per oz\", \"avg$/oz\", \"avg\", \"mean\"])\n        size_c = find_col(rec, [\"size\", \"oz\", \"ounce\"]) \n        concR_c = find_col(rec, [\"concentration\", \"format\", \"conc\", \"type\"]) \n        msrpR_c = find_col(rec, [\"recommended msrp\", \"new msrp\", \"msrp (rec)\", \"msrp\"])\n        rppoz_c = find_col(rec, [\"recommended price/oz\", \"rec price/oz\", \"price per oz\"])\n        if not all([sr_c, avg_c, size_c, concR_c, msrpR_c]):\n            return 0.0, \"Missing key columns\"\n        # bucket function\n        def to_oz(v):\n            if pd.isna(v):\n                return np.nan\n            s = str(v).lower()\n            m = re.search(r\"([0-9]*\\.?[0-9]+)\", s)\n            if not m:\n                return np.nan\n            num = float(m.group(1))\n            if \"ml\" in s:\n                return num/29.5735\n            return num\n        def bucket(x):\n            if pd.isna(x):\n                return None\n            try:\n                v = float(x)\n            except:\n                return None\n            if 0.30 <= v <= 1.40:\n                return \"0.30\u20131.4\"\n            if 1.50 <= v <= 2.90:\n                return \"1.5\u20132.9\"\n            if 3.00 <= v <= 4.20:\n                return \"3.0\u20134.2\"\n            if 4.30 <= v <= 6.80:\n                return \"4.3\u20136.8\"\n            return None\n        # Normalize avg table to buckets\n        a = avg[[sr_c, avg_c] + ([concA_c] if concA_c else [])].copy()\n        a.columns = [\"_sr\", \"_avg\"] + ([\"_conc\"] if concA_c else [])\n        a[\"_sr_norm\"] = a[\"_sr\"].astype(str).str.lower().str.replace(\" \", \"\", regex=False)\n        # map to canonical keys\n        def map_sr(s):\n            t = s.replace(\"oz\", \"\").replace(\"\u2013\", \"-\")\n            if any(k in t for k in [\"0.3-1.4\",\"0.30-1.4\",\"0.3-1.4\",\"travel\"]):\n                return \"0.30\u20131.4\"\n            if any(k in t for k in [\"1.5-2.9\",\"1.5\u20132.9\"]):\n                return \"1.5\u20132.9\"\n            if any(k in t for k in [\"3.0-4.2\",\"3-4.2\",\"3.0\u20134.2\"]):\n                return \"3.0\u20134.2\"\n            if any(k in t for k in [\"4.3-6.8\",\"4.3\u20136.8\",\"jumbo\"]):\n                return \"4.3\u20136.8\"\n            return None\n        a[\"_sr_key\"] = a[\"_sr_norm\"].apply(map_sr)\n        a[\"_avg\"] = pd.to_numeric(a[\"_avg\"], errors=\"coerce\")\n        a = a.dropna(subset=[\"_sr_key\",\"_avg\"]).copy()\n        if concA_c:\n            a[\"_conc\"] = a[\"_conc\"].astype(str).str.lower()\n        # Prepare rec\n        r = rec[[size_c, concR_c, msrpR_c] + ([rppoz_c] if rppoz_c else [])].copy()\n        r.columns = [\"_size\",\"_conc\",\"_msrp\"] + ([\"_rppoz\"] if rppoz_c else [])\n        r[\"_oz\"] = r[\"_size\"].apply(to_oz)\n        r[\"_bucket\"] = r[\"_oz\"].apply(bucket)\n        r[\"_conc_norm\"] = r[\"_conc\"].astype(str).str.lower().str.replace(\"eau de parfum\",\"edp\").str.replace(\"eau de toilette\",\"edt\").str.replace(\"parfum\",\"edp\")\n        r[\"_rppoz_calc\"] = r[\"_msrp\"] / r[\"_oz\"]\n        # Merge to get avg cpz\n        if concA_c:\n            merged = pd.merge(r, a, left_on=[\"_bucket\",\"_conc_norm\"], right_on=[\"_sr_key\",\"_conc\"], how=\"left\")\n        else:\n            merged = pd.merge(r, a.drop_duplicates(subset=[\"_sr_key\"][0:1]), left_on=[\"_bucket\"], right_on=[\"_sr_key\"], how=\"left\")\n        merged = merged.replace([np.inf,-np.inf], np.nan).dropna(subset=[\"_rppoz_calc\",\"_avg\"]) \n        if len(merged) == 0:\n            return 0.0, \"No matchable rows for \u00b16% check\"\n        dev = (merged[\"_rppoz_calc\"] - merged[\"_avg\"]).abs() / merged[\"_avg\"].replace(0,np.nan)\n        ok = (dev <= 0.06).mean()\n        score = float(ok) * weight\n        fb = f\"Within \u00b16%: {ok:.2%} of matched SKUs\"\n        return score, fb\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "COGS-to-MSRP Consistency and Concentration Premiums", "description": "Check that markups are reasonably consistent and that MSRP/oz premiums by concentration align with COGS/oz premiums and ordering (EDT < EDP < Elixir).", "weight": 0.7, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    weight = 0.7\n    out = context.get_primary_output()\n    if not out or not out.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(out.id)\n        xls = pd.ExcelFile(path)\n        sheet_names = xls.sheet_names\n        s_lower = [s.lower() for s in sheet_names]\n        def find_sheet(cands):\n            for i, s in enumerate(s_lower):\n                for c in cands:\n                    if c in s:\n                        return sheet_names[i]\n            return None\n        rec_sh = find_sheet([\"recommend\", \"pricing\", \"msrp\"])\n        if not rec_sh:\n            return 0.0, \"Missing recommendations sheet\"\n        rec = xls.parse(rec_sh)\n        def find_col(df, cand_list):\n            cols = [str(c) for c in df.columns]\n            low = [c.lower().strip() for c in cols]\n            for i,c in enumerate(low):\n                for cand in cand_list:\n                    if cand in c:\n                        return cols[i]\n            return None\n        size_c = find_col(rec, [\"size\", \"oz\", \"ounce\"]) \n        conc_c = find_col(rec, [\"concentration\", \"format\", \"conc\", \"type\"]) \n        msrp_c = find_col(rec, [\"recommended msrp\", \"new msrp\", \"msrp (rec)\", \"msrp\"])\n        cogs_c = find_col(rec, [\"cogs\", \"cost\"])\n        if not all([size_c, conc_c, msrp_c, cogs_c]):\n            return 0.0, \"Missing size/concentration/MSRP/COGS columns\"\n        def to_oz(v):\n            if pd.isna(v):\n                return np.nan\n            s = str(v).lower()\n            m = re.search(r\"([0-9]*\\.?[0-9]+)\", s)\n            if not m:\n                return np.nan\n            num = float(m.group(1))\n            if \"ml\" in s:\n                return num/29.5735\n            return num\n        df = rec[[size_c, conc_c, msrp_c, cogs_c]].copy()\n        df.columns = [\"_size\",\"_conc\",\"_msrp\",\"_cogs\"]\n        df[\"_oz\"] = df[\"_size\"].apply(to_oz)\n        df = df.replace([np.inf,-np.inf], np.nan).dropna(subset=[\"_oz\",\"_msrp\",\"_cogs\"]) \n        df = df[(df[\"_oz\"]>0) & (df[\"_cogs\"]>0) & (df[\"_msrp\"]>0)]\n        if len(df) < 3:\n            return 0.0, \"Insufficient rows\"\n        df[\"_markup\"] = df[\"_msrp\"]/df[\"_cogs\"]\n        # Consistency via CV of markup\n        if df[\"_markup\"].mean() == 0:\n            cv = 1.0\n        else:\n            cv = float(df[\"_markup\"].std(ddof=0) / df[\"_markup\"].mean())\n        # Map to score: full if CV<=0.15, linear down to 0 at CV>=0.40\n        if cv <= 0.15:\n            s_cons = 1.0\n        elif cv >= 0.40:\n            s_cons = 0.0\n        else:\n            s_cons = (0.40 - cv) / (0.40 - 0.15)\n        # Concentration premiums\n        df[\"_conc_norm\"] = df[\"_conc\"].astype(str).str.lower().str.replace(\"eau de parfum\",\"edp\").str.replace(\"eau de toilette\",\"edt\").str.replace(\"parfum\",\"edp\")\n        g = df.groupby(\"_conc_norm\").agg(msrp_oz=(\"_msrp\", lambda s: (s.sum()/df.loc[s.index, \"_oz\"].sum()) if df.loc[s.index, \"_oz\"].sum()>0 else np.nan),\n                                         cogs_oz=(\"_cogs\", lambda s: (s.sum()/df.loc[s.index, \"_oz\"].sum()) if df.loc[s.index, \"_oz\"].sum()>0 else np.nan))\n        # Ensure values exist\n        concs = g.index.tolist()\n        order_ok = 1.0\n        # Check ordering EDT <= EDP <= Elixir if present\n        def val(x, col):\n            return float(g.loc[x, col]) if x in g.index and pd.notna(g.loc[x, col]) else np.nan\n        edt = val(\"edt\", \"msrp_oz\")\n        edp = val(\"edp\", \"msrp_oz\")\n        elx = val(\"elixir\", \"msrp_oz\")\n        checks = []\n        if not np.isnan(edt) and not np.isnan(edp):\n            checks.append(1.0 if edt <= edp else 0.0)\n        if not np.isnan(edp) and not np.isnan(elx):\n            checks.append(1.0 if edp <= elx else 0.0)\n        if checks:\n            order_ok = sum(checks)/len(checks)\n        # Premium alignment vs COGS\n        prem_ok = 1.0\n        pairs = [(\"edp\",\"edt\"),(\"elixir\",\"edp\")]\n        scs = []\n        for a,b in pairs:\n            ma = val(a, \"msrp_oz\"); mb = val(b, \"msrp_oz\")\n            ca = val(a, \"cogs_oz\"); cb = val(b, \"cogs_oz\")\n            if all([not np.isnan(x) for x in [ma,mb,ca,cb]]) and mb>0 and cb>0:\n                msrp_ratio = ma/mb\n                cogs_ratio = ca/cb\n                if cogs_ratio<=0 or msrp_ratio<=0:\n                    sc = 0.0\n                else:\n                    rel_err = abs(msrp_ratio - cogs_ratio) / cogs_ratio\n                    if rel_err <= 0.15:\n                        sc = 1.0\n                    elif rel_err >= 0.40:\n                        sc = 0.0\n                    else:\n                        sc = (0.40 - rel_err)/(0.40 - 0.15)\n                scs.append(sc)\n        if scs:\n            prem_ok = sum(scs)/len(scs)\n        score = (0.55*s_cons + 0.20*order_ok + 0.25*prem_ok) * weight\n        fb = f\"Markup CV={cv:.3f} -> {s_cons:.2f}; Order={order_ok:.2f}; Premium align={prem_ok:.2f}\"\n        return score, fb\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Presentation and Strategic Rationale", "description": "LLM judge assesses clarity, professionalism, and strategic usefulness for a premium rebranding context.", "is_required": false, "max_points": 1.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Quality and Rationale Completeness", "description": "Evaluate whether the model is presentation-ready and the rationale addresses COGS-MSRP relationship, concentration-price relationships, and comparison to competitor averages.", "weight": 1.0, "judge_prompt": "Assess only presentation and explanatory quality (not recalculations). Consider:\n- Professional formatting: clear tabs, labels, readable tables, consistent units (oz), rounded price points suitable for retail (e.g., .00 or .50), and visible formulas/logic artifacts (notes, helper columns) that make the logic auditable.\n- Rationale completeness: Does the Assumptions/Methodology and/or a Rationale column explain:\n  (a) Relationship between COGS and MSRP (markup or margin logic)\n  (b) Price relationship across concentrations (EDT < EDP < Elixir) and the premium rationale\n  (c) How each new MSRP compares to competitor averages (explicit tie to Avg $/oz and \u00b16% rule)\n- Strategic fit: Do recommendations feel consistent with a premium rebranding strategy (not bargain pricing), while still competitive?\n\nScoring:\n- 1.0: Professionally formatted, rationale covers (a), (b), and (c) clearly; pricing looks premium yet competitive.\n- 0.6: Mostly clear/professional; covers at least two of (a)/(b)/(c) adequately.\n- 0.3: Minimal rationale or formatting issues; touches only one of (a)/(b)/(c).\n- 0.0: Unprofessional layout; no meaningful rationale.\n", "expectation": "Well-labeled workbook with clear narratives and concise SKU-level rationales that tie to costs, concentration premiums, and competitor benchmarks, suitable for leadership review."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "1e5a1d7f-12c1-48c6-afd9-82257b3f2409", "rubric": {"category_name": "Property Manager Weekly Schedule (.docx) - Real Estate Operations", "rationale": "Pattern B (Document). The deliverable is a DOCX document with a structured table. Stage 1 uses an LLM gate to enforce exact shape (DOCX with a single primary table containing the four required columns). Stage 2 mixes code checks (regex/text extraction for headers, times, week-of-month coverage, PM-duty keywords) with an LLM reasonableness check enabled by the Stage 1 structure. Stage 3 uses an LLM for holistic professional quality assessment.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Format and Structure Gate (LLM ONLY)", "description": "Hard gate verifying the output is a DOCX with a primary table containing exactly four required columns and sufficient rows, fit for downstream verification.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "DOCX Table Structure Requirement", "description": "Validate the candidate produced a .docx file containing a clearly structured weekly schedule table with the four required columns and sufficient entries.", "weight": 4.0, "judge_prompt": "You are evaluating whether the submitted output satisfies strict STRUCTURE requirements for a weekly Property Manager schedule. Use the following checklist. Score ONLY on structure/presence, not content quality.\n\nFormat Requirements:\n- The file must be a DOCX (Word) document. Not PDF, not Excel, not plain text.\n- The first page should include a clear title such as \u201cWeekly Property Manager Schedule\u201d (flexible naming is okay: e.g., \u201cPM Weekly Schedule\u201d, \u201cWeekly Operations Schedule for Property Managers\u201d).\n\nTable Requirements (critical):\n- There must be a primary table containing scheduling rows.\n- The table must have FOUR columns with visible headers, corresponding to:\n  1) Time (time of day to perform)\n  2) Activity (what they should focus on)\n  3) Details/Tracker (tasks to handle and the source/tool to pull from)\n  4) Week of the Month (which week this task is emphasized)\n- Be flexible with header wording, but the intended meaning must clearly match each of the four columns.\n- The schedule table must have at least 12 data rows (excluding header). More is fine. Rows should be clearly distinguishable under the headers.\n- The \u201cWeek of the Month\u201d column should include entries covering the concept of Weeks 1\u20134 in some form (e.g., Week 1, Wk 1, First week). Variants/synonyms are acceptable.\n\nScoring (0\u20134 points):\n- 4.0: DOCX present + clear title on first page + one primary schedule table with 4 appropriate column headers + at least 12 data rows + Week of Month entries evident for Weeks 1\u20134 (or clear equivalents). All structure is unambiguous.\n- 3.0: DOCX present + table and 4 headers present + at least 8 data rows, but missing either the clear title OR some Week-of-Month coverage details (e.g., only Weeks 1\u20133 explicitly shown).\n- 1.0\u20132.0: DOCX present but structural issues: missing one of the four required columns, unclear headers, insufficient rows (<8), or the schedule is not clearly a single primary table.\n- 0.0: Not a DOCX, or no recognizable schedule table.\n\nOnly evaluate structural presence/format. Do NOT judge the correctness or quality of the entries. Return a numeric score from 0.0 to 4.0.", "expectation": "A DOCX containing a single primary table with headers: Time | Activity | Details/Tracker | Week of the Month; at least 12 rows; and Week 1\u20134 coverage indicated."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Consistency Verification", "description": "Lightweight deterministic checks using text extraction, plus an LLM consistency check now that the shape is enforced.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 2.0, "rules": [{"type": "code", "name": "Header Presence via Text Extraction", "description": "Confirm column header concepts are present in the DOCX text: Time, Activity, Details/Tracker (or equivalents), Week of the Month.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output:\n            return 0.0\n        # Must be a document and preferably .docx\n        if not output.is_document:\n            return 0.0\n        path = context.files.get_path(output.id)\n        if not str(path).lower().endswith('.docx'):\n            return 0.0\n        text = context.files.read_docx_text(output.id)\n        if not text:\n            return 0.0\n        tlower = text.lower()\n        # Flexible matches for each header concept\n        checks = []\n        # Time column\n        checks.append(any(k in tlower for k in [\"time of day\", \"time  \", \" time \", \"time:\", \"time\\n\", \"time\\t\"]))\n        # Activity column\n        checks.append(any(k in tlower for k in [\"activity\", \"focus\", \"focus area\"]))\n        # Details/Tracker column\n        checks.append(any(k in tlower for k in [\"details/tracker\", \"details & tracker\", \"details and tracker\", \"details\", \"tracker\"]))\n        # Week of the Month column\n        wk_terms = [\"week of the month\", \"week-of-the-month\", \"week of month\", \"wk of month\", \"wk of the month\", \"week#\", \"week #\", \"wk #\"]\n        checks.append(any(k in tlower for k in wk_terms))\n        score = (sum(1 for c in checks if c) / 4.0) * 0.8\n        return score\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Time Format and Coverage", "description": "Verify the presence of multiple valid time-of-day entries, including AM and PM coverage and spread across dayparts.", "weight": 1.2, "code": "import re\nfrom datetime import datetime\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        path = context.files.get_path(output.id)\n        if not str(path).lower().endswith('.docx'):\n            return 0.0\n        text = context.files.read_docx_text(output.id)\n        if not text:\n            return 0.0\n        tlower = text.lower()\n        # Find time patterns like 8 AM, 8:00 AM, 13:30, etc.\n        patterns = [\n            r\"\\b(?:[1-9]|1[0-2])(?::[0-5]\\d)?\\s?(?:am|pm|a\\.m\\.|p\\.m\\.)\\b\",\n            r\"\\b(?:[01]?\\d|2[0-3]):[0-5]\\d\\b\"\n        ]\n        times = set()\n        for pat in patterns:\n            for m in re.findall(pat, tlower):\n                times.add(m)\n        n_times = len(times)\n        # Component 1: count threshold (>=8 distinct times)\n        comp1 = min(n_times / 8.0, 1.0) * 0.5\n        # Component 2: presence of both am and pm\n        has_am = bool(re.search(r\"\\b(am|a\\.m\\.)\\b\", tlower))\n        has_pm = bool(re.search(r\"\\b(pm|p\\.m\\.)\\b\", tlower))\n        comp2 = 0.3 if (has_am and has_pm) else 0.0\n        # Component 3: presence of morning and afternoon indicators or hour ranges\n        morning_hit = bool(re.search(r\"\\b(morning|8\\s?am|9\\s?am|10\\s?am|11\\s?am)\\b\", tlower))\n        afternoon_hit = bool(re.search(r\"\\b(afternoon|12\\s?pm|1\\s?pm|2\\s?pm|3\\s?pm|4\\s?pm|5\\s?pm)\\b\", tlower))\n        comp3 = 0.2 if (morning_hit and afternoon_hit) else 0.0\n        return min(comp1 + comp2 + comp3, 1.2)\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Week-of-Month Coverage", "description": "Confirm coverage references for Weeks 1\u20134 (flexible expressions: Week 1/Wk 1/1st week/etc.).", "weight": 1.0, "code": "import re\n\ndef _has_week_ref(text_lower, num):\n    words = {1:[\"first\", \"1st\"], 2:[\"second\", \"2nd\"], 3:[\"third\", \"3rd\"], 4:[\"fourth\", \"4th\"]}\n    patterns = [\n        rf\"\\bweek\\s*{num}\\b\", rf\"\\bwk\\s*{num}\\b\", rf\"\\bweek\\s*#?\\s*{num}\\b\",\n        rf\"\\b{num}(st|nd|rd|th)\\s*week\\b\"\n    ]\n    for w in words.get(num, []):\n        patterns.append(rf\"\\b{w}\\s+week\\b\")\n    return any(re.search(p, text_lower) for p in patterns)\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        path = context.files.get_path(output.id)\n        if not str(path).lower().endswith('.docx'):\n            return 0.0\n        text = context.files.read_docx_text(output.id)\n        if not text:\n            return 0.0\n        tlower = text.lower()\n        hits = sum(1 for i in range(1,5) if _has_week_ref(tlower, i))\n        return (hits / 4.0) * 1.0\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Cyclical PM Duties Keywords", "description": "Check for presence of common PM duty terms indicating cyclical operational coverage.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        path = context.files.get_path(output.id)\n        if not str(path).lower().endswith('.docx'):\n            return 0.0\n        text = context.files.read_docx_text(output.id)\n        if not text:\n            return 0.0\n        tlower = text.lower()\n        keywords = [\n            \"rent\", \"delinquen\", \"work order\", \"maintenance\", \"inspection\", \"vendor\",\n            \"lease\", \"renewal\", \"owner report\", \"budget\", \"turnover\", \"move-in\", \"move out\",\n            \"notice\", \"cam\", \"utility\", \"insurance\", \"bid\", \"rfp\", \"capex\", \"violation\", \"hoa\",\n            \"common area\", \"site walk\", \"bank deposit\", \"invoice\", \"accounts payable\", \"accounts receivable\",\n            \"ap \", \" ar \", \"reserve\", \"escrow\"\n        ]\n        hits = 0\n        seen = set()\n        for k in keywords:\n            if k in tlower and k not in seen:\n                hits += 1\n                seen.add(k)\n        # Reward up to 10 unique hits\n        return min(hits / 10.0, 1.0) * 0.5\n    except Exception:\n        return 0.0"}, {"type": "llm_judge", "name": "Reasonableness and Internal Consistency", "description": "LLM checks if time slots align with activities, Week-of-Month emphasis is logical, and Details/Tracker references tools/sources.", "weight": 0.5, "judge_prompt": "Evaluate the DOCX schedule for internal consistency and plausibility. Do not judge cosmetic quality beyond clarity needed for verification.\n\nCheck:\n1) Time vs. Activity alignment: Administrative tasks (e.g., reporting, owner updates) occur during reasonable business hours; quieter-hours tasks (e.g., email triage) may be placed earlier in the day. No obvious conflicts.\n2) Week-of-Month logic: Items like owner/financial reporting, reconciliations, inspections, or rent-related workflows are placed in sensible weeks (e.g., rent follow-up early to mid-month, reporting late month, renewals/inspections spread logically).\n3) Details/Tracker column references specific systems or sources when appropriate (e.g., Yardi/AppFolio/Buildium, maintenance portal, email, calendar, bank portal, shared drive).\n\nScoring (0\u20130.5):\n- 0.5: Clear alignment across all three checks.\n- 0.3: Mostly aligned; minor inconsistencies.\n- 0.1: Weak alignment; several questionable placements.\n- 0.0: Illogical or contradictory scheduling.\n\nReturn a numeric score from 0.0 to 0.5.", "expectation": "Schedule logically maps key PM workflows to reasonable times and weeks; trackers cite real operational systems."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality Assessment", "description": "Holistic review of the document\u2019s clarity, usability, and professionalism for a property management team.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Practical Utility", "description": "Assess readability, clarity of the table, unambiguous instructions, and practical usefulness for Property Managers.", "weight": 2.0, "judge_prompt": "Assess the professional quality of the DOCX schedule as a managerial deliverable. Consider:\n- Clarity and readability: clean headers, consistent time formats, legible rows.\n- Actionability: Activities and Details/Tracker entries are specific enough to guide daily work without ambiguity.\n- Coverage and balance: Includes core PM functions (rent, maintenance, inspections, renewals, vendor/AP/AR, reporting) without overloading any single day/time; cadence reflects cyclical nature.\n- Document professionalism: clear title, logical ordering, consistent tone suitable for a property management team.\n\nScoring (0\u20132):\n- 2.0: Excellent\u2014immediately usable, comprehensive yet concise, professionally presented.\n- 1.2: Good\u2014minor issues but clearly useful and professional.\n- 0.6: Fair\u2014useful but several clarity or coverage gaps.\n- 0.0: Poor\u2014confusing, incomplete, or not practical.\n\nReturn a numeric score from 0.0 to 2.0.", "expectation": "A clean, well-structured schedule that a PM team can adopt immediately with minimal clarification."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "327fbc21-7d26-4964-bf7c-f4f41e55c54d", "rubric": {"category_name": "Wholesale Trade \u2014 First-Line Supervisors (Non-Retail): By Door May Sales Plan (Candy)", "rationale": "Self-documenting, staged rubric for an analytical planning task. Stage 1 (LLM-only) enforces a strict, verifiable Excel shape that exposes all necessary artifacts. Stage 2 uses code rules to validate correctness and policy adherence (weekly weights, rounding, rollups, comp target, STD trend influence). Stage 3 assesses professionalism and communication quality.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (LLM only)", "description": "Output must be a well-structured Excel workbook enabling verification of the May By Door Sales Plan with rollups and a structured summary.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Output Format Requirement (Excel)", "description": "Verify the candidate produced an Excel file with the exact structure needed for verification: required sheets, tables, headers, and summary values.", "weight": 4.0, "judge_prompt": "You are checking ONLY the structure/format of the output, not the correctness of calculations. The output must be an Excel workbook (.xlsx) with the following structure. Be flexible about small header naming variations (e.g., \"Plan W1\" vs \"W1 Plan\"), but the sections and fields must be clearly present.\n\nFormat Requirements:\n- Must be an Excel workbook (.xlsx).\n- Professionally structured with clear headers and no merged-header ambiguity for the required tables.\n\nRequired Sheets and Structures:\n1) Sheet: \"By Door Plan\" (required)\n   A single table with these columns (exact or close variants acceptable, but intent must be clear):\n   - Store ID (e.g., \"Store ID\", \"ID\", \"Store ID#\")\n   - Status (e.g., \"Status\", \"Open/Closed\", \"Door Status\")\n   - LY W1, LY W2, LY W3, LY W4\n   - LY May Total (e.g., \"LY May Total\", \"LY P4 May\", \"LY Month Total\")\n   - STD Sales TY (e.g., \"STD Sales TY\", \"STD TY\")\n   - STD Sales LY (e.g., \"STD Sales LY\", \"STD LY\")\n   - STD Trend % (e.g., \"STD Trend %\", \"STD % Change\")\n   - Plan W1, Plan W2, Plan W3, Plan W4\n   - Plan May Total (e.g., \"May Plan Total\", \"Month Plan Total\")\n   - Plan vs LY % (e.g., \"% vs LY\", \"% Change vs LY\")\n   Notes:\n   - Only active/open stores should have nonzero plans. Closed stores present are acceptable but must be identified via Status column.\n   - Table must be machine-readable (clear header row, one row per store).\n\n2) Sheet: \"Rollup Summary\" (required)\n   A rollup table with rows for these lines:\n   - Total Stores\n   - Closed Stores\n   - Comp Stores (Comp = Total \u2013 Closed)\n   Columns required (or close variants):\n   - LY W1, LY W2, LY W3, LY W4, LY May Total\n   - Plan W1, Plan W2, Plan W3, Plan W4, Plan May Total\n   - Plan vs LY % (for each rollup line)\n\n3) Sheet: \"Planning Notes\" (required)\n   - A short 1\u20132 sentence narrative is fine, but MUST include a small 2-column key-value table titled \"Summary Values\" (or clearly labeled similarly) with these keys:\n     \u2022 Total Plan $  \n     \u2022 Total vs LY %  \n     \u2022 Comp Plan $  \n     \u2022 Comp vs LY %  \n     \u2022 Closed LY $  \n   The table must be easy to read (first column keys, second column values) so code can extract values.\n\nScoring (structure only):\n- 1.0: All 3 sheets present and clearly structured as above. \"By Door Plan\" has all required columns; \"Rollup Summary\" has the three required rows and required columns; \"Planning Notes\" includes the \"Summary Values\" key-value table with all 5 keys.\n- 0.8: Minor header naming variations or minor omissions (e.g., missing STD Trend % but both STD TY and STD LY present; or the Summary Values table is present with 4/5 keys).\n- 0.6: One required sheet missing OR multiple critical columns missing in By Door Plan or Rollup Summary.\n- 0.3: Excel file present but only one of the required sheets/tables is present.\n- 0.0: Not an Excel file or missing the \"By Door Plan\" sheet entirely.\n\nImportant: Do NOT judge correctness of numbers or formulas. Only confirm the presence and readability of the required structure.", "expectation": "A cleanly structured .xlsx with three sheets: By Door Plan (store-level LY, STD, and Plan columns), Rollup Summary (Total, Closed, Comp with LY/Plan by week and totals), and Planning Notes with a Summary Values key-value table covering Total/Comp metrics and Closed LY."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Correctness and Policy Checks)", "description": "Now that the structure is in place, verify calculations, constraints, weekly weights, rollups, and summary cross-checks using code rules. These rules assume the Stage 1 structure exists.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Sums and Required Columns (By Door)", "description": "Check existence of key LY and Plan columns and that LY May Total = sum(LY W1..W4) and Plan May Total = sum(Plan W1..W4) for most rows.", "weight": 1.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0, \"No spreadsheet output\"\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheet_names = [s for s in xls.sheet_names]\n        # Find By Door sheet (prefer exact, else fuzzy)\n        def match_sheet(targets):\n            for s in sheet_names:\n                sl = s.strip().lower()\n                for t in targets:\n                    if t in sl:\n                        return s\n            return None\n        by_door = (match_sheet([\"by door plan\"]) or\n                   match_sheet([\"by door\", \"by store\", \"store plan\", \"door plan\", \"may plan\"]) or\n                   (sheet_names[0] if sheet_names else None))\n        if not by_door:\n            return 0.0, \"By Door sheet not found\"\n        df = pd.read_excel(path, sheet_name=by_door)\n        cols = {c: str(c).strip() for c in df.columns}\n        low = {c: str(c).strip().lower() for c in df.columns}\n        def find_col(which):\n            # returns first matching column name or None\n            for c in df.columns:\n                cl = low[c]\n                if which == 'ly_w1' and ('ly' in cl and re.search(r\"\\bw\\s*1\\b|week\\s*1\", cl)): return c\n                if which == 'ly_w2' and ('ly' in cl and re.search(r\"\\bw\\s*2\\b|week\\s*2\", cl)): return c\n                if which == 'ly_w3' and ('ly' in cl and re.search(r\"\\bw\\s*3\\b|week\\s*3\", cl)): return c\n                if which == 'ly_w4' and ('ly' in cl and re.search(r\"\\bw\\s*4\\b|week\\s*4\", cl)): return c\n                if which == 'plan_w1' and ('plan' in cl and re.search(r\"\\bw\\s*1\\b|week\\s*1\", cl)): return c\n                if which == 'plan_w2' and ('plan' in cl and re.search(r\"\\bw\\s*2\\b|week\\s*2\", cl)): return c\n                if which == 'plan_w3' and ('plan' in cl and re.search(r\"\\bw\\s*3\\b|week\\s*3\", cl)): return c\n                if which == 'plan_w4' and ('plan' in cl and re.search(r\"\\bw\\s*4\\b|week\\s*4\", cl)): return c\n                if which == 'ly_total' and ('ly' in cl and 'total' in cl and ('may' in cl or 'p4' in cl or 'month' in cl)): return c\n                if which == 'plan_total' and ('plan' in cl and 'total' in cl): return c\n            return None\n        required = ['ly_w1','ly_w2','ly_w3','ly_w4','ly_total','plan_w1','plan_w2','plan_w3','plan_w4','plan_total']\n        found = {k: find_col(k) for k in required}\n        have_required = sum(1 for k in required if found[k] is not None)\n        col_score = have_required / len(required)\n        if col_score == 0:\n            return 0.0, \"Required LY/Plan columns missing\"\n        # sum checks\n        def to_num(s):\n            return pd.to_numeric(s, errors='coerce')\n        ly_sum = to_num(df[found['ly_w1']]) + to_num(df[found['ly_w2']]) + to_num(df[found['ly_w3']]) + to_num(df[found['ly_w4']])\n        ly_tot = to_num(df[found['ly_total']])\n        plan_sum = to_num(df[found['plan_w1']]) + to_num(df[found['plan_w2']]) + to_num(df[found['plan_w3']]) + to_num(df[found['plan_w4']])\n        plan_tot = to_num(df[found['plan_total']])\n        tol = 1.0  # dollars tolerance\n        ly_ok = ((ly_sum - ly_tot).abs() <= tol) | (ly_sum.isna() & ly_tot.isna())\n        plan_ok = ((plan_sum - plan_tot).abs() <= tol) | (plan_sum.isna() & plan_tot.isna())\n        # Consider only rows where both components present\n        ly_valid = ly_ok[ly_tot.notna() & ly_sum.notna()]\n        plan_valid = plan_ok[plan_tot.notna() & plan_sum.notna()]\n        frac_ly = float(ly_valid.mean()) if len(ly_valid) else 0.0\n        frac_plan = float(plan_valid.mean()) if len(plan_valid) else 0.0\n        sum_score = (frac_ly + frac_plan) / 2.0\n        # overall: 50% columns, 50% sum consistency\n        score = 0.5*col_score + 0.5*sum_score\n        return max(0.0, min(1.0, score)), f\"Columns {have_required}/{len(required)}; LY sum ok={frac_ly:.2f}; Plan sum ok={frac_plan:.2f}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Rounding, Minimums, and Closed-Store Zero Plan", "description": "Plans > 0 must be multiples of $50 and >= $50. Closed stores must have zero plan (all weeks).", "weight": 1.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheet_names = xls.sheet_names\n        def match_sheet(targets):\n            for s in sheet_names:\n                sl = s.strip().lower()\n                for t in targets:\n                    if t in sl:\n                        return s\n            return None\n        by_door = (match_sheet([\"by door plan\"]) or match_sheet([\"by door\",\"by store\",\"store plan\",\"door plan\",\"may plan\"]) or sheet_names[0])\n        df = pd.read_excel(path, sheet_name=by_door)\n        low = {c: str(c).strip().lower() for c in df.columns}\n        def col_like(subs):\n            for c in df.columns:\n                cl = low[c]\n                if all(s in cl for s in subs):\n                    return c\n            return None\n        plan_w = []\n        for w in ['1','2','3','4']:\n            c = None\n            for col in df.columns:\n                cl = low[col]\n                if ('plan' in cl) and (('w'+w) in cl or ('week '+w) in cl):\n                    c = col\n                    break\n            if c is None:\n                # fallback simple names\n                c = col_like(['plan', 'w'+w]) or col_like(['plan','week',w])\n            if c is not None:\n                plan_w.append(c)\n        if len(plan_w) < 4:\n            return 0.0, \"Missing plan week columns\"\n        # status/closed detection\n        status_col = None\n        for c in df.columns:\n            cl = low[c]\n            if any(k in cl for k in ['status','open/closed','door status','store status']):\n                status_col = c\n                break\n        status = df[status_col].astype(str).str.lower() if status_col else pd.Series(['']*len(df))\n        is_closed = status.str.contains('closed') | status.str.contains('inactive')\n        # Compliance checks\n        plans = [pd.to_numeric(df[c], errors='coerce').fillna(0) for c in plan_w]\n        plan_mat = np.vstack([p.values for p in plans]).T  # rows x 4\n        positive_mask = plan_mat > 0\n        multiples_ok = ((plan_mat % 50) == 0) | (~positive_mask)\n        min_ok = (plan_mat >= 50) | (~positive_mask)\n        rounding_min_ok = (multiples_ok & min_ok).all(axis=1)\n        # Closed stores must have all plan weeks == 0\n        closed_zero_ok = (~is_closed) | ((plan_mat.sum(axis=1) == 0))\n        # Score: half rounding/min, half closed zero\n        score1 = float(rounding_min_ok.mean()) if len(rounding_min_ok) else 0.0\n        score2 = float(closed_zero_ok.mean()) if len(closed_zero_ok) else 0.0\n        score = 0.5*score1 + 0.5*score2\n        return max(0.0, min(1.0, score)), f\"Rounding+Min ok={score1:.2f}; Closed-zero ok={score2:.2f}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Rollup Weekly Weight Targets (Total Stores)", "description": "On Rollup Summary: W1=61\u201363%, W2=22\u201324%, W3=7\u20138%, W4=7\u20138% of Plan May Total for Total Stores.", "weight": 1.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        names = xls.sheet_names\n        def match_sheet(targets):\n            for s in names:\n                sl = s.lower()\n                if any(t in sl for t in targets):\n                    return s\n            return None\n        roll = match_sheet([\"rollup summary\",\"summary\",\"rollup\"]) or names[-1]\n        df = pd.read_excel(path, sheet_name=roll)\n        # identify label column (first non-numeric header)\n        label_col = df.columns[0]\n        labels = df[label_col].astype(str).str.lower()\n        row = df[labels.str.contains('total stores')]\n        if row.empty:\n            return 0.0, \"Total Stores row not found\"\n        row = row.iloc[0]\n        low = {c: str(c).strip().lower() for c in df.columns}\n        def find(col_keywords):\n            for c in df.columns:\n                cl = low[c]\n                if all(k in cl for k in col_keywords):\n                    return c\n            return None\n        pw = []\n        for w in ['1','2','3','4']:\n            c = None\n            for col in df.columns:\n                cl = low[col]\n                if 'plan' in cl and (('w'+w) in cl or ('week '+w) in cl):\n                    c = col; break\n            if c is None:\n                c = find(['plan','w'+w]) or find(['plan','week',w])\n            if c is not None:\n                pw.append(c)\n        ptot = find(['plan','total'])\n        if len(pw) < 4 or ptot is None:\n            return 0.0, \"Plan week or total columns not found\"\n        vals = [pd.to_numeric(row[c], errors='coerce') for c in pw]\n        tot = pd.to_numeric(row[ptot], errors='coerce')\n        if pd.isna(tot) or tot == 0:\n            return 0.0, \"Total plan is zero/NA\"\n        pct = [float(v)/float(tot) if pd.notna(v) else np.nan for v in vals]\n        targets = [(0.61,0.63),(0.22,0.24),(0.07,0.08),(0.07,0.08)]\n        checks = []\n        for p,(lo,hi) in zip(pct, targets):\n            checks.append(1.0 if (pd.notna(p) and lo <= p <= hi) else 0.0)\n        score = sum(checks)/len(checks)\n        return score, f\"Weekly weight matches: {checks}; pct={pct}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Comp Plan Target (-15% \u00b1 3pp)", "description": "On Rollup Summary, Comp Stores plan vs LY percent change should be about -15% (between -18% and -12%).", "weight": 0.8, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        names = xls.sheet_names\n        def match_sheet(targets):\n            for s in names:\n                if any(t in s.lower() for t in targets):\n                    return s\n            return None\n        roll = match_sheet([\"rollup summary\",\"summary\",\"rollup\"]) or names[-1]\n        df = pd.read_excel(path, sheet_name=roll)\n        label_col = df.columns[0]\n        labels = df[label_col].astype(str).str.lower()\n        row = df[labels.str.contains('comp')]\n        if row.empty:\n            return 0.0, \"Comp Stores row not found\"\n        row = row.iloc[0]\n        low = {c: str(c).strip().lower() for c in df.columns}\n        def find(col_keywords):\n            for c in df.columns:\n                cl = low[c]\n                if all(k in cl for k in col_keywords):\n                    return c\n            return None\n        # Try to read Plan vs LY % directly\n        pvc = None\n        for c in df.columns:\n            cl = low[c]\n            if ('%' in cl or 'pct' in cl or 'percent' in cl) and (('vs ly' in cl) or ('change' in cl) or ('chg' in cl) or ('over ly' in cl)):\n                pvc = c; break\n        if pvc is not None:\n            val = pd.to_numeric(str(row[pvc]).replace('%',''), errors='coerce')\n            if pd.isna(val):\n                pvc = None\n        if pvc is None:\n            # compute from totals\n            ly_tot = find(['ly','total'])\n            plan_tot = find(['plan','total'])\n            if ly_tot is None or plan_tot is None:\n                return 0.0, \"Missing totals to compute %\"\n            ly = pd.to_numeric(row[ly_tot], errors='coerce')\n            pl = pd.to_numeric(row[plan_tot], errors='coerce')\n            if pd.isna(ly) or ly == 0 or pd.isna(pl):\n                return 0.0, \"Invalid totals\"\n            val = (pl/ly - 1.0) * 100.0\n        # check range\n        if -18.0 <= val <= -12.0:\n            return 1.0, f\"Comp % vs LY {val:.2f}% in target\"\n        elif -20.0 <= val <= -10.0:\n            return 0.5, f\"Comp % vs LY {val:.2f}% close to target\"\n        else:\n            return 0.0, f\"Comp % vs LY {val:.2f}% out of range\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "STD Trend Influence on Plan vs LY", "description": "Store-level Plan vs LY % should be positively correlated with STD Trend % (higher STD trend => less negative/more positive plan vs LY).", "weight": 0.7, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        names = xls.sheet_names\n        def match_sheet(targets):\n            for s in names:\n                if any(t in s.lower() for t in targets):\n                    return s\n            return names[0]\n        by_door = match_sheet([\"by door plan\",\"by door\",\"store plan\",\"door plan\"]) \n        df = pd.read_excel(path, sheet_name=by_door)\n        low = {c: str(c).strip().lower() for c in df.columns}\n        def find(fn):\n            for c in df.columns:\n                if fn(low[c]):\n                    return c\n            return None\n        # plan and LY totals at store level\n        plan_tot = find(lambda cl: ('plan' in cl and 'total' in cl))\n        ly_tot = None\n        for c in df.columns:\n            cl = low[c]\n            if ('ly' in cl and 'total' in cl and ('may' in cl or 'p4' in cl or 'month' in cl)):\n                ly_tot = c; break\n        # STD trend or STD TY/LY\n        std_trend = None\n        for c in df.columns:\n            cl = low[c]\n            if 'trend' in cl and ('std' in cl or '%' in cl):\n                std_trend = c; break\n        std_ty = None; std_ly = None\n        if std_trend is None:\n            for c in df.columns:\n                cl = low[c]\n                if ('std' in cl and 'ty' in cl) or ('std' in cl and 'this year' in cl):\n                    std_ty = c; break\n            for c in df.columns:\n                cl = low[c]\n                if ('std' in cl and 'ly' in cl) or ('std' in cl and 'last year' in cl):\n                    std_ly = c; break\n        if plan_tot is None or ly_tot is None:\n            return 0.0, \"Missing plan/LY totals\"\n        plan_vals = pd.to_numeric(df[plan_tot], errors='coerce')\n        ly_vals = pd.to_numeric(df[ly_tot], errors='coerce')\n        # active filter via Status if present\n        status = None\n        for c in df.columns:\n            if any(k in low[c] for k in ['status','open/closed','door status','store status']):\n                status = c; break\n        is_closed = df[status].astype(str).str.lower().str.contains('closed') if status else pd.Series([False]*len(df))\n        mask_base = (~plan_vals.isna()) & (~ly_vals.isna()) & (ly_vals != 0) & (~is_closed)\n        plan_vs_ly = (plan_vals/ly_vals - 1.0) * 100.0\n        if std_trend is not None:\n            std_vals = pd.to_numeric(df[std_trend], errors='coerce')\n        elif std_ty is not None and std_ly is not None:\n            ty = pd.to_numeric(df[std_ty], errors='coerce')\n            ly = pd.to_numeric(df[std_ly], errors='coerce')\n            std_vals = (ty/ly - 1.0) * 100.0\n        else:\n            return 0.0, \"STD trend columns not found\"\n        m = mask_base & (~std_vals.isna())\n        x = std_vals[m].values.astype(float)\n        y = plan_vs_ly[m].values.astype(float)\n        if len(x) < 5:\n            return 0.0, \"Not enough rows to assess correlation\"\n        corr = np.corrcoef(x, y)[0,1]\n        if np.isnan(corr):\n            return 0.0, \"Correlation undefined\"\n        if corr >= 0.20:\n            return 1.0, f\"Correlation {corr:.2f} strong\"\n        elif corr >= 0.10:\n            return 0.5, f\"Correlation {corr:.2f} moderate\"\n        else:\n            return 0.0, f\"Correlation {corr:.2f} weak\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Rollups Add Up (Comp = Total \u2212 Closed)", "description": "On Rollup Summary: Comp totals approximately equal Total \u2212 Closed for both LY and Plan May Total.", "weight": 1.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        names = xls.sheet_names\n        def match_sheet(targets):\n            for s in names:\n                if any(t in s.lower() for t in targets):\n                    return s\n            return names[-1]\n        roll = match_sheet([\"rollup summary\",\"summary\",\"rollup\"]) \n        df = pd.read_excel(path, sheet_name=roll)\n        label_col = df.columns[0]\n        labels = df[label_col].astype(str).str.lower()\n        low = {c: str(c).strip().lower() for c in df.columns}\n        def find(col_keywords):\n            for c in df.columns:\n                cl = low[c]\n                if all(k in cl for k in col_keywords):\n                    return c\n            return None\n        rows = {\n            'total': df[labels.str.contains('total stores', na=False)],\n            'closed': df[labels.str.contains('closed', na=False)],\n            'comp': df[labels.str.contains('comp', na=False)],\n        }\n        if any(r.empty for r in rows.values()):\n            return 0.0, \"Missing rollup rows\"\n        lyc = find(['ly','total'])\n        plc = find(['plan','total'])\n        if lyc is None or plc is None:\n            return 0.0, \"Missing total columns\"\n        T_ly = pd.to_numeric(rows['total'].iloc[0][lyc], errors='coerce')\n        C_ly = pd.to_numeric(rows['closed'].iloc[0][lyc], errors='coerce')\n        P_ly = pd.to_numeric(rows['comp'].iloc[0][lyc], errors='coerce')\n        T_pl = pd.to_numeric(rows['total'].iloc[0][plc], errors='coerce')\n        C_pl = pd.to_numeric(rows['closed'].iloc[0][plc], errors='coerce')\n        P_pl = pd.to_numeric(rows['comp'].iloc[0][plc], errors='coerce')\n        def approx(a,b):\n            if pd.isna(a) or pd.isna(b):\n                return False\n            tol = max(0.01, 0.005*abs(a))\n            return abs(a - b) <= tol\n        ok_ly = approx(P_ly, T_ly - C_ly)\n        ok_pl = approx(P_pl, T_pl - C_pl)\n        score = (1.0 if ok_ly else 0.0 + 1.0 if ok_pl else 0.0)/2.0\n        return score, f\"Comp=Total-Closed LY={ok_ly}, Plan={ok_pl}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Summary Values Cross-Check", "description": "Validate that Planning Notes \u2192 Summary Values match Rollup Summary totals within reasonable tolerance.", "weight": 1.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_spreadsheet:\n            return 0.0\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        names = xls.sheet_names\n        def match_sheet(targets, default_idx=0):\n            for s in names:\n                if any(t in s.lower() for t in targets):\n                    return s\n            return names[default_idx]\n        roll = match_sheet([\"rollup summary\",\"summary\",\"rollup\"], default_idx=len(names)-1)\n        notes = match_sheet([\"planning notes\",\"notes\",\"assumptions\",\"plan notes\"], default_idx=0)\n        dfr = pd.read_excel(path, sheet_name=roll)\n        dfn = pd.read_excel(path, sheet_name=notes, header=None)\n        # Extract rollup values\n        label_col = dfr.columns[0]\n        labels = dfr[label_col].astype(str).str.lower()\n        low = {c: str(c).strip().lower() for c in dfr.columns}\n        def find(col_keywords):\n            for c in dfr.columns:\n                cl = low[c]\n                if all(k in cl for k in col_keywords):\n                    return c\n            return None\n        lyc = find(['ly','total'])\n        plc = find(['plan','total'])\n        pvc = None\n        for c in dfr.columns:\n            cl = low[c]\n            if ('%' in cl or 'pct' in cl or 'percent' in cl) and (('vs ly' in cl) or ('change' in cl) or ('chg' in cl) or ('over ly' in cl)):\n                pvc = c; break\n        if lyc is None or plc is None:\n            return 0.0, \"Missing totals in rollup\"\n        def get_row(name):\n            r = dfr[labels.str.contains(name, na=False)]\n            return r.iloc[0] if not r.empty else None\n        r_total = get_row('total stores')\n        r_closed = get_row('closed')\n        r_comp = get_row('comp')\n        if r_total is None or r_closed is None or r_comp is None:\n            return 0.0, \"Missing rollup rows\"\n        def to_num(v):\n            if isinstance(v, str):\n                v = v.replace('$','').replace(',','').replace('%','')\n            return pd.to_numeric(v, errors='coerce')\n        roll_vals = {\n            'Total Plan $': to_num(r_total[plc]),\n            'Total vs LY %': to_num((r_total[pvc] if pvc is not None else ((to_num(r_total[plc])/to_num(r_total[lyc]) - 1)*100) if to_num(r_total[lyc]) not in [0,np.nan] else np.nan)),\n            'Comp Plan $': to_num(r_comp[plc]),\n            'Comp vs LY %': to_num((r_comp[pvc] if pvc is not None else ((to_num(r_comp[plc])/to_num(r_comp[lyc]) - 1)*100) if to_num(r_comp[lyc]) not in [0,np.nan] else np.nan)),\n            'Closed LY $': to_num(r_closed[lyc])\n        }\n        # Extract Summary Values key-value table from notes sheet (first two columns)\n        kv = {}\n        if dfn.shape[1] >= 2:\n            for i in range(len(dfn)):\n                k = str(dfn.iloc[i,0]).strip()\n                v = dfn.iloc[i,1]\n                if k and k.lower() in [\n                    'total plan $','total vs ly %','comp plan $','comp vs ly %','closed ly $']:\n                    kv[k] = v\n        if not kv:\n            return 0.0, \"Summary Values table not found\"\n        # normalize kv values\n        extracted = {}\n        for k in ['Total Plan $','Total vs LY %','Comp Plan $','Comp vs LY %','Closed LY $']:\n            val = kv.get(k)\n            if val is None and k.lower() in {kk.lower() for kk in kv.keys()}:\n                # case-insensitive fallback\n                for kk in kv.keys():\n                    if kk.lower() == k.lower():\n                        val = kv[kk]\n                        break\n            if isinstance(val, str):\n                val = val.replace('$','').replace(',','').replace('%','')\n            extracted[k] = pd.to_numeric(val, errors='coerce')\n        # compare with tolerances\n        matches = []\n        # dollar tolerance: max($100, 1%)\n        for k in ['Total Plan $','Comp Plan $','Closed LY $']:\n            a = float(extracted.get(k, np.nan))\n            b = float(roll_vals.get(k, np.nan))\n            if np.isnan(a) or np.isnan(b):\n                matches.append(0.0)\n            else:\n                tol = max(100.0, 0.01*abs(b))\n                matches.append(1.0 if abs(a-b) <= tol else 0.0)\n        # percent tolerance: 0.5 pp\n        for k in ['Total vs LY %','Comp vs LY %']:\n            a = float(extracted.get(k, np.nan))\n            b = float(roll_vals.get(k, np.nan))\n            if np.isnan(a) or np.isnan(b):\n                matches.append(0.0)\n            else:\n                matches.append(1.0 if abs(a-b) <= 0.5 else 0.0)\n        score = sum(matches)/len(matches)\n        return score, f\"Summary matches: {matches}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality Assessment (LLM)", "description": "Assess professional quality, clarity, and appropriateness of the deliverable for a Merchandise Planner audience.", "is_required": false, "max_points": 1.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professionalism and Communication Quality", "description": "Evaluate presentation quality, clarity of structure, and usefulness of the summary for decision-makers.", "weight": 1.0, "judge_prompt": "Evaluate the overall professional quality and communication effectiveness of the Excel deliverable. Do NOT re-check the strict structure (Stage 1) or numerical correctness (Stage 2). Focus on presentation and clarity for a Merchandise Planner audience.\n\nConsider:\n- Readability and formatting of By Door Plan (clear headers, consistent number formatting with $ and % where appropriate, no obvious clutter).\n- Rollup Summary readability (clearly labeled lines and columns, intuitive layout for weekly and monthly totals).\n- Planning Notes clarity: concise 1\u20132 sentence summary that communicates Total and Comp May plan $ and % vs LY, and calls out LY volume from closed stores.\n- Practical usability: filters/freeze panes, reasonable ordering/sorting, and any brief, helpful notes on assumptions or anomalies referenced from the Notes column.\n\nScoring:\n- 1.0: Highly professional and immediately usable by planning teams; excellent clarity and formatting.\n- 0.7: Generally professional with minor issues; fully usable.\n- 0.4: Adequate but cluttered/confusing in places; usable with effort.\n- 0.1: Barely usable presentation.\n- 0.0: Poor presentation quality or confusing enough to hinder use.", "expectation": "Well-formatted workbook with readable tables, clear labels, and a crisp summary that a Merchandise Planner could use immediately."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "75401f7c-396d-406d-b08e-938874ad1045", "rubric": {"category_name": "Motion Graphics/CGI Showreel (Video Editing)", "rationale": "Self-documenting delivery forces the editor to provide a verifiable cue sheet and technical spec alongside the MP4. Stage 1 demands a specific delivery shape (video + CSV cue sheet + tech/rights docs) that makes automated checks tractable. Stage 2 performs deterministic validations using the cue sheet and tech text to confirm constraints (duration limits, opening/closing shots, SFX placement, encoding, licensing). Stage 3 uses an LLM to judge creative and professional quality.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Delivery Shape Gate", "description": "LLM-only gate verifying the delivery includes the MP4 and self-documenting artifacts with the exact structures needed for verification.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 2.8, "rules": [{"type": "llm_judge", "name": "Structured Delivery Package Present", "description": "Verify the candidate provided a complete, verifiable delivery with the exact artifacts and structures below. Only check presence/structure, not correctness of content.", "weight": 4.0, "judge_prompt": "You are validating the SHAPE of the delivery for a video showreel (Goodsin Studios). Only check the presence and structural completeness of required artifacts. Do NOT judge the quality or the correctness of technical specs or timing. Be flexible on file naming but strict about required structures.\n\nRequired artifacts (all must be in the outputs):\n1) Final Showreel Video\n   - A single .mp4 file (1920x1080 target, H.264). Don\u2019t verify codec here\u2014only that there is an .mp4 video file.\n\n2) Cue Sheet (CSV) \u2014 exact column structure\n   - A .csv file containing a tabular cue/timeline with these columns (case-insensitive, flexible spacing/underscores allowed):\n     [Start | End | Duration | Visual Source | Audio Primary | SFX | Embedded Audio | Tags]\n   - Start/End/Duration should be filled per row (timecodes or seconds). Visual Source should name the source clip (e.g., logos.mp4, CastleExplosion(TyFlow+Phoenix).mp4, etc.).\n   - SFX can be semicolon-delimited where multiple apply. Embedded Audio is a boolean/yes-no. Tags are free-form keywords (e.g., fire, smoke, explosion, water, destruction, compositing, roto).\n\n3) Technical & Delivery Notes (document)\n   - A PDF, DOCX, or Markdown (.md) document that includes clearly labeled sections:\n     a) Export/Encoding Settings (container, codec, resolution, frame rate, audio channels/rate)\n     b) Duration Statement (total runtime)\n     c) SFX Mapping Summary (which SFX files were used with which shots)\n     d) Music/Audio Summary (music track name used; if different from the provided one, note that it is royalty-free)\n\n4) Music Licensing Evidence (can be its own PDF/DOCX/MD or a clearly marked section in the Technical & Delivery Notes)\n   - Must include: the music track name used, a link/URL to licensing or terms, and a statement indicating it is royalty-free or free to use without a license/attribution.\n\nStructural expectations to enable later checks (still shape-only):\n- The cue sheet must include entries for the opening (logos.mp4) and closing (logo_2.mp4) visual sources as first and last shots respectively (by order in the cue sheet). You are only checking that these entries exist in the table in the appropriate positions, not that the video itself is correct.\n- The SFX Mapping Summary section should mention the three SFX files by name (or close variants):\n  \u2022 Mountain Audio - Electricity.mp3 (paired with logos.mp4)\n  \u2022 ExplosionFire PS01_92.wav (paired with CastleExplosion(TyFlow+Phoenix).mp4)\n  \u2022 LargeMultiImpactsW PE280701.wav (paired with Shores_Comp_04222020.mp4)\n- The document should note that embedded production audio is only used in BuildingExplosion+Destruction(TyFlow+Phoenix).mp4 and Helicopter_DustSim(TyFlow+Phoenix).mp4.\n\nScoring (shape only):\n- 4.0: MP4 present + Cue Sheet CSV with all required columns + Tech/Delivery notes with the 4 labeled sections + Music licensing evidence (section or separate doc) + cue sheet shows first shot logos.mp4 and last shot logo_2.mp4 + SFX mapping summary lists the three required SFX and the allowed embedded-audio constraint is stated.\n- 3.0: MP4 + Cue Sheet CSV complete + Tech/Delivery notes mostly complete (missing one minor subsection) + licensing present; first/last shot noted; SFX mapping summary present but missing one detail.\n- 2.0: MP4 present + Cue Sheet CSV present but missing 1\u20132 required columns OR Tech/Delivery notes missing multiple sections OR licensing info too thin.\n- 1.0: Only MP4 present OR documents present but missing cue sheet structure.\n- 0.0: Not an MP4 or core artifacts missing.\n\nOnly evaluate the presence and structure, not whether contents are accurate.", "expectation": "A complete delivery set: MP4 + properly structured Cue Sheet CSV + Technical/Delivery document with the specified sections + Music licensing evidence, with cue sheet including first logos.mp4 and last logo_2.mp4 rows, and SFX mapping summary mentioning the three required SFX and embedded-audio constraint."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verifiable Correctness Checks", "description": "Deterministic, programmatic verification using the provided cue sheet and documents. Checks duration, required shot order, SFX placement, encoding specs, licensing proof, and policy constraints.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Delivery Files Present and Parsable", "description": "Find MP4, parse Cue Sheet CSV, and extract Technical/Delivery text. Partial credit if some pieces are missing but others are present.", "weight": 0.3, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs() or []\n        mp4_found = False\n        cue_df = None\n        tech_text = None\n\n        # Identify MP4\n        for r in outputs:\n            try:\n                p = context.files.get_path(r.id)\n                if str(p).lower().endswith('.mp4'):\n                    mp4_found = True\n                    break\n            except Exception:\n                continue\n\n        # Identify and parse Cue Sheet CSV\n        for r in outputs:\n            try:\n                p = context.files.get_path(r.id)\n                if str(p).lower().endswith('.csv'):\n                    df = context.files.read_csv(r.id)\n                    cols = [c.strip().lower() for c in df.columns]\n                    required = ['start','end','duration','visual source','audio primary','sfx','embedded audio','tags']\n                    if all(any(rc == c or rc.replace(' ','_') == c.replace(' ','_') for c in cols) for rc in required):\n                        # Normalize columns\n                        rename_map = {}\n                        for c in df.columns:\n                            lc = c.strip().lower().replace('  ',' ').replace('_',' ')\n                            if lc == 'visual source':\n                                rename_map[c] = 'visual_source'\n                            elif lc == 'audio primary':\n                                rename_map[c] = 'audio_primary'\n                            elif lc == 'embedded audio':\n                                rename_map[c] = 'embedded_audio'\n                            else:\n                                rename_map[c] = lc.replace(' ','_')\n                        cue_df = df.rename(columns=rename_map)\n                        break\n            except Exception:\n                continue\n\n        # Extract technical text from any MD/PDF/DOCX\n        def read_text_resource(res):\n            try:\n                p = context.files.get_path(res.id)\n                pl = str(p).lower()\n                if pl.endswith('.md') or (hasattr(res, 'is_text_format') and res.is_text_format):\n                    return context.files.read_text(res.id)\n                if pl.endswith('.pdf') or (hasattr(res, 'is_document') and res.is_document and pl.endswith('.pdf')):\n                    return context.files.read_pdf_text(res.id)\n                if pl.endswith('.docx') or (hasattr(res, 'is_document') and res.is_document and pl.endswith('.docx')):\n                    return context.files.read_docx_text(res.id)\n            except Exception:\n                return None\n            return None\n\n        texts = []\n        for r in outputs:\n            t = read_text_resource(r)\n            if t:\n                texts.append(t)\n        tech_text = '\\n\\n'.join(texts) if texts else None\n\n        score = 0.0\n        if mp4_found:\n            score += 0.34\n        if cue_df is not None:\n            score += 0.33\n        if tech_text and len(tech_text.strip()) > 50:\n            score += 0.33\n        # Cap to 1.0\n        return min(1.0, score)\n    except Exception as e:\n        return 0.0"}, {"type": "code", "name": "Runtime <= 01:20 (80s)", "description": "Check total runtime <= 80 seconds using cue sheet (prefer End column, else sum of Durations). Small tolerance allowed (<=82s partial).", "weight": 0.9, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    def to_seconds(val):\n        if pd.isna(val):\n            return None\n        if isinstance(val, (int, float)):\n            return float(val)\n        s = str(val).strip()\n        if not s:\n            return None\n        s = s.replace(',', '.')\n        # Match HH:MM:SS(.ms) or MM:SS(.ms) or SS(.ms) or HH:MM:SS:FF\n        parts = s.split(':')\n        try:\n            if len(parts) == 4:  # HH:MM:SS:FF assume 24 fps\n                hh, mm, ss, ff = parts\n                secs = int(hh)*3600 + int(mm)*60 + int(ss) + int(ff)/24.0\n                return float(secs)\n            if len(parts) == 3:\n                hh, mm, ss = parts\n                return int(hh)*3600 + int(mm)*60 + float(ss)\n            if len(parts) == 2:\n                mm, ss = parts\n                return int(mm)*60 + float(ss)\n            return float(s)\n        except Exception:\n            try:\n                return float(s)\n            except Exception:\n                return None\n\n    try:\n        outputs = context.get_all_outputs() or []\n        cue_df = None\n        for r in outputs:\n            try:\n                p = context.files.get_path(r.id)\n                if str(p).lower().endswith('.csv'):\n                    df = context.files.read_csv(r.id)\n                    cols = [c.strip().lower().replace('_',' ') for c in df.columns]\n                    if 'start' in cols and ('end' in cols or 'duration' in cols):\n                        # normalize\n                        rename = {}\n                        for c in df.columns:\n                            lc = c.strip().lower().replace('_',' ')\n                            if lc == 'start': rename[c] = 'start'\n                            elif lc == 'end': rename[c] = 'end'\n                            elif lc == 'duration': rename[c] = 'duration'\n                            else: rename[c] = lc.replace(' ','_')\n                        cue_df = df.rename(columns=rename)\n                        break\n            except Exception:\n                continue\n        if cue_df is None or cue_df.empty:\n            return 0.0\n\n        # Compute total runtime\n        end_secs = cue_df['end'].apply(to_seconds) if 'end' in cue_df.columns else None\n        dur_secs = cue_df['duration'].apply(to_seconds) if 'duration' in cue_df.columns else None\n\n        total = None\n        if end_secs is not None and end_secs.notna().any():\n            total = end_secs.max()\n        if (total is None or pd.isna(total)) and dur_secs is not None and dur_secs.notna().any():\n            total = dur_secs.fillna(0).sum()\n        if total is None or pd.isna(total):\n            return 0.0\n\n        if total <= 80.5:\n            return 1.0\n        elif total <= 82.0:\n            return 0.5\n        else:\n            return 0.0\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Opening and Closing Shots Correct", "description": "From the cue sheet ordering, first shot Visual Source should be logos.mp4, last should be logo_2.mp4.", "weight": 0.7, "code": "import pandas as pd\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs() or []\n        cue_df = None\n        for r in outputs:\n            try:\n                p = context.files.get_path(r.id)\n                if str(p).lower().endswith('.csv'):\n                    df = context.files.read_csv(r.id)\n                    cols = [c.strip().lower() for c in df.columns]\n                    if any('visual' in c and 'source' in c for c in cols):\n                        # normalize\n                        rename = {}\n                        for c in df.columns:\n                            lc = c.strip().lower().replace('_',' ')\n                            if lc == 'visual source': rename[c] = 'visual_source'\n                            else: rename[c] = c\n                        cue_df = df.rename(columns=rename)\n                        break\n            except Exception:\n                continue\n        if cue_df is None or cue_df.empty or 'visual_source' not in cue_df.columns:\n            return 0.0\n        vs = cue_df['visual_source'].astype(str).str.lower().tolist()\n        first_ok = len(vs) > 0 and ('logos.mp4' in vs[0])\n        last_ok = len(vs) > 0 and ('logo_2.mp4' in vs[-1])\n        if first_ok and last_ok:\n            return 1.0\n        if first_ok or last_ok:\n            return 0.5\n        return 0.0\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "SFX Placement Mapping", "description": "Verify required SFX are mapped to their corresponding shots in the cue sheet rows: Electricity->logos.mp4; ExplosionFire->CastleExplosion(TyFlow+Phoenix).mp4; LargeMultiImpactsW->Shores_Comp_04222020.mp4.", "weight": 1.0, "code": "import pandas as pd\nimport re\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs() or []\n        cue_df = None\n        for r in outputs:\n            try:\n                p = context.files.get_path(r.id)\n                if str(p).lower().endswith('.csv'):\n                    df = context.files.read_csv(r.id)\n                    cols = [c.strip().lower().replace('_',' ') for c in df.columns]\n                    if 'visual source' in cols and 'sfx' in cols:\n                        # normalize\n                        rename = {}\n                        for c in df.columns:\n                            lc = c.strip().lower().replace('_',' ')\n                            if lc == 'visual source': rename[c] = 'visual_source'\n                            elif lc == 'sfx': rename[c] = 'sfx'\n                            else: rename[c] = c\n                        cue_df = df.rename(columns=rename)\n                        break\n            except Exception:\n                continue\n        if cue_df is None or cue_df.empty:\n            return 0.0\n        def present_sfx(df, clip_key, sfx_key_patterns):\n            rows = df[df['visual_source'].astype(str).str.lower().str.contains(clip_key)]\n            if rows.empty:\n                return 0.0\n            ok_any = 0\n            for _, row in rows.iterrows():\n                s = str(row.get('sfx','')).lower()\n                if any(re.search(p, s) for p in sfx_key_patterns):\n                    ok_any = 1\n                    break\n            return float(ok_any)\n        # Patterns\n        logos_ok = present_sfx(cue_df, 'logos.mp4', [r'electric', r'mountain\\s*audio'])\n        castle_ok = present_sfx(cue_df, 'castleexplosion', [r'explosionfire', r'ps01', r'92'])\n        shores_ok = present_sfx(cue_df, 'shores_comp_04222020', [r'largemultiimpactsw', r'pe280701'])\n        # Average of checks\n        score = (logos_ok + castle_ok + shores_ok) / 3.0\n        return max(0.0, min(1.0, score))\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Embedded Audio Policy", "description": "Only BuildingExplosion+Destruction(TyFlow+Phoenix).mp4 and Helicopter_DustSim(TyFlow+Phoenix).mp4 may use embedded production audio; others should be false/no.", "weight": 0.3, "code": "import pandas as pd\n\ndef evaluate(workflow, context):\n    def truthy(x):\n        s = str(x).strip().lower()\n        return s in ['1','true','yes','y','on']\n    try:\n        outputs = context.get_all_outputs() or []\n        cue_df = None\n        for r in outputs:\n            try:\n                p = context.files.get_path(r.id)\n                if str(p).lower().endswith('.csv'):\n                    df = context.files.read_csv(r.id)\n                    cols = [c.strip().lower() for c in df.columns]\n                    if any('visual' in c and 'source' in c for c in cols) and any('embedded' in c and 'audio' in c for c in cols):\n                        # normalize\n                        rename = {}\n                        for c in df.columns:\n                            lc = c.strip().lower().replace('_',' ')\n                            if lc == 'visual source': rename[c] = 'visual_source'\n                            elif lc == 'embedded audio': rename[c] = 'embedded_audio'\n                            else: rename[c] = c\n                        cue_df = df.rename(columns=rename)\n                        break\n            except Exception:\n                continue\n        if cue_df is None or cue_df.empty:\n            return 0.0\n        allowed = ['buildingexplosion+destruction', 'helicopter_dustsim']\n        violations = 0\n        total_checks = 0\n        for _, row in cue_df.iterrows():\n            vs = str(row.get('visual_source','')).lower()\n            emb = row.get('embedded_audio', '')\n            if truthy(emb):\n                total_checks += 1\n                if not any(a in vs for a in allowed):\n                    violations += 1\n        if total_checks == 0:\n            return 1.0  # no embedded audio used; fully compliant\n        compliance = 1.0 - (violations / max(1, total_checks))\n        return max(0.0, min(1.0, compliance))\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Tech Specs: Container/Codec/Resolution/Audio", "description": "From technical notes text, verify: MP4 container, H.264/AVC codec, 1920x1080 resolution, stereo audio (48kHz preferred). Partial credit per item found.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs() or []\n        texts = []\n        def read_text(res):\n            try:\n                p = context.files.get_path(res.id)\n                pl = str(p).lower()\n                if pl.endswith('.md'):\n                    return context.files.read_text(res.id)\n                if pl.endswith('.pdf'):\n                    return context.files.read_pdf_text(res.id)\n                if pl.endswith('.docx'):\n                    return context.files.read_docx_text(res.id)\n            except Exception:\n                return None\n            return None\n        for r in outputs:\n            t = read_text(r)\n            if t:\n                texts.append(t)\n        text = ('\\n'.join(texts)).lower()\n        if not text:\n            return 0.0\n        checks = 0\n        total = 4\n        if re.search(r'\\bmp4\\b|container\\s*:\\s*mp4', text):\n            checks += 1\n        if re.search(r'h\\.264|avc1|h264|mpeg-4\\s+avc', text):\n            checks += 1\n        if re.search(r'1920\\s*[x\u00d7]\\s*1080|1080p', text):\n            checks += 1\n        if re.search(r'stereo|2\\.?0\\s*ch|2\\s*channels', text) and re.search(r'48\\s*k\\s*h?z|48000\\s*hz', text):\n            checks += 1\n        return checks / total\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Music Licensing Evidence", "description": "Check that a music file is named in the cue sheet and that the documents contain a URL and a statement indicating royalty-free/CC/no-attribution licensing.", "weight": 0.2, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs() or []\n        # Music in cue sheet\n        music_name = None\n        for r in outputs:\n            try:\n                p = context.files.get_path(r.id)\n                if str(p).lower().endswith('.csv'):\n                    df = context.files.read_csv(r.id)\n                    cols = [c.strip().lower().replace('_',' ') for c in df.columns]\n                    if 'audio primary' in cols:\n                        # get most frequent non-empty\n                        series = df[df.columns[cols.index('audio primary')]].astype(str).str.strip()\n                        vc = series[series != ''].value_counts()\n                        if len(vc) > 0:\n                            music_name = vc.idxmax()\n                            break\n            except Exception:\n                continue\n        # Documents text\n        texts = []\n        def read_text(res):\n            try:\n                p = context.files.get_path(res.id)\n                pl = str(p).lower()\n                if pl.endswith('.md'):\n                    return context.files.read_text(res.id)\n                if pl.endswith('.pdf'):\n                    return context.files.read_pdf_text(res.id)\n                if pl.endswith('.docx'):\n                    return context.files.read_docx_text(res.id)\n            except Exception:\n                return None\n            return None\n        for r in outputs:\n            t = read_text(r)\n            if t:\n                texts.append(t)\n        text = ('\\n'.join(texts)) if texts else ''\n        text_l = text.lower()\n        if not text:\n            return 0.0\n        url_ok = re.search(r'https?://\\S+', text) is not None\n        license_ok = any(k in text_l for k in ['royalty-free','royalty free','creative commons','cc0','no attribution','free to use'])\n        name_ok = (music_name is not None) and (music_name.lower() in text_l)\n        score = (1 if url_ok else 0) + (1 if license_ok else 0) + (1 if name_ok else 0)\n        return score/3.0\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Early Advanced Shots Evidenced by Tags", "description": "Within the first ~20 seconds (by Start time), at least one cue row should include Tags indicating advanced FX (water, fire, smoke, explosion, destruction).", "weight": 0.1, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    def to_seconds(val):\n        if pd.isna(val):\n            return None\n        if isinstance(val, (int, float)):\n            return float(val)\n        s = str(val).strip()\n        if not s:\n            return None\n        s = s.replace(',', '.')\n        parts = s.split(':')\n        try:\n            if len(parts) == 3:\n                hh, mm, ss = parts\n                return int(hh)*3600 + int(mm)*60 + float(ss)\n            if len(parts) == 2:\n                mm, ss = parts\n                return int(mm)*60 + float(ss)\n            return float(s)\n        except Exception:\n            try:\n                return float(s)\n            except Exception:\n                return None\n    try:\n        outputs = context.get_all_outputs() or []\n        cue_df = None\n        for r in outputs:\n            try:\n                p = context.files.get_path(r.id)\n                if str(p).lower().endswith('.csv'):\n                    df = context.files.read_csv(r.id)\n                    cols = [c.strip().lower().replace('_',' ') for c in df.columns]\n                    if 'start' in cols and 'tags' in cols:\n                        # normalize\n                        df = df.rename(columns={df.columns[cols.index('start')]: 'start', df.columns[cols.index('tags')]: 'tags'})\n                        cue_df = df\n                        break\n            except Exception:\n                continue\n        if cue_df is None or cue_df.empty:\n            return 0.0\n        advanced = ['water','fire','smoke','explosion','destruction']\n        ok = 0\n        for _, row in cue_df.iterrows():\n            st = to_seconds(row.get('start'))\n            if st is None:\n                continue\n            if st <= 20.0:\n                tags = str(row.get('tags','')).lower()\n                if any(a in tags for a in advanced):\n                    ok = 1\n                    break\n        return float(ok)\n    except Exception:\n        return 0.0"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Creative and Professional Quality", "description": "LLM qualitative assessment of editorial craft: pacing, sync, curation, sound design integration, polish, and client appeal.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Editorial Excellence and Presentation", "description": "Judge the overall creative and professional quality of the showreel relative to the brief.", "weight": 2.0, "judge_prompt": "Evaluate the final showreel video\u2019s creative and professional quality against the brief. Assume Stage 1/2 handled format and rules; you are judging aesthetic and editorial execution.\n\nConsider:\n1) Pacing and Rhythm: Is the cut energetic, tight, and varied? Are shot lengths appropriate for a high-energy reel under ~1:20?\n2) Music Sync: Do cuts and key impacts align to musical beats/drops reasonably often? Does the music support excitement without clashing?\n3) Shot Curation: Are the most advanced FX (water/fire/smoke/explosions/destruction, compositing, roto) highlighted early and showcased clearly?\n4) Sound Design Integration: Are required SFX tastefully integrated and balanced with music? Embedded audio moments feel intentional and not jarring?\n5) Visual Polish: Color consistency, exposure, stabilization, tasteful transitions, typography/brand moments (opening and end logos) feel professional.\n6) Narrative Flow and Branding: Strong open with logos.mp4, strong close with logo_2.mp4, logical flow with escalating or sustained intensity.\n\nScoring Guide:\n- 2.0: Outstanding editorial craft; compelling pacing; clear sync; excellent curation and polish; broadcast-ready.\n- 1.5: Strong overall; minor rough edges; effective pacing/sync; solid polish.\n- 1.0: Adequate; meets basic expectations but lacking in impact or consistency.\n- 0.5: Weak; uneven pacing/sync; mediocre curation; noticeable polish issues.\n- 0.0: Poor; disorganized; fails to convey high-energy or professional finish.\n\nFocus on audience impact and professional standards for a CGI/mograph studio reel.", "expectation": "A fast, cohesive, professionally polished reel that showcases advanced CG early, syncs to high-energy music, integrates SFX tastefully, and leaves a strong brand impression."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "788d2bc6-82df-4dc7-8467-a0f31405dc14", "rubric": {"category_name": "Sales Enablement Presentation: Amazon & TikTok Full-Stack Growth Partner", "rationale": "Pattern B (Document). The rubric enforces a strict, verifiable slide-deck shape first (LLM-only Stage 1 gate), then uses code heuristics to verify coverage and consistency (Stage 2), and concludes with an LLM quality assessment (Stage 3). Stage 1 demands a PDF deck with explicit, checkable slide-level elements and required service coverage so later verification is trivial.", "max_total_score": 20.0, "stages": [{"name": "Format and Structural Gate", "description": "MANDATORY gate. Ensures a modern, client-ready PDF deck (15\u201318 slides) with per-slide structure and required service coverage so subsequent checks are feasible.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Deck Structure, Coverage, and Format Compliance", "description": "Output must be a modern PDF presentation deck with 15\u201318 slides. Each slide should include a clear title, a 1\u20132 sentence summary, and concise bullets. Majority of slides should include visuals. Must cover specified service categories.", "weight": 8.0, "judge_prompt": "You are checking ONLY structure/format and presence of required elements, not the writing quality or persuasiveness.\n\nEvaluate the candidate output (primary file) for the following requirements:\n\nFormat requirements:\n- The output must be a PDF presentation deck (not DOCX, not PPTX screenshots, not plain text).\n- Approximately 15\u201318 slides. Allow small variance of \u00b11 slide only if the rest of the structure is excellent.\n- Modern, professional layout with consistent typography and spacing across slides.\n\nPer-slide structure requirements (apply to the majority of slides):\n- Each slide contains:\n  1) A clear slide title (headline)\n  2) A brief 1\u20132 sentence summary (can be a short paragraph)\n  3) A concise bulleted list (ideally \u22653 bullets)\n- Visual elements on most slides (icons, images, charts, creative samples, dashboards). Target: visuals present on \u2265 10 slides.\n\nCoverage requirements (dedicated slides; adjacent/combined acceptable if clearly delineated):\n1) Agency Overview/Positioning as a full-stack growth partner\n2) Amazon Account Management\n3) PPC Strategy (Amazon ads: Sponsored Products/Brands/Display/DSP)\n4) Creative Optimization (A+ Content, Brand Story, image revamps)\n5) TikTok Shop Setup\n6) Influencer/Creator Outreach (UGC/affiliate, creator sourcing/briefing)\n7) Analytics & Reporting (dashboards, KPIs)\n8) Review Generation/Programs (review velocity/compliance)\n9) Proof/Results (case studies, outcomes, testimonials, before/after) \u2013 at least one slide\n10) Process/Engagement Model or Onboarding\n11) Next Steps/Contact/CTA\n\nScoring (0\u20138 points):\n- 2 pts: PDF format and slide count 15\u201318 (award 1.5 if 14 or 19\u201320 with otherwise strong structure; 0 if not PDF or way off in count).\n- 3 pts: Per-slide structure present on the majority of slides (title + 1\u20132 sentence summary + bullets \u22653). Award proportionally (e.g., 3.0 if \u226512 slides comply; 2.0 if ~9\u201311; 1.0 if ~6\u20138; 0 if minimal/absent).\n- 1 pt: Visuals present on most slides (\u226510). 1.0 if yes, 0.5 if some visuals but <10, 0 if minimal/none.\n- 2 pts: Coverage of required categories (list above). Award proportionally based on how many are clearly present (2.0 all or nearly all; 1.5 if missing 1\u20132; 1.0 if missing 3\u20134; 0.5 if missing 5\u20136; 0 if very incomplete).\n\nImportant:\n- Be flexible with exact slide/section names; evaluate intent and structure.\n- Do NOT judge the accuracy of claims, image licensing, or writing quality here\u2014only check that the structure exists to verify later.\n- Return a single numeric score from 0 to 8 based on the guidance above.", "expectation": "A polished PDF deck with 15\u201318 slides, each containing title, short summary, bullets, and visuals on most slides. Includes all required service categories and a CTA."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Verification and Consistency Checks", "description": "Code-based heuristics verifying coverage breadth, structural signals, and presence of CTA using PDF text.", "is_required": false, "max_points": 7.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Document Format Confirmation (PDF)", "description": "Confirms the primary output is a PDF document to safely proceed with text-based checks.", "weight": 0.5, "code": "def evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output:\n            return 0.0\n        if not output.is_document:\n            return 0.0\n        # Prefer PDF strictly for this task\n        try:\n            path = context.files.get_path(output.id)\n            if path and str(path).lower().endswith('.pdf'):\n                return 0.5\n            else:\n                return 0.0\n        except Exception:\n            return 0.0\n    except Exception:\n        return 0.0\n"}, {"type": "code", "name": "Slide/Bullet Heuristic Strength", "description": "Heuristically estimates slide-like structure via title-like lines, total text length, and bullet presence.", "weight": 2.0, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 2.0\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        path = context.files.get_path(output.id)\n        if not str(path).lower().endswith('.pdf'):\n            return 0.0\n        text = context.files.read_pdf_text(output.id) or ''\n        if not text:\n            return 0.0\n        # Normalize\n        t = text.replace('\\r', '')\n        lines = [ln.strip() for ln in t.split('\\n') if ln.strip()]\n        # Bullet markers\n        bullet_pattern = re.compile(r'^(?:[\\u2022\\u2219\\u25E6\\u00B7\\-\\\u2013\\\u2014\\*]|\\d+\\.|[a-zA-Z]\\))\\s+')\n        bullet_count = sum(1 for ln in lines if bullet_pattern.match(ln))\n        # Title-like: reasonably short, not ending with period, mostly letters, not starting with bullet\n        def is_title(ln):\n            if len(ln) < 3 or len(ln) > 65:\n                return False\n            if bullet_pattern.match(ln):\n                return False\n            if ln.endswith('.') or ln.endswith(':'):\n                return False\n            letters = sum(ch.isalpha() for ch in ln)\n            if letters / max(1, len(ln)) < 0.5:\n                return False\n            # favor Title Case or ALL CAPS words\n            words = ln.split()\n            cap_words = sum(1 for w in words if (len(w)>1 and (w.isupper() or w.istitle())))\n            return cap_words >= max(1, int(0.5 * len(words)))\n        title_like = [ln for ln in lines if is_title(ln)]\n        # Deduplicate titles by lowercase\n        seen = set()\n        unique_titles = []\n        for ln in title_like:\n            key = ln.lower()\n            if key not in seen:\n                seen.add(key)\n                unique_titles.append(ln)\n        title_count = len(unique_titles)\n        # Scoring components\n        char_len = len(t)\n        s1 = min(char_len / 1200.0, 1.0) * 0.5  # text mass proxy\n        s2 = min(bullet_count / 10.0, 1.0) * 0.5  # bullet presence proxy\n        s3 = min(title_count / 12.0, 1.0) * 1.0   # slide title proxy\n        score = s1 + s2 + s3\n        # Cap to weight\n        return max(0.0, min(score, weight))\n    except Exception:\n        return 0.0\n"}, {"type": "code", "name": "Amazon Coverage Keywords", "description": "Checks breadth of Amazon-related coverage (account mgmt, ads, creative, retail ops).", "weight": 2.0, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 2.0\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        path = context.files.get_path(output.id)\n        if not str(path).lower().endswith('.pdf'):\n            return 0.0\n        text = (context.files.read_pdf_text(output.id) or '').lower()\n        terms = [\n            'amazon', 'seller central', 'vendor central', 'asin', 'storefront', 'brand registry',\n            'a+ content', 'brand story', 'listing optimization', 'image revamp', 'catalog',\n            'fba', 'fbm', 'fulfillment', 'buy box', 'account management',\n            'ppc', 'sponsored products', 'sponsored brands', 'sponsored display', 'dsp', 'acos', 'roas'\n        ]\n        matched = set()\n        for term in terms:\n            if term in text:\n                matched.add(term)\n        # Reward broad coverage; need ~8+ unique matches for full credit\n        coverage = min(len(matched) / 8.0, 1.0)\n        return coverage * weight\n    except Exception:\n        return 0.0\n"}, {"type": "code", "name": "TikTok Coverage Keywords", "description": "Checks breadth of TikTok Shop and creator-led growth coverage.", "weight": 2.0, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 2.0\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        path = context.files.get_path(output.id)\n        if not str(path).lower().endswith('.pdf'):\n            return 0.0\n        text = (context.files.read_pdf_text(output.id) or '').lower()\n        terms = [\n            'tiktok shop', 'tiktok ads', 'spark ads', 'ugc', 'creator', 'influencer', 'affiliate',\n            'creator marketplace', 'live shopping', 'live', 'shopping ads', 'product seeding',\n            'briefing', 'content calendar', 'whitelisting'\n        ]\n        matched = set()\n        for term in terms:\n            if term in text:\n                matched.add(term)\n        coverage = min(len(matched) / 6.0, 1.0)\n        return coverage * weight\n    except Exception:\n        return 0.0\n"}, {"type": "code", "name": "CTA / Contact Presence", "description": "Looks for a clear call-to-action or contact details (Next Steps, email, scheduling link).", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 0.5\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        path = context.files.get_path(output.id)\n        if not str(path).lower().endswith('.pdf'):\n            return 0.0\n        text = (context.files.read_pdf_text(output.id) or '').lower()\n        patterns = [\n            'next steps', 'contact', 'get in touch', 'email', 'schedule', 'book a call', 'calendly',\n            'reach out', 'meeting', 'call us', 'scan the qr', 'qr code', 'www.', 'http://', 'https://'\n        ]\n        email_re = re.compile(r'[a-z0-9._%+-]+@[a-z0-9.-]+\\.[a-z]{2,}')\n        has_keyword = any(pat in text for pat in patterns)\n        has_email = email_re.search(text) is not None\n        return weight if (has_keyword or has_email) else 0.0\n    except Exception:\n        return 0.0\n"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Quality and Audience Fit", "description": "LLM assessment of professional polish, narrative flow, visual coherence, and appropriateness for CEOs/founders/brand leads.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professionalism, Clarity, and Persuasive Fit", "description": "Holistic quality check: premium yet approachable tone, coherent flow, visual consistency, and executive-readiness.", "weight": 5.0, "judge_prompt": "Evaluate the overall quality of the PDF deck for:\n- Professional polish and consistency (typography, spacing, color use, alignment)\n- Clear, concise messaging that is easy to digest (titles, 1\u20132 sentence summaries, bullet clarity)\n- Visual coherence and premium feel (icons/images/charts that support the message without clutter)\n- Logical narrative for executive buyers (overview \u2192 capabilities \u2192 proof \u2192 process \u2192 next steps)\n- Appropriateness for CEOs/founders/brand leads (strategic framing, outcomes-oriented language)\n\nScoring (0\u20135):\n- 5: Exceptional polish and clarity; compelling narrative; visuals elevate the story; highly executive-ready.\n- 4: Strong and professional; minor inconsistencies; clear value story.\n- 3: Solid but some rough edges or clutter; message mostly clear.\n- 2: Noticeable inconsistencies; uneven clarity; visuals or structure hinder comprehension.\n- 1: Low polish; confusing flow; visuals poorly used.\n- 0: Not a professional deck.\n\nDo not re-check structural compliance already handled in Stage 1; focus on overall quality and suitability.\n", "expectation": "A premium, executive-ready deck with coherent story, crisp copy, and tasteful visuals aligned to Amazon and TikTok growth services."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "15ddd28d-8445-4baa-ac7f-f41372e1344e", "rubric": {"category_name": "Manufacturing - Buyers and Purchasing Agents: Negotiation Strategy (Modlev Tail Lamp)", "rationale": "Pattern B (Document). The task requires a 2\u20133-page professional negotiation strategy in DOCX/PDF with specific sections and verifiable elements. Stage 1 uses LLM-only gating to enforce structure and format. Stage 2 mixes code rules to verify presence of key facts, constraints, and negotiation concepts referenced in the brief. Stage 3 applies LLM quality assessment for executive readability and strategic coherence.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Format and Structural Gate", "description": "LLM-only gate to ensure the document is a 2\u20133 page PDF/DOCX with required sections and mandated elements, enabling verification in later stages.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Document Format + Core Sections Present", "description": "Check the file type, length, professional formatting, and presence of core section headers.", "weight": 3.0, "judge_prompt": "You are the Stage 1 shape-enforcement judge. Inspect the PRIMARY OUTPUT only.\n\nGoal: Verify STRICT structure and format (not content quality). Be flexible on exact header names but insist the sections are clearly present.\n\nFormat Requirements:\n- Must be a PDF or DOCX (not plain text/Excel).\n- Length target: 2\u20133 pages (\u00b10.5 page acceptable if content is dense and clearly sectioned).\n- Professional formatting with visible section headers.\n\nRequired Core Sections (headers or equivalent clearly visible):\n1) Title indicating strategy for Modlev tail lamp (e.g., \u201cModlev Tail Lamp Negotiation Strategy\u201d or similar).\n2) Situation Summary and Objectives.\n3) Immediate Risk Mitigation (next 3 weeks) for supply continuity.\n4) Preferred Path with LPI (genuine engagement to understand supplier issues).\n5) Negotiation Strategy including explicit reference to BATNA and ZOPA.\n6) Local Sourcing Transition Plan with a timeline.\n7) Tooling Ownership and Exit Plan.\n8) Action Plan with owners and dates (or RACI-style responsibilities).\n\nScoring (return a single numeric score out of 3.0):\n- 3.0: Valid PDF/DOCX length, and \u22657/8 required sections clearly present, including items (5) and (6).\n- 2.0: Valid PDF/DOCX length, and 5\u20136 sections present.\n- 1.0: Valid PDF/DOCX length, and 3\u20134 sections present.\n- 0.5: Valid PDF/DOCX length, and 2 sections present.\n- 0.0: Not PDF/DOCX OR fewer than 2 sections OR clearly not within ~2\u20133 pages.\n\nOnly evaluate structural presence and format. Do NOT assess calculation correctness or strategic quality.", "expectation": "A professionally formatted 2\u20133 page PDF/DOCX with all major sections present, enabling deterministic verification of references and timelines later."}, {"type": "llm_judge", "name": "Mandated Negotiation Elements + Structured Table Present", "description": "Check if the document explicitly contains BATNA and ZOPA terms, at least one structured table (timeline/action plan or risk register), and visible negotiation levers list.", "weight": 1.0, "judge_prompt": "You are still in Stage 1 (shape enforcement). Check the PRIMARY OUTPUT for the presence of the following mandated elements:\n\nMandated Elements:\n- Explicit mention of both BATNA and ZOPA (either the acronyms or the full phrases).\n- At least one structured table for either: (a) Timeline/Action Plan (with dates/owners/milestones), or (b) Risks/Assumptions.\n- A visible subsection or bullet list of negotiation levers (e.g., flexible delivery, advance payments, clean exit clauses, residual low-volume business), at least two levers listed.\n\nScoring (return a single numeric score out of 1.0):\n- 1.0: All three present (BATNA + ZOPA, \u22651 table, \u22652 levers).\n- 0.7: Any two present.\n- 0.4: Any one present.\n- 0.0: None present.\n\nDo not judge quality\u2014only confirm presence/visibility of these elements.", "expectation": "The document explicitly names BATNA and ZOPA, includes a structured table (timeline/action plan or risk register), and lists at least two negotiation levers."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Facts, Constraints, and Internal Consistency)", "description": "Code rules verify that key facts and constraints from the brief are referenced and that the plan reflects realistic timelines and core concepts.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Key Facts Referenced (Demand, Capacity, Transfer Times, Dev Durations)", "description": "Checks that the document references critical numeric facts: 800/month demand; LPI capacity 1,500 and/or 2,500; 25-day tooling transfer; 3\u20134 months plastics and 4\u20135 months electronics redevelopment durations.", "weight": 1.2, "code": "import re\\n\\n\\ndef evaluate(workflow, context):\\n    \\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0, 'No document output.'\\n\\n    text = ''\\n    # Try reading PDF then DOCX, then fallback\\n    try:\\n        text = context.files.read_pdf_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_docx_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_text(output.id)\\n            except Exception:\\n                return 0.0, 'Could not read document text.'\\n\\n    t = text.lower()\\n\\n    score = 0.0\\n    weight = 1.2\\n    parts = []\\n\\n    # 800 monthly demand\\n    has_800 = re.search(r'\\\\b800\\\\b', t) is not None\\n    if has_800:\\n        score += weight * (1/5)\\n        parts.append('800/month mentioned')\\n\\n    # LPI capacity 1,500 or 2,500\\n    has_1500 = re.search(r'\\\\b1[ ,]?500\\\\b', t) is not None\\n    has_2500 = re.search(r'\\\\b2[ ,]?500\\\\b', t) is not None\\n    if has_1500 or has_2500:\\n        score += weight * (1/5)\\n        parts.append('Capacity 1500/2500 mentioned')\\n\\n    # 25-day tooling transfer\\n    has_25_days = re.search(r'\\\\b25\\\\b\\s*[-\u2013]?\\s*day', t) or re.search(r'\\\\b25\\\\b.*?day', t)\\n    if has_25_days:\\n        score += weight * (1/5)\\n        parts.append('25-day transfer mentioned')\\n\\n    # 3\u20134 months (plastics)\\n    has_3_4_mo = (re.search(r'3\\s*[-\u2013to]+\\s*4\\s+month', t) or re.search(r'3\\s*(to|\u2013|-)\\s*4\\s+month', t)) is not None\\n    if has_3_4_mo:\\n        score += weight * (1/5)\\n        parts.append('3\u20134 months mentioned')\\n\\n    # 4\u20135 months (electronics)\\n    has_4_5_mo = (re.search(r'4\\s*[-\u2013to]+\\s*5\\s+month', t) or re.search(r'4\\s*(to|\u2013|-)\\s*5\\s+month', t)) is not None\\n    if has_4_5_mo:\\n        score += weight * (1/5)\\n        parts.append('4\u20135 months mentioned')\\n\\n    # Bound score to weight\\n    score = min(score, weight)\\n    feedback = ', '.join(parts) if parts else 'No key numeric facts found.'\\n    return score, feedback\\n"}, {"type": "code", "name": "BATNA and ZOPA Explicitly Defined", "description": "Checks for explicit mention of both BATNA and ZOPA (or their full forms).", "weight": 1.0, "code": "import re\\n\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0\\n\\n    text = ''\\n    try:\\n        text = context.files.read_pdf_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_docx_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_text(output.id)\\n            except Exception:\\n                return 0.0\\n\\n    t = text.lower()\\n\\n    has_batna = ('batna' in t) or ('best alternative to a negotiated agreement' in t)\\n    has_zopa = ('zopa' in t) or ('zone of possible agreement' in t)\\n\\n    score = 0.0\\n    weight = 1.0\\n    if has_batna and has_zopa:\\n        score = weight\\n    elif has_batna or has_zopa:\\n        score = weight * 0.5\\n    else:\\n        score = 0.0\\n\\n    return score\\n"}, {"type": "code", "name": "Timeline Structure and Parallelization Signals", "description": "Checks for signals that timelines are planned realistically and in parallel: mentions of weeks/months, plastics and electronics, and words like parallel/concurrent.", "weight": 1.0, "code": "import re\\n\\n\\nPARALLEL_WORDS = ['parallel', 'concurrent', 'simultaneous', 'simultaneously']\\n\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0, 'No document.'\\n\\n    text = ''\\n    try:\\n        text = context.files.read_pdf_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_docx_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_text(output.id)\\n            except Exception:\\n                return 0.0, 'Unreadable.'\\n\\n    t = text.lower()\\n    weight = 1.0\\n    score = 0.0\\n    info = []\\n\\n    # Parallel/concurrent signal\\n    if any(w in t for w in PARALLEL_WORDS):\\n        score += 0.3 * weight\\n        info.append('parallel/concurrent mentioned')\\n\\n    # Mentions of both plastics and electronics\\n    has_plastic = re.search(r'plastic', t) is not None\\n    has_elec = re.search(r'electronic', t) is not None\\n    if has_plastic and has_elec:\\n        score += 0.3 * weight\\n        info.append('plastics + electronics both mentioned')\\n\\n    # Mentions weeks for immediate risk window\\n    if re.search(r'\\\\bweek(s)?\\\\b', t) or re.search(r'\\\\b3\\\\s*week', t) or re.search(r'\\\\b21\\\\s*day', t):\\n        score += 0.2 * weight\\n        info.append('weeks/near-term window mentioned')\\n\\n    # Mentions months for development timeline\\n    if re.search(r'\\\\bmonth(s)?\\\\b', t):\\n        score += 0.2 * weight\\n        info.append('months mentioned')\\n\\n    score = min(score, weight)\\n    return score, '; '.join(info) if info else 'No timeline signals found.'\\n"}, {"type": "code", "name": "Tooling Ownership and Transfer Plan Referenced", "description": "Checks for presence of tooling ownership and transfer plan signals (tooling, ownership/owned by LiIon, transfer/ship/hand-over, 25 days/Korea/India).", "weight": 0.8, "code": "import re\\n\\n\\nKEY_OWNERSHIP = ['owned', 'ownership', 'owner']\\nKEY_TRANSFER = ['transfer', 'handover', 'hand-over', 'ship', 'shipping', 'logistics', 'repatriation', 'repatriate']\\nKEY_GEO = ['korea', 'south korea', 'india']\\n\\n\\ndef evaluate(workflow, context):\\n    output = context.get_primary_output()\\n    if not output or not output.is_document:\\n        return 0.0, 'No document.'\\n\\n    text = ''\\n    try:\\n        text = context.files.read_pdf_text(output.id)\\n    except Exception:\\n        try:\\n            text = context.files.read_docx_text(output.id)\\n        except Exception:\\n            try:\\n                text = context.files.read_text(output.id)\\n            except Exception:\\n                return 0.0, 'Unreadable.'\\n\\n    t = text.lower()\\n    weight = 0.8\\n    score = 0.0\\n    info = []\\n\\n    # Must mention tooling\\n    if 'tooling' not in t and 'tool' not in t and 'mould' not in t and 'mold' not in t:\\n        return 0.0, 'No tooling mentioned.'\\n\\n    info.append('tooling mentioned')\\n    score += 0.2 * weight\\n\\n    # Ownership signals (preferably LiIon)\\n    if any(k in t for k in KEY_OWNERSHIP):\\n        score += 0.2 * weight\\n        info.append('ownership/owned mentioned')\\n\\n    if ('liion' in t) or ('li ion' in t) or ('liion motors' in t):\\n        score += 0.1 * weight\\n        info.append('LiIon referenced')\\n\\n    # Transfer/logistics signals\\n    if any(k in t for k in KEY_TRANSFER):\\n        score += 0.2 * weight\\n        info.append('transfer/logistics plan mentioned')\\n\\n    # 25-day or route geography signals\\n    if re.search(r'\\\\b25\\\\b\\s*[-\u2013]?\\s*day', t) or '25 days' in t:\\n        score += 0.1 * weight\\n        info.append('25-day transfer window mentioned')\\n\\n    if any(g in t for g in KEY_GEO):\\n        score += 0.2 * weight\\n        info.append('geography (Korea/India) mentioned')\\n\\n    score = min(score, weight)\\n    return score, '; '.join(info)\\n"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Executive Readiness", "description": "LLM assessment of overall professional quality, practicality, and negotiation strategy coherence for executive use.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Executive-Level Quality and Practicality", "description": "Assess clarity, concision, structure, and practicality for executive negotiations.", "weight": 1.2, "judge_prompt": "Evaluate the PRIMARY OUTPUT for executive readability and practical usefulness in an executive negotiation setting. Consider: (a) clear, logical flow; (b) concise, scannable structure (headers, bullets, table(s)); (c) actionable guidance that a CPO/CEO could use in a live meeting; (d) professionalism and appropriate tone. Do not re-score structure requirements from Stage 1; focus on quality.\n\nScoring out of 1.2:\n- 1.2: Outstanding executive-ready brief: crisp, pragmatic, well-structured, immediately usable in negotiations.\n- 0.9: Strong, minor issues (slightly wordy or a few unclear linkages) but still very usable.\n- 0.6: Adequate but mixed: some actionable content yet cluttered or generic in parts.\n- 0.3: Weak: disorganized or mostly generic, limited practical value.\n- 0.0: Poor quality, not useful for executives.", "expectation": "A crisp, pragmatic, and well-structured 2\u20133-page brief that executives can use directly."}, {"type": "llm_judge", "name": "Negotiation Coherence and Risk Coverage", "description": "Assess if the strategy coherently addresses continuity risk, leverages negotiation levers, plans a diplomatic exit, and aligns with stated constraints.", "weight": 0.8, "judge_prompt": "Assess whether the negotiation strategy is coherent and risk-aware: (a) shows a credible plan to avoid production disruption over the next three weeks; (b) uses negotiation levers appropriately (flexible deliveries, advance payments, clean exit clauses, residual low-volume business, etc.); (c) proposes a diplomatic handling of LPI exit while protecting LiIon\u2019s interests and tooling; (d) aligns with brief constraints (demand 800/month; LPI capacity 1500\u20132500; 25-day tooling transfer; 3\u20134 months plastics, 4\u20135 months electronics, parallelizable). Focus on coherence and feasibility, not exact calculations.\n\nScoring out of 0.8:\n- 0.8: Highly coherent and feasible; addresses continuity risk and exit diplomacy with clear, realistic levers and governance.\n- 0.6: Generally coherent; minor feasibility gaps.\n- 0.4: Partial; misses key risks or misuses constraints.\n- 0.2: Weak; largely generic or infeasible.\n- 0.0: Not coherent or ignores key constraints.", "expectation": "A negotiation plan that is coherent, risk-aware, and clearly aligned with the constraints and timeline realities."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "d025a41c-c439-4ee1-bc79-dd5c94b27a2d", "rubric": {"category_name": "Finance/Insurance CSR Coaching \u2013 Case Feedback (Structured Doc + Verifiable Coaching)", "rationale": "This rubric enforces a self-documenting, verifiable deliverable for coaching feedback on three live-chat cases. Stage 1 (LLM-only) mandates a strict DOCX structure with labeled triplets (Problematic Statement, Why it\u2019s problematic, Improved Alternative) so verification is trivial. Stage 2 mixes code and LLM to validate per-case coverage, explanation length (1\u20133 sentences), and that alternatives are meaningfully different, plus an etiquette-alignment check. Stage 3 evaluates overall usefulness and professional quality for a banking CSR audience.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2013 Document Shape Gate (MANDATORY)", "description": "Enforce exact deliverable shape so verification is possible. Checks DOCX format, bold section headings, labeled triplets, minimum coverage, 1.5 spacing, and <5 pages.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured DOCX with Labeled Triplets", "description": "Output must be a Word document titled \u201cCase Feedback\u201d with bold section headers for each case and per-item labeled triplets enabling verification.", "weight": 4.0, "judge_prompt": "You are evaluating the candidate\u2019s output ONLY for required FORMAT and STRUCTURE (not the content quality). Use the following checklist and score strictly by structure.\n\nRequired file/format:\n- Must be a Word document (DOCX). Not PDF, not plain text.\n- The document filename includes \u201cCase Feedback\u201d OR the top title reads \u201cCase Feedback\u201d.\n\nLayout and formatting:\n- The document uses 1.5 line spacing consistently throughout.\n- Total length is under 5 pages.\n\nSection structure (must be clearly visible):\n- Three bold, top-level section headers with these exact titles: \u201cCase One\u201d, \u201cCase Two\u201d, \u201cCase Three\u201d.\n- Under each case header, a list (bulleted or numbered) of items.\n- Each list item must contain THREE clearly labeled parts in this order:\n  1) \u201cProblematic Statement:\u201d \u2013 the rep\u2019s original wording (quoted or precisely paraphrased)\n  2) \u201cWhy it\u2019s problematic:\u201d \u2013 a 1\u20133 sentence explanation\n  3) \u201cImproved Alternative:\u201d \u2013 a revised customer-facing statement\n- Each case must include at least 3 such items (triplets). More is fine, but at least 3 per case are required.\n\nScoring rubric (STRUCTURE ONLY):\n- 4.0: DOCX; has \u201cCase Feedback\u201d title/filename; all three bold case headers present; each case has \u22653 items; every item contains all three required labels in the correct order; 1.5 spacing throughout; <5 pages.\n- 3.0: Minor deviations but still fully verifiable: e.g., one case has only 2 items; or headers slightly vary (e.g., \u201cCase 1\u201d instead of \u201cCase One\u201d); or 1.5 spacing appears mostly consistent; still <5 pages and labeled triplets are used throughout.\n- 2.0: Partially structured but insufficient for full verification: DOCX with case sections present but missing labels or mixed order; or fewer than 2 items in any case; or no clear list format; still <5 pages.\n- 0.0: Not a DOCX; missing multiple case sections; no labeled triplets; exceeds 5 pages; or otherwise fails the shape requirements.\n\nOnly evaluate the presence and structure. Do NOT judge correctness or quality of the advice.\n", "expectation": "A DOCX file named/titled \u201cCase Feedback\u201d with bold \u201cCase One/Two/Three\u201d headers and, under each, \u22653 list items using the exact labeled triplet format: Problematic Statement, Why it\u2019s problematic (1\u20133 sentences), Improved Alternative; 1.5 line spacing; <5 pages."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification (Content Validity & Basic Consistency)", "description": "Now that the shape is enforced, verify measurable correctness aspects: per-case coverage, explanation length (1\u20133 sentences), and that alternatives are meaningfully different. Also check alignment with live chat etiquette best practices.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Per-Case Triplet Presence and Counts", "description": "Checks each case section contains at least 3 labeled triplets by counting occurrences of \u201cProblematic Statement:\u201d per case. Scores by coverage proportion (capped at 3 per case).", "weight": 1.6, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Args:\n        workflow: Workflow object\n        context: ValidationContext with .files accessor\n    Returns:\n        float or (float, str)\n    \"\"\"\n    weight = 1.6\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output resource found.\"\n    if not output.is_document:\n        return 0.0, \"Primary output is not a document.\"\n\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    if not text:\n        return 0.0, \"Could not read document text.\"\n\n    # Build case sections\n    lower = text.lower()\n    # Find case heading positions (flexible: one/two/three or 1/2/3)\n    pattern = re.compile(r'\\bcase\\s*(one|two|three|1|2|3)\\b', re.I)\n    matches = list(pattern.finditer(text))\n    # Map normalized key\n    def norm_case(g):\n        g = g.lower()\n        return {'one':'one','1':'one','i':'one', 'two':'two','2':'two','ii':'two', 'three':'three','3':'three','iii':'three'}.get(g, g)\n\n    # Build ordered sections\n    sections = {}\n    if matches:\n        # Append end sentinel\n        bounds = []\n        for m in matches:\n            grp = m.group(1)\n            bounds.append((norm_case(grp), m.start()))\n        bounds.sort(key=lambda x: x[1])\n        for idx, (ckey, start) in enumerate(bounds):\n            end = bounds[idx+1][1] if idx+1 < len(bounds) else len(text)\n            if ckey in ('one','two','three'):\n                sections[ckey] = text[start:end]\n    # Fallback: naive split if headings not found\n    for key in ('one','two','three'):\n        if key not in sections:\n            # Try simple keyword search\n            pos = lower.find(f\"case {key}\")\n            if pos != -1:\n                end = len(text)\n                sections[key] = text[pos:end]\n\n    # Count triplets per case via the label \"Problematic Statement:\"\n    label_re = re.compile(r'problematic\\s*statement\\s*:', re.I)\n    counts = {}\n    for key in ('one','two','three'):\n        sec = sections.get(key, '')\n        counts[key] = len(label_re.findall(sec)) if sec else 0\n\n    # Score: average over cases, each capped at 3\n    per_case_scores = []\n    for key in ('one','two','three'):\n        c = counts.get(key, 0)\n        per_case_scores.append(min(c, 3) / 3.0)\n    avg_ratio = sum(per_case_scores) / 3.0 if per_case_scores else 0.0\n    score = weight * max(0.0, min(1.0, avg_ratio))\n\n    feedback = f\"Triplet counts \u2014 Case One: {counts.get('one',0)}, Case Two: {counts.get('two',0)}, Case Three: {counts.get('three',0)}.\"\n    return score, feedback"}, {"type": "code", "name": "Explanation Length = 1\u20133 Sentences", "description": "Parses each item\u2019s \u201cWhy it\u2019s problematic:\u201d text and checks it contains 1\u20133 sentences. Scores by proportion of items meeting the constraint.", "weight": 1.2, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 1.2\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output resource found.\"\n    if not output.is_document:\n        return 0.0, \"Primary output is not a document.\"\n\n    # Read text (favor DOCX)\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    if not text:\n        return 0.0, \"Could not read document text.\"\n\n    # Regex to capture triplets (non-greedy, tolerant to newlines)\n    item_re = re.compile(\n        r\"Problematic\\s*Statement\\s*:\\s*(?P<orig>.*?)\\s*Why\\s*it'?s\\s*problematic\\s*:\\s*(?P<why>.*?)\\s*Improved\\s*Alternative\\s*:\\s*(?P<alt>.*?)(?=Problematic\\s*Statement\\s*:|Case\\s*(One|Two|Three|1|2|3)\\b|\\Z)\",\n        re.I | re.S,\n    )\n\n    items = list(item_re.finditer(text))\n    if not items:\n        return 0.0, \"No labeled triplets detected.\"\n\n    def count_sentences(s):\n        # Split on sentence enders, keep non-empty with at least one letter\n        parts = re.split(r\"(?<=[.!?])\\s+\", s.strip())\n        parts = [p.strip() for p in parts if re.search(r\"[A-Za-z]\", p or \"\")]  # filter empty/non-text\n        return len(parts)\n\n    valid = 0\n    total = 0\n    for m in items:\n        why = (m.group('why') or '').strip()\n        if not why:\n            total += 1\n            continue\n        n = count_sentences(why)\n        if 1 <= n <= 3:\n            valid += 1\n        total += 1\n\n    ratio = (valid / total) if total else 0.0\n    score = weight * max(0.0, min(1.0, ratio))\n    return score, f\"Explanation sentence-length within 1\u20133 for {valid}/{total} items.\""}, {"type": "code", "name": "Alternatives Are Meaningfully Different", "description": "Checks that each \u201cImproved Alternative\u201d is not a trivial restatement of the original statement using a simple token Jaccard similarity. Rewards items with Jaccard < 0.85.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    weight = 0.8\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output resource found.\"\n    if not output.is_document:\n        return 0.0, \"Primary output is not a document.\"\n\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    if not text:\n        return 0.0, \"Could not read document text.\"\n\n    item_re = re.compile(\n        r\"Problematic\\s*Statement\\s*:\\s*(?P<orig>.*?)\\s*Why\\s*it'?s\\s*problematic\\s*:\\s*(?P<why>.*?)\\s*Improved\\s*Alternative\\s*:\\s*(?P<alt>.*?)(?=Problematic\\s*Statement\\s*:|Case\\s*(One|Two|Three|1|2|3)\\b|\\Z)\",\n        re.I | re.S,\n    )\n    items = list(item_re.finditer(text))\n    if not items:\n        return 0.0, \"No labeled triplets detected.\"\n\n    def normalize(s):\n        s = s.lower()\n        # Remove quotes/punctuation, keep letters/numbers/spaces\n        s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n        s = re.sub(r\"\\s+\", \" \", s).strip()\n        return s\n\n    def jaccard(a, b):\n        ta = set([t for t in normalize(a).split() if t])\n        tb = set([t for t in normalize(b).split() if t])\n        if not ta or not tb:\n            return 0.0 if (ta or tb) else 1.0\n        inter = len(ta & tb)\n        union = len(ta | tb)\n        return inter / union if union else 1.0\n\n    good = 0\n    total = 0\n    for m in items:\n        orig = (m.group('orig') or '').strip()\n        alt = (m.group('alt') or '').strip()\n        if not orig or not alt:\n            total += 1\n            continue\n        sim = jaccard(orig, alt)\n        # Consider meaningfully different if Jaccard similarity < 0.85 and not identical after normalization\n        if sim < 0.85 and normalize(orig) != normalize(alt):\n            good += 1\n        total += 1\n\n    ratio = (good / total) if total else 0.0\n    score = weight * max(0.0, min(1.0, ratio))\n    return score, f\"Alternatives meaningfully different for {good}/{total} items.\""}, {"type": "llm_judge", "name": "Etiquette Alignment and Specificity", "description": "Judge whether explanations correctly identify issues (tone, empathy, clarity, ownership, personalization, proactive updates) and alternatives reflect live-chat best practices for banking CSRs.", "weight": 0.4, "judge_prompt": "Evaluate whether the feedback aligns with live-chat etiquette best practices and is specific to banking customer service contexts. Consider:\n- Explanations accurately identify issues (e.g., lack of empathy, robotic tone, jargon, blame, insufficient ownership, missing next steps/status updates, overpromising, lack of personalization, not confirming resolution).\n- Alternatives demonstrate best practices: empathetic phrasing, positive framing, clear next steps/ownership, concise and friendly language, appropriate compliance/safety (no sensitive details or promises outside policy), and bank-relevant wording.\n- Feedback is concrete and actionable (not generic platitudes).\n\nScoring:\n- 0.4: Strong alignment on all dimensions; alternatives clearly improve customer experience in a bank context.\n- 0.2\u20130.3: Mostly aligned with minor gaps.\n- 0.1: Superficial or generic alignment; limited actionability.\n- 0.0: Misguided or contrary to best practices.\n", "expectation": "Items diagnose real chat-quality issues and propose clearly better, empathetic, compliant, and customer-friendly alternatives suited to banking support."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Overall Quality and Usefulness", "description": "Holistic assessment of professional polish, clarity, and usefulness of the coaching for a bank CSR peer.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Coaching Quality", "description": "Evaluates clarity, organization, professionalism, and practical value for the target audience (bank CSRs).", "weight": 2.0, "judge_prompt": "Judge the overall quality of the deliverable as professional coaching for a banking CSR peer. Consider:\n- Organization and readability: clear headings, clean lists, consistent formatting, concise phrasing.\n- Practicality: Are the alternatives phrased so they could be pasted into a chat? Do they model an appropriate tone and structure (greeting if needed, empathy, solution/next step, check for more help)?\n- Relevance: Tailored for banking context; avoids risky promises, sensitive data, or policy violations.\n- Brevity and focus: Stays under 5 pages while being sufficiently comprehensive.\n\nScoring:\n- 2.0: Excellent professional coaching\u2014clear, concise, actionable, polished for banking CSRs.\n- 1.0\u20131.5: Good quality with minor issues (e.g., some wording could be sharper or more bank-specific).\n- 0.5: Barely adequate; uneven clarity or usefulness.\n- 0.0: Low-quality or unusable coaching.\n", "expectation": "A polished, concise, and actionable coaching document that a banking CSR can immediately apply in live chat work."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "84322284-5c2c-4873-b507-b147449d209d", "rubric": {"category_name": "Retail PI Weekly Report Review (Saide\u2019s Fashion and Style)", "rationale": "Task Type: Pattern B (Document) with light Pattern C elements (structured tables embedded in a PDF). The deliverable is a professionally formatted PDF report that reconstructs a weekly timeline, catalogs observations, and concludes with assessment and recommendations. The rubric uses a strict Stage 1 LLM gate to enforce a verifiable report shape (required sections and tables), Stage 2 mixed verification to check for concrete anchors (timestamps, cross-references, recommendations density, CCTV specificity), and Stage 3 LLM quality assessment for professionalism, clarity, and client value.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Structured Report Gate (PDF only)", "description": "LLM-only format/structure gate ensuring the output is a PDF with required sections and tables so later verification is trivial.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Investigative Report Requirement (PDF)", "description": "Check that the output is a PDF with all required sections and specific tables present. Do not judge the content quality or correctness\u2014only presence/structure/format.", "weight": 4.0, "judge_prompt": "You are evaluating a single candidate output. Apply ONLY a structural/format check. Be flexible with header wording but strict about the presence of required sections and tables.\n\nFormat Requirements:\n- Must be a PDF file (not DOCX, not Markdown). If not PDF \u2192 score 0.0.\n- At least 2 pages with professional formatting.\n\nRequired Sections (allow close variants of header names):\n1) Executive Summary (or Overview) \u2014 must appear on or very near the first page.\n2) Sources and Materials Reviewed \u2014 reference to field investigator\u2019s notes/time log/template (the actual files may not be attached; just list them).\n3) Timeline Reconstruction \u2014 must include a clearly presented table of events. Expected columns (or close analogs): Date | Time | Location/Area | Staff Involved | Event/Observation | Evidence/Reference (e.g., camera ID, log ref). The table should show multiple days and at least ~10 timestamped entries if the week was fully covered.\n4) Observations & Behavioral Indicators \u2014 must be a table that profiles persons/roles. Expected columns: Person/Role | Behavior | Context | Risk Level (Low/Medium/High) | Evidence/Reference.\n5) Issues Requiring Attention (or Areas of Concern).\n6) CCTV Cross-Reference Plan \u2014 concrete camera/time windows to check later (e.g., Camera IDs, time ranges, locations).\n7) Gaps / Data Requests / Next Steps.\n8) Professional Assessment and Recommendations \u2014 a concluding section with the investigator\u2019s professional perspective and clear recommendations.\n9) Appendix \u2014 must include a Weekly Time Log Summary (table with Date, Start, End, Total Hours, Activity) and an Exhibit/Reference Index (or similar mapping of evidence IDs referenced in body text).\n\nScoring (STRUCTURE ONLY):\n- 4.0: PDF; 2+ pages; all 9 sections present; both required tables (Timeline, Observations) appear as tables with sensible column headings; Appendix contains both Time Log Summary and Exhibit/Reference Index.\n- 3.2: PDF; 2+ pages; missing exactly 1 non-critical section OR one of the two Appendix elements (Time Log Summary or Exhibit Index) is missing, but both main tables exist.\n- 2.4: PDF; 2+ pages; missing up to 2 required sections OR one of the two main required tables (Timeline or Observations) is weakly presented (e.g., not obviously a table but still structured).\n- 1.2: PDF; 2+ pages; missing 3+ major items OR both main tables are absent/unrecognizable.\n- 0.0: Not a PDF OR fewer than 2 pages OR lacks the majority of required sections.\n\nImportant: DO NOT judge the accuracy, writing quality, or investigative insight here. Only verify the presence and structure needed for later checks.", "expectation": "A professionally formatted PDF report with the specified sections and tables, enabling later verification."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification: Anchors, Cross-References, and Internal Consistency", "description": "Mixed code + LLM verification. Because Stage 1 enforces a structured PDF, these rules check detectable anchors (timestamps, sections, references) and basic consistency without judging prose quality.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Timeline Evidence Density and Chronology Markers", "description": "Checks for sufficient timestamps and weekday/date markers that indicate a reconstructed week timeline spanning multiple days.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No primary document output.\"\n        # Extract text\n        text = \"\"\n        try:\n            if output.file_extension.lower() == '.pdf':\n                text = context.files.read_pdf_text(output.id)\n            elif output.file_extension.lower() in ('.docx', '.doc'):\n                text = context.files.read_docx_text(output.id)\n            else:\n                text = context.files.read_text(output.id)\n        except Exception:\n            # Fallback to generic text extraction\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0, \"Unable to read document text.\"\n        if not text:\n            return 0.0, \"Empty document text.\"\n        low = text.lower()\n        # Count time stamps like 09:30, 9:30 AM, 14:05, etc.\n        time_patterns = [\n            r\"\\b([01]?\\d|2[0-3]):[0-5]\\d\\s?(am|pm)?\\b\",\n            r\"\\b([1-9]|1[0-2])\\s?:\\s?[0-5]\\d\\s?(am|pm)\\b\",\n        ]\n        timestamps = 0\n        for pat in time_patterns:\n            timestamps += len(re.findall(pat, low))\n        # Count days of week mentions\n        days = [\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"sunday\"]\n        days_present = sum(1 for d in days if d in low)\n        # Date patterns like 2025-10-03 or 10/03/2025\n        date_patterns = [r\"\\b\\d{4}-\\d{2}-\\d{2}\\b\", r\"\\b\\d{1,2}/\\d{1,2}/\\d{2,4}\\b\"]\n        dates = 0\n        for pat in date_patterns:\n            dates += len(re.findall(pat, low))\n        # Scoring: require reasonable density and multi-day coverage\n        # Full credit if >=10 timestamps and at least 3 distinct days-of-week or >=3 date hits\n        score = 0.0\n        if timestamps >= 10 and (days_present >= 3 or dates >= 3):\n            score = 1.0\n        elif timestamps >= 6 and (days_present >= 2 or dates >= 2):\n            score = 0.7\n        elif timestamps >= 3:\n            score = 0.4\n        else:\n            score = 0.0\n        return score, f\"Timestamps={timestamps}, days_present={days_present}, dates={dates}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Section/Table Header Signals Present in Text", "description": "Verifies that key headings and column cues for Timeline, Observations, Risk, Appendix Time Log, and Exhibit Index appear in the extracted text.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No primary document output.\"\n        # Extract text\n        text = \"\"\n        try:\n            if output.file_extension.lower() == '.pdf':\n                text = context.files.read_pdf_text(output.id)\n            elif output.file_extension.lower() in ('.docx', '.doc'):\n                text = context.files.read_docx_text(output.id)\n            else:\n                text = context.files.read_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0, \"Unable to read document text.\"\n        low = text.lower()\n        # Look for indicative headers\n        headers = {\n            'timeline': any(k in low for k in [\"timeline reconstruction\",\"timeline\",\"chronology\"]),\n            'observations': any(k in low for k in [\"observations & behavioral\",\"behavioral indicators\",\"observations\", \"behavioral\"]),\n            'risk': 'risk' in low,\n            'appendix': 'appendix' in low,\n            'time_log': any(k in low for k in [\"time log\",\"weekly time log\",\"hours\",\"start\",\"end\",\"total hours\"]),\n            'exhibit_index': any(k in low for k in [\"exhibit index\",\"reference index\",\"evidence index\",\"exhibits\"]),\n        }\n        # Column cues around timeline\n        timeline_col_cues = sum(1 for k in [\"date\",\"time\",\"location\",\"staff\",\"event\",\"observation\",\"evidence\",\"camera\",\"ref\"] if k in low)\n        # Column cues around observations table\n        obs_col_cues = sum(1 for k in [\"person\",\"role\",\"behavior\",\"context\",\"risk\",\"evidence\",\"reference\"] if k in low)\n        # Score logic: reward presence of key sections and column cues\n        base = sum(1 for v in headers.values() if v)\n        # Normalize to [0,1]\n        # Require at least 4 headers and decent column cues for full credit\n        score = 0.0\n        if base >= 5 and timeline_col_cues >= 4 and obs_col_cues >= 4:\n            score = 1.0\n        elif base >= 4 and (timeline_col_cues >= 3 or obs_col_cues >= 3):\n            score = 0.7\n        elif base >= 3:\n            score = 0.4\n        else:\n            score = 0.0\n        feedback = f\"headers={headers}, timeline_col_cues={timeline_col_cues}, obs_col_cues={obs_col_cues}\"\n        return score, feedback\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "CCTV Cross-Reference Specificity", "description": "Checks for presence of camera identifiers and time windows suggesting actionable video pulls.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No primary document output.\"\n        # Extract text\n        text = \"\"\n        try:\n            if output.file_extension.lower() == '.pdf':\n                text = context.files.read_pdf_text(output.id)\n            elif output.file_extension.lower() in ('.docx', '.doc'):\n                text = context.files.read_docx_text(output.id)\n            else:\n                text = context.files.read_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0, \"Unable to read document text.\"\n        low = text.lower()\n        # Camera identifiers and time ranges\n        cam_hits = len(re.findall(r\"\\b(camera|cam|cctv|dvr)\\s*([#:]?\\s*\\w+)?\", low))\n        pos_hits = len(re.findall(r\"\\bpos\\b|register|till|checkout\", low))\n        locations = len(re.findall(r\"fitting room|stock room|back room|receiving|cash wrap|sales floor|entrance|exit|loading dock\", low))\n        # Time window patterns: 09:00-09:30, 9:00 to 9:45 am, etc.\n        window_hits = len(re.findall(r\"([01]?\\d|2[0-3]):[0-5]\\d\\s?(am|pm)?\\s?(-|to)\\s?([01]?\\d|2[0-3]):[0-5]\\d\\s?(am|pm)?\", low))\n        # Score: need both camera mentions and time windows for full credit\n        score = 0.0\n        if cam_hits >= 3 and window_hits >= 2:\n            score = 1.0\n        elif cam_hits >= 2 and window_hits >= 1:\n            score = 0.7\n        elif cam_hits >= 1 or window_hits >= 1:\n            score = 0.4\n        else:\n            score = 0.0\n        return score, f\"cam_hits={cam_hits}, window_hits={window_hits}, pos_hits={pos_hits}, loc_hits={locations}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Recommendations Count and Actionability Cues", "description": "Looks for a Recommendations section with at least 3 discrete action items and presence of action verbs typical to retail loss prevention.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No primary document output.\"\n        # Extract text\n        text = \"\"\n        try:\n            if output.file_extension.lower() == '.pdf':\n                text = context.files.read_pdf_text(output.id)\n            elif output.file_extension.lower() in ('.docx', '.doc'):\n                text = context.files.read_docx_text(output.id)\n            else:\n                text = context.files.read_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0, \"Unable to read document text.\"\n        low = text.lower()\n        # Find recommendations section boundaries\n        start = low.find('recommend')\n        if start == -1:\n            return 0.0, \"No recommendations section detected.\"\n        # Heuristic: next major heading\n        next_heads = [h for h in ['appendix','annex','attachment','exhibit','references','signature','end of report'] if h in low[start+10:]]\n        end = len(low)\n        if next_heads:\n            # Choose nearest next heading\n            end = min(low.find(h, start+10) for h in next_heads if low.find(h, start+10) != -1)\n        section = low[start:end]\n        # Count bullet/numbered items in section\n        bullets = len(re.findall(r\"(^|\\n)\\s*(\\-|\\u2022|\\*)\\s+\", section))\n        numbered = len(re.findall(r\"(^|\\n)\\s*\\d+\\.?\\)?:?\\s+\", section))\n        items = max(bullets, numbered)\n        # Action verbs common in LP\n        verbs = ['install','audit','restrict','train','monitor','review','segregate','reconcile','investigate','escalate','flag','compare','observe','shadow','rotate','lock','seal','seal logs','count','inventory','limit','control','approve','verify']\n        verb_hits = sum(v in section for v in verbs)\n        score = 0.0\n        if items >= 3 and verb_hits >= 3:\n            score = 1.0\n        elif items >= 2 and verb_hits >= 2:\n            score = 0.7\n        elif items >= 1:\n            score = 0.4\n        else:\n            score = 0.0\n        return score, f\"items={items}, verb_hits={verb_hits}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Exhibit/Reference Cross-Referencing", "description": "Checks that exhibits/references are cited in body text and that an Appendix/Index exists, indicating traceability from observations to references.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0, \"No primary document output.\"\n        # Extract text\n        text = \"\"\n        try:\n            if output.file_extension.lower() == '.pdf':\n                text = context.files.read_pdf_text(output.id)\n            elif output.file_extension.lower() in ('.docx', '.doc'):\n                text = context.files.read_docx_text(output.id)\n            else:\n                text = context.files.read_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0, \"Unable to read document text.\"\n        low = text.lower()\n        # Body citations\n        body_exhibit = len(re.findall(r\"\\b(exhibit|ref(?:erence)?|evidence)\\s*([#:]?\\s*[a-z0-9\\-]+)?\", low))\n        appendix_present = 'appendix' in low or 'exhibit index' in low or 'reference index' in low\n        # Score: need both body citations and appendix/index\n        if body_exhibit >= 3 and appendix_present:\n            return 1.0, f\"body_exhibit={body_exhibit}, appendix_present={appendix_present}\"\n        elif body_exhibit >= 1 and appendix_present:\n            return 0.7, f\"body_exhibit={body_exhibit}, appendix_present={appendix_present}\"\n        elif body_exhibit >= 1 or appendix_present:\n            return 0.4, f\"body_exhibit={body_exhibit}, appendix_present={appendix_present}\"\n        else:\n            return 0.0, f\"body_exhibit={body_exhibit}, appendix_present={appendix_present}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "llm_judge", "name": "Consistency and Evidence Linkage (LLM)", "description": "Judge whether the report\u2019s timeline and observations sections cross-reference each other and whether suspicious behaviors are explicitly tied to evidence (e.g., camera/time or reference IDs).", "weight": 0.4, "judge_prompt": "Review the PDF and focus ONLY on internal consistency and linkages, not prose quality. Answer the following:\n- Does the Timeline section present events in chronological order across multiple days?\n- Do observation entries (people/roles and behaviors) reference specific events/times or evidence (e.g., camera IDs, exhibits)?\n- Are Issues/Areas of Concern explicitly grounded in the events/observations, not generic?\n\nScoring:\n- 0.4: Clear, consistent chronology; multiple observation entries link to specific times/cameras/exhibits; Issues are grounded in cited events.\n- 0.25: Mostly consistent chronology; some links to evidence/times; Issues are partially grounded.\n- 0.1: Weak chronology; few/no explicit links; Issues mostly generic.\n- 0.0: No discernible chronology and no links to evidence.\n\nBe concise and focus on observable linkages only.", "expectation": "A coherent, chronological timeline with observations that cite times/cameras/exhibits and issues tied to those citations."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professionalism and Client Value", "description": "LLM evaluates writing quality, clarity, tone, and client usefulness of conclusions/recommendations.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Strategic Value", "description": "Assesses clarity, organization, tone, and relevance of recommendations for a retail loss-prevention audience.", "weight": 2.0, "judge_prompt": "Evaluate the overall professionalism and client value of the PDF report:\n- Clarity and organization: Are sections easy to navigate? Are tables legible and labeled? Are headings consistent?\n- Tone: Professional, objective, and appropriate for a retail loss-prevention context.\n- Conclusions and recommendations: Are they specific, actionable, and tailored to retail operations (POS, inventory handling, staffing, CCTV)?\n- Practical utility: Would a store leadership/LP team be able to act on this report next week?\n\nScoring:\n- 2.0: Highly professional; clear and well-organized; recommendations are specific/actionable; strong client utility.\n- 1.4: Generally professional; minor organizational/tone issues; recommendations mostly actionable.\n- 0.8: Mixed clarity/tone; recommendations somewhat generic; limited utility.\n- 0.0: Poorly presented; unclear; recommendations not actionable.\n\nDo not re-grade structure or correctness already handled in prior stages.", "expectation": "A clear, objective, professionally written report with actionable, retail-relevant recommendations."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "d4525420-a427-4ef2-b4e9-2dcc2d31b3b6", "rubric": {"category_name": "Retail: Overnight Hourly Manager Candidate Selection (Self-Documenting)", "rationale": "This rubric enforces a self-documenting deliverable that proves the selection decision. Stage 1 uses an LLM judge to require a PDF/DOCX with a strict, verification-friendly structure (selection summary paragraph, explicit selected candidate, evaluation matrix, criteria weights with percentages, and evidence references). Stage 2 uses code rules to verify sentence count, priority weights, cross-referencing of the selected candidate with the matrix, and presence of required evidence types. Stage 3 uses an LLM judge to assess overall professionalism, clarity, and alignment with overnight leadership needs.", "max_total_score": 10.0, "stages": [{"name": "Stage 1: Shape Enforcement Gate", "description": "Validate the output is a properly structured decision document enabling verification. LLM-only gate per guidelines.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Document Format and Structural Completeness", "description": "Output must be a PDF/DOCX decision memo with all required sections and artifacts for verification.", "weight": 4.0, "judge_prompt": "You are validating ONLY the structure and format of the candidate's deliverable (not content quality or correctness). Review the PRIMARY OUTPUT and score based on the following requirements.\n\nFormat Requirements:\n- Must be a PDF or DOCX document (not Excel/CSV, not plain text/markdown).\n- At least 1 page, professionally formatted with clear section headers.\n\nRequired Sections and Elements (be flexible on exact wording but strict on presence):\n1) A visible title/header indicating this is an Overnight (Hourly) Manager selection decision (e.g., \"Overnight Manager Candidate Selection\").\n2) A line near the top: \"Selected Candidate: <Full Name>\" (or very close wording like \"Chosen Candidate\" or \"Selected Hire\").\n3) A section header \"Selection Summary\" (or very similar) followed by a short paragraph of approximately 5\u20137 sentences explaining the choice.\n4) A section with an \"Evaluation Matrix\" (or \"Candidate Comparison Table\") containing a visible table that compares multiple candidates. The table should include columns broadly covering: Attendance/Reliability, Productivity/CPH (cases per hour), Performance vs. Expectations, Leadership/Management Potential (or Role Readiness/Adaptability), Interview Notes, and an Overall assessment (Overall/Rank/Score). Column names can vary slightly but must clearly map to these areas.\n5) A section \"Criteria Weights\" (or similar) explicitly listing numeric percentages for at least these three categories: Leadership/Role Readiness (or Management Potential/Adaptability), Attendance/Reliability, and Productivity/CPH. The items must show a % value next to each criterion.\n6) A section \"Evidence References\" (or similar) that lists reference sources used from the provided dataset, mentioning all of the following data types in some form: attendance, productivity (or cases per hour/CPH), employee evaluations/performance, and interview notes.\n\nScoring (0\u20134):\n- 4.0: Valid PDF/DOCX AND all 6 required elements present. The Evaluation Matrix is a visible table; Criteria Weights include numeric percentages for the three categories; Selection Summary is present as a paragraph of about 5\u20137 sentences; \"Selected Candidate\" line is present.\n- 3.0: Valid PDF/DOCX AND core structure present (title, Selected Candidate line, Selection Summary, and an Evaluation Matrix table). At most one supporting element (Criteria Weights with %s OR Evidence References) missing or clearly incomplete.\n- 2.0: Valid PDF/DOCX, but multiple required sections are missing or the Evaluation Matrix is not a recognizable table.\n- 0.0: Not PDF/DOCX, OR missing core items (e.g., no Selection Summary or no Selected Candidate line).\n\nOnly check PRESENCE and STRUCTURE. Do NOT judge correctness or writing quality.", "expectation": "A professional PDF/DOCX decision memo with: title; Selected Candidate line; a 5\u20137 sentence Selection Summary; a comparative Evaluation Matrix table; numeric Criteria Weights for leadership/role readiness, attendance/reliability, and productivity/CPH; and an Evidence References section citing attendance, productivity/CPH, evaluations, and interview notes."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Verification and Consistency Checks", "description": "Code-based checks to verify sentence count, priority weighting, cross-references, and evidence mentions based on the structured document enforced in Stage 1.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Selection Summary Length and Selected Candidate Label", "description": "Verify the document contains a 'Selected Candidate:' label and that the Selection Summary paragraph is 5\u20137 sentences.", "weight": 1.0, "code": "import re\n\ndef _read_any_text(context, output):\n    text = ''\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        text = ''\n    if not text or len(text.strip()) < 10:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = ''\n    if (not text or len(text.strip()) < 10) and getattr(output, 'is_text_format', False):\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            text = ''\n    return text or ''\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not getattr(output, 'is_document', False):\n        return 0.0\n    text = _read_any_text(context, output)\n    if not text:\n        return 0.0\n\n    # Check Selected Candidate label\n    label_match = re.search(r\"Selected\\s*Candidate\\s*:\\s*(.+)\", text, flags=re.IGNORECASE)\n    has_label = bool(label_match)\n\n    # Try to isolate Selection Summary section\n    lower = text.lower()\n    start = None\n    for hdr in [\"selection summary\", \"summary of selection\", \"decision summary\"]:\n        idx = lower.find(hdr)\n        if idx != -1:\n            start = idx\n            break\n    summary_text = ''\n    if start is not None:\n        tail = text[start:]\n        # end at next header keyword\n        end_idx = len(tail)\n        for nxt in [\"evaluation matrix\", \"candidate comparison\", \"criteria weights\", \"evidence references\", \"appendix\", \"references\"]:\n            j = tail.lower().find(nxt)\n            if j != -1 and j > 0:\n                end_idx = min(end_idx, j)\n        summary_text = tail[:end_idx]\n    else:\n        # fallback: use first 1200 chars as summary region\n        summary_text = text[:1200]\n\n    # Count sentences (basic heuristic)\n    # Remove headers/labels\n    summary_body = re.sub(r\"(?i)selection summary\\s*[:\\-]?\", \"\", summary_text)\n    sentences = re.split(r\"(?<=[.!?])\\s+\", summary_body.strip())\n    sentences = [s for s in sentences if re.search(r\"[A-Za-z]\", s)]\n    n = len(sentences)\n\n    score = 0.0\n    if has_label and 5 <= n <= 7:\n        score = 1.0\n    elif has_label and (n == 4 or n == 8):\n        score = 0.5\n    elif has_label:\n        score = 0.25\n    else:\n        score = 0.0\n\n    return score"}, {"type": "code", "name": "Criteria Weights Priority Check", "description": "Verify numeric percentages exist for Leadership/Role Readiness, Attendance/Reliability, and Productivity/CPH, and that leadership > productivity and productivity is relatively low (<=20%).", "weight": 1.0, "code": "import re\n\ndef _read_any_text(context, output):\n    text = ''\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        text = ''\n    if not text or len(text.strip()) < 10:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = ''\n    if (not text or len(text.strip()) < 10) and getattr(output, 'is_text_format', False):\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            text = ''\n    return text or ''\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not getattr(output, 'is_document', False):\n        return 0.0\n    text = _read_any_text(context, output)\n    if not text:\n        return 0.0\n    lower = text.lower()\n\n    # Try to isolate Criteria Weights section\n    start = None\n    for hdr in [\"criteria weights\", \"weighting\", \"priority weights\", \"criteria weighting\"]:\n        idx = lower.find(hdr)\n        if idx != -1:\n            start = idx\n            break\n    section = text\n    if start is not None:\n        tail = text[start:]\n        end_idx = len(tail)\n        for nxt in [\"evidence references\", \"evaluation matrix\", \"candidate comparison\", \"appendix\", \"selection summary\", \"references\"]:\n            j = tail.lower().find(nxt)\n            if j != -1 and j > 0:\n                end_idx = min(end_idx, j)\n        section = tail[:end_idx]\n\n    # Extract criterion -> % pairs\n    pairs = re.findall(r\"(?i)(leadership|role\\s*readiness|management|adaptability|attendance|reliability|productivity|cases\\s*per\\s*hour|cph)\\s*[:\\-]?\\s*(\\d{1,3})\\s*%\", section)\n\n    # Map synonyms\n    weights = {\"leadership\": None, \"attendance\": None, \"productivity\": None}\n    for k, v in pairs:\n        k_l = k.lower()\n        val = None\n        try:\n            val = int(v)\n        except Exception:\n            continue\n        if val is None:\n            continue\n        if any(x in k_l for x in [\"leadership\", \"role readiness\", \"management\", \"adaptability\"]):\n            weights[\"leadership\"] = val if weights[\"leadership\"] is None else max(weights[\"leadership\"], val)\n        elif any(x in k_l for x in [\"attendance\", \"reliability\"]):\n            weights[\"attendance\"] = val if weights[\"attendance\"] is None else max(weights[\"attendance\"], val)\n        elif any(x in k_l for x in [\"productivity\", \"cases per hour\", \"cph\"]):\n            weights[\"productivity\"] = val if weights[\"productivity\"] is None else max(weights[\"productivity\"], val)\n\n    # Scoring logic\n    have_all = all(weights[k] is not None for k in [\"leadership\", \"attendance\", \"productivity\"])\n    if not have_all:\n        # partial: if we at least have leadership and productivity\n        if weights[\"leadership\"] is not None and weights[\"productivity\"] is not None:\n            # check leadership > productivity\n            return 0.5 if weights[\"leadership\"] > weights[\"productivity\"] else 0.25\n        return 0.0\n\n    leadership = weights[\"leadership\"]\n    productivity = weights[\"productivity\"]\n\n    # Conditions: leadership > productivity; productivity <= 20\n    cond1 = leadership > productivity\n    cond2 = productivity <= 20\n\n    if cond1 and cond2:\n        return 1.0\n    if cond1:\n        return 0.75\n    if cond2:\n        return 0.5\n    return 0.25"}, {"type": "code", "name": "Cross-Reference: Candidate Appears in Evaluation Matrix", "description": "Ensure the selected candidate also appears in the Evaluation Matrix section (or name appears multiple times if headers not found).", "weight": 1.0, "code": "import re\n\ndef _read_any_text(context, output):\n    text = ''\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        text = ''\n    if not text or len(text.strip()) < 10:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = ''\n    if (not text or len(text.strip()) < 10) and getattr(output, 'is_text_format', False):\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            text = ''\n    return text or ''\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not getattr(output, 'is_document', False):\n        return 0.0\n    text = _read_any_text(context, output)\n    if not text:\n        return 0.0\n\n    m = re.search(r\"Selected\\s*Candidate\\s*:\\s*(.+)\", text, flags=re.IGNORECASE)\n    if not m:\n        return 0.0\n    candidate = m.group(1).strip()\n    candidate = re.sub(r\"[\\r\\n].*\", \"\", candidate)  # first line only\n\n    lower = text.lower()\n    # Isolate matrix section if possible\n    start = None\n    for hdr in [\"evaluation matrix\", \"candidate comparison\", \"comparison table\"]:\n        idx = lower.find(hdr)\n        if idx != -1:\n            start = idx\n            break\n    matrix = text\n    if start is not None:\n        tail = text[start:]\n        end_idx = len(tail)\n        for nxt in [\"criteria weights\", \"evidence references\", \"appendix\", \"selection summary\", \"references\"]:\n            j = tail.lower().find(nxt)\n            if j != -1 and j > 0:\n                end_idx = min(end_idx, j)\n        matrix = tail[:end_idx]\n\n    # Search candidate in matrix section\n    found_in_matrix = candidate.lower() in matrix.lower()\n    if found_in_matrix:\n        return 1.0\n\n    # Fallback: name appears at least twice in entire document\n    count_all = len(re.findall(re.escape(candidate), text, flags=re.IGNORECASE))\n    if count_all >= 2:\n        return 0.5\n\n    return 0.0"}, {"type": "code", "name": "Evidence References Coverage", "description": "Check the deliverable references key data sources: attendance, productivity/CPH, evaluations/performance, and interview notes.", "weight": 1.0, "code": "import re\n\ndef _read_any_text(context, output):\n    text = ''\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        text = ''\n    if not text or len(text.strip()) < 10:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = ''\n    if (not text or len(text.strip()) < 10) and getattr(output, 'is_text_format', False):\n        try:\n            text = context.files.read_text(output.id)\n        except Exception:\n            text = ''\n    return text or ''\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not getattr(output, 'is_document', False):\n        return 0.0\n    text = _read_any_text(context, output)\n    if not text:\n        return 0.0\n\n    lower = text.lower()\n    # Prefer the Evidence References section if present\n    start = None\n    for hdr in [\"evidence references\", \"sources\", \"data references\", \"reference data\"]:\n        idx = lower.find(hdr)\n        if idx != -1:\n            start = idx\n            break\n    section = text\n    if start is not None:\n        tail = text[start:]\n        end_idx = len(tail)\n        for nxt in [\"criteria weights\", \"evaluation matrix\", \"candidate comparison\", \"appendix\", \"selection summary\"]:\n            j = tail.lower().find(nxt)\n            if j != -1 and j > 0:\n                end_idx = min(end_idx, j)\n        section = tail[:end_idx]\n\n    s = section.lower()\n    has_attendance = (\"attendance\" in s)\n    has_productivity = (\"productivity\" in s) or (\"cases per hour\" in s) or (\" cph\" in s) or (\"cph:\" in s)\n    has_evals = (\"evaluation\" in s) or (\"performance\" in s)\n    has_interview = (\"interview\" in s) or (\"interview notes\" in s)\n\n    count = sum([has_attendance, has_productivity, has_evals, has_interview])\n    if count == 4:\n        return 1.0\n    if count == 3:\n        return 0.75\n    if count == 2:\n        return 0.5\n    if count == 1:\n        return 0.25\n    return 0.0"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3: Quality and Professionalism", "description": "LLM judge assesses clarity, professionalism, managerial reasoning, and alignment with priorities (leadership/role-readiness highest; productivity lowest).", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Decision Rationale Quality", "description": "Assess the clarity, professionalism, and strategic alignment of the Selection Summary and overall memo.", "weight": 2.0, "judge_prompt": "Evaluate the deliverable for overall quality and alignment with the task:\n\nConsider:\n- The Selection Summary is coherent, professional, and within 5\u20137 sentences.\n- The rationale prioritizes leadership/role-readiness and adaptability (highest) and de-emphasizes productivity/CPH (lowest), while still addressing reliability for overnight shifts (attendance, dependability).\n- The chosen candidate is justified with evidence references (attendance, evaluations, interviews) and a sensible comparison to others.\n- Writing quality: concise, clear, and suitable for a store manager audience.\n\nScoring (0\u20132):\n- 2.0: Clear, professional, tightly reasoned; priorities correctly emphasized (leadership highest, productivity lowest); strong evidence-backed justification aligned to overnight operations.\n- 1.0: Generally clear and reasonable; minor issues in emphasis, evidence specificity, or tone.\n- 0.0: Unclear, off-priority (e.g., productivity emphasized), or weak/absent justification.\n\nFocus on quality and alignment, not verifying data calculations.", "expectation": "A concise, professional, and evidence-backed summary that clearly prioritizes leadership/role-readiness over productivity, with strong overnight reliability considerations."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "3c19c6d1-672c-467a-8437-6fe21afb8eae", "rubric": {"category_name": "Monthly Project Report (October 2025) \u2013 BridgeMind AI (Project Management Specialists)", "rationale": "Task Type: Mixed (Pattern C) \u2013 a structured presentation document with embedded data/analysis summaries. We enforce a strict, self-documenting slide structure first (Stage 1 LLM Gate), then verify key correctness and consistency items (Stage 2 mixed LLM + code), and finally assess professional quality for grant-assessor suitability (Stage 3 LLM). Because outputs must be files and the requested format is a presentation, Stage 1 relies solely on an LLM judge to validate the visible slide structure, titles, and included sections. Stage 2 uses additional LLM checks for content correctness and cross-references, plus light code checks for file-type and token presence (when PDF). Stage 3 evaluates professionalism and audience fit.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2013 Presentation Structure Gate (LLM only)", "description": "Gate: Verify the candidate produced a presentation in the correct shape so later verification is possible.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Presentation Requirement (PPTX/PDF)", "description": "Check the output is a PowerPoint deck (PPTX) or a PDF export of slides, with all required slides, titles, and structural elements present. Only check structure and presence, not content quality or calculation correctness.", "weight": 4.0, "judge_prompt": "You are validating the primary output presentation for strict structural compliance. Only check PRESENCE/STRUCTURE, not content correctness or quality.\n\nAcceptable formats: PPTX or PDF export of slides.\n\nRequired structure (be flexible with minor wording variations, but all slide purposes must be present):\n- Slide 1 \u2013 Title slide dated as of 30 October 2025 (or 30th October 2025). Should clearly indicate: BridgeMind AI, Bridge Mind, Common Ground Bikes, and that it is a Monthly Project Report.\n- Slide 2 \u2013 High-level overview of the project summarising key points from Progress, Spend, Risks, and Current Focus (top-level summary).\n- Slide 3 \u2013 Contents/About this report. Must include the basic project descriptors as bullets:\n  \u2022 Date of Report: 30th October (or 30 October)\n  \u2022 Supplier Name: Bridge Mind\n  \u2022 Proposal Title: \u2018BridgeMind AI\u2019 - An easy to use software application to improve your bicycle maintenance business.\n  \u2022 Proposal Number: IUK6060_BIKE\n  Then a numbered list naming the rest of the presentation sections:\n  1) Progress Summary\n  2) Project Spend to date\n  3) Risk Review\n  4) Current Focus\n  5) Auditor Q&A\n  6) ANNEX A - Project Summary\n- Slide 4 \u2013 Progress Summary (summary of tabular data from INPUT 2; exclude financial info). Presence of a progress summary table or bullet summary is sufficient here.\n- Slide 5 \u2013 Project Spend to date (summary of tabular data from INPUT 2; include financial info). Should indicate spending figures or financial summary elements.\n- Slide 6 \u2013 Risk Review (summary from INPUT 3 \u2013 Risk Register). Should present risks and some risk attributes (e.g., ratings, owners, or mitigations).\n- Slide 7 \u2013 Current Focus (from INPUT 4 \u2013 Project Log). Summarises current project considerations/priorities.\n- Slide 8 \u2013 Auditor Q&A (slide that explicitly opens the floor for questions).\n- Slide 9 \u2013 ANNEX A \u2013 Project Summary (summary of the project; based on INPUT 1 Project Summary).\n\nFlexible naming: Allow minor variations in headings (e.g., \u201cOverview\u201d vs \u201cHigh-Level Overview\u201d), but the purpose and required elements must be present. Extra slides are acceptable as long as the required ones are clearly present and in order.\n\nScoring (return a single numeric score between 0.0 and 1.0, which will be scaled by weight):\n- 1.0: Format is PPTX or PDF; all 9 required slides and purposes present; Slide 1 date appears as 30 October 2025 (or 30th October 2025); Slide 3 includes the four descriptors and the exact 1\u20136 list as specified; slides 4\u20139 match their purposes.\n- 0.8: Minor deviations (e.g., slight header wording differences, small ordering issues) but all required purposes are clearly present and Slide 1 date is correct.\n- 0.5: Missing one required slide/purpose or Slide 1 date is incorrect/ambiguous, but the deck is otherwise largely complete.\n- 0.2: Multiple required slides/purposes missing or mislabeled, or wrong file type but still a multi-page document resembling a deck.\n- 0.0: Not a PPTX/PDF presentation, or fewer than 5 slides/pages, or structure is fundamentally non-compliant.", "expectation": "A 9+ slide PPTX or PDF deck with the exact required slide purposes and a correct date on the title slide, plus the Slide 3 detailed descriptor bullets and numbered contents list."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification (Correctness and Consistency)", "description": "Now that the structure is enforced, verify correctness and consistency of key details and cross-references. Mix of LLM and code rules.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Core Details and Contents List Accuracy", "description": "Verify Slide 1 date and Slide 3 descriptor details and numbered contents list match the specification, and that the project timeline context (month 2 of 6) is reflected somewhere in the deck.", "weight": 2.0, "judge_prompt": "Evaluate the presentation for the following correctness items (not stylistic quality):\n\n1) Title Slide Date (Slide 1): Must show 30 October 2025 or 30th October 2025.\n2) Slide 3 \u2013 Basic descriptors must be explicitly present and correct:\n   - Date of Report: 30th October (or 30 October)\n   - Supplier Name: Bridge Mind\n   - Proposal Title: \u2018BridgeMind AI\u2019 - An easy to use software application to improve your bicycle maintenance business.\n   - Proposal Number: IUK6060_BIKE\n3) Slide 3 \u2013 The numbered list must name exactly (allowing minor casing/punctuation variation):\n   1) Progress Summary\n   2) Project Spend to date\n   3) Risk Review\n   4) Current Focus\n   5) Auditor Q&A\n   6) ANNEX A - Project Summary\n4) Timeline context: Somewhere in the deck (e.g., overview or progress), it should indicate this is month 2 of a 6-month project and that this report is the first monthly report (month 1 was not reported).\n\nScoring (0.0\u20131.0):\n- 1.0: All items (1\u20134) correct and explicitly visible.\n- 0.7: One minor omission or discrepancy (e.g., wording close but clearly the same meaning).\n- 0.4: Two issues incorrect or missing.\n- 0.1: Three issues incorrect or missing.\n- 0.0: Four or more issues incorrect/missing.", "expectation": "All descriptors and numbering exactly match; date correct; deck explicitly reflects month 2 of 6 and notes that month 1 had no report."}, {"type": "llm_judge", "name": "Section Fidelity to Referenced Inputs", "description": "Check that slides 4\u20137 accurately align with the requested content boundaries and cite the intended sources (INPUT 1\u20134), at least in captions/labels. Do not verify data accuracy; only fidelity and inclusion/exclusion rules.", "weight": 2.0, "judge_prompt": "Check slides 4\u20137 for fidelity to the requested content scope and the referenced inputs. Because actual input files may not be available, confirm the deck self-documents the sources (e.g., captions like \u201cSource: INPUT 2\u201d). Do NOT verify numerical accuracy.\n\nRequired:\n- Slide 4 \u2013 Progress Summary: Based on INPUT 2; EXCLUDES financial figures. Should show progress metrics/status but not currency values.\n- Slide 5 \u2013 Project Spend to date: Based on INPUT 2; INCLUDES financial figures (e.g., \u00a3 amounts, totals, or similar).\n- Slide 6 \u2013 Risk Review: Based on INPUT 3 (Risk Register); shows risks with attributes (e.g., rating, owner, mitigation, status).\n- Slide 7 \u2013 Current Focus: Based on INPUT 4 (Project Log); shows near-term priorities/considerations.\n- Slide 8 \u2013 Auditor Q&A: Clearly invites questions.\n- Slide 9 \u2013 Annex A \u2013 Project Summary: Summarises the project (source: INPUT 1).\n\nScoring (0.0\u20131.0):\n- 1.0: All slides adhere to scope and indicate their intended sources. Slide 4 avoids financials; Slide 5 includes them.\n- 0.7: One minor deviation (e.g., missing a source label but content clearly aligns) OR slight bleed of financial detail on Slide 4 that\u2019s trivially ignorable.\n- 0.4: Two deviations (e.g., missing source labels and slight scope mismatch on one slide).\n- 0.1: Three deviations or clear scope confusion.\n- 0.0: Four+ deviations or slides 4\u20137 not aligned to their intended purposes.", "expectation": "Slides 4\u20137 visibly align with required scopes and self-document their sources (INPUT 1\u20134). Slide 4 excludes financials; Slide 5 includes them."}, {"type": "code", "name": "Valid File Type and Non-Empty", "description": "Deterministic check that the primary output is PPTX or PDF and is non-empty.", "weight": 0.5, "code": "def evaluate(workflow, context):\n    import os\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No primary output.\"\n    try:\n        p = context.files.get_path(output.id)\n    except Exception as e:\n        return 0.0, f\"Could not get file path: {e}\"\n    suffix = str(p.suffix).lower()\n    if suffix not in [\".pptx\", \".pdf\"]:\n        return 0.0, f\"Invalid file type: {suffix}. Expected .pptx or .pdf.\"\n    try:\n        size = os.path.getsize(p)\n    except Exception as e:\n        return 0.0, f\"Could not read file size: {e}\"\n    if size <= 0:\n        return 0.0, \"File is empty (0 bytes).\"\n    # Basic sanity: likely a deck if reasonably large (>1KB)\n    if size < 1024:\n        return 0.5, f\"File is very small ({size} bytes).\"\n    return 1.0, f\"Valid {suffix} with size {size} bytes.\""}, {"type": "code", "name": "Date and Identifier Tokens (PDF-only best-effort)", "description": "If PDF, confirm presence of critical tokens: the date (30 October 2025 or 30th October 2025), Proposal Number (IUK6060_BIKE), and the project name (BridgeMind AI/Bridge Mind). If PPTX, this automated check is skipped (not penalized).", "weight": 0.5, "code": "def evaluate(workflow, context):\n    import re\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No primary output.\"\n    # Identify file extension\n    try:\n        p = context.files.get_path(output.id)\n        suffix = str(p.suffix).lower()\n    except Exception:\n        suffix = \"\"\n\n    if suffix == \".pdf\":\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception as e:\n            return 0.0, f\"Failed to read PDF text: {e}\"\n        tokens = {\n            \"date\": re.search(r\"\\b30(?:th)?\\s+October\\s+2025\\b\", text, flags=re.IGNORECASE) is not None,\n            \"proposal\": (\"IUK6060_BIKE\" in text),\n            \"project\": (re.search(r\"Bridge\\s*Mind|BridgeMind\\s*AI\", text, flags=re.IGNORECASE) is not None),\n        }\n        score = sum(1 for v in tokens.values() if v) / 3.0\n        missing = [k for k, v in tokens.items() if not v]\n        if missing:\n            return score, f\"Missing tokens: {', '.join(missing)}\"\n        return 1.0, \"All tokens found in PDF.\"\n    else:\n        # PPTX or other formats can't be parsed here; rely on LLM checks\n        return 1.0, \"Skipped token extraction for non-PDF; not penalized.\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Professional Quality Assessment", "description": "Holistic quality review for professionalism, clarity, and audience fit (grant assessor).", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation Quality", "description": "Assess the deck\u2019s clarity, professionalism, and suitability for a UK grant assessor. Consider visual organization, readability, concision, and whether slides are audience-appropriate.", "weight": 2.0, "judge_prompt": "Evaluate overall professional quality (not structure correctness):\n- Clarity: Are sections easy to follow? Slide titles meaningful? Logical flow from overview to details to annex?\n- Readability: Adequate font sizes, whitespace, and contrast? Tables/figures legible?\n- Appropriateness: Tailored to a UK funding assessor, with succinct summaries, transparent spend depiction, and clearly stated risks/mitigations?\n- Actionability: Current Focus and Q&A slides help drive next steps and discussion.\n- Polish: Consistent branding (Bridge Mind / BridgeMind AI), slide numbering or navigation, and error-free writing.\n\nScoring (0.0\u20131.0):\n- 1.0: Highly professional, clear, and assessor-ready; strong readability and polish.\n- 0.7: Generally professional with minor rough edges.\n- 0.4: Mixed quality; readability or clarity issues in multiple places.\n- 0.1: Hard to follow; poor readability or disorganized content.\n- 0.0: Very low quality or obviously unsuitable for an assessor.", "expectation": "A concise, well-structured, and visually clear deck suitable for a UK grant assessor, with readable tables, consistent style, and action-oriented sections."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "6dcae3f5-bf1c-48e0-8b4b-23e6486a934c", "rubric": {"category_name": "Residency Key Indicator Benchmarking and Early-Flag System", "rationale": "Mixed-output task: an analytical Excel workbook plus a professional notification email document. The rubric follows the self-documenting approach: Stage 1 (LLM-only) mandates an explicit workbook/document structure to make verification trivial; Stage 2 uses code rules to validate internal consistency, numeric reasonableness, and cross-references between workbook and email; Stage 3 uses LLM to assess presentation quality and communication effectiveness.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 Gate \u2014 Structured Output Requirement (LLM-only)", "description": "Enforce exact deliverable shapes so verification is possible. Output must include a structured Excel workbook for benchmarks/flags and a Word/PDF email summary.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Workbook + Email Structure Check", "description": "Verify the candidate produced both: (A) a single Excel workbook with required sheets/sections and (B) a Word or PDF email document to PD. Only check presence/structure, not correctness of calculations.", "weight": 4.0, "judge_prompt": "You are the Stage 1 gate. Review ALL submitted files. This task requires BOTH an Excel workbook and a Word/PDF email.\n\nPass criteria: The Excel workbook must exist and contain the following sheets with the listed minimum structures (flexible on exact sheet names; accept close variants like pluralization or synonyms). The email must exist as a DOCX or PDF.\n\nRequired Excel workbook structure:\n\n1) Sheet: \"Source Data (PGY-5 2021\u20132025)\" (or similar: e.g., \"Source Data\", \"PGY-5 Key Indicators 2021-2025\")\n   - Long (tidy) table with columns similar to:\n     \u2022 Resident Name\n     \u2022 PGY (1\u20135)\n     \u2022 Year (2021\u20132025)\n     \u2022 Metric (key indicator name)\n     \u2022 Value (numeric)\n   - Must include at least these metrics across rows: \"Total Key Indicators\" AND \"Total Case Numbers\" plus some ACGME key indicators.\n\n2) Sheet: \"Benchmarks\" (or similar: \"Benchmark Averages\", \"PGY Benchmarks\")\n   - Table with columns similar to:\n     \u2022 Metric (or Metric Name)\n     \u2022 PGY\n     \u2022 Benchmark Average (or Mean)\n     \u2022 Benchmark StdDev (or Standard Deviation)\n     \u2022 N (optional)\n   - One row per (Metric, PGY) pairing.\n\n3) Sheet: \"Current Comparison\" (or similar: \"Resident Comparison\", \"Z-Score Comparison\")\n   - Table with columns similar to:\n     \u2022 Resident Name\n     \u2022 PGY\n     \u2022 Metric\n     \u2022 Value\n     \u2022 Benchmark Average\n     \u2022 Benchmark StdDev\n     \u2022 Z-Score (or Z)\n     \u2022 Below 2 SD? (Yes/No or boolean)\n\n4) Sheet: \"Flagged Residents\" (or similar: \"Below 2 SD\", \"Outliers/Flags\")\n   - Subset of Current Comparison where residents are at or below \u22122 SD for any metric.\n   - Columns similar to Current Comparison, including Z-Score and an indicator/threshold reference.\n   - Note: We do NOT require visual red highlighting in Stage 1; the presence of this structured sheet is the verifiable artifact.\n\n5) Sheet: \"Requirements Met\" (or similar: \"ACGME Requirements Met\")\n   - Table with columns similar to:\n     \u2022 Resident Name\n     \u2022 Metric (key indicator)\n     \u2022 ACGME Requirement (numeric)\n     \u2022 PGY Met (1\u20135)\n     \u2022 Source Link or Citation (optional; can reference the ACGME PDF)\n   - Shows for each PGY-5 graduate the PGY in which they first met each requirement.\n\nOptional (nice to have, not required for full credit):\n- A \"Summary\" sheet listing counts of flagged residents by PGY and a brief narrative.\n- Workbook name including \"Chief Key Indicator 5-Year\" (cannot strictly verify filename; do not penalize if structure is correct).\n\nRequired Email Document (DOCX or PDF):\n- Drafted to Program Director, Dr. Smith.\n- States the task is completed.\n- Includes number, program year (PGY), and names of any residents with metrics \u22652 SD below mean, and specifies which metrics.\n- Ends with a sign-off from \"Residency Program Coordinator\" (exact phrase should be visible).\n- Minimum: coherent, professional email body (1+ paragraph); format as a letter/email. Do NOT assess correctness of counts here.\n\nScoring (map to 0\u20134 scale; be flexible on sheet/column name variants):\n- 4.0: Valid Excel AND valid email present. All 5 required sheets exist with appropriate tables/columns as described.\n- 3.0: Valid Excel AND email present but missing exactly 1 required sheet OR one required sheet has notably incomplete columns.\n- 2.0: Valid Excel present but missing 2 required sheets OR email is missing. Remaining sheets show partial but recognizable structures.\n- 1.0: Excel present but with only 1\u20132 of the required sheets or tables too incomplete to verify; OR only an email without a valid Excel workbook.\n- 0.0: No Excel workbook OR Excel not structured; OR only raw text/unsupported formats.\n\nOnly check PRESENCE and STRUCTURE. Do NOT verify calculations, thresholds, or whether names/counts are correct.", "expectation": "A single Excel workbook with the five required sheets (Source Data, Benchmarks, Current Comparison, Flagged Residents, Requirements Met) and a DOCX/PDF email to Dr. Smith that summarizes flagged residents and ends with \"Residency Program Coordinator.\""}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification and Consistency Checks (Code + LLM)", "description": "Now that structure exists, verify internal consistency, numeric sanity, and cross-references between workbook and email. Do not require external data files; rely on internal coherence.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Files Present and Types", "description": "Confirm at least one spreadsheet (Excel) and at least one document (DOCX or PDF) are present among outputs.", "weight": 0.5, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    try:\n        outputs = context.get_all_outputs()\n        if not outputs:\n            return 0.0, \"No outputs provided.\"\n        has_xl = any(getattr(r, 'is_spreadsheet', False) for r in outputs)\n        has_doc = any(getattr(r, 'is_document', False) for r in outputs)\n        score = 0.0\n        if has_xl:\n            score += 0.25\n        if has_doc:\n            score += 0.25\n        return score, f\"Spreadsheet: {'yes' if has_xl else 'no'}; Document: {'yes' if has_doc else 'no'}.\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Workbook Structural Validation (Sheets and Columns)", "description": "Find the main Excel workbook, then check required sheets exist (fuzzy names) and contain key columns to enable verification.", "weight": 1.0, "code": "import re\nimport pandas as pd\n\nREQ_SHEETS = {\n    'source': ['source', 'pgy-5', 'pgy5', '2021', '2025', 'data'],\n    'benchmarks': ['benchmark', 'avg', 'average', 'std', 'standard'],\n    'comparison': ['comparison', 'z-score', 'z score', 'current'],\n    'flags': ['flag', 'below', 'outlier', 'alert'],\n    'requirements': ['requirement', 'acgme', 'met']\n}\n\nREQ_COLS = {\n    'benchmarks': [ ['metric'], ['pgy'], ['average','mean'], ['std','stddev','standard'] ],\n    'comparison': [ ['resident','name'], ['pgy'], ['metric'], ['value','count'], ['average','mean'], ['std','stddev','standard'], ['z'] ],\n    'flags': [ ['resident','name'], ['pgy'], ['metric'], ['value','count'], ['average','mean'], ['std','stddev','standard'], ['z'] ],\n    'requirements': [ ['resident','name'], ['metric'], ['require'], ['pgy'] ]\n}\n\ndef find_excel_output(context):\n    outs = context.get_all_outputs() or []\n    # Prefer primary if spreadsheet\n    primary = context.get_primary_output()\n    if primary is not None and getattr(primary, 'is_spreadsheet', False):\n        return primary\n    for r in outs:\n        if getattr(r, 'is_spreadsheet', False):\n            return r\n    return None\n\ndef match_sheet(sheet_names, patterns):\n    low = [s.lower() for s in sheet_names]\n    for s in sheet_names:\n        sl = s.lower()\n        for p in patterns:\n            if p in sl:\n                return s\n    return None\n\ndef has_cols(df, req_groups):\n    cols = [str(c).strip().lower() for c in df.columns]\n    ok = 0\n    for group in req_groups:\n        present = any(any(k in c for k in group) for c in cols)\n        if present:\n            ok += 1\n    return ok, len(req_groups)\n\n\ndef evaluate(workflow, context):\n    try:\n        xl_res = find_excel_output(context)\n        if not xl_res:\n            return 0.0, \"No spreadsheet found.\"\n        path = context.files.get_path(xl_res.id)\n        try:\n            xls = pd.ExcelFile(path)\n            sheets = xls.sheet_names\n        except Exception as e:\n            return 0.0, f\"Cannot open Excel: {e}\"\n        found = {}\n        for key, pats in REQ_SHEETS.items():\n            nm = match_sheet(sheets, pats)\n            if nm:\n                found[key] = nm\n        # Score by presence of required sheets\n        required_keys = ['benchmarks','comparison','flags','requirements']  # 'source' sheet is helpful but allow some flexibility\n        presence_score = sum(1 for k in required_keys if k in found)\n        # Column checks on found sheets\n        col_hits = 0\n        col_total = 0\n        for key in ['benchmarks','comparison','flags','requirements']:\n            if key in found:\n                try:\n                    df = context.files.read_excel(xl_res.id, sheet_name=found[key])\n                    h_ok, h_total = has_cols(df, REQ_COLS[key])\n                    col_hits += h_ok\n                    col_total += h_total\n                except Exception:\n                    pass\n        # Scoring: 0.4 for sheet presence, 0.6 for columns\n        sheet_score = (presence_score/len(required_keys)) * 0.4\n        col_score = (col_hits/col_total)*0.6 if col_total>0 else 0.0\n        score = (sheet_score + col_score) * 1.0  # weight is 1.0\n        feedback = f\"Sheets found: {list(found.keys())}; presence_score={presence_score}/{len(required_keys)}; col_coverage={col_hits}/{col_total}\"\n        return score, feedback\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Benchmarks Sanity + Requirements Logical Checks", "description": "Validate that Benchmarks sheet has reasonable numbers (PGY in 1\u20135, non-negative averages, stddev \u2265 0) and that Requirements Met has plausible values (PGY Met in 1\u20135, requirement > 0).", "weight": 1.2, "code": "import re\nimport numpy as np\nimport pandas as pd\n\n# Helpers to locate sheets and columns\nPAT_BENCH = ['benchmark', 'avg', 'average', 'std', 'standard']\nPAT_REQ = ['requirement', 'acgme', 'met']\n\nCOL_ALIASES = {\n    'metric': ['metric','metric name','indicator','key indicator'],\n    'pgy': ['pgy','program year','level'],\n    'avg': ['benchmark average','average','mean','avg'],\n    'std': ['benchmark stddev','std dev','std','standard deviation','stdev','stddev'],\n    'n': ['n','count','samples'],\n    'req_val': ['requirement','required','acgme requirement'],\n    'pgy_met': ['pgy met','pgy','met pgy','year met']\n}\n\ndef find_excel_output(context):\n    outs = context.get_all_outputs() or []\n    primary = context.get_primary_output()\n    if primary is not None and getattr(primary, 'is_spreadsheet', False):\n        return primary\n    for r in outs:\n        if getattr(r, 'is_spreadsheet', False):\n            return r\n    return None\n\ndef match_sheet_name(sheet_names, patterns):\n    for s in sheet_names:\n        sl = s.lower()\n        if any(p in sl for p in patterns):\n            return s\n    return None\n\ndef pick_col(cols, aliases):\n    cols_l = [str(c).strip().lower() for c in cols]\n    for a in aliases:\n        for c in cols_l:\n            if a in c:\n                # return original column name\n                idx = cols_l.index(c)\n                return list(cols)[idx]\n    return None\n\n\ndef evaluate(workflow, context):\n    try:\n        xl = find_excel_output(context)\n        if not xl:\n            return 0.0, \"No spreadsheet found.\"\n        path = context.files.get_path(xl.id)\n        xls = pd.ExcelFile(path)\n        sheets = xls.sheet_names\n        bench_nm = match_sheet_name(sheets, PAT_BENCH)\n        req_nm = match_sheet_name(sheets, PAT_REQ)\n        score = 0.0\n        fb_parts = []\n        # Benchmarks sanity\n        if bench_nm:\n            dfb = context.files.read_excel(xl.id, sheet_name=bench_nm)\n            mcol = pick_col(dfb.columns, COL_ALIASES['metric'])\n            pcol = pick_col(dfb.columns, COL_ALIASES['pgy'])\n            acol = pick_col(dfb.columns, COL_ALIASES['avg'])\n            scol = pick_col(dfb.columns, COL_ALIASES['std'])\n            ncol = pick_col(dfb.columns, COL_ALIASES['n'])\n            ok_cols = all([mcol, pcol, acol, scol])\n            if ok_cols:\n                # Convert types\n                pgy_ok = 0\n                avg_ok = 0\n                std_ok = 0\n                n_ok = 0\n                total = len(dfb)\n                for _, r in dfb.iterrows():\n                    # PGY\n                    try:\n                        pgy = int(pd.to_numeric(r[pcol], errors='coerce'))\n                        if 1 <= pgy <= 5:\n                            pgy_ok += 1\n                    except Exception:\n                        pass\n                    # Average\n                    try:\n                        av = float(pd.to_numeric(r[acol], errors='coerce'))\n                        if av >= 0:\n                            avg_ok += 1\n                    except Exception:\n                        pass\n                    # Std\n                    try:\n                        sd = float(pd.to_numeric(r[scol], errors='coerce'))\n                        if sd >= 0:\n                            std_ok += 1\n                    except Exception:\n                        pass\n                    # N (optional)\n                    if ncol is not None:\n                        try:\n                            nval = float(pd.to_numeric(r[ncol], errors='coerce'))\n                            if nval >= 1:\n                                n_ok += 1\n                        except Exception:\n                            pass\n                # Score components\n                comp = 0.0\n                if total > 0:\n                    comp += 0.3 * (pgy_ok/total)\n                    comp += 0.3 * (avg_ok/total)\n                    comp += 0.3 * (std_ok/total)\n                    if ncol is not None:\n                        comp += 0.1 * (n_ok/total)\n                    else:\n                        comp += 0.1  # give full credit for N if not used\n                score += 0.8 * comp  # up to 0.8 for benchmarks\n                fb_parts.append(f\"Bench rows: {total}, PGY ok {pgy_ok}, Avg ok {avg_ok}, Std ok {std_ok}\")\n            else:\n                fb_parts.append(\"Benchmarks missing required columns.\")\n        else:\n            fb_parts.append(\"Benchmarks sheet not found.\")\n        # Requirements Met logical checks\n        if req_nm:\n            dfr = context.files.read_excel(xl.id, sheet_name=req_nm)\n            r_name_col = pick_col(dfr.columns, ['resident','name'])\n            r_metric_col = pick_col(dfr.columns, COL_ALIASES['metric']) or pick_col(dfr.columns, ['indicator'])\n            req_col = pick_col(dfr.columns, COL_ALIASES['req_val'])\n            pgy_met_col = pick_col(dfr.columns, COL_ALIASES['pgy_met'])\n            if all([r_name_col, r_metric_col, req_col, pgy_met_col]):\n                tot = len(dfr)\n                req_ok = 0\n                pgy_ok2 = 0\n                dup_penalty = 0\n                # Check duplicates per (resident, metric)\n                if tot > 0:\n                    dups = dfr[[r_name_col, r_metric_col]].duplicated().sum()\n                    dup_penalty = min(dups, 5)  # cap penalty influence\n                for _, r in dfr.iterrows():\n                    try:\n                        rv = float(pd.to_numeric(r[req_col], errors='coerce'))\n                        if rv > 0:\n                            req_ok += 1\n                    except Exception:\n                        pass\n                    try:\n                        pm = int(pd.to_numeric(r[pgy_met_col], errors='coerce'))\n                        if 1 <= pm <= 5:\n                            pgy_ok2 += 1\n                    except Exception:\n                        pass\n                comp2 = 0.0\n                if tot > 0:\n                    comp2 += 0.3 * (req_ok/tot)\n                    comp2 += 0.3 * (pgy_ok2/tot)\n                    comp2 += 0.4 * (1 - (dup_penalty/max(tot,1)))\n                score += 0.4 * comp2  # up to 0.4 for requirements\n                fb_parts.append(f\"Req rows: {tot}, Req>0 {req_ok}, PGY in range {pgy_ok2}, dups={dup_penalty}\")\n            else:\n                fb_parts.append(\"Requirements Met missing required columns.\")\n        else:\n            fb_parts.append(\"Requirements Met sheet not found.\")\n        # Cap to weight 1.2\n        return min(score, 1.2), \"; \".join(fb_parts)\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Flagged Residents Z-Score Consistency", "description": "Check that rows in Flagged Residents obey the flag condition (Z <= -2 or Value <= Mean-2*Std) and Z-scores are numerically consistent with Value/Mean/Std where Std>0. If no flagged rows exist, grant full credit.", "weight": 0.8, "code": "import math\nimport pandas as pd\n\nPAT_FLAGS = ['flag', 'below', 'outlier', 'alert']\n\nALIASES = {\n    'name': ['resident','name'],\n    'pgy': ['pgy','level'],\n    'metric': ['metric','indicator','key indicator'],\n    'value': ['value','count','number'],\n    'mean': ['benchmark average','average','mean','avg'],\n    'std': ['benchmark stddev','std dev','std','standard deviation','stdev','stddev'],\n    'z': ['z','z-score','z score']\n}\n\ndef find_excel_output(context):\n    outs = context.get_all_outputs() or []\n    primary = context.get_primary_output()\n    if primary is not None and getattr(primary, 'is_spreadsheet', False):\n        return primary\n    for r in outs:\n        if getattr(r, 'is_spreadsheet', False):\n            return r\n    return None\n\ndef match_sheet_name(sheet_names, patterns):\n    for s in sheet_names:\n        sl = s.lower()\n        if any(p in sl for p in patterns):\n            return s\n    return None\n\ndef pick_col(cols, aliases):\n    cols_l = [str(c).strip().lower() for c in cols]\n    for a in aliases:\n        for i, c in enumerate(cols_l):\n            if a in c:\n                return list(cols)[i]\n    return None\n\n\ndef evaluate(workflow, context):\n    try:\n        xl = find_excel_output(context)\n        if not xl:\n            return 0.0, \"No spreadsheet found.\"\n        path = context.files.get_path(xl.id)\n        xls = pd.ExcelFile(path)\n        fl_nm = match_sheet_name(xls.sheet_names, PAT_FLAGS)\n        if not fl_nm:\n            return 0.0, \"Flagged sheet not found.\"\n        df = context.files.read_excel(xl.id, sheet_name=fl_nm)\n        # Map columns\n        c_name = pick_col(df.columns, ALIASES['name'])\n        c_pgy  = pick_col(df.columns, ALIASES['pgy'])\n        c_metric = pick_col(df.columns, ALIASES['metric'])\n        c_val = pick_col(df.columns, ALIASES['value'])\n        c_mean = pick_col(df.columns, ALIASES['mean'])\n        c_std = pick_col(df.columns, ALIASES['std'])\n        c_z = pick_col(df.columns, ALIASES['z'])\n        needed = [c_val, c_mean, c_std]\n        if not all(needed):\n            return 0.0, \"Flagged sheet missing Value/Mean/Std columns.\"\n        if len(df) == 0:\n            return 0.8, \"No flagged rows; full credit.\"\n        good = 0\n        approx = 0\n        total = 0\n        for _, r in df.iterrows():\n            try:\n                v = float(pd.to_numeric(r[c_val], errors='coerce'))\n                m = float(pd.to_numeric(r[c_mean], errors='coerce'))\n                s = float(pd.to_numeric(r[c_std], errors='coerce'))\n            except Exception:\n                continue\n            total += 1\n            # Check flag condition\n            flag_cond = (s > 0 and (v <= m - 2*s)) or (s == 0 and v < m)\n            # Z approx\n            z_calc = None\n            try:\n                if s > 0:\n                    z_calc = (v - m)/s\n            except Exception:\n                pass\n            z_ok = True\n            if c_z and z_calc is not None:\n                try:\n                    z_rep = float(pd.to_numeric(r[c_z], errors='coerce'))\n                    z_ok = (abs(z_rep - z_calc) <= 0.2)\n                except Exception:\n                    z_ok = True\n            if flag_cond and z_ok:\n                good += 1\n            elif z_ok:\n                approx += 1\n        if total == 0:\n            return 0.4, \"No numeric rows to verify; partial credit.\"\n        # Score: majority correct -> full; else proportional\n        frac_good = good/total\n        frac_approx = (good+approx)/total\n        score = 0.8 * max(frac_good, 0.6*frac_approx)\n        return score, f\"Flag checks: good={good}, approx={approx}, total={total}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}, {"type": "code", "name": "Email \u2194 Flags Cross-Reference", "description": "Check the email document mentions flagged residents and counts. If none flagged, email should say none. Also check that metric names appear in the email.", "weight": 0.5, "code": "import re\nimport pandas as pd\n\ndef get_doc_text(context):\n    outs = context.get_all_outputs() or []\n    # Prefer DOCX, then PDF\n    docx = [r for r in outs if getattr(r, 'is_document', False)]\n    if not docx:\n        return None\n    # Try DOCX first\n    for r in docx:\n        try:\n            return context.files.read_docx_text(r.id)\n        except Exception:\n            pass\n    for r in docx:\n        try:\n            return context.files.read_pdf_text(r.id)\n        except Exception:\n            pass\n    return None\n\ndef find_excel_output(context):\n    outs = context.get_all_outputs() or []\n    primary = context.get_primary_output()\n    if primary is not None and getattr(primary, 'is_spreadsheet', False):\n        return primary\n    for r in outs:\n        if getattr(r, 'is_spreadsheet', False):\n            return r\n    return None\n\ndef find_flags_df(context, xl):\n    import pandas as pd\n    path = context.files.get_path(xl.id)\n    xls = pd.ExcelFile(path)\n    for s in xls.sheet_names:\n        sl = s.lower()\n        if any(p in sl for p in ['flag','below','outlier','alert']):\n            try:\n                return context.files.read_excel(xl.id, sheet_name=s)\n            except Exception:\n                continue\n    return None\n\n\ndef evaluate(workflow, context):\n    try:\n        xl = find_excel_output(context)\n        if not xl:\n            return 0.0, \"No spreadsheet found.\"\n        dff = find_flags_df(context, xl)\n        if dff is None:\n            return 0.0, \"Flagged Residents sheet not found.\"\n        # Extract names and metrics\n        cols = [str(c).lower() for c in dff.columns]\n        name_col = None\n        metric_col = None\n        for i,c in enumerate(cols):\n            if 'resident' in c or 'name' in c:\n                name_col = dff.columns[i]\n                break\n        for i,c in enumerate(cols):\n            if 'metric' in c or 'indicator' in c:\n                metric_col = dff.columns[i]\n                break\n        names = []\n        metrics = []\n        if name_col is not None:\n            names = [str(x).strip() for x in dff[name_col].dropna().unique().tolist()]\n        if metric_col is not None:\n            metrics = [str(x).strip().lower() for x in dff[metric_col].dropna().unique().tolist()]\n        text = get_doc_text(context) or \"\"\n        tlow = text.lower()\n        # If none flagged\n        if len(names) == 0:\n            # Look for indication of none\n            if any(k in tlow for k in ['no residents','none identified','no one is below','no key indicators below','no resident is below']):\n                return 0.5, \"No flags and email states none.\"\n            else:\n                return 0.3, \"No flags but email does not clearly state none.\"\n        # Names coverage\n        covered = 0\n        for nm in names:\n            if nm and nm.lower() in tlow:\n                covered += 1\n        name_cov = covered / max(1, len(names))\n        # Count mention (extract a number)\n        count_nums = re.findall(r\"(\\d{1,3})\\s+(resident|residents)\", tlow)\n        count_match = 0\n        if count_nums:\n            try:\n                reported = int(count_nums[0][0])\n                if reported == len(names):\n                    count_match = 1\n            except Exception:\n                pass\n        # Metric mentions (at least one metric string appears)\n        metric_hit = 0\n        for m in metrics[:10]:  # limit checks\n            if m and m in tlow:\n                metric_hit = 1\n                break\n        score = 0.3 * name_cov + 0.15 * count_match + 0.05 * metric_hit\n        return score, f\"Names covered: {covered}/{len(names)}; count_match={count_match}; metric_hit={metric_hit}\"\n    except Exception as e:\n        return 0.0, f\"Error: {e}\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Presentation and Professional Quality (LLM)", "description": "Evaluate clarity, usability, and professionalism of the workbook and email. Do not re-check structure or math.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Workbook Clarity and Usability", "description": "Assess whether the Excel workbook is professionally presented and easy to navigate for the PD.", "weight": 1.0, "judge_prompt": "Assess the Excel workbook\u2019s presentation quality (not math correctness). Consider:\n- Navigability: clear sheet names, logical flow (Source Data \u2192 Benchmarks \u2192 Comparison \u2192 Flags \u2192 Requirements Met)\n- Labeling: readable headers, consistent metric naming, units if applicable\n- Readability: column widths, freeze panes or header rows, filters, no major clutter\n- Practicality: the PD could quickly see flagged residents and understand the basis (means/stddevs, z-scores)\nScoring:\n- 1.0: Professional, clear, and easy to use; obvious flow and labeling\n- 0.7: Generally clear with minor issues (naming inconsistencies, minor clutter)\n- 0.4: Usable but confusing in places; unclear labels or flow\n- 0.2: Hard to use; poor labeling/organization\n- 0.0: Not a usable workbook", "expectation": "A clean, clearly labeled workbook where the PD can quickly find flags and understand the benchmark basis."}, {"type": "llm_judge", "name": "Email Professionalism and Completeness", "description": "Assess the email to Dr. Smith for tone, clarity, and required elements.", "weight": 1.0, "judge_prompt": "Assess the DOCX/PDF email for:\n- Professional tone and formatting as a complete email/letter\n- States the task is completed\n- Mentions number of residents flagged (or explicitly states none)\n- Includes PGY and names of any flagged residents and which metrics are below 2 SD\n- Ends with the sign-off text \"Residency Program Coordinator\"\nScoring:\n- 1.0: Fully professional; includes all required elements and is concise/clear\n- 0.7: Professional but missing one minor element (e.g., PGY detail or explicit count)\n- 0.4: Understandable but missing multiple required elements\n- 0.2: Minimal, unclear, or unprofessional\n- 0.0: Not a valid email document", "expectation": "A concise, professional email to Dr. Smith with the requested summary and the exact sign-off phrase."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "feb5eefc-39f1-4451-9ef9-bffe011b71dd", "rubric": {"category_name": "GRAT vs CRAT Comparative Analysis (Finance)", "rationale": "This rubric enforces a self-documenting, verifiable PDF deliverable. Stage 1 (LLM-only) requires a strict, inspection-ready document structure with a side-by-side comparison table, scenario assumptions, and a clear recommendation\u2014enabling deterministic checks in Stage 2. Stage 2 uses code to verify client-specific facts, presence of key trust terminology, scenario assumptions (including 7520/AFR), and a recommendation. Stage 3 applies LLM judgment for professional quality and client-appropriate reasoning. The gate ensures unusable outputs (wrong format/shape) receive 0, aligning with the self-documenting philosophy.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Document Shape Gate (LLM Only)", "description": "Mandatory format and structure check for a professional PDF deliverable enabling verification in later stages.", "is_required": true, "max_points": 5.0, "min_score_to_pass": 3.5, "rules": [{"type": "llm_judge", "name": "Structured PDF Format and Section Requirements", "description": "Checks that the output is a PDF (<=12 pages) with all required sections, tables, and structural elements to enable verification. Do NOT judge content quality or calculation correctness here\u2014only structure and presence.", "weight": 5.0, "judge_prompt": "You are evaluating whether the candidate output satisfies the REQUIRED structure for a professional PDF report. Only check FORMAT and PRESENCE of required sections as rendered in the PDF. Do NOT assess quality or correctness of content or math\u2014only whether the shape exists to enable verification.\n\nRequired format:\n- Must be a single PDF (not DOCX/Word/Excel/plain text)\n- Maximum length: 12 pages\n- Professionally formatted with clear section headers and readable tables\n\nRequired sections and elements (be flexible with exact header names but require equivalent content):\n1) Title and Executive Summary/Recommendation (on first page)\n   - A clear title referencing GRAT vs CRAT comparative analysis\n   - A brief executive summary that previews the recommendation\n\n2) Client Profile and Objectives\n   - Must explicitly include: age 62, married status, sale of business for $16,000,000 in 2015\n   - Estate tax context for 2015: exemption $5.43M individual / $10.86M married, 40% rate (exact figures or clearly equivalent expressions)\n   - Goal: reduce future estate tax, benefit children, consider philanthropy\n\n3) GRAT Overview (with labeled subsections)\n   - Purpose/Intent\n   - Mechanics & Funding\n   - Term/Duration\n   - Tax Implications\n   - Advantages\n   - Disadvantages/Risks\n\n4) CRAT Overview (with parallel subsections)\n   - Purpose/Intent\n   - Mechanics & Funding\n   - Term/Duration (lifetime or term)\n   - Tax Implications (including charitable deduction concept)\n   - Advantages\n   - Disadvantages/Risks\n\n5) Side-by-Side Comparison Table\n   - A visible table with columns similar to: [Attribute | GRAT | CRAT]\n   - Rows should cover at least 8 of the following attributes:\n     - Purpose/Intent\n     - Funding/Contributions\n     - Term/Duration\n     - Tax Treatment (income and estate)\n     - Income to Grantor\n     - Remainder Beneficiary (heirs vs charity)\n     - Estate Tax Impact\n     - Advantages/Pros\n     - Disadvantages/Cons/Risks\n     - Primary Use Cases / Suitability for this client\n\n6) Scenario Illustration for this Client\n   - Must include a labeled Assumptions area/table referencing: 2015, $16,000,000 proceeds, client age 62, marital status, and at least one rate assumption (e.g., 7520/AFR, payout/annuity rate, growth/return)\n   - Must include a labeled Outputs/Estimates area/table indicating, for both GRAT and CRAT, who ultimately benefits (heirs vs charity), indicative annuity or payout, and expected transfer/benefit outcomes at a high level (illustrative is fine)\n   - A brief disclaimer that figures are illustrative and not tax/ legal advice\n\n7) Clear Recommendation Section\n   - A distinct section that states a specific recommendation (e.g., favor GRAT, favor CRAT, a combination, or neither) and briefly cites reasons tied to estate tax reduction for children\n\n8) Appendix/Sources\n   - Labeled references or sources (e.g., IRS 2015 exemption context, definitions)\n\nScoring (apply to this rule; return a score from 0.0 to 5.0):\n- 5.0: PDF <=12 pages AND all required sections present (1\u20138), including the side-by-side table and scenario assumptions/outputs\n- 4.0: PDF <=12 pages; only one minor supporting element missing (e.g., Appendix/Sources or brief disclaimer), all core sections present (1\u20137)\n- 3.0: PDF <=12 pages; missing exactly one core section (Scenario Illustration OR Side-by-Side Table OR Recommendation), others present\n- 1.0: PDF <=12 pages; has significant structural gaps (missing two or more core sections)\n- 0.0: Not a PDF, OR more than 12 pages, OR lacks most required structure\n\nBe flexible on section titles but strict about whether equivalent content is visibly present. Only judge structure and presence, not the quality or correctness of explanations or figures.", "expectation": "A single, <=12-page PDF with clearly labeled sections for client profile, GRAT and CRAT overviews, a side-by-side comparison table, a scenario illustration with assumptions and outputs, a clear recommendation, and an appendix/sources."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification (Code + LLM)", "description": "Deterministic checks leveraging the mandated structure: client-specific facts, terminology, scenario assumptions, and a clear recommendation.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Client-Specific Facts Embedded (2015, $16M, Exemptions, 40%)", "description": "Verifies the document references the client\u2019s key facts and 2015 estate tax context with flexible pattern matching.", "weight": 0.9, "code": "import re\n\ndef _read_document_text(context, output):\n    text = \"\"\n    # Try PDF first\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = \"\"\n    return text or \"\"\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = _read_document_text(context, output).lower()\n    if not text:\n        return 0.0\n\n    score = 0.0\n\n    # 2015\n    if re.search(r\"\\b2015\\b\", text):\n        score += 0.2\n\n    # $16,000,000 (various forms)\n    patterns_16m = [\n        r\"\\b16,000,000\\b\",\n        r\"\\b16000000\\b\",\n        r\"\\$\\s*16\\s*(m|mm|million)\\b\",\n        r\"\\b16\\s*(m|mm|million)\\b\",\n    ]\n    if any(re.search(p, text) for p in patterns_16m):\n        score += 0.25\n\n    # 2015 exemptions $5.43M and $10.86M (accept flexible forms)\n    patterns_5_43 = [r\"5\\.43\\s*(m|mm|million)\\b\", r\"\\b5,430,000\\b\", r\"\\b5430000\\b\"]\n    patterns_10_86 = [r\"10\\.86\\s*(m|mm|million)\\b\", r\"\\b10,860,000\\b\", r\"\\b10860000\\b\"]\n\n    has_5_43 = any(re.search(p, text) for p in patterns_5_43)\n    has_10_86 = any(re.search(p, text) for p in patterns_10_86)\n    if has_5_43 and has_10_86:\n        score += 0.25\n    elif has_5_43 or has_10_86:\n        score += 0.15\n\n    # 40% rate\n    if re.search(r\"40\\s*%|\\b0\\.40\\b|forty percent\", text):\n        score += 0.2\n\n    return min(score, 0.9)\n"}, {"type": "code", "name": "Both Trusts Explicitly Defined (Long-form + Acronyms)", "description": "Checks for presence of full names and acronyms for GRAT and CRAT.", "weight": 0.5, "code": "import re\n\ndef _read_document_text(context, output):\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = \"\"\n    return text or \"\"\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = _read_document_text(context, output).lower()\n    if not text:\n        return 0.0\n\n    score = 0.0\n    if \"grantor retained annuity trust\" in text:\n        score += 0.125\n    if re.search(r\"\\bgrat\\b\", text):\n        score += 0.125\n    if \"charitable remainder annuity trust\" in text:\n        score += 0.125\n    if re.search(r\"\\bcrat\\b\", text):\n        score += 0.125\n\n    return min(score, 0.5)\n"}, {"type": "code", "name": "Side-by-Side Comparison Presence (Heuristic)", "description": "Heuristically detects a comparison section contrasting GRAT vs CRAT across multiple attributes.", "weight": 0.6, "code": "import re\n\ndef _read_document_text(context, output):\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = \"\"\n    return text or \"\"\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = _read_document_text(context, output).lower()\n    if not text:\n        return 0.0\n\n    # Require both acronyms to consider comparison scoring\n    if not (re.search(r\"\\bgrat\\b\", text) and re.search(r\"\\bcrat\\b\", text)):\n        return 0.0\n\n    has_comp_kw = any(k in text for k in [\"comparison\", \"side-by-side\", \"side by side\", \"versus\", \" vs \"])\n\n    attrs = [\n        \"purpose\", \"funding\", \"duration\", \"term\", \"tax\", \"advantages\", \"pros\",\n        \"disadvantages\", \"cons\", \"risk\", \"risks\", \"estate tax\", \"income\", \"remainder\"\n    ]\n    present = set()\n    for a in attrs:\n        if a in text:\n            present.add(a)\n    # Attribute coverage scored up to 6 distinct hits\n    attr_count = min(len(present), 6)\n\n    score = 0.0\n    # Attribute coverage contributes up to 0.45\n    score += (attr_count / 6.0) * 0.45\n    # Explicit comparison cue contributes 0.15\n    if has_comp_kw:\n        score += 0.15\n\n    return min(score, 0.6)\n"}, {"type": "code", "name": "Scenario Assumptions and Rate Elements (7520/AFR, Payout/Term/Growth)", "description": "Checks for a labeled scenario/assumptions section with key rate elements to make the illustration auditable.", "weight": 0.6, "code": "import re\n\ndef _read_document_text(context, output):\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = \"\"\n    return text or \"\"\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = _read_document_text(context, output).lower()\n    if not text:\n        return 0.0\n\n    checks = 0\n    total = 5\n\n    # Labeled scenario/assumptions\n    if any(k in text for k in [\"assumption\", \"assumptions\", \"scenario\", \"illustration\"]):\n        checks += 1\n\n    # 7520/AFR present\n    if any(k in text for k in [\"7520\", \"afr\", \"section 7520\"]):\n        checks += 1\n\n    # Payout/annuity rate\n    if any(k in text for k in [\"payout\", \"annuity rate\", \"distribution rate\", \"annuity payout\"]):\n        checks += 1\n\n    # Term/duration/years\n    if any(k in text for k in [\"term\", \"duration\", \"years\", \"year term\", \"lifetime\"]):\n        checks += 1\n\n    # Growth/return assumption\n    if any(k in text for k in [\"growth rate\", \"investment return\", \"assumed return\", \"% return\"]):\n        checks += 1\n\n    return (checks / total) * 0.6\n"}, {"type": "code", "name": "Clear Recommendation Statement (Estate-Tax Focus)", "description": "Detects an explicit recommendation and link to estate tax reduction objective.", "weight": 0.3, "code": "import re\n\ndef _read_document_text(context, output):\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = \"\"\n    return text or \"\"\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = _read_document_text(context, output).lower()\n    if not text:\n        return 0.0\n\n    score = 0.0\n    if any(k in text for k in [\"recommendation\", \"we recommend\", \"i recommend\", \"recommend \", \"should consider\", \"we do not recommend\"]):\n        score += 0.15\n    if \"estate tax\" in text or \"estate-tax\" in text or \"estate tax exposure\" in text:\n        score += 0.15\n\n    return min(score, 0.3)\n"}, {"type": "code", "name": "Professional Disclaimers Present", "description": "Looks for basic disclaimers (illustrative only, not tax/legal advice, consult advisors).", "weight": 0.1, "code": "import re\n\ndef _read_document_text(context, output):\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = \"\"\n    return text or \"\"\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = _read_document_text(context, output).lower()\n    if not text:\n        return 0.0\n\n    phrases = [\n        \"not tax advice\", \"not legal advice\", \"consult your tax advisor\", \"consult your tax adviser\",\n        \"consult your legal advisor\", \"for illustrative purposes\", \"hypothetical only\", \"no guarantee\"\n    ]\n    hits = sum(1 for p in phrases if p in text)\n    if hits >= 2:\n        return 0.1\n    elif hits == 1:\n        return 0.05\n    else:\n        return 0.0\n"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Quality and Client Fit (LLM)", "description": "Holistic professional assessment of clarity, accuracy (at a high level), client fit, and persuasiveness of the recommendation.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Quality and Client-Focused Reasoning", "description": "Judges whether the document is professionally written, logically compares GRAT vs CRAT, and delivers a defensible recommendation tailored to the client\u2019s estate-tax objective.", "weight": 2.0, "judge_prompt": "Evaluate the document\u2019s overall professional quality and client fit. You may consider content strength here (unlike Stage 1), but do not re-check detailed math.\n\nConsider:\n- Clarity and correctness at a high level when explaining GRAT and CRAT mechanics, tax implications, advantages, disadvantages, and risks\n- Whether the side-by-side comparison and scenario make the trade-offs understandable\n- Tailoring to the specific client: age 62, married, $16M liquidity from 2015 sale, desire to reduce estate tax exposure for children while considering philanthropy\n- Strength and defensibility of the recommendation: does it clearly state which option best reduces estate tax exposure for heirs (or a combination/alternative) and why?\n- Professionalism: structure, tone, readability, tables/figures, brief but sufficient caveats\n\nScoring for this rule (0.0 to 2.0):\n- 2.0: Exceptional professional artifact\u2014clear, accurate, tailored to facts, and a persuasive, defensible recommendation aligned to minimizing estate tax for children\n- 1.4: Strong and professional with minor gaps or generic phrasing; recommendation still defensible\n- 0.8: Adequate but partially generic or shallow; recommendation only loosely tied to estate-tax objective\n- 0.3: Weak/unclear comparison or recommendation; difficult to follow\n- 0.0: Misleading or materially incorrect (e.g., implies CRAT remainder goes to heirs or GRAT gives a charitable deduction)\n\nProvide a score only; no need to restate content.", "expectation": "A polished, client-appropriate comparison with a clear, defensible recommendation aligned to minimizing estate tax exposure for the client\u2019s children."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "575f8679-b4c1-47a2-8e96-d570d4ed9269", "rubric": {"category_name": "Government | Child, Family, and School Social Workers \u2014 Program Evaluation Plan (Immigrant Families\u2019 Mental Health)", "rationale": "Three-stage, self-documenting rubric. Stage 1 (LLM-only) enforces exact document structure and required sections so verification is trivial. Stage 2 mixes code rules to verify presence of mandated elements (validated tools, methods completeness, data/analysis specificity). Stage 3 uses an LLM judge for holistic quality, practicality, and tailoring to immigrant families in Northwest Kansas.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Shape and Structure Gate (LLM Only)", "description": "Gate: Verify deliverable is a well-structured Word document with all REQUIRED sections and an Appendix containing instruments/tools aligned to the task.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Document Format and Required Sections Present", "description": "Checks that the output is a DOCX Word document (preferred) with all required sections, clearly labeled, professionally formatted, and with an Appendix including validated tools (PHQ-9, GAD-7).", "weight": 4.0, "judge_prompt": "You are evaluating whether the candidate\u2019s final deliverable is correctly shaped and complete in structure. Only check the PRESENCE and STRUCTURE of required elements (not content quality or correctness).\n\nDeliverable requirement:\n- Must be a Word document in DOCX format. Be flexible if a PDF export is provided with clear Word-style formatting.\n- Professional formatting with headings and clear section separation.\n- At least 2 pages of content.\n\nRequired sections and elements (flexible with exact header names, but meaning must match):\n1) Program Overview (or Overview/Introduction):\n   - Brief description of the program\u2019s goals and target population (immigrant families in Northwest Kansas).\n2) Evaluation Framework:\n   - Explains use of both Formative and Summative evaluation methods.\n   - Sub-sections or clearly separated paragraphs: \u201cFormative evaluation\u201d (monitoring and improvement during implementation) and \u201cSummative evaluation\u201d (outcomes/impact at an endpoint).\n3) Data Collection and Analysis Methods:\n   - Specifics about: data collection tools (e.g., surveys, assessments, interview guides, observation forms), data sources (e.g., participants, staff, community partners), quantitative and qualitative measures, and how data will be analyzed to track progress and measure impact.\n   - This section should include bullet lists and/or tables OR clearly enumerated items.\n4) Appendix: Instruments and Tools:\n   - Summaries, sample questions, or citations/links to validated tools (PHQ-9, GAD-7) aligned to the evaluation plan.\n   - Tools should be appropriate for immigrant populations (you only need to check that tools are listed/referenced here, not quality).\n\nScoring (structure only):\n- 4.0: DOCX or equivalent PDF export; all four major sections present with clear headers; the Evaluation Framework explicitly separates/formally labels both Formative and Summative; Data Collection/Analysis lists tools, sources, measures, and analysis approach; Appendix contains instruments/tools with PHQ-9 and GAD-7 clearly referenced (as summaries, sample items, or citations/links); length \u2265 2 pages.\n- 3.0: Valid DOCX/PDF and \u2265 2 pages; all four major sections present but one sub-element is weak or missing (e.g., Appendix present but only one of PHQ-9 or GAD-7 is mentioned; or Data Collection/Analysis present but missing either measures or analysis detail; or Formative/Summative not clearly separated but both are mentioned).\n- 2.0: Valid DOCX/PDF and \u2265 2 pages; 3 of 4 required sections present, or multiple sub-elements missing.\n- 1.0: Valid DOCX/PDF but only 1\u20132 sections present or sections are not clearly identifiable.\n- 0.0: Not a DOCX/PDF OR fewer than 2 pages.\n\nImportant: Do NOT evaluate content quality or correctness here\u2014only presence/structure and format.", "expectation": "A DOCX (or PDF-export) with clearly labeled: Program Overview; Evaluation Framework detailing both Formative and Summative; Data Collection and Analysis Methods with tools, sources, measures, analysis; and an Appendix listing instruments (explicitly PHQ-9 and GAD-7). At least 2 pages."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification of Required Elements (Code + Light Semantics)", "description": "Now that the structure is enforced, verify key elements exist and are internally consistent: validated tools in Appendix, completeness of formative/summative explanations, and specificity of data and analysis methods.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Appendix Instruments Presence and Validated Tools", "description": "Checks the presence of an Appendix and explicit references to PHQ-9 and GAD-7, plus at least one URL citation/link.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n\n    text = \"\"\n    try:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = context.files.read_pdf_text(output.id)\n    except Exception:\n        text = text or \"\"\n\n    if not text:\n        return 0.0, \"Unable to read document text.\"\n\n    t = text.lower()\n    has_appendix = \"appendix\" in t\n\n    phq_pat = re.compile(r\"\\bphq[-\\s]?9\\b\", re.I)\n    gad_pat = re.compile(r\"\\bgad[-\\s]?7\\b\", re.I)\n\n    has_phq9 = bool(phq_pat.search(text))\n    has_gad7 = bool(gad_pat.search(text))\n\n    links = re.findall(r\"https?://\\S+\", text)\n    has_link = len(links) > 0\n\n    score_parts = 0.0\n    feedback = []\n\n    # Instruments credit\n    if has_phq9 and has_gad7:\n        score_parts += 0.7\n    elif has_phq9 or has_gad7:\n        score_parts += 0.4\n        feedback.append(\"Only one validated tool (PHQ-9 or GAD-7) explicitly referenced.\")\n    else:\n        feedback.append(\"Neither PHQ-9 nor GAD-7 explicitly referenced.\")\n\n    # Appendix credit\n    if has_appendix:\n        score_parts += 0.2\n    else:\n        feedback.append(\"Appendix section not detected.\")\n\n    # Link credit\n    if has_link:\n        score_parts += 0.1\n    else:\n        feedback.append(\"No citation/link detected for instruments or references.\")\n\n    score = max(0.0, min(1.0, score_parts))\n    return score, \"; \".join(feedback) if feedback else \"All required instrument references present.\""}, {"type": "code", "name": "Evaluation Methods Completeness (Formative + Summative)", "description": "Verifies both formative and summative methods are discussed with sufficient explanation (>=2 sentences referencing each), indicating purpose and use.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n\n    text = \"\"\n    try:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = context.files.read_pdf_text(output.id)\n    except Exception:\n        text = text or \"\"\n\n    if not text:\n        return 0.0, \"Unable to read document text.\"\n\n    # Split into sentences\n    sentences = re.findall(r\"[^.!?]+[.!?]\", text)\n    s_lc = [s.lower() for s in sentences]\n\n    form_sents = [s for s in s_lc if \"formative\" in s]\n    summ_sents = [s for s in s_lc if \"summative\" in s]\n\n    has_formative = len(form_sents) >= 2\n    has_summative = len(summ_sents) >= 2\n\n    # Look for purpose keywords within respective sentences\n    form_keywords = (\"improve\", \"refine\", \"feedback\", \"fidelity\", \"monitor\", \"process\")\n    summ_keywords = (\"impact\", \"outcome\", \"effectiveness\", \"pre-post\", \"change\", \"result\")\n\n    form_kw = any(any(k in s for k in form_keywords) for s in form_sents)\n    summ_kw = any(any(k in s for k in summ_keywords) for s in summ_sents)\n\n    score = 0.0\n    feedback = []\n\n    # Base credit: presence\n    if has_formative:\n        score += 0.4\n    else:\n        feedback.append(\"Formative evaluation not explained in at least two sentences.\")\n    if has_summative:\n        score += 0.4\n    else:\n        feedback.append(\"Summative evaluation not explained in at least two sentences.\")\n\n    # Purpose clarity bonus\n    if has_formative and form_kw:\n        score += 0.1\n    else:\n        feedback.append(\"Formative section lacks clear purpose (improvement/monitoring).\")\n    if has_summative and summ_kw:\n        score += 0.1\n    else:\n        feedback.append(\"Summative section lacks clear purpose (outcomes/impact).\")\n\n    # Cross-reference bonus: mention both together at least once\n    text_lc = text.lower()\n    if \"formative\" in text_lc and \"summative\" in text_lc:\n        score += 0.0  # reserved; keep capped at 1.0\n\n    score = max(0.0, min(1.0, score))\n    return score, \"; \".join(feedback) if feedback else \"Formative and summative methods are sufficiently explained.\""}, {"type": "code", "name": "Data Collection Specificity and Analysis Plan", "description": "Checks for specific tools, data sources, measures (quantitative and qualitative), and analysis methods.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output detected.\"\n\n    text = \"\"\n    try:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            text = context.files.read_pdf_text(output.id)\n    except Exception:\n        text = text or \"\"\n\n    if not text:\n        return 0.0, \"Unable to read document text.\"\n\n    t = text.lower()\n\n    # Tools\n    tools_vocab = {\n        \"survey\", \"questionnaire\", \"assessment\", \"phq\", \"gad\", \"interview\", \"focus group\", \"observation\", \"observation form\"\n    }\n    tools_found = {w for w in tools_vocab if w in t}\n\n    # Data sources\n    sources_vocab = {\n        \"participant\", \"participants\", \"family\", \"families\", \"parent\", \"parents\", \"youth\", \"staff\", \"case worker\", \"caseworker\", \"provider\", \"community partner\", \"community partners\", \"school\"\n    }\n    sources_found = {w for w in sources_vocab if w in t}\n\n    # Measures\n    has_quant = \"quantitative\" in t or any(w in t for w in [\"scale\", \"score\", \"pre-post\", \"pretest\", \"posttest\", \"likert\"]) \n    has_qual = \"qualitative\" in t or any(w in t for w in [\"thematic\", \"coding\", \"open-ended\", \"interview transcript\", \"focus group transcript\"]) \n\n    # Analysis methods\n    analysis_vocab = {\n        \"descriptive statistics\", \"mean\", \"median\", \"regression\", \"t-test\", \"paired t\", \"anova\", \"chi-square\", \"thematic analysis\", \"content analysis\", \"trend\", \"time series\"\n    }\n    analysis_found = {w for w in analysis_vocab if w in t}\n\n    score = 0.0\n    feedback = []\n\n    # Tools specificity (0.35)\n    tools_score = 0.0\n    if len(tools_found) >= 3:\n        tools_score = 0.35\n    elif len(tools_found) == 2:\n        tools_score = 0.25\n        feedback.append(\"Only two distinct data collection tools detected.\")\n    elif len(tools_found) == 1:\n        tools_score = 0.15\n        feedback.append(\"Only one data collection tool detected.\")\n    else:\n        feedback.append(\"No specific data collection tools detected.\")\n    score += tools_score\n\n    # Sources specificity (0.25)\n    sources_score = 0.0\n    # Prioritize participants, staff, community partners\n    core_sources = 0\n    if any(s in t for s in [\"participant\", \"participants\", \"families\", \"family\", \"parent\", \"parents\", \"youth\"]):\n        core_sources += 1\n    if \"staff\" in t or \"case worker\" in t or \"caseworker\" in t or \"provider\" in t:\n        core_sources += 1\n    if \"community partner\" in t or \"community partners\" in t or \"school\" in t:\n        core_sources += 1\n\n    if core_sources >= 3:\n        sources_score = 0.25\n    elif core_sources == 2:\n        sources_score = 0.18\n        feedback.append(\"Only two core data sources (participants, staff, partners) detected.\")\n    elif core_sources == 1:\n        sources_score = 0.10\n        feedback.append(\"Only one core data source detected.\")\n    else:\n        feedback.append(\"No core data sources (participants, staff, partners) detected.\")\n    score += sources_score\n\n    # Measures breadth (0.20)\n    measures_score = 0.0\n    if has_quant and has_qual:\n        measures_score = 0.20\n    elif has_quant or has_qual:\n        measures_score = 0.12\n        feedback.append(\"Only one of quantitative/qualitative measures referenced.\")\n    else:\n        feedback.append(\"Neither quantitative nor qualitative measures detected.\")\n    score += measures_score\n\n    # Analysis methods specificity (0.20)\n    analysis_score = 0.0\n    if len(analysis_found) >= 2:\n        analysis_score = 0.20\n    elif len(analysis_found) == 1:\n        analysis_score = 0.12\n        feedback.append(\"Only one analysis method detected.\")\n    else:\n        feedback.append(\"No specific analysis methods detected.\")\n    score += analysis_score\n\n    # Normalize to 0..1 (already constructed from weighted parts)\n    score = max(0.0, min(1.0, score))\n    return score, \"; \".join(feedback) if feedback else \"Data tools, sources, measures, and analysis are sufficiently specified.\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Tailoring (LLM)", "description": "Holistic assessment of clarity, practicality, ethics, and tailoring to immigrant families in Northwest Kansas for an Executive Director audience.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professionalism, Practicality, and Cultural Responsiveness", "description": "Evaluates overall quality: clear, feasible plan; ethical safeguards; cultural/linguistic appropriateness; tailoring to immigrant families in Northwest Kansas; and usefulness to an Executive Director.", "weight": 2.0, "judge_prompt": "Judge the overall quality of the evaluation plan, now that structure and key elements exist. Focus on clarity, practicality, ethics, and tailoring to immigrant families in Northwest Kansas.\n\nConsider:\n- Clarity and coherence for an Executive Director (well-structured narrative; concise but complete; actionable next steps/timeline are reasonably implied or stated).\n- Practicality and feasibility (realistic scope, staffing, frequency of data collection; feasible analysis plan; how findings will be used for decisions).\n- Ethical and privacy safeguards (confidentiality, consent, duty to report, risk management, data security; sensitivity to immigration-related risks; trauma-informed approach).\n- Cultural and linguistic responsiveness (language access, translation/adaptation of tools, culturally relevant examples; notes on validity for immigrant populations; navigation supports).\n- Local relevance (Northwest Kansas context: rural access, community partners like schools/clinics/faith orgs; if mentioned, credit accordingly).\n\nScoring:\n- 2.0: Highly professional, clearly written, practical and feasible; strong ethical/privacy safeguards; explicitly addresses language access and cultural responsiveness; tailored to immigrant families in Northwest Kansas with relevant local considerations; provides strong value to an Executive Director.\n- 1.5: Generally strong and practical; covers ethics and cultural responsiveness but with minor gaps; mostly tailored to immigrant families and somewhat to Northwest Kansas; useful for leadership.\n- 1.0: Adequate but uneven clarity or feasibility; ethics/cultural responsiveness referenced superficially; limited tailoring to local context; still usable but needs refinement.\n- 0.5: Minimal professionalism or practicality; ethical/cultural elements largely missing; little sign of tailoring.\n- 0.0: Not professional or practical; ignores ethics/cultural factors; not tailored.\n\nDo not re-check structure from Stage 1; focus on holistic quality and appropriateness.", "expectation": "A professional, leadership-ready plan that is feasible, ethically sound, and clearly tailored to immigrant families in Northwest Kansas, including language access and cultural responsiveness."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "0fad6023-767b-42c1-a1b3-027cd4f583cb", "rubric": {"category_name": "Retail Planogram Tool - Meat & Seafood FSC (Excel)", "rationale": "This rubric enforces a self-documenting, verifiable Excel planogram (POG) tool for a 24-foot full-service case (FSC). Stage 1 (LLM-only) mandates a precise Excel shape: named sheets, a visual pan layout with editable pan widths and descriptions, and a clear used-vs-available space summary plus an Instructions tab. Stage 2 uses code to verify presence and consistency of key calculations and structures that Stage 1 made discoverable. Stage 3 holistically assesses usability and presentation for beginner-level Excel users, including printer-friendliness.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 - Structured Output Gate (LLM Only)", "description": "Gate check that the output is a single Excel file titled \u201cMeat Seafood FSC POG Template\u201d, with a POG builder sheet showing all pans across a 24-foot case, editable pan widths, editable item descriptions, used-vs-available space calculation, and a beginner-friendly Instructions sheet. This checks the presence and structure only, not calculation correctness.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Excel POG Required Structure", "description": "Check that the candidate produced an Excel-based POG with the exact structural elements needed for later code verification.", "weight": 4.0, "judge_prompt": "You are validating STRUCTURE ONLY (not correctness) of an Excel planogram tool for a 24-foot Full-Service Case (FSC) used by Meat and Seafood Team Leaders.\n\nFile and Format Requirements:\n- Must be an Excel spreadsheet (.xlsx). Title should be \u201cMeat Seafood FSC POG Template\u201d (flexible on capitalization/spaces but semantically the same). Not a PDF/Word.\n\nSheet Requirements (flexible on exact names, but must be clearly identifiable):\n1) Primary POG builder sheet (acceptable names include: \u201cPOG Builder\u201d, \u201cPOG\u201d, \u201cPlanogram\u201d, \u201cFSC POG\u201d, \u201cBuilder\u201d). It must contain ALL of the following:\n   - Visual representation of the 24-foot case laid out as a sequence of pans spanning end-to-end. The visual can be a grid/bar layout or merged-cell layout representing pans along the case.\n   - Editable pan width per pan (must clearly support 6- or 8-inch pans; a dropdown, data validation, or free numeric input labeled as inches is acceptable). The user must be able to change pan widths.\n   - An editable text field per pan to describe what is in each pan (e.g., Item/Description column or cells in the layout).\n   - A clearly labeled summary of space usage that compares Used space versus Available space (24 ft = 288 inches). This should show at least: Available length (24 ft or 288 in), Used length (sum of pan widths), and Remaining or Overage. Values must be visible in the sheet.\n   - Printer-friendly layout: visible page setup cues (e.g., one-page width, landscape orientation, clear borders/headers) OR a separate print-optimized view. The POG should be readable when printed.\n2) Instructions sheet (acceptable names include: \u201cInstructions\u201d, \u201cHow To\u201d, \u201cRead Me\u201d). It must:\n   - Be written for beginner-level Excel users.\n   - Include step-by-step guidance for: editing pan widths, entering descriptions, understanding used vs available, and printing the plan.\n   - Mention the 24-foot total and the concept of 6- and 8-inch pans.\n\nScoring (0 to 4.0):\n- 4.0: Valid Excel + POG builder sheet present with all required elements (visual pan layout, editable widths, per-pan descriptions, used vs available summary) + Instructions sheet present with beginner-friendly, step-by-step guidance + printer-friendly setup apparent.\n- 3.0: Valid Excel + POG builder has visual pans, editable widths, and per-pan descriptions + used vs available shown, but printer-friendly setup is unclear OR instructions are present but not clearly step-by-step or miss one key topic.\n- 2.0: Valid Excel + POG builder present but missing one core element (e.g., no editable widths OR no per-pan descriptions OR no used vs available summary) OR no instructions tab.\n- 1.0: Valid Excel with an attempt at a POG but structure is largely incomplete (e.g., only a rough table without a coherent layout; multiple critical elements missing).\n- 0.0: Not an Excel file or lacks any identifiable POG builder sheet.\n\nOnly evaluate presence and structure. Do not assess math correctness or design quality.", "expectation": "A clean Excel file named as requested, with a clear POG builder sheet showing the full 24-ft case via pans, editable widths and descriptions, plus a used vs available summary, and a separate Instructions sheet for beginners."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 - Correctness and Consistency Checks (Code + Light LLM)", "description": "Deterministic verification of key elements enabled by Stage 1 structure: file type/name, sheet discovery, presence of pan width and item description structures, used vs available calculations, predominant use of 6 or 8 inches, and practical instructions content.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Proper File Type and Title", "description": "Verify output is a spreadsheet and the filename matches the requested title (flexibly).", "weight": 0.5, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No primary output.\"\n    # Check spreadsheet\n    if not getattr(output, 'is_spreadsheet', False):\n        return 0.0, \"Output is not a spreadsheet.\"\n    # Check filename/title\n    try:\n        path = context.files.get_path(output.id)\n        name = path.name.lower()\n        target_words = [\"meat\", \"seafood\", \"fsc\", \"pog\", \"template\"]\n        match_score = sum(1 for w in target_words if w in name)\n        score = 0.5 if match_score >= 4 else 0.3 if match_score >= 3 else 0.0\n        fb = f\"Filename='{path.name}', keyword matches={match_score}/5.\"\n        return score, fb\n    except Exception as e:\n        return 0.0, f\"Error checking filename: {e}\""}, {"type": "code", "name": "Discover POG and Instructions Sheets", "description": "Fuzzy-find sheets likely to be the POG builder and Instructions. Rewards presence of both.", "weight": 0.7, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    def find_sheet(sheets, keywords):\n        for s in sheets:\n            s_low = s.lower()\n            if any(k in s_low for k in keywords):\n                return s\n        return None\n\n    output = context.get_primary_output()\n    if not output or not getattr(output, 'is_spreadsheet', False):\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        xf = pd.ExcelFile(path)\n        sheets = xf.sheet_names\n        pog_sheet = find_sheet(sheets, [\"pog\", \"planogram\", \"builder\", \"fsc\"])    \n        instr_sheet = find_sheet(sheets, [\"instruction\", \"how to\", \"read me\", \"readme\"])\n        score = 0.0\n        if pog_sheet:\n            score += 0.4\n        if instr_sheet:\n            score += 0.3\n        fb = f\"Sheets found: {sheets}. POG='{pog_sheet}', Instructions='{instr_sheet}'.\"\n        return score, fb\n    except Exception as e:\n        return 0.0, f\"Error reading sheets: {e}\""}, {"type": "code", "name": "POG Sheet Has Pan Widths and Item Descriptions", "description": "Detects a table/area on the POG-like sheet with columns for width and item/description.", "weight": 0.8, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    def find_sheet(sheets, keywords):\n        for s in sheets:\n            s_low = s.lower()\n            if any(k in s_low for k in keywords):\n                return s\n        return None\n\n    def find_headers(df):\n        # Search first ~50 rows for a header row containing width + item/description\n        max_rows = min(50, df.shape[0])\n        for r in range(max_rows):\n            row = df.iloc[r].astype(str).str.strip().str.lower().tolist()\n            row_set = set(row)\n            has_width = any(\"width\" in c or \"inch\" in c or \"inches\" in c for c in row)\n            has_item = any(\"item\" in c or \"desc\" in c or \"product\" in c for c in row)\n            if has_width and has_item:\n                return r\n        return None\n\n    def sample_width_values(df, header_row):\n        # Try to locate width column index by best keyword match in header row\n        headers = df.iloc[header_row].astype(str).str.strip().str.lower()\n        idx_candidates = [i for i, h in enumerate(headers) if (\"width\" in h) or (\"inch\" in h) or (\"inches\" in h)]\n        if not idx_candidates:\n            return [], None\n        w_idx = idx_candidates[0]\n        # Collect numeric widths below header\n        values = []\n        for r in range(header_row+1, df.shape[0]):\n            val = df.iat[r, w_idx]\n            try:\n                num = float(val)\n                values.append(num)\n            except Exception:\n                continue\n        return values, w_idx\n\n    output = context.get_primary_output()\n    if not output or not getattr(output, 'is_spreadsheet', False):\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        xf = pd.ExcelFile(path)\n        pog_sheet = None\n        for s in xf.sheet_names:\n            if any(k in s.lower() for k in [\"pog\", \"planogram\", \"builder\", \"fsc\"]):\n                pog_sheet = s; break\n        if not pog_sheet:\n            return 0.0, \"No POG-like sheet detected.\"\n        df = pd.read_excel(path, sheet_name=pog_sheet, header=None)\n        header_row = find_headers(df)\n        if header_row is None:\n            return 0.2, \"Could not find a header row with width+item/description; awarding minimal credit for sheet presence.\"\n        widths, w_idx = sample_width_values(df, header_row)\n        score = 0.8 if widths else 0.5\n        fb = f\"Header row at {header_row}, width_col={w_idx}, width_samples_count={len(widths)}.\"\n        return score, fb\n    except Exception as e:\n        return 0.0, f\"Error analyzing POG sheet: {e}\""}, {"type": "code", "name": "Used vs Available Calculation and Bounds", "description": "Finds labels for used/available/remaining and checks plausibility vs 24 ft (288 in).", "weight": 1.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    def find_sheet(sheets, keywords):\n        for s in sheets:\n            if any(k in s.lower() for k in keywords):\n                return s\n        return None\n\n    def find_labeled_number(df, label_keywords):\n        # Returns first (row, col, value) where a nearby cell has a number\n        h, w = df.shape\n        for r in range(h):\n            for c in range(w):\n                cell = str(df.iat[r, c]).strip().lower()\n                if any(k in cell for k in label_keywords):\n                    # try same row next col, or next row same col\n                    for (rr, cc) in [(r, c+1), (r+1, c), (r, c-1), (r-1, c)]:\n                        if 0 <= rr < h and 0 <= cc < w:\n                            val = df.iat[rr, cc]\n                            try:\n                                num = float(str(val).replace(',', ''))\n                                return rr, cc, num\n                            except Exception:\n                                pass\n        return None\n\n    output = context.get_primary_output()\n    if not output or not getattr(output, 'is_spreadsheet', False):\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        xf = pd.ExcelFile(path)\n        pog_sheet = find_sheet(xf.sheet_names, [\"pog\", \"planogram\", \"builder\", \"fsc\"])\n        if not pog_sheet:\n            return 0.0, \"No POG-like sheet.\"\n        df = pd.read_excel(path, sheet_name=pog_sheet, header=None)\n        avail = find_labeled_number(df, [\"available\", \"case length\", \"capacity\", \"total length\"])\n        used = find_labeled_number(df, [\"used\", \"utilized\", \"sum of widths\", \"total used\"])\n        remain = find_labeled_number(df, [\"remaining\", \"left\", \"over\", \"overage\", \"balance\"])\n\n        score = 0.0\n        fb_parts = []\n        if avail:\n            score += 0.35\n            fb_parts.append(f\"Available={avail[2]}\")\n        if used:\n            score += 0.35\n            fb_parts.append(f\"Used={used[2]}\")\n        if remain:\n            score += 0.15\n            fb_parts.append(f\"Remaining/Overage={remain[2]}\")\n        # Plausibility vs 288 inches if both used and available present\n        if used and avail:\n            A, U = avail[2], used[2]\n            # Accept either inches or ft; detect scale by proximity to 288 or 24\n            scale_ok = (200 <= A <= 400) or (20 <= A <= 30)\n            plaus = 0.15 if scale_ok else 0.05\n            score += plaus\n            fb_parts.append(f\"ScaleOK={scale_ok}\")\n        return min(score, 1.0), \"; \".join(fb_parts) if fb_parts else \"No labeled numbers found.\"\n    except Exception as e:\n        return 0.0, f\"Error checking used/available: {e}\""}, {"type": "code", "name": "Widths Predominantly 6 or 8 Inches", "description": "From the POG widths column, verify that most entered widths are 6 or 8 (editable is allowed, but domain should reflect pan reality).", "weight": 0.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    def find_sheet(sheets, keywords):\n        for s in sheets:\n            if any(k in s.lower() for k in keywords):\n                return s\n        return None\n\n    def find_width_column(df):\n        # Try to find header row and width col\n        max_rows = min(50, df.shape[0])\n        for r in range(max_rows):\n            row = df.iloc[r].astype(str).str.strip().str.lower().tolist()\n            if any(\"width\" in c or \"inch\" in c for c in row):\n                for ci, h in enumerate(row):\n                    if (\"width\" in h) or (\"inch\" in h):\n                        return r, ci\n        return None, None\n\n    output = context.get_primary_output()\n    if not output or not getattr(output, 'is_spreadsheet', False):\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        xf = pd.ExcelFile(path)\n        pog_sheet = find_sheet(xf.sheet_names, [\"pog\", \"planogram\", \"builder\", \"fsc\"])\n        if not pog_sheet:\n            return 0.0, \"No POG sheet.\"\n        df = pd.read_excel(path, sheet_name=pog_sheet, header=None)\n        hr, wc = find_width_column(df)\n        if hr is None:\n            return 0.0, \"No width column found.\"\n        nums = []\n        for r in range(hr+1, df.shape[0]):\n            val = df.iat[r, wc]\n            try:\n                nums.append(float(str(val).replace(',', '')))\n            except Exception:\n                pass\n        if not nums:\n            return 0.2, \"Width column found but no numeric values parsed.\"\n        count_6_8 = sum(1 for n in nums if abs(n-6) < 0.25 or abs(n-8) < 0.25)\n        ratio = count_6_8 / max(1, len(nums))\n        score = 0.5 if ratio >= 0.6 else 0.3 if ratio >= 0.4 else 0.1\n        return score, f\"Width values parsed={len(nums)}, 6/8 fraction={ratio:.2f}.\"\n    except Exception as e:\n        return 0.0, f\"Error analyzing width values: {e}\""}, {"type": "code", "name": "Instructions Are Practical and Actionable", "description": "Check that the Instructions sheet contains multiple step indicators and mentions width/print/24ft concepts.", "weight": 0.5, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    def find_sheet(sheets, keywords):\n        for s in sheets:\n            if any(k in s.lower() for k in keywords):\n                return s\n        return None\n\n    output = context.get_primary_output()\n    if not output or not getattr(output, 'is_spreadsheet', False):\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        xf = pd.ExcelFile(path)\n        instr_sheet = find_sheet(xf.sheet_names, [\"instruction\", \"how to\", \"read me\", \"readme\"])\n        if not instr_sheet:\n            return 0.0, \"No Instructions sheet.\"\n        df = pd.read_excel(path, sheet_name=instr_sheet, header=None)\n        text = \"\\n\".join(df.astype(str).fillna(\"\").values.ravel().tolist()).lower()\n        # Count numbered steps like '1.' / '2)' / '3 -'\n        steps = re.findall(r\"(^|\\n)\\s*\\d+\\s*[\\.)\\-:]\\s+\", text)\n        step_count = len(steps)\n        keywords = {\n            'width': any(w in text for w in [\"width\", \"6 inch\", \"8 inch\", \"6-inch\", \"8-inch\", \"inches\"]),\n            'print': any(p in text for p in [\"print\", \"printer\", \"landscape\", \"page setup\", \"fit to page\"]),\n            'total': any(t in text for t in [\"24 ft\", \"24-foot\", \"24 feet\", \"288 in\", \"288 inches\"]) \n        }\n        score = 0.0\n        if step_count >= 6:\n            score += 0.25\n        elif step_count >= 3:\n            score += 0.15\n        score += 0.1 if keywords['width'] else 0.0\n        score += 0.1 if keywords['print'] else 0.0\n        score += 0.05 if keywords['total'] else 0.0\n        return min(score, 0.5), f\"Steps={step_count}, mentions={keywords}.\"\n    except Exception as e:\n        return 0.0, f\"Error reading instructions: {e}\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 - Usability and Presentation (LLM)", "description": "Holistic LLM assessment of beginner-friendliness, clarity, and printer-readiness beyond basic structure.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Beginner-Friendly Usability and Print Readiness", "description": "Judge whether the tool is easy for beginner Excel users and is printer-friendly for store use.", "weight": 2.0, "judge_prompt": "Evaluate the POG tool\u2019s overall usability and presentation for a beginner-level Excel user. Consider only qualities beyond basic structural presence (already evaluated). Look for:\n- Clear labeling and headings (case length, used/remaining, pan width, item/description).\n- Beginner-friendly touches: data validation or dropdown for 6/8 inches, concise legends, color coding that\u2019s readable when printed, frozen header rows/columns, obvious input cells vs calculated cells.\n- Visual clarity of the pan layout (consistent alignment, borders, fit-to-page or landscape setup) and that it looks printable without cutoffs.\n- Instructions clarity: step-by-step ordering, brevity, scannability, and direct references to core actions (edit widths, enter descriptions, confirm used\u2264available, print steps).\n\nScoring (0 to 2.0):\n- 2.0: Highly usable and polished: clear labels, obvious inputs, readable print layout, thoughtful guardrails (e.g., data validation), and crisp, stepwise instructions.\n- 1.0: Generally usable with minor rough edges (some labels unclear, print preview acceptable but not ideal, or instructions are adequate but not very concise).\n- 0.5: Usable but rough: cluttered layout or weak instructions; a beginner could struggle without help.\n- 0.0: Confusing/poorly formatted; not realistically usable by a beginner.\n\nDo not re-check structure; focus on user experience and presentation quality.", "expectation": "A clean, readable, and guided Excel experience with obvious inputs, minimal friction, and a print-ready layout suitable for store teams."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "c2e8f271-7858-412f-b460-472463ad81d9", "rubric": {"category_name": "Engineering Coding Standards Draft (Professional, Scientific, and Technical Services)", "rationale": "Pattern B (Document). The task is to produce a short, source-of-truth Word document with a clear, verifiable structure. Stage 1 uses an LLM judge to enforce exact structure and format constraints (DOCX/PDF, section presence, page limit). Stage 2 uses code rules to verify factual coverage and concrete, testable elements aligned to the specified tech stack and required topics. Stage 3 uses an LLM judge for holistic quality and usability for the intended audience (four software teams, VP review, staged rollout).", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Format and Structural Gate", "description": "LLM-only gate to ensure the output is a well-structured document (preferably DOCX), within length limits, and containing all required sections so verification is possible.", "is_required": true, "max_points": 3.0, "min_score_to_pass": 2.1, "rules": [{"type": "llm_judge", "name": "Structured Document Requirement (Gate)", "description": "Verify the candidate produced a properly structured Word/PDF document that contains the required sections with clear headers and stays within page limits.", "weight": 3.0, "judge_prompt": "You are evaluating whether the submitted output is a properly structured, reviewable coding standards document for an engineering organization. Only verify FORMAT and PRESENCE of required sections. Do not judge content quality or correctness.\n\nFormat Requirements:\n- Preferred: DOCX (Word). Acceptable: PDF. Not acceptable: plain text/markdown/spreadsheet.\n- Length: 1\u20136 pages. If more than 6 pages, score = 0. If extremely short (< 1 page), deduct.\n- Professional structure with clear section headers and readable layout.\n- A clear document title indicating Coding Standards (e.g., \"Engineering Coding Standards\", \"Company Coding Standards\", or similar) on the first page.\n\nRequired Sections (names can vary; check concept, not exact wording):\n1) Scope and Audience (e.g., \"Scope\", \"Applicability\", or \"Audience\")\n2) Baseline Style / Community Style (referencing a recognized guide such as Google TS, TS.dev, TypeScript Handbook, AWS TS practices; section may be called \"Baseline Style\", \"Style Guide\", or similar)\n3) TypeScript/Node standards (backend)\n4) React/Next.js standards (frontend/APIs)\n5) Database/ORM standards (Drizzle + Postgres/Neon)\n6) Testing standards (explicitly includes React Testing Library)\n7) Git Workflow standards including:\n   - PR titles\n   - Branch naming\n   - Commit message guidelines\n8) Documentation standards (e.g., JSDoc/TSDoc/readme patterns)\n9) Monorepo conventions (how to structure packages/workspaces)\n10) Versioning, Maintenance, and Governance (owner/DRI, update cadence, change log, contribution process)\n11) Rollout Plan (staged adoption after VP review)\n12) References (external links or citations to recognized guides; names can vary)\n\nScoring (return a single number 0.0\u20133.0):\n- 3.0: DOCX or PDF; within 1\u20136 pages; clear title; all 12 sections present with discernible headers.\n- 2.7: Missing up to 2 non-core sections (Sections 2, 9, 12 are considered non-core) OR uses PDF instead of DOCX but otherwise complete.\n- 2.4: Missing up to 3 non-core sections OR light structural issues (headers present but not perfectly formatted).\n- 2.1: All core areas present but missing 1 core section (Core sections: 1, 3, 4, 5, 6, 7, 8, 10, 11) OR marginal length issues (barely under 1 page but still structured) OR title weak but present.\n- 1.2: Multiple core sections missing OR unclear/flat structure (hard to tell where sections start/end) but still DOCX/PDF and \u22646 pages.\n- 0.0: Not DOCX/PDF, or >6 pages, or severely malformed (no recognizable sections/title).\n\nOnly evaluate structural presence and format. Do not verify content accuracy. Provide a brief reason in your feedback.", "expectation": "A 1\u20136 page DOCX (preferred) or PDF with a clear title and section headers covering scope, baseline style, backend/frontend standards, DB/ORM, testing (RTL), Git workflow (PR titles, branch naming, commits), documentation, monorepo, governance/versioning, rollout plan, and references."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Coverage Verification", "description": "Code-based and deterministic checks that the document covers required topics, is aligned to the specified tech stack and references, and includes concrete, testable conventions.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Required Topics Present (Testing, Documentation, PR/Branch, Commits)", "description": "Verify presence of the four explicitly required topics using robust keyword checks.", "weight": 1.6, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output found.\"\n\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0, \"Unable to read document text.\"\n\n    lt = text.lower()\n\n    # Required topics\n    testing_signals = [\n        \"testing\", \"react testing library\", \"rtl\", \"unit test\", \"integration test\", \"jest\"\n    ]\n    documentation_signals = [\n        \"documentation\", \"jsdoc\", \"tsdoc\", \"readme\", \"docs/\", \"docstring\"\n    ]\n    pr_branch_signals = [\n        \"pull request\", \"pr title\", \"branch naming\", \"branch name\", \"feature/\", \"bugfix/\", \"hotfix/\"\n    ]\n    commit_signals = [\n        \"commit message\", \"conventional commits\", \"feat:\", \"fix:\", \"chore:\", \"refactor:\", \"docs:\", \"perf:\", \"style:\"\n    ]\n\n    def has_any(signals):\n        return any(s in lt for s in signals)\n\n    topics = [\n        (\"Testing\", has_any(testing_signals)),\n        (\"Documentation\", has_any(documentation_signals)),\n        (\"PR/Branch Naming\", has_any(pr_branch_signals)),\n        (\"Commit Messages\", has_any(commit_signals)),\n    ]\n\n    covered = sum(1 for _, ok in topics if ok)\n    score_ratio = covered / 4.0\n    weight = 1.6\n    missing = [name for name, ok in topics if not ok]\n    feedback = f\"Covered {covered}/4 required topics. Missing: {', '.join(missing) if missing else 'None'}.\"\n    return weight * score_ratio, feedback"}, {"type": "code", "name": "Tech Stack Alignment Coverage", "description": "Check that the document references the specified stack: TypeScript/Node, React/Next.js, Drizzle, Postgres/Neon, RTL, Prettier, Monorepo.", "weight": 1.6, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output found.\"\n\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0, \"Unable to read document text.\"\n\n    lt = text.lower()\n\n    signals = {\n        \"typescript\": [\"typescript\", \"ts \"],\n        \"node\": [\"node.js\", \"nodejs\", \"node\"],\n        \"react\": [\"react\"],\n        \"nextjs\": [\"next.js\", \"nextjs\", \"next js\"],\n        \"drizzle\": [\"drizzle\"],\n        \"postgres\": [\"postgres\", \"postgresql\"],\n        \"neon\": [\"neon\"],\n        \"rtl\": [\"react testing library\", \"rtl\"],\n        \"prettier\": [\"prettier\"],\n        \"monorepo\": [\"monorepo\", \"mono repo\", \"workspace\", \"pnpm workspaces\", \"yarn workspaces\"],\n    }\n\n    def present(keys):\n        for k in keys:\n            if k in lt:\n                return True\n        return False\n\n    found = 0\n    details = []\n    for name, keys in signals.items():\n        ok = present(keys)\n        found += 1 if ok else 0\n        details.append(f\"{name}:{'yes' if ok else 'no'}\")\n\n    total = len(signals)\n    ratio = found / total if total else 0\n    weight = 1.6\n    return weight * ratio, \"Signals => \" + \", \".join(details)"}, {"type": "code", "name": "External References Included", "description": "Verify that at least two of the recommended external guides are cited or linked.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output found.\"\n\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0, \"Unable to read document text.\"\n\n    lt = text.lower()\n\n    refs = {\n        \"google_ts\": [\"google\", \"google.github.io/styleguide/tsguide\"],\n        \"ts_dev\": [\"ts.dev\", \"ts dev\"],\n        \"ts_handbook\": [\"typescript handbook\", \"typescriptlang.org/docs/handbook\"],\n        \"aws_ts\": [\"aws\", \"typescript best practices\", \"cdk\", \"prescriptive-guidance\"],\n    }\n\n    count = 0\n    details = []\n    for name, keys in refs.items():\n        present = any(k in lt for k in keys)\n        if present:\n            count += 1\n        details.append(f\"{name}:{'yes' if present else 'no'}\")\n\n    weight = 0.8\n    # Full credit if >=2 references, otherwise proportional (0, 0.5, 1.0)\n    if count >= 2:\n        score = weight\n    elif count == 1:\n        score = weight * 0.5\n    else:\n        score = 0.0\n\n    return score, f\"References found: {count}. Details => \" + \", \".join(details)"}, {"type": "code", "name": "Git Conventions Specificity (Commits/Branches/PR titles)", "description": "Check for presence of concrete conventions: Conventional Commits or similar tokens, branch naming examples/patterns, and PR title guidance.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output found.\"\n\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0, \"Unable to read document text.\"\n\n    lt = text.lower()\n\n    # Commit conventions (Conventional Commits or clear tokens)\n    commit_token_re = re.compile(r\"\\b(feat|fix|chore|docs|refactor|test|perf|style)(\\(|:)\\b\")\n    commits_ok = bool(commit_token_re.search(lt)) or \"conventional commits\" in lt\n\n    # Branch naming examples/patterns\n    branch_ok = any(s in lt for s in [\n        \"feature/\", \"bugfix/\", \"hotfix/\", \"release/\", \"chore/\", \"refactor/\"\n    ]) or \"branch naming\" in lt\n\n    # PR title guidance\n    pr_ok = (\"pr title\" in lt) or (\"pull request title\" in lt) or (\"title format\" in lt and \"pr\" in lt)\n\n    parts = [commits_ok, branch_ok, pr_ok]\n    ratio = sum(1 for x in parts if x) / 3.0\n    weight = 0.5\n    return weight * ratio, f\"Commits:{commits_ok}, Branches:{branch_ok}, PR titles:{pr_ok}\""}, {"type": "code", "name": "Governance, Versioning, and Rollout", "description": "Check for owner/DRI, versioning/change log or review cadence, and a staged rollout plan after VP review.", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output found.\"\n\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                return 0.0, \"Unable to read document text.\"\n\n    lt = text.lower()\n\n    owner_ok = any(k in lt for k in [\"owner\", \"dri\", \"approver\", \"responsible\", \"document owner\"])\n    versioning_ok = any(k in lt for k in [\"version\", \"v1.\", \"last updated\", \"change log\", \"changelog\", \"review cadence\", \"monthly review\", \"quarterly review\", \"release cycle\", \"contribution process\", \"how to propose changes\"])\n    rollout_ok = any(k in lt for k in [\"rollout\", \"staged\", \"phase\", \"pilot\", \"vp review\", \"after vp review\", \"adoption plan\"])\n\n    parts = [owner_ok, versioning_ok, rollout_ok]\n    ratio = sum(1 for x in parts if x) / 3.0\n    weight = 0.5\n    return weight * ratio, f\"Owner/DRI:{owner_ok}, Versioning/ChangeLog/Cadence:{versioning_ok}, Rollout plan:{rollout_ok}\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Usability", "description": "LLM judge assesses clarity, readability, concision (\u22646 pages), and practical usability for teams and reviewers.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity, Actionability, and Fitness for Purpose", "description": "Qualitative assessment of presentation quality, clarity, and how actionable the document is for engineers and reviewers.", "weight": 2.0, "judge_prompt": "Assess the overall professional quality and usability of the Coding Standards document. Consider the intended audience (four software teams, Engineering Manager, VP of Engineering) and the purpose (reduce review friction and accelerate delivery).\n\nEvaluate on:\n- Clarity and organization: logical sections, scannable headings, concise writing, no fluff.\n- Actionability: concrete rules, examples or snippets, checklists, and references that enable fast adoption.\n- Appropriateness: tone and scope for a 1\u20136 page initial standard that can be expanded; avoids over-prescription while covering key pitfalls.\n- Consistency with stack: guidance aligns with TypeScript/Node, React/Next.js, Drizzle + Postgres (Neon), React Testing Library, Prettier, monorepo.\n- Reviewability and maintainability: version/date, owner/DRI, change process, and staged rollout are easy to find.\n\nScoring (0.0\u20132.0):\n- 2.0: Clear, concise, highly actionable; obviously usable by teams; well-aligned with the stack; easy to maintain.\n- 1.5: Generally clear and actionable with minor issues or small gaps.\n- 1.0: Mixed clarity or limited actionability; some alignment issues or missing helpful examples.\n- 0.5: Hard to follow, generic, or not clearly applicable to this stack.\n- 0.0: Unprofessional, confusing, or clearly unsuitable for the task.\n\nProvide a brief rationale for the score.", "expectation": "A concise, well-structured, immediately usable standard with concrete, stack-aligned rules and quick-reference elements that teams can adopt with minimal friction."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "1137e2bb-bdf9-4876-b572-f29b7de5e595", "rubric": {"category_name": "Wholesale Order Audit \u2013 Price and Case-Pack Validation", "rationale": "Mixed deliverable: a structured Excel audit plus a brief Word/PDF summary. Stage 1 uses LLM-only gating to mandate an exact, verification-friendly shape. Stage 2 applies code checks to verify correctness of price mismatch flags, case-pack logic, total error math, and summary aggregation. Stage 3 uses LLM for professional quality and strategic usefulness of insights.", "max_total_score": 25.0, "stages": [{"name": "Stage 1 \u2013 Shape Enforcement Gate (LLM-only)", "description": "Gate: Verify the deliverables exist with the mandated, verifiable structure enabling deterministic checks in Stage 2.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 7.0, "rules": [{"type": "llm_judge", "name": "Excel Structure: Line Audit Sheet", "description": "Check that an Excel (.xlsx) deliverable exists and contains a primary line-level audit sheet with required base and derived columns.", "weight": 6.0, "judge_prompt": "You are verifying the STRUCTURE ONLY (not correctness) of the Excel deliverable. Consider all provided outputs in this workflow.\n\nRequirements \u2013 Excel (.xlsx) must exist and include a line-level audit sheet named one of:\n- \"Line Audit\", \"Order Line Audit\", or similarly clear (flexible naming ok: e.g., \"Audit - Lines\", \"Line-Level Errors\"). This is the main sheet for row-by-row validation.\n\nOn that sheet, confirm presence of the following columns (exact names flexible; synonyms allowed). They can appear in any order:\n- SKU identifier: one of [SKU, Item, Item Number, Product ID]\n- PO identifier: one of [PO, PO Number, Purchase Order, Order ID]\n- Ordered quantity: one of [Ordered Units, Order Qty, Ordered Qty, Quantity]\n- Entered Unit Price: one of [Entered Unit Price, Invoice Unit Price, Actual Unit Price]\n- Expected Unit Price: one of [Expected Unit Price, Standard Unit Price, List Unit Price]\n- UOM: one of [UOM, Unit Order Multiple, Order UOM] (values typically EA or CASE)\n- Case pack: one of [Case Pack, CasePack, Pack Size]\n- Ship-to: one of [Ship-to Location, Ship To, ShipTo]\n\nDerived audit columns (must be present):\n- Price mismatch flag: one of [Price Mismatch, Price Mismatch Error, Price Error Flag] as a binary/boolean indicator (0/1, TRUE/FALSE)\n- Case pack flag: one of [Case Pack Error, Case-Pack Violation, Multiple Violation] as a binary/boolean indicator (0/1, TRUE/FALSE)\n- Total errors: one of [Total Errors, Error Count] representing the per-line total (sum of the two flags; can be 0,1, or 2)\n- Error summary text: one of [Error Type Summary, Error Summary, Error Notes] containing a short human-readable description per line (e.g., \"Price mismatch; Case-pack violation\").\n\nScoring:\n- 6.0: Excel exists and the main line-level audit sheet contains ALL base columns and ALL derived columns above.\n- 4.5: Excel exists; sheet present; missing exactly one of the DERIVED columns OR exactly one of the BASE columns.\n- 3.0: Excel exists; sheet present; missing two required columns total (base+derived).\n- 1.5: Excel exists but sheet is not clearly a line-level audit OR missing three+ required columns.\n- 0.0: No Excel, or no appropriate line-level audit sheet.\n\nOnly judge structure and column presence. Do NOT judge calculation correctness.", "expectation": "A clean, verifiable line-level audit sheet with base fields and added error columns."}, {"type": "llm_judge", "name": "Excel Structure: SKU Error Summary (with PO drilldown)", "description": "Check that the Excel deliverable includes a summary sheet/pivot aggregating errors by SKU with drilldown to PO.", "weight": 1.0, "judge_prompt": "Verify, in the same Excel file, a summary or pivot sheet exists named one of: \"SKU Error Summary\", \"Error Summary\", \"Pivot - Errors\", or similar. Flexible naming allowed.\n\nRequired structure:\n- Rows must aggregate at least at SKU level (one row per SKU or hierarchical rows with SKU at top level).\n- Must display, for each SKU, counts of: Price Mismatch Errors, Case Pack Errors, and Total Errors.\n- Must provide a drilldown or secondary grouping to PO (e.g., pivot hierarchy SKU -> PO, or a companion section/table enabling breakdown by PO). Drilldown can be shown via: nested rows, a second table, or explicit PO column alongside SKU.\n\nScoring:\n- 1.0: Sheet present with SKU-level aggregation AND visible PO drilldown/breakdown AND three error metrics (price mismatch, case pack, total).\n- 0.7: Sheet present with SKU-level aggregation and three metrics, but PO drilldown is weak/implicit (e.g., separate PO column but not fully organized; or an additional table listing PO errors without aggregation).\n- 0.3: Sheet present but missing either SKU-level aggregation or one of the three metrics.\n- 0.0: No suitable summary/pivot sheet found.\n\nCheck presence and layout only; no need to verify numbers yet.", "expectation": "A summary/pivot that aggregates errors by SKU and allows drilldown to PO-level counts."}, {"type": "llm_judge", "name": "Brief Summary Document (Word/PDF)", "description": "Check that a separate brief summary document exists (DOCX or PDF) with essential sections.", "weight": 1.0, "judge_prompt": "Confirm there is a separate Word (DOCX) or PDF document. It should be professionally formatted and include the following structural elements (flexible section naming allowed):\n- Overview/Executive Summary (appears early; 1\u20132 short paragraphs)\n- Error Types Identified (mentions both price mismatch and case-pack errors)\n- SKUs/POs with higher frequency (high-level callouts or examples)\n- Recommendations/Next Steps (at least 2 concrete, actionable items)\n\nAlso check: minimum length ~3 paragraphs total. Headings or clear section breaks should be visible.\n\nScoring:\n- 1.0: Valid DOCX/PDF with all four elements and at least ~3 paragraphs.\n- 0.7: Valid DOCX/PDF with 3 of 4 elements.\n- 0.3: Valid DOCX/PDF with 2 of 4 elements or very sparse content.\n- 0.0: Not DOCX/PDF or missing/too short.\n\nOnly check presence/structure, not content quality.", "expectation": "A concise brief that covers error types, where issues concentrate, and actionable recommendations."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Correctness Verification (Code + LLM-friendly structure)", "description": "Deterministic checks of flags, totals, and aggregated counts based on the Stage 1 mandated structure.", "is_required": false, "max_points": 13.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Price Mismatch Flag Accuracy", "description": "Verify the per-line price mismatch flag corresponds to Entered Unit Price != Expected Unit Price within a small currency tolerance.", "weight": 4.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\nMAX_SCORE = 4.0\n\nPRICE_COL_HINTS_ENTERED = [\"entered unit price\", \"invoice unit price\", \"actual unit price\", \"entered price\", \"invoice price\", \"actual price\"]\nPRICE_COL_HINTS_EXPECTED = [\"expected unit price\", \"standard unit price\", \"list unit price\", \"expected price\", \"std price\", \"list price\"]\nFLAG_COL_HINTS = [\"price mismatch\", \"price mismatch error\", \"price error flag\", \"price error\", \"mismatch flag\"]\n\nBASE_QTY_HINTS = [\"ordered units\", \"order qty\", \"ordered qty\", \"quantity\", \"qty\"]\n\nTOL = 0.005  # ~half-cent tolerance\n\n\ndef evaluate(workflow, context):\n    # Find the Excel resource among outputs\n    excel_res = None\n    for r in context.get_all_outputs():\n        if getattr(r, 'is_spreadsheet', False):\n            excel_res = r\n            break\n    if excel_res is None:\n        return 0.0, \"No Excel output found.\"\n\n    # Load sheets, try to identify line-audit sheet by having needed columns\n    try:\n        xfile_path = context.files.get_path(excel_res.id)\n        xls = pd.ExcelFile(xfile_path)\n    except Exception as e:\n        return 0.0, f\"Failed to open Excel: {e}\"\n\n    def norm(s):\n        return re.sub(r\"\\s+\", \" \", str(s).strip().lower())\n\n    def find_col(cols, hints):\n        # returns first column name that matches any hints\n        ncols = [norm(c) for c in cols]\n        for i, nc in enumerate(ncols):\n            for h in hints:\n                if h in nc:\n                    return cols[i]\n        return None\n\n    line_df = None\n    chosen_sheet = None\n    for sheet in xls.sheet_names:\n        try:\n            df = pd.read_excel(xfile_path, sheet_name=sheet)\n            cols = list(df.columns)\n            if len(cols) == 0:\n                continue\n            # look for presence of entered price and expected price and a price mismatch flag\n            c_entered = find_col(cols, PRICE_COL_HINTS_ENTERED)\n            c_expected = find_col(cols, PRICE_COL_HINTS_EXPECTED)\n            c_flag = find_col(cols, FLAG_COL_HINTS)\n            if c_entered and c_expected and c_flag:\n                line_df = df\n                chosen_sheet = sheet\n                break\n        except Exception:\n            continue\n\n    if line_df is None:\n        return 0.0, \"Could not locate a line-level audit sheet with price columns and flag.\"\n\n    # Identify columns\n    cols = list(line_df.columns)\n    c_entered = find_col(cols, PRICE_COL_HINTS_ENTERED)\n    c_expected = find_col(cols, PRICE_COL_HINTS_EXPECTED)\n    c_flag = find_col(cols, FLAG_COL_HINTS)\n\n    if not (c_entered and c_expected and c_flag):\n        return 0.0, \"Required columns not found for price mismatch check.\"\n\n    df = line_df.copy()\n\n    def to_num(x):\n        if pd.isna(x):\n            return np.nan\n        if isinstance(x, str):\n            x = x.replace('$', '').replace(',', '').strip()\n        try:\n            return float(x)\n        except Exception:\n            return np.nan\n\n    entered = df[c_entered].apply(to_num)\n    expected = df[c_expected].apply(to_num)\n\n    # Compute our ground-truth mismatch: 1 if values differ by more than tolerance\n    comp = (~entered.isna()) & (~expected.isna())\n    gt_flag = pd.Series(0, index=df.index)\n    gt_flag[comp & ((entered - expected).abs() > TOL)] = 1\n\n    # Normalize candidate flag to 0/1\n    cand_raw = df[c_flag]\n    def to_flag(v):\n        if pd.isna(v):\n            return 0\n        if isinstance(v, str):\n            vs = v.strip().lower()\n            if vs in [\"1\", \"true\", \"yes\", \"y\"]:\n                return 1\n            if vs in [\"0\", \"false\", \"no\", \"n\", \"none\", \"\"]:\n                return 0\n        try:\n            return 1 if float(v) != 0 else 0\n        except Exception:\n            return 0\n    cand_flag = cand_raw.apply(to_flag)\n\n    # Evaluate accuracy only on rows where both prices are present\n    if comp.sum() == 0:\n        # no comparable rows; cannot verify\n        return MAX_SCORE * 0.25, \"No comparable price rows; partial credit.\"\n\n    correct = (cand_flag[comp].astype(int) == gt_flag[comp].astype(int)).sum()\n    acc = correct / comp.sum()\n    score = MAX_SCORE * acc\n\n    feedback = f\"Sheet '{chosen_sheet}': Price mismatch flag accuracy {acc:.1%} over {int(comp.sum())} comparable rows.\"\n    return max(0.0, min(MAX_SCORE, float(score))), feedback"}, {"type": "code", "name": "Case Pack Rule Accuracy", "description": "Verify case-pack violations are flagged when UOM=CASE and ordered units are not divisible by case pack. UOM=EA should not enforce multiples.", "weight": 4.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\nMAX_SCORE = 4.0\n\nUOM_HINTS = [\"uom\", \"unit order multiple\", \"order uom\"]\nCASEPACK_HINTS = [\"case pack\", \"casepack\", \"pack size\"]\nORDERED_HINTS = [\"ordered units\", \"order qty\", \"ordered qty\", \"quantity\", \"qty\"]\nFLAG_HINTS = [\"case pack error\", \"case-pack violation\", \"multiple violation\", \"case pack flag\"]\n\n\ndef evaluate(workflow, context):\n    excel_res = None\n    for r in context.get_all_outputs():\n        if getattr(r, 'is_spreadsheet', False):\n            excel_res = r\n            break\n    if excel_res is None:\n        return 0.0, \"No Excel output found.\"\n\n    try:\n        xfile_path = context.files.get_path(excel_res.id)\n        xls = pd.ExcelFile(xfile_path)\n    except Exception as e:\n        return 0.0, f\"Failed to open Excel: {e}\"\n\n    def norm(s):\n        return re.sub(r\"\\s+\", \" \", str(s).strip().lower())\n\n    def find_col(cols, hints):\n        ncols = [norm(c) for c in cols]\n        for i, nc in enumerate(ncols):\n            for h in hints:\n                if h in nc:\n                    return cols[i]\n        return None\n\n    line_df = None\n    chosen_sheet = None\n    for sheet in xls.sheet_names:\n        try:\n            df = pd.read_excel(xfile_path, sheet_name=sheet)\n            cols = list(df.columns)\n            c_uom = find_col(cols, UOM_HINTS)\n            c_cp = find_col(cols, CASEPACK_HINTS)\n            c_ord = find_col(cols, ORDERED_HINTS)\n            c_flag = find_col(cols, FLAG_HINTS)\n            if c_uom and c_cp and c_ord and c_flag:\n                line_df = df\n                chosen_sheet = sheet\n                break\n        except Exception:\n            continue\n\n    if line_df is None:\n        return 0.0, \"Could not locate a line-level audit sheet with UOM/Case Pack/Ordered/Flag.\"\n\n    cols = list(line_df.columns)\n    c_uom = find_col(cols, UOM_HINTS)\n    c_cp = find_col(cols, CASEPACK_HINTS)\n    c_ord = find_col(cols, ORDERED_HINTS)\n    c_flag = find_col(cols, FLAG_HINTS)\n\n    df = line_df.copy()\n\n    def to_num(x):\n        if pd.isna(x):\n            return np.nan\n        if isinstance(x, str):\n            x = x.replace(',', '').replace('$', '').strip()\n        try:\n            return float(x)\n        except Exception:\n            return np.nan\n\n    uom = df[c_uom].astype(str).str.strip().str.lower()\n    casepack = df[c_cp].apply(to_num)\n    ordered = df[c_ord].apply(to_num)\n\n    # Ground-truth: violation only when UOM indicates CASE and not divisible by casepack (valid positive integer)\n    is_case = uom.str.contains(\"case\")\n    valid = is_case & (~ordered.isna()) & (~casepack.isna()) & (casepack > 0)\n    gt_flag = pd.Series(0, index=df.index)\n    # Non-integer divisibility check with tolerance for float representations\n    div_ok = (ordered % casepack).abs() < 1e-6\n    gt_flag[valid & (~div_ok)] = 1\n\n    # Candidate flag normalization\n    def to_flag(v):\n        if pd.isna(v):\n            return 0\n        if isinstance(v, str):\n            vs = v.strip().lower()\n            if vs in [\"1\", \"true\", \"yes\", \"y\"]:\n                return 1\n            if vs in [\"0\", \"false\", \"no\", \"n\", \"none\", \"\"]:\n                return 0\n        try:\n            return 1 if float(v) != 0 else 0\n        except Exception:\n            return 0\n\n    cand_flag = df[c_flag].apply(to_flag)\n\n    # Evaluate only on rows where rule is applicable (valid)\n    if valid.sum() == 0:\n        # nothing to test; partial credit\n        return MAX_SCORE * 0.5, \"No applicable CASE rows to verify; partial credit.\"\n\n    correct = (cand_flag[valid].astype(int) == gt_flag[valid].astype(int)).sum()\n    acc = correct / valid.sum()\n    score = MAX_SCORE * acc\n    feedback = f\"Sheet '{chosen_sheet}': Case-pack flag accuracy {acc:.1%} over {int(valid.sum())} applicable CASE rows.\"\n    return max(0.0, min(MAX_SCORE, float(score))), feedback"}, {"type": "code", "name": "Total Errors Math Consistency", "description": "Verify Total Errors equals Price Mismatch flag + Case Pack flag per line.", "weight": 2.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\nMAX_SCORE = 2.0\n\nPM_HINTS = [\"price mismatch\", \"price mismatch error\", \"price error flag\", \"price error\", \"mismatch flag\"]\nCP_HINTS = [\"case pack error\", \"case-pack violation\", \"multiple violation\", \"case pack flag\"]\nTOT_HINTS = [\"total errors\", \"error count\", \"total error\"]\n\n\ndef evaluate(workflow, context):\n    excel_res = None\n    for r in context.get_all_outputs():\n        if getattr(r, 'is_spreadsheet', False):\n            excel_res = r\n            break\n    if excel_res is None:\n        return 0.0, \"No Excel output found.\"\n\n    try:\n        xfile_path = context.files.get_path(excel_res.id)\n        xls = pd.ExcelFile(xfile_path)\n    except Exception as e:\n        return 0.0, f\"Failed to open Excel: {e}\"\n\n    def norm(s):\n        return re.sub(r\"\\s+\", \" \", str(s).strip().lower())\n\n    def find_col(cols, hints):\n        ncols = [norm(c) for c in cols]\n        for i, nc in enumerate(ncols):\n            for h in hints:\n                if h in nc:\n                    return cols[i]\n        return None\n\n    line_df = None\n    for sheet in xls.sheet_names:\n        try:\n            df = pd.read_excel(xfile_path, sheet_name=sheet)\n            cols = list(df.columns)\n            c_pm = find_col(cols, PM_HINTS)\n            c_cp = find_col(cols, CP_HINTS)\n            c_tot = find_col(cols, TOT_HINTS)\n            if c_pm and c_cp and c_tot:\n                line_df = df\n                break\n        except Exception:\n            continue\n\n    if line_df is None:\n        return 0.0, \"Could not locate a line-level sheet with Price/Case/Total error columns.\"\n\n    def to_flag(v):\n        if pd.isna(v):\n            return 0\n        if isinstance(v, str):\n            vs = v.strip().lower()\n            if vs in [\"1\", \"true\", \"yes\", \"y\"]:\n                return 1\n            if vs in [\"0\", \"false\", \"no\", \"n\", \"none\", \"\"]:\n                return 0\n        try:\n            return 1 if float(v) != 0 else 0\n        except Exception:\n            return 0\n\n    cols = list(line_df.columns)\n    c_pm = next((c for c in cols if any(h in str(c).lower() for h in PM_HINTS)), None)\n    c_cp = next((c for c in cols if any(h in str(c).lower() for h in CP_HINTS)), None)\n    c_tot = next((c for c in cols if any(h in str(c).lower() for h in TOT_HINTS)), None)\n\n    pm = line_df[c_pm].apply(to_flag)\n    cp = line_df[c_cp].apply(to_flag)\n\n    # total may be 0,1,2 or sometimes boolean/strings\n    def to_int(v):\n        if pd.isna(v):\n            return 0\n        try:\n            return int(float(str(v).strip().replace(',', '')))\n        except Exception:\n            s = str(v).strip().lower()\n            if s in [\"true\", \"yes\", \"y\"]:\n                return 1\n            if s in [\"false\", \"no\", \"n\", \"none\", \"\"]:\n                return 0\n            return 0\n    tot = line_df[c_tot].apply(to_int)\n\n    expected = pm + cp\n    comp = (expected == tot)\n    if len(comp) == 0:\n        return MAX_SCORE * 0.25, \"No rows to compare; partial credit.\"\n\n    acc = comp.mean()\n    score = MAX_SCORE * acc\n    return max(0.0, min(MAX_SCORE, float(score))), f\"Total Errors matches PM+CP on {acc:.1%} of rows.\""}, {"type": "code", "name": "Summary Aggregation Consistency", "description": "Verify the summary sheet\u2019s SKU (and SKU+PO, if present) counts match line-level flags aggregated.", "weight": 3.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\nMAX_SCORE = 3.0\n\nSKU_HINTS = [\"sku\", \"item\", \"item number\", \"product id\"]\nPO_HINTS = [\"po\", \"po number\", \"purchase order\", \"order id\"]\nPM_HINTS = [\"price mismatch\", \"price mismatch error\", \"price error\"]\nCP_HINTS = [\"case pack\", \"case-pack\", \"multiple violation\"]\nTOT_HINTS = [\"total errors\", \"error count\", \"total error\"]\n\nSUMMARY_SHEET_HINTS = [\"summary\", \"pivot\", \"sku error\", \"error summary\", \"sku errors\"]\n\n\ndef evaluate(workflow, context):\n    excel_res = None\n    for r in context.get_all_outputs():\n        if getattr(r, 'is_spreadsheet', False):\n            excel_res = r\n            break\n    if excel_res is None:\n        return 0.0, \"No Excel output found.\"\n\n    try:\n        xfile_path = context.files.get_path(excel_res.id)\n        xls = pd.ExcelFile(xfile_path)\n    except Exception as e:\n        return 0.0, f\"Failed to open Excel: {e}\"\n\n    def norm(s):\n        return re.sub(r\"\\s+\", \" \", str(s).strip().lower())\n\n    def find_col(cols, hints):\n        ncols = [norm(c) for c in cols]\n        for i, nc in enumerate(ncols):\n            for h in hints:\n                if h in nc:\n                    return cols[i]\n        return None\n\n    # Identify line sheet with flags\n    line_df = None\n    for sheet in xls.sheet_names:\n        try:\n            df = pd.read_excel(xfile_path, sheet_name=sheet)\n            cols = list(df.columns)\n            c_sku = find_col(cols, SKU_HINTS)\n            c_po = find_col(cols, PO_HINTS)\n            c_pm = find_col(cols, PM_HINTS)\n            c_cp = find_col(cols, CP_HINTS)\n            c_tot = find_col(cols, TOT_HINTS)\n            if c_sku is not None and c_pm is not None and c_cp is not None and c_tot is not None:\n                line_df = df\n                break\n        except Exception:\n            continue\n\n    if line_df is None:\n        return 0.0, \"Could not locate a suitable line-level sheet with SKU and error flags.\"\n\n    # Normalize flags to integers\n    def to_flag(v):\n        if pd.isna(v):\n            return 0\n        if isinstance(v, str):\n            vs = v.strip().lower()\n            if vs in [\"1\", \"true\", \"yes\", \"y\"]:\n                return 1\n            if vs in [\"0\", \"false\", \"no\", \"n\", \"none\", \"\"]:\n                return 0\n        try:\n            return 1 if float(v) != 0 else 0\n        except Exception:\n            return 0\n\n    cols = list(line_df.columns)\n    c_sku = find_col(cols, SKU_HINTS)\n    c_po = find_col(cols, PO_HINTS)\n    c_pm = find_col(cols, PM_HINTS)\n    c_cp = find_col(cols, CP_HINTS)\n    c_tot = find_col(cols, TOT_HINTS)\n\n    lf = line_df.copy()\n    lf['_pm'] = lf[c_pm].apply(to_flag)\n    lf['_cp'] = lf[c_cp].apply(to_flag)\n    lf['_tot'] = lf[c_tot].apply(lambda v: int(float(str(v).replace(',',''))) if pd.notna(v) and str(v).strip() != '' else 0)\n\n    # Ground-truth aggregation by SKU\n    by_sku = lf.groupby(c_sku).agg(pm_gt=('_pm', 'sum'), cp_gt=('_cp', 'sum'), tot_gt=('_tot', 'sum'))\n\n    # Ground-truth by SKU+PO if PO exists\n    by_sku_po = None\n    if c_po is not None:\n        by_sku_po = lf.groupby([c_sku, c_po]).agg(pm_gt=('_pm', 'sum'), cp_gt=('_cp', 'sum'), tot_gt=('_tot', 'sum'))\n\n    # Find candidate summary sheet\n    summary_sheet = None\n    for sheet in xls.sheet_names:\n        ns = norm(sheet)\n        if any(h in ns for h in SUMMARY_SHEET_HINTS):\n            summary_sheet = sheet\n            break\n    if summary_sheet is None:\n        # fallback: pick a sheet that is not the line sheet but has fewer columns and looks like aggregation\n        candidate = None\n        for sheet in xls.sheet_names:\n            if sheet == summary_sheet:\n                continue\n            try:\n                df = pd.read_excel(xfile_path, sheet_name=sheet)\n                if df.shape[1] <= 8:\n                    candidate = sheet\n                    break\n            except Exception:\n                continue\n        summary_sheet = candidate\n\n    if summary_sheet is None:\n        return MAX_SCORE * 0.2, \"No identifiable summary sheet; minimal credit.\"\n\n    try:\n        sd = pd.read_excel(xfile_path, sheet_name=summary_sheet)\n    except Exception as e:\n        return MAX_SCORE * 0.2, f\"Failed to read summary sheet: {e}\"\n\n    # Flatten columns in case of pivot multi-index\n    sd.columns = [\" \".join([str(x) for x in col]).strip() if isinstance(col, tuple) else str(col) for col in sd.columns]\n\n    # Identify columns in summary\n    sc_sku = find_col(sd.columns, SKU_HINTS)\n    sc_po = find_col(sd.columns, PO_HINTS)\n    sc_pm = find_col(sd.columns, PM_HINTS)\n    sc_cp = find_col(sd.columns, CP_HINTS)\n    sc_tot = find_col(sd.columns, TOT_HINTS)\n\n    # Clean summary entries\n    def to_int(v):\n        try:\n            s = str(v).strip().replace(',', '')\n            if s == '' or s.lower() in ['nan', 'none']:\n                return 0\n            return int(float(s))\n        except Exception:\n            return 0\n\n    matches = 0\n    checks = 0\n\n    if sc_sku is not None and sc_pm is not None and sc_cp is not None and sc_tot is not None:\n        # Compare by SKU totals\n        tmp = sd[[sc_sku, sc_pm, sc_cp, sc_tot]].dropna(subset=[sc_sku]).copy()\n        tmp[sc_pm] = tmp[sc_pm].apply(to_int)\n        tmp[sc_cp] = tmp[sc_cp].apply(to_int)\n        tmp[sc_tot] = tmp[sc_tot].apply(to_int)\n\n        # Align on SKU keys present in both\n        common = set(by_sku.index.astype(str)).intersection(set(tmp[sc_sku].astype(str)))\n        if len(common) > 0:\n            # Build lookup\n            tmp_idxed = tmp.copy()\n            tmp_idxed['_key'] = tmp_idxed[sc_sku].astype(str)\n            tmp_idxed = tmp_idxed.set_index('_key')\n            for k in common:\n                gt = by_sku.loc[by_sku.index.astype(str) == k].iloc[0]\n                row = tmp_idxed.loc[k]\n                checks += 3\n                matches += int(gt['pm_gt'] == row[sc_pm])\n                matches += int(gt['cp_gt'] == row[sc_cp])\n                matches += int(gt['tot_gt'] == row[sc_tot])\n\n    # If PO breakdown exists in summary, also test SKU+PO combos\n    if sc_sku is not None and sc_po is not None and by_sku_po is not None and sc_pm is not None and sc_cp is not None and sc_tot is not None:\n        tmp = sd[[sc_sku, sc_po, sc_pm, sc_cp, sc_tot]].dropna(subset=[sc_sku, sc_po]).copy()\n        tmp[sc_pm] = tmp[sc_pm].apply(to_int)\n        tmp[sc_cp] = tmp[sc_cp].apply(to_int)\n        tmp[sc_tot] = tmp[sc_tot].apply(to_int)\n\n        # Build multi-index comparison\n        # Normalize types to strings for key matching\n        by_sku_po_idx = { (str(k[0]), str(k[1])): v for k, v in by_sku_po.itertuples(index=True) }\n        tmp['_k1'] = tmp[sc_sku].astype(str)\n        tmp['_k2'] = tmp[sc_po].astype(str)\n\n        for _, r in tmp.iterrows():\n            key = (r['_k1'], r['_k2'])\n            if key in by_sku_po_idx:\n                gt = by_sku_po.loc[(by_sku_po.index.get_level_values(0).astype(str)==key[0]) & (by_sku_po.index.get_level_values(1).astype(str)==key[1])].iloc[0]\n                checks += 3\n                matches += int(gt['pm_gt'] == r[sc_pm])\n                matches += int(gt['cp_gt'] == r[sc_cp])\n                matches += int(gt['tot_gt'] == r[sc_tot])\n\n    if checks == 0:\n        return MAX_SCORE * 0.3, \"Summary present but insufficient overlap to verify; minimal credit.\"\n\n    acc = matches / checks\n    score = MAX_SCORE * acc\n    return max(0.0, min(MAX_SCORE, float(score))), f\"Summary aggregation match rate {acc:.1%} across {checks} checks.\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Presentation Quality and Strategic Value", "description": "LLM evaluation of professionalism, clarity, and usefulness for management decision-making.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Excel Usability and Clarity", "description": "Assess whether the Excel workbook is easy to navigate and interpret for the Order Management/Finance audience.", "weight": 2.0, "judge_prompt": "Assess only presentation/usability of the Excel (do not re-verify structure):\n- Sheets are clearly named, with the main audit and SKU summary easy to find\n- Headings frozen, filters applied, or readable formatting aids present\n- Error columns are visually distinct or grouped logically; no cryptic abbreviations without labels\n- The summary sheet can be used quickly by a manager to see top SKUs/POs with issues\n\nScoring:\n- 2.0: Highly usable and professional; clear labeling and navigation aids\n- 1.2: Generally usable; minor clarity issues\n- 0.6: Barely usable; confusing layout/labels\n- 0.0: Disorganized or difficult to interpret", "expectation": "A manager can quickly find problem SKUs and understand per-line errors."}, {"type": "llm_judge", "name": "Brief: Insight and Actionability", "description": "Evaluate whether the DOCX/PDF brief concisely explains what was found and recommends credible next steps.", "weight": 2.0, "judge_prompt": "Evaluate the brief\u2019s quality (not just presence):\n- Clarity in describing the two error types and where they concentrate (SKUs/POs)\n- Highlights drivers or patterns (e.g., certain UOMs, ship-to locations, or case packs)\n- Provides at least 2 actionable recommendations prioritized for impact (e.g., system rule checks, price master review, retailer compliance guidance)\n- Tone and formatting appropriate for management\n\nScoring:\n- 2.0: Clear insights + prioritized, actionable next steps\n- 1.2: Some insights; recommendations present but generic\n- 0.6: Minimal insight; vague recommendations\n- 0.0: Unclear or not useful", "expectation": "Concise, management-ready summary with specific, impactful next steps."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "61b0946a-5c1c-4bf6-8607-84d7c7e0dfe0", "rubric": {"category_name": "Collaborative Cadaver Program Proposal - Mixed Analysis + Document", "rationale": "Pattern C (Mixed): A professional proposal document (PDF/DOCX) that embeds structured, verifiable cost analysis and a chart. Stage 1 uses an LLM gate to enforce a specific, self-documenting structure. Stage 2 applies code checks for numeric presence, assumptions, monotonicity heuristics, department-to-anatomical-area mapping, and references. Stage 3 evaluates overall quality and persuasiveness for hospital leadership stakeholders.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 - Structured Output Format Gate", "description": "LLM-only gate verifying the exact document structure that enables verification.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.5, "rules": [{"type": "llm_judge", "name": "Document Format and Structural Completeness", "description": "Output must be a properly structured proposal document with specific sections, tables, and a chart enabling verification.", "weight": 6.0, "judge_prompt": "You are evaluating whether the candidate output satisfies the REQUIRED structure for a mixed analysis + document deliverable. Only check PRESENCE and STRUCTURE, not correctness of calculations.\n\nAcceptable file types: PDF or DOCX only. The primary output must be a single professional proposal document. It must be at least 2 pages. File name should include the phrase \"Collaborative Cadaver Program Proposal\".\n\nRequired Sections and Elements (flexible with header wording, but content must be clearly present):\n1) Title and Overview (on first page):\n   - Title including \"Collaborative Cadaver Program Proposal\"\n   - Organization and department context (Hope Hospital, General Surgery)\n   - Date or version acceptable\n\n2) Executive Summary or Introduction:\n   - Overview of the program purpose and objectives (respectful full utilization, shared use across 4 departments, cost savings)\n\n3) Cost Savings Analysis (CORE):\n   - A clearly labeled chart or graph visualizing savings or per-department cost versus number of participating departments (1, 2, 3, 4). The chart must be visibly embedded (e.g., bar/line chart).\n   - A clearly visible table with columns similar to: [Departments Participating | Per-Department Annual Cadaver Cost (USD) | Total Hospital Annual Cadaver Cost (USD) | Savings vs Baseline (USD)]. Currency should be expressed with $.\n   - A short narrative explaining the cost methodology referencing that Anatomy Lab fee is included in cadaver costs and supplies/education are excluded. If the specific \"Cadaver Budget.xlsx\" source is unavailable, an explicit statement of assumptions is acceptable.\n\n4) Departmental Utilization Map (CORE):\n   - For each department (General Surgery, Thoracic Surgery, Otolaryngology, Orthopedic Surgery), a concise mapping of cadaver areas to be used (e.g., abdomen, thorax/chest, head and neck, limbs/spine). Can be a list and/or an illustrative image.\n\n5) Procedure Capacity by Complexity & Freeze/Thaw (CORE):\n   - A section that explains freeze/thaw cycles (10\u201312 cycles guideline), 3-hour usability post-thaw, and complexity definitions (simple, standard, complex).\n   - A summarized table or matrix estimating the approximate range of procedures per cadaver per department for participation levels (1\u20134) across complexity categories. It must explicitly state that the estimates do not account for mixing complexity.\n\n6) Appendix: Assumptions & Methodology:\n   - Notes on inclusion of Anatomy Lab fee and exclusion of supplies and education expenses\n   - Any core assumptions used in the cost model\n\n7) References:\n   - At least 2 sources (internal or external). URLs or formal citations acceptable.\n\nScoring (structure only):\n- 6.0: PDF/DOCX; \u22652 pages; all 7 elements present; cost chart and cost table both present; headers clear.\n- 5.0: PDF/DOCX and \u22652 pages; missing one minor supporting element (e.g., only one of Appendix or References missing) but all CORE parts present and chart + table present.\n- 3.5: PDF/DOCX and \u22652 pages; missing one CORE element (Cost Savings table or chart missing; or Departmental Utilization Map missing; or Procedure Capacity section missing), but other sections present.\n- 2.0: PDF/DOCX and \u22652 pages, but substantially incomplete (e.g., only intro/cost section without utilization and capacity sections).\n- 0.0: Not PDF/DOCX OR <2 pages OR missing multiple core sections.\n\nDo not assess calculation accuracy or writing quality here\u2014only the structural presence enabling verification.", "expectation": "A professional PDF/DOCX proposal titled \"Collaborative Cadaver Program Proposal\" with the specified sections, a cost table, and a chart."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 - Verification and Consistency Checks", "description": "Code-based checks leveraging the mandated structure to verify numeric presence, assumptions, monotonicity, mapping completeness, and references.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Cost Savings: Participation Labels and Currency Presence", "description": "Verify the Cost Savings section references all four participation levels (1\u20134) and includes currency amounts.", "weight": 2.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output.\"\n\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        text = \"\"\n    if not text:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = \"\"\n    if not text:\n        return 0.0, \"Unable to extract text from document.\"\n\n    t = text\n    tl = t.lower()\n\n    # Focus window around Cost Savings section if present\n    idx = tl.find('cost savings')\n    if idx == -1:\n        idx = tl.find('savings analysis')\n    start = max(0, idx - 300) if idx != -1 else 0\n    end = min(len(tl), (idx + 2000)) if idx != -1 else len(tl)\n    seg = tl[start:end]\n\n    patterns = [\n        r\"\\b1\\b\\s*(?:dept|department|departments|participant|participating|participation)?\",\n        r\"\\b2\\b\\s*(?:dept|department|departments|participant|participating|participation)?\",\n        r\"\\b3\\b\\s*(?:dept|department|departments|participant|participating|participation)?\",\n        r\"\\b4\\b\\s*(?:dept|department|departments|participant|participating|participation)?\",\n    ]\n    count_labels = sum(1 for p in patterns if re.search(p, seg, flags=re.IGNORECASE))\n\n    currency_re = re.compile(r\"\\$\\s?\\d{1,3}(?:,\\d{3})*(?:\\.\\d{2})?\", re.IGNORECASE)\n    currency_hits = currency_re.findall(seg)\n    n_currency = len(currency_hits)\n\n    score_labels = count_labels / 4.0\n    score_currency = min(1.0, n_currency / 4.0)\n    score = 0.6 * score_labels + 0.4 * score_currency\n\n    return max(0.0, min(1.0, score)), f\"Participation labels: {count_labels}/4; currency items in section: {n_currency}.\""}, {"type": "code", "name": "Cost Savings: Monotonicity Heuristic (Savings up or Per-Dept Cost down)", "description": "Heuristically check that as participating departments increase (1\u21924), savings are non-decreasing or per-department costs are non-increasing.", "weight": 3.0, "code": "import re\n\ndef _extract_text(context, output):\n    text = \"\"\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        text = \"\"\n    if not text:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = \"\"\n    return text or \"\"\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0, \"No document output.\"\n\n    text = _extract_text(context, output)\n    if not text:\n        return 0.0, \"Unable to extract text.\"\n\n    tl = text.lower()\n\n    def find_window(n):\n        # find the first occurrence of the label like 'n department(s)' or 'n participating'\n        pats = [\n            rf\"\\b{n}\\b\\s*(?:dept|department|departments)\",\n            rf\"\\b{n}\\b\\s*(?:participant|participants|participating|participation)\",\n            rf\"\\bparticipat(?:e|es|ing|ion)\\b[^\\n\\r]{0,40}\\b{n}\\b\",\n            rf\"\\b{n}\\b\\s*dept\\b\",\n        ]\n        for p in pats:\n            m = re.search(p, tl, flags=re.IGNORECASE)\n            if m:\n                s = max(0, m.start())\n                e = min(len(tl), m.end() + 200)\n                return tl[s:e]\n        return \"\"\n\n    money_re = re.compile(r\"\\$\\s?\\d{1,3}(?:,\\d{3})*(?:\\.\\d{2})?\")\n\n    def first_amount_near(win, keyword_opts):\n        # try pattern keyword -> amount or amount near keyword\n        for kw in keyword_opts:\n            # keyword before amount\n            m = re.search(kw + r\"[^$\\n\\r]{0,60}(\\$\\s?\\d{1,3}(?:,\\d{3})*(?:\\.\\d{2})?)\", win, flags=re.IGNORECASE)\n            if m:\n                return m.group(1)\n            # amount before keyword\n            m = re.search(r\"(\\$\\s?\\d{1,3}(?:,\\d{3})*(?:\\.\\d{2})?)\" + r\"[^\\n\\r]{0,60}\" + kw, win, flags=re.IGNORECASE)\n            if m:\n                return m.group(1)\n        # fallback: first amount in window\n        m = money_re.search(win)\n        if m:\n            return m.group(0)\n        return None\n\n    def to_number(s):\n        try:\n            return float(re.sub(r\"[^0-9.]\", \"\", s))\n        except Exception:\n            return None\n\n    savings = {}\n    per_dept = {}\n    for n in [1,2,3,4]:\n        win = find_window(n)\n        if not win:\n            continue\n        sav = first_amount_near(win, [r\"savings?\", r\"saved\", r\"vs baseline\", r\"reduction\"])  # try to get savings\n        cost = first_amount_near(win, [r\"per[- ]?department\", r\"per dept\", r\"per dept cost\", r\"per[- ]?cadaver cost\", r\"per[- ]?department annual cadaver cost\"])  # try per-department cost\n        if sav:\n            val = to_number(sav)\n            if val is not None:\n                savings[n] = val\n        if cost:\n            val = to_number(cost)\n            if val is not None:\n                per_dept[n] = val\n\n    def monotonic_non_decreasing(vals):\n        inv = 0\n        for i in range(1, len(vals)):\n            if vals[i] + 1e-9 < vals[i-1]:\n                inv += 1\n        return inv == 0, inv\n\n    def monotonic_non_increasing(vals):\n        inv = 0\n        for i in range(1, len(vals)):\n            if vals[i] - 1e-9 > vals[i-1]:\n                inv += 1\n        return inv == 0, inv\n\n    score = 0.0\n    detail = []\n\n    if len(savings) >= 3:\n        ordered = [savings.get(i) for i in [1,2,3,4] if i in savings]\n        ok, inv = monotonic_non_decreasing(ordered)\n        if ok:\n            score = 1.0\n            detail.append(f\"Savings monotonic non-decreasing across {len(ordered)} levels.\")\n        else:\n            # partial if only one inversion\n            score = 0.6 if inv == 1 else 0.3\n            detail.append(f\"Savings non-monotonic: inversions={inv} across {len(ordered)} levels.\")\n    elif len(per_dept) >= 3:\n        ordered = [per_dept.get(i) for i in [1,2,3,4] if i in per_dept]\n        ok, inv = monotonic_non_increasing(ordered)\n        if ok:\n            score = 1.0\n            detail.append(f\"Per-department cost monotonic non-increasing across {len(ordered)} levels.\")\n        else:\n            score = 0.6 if inv == 1 else 0.3\n            detail.append(f\"Per-department cost non-monotonic: inversions={inv} across {len(ordered)} levels.\")\n    else:\n        score = 0.0\n        detail.append(\"Insufficient labeled amounts near participation levels.\")\n\n    return max(0.0, min(1.0, score)), \"; \".join(detail)"}, {"type": "code", "name": "Assumptions: Lab Fee Included, Supplies/Education Excluded", "description": "Check for inclusion of Anatomy Lab fee and explicit exclusion of supplies/education in assumptions/methodology or narrative.", "weight": 2.0, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = ''\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        text = ''\n    if not text:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = ''\n    if not text:\n        return 0.0\n\n    tl = text.lower()\n\n    lab_included = False\n    # Look for explicit inclusion of anatomy lab fee\n    if 'lab fee' in tl or 'anatomy lab fee' in tl or ('anatomy lab' in tl and 'fee' in tl):\n        # also check verbs like include\n        if 'include' in tl or 'included' in tl or 'including' in tl:\n            lab_included = True\n        else:\n            # still count if clearly stated as part of cost\n            if 'cost' in tl and 'lab fee' in tl:\n                lab_included = True\n\n    # Exclusion of supplies and education\n    excl = False\n    # check proximity of exclude terms to supplies/education\n    for m in re.finditer(r\"exclud(?:e|es|ed|ing)\", tl):\n        s = max(0, m.start()-60); e = min(len(tl), m.end()+60)\n        window = tl[s:e]\n        if ('suppl' in window) and ('educat' in window):\n            excl = True\n            break\n    if not excl:\n        # alternative phrasing: 'not including supplies and education'\n        for m in re.finditer(r\"not includ(?:e|ed|ing)\", tl):\n            s = max(0, m.start()-60); e = min(len(tl), m.end()+60)\n            window = tl[s:e]\n            if ('suppl' in window) and ('educat' in window):\n                excl = True\n                break\n\n    score = 1.0 if (lab_included and excl) else (0.5 if (lab_included or excl) else 0.0)\n    feedback = f\"Lab fee included: {lab_included}; Supplies/Education excluded: {excl}.\"\n    return score, feedback"}, {"type": "code", "name": "Complexity Coverage and No-Mix Statement", "description": "Verify presence of complexity categories (simple, standard, complex) and explicit statement that estimates do not account for mixing complexity.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = ''\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        pass\n    if not text:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = ''\n    if not text:\n        return 0.0\n\n    tl = text.lower()\n    cats = {\n        'simple': 'simple' in tl,\n        'standard': 'standard' in tl,\n        'complex': 'complex' in tl\n    }\n\n    # no-mix statement\n    mix = False\n    # variants like: \"does not account for mixing complexity\", \"no mixing of complexity\"\n    if re.search(r\"does\\s+not\\s+account[^\\n\\r]{0,50}mix\", tl):\n        mix = True\n    elif re.search(r\"no\\s+mix(?:ing)?\\s+of\\s+complex\", tl):\n        mix = True\n\n    coverage = sum(1 for v in cats.values() if v) / 3.0\n    score = 0.6 * coverage + 0.4 * (1.0 if mix else 0.0)\n    return max(0.0, min(1.0, score)), f\"Complexity labels present: {cats}; No-mix noted: {mix}.\""}, {"type": "code", "name": "Department-to-Anatomical-Area Mapping", "description": "Check that each department is mapped to plausible cadaver areas near its mention (abdomen, thorax/chest, head and neck, limbs/spine).", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = ''\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        pass\n    if not text:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = ''\n    if not text:\n        return 0.0\n\n    tl = text.lower()\n\n    departments = {\n        'general surgery': [r'abdomen', r'abdominal'],\n        'thoracic surgery': [r'thorax', r'chest', r'thoracic'],\n        'otolaryngology': [r'head and neck', r'head', r'neck', r'larynx', r'pharynx'],\n        'orthopedic surgery': [r'limb', r'limbs', r'arm', r'leg', r'spine', r'extremit']\n    }\n\n    # Allow ENT variant\n    if 'ent' in tl:\n        tl += '\\notolaryngology'\n\n    covered = 0\n    details = []\n\n    for dept, area_kw in departments.items():\n        m = re.search(re.escape(dept), tl)\n        ok = False\n        if m:\n            s = max(0, m.start()-150); e = min(len(tl), m.end()+150)\n            win = tl[s:e]\n            for kw in area_kw:\n                if re.search(kw, win):\n                    ok = True\n                    break\n        else:\n            # try department name variants\n            if dept == 'otolaryngology' and ('ent' in tl or 'ear, nose' in tl):\n                # search near ENT keyword\n                m2 = re.search(r'\\bent\\b', tl)\n                if m2:\n                    s = max(0, m2.start()-150); e = min(len(tl), m2.end()+150)\n                    win = tl[s:e]\n                    for kw in area_kw:\n                        if re.search(kw, win):\n                            ok = True\n                            break\n        covered += 1 if ok else 0\n        details.append(f\"{dept}: {'mapped' if ok else 'not mapped'}\")\n\n    score = covered / 4.0\n    return score, \"; \".join(details)"}, {"type": "code", "name": "References Present (>=2 links/citations)", "description": "Heuristically check that at least two sources are cited (preferably URLs).", "weight": 0.5, "code": "import re\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n    text = ''\n    try:\n        text = context.files.read_docx_text(output.id)\n    except Exception:\n        pass\n    if not text:\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            text = ''\n    if not text:\n        return 0.0\n\n    # Count URLs as a proxy for references; also count bracketed citations like [1]\n    urls = re.findall(r'https?://\\S+', text)\n    bracket_cites = re.findall(r'\\[[0-9]{1,2}\\]', text)\n\n    n_sources = max(len(urls), len(set(bracket_cites)))\n    if len(urls) >= 2:\n        score = 1.0\n    elif len(urls) == 1 or n_sources >= 2:\n        score = 0.5\n    else:\n        score = 0.0\n\n    return score, f\"URLs: {len(urls)}; Bracketed cites: {len(bracket_cites)}\""}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 - Holistic Quality Assessment", "description": "LLM judges professional presentation, clarity, and persuasive value for hospital leadership.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professionalism and Persuasiveness", "description": "Assess tone, coherence, and how convincingly the proposal motivates adoption by surgery department leadership.", "weight": 2.0, "judge_prompt": "Evaluate the overall professionalism, clarity, and persuasiveness of the proposal for an audience of department chairs and residency program leaders.\n\nConsider:\n- Executive Summary/Introduction clarity and relevance\n- Logical flow from problem (waste) to solution (collaboration) to benefits (cost savings, respect for donors)\n- Appropriateness of tone and terminology for clinical education leadership\n- Clear articulation of objectives and stakeholder value\n\nScoring:\n- 2.0: Highly professional, compelling, logically structured, and audience-appropriate\n- 1.0: Generally professional with minor issues (clarity or flow)\n- 0.5: Mixed quality; notable clarity or tone issues\n- 0.0: Unprofessional or confusing\n", "expectation": "A polished, leadership-ready mini proposal that clearly motivates the Collaborative Cadaver Program."}, {"type": "llm_judge", "name": "Visual Communication and Chart Quality", "description": "Assess whether the chart is readable, correctly labeled, and effectively communicates the cost trend as participation increases.", "weight": 1.0, "judge_prompt": "Evaluate the chart/graph used in the Cost Savings Analysis.\n\nConsider:\n- Clear title, axis labels, and units (USD)\n- Correct depiction of trend vs. number of departments (1\u20134)\n- Legibility and appropriate scale\n\nScoring:\n- 1.0: Clear, labeled, legible chart accurately showing the trend\n- 0.5: Present but with minor labeling/legibility issues\n- 0.0: Missing or too unclear to interpret\n", "expectation": "A legible bar/line chart with labeled axes and amounts in USD for participation levels 1\u20134."}, {"type": "llm_judge", "name": "Actionability and Implementation Clarity", "description": "Assess whether a reader could proceed with next steps (e.g., scheduling, lab coordination) based on the proposal.", "weight": 1.0, "judge_prompt": "Evaluate how actionable the proposal is for immediate next steps.\n\nConsider:\n- Concrete suggestions on operational coordination (Anatomy Lab scheduling, freeze/thaw planning)\n- Clarity on how departments share cadavers and sequence usage\n- Risks/constraints acknowledged (e.g., 10\u201312 cycles, 3-hour window) and mitigations\n\nScoring:\n- 1.0: Clear, actionable next steps and constraints\n- 0.5: Some actionable guidance but incomplete\n- 0.0: Vague; lacks implementation clarity\n", "expectation": "A brief but clear set of next steps and considerations enabling pilot or rollout."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "f9a1c16c-53fd-4c8f-88cc-5c325ec2f0bb", "rubric": {"category_name": "A/V Touring Stage Plot (Audio and Video Technicians)", "rationale": "This rubric enforces a self-documenting, verifiable one-page PDF stage plot. Stage 1 (LLM-only) strictly checks the visual shape: PDF, one page landscape, required top-of-page Input/Output lists, on-stage diagram with icons, labels, and orientation. Stage 2 mixes code rules and an LLM rule to verify numbered inputs/outputs, wedges, IEM splits, and role-to-wedge/mic mapping using flexible text parsing, plus spatial logic. Stage 3 evaluates professional visual quality and readiness for venue advancing.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 Gate: Visual Stage Plot Structure", "description": "LLM-only check that the output is a one-page landscape PDF stage plot with explicit Input/Output lists, required icons, labels, and orientation. This is a strict shape gate.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Visual Stage Plot Structure Gate", "description": "Check that the candidate produced a one-page landscape PDF with the mandated structure and elements.", "weight": 4.0, "judge_prompt": "You are evaluating a stage plot file for a touring band. Only evaluate structure/format presence (not correctness of routing). The output MUST be a one-page landscape PDF visual stage plot with the following structural elements. Be flexible with exact wording, but the items must clearly exist.\n\nRequired document format and layout:\n- File format: PDF (preferred). If DOCX is provided but otherwise perfect, score modestly lower, but do not fail solely for DOCX if all other requirements are met.\n- Exactly one page.\n- Landscape orientation.\n- The visual plot shows the front of stage at the bottom of the page.\n- Stage orientation labels present: \u201cStage Left\u201d and \u201cStage Right\u201d (or SL/SR) and a clear \u201cFront of Stage\u201d/\u201cFOH\u201d indicator.\n\nTop-of-page lists (side-by-side):\n- Inputs list (left side) and Outputs list (right side), or clearly side-by-side at the top. Each list must be labeled (e.g., \u201cInputs\u201d, \u201cOutputs\u201d).\n- Inputs must be numbered as \u201cInput 1 - \u2026, Input 2 - \u2026\u201d etc.\n- Outputs must be numbered as \u201cOutput 1 - \u2026, Output 2 - \u2026\u201d etc.\n- Inputs should include at minimum: Vox1 vocal mic, Vox2 vocal mic, Drums vocal mic, Bass speech mic, Accordion DI, Acoustic Guitar DI. Wording can vary, but the function must be clear.\n- Outputs must include: monitor wedges and IEM XLR splits for Vox1 and Vox2. Wedges must be numbered counterclockwise from Stage Right (at minimum, a clearly numbered set of wedges 1\u20135, one for each title: Bass, Vox1, Vox2, Guitar, Drums). The two IEM splits for Vox1 and Vox2 must be clearly identified as outputs (e.g., \u201cIEM Split Vox1\u201d, similar).\n\nOn-stage diagram requirements (visual presence, not routing correctness):\n- Icons/graphics for: two vocal mics (Vox1 at Stage Right, Vox2 at Stage Left), one drum vocal mic, one bass speech mic, two DI boxes (Accordion and Acoustic Guitar), monitor wedges (5 total), IEM split boxes near Vox1 and Vox2 vocal mics, a 4-piece drum kit with hi-hat, at least two cymbals and a ride, amps behind Bass (Stage Right) and Guitar (Stage Left).\n- The two singers are centered downstage, flanked by Bass (SR side) and Guitar (SL side), with Vox1 on Stage Right and Vox2 on Stage Left.\n- Drummer upstage; drummer\u2019s wedge placed diagonally in front at approximately the 10 o\u2019clock position.\n- Each band member\u2019s mic and wedge are labeled with their title next to the respective item. Titles: Bass, Vox1, Vox2, Guitar, Drums.\n\nScoring (structure only):\n- 4.0: One-page landscape PDF with both side-by-side lists at top, clearly numbered Inputs and Outputs; diagram includes all required icons and labels; orientation and SL/SR/Front-of-Stage labels present; wedges numbered 1\u20135 with intended owner; IEM splits for Vox1/Vox2 listed in Outputs.\n- 3.0: Minor omissions (e.g., DOCX instead of PDF OR one missing icon detail OR lists not perfectly side-by-side but both clearly at the top). Core elements present: numbered Inputs and Outputs, all five wedges, IEM splits for Vox1/Vox2, labeled titles next to mics/wedges, orientation labels.\n- 2.0: Several structural gaps (e.g., missing one of the required Inputs or Outputs categories OR wedges not numbered OR missing IEM splits OR missing SL/SR/Front labels) but still a one-page stage plot with a diagram and some lists.\n- 1.0: Valid document but lacks most structural requirements (e.g., unnumbered lists, missing multiple required elements, unclear labeling, not landscape).\n- 0.0: Not a PDF/DOCX visual document, or no stage plot, or more than one page, or no top-of-page Input/Output lists.\n\nOnly check presence/format, not correctness of wiring or mixes. Return a score 0.0\u20134.0.", "expectation": "A one-page landscape PDF with top-of-page numbered Inputs/Outputs, full icon set (mics, wedges, IEM splits, DI boxes, amps, drum kit), orientation labels, correct on-stage layout, and titles next to each member\u2019s mic and wedge."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Verification (Correctness and Consistency)", "description": "Code and LLM checks that leverage the Stage 1 structure to verify enumerations, mappings, and spatial logic.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Inputs and Outputs Presence and Mapping", "description": "Verify via text extraction that required Inputs and Outputs exist, are numbered, and reference required items: 6 key inputs; wedges for 5 roles; IEM splits for Vox1 and Vox2.", "weight": 1.5, "code": "import re\nimport json\nimport pandas as pd\nimport numpy as np\n\ndef _read_document_text(context, output):\n    text = \"\"\n    try:\n        if hasattr(output, 'is_document') and output.is_document:\n            # Prefer PDF text\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    pass\n        if not text:\n            # Fallback for text-like resources\n            try:\n                text = context.files.read_text(output.id)\n            except Exception:\n                text = \"\"\n    except Exception:\n        text = \"\"\n    return text or \"\"\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output:\n        return 0.0, \"No output file provided.\"\n    if not getattr(output, 'is_document', False):\n        return 0.0, \"Primary output is not a document (PDF/DOCX).\"\n\n    raw = _read_document_text(context, output)\n    if not raw.strip():\n        # Some PDFs are image-only; give minimal credit only if Stage 1 LLM passed. Here we must return 0 on text checks.\n        return 0.0, \"No extractable text; cannot verify numbered Inputs/Outputs or labels.\"\n\n    text = raw.lower()\n    text = re.sub(r\"\\s+\", \" \", text)\n\n    # Basic presence of lists and numbering\n    has_inputs_label = 'input' in text\n    has_outputs_label = 'output' in text\n    numbered_inputs = bool(re.search(r\"input\\s*\\d+\", text))\n    numbered_outputs = bool(re.search(r\"output\\s*\\d+\", text))\n\n    # Expected inputs (6 minimal)\n    # Vox1 vocal mic, Vox2 vocal mic, Drums vocal mic, Bass speech mic, Accordion DI, Acoustic Guitar DI\n    checks = {}\n    def contains_all(*needles):\n        return all(n in text for n in needles)\n\n    # Support common variants\n    checks['vox1_vocal'] = (('vox1' in text or 'vox 1' in text) and ('vocal' in text or 'mic' in text))\n    checks['vox2_vocal'] = (('vox2' in text or 'vox 2' in text) and ('vocal' in text or 'mic' in text))\n    checks['drums_vocal'] = (('drum' in text or 'drums' in text) and ('vocal' in text or 'vox' in text or 'mic' in text))\n    # Bass speech mic\n    bass_present = 'bass' in text\n    speech_terms = any(t in text for t in ['speech mic', 'talkback', 'talk mic', 'speech'])\n    checks['bass_speech_mic'] = bass_present and (speech_terms or re.search(r\"bass[^\\n]*mic\", text) is not None)\n    # Accordion DI\n    checks['accordion_di'] = ('accordion' in text and 'di' in text)\n    # Acoustic Guitar DI\n    checks['acoustic_di'] = ((('acoustic' in text) or ('ac gtr' in text) or ('ac. gtr' in text)) and 'di' in text)\n\n    # Outputs: wedges for 5 roles, IEM splits for Vox1 & Vox2\n    # Count wedges (look for 'wedge' or 'monitor') and numbers\n    wedge_mentions = re.findall(r\"(wedge|monitor)\\s*(\\d+)?\", text)\n    unique_wedge_numbers = set()\n    for m in re.finditer(r\"(?:wedge|monitor)\\s*(\\d+)\", text):\n        try:\n            unique_wedge_numbers.add(int(m.group(1)))\n        except Exception:\n            pass\n\n    # For each role, try to find they have a wedge/monitor assignment in same line\n    role_to_regex = {\n        'bass': r\"(wedge|monitor)[^\\n]{0,60}bass|bass[^\\n]{0,60}(wedge|monitor)\",\n        'vox1': r\"(wedge|monitor)[^\\n]{0,60}vox\\s*1|vox\\s*1[^\\n]{0,60}(wedge|monitor)|vox1\",\n        'vox2': r\"(wedge|monitor)[^\\n]{0,60}vox\\s*2|vox\\s*2[^\\n]{0,60}(wedge|monitor)|vox2\",\n        'guitar': r\"(wedge|monitor)[^\\n]{0,60}guitar|guitar[^\\n]{0,60}(wedge|monitor)\",\n        'drums': r\"(wedge|monitor)[^\\n]{0,60}drum|drum[^\\n]{0,60}(wedge|monitor)\"\n    }\n    role_wedge_hits = {role: bool(re.search(pattern, text)) for role, pattern in role_to_regex.items()}\n\n    # IEM splits for Vox1 and Vox2 in outputs\n    iem_terms = ('iem' in text or 'in-ear' in text)\n    vox1_iem = iem_terms and (('vox1' in text or 'vox 1' in text) and ('split' in text or 'xlr' in text or 'send' in text))\n    vox2_iem = iem_terms and (('vox2' in text or 'vox 2' in text) and ('split' in text or 'xlr' in text or 'send' in text))\n\n    # Scoring\n    score = 0.0\n    feedback = []\n\n    # Subscore A: Lists and numbering (max 0.3)\n    sub_a = 0.0\n    if has_inputs_label: sub_a += 0.1\n    if has_outputs_label: sub_a += 0.1\n    if numbered_inputs: sub_a += 0.05\n    if numbered_outputs: sub_a += 0.05\n\n    # Subscore B: Required Inputs present (6 checks, max 0.7)\n    sub_b = sum(1 for v in checks.values() if v) / max(1, len(checks)) * 0.7\n\n    # Subscore C: Outputs: wedges and IEM splits (max 0.5)\n    sub_c = 0.0\n    # At least 5 wedges numbered\n    if len(unique_wedge_numbers) >= 5:\n        sub_c += 0.2\n    # Each role has wedge mention\n    sub_c += (sum(1 for v in role_wedge_hits.values() if v) / 5.0) * 0.2\n    # IEM splits for both vox\n    if vox1_iem: sub_c += 0.05\n    if vox2_iem: sub_c += 0.05\n\n    raw_total = sub_a + sub_b + sub_c  # max theoretical 1.5\n    # Cap at rule weight\n    score = min(raw_total, 1.5)\n\n    # Feedback\n    missing_inputs = [k for k, v in checks.items() if not v]\n    missing_roles = [r for r, v in role_wedge_hits.items() if not v]\n    if not numbered_inputs: feedback.append(\"Inputs not clearly numbered.\")\n    if not numbered_outputs: feedback.append(\"Outputs not clearly numbered.\")\n    if len(unique_wedge_numbers) < 5: feedback.append(\"Fewer than 5 numbered wedges detected.\")\n    if missing_inputs: feedback.append(\"Missing inputs detected: \" + \", \".join(missing_inputs))\n    if missing_roles: feedback.append(\"No wedge assignment text found for: \" + \", \".join(missing_roles))\n    if not vox1_iem or not vox2_iem: feedback.append(\"IEM split outputs for Vox1/Vox2 not clearly found.\")\n\n    return score, \"; \".join(feedback) if feedback else \"OK\""}, {"type": "code", "name": "Titles, Mics, and Wedge Labeling", "description": "Verify presence of all five titles; that mics are identified for Vox1, Vox2, Drums, and Bass (speech); and wedge mention for all five roles.", "weight": 1.0, "code": "import re\nimport json\nimport pandas as pd\nimport numpy as np\n\ndef _read_document_text(context, output):\n    try:\n        try:\n            return context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                return context.files.read_docx_text(output.id)\n            except Exception:\n                return context.files.read_text(output.id)\n    except Exception:\n        return \"\"\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not getattr(output, 'is_document', False):\n        return 0.0, \"No document to evaluate.\"\n\n    text = _read_document_text(context, output).lower()\n    text = re.sub(r\"\\s+\", \" \", text)\n\n    titles = {\n        'bass': ['bass'],\n        'vox1': ['vox1','vox 1'],\n        'vox2': ['vox2','vox 2'],\n        'guitar': ['guitar','gtr'],\n        'drums': ['drum','drums']\n    }\n\n    # Title presence\n    title_presence = {}\n    for role, variants in titles.items():\n        title_presence[role] = any(v in text for v in variants)\n\n    # Mic presence requirements: vox1, vox2, drums, and bass speech mic\n    def has_term_near(term1s, term2s, window=60):\n        for m1 in term1s:\n            for m in re.finditer(re.escape(m1), text):\n                start = max(0, m.start()-window)\n                end = min(len(text), m.end()+window)\n                if any(t2 in text[start:end] for t2 in term2s):\n                    return True\n        return False\n\n    mic_requirements = {\n        'vox1_mic': has_term_near(titles['vox1'], ['mic','vocal','vox']),\n        'vox2_mic': has_term_near(titles['vox2'], ['mic','vocal','vox']),\n        'drums_mic': has_term_near(titles['drums'], ['mic','vocal','vox']),\n        'bass_speech_mic': has_term_near(titles['bass'], ['speech','talkback','talk mic','mic'])\n    }\n\n    # Wedge mentions for all five roles\n    wedge_mentions = {}\n    for role, variants in titles.items():\n        wedge_mentions[role] = has_term_near(variants, ['wedge','monitor'])\n\n    # Scoring: titles (0.3), mics (0.4), wedges (0.3) => total 1.0\n    sub_titles = (sum(1 for v in title_presence.values() if v) / 5.0) * 0.3\n    sub_mics = (sum(1 for v in mic_requirements.values() if v) / 4.0) * 0.4\n    sub_wedges = (sum(1 for v in wedge_mentions.values() if v) / 5.0) * 0.3\n\n    score = sub_titles + sub_mics + sub_wedges\n    score = min(score, 1.0)\n\n    missing_titles = [r for r,v in title_presence.items() if not v]\n    missing_mics = [k for k,v in mic_requirements.items() if not v]\n    missing_wedges = [r for r,v in wedge_mentions.items() if not v]\n\n    feedback = []\n    if missing_titles:\n        feedback.append(\"Missing titles: \" + \", \".join(missing_titles))\n    if missing_mics:\n        feedback.append(\"Missing mic labeling near: \" + \", \".join(missing_mics))\n    if missing_wedges:\n        feedback.append(\"Missing wedge/monitor labeling near: \" + \", \".join(missing_wedges))\n\n    return score, \"; \".join(feedback) if feedback else \"OK\""}, {"type": "code", "name": "Orientation and Numbering Sanity Checks", "description": "Verify Stage Left/Stage Right labels, Front of Stage indication, and wedge numbering spans at least 1\u20135.", "weight": 0.5, "code": "import re\nimport json\nimport pandas as pd\nimport numpy as np\n\ndef _read_document_text(context, output):\n    try:\n        try:\n            return context.files.read_pdf_text(output.id)\n        except Exception:\n            return context.files.read_docx_text(output.id)\n    except Exception:\n        return \"\"\n\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not getattr(output, 'is_document', False):\n        return 0.0, \"No document to evaluate.\"\n\n    text = _read_document_text(context, output).lower()\n    text = re.sub(r\"\\s+\", \" \", text)\n\n    has_sl = ('stage left' in text) or re.search(r\"\\bsl\\b\", text) is not None\n    has_sr = ('stage right' in text) or re.search(r\"\\bsr\\b\", text) is not None\n    has_front = ('front of stage' in text) or ('foh' in text) or ('downstage' in text)\n\n    # Find wedge numbers\n    wedge_nums = set()\n    for m in re.finditer(r\"(?:wedge|monitor)\\s*(\\d+)\", text):\n        try:\n            wedge_nums.add(int(m.group(1)))\n        except Exception:\n            pass\n\n    span_1_5 = all(n in wedge_nums for n in [1,2,3,4,5])\n\n    # Scoring: labels (0.3) + numbering (0.2) = 0.5\n    score = 0.0\n    score += (0.1 if has_sl else 0.0)\n    score += (0.1 if has_sr else 0.0)\n    score += (0.1 if has_front else 0.0)\n    score += (0.2 if span_1_5 else 0.0)\n\n    feedback = []\n    if not has_sl: feedback.append(\"Missing Stage Left label.\")\n    if not has_sr: feedback.append(\"Missing Stage Right label.\")\n    if not has_front: feedback.append(\"Missing Front-of-Stage/FOH indicator.\")\n    if not span_1_5: feedback.append(\"Wedges not clearly numbered 1\u20135.\")\n\n    return min(score, 0.5), \"; \".join(feedback) if feedback else \"OK\""}, {"type": "llm_judge", "name": "Spatial Logic Cross-Check", "description": "LLM verifies reasonable placement: Vox1 on SR, Vox2 on SL; amps behind Bass (SR) and Guitar (SL); drummer wedge roughly 10 o\u2019clock; front-of-stage at bottom; IEM splits placed at or near the vocal mics; five wedges present and sensibly assigned.", "weight": 1.0, "judge_prompt": "Using the rendered document, check spatial/logical correctness (not aesthetic quality):\n- Vox1 is on Stage Right and Vox2 on Stage Left, centered downstage; Bass flanks Vox1 on SR side; Guitar flanks Vox2 on SL side.\n- Bass amp behind Bass on SR; Guitar amp behind Guitar on SL.\n- Drummer is upstage; wedge placed diagonally front-left (approx. 10 o\u2019clock relative to the drummer).\n- Front of stage is visually indicated at the bottom; SL/SR labels present.\n- Two IEM split boxes/labels located at or near the two vocal mics (Vox1, Vox2).\n- Exactly five wedges are depicted and labeled/assigned to Bass, Vox1, Vox2, Guitar, and Drums.\n\nScoring:\n- 1.0: All listed spatial items appear correct and unambiguous.\n- 0.7: One minor discrepancy (e.g., drummer wedge angle unclear but present).\n- 0.4: Two to three discrepancies OR unclear assignments.\n- 0.1: Major spatial confusion (e.g., amps in wrong places, singers reversed).\n- 0.0: No discernible spatial logic or missing key placements.\n\nReturn a score 0.0\u20131.0 and brief justification.", "expectation": "Spatial arrangement matches brief: Vox1 SR, Vox2 SL; amps behind respective players; drummer wedge approx. 10 o\u2019clock; 5 wedges present; front-of-stage at bottom; IEM splits near vocal mics."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3: Presentation Quality", "description": "LLM assessment of professional clarity and readiness for advancing to venues.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Quality and Clarity", "description": "Assess whether the plot is visually clean, readable, and professionally suitable to send to venues.", "weight": 2.0, "judge_prompt": "Assess presentation quality (not structure correctness already checked):\n- Is the stage plot clean, legible, and professional (fonts, spacing, contrast)?\n- Are icons consistent and clear (mics, wedges, DI boxes, IEM splits, amps, drums)?\n- Are titles and labels adjacent to their items and readable at normal zoom? Are the top lists aligned and easy to scan?\n- Is the whole plot usable by venue A1/monitor engineer without ambiguity (all essential labels present, no clutter, sensible scale, one-page)?\n\nScoring:\n- 2.0: Polished, professional, crisp and highly readable; venue-ready without edits.\n- 1.4: Generally professional with minor clutter or small readability issues.\n- 0.8: Adequate but needs some cleanup or better labeling/contrast.\n- 0.3: Messy or hard to read; would likely confuse a venue.\n- 0.0: Unusable presentation.\n\nReturn a score 0.0\u20132.0 and a brief rationale.", "expectation": "A professional, single-page, venue-ready stage plot with clear labeling, consistent iconography, and excellent readability."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "b39a5aa7-cd1b-47ad-b249-90afd22f8f21", "rubric": {"category_name": "CBA Compensation Modeling (Finance/Insurance \u2014 Financial Managers)", "rationale": "Analytical, Excel-based financial modeling task. The rubric enforces a strict, audit-friendly workbook shape first (LLM gate), then verifies correctness and internal consistency with code checks made feasible by the mandated shape, and finally assesses professional quality and usability for a finance executive audience.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate", "description": "LLM-only gate to enforce the exact, verifiable Excel structure so subsequent checks are trivial. If this gate fails, the entire category scores 0.", "is_required": true, "max_points": 8.0, "min_score_to_pass": 6.0, "rules": [{"type": "llm_judge", "name": "Workbook Structure and Required Sections", "description": "Check that the output is a single Excel workbook with the exact structural requirements enabling verification: specific sheets, clearly labeled sections/tables, summary/projection tables by quarter, YoY growth, and separate calculation tabs.", "weight": 8.0, "judge_prompt": "You are verifying ONLY the presence and structure of an Excel workbook (not correctness of calculations). Be flexible about exact wording, but strict about the required elements. If multiple sheets/tables satisfy a requirement with similar names, accept them.\n\nCheck the output is an Excel workbook (.xlsx or .xls). Then verify the following required structure:\n\nRequired Sheets and Sections (names may vary slightly as indicated):\n1) Inputs/Drivers sheet (name like \"Inputs\", \"Drivers\", or similar)\n   - Contains clearly labeled input fields suitable for scenario analysis.\n   - Includes a visible \"Global Parameters\" area with at least these fields:\n     \u2022 Current Year (e.g., 2025)\n     \u2022 Projection Horizon: 2 years beyond current year (explicit or implied)\n     \u2022 Escalation/Inflation or similar growth drivers for Year+1 and Year+2\n     \u2022 Benefits/Employer Taxes or similar % load\n   - Includes a \"Compensation Drivers\" table with columns similar to: [Driver | Value | Unit/Notes]\n     \u2022 At least 6 numeric driver rows reflecting CBA elements (e.g., Rehearsal Rate, Performance Rate, Overtime Premium, Principal Premium, Doubling, Cartage/Per Diem, Minimum Call Hours, Dues %, etc.). Accept close equivalents.\n\n2) Assumptions sheet (name like \"Assumptions\")\n   - Contains a clearly labeled list or table of Compensation Types with related rules/rates/notes. A structure like [Compensation Type | Rule/Rate | Notes] is expected. At least 5 types.\n\n3) Roster sheet (name like \"Roster\", \"Headcount\", or similar)\n   - Tabular roster with columns similar to: [Employee ID | Name | Instrument/Role | Base Rate | Status/FTE | Notes]. At least 5 rows expected.\n\n4) Summary \u2014 Current Year sheet (name like \"Summary - Current Year\", \"CY Summary\", \"Summary (CY)\")\n   - Shows compensation expense by compensation type and by quarter (Q1\u2013Q4) for the current year.\n   - Table structure similar to: [Compensation Type | Q1 | Q2 | Q3 | Q4 | Year Total]. A Total row at the bottom is expected.\n\n5) Projections sheet (name like \"Projections\", \"Forecast\")\n   - Shows results by quarter for the next two years (8 quarters) with YoY (year-over-year) growth rate visible.\n   - Accept either:\n     \u2022 Wide: Columns for quarters like Q1 2026, Q2 2026, \u2026 Q4 2027, and a visible YoY growth metric (column or clearly labeled row/section), or\n     \u2022 Long: A [Quarter | Compensation Type | Amount | YoY Growth %] style table.\n   - YoY Growth for each quarter must be visible (column named like \"YoY\", \"Y/Y Growth\", or equivalent).\n\n6) Calculations sheet(s) (names like \"Calculations\", or prefix \"Calc - \u2026\")\n   - One or more tabs showing detailed computations linking Roster, Inputs/Drivers, and Assumptions to outputs.\n   - Should include line-level columns similar to: [Employee ID | Quarter | Compensation Type | Units | Rate | Amount] and visible subtotals/aggregations by quarter/type.\n   - Must be separate from the Summary and Inputs sheets.\n\nFormatting Expectations:\n- Tables should be clearly labeled with headers.\n- Quarter labels should be recognizable (e.g., Q1, Q2, or Q1 YYYY, etc.).\n- The structure should be readable without hidden/obscure content.\n\nScoring (8 points total):\n- 8.0: All required sheets present with appropriate sections, tables, and visible YoY growth in projections.\n- 7.0: One minor omission (e.g., YoY growth visible but not labeled consistently, or Inputs missing one of the specified global parameters) but overall structure supports verification.\n- 6.0: Missing one required sheet or a major section within a sheet, but enough structure remains to attempt verification.\n- 3.0: Multiple required elements missing; structure insufficient for reliable verification.\n- 0.0: Not an Excel workbook OR structure grossly non-compliant (e.g., missing most required sheets).\n\nOnly judge presence/structure/layout. Do NOT judge numerical correctness or formatting quality beyond clear section/table presence.", "expectation": "A single Excel workbook having Inputs/Drivers, Assumptions, Roster, Summary (current year by quarter and by type), Projections (next 8 quarters with YoY), and separate Calculations tab(s) with detailed, traceable computations."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Consistency Checks", "description": "Now that structure is enforced, verify internal consistency, bounds, and basic computational reasonableness using code rules plus one LLM rule for traceability.", "is_required": false, "max_points": 7.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Inputs Completeness and Drivers Availability", "description": "Verify Inputs/Drivers sheet exists with at least 6 numeric driver fields, plus presence of a current year and key CBA-relevant driver keywords (e.g., benefits, dues, escalation).", "weight": 1.2, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheets = xls.sheet_names\n        inputs_sheet = None\n        for s in sheets:\n            if re.search(r\"input|driver\", s, re.I):\n                inputs_sheet = s\n                break\n        if not inputs_sheet:\n            return 0.0, \"Inputs/Drivers sheet not found by name.\"\n        df = pd.read_excel(path, sheet_name=inputs_sheet, header=0)\n        # Coerce to numeric to count numeric driver cells\n        df_num = df.apply(pd.to_numeric, errors='coerce')\n        numeric_cells = int(np.isfinite(df_num.values).sum())\n        # Look for textual cues in the sheet to confirm presence of key fields\n        df_str = df.astype(str).applymap(lambda x: x.strip().lower())\n        text_blob = ' '.join(df_str.fillna('').values.ravel().tolist())\n        has_current_year = bool(re.search(r\"current\\s*year|cy\\s*:?\\s*20\\d{2}|20\\d{2}\\s*current\", text_blob)) or bool(re.search(r\"\\b20\\d{2}\\b\", text_blob))\n        has_benefits_or_dues = any(k in text_blob for k in [\"benefit\", \"benefits\", \"employer tax\", \"fica\", \"dues\", \"union\"])\n        has_escalation = any(k in text_blob for k in [\"escalation\", \"inflation\", \"cola\", \"growth\"])\n        score = 0.0\n        # At least 6 numeric driver fields\n        if numeric_cells >= 6:\n            score += 0.6\n        elif numeric_cells >= 4:\n            score += 0.3\n        # Current year identified\n        if has_current_year:\n            score += 0.3\n        # Presence of relevant driver types\n        if has_benefits_or_dues:\n            score += 0.15\n        if has_escalation:\n            score += 0.15\n        score = min(score, 1.0)\n        feedback = f\"Inputs numeric_cells={numeric_cells}, current_year={has_current_year}, benefits/dues={has_benefits_or_dues}, escalation={has_escalation}\"\n        return score, feedback\n    except Exception as e:\n        return 0.0, f\"Error reading Inputs: {e}\""}, {"type": "code", "name": "Current-Year Summary Structure and Non-Negativity", "description": "Confirm Summary (current year) has Q1\u2013Q4 columns and totals are non-negative.", "weight": 1.2, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheets = xls.sheet_names\n        summary_sheet = None\n        for s in sheets:\n            if re.search(r\"summary\", s, re.I) and re.search(r\"current|cy|year\", s, re.I):\n                summary_sheet = s\n                break\n        if not summary_sheet:\n            # fallback to first sheet containing 'summary'\n            for s in sheets:\n                if re.search(r\"summary\", s, re.I):\n                    summary_sheet = s\n                    break\n        if not summary_sheet:\n            return 0.0, \"Summary (current year) sheet not found.\"\n        df = pd.read_excel(path, sheet_name=summary_sheet)\n        df.columns = [str(c).strip() for c in df.columns]\n        cols = df.columns.tolist()\n        quarter_cols = [c for c in cols if re.search(r\"^\\s*q[1-4](\\s*20\\d{2})?\\s*$\", str(c).lower()) or re.search(r\"^\\s*q[1-4]\\s*[- ]\\s*20\\d{2}\\s*$\", str(c).lower())]\n        # If quarters embedded as labels in rows, try scanning header row 0\n        score = 0.0\n        if len(set([re.sub(r\"\\D\", \"\", c.lower())[:2] for c in quarter_cols])) >= 4:\n            score += 0.6  # Has Q1..Q4\n        elif any(re.match(r\"^q[1-4]$\", str(c).lower()) for c in cols):\n            score += 0.3\n        # Non-negativity across detected quarter cols (if any), else across all numeric cells\n        df_num = df.apply(pd.to_numeric, errors='coerce')\n        if quarter_cols:\n            vals = df_num[quarter_cols].values.flatten()\n        else:\n            vals = df_num.values.flatten()\n        vals = vals[np.isfinite(vals)]\n        if vals.size == 0:\n            nn = False\n        else:\n            nn = np.nanmin(vals) >= -1e-6\n        if nn:\n            score += 0.4\n        # Bonus if a Year Total column exists\n        has_year_total = any(re.search(r\"year\\s*total|annual\\s*total|total\\s*\\(year\\)\", str(c).lower()) for c in cols)\n        if has_year_total:\n            score += 0.2\n        score = min(score, 1.0)\n        feedback = f\"quarters_detected={len(set([c.lower()[:2] for c in quarter_cols]))}, non_negative={nn}, year_total={has_year_total}\"\n        return score, feedback\n    except Exception as e:\n        return 0.0, f\"Error reading Summary: {e}\""}, {"type": "code", "name": "Projections Coverage and YoY Plausibility", "description": "Verify projections span 8 future quarters and include YoY growth visible; where possible, compare provided YoY to computed YoY from Summary/Projections.", "weight": 2.6, "code": "import re\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\n\ndef _parse_quarter_label(label):\n    # Returns (year:int or None, q:int or None)\n    s = str(label).strip()\n    m = re.search(r\"q\\s*([1-4])\\s*(?:[-/ ]\\s*)?(20\\d{2})?\", s, re.I)\n    if not m:\n        return (None, None)\n    q = int(m.group(1))\n    yr = int(m.group(2)) if m.group(2) else None\n    return (yr, q)\n\ndef _extract_quarter_totals_wide(df):\n    qcols = []\n    for c in df.columns:\n        yr, q = _parse_quarter_label(c)\n        if q is not None:\n            qcols.append(c)\n    totals = {}\n    if not qcols:\n        return totals\n    df_num = df.apply(pd.to_numeric, errors='coerce')\n    for c in qcols:\n        yr, q = _parse_quarter_label(c)\n        val = pd.to_numeric(df_num[c], errors='coerce').replace([np.inf, -np.inf], np.nan).sum(skipna=True)\n        totals[(yr, q, str(c))] = float(val) if np.isfinite(val) else 0.0\n    return totals\n\ndef _extract_quarter_totals_long(df):\n    cols = [c.lower().strip() for c in df.columns]\n    if not any('quarter' in c for c in cols):\n        return {}\n    # Identify Quarter and Amount columns heuristically\n    qcol = [c for c in df.columns if 'quarter' in str(c).lower()]\n    if not qcol:\n        return {}\n    qcol = qcol[0]\n    # Amount column preference order\n    amt_candidates = [c for c in df.columns if re.search(r\"amount|total|value|cost|expense\", str(c), re.I)]\n    if not amt_candidates:\n        # fallback: any numeric column\n        num_df = df.apply(pd.to_numeric, errors='coerce')\n        num_cols = [c for c in df.columns if num_df[c].notna().any()]\n        if not num_cols:\n            return {}\n        amt_col = num_cols[-1]\n    else:\n        amt_col = amt_candidates[0]\n    totals = defaultdict(float)\n    for _, row in df.iterrows():\n        yr, q = _parse_quarter_label(row[qcol])\n        if q is None:\n            continue\n        try:\n            v = float(pd.to_numeric(row[amt_col], errors='coerce'))\n        except Exception:\n            v = np.nan\n        if np.isfinite(v):\n            totals[(yr, q, None)] += v\n    return dict(totals)\n\ndef _to_percent(v):\n    if v is None or not np.isfinite(v):\n        return np.nan\n    v = float(v)\n    # If looks like 5 (i.e., > 1.5), interpret as percent and divide by 100\n    if abs(v) > 1.5:\n        return v / 100.0\n    return v\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheets = xls.sheet_names\n        proj_sheet = None\n        for s in sheets:\n            if re.search(r\"project|forecast\", s, re.I):\n                proj_sheet = s\n                break\n        if not proj_sheet:\n            return 0.0, \"Projections sheet not found.\"\n        dfp = pd.read_excel(path, sheet_name=proj_sheet)\n        # Try wide first\n        totals_wide = _extract_quarter_totals_wide(dfp)\n        totals_long = _extract_quarter_totals_long(dfp)\n        totals = totals_long if totals_long else totals_wide\n        # Count unique quarters\n        uniq_quarters = set((yr, q) for (yr, q, _) in totals.keys() if q is not None)\n        q_count = len(uniq_quarters)\n        score = 0.0\n        # Coverage of 8 quarters\n        if q_count >= 8:\n            score += 1.0  # full credit for coverage\n        else:\n            score += max(0.0, min(1.0, q_count / 8.0)) * 0.8  # partial if some quarters\n        # Detect YoY column(s)\n        yoy_cols = [c for c in dfp.columns if re.search(r\"yo\\s*/?\\s*y|yoy|year[- ]?over[- ]?year\", str(c), re.I)]\n        has_yoy_col = len(yoy_cols) > 0\n        if has_yoy_col:\n            score += 0.6\n        # Attempt to compare provided YoY with computed YoY (best-effort)\n        # Build quarter->total mapping by year and q\n        qt_map = {}\n        for (yr, q, _) in totals.keys():\n            qt_map[(yr, q)] = qt_map.get((yr, q), 0.0) + totals[(yr, q, _)]\n        # Compute expected yoy where possible\n        diffs = []\n        if has_yoy_col and qt_map:\n            # Try to align rows with quarters and yoy\n            dfp_str = dfp.copy()\n            # Look for an explicit quarter column\n            qcol = None\n            for c in dfp.columns:\n                if re.search(r\"quarter\", str(c), re.I):\n                    qcol = c\n                    break\n            if qcol is not None:\n                for _, row in dfp.iterrows():\n                    yr, q = _parse_quarter_label(row[qcol])\n                    if q is None:\n                        continue\n                    this_val = qt_map.get((yr, q), np.nan)\n                    base_val = qt_map.get((yr-1 if yr else None, q), np.nan) if yr else np.nan\n                    if np.isfinite(this_val) and np.isfinite(base_val) and base_val != 0:\n                        exp_yoy = (this_val / base_val) - 1.0\n                        # take first yoy col\n                        yoy_v = _to_percent(pd.to_numeric(row[yoy_cols[0]], errors='coerce'))\n                        if np.isfinite(yoy_v):\n                            diffs.append(abs(exp_yoy - yoy_v))\n        if diffs:\n            mae = float(np.mean(diffs))\n            # Reward close alignment\n            if mae <= 0.02:\n                score += 1.0\n            elif mae <= 0.05:\n                score += 0.8\n            elif mae <= 0.10:\n                score += 0.5\n            elif mae <= 0.20:\n                score += 0.2\n            else:\n                score += 0.0\n        else:\n            # If no direct comparison possible, check reasonableness of implied yoy\n            # Use ordering of quarters by (year, q)\n            pairs = sorted([(k, v) for k, v in qt_map.items() if k[0] is not None and k[1] is not None])\n            ok = 0\n            total = 0\n            for (yr, q), val in pairs:\n                prev = (yr-1, q)\n                if prev in qt_map and qt_map[prev] != 0:\n                    yoy = (val / qt_map[prev]) - 1.0\n                    total += 1\n                    if -0.5 <= yoy <= 2.0:\n                        ok += 1\n            if total > 0:\n                score += (ok / total) * 0.8\n        score = min(score, 1.0)\n        feedback = f\"quarters={q_count}, yoy_col={has_yoy_col}, comparisons={len(diffs) if diffs else 0}\"\n        return score, feedback\n    except Exception as e:\n        return 0.0, f\"Error reading Projections: {e}\""}, {"type": "code", "name": "Reconciliation: Calculations vs Summary (CY)", "description": "Aggregate Calculations by quarter and compare to Summary (CY) totals. Reward close agreement.", "weight": 1.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef _quarter_cols(cols):\n    qcols = []\n    for c in cols:\n        s = str(c).strip().lower()\n        if re.match(r\"q[1-4](\\s*20\\d{2})?$\", s) or re.match(r\"q[1-4]\\s*[- ]\\s*20\\d{2}$\", s):\n            qcols.append(c)\n    return qcols\n\ndef _extract_summary_quarters(path, sheets):\n    summary_sheet = None\n    for s in sheets:\n        if re.search(r\"summary\", s, re.I) and re.search(r\"current|cy|year\", s, re.I):\n            summary_sheet = s\n            break\n    if not summary_sheet:\n        for s in sheets:\n            if re.search(r\"summary\", s, re.I):\n                summary_sheet = s\n                break\n    if not summary_sheet:\n        return {}\n    dfs = pd.read_excel(path, sheet_name=summary_sheet)\n    qcols = _quarter_cols(dfs.columns)\n    if not qcols:\n        return {}\n    dfs_num = dfs.apply(pd.to_numeric, errors='coerce')\n    totals = {}\n    for c in qcols:\n        v = pd.to_numeric(dfs_num[c], errors='coerce').replace([np.inf, -np.inf], np.nan).sum(skipna=True)\n        totals[str(c)] = float(v) if np.isfinite(v) else 0.0\n    return totals\n\ndef _extract_calc_quarters(path, sheets):\n    calc_sheets = [s for s in sheets if re.search(r\"^calc|calculation\", s, re.I)]\n    totals = {}\n    for cs in calc_sheets:\n        dfc = pd.read_excel(path, sheet_name=cs)\n        dfc_num = dfc.apply(pd.to_numeric, errors='coerce')\n        # Try long format with a Quarter column\n        qcol = None\n        for c in dfc.columns:\n            if re.search(r\"quarter\", str(c), re.I):\n                qcol = c\n                break\n        amt_candidates = [c for c in dfc.columns if re.search(r\"amount|total|value|cost|expense\", str(c), re.I)]\n        amt_col = amt_candidates[0] if amt_candidates else None\n        if qcol is not None and amt_col is not None:\n            for _, row in dfc.iterrows():\n                qlbl = str(row[qcol])\n                if isinstance(qlbl, str) and re.search(r\"q[1-4]\", qlbl, re.I):\n                    try:\n                        v = float(pd.to_numeric(row[amt_col], errors='coerce'))\n                    except Exception:\n                        v = np.nan\n                    if np.isfinite(v):\n                        totals[qlbl] = totals.get(qlbl, 0.0) + v\n        else:\n            # Try wide quarter columns\n            qcols = _quarter_cols(dfc.columns)\n            for c in qcols:\n                v = pd.to_numeric(dfc_num[c], errors='coerce').replace([np.inf, -np.inf], np.nan).sum(skipna=True)\n                if np.isfinite(v):\n                    totals[str(c)] = totals.get(str(c), 0.0) + float(v)\n    return totals\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheets = xls.sheet_names\n        sum_q = _extract_summary_quarters(path, sheets)\n        calc_q = _extract_calc_quarters(path, sheets)\n        if not sum_q or not calc_q:\n            return 0.0, \"Could not extract quarter totals from Summary or Calculations.\"\n        # Try to align by quarter names ignoring year suffixes\n        def norm(k):\n            s = str(k).lower()\n            m = re.search(r\"(q[1-4])\", s)\n            return m.group(1) if m else s\n        by_q_sum = {}\n        for k, v in sum_q.items():\n            by_q_sum[norm(k)] = by_q_sum.get(norm(k), 0.0) + v\n        by_q_calc = {}\n        for k, v in calc_q.items():\n            by_q_calc[norm(k)] = by_q_calc.get(norm(k), 0.0) + v\n        common_qs = sorted(set(by_q_sum.keys()) & set(by_q_calc.keys()))\n        if not common_qs:\n            return 0.0, \"No overlapping quarter labels between Summary and Calculations.\"\n        rel_errors = []\n        for q in common_qs:\n            s = by_q_sum[q]\n            c = by_q_calc[q]\n            if s == 0:\n                continue\n            rel_errors.append(abs(c - s) / max(1e-9, abs(s)))\n        if not rel_errors:\n            return 0.0, \"No comparable non-zero quarter totals.\"\n        mae = float(np.mean(rel_errors))\n        # Score based on closeness\n        if mae <= 0.005:\n            score = 1.0\n        elif mae <= 0.02:\n            score = 0.8\n        elif mae <= 0.05:\n            score = 0.5\n        elif mae <= 0.10:\n            score = 0.25\n        else:\n            score = 0.0\n        feedback = f\"MAE={mae:.4f}, quarters_compared={len(common_qs)}\"\n        return score, feedback\n    except Exception as e:\n        return 0.0, f\"Error reconciling: {e}\""}, {"type": "llm_judge", "name": "Methodology Traceability (LLM)", "description": "Check that calculations visibly trace to Inputs/Drivers and Roster, with clear columns (Units, Rate, Amount) and labeled subtotals by quarter/type.", "weight": 0.5, "judge_prompt": "Assess whether the Calculations tab(s) provide traceability and auditability:\n- Do they include row-level columns like Employee ID/Name, Quarter, Compensation Type, Units, Rate, Amount?\n- Is it clear how Inputs/Drivers and Assumptions are applied (labels, notes, or references)?\n- Are there visible subtotals/aggregations by quarter and by compensation type, separate from the final outputs?\n\nScoring (0.5 max):\n- 0.5: Clear line-level detail with Units, Rates, Amounts, references to Inputs/Drivers and Roster, plus labeled subtotals by quarter/type.\n- 0.3: Most traceability present but some parts unclear (e.g., missing notes or incomplete subtotals).\n- 0.1: Minimal traceability; difficult to follow from Inputs to outputs.\n- 0.0: Calculations not present or entirely opaque.\n\nOnly judge traceability/presence, not numerical correctness.", "expectation": "Detailed, labeled calculations that a reviewer can follow end-to-end from drivers and roster to summary/projection totals."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Usability", "description": "Holistic LLM assessment of professionalism, usability for ad hoc analysis, and stakeholder appropriateness.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Presentation, Usability, and Stakeholder Fit", "description": "Evaluate professional formatting, clarity of instructions, ease of running ad hoc analyses, and alignment with finance leadership expectations.", "weight": 5.0, "judge_prompt": "Evaluate the overall professional quality and usability of the workbook:\n\n1) Presentation and Formatting:\n- Consistent, professional styling; clear headings; readable tables; appropriate number formatting (currency/percentages).\n- Clean separation of Inputs, Calculations, and Outputs (Summary/Projections).\n- Helpful features: freeze panes, print-ready summaries, simple color key (e.g., blue inputs), named ranges or notes indicating editable cells.\n\n2) Usability for Ad Hoc Analysis:\n- Clear instructions or README area for how to change drivers.\n- Inputs are consolidated and obvious; data validation where appropriate (e.g., percentages, positive values).\n- Scenario controls (e.g., scenario name, year selectors) that make it easy to compare/adjust future terms.\n\n3) Stakeholder Appropriateness:\n- Structure aligns with a finance executive\u2019s needs (current-year summary by type and quarter, clear projections with YoY, and transparent calculations).\n- Terminology and drivers reflect a CBA context (e.g., rates/premiums, overtime thresholds, benefits/dues).\n\nScoring (5 max):\n- 5.0: Highly professional, intuitive, well-documented; easy to run ad hoc analyses; clearly tailored to the CBA finance context.\n- 4.0: Strong and usable with minor polish gaps.\n- 3.0: Adequate but lacks some clarity/usability features.\n- 1.0: Barely professional; hard to use without guidance.\n- 0.0: Poorly presented and difficult to use.\n\nDo not judge calculation correctness; focus on presentation, clarity, and usability.", "expectation": "A polished, self-explanatory model ready for finance leadership review and scenario analysis."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "19403010-3e5c-494e-a6d3-13594e99f6af", "rubric": {"category_name": "XR Retailer 2023 Makeup Sales Analysis - Single-Sheet Excel Recap", "rationale": "This is an analytical, single-sheet Excel recap. Stage 1 (LLM judge) strictly enforces the required shape so later verification is trivial. Stage 2 (code) performs deterministic arithmetic and consistency checks enabled by the forced structure. Stage 3 (LLM) evaluates presentation quality and executive readiness.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Structured Output Format Requirement (GATE)", "description": "MANDATORY structure for a one-sheet Excel recap enabling verification. LLM-only gate. If this fails, the entire category is zeroed.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.5, "rules": [{"type": "llm_judge", "name": "Single-Sheet Excel + Required Sections and Tables", "description": "Check the candidate output is a single-sheet Excel recap with title and Sections 1\u20135 in the specified tabular structure.", "weight": 6.0, "judge_prompt": "You are evaluating whether the delivered output is a properly structured, verifiable single-sheet Excel recap for: XR Retailer 2023 Sales Performance Analysis Makeup Category Final.\n\nFormat Requirements (MANDATORY):\n- Must be an Excel spreadsheet (.xlsx), not PDF/Word.\n- Must contain exactly one visible worksheet (a single-sheet recap). If workbook has multiple sheets, only one may be visible; others must be hidden. If you cannot confirm visibility, accept 1-sheet content.\n- A prominent top title on the sheet should read exactly or very close to: \"XR Retailer 2023 Sales Performance Analysis Makeup Category Final\". Accept small variations in capitalization or spacing. Because Excel sheet names are limited to 31 characters, the sheet name may be abbreviated, but the top title text should be present on the sheet.\n\nSection Requirements (check presence and structure, not math correctness):\n1) Section 1 \u2014 OVERALL BUSINESS\n   - A clearly labeled section header like \"OVERALL BUSINESS\".\n   - A compact metric table or equivalent layout that shows all four metrics:\n     \u2022 Sales Dollars TY (2023)\n     \u2022 Sales Dollars LY (2022)\n     \u2022 $ Change (2023 vs 2022)\n     \u2022 % Change (2023 vs 2022)\n   - Numbers must be visible (not just formulas).\n\n2) Section 2 \u2014 Discontinued SKUs \u2013 Risk to 2024 Business\n   - A labeled section with the following visible metrics:\n     \u2022 Total sales $$ of Ongoing SKUs (Material Status 05 or 06)\n     \u2022 Total sales $$ of Discontinued SKUs (Material Status 07 or 08)\n     \u2022 % of Sales (discos) = Discontinued $ / Total 2023 $ (TY)\n   - Exact phrasing may vary; accept clear equivalents (e.g., \"Ongoing\", \"Active\", \"Discontinued\", \"Disco\").\n\n3) Section 3 \u2014 Top Volume Drivers\n   - A labeled table with exactly these 9 logical columns (accept synonyms/close variants, but all 9 must be present):\n     1. Function\n     2. XR Sales Dollars 2023 (TY)\n     3. XR Sales Dollars 2022 (LY)\n     4. Sales Dollars $ Change TY vs LY\n     5. Sales Dollars % Change TY vs LY\n     6. % to total business 2023\n     7. % to total business LY 2022\n     8. $ DISCO (2023, mat code 07/08)\n     9. % DISCO\n   - Exactly the 3 functions with highest 2023 sales plus an additional \"Total\" row summing those 3.\n\n4) Section 4 \u2014 Largest Volume Increases\n   - Same 9-column table structure.\n   - Exactly the 3 functions with largest dollar increases (2023 vs 2022) plus a \"Total\" row.\n\n5) Section 5 \u2014 Largest Volume Detractors\n   - Same 9-column table structure.\n   - Exactly the 3 functions with largest dollar decreases plus a \"Total\" row.\n\nScoring (6.0 max):\n- 6.0: Valid .xlsx single-sheet + top title present + all five sections present with required tables/metrics; Sections 3\u20135 each have all 9 columns, 3 function rows, and a Total row.\n- 5.0: Valid .xlsx single-sheet + title + Sections 1\u20132 complete + Sections 3\u20135 present with minor column-name variations or one minor omission (e.g., a single percentage column missing in one section) but still clearly 9-column intent and includes 3 rows + Total per section.\n- 4.5: Valid .xlsx single-sheet + title + Sections 1\u20132 complete + at least two of Sections 3\u20135 fully usable (9 columns present) and the third section is present but missing up to two columns OR missing the Total row in at most one section.\n- 3.0: Valid .xlsx but missing the title or only partially includes required sections (e.g., only Sections 1\u20132 and one of Sections 3\u20135) OR Sections 3\u20135 lack most of the required columns/rows.\n- 0.0: Not an Excel file, multiple sheets with no clear single-sheet recap, or Sections 1\u20132 missing.\n\nImportant: Judge only for the presence and structure (sheet/sections/tables/columns/rows). Do not check the math or quality of insights here. Return a score from 0.0 to 6.0.", "expectation": "A single-sheet .xlsx with a clear title, Section 1 and 2 metrics, and three 9-column tables (Sections 3\u20135) each with 3 rows plus a Total row."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Consistency Checks", "description": "Deterministic code checks leveraging the mandated shape: arithmetic correctness, internal consistency, and reasonable bounds.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Section 1 Arithmetic Consistency (TY/LY, $ Change, % Change)", "description": "Validates that $ Change \u2248 TY \u2212 LY and % Change \u2248 (TY \u2212 LY)/LY. Tolerant to formats and placements.", "weight": 1.2, "code": "import re\nimport numpy as np\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output found.\"\n    try:\n        df = context.files.read_excel(output.id, sheet_name=0, header=None)\n    except Exception as e:\n        return 0.0, f\"Failed to read Excel: {e}\"\n\n    # Helpers\n    def to_number(x):\n        if pd.isna(x):\n            return None\n        if isinstance(x, (int, float, np.number)):\n            if np.isfinite(x):\n                return float(x)\n            return None\n        s = str(x).strip()\n        if s == \"\":\n            return None\n        s = s.replace(\"\\u200b\", \"\")  # zero-width space\n        neg = False\n        if s.startswith(\"(\") and s.endswith(\")\"):\n            neg = True\n            s = s[1:-1]\n        s = s.replace(\",\", \"\").replace(\"$\", \"\")\n        pct = False\n        if s.endswith(\"%\"):\n            pct = True\n            s = s[:-1]\n        s = s.strip()\n        # Handle dashes/NA\n        if re.fullmatch(r\"[-\\u2013\\u2014]\", s) or s.lower() in {\"na\",\"n/a\",\"--\",\"-\"}:\n            return None\n        try:\n            val = float(s)\n            if pct:\n                val = val / 100.0\n            if neg:\n                val = -val\n            return val\n        except:\n            return None\n\n    # Build text grid for fuzzy search\n    nrows, ncols = df.shape\n    txt = df.fillna(\"\").astype(str).applymap(lambda v: v.lower().strip())\n\n    def find_positions(patterns):\n        found = []\n        for r in range(nrows):\n            for c in range(ncols):\n                cell = txt.iat[r, c]\n                if any(p in cell for p in patterns):\n                    found.append((r, c, cell))\n        return found\n\n    def nearest_number(r, c):\n        # Search rightwards in row, then below in column, then neighbors\n        # Rightwards in same row\n        for dc in range(1, 4):\n            if c+dc < ncols:\n                val = to_number(df.iat[r, c+dc])\n                if val is not None:\n                    return val\n        # Below in same column\n        for dr in range(1, 4):\n            if r+dr < nrows:\n                val = to_number(df.iat[r+dr, c])\n                if val is not None:\n                    return val\n        # Immediate neighbors\n        for dr in range(-1, 2):\n            for dc in range(-1, 2):\n                rr, cc = r+dr, c+dc\n                if 0 <= rr < nrows and 0 <= cc < ncols:\n                    val = to_number(df.iat[rr, cc])\n                    if val is not None:\n                        return val\n        return None\n\n    # Find TY and LY\n    ty_labels = [\"sales dollars ty\", \"2023\", \"ty 2023\", \"xr sales dollars 2023\", \"sales 2023\", \"ty (2023)\"]\n    ly_labels = [\"sales dollars ly\", \"2022\", \"ly 2022\", \"xr sales dollars 2022\", \"sales 2022\", \"ly (2022)\"]\n    chg_labels = [\"$ change\", \"dollar change\", \"chg $\", \"ty vs ly $\", \"ty-ly $\"]\n    pct_labels = [\"% change\", \"pct chg\", \"% chg\", \"ty vs ly %\", \"% ty vs ly\"]\n\n    def get_metric(patterns):\n        pos = find_positions(patterns)\n        for r, c, _ in pos:\n            val = nearest_number(r, c)\n            if val is not None:\n                return val\n        return None\n\n    ty = get_metric(ty_labels)\n    ly = get_metric(ly_labels)\n    rep_dchg = get_metric(chg_labels)\n    rep_pct = get_metric(pct_labels)\n\n    if ty is None or ly is None or rep_dchg is None or rep_pct is None:\n        return 0.0, \"Could not locate all Section 1 metrics (TY, LY, $ Change, % Change).\"\n\n    calc_dchg = ty - ly\n    # Tolerances\n    tol_abs = max(1.0, 0.005 * max(abs(ty), abs(ly), 1.0))\n    ok_dchg = abs((rep_dchg or 0) - calc_dchg) <= tol_abs\n\n    if ly == 0:\n        # If LY=0, % change often undefined or shown as 100% if TY>0. Accept broad tolerance.\n        ok_pct = True if (rep_pct is not None) else False\n    else:\n        calc_pct = calc_dchg / ly\n        ok_pct = abs((rep_pct or 0) - calc_pct) <= 0.02  # within 2 percentage points\n\n    score = 0\n    parts = []\n    if ok_dchg:\n        score += 0.6\n        parts.append(\"$ Change matches TY-LY\")\n    if ok_pct:\n        score += 0.6\n        parts.append(\"% Change matches (TY-LY)/LY\")\n    return score, \"; \".join(parts) if parts else \"Section 1 arithmetic checks failed.\"\n"}, {"type": "code", "name": "Section 2 Discontinued Math Consistency", "description": "Checks that % of Sales (discos) \u2248 Discontinued $ / TY and Discontinued $ \u2264 TY. If Ongoing $ is present, also checks Ongoing + Discontinued \u2248 TY.", "weight": 0.8, "code": "import re\nimport numpy as np\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output found.\"\n    try:\n        df = context.files.read_excel(output.id, sheet_name=0, header=None)\n    except Exception as e:\n        return 0.0, f\"Failed to read Excel: {e}\"\n\n    def to_number(x):\n        if pd.isna(x):\n            return None\n        if isinstance(x, (int, float, np.number)):\n            if np.isfinite(x):\n                return float(x)\n            return None\n        s = str(x).strip()\n        if s == \"\":\n            return None\n        neg = False\n        if s.startswith(\"(\") and s.endswith(\")\"):\n            neg = True\n            s = s[1:-1]\n        s = s.replace(\",\", \"\").replace(\"$\", \"\")\n        pct = False\n        if s.endswith(\"%\"):\n            pct = True\n            s = s[:-1]\n        s = s.strip()\n        if s.lower() in {\"na\",\"n/a\",\"--\",\"-\"}:\n            return None\n        try:\n            val = float(s)\n            if pct:\n                val = val / 100.0\n            if neg:\n                val = -val\n            return val\n        except:\n            return None\n\n    txt = df.fillna(\"\").astype(str).applymap(lambda v: v.lower().strip())\n    nrows, ncols = df.shape\n\n    def find_positions(patterns):\n        out = []\n        for r in range(nrows):\n            for c in range(ncols):\n                cell = txt.iat[r, c]\n                if any(p in cell for p in patterns):\n                    out.append((r, c))\n        return out\n\n    def nearest_number(r, c):\n        # Search right then down\n        for dc in range(1, 4):\n            if c+dc < ncols:\n                v = to_number(df.iat[r, c+dc])\n                if v is not None:\n                    return v\n        for dr in range(1, 4):\n            if r+dr < nrows:\n                v = to_number(df.iat[r+dr, c])\n                if v is not None:\n                    return v\n        # neighbors\n        for dr in range(-1, 2):\n            for dc in range(-1, 2):\n                rr, cc = r+dr, c+dc\n                if 0 <= rr < nrows and 0 <= cc < ncols:\n                    v = to_number(df.iat[rr, cc])\n                    if v is not None:\n                        return v\n        return None\n\n    # Find TY from Section 1 as reference\n    ty_pos = find_positions([\"sales dollars ty\", \"2023\", \"ty 2023\", \"sales 2023\"])\n    TY = None\n    for r, c in ty_pos:\n        TY = nearest_number(r, c)\n        if TY is not None:\n            break\n\n    disco_pos = find_positions([\"discontinued\", \"disco\", \"07\", \"08\"])\n    ongoing_pos = find_positions([\"ongoing\", \"active\", \"05\", \"06\"]) \n    pct_disco_pos = find_positions([\"% of sales (disco\", \"% disco\", \"percent disco\"]) or find_positions([\"% of sales\", \"disco %\"]) \n\n    disco = None\n    for r, c in disco_pos:\n        v = nearest_number(r, c)\n        if v is not None:\n            disco = v\n            break\n\n    ongoing = None\n    for r, c in ongoing_pos:\n        v = nearest_number(r, c)\n        if v is not None:\n            ongoing = v\n            break\n\n    pct_disco = None\n    for r, c in pct_disco_pos:\n        v = nearest_number(r, c)\n        if v is not None:\n            pct_disco = v\n            break\n\n    if TY is None or disco is None or pct_disco is None:\n        return 0.0, \"Missing TY, Discontinued $, or % Disco.\"\n\n    score = 0.0\n    fb = []\n\n    # Check percent relation\n    if TY != 0:\n        calc_pct = disco / TY\n        if abs(calc_pct - pct_disco) <= 0.02:  # within 2 percentage points\n            score += 0.5\n            fb.append(\"% Disco matches Discontinued$/TY\")\n    else:\n        # If TY is zero, accept any displayed percent\n        score += 0.3\n        fb.append(\"TY is zero; percent check relaxed\")\n\n    # Bound check: Discontinued <= TY (allow small tolerance)\n    if disco <= TY + max(1.0, 0.005 * max(abs(TY), 1.0)):\n        score += 0.2\n        fb.append(\"Discontinued $ \u2264 TY\")\n\n    # Optional: ongoing + disco \u2248 TY\n    if ongoing is not None:\n        if abs((ongoing + disco) - TY) <= max(1.0, 0.05 * max(abs(TY), 1.0)):\n            score += 0.1\n            fb.append(\"Ongoing + Discontinued \u2248 TY\")\n\n    return score, \"; \".join(fb) if fb else \"Section 2 checks partial or failed.\"\n"}, {"type": "code", "name": "Sections 3\u20135 Table Calculations and Totals", "description": "Detects the three 9-column tables, validates per-row change math, % relations, and the Total row sums for each section.", "weight": 1.0, "code": "import re\nimport numpy as np\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output found.\"\n    try:\n        raw = context.files.read_excel(output.id, sheet_name=0, header=None)\n    except Exception as e:\n        return 0.0, f\"Failed to read Excel: {e}\"\n\n    # Prepare text and numeric grids\n    df = raw.copy()\n    txt = df.fillna(\"\").astype(str).applymap(lambda s: s.lower().strip())\n    nrows, ncols = txt.shape\n\n    def to_number(x):\n        if pd.isna(x):\n            return None\n        if isinstance(x, (int, float, np.number)):\n            if np.isfinite(x):\n                return float(x)\n            return None\n        s = str(x).strip()\n        if s == \"\":\n            return None\n        neg = False\n        if s.startswith(\"(\") and s.endswith(\")\"):\n            neg = True\n            s = s[1:-1]\n        s = s.replace(\",\", \"\").replace(\"$\", \"\")\n        pct = False\n        if s.endswith(\"%\"):\n            pct = True\n            s = s[:-1]\n        s = s.strip()\n        if s.lower() in {\"na\",\"n/a\",\"--\",\"-\"}:\n            return None\n        try:\n            val = float(s)\n            if pct:\n                val = val / 100.0\n            if neg:\n                val = -val\n            return val\n        except:\n            return None\n\n    # Heuristic: find header rows for the 9-column tables by presence of key tokens\n    def is_header_row(r):\n        row = txt.iloc[r, :].tolist()\n        row_join = \" | \".join(row)\n        return (\n            (any(\"function\" in c for c in row)) and\n            (\"2023\" in row_join or \"ty\" in row_join) and\n            (\"2022\" in row_join or \"ly\" in row_join) and\n            (\"disco\" in row_join) and\n            (\"%\" in row_join)\n        )\n\n    header_rows = [r for r in range(nrows) if is_header_row(r)]\n    if not header_rows:\n        return 0.0, \"No recognizable 9-column tables found.\"\n\n    # Map columns by fuzzy matching\n    def normalize(s):\n        return re.sub(r\"[^a-z0-9]\", \"\", s)\n\n    patterns = {\n        \"function\": [\"function\"],\n        \"ty\": [\"2023\", \"ty\"],\n        \"ly\": [\"2022\", \"ly\"],\n        \"chg_dollar\": [\"$change\", \"dollarchange\", \"chg$\", \"tyvsly$\", \"change$\"],\n        \"chg_pct\": [\"%change\", \"pctchg\", \"%chg\", \"tyvsly%\", \"change%\"],\n        \"pct_tot_ty\": [\"%totaltobusiness2023\", \"%totaltobusiness23\", \"%totaltobusiness%2023\"],\n        \"pct_tot_ly\": [\"%totaltobusiness2022\", \"%totaltobusinessly\", \"%totaltobusiness22\"],\n        \"$disco\": [\"$disco\", \"disco$\", \"dollar$disco\"],\n        \"%disco\": [\"%disco\", \"disc%\", \"percentdisco\"]\n    }\n\n    def map_columns(r):\n        mapping = {}\n        headers = [normalize(txt.iat[r, c]) for c in range(ncols)]\n        for key, pats in patterns.items():\n            for c in range(ncols):\n                h = headers[c]\n                if any(p in h for p in pats) and key not in mapping:\n                    mapping[key] = c\n        # Relaxed mapping for % to total columns if not found\n        if \"pct_tot_ty\" not in mapping:\n            for c in range(ncols):\n                h = headers[c]\n                if (\"%\" in h) and (\"total\" in h) and (\"2023\" in h or \"23\" in h):\n                    mapping[\"pct_tot_ty\"] = c\n                    break\n        if \"pct_tot_ly\" not in mapping:\n            for c in range(ncols):\n                h = headers[c]\n                if (\"%\" in h) and (\"total\" in h) and (\"2022\" in h or \"22\" in h or \"ly\" in h):\n                    mapping[\"pct_tot_ly\"] = c\n                    break\n        return mapping\n\n    # Attempt to read TY & LY overall from Section 1 for % to total validation\n    def find_section1_total(label_tokens):\n        for r in range(nrows):\n            for c in range(ncols):\n                cell = txt.iat[r, c]\n                if any(tok in cell for tok in label_tokens):\n                    # look right then down\n                    for dc in range(1, 4):\n                        if c+dc < ncols:\n                            v = to_number(df.iat[r, c+dc])\n                            if v is not None:\n                                return v\n                    for dr in range(1, 4):\n                        if r+dr < nrows:\n                            v = to_number(df.iat[r+dr, c])\n                            if v is not None:\n                                return v\n        return None\n\n    TY_total = find_section1_total([\"sales dollars ty\", \"2023\", \"sales 2023\", \"ty 2023\"]) or None\n    LY_total = find_section1_total([\"sales dollars ly\", \"2022\", \"sales 2022\", \"ly 2022\"]) or None\n\n    checks_done = 0\n    checks_pass = 0\n\n    for hr in header_rows:\n        colmap = map_columns(hr)\n        required_keys = [\"function\", \"ty\", \"ly\", \"chg_dollar\", \"chg_pct\", \"$disco\", \"%disco\"]\n        if not all(k in colmap for k in required_keys):\n            continue\n        # Gather up to 4 data rows until blank line or next header\n        rows = []\n        r = hr + 1\n        while r < nrows:\n            # stop if we hit another header-like row\n            if is_header_row(r):\n                break\n            func_cell = txt.iat[r, colmap[\"function\"]] if colmap[\"function\"] < ncols else \"\"\n            # end block on fully blank row\n            if all(txt.iat[r, c] == \"\" for c in range(min(ncols, colmap[\"function\"]+3))):\n                break\n            # Capture row if function cell is non-empty\n            if func_cell != \"\":\n                rows.append(r)\n            # stop when we have at least 3 rows and a potential total row appears\n            if len(rows) >= 4:\n                pass\n            r += 1\n        if len(rows) < 3:\n            continue\n\n        # Identify total row among captured rows by 'total' token if present; else assume 4th is total\n        total_row = None\n        data_rows = rows[:]\n        for rr in rows:\n            if \"total\" in txt.iat[rr, colmap[\"function\"]]:\n                total_row = rr\n                break\n        if total_row is None and len(rows) >= 4:\n            total_row = rows[3]\n        if total_row is not None and total_row in data_rows:\n            data_rows = [rr for rr in data_rows if rr != total_row]\n        # Keep first 3 data rows\n        data_rows = data_rows[:3]\n\n        # Per-row arithmetic checks for first 3 rows\n        tyc, lyc = colmap[\"ty\"], colmap[\"ly\"]\n        ddc, pcc = colmap[\"chg_dollar\"], colmap[\"chg_pct\"]\n        dsc, pdc = colmap[\"$disco\"], colmap[\"%disco\"]\n\n        sum_ty = 0.0\n        sum_ly = 0.0\n        sum_d = 0.0\n        sum_dis = 0.0\n\n        for rr in data_rows:\n            TY = to_number(df.iat[rr, tyc]); LY = to_number(df.iat[rr, lyc])\n            D  = to_number(df.iat[rr, ddc]); PC = to_number(df.iat[rr, pcc])\n            DIS= to_number(df.iat[rr, dsc]); PDIS = to_number(df.iat[rr, pdc])\n            # Require core numbers\n            if TY is None or LY is None or D is None or PC is None or DIS is None or PDIS is None:\n                continue\n            tol_abs = max(1.0, 0.01 * max(abs(TY), abs(LY), 1.0))\n            # $ change\n            checks_done += 1\n            if abs((TY - LY) - D) <= tol_abs:\n                checks_pass += 1\n            # % change\n            checks_done += 1\n            if (LY == 0 and PC is not None) or (LY != 0 and abs(((TY - LY) / LY) - PC) <= 0.03):\n                checks_pass += 1\n            # % disco\n            checks_done += 1\n            if (TY == 0 and PDIS is not None) or (TY != 0 and abs((DIS / TY) - PDIS) <= 0.02):\n                checks_pass += 1\n            # bound DISCO <= TY\n            checks_done += 1\n            if DIS is not None and TY is not None and DIS <= TY + max(1.0, 0.005 * max(abs(TY), 1.0)):\n                checks_pass += 1\n\n            sum_ty += TY; sum_ly += LY; sum_d += D; sum_dis += DIS\n\n        # Totals row checks if present\n        if total_row is not None:\n            T_TY = to_number(df.iat[total_row, tyc])\n            T_LY = to_number(df.iat[total_row, lyc])\n            T_D  = to_number(df.iat[total_row, ddc])\n            T_DIS= to_number(df.iat[total_row, dsc])\n            if T_TY is not None:\n                checks_done += 1\n                if abs(T_TY - sum_ty) <= max(1.0, 0.01 * max(abs(sum_ty), 1.0)):\n                    checks_pass += 1\n            if T_LY is not None:\n                checks_done += 1\n                if abs(T_LY - sum_ly) <= max(1.0, 0.01 * max(abs(sum_ly), 1.0)):\n                    checks_pass += 1\n            if T_D is not None:\n                checks_done += 1\n                if abs(T_D - sum_d) <= max(1.0, 0.02 * max(abs(sum_d), 1.0)):\n                    checks_pass += 1\n            if T_DIS is not None:\n                checks_done += 1\n                if abs(T_DIS - sum_dis) <= max(1.0, 0.02 * max(abs(sum_dis), 1.0)):\n                    checks_pass += 1\n\n        # % to total sanity if overall TY/LY totals found\n        if TY_total is not None:\n            for rr in data_rows:\n                TY = to_number(df.iat[rr, tyc])\n                if TY is None or TY_total == 0:\n                    continue\n                # try to find % to total 2023 column\n                for key in (\"pct_tot_ty\",):\n                    if key in colmap:\n                        pt = to_number(df.iat[rr, colmap[key]])\n                        if pt is not None:\n                            checks_done += 1\n                            if abs(pt - (TY / TY_total)) <= 0.03:\n                                checks_pass += 1\n        if LY_total is not None:\n            for rr in data_rows:\n                LY = to_number(df.iat[rr, lyc])\n                if LY is None or LY_total == 0:\n                    continue\n                for key in (\"pct_tot_ly\",):\n                    if key in colmap:\n                        pt = to_number(df.iat[rr, colmap[key]])\n                        if pt is not None:\n                            checks_done += 1\n                            if abs(pt - (LY / LY_total)) <= 0.03:\n                                checks_pass += 1\n\n    if checks_done == 0:\n        return 0.0, \"No verifiable rows detected in Sections 3\u20135.\"\n    frac = checks_pass / checks_done\n    return frac, f\"Passed {checks_pass}/{checks_done} checks\"\n"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Presentation Quality and Executive Readiness", "description": "LLM judge evaluates clarity, formatting, and stakeholder readiness of the single-sheet recap.", "is_required": false, "max_points": 1.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Usability", "description": "Checks number formatting, clarity, and whether an executive could quickly understand the page.", "weight": 1.0, "judge_prompt": "Evaluate the single-sheet Excel recap for professional presentation and executive usability. Do NOT re-check structural presence beyond what is necessary for quality. Consider:\n- Readability and visual hierarchy: clear section headers, spacing, alignment, consistent fonts.\n- Number formatting: currency shown as $ with thousands separators; percentages with % and appropriate decimals; consistent rounding.\n- Compactness and scan-ability: fits on one page, no clutter, clear separation of Sections 1\u20135.\n- Accurate labels and legends: columns and metrics are labeled unambiguously; the page title references XR Retailer, 2023, and Makeup category.\n- Light annotations where helpful (e.g., brief notes for definitions or data sources) without overloading the page.\n\nScoring (1.0 max):\n- 1.0: Highly professional, clean layout, consistent currency/percent formatting, very easy to scan.\n- 0.7: Generally professional with minor formatting inconsistencies (e.g., a few numbers missing currency/percent) but still clear.\n- 0.4: Mixed formatting and readability issues; still usable but not executive-ready.\n- 0.0: Poorly formatted, hard to read, or not suitable for a one-page executive recap.\n\nReturn a score from 0.0 to 1.0.", "expectation": "An executive-ready, clean single-sheet with consistent number formats and clear hierarchy."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "68d8d901-dd0b-4a7e-bf9a-1074fddf1a96", "rubric": {"category_name": "Manufacturing \u2014 First-Line Supervisor: Freeze-Dried Beef Trial Planning Workbook", "rationale": "Self-documenting rubric that forces an editable Excel workbook with three specific tabs enabling trivial verification of staffing, schedule, and sequencing for two staggered dryers. Stage 1 (LLM-only) enforces the exact workbook shape and section structure. Stage 2 uses code to verify key numeric targets, staffing counts, role coverage, and two-dryer staggering. Stage 3 assesses professional clarity and operational soundness.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 Shape Enforcement Gate (LLM Only)", "description": "Checks that the candidate produced an editable Excel workbook with three tabs: Work Schedule, Production Assignments, Production Sequences. Verifies required sections/tables exist so later code rules can read and validate them. Only structure/presence is evaluated here (not correctness).", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Excel Workbook Presence and Completeness", "description": "LLM verifies the workbook is an Excel file with three tabs having specific structural sections and tables enabling verification.", "weight": 4.0, "judge_prompt": "You are evaluating the SHAPE (structure) of an Excel workbook for a manufacturing beef trial plan. Only assess presence and structure, not correctness. Be flexible with capitalization/pluralization and minor naming variants.\n\nFormat requirements:\n- Must be an editable Excel workbook (.xlsx preferred).\n- Must contain three tabs (sheets) named approximately:\n  1) \"Work Schedule\"\n  2) \"Production Assignments\"\n  3) \"Production Sequences\"\n  Accept small variations like \"Work Schedules\", \"Production Assignment\", \"Production Sequence\", etc.\n\nRequired structure by sheet:\n\nA) Work Schedule sheet must include ALL of the following sections/tables (headers visible):\n1) Project Overview (or similar): a table with columns similar to [Metric | Value | Unit | Notes/Assumptions]. Must include rows for at least these key data points:\n   - Production Target (lbs) \u2014 expected to be at least 250,000\n   - Trial Duration (days or weeks)\n   - Shift Length (hours)\n   - Shifts per Day\n   - Total Run Hours (or easily derivable)\n   - Labor Availability (employee count)\n   - Equipment/Dryers (must indicate 2 dryers)\n   - Batch Size policy indicating full batches only\n   - Dryer Cycle Time (hours)\n   - Batch Size (lbs) and/or Daily Capacity (lbs)\n2) Shift Plan (or similar): a table with columns similar to [Shift | Start Time | End Time | Hours | Headcount | Notes].\n3) Capacity/Target Summary (or similar): a table summarizing [Target lbs | Planned Output lbs | Gap to Target] or clearly equivalent metrics.\n\nB) Production Assignments sheet must include:\n- A table listing 20 personnel (at least 20 rows of distinct people) with columns approximating: [Person/Name | Role | Stage/Department (e.g., Raw Prep, Freeze Drying, Packaging) | Role Description (brief, at least ~10 words) | Shift | Backup/Cross-Training]. Minor variations acceptable.\n\nC) Production Sequences sheet must include:\n- A detailed, stepwise table for TWO dryers (e.g., Dryer 1/2 or A/B) with columns approximating: [Dryer | Step # | Sub-step/Task | Responsible (role or person) | Start Time | Duration (hours) | End Time | Predecessor/Dependency | Notes].\n- The sequence must clearly reflect staggered dryer cycles (end times or phases are offset, not identical). This can be shown by start/end times or sequencing notes.\n\nScoring (out of 4.0):\n- 4.0: Valid Excel + all 3 sheets present + all required sections/tables present per sheet + 20 personnel listed + sequences for two dryers with time fields (start/duration/end) and clear stagger.\n- 3.0: Valid Excel + all 3 sheets present + minor omissions in one section (e.g., missing one key row in Project Overview or missing one time column) but still includes 20 personnel and two-dryer sequences that are obviously staggered.\n- 2.0: Valid Excel + all 3 sheets present but missing a major section (e.g., no Shift Plan) OR personnel list <20 OR production sequences lack time fields though two dryers are shown.\n- 1.0: Valid Excel but only 1\u20132 sheets present or tables are largely missing/unstructured.\n- 0.0: Not an Excel file, unreadable, or sheets not relevant.\n\nOnly judge presence/structure, not correctness of numbers or feasibility.", "expectation": "An editable .xlsx with three clearly labeled tabs, each containing the specified tables/sections so verification and analysis are straightforward."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Consistency Verification (Code + LLM-amenable checks)", "description": "Deterministic checks using code against the structured workbook enforced by Stage 1. Verifies numeric targets, staffing counts, coverage across stages, two-dryer sequencing, and staggered timing.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Spreadsheet and Sheet Name Presence (redundant check)", "description": "Confirm the primary output is a spreadsheet and the three sheets exist with flexible naming.", "weight": 0.5, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        names = [str(s).strip().lower() for s in xls.sheet_names]\n        def has_sheet(keywords):\n            for n in names:\n                if all(k in n for k in keywords):\n                    return True\n            return False\n        ok_work = has_sheet([\"work\",\"sched\"]) or has_sheet([\"schedule\"])  # be flexible\n        ok_assign = has_sheet([\"assign\"]) and (has_sheet([\"production\",\"assign\"]) or ok_work) or has_sheet([\"production\",\"assign\"]) or has_sheet([\"staff\"])  # flexible matching\n        ok_seq = has_sheet([\"sequence\"]) or has_sheet([\"prod\",\"seq\"]) or has_sheet([\"process\",\"seq\"]) \n        score = 0.0\n        if ok_work:\n            score += 0.2\n        if ok_assign:\n            score += 0.15\n        if ok_seq:\n            score += 0.15\n        return min(score, 0.5)\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Assignments: >=20 personnel and role coverage across stages", "description": "Check the Production Assignments sheet has at least 20 distinct personnel and roles cover Raw Prep, Freeze Drying, and Packaging.", "weight": 1.1, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef _find_sheet(xls, keywords):\n    names = [str(s).strip().lower() for s in xls.sheet_names]\n    for i, n in enumerate(names):\n        if all(k in n for k in keywords):\n            return xls.sheet_names[i]\n    # fallbacks\n    for i, n in enumerate(names):\n        if \"assign\" in n or (\"staff\" in n and \"production\" in n):\n            return xls.sheet_names[i]\n    return None\n\ndef evaluate(workflow, context):\n    out = context.get_primary_output()\n    if not out or not out.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(out.id)\n        xls = pd.ExcelFile(path)\n        sheet = _find_sheet(xls, [\"production\",\"assign\"]) or _find_sheet(xls, [\"assign\"]) or xls.sheet_names[0]\n        df = pd.read_excel(path, sheet_name=sheet)\n        # Normalize\n        df_cols_lower = [str(c).strip().lower() for c in df.columns]\n        df.columns = df_cols_lower\n        # Count rows with any non-null across visible columns\n        data = df.dropna(how='all')\n        # Identify a likely name/person column\n        name_col = None\n        for c in df.columns:\n            if any(k in c for k in [\"name\",\"person\",\"employee\",\"staff\"]):\n                name_col = c\n                break\n        if name_col is None and len(df.columns)>0:\n            name_col = df.columns[0]\n        # Distinct personnel count\n        names_series = data[name_col].astype(str).str.strip()\n        names_series = names_series[names_series.str.len()>0]\n        distinct_personnel = names_series.str.lower().nunique()\n        # Role coverage\n        role_text_cols = [c for c in df.columns if any(k in c for k in [\"role\",\"stage\",\"dept\",\"department\",\"area\"])]\n        if not role_text_cols:\n            role_text_cols = df.columns.tolist()\n        joined = data[role_text_cols].astype(str).apply(lambda s: ' '.join(s.values.tolist()), axis=1).str.lower()\n        has_prep = joined.str.contains(r\"prep|raw\").any()\n        has_freeze = joined.str.contains(r\"freeze|dryer\").any()\n        has_pack = joined.str.contains(r\"pack\").any()\n        coverage = sum([has_prep, has_freeze, has_pack]) / 3.0\n        # Personnel count scoring\n        if distinct_personnel >= 20:\n            count_score = 1.0\n        elif distinct_personnel >= 15:\n            count_score = 0.7\n        elif distinct_personnel >= 10:\n            count_score = 0.4\n        else:\n            count_score = 0.0\n        # Combine (80% count, 20% coverage)\n        raw_score = 0.8*count_score + 0.2*coverage\n        return raw_score * 1.1\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Target >= 250,000 lbs and 24/7 shift reasonableness", "description": "From Work Schedule, confirm production target meets/exceeds 250,000 lbs and shift plan roughly covers 24 hours/day (shift_length * shifts_per_day \u2248 24 \u00b12).", "weight": 0.8, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef _find_sheet(xls, keys):\n    names = [str(s).strip().lower() for s in xls.sheet_names]\n    for i,n in enumerate(names):\n        if all(k in n for k in keys):\n            return xls.sheet_names[i]\n    for i,n in enumerate(names):\n        if \"work\" in n and \"sched\" in n:\n            return xls.sheet_names[i]\n    return None\n\ndef _sheet_text(df):\n    try:\n        return ' '.join(df.astype(str).fillna('').values.ravel().tolist()).lower()\n    except Exception:\n        return ''\n\ndef _find_number_near(text, key_words):\n    # returns the largest integer-like number found within +/- 60 chars of any key occurrence\n    best = None\n    for kw in key_words:\n        for m in re.finditer(re.escape(kw), text):\n            start = max(0, m.start()-80)\n            end = min(len(text), m.end()+80)\n            window = text[start:end]\n            for n in re.findall(r\"\\b\\d[\\d,\\.]{2,}\\b\", window):\n                try:\n                    val = float(str(n).replace(',', ''))\n                    if best is None or val>best:\n                        best = val\n                except:\n                    pass\n    return best\n\ndef evaluate(workflow, context):\n    out = context.get_primary_output()\n    if not out or not out.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(out.id)\n        xls = pd.ExcelFile(path)\n        sheet = _find_sheet(xls, [\"work\",\"schedule\"]) or xls.sheet_names[0]\n        df = pd.read_excel(path, sheet_name=sheet, header=None)\n        text = _sheet_text(df)\n        # Target\n        target = _find_number_near(text, [\"production target\",\"target\",\"lbs target\",\"target lbs\"]) or _find_number_near(text, [\"production\",\"lbs\"]) or 0\n        cond_target = target >= 250000\n        # Shifts: try to find shift length and shifts per day\n        # shift length\n        shift_len = _find_number_near(text, [\"shift length\",\"hours per shift\",\"shift hours\",\"shift hr\",\"shift-hr\"]) or 0\n        # shifts per day\n        shifts_per_day = _find_number_near(text, [\"shifts per day\",\"# of shifts\",\"num shifts\",\"number of shifts\"]) or 0\n        cond_24 = False\n        if shift_len>0 and shifts_per_day>0:\n            product = shift_len * shifts_per_day\n            cond_24 = (22 <= product <= 26)\n        # two dryers mention\n        two_dryers = (\"2 dryer\" in text) or (\"two dryer\" in text) or (\"2 dryers\" in text) or (\"two dryers\" in text) or (\"dryers: 2\" in text)\n        # Scoring: target (0.4), 24h coverage (0.3), two dryers mention (0.1)\n        score = 0.0\n        if cond_target:\n            score += 0.4\n        if cond_24:\n            score += 0.3\n        if two_dryers:\n            score += 0.1\n        return score * (0.8/0.8)  # already bounded to 0.8\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Sequences: two dryers identified and time columns present", "description": "Production Sequences includes two dryers (1/2 or A/B) and at least two of Start/Duration/End time columns.", "weight": 0.7, "code": "import re\nimport pandas as pd\n\ndef _find_seq_sheet(xls):\n    names = [str(s).strip().lower() for s in xls.sheet_names]\n    for i,n in enumerate(names):\n        if \"sequence\" in n:\n            return xls.sheet_names[i]\n    for i,n in enumerate(names):\n        if (\"process\" in n and \"seq\" in n) or (\"production\" in n and \"seq\" in n):\n            return xls.sheet_names[i]\n    return None\n\ndef evaluate(workflow, context):\n    out = context.get_primary_output()\n    if not out or not out.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(out.id)\n        xls = pd.ExcelFile(path)\n        sheet = _find_seq_sheet(xls) or xls.sheet_names[-1]\n        df = pd.read_excel(path, sheet_name=sheet)\n        cols = [str(c).strip().lower() for c in df.columns]\n        # Dryer identification\n        has_dryer_col = any(\"dryer\" in c for c in cols)\n        # If no explicit column, search cell text for \"dryer 1/2\" or \"dryer a/b\"\n        text_join = ' '.join(df.astype(str).fillna('').values.ravel().tolist()).lower()\n        has_two_dryers = False\n        if has_dryer_col:\n            vals = df[[c for c in df.columns if \"dryer\" in str(c).lower()][0]].astype(str).str.lower()\n            tags = set([v.strip() for v in vals if v.strip()])\n            has_two_dryers = any(t in v for v in [\"1\",\"a\"] for t in tags) and any(t in v for v in [\"2\",\"b\"] for t in tags)\n        else:\n            has_two_dryers = (\"dryer 1\" in text_join and \"dryer 2\" in text_join) or (\"dryer a\" in text_join and \"dryer b\" in text_join)\n        # Time columns\n        has_start = any(\"start\" in c for c in cols)\n        has_end = any(\"end\" in c for c in cols)\n        has_dur = any(\"dur\" in c or \"hour\" in c or \"hrs\" in c for c in cols)\n        time_cols_count = sum([has_start, has_end, has_dur])\n        score = 0.0\n        if has_two_dryers:\n            score += 0.35\n        if time_cols_count >= 2:\n            score += 0.35\n        return min(score, 0.7)\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Staggered freeze cycles (timing sanity)", "description": "Attempt to detect non-identical freeze cycle end times between Dryer 1/A and Dryer 2/B. Partial credit if durations are positive but exact times unavailable.", "weight": 0.7, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef _find_seq_sheet(xls):\n    for nm in xls.sheet_names:\n        n = str(nm).strip().lower()\n        if \"sequence\" in n:\n            return nm\n    return xls.sheet_names[-1]\n\ndef evaluate(workflow, context):\n    out = context.get_primary_output()\n    if not out or not out.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(out.id)\n        xls = pd.ExcelFile(path)\n        sheet = _find_seq_sheet(xls)\n        df = pd.read_excel(path, sheet_name=sheet)\n        df_cols = [str(c).strip().lower() for c in df.columns]\n        df.columns = df_cols\n        # Identify columns\n        dryer_col = None\n        for c in df.columns:\n            if \"dryer\" in c:\n                dryer_col = c\n                break\n        # Try to find step/task name column\n        step_col = None\n        for c in df.columns:\n            if any(k in c for k in [\"step\",\"task\",\"activity\",\"sub-step\",\"sub step\"]):\n                step_col = c\n                break\n        # time cols\n        start_col = next((c for c in df.columns if \"start\" in c), None)\n        end_col = next((c for c in df.columns if c.startswith(\"end\") or \" end\" in c or c==\"end\"), None)\n        dur_col = next((c for c in df.columns if \"dur\" in c or \"hour\" in c or \"hrs\" in c), None)\n        # If we cannot identify times at all\n        if not any([start_col, end_col, dur_col]):\n            return 0.2  # minimal partial if sheet exists (Stage 1 ensured structure)\n        d = df.dropna(how='all')\n        # Mark freeze steps\n        if step_col is not None:\n            freeze_mask = d[step_col].astype(str).str.lower().str.contains(\"freez\")\n            d = d[freeze_mask]\n        # Group by dryer\n        if dryer_col is None:\n            # infer dryer by text search across row if possible\n            infer_tags = []\n            for i, row in d.iterrows():\n                rowtxt = ' '.join([str(v).lower() for v in row.values])\n                if \"dryer 1\" in rowtxt or \"dryer a\" in rowtxt:\n                    infer_tags.append(\"A\")\n                elif \"dryer 2\" in rowtxt or \"dryer b\" in rowtxt:\n                    infer_tags.append(\"B\")\n                else:\n                    infer_tags.append(\"\")\n            d = d.assign(_dryer=infer_tags)\n            dryer_col = \"_dryer\"\n        # parse times numerically\n        def to_hours(x):\n            try:\n                s = str(x).strip().lower()\n                if s in (\"\", \"nan\", \"none\"):\n                    return np.nan\n                # if looks like HH:MM\n                if re.match(r\"^\\d{1,2}:\\d{2}$\", s):\n                    hh,mm = s.split(\":\")\n                    return int(hh) + int(mm)/60.0\n                # numbers\n                s = s.replace(\"hrs\",\"h\").replace(\"hr\",\"h\").replace(\"hours\",\"h\").replace(\"hour\",\"h\")\n                m = re.search(r\"([\\d\\.]+)\\s*h\", s)\n                if m:\n                    return float(m.group(1))\n                return float(s.replace(\",\",\"\"))\n            except:\n                return np.nan\n        start_hours = d[start_col].apply(to_hours) if start_col in d.columns else pd.Series([np.nan]*len(d), index=d.index)\n        end_hours = d[end_col].apply(to_hours) if end_col in d.columns else pd.Series([np.nan]*len(d), index=d.index)\n        dur_hours = d[dur_col].apply(to_hours) if dur_col in d.columns else pd.Series([np.nan]*len(d), index=d.index)\n        # compute end if missing\n        comp_end = end_hours.copy()\n        missing_end = comp_end.isna() & (~start_hours.isna()) & (~dur_hours.isna())\n        comp_end[missing_end] = start_hours[missing_end] + dur_hours[missing_end]\n        # compare between dryers\n        groups = d.assign(_end=comp_end).groupby(dryer_col)[\"_end\"].median()\n        # need at least two groups with finite values\n        vals = groups.dropna()\n        if len(vals)>=2:\n            diff = abs(vals.iloc[0] - vals.iloc[1])\n            if diff >= 0.5:\n                return 0.7\n            elif diff > 0:\n                return 0.5\n            else:\n                return 0.3\n        # If we couldn't compute ends, but durations are positive in both groups, give partial\n        dur_groups = d.groupby(dryer_col)[dur_hours.name if hasattr(dur_hours,'name') else dur_col].apply(lambda s: pd.to_numeric(s, errors='coerce').dropna().median() if len(s)>0 else np.nan)\n        if dur_groups.dropna().shape[0] >= 2:\n            return 0.3\n        return 0.2\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Full-batch policy visibly documented", "description": "Confirm the workbook text mentions the full-batch requirement (e.g., \"full batch\", \"full batches only\").", "weight": 0.2, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    out = context.get_primary_output()\n    if not out or not out.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(out.id)\n        xls = pd.ExcelFile(path)\n        text_all = []\n        for s in xls.sheet_names:\n            df = pd.read_excel(path, sheet_name=s, header=None)\n            text_all.append(' '.join(df.astype(str).fillna('').values.ravel().tolist()).lower())\n        text = ' '.join(text_all)\n        if any(p in text for p in [\"full batch\", \"full batches only\", \"must use full batch\", \"use full batch sizes\"]):\n            return 0.2\n        return 0.0\n    except Exception:\n        return 0.0"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Operational Soundness (LLM)", "description": "Holistic LLM assessment of clarity, professionalism, and operational feasibility/optimization for a 24/7, two-dryer, staggered operation with 20 personnel.", "is_required": false, "max_points": 2.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Operational Feasibility and Throughput Optimization", "description": "LLM reviews whether the schedule, assignments, and sequences align to a feasible 24/7 plan, balance staffing across stages, and optimize two-dryer throughput with staggered cycles.", "weight": 1.2, "judge_prompt": "Evaluate the workbook for operational feasibility and throughput optimization. Consider:\n- Does the Work Schedule reflect a realistic 24/7 plan (e.g., feasible shift lengths and count) and capacity planning aligned with two dryers?\n- Are Production Assignments balanced across Raw Prep, Freeze Drying, and Packaging with clear role descriptions and staffing that can support continuous staggered operation?\n- Do the Production Sequences logically minimize idle time and bottlenecks, showing two dryers in overlapping but staggered cycles, with sensible sub-steps and responsible roles?\n- Are assumptions stated where needed (e.g., cycle time, batch size), and do they cohere across sheets?\n\nScoring guidance (out of 1.2):\n- 1.2: Plan appears feasible and well-optimized; staffing and sequences are coherent and support staggered two-dryer throughput.\n- 0.8: Generally feasible with minor inefficiencies or unclear assumptions.\n- 0.4: Significant feasibility gaps (e.g., obvious bottlenecks or misaligned staffing) but some useful content.\n- 0.0: Infeasible, contradictory, or missing key elements needed to run the operation.", "expectation": "A coherent, feasible plan that demonstrates staggered two-dryer operation and balanced staffing to hit targets."}, {"type": "llm_judge", "name": "Professional Presentation and Clarity", "description": "LLM evaluates formatting, clarity, and usability for supervisors and management.", "weight": 0.8, "judge_prompt": "Assess the workbook's professional presentation and clarity for production supervision:\n- Are sheets clearly labeled with readable headers, consistent units, and understandable tables?\n- Are notes/assumptions concise and helpful? Are role descriptions clear and action-oriented?\n- Is the sequencing view (timeline/table) scannable for shift leads (e.g., columns aligned, times formatted, dependencies understandable)?\n- Is the workbook obviously editable and shareable (no locked/nonstandard format)?\n\nScoring guidance (out of 0.8):\n- 0.8: Professional, clear, and immediately usable by supervisors.\n- 0.5: Generally clear with minor formatting issues.\n- 0.2: Hard to read or inconsistent formatting but some structure present.\n- 0.0: Poor presentation or unreadable.", "expectation": "Clean, consistent headers and units, readable tables, concise notes, and a supervisor-friendly sequencing layout."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "211d0093-2c64-4bd0-828c-0201f18924e7", "rubric": {"category_name": "Retail DTL (Daily Task List) - Department Supervisor", "rationale": "Pattern B (Document). The deliverable is a PDF form-like document that staff use throughout the day. Stage 1 uses an LLM judge to strictly enforce the document\u2019s structure (title, required sections, tables, sign-offs). Stage 2 mixes code and LLM to verify presence of key fields/keywords and process compliance, leveraging PDF text extraction. Stage 3 assesses professional usability and presentation quality for frontline staff in a retail environment.", "max_total_score": 10.0, "stages": [{"name": "Stage 1: Shape Enforcement Gate (Structure and Format)", "description": "LLM-only gate. Verifies the output is a PDF with the exact structural components required for a Daily Task List used by retail staff and supervisors.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured PDF DTL Requirements", "description": "Check the candidate\u2019s final deliverable is a PDF DTL with all required sections, tables, and sign-offs enabling verification and use on the sales floor.", "weight": 4.0, "judge_prompt": "You are evaluating whether the candidate produced a properly structured Daily Task List (DTL) document for a retail electronics department.\n\nCheck ONLY for FORMAT and STRUCTURE (not content quality). The output MUST be a PDF.\n\nRequired structural elements (be flexible with section names but strict about presence):\n\nA) Document Format\n- File is PDF (not Word, not Excel, not plain text)\n- Clear title indicating Daily Task List (e.g., \"Daily Task List\", \"Department Daily Checklist\", or similar)\n\nB) Header/Meta Section (near the top)\n- Department/Store name or department identifier\n- Date field\n- Manager on Duty field\n- Optional but helpful: Shift (e.g., Open/Mid/Close) and Prepared By\n\nC) Assignment Instructions section (a short paragraph/bullets) covering the daily workflow:\n- At the beginning of the day, the first employee assigns tasks evenly to all scheduled team members\n- Once a task is completed, the employee initials/signs the corresponding section\n- The manager signs off on completed tasks\n- The closing employee verifies all tasks are completed and files the DTL in the Manager\u2019s Office\n\nD) Task Assignments Table (core, must be obvious)\n- A table/grid that includes these columns (naming can vary slightly):\n  [Task | Assigned Employee | Start Time | Due Time/Deadline | Employee Initials/Signature | Manager Sign-off | Notes]\n- At least 10 task rows (blank rows are fine) so a typical day can be covered\n\nE) End-of-Day Verification section\n- A clear area for the closing employee to confirm all tasks completed and the DTL filed (include Closing Employee name/sign/date fields or equivalent)\n\nF) Manager Final Sign-off at the very end of the DTL\n- A distinct block for Manager Name (printed), Manager Signature, and Date\n\nScoring:\n- 4.0: PDF + all elements A\u2013F present (task table with the specified columns and 10+ rows)\n- 3.2: PDF + all core elements present but minor omissions (e.g., one missing minor header field or slightly fewer than 10 rows)\n- 2.0: PDF + some core parts missing (e.g., task table missing key columns OR missing End-of-Day Verification OR missing Manager Final Sign-off)\n- 0.0: Not a PDF, or multiple core structural elements missing, or no recognizable DTL structure\n\nOnly check presence/structure. Do not judge content accuracy or writing quality.", "expectation": "A single PDF with a clear DTL structure: header/meta, process instructions, a comprehensive task table with assignment/initials/manager sign-off/notes columns, an end-of-day verification block, and a final manager sign-off at the end."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2: Verification (Content Presence and Process Compliance)", "description": "Now that structure is enforced, verify correctness and completeness using code and an LLM judge. Focus on presence of required fields/keywords and process compliance language.", "is_required": false, "max_points": 5.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Core Fields Presence (Header + Sections)", "description": "Verify the PDF text contains core header fields and process elements (titles, date/department/manager, instructions), using flexible matching.", "weight": 1.5, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output:\n            return 0.0\n        if not output.is_document:\n            return 0.0\n        text = \"\"\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                text = \"\"\n        if not text:\n            return 0.0\n        t = text.lower()\n\n        # Header keywords\n        header_hits = 0\n        if \"daily task list\" in t or \"department daily checklist\" in t or \"dtl\" in t:\n            header_hits += 1\n        if \"date\" in t:\n            header_hits += 1\n        if \"department\" in t or \"store\" in t:\n            header_hits += 1\n        if \"manager\" in t and (\"manager on duty\" in t or \"manager:\" in t or \"manager on-duty\" in t):\n            header_hits += 1\n        header_score = min(header_hits / 4.0, 1.0)\n\n        # Instructions coverage (4 sub-points)\n        instr_points = 0\n        # a) beginning of day assignment\n        if (\"assign\" in t and \"task\" in t and (\"beginning\" in t or \"opening\" in t or \"first employee\" in t)) or (\"assigns tasks evenly\" in t):\n            instr_points += 1\n        # b) employees initial/sign upon completion\n        if (\"initial\" in t or \"sign\" in t) and (\"complete\" in t or \"completion\" in t):\n            instr_points += 1\n        # c) manager sign-off per task\n        if (\"manager\" in t) and (\"sign-off\" in t or \"sign off\" in t or \"signature\" in t or \"approve\" in t):\n            instr_points += 1\n        # d) closing employee verifies and files\n        if (\"closing\" in t or \"end-of-day\" in t or \"end of day\" in t) and (\"verify\" in t or \"verification\" in t) and (\"file\" in t or \"filing\" in t or \"cabinet\" in t):\n            instr_points += 1\n        instr_score = instr_points / 4.0\n\n        # Combine (equal weight header/instructions)\n        score = (header_score + instr_score) / 2.0\n        score = max(0.0, min(1.0, score))\n        return score\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Manager Final Sign-off Present", "description": "Check the PDF contains a distinct manager final sign-off area including Manager, Signature/Sign-off, and Date near each other.", "weight": 1.0, "code": "import re\n\ndef _window_has_all(t, anchor, keywords, window=160):\n    for m in re.finditer(anchor, t):\n        start = max(0, m.start() - window)\n        end = min(len(t), m.end() + window)\n        segment = t[start:end]\n        if all(k in segment for k in keywords):\n            return True\n    return False\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        text = \"\"\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                text = \"\"\n        if not text:\n            return 0.0\n        t = text.lower()\n        ok = _window_has_all(t, \"manager\", [\"sign\", \"date\"]) or _window_has_all(t, \"manager\", [\"signature\", \"date\"]) or _window_has_all(t, \"manager\", [\"sign-off\", \"date\"]) or _window_has_all(t, \"manager\", [\"sign off\", \"date\"]) \n        if ok:\n            return 1.0\n        # Partial credit if manager + (sign OR date) appear anywhere\n        if \"manager\" in t and (\"sign\" in t or \"signature\" in t or \"sign-off\" in t or \"sign off\" in t) and \"date\" in t:\n            return 0.6\n        return 0.0\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "PDF Format Confirmation", "description": "Confirm the primary output is actually a PDF file.", "weight": 0.5, "code": "def evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output:\n            return 0.0\n        if not output.is_document:\n            return 0.0\n        path = None\n        try:\n            path = context.files.get_path(output.id)\n        except Exception:\n            path = None\n        if not path:\n            return 0.0\n        suffix = str(path).lower().strip()\n        if suffix.endswith('.pdf'):\n            return 1.0\n        return 0.0\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Task Table Columns Presence", "description": "Verify the text likely contains all key task table column headers (flexible matching).", "weight": 1.0, "code": "def evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output or not output.is_document:\n            return 0.0\n        text = \"\"\n        try:\n            text = context.files.read_pdf_text(output.id)\n        except Exception:\n            try:\n                text = context.files.read_docx_text(output.id)\n            except Exception:\n                text = \"\"\n        if not text:\n            return 0.0\n        t = text.lower()\n        cols = [\n            [\"task\"],\n            [\"assigned\", \"employee\"],\n            [\"start\", \"time\"],\n            [\"due\", \"time\"],\n            [\"initial\"],\n            [\"manager\", \"sign\"],\n            [\"notes\"]\n        ]\n        hits = 0\n        for group in cols:\n            # group can be one or more tokens that should all appear\n            if all(any(token in t for token in [g]) for g in group):\n                hits += 1\n        ratio = hits / len(cols)\n        ratio = max(0.0, min(1.0, ratio))\n        return ratio\n    except Exception:\n        return 0.0"}, {"type": "llm_judge", "name": "Process Compliance Language", "description": "Check that the document\u2019s instructional language explicitly covers the daily workflow: initial assignment by first employee, employee initials on completion, manager sign-off per task, and closing employee verification/filing.", "weight": 1.0, "judge_prompt": "Evaluate whether the PDF\u2019s instructional text clearly documents the following four process requirements (flexible wording allowed):\n1) At the beginning of the day, the first/ opening employee assigns tasks evenly to scheduled team members.\n2) After completing a task, the assigned employee initials/signs.\n3) Manager signs off on completed tasks (per task or as clearly implied at the task level).\n4) The closing employee verifies all tasks are completed and files the DTL in the Manager\u2019s Office.\n\nScoring:\n- 1.0: All four process elements are present explicitly or with clearly equivalent language.\n- 0.7: Three elements clearly present.\n- 0.4: Two elements present.\n- 0.2: One element present.\n- 0.0: None present or instructions missing.\n\nFocus on presence of these statements only, not writing quality.", "expectation": "Concise instructions that cover all four required steps."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3: Quality and Usability Assessment", "description": "Holistic LLM assessment of professional presentation and frontline usability.", "is_required": false, "max_points": 1.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Usability", "description": "Evaluate whether the DTL is easy to use on the sales floor: clear layout, writable fields, legible fonts, adequate spacing, and intuitive flow.", "weight": 1.0, "judge_prompt": "Judge the PDF\u2019s practical usability for retail staff throughout a shift:\n- Clarity: Section headers are clear; the flow from header \u2192 instructions \u2192 task table \u2192 end-of-day \u2192 manager final sign-off is intuitive.\n- Usability: Sufficient space for handwriting names, initials, signatures, times, and notes; gridlines or clear field delineation; legible font sizes.\n- Professional polish: Consistent formatting, alignment, and labeling appropriate for a retail electronics department.\n\nScoring:\n- 1.0: Highly usable and professional; ample writable fields; clean, consistent layout.\n- 0.7: Generally usable with minor formatting/space issues.\n- 0.4: Usable but with notable layout or spacing problems.\n- 0.2: Barely usable; confusing layout or cramped fields.\n- 0.0: Not usable or severely unprofessional.\n\nDo not re-check structure already graded in Stage 1; focus on usability and presentation quality.", "expectation": "A neat, well-structured, form-like PDF that staff can easily fill out during the day."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "76d10872-9ffa-4ede-83ee-e0f1ec5e2b8d", "rubric": {"category_name": "Government \u2014 Child, Family, and School Social Workers \u2014 New Case Creation Report (Child Support Enforcement)", "rationale": "This rubric enforces a self-documenting, verifiable PDF report shaped by a Case Creation Guide. Stage 1 (LLM-only) strictly gates structure and format so later checks are trivial. Stage 2 mixes code (regex/bounds/consistency) with an LLM cross-check to verify correctness given the mandated shape. Stage 3 assesses professional quality and readiness for internal case initiation and enforcement use.", "max_total_score": 12.5, "stages": [{"name": "Stage 1 \u2014 Structure and Format Gate", "description": "LLM-only verification that the output is a properly structured New Case Creation Report PDF following the Case Creation Guide layout, with all required sections present to enable automated verification.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Case Report Format Requirement (PDF Gate)", "description": "Verify the candidate output is a PDF New Case Creation Report with all required sections and tables present, enabling downstream verification.", "weight": 4.0, "judge_prompt": "You are validating the SHAPE (not the content accuracy) of a New Case Creation Report for a child support enforcement case. Only check format/structure/presence. Do not judge correctness of values.\n\nOutput requirements:\n- Must be a PDF document (not Word, not Excel, not plain text)\n- At least 2 pages in length\n- Professional layout with visible section headers and tables where specified\n\nRequired Sections and Elements (be flexible with exact header names, but the intent must be clear):\n1) Cover/Header Block (first page):\n   - Title: \"New Case Creation Report\" or similar\n   - Case identifier (e.g., Case ID / DCS Case #)\n   - Case Open Date\n   - Preparer/Investigator name and organization/unit\n\n2) Parties \u2014 Custodial Parent (CP) and Non-Custodial Parent (NCP):\n   - For CP: name, at least one contact datum (address/phone/email), DOB\n   - For NCP: name, at least one contact datum, DOB\n   - Optional but encouraged: SSN (masked), relationship/marital status\n\n3) Children Table:\n   - Table with columns similar to: [Child Name | DOB | SSN (masked) | Sex]\n   - One row per child\n\n4) Paternity Section:\n   - Status per child: Established/Adjudicated/Pending/Not Required\n   - If established/adjudicated: method (e.g., DNA), test/result date, optional probability/test ID\n\n5) Order Details Section (from child support order):\n   - Court/Jurisdiction and Order Number\n   - Order Signed Date and Effective Date\n   - Support terms: Base Support Amount, frequency (weekly/biweekly/monthly), medical support and/or childcare support if applicable, arrears (if any)\n\n6) Employment/Income Verification:\n   - Employer name (if known), contact (address or phone), verification date\n   - Wage frequency and/or income amount if present\n\n7) Enforcement/Service Plan:\n   - Planned actions (e.g., IWO wage withholding), service method, next steps and target dates\n\n8) DCS System Entry Mapping Table:\n   - A table explicitly mapping system fields to values, e.g., columns like [DCS System Field | Value | Notes]\n   - Includes fields such as CP Name, NCP Name, Child entries, Order Number, Effective Date, Support Amount/Frequency\n\n9) Attachments/Document Log:\n   - List indicating inclusion/review of: case detail summary, paternity results, child support order (and any other relevant attachments)\n\n10) Sign-off/Certification:\n   - Preparer name/signature line and date; optional reviewer line\n\nOptional but acceptable extras: Narrative case notes, Appendix.\n\nScoring (STRUCTURE ONLY):\n- 1.0: PDF with all 10 required sections present, reasonably labeled; children and system mapping are in table form\n- 0.9: All required sections present except one minor element within a section (e.g., missing frequency label in Order Details), optional extras may be absent\n- 0.6: Missing exactly one required section OR not using a table where one is mandated (Children or DCS System Entry Mapping)\n- 0.3: Missing two required sections OR fewer than 2 pages\n- 0.0: Not a PDF OR grossly wrong format (missing three or more required sections)\n\nOnly evaluate structure/presence and basic layout, not correctness of the data.", "expectation": "A multi-page PDF with clear headers and tables covering Parties, Children, Paternity, Order Details, Employment/Income, Enforcement Plan, System Field Mapping, Attachments Log, and Sign-off."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Correctness and Consistency Verification", "description": "Automated and LLM checks for internal consistency, plausible formats, and cross-references enabled by the Stage 1 structure.", "is_required": false, "max_points": 5.5, "min_score_to_pass": 2.75, "rules": [{"type": "code", "name": "Field Format Sanity Checks", "description": "Check presence of key fields and plausible formats (dates, currency, names) across core sections.", "weight": 1.2, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output:\n            return 0.0\n        text = ''\n        if output.is_document:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    text = ''\n        elif output.is_text_format:\n            text = context.files.read_text(output.id)\n        if not text or len(text) < 200:\n            return 0.0\n        low = text.lower()\n\n        checks = []\n        # Presence of overall report identity\n        checks.append('case creation' in low or 'new case' in low)\n        # Case ID\n        checks.append('case id' in low or 'dcs case' in low or re.search(r'case\\s*(id|#|no\\.)', low) is not None)\n        # Case Open Date\n        date_pat = r\"\\b(0?[1-9]|1[0-2])[\\/-](0?[1-9]|[12][0-9]|3[01])[\\/-](19|20)\\d{2}\\b\"\n        checks.append(('case open' in low and re.search(date_pat, low) is not None))\n        # Preparer/Investigator\n        checks.append(('prepared by' in low) or ('preparer' in low) or ('investigator' in low))\n        # Parties: CP and NCP names\n        checks.append(('custodial parent' in low or 'cp' in low) and ('non-custodial parent' in low or 'ncp' in low))\n        # Children & DOB\n        child_block = ('child' in low) and ('dob' in low or 'date of birth' in low)\n        checks.append(child_block)\n        # Order effective date\n        eff = ('effective date' in low and re.search(date_pat, low) is not None)\n        checks.append(eff)\n        # Base support currency amount\n        cur_pat = r\"\\$\\s?\\d{1,3}(?:,\\d{3})*(?:\\.\\d{2})?\"\n        checks.append(('support' in low) and (re.search(cur_pat, text) is not None))\n\n        score = sum(1 for c in checks if c)\n        total = len(checks)\n        return score / total if total else 0.0\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "PII Masking and Contact Format Checks", "description": "Encourage privacy hygiene while verifying presence of usable contact formats (phone, ZIP). Penalize if unmasked SSNs are present without any masked form.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output:\n            return 0.0\n        text = ''\n        if output.is_document:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    text = ''\n        elif output.is_text_format:\n            text = context.files.read_text(output.id)\n        if not text or len(text) < 200:\n            return 0.0\n        low = text.lower()\n\n        masked_pat = re.compile(r\"(?:\\*{3}-\\*{2}-\\d{4}|x{3}-x{2}-\\d{4}|xxx-xx-\\d{4})\", re.I)\n        full_ssn_pat = re.compile(r\"\\b\\d{3}-\\d{2}-\\d{4}\\b\")\n        phone_pat = re.compile(r\"\\(?\\d{3}\\)?[\\s\\-.]?\\d{3}[\\s\\-.]?\\d{4}\")\n        zip_pat = re.compile(r\"\\b\\d{5}(?:-\\d{4})?\\b\")\n\n        has_masked = masked_pat.search(text) is not None\n        has_full = full_ssn_pat.search(text) is not None\n        has_phone = phone_pat.search(text) is not None\n        has_zip = zip_pat.search(text) is not None\n\n        # Scoring: 4 subchecks\n        # - has_phone (contact format)\n        # - has_zip (address format)\n        # - masked SSN present OR no SSNs at all\n        # - no unmasked SSNs (if unmasked present without any masked, penalize)\n        sub = []\n        sub.append(has_phone)\n        sub.append(has_zip)\n        # If any SSN artifacts at all\n        if has_full:\n            sub.append(False)  # prefer masked\n        else:\n            sub.append(True)   # no unmasked is acceptable\n        # Bonus credit if masked present\n        sub.append(has_masked or not has_full)\n\n        score = sum(1 for c in sub if c)\n        return score / len(sub)\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Order and Paternity Logical Consistency (Code)", "description": "Check for basic logical consistency between paternity status and order details; verify support frequency presence.", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output:\n            return 0.0\n        text = ''\n        if output.is_document:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    text = ''\n        elif output.is_text_format:\n            text = context.files.read_text(output.id)\n        if not text or len(text) < 200:\n            return 0.0\n        low = text.lower()\n        date_pat = r\"\\b(0?[1-9]|1[0-2])[\\/-](0?[1-9]|[12][0-9]|3[01])[\\/-](19|20)\\d{2}\\b\"\n\n        subs = []\n        # Frequency present near support\n        freq_ok = re.search(r\"(weekly|biweekly|semi\\-?monthly|monthly|quarterly)\", low) is not None\n        subs.append(freq_ok)\n\n        # Paternity established implies some evidence keywords\n        if 'paternity' in low and ('established' in low or 'adjudicated' in low):\n            evidence = any(k in low for k in ['dna', 'genetic', 'probability', 'test id', 'lab'])\n            subs.append(evidence)\n        else:\n            subs.append(True)  # not applicable\n\n        # Paternity pending should not simultaneously show finalized order signed/effective dates\n        pending = 'paternity' in low and 'pending' in low\n        has_signed = ('order signed' in low and re.search(date_pat, low) is not None)\n        has_effective = ('effective date' in low and re.search(date_pat, low) is not None)\n        if pending and (has_signed or has_effective):\n            subs.append(False)\n        else:\n            subs.append(True)\n\n        return sum(1 for c in subs if c) / len(subs)\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "DCS System Mapping Table Presence", "description": "Verify the presence of a system entry mapping table that lists DCS system fields and values.", "weight": 0.6, "code": "import re\n\ndef evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output:\n            return 0.0\n        text = ''\n        if output.is_document:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    text = ''\n        elif output.is_text_format:\n            text = context.files.read_text(output.id)\n        if not text or len(text) < 200:\n            return 0.0\n        low = text.lower()\n\n        has_table_header = ('system field' in low or 'dcs system field' in low or 'system entry' in low)\n        keys = ['cp name', 'ncp name', 'order number', 'effective date', 'support amount', 'frequency']\n        found = sum(1 for k in keys if k in low)\n        # Require header plus at least 3 key fields\n        if has_table_header:\n            ratio = min(1.0, 0.2 + (found / max(1, len(keys))))\n        else:\n            ratio = 0.0\n        return ratio\n    except Exception:\n        return 0.0"}, {"type": "code", "name": "Attachments/Document Log Presence", "description": "Confirm that the report logs referenced attachments: case detail summary, paternity results, child support order.", "weight": 0.4, "code": "def evaluate(workflow, context):\n    try:\n        output = context.get_primary_output()\n        if not output:\n            return 0.0\n        text = ''\n        if output.is_document:\n            try:\n                text = context.files.read_pdf_text(output.id)\n            except Exception:\n                try:\n                    text = context.files.read_docx_text(output.id)\n                except Exception:\n                    text = ''\n        elif output.is_text_format:\n            text = context.files.read_text(output.id)\n        if not text or len(text) < 200:\n            return 0.0\n        low = text.lower()\n        base = ('attachments' in low) or ('document log' in low) or ('attachment log' in low)\n        items = 0\n        for term in ['case detail summary', 'paternity results', 'child support order']:\n            if term in low:\n                items += 1\n        if not base:\n            return 0.0\n        return items / 3.0\n    except Exception:\n        return 0.0"}, {"type": "llm_judge", "name": "Cross-Section Consistency (LLM)", "description": "Check whether values that should match across sections are consistent (e.g., support amount and frequency in Order Details vs. System Mapping; parties/children names across sections; paternity statuses across children and references).", "weight": 1.5, "judge_prompt": "Now that the document shape is valid, check for internal consistency across sections. Focus on whether repeated values and references match. Do not score presentation aesthetics here.\n\nConsistency items to verify:\n- Base support amount and payment frequency in Order Details match the values mapped in the DCS System Entry table\n- Order number and effective date are the same wherever repeated (e.g., header/order section vs. mapping)\n- Parties: CP and NCP names are consistent across Parties, Order Details, and any enforcement plan references\n- Children: the number of children listed equals the number referenced elsewhere; child names and DOBs are consistent wherever repeated\n- Paternity: status per child in Paternity Section aligns with any narrative mentions; if established/adjudicated, the result/test references are coherent\n- Employer: employer named in Employment/Income appears (or is referenced) in Enforcement Plan for IWO if wage withholding is planned\n\nScoring:\n- 1.0: All cross-references consistent; any minor difference is clearly typographical and non-substantive\n- 0.7: One minor inconsistency found (e.g., a missing frequency label in one place but present elsewhere)\n- 0.4: Multiple minor inconsistencies or one significant mismatch (e.g., support amount differs)\n- 0.1: Major inconsistencies across several sections\n- 0.0: Values are broadly inconsistent or incomparable (e.g., different parties/children/order numbers)\n\nOnly consider consistency across the document; do not evaluate against external truth or quality of prose.", "expectation": "All repeated fields align across sections; amounts/dates/names match exactly between Order Details and System Mapping; references to employer and children are coherent."}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Readiness", "description": "LLM-only evaluation of presentation quality, template adherence polish, and readiness for internal record-keeping and review.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Readiness", "description": "Assess whether the report is professionally formatted, readable, and ready for filing and internal review.", "weight": 2.0, "judge_prompt": "Assess the final document for professional presentation and readiness:\n- Clarity and readability: headings, spacing, fonts, and table readability\n- Logical flow mirrors the Case Creation Guide; sections ordered sensibly from header to sign-off\n- Tables (Children, System Mapping) are well-structured, with clear headers and legible entries\n- Dates, amounts, and identifiers are labeled and easy to find\n- Overall, the report appears ready for inclusion in the formal case file and to initiate enforcement\n\nScoring:\n- 1.0: Highly professional and ready with no adjustments needed\n- 0.7: Minor formatting or clarity issues that do not impede use\n- 0.4: Noticeable formatting issues that slow review but are fixable\n- 0.2: Poorly presented; would require significant edits before filing\n- 0.0: Unusable presentation", "expectation": "A clean, professional, multi-section PDF with clear tables and labels, suitable for immediate filing."}, {"type": "llm_judge", "name": "Template Adherence and Appropriateness", "description": "Evaluate how well the layout adheres to the Case Creation Guide and whether the content level is appropriate for internal DCS use.", "weight": 1.0, "judge_prompt": "Evaluate adherence to the Case Creation Guide\u2019s layout and the appropriateness of the content for internal DCS operations:\n- Section headers and ordering closely follow the Guide (allow minor naming differences)\n- Required categories are populated; optional extras don\u2019t distract from core data\n- Information density is appropriate (not overly verbose, not too sparse)\n- Sensitive data, if included, is purposeful for internal processing\n\nScoring:\n- 1.0: Strong adherence; content is crisply tailored for internal use\n- 0.7: Minor deviations from the Guide or slight over/under-detailing\n- 0.4: Several deviations or content that makes system entry less straightforward\n- 0.0: Largely ignores the Guide or content is not appropriate for internal use", "expectation": "A Guide-aligned, concise, operations-ready report that supports accurate system entry and enforcement setup."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "02314fc6-a24e-42f4-a8cd-362cae0f0ec1", "rubric": {"category_name": "Retail Store Safety Checklist (Monthly)", "rationale": "This rubric enforces a self-documenting, verifiable PDF checklist. Stage 1 (LLM gate) mandates an exact, review-ready structure with sectioned checklists, scoring summary, and corrective action plan. Stage 2 (code) verifies key metadata, headings, policy threshold logic, and presence of corrective action fields via robust text extraction. Stage 3 (LLM) evaluates professional quality, clarity, and operational usefulness for GM/DM/LP review.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2014 PDF Format and Structural Gate", "description": "LLM verifies the deliverable is a properly structured, multi-section PDF safety checklist with scoring and follow-up components.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.0, "rules": [{"type": "llm_judge", "name": "Structured PDF Safety Checklist Requirements", "description": "Confirm the output is a PDF with all required sections, tables, scoring, and follow-up structure for monthly store safety checks.", "weight": 6.0, "judge_prompt": "You are evaluating whether the candidate output is a properly structured PDF safety checklist for monthly store operations. Only check presence and structure (not correctness of content). Be flexible with naming but strict about structure.\n\nFORMAT REQUIREMENTS\n- Must be a PDF (not Word, not Excel, not plain text)\n- At least 2 pages\n- Professional layout with clear section headers and tables\n- Front-matter header with operational identifiers:\n  \u2022 Store Name and Store ID/Number\n  \u2022 Location/Address (City/State acceptable)\n  \u2022 Month/Year (or Date) of inspection\n  \u2022 Safety Coordinator name (and optional signature/date)\n  \u2022 GM and DM names (and signature lines) and LP contact/recipient\n  \u2022 Distribution note indicating submission to GM, DM, and LP\n\nSECTION REQUIREMENTS (7 safety areas + scoring/follow-up)\nEach of the following must appear as a clearly labeled section with a checklist table. Section names can vary slightly but intent must be obvious.\n1) Parking Lot, Sidewalks & Ramps\n2) General Store Conditions\n3) First Aid & Emergency Procedures\n4) Safety and Compliance\n5) Food Safety\n6) Fire Prevention and Protection\n7) Record Keeping & Posters\n\nChecklist Table Structure (for each section):\n- A tabular checklist with item rows and columns that enable verification. Accept any clear equivalents. Expected columns include:\n  \u2022 Item/Requirement (or Checklist Item)\n  \u2022 Status with discrete choices (Yes/No/NA or Compliant/Non-Compliant/NA)\n  \u2022 Severity/Risk Level (High/Med/Low) \u2014 optional but preferred\n  \u2022 Notes/Evidence (and optional Photo Ref/Attachment)\n  \u2022 Responsible Owner/Assignee\n- Each section should also include a brief per-section summary or counters: at least Items Reviewed, N/A count, and Non-Compliant/Missed count.\n\nGLOBAL SCORING + FOLLOW-UP REQUIREMENTS\n- A consolidated scoring summary including:\n  \u2022 Total Items Reviewed\n  \u2022 Total N/A\n  \u2022 Total Missed/Non-Compliant\n  \u2022 A clear policy statement: \u201cStores can miss up to 10 items; more than 10 requires a Corrective Action Plan to the District Manager.\u201d\n- A Corrective Action Plan (CAP) section with a table for each missed item, including fields such as: Issue/Hazard, Location, Risk/Severity, Corrective Action, Owner/Responsible, Due Date/Target Date, Status, Completion Date, Verification/Sign-off.\n- DM follow-up statement (e.g., DM will follow up to ensure CAP completion).\n- Final sign-off section (Safety Coordinator, GM, DM, and optionally LP) with names/signatures/dates.\n- Optional: Appendix or placeholders for photo evidence.\n\nSCORING (structure only):\n- 6.0: PDF + all 7 key-area sections each with a checklist table; per-section mini-summaries; global scoring summary with the 10-item policy; CAP table; DM follow-up note; sign-offs; front-matter identifiers; distribution to GM/DM/LP.\n- 5.0: As above, but missing 1\u20132 minor optional elements (e.g., photo appendix, severity column, or a couple of signature lines) while all core structures exist.\n- 4.0: All 7 key-area checklist sections present and a global summary present, but missing 1\u20132 core elements (e.g., per-section mini-summaries or CAP table or explicit 10-item policy wording) while still clearly a structured PDF \u22652 pages.\n- 2.0: Partial structure (e.g., less than 7 sections or mostly narrative without tables) but still a PDF with some checklist elements.\n- 0.0: Not a PDF, or lacks recognizable checklist structure.\n\nOnly check presence/structure; do not judge correctness of items or scores.", "expectation": "A multi-page PDF with front-matter identifiers, seven sectioned checklists with tables and summaries, a consolidated scoring summary with the 10-item policy, a corrective action plan table, DM follow-up note, and final sign-offs."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Verification: Policy and Content Checks (Code)", "description": "Code-based checks validate presence of key metadata, headings, scoring logic, and corrective action fields in the PDF text.", "is_required": false, "max_points": 3.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Metadata and Routing Fields Present", "description": "Verify presence of store identifiers, date/month, and routing fields for GM/DM/LP and Safety Coordinator.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Checks for key metadata/routing fields in the PDF/DOCX text.\n    Returns up to 0.8 points.\n    \"\"\"\n    MAX_POINTS = 0.8\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n\n    if not text:\n        return 0.0\n\n    t = text.lower()\n\n    # Define requirement groups with alternative patterns (flexible matching)\n    reqs = {\n        'store_id_or_number': [r\"store\\s*(id|number|#)\"],\n        'store_name': [r\"store name\", r\"store:\\s*\"],\n        'location_or_address': [r\"address\", r\"location\", r\"city\", r\"state\"],\n        'date_or_month': [r\"\\bdate\\b\", r\"\\bmonth\\b\", r\"month/year\", r\"inspection date\"],\n        'safety_coordinator': [r\"safety coordinator\", r\"prepared by\"],\n        'gm': [r\"general manager\", r\"\\bgm\\b\"],\n        'dm': [r\"district manager\", r\"\\bdm\\b\"],\n        'lp': [r\"loss prevention\", r\"\\blp\\b\"],\n    }\n\n    hits = 0\n    missing = []\n    for key, patterns in reqs.items():\n        found = any(re.search(p, t) for p in patterns)\n        if found:\n            hits += 1\n        else:\n            missing.append(key)\n\n    score = (hits / len(reqs)) * MAX_POINTS\n    feedback = f\"Found {hits}/{len(reqs)} metadata fields. Missing: {', '.join(missing) if missing else 'none'}.\"\n    return (score, feedback)\n"}, {"type": "code", "name": "Key Area Headings Present", "description": "Verify all seven key area headings appear (flexible synonyms).", "weight": 1.0, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Checks for presence of the 7 required key area sections via fuzzy matching in PDF/DOCX text.\n    Returns up to 1.0 point.\n    \"\"\"\n    MAX_POINTS = 1.0\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n\n    if not text:\n        return 0.0\n\n    t = text.lower()\n\n    def present_any(patterns):\n        return any(re.search(p, t) for p in patterns)\n\n    checks = []\n\n    # 1) Parking Lot, Sidewalks & Ramps\n    checks.append(\n        (\"parking/sidewalks/ramps\",\n         (\"parking lot\" in t) and (\"sidewalk\" in t or \"ramp\" in t))\n    )\n\n    # 2) General Store Conditions\n    checks.append((\"general store conditions\", present_any([r\"general store condition\", r\"store condition\", r\"general conditions\"])) )\n\n    # 3) First Aid & Emergency Procedures\n    checks.append((\"first aid & emergency\", present_any([r\"first aid\", r\"emergency procedures\", r\"emergency response\"])) )\n\n    # 4) Safety and Compliance\n    checks.append((\"safety and compliance\", present_any([r\"safety and compliance\", r\"safety\\s+.*compliance\", r\"compliance checklist\"])) )\n\n    # 5) Food Safety\n    checks.append((\"food safety\", present_any([r\"food safety\"])) )\n\n    # 6) Fire Prevention and Protection\n    checks.append((\"fire prevention & protection\", present_any([r\"fire prevention\", r\"fire protection\"])) )\n\n    # 7) Record Keeping & Posters\n    checks.append((\"record keeping & posters\", present_any([r\"record keeping\", r\"posters\"])) )\n\n    passed = [name for name, ok in checks if ok]\n    missing = [name for name, ok in checks if not ok]\n\n    score = (len(passed) / len(checks)) * MAX_POINTS\n    feedback = f\"Key areas present: {', '.join(passed) if passed else 'none'}; Missing: {', '.join(missing) if missing else 'none'}.\"\n    return (score, feedback)\n"}, {"type": "code", "name": "Scoring Logic and 10-Item Threshold Enforcement", "description": "Verify presence of missed-item counting and the policy linkage to the corrective action plan and DM follow-up.", "weight": 0.8, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Parses PDF/DOCX text to detect 'missed items' count and ensure the >10 rule triggers CAP + DM follow-up.\n    Returns up to 0.8 points.\n    \"\"\"\n    MAX_POINTS = 0.8\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n\n    if not text:\n        return 0.0\n\n    t = text.lower()\n    score = 0.0\n    details = []\n\n    # Try to extract a specific missed-items count\n    m = re.search(r\"items?\\s*(missed|non[- ]?compliant|fail(?:ed)?)\\s*[:\\-]?\\s*(\\d{1,4})\", t)\n    cap_present = bool(re.search(r\"corrective action( plan)?|\\bcap\\b\", t))\n    dm_follow = bool(re.search(r\"district manager|\\bdm\\b\", t) and re.search(r\"follow\\s*-?up|verify|ensure completion\", t))\n    policy_stmt = bool(re.search(r\"(up to|no more than|<=?)\\s*10\\s*(items|misses)|more than\\s*10\\s*(items|misses)\\s*(requires|need|must)\", t))\n\n    if m:\n        try:\n            missed = int(m.group(2))\n        except Exception:\n            missed = None\n        if missed is not None and missed > 10:\n            # Require CAP + DM follow-up for full credit\n            if cap_present and dm_follow:\n                score = MAX_POINTS\n                details.append(f\"Missed={missed} (>10) with CAP and DM follow-up found.\")\n            elif cap_present:\n                score = MAX_POINTS * 0.6\n                details.append(f\"Missed={missed} (>10); CAP found but DM follow-up not detected.\")\n            else:\n                score = 0.0\n                details.append(f\"Missed={missed} (>10) but CAP not detected.\")\n        elif missed is not None:\n            # <=10: look for policy statement and follow-up scheduling language\n            sched = bool(re.search(r\"schedule|due date|target date|timeline\", t))\n            base = 0.6 if policy_stmt else 0.4\n            if sched:\n                base += 0.2\n            score = min(MAX_POINTS, base)\n            details.append(f\"Missed={missed} (<=10). Policy stmt={policy_stmt}, scheduling={sched}.\")\n        else:\n            # Could not parse number, fallback minimal\n            score = MAX_POINTS * 0.2\n            details.append(\"Missed count pattern found but not parseable.\")\n    else:\n        # No explicit missed count \u2014 look for global summary indicators\n        has_summary = bool(re.search(r\"(overall|global)\\s*(summary|score)|total\\s*(missed|non[- ]?compliant)\", t))\n        base = 0.2 if has_summary else 0.0\n        if policy_stmt:\n            base += 0.2\n        if cap_present:\n            base += 0.2\n        score = min(MAX_POINTS, base)\n        details.append(f\"No explicit 'missed' count; summary={has_summary}, policy={policy_stmt}, CAP={cap_present}.\")\n\n    feedback = \"; \".join(details)\n    return (score, feedback)\n"}, {"type": "code", "name": "Corrective Action Plan Table Fields", "description": "Verify CAP section includes fields conducive to follow-up and verification.", "weight": 0.4, "code": "import re\n\ndef evaluate(workflow, context):\n    \"\"\"\n    Checks for presence of CAP table field names (flexible synonyms) in PDF/DOCX text.\n    Returns up to 0.4 points.\n    \"\"\"\n    MAX_POINTS = 0.4\n    output = context.get_primary_output()\n    if not output or not output.is_document:\n        return 0.0\n\n    text = \"\"\n    try:\n        text = context.files.read_pdf_text(output.id)\n    except Exception:\n        try:\n            text = context.files.read_docx_text(output.id)\n        except Exception:\n            return 0.0\n\n    if not text:\n        return 0.0\n\n    t = text.lower()\n\n    # Field categories with synonyms\n    field_groups = {\n        'issue_hazard': [r\"issue\", r\"hazard\", r\"finding\"],\n        'location': [r\"location\", r\"area\", r\"department\"],\n        'risk_severity': [r\"risk\", r\"severity\", r\"priority\"],\n        'corrective_action': [r\"corrective action\", r\"action plan\", r\"remediation\"],\n        'owner_responsible': [r\"owner\", r\"responsible\", r\"assignee\"],\n        'due_date': [r\"due date\", r\"target date\", r\"deadline\"],\n        'status': [r\"status\", r\"open\", r\"closed\", r\"in progress\"],\n        'completion_date': [r\"completion date\", r\"completed\", r\"closed date\"],\n        'verification': [r\"verification\", r\"verified\", r\"sign-off\", r\"approval\"],\n    }\n\n    hits = 0\n    for grp, pats in field_groups.items():\n        if any(re.search(p, t) for p in pats):\n            hits += 1\n\n    ratio = hits / len(field_groups)\n    score = ratio * MAX_POINTS\n    feedback = f\"CAP fields present: {hits}/{len(field_groups)} (Issue, Location, Risk, Corrective Action, Owner, Due Date, Status, Completion, Verification).\"\n    return (score, feedback)\n"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Professional Quality and Usability (LLM)", "description": "LLM evaluates clarity, professionalism, and operational usability for monthly GM/DM/LP review and follow-up.", "is_required": false, "max_points": 1.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Presentation and Operational Usability", "description": "Assess professional formatting, clarity, actionability, and alignment with monthly safety operations.", "weight": 1.0, "judge_prompt": "Evaluate the PDF\u2019s overall professional quality and operational usability (not correctness of specific items). Consider:\n- Professional formatting: clear headers, consistent tables, legible fonts, pagination, and logical flow\n- Clarity: unambiguous checklist items; status choices are obvious; guidance and instructions are concise\n- Actionability: CAP section is easy to use; fields support ownership, deadlines, and verification\n- Risk awareness: severity/priority coding is present or risk handling is clear\n- Monthly cadence and routing: mentions monthly cycle and recipients (GM, DM, LP); includes sign-offs and dates\n- Completeness for store operations: suitable for floor use and leadership review; photo evidence references optional but helpful\n\nScoring:\n- 1.0: Professional, clear, and highly usable for monthly safety ops; easy CAP execution; strong alignment with GM/DM/LP needs\n- 0.7: Generally professional and usable; minor clarity/layout issues\n- 0.4: Usable with noticeable gaps (confusing labels, missing cues for action/verification)\n- 0.0: Poorly formatted or not practically usable for store ops follow-up", "expectation": "A clean, practical checklist document that store teams and leadership can readily use to track, act, and verify safety compliance each month."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "15d37511-75c5-4c7f-81f1-16e00c0d95f3", "rubric": {"category_name": "Wholesale Trade \u2014 Enterprise Sales Financial Model (UV Devices)", "rationale": "This rubric enforces a self-documenting, verifiable spreadsheet for Year 1 revenue and gross margin projections across two UV device products and their consumables, incorporating the client\u2019s 2,000-unit volume and tiered pricing. Stage 1 (LLM) strictly gates the file\u2019s structure. Stage 2 (code) verifies calculations, consistency, and inclusion of consumables and tier definitions. Stage 3 (LLM) assesses presentation quality and executive readiness.", "max_total_score": 20.0, "stages": [{"name": "Stage 1 \u2014 Structured Spreadsheet Format Gate", "description": "LLM checks that the output is an Excel workbook with the exact structural elements required to enable verification. This is a required gate. No code in this stage.", "is_required": true, "max_points": 6.0, "min_score_to_pass": 4.5, "rules": [{"type": "llm_judge", "name": "Structured Output Format Requirement", "description": "Output must be a clearly structured Excel workbook with specified sheets, sections, and tables enabling simple verification of Year 1 projection, assumptions, and tiered pricing.", "weight": 6.0, "judge_prompt": "You are evaluating whether the candidate produced a properly structured Excel workbook for a Year 1 revenue and gross margin projection for VerteCleanUV products in GloNGroRealEstate\u2019s marketplace.\n\nFORMAT REQUIREMENTS (GATE \u2014 must be Excel, not CSV/Google link/inline text):\n- File type: Excel (.xlsx). Not PDF, not DOCX, not CSV, not plain text.\n- Workbook has at least two sheets:\n  1) A main projection sheet named \"Year 1 Projection\" (or close variants like \"Yr1 Projection\", \"Year 1 Summary\", \"Projection\"), and\n  2) An assumptions/pricing sheet named \"Pricing & Assumptions\" (or close variants like \"Assumptions\", \"Pricing and Assumptions\").\n\nSHEET 1: Year 1 Projection (core table)\n- Contains a single clear tabular area with these columns visible (flexible naming allowed; must be obviously mappable):\n  \u2022 Product Name\n  \u2022 Quantity (Units)\n  \u2022 Unit Retail Price\n  \u2022 Unit Cost (GloNGroRealEstate product cost)\n  \u2022 Margin $ per Unit\n  \u2022 Margin %\n  \u2022 Total Gross Margin $\n- Must include rows for BOTH devices:\n  \u2022 BrightzoneUV Duct (HVAC duct device)\n  \u2022 BrightzoneUV Ceiling (ceiling-mounted device)\n- Must also include at least one row for annual consumables (e.g., lamp/bulb/filter/cartridge/consumable) associated with the devices.\n- Must show a clear Year 1 Total Gross Margin (a visible total/summary for the year) on this sheet.\n\nSHEET 2: Pricing & Assumptions\n- Includes a visible section for client volume assumptions stating Year 1 total device volume of 2,000 units (combined across the two devices) and an allocation assumption between products (any reasonable split).\n- Includes a visible section for Tiered Pricing that defines two tiers: \"< 1,000 units\" and \">= 1,000 units\" (or equivalent wording), with an indication that a discount applies when the >=1,000 tier is met in a fiscal year.\n- Includes input tables/fields for Retail Price and Product Cost for each device and for their annual consumables (values may be placeholders if the referenced email is unavailable, but the structure must be present).\n- Includes a brief source note referencing the \"Pricing email\" as the information source for pricing and costs.\n\nSCORING (structure only, do not evaluate mathematical correctness):\n- 6.0: Valid .xlsx + both sheets present with clearly labeled sections; projection has all required columns; both device rows + at least one consumables row; Year 1 Total Gross Margin visible; assumptions show 2,000 total device units and tiered pricing with two tiers.\n- 4.5: Minor deviations in sheet names/labels but clearly equivalent; all core elements present; totals and tiers clearly shown.\n- 3.0: Mostly present but missing one major element (e.g., no consumables row OR no explicit 2,000-unit assumption OR no visible total).\n- 1.5: Workbook exists but multiple structural gaps (e.g., missing one of the two sheets, several missing columns, unclear sections).\n- 0.0: Not an Excel file OR lacks the projection sheet OR the assumptions/tiered pricing sheet, making verification impossible.\n\nOnly assess presence/structure and clear labeling, not the accuracy of numbers or calculations.", "expectation": "A two-sheet Excel model: a projection table with required columns and rows for both devices and consumables, plus an assumptions sheet with the 2,000-unit volume, tiered pricing tiers (<1000, >=1000), price/cost inputs, and a Year 1 total gross margin shown on the projection sheet."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2014 Calculation and Consistency Verification", "description": "Code-based checks validate that the structured workbook performs coherent calculations, includes the required items, and is internally consistent with the assumptions.", "is_required": false, "max_points": 10.0, "min_score_to_pass": 0.0, "rules": [{"type": "code", "name": "Column Mapping and Numeric Validity", "description": "Verify presence of mappable required columns in the projection sheet and that key fields are numeric and non-negative where appropriate.", "weight": 1.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output found.\"\n\n    # Try loading Excel and identifying the projection sheet\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        sheet_candidates = [s for s in xls.sheet_names]\n        proj_name = None\n        for s in sheet_candidates:\n            sl = s.lower()\n            if ('projection' in sl) or ('year' in sl and '1' in sl) or ('summary' in sl):\n                proj_name = s\n                break\n        if proj_name is None:\n            proj_name = sheet_candidates[0]\n        df = pd.read_excel(path, sheet_name=proj_name)\n    except Exception as e:\n        return 0.0, f\"Failed to read Excel: {e}\"\n\n    if df.shape[1] < 3 or df.shape[0] == 0:\n        return 0.0, \"Projection sheet appears empty or too few columns.\"\n\n    # Normalize columns\n    cols = [str(c).strip().lower() for c in df.columns]\n    df.columns = cols\n\n    # Candidate column mappings (flexible)\n    candidates = {\n        'name': ['product name','product','item','name','sku'],\n        'qty': ['quantity','qty','units','unit qty','unit quantity'],\n        'price': ['unit retail price','retail price','unit price','price','marketplace price','proposed marketplace retail pricing'],\n        'cost': ['unit cost','cost','product cost','glo cost','glo ngro cost','glo real estate product cost'],\n        'unit_margin': ['margin $ per unit','unit margin','gross margin per unit','margin per unit','gm per unit'],\n        'margin_pct': ['margin %','margin percent','gm %','gross margin %','margin pct'],\n        'total_gm': ['total gross margin $','gross margin total','total margin','extended margin','total gm','total gross margin']\n    }\n\n    mapping = {}\n    for key, opts in candidates.items():\n        found = None\n        for o in opts:\n            for c in df.columns:\n                if o in c:\n                    found = c\n                    break\n            if found:\n                break\n        if found:\n            mapping[key] = found\n\n    required_keys = ['name','qty','price','cost','unit_margin','margin_pct','total_gm']\n    have = [k for k in required_keys if k in mapping]\n    # Score partially based on how many required columns are mappable\n    col_score = len(have)/len(required_keys)\n\n    # Numeric checks\n    num_score = 0.0\n    feedback = []\n    try:\n        # Coerce numeric for detected numeric fields, ignore errors\n        for k in ['qty','price','cost','unit_margin','margin_pct','total_gm']:\n            if k in mapping:\n                series = df[mapping[k]].astype(str)\n                series = series.str.replace(r'[%$,()]', '', regex=True).str.replace(',', '', regex=False)\n                df[mapping[k]] = pd.to_numeric(series, errors='coerce')\n        # Reasonable numeric validity: non-negative qty/price/cost; margin pct finite\n        checks = []\n        if 'qty' in mapping:\n            checks.append((df[mapping['qty']] >= 0).sum()/len(df))\n        if 'price' in mapping:\n            checks.append((df[mapping['price']] >= 0).sum()/len(df))\n        if 'cost' in mapping:\n            checks.append((df[mapping['cost']] >= 0).sum()/len(df))\n        if 'margin_pct' in mapping:\n            pct = df[mapping['margin_pct']]\n            good = pct.replace([np.inf, -np.inf], np.nan).notna().sum()/len(df)\n            checks.append(good)\n        num_score = np.mean(checks) if checks else 0.0\n    except Exception as e:\n        feedback.append(f\"Numeric coercion/validity issue: {e}\")\n        num_score = 0.0\n\n    # Weighted average within this rule: 60% columns, 40% numeric validity\n    score = 0.6*col_score + 0.4*num_score\n    score = float(max(0.0, min(1.0, score)))\n    fb = f\"Columns mapped: {len(have)}/{len(required_keys)}; Numeric validity approx: {num_score:.2f}.\"\n    if feedback:\n        fb += \" \" + \" \".join(feedback)\n    return score, fb"}, {"type": "code", "name": "Both Device Rows Present and Device Quantity Totals 2,000", "description": "Confirm rows for both BrightzoneUV devices exist and that their combined Year 1 device quantity equals 2,000 units (consumables excluded).", "weight": 2.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        # pick projection-like sheet\n        proj = None\n        for s in xls.sheet_names:\n            sl = s.lower()\n            if ('projection' in sl) or ('year' in sl and '1' in sl) or ('summary' in sl):\n                proj = s\n                break\n        if proj is None:\n            proj = xls.sheet_names[0]\n        df = pd.read_excel(path, sheet_name=proj)\n    except Exception as e:\n        return 0.0, f\"Read error: {e}\"\n\n    if df.empty:\n        return 0.0, \"Projection sheet empty.\"\n\n    # Normalize\n    df_cols = [str(c).strip().lower() for c in df.columns]\n    df.columns = df_cols\n\n    # Locate product name and quantity columns\n    name_col = None\n    qty_col = None\n    for c in df.columns:\n        if any(k in c for k in ['product name','product','item','name','sku']):\n            name_col = c if name_col is None else name_col\n        if any(k in c for k in ['quantity','qty','units']):\n            qty_col = c if qty_col is None else qty_col\n    if name_col is None or qty_col is None:\n        return 0.0, \"Missing product name or quantity column.\"\n\n    # Clean qty\n    q = df[qty_col].astype(str).str.replace(r'[%$,()]', '', regex=True).str.replace(',', '', regex=False)\n    qty = pd.to_numeric(q, errors='coerce').fillna(0)\n    names = df[name_col].astype(str).str.lower()\n\n    # Identify device rows\n    is_duct = names.str.contains('brightzoneuv') & names.str.contains('duct') | names.str.contains('hvac duct')\n    is_ceiling = names.str.contains('brightzoneuv') & names.str.contains('ceiling')\n\n    has_duct = bool(is_duct.any())\n    has_ceiling = bool(is_ceiling.any())\n\n    device_qty = qty[is_duct | is_ceiling].sum()\n\n    # Score components\n    comp = 0\n    comp += 1 if has_duct else 0\n    comp += 1 if has_ceiling else 0\n    comp += 1 if abs(device_qty - 2000) < 1e-6 else 0\n    score = comp/3.0\n\n    fb = f\"Devices present \u2014 Duct: {has_duct}, Ceiling: {has_ceiling}; Device qty total: {device_qty}.\"\n    return score, fb"}, {"type": "code", "name": "Row-Level Margin Math Checks", "description": "Verify Unit Margin \u2248 Unit Retail Price \u2212 Unit Cost, and Total Gross Margin \u2248 Unit Margin \u00d7 Quantity, within a small tolerance for rows that have needed values.", "weight": 2.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        proj = None\n        for s in xls.sheet_names:\n            sl = s.lower()\n            if ('projection' in sl) or ('year' in sl and '1' in sl) or ('summary' in sl):\n                proj = s\n                break\n        if proj is None:\n            proj = xls.sheet_names[0]\n        df = pd.read_excel(path, sheet_name=proj)\n    except Exception:\n        return 0.0\n\n    if df.empty:\n        return 0.0\n\n    # Map columns flexibly\n    df.columns = [str(c).strip().lower() for c in df.columns]\n    def find(colopts):\n        for o in colopts:\n            for c in df.columns:\n                if o in c:\n                    return c\n        return None\n    name_c = find(['product name','product','item','name','sku'])\n    qty_c = find(['quantity','qty','units'])\n    price_c = find(['unit retail price','retail price','unit price','price'])\n    cost_c = find(['unit cost','product cost','cost'])\n    unitm_c = find(['margin $ per unit','unit margin','gross margin per unit','margin per unit'])\n    totalm_c = find(['total gross margin $','gross margin total','total margin','extended margin','total gm'])\n\n    required = [qty_c, price_c, cost_c, unitm_c, totalm_c]\n    if any(c is None for c in required):\n        return 0.0\n\n    def to_num(s):\n        return pd.to_numeric(pd.Series(s).astype(str).str.replace(r'[%,$()]', '', regex=True).str.replace(',', '', regex=False), errors='coerce')\n\n    qty = to_num(df[qty_c])\n    price = to_num(df[price_c])\n    cost = to_num(df[cost_c])\n    unitm = to_num(df[unitm_c])\n    totalm = to_num(df[totalm_c])\n\n    # Exclude total rows\n    if name_c:\n        names = df[name_c].astype(str).str.lower()\n        mask_detail = ~names.str.contains('total')\n    else:\n        mask_detail = pd.Series([True]*len(df))\n\n    # Tolerance proportional to price\n    tol_unit = (0.001*price).fillna(0) + 0.01\n    tol_total = (0.001*price*qty).fillna(0) + 0.5\n\n    unit_ok = (unitm - (price - cost)).abs() <= tol_unit\n    total_ok = (totalm - (unitm * qty)).abs() <= tol_total\n\n    # Only evaluate rows where needed values are present\n    valid_rows = mask_detail & price.notna() & cost.notna() & unitm.notna() & qty.notna() & totalm.notna()\n\n    if valid_rows.sum() == 0:\n        return 0.0\n\n    row_score = ((unit_ok & total_ok) & valid_rows).sum() / valid_rows.sum()\n    return float(row_score)"}, {"type": "code", "name": "Margin Percentage Consistency and Bounds", "description": "Check that Margin % \u2248 Unit Margin / Unit Retail Price when price > 0, and that Margin % stays within [0, 1.0].", "weight": 1.5, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        # Choose projection sheet\n        proj = None\n        for s in xls.sheet_names:\n            sl = s.lower()\n            if ('projection' in sl) or ('year' in sl and '1' in sl) or ('summary' in sl):\n                proj = s\n                break\n        if proj is None:\n            proj = xls.sheet_names[0]\n        df = pd.read_excel(path, sheet_name=proj)\n    except Exception:\n        return 0.0\n\n    if df.empty:\n        return 0.0\n\n    df.columns = [str(c).strip().lower() for c in df.columns]\n    def find(colopts):\n        for o in colopts:\n            for c in df.columns:\n                if o in c:\n                    return c\n        return None\n\n    price_c = find(['unit retail price','retail price','unit price','price'])\n    unitm_c = find(['margin $ per unit','unit margin','gross margin per unit','margin per unit'])\n    pct_c = find(['margin %','margin percent','gm %','gross margin %'])\n    if any(c is None for c in [price_c, unitm_c, pct_c]):\n        return 0.0\n\n    def to_num(s):\n        return pd.to_numeric(pd.Series(s).astype(str).str.replace(r'[%,$()]', '', regex=True).str.replace(',', '', regex=False), errors='coerce')\n\n    price = to_num(df[price_c])\n    unitm = to_num(df[unitm_c])\n    pct = to_num(df[pct_c])\n\n    with np.errstate(divide='ignore', invalid='ignore'):\n        expected = unitm / price\n    # Only evaluate where price>0\n    mask = price > 0\n    if mask.sum() == 0:\n        return 0.0\n    tol = 0.01 + 0.005*expected.abs().fillna(0)\n    consistency = (pct - expected).abs() <= tol\n    bounds = (pct >= 0) & (pct <= 1.0)\n    ok = (consistency & bounds & mask).sum() / mask.sum()\n    return float(ok)"}, {"type": "code", "name": "Grand Total Gross Margin Matches Sum of Rows", "description": "Check that a visible Year 1 total gross margin equals the sum of row-level totals (excluding any total rows), within a small tolerance.", "weight": 2.0, "code": "import re\nimport pandas as pd\nimport numpy as np\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        proj = None\n        for s in xls.sheet_names:\n            sl = s.lower()\n            if ('projection' in sl) or ('year' in sl and '1' in sl) or ('summary' in sl):\n                proj = s\n                break\n        if proj is None:\n            proj = xls.sheet_names[0]\n        df = pd.read_excel(path, sheet_name=proj)\n    except Exception as e:\n        return 0.0, f\"Read error: {e}\"\n\n    if df.empty:\n        return 0.0, \"Projection sheet empty.\"\n\n    df.columns = [str(c).strip().lower() for c in df.columns]\n    def find(colopts):\n        for o in colopts:\n            for c in df.columns:\n                if o in c:\n                    return c\n        return None\n\n    name_c = find(['product name','product','item','name','sku'])\n    totalm_c = find(['total gross margin $','gross margin total','total margin','extended margin','total gm','total gross margin'])\n    if totalm_c is None:\n        return 0.0, \"Total gross margin column not found.\"\n\n    # Clean totals\n    total_col = pd.to_numeric(pd.Series(df[totalm_c]).astype(str).str.replace(r'[%,$()]', '', regex=True).str.replace(',', '', regex=False), errors='coerce')\n\n    # Exclude rows marked as total in name column\n    if name_c:\n        names = df[name_c].astype(str).str.lower()\n        detail_mask = ~names.str.contains('total')\n    else:\n        detail_mask = pd.Series([True]*len(df))\n\n    detail_sum = total_col[detail_mask].sum(skipna=True)\n\n    # Attempt to detect a reported total: either a row named 'total' or the maximum value if there's a dedicated total row\n    reported_total = None\n    if name_c:\n        total_rows = names.str.contains('total')\n        if total_rows.any():\n            reported_total = pd.to_numeric(pd.Series(df.loc[total_rows, totalm_c]).astype(str).str.replace(r'[%,$()]', '', regex=True).str.replace(',', '', regex=False), errors='coerce').sum(skipna=True)\n    if reported_total is None or (isinstance(reported_total, float) and np.isnan(reported_total)):\n        # Fallback: take the last non-null value if it seems like a summary row\n        reported_total = total_col.tail(5).max()\n\n    if reported_total is None or np.isnan(reported_total):\n        return 0.0, \"Could not locate a visible Year 1 total gross margin value.\"\n\n    tol = max(0.005*abs(detail_sum), 1.0)\n    ok = abs(reported_total - detail_sum) <= tol\n    score = 1.0 if ok else max(0.0, 1.0 - min(1.0, abs(reported_total - detail_sum)/(abs(detail_sum)+1e-6)))\n    fb = f\"Detail sum={detail_sum:.2f}, Reported total={reported_total:.2f}, tol={tol:.2f}.\"\n    return float(score), fb"}, {"type": "code", "name": "Consumables Included", "description": "Ensure at least one consumables line exists (e.g., mentions filter, lamp, bulb, cartridge, consumable, replacement).", "weight": 1.0, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        proj = None\n        for s in xls.sheet_names:\n            sl = s.lower()\n            if ('projection' in sl) or ('year' in sl and '1' in sl) or ('summary' in sl):\n                proj = s\n                break\n        if proj is None:\n            proj = xls.sheet_names[0]\n        df = pd.read_excel(path, sheet_name=proj)\n    except Exception:\n        return 0.0\n\n    if df.empty:\n        return 0.0\n\n    df.columns = [str(c).strip().lower() for c in df.columns]\n    name_col = None\n    for c in df.columns:\n        if any(k in c for k in ['product name','product','item','name','sku']):\n            name_col = c\n            break\n    if name_col is None:\n        return 0.0\n\n    names = df[name_col].astype(str).str.lower()\n    patterns = ['consumable','filter','lamp','bulb','cartridge','replacement']\n    has_consumable = any(names.str.contains(p, regex=False).any() for p in patterns)\n\n    return 1.0 if has_consumable else 0.0"}, {"type": "code", "name": "Tiered Pricing Definitions Present in Assumptions", "description": "Verify assumptions sheet contains a tiered pricing definition with two tiers (<1,000 and >=1,000) and indicates a discount at the higher tier.", "weight": 2.0, "code": "import re\nimport pandas as pd\n\ndef evaluate(workflow, context):\n    output = context.get_primary_output()\n    if not output or not output.is_spreadsheet:\n        return 0.0, \"No spreadsheet output.\"\n    try:\n        path = context.files.get_path(output.id)\n        xls = pd.ExcelFile(path)\n        # Find assumptions/pricing sheet\n        asn = None\n        for s in xls.sheet_names:\n            sl = s.lower()\n            if 'assumption' in sl or 'pricing' in sl:\n                asn = s\n                break\n        if asn is None:\n            # fallback: last sheet\n            asn = xls.sheet_names[-1]\n        df = pd.read_excel(path, sheet_name=asn, header=None)\n    except Exception as e:\n        return 0.0, f\"Read error: {e}\"\n\n    # Flatten all text\n    text = ' '.join(df.astype(str).stack().astype(str).tolist()).lower()\n\n    has_tier = ('< 1,000' in text or '< 1000' in text or 'less than 1,000' in text or 'under 1000' in text or 'below 1000' in text)\n    has_high = ('>= 1,000' in text or '>= 1000' in text or '1,000+' in text or '1000+' in text or '1k+' in text or 'more than 1,000' in text or 'over 1000')\n    mentions_discount = ('discount' in text or 'tier' in text)\n\n    comp = int(has_tier) + int(has_high) + int(mentions_discount)\n    score = comp/3.0\n    fb = f\"Tier low:<1000 present={has_tier}, high:>=1000 present={has_high}, discount mention={mentions_discount}.\"\n    return score, fb"}], "on_failure_action": "continue", "on_failure_score": 0.0}, {"name": "Stage 3 \u2014 Presentation and Executive Readiness", "description": "LLM judges the clarity, professionalism, and decision-usefulness of the spreadsheet for an executive audience.", "is_required": false, "max_points": 4.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Clarity, Organization, and Formatting", "description": "Assess if the workbook is cleanly formatted, easy to read, and clearly labels totals, assumptions, and pricing tiers.", "weight": 2.0, "judge_prompt": "Evaluate the workbook\u2019s presentation quality for an executive audience:\n- Are sheets/tables cleanly formatted (readable headers, aligned currency/percent formats, reasonable decimals)?\n- Are sections clearly labeled (Projection, Pricing & Assumptions, Tiered Pricing, Volume Assumptions)?\n- Is the Year 1 Total Gross Margin prominently displayed and easy to find?\n- Is it trivial to tie rows to the two devices and to consumables?\n\nScoring:\n- 2.0: Highly professional formatting, clear labels, currency/percent formats used consistently, total is prominent.\n- 1.0: Generally clear with minor formatting issues or slightly hidden totals.\n- 0.0: Sloppy or confusing formatting; difficult to locate key information.", "expectation": "A tidy, readable workbook with consistent number formats, clear headers, and obvious totals."}, {"type": "llm_judge", "name": "Executive Readiness and Decision Usefulness", "description": "Assess whether the model includes the right context and annotations to support VP-level decision-making.", "weight": 2.0, "judge_prompt": "Assess decision-usefulness for an executive:\n- Do assumptions clearly state the 2,000-unit Year 1 device volume and how it\u2019s allocated across the two products?\n- Is tier applicability understandable (which tier is triggered for each product given the allocation)?\n- Are pricing and cost inputs traceable to the Pricing email (even if the exact document is not provided), with notes or placeholders clearly marked?\n- Are any key assumptions or limitations noted (e.g., placeholder pricing, allocation rationale)?\n\nScoring:\n- 2.0: Clearly documents assumptions, tier applicability, and source references; ready for executive discussion.\n- 1.0: Mostly sufficient but missing one notable element (e.g., unclear allocation or source reference).\n- 0.0: Lacks essential annotations; hard to use for decision-making.", "expectation": "Concise assumptions, clear tie to pricing source, and explicit tier logic so an executive can quickly understand and approve."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
{"task_id": "5349dd7b-bf0a-4544-9a17-75b7013767e6", "rubric": {"category_name": "Carrier Flat-Rate Cost Analysis 2026 (Manufacturing - Shipping/Receiving)", "rationale": "Self-documenting, staged evaluation for an analytical Excel deliverable. Stage 1 (LLM) strictly enforces a verifiable Excel structure with specific sheets and tables so that Stage 2 code rules can deterministically check math and consistency. Stage 3 assesses professional quality and actionability for the shipping team.", "max_total_score": 10.0, "stages": [{"name": "Stage 1 \u2013 Shape Enforcement (Gate)", "description": "LLM-only gate that verifies the candidate produced a properly structured Excel workbook with the exact sheets and tables required to enable verification.", "is_required": true, "max_points": 4.0, "min_score_to_pass": 3.0, "rules": [{"type": "llm_judge", "name": "Structured Output Format Requirement", "description": "Output must be an Excel workbook with required sheets and clearly labeled tables per sheet to support verification of rates, volumes, projections, and recommendations.", "weight": 4.0, "judge_prompt": "You are checking only the FORMAT and STRUCTURE of the candidate\u2019s output. Do not judge correctness of numbers.\n\nConfirm the output is a valid Excel (.xlsx) file that contains the following sheets (names may vary slightly; accept close variants as long as intent is clear):\n\nRequired sheets and table structures:\n1) Rate Increases 2020-2025\n   - A clearly labeled table with columns including: Year, USPS, UPS, FedEx (percent changes). A Notes/Source column with URLs is expected.\n   - Rows present for each year: 2020, 2021, 2022, 2023, 2024, 2025.\n   - An \"Average 2020-2025\" row (or equivalent), and a \"2026 Estimate\" row/values derived from the average.\n\n2) Current Flat Rates\n   - A table listing currently published flat-rate prices for each carrier and package size actually offered.\n   - Columns should include: Carrier, Package Size (pak/pak, small box, medium box, large box, extra large box), Flat Rate (USD), Service/Program, Business Rate? (Yes/No), As-of Date, Source URL.\n   - Carriers/sizes not offered should be omitted or marked N/A and excluded from later comparisons.\n   - Only standard delivery speeds; business rates if available.\n\n3) 2026 Volume Projections\n   - A table with all five package sizes and projected units exactly as provided:\n     \u2022 Pak: 1000\n     \u2022 Small Box: 2300\n     \u2022 Medium Box: 2100\n     \u2022 Large Box: 540\n     \u2022 Extra Large Box: 120\n\n4) 2026 Projected Costs\n   - A calculation table with one row per (package size, carrier) that is actually offered.\n   - Columns should include: Package Size, Carrier, 2025 Rate (or Current/Base Rate), 2026 Increase %, 2026 Projected Rate, Projected Units, Total Cost 2026.\n   - Only standard delivery speeds; business rates if available.\n\n5) Recommendations\n   - One row per package size with: Package Size, Recommended Carrier, 2026 Projected Rate and/or Total Cost 2026, and a short rationale.\n   - Recommendations must exclude carriers that do not offer that size.\n\nOptional (nice-to-have):\n- Assumptions & Methodology (text with sources and approach).\n\nScoring (structure only):\n- 4.0: Excel format + all 5 required sheets present with clearly labeled tables and appropriate columns.\n- 3.2: All required sheets present but minor structural omissions (e.g., missing a minor column) OR only the optional sheet missing.\n- 2.4: One required sheet missing OR multiple notable structural omissions.\n- 0.0: Not an Excel file or multiple required sheets/tables missing.\n\nBe flexible with small naming differences (e.g., \"Projected\" vs \"Forecast\"), but the intent and tables must be clearly present. Do NOT evaluate numerical correctness; only verify the presence and structure needed for verification.", "expectation": "A well-structured Excel workbook with the five required sheets and clearly labeled tables enabling automated checks in Stage 2."}], "on_failure_action": "zero_category", "on_failure_score": 0.0}, {"name": "Stage 2 \u2013 Verification and Math Checks", "description": "Deterministic code checks for volumes, rate-years coverage, plausibility, projection math, and cost-minimizing recommendations.", "is_required": true, "max_points": 5.0, "min_score_to_pass": 2.5, "rules": [{"type": "code", "name": "Spreadsheet Readability and Core Sheets Present (Light Check)", "description": "Confirm the primary output is an Excel workbook and that core sheets are detectable. Light redundancy with Stage 1; prevents downstream crashes.", "weight": 0.5, "code": "import pandas as pd\\nimport re\\n\\ndef evaluate(workflow, context):\\n    try:\\n        output = context.get_primary_output()\\n        if not output or not output.is_spreadsheet:\\n            return 0.0, 'Primary output missing or not a spreadsheet.'\\n\\n        # Get sheet names\\n        path = context.files.get_path(output.id)\\n        xls = pd.ExcelFile(path)\\n        sheets = [s.lower() for s in xls.sheet_names]\\n\\n        # Look for key sheets via fuzzy matching\\n        def find_any(keys):\\n            return any(any(k in s for k in keys) for s in sheets)\\n\\n        has_rates = find_any(['increase', 'rate', 'gri'])\\n        has_current = find_any(['current flat', 'flat rate', 'published rate'])\\n        has_volume = find_any(['volume', 'projection', '2026'])\\n        has_projected = find_any(['projected cost', 'projection', '2026'])\\n        has_reco = find_any(['recommend'])\\n\\n        found = sum([has_rates, has_current, has_volume, has_projected, has_reco])\\n        score = (found / 5.0) * 0.5\\n        return score, f'Detected {found}/5 core sheets.'\\n    except Exception as e:\\n        return 0.0, f'Error reading spreadsheet: {e}'"}, {"type": "code", "name": "2026 Volume Projections Match", "description": "Verify the five package sizes exist with the exact projected unit volumes specified.", "weight": 1.0, "code": "import pandas as pd\\nimport numpy as np\\nimport re\\n\\nREQ = {\\n    'pak': 1000,\\n    'small box': 2300,\\n    'medium box': 2100,\\n    'large box': 540,\\n    'extra large box': 120\\n}\\n\\ndef norm_size(s):\\n    if not isinstance(s, str):\\n        s = str(s)\\n    t = s.strip().lower()\\n    # normalize common variants\\n    t = t.replace('pkg', 'pak').replace('pack', 'pak').replace('pak', 'pak')\\n    t = t.replace('x-large', 'extra large').replace('xl', 'extra large')\\n    t = re.sub(r'\\s+', ' ', t)\\n    if 'pak' in t or 'pack/pak' in t or t == 'pack':\\n        return 'pak'\\n    if 'small' in t and 'box' in t:\\n        return 'small box'\\n    if 'medium' in t and 'box' in t:\\n        return 'medium box'\\n    if 'large' in t and 'extra' not in t and 'box' in t:\\n        return 'large box'\\n    if ('extra' in t and 'large' in t) or ('x large' in t) or ('x-large' in t):\\n        return 'extra large box'\\n    return t\\n\\ndef evaluate(workflow, context):\\n    try:\\n        out = context.get_primary_output()\\n        if not out or not out.is_spreadsheet:\\n            return 0.0, 'No spreadsheet output.'\\n        path = context.files.get_path(out.id)\\n        xls = pd.ExcelFile(path)\\n        # Find volume sheet\\n        vol_sheet = None\\n        for s in xls.sheet_names:\\n            sl = s.lower()\\n            if ('volume' in sl or 'projection' in sl or '2026' in sl) and ('recommend' not in sl) and ('projected cost' not in sl):\\n                vol_sheet = s\\n                break\\n        if not vol_sheet:\\n            return 0.0, 'Volume projections sheet not found.'\\n        df = pd.read_excel(path, sheet_name=vol_sheet)\\n        if df.empty:\\n            return 0.0, 'Volume sheet is empty.'\\n        # Identify columns\\n        cols = [str(c).lower() for c in df.columns]\\n        # Package size column\\n        pkg_col = None\\n        for c in df.columns:\\n            cl = str(c).lower()\\n            if ('package' in cl and 'size' in cl) or ('size' in cl) or ('package' in cl):\\n                pkg_col = c\\n                break\\n        # Units column\\n        units_col = None\\n        for c in df.columns:\\n            cl = str(c).lower()\\n            if ('unit' in cl) or ('volume' in cl) or ('qty' in cl) or ('quantity' in cl) or ('projected' in cl):\\n                units_col = c\\n                break\\n        if pkg_col is None or units_col is None:\\n            return 0.0, 'Could not identify package size and units columns.'\\n        # Build mapping\\n        found = {}\\n        for _, r in df[[pkg_col, units_col]].dropna().iterrows():\\n            size = norm_size(r[pkg_col])\\n            try:\\n                val = float(r[units_col])\\n            except Exception:\\n                continue\\n            found[size] = int(round(val))\\n        # Compare\\n        checks = []\\n        mismatches = []\\n        for k, v in REQ.items():\\n            fv = found.get(k)\\n            ok = (fv == v)\\n            checks.append(ok)\\n            if not ok:\\n                mismatches.append(f\"{k}: expected {v}, found {fv}\")\\n        score = (sum(checks) / 5.0) * 1.0\\n        fb = 'All volumes match.' if all(checks) else 'Mismatches: ' + '; '.join(mismatches)\\n        return score, fb\\n    except Exception as e:\\n        return 0.0, f'Error validating volumes: {e}'"}, {"type": "code", "name": "Rate Increases Coverage and Plausibility (2020\u20132025 + 2026 Estimate)", "description": "Check that 2020\u20132025 per-carrier rates exist, are numeric and plausible, and that an average and/or 2026 Estimate is present and consistent.", "weight": 1.5, "code": "import pandas as pd\\nimport numpy as np\\nimport re\\n\\nCARRIERS = ['usps','ups','fedex']\\nYEARS = [2020,2021,2022,2023,2024,2025]\\n\\ndef parse_pct(x):\\n    if x is None or (isinstance(x, float) and np.isnan(x)):\\n        return None\\n    if isinstance(x, str):\\n        s = x.strip().lower().replace('%','')\\n        s = s.replace('+','').replace(',','')\\n        try:\\n            val = float(s)\\n        except:\\n            return None\\n    elif isinstance(x, (int,float)):\\n        val = float(x)\\n    else:\\n        return None\\n    # Accept both 6.9 and 0.069 formats\\n    if abs(val) > 1.0:\\n        val = val / 100.0\\n    return val\\n\\ndef evaluate(workflow, context):\\n    try:\\n        out = context.get_primary_output()\\n        if not out or not out.is_spreadsheet:\\n            return 0.0, 'No spreadsheet output.'\\n        path = context.files.get_path(out.id)\\n        xls = pd.ExcelFile(path)\\n        rate_sheet = None\\n        for s in xls.sheet_names:\\n            sl = s.lower()\\n            if ('increase' in sl or 'rate' in sl or 'gri' in sl):\\n                rate_sheet = s\\n                break\\n        if not rate_sheet:\\n            return 0.0, 'Rate increases sheet not found.'\\n        df = pd.read_excel(path, sheet_name=rate_sheet)\\n        if df.empty:\\n            return 0.0, 'Rate increases sheet is empty.'\\n        # Find year column\\n        year_col = None\\n        for c in df.columns:\\n            cl = str(c).lower()\\n            if 'year' in cl:\\n                year_col = c\\n                break\\n        if year_col is None:\\n            # try first column as year\\n            year_col = df.columns[0]\\n        # Map carrier columns\\n        carr_cols = {}\\n        for c in df.columns:\\n            cl = str(c).lower()\\n            for carr in CARRIERS:\\n                if carr in cl and carr not in carr_cols:\\n                    carr_cols[carr] = c\\n        if len(carr_cols) == 0:\\n            return 0.0, 'Carrier columns not found on rate sheet.'\\n        # Filter rows for target years or summary rows\\n        df['_y'] = df[year_col].astype(str).str.lower()\\n        # Collect yearly values\\n        coverage_ok = 0\\n        total_needed = len(CARRIERS) * len(YEARS)\\n        plausible_ok = 0\\n        vals_by_carr = {c: [] for c in CARRIERS}\\n        for y in YEARS:\\n            mask = df['_y'].str.contains(str(y)) & ~df['_y'].str.contains('avg|average|estimate')\\n            if not mask.any():\\n                continue\\n            row = df[mask].iloc[0]\\n            for carr, col in carr_cols.items():\\n                v = parse_pct(row.get(col))\\n                if v is not None:\\n                    coverage_ok += 1\\n                    if -0.01 <= v <= 0.20:\\n                        plausible_ok += 1\\n                    vals_by_carr[carr].append(v)\\n        # Compute averages\\n        avgs = {}\\n        for carr in CARRIERS:\\n            arr = [v for v in vals_by_carr[carr] if v is not None]\\n            if len(arr) > 0:\\n                avgs[carr] = float(np.mean(arr))\\n        # Look for 2026 estimate row\\n        est_ok = 0\\n        mask_26 = df['_y'].str.contains('2026') | df['_y'].str.contains('estimate')\\n        if mask_26.any():\\n            row = df[mask_26].iloc[0]\\n            for carr, col in carr_cols.items():\\n                est = parse_pct(row.get(col))\\n                if carr in avgs and est is not None:\\n                    if abs(est - avgs[carr]) <= 0.003:  # ~0.3 percentage point tolerance\\n                        est_ok += 1\\n        # Scoring components\\n        coverage_score = min(coverage_ok / total_needed, 1.0) if total_needed > 0 else 0.0\\n        plaus_score = min(plausible_ok / total_needed, 1.0) if total_needed > 0 else 0.0\\n        est_score = (est_ok / len(CARRIERS)) if len(CARRIERS) > 0 else 0.0\\n        # weights within this rule: coverage 0.6, plausibility 0.4, estimate consistency 0.5 -> capped at 1.5\\n        raw = 0.6*coverage_score + 0.4*plaus_score + 0.5*est_score\\n        raw = min(raw, 1.5)\\n        return raw, f'Coverage: {coverage_ok}/{total_needed}, Plausible: {plausible_ok}/{total_needed}, Estimate matches: {est_ok}/{len(CARRIERS)}.'\\n    except Exception as e:\\n        return 0.0, f'Error validating rate increases: {e}'"}, {"type": "code", "name": "Projection Math and Consistency", "description": "Verify that 2026 Projected Rate = 2025 Rate \u00d7 (1 + Increase %) and Total Cost 2026 = Projected Units \u00d7 2026 Projected Rate; also that Increase % aligns with the average from the rate sheet.", "weight": 1.5, "code": "import pandas as pd\\nimport numpy as np\\nimport re\\n\\nCARRIERS = ['usps','ups','fedex']\\n\\ndef parse_pct(x):\\n    if x is None or (isinstance(x, float) and np.isnan(x)):\\n        return None\\n    if isinstance(x, str):\\n        s = x.strip().lower().replace('%','')\\n        s = s.replace('+','').replace(',','')\\n        try:\\n            val = float(s)\\n        except:\\n            return None\\n    elif isinstance(x, (int,float)):\\n        val = float(x)\\n    else:\\n        return None\\n    if abs(val) > 1.0:\\n        val = val / 100.0\\n    return val\\n\\ndef find_rate_avgs(path):\\n    xls = pd.ExcelFile(path)\\n    rate_sheet = None\\n    for s in xls.sheet_names:\\n        sl = s.lower()\\n        if ('increase' in sl or 'rate' in sl or 'gri' in sl):\\n            rate_sheet = s\\n            break\\n    if not rate_sheet:\\n        return {}\\n    df = pd.read_excel(path, sheet_name=rate_sheet)\\n    if df.empty:\\n        return {}\\n    year_col = None\\n    for c in df.columns:\\n        if 'year' in str(c).lower():\\n            year_col = c\\n            break\\n    if year_col is None:\\n        year_col = df.columns[0]\\n    df['_y'] = df[year_col].astype(str).str.lower()\\n    carr_cols = {}\\n    for c in df.columns:\\n        cl = str(c).lower()\\n        for carr in CARRIERS:\\n            if carr in cl and carr not in carr_cols:\\n                carr_cols[carr] = c\\n    years = ['2020','2021','2022','2023','2024','2025']\\n    vals = {c: [] for c in CARRIERS}\\n    for y in years:\\n        m = df['_y'].str.contains(y) & ~df['_y'].str.contains('avg|average|estimate')\\n        if m.any():\\n            row = df[m].iloc[0]\\n            for carr, col in carr_cols.items():\\n                v = parse_pct(row.get(col))\\n                if v is not None:\\n                    vals[carr].append(v)\\n    avgs = {c: float(np.mean(v)) for c, v in vals.items() if len(v)>0}\\n    return avgs\\n\\ndef evaluate(workflow, context):\\n    try:\\n        out = context.get_primary_output()\\n        if not out or not out.is_spreadsheet:\\n            return 0.0, 'No spreadsheet output.'\\n        path = context.files.get_path(out.id)\\n        xls = pd.ExcelFile(path)\\n        # Find projected costs sheet\\n        proj_sheet = None\\n        for s in xls.sheet_names:\\n            sl = s.lower()\\n            if ('projected cost' in sl) or ('projection' in sl and 'recommend' not in sl):\\n                proj_sheet = s\\n                break\\n        if not proj_sheet:\\n            return 0.0, 'Projected costs sheet not found.'\\n        df = pd.read_excel(path, sheet_name=proj_sheet)\\n        if df.empty:\\n            return 0.0, 'Projected costs sheet is empty.'\\n        # Identify columns\\n        def pick(col_names, must_have):\\n            for c in df.columns:\\n                cl = str(c).lower()\\n                if all(m in cl for m in must_have):\\n                    return c\\n            for name in col_names:\\n                for c in df.columns:\\n                    if name in str(c).lower():\\n                        return c\\n            return None\\n\\n        col_pkg = pick([], ['package']) or pick([], ['size'])\\n        col_car = pick(['carrier'], ['carrier'])\\n        col_base = pick(['2025','base','current'], ['rate'])\\n        col_incr = pick(['increase','est','avg'], ['%']) or pick(['increase','%'], ['increase'])\\n        col_proj_rate = pick(['2026','projected'], ['rate'])\\n        col_units = pick(['unit','volume','qty','quantity','projected'], [])\\n        col_total = pick(['total','2026'], [])\\n\\n        required_cols = [col_pkg, col_car, col_base, col_incr, col_proj_rate, col_units, col_total]\\n        if any(c is None for c in required_cols):\\n            return 0.0, 'Missing required columns on projected costs sheet.'\\n        # Carrier average increases\\n        avgs = find_rate_avgs(path)\\n        # Evaluate rows\\n        n = 0\\n        pass_incr = 0\\n        pass_rate = 0\\n        pass_total = 0\\n        for _, r in df.iterrows():\\n            try:\\n                n += 1\\n                base = float(r[col_base])\\n                incr = parse_pct(r[col_incr])\\n                proj_rate = float(r[col_proj_rate])\\n                units = float(r[col_units])\\n                total = float(r[col_total])\\n                carr = str(r[col_car]).strip().lower()\\n                # Increase consistency (if available)\\n                ok_incr = True\\n                for key in avgs.keys():\\n                    if key in carr:\\n                        if incr is None:\\n                            ok_incr = False\\n                        else:\\n                            ok_incr = abs(incr - avgs[key]) <= 0.003\\n                        break\\n                # Math checks\\n                ok_rate = abs(proj_rate - (base * (1.0 + (incr if incr is not None else 0.0)))) <= 0.02\\n                ok_total = abs(total - (proj_rate * units)) <= max(0.02*total, 1.0)\\n                pass_incr += 1 if ok_incr else 0\\n                pass_rate += 1 if ok_rate else 0\\n                pass_total += 1 if ok_total else 0\\n            except Exception:\\n                continue\\n        if n == 0:\\n            return 0.0, 'No rows to evaluate in projected costs.'\\n        frac_incr = pass_incr / n\\n        frac_rate = pass_rate / n\\n        frac_total = pass_total / n\\n        # weights within this rule: increase consistency 0.5, projected rate formula 0.5, total cost formula 0.5 -> total 1.5\\n        score = 0.5*frac_incr + 0.5*frac_rate + 0.5*frac_total\\n        score = min(score, 1.5)\\n        fb = f'Rows checked: {n}; increase ok: {pass_incr}; rate ok: {pass_rate}; total ok: {pass_total}.'\\n        return score, fb\\n    except Exception as e:\\n        return 0.0, f'Error validating projections: {e}'"}, {"type": "code", "name": "Recommendations Are Cost-Minimizing", "description": "Check that for each package size, the recommended carrier in the Recommendations sheet matches the lowest Total Cost 2026 among carriers that actually offer that size.", "weight": 0.5, "code": "import pandas as pd\\nimport numpy as np\\nimport re\\n\\ndef norm_size(s):\\n    if not isinstance(s, str):\\n        s = str(s)\\n    t = s.strip().lower()\\n    t = t.replace('pkg', 'pak').replace('pack', 'pak')\\n    t = t.replace('x-large', 'extra large').replace('xl', 'extra large')\\n    t = re.sub(r'\\s+', ' ', t)\\n    if 'pak' in t:\\n        return 'pak'\\n    if 'small' in t and 'box' in t:\\n        return 'small box'\\n    if 'medium' in t and 'box' in t:\\n        return 'medium box'\\n    if 'large' in t and 'extra' not in t and 'box' in t:\\n        return 'large box'\\n    if ('extra' in t and 'large' in t) or ('x large' in t) or ('x-large' in t):\\n        return 'extra large box'\\n    return t\\n\\ndef evaluate(workflow, context):\\n    try:\\n        out = context.get_primary_output()\\n        if not out or not out.is_spreadsheet:\\n            return 0.0, 'No spreadsheet output.'\\n        path = context.files.get_path(out.id)\\n        xls = pd.ExcelFile(path)\\n        # Find projected costs sheet\\n        proj_sheet = None\\n        reco_sheet = None\\n        for s in xls.sheet_names:\\n            sl = s.lower()\\n            if ('projected cost' in sl) or ('projection' in sl and 'recommend' not in sl):\\n                proj_sheet = s if proj_sheet is None else proj_sheet\\n            if 'recommend' in sl:\\n                reco_sheet = s\\n        if not proj_sheet or not reco_sheet:\\n            return 0.0, 'Missing projected costs or recommendations sheet.'\\n        dproj = pd.read_excel(path, sheet_name=proj_sheet)\\n        dreco = pd.read_excel(path, sheet_name=reco_sheet)\\n        if dproj.empty or dreco.empty:\\n            return 0.0, 'Projected costs or recommendations sheet is empty.'\\n        # Identify columns\\n        def pick(df, col_names, must_have):\\n            for c in df.columns:\\n                cl = str(c).lower()\\n                if all(m in cl for m in must_have):\\n                    return c\\n            for name in col_names:\\n                for c in df.columns:\\n                    if name in str(c).lower():\\n                        return c\\n            return None\\n        p_pkg = pick(dproj, [], ['package']) or pick(dproj, [], ['size'])\\n        p_car = pick(dproj, ['carrier'], ['carrier'])\\n        p_total = pick(dproj, ['total','2026'], [])\\n        r_pkg = pick(dreco, [], ['package']) or pick(dreco, [], ['size'])\\n        r_car = pick(dreco, ['recommended','carrier'], [])\\n        if any(c is None for c in [p_pkg, p_car, p_total, r_pkg, r_car]):\\n            return 0.0, 'Required columns not found for recommendation check.'\\n        # Build min-cost map from projected costs\\n        mins = {}\\n        for _, r in dproj[[p_pkg, p_car, p_total]].dropna().iterrows():\\n            size = norm_size(r[p_pkg])\\n            carr = str(r[p_car]).strip().lower()\\n            try:\\n                total = float(r[p_total])\\n            except Exception:\\n                continue\\n            if size not in mins or total < mins[size][1]:\\n                mins[size] = (carr, total)\\n        # Compare with recommendations\\n        n = 0\\n        ok = 0\\n        mism = []\\n        for _, r in dreco[[r_pkg, r_car]].dropna().iterrows():\\n            size = norm_size(r[r_pkg])\\n            rec = str(r[r_car]).strip().lower()\\n            if size in mins:\\n                n += 1\\n                if mins[size][0] in rec:\\n                    ok += 1\\n                else:\\n                    mism.append(f\"{size}: rec {rec} vs min {mins[size][0]}\")\\n        if n == 0:\\n            return 0.0, 'No comparable recommendation rows found.'\\n        score = (ok / n) * 0.5\\n        fb = 'All recommendations align with min cost.' if ok == n else 'Mismatches: ' + '; '.join(mism)\\n        return score, fb\\n    except Exception as e:\\n        return 0.0, f'Error checking recommendations: {e}'"}], "on_failure_action": "skip_remaining", "on_failure_score": 0.0}, {"name": "Stage 3 \u2013 Professional Quality and Actionability", "description": "LLM judge assesses clarity, presentation, and usefulness for a shipping team implementing 2026 decisions.", "is_required": false, "max_points": 1.0, "min_score_to_pass": 0.0, "rules": [{"type": "llm_judge", "name": "Professional Presentation and Actionable Insight", "description": "Evaluate whether the workbook is easy to understand, professionally presented, and provides clear, actionable guidance to the shipping team.", "weight": 1.0, "judge_prompt": "Assess the overall professional quality and actionability of the Excel workbook (do not re-check structure already covered in Stage 1 or math already covered in Stage 2):\\n- Clarity and labeling: sheet names, column headers, units ($, %), and notes are clear.\\n- Readability: tables are clean, with consistent formatting, currency/percentage formats applied, and no confusing clutter.\\n- Sources and assumptions: sources appear credible (e.g., carrier sites) and assumptions are stated concisely.\\n- Actionability: the Recommendations sheet makes it easy for the shipping team to implement (clear winners by size, ties explained, excluded sizes noted).\\n- Appropriateness: uses standard delivery speeds and business rates (when available) without upsell options.\\n\\nScoring:\\n- 1.0: Highly clear, professional, and immediately actionable.\\n- 0.7: Generally good with minor clarity/formatting gaps.\\n- 0.4: Usable but requires effort to interpret or is inconsistently formatted.\\n- 0.0: Confusing, unprofessional, or not practically actionable.", "expectation": "A clean, clearly labeled, and immediately actionable workbook that a shipping team can use to assign carriers by package size for 2026."}], "on_failure_action": "continue", "on_failure_score": 0.0}]}}
