"""
Rubric Generation Schemas for LLM-based decomposition.

These are pure data holders used when the manager agent generates rubrics
from natural language preferences. They are immediately converted to
WorkflowRubric objects for execution.

Design: Keep generation format separate from execution format for clarity.
"""

from typing import Literal

from pydantic import BaseModel, Field, field_validator


class CodeRule(BaseModel):
    """Code-based evaluation rule generated by LLM.

    The LLM generates Python source code as a string, which will be
    compiled into an executable function during conversion.
    """

    type: Literal["code"] = "code"
    name: str = Field(description="Short name for this criterion")
    description: str = Field(description="What this rule evaluates")
    weight: float = Field(
        default=1.0,
        ge=0.0,
        description="Relative importance (non-negative)",
    )
    code: str = Field(
        description=(
            "Python source defining: evaluate(task_input: str, candidate_output: str) -> float\n"
            "Must return score in [0, 1]. Can import: re, numpy as np, pandas as pd"
        ),
    )


class LLMJudgeRule(BaseModel):
    """LLM-judge evaluation rule generated by LLM.

    Uses another LLM call to evaluate the criterion qualitatively.
    """

    type: Literal["llm_judge"] = "llm_judge"
    name: str = Field(description="Short name for this criterion")
    description: str = Field(description="What this rule evaluates")
    weight: float = Field(
        default=1.0,
        ge=0.0,
        description="Relative importance (non-negative)",
    )
    judge_prompt: str = Field(
        description="Prompt template for the judge LLM to evaluate this criterion",
    )
    expectation: str | None = Field(
        default=None,
        description="Optional guidance describing ideal outcome",
    )


class ManagerAgentGeneratedRubric(BaseModel):
    """Complete rubric specification generated by LLM.

    This is the structured output from the rubric decomposition manager.
    It contains rules as strings (code or prompts) that will be converted
    to executable WorkflowRubric objects.

    Design: This is ephemeral - always convert to WorkflowRubric immediately.
    """

    rationale: str | None = Field(
        default=None,
        description="LLM's reasoning for this rubric design",
    )
    rubric_id: str = Field(
        default="auto_generated",
        description="Identifier for this rubric set",
    )
    rules: list[CodeRule | LLMJudgeRule] = Field(
        min_length=1,
        description="Evaluation rules (must have at least one)",
    )

    @field_validator("rules")
    @classmethod
    def validate_rules(
        cls, value: list[CodeRule | LLMJudgeRule]
    ) -> list[CodeRule | LLMJudgeRule]:
        """Ensure rules are non-empty and weights are valid."""
        if not value:
            raise ValueError("Rubric must contain at least one rule")

        total_weight = sum(rule.weight for rule in value)
        if total_weight <= 0.0:
            raise ValueError("Rule weights must sum to a positive value")

        return value

    def get_total_weight(self) -> float:
        """Get sum of all rule weights."""
        return sum(rule.weight for rule in self.rules)

    def to_summary_dict(self) -> dict:
        """Convert to summary dict for logging with full rule details.

        Returns:
            Dict with rubric_id, rationale, rules (with code/prompts), etc.
        """
        from typing import Any

        rules_list: list[dict[str, Any]] = []

        for rule in self.rules:
            rule_dict: dict[str, Any] = {
                "name": rule.name,
                "type": rule.type,
                "weight": rule.weight,
                "description": rule.description,
            }

            if isinstance(rule, CodeRule):
                rule_dict["code"] = rule.code
                rule_dict["code_length"] = len(rule.code)

            elif isinstance(rule, LLMJudgeRule):
                rule_dict["judge_prompt"] = rule.judge_prompt
                rule_dict["prompt_length"] = len(rule.judge_prompt)
                rule_dict["expectation"] = rule.expectation

            rules_list.append(rule_dict)

        return {
            "rubric_id": self.rubric_id,
            "rationale": self.rationale,
            "total_rules": len(self.rules),
            "total_weight": self.get_total_weight(),
            "rules": rules_list,
        }
